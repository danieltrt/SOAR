file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport io\nimport os\n\nfrom setuptools import find_packages, setup\n\nroot = os.path.abspath(os.path.dirname(__file__))\n\n# required packages for NLP Architect\nwith open(""requirements.txt"") as fp:\n    reqs = []\n    for r in fp.readlines():\n        if ""#"" in r:\n            continue\n        reqs.append(r)\n    install_requirements = reqs\n\neverything = [\n    ""tensorflow_hub"",\n    ""elasticsearch"",\n    ""wordfreq"",\n    ""newspaper3k"",\n    ""pywikibot"",\n    ""num2words"",\n    ""bokeh"",\n    ""pandas"",\n    ""hyperopt"",\n    ""termcolor"",\n]\n\ndev = [\n    ""sphinx==1.8.5"",\n    ""sphinx_rtd_theme"",\n    ""flake8-html"",\n    ""black"",\n    ""pep8"",\n    ""flake8"",\n    ""pytest"",\n    ""pytest-cov"",\n    ""pytest-mock"",\n    ""pytest-xdist"",\n    ""pylint"",\n]\nextras = {""all"": everything, ""dev"": dev}\n\n# read official README.md\nwith open(""README.md"", encoding=""utf8"") as fp:\n    long_desc = fp.read()\n\nwith io.open(os.path.join(root, ""nlp_architect"", ""version.py""), encoding=""utf8"") as f:\n    version_f = {}\n    exec(f.read(), version_f)\n    version = version_f[""NLP_ARCHITECT_VERSION""]\n\nsetup(\n    name=""nlp-architect"",\n    version=version,\n    description=""Intel AI Lab NLP and NLU research model library"",\n    long_description=long_desc,\n    long_description_content_type=""text/markdown"",\n    keywords=""NLP NLU deep learning natural language processing tensorflow pytorch"",\n    author=""Intel AI Lab"",\n    author_email=""nlp_architect@intel.com"",\n    url=""https://github.com/NervanaSystems/nlp-architect"",\n    license=""Apache 2.0"",\n    python_requires="">=3.6.*"",\n    packages=find_packages(\n        exclude=[""tests.*"", ""tests"", ""server.*"", ""server"", ""examples.*"", ""examples"", ""solutions.*"", ""solutions""]\n    ),\n    install_requires=install_requirements,\n    extras_require=extras,\n    scripts=[""nlp_architect/nlp-train"", ""nlp_architect/nlp-inference""],\n    include_package_data=True,\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""Intended Audience :: End Users/Desktop"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Topic :: Scientific/Engineering"",\n        ""Topic :: Scientific/Engineering :: "" + ""Artificial Intelligence"",\n        ""Topic :: Software Development :: Libraries"",\n        ""Topic :: Software Development :: Libraries :: "" + ""Python Modules"",\n        ""Topic :: Scientific/Engineering :: Information Analysis"",\n        ""Environment :: Console"",\n        ""Environment :: Web Environment"",\n        ""Operating System :: POSIX"",\n        ""Operating System :: MacOS :: MacOS X"",\n    ],\n)\n'"
examples/__init__.py,0,b''
nlp_architect/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom os import path\nfrom pathlib import Path\nimport logging\n\nlogging.basicConfig(format=""%(asctime)s %(levelname)s %(message)s"", level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nLIBRARY_PATH = Path(path.realpath(__file__)).parent\nLIBRARY_ROOT = LIBRARY_PATH.parent\nLIBRARY_OUT = Path(Path.home()) / ""nlp-architect"" / ""cache""\nLIBRARY_DATASETS = LIBRARY_ROOT / ""datasets""\n\ntry:\n    # Capirca uses Google\'s abseil-py library, which uses a Google-specific\n    # wrapper for logging. That wrapper will write a warning to sys.stderr if\n    # the Google command-line flags library has not been initialized.\n    #\n    # https://github.com/abseil/abseil-py/blob/pypi-v0.7.1/absl/logging/__init__.py#L819-L825\n    #\n    # This is not right behavior for Python code that is invoked outside of a\n    # Google-authored main program. Use knowledge of abseil-py to disable that\n    # warning; ignore and continue if something goes wrong.\n    import absl.logging\n\n    # https://github.com/abseil/abseil-py/issues/99\n    logging.root.removeHandler(absl.logging._absl_handler)\n    # https://github.com/abseil/abseil-py/issues/102\n    absl.logging._warn_preinit_stderr = False\n    absl.logging.set_verbosity(""info"")\n    absl.logging.set_stderrthreshold(""info"")\nexcept Exception:\n    pass\n'"
nlp_architect/version.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nMAJOR_V, MINOR_V, PATCH_V, STAGE = 0, 5, 4, """"\n\n\ndef nlp_architect_version():\n    if PATCH_V != 0:\n        v = ""{}.{}.{}"".format(MAJOR_V, MINOR_V, PATCH_V)\n    else:\n        v = ""{}.{}"".format(MAJOR_V, MINOR_V)\n    if len(STAGE) != 0:\n        v += "".{}"".format(STAGE)\n    return v\n\n\nNLP_ARCHITECT_VERSION = nlp_architect_version()\n'"
scripts/server.py,0,"b'#! /usr/bin/env python\n# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nfrom subprocess import run\n\nfrom nlp_architect import LIBRARY_PATH\n\n\ndef run_cmd(command):\n    return run(command.split(), shell=False)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-p"", type=int, default=8000, help=""server port"")\n    args = parser.parse_args()\n    port = args.p\n    serve_file = LIBRARY_PATH / \'server\' / \'serve.py\'\n    cmd_str = \'hug -p {} -f {}\'.format(port, serve_file)\n    print(\'Starting NLP Architect demo server\')\n    run_cmd(cmd_str)\n'"
server/__init__.py,0,b''
server/serve.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=c-extension-no-member\n"""""" REST Server to respond to different API requests """"""\nimport gzip\nimport json\nfrom os import path\n\nimport hug\nfrom falcon import status_codes\n\nfrom service import Service, parse_headers, format_response\n\nservices = {}\n\napi = hug.API(__name__)\napi.http.add_middleware(hug.middleware.CORSMiddleware(api, max_age=10))\n\n\ndef prefetch_models():\n    models = [\'bist\', \'ner\', \'intent_extraction\']\n    for model in models:\n        services[model] = Service(model)\n\n\n@hug.get(\'/comprehension_paragraphs\')\ndef get_paragraphs():\n    if not services[\'machine_comprehension\']:\n        services[\'machine_comprehension\'] = Service(\'machine_comprehension\')\n    return services[\'machine_comprehension\'].get_paragraphs()\n\n\n# pylint: disable=inconsistent-return-statements\n@hug.post()\ndef inference(request, body, response):\n    """"""Makes an inference to a certain model""""""\n    print(body)\n    if request.headers.get(\'CONTENT-TYPE\') == \'application/gzip\':\n        try:\n            original_data = gzip.decompress(request.stream.read())\n            input_docs = json.loads(str(original_data, \'utf-8\'))[""docs""]\n            model_name = json.loads(str(original_data, \'utf-8\'))[""model_name""]\n        except Exception:\n            response.status = hug.HTTP_500\n            return {\'status\': \'unexpected gzip error\'}\n    elif request.headers.get(\'CONTENT-TYPE\') == \'application/json\':\n        if isinstance(body, str):\n            body = json.loads(body)\n        model_name = body.get(\'model_name\')\n        input_docs = body.get(\'docs\')\n    else:\n        response.status = status_codes.HTTP_400\n        return {\'status\': \'Content-Type header must be application/json or application/gzip\'}\n    if not model_name:\n        response.status = status_codes.HTTP_400\n        return {\'status\': \'model_name is required\'}\n    # If we\'ve already initialized it, no use in reinitializing\n    if not services.get(model_name):\n        services[model_name] = Service(model_name)\n    if not isinstance(input_docs, list):  # check if it\'s an array instead\n        response.status = status_codes.HTTP_400\n        return {\'status\': \'request not in proper format \'}\n    headers = parse_headers(request.headers)\n    parsed_doc = services[model_name].get_service_inference(input_docs, headers)\n    resp_format = request.headers[""RESPONSE-FORMAT""]\n    ret = format_response(resp_format, parsed_doc)\n    if request.headers.get(\'CONTENT-TYPE\') == \'application/gzip\':\n        response.content_type = resp_format\n        response.body = ret\n        # no return due to the fact that hug seems to assume json type upon return\n    else:\n        return ret\n\n\n@hug.static(\'/\')\ndef static():\n    """"""Statically serves a directory to client""""""\n    return [path.join(path.dirname(path.realpath(__file__)), \'angular-ui/dist/angular-ui\')]\n    # return [os.path.realpath(os.path.join(\'./\', \'server/angular-ui/dist/angular-ui\'))]\n\n\n@hug.get([\'/home\', \'/visual/{page}\', \'/annotate/{page}\', \'/machine_comprehension\'],\n         output=hug.output_format.file)\ndef get_index():\n    index = path.join(path.dirname(path.realpath(__file__)),\n                      \'angular-ui/dist/angular-ui/index.html\')\n    return index\n\n\nprefetch_models()\n'"
server/service.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n"""""" Service file used to import different features for server """"""\nimport json\nimport logging\nimport os.path\nfrom importlib import import_module\n\nfrom nlp_architect.utils.io import gzip_str\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef format_response(resp_format, parsed_doc):\n    """"""\n    Transform string of server\'s response to the requested format\n\n    Args:\n        resp_format(str): the desired response format\n        parsed_doc: the server\'s response\n\n    Returns:\n        formatted response\n    """"""\n    logger.info(\'preparing response JSON\')\n    ret = None\n    if (resp_format == ""json"") or (\'json\' in resp_format) or (not resp_format):\n        # if not specified resp_format then default is json\n        ret = parsed_doc\n    if resp_format == ""gzip"" or \'gzip\' in resp_format:\n        ret = gzip_str(parsed_doc)\n    return ret\n\n\ndef parse_headers(req_headers):\n    """"""\n    Load headers from request to dictionary\n\n    Args:\n        req_headers (dict): the request\'s headers\n\n    Returns:\n        dict: dictionary hosting the request headers\n    """"""\n    headers_lst = [""CONTENT-TYPE"", ""CONTENT-ENCODING"", ""RESPONSE-FORMAT"",\n                   ""CLEAN"", ""DISPLAY-POST-PREPROCCES"",\n                   ""DISPLAY-TOKENS"", ""DISPLAY-TOKEN-TEXT"", ""IS-HTML""]\n    headers = {}\n    for header_tag in headers_lst:\n        if header_tag in req_headers:\n            headers[header_tag] = req_headers[header_tag]\n        else:\n            headers[header_tag] = None\n    return headers\n\n\ndef set_headers(res):\n    """"""\n    set expected headers for request (CORS)\n\n    Args:\n        res (:obj:`falcon.Response`): the request\n    """"""\n    res.set_header(\'Access-Control-Allow-Origin\', \'*\')\n    res.set_header(""Access-Control-Allow-Credentials"", ""true"")\n    res.set_header(\'Access-Control-Allow-Methods\', ""GET,HEAD,OPTIONS,POST,PUT"")\n    res.set_header(\'Access-Control-Allow-Headers\',\n                   ""Access-Control-Allow-Headers, Access-Control-Allow-Origin,""\n                   "" Origin,Accept, X-Requested-With, Content-Type, ""\n                   ""Access-Control-Request-Method, ""\n                   ""Access-Control-Request-Headers, Response-Format, clean, ""\n                   ""display-post-preprocces, display-tokens, ""\n                   ""display-token-text"")\n\n\ndef package_home(gdict):\n    """"""\n    help function for running paths from out-of-class scripts\n    """"""\n    filename = gdict[""__file__""]\n    return os.path.dirname(filename)\n\n\ndef extract_module_name(model_path):\n    """"""\n    Extract the module\'s name from path\n\n    Args:\n        model_path(str): the module\'s class path\n\n    Returns:\n        str: the modules name\n    """"""\n    class_name = """".join(model_path.split(""."")[0].title().split(""_""))\n    return class_name\n\n\nclass Service(object):\n    """"""Handles loading and inference using specific models""""""\n    def __init__(self, service_name):\n        self.service_type = None\n        self.is_spacy = False\n        self.service = self.load_service(service_name)\n\n    def get_paragraphs(self):\n        return self.service.get_paragraphs()\n\n    # pylint: disable=eval-used\n    def get_service_inference(self, docs, headers):\n        """"""\n        get parser response from service API\n\n        Args:\n            headers (list(str)): the headers of the request\n            docs: input received from the request\n\n        Returns:\n            the service API output\n        """"""\n        logger.info(\'sending documents to parser\')\n        response_data = []\n        for i, doc in enumerate(docs):\n            inference_doc = self.service.inference(doc[""doc""])\n            if self.is_spacy is True:\n                parsed_doc = inference_doc.displacy_doc()\n                doc_dic = {""id"": doc[""id""], ""doc"": parsed_doc}\n                # Potentially a security risk\n                if headers[\'IS-HTML\'] is not None and eval(headers[\'IS-HTML\']):\n                    # a visualizer requestadd type of service (core/annotate) to response\n                    doc_dic[""type""] = self.service_type\n                response_data.append(doc_dic)\n            else:\n                inference_doc[\'id\'] = i + 1\n                response_data.append(inference_doc)\n        return response_data\n\n    def load_service(self, name):\n        """"""\n        Initialize and load service from input given name, using ""services.json"" properties file\n\n        Args:\n            name (str):\n                The name of service to upload using server\n\n        Returns:\n            The loaded service\n        """"""\n        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), ""services.json"")) \\\n                as prop_file:\n            properties = json.load(prop_file)\n        folder_path = properties[""api_folders_path""]\n        service_name_error = ""\'{0}\' is not an existing service - "" \\\n                             ""please try using another service."".format(name)\n        if name in properties:\n            model_relative_path = properties[name][""file_name""]\n        else:\n            logger.error(service_name_error)\n            raise Exception(service_name_error)\n        if not model_relative_path:\n            logger.error(service_name_error)\n            raise Exception(service_name_error)\n        module_path = ""."".join(model_relative_path.split(""."")[:-1])\n        module_name = extract_module_name(model_relative_path)\n        module = import_module(folder_path + module_path)\n        class_api = getattr(module, module_name)\n        upload_service = class_api()\n        upload_service.load_model()\n        self.service_type = properties[name][""type""]\n        self.is_spacy = properties[name].get(\'spacy\', False)\n        return upload_service\n'"
solutions/__init__.py,0,b''
solutions/start_ui.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport socket\nfrom os import path\nfrom subprocess import run\nimport sys\n\nSOLUTIONS_PATH = path.dirname(path.realpath(__file__))\n\nsolution_uis = {\n    ""set_expansion"": path.join(SOLUTIONS_PATH, ""set_expansion"", ""ui""),\n    ""trend_analysis"": path.join(SOLUTIONS_PATH, ""trend_analysis"", ""ui_main.py""),\n}\n\n\ndef check_if_ip(ip_str):\n    try:\n        socket.inet_aton(ip_str)\n        return True\n    except socket.error:\n        return False\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--solution"",\n        type=str,\n        choices=[""set_expansion"", ""trend_analysis""],\n        help=""Solution UI to initialize"",\n    )\n    parser.add_argument(""--address"", type=str, default="""", help=""IP address to use for UI server "")\n    parser.add_argument(""--port"", type=int, default=1010, help=""Port number"")\n    parser.add_argument(\n        ""--no_shell"", action=""store_true"", help=""not running through shell interface""\n    )\n    args = parser.parse_args()\n    if args.address and not check_if_ip(args.address):\n        print(""given address is not in a valid ip address format"")\n        sys.exit()\n\n    cmd_str = ""bokeh serve --show {} --port {}"".format(solution_uis[args.solution], args.port)\n    if args.address:\n        cmd_str += "" --address={} "" ""--allow-websocket-origin={}:{}"".format(\n            args.address, args.address, args.port\n        )\n\n    run(cmd_str, shell=not args.no_shell, check=True)\n'"
tests/__init__.py,0,b''
tests/test_absa.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport json\nfrom pathlib import Path\n\nfrom nlp_architect import LIBRARY_ROOT\nfrom nlp_architect.common.core_nlp_doc import CoreNLPDoc\nfrom nlp_architect.models.absa.inference.data_types import SentimentDoc\nfrom nlp_architect.models.absa.inference.inference import SentimentInference\n\n\ndef test_inference():\n    lexicons_dir = Path(LIBRARY_ROOT) / ""examples"" / ""absa""\n    inference = SentimentInference(\n        lexicons_dir / ""aspects.csv"", lexicons_dir / ""opinions.csv"", parse=False\n    )\n    data_dir = Path(LIBRARY_ROOT) / ""tests"" / ""fixtures"" / ""data"" / ""absa""\n    for i in range(1, 4):\n        with open(data_dir / ""core_nlp_doc_{}.json"".format(i)) as f:\n            predicted_doc = inference.run(parsed_doc=json.load(f, object_hook=CoreNLPDoc.decoder))\n        with open(data_dir / ""sentiment_doc_{}.json"".format(i)) as f:\n            expected_doc = json.load(f, object_hook=SentimentDoc.decoder)\n        assert expected_doc == predicted_doc\n'"
tests/test_data_utils.py,10,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport math\nimport os\nimport torch\nfrom nlp_architect.data.utils import split_column_dataset\nfrom tests.utils import count_examples\nfrom nlp_architect.nn.torch.data.dataset import CombinedTensorDataset\nfrom torch.utils.data import TensorDataset\n\n\ndef test_concat_dataset():\n    token_ids = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.long)\n    label_ids = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.long)\n    labeled_dataset = TensorDataset(token_ids, label_ids)\n    unlabeled_dataset = TensorDataset(token_ids)\n    concat_dataset = CombinedTensorDataset([labeled_dataset, unlabeled_dataset])\n    expected_tokens = torch.tensor(\n        [[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.long\n    )\n    expected_labels = torch.tensor(\n        [[1, 2, 3], [4, 5, 6], [7, 8, 9], [0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=torch.long\n    )\n    assert torch.equal(concat_dataset.tensors[0], expected_tokens)\n    assert torch.equal(concat_dataset.tensors[1], expected_labels)\n\n\ndef test_split_dataset():\n    current_dir = os.path.dirname(os.path.realpath(__file__))\n    data_dir = os.path.join(current_dir, ""fixtures/data/distillation"")\n    num_of_examples = count_examples(data_dir + os.sep + ""train.txt"")\n    labeled_precentage = 0.4\n    unlabeled_precentage = 0.5\n    if os.path.exists(data_dir):\n        labeled_file = ""labeled.txt""\n        unlabeled_file = ""unlabeled.txt""\n        split_column_dataset(\n            dataset=os.path.join(data_dir, ""train.txt""),\n            first_count=math.ceil(num_of_examples * labeled_precentage),\n            second_count=math.ceil(num_of_examples * unlabeled_precentage),\n            out_folder=data_dir,\n            first_filename=labeled_file,\n            second_filename=unlabeled_file,\n        )\n        check_labeled_count = count_examples(data_dir + os.sep + labeled_file)\n        assert check_labeled_count == math.ceil(num_of_examples * labeled_precentage)\n        check_unlabeled_count = count_examples(data_dir + os.sep + unlabeled_file)\n        assert check_unlabeled_count == math.ceil(num_of_examples * unlabeled_precentage)\n        os.remove(data_dir + os.sep + ""labeled.txt"")\n        os.remove(data_dir + os.sep + ""unlabeled.txt"")\n'"
tests/test_ner_taggers.py,8,"b'import argparse\nimport os\nimport tempfile\nimport shutil\nimport torch\nfrom nlp_architect.procedures import TrainTagger\nfrom nlp_architect.nn.torch.modules.embedders import IDCNN, CNNLSTM\n\n\nCURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\nDATA_DIR = os.path.join(CURRENT_DIR, ""fixtures/conll_sample"")\nOUTPUT_DIR = tempfile.mkdtemp()\nPARSER = argparse.ArgumentParser()\nTRAIN_PROCEDURE = TrainTagger()\nTRAIN_PROCEDURE.add_arguments(PARSER)\nEMBEDDINGS_PATH = None\nBATCH_SIZE = 128\nLEARNING_RATE = 0.0008\nEPOCHS = 1\nTRAIN_FILENAME = ""data.txt""\n\n\ndef test_taggers():\n    words = torch.tensor([[1, 2, 3, 4, 5, 0, 0, 0]], dtype=torch.long)  # (1,8)\n    word_chars = torch.tensor(\n        [[[1, 2, 0], [3, 0, 0], [4, 5, 0], [5, 4, 3], [2, 2, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n        dtype=torch.long,\n    )  # (1, 8, 3)\n    shapes = torch.tensor([[1, 2, 3, 3, 3, 0, 0, 0]], dtype=torch.long)  # (1,8)\n    mask = torch.tensor([[1, 1, 1, 1, 1, 0, 0, 0]], dtype=torch.long)  # (1,8)\n    labels = torch.tensor([[1, 1, 3, 4, 1, 0, 0, 0]], dtype=torch.long)  # (1,8)\n\n    word_vocab_size = 5\n    label_vocab_size = 4\n\n    inputs = {\n        ""words"": words,\n        ""word_chars"": word_chars,\n        ""shapes"": shapes,\n        ""mask"": mask,\n        ""labels"": labels,\n    }\n\n    idcnn_model = IDCNN(word_vocab_size + 1, label_vocab_size + 1)\n    lstm_model = CNNLSTM(word_vocab_size + 1, label_vocab_size + 1)\n    expected_output_shape = torch.Size([1, 8, label_vocab_size + 1])\n\n    idcnn_logits = idcnn_model(**inputs)\n    assert idcnn_logits.shape == expected_output_shape\n    lstm_logits = lstm_model(**inputs)\n    assert lstm_logits.shape == expected_output_shape\n\n\ndef test_tagging_procedure_sanity():\n\n    # run idcnn softmax\n    model_type = ""id-cnn""\n    idcnn_softmax_args = PARSER.parse_args(\n        [\n            ""--data_dir"",\n            DATA_DIR,\n            ""--output_dir"",\n            OUTPUT_DIR,\n            ""--embedding_file"",\n            EMBEDDINGS_PATH,\n            ""-b"",\n            str(BATCH_SIZE),\n            ""--lr"",\n            str(LEARNING_RATE),\n            ""-e"",\n            str(EPOCHS),\n            ""--train_filename"",\n            TRAIN_FILENAME,\n            ""--dev_filename"",\n            TRAIN_FILENAME,\n            ""--test_filename"",\n            TRAIN_FILENAME,\n            ""--model_type"",\n            model_type,\n            ""--overwrite_output_dir"",\n        ]\n    )\n    TRAIN_PROCEDURE.run_procedure(idcnn_softmax_args)\n\n    # run idcnn crf\n    idcnn_crf_args = PARSER.parse_args(\n        [\n            ""--data_dir"",\n            DATA_DIR,\n            ""--output_dir"",\n            OUTPUT_DIR,\n            ""--embedding_file"",\n            EMBEDDINGS_PATH,\n            ""-b"",\n            str(BATCH_SIZE),\n            ""--lr"",\n            str(LEARNING_RATE),\n            ""-e"",\n            str(EPOCHS),\n            ""--train_filename"",\n            TRAIN_FILENAME,\n            ""--dev_filename"",\n            TRAIN_FILENAME,\n            ""--test_filename"",\n            TRAIN_FILENAME,\n            ""--model_type"",\n            model_type,\n            ""--use_crf"",\n            ""--overwrite_output_dir"",\n        ]\n    )\n    TRAIN_PROCEDURE.run_procedure(idcnn_crf_args)\n\n    # run lstm softmax\n    model_type = ""cnn-lstm""\n    lstm_softmax_args = PARSER.parse_args(\n        [\n            ""--data_dir"",\n            DATA_DIR,\n            ""--output_dir"",\n            OUTPUT_DIR,\n            ""--embedding_file"",\n            EMBEDDINGS_PATH,\n            ""-b"",\n            str(BATCH_SIZE),\n            ""--lr"",\n            str(LEARNING_RATE),\n            ""-e"",\n            str(EPOCHS),\n            ""--train_filename"",\n            TRAIN_FILENAME,\n            ""--dev_filename"",\n            TRAIN_FILENAME,\n            ""--test_filename"",\n            TRAIN_FILENAME,\n            ""--model_type"",\n            model_type,\n            ""--overwrite_output_dir"",\n        ]\n    )\n    TRAIN_PROCEDURE.run_procedure(lstm_softmax_args)\n\n    # run lstm crf\n    lstm_crf_args = PARSER.parse_args(\n        [\n            ""--data_dir"",\n            DATA_DIR,\n            ""--output_dir"",\n            OUTPUT_DIR,\n            ""--embedding_file"",\n            EMBEDDINGS_PATH,\n            ""-b"",\n            str(BATCH_SIZE),\n            ""--lr"",\n            str(LEARNING_RATE),\n            ""-e"",\n            str(EPOCHS),\n            ""--train_filename"",\n            TRAIN_FILENAME,\n            ""--dev_filename"",\n            TRAIN_FILENAME,\n            ""--test_filename"",\n            TRAIN_FILENAME,\n            ""--model_type"",\n            model_type,\n            ""--use_crf"",\n            ""--overwrite_output_dir"",\n        ]\n    )\n    TRAIN_PROCEDURE.run_procedure(lstm_crf_args)\n\n    # remove output files\n    shutil.rmtree(OUTPUT_DIR)\n    assert True\n'"
tests/test_quantization.py,47,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport unittest\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom nlp_architect.nn.torch.quantization import (\n    FakeLinearQuantizationWithSTE,\n    QuantizedLinear,\n    get_dynamic_scale,\n    get_scale,\n    QuantizedEmbedding,\n)\n\n\ndef fake_quantize_np(x, scale, bits):\n    return quantize_np(x, scale, bits) / scale\n\n\ndef quantize_np(x, scale, bits):\n    return np.clip(np.round(x * scale), -(2 ** (bits - 1) - 1), 2 ** (bits - 1) - 1)\n\n\nclass FakeLinearQuantizationWithSTETester(unittest.TestCase):\n    def test_quantization_forward(self):\n        fake_quantize = FakeLinearQuantizationWithSTE().apply\n        x = torch.randn(1, 100)\n        scale = (2 ** (8 - 1) - 1) / np.abs(x).max()\n        self.assertTrue((fake_quantize(x, scale, 8) == fake_quantize_np(x, scale, 8)).all())\n\n    def test_quantization_backward(self):\n        fake_quantize = FakeLinearQuantizationWithSTE().apply\n        x = torch.randn(1, 100, requires_grad=True)\n        with torch.no_grad():\n            scale = (2 ** (8 - 1) - 1) / x.abs().max()\n        y = torch.sum(fake_quantize(x, scale, 8))\n        y.backward()\n        self.assertTrue((x.grad == torch.ones_like(x)).all())\n\n\nclass QuantizedLinearTest(unittest.TestCase):\n    def test_dynamic_quantized_linear_forward(self):\n        """"""Test QuantizedLinear forward method by giving in the input and\n        weight values that are already quantized, therefore the quantization\n        step should have no effect on the values and we know what values\n        are expected""""""\n        x = torch.randn(1, 100).mul(127.0).round().clamp(-127.0, 127.0)\n        qlinear = QuantizedLinear(100, 1, bias=False, requantize_output=False, mode=""dynamic"")\n        with torch.no_grad():\n            scale = 127.0 / qlinear.weight.abs().max()\n        self.assertTrue(\n            (\n                qlinear.fake_quantized_weight == fake_quantize_np(qlinear.weight.detach(), scale, 8)\n            ).all()\n        )\n        qlinear.weight.data = (\n            torch.randn_like(qlinear.weight).mul(127.0).round().clamp(-127.0, 127.0)\n        )\n        y = qlinear(x)\n        self.assertEqual(y.shape, (1, 1))\n        self.assertTrue((y == (x @ qlinear.weight.t())).all())\n\n    def test_static_quantized_inference(self):\n        qlinear = QuantizedLinear(10, 5, mode=""EMA"")\n        weight = qlinear.weight.data.detach()\n        weight_scale = get_dynamic_scale(weight, 8)\n        weight_int = quantize_np(weight, weight_scale, 8)\n        self.assertTrue((weight_int == torch.round(weight_int)).all())\n        self.assertTrue(weight_int.abs().max() <= 127)\n        x = torch.randn(3, 10) * 2 ** 0.5 - 0.36\n        x_thresh = 3.0\n        output_thresh = 2.3\n        output_scale = 127.0 / output_thresh\n        x_scale = 127.0 / x_thresh\n        qlinear.input_thresh = torch.tensor(x_thresh)\n        qlinear.output_thresh = torch.tensor(output_thresh)\n        x_int = quantize_np(x, x_scale, 8)\n        self.assertTrue((x_int == torch.round(x_int)).all())\n        self.assertTrue(x_int.abs().max() <= 127)\n        bias = qlinear.bias.data\n        bias_scale = x_scale * weight_scale\n        bias_int = quantize_np(bias, bias_scale, 32)\n        self.assertTrue((bias_int == torch.round(bias_int)).all())\n        self.assertTrue(bias_int.abs().max() <= 2 ** (32 - 1) - 1)\n        output_int = x_int @ weight_int.t() + bias_int\n        output_int = torch.clamp(output_int, -(2 ** (32 - 1) - 1), 2 ** (32 - 1) - 1)\n        output = torch.round(output_int / bias_scale * output_scale).clamp(-127, 127) / output_scale\n        qlinear.eval()\n        qlinear_output = qlinear(x)\n        self.assertTrue((qlinear_output - output).norm() < 10 ** -6)\n\n    def test_ema_quantization(self):\n        ema_decay = 0.9\n        qlinear = QuantizedLinear(10, 5, bias=False, ema_decay=ema_decay, mode=""EMA"")\n        for i in range(5):\n            x = torch.randn(3, 10)\n            tmp_input_thresh = x.abs().max()\n            if i == 0:\n                input_ema = tmp_input_thresh\n            else:\n                input_ema -= (1 - ema_decay) * (input_ema - tmp_input_thresh)\n            y = (\n                fake_quantize_np(x, get_scale(8, input_ema), 8) @ qlinear.fake_quantized_weight.t()\n            ).detach()\n            tmp_output_thresh = y.abs().max()\n            if i == 0:\n                output_ema = tmp_output_thresh\n            else:\n                output_ema -= (1 - ema_decay) * (output_ema - tmp_output_thresh)\n            y = fake_quantize_np(y, get_scale(8, output_ema), 8)\n            y_hat = qlinear(x)\n            self.assertTrue((y == y_hat).all())\n        self.assertEqual(qlinear.input_thresh, input_ema)\n        self.assertEqual(qlinear.output_thresh, output_ema)\n\n    def test_ema_quantization_data_parallel(self):\n        if not torch.cuda.is_available() or torch.cuda.device_count() <= 1:\n            return\n        ema_decay = 0.9\n        fake_quantize = FakeLinearQuantizationWithSTE().apply\n        qlinear = nn.DataParallel(\n            QuantizedLinear(10, 5, bias=False, ema_decay=ema_decay, mode=""EMA"")\n        ).cuda()\n        for i in range(5):\n            x = torch.randn(2, 10).cuda()\n            tmp_input_thresh = x[0].abs().max()\n            if i == 0:\n                input_ema = tmp_input_thresh\n            else:\n                input_ema -= (1 - ema_decay) * (input_ema - tmp_input_thresh)\n            y = (\n                fake_quantize(x, get_scale(8, input_ema), 8)\n                @ qlinear.module.fake_quantized_weight.t()\n            ).detach()\n            tmp_output_thresh = y[0].abs().max()\n            if i == 0:\n                output_ema = tmp_output_thresh\n            else:\n                output_ema -= (1 - ema_decay) * (output_ema - tmp_output_thresh)\n            qlinear(x)\n        self.assertEqual(qlinear.module.input_thresh, input_ema)\n        self.assertEqual(qlinear.module.output_thresh, output_ema)\n\n    def test_start_quantization_delay(self):\n        quantization_delay = 2\n        qlinear = QuantizedLinear(10, 5, start_step=quantization_delay, mode=""DYNAMIC"")\n        linear = nn.Linear(10, 5)\n        linear.weight.data = qlinear.weight\n        linear.bias.data = qlinear.bias\n        for _ in range(quantization_delay):\n            x = torch.randn(3, 10)\n            qy = qlinear(x)\n            y = linear(x)\n            self.assertTrue((y == qy).all())\n        qy = qlinear(x)\n        self.assertFalse((y == qy).all())\n\n    def test_start_quantization_delay_data_parallel(self):\n        if not torch.cuda.is_available():\n            return\n        quantization_delay = 2\n        qlinear = QuantizedLinear(10, 5, start_step=quantization_delay, mode=""DYNAMIC"")\n        linear = nn.Linear(10, 5)\n        linear.weight.data = qlinear.weight\n        linear.bias.data = qlinear.bias\n        qlinear = nn.DataParallel(qlinear).cuda()\n        linear = nn.DataParallel(linear).cuda()\n        for _ in range(quantization_delay):\n            x = torch.randn(3, 10).cuda()\n            qy = qlinear(x)\n            y = linear(x)\n            self.assertTrue((y == qy).all())\n        qy = qlinear(x)\n        self.assertFalse((y == qy).all())\n\n    def test_dynamic_quantized_linear_backward(self):\n        x = torch.randn(1, 100, requires_grad=True)\n        linear = QuantizedLinear(100, 1, bias=False, mode=""DYNAMIC"")\n        y = linear(x)\n        y.backward()\n        self.assertTrue((x.grad == linear.fake_quantized_weight).all())\n        with torch.no_grad():\n            scale = (2 ** (8 - 1) - 1) / x.abs().max()\n        self.assertTrue((fake_quantize_np(x.detach(), scale, 8) == linear.weight.grad).all())\n\n    def test_training_and_inference_differences_ema(self):\n        qlinear = QuantizedLinear(10, 5, mode=""EMA"", bias=False)\n        x = torch.randn(3, 10) * 2 + 0.1\n        y = qlinear(x)\n        qlinear.eval()\n        y_hat = qlinear(x)\n        self.assertTrue((y - y_hat).norm() < 1e-6)\n\n    def test_training_and_inference_differences_dynamic(self):\n        qlinear = QuantizedLinear(10, 5, bias=False)\n        x = torch.randn(3, 10) * 2 + 0.1\n        y = qlinear(x)\n        qlinear.eval()\n        y_hat = qlinear(x)\n        self.assertTrue((y - y_hat).norm() < 1e-6)\n\n    def test_none_quantized_linear(self):\n        qlinear = QuantizedLinear(10, 5, mode=""NONE"")\n        linear = nn.Linear(10, 5)\n        linear.weight.data = qlinear.weight\n        linear.bias.data = qlinear.bias\n        x = torch.randn(3, 10)\n        y = linear(x)\n        y_hat = qlinear(x)\n        self.assertTrue((y - y_hat).norm() < 1e-6)\n\n    def test_export_to_8bit_with_bias(self):\n        qlinear = QuantizedLinear(10, 5, mode=""EMA"")\n        qlinear.eval()\n        state_dict = qlinear.state_dict()\n        self.assertTrue(""weight"" in state_dict)\n        self.assertTrue(""bias"" in state_dict)\n        self.assertTrue(""quantized_weight"" not in state_dict)\n        self.assertTrue(""_quantized_bias"" not in state_dict)\n        self.assertTrue(""bias_scale"" not in state_dict)\n        qlinear.mode_8bit = True\n        state_dict = qlinear.state_dict()\n        self.assertTrue(""weight"" not in state_dict)\n        self.assertTrue(""bias"" not in state_dict)\n        self.assertTrue(""quantized_weight"" in state_dict)\n        self.assertTrue(state_dict[""quantized_weight""].dtype == torch.int8)\n        self.assertTrue(""_quantized_bias"" in state_dict)\n        self.assertTrue(state_dict[""_quantized_bias""].dtype == torch.int32)\n        self.assertTrue(""bias_scale"" in state_dict)\n        qlinear.mode_8bit = False\n        state_dict = qlinear.state_dict()\n        self.assertTrue(""weight"" in state_dict)\n        self.assertTrue(""bias"" in state_dict)\n        self.assertTrue(""quantized_weight"" not in state_dict)\n        self.assertTrue(""_quantized_bias"" not in state_dict)\n        self.assertTrue(""bias_scale"" not in state_dict)\n\n    def test_export_to_8bit_without_bias(self):\n        qlinear = QuantizedLinear(10, 5, bias=False, mode=""EMA"")\n        qlinear.eval()\n        qlinear.mode_8bit = True\n        state_dict = qlinear.state_dict()\n        self.assertTrue(""weight"" not in state_dict)\n        self.assertTrue(""bias"" not in state_dict)\n        self.assertTrue(""quantized_weight"" in state_dict)\n        self.assertTrue(state_dict[""quantized_weight""].dtype == torch.int8)\n        self.assertTrue(""_quantized_bias"" not in state_dict)\n        self.assertTrue(""bias_scale"" not in state_dict)\n        qlinear.mode_8bit = False\n        state_dict = qlinear.state_dict()\n        self.assertTrue(""weight"" in state_dict)\n        self.assertTrue(""bias"" not in state_dict)\n        self.assertTrue(""quantized_weight"" not in state_dict)\n        self.assertTrue(""_quantized_bias"" not in state_dict)\n        self.assertTrue(""bias_scale"" not in state_dict)\n\n    def test_import_from_8bit_without_bias(self):\n        exporter = QuantizedLinear(10, 5, bias=False, mode=""dynamic"")\n        exporter.eval()\n        exporter.mode_8bit = True\n        state_dict = exporter.state_dict()\n        exporter.mode_8bit = False\n        importer = QuantizedLinear(10, 5, bias=False, mode=""dynamic"")\n        self.assertTrue((exporter.weight != importer.weight).any())\n        importer.eval()\n        importer.load_state_dict(state_dict, strict=False)\n        x = torch.randn(3, 10)\n        self.assertTrue((exporter(x) == importer(x)).all())\n\n    def test_import_from_8bit_with_bias(self):\n        # QuantizationMode dynamic\n        exporter = QuantizedLinear(10, 5, mode=""dynamic"")\n        exporter.eval()\n        exporter.mode_8bit = True\n        state_dict = exporter.state_dict()\n        exporter.mode_8bit = False\n        importer = QuantizedLinear(10, 5, mode=""dynamic"")\n        self.assertTrue((exporter.weight != importer.weight).any())\n        self.assertTrue((exporter.bias != importer.bias).any())\n        importer.eval()\n        importer.load_state_dict(state_dict, strict=False)\n        x = torch.randn(3, 10)\n        self.assertTrue((exporter(x) == importer(x)).all())\n        # QuantizationMode ema\n        exporter = QuantizedLinear(10, 5, requantize_output=False, mode=""ema"")\n        x = torch.randn(3, 10)\n        exporter(x)\n        self.assertTrue(exporter.input_thresh != 0.0)\n        exporter.eval()\n        exporter.mode_8bit = True\n        state_dict = exporter.state_dict()\n        exporter.mode_8bit = False\n        importer = QuantizedLinear(10, 5, requantize_output=False, mode=""ema"")\n        self.assertTrue((exporter.weight != importer.weight).any())\n        self.assertTrue((exporter.bias != importer.bias).any())\n        importer.eval()\n        importer.load_state_dict(state_dict, strict=False)\n        self.assertTrue((exporter(x) == importer(x)).all())\n\n    def test_train_block_when_loading_quantized_model(self):\n        exporter = QuantizedLinear(10, 5, mode=""dynamic"")\n        exporter.eval()\n        exporter.mode_8bit = True\n        state_dict = exporter.state_dict()\n        importer = QuantizedLinear(10, 5, mode=""dynamic"")\n        importer.eval()\n        importer.load_state_dict(state_dict, strict=False)\n        with self.assertRaises(RuntimeError):\n            importer.train()\n\n    def test_restrict_loading_to_train_model(self):\n        exporter = QuantizedLinear(10, 5, mode=""dynamic"")\n        exporter.eval()\n        exporter.mode_8bit = True\n        state_dict = exporter.state_dict()\n        importer = QuantizedLinear(10, 5, mode=""dynamic"")\n        with self.assertRaises(RuntimeError):\n            importer.load_state_dict(state_dict, strict=False)\n\n\nclass QuantizedEmbeddingTest(unittest.TestCase):\n    def test_quantized_embedding_training_forward(self):\n        embedding = QuantizedEmbedding(10, 3, mode=""ema"")\n        with torch.no_grad():\n            scale = 127.0 / embedding.weight.abs().max()\n        self.assertTrue(\n            (\n                embedding.fake_quantized_weight\n                == fake_quantize_np(embedding.weight.detach(), scale, 8)\n            ).all()\n        )\n        embedding.weight.data = (\n            torch.randn_like(embedding.weight).mul(127.0).round().clamp(-127.0, 127.0)\n        )\n        indices = torch.tensor(np.arange(10))\n        ground = F.embedding(indices, embedding.weight)\n        quantized = embedding(indices)\n        self.assertTrue((ground == quantized).all())\n\n    def test_quantized_embedding_inference_forward(self):\n        embedding = QuantizedEmbedding(10, 3, mode=""ema"")\n        with torch.no_grad():\n            scale = 127.0 / embedding.weight.abs().max()\n        self.assertTrue(\n            (\n                embedding.fake_quantized_weight\n                == fake_quantize_np(embedding.weight.detach(), scale, 8)\n            ).all()\n        )\n        embedding.weight.data = (\n            torch.randn_like(embedding.weight).mul(127.0).round().clamp(-127.0, 127.0)\n        )\n        indices = torch.tensor(np.arange(10))\n        embedding.eval()\n        ground = F.embedding(indices, embedding.weight)\n        quantized = embedding(indices)\n        self.assertTrue((ground == quantized).all())\n\n    def test_quantized_embedding_backward(self):\n        embedding = QuantizedEmbedding(10, 3, mode=""ema"")\n        linear = nn.Linear(3, 1)\n        indices = torch.tensor([2])\n        h = embedding(indices)\n        y = linear(h)\n        y.backward()\n        grad = torch.zeros_like(embedding.weight)\n        grad[indices.item(), :] = linear.weight.t().squeeze()\n        self.assertTrue((embedding.weight.grad == grad).all())\n        self.assertTrue((linear.weight.grad == h).all())\n\n    def test_delay_quantization_start(self):\n        qembedding = QuantizedEmbedding(10, 3, mode=""ema"", start_step=1)\n        embedding = nn.Embedding(10, 3)\n        embedding.weight.data = qembedding.weight\n        indices = torch.tensor(np.arange(10))\n        self.assertTrue((embedding(indices) == qembedding(indices)).all())\n        self.assertTrue((embedding(indices) != qembedding(indices)).any())\n\n    def test_quantization_turned_off(self):\n        qembedding = QuantizedEmbedding(10, 3, mode=""none"")\n        embedding = nn.Embedding(10, 3)\n        embedding.weight.data = qembedding.weight\n        indices = torch.tensor(np.arange(10))\n        self.assertTrue((embedding(indices) == qembedding(indices)).all())\n        self.assertTrue((embedding(indices) == qembedding(indices)).all())\n\n    def test_export_to_8bit(self):\n        qembed = QuantizedEmbedding(10, 5, mode=""EMA"")\n        qembed.eval()\n        state_dict = qembed.state_dict()\n        self.assertTrue(""quantized_weight"" not in state_dict)\n        self.assertTrue(""weight"" in state_dict)\n        qembed.mode_8bit = True\n        state_dict = qembed.state_dict()\n        self.assertTrue(""quantized_weight"" in state_dict)\n        self.assertTrue(state_dict[""quantized_weight""].dtype == torch.int8)\n        self.assertTrue(""weight"" not in state_dict)\n        qembed.mode_8bit = False\n        state_dict = qembed.state_dict()\n        self.assertTrue(""quantized_weight"" not in state_dict)\n        self.assertTrue(""weight"" in state_dict)\n\n    def test_load_from_8bit(self):\n        exporter = QuantizedEmbedding(10, 5, mode=""EMA"")\n        exporter.eval()\n        exporter.mode_8bit = True\n        state_dict = exporter.state_dict()\n        exporter.mode_8bit = False\n        importer = QuantizedEmbedding(10, 5, mode=""EMA"")\n        self.assertTrue((exporter.weight != importer.weight).any())\n        importer.eval()\n        importer.load_state_dict(state_dict, strict=False)\n        indices = torch.tensor(np.arange(10))\n        self.assertTrue((exporter(indices) == importer(indices)).all())\n'"
tests/test_server_sanity.py,0,"b'# # ******************************************************************************\n# # Copyright 2017-2018 Intel Corporation\n# #\n# # Licensed under the Apache License, Version 2.0 (the ""License"");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an ""AS IS"" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# # ******************************************************************************\n# # pylint: disable=redefined-outer-name\n# # pylint: disable=c-extension-no-member\n# import gzip\n# import io\n# import json\n# import os\n# import sys\n# from io import open\n# from os.path import dirname\n\n# import hug\n# import pytest\n\n# import nlp_architect.server.serve\n# from nlp_architect.server.serve import api\n# from nlp_architect.utils.text import try_to_load_spacy\n\n# if not try_to_load_spacy(\'en\'):\n#     pytest.skip(""\\n\\nSkipping test_server_sanity.py. Reason: \'spacy en\' model not installed. ""\n#                 ""Please see https://spacy.io/models/ for installation instructions.\\n""\n#                 ""The terms and conditions of the data set and/or model license apply.\\n""\n#                 ""Intel does not grant any rights to the data and/or model files.\\n"",\n#                 allow_module_level=True)\n\n# sys.path.insert(0, (dirname(dirname(os.path.abspath(__file__)))))\n\n# headers = {""clean"": ""True"", ""display_post_preprocces"": ""True"",\n#            ""display_tokens"": """", ""display_token_text"": ""True"",\n#            ""IS-HTML"": ""False""}\n# server_data_rel_path = \'fixtures/data/server/\'\n\n\n# def load_test_data(service_name):\n#     """"""\n#     load test data (input and expected response) for given service from \'tests_data.json\'\n#     Args:\n#         service_name (str):  the service name\n#     Returns:\n#         str: the test data of the service\n#     """"""\n#     with open(os.path.join(os.path.dirname(__file__), server_data_rel_path + \'tests_data.json\'),\n#               \'r\') as f:\n#         service_test_data = json.loads(f.read())[service_name]\n#     return service_test_data\n\n\n# def assert_response_struct(result_doc, expected_result):\n#     # 1. assert docs list\n#     assert isinstance(result_doc, list)\n#     assert len(result_doc) == len(expected_result)\n#     # 2. assert the structure of doc item\n#     result_item = result_doc[0]\n#     expected_result_item = expected_result[0]\n#     assert isinstance(result_item, dict)\n#     for key in expected_result_item.keys():\n#         assert key in result_item\n#     # 3. assert the structure of doc item dict\n#     result_dict = result_item[\'doc\']\n#     expected_result_dict = expected_result_item[\'doc\']\n#     if isinstance(result_dict, list):\n#         result_dict = result_dict[0]\n#         expected_result_dict = expected_result_dict[0]\n#     assert isinstance(result_dict, dict)\n#     for key in expected_result_dict.keys():\n#         assert key in result_dict\n#     # 4. check CoreNLPDoc\n#     if \'sentences\' in expected_result_dict.keys():\n#         assert isinstance(result_dict[\'sentences\'], list)\n#         # assert sentence:\n#         assert isinstance(result_dict[\'sentences\'][0], list)\n#         # assert word-token\n#         result_word_dict = result_dict[\'sentences\'][0][0]\n#         expected_result_word_dict = expected_result_item[\'sentences\'][0][0]\n#         for key in expected_result_word_dict.keys():\n#             assert key in result_word_dict\n#     # 5. check HighLevelDoc\n#     elif \'annotation_set\' in expected_result_dict.keys():\n#         assert isinstance(result_dict[\'annotation_set\'], list)\n#         assert isinstance(result_dict[\'spans\'], list)\n#         result_spans = result_dict[\'spans\'][0]\n#         expected_result_spans = expected_result_dict[\'spans\'][0]\n#         assert isinstance(result_spans, dict)\n#         for key in expected_result_spans.keys():\n#             assert key in result_spans\n#     # 6. check displacy html rendering input\n#     elif \'arcs\' in expected_result_dict.keys():\n#         assert isinstance(result_dict[\'arcs\'], list)\n#         assert isinstance(result_dict[\'words\'], list)\n#         result_arcs = result_dict[\'arcs\'][0]\n#         expected_result_arcs = expected_result_dict[\'arcs\'][0]\n#         assert isinstance(result_arcs, dict)\n#         for key in expected_result_arcs.keys():\n#             assert key in result_arcs\n#         result_words = result_dict[\'words\'][0]\n#         expected_result_words = expected_result_dict[\'words\'][0]\n#         assert isinstance(result_words, dict)\n#         for key in expected_result_words.keys():\n#             assert key in result_words\n\n\n# @pytest.mark.parametrize(\'service_name\', [\'bist\', \'ner\'])\n# def test_request(service_name):\n#     test_data = load_test_data(service_name)\n#     test_data[\'input\'][\'model_name\'] = service_name\n#     doc = json.dumps(test_data[""input""])\n#     expected_result = json.dumps(test_data[""response""])\n#     myHeaders = headers.copy()\n#     myHeaders[""content-type""] = ""application/json""\n#     myHeaders[""Response-Format""] = ""json""\n#     # pylint: disable=no-member\n#     response = hug.test.post(api, \'/inference\', body=doc, headers=myHeaders)\n\n#     assert_response_struct(response.data, json.loads(expected_result))\n#     assert response.status == hug.HTTP_OK\n\n\n# @pytest.mark.parametrize(\'service_name\', [\'bist\', \'ner\'])\n# def test_gzip_file_request(service_name):\n#     file_path = os.path.join(os.path.dirname(__file__), server_data_rel_path + service_name\n#                              + ""_sentences_examples.json.gz"")\n#     with open(file_path, \'rb\') as file_data:\n#         doc = file_data.read()\n#     expected_result = json.dumps(load_test_data(service_name)[""response""])\n#     myHeaders = headers.copy()\n#     myHeaders[""content-type""] = ""application/gzip""\n#     myHeaders[""Response-Format""] = ""gzip""\n#     myHeaders[""content-encoding""] = ""gzip""\n#     # pylint: disable=no-member\n#     response = hug.test.post(api, \'/inference\', body=doc, headers=myHeaders)\n#     result_doc = get_decompressed_gzip(response.data)\n#     assert_response_struct(result_doc, json.loads(expected_result))\n#     assert response.status == hug.HTTP_OK\n\n\n# @pytest.mark.parametrize(\'service_name\', [\'bist\', \'ner\'])\n# def test_json_file_request(service_name):\n#     file_path = os.path.join(os.path.dirname(__file__), server_data_rel_path + service_name\n#                              + ""_sentences_examples.json"")\n#     with open(file_path, \'rb\') as file:\n#         doc = file.read()\n#     expected_result = json.dumps(load_test_data(service_name)[""response""])\n#     myHeaders = headers.copy()\n#     myHeaders[""Content-Type""] = ""application/json""\n#     myHeaders[""RESPONSE-FORMAT""] = ""json""\n#     # pylint: disable=no-member\n#     response = hug.test.post(nlp_architect.server.serve, \'/inference\', body=doc, headers=myHeaders)\n#     assert_response_struct(response.data, json.loads(expected_result))\n#     assert response.status == hug.HTTP_OK\n\n\n# def get_decompressed_gzip(req_resp):\n#     tmp_file = io.BytesIO()\n#     tmp_file.write(req_resp)\n#     tmp_file.seek(0)\n#     with gzip.GzipFile(fileobj=tmp_file, mode=\'rb\') as file_out:\n#         gunzipped_bytes_obj = file_out.read()\n#     return json.loads(gunzipped_bytes_obj.decode())\n'"
tests/test_spacy_bist.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=redefined-outer-name\nimport pytest\n\nfrom nlp_architect.utils.text import try_to_load_spacy\nfrom nlp_architect.pipelines.spacy_bist import SpacyBISTParser\n\nif not try_to_load_spacy(""en""):\n    pytest.skip(\n        ""\\n\\nSkipping test_spacy_bist.py. Reason: \'spacy en\' model not installed. ""\n        ""Please see https://spacy.io/models/ for installation instructions.\\n""\n        ""The terms and conditions of the data set and/or model license apply.\\n""\n        ""Intel does not grant any rights to the data and/or model files.\\n"",\n        allow_module_level=True,\n    )\n\n\nclass TestData:\n    """"""Test cases for the test functions below.""""""\n\n    output_structure = [\n        ""This is a single-sentence document"",\n        ""This is a document... This is the second sentence"",\n    ]\n\n    sentence_breaking = [\n        (""This is a single-sentence document"", 1),\n        (""This is a document... This is the second sentence"", 2),\n    ]\n\n    dependencies = [\n        (\n            ""i don\'t have other assistance"",\n            [[(""nsubj"", 3), (""aux"", 3), (""neg"", 3), (""root"", -1), (""amod"", 5), (""dobj"", 3)]],\n        )\n    ]\n\n    pos_tags = [""He\'s the best player in NBA history""]\n\n\nclass Fixtures:\n    default_parser = SpacyBISTParser()\n\n    ptb_pos_tags = {\n        ""CC"",\n        ""CD"",\n        ""DT"",\n        ""EX"",\n        ""FW"",\n        ""IN"",\n        ""JJ"",\n        ""JJR"",\n        ""JJS"",\n        ""LS"",\n        ""MD"",\n        ""NN"",\n        ""NNS"",\n        ""NNP"",\n        ""NNPS"",\n        ""PDT"",\n        ""POS"",\n        ""PRP"",\n        ""PRP$"",\n        ""RB"",\n        ""RBR"",\n        ""RBS"",\n        ""RP"",\n        ""SYM"",\n        ""TO"",\n        ""UH"",\n        ""VB"",\n        ""VBD"",\n        ""VBG"",\n        ""VBN"",\n        ""VBP"",\n        ""VBZ"",\n        ""WDT"",\n        ""WP"",\n        ""WP$"",\n        ""WRB"",\n    }\n\n    token_label_types = {\n        ""start"": int,\n        ""len"": int,\n        ""pos"": str,\n        ""ner"": str,\n        ""lemma"": str,\n        ""gov"": int,\n        ""rel"": str,\n    }\n\n\n@pytest.mark.parametrize(""show_tok"", [True, False])\n@pytest.mark.parametrize(""show_doc"", [True, False])\n@pytest.mark.parametrize(""text"", TestData.output_structure)\ndef test_output_structure(parser, text, show_tok, show_doc, token_label_types):\n    """"""Test that the output object structure hasn\'t changed.\n\n    Args:\n        parser (SpacyBistParser)\n        text (str): Input test case.\n        show_tok (bool): Specifies whether to include token text in output.\n        show_doc (bool): Specifies whether to include document text in output.\n        token_label_types (dict): Mapping of label names to their type.\n    """"""\n    parsed_doc = parser.parse(doc_text=text, show_tok=show_tok, show_doc=show_doc)\n    assert isinstance(parsed_doc.sentences, list)\n    assert isinstance(parsed_doc.doc_text, str) if show_doc else not parsed_doc.doc_text\n    for sentence in parsed_doc:\n        for token in sentence:\n            assert isinstance(token[""text""], str) if show_tok else ""text"" not in token\n            for label, label_type in token_label_types.items():\n                assert isinstance(token.get(label), label_type)\n\n\n@pytest.mark.parametrize(""method_name"", [""parse""])\n@pytest.mark.parametrize(""text, sent_count"", TestData.sentence_breaking)\ndef test_sentence_breaking(parser, text, sent_count, method_name):\n    """"""Test that documents are broken into the expected number of sentences.\n\n    Args:\n        parser (SpacyBistParser)\n        text (str): Input test case.\n        sent_count (int): Expected number of sentences in `text`.\n        method_name (str): Parse method to test.\n    """"""\n    parse_method = getattr(parser, method_name)\n    parsed_doc = parse_method(doc_text=text)\n    assert len(parsed_doc.sentences) == sent_count\n\n\n@pytest.mark.parametrize(""method_name"", [""parse""])\n@pytest.mark.parametrize(""text, deps"", TestData.dependencies)\ndef test_dependencies(parser, text, deps, method_name):\n    """"""Test that dependencies are predicted correctly.\n\n    Args:\n        parser (SpacyBistParser)\n        text (str): Input test case.\n        deps (list of list of tuples): Expected dependencies in `text`.\n        method_name (str): Parse method to test.\n    """"""\n    parse_method = getattr(parser, method_name)\n    parsed_doc = parse_method(doc_text=text)\n    for i_sentence, sentence in enumerate(parsed_doc):\n        for i_token, token in enumerate(sentence):\n            assert (token[""rel""], token[""gov""]) == deps[i_sentence][i_token]\n\n\n@pytest.mark.parametrize(""method_name"", [""parse""])\n@pytest.mark.parametrize(""text"", TestData.pos_tags)\ndef test_pos_tag(parser, text, method_name, ptb_pos_tags):\n    """"""Tests that produced POS tags are valid PTB POS tags.\n\n    Args:\n        parser (SpacyBistParser)\n        text (str): Input test case.\n        method_name (str): Parse method to test.\n        ptb_pos_tags (set of str): Valid PTB POS tags.\n    """"""\n    parse_method = getattr(parser, method_name)\n    parsed_doc = parse_method(doc_text=text)\n    assert all([token[""pos""] in ptb_pos_tags for sent in parsed_doc for token in sent])\n\n\n@pytest.fixture(scope=""session"")\ndef parser():\n    return Fixtures.default_parser\n\n\n@pytest.fixture\ndef ptb_pos_tags():\n    return Fixtures.ptb_pos_tags\n\n\n@pytest.fixture\ndef token_label_types():\n    return Fixtures.token_label_types\n'"
tests/test_spacy_np_annotator.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=redefined-outer-name\nimport errno\nimport os\nfrom os import path\n\nimport pytest\nfrom nlp_architect.pipelines.spacy_np_annotator import (\n    NPAnnotator,\n    get_noun_phrases,\n    SpacyNPAnnotator,\n)\nfrom nlp_architect.utils.io import download_unlicensed_file\nfrom nlp_architect.utils.text import SpacyInstance, try_to_load_spacy\n\nMODEL_URL = ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/models/chunker/""\nMODEL_FILE = ""model.h5""\nMODEL_INFO = ""model_info.dat.params""\nlocal_models_path = path.join(path.dirname(path.realpath(__file__)), ""fixtures/data/chunker"")\n\nif not try_to_load_spacy(""en""):\n    pytest.skip(\n        ""\\n\\nSkipping test_spacy_np_annotator.py. Reason: \'spacy en\' model not installed.""\n        ""Please see https://spacy.io/models/ for installation instructions.\\n""\n        ""The terms and conditions of the data set and/or model license apply.\\n""\n        ""Intel does not grant any rights to the data and/or model files.\\n"",\n        allow_module_level=True,\n    )\n\n\ndef check_dir():\n    if not os.path.exists(local_models_path):\n        try:\n            os.makedirs(local_models_path)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n\n\ndef download(url, filename, local_path):\n    if not os.path.exists(local_path):\n        download_unlicensed_file(url, filename, local_path)\n\n\n@pytest.fixture(scope=""session"", autouse=True)\ndef setup():\n    check_dir()\n\n\n@pytest.fixture\ndef model_path():\n    path_to_model = path.join(local_models_path, MODEL_FILE)\n    download(MODEL_URL, MODEL_FILE, path_to_model)\n    return path_to_model\n\n\n@pytest.fixture\ndef settings_path():\n    path_to_model = path.join(local_models_path, MODEL_INFO)\n    download(MODEL_URL, MODEL_INFO, path_to_model)\n    return path_to_model\n\n\ndef test_np_annotator_load(model_path, settings_path):\n    assert NPAnnotator.load(model_path, settings_path)\n\n\n@pytest.mark.parametrize(\n    ""text"", [""The quick brown fox jumped over the lazy dog. "" ""The quick fox jumped.""]\n)\n@pytest.mark.parametrize(""phrases"", [[""lazy dog""]])\ndef test_np_annotator_linked(model_path, settings_path, text, phrases):\n    annotator = SpacyInstance(model=""en"", disable=[""textcat"", ""ner"", ""parser""]).parser\n    annotator.add_pipe(annotator.create_pipe(""sentencizer""), first=True)\n    annotator.add_pipe(NPAnnotator.load(model_path, settings_path), last=True)\n    doc = annotator(text)\n    noun_phrases = [p.text for p in get_noun_phrases(doc)]\n    for p in phrases:\n        assert p in noun_phrases\n\n\n@pytest.mark.parametrize(\n    ""text"", [""The quick brown fox jumped over the lazy dog. "" ""The quick fox jumped.""]\n)\n@pytest.mark.parametrize(""phrases"", [[""lazy dog""]])\ndef test_spacy_np_annotator(model_path, settings_path, text, phrases):\n    annotator = SpacyNPAnnotator(model_path, settings_path, spacy_model=""en"", batch_size=32)\n    spans = annotator(text)\n    for p in phrases:\n        assert p in spans\n'"
tests/test_string_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom nlp_architect.utils.string_utils import StringUtils\n\n\ndef test_is_determiner():\n    assert StringUtils.is_determiner(""the"")\n    assert StringUtils.is_determiner(""on"") is False\n\n\ndef test_is_preposition():\n    assert StringUtils.is_preposition(""the"") is False\n    assert StringUtils.is_preposition(""on"")\n\n\ndef test_is_pronoun():\n    assert StringUtils.is_pronoun(""anybody"")\n    assert StringUtils.is_pronoun(""the"") is False\n\n\ndef test_is_stopword():\n    assert StringUtils.is_stop(""always"")\n    assert StringUtils.is_stop(""sunday"") is False\n'"
tests/test_tcn.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom examples.word_language_model_with_tcn.adding_problem.adding_model import TCNForAdding\nfrom examples.word_language_model_with_tcn.toy_data.adding import Adding\n\n\ndef test_tcn_adding():\n    """"""\n    Sanity check -\n    Test function checks to make sure training loss drops to ~0 on small dummy dataset\n    """"""\n    n_features = 2\n    hidden_sizes = [64] * 3\n    kernel_size = 3\n    dropout = 0.0\n    seq_len = 10\n    n_train = 5000\n    n_val = 100\n    batch_size = 32\n    n_epochs = 10\n    num_iterations = int(n_train * n_epochs * 1.0 / batch_size)\n    lr = 0.002\n    grad_clip_value = 10\n    results_dir = ""./""\n\n    adding_dataset = Adding(seq_len=seq_len, n_train=n_train, n_test=n_val)\n\n    model = TCNForAdding(\n        seq_len, n_features, hidden_sizes, kernel_size=kernel_size, dropout=dropout\n    )\n\n    model.build_train_graph(lr, max_gradient_norm=grad_clip_value)\n\n    training_loss = model.run(\n        adding_dataset, num_iterations=num_iterations, log_interval=1e6, result_dir=results_dir\n    )\n\n    assert training_loss < 1e-3\n'"
tests/test_utils_embedding.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport numpy as np\n\nfrom nlp_architect.utils.embedding import FasttextEmbeddingsModel\nfrom nlp_architect.utils.testing import NLPArchitectTestCase\n\ntexts = [\n    ""The quick brown fox jumped over the fence"",\n    ""NLP Architect is an open source library"",\n    ""NLP Architect is made by Intel AI"",\n    ""python is the scripting language used in NLP Architect"",\n]\n\n\nclass TestFasttextEmbeddingModel(NLPArchitectTestCase):\n    def setUp(self):\n        super().setUp()\n        self.data = [t.split() for t in texts]\n        self.model = FasttextEmbeddingsModel()\n        self.file_path = str(self.TEST_DIR / ""fasttext_emb_model"")\n\n    def test_train(self):\n        self.model.train(self.data, epochs=50)\n        assert self.model\n\n    def test_query(self):\n        wv = self.model[""NLP""]\n        assert wv is not None\n        assert isinstance(wv, np.ndarray)\n\n    def test_save_load(self):\n        self.model.train(self.data, epochs=50)\n        self.model.save(self.file_path)\n        new_model = FasttextEmbeddingsModel.load(self.file_path)\n        assert new_model is not None\n        assert isinstance(new_model[""word""], np.ndarray)\n'"
tests/utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport os\n\n\ndef count_examples(file):\n    ctr = 0\n    if os.path.exists(file):\n        with open(file) as fp:\n            for line in fp:\n                line = line.strip()\n                if len(line) == 0:\n                    ctr += 1\n    else:\n        print(""File:"" + file + "" doesn\'t exist"")\n    return ctr\n'"
docs-source/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# flake8: noqa\nimport os\nimport sys\n\nimport sphinx_rtd_theme  # noqa: E402\nfrom sphinx.ext import apidoc\n\nfrom nlp_architect.version import NLP_ARCHITECT_VERSION\n\n# -- Options for HTML output ----------------------------------------------\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\n\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.doctest\',\n    # \'sphinx.ext.intersphinx\',\n    # \'sphinx.ext.todo\',\n    # \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    # \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx_rtd_theme\',\n]\n# Autodoc settings\n# autodoc_default_flags = [\'members\', \'undoc-members\', \'inherited-members\']\n\n# Autosummary settings\nautosummary_generate = True\n\n# Napoleon settings (used to parse google and numpy style docstrings)\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = False\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = False\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = []\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'NLP Architect by Intel\xc2\xae AI Lab\'\ncopyright = u\'NLP Architect by Intel\xc2\xae AI Lab is a trademarks of Intel Corporation or its subsidiaries \\\n            in the U.S. and/or other countries. * Other names and brands may be claimed as \\\n            the property of others.\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y.Z version.\nversion = NLP_ARCHITECT_VERSION\n# The full version, including git-hash and alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'default\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n# html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\nhtml_theme_options = {\n    # \'canonical_url\': \'\',\n    # \'analytics_id\': \'\',\n    \'logo_only\': True,\n    \'display_version\': False,\n    # \'prev_next_buttons_location\': \'bottom\',\n    \'prev_next_buttons_location\': None,\n    \'style_external_links\': False,\n    # \'vcs_pageview_mode\': \'\',\n    # Toc options\n    \'collapse_navigation\': True,\n    \'sticky_navigation\': True,\n    \'navigation_depth\': 4,\n    \'includehidden\': True,\n    \'titles_only\': False\n}\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \'assets/logo.png\'\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\nhtml_favicon = \'\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'static/\']\nhtml_css_files = [\n    \'nlp_arch_theme.css\',\n    # \'https://fonts.googleapis.com/css?family=Lato\',\n    # \'https://fonts.googleapis.com/css?family=Oswald\',\n    \'https://fonts.googleapis.com/css?family=Roboto+Mono\',\n    \'https://fonts.googleapis.com/css?family=Open+Sans:100,900\'\n]\n\nhtml_js_files = [\n    \'install.js\'\n]\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\nhtml_show_sourcelink = False\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\nhtml_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\nhtml_show_copyright = False\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'intelaidoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [(\'index\', u\'NLP Architect Documentation\',\n                    u\'Intel Corporation\', \'manual\'), ]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', u\'NLP Architect Documentation\',\n     [u\'Intel\'], 1)\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  directory menu entry, description, category)\ntexinfo_documents = [(\'index\', u\'NLP Architect Documentation\',\n                      u\'Intel Corporation\'), ]\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'http://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n}\n\n# These go in every file\nrst_epilog = """"""\n.. include :: <isonum.txt>\n.. |ngraph| replace:: ngraph\n.. |NLP-Architect| replace:: NLP Architect\n.. |Geon| replace:: Nervana Graph\n.. |TF| replace:: TensorFlow\\ |trade|\n""""""\ndef run_apidoc(_):\n    api_docs = os.path.join(os.path.abspath(""./source/""), ""generated_api"")\n    argv = [""-f"", ""-o"", api_docs, os.path.abspath(""../nlp_architect/"")]\n    \n    apidoc.main(argv)\n    os.remove(os.path.join(api_docs, ""modules.rst""))\n    os.remove(os.path.join(api_docs, ""nlp_architect.rst""))\n\ndef setup(app):\n    app.connect(""builder-inited"", run_apidoc)\n'"
examples/absa/__init__.py,0,b''
examples/chunker/__init__.py,0,b''
examples/chunker/inference.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\nimport pickle\nfrom os import path\n\nimport numpy as np\n\nfrom nlp_architect.models.chunker import SequenceChunker\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.io import check_size, validate_existing_filepath\nfrom nlp_architect.utils.text import SpacyInstance\n\n\ndef vectorize(docs, w_vocab, c_vocab):\n    data = []\n    for doc in docs:\n        words = np.asarray(\n            [w_vocab[w.lower()] if w_vocab[w.lower()] is not None else 1 for w in doc]\n        ).reshape(1, -1)\n        if c_vocab is not None:\n            sentence_chars = []\n            for w in doc:\n                word_chars = []\n                for c in w:\n                    _cid = c_vocab[c]\n                    word_chars.append(_cid if _cid is not None else 1)\n                sentence_chars.append(word_chars)\n            sentence_chars = np.expand_dims(pad_sentences(sentence_chars, word_length), axis=0)\n            data.append((words, sentence_chars))\n        else:\n            data.append(words)\n    return data\n\n\ndef build_annotation(documents, annotations):\n    for d, i in zip(documents, annotations):\n        for w, c in zip(d, i):\n            print(""{}\\t{}"".format(w, c))\n        print("""")\n\n\nif __name__ == ""__main__"":\n    # read input args and validate\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--input_file"",\n        type=validate_existing_filepath,\n        required=True,\n        help=""Input texts file path (samples to pass for inference)"",\n    )\n    parser.add_argument(\n        ""--model_name"",\n        default=""chunker_model"",\n        type=str,\n        required=True,\n        help=""Model name (used for saving the model)"",\n    )\n    parser.add_argument(\n        ""-b"", type=int, action=check_size(1, 9999), default=1, help=""inference batch size""\n    )\n    args = parser.parse_args()\n    model_path = path.join(\n        path.dirname(path.realpath(__file__)), ""{}.h5"".format(str(args.model_name))\n    )\n    settings_path = path.join(\n        path.dirname(path.realpath(__file__)), ""{}.params"".format(str(args.model_name))\n    )\n    validate_existing_filepath(model_path)\n    validate_existing_filepath(settings_path)\n\n    # load model and parameters\n    model = SequenceChunker()\n    model.load(model_path)\n    word_length = model.max_word_len\n    with open(settings_path, ""rb"") as fp:\n        model_params = pickle.load(fp)\n        word_vocab = model_params[""word_vocab""]\n        chunk_vocab = model_params[""chunk_vocab""]\n        char_vocab = model_params.get(""char_vocab"", None)\n\n    # parse documents and get tokens\n    nlp = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n    with open(args.input_file) as fp:\n        document_texts = [nlp.tokenize(t.strip()) for t in fp.readlines()]\n\n    # vectorize input tokens and run inference\n    doc_vecs = vectorize(document_texts, word_vocab, char_vocab)\n    document_annotations = []\n    for vec in doc_vecs:\n        doc_chunks = model.predict(vec, batch_size=args.b)\n        chunk_a = [chunk_vocab.id_to_word(cid) for cid in doc_chunks.argmax(2).flatten()]\n        document_annotations.append(chunk_a)\n\n    # print document text and annotations\n    build_annotation(document_texts, document_annotations)\n'"
examples/chunker/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import division, print_function, unicode_literals, absolute_import\n\nimport argparse\nimport pickle\nfrom os import path\n\nfrom tensorflow import keras\n\nfrom nlp_architect.nn.tensorflow.python.keras.callbacks import ConllCallback\nfrom nlp_architect.data.sequential_tagging import CONLL2000\nfrom nlp_architect.models.chunker import SequenceChunker\nfrom nlp_architect.utils.embedding import load_word_embeddings, get_embedding_matrix\nfrom nlp_architect.utils.io import (\n    validate_existing_filepath,\n    validate_parent_exists,\n    validate,\n    validate_existing_directory,\n)\nfrom nlp_architect.utils.metrics import get_conll_scores\n\n\ndef create_argument_parser():\n    _parser = argparse.ArgumentParser()\n    _parser.add_argument(\n        ""--data_dir"",\n        type=validate_existing_directory,\n        help=""Path to directory containing CONLL2000 files"",\n    )\n    _parser.add_argument(\n        ""--embedding_model"",\n        type=validate_existing_filepath,\n        help=""Word embedding model path (GloVe/Fasttext/textual)"",\n    )\n    _parser.add_argument(""--sentence_length"", default=50, type=int, help=""Maximum sentence length"")\n    _parser.add_argument(\n        ""--char_features"",\n        default=False,\n        action=""store_true"",\n        help=""use word character features in addition to words"",\n    )\n    _parser.add_argument(\n        ""--max_word_length"",\n        type=int,\n        default=12,\n        help=""maximum number of character in one word "" ""(if --char_features is enabled)"",\n    )\n    _parser.add_argument(\n        ""--feature_size"",\n        default=100,\n        type=int,\n        help=""Feature vector size (in embedding and LSTM layers)"",\n    )\n    _parser.add_argument(\n        ""--use_cudnn"", default=False, action=""store_true"", help=""use CUDNN based LSTM cells""\n    )\n    _parser.add_argument(\n        ""--classifier"",\n        default=""crf"",\n        choices=[""crf"", ""softmax""],\n        type=str,\n        help=""classifier to use in last layer"",\n    )\n    _parser.add_argument(""-b"", default=10, type=int, help=""batch size"")\n    _parser.add_argument(""-e"", default=10, type=int, help=""number of epochs run fit model"")\n    _parser.add_argument(\n        ""--model_name"",\n        default=""chunker_model"",\n        type=str,\n        help=""Model name (used for saving the model)"",\n    )\n    return _parser\n\n\ndef _save_model():\n    model_params = {\n        ""word_vocab"": dataset.word_vocab,\n        ""pos_vocab"": dataset.pos_vocab,\n        ""chunk_vocab"": dataset.chunk_vocab,\n    }\n    if args.char_features is True:\n        model_params.update({""char_vocab"": dataset.char_vocab})\n    with open(settings_path, ""wb"") as fp:\n        pickle.dump(model_params, fp)\n    model.save(model_path)\n\n\nif __name__ == ""__main__"":\n    # read input args and validate\n    parser = create_argument_parser()\n    args = parser.parse_args()\n    validate((args.sentence_length, int, 1, 1000))\n    validate((args.feature_size, int, 1, 10000))\n    validate((args.b, int, 1, 100000))\n    validate((args.e, int, 1, 100000))\n    model_path = path.join(\n        path.dirname(path.realpath(__file__)), ""{}.h5"".format(str(args.model_name))\n    )\n    settings_path = path.join(\n        path.dirname(path.realpath(__file__)), ""{}.params"".format(str(args.model_name))\n    )\n    validate_parent_exists(model_path)\n\n    # load dataset and get tokens/chunks/pos tags\n    dataset = CONLL2000(\n        data_path=args.data_dir,\n        sentence_length=args.sentence_length,\n        extract_chars=args.char_features,\n        max_word_length=args.max_word_length,\n    )\n    train_set = dataset.train_set\n    test_set = dataset.test_set\n    words_train, pos_train, chunk_train = train_set[:3]\n    words_test, pos_test, chunk_test = test_set[:3]\n\n    # get label sizes, transform y\'s into 1-hot encoding\n    chunk_labels = len(dataset.chunk_vocab) + 1\n    pos_labels = len(dataset.pos_vocab) + 1\n    word_vocab_size = len(dataset.word_vocab) + 2\n    char_vocab_size = None\n    if args.char_features is True:\n        char_train = train_set[3]\n        char_test = test_set[3]\n        char_vocab_size = len(dataset.char_vocab) + 2\n\n    pos_train = keras.utils.to_categorical(pos_train, num_classes=pos_labels)\n    chunk_train = keras.utils.to_categorical(chunk_train, num_classes=chunk_labels)\n    pos_test = keras.utils.to_categorical(pos_test, num_classes=pos_labels)\n    chunk_test = keras.utils.to_categorical(chunk_test, num_classes=chunk_labels)\n\n    # build model with input parameters\n    model = SequenceChunker(use_cudnn=args.use_cudnn)\n    model.build(\n        word_vocab_size,\n        pos_labels,\n        chunk_labels,\n        char_vocab_size=char_vocab_size,\n        max_word_len=args.max_word_length,\n        feature_size=args.feature_size,\n        classifier=args.classifier,\n    )\n\n    # initialize word embedding if external model selected\n    if args.embedding_model is not None:\n        embedding_model, _ = load_word_embeddings(args.embedding_model)\n        embedding_mat = get_embedding_matrix(embedding_model, dataset.word_vocab)\n        model.load_embedding_weights(embedding_mat)\n\n    # train the model\n    if args.char_features is True:\n        train_features = [words_train, char_train]\n        test_features = [words_test, char_test]\n    else:\n        train_features = words_train\n        test_features = words_test\n    train_labels = [pos_train, chunk_train]\n    test_labels = [pos_test, chunk_test]\n    chunk_f1_cb = ConllCallback(test_features, chunk_test, dataset.chunk_vocab.vocab, batch_size=64)\n    model.fit(\n        train_features,\n        train_labels,\n        epochs=args.e,\n        batch_size=args.b,\n        validation_data=(test_features, test_labels),\n        callbacks=[chunk_f1_cb],\n    )\n\n    # save model\n    _save_model()\n\n    # load model\n    model = SequenceChunker(use_cudnn=args.use_cudnn)\n    model.load(model_path)\n\n    # print evaluation metric\n    chunk_pred = model.predict(test_features, 64)\n    res = get_conll_scores(chunk_pred, chunk_test, dataset.chunk_vocab.reverse_vocab())\n    print(res)\n'"
examples/cross_doc_coref/__init__.py,0,b''
examples/cross_doc_coref/cross_doc_coref_sieves.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nfrom typing import List\n\nfrom nlp_architect import LIBRARY_ROOT\nfrom nlp_architect.common.cdc.cluster import Clusters\nfrom nlp_architect.common.cdc.topics import Topics\nfrom nlp_architect.data.cdc_resources.relations.referent_dict_relation_extraction import (\n    ReferentDictRelationExtraction,\n)\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import RelationType\nfrom nlp_architect.data.cdc_resources.relations.wikipedia_relation_extraction import (\n    WikipediaRelationExtraction,\n)\nfrom nlp_architect.data.cdc_resources.relations.word_embedding_relation_extraction import (\n    WordEmbeddingRelationExtraction,\n)\nfrom nlp_architect.models.cross_doc_coref.sieves_config import (\n    EventSievesConfiguration,\n    EntitySievesConfiguration,\n)\nfrom nlp_architect.models.cross_doc_coref.sieves_resource import SievesResources\nfrom nlp_architect.models.cross_doc_coref.system.sieves_container_init import (\n    SievesContainerInitialization,\n)\nfrom nlp_architect.models.cross_doc_sieves import run_event_coref, run_entity_coref\n\n\ndef run_example(cdc_settings):\n    event_mentions_topics = Topics()\n    event_mentions_topics.create_from_file(\n        str(LIBRARY_ROOT / ""datasets"" / ""ecb"" / ""ecb_all_event_mentions.json"")\n    )\n\n    event_clusters = None\n    if cdc_settings.event_config.run_evaluation:\n        logger.info(""Running event coreference resolution"")\n        event_clusters = run_event_coref(event_mentions_topics, cdc_settings)\n\n    entity_mentions_topics = Topics()\n    entity_mentions_topics.create_from_file(\n        str(LIBRARY_ROOT / ""datasets"" / ""ecb"" / ""ecb_all_entity_mentions.json"")\n    )\n    entity_clusters = None\n    if cdc_settings.entity_config.run_evaluation:\n        logger.info(""Running entity coreference resolution"")\n        entity_clusters = run_entity_coref(entity_mentions_topics, cdc_settings)\n\n    return event_clusters, entity_clusters\n\n\ndef load_modules(cdc_resources):\n    models = list()\n    models.append(\n        WikipediaRelationExtraction(\n            cdc_resources.wiki_search_method,\n            wiki_file=cdc_resources.wiki_folder,\n            host=cdc_resources.elastic_host,\n            port=cdc_resources.elastic_port,\n            index=cdc_resources.elastic_index,\n        )\n    )\n    models.append(\n        WordEmbeddingRelationExtraction(\n            cdc_resources.embed_search_method,\n            glove_file=cdc_resources.glove_file,\n            elmo_file=cdc_resources.elmo_file,\n            cos_accepted_dist=0.75,\n        )\n    )\n    models.append(\n        ReferentDictRelationExtraction(\n            cdc_resources.referent_dict_method, cdc_resources.referent_dict_file\n        )\n    )\n    return models\n\n\ndef create_example_settings():\n    event_config = EventSievesConfiguration()\n    event_config.sieves_order = [\n        (RelationType.SAME_HEAD_LEMMA, 1.0),\n        (RelationType.WIKIPEDIA_DISAMBIGUATION, 0.1),\n        (RelationType.WORD_EMBEDDING_MATCH, 0.7),\n    ]\n\n    entity_config = EntitySievesConfiguration()\n    entity_config.sieves_order = [\n        (RelationType.SAME_HEAD_LEMMA, 1.0),\n        (RelationType.WIKIPEDIA_REDIRECT_LINK, 0.1),\n        (RelationType.WIKIPEDIA_DISAMBIGUATION, 0.1),\n        (RelationType.WORD_EMBEDDING_MATCH, 0.7),\n        (RelationType.REFERENT_DICT, 0.5),\n    ]\n\n    # CDCResources hold default attribute values that might need to be change,\n    # (using the defaults values in this example), use to configure attributes\n    # such as resources files location, output directory, resources init methods and other.\n    # check in class and see if any attributes require change in your set-up\n    resource_location = SievesResources()\n    return SievesContainerInitialization(\n        event_config, entity_config, load_modules(resource_location)\n    )\n\n\ndef print_results(clusters: List[Clusters], type: str):\n    print(""-="" + type + "" Clusters=-"")\n    for topic_cluster in clusters:\n        print(""\\n\\tTopic="" + topic_cluster.topic_id)\n        for cluster in topic_cluster.clusters_list:\n            cluster_mentions = list()\n            for mention in cluster.mentions:\n                mentions_dict = dict()\n                mentions_dict[""id""] = mention.mention_id\n                mentions_dict[""text""] = mention.tokens_str\n                cluster_mentions.append(mentions_dict)\n\n            print(""\\t\\tCluster("" + str(cluster.coref_chain) + "") Mentions="" + str(cluster_mentions))\n\n\ndef run_cdc_pipeline():\n    cdc_settings = create_example_settings()\n    event_clusters, entity_clusters = run_example(cdc_settings)\n\n    print(""-=Cross Document Coref Results=-"")\n    print_results(event_clusters, ""Event"")\n    print(""################################"")\n    print_results(entity_clusters, ""Entity"")\n\n\nif __name__ == ""__main__"":\n    logger = logging.getLogger(__name__)\n\n    run_cdc_pipeline()\n'"
examples/cross_doc_coref/relation_extraction_example.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\n\nfrom nlp_architect import LIBRARY_ROOT\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.relations.computed_relation_extraction import (\n    ComputedRelationExtraction,\n)\nfrom nlp_architect.data.cdc_resources.relations.referent_dict_relation_extraction import (\n    ReferentDictRelationExtraction,\n)\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import (\n    RelationType,\n    EmbeddingMethod,\n)\nfrom nlp_architect.data.cdc_resources.relations.verbocean_relation_extraction import (\n    VerboceanRelationExtraction,\n)\nfrom nlp_architect.data.cdc_resources.relations.wikipedia_relation_extraction import (\n    WikipediaRelationExtraction,\n)\nfrom nlp_architect.data.cdc_resources.relations.word_embedding_relation_extraction import (\n    WordEmbeddingRelationExtraction,\n)\nfrom nlp_architect.data.cdc_resources.relations.wordnet_relation_extraction import (\n    WordnetRelationExtraction,\n)\n\n\ndef run_example():\n    logger.info(""Running relation extraction example......"")\n    computed = ComputedRelationExtraction()\n    ref_dict = ReferentDictRelationExtraction(\n        ref_dict=str(LIBRARY_ROOT / ""datasets"" / ""coref.dict1.tsv"")\n    )\n    vo = VerboceanRelationExtraction(\n        vo_file=str(LIBRARY_ROOT / ""datasets"" / ""verbocean.unrefined.2004-05-20.txt"")\n    )\n    wiki = WikipediaRelationExtraction()\n    wn = WordnetRelationExtraction()\n    embed = WordEmbeddingRelationExtraction(method=EmbeddingMethod.ELMO)\n\n    mention_x1 = MentionDataLight(\n        ""IBM"",\n        mention_context=""IBM manufactures and markets computer hardware, middleware and software"",\n    )\n    mention_y1 = MentionDataLight(\n        ""International Business Machines"",\n        mention_context=""International Business Machines Corporation is an ""\n        ""American multinational information technology company"",\n    )\n\n    computed_relations = computed.extract_all_relations(mention_x1, mention_y1)\n    ref_dict_relations = ref_dict.extract_all_relations(mention_x1, mention_y1)\n    vo_relations = vo.extract_all_relations(mention_x1, mention_y1)\n    wiki_relations = wiki.extract_all_relations(mention_x1, mention_y1)\n    embed_relations = embed.extract_all_relations(mention_x1, mention_y1)\n    wn_relaions = wn.extract_all_relations(mention_x1, mention_y1)\n\n    if RelationType.NO_RELATION_FOUND in computed_relations:\n        logger.info(""No Computed relation found"")\n    else:\n        logger.info(""Found Computed relations-%s"", str(list(computed_relations)))\n\n    if RelationType.NO_RELATION_FOUND in ref_dict_relations:\n        logger.info(""No Referent-Dict relation found"")\n    else:\n        logger.info(""Found Referent-Dict relations-%s"", str(list(ref_dict_relations)))\n\n    if RelationType.NO_RELATION_FOUND in vo_relations:\n        logger.info(""No Verb-Ocean relation found"")\n    else:\n        logger.info(""Found Verb-Ocean relations-%s"", str(list(vo_relations)))\n\n    if RelationType.NO_RELATION_FOUND in wiki_relations:\n        logger.info(""No Wikipedia relation found"")\n    else:\n        logger.info(""Found Wikipedia relations-%s"", str(wiki_relations))\n    if RelationType.NO_RELATION_FOUND in embed_relations:\n        logger.info(""No Embedded relation found"")\n    else:\n        logger.info(""Found Embedded relations-%s"", str(list(embed_relations)))\n    if RelationType.NO_RELATION_FOUND in wn_relaions:\n        logger.info(""No Wordnet relation found"")\n    else:\n        logger.info(""Found Wordnet relations-%s"", str(wn_relaions))\n\n\nif __name__ == ""__main__"":\n    logger = logging.getLogger(__name__)\n    run_example()\n'"
examples/crosslingembs/evaluate.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nlp_architect.data.fasttext_emb import get_eval_data\n\n\nclass Evaluate:\n    """"""\n    Class for evaluating the performance of mapping W\n    """"""\n\n    def __init__(\n        self, W, src_vec, tgt_vec, src_dico, tgt_dico, src_lang, tgt_lang, eval_path, vocab_size\n    ):\n        self.W = W\n        self.src_ten = src_vec\n        self.tgt_ten = tgt_vec\n        self.src_dico = src_dico\n        self.tgt_dico = tgt_dico\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.eval_path = eval_path\n        self.vocab_size = vocab_size\n        self.best_cos_score = -1e9\n        self.drop_lr = False\n        self.second_drop = False\n        self.save_model = False\n        self.max_dict_size = 15000\n\n        # Lookup ids\n        self.tgt_ids = [x for x in range(self.vocab_size)]\n\n        # Placeholders\n        self.src_ph = tf.placeholder(tf.int32, shape=[None], name=""EvalSrcPh"")\n        self.tgt_ph = tf.placeholder(tf.int32, shape=[None], name=""EvalTgtPh"")\n        self.score_ph = tf.placeholder(tf.float32, shape=[None, self.vocab_size], name=""ScorePh"")\n\n        # Preprocessing for evaluation\n        self.load_eval_dataset()\n        self.prepare_emb()\n\n        # Graph for evaluation\n        self.eval_nn = self.build_eval_graph(""NNEval"", self.src_ten, self.tgt_ten, en_mean=False)\n        self.csls_subgraphs = self.build_csls_subgraphs()\n\n    def load_eval_dataset(self):\n        """"""\n        This method read through the test file and gets their embedding indices\n        """"""\n        dict_path = get_eval_data(self.eval_path, self.src_lang, self.tgt_lang)\n\n        pairs = []\n        not_found_all = 0\n        not_found_L1 = 0\n        not_found_L2 = 0\n\n        # Open the file and check if src and tgt word exists in the vocab\n        with open(dict_path, ""r"") as f:\n            for _, line in enumerate(f):\n                word1, word2 = line.rstrip().split()\n                if word1 in self.src_dico and word2 in self.tgt_dico:\n                    pairs.append((self.src_dico.index(word1), self.tgt_dico.index(word2)))\n                else:\n                    not_found_all += 1\n                    not_found_L1 += int(word1 not in self.src_dico)\n                    not_found_L2 += int(word2 not in self.tgt_dico)\n        print(\n            ""Found %i pairs of words in the dictionary (%i unique). ""\n            "" %i other pairs contained at least one unknown word ""\n            "" (%i in src_lang, %i in tgt_lang)""\n            % (\n                len(pairs),\n                len(set([x for x, _ in pairs])),\n                not_found_all,\n                not_found_L1,\n                not_found_L2,\n            )\n        )\n        src_ind = [pairs[x][0] for x in range(len(pairs))]\n        tgt_ind = [pairs[x][1] for x in range(len(pairs))]\n        self.src_ind = np.asarray(src_ind)\n        self.tgt_ind = np.asarray(tgt_ind)\n\n    def prepare_emb(self):\n        """"""\n        Maps source embedding with the help of learnt mapping W.\n        It also normalizes the embeddings to make it easy for similarity measurements.\n        """"""\n        with tf.variable_scope(""PrepEmb"", reuse=tf.AUTO_REUSE):\n            self.src_ten = tf.cast(tf.convert_to_tensor(self.src_ten), tf.float32)\n            self.tgt_ten = tf.cast(tf.convert_to_tensor(self.tgt_ten), tf.float32)\n            # Mapping\n            self.src_ten = tf.matmul(self.src_ten, self.W)\n            # Normalization\n            self.src_ten = tf.nn.l2_normalize(self.src_ten, axis=1)\n            self.tgt_ten = tf.nn.l2_normalize(self.tgt_ten, axis=1)\n\n    def build_eval_graph(self, name, emb_ten, query_ten, knn=100, en_knn=True, en_mean=True):\n        """"""\n        Build a simple evaluation graph\n\n        Arguments:\n            name (str) : Name of the graph\n            emb_ten (tensor): Embedding tensor\n            query_ten (tensor): Query tensor\n            knn (int): K value for NN\n            en_knn (bool): enable or disable knn portion of the graph\n            en_mean (bool): enable or disabel mean portion of the graph\n\n        Returns:\n            Returns a graph depending on booleans above\n        """"""\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n            # Look Up the words\n            emb_lut = tf.nn.embedding_lookup(emb_ten, self.src_ph, name=""EvalSrcLut"")\n            query_lut = tf.nn.embedding_lookup(query_ten, self.tgt_ph, name=""EvalTgtLut"")\n            # Cast\n            emb = tf.cast(emb_lut, tf.float32)\n            query = tf.cast(query_lut, tf.float32)\n            # MM\n            sim_scores = tf.matmul(emb, tf.transpose(query))\n            # Topk\n            if en_knn and not en_mean:\n                top_matches = tf.nn.top_k(sim_scores, knn)\n                return top_matches\n            if en_knn and en_mean:\n                top_matches = tf.nn.top_k(sim_scores, knn)\n                best_distances = tf.reduce_mean(top_matches[0], axis=1)\n                return best_distances\n            return sim_scores\n\n    def build_sim_graph(self, name):\n        """"""\n        Builds a similarity measurement graph\n\n        Arguments:\n            name(str): Name of the graph\n        Returns:\n            Returns a handle to the graph\n        """"""\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n            # Look up the words\n            emb_lut = tf.nn.embedding_lookup(self.src_ten, self.src_ph, name=""Src_Lut"")\n            query_lut = tf.nn.embedding_lookup(self.tgt_ten, self.tgt_ph, name=""Tgt_Lut"")\n            # Cast\n            emb = tf.cast(emb_lut, tf.float32)\n            query = tf.cast(query_lut, tf.float32)\n            # Score it\n            score = emb * query\n            cos_sum = tf.reduce_sum(score, axis=1)\n            cos_mean = tf.reduce_mean(cos_sum)\n            return cos_mean\n\n    def build_csls_subgraphs(self):\n        """"""\n        Builds various graphs needed for CSLS calculation\n\n        Returns:\n            Returns a dictionary with various graphs constructed\n\n        """"""\n        # Graph for calculating only score\n        scores_s2t = self.build_eval_graph(\n            ""ScoreS2T"", self.src_ten, self.tgt_ten, en_knn=False, en_mean=False\n        )\n        scores_t2s = self.build_eval_graph(\n            ""ScoreT2S"", self.tgt_ten, self.src_ten, en_knn=False, en_mean=False\n        )\n        # Graphs for calculating average between source and target\n        avg1_s2t = self.build_eval_graph(""Avg1"", self.src_ten, self.tgt_ten, knn=10)\n        avg2_s2t = self.build_eval_graph(""Avg2"", self.tgt_ten, self.src_ten, knn=10)\n        # Graph for selecting top 100 elements\n        top100_matches = tf.nn.top_k(self.score_ph, 100)\n        # Graph for selecting top 2 elements\n        top2_matches = tf.nn.top_k(self.score_ph, 2)\n        # Graph for calculating similarity\n        csls_mean_score = self.build_sim_graph(""SimGraph"")\n\n        # Dictionary\n        csls_graphs = {\n            ""ScoreGraph"": scores_s2t,\n            ""ScoreG_T2S"": scores_t2s,\n            ""Avg1S2T"": avg1_s2t,\n            ""Avg2S2T"": avg2_s2t,\n            ""Top100"": top100_matches,\n            ""Top2"": top2_matches,\n            ""CSLS_Criteria"": csls_mean_score,\n        }\n        return csls_graphs\n\n    def calc_nn_acc(self, sess, batch_size=512):\n        """"""\n        Evaluates accuracy of mapping using Nearest neighbors\n        Arguments:\n            sess(tf.session): Tensorflow Session\n            batch_size(int): Size of batch\n        """"""\n        top_matches = []\n        eval_size = len(self.src_ind)\n\n        # Loop through all the eval dataset\n        for i in range(0, eval_size, batch_size):\n            src_ids = [self.src_ind[x] for x in range(i, min(i + batch_size, eval_size))]\n            eval_dict = {self.src_ph: src_ids, self.tgt_ph: self.tgt_ids}\n            matches = sess.run(self.eval_nn, feed_dict=eval_dict)\n            top_matches.append(matches[1])\n        top_matches = np.concatenate(top_matches)\n\n        print(""Accuracy using Nearest Neighbors is"")\n        self.calc_accuracy(top_matches)\n\n    def calc_accuracy(self, top_matches):\n        """"""\n        Takes top_matches generated by tf.nn.top_k and calculates accuracy\n        Arguments:\n             top_matches: Output of tf.nn.topk_k[1]\n        """"""\n        # Calculate Translation accuracy\n        # Accuracy at K\n        for k in [1, 5, 10]:\n            top_k_matches = top_matches[:, :k]\n            # Checks for one match\n            one_matching = (top_k_matches == self.tgt_ind[:, None]).sum(1)\n            # Checks for multiple translations\n            matching = {}\n            for i, src_id in enumerate(self.src_ind):\n                matching[src_id] = min(matching.get(src_id, 0) + one_matching[i], 1)\n            precision_at_k = 100 * np.mean(list(matching.values()))\n            print(""%i source words  - Precision at k = %i: %f"" % (len(matching), k, precision_at_k))\n\n    def calc_csls_score(self, sess, batch_size=512):\n        """"""\n        Calculates similarity score between two embeddings\n        Arguments:\n            sess(tf.session): Tensorflow Session\n            batch_size(int): Size of batch to process\n        Returns:\n            Returns similarity score numpy array\n        """"""\n        score_val = []\n        eval_size = len(self.src_ind)\n        # Calculate scores\n        for i in range(0, eval_size, batch_size):\n            score_src_ids = [self.src_ind[x] for x in range(i, min(i + batch_size, eval_size))]\n            eval_dict = {self.src_ph: score_src_ids, self.tgt_ph: self.tgt_ids}\n            score_val.append(sess.run(self.csls_subgraphs[""ScoreGraph""], feed_dict=eval_dict))\n        score_val = np.concatenate(score_val)\n        return score_val\n\n    def calc_avg_dist(self, sess, batch_size=512):\n        """"""\n        Calculates average distance between two embeddings\n        Arguments:\n            sess(tf.session): Tensorflow session\n            batch_size(int): batch_size\n        Returns:\n            Returns numpy array of average values of size vocab_size\n        """"""\n        avg1_val = []\n        avg2_val = []\n\n        # Calculate Average\n        for i in range(0, self.vocab_size, batch_size):\n            avg_src_ids = [x for x in range(i, min(i + batch_size, self.vocab_size))]\n            avg1_dict = {self.src_ph: avg_src_ids, self.tgt_ph: self.tgt_ids}\n            avg1_val.append(sess.run(self.csls_subgraphs[""Avg1S2T""], feed_dict=avg1_dict))\n            avg2_val.append(sess.run(self.csls_subgraphs[""Avg2S2T""], feed_dict=avg1_dict))\n        avg1_val = np.concatenate(avg1_val)\n        avg2_val = np.concatenate(avg2_val)\n        return avg1_val, avg2_val\n\n    def run_csls_metrics(self, sess, batch_size=512):\n        """"""\n        Runs the whole CSLS metrics\n        Arguments:\n            sess(tf.session): Tensorflow Session\n            batch_size(int): Batch Size\n        """"""\n        top_matches = []\n        score = self.calc_csls_score(sess)\n        avg1, avg2 = self.calc_avg_dist(sess)\n        csls_scores = 2 * score - (avg1[self.src_ind][:, None] + avg2[None, :])\n        # Calculate top matches\n        for i in range(0, len(self.src_ind), batch_size):\n            scores = [csls_scores[x] for x in range(i, min(i + batch_size, len(self.src_ind)))]\n            top_matches_val = sess.run(\n                self.csls_subgraphs[""Top100""], feed_dict={self.score_ph: scores}\n            )[1]\n            top_matches.append(top_matches_val)\n        top_matches = np.concatenate(top_matches)\n        print(""Accuracy using CSLS is"")\n        self.calc_accuracy(top_matches)\n        self.calc_csls(sess)\n\n    def calc_csls(self, sess):\n        """"""\n        Calculates the value of CSLS criterion\n        Arguments:\n            sess(tf.session): Tensorflow session\n        """"""\n        good_pairs = self.generate_dictionary(sess)\n        eval_dict = {self.src_ph: good_pairs[0], self.tgt_ph: good_pairs[1]}\n        cos_mean = sess.run(self.csls_subgraphs[""CSLS_Criteria""], feed_dict=eval_dict)\n        print(""CSLS Score is "" + str(cos_mean))\n\n        # Drop LR only after the second drop in CSLS\n        if cos_mean < self.best_cos_score:\n            self.drop_lr = True & self.second_drop\n            self.second_drop = True\n\n        # Save model whenever cos score is better than saved score\n        if cos_mean > self.best_cos_score:\n            self.save_model = True\n        else:\n            self.save_model = False\n\n        # Update best cos score\n        if cos_mean > self.best_cos_score:\n            self.best_cos_score = cos_mean\n            self.drop_lr = False\n\n    def get_candidates(self, sess, avg1, avg2, batch_size=512, swap_score=False):\n        """"""\n        Get the candidates based on type of dictionary\n        Arguments:\n             sess(tf.session): Tensorflow session\n             dict_type(str): S2T-Source2Target, S2T&T2S (Both)\n        Returns:\n            Numpy array of max_dict x 2 or smaller\n        """"""\n        all_scores = []\n        all_targets = []\n        for i in range(0, self.max_dict_size, batch_size):\n            src_ids = [x for x in range(i, min(i + batch_size, self.max_dict_size))]\n            dict_dict = {self.src_ph: src_ids, self.tgt_ph: self.tgt_ids}\n            if swap_score:\n                temp_score = sess.run(self.csls_subgraphs[""ScoreG_T2S""], feed_dict=dict_dict)\n            else:\n                temp_score = sess.run(self.csls_subgraphs[""ScoreGraph""], feed_dict=dict_dict)\n            batch_score = 2 * temp_score - (avg1[src_ids][:, None] + avg2[None, :])\n            top_matches = sess.run(\n                self.csls_subgraphs[""Top2""], feed_dict={self.score_ph: batch_score}\n            )\n            all_scores.append(top_matches[0])\n            all_targets.append(top_matches[1])\n        all_scores = np.concatenate(all_scores)\n        all_targets = np.concatenate(all_targets)\n        all_pairs = np.concatenate(\n            [np.arange(0, self.max_dict_size, dtype=np.int64)[:, None], all_targets[:, 0][:, None]],\n            1,\n        )\n\n        # Scores with high confidence will have large difference between first two guesses\n        diff = all_scores[:, 0] - all_scores[:, 1]\n        reordered = np.argsort(diff, axis=0)\n        reordered = reordered[::-1]\n        all_pairs = all_pairs[reordered]\n\n        # Select words which are in top max_dict\n        selected = np.max(all_pairs, axis=1) <= self.max_dict_size\n        all_pairs = all_pairs[selected]\n\n        # Make sure size is less than max_dict\n        all_pairs = all_pairs[: self.max_dict_size]\n        return all_pairs\n\n    def generate_dictionary(self, sess, dict_type=""S2T""):\n        """"""\n        Generates best translation pairs\n        Arguments:\n             sess(tf.session): Tensorflow session\n             dict_type(str): S2T-Source2Target, S2T&T2S (Both)\n        Returns:\n            Numpy array of max_dict x 2 or smaller\n        """"""\n        avg1, avg2 = self.calc_avg_dist(sess)\n        s2t_dico = self.get_candidates(sess, avg1, avg2)\n        print(""Completed generating S2T dictionary of size "" + str(len(s2t_dico)))\n        if dict_type == ""S2T"":\n            map_src_ind = np.asarray([s2t_dico[x][0] for x in range(len(s2t_dico))])\n            tra_tgt_ind = np.asarray([s2t_dico[x][1] for x in range(len(s2t_dico))])\n            return [map_src_ind, tra_tgt_ind]\n        if dict_type == ""S2T&T2S"":\n            # This case we are running Target 2 Source mappings\n            t2s_dico = self.get_candidates(sess, avg2, avg1, swap_score=True)\n            print(""Completed generating T2S dictionary of size "" + str(len(t2s_dico)))\n            t2s_dico = np.concatenate([t2s_dico[:, 1:], t2s_dico[:, :1]], 1)\n            # Find the common pairs between S2T and T2S\n            s2t_candi = set([(a, b) for a, b in s2t_dico])\n            t2s_candi = set([(a, b) for a, b in t2s_dico])\n            final_pairs = s2t_candi & t2s_candi\n            dico = np.asarray(list([[a, b] for (a, b) in final_pairs]))\n            print(""Completed generating final dictionary of size "" + str(len(final_pairs)))\n            return dico\n'"
examples/crosslingembs/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import print_function, division\n\nimport argparse\nimport copy\n\nimport evaluate\nimport tensorflow as tf\n\nfrom nlp_architect.data.fasttext_emb import FastTextEmb\nfrom nlp_architect.models.crossling_emb import WordTranslator\nfrom nlp_architect.utils.io import validate_existing_directory, validate_parent_exists, check_size\n\nif __name__ == ""__main__"":\n\n    print(""\\t\\t"" + 40 * ""="")\n    print(""\\t\\t= Unsupervised Crosslingual Embeddings ="")\n    print(""\\t\\t"" + 40 * ""="")\n\n    # Parsing arguments for model parameters\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--emb_dim"", type=int, default=300, help=""Embedding Dimensions"", action=check_size(1, 1024)\n    )\n    parser.add_argument(\n        ""--vocab_size"",\n        type=int,\n        default=200000,\n        help=""Vocabulary Size"",\n        action=check_size(1, 1000000),\n    )\n    parser.add_argument(\n        ""--lr"", type=float, default=0.1, help=""Learning Rate"", action=check_size(0.00001, 2.0)\n    )\n    parser.add_argument(\n        ""--beta"",\n        type=float,\n        default=0.001,\n        help=""Beta for W orthogornaliztion"",\n        action=check_size(0.0000001, 5.0),\n    )\n    parser.add_argument(\n        ""--smooth_val"",\n        type=float,\n        default=0.1,\n        help=""Label smoother for\\\n                        discriminator"",\n        action=check_size(0.0001, 0.2),\n    )\n    parser.add_argument(\n        ""--batch_size"", type=int, default=32, help=""Batch size"", action=check_size(8, 1024)\n    )\n    parser.add_argument(\n        ""--epochs"", type=int, default=5, help=""Number of epochs to run"", action=check_size(1, 20)\n    )\n    parser.add_argument(\n        ""--iters_epoch"",\n        type=int,\n        default=1000000,\n        help=""Iterations to run\\\n                        each epoch"",\n        action=check_size(1, 2000000),\n    )\n    parser.add_argument(\n        ""--disc_runs"",\n        type=int,\n        default=5,\n        help=""Number of times\\\n                        discriminator is run each iteration"",\n        action=check_size(1, 20),\n    )\n    parser.add_argument(\n        ""--most_freq"",\n        type=int,\n        default=75000,\n        help=""Number of words to\\\n                        show discriminator"",\n        action=check_size(1, 1000000),\n    )\n    parser.add_argument(\n        ""--src_lang"", type=str, default=""en"", help=""Source Language"", action=check_size(1, 3)\n    )\n    parser.add_argument(\n        ""--tgt_lang"", type=str, default=""fr"", help=""Target Language"", action=check_size(1, 3)\n    )\n    parser.add_argument(\n        ""--data_dir"",\n        default=None,\n        help=""Data path for training and\\\n                        and evaluation data"",\n        type=validate_existing_directory,\n    )\n    parser.add_argument(\n        ""--eval_dir"", default=None, help=""Path for eval words"", type=validate_existing_directory\n    )\n    parser.add_argument(\n        ""--weight_dir"",\n        default=None,\n        help=""path to save mapping\\\n                        weights"",\n        type=validate_parent_exists,\n    )\n    hparams = parser.parse_args()\n    # Load Source Embeddings\n    src = FastTextEmb(hparams.data_dir, hparams.src_lang, hparams.vocab_size)\n    src_dict, src_vec = src.load_embeddings()\n    # Load Target Embeddings\n    tgt = FastTextEmb(hparams.data_dir, hparams.tgt_lang, hparams.vocab_size)\n    tgt_dict, tgt_vec = tgt.load_embeddings()\n\n    # GAN instance\n    train_model = WordTranslator(hparams, src_vec, tgt_vec, hparams.vocab_size)\n\n    # Copy embeddings\n    src_vec_eval = copy.deepcopy(src_vec)\n    tgt_vec_eval = copy.deepcopy(tgt_vec)\n\n    # Evaluator instance\n    eval_model = evaluate.Evaluate(\n        train_model.generator.W,\n        src_vec_eval,\n        tgt_vec_eval,\n        src_dict,\n        tgt_dict,\n        hparams.src_lang,\n        hparams.tgt_lang,\n        hparams.eval_dir,\n        hparams.vocab_size,\n    )\n\n    # Tensorflow session\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        local_lr = hparams.lr\n        for epoch in range(hparams.epochs):\n            # Train the model\n            train_model.run(sess, local_lr)\n            # Evaluate using nearest neighbors measure\n            eval_model.calc_nn_acc(sess)\n            # Evaluate using CSLS similarity measure\n            eval_model.run_csls_metrics(sess)\n            # Drop learning rate\n            local_lr = train_model.set_lr(local_lr, eval_model.drop_lr)\n            # Save model if it is good\n            train_model.save_model(eval_model.save_model, sess)\n            print(""End of epoch "" + str(epoch))\n        # Apply procrustes to improve CSLS score\n        final_pairs = eval_model.generate_dictionary(sess, dict_type=""S2T&T2S"")\n        train_model.apply_procrustes(sess, final_pairs)\n        # Run metrics to see improvement\n        eval_model.run_csls_metrics(sess)\n        # Save the model if there is imporvement\n        train_model.save_model(eval_model.save_model, sess)\n        # Write cross lingual embeddings to file\n        train_model.generate_xling_embed(sess, src_dict, tgt_dict, tgt_vec)\n    print(""Completed Training"")\n'"
examples/intent_extraction/interactive.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\nimport pickle\n\nimport numpy as np\n\nfrom nlp_architect.models.intent_extraction import MultiTaskIntentModel, Seq2SeqIntentModel\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.io import validate_existing_filepath\nfrom nlp_architect.utils.text import SpacyInstance\n\nnlp = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n\n\ndef read_input_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--model_path"", type=validate_existing_filepath, required=True, help=""Path of model weights""\n    )\n    parser.add_argument(\n        ""--model_info_path"",\n        type=validate_existing_filepath,\n        required=True,\n        help=""Path of model topology"",\n    )\n    input_args = parser.parse_args()\n    return input_args\n\n\ndef load_saved_model():\n    if model_type == ""seq2seq"":\n        model = Seq2SeqIntentModel()\n    else:\n        model = MultiTaskIntentModel()\n    model.load(args.model_path)\n    return model\n\n\ndef process_text(text):\n    input_text = "" "".join(text.strip().split())\n    return nlp.tokenize(input_text)\n\n\ndef vectorize(doc, vocab, char_vocab=None):\n    words = np.asarray([vocab[w.lower()] if w.lower() in vocab else 1 for w in doc]).reshape(1, -1)\n    if char_vocab is not None:\n        sentence_chars = []\n        for w in doc:\n            word_chars = []\n            for c in w:\n                if c in char_vocab:\n                    _cid = char_vocab[c]\n                else:\n                    _cid = 1\n                word_chars.append(_cid)\n            sentence_chars.append(word_chars)\n        sentence_chars = np.expand_dims(pad_sentences(sentence_chars, model.word_length), axis=0)\n        return [words, sentence_chars]\n    else:\n        return words\n\n\nif __name__ == ""__main__"":\n    args = read_input_args()\n    with open(args.model_info_path, ""rb"") as fp:\n        model_info = pickle.load(fp)\n    assert model_info is not None, ""No model topology information loaded""\n    model_type = model_info[""type""]\n    model = load_saved_model()\n    word_vocab = model_info[""word_vocab""]\n    tags_vocab = {v: k for k, v in model_info[""tags_vocab""].items()}\n    if model_type == ""mtl"":\n        char_vocab = model_info[""char_vocab""]\n        intent_vocab = {v: k for k, v in model_info[""intent_vocab""].items()}\n    while True:\n        text = input(""Enter sentence >> "")\n        text_arr = process_text(text)\n        if model_type == ""mtl"":\n            doc_vec = vectorize(text_arr, word_vocab, char_vocab)\n            intent, tags = model.predict(doc_vec, batch_size=1)\n            intent = int(intent.argmax(1).flatten())\n            print(""Detected intent type: {}"".format(intent_vocab.get(intent, None)))\n        else:\n            doc_vec = vectorize(text_arr, word_vocab, None)\n            tags = model.predict(doc_vec, batch_size=1)\n        tags = tags.argmax(2).flatten()\n        tag_str = [tags_vocab.get(n, None) for n in tags]\n        for t, n in zip(text_arr, tag_str):\n            print(""{}\\t{}\\t"".format(t, n))\n        print()\n'"
examples/intent_extraction/train_mtl_model.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\nimport pickle\nfrom os import path\n\nfrom tensorflow.python.keras.utils import to_categorical\n\nfrom nlp_architect.nn.tensorflow.python.keras.callbacks import ConllCallback\nfrom nlp_architect.data.intent_datasets import SNIPS\nfrom nlp_architect.models.intent_extraction import MultiTaskIntentModel\nfrom nlp_architect.utils.embedding import get_embedding_matrix, load_word_embeddings\nfrom nlp_architect.utils.generic import one_hot\nfrom nlp_architect.utils.io import (\n    validate,\n    validate_existing_directory,\n    validate_existing_filepath,\n    validate_parent_exists,\n)\nfrom nlp_architect.utils.metrics import get_conll_scores\n\n\ndef validate_input_args():\n    global model_path\n    validate((args.b, int, 1, 100000000))\n    validate((args.e, int, 1, 100000000))\n    validate((args.sentence_length, int, 1, 10000))\n    validate((args.token_emb_size, int, 1, 10000))\n    validate((args.intent_hidden_size, int, 1, 10000))\n    validate((args.lstm_hidden_size, int, 1, 10000))\n    validate((args.tagger_dropout, float, 0, 1))\n    model_path = path.join(path.dirname(path.realpath(__file__)), str(args.model_path))\n    validate_parent_exists(model_path)\n    model_info_path = path.join(path.dirname(path.realpath(__file__)), str(args.model_info_path))\n    validate_parent_exists(model_info_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-b"", type=int, default=10, help=""Batch size"")\n    parser.add_argument(""-e"", type=int, default=10, help=""Number of epochs"")\n    parser.add_argument(\n        ""--dataset_path"", type=validate_existing_directory, required=True, help=""dataset directory""\n    )\n    parser.add_argument(""--sentence_length"", type=int, default=30, help=""Max sentence length"")\n    parser.add_argument(\n        ""--token_emb_size"", type=int, default=100, help=""Token features embedding vector size""\n    )\n    parser.add_argument(\n        ""--intent_hidden_size"", type=int, default=100, help=""Intent detection LSTM hidden size""\n    )\n    parser.add_argument(\n        ""--lstm_hidden_size"", type=int, default=150, help=""Slot tags LSTM hidden size""\n    )\n    parser.add_argument(""--tagger_dropout"", type=float, default=0.5, help=""Slot tags dropout value"")\n    parser.add_argument(\n        ""--embedding_model"",\n        type=validate_existing_filepath,\n        help=""Path to word embedding model file"",\n    )\n    parser.add_argument(\n        ""--use_cudnn"", default=False, action=""store_true"", help=""use CUDNN based LSTM cells""\n    )\n    parser.add_argument(""--model_path"", type=str, default=""model.h5"", help=""Model file path"")\n    parser.add_argument(\n        ""--model_info_path"",\n        type=str,\n        default=""model_info.dat"",\n        help=""Path for saving model topology"",\n    )\n    args = parser.parse_args()\n    validate_input_args()\n\n    # load dataset\n    print(""Loading dataset"")\n    dataset = SNIPS(path=args.dataset_path, sentence_length=args.sentence_length)\n\n    train_x, train_char, train_i, train_y = dataset.train_set\n    test_x, test_char, test_i, test_y = dataset.test_set\n\n    test_y = to_categorical(test_y, dataset.label_vocab_size)\n    train_y = to_categorical(train_y, dataset.label_vocab_size)\n    train_i = one_hot(train_i, len(dataset.intents_vocab))\n    test_i = one_hot(test_i, len(dataset.intents_vocab))\n\n    train_inputs = [train_x, train_char]\n    train_outs = [train_i, train_y]\n    test_inputs = [test_x, test_char]\n    test_outs = [test_i, test_y]\n\n    print(""Building model"")\n    model = MultiTaskIntentModel(use_cudnn=args.use_cudnn)\n    model.build(\n        dataset.word_len,\n        dataset.label_vocab_size,\n        dataset.intent_size,\n        dataset.word_vocab_size,\n        dataset.char_vocab_size,\n        word_emb_dims=args.token_emb_size,\n        tagger_lstm_dims=args.lstm_hidden_size,\n        dropout=args.tagger_dropout,\n    )\n\n    # initialize word embedding if external model selected\n    if args.embedding_model is not None:\n        print(""Loading external word embedding"")\n        embedding_model, _ = load_word_embeddings(args.embedding_model)\n        embedding_mat = get_embedding_matrix(\n            embedding_model, dataset.word_vocab, dataset.word_vocab_size\n        )\n        model.load_embedding_weights(embedding_mat)\n\n    conll_cb = ConllCallback(test_inputs, test_y, dataset.tags_vocab.vocab, batch_size=args.b)\n    # train model\n    model.fit(\n        x=train_inputs,\n        y=train_outs,\n        batch_size=args.b,\n        epochs=args.e,\n        validation=(test_inputs, test_outs),\n        callbacks=[conll_cb],\n    )\n    print(""Training done"")\n\n    print(""Saving model"")\n    model.save(args.model_path)\n    with open(args.model_info_path, ""wb"") as fp:\n        info = {\n            ""type"": ""mtl"",\n            ""tags_vocab"": dataset.tags_vocab.vocab,\n            ""word_vocab"": dataset.word_vocab.vocab,\n            ""char_vocab"": dataset.char_vocab.vocab,\n            ""intent_vocab"": dataset.intents_vocab.vocab,\n        }\n        pickle.dump(info, fp)\n\n    # test performance\n    predictions = model.predict(test_inputs, batch_size=args.b)\n    eval = get_conll_scores(\n        predictions, test_y, {v: k for k, v in dataset.tags_vocab.vocab.items()}\n    )\n    print(eval)\n'"
examples/intent_extraction/train_seq2seq_model.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\nimport pickle\nfrom os import path\n\nfrom tensorflow.python.keras.utils import to_categorical\n\nfrom nlp_architect.nn.tensorflow import ConllCallback\nfrom nlp_architect.data.intent_datasets import SNIPS\nfrom nlp_architect.models.intent_extraction import Seq2SeqIntentModel\nfrom nlp_architect.utils.io import (\n    validate,\n    validate_existing_directory,\n    validate_existing_filepath,\n    validate_parent_exists,\n)\nfrom nlp_architect.utils.metrics import get_conll_scores\n\n\ndef validate_input_args():\n    global model_path\n    validate((args.b, int, 1, 100000000))\n    validate((args.e, int, 1, 100000000))\n    validate((args.sentence_length, int, 1, 10000))\n    validate((args.token_emb_size, int, 1, 10000))\n    validate((args.lstm_hidden_size, int, 1, 10000))\n    validate((args.encoder_depth, int, 1, 10))\n    validate((args.decoder_depth, int, 1, 10))\n    validate((args.encoder_dropout, float, 0, 1))\n    validate((args.decoder_dropout, float, 0, 1))\n    model_path = path.join(path.dirname(path.realpath(__file__)), str(args.model_path))\n    validate_parent_exists(model_path)\n    model_info_path = path.join(path.dirname(path.realpath(__file__)), str(args.model_info_path))\n    validate_parent_exists(model_info_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-b"", type=int, default=10, help=""Batch size"")\n    parser.add_argument(""-e"", type=int, default=10, help=""Number of epochs"")\n    parser.add_argument(\n        ""--dataset_path"", type=validate_existing_directory, required=True, help=""dataset directory""\n    )\n    parser.add_argument(""--sentence_length"", type=int, default=30, help=""Max sentence length"")\n    parser.add_argument(\n        ""--token_emb_size"", type=int, default=100, help=""Token features embedding vector size""\n    )\n    parser.add_argument(\n        ""--lstm_hidden_size"", type=int, default=150, help=""Encoder LSTM hidden size""\n    )\n    parser.add_argument(""--encoder_depth"", type=int, default=1, help=""Encoder LSTM depth"")\n    parser.add_argument(""--decoder_depth"", type=int, default=1, help=""Decoder LSTM depth"")\n    parser.add_argument(""--encoder_dropout"", type=float, default=0.5, help=""Encoder dropout value"")\n    parser.add_argument(""--decoder_dropout"", type=float, default=0.5, help=""Decoder dropout value"")\n    parser.add_argument(\n        ""--embedding_model"",\n        type=validate_existing_filepath,\n        help=""Path to word embedding model file"",\n    )\n    parser.add_argument(""--model_path"", type=str, default=""model.h5"", help=""Model file path"")\n    parser.add_argument(\n        ""--model_info_path"",\n        type=str,\n        default=""model_info.dat"",\n        help=""Path for saving model topology"",\n    )\n    args = parser.parse_args()\n    validate_input_args()\n    dataset = SNIPS(path=args.dataset_path, sentence_length=args.sentence_length)\n\n    train_x, _, train_i, train_y = dataset.train_set\n    test_x, _, test_i, test_y = dataset.test_set\n\n    test_y = to_categorical(test_y, dataset.label_vocab_size)\n    train_y = to_categorical(train_y, dataset.label_vocab_size)\n\n    model = Seq2SeqIntentModel()\n    model.build(\n        dataset.word_vocab_size,\n        dataset.label_vocab_size,\n        args.token_emb_size,\n        args.encoder_depth,\n        args.decoder_depth,\n        args.lstm_hidden_size,\n        args.encoder_dropout,\n        args.decoder_dropout,\n    )\n\n    conll_cb = ConllCallback(test_x, test_y, dataset.tags_vocab.vocab, batch_size=args.b)\n\n    # train model\n    model.fit(\n        x=train_x,\n        y=train_y,\n        batch_size=args.b,\n        epochs=args.e,\n        validation=(test_x, test_y),\n        callbacks=[conll_cb],\n    )\n    print(""Training done."")\n\n    print(""Saving model"")\n    model.save(args.model_path)\n    with open(args.model_info_path, ""wb"") as fp:\n        info = {\n            ""type"": ""seq2seq"",\n            ""tags_vocab"": dataset.tags_vocab.vocab,\n            ""word_vocab"": dataset.word_vocab.vocab,\n            ""char_vocab"": dataset.char_vocab.vocab,\n            ""intent_vocab"": dataset.intents_vocab.vocab,\n        }\n        pickle.dump(info, fp)\n\n    # test performance\n    predictions = model.predict(test_x, batch_size=args.b)\n    eval = get_conll_scores(\n        predictions, test_y, {v: k for k, v in dataset.tags_vocab.vocab.items()}\n    )\n    print(eval)\n'"
examples/memn2n_dialogue/__init__.py,0,b''
examples/memn2n_dialogue/babi_dialog.py,0,"b'#!/usr/bin/env python\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport itertools\nimport os\nimport pickle\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom nlp_architect.utils.generic import license_prompt\nfrom nlp_architect.utils.io import download_unlicensed_file, valid_path_append\n\n\ndef pad_sentences(sentences, sentence_length=0, pad_val=0.0):\n    """"""\n    Pad all sentences to have the same length (number of words)\n    """"""\n    lengths = [len(sent) for sent in sentences]\n\n    nsamples = len(sentences)\n    if sentence_length == 0:\n        sentence_length = np.max(lengths)\n\n    X = (np.ones((nsamples, sentence_length)) * pad_val).astype(dtype=np.int32)\n    for i, sent in enumerate(sentences):\n        trunc = sent[-sentence_length:]\n        X[i, : len(trunc)] = trunc\n    return X\n\n\ndef pad_stories(stories, sentence_length, max_story_length, pad_val=0.0):\n    """"""\n    Pad all stories to have the same number of sentences (max_story_length).\n    """"""\n    nsamples = len(stories)\n\n    X = (np.ones((nsamples, max_story_length, sentence_length)) * pad_val).astype(dtype=np.int32)\n\n    for i, story in enumerate(stories):\n        X[i, : len(story)] = story\n\n    return X\n\n\nclass BABI_Dialog(object):\n    """"""\n    This class loads in the Facebook bAbI goal oriented dialog dataset and\n    vectorizes them into user utterances, bot utterances, and answers.\n\n    As described in: ""Learning End-to-End Goal Oriented Dialog"".\n    https://arxiv.org/abs/1605.07683.\n\n    For a particular task, the class will read both train and test files\n    and combine the vocabulary.\n\n    Args:\n        path (str): Directory to store the dataset\n        task (str): a particular task to solve (all bAbI tasks are train\n            and tested separately)\n        oov (bool, optional): Load test set with out of vocabulary entity words\n        use_match_type (bool, optional): Flag to use match-type features\n        use_time (bool, optional): Add time words to each memory, encoding when the\n            memory was formed\n        use_speaker_tag (bool, optional): Add speaker words to each memory\n            (<BOT> or <USER>) indicating who spoke each memory.\n        cache_match_type (bool, optional): Flag to save match-type features after processing\n        cache_vectorized (bool, optional): Flag to save all vectorized data after processing\n\n    Attributes:\n        data_dict (dict): Dictionary containing final vectorized train, val, and test datasets\n        cands (np.array): Vectorized array of potential candidate answers, encoded\n        as integers, as returned by BABI_Dialog class. Shape = [num_cands, max_cand_length]\n        num_cands (int): Number of potential candidate answers.\n        max_cand_len (int): Maximum length of a candidate answer sentence in number of words.\n        memory_size (int): Maximum number of sentences to keep in memory at any given time.\n        max_utt_len (int): Maximum length of any given sentence / user utterance\n        vocab_size (int): Number of unique words in the vocabulary + 2 (0 is reserved for\n            a padding symbol, and 1 is reserved for OOV)\n        use_match_type (bool, optional): Flag to use match-type features\n        kb_ents_to_type (dict, optional): For use with match-type features, dictionary of\n            entities found in the dataset mapping to their associated match-type\n        kb_ents_to_cand_idxs (dict, optional): For use with match-type features, dictionary\n            mapping from each entity in the  knowledge base to the set of indicies in the\n            candidate_answers array that contain that entity.\n        match_type_idxs (dict, optional): For use with match-type features, dictionary\n            mapping from match-type to the associated fixed index of the candidate vector\n            which indicated this match type.\n    """"""\n\n    def __init__(\n        self,\n        path=""."",\n        task=1,\n        oov=False,\n        use_match_type=False,\n        use_time=True,\n        use_speaker_tag=True,\n        cache_match_type=False,\n        cache_vectorized=False,\n    ):\n        self.url = ""http://www.thespermwhale.com/jaseweston/babi""\n        self.size = 3032766\n        self.filename = ""dialog-bAbI-tasks.tgz""\n        self.path = path\n        self.task = task - 1\n        self.oov = oov\n        self.use_match_type = use_match_type\n        self.cache_vectorized = cache_vectorized\n        self.match_type_vocab = None\n        self.match_type_idxs = None\n\n        self.tasks = [\n            ""dialog-babi-task1-API-calls-"",\n            ""dialog-babi-task2-API-refine-"",\n            ""dialog-babi-task3-options-"",\n            ""dialog-babi-task4-phone-address-"",\n            ""dialog-babi-task5-full-dialogs-"",\n            ""dialog-babi-task6-dstc2-"",\n        ]\n\n        print(""Preparing bAbI-dialog dataset. Looking in ./%s"" % path)\n\n        assert task in range(1, 7), ""given task is not in the bAbI-dialog dataset""\n\n        print(""Task is %s"" % (self.tasks[self.task]))\n\n        (\n            self.train_file,\n            self.dev_file,\n            self.test_file,\n            self.cand_file,\n            self.kb_file,\n            self.vocab_file,\n            self.vectorized_file,\n        ) = self.load_data()\n\n        # Parse files into sets of dialogue and user/bot utterance pairs\n        self.train_dialog = self.parse_dialog(self.train_file, use_time, use_speaker_tag)\n        self.dev_dialog = self.parse_dialog(self.dev_file, use_time, use_speaker_tag)\n        self.test_dialog = self.parse_dialog(self.test_file, use_time, use_speaker_tag)\n\n        self.candidate_answers_w = self.load_candidate_answers()\n\n        if self.use_match_type:\n            self.kb_ents_to_type = self.load_kb()\n            self.kb_ents_to_cand_idxs = self.create_match_maps()\n        else:\n            self.kb_ents_to_type = None\n            self.kb_ents_to_cand_idxs = None\n\n        self.compute_statistics()\n\n        if self.use_match_type:\n            self.encode_match_feats()\n\n        if self.cache_vectorized and os.path.exists(self.vectorized_file):\n            print(""Loading cached vectorized data from: {}"".format(self.vectorized_file))\n            with open(self.vectorized_file, ""rb"") as f:\n                (self.train, self.dev, self.test, self.cands) = pickle.load(f)\n\n        else:\n            self.train = self.vectorize_stories(self.train_dialog)\n            self.dev = self.vectorize_stories(self.dev_dialog)\n            self.test = self.vectorize_stories(self.test_dialog)\n            self.cands = self.vectorize_cands(self.candidate_answers_w)\n\n            if self.cache_vectorized:\n                print(""Caching vectorized data at {}"".format(self.vectorized_file))\n                with open(self.vectorized_file, ""wb"") as f:\n                    pickle.dump((self.train, self.dev, self.test, self.cands), f)\n\n        self.data_dict[""train""] = {\n            ""memory"": {""data"": self.train[0], ""axes"": (""batch"", ""memory_axis"", ""sentence_axis"")},\n            ""memory_mask"": {""data"": self.train[1], ""axes"": (""batch"", ""memory_axis"")},\n            ""user_utt"": {""data"": self.train[2], ""axes"": (""batch"", ""sentence_axis"")},\n            ""answer"": {""data"": self.train[3], ""axes"": (""batch"", ""cand_axis"")},\n        }\n\n        self.data_dict[""dev""] = {\n            ""memory"": {""data"": self.dev[0], ""axes"": (""batch"", ""memory_axis"", ""sentence_axis"")},\n            ""memory_mask"": {""data"": self.dev[1], ""axes"": (""batch"", ""memory_axis"")},\n            ""user_utt"": {""data"": self.dev[2], ""axes"": (""batch"", ""sentence_axis"")},\n            ""answer"": {""data"": self.dev[3], ""axes"": (""batch"", ""cand_axis"")},\n        }\n\n        self.data_dict[""test""] = {\n            ""memory"": {""data"": self.test[0], ""axes"": (""batch"", ""memory_axis"", ""sentence_axis"")},\n            ""memory_mask"": {""data"": self.test[1], ""axes"": (""batch"", ""memory_axis"")},\n            ""user_utt"": {""data"": self.test[2], ""axes"": (""batch"", ""sentence_axis"")},\n            ""answer"": {""data"": self.test[3], ""axes"": (""batch"", ""cand_axis"")},\n        }\n\n        # Add question-specific candidate answers if we are using match-type\n        if self.use_match_type:\n            self.data_dict[""train""][""cands_mat""] = {\n                ""data"": self.create_cands_mat(""train"", cache_match_type),\n                ""axes"": (""batch"", ""cand_axis"", ""REC""),\n            }\n            self.data_dict[""dev""][""cands_mat""] = {\n                ""data"": self.create_cands_mat(""dev"", cache_match_type),\n                ""axes"": (""batch"", ""cand_axis"", ""REC""),\n            }\n            self.data_dict[""test""][""cands_mat""] = {\n                ""data"": self.create_cands_mat(""test"", cache_match_type),\n                ""axes"": (""batch"", ""cand_axis"", ""REC""),\n            }\n\n    def load_data(self):\n        """"""\n        Fetch and extract the Facebook bAbI-dialog dataset if not already downloaded.\n\n        Returns:\n            tuple: training and test filenames are returned\n        """"""\n        if self.task < 5:\n            self.candidate_answer_filename = ""dialog-babi-candidates.txt""\n            self.kb_filename = ""dialog-babi-kb-all.txt""\n            self.cands_mat_filename = ""babi-cands-with-matchtype_{}.npy""\n            self.vocab_filename = ""dialog-babi-vocab-task{}"".format(\n                self.task + 1\n            ) + ""_matchtype{}.pkl"".format(self.use_match_type)\n        else:\n            self.candidate_answer_filename = ""dialog-babi-task6-dstc2-candidates.txt""\n            self.kb_filename = ""dialog-babi-task6-dstc2-kb.txt""\n            self.cands_mat_filename = ""dstc2-cands-with-matchtype_{}.npy""\n            self.vocab_filename = ""dstc2-vocab-task{}_matchtype{}.pkl"".format(\n                self.task + 1, self.use_match_type\n            )\n\n        self.vectorized_filename = ""vectorized_task{}.pkl"".format(self.task + 1)\n\n        self.data_dict = {}\n        self.vocab = None\n        self.workdir, filepath = valid_path_append(self.path, """", self.filename)\n        if not os.path.exists(filepath):\n            if (\n                license_prompt(""bAbI-dialog"", ""https://research.fb.com/downloads/babi/"", self.path)\n                is False\n            ):\n                sys.exit(0)\n\n            download_unlicensed_file(self.url, self.filename, filepath, self.size)\n\n        self.babi_dir_name = self.filename.split(""."")[0]\n\n        self.candidate_answer_filename = self.babi_dir_name + ""/"" + self.candidate_answer_filename\n        self.kb_filename = self.babi_dir_name + ""/"" + self.kb_filename\n        self.cands_mat_filename = os.path.join(\n            self.workdir, self.babi_dir_name + ""/"" + self.cands_mat_filename\n        )\n        self.vocab_filename = self.babi_dir_name + ""/"" + self.vocab_filename\n        self.vectorized_filename = self.babi_dir_name + ""/"" + self.vectorized_filename\n\n        task_name = self.babi_dir_name + ""/"" + self.tasks[self.task] + ""{}.txt""\n\n        train_file = os.path.join(self.workdir, task_name.format(""trn""))\n        dev_file = os.path.join(self.workdir, task_name.format(""dev""))\n        test_file_postfix = ""tst-OOV"" if self.oov else ""tst""\n        test_file = os.path.join(self.workdir, task_name.format(test_file_postfix))\n\n        cand_file = os.path.join(self.workdir, self.candidate_answer_filename)\n        kb_file = os.path.join(self.workdir, self.kb_filename)\n        vocab_file = os.path.join(self.workdir, self.vocab_filename)\n        vectorized_file = os.path.join(self.workdir, self.vectorized_filename)\n\n        if (\n            os.path.exists(train_file) is False\n            or os.path.exists(dev_file) is False\n            or os.path.exists(test_file) is False\n            or os.path.exists(cand_file) is False\n        ):\n            with tarfile.open(filepath, ""r:gz"") as f:\n                f.extractall(self.workdir)\n\n        return train_file, dev_file, test_file, cand_file, kb_file, vocab_file, vectorized_file\n\n    @staticmethod\n    def parse_dialog(fn, use_time=True, use_speaker_tag=True):\n        """"""\n        Given a dialog file, parse into user and bot utterances, adding time and speaker tags.\n\n        Args:\n            fn (str): Filename to parse\n            use_time (bool, optional): Flag to append \'time-words\' to the end of each utterance\n            use_speaker_tag (bool, optional): Flag to append tags specifiying the speaker to\n                each utterance.\n        """"""\n        with open(fn, ""r"") as f:\n            text = f.readlines()\n\n        # Going to be filled with triplets of (memory, last user utterance,\n        # desired bot utterance)\n        all_dialogues = []\n        current_memory = []\n\n        for line in tqdm(text, desc=""Parsing""):\n            line = line.replace(""\\n"", """")\n\n            # End of dialgue\n            if not line:\n                current_memory = []\n                continue\n\n            number = line.split("" "")[0]\n\n            if ""\\t"" not in line:\n                # Line is returned results form API call, store as memory in\n                # current dialogue\n                current_memory.append(line.split("" "") + [""<USER>""])\n            else:\n                user_utt, bot_utt = "" "".join(line.split("" "")[1:]).split(""\\t"")\n\n                # Split utterances into words so we can encode as BOW query\n                user_utt_w = user_utt.split("" "")\n                bot_utt_w = bot_utt.split("" "")\n\n                # Add training example\n                # Don\'t split bot utterance so we can directly compare with\n                # candidate answers (and make onehot)\n                all_dialogues.append((current_memory[:], user_utt_w[:], bot_utt))\n\n                # Add time words and speaker tag\n                if use_time:\n                    user_utt_w += [str(number) + ""_TIME""]\n                    bot_utt_w += [str(number) + ""_TIME""]\n                if use_speaker_tag:\n                    user_utt_w += [""<USER>""]\n                    bot_utt_w += [""<BOT>""]\n\n                # Add split and modified user and bot utterances to memory\n                current_memory += [user_utt_w, bot_utt_w]\n\n        return all_dialogues\n\n    def words_to_vector(self, words):\n        """"""\n        Convert a list of words into vector form.\n\n        Args:\n            words (list) : List of words.\n\n        Returns:\n            list : Vectorized list of words.\n        """"""\n        return [\n            self.word_to_index[w] if w in self.vocab else self.word_to_index[""<OOV>""] for w in words\n        ]\n\n    def one_hot_vector(self, answer):\n        """"""\n        Create one-hot representation of an answer.\n\n        Args:\n            answer (string) : The word answer.\n\n        Returns:\n            list : One-hot representation of answer.\n        """"""\n        vector = np.zeros(self.num_cands)\n        vector[self.candidate_answers.index(answer)] = 1\n        return vector\n\n    def vectorize_stories(self, data):\n        """"""\n        Convert (memory, user_utt, answer) word data into vectors.\n\n        If sentence length < max_utt_len it is padded with 0\'s\n        If memory length < memory size, it is padded with empty memorys (max_utt_len 0\'s)\n\n        Args:\n            data (tuple) : Tuple of memories, user_utt, answer word data.\n\n        Returns:\n            tuple : Tuple of memories, memory_lengths, user_utt, answer vectors.\n        """"""\n        m, ml, m_mask, u, a = [], [], [], [], []\n        for mem, utt, answer in tqdm(data, desc=""Vectorizing""):\n            m.append([self.words_to_vector(sent) for sent in mem])\n            ml.append(len(mem))\n            mask_zero_len = self.memory_size - ml[-1]\n            m_mask.append([1.0 for _ in range(ml[-1])] + [0.0 for _ in range(mask_zero_len)])\n\n            u.append(self.words_to_vector(utt))\n            a.append(self.one_hot_vector(answer))\n\n        m = np.array([pad_sentences(sents, self.max_utt_len) for sents in m])\n        m = pad_stories(m, self.max_utt_len, self.memory_size)\n        m_mask = np.array(m_mask)\n\n        u = pad_sentences(u, self.max_utt_len)\n        a = np.array(a)\n\n        return (m, m_mask, u, a)\n\n    def vectorize_cands(self, data):\n        """"""\n        Convert candidate answer word data into vectors.\n\n        If sentence length < max_cand_len it is padded with 0\'s\n\n        Args:\n            data (list of lists) : list of candidate answers split into words\n\n        Returns:\n            tuple (2d numpy array): padded numpy array of word indexes forr all candidate answers\n        """"""\n        c = []\n        for cand in data:\n            c.append(self.words_to_vector(cand))\n\n        c = pad_sentences(c, self.max_cand_len)\n        return c\n\n    def get_vocab(self, dialog):\n        """"""\n        Compute vocabulary from the set of dialogs.\n        """"""\n        # Extract only the memory words and user utterance words (these will\n        # contain all vocab in the end)\n        dialog_words = [x[0] + [x[1]] for x in dialog]\n\n        # Concatenate separate dialogues\n        all_utts = list(itertools.chain.from_iterable(dialog_words))\n        # Concatenate all utterances to get list of words\n        all_words = list(itertools.chain.from_iterable(all_utts))\n\n        # Add candidate answer words\n        all_words += list(itertools.chain.from_iterable(self.candidate_answers_w))\n\n        if self.use_match_type:\n            # Add match-type words\n            self.match_type_vocab = list(set(self.kb_ents_to_type.values()))\n            all_words += list(self.match_type_vocab) + list(self.kb_ents_to_type.keys())\n            # Also compute indicies into each candidate vector for the\n            # different match types (fixed position)\n            self.match_type_idxs = {\n                mt: i + self.max_cand_len_pre_match for i, mt in enumerate(self.match_type_vocab)\n            }\n\n        vocab = list(set(all_words))\n        return vocab\n\n    def compute_statistics(self):\n        """"""\n        Compute vocab, word index, and max length of stories and queries.\n        """"""\n        all_dialog = self.train_dialog + self.dev_dialog + self.test_dialog\n\n        self.max_cand_len_pre_match = max(list(map(len, self.candidate_answers_w)))\n\n        vocab = self.get_vocab(all_dialog)\n\n        self.vocab = vocab\n        # Reserve 0 for masking via pad_sequences, 1 for oov\n        self.vocab_size = len(vocab) + 2\n\n        if os.path.exists(self.vocab_file):\n            with open(self.vocab_file, ""rb"") as f:\n                self.word_to_index = pickle.load(f)\n\n            self.index_to_word = dict((self.word_to_index[k], k) for k in self.word_to_index)\n        else:\n            self.word_to_index = dict((c, i + 2) for i, c in enumerate(vocab))\n            self.index_to_word = dict((i + 2, c) for i, c in enumerate(vocab))\n\n            self.word_to_index[""<PAD>""] = 0\n            self.word_to_index[""<OOV>""] = 1\n            self.index_to_word[0] = """"  # empty so we dont print a bunch of padding\n            self.index_to_word[1] = ""<OOV>""\n\n            with open(self.vocab_file, ""wb"") as f:\n                pickle.dump(self.word_to_index, f)\n\n        memories = [m for m, _, _ in all_dialog]\n        self.memory_size = max(list(map(len, memories)))\n\n        dialog_words = [x[0] + [x[1]] for x in all_dialog]\n        # Concatenate separate dialogues\n        all_utts = list(itertools.chain.from_iterable(dialog_words))\n        self.max_utt_len = max(list(map(len, all_utts)))\n\n        self.num_cands = len(self.candidate_answers)\n\n        if self.use_match_type:\n            # Add num_match_types slots to each candidate answer so they can\n            # be filled if we find a matching word\n            self.max_cand_len = self.max_cand_len_pre_match + len(self.match_type_vocab)\n        else:\n            self.max_cand_len = self.max_cand_len_pre_match\n\n    @staticmethod\n    def clean_cands(cand):\n        """"""\n        Remove leading line number and final newline from candidate answer\n        """"""\n        return "" "".join(cand.split("" "")[1:]).replace(""\\n"", """")\n\n    def load_candidate_answers(self):\n        """"""\n        Load candidate answers from file, compute number, and store for final softmax\n        """"""\n        with open(self.cand_file, ""r"") as f:\n            cands_text = f.readlines()\n        self.candidate_answers = list(map(self.clean_cands, cands_text))\n\n        # Create BOW representation of candidate answers for final prediction\n        # softmax\n        candidate_answers_w = list(map(lambda x: x.split("" ""), self.candidate_answers))\n        return candidate_answers_w\n\n    def process_interactive(self, line_in, context, response, db_results, time_feat):\n        """"""\n        Parse a given user\'s input into the same format as training, build the memory\n        from the given context and previous response, update the context.\n        """"""\n        # Parse user input\n        line_in = line_in.replace(""\\n"", """")\n        line_w = line_in.split("" "")\n\n        # Enocde user input and current context\n        user_utt_w = self.words_to_vector(line_w)\n\n        # Add last bot response to context before we create memory\n        if response:\n            bot_utt_w = response.split("" "")\n            bot_utt_w += [str(time_feat - 1) + ""_TIME"", ""<BOT>""]\n            context += [bot_utt_w]\n\n        # If line_in is not silence, we are cutting off the api call\n        # and making a correction\n        if db_results and line_in == ""<SILENCE>"":\n            for result in db_results:\n                res_utt_w = [str(time_feat)] + result.split("" "") + [""<USER>""]\n                context += [res_utt_w]\n                time_feat += 1\n\n        # truncate context to max memory size\n        context = context[-self.memory_size :]\n        memory = [self.words_to_vector(sent) for sent in context]\n\n        # Compute memory mask\n        ml = len(memory)\n        mask_zero_len = self.memory_size - ml\n        m_mask = [1.0 for _ in range(ml)] + [0.0 for _ in range(mask_zero_len)]\n        m_mask = np.array(m_mask)\n\n        # Pad memory to max memory fize\n        memory = pad_sentences(memory, self.max_utt_len)\n        memory_pad = (np.zeros((self.memory_size, self.max_utt_len))).astype(dtype=np.int32)\n        memory_pad[: len(memory)] = memory\n\n        # Pad user utt to sentence size\n        user_utt_pad = (np.zeros((self.max_utt_len,))).astype(dtype=np.int32)\n        user_utt_trunc = user_utt_w[-self.max_utt_len :]\n        user_utt_pad[: len(user_utt_trunc)] = user_utt_trunc\n\n        # Add time features and store user utterance in context\n        user_utt_w_mem = line_w + [str(time_feat) + ""_TIME"", ""<USER>""]\n        context += [user_utt_w_mem]\n\n        # Compute candidate_matrix if match features, otherwise just return\n        # None & it\'ll use default\n        if self.use_match_type:\n            # dupliucate candidates for all examples\n            cands_mat = np.array(self.cands)\n\n            all_words = list(np.unique(memory.flatten())) + list(np.unique(user_utt_trunc))\n\n            # For each word, if it\'s an entity, add it\'s match-type word to the\n            # candidate\n            for word in all_words:\n                if word in self.kb_ents_to_type.keys():\n                    cand_idxs = self.kb_ents_to_cand_idxs[word]\n                    ent_type_word = self.kb_ents_to_type[word]\n                    match_type_word_idx = self.match_type_idxs[ent_type_word]\n\n                    cands_mat[cand_idxs, match_type_word_idx] = ent_type_word\n        else:\n            cands_mat = np.array(self.cands)\n\n        time_feat += 1\n\n        return user_utt_pad, context, memory_pad, cands_mat, time_feat\n\n    def load_kb(self):\n        """"""\n        Load knowledge base from file, parse into entities and types\n        """"""\n        with open(self.kb_file, ""r"") as f:\n            kb_text = f.readlines()\n\n        if self.task < 5:\n            # Split each kb entry into entity and entity type match, store as\n            # dict\n            kb_ents_to_type = {\n                x.strip().split(""\\t"")[-1]: x.strip().split("" "")[2].split(""\\t"")[-2] + ""_MATCH""\n                for x in kb_text\n            }\n        else:\n            kb_ents_to_type = {\n                x.strip().split("" "")[3]: x.strip().split("" "")[2] + ""_MATCH"" for x in kb_text\n            }\n\n        return kb_ents_to_type\n\n    def create_match_maps(self):\n        """"""\n        Create dictionary mapping from each entity in the knowledge base to the set of\n        indicies in the candidate_answers array that contain that entity. Will be used for\n        quickly adding the match type features to the candidate answers during fprop.\n        """"""\n\n        kb_ents = self.kb_ents_to_type.keys()\n\n        kb_ents_to_cand_idxs = {}\n\n        for ent in kb_ents:\n            kb_ents_to_cand_idxs[ent] = []\n\n            for c_idx, cand in enumerate(self.candidate_answers_w):\n                if ent in cand:\n                    kb_ents_to_cand_idxs[ent].append(c_idx)\n\n        return kb_ents_to_cand_idxs\n\n    def encode_match_feats(self):\n        """"""\n        Replace entity names and match type names with indexes\n        """"""\n        self.kb_ents_to_type = {\n            self.word_to_index[k]: self.word_to_index[v] for k, v in self.kb_ents_to_type.items()\n        }\n        self.kb_ents_to_cand_idxs = {\n            self.word_to_index[k]: v for k, v in self.kb_ents_to_cand_idxs.items()\n        }\n        self.match_type_idxs = {self.word_to_index[k]: v for k, v in self.match_type_idxs.items()}\n\n    def create_cands_mat(self, data_split, cache_match_type):\n        """"""\n        Add match type features to candidate answers for each example in the dataaset.\n        Caches once complete.\n        """"""\n        cands_mat_filename = self.cands_mat_filename.format(data_split)\n\n        data_dict = self.data_dict[data_split]\n\n        # Returned cached matric if it exists\n        if os.path.exists(cands_mat_filename):\n            return np.load(cands_mat_filename)\n\n        ndata = data_dict[""user_utt""][""data""].shape[0]\n        # dupliucate candidates for all examples\n        cands_mat = np.array([self.cands] * ndata)\n\n        print(""Adding match type features to {} set"".format(data_split))\n\n        for idx, (mem, utt) in enumerate(\n            tqdm(zip(data_dict[""memory""][""data""], data_dict[""user_utt""][""data""]))\n        ):\n            # Get list of unique words currently in memory of user utt\n            all_words = list(np.unique(mem.flatten())) + list(np.unique(utt))\n\n            # For each word, if it\'s an entity, add it\'s match-type word to the\n            # candidate\n            for word in all_words:\n                if word in self.kb_ents_to_type.keys():\n                    cand_idxs = self.kb_ents_to_cand_idxs[word]\n                    ent_type_word = self.kb_ents_to_type[word]\n                    match_type_word_idx = self.match_type_idxs[ent_type_word]\n\n                    cands_mat[idx][cand_idxs, match_type_word_idx] = ent_type_word\n\n        if cache_match_type:\n            # Cache computed matrix\n            print(""Saving candidate matrix for {} to {}"".format(data_split, cands_mat_filename))\n            np.save(cands_mat_filename, cands_mat)\n\n        return cands_mat\n'"
examples/memn2n_dialogue/interactive.py,0,"b'#!/usr/bin/env python\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nExample that trains an End-to-End Memory Network on the Facebook bAbI\ngoal-oriented dialog dataset.\n\nReference:\n    ""Learning End-to-End Goal Oriented Dialog""\n    https://arxiv.org/abs/1605.07683.\n\nUsage:\n\n    python train_memn2n.py --task 5\n\n    use --task to specify which bAbI-dialog task to run on\n        - Task 1: Issuing API Calls\n        - Task 2: Updating API Calls\n        - Task 3: Displaying Options\n        - Task 4: Providing Extra Information\n        - Task 5: Conducting Full Dialogs\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nimport tensorflow as tf\nfrom interactive_utils import interactive_loop\n\nfrom examples.memn2n_dialogue.babi_dialog import BABI_Dialog\nfrom examples.memn2n_dialogue.memn2n_dialogue import MemN2N_Dialog\nfrom nlp_architect.utils.io import validate_parent_exists, validate\n\n# parse the command line arguments\ntf.flags.DEFINE_integer(""task"", 1, ""the task ID to train/test on from bAbI-dialog dataset (1-6)"")\ntf.flags.DEFINE_integer(""emb_size"", 20, ""Size of the word-embedding used in the model."")\ntf.flags.DEFINE_integer(""nhops"", 3, ""Number of memory hops in the network"")\ntf.flags.DEFINE_boolean(""use_match_type"", False, ""use match type features"")\ntf.flags.DEFINE_boolean(""cache_match_type"", False, ""cache match type answers"")\ntf.flags.DEFINE_boolean(""cache_vectorized"", False, ""cache vectorized data"")\ntf.flags.DEFINE_boolean(""use_oov"", False, ""use OOV test set"")\ntf.flags.DEFINE_string(""data_dir"", ""data/"", ""File to save model weights to."")\ntf.flags.DEFINE_string(""weights_save_path"", ""saved_tf/"", ""File to save model weights to."")\nFLAGS = tf.flags.FLAGS\n\n\nvalidate((FLAGS.task, int, 1, 7), (FLAGS.nhops, int, 1, 100), (FLAGS.emb_size, int, 1, 10000))\n\n# Validate inputs\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\nweights_save_path = os.path.join(current_dir, FLAGS.weights_save_path)\nvalidate_parent_exists(weights_save_path)\ndata_dir = os.path.join(current_dir, FLAGS.data_dir)\nvalidate_parent_exists(data_dir)\n\nbabi = BABI_Dialog(\n    path=data_dir,\n    task=FLAGS.task,\n    oov=FLAGS.use_oov,\n    use_match_type=FLAGS.use_match_type,\n    cache_match_type=FLAGS.cache_match_type,\n    cache_vectorized=FLAGS.cache_vectorized,\n)\n\nwith tf.Session() as sess:\n    memn2n = MemN2N_Dialog(\n        32,\n        babi.vocab_size,\n        babi.max_utt_len,\n        babi.memory_size,\n        FLAGS.emb_size,\n        babi.num_cands,\n        babi.max_cand_len,\n        hops=FLAGS.nhops,\n        max_grad_norm=40.0,\n        session=sess,\n    )\n\n    if os.path.exists(weights_save_path):\n        print(""Loading weights from {}"".format(weights_save_path))\n        memn2n.saver.restore(sess, weights_save_path)\n        print(""Beginning interactive mode..."")\n        interactive_loop(memn2n, babi)\n    else:\n        print(""Could not find weights at {}. Exiting."".format(weights_save_path))\n'"
examples/memn2n_dialogue/interactive_utils.py,0,"b'#!/usr/bin/env python\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport itertools\nfrom builtins import input as text_input\nfrom copy import copy\nfrom functools import reduce\n\nimport numpy as np\n\n\ndef interactive_loop(model, babi):\n    """"""\n    Loop used to interact with trained conversational agent with access to knowledge base API\n    """"""\n    context = []\n    response = None\n    time_feat = 1\n    interactive_output = None\n    db_results = []\n    allow_oov = False\n\n    print(""Building database..."")\n    print_help()\n    db, names_to_idxs, kb_text = build_kb_db(babi)\n\n    while True:\n        line_in = text_input("">>> "").strip().lower()\n        if not line_in:\n            line_in = ""<SILENCE>""\n        if line_in in (""exit"", ""quit""):\n            break\n        if line_in == ""help"":\n            print_help()\n            continue\n        if line_in in (""restart"", ""clear""):\n            context = []\n            response = None\n            time_feat = 1\n            interactive_output = None\n            db_results = []\n            print(""Memory cleared"")\n            continue\n        if line_in == ""vocab"":\n            print_human_vocab(babi)\n            continue\n        if line_in == ""show_memory"":\n            print_memory(context)\n            continue\n        if line_in == ""allow_oov"":\n            allow_oov = not allow_oov\n            print(""Allow OOV = {}"".format(allow_oov))\n            continue\n        if ""api_call"" in line_in:\n            db_results = issue_api_call(line_in, db, names_to_idxs, kb_text, babi)\n\n        old_context = copy(context)\n        user_utt, context, memory, cands_mat, time_feat = babi.process_interactive(\n            line_in, context, response, db_results, time_feat\n        )\n\n        if babi.word_to_index[""<OOV>""] in user_utt and allow_oov is False:\n            oov_word = line_in.split("" "")[list(user_utt).index(babi.word_to_index[""<OOV>""])]\n            print(\n                \'Sorry, ""{}"" is outside my vocabulary. \'.format(oov_word)\n                + ""Please say \'allow_oov\' to toggle OOV words""\n                + "", or \'help\' for more commands.""\n            )\n            context = old_context\n            continue\n\n        interactive_output = model.predict(\n            np.expand_dims(memory, 0), np.expand_dims(user_utt, 0), np.expand_dims(cands_mat, 0)\n        )\n        pred_cand_idx = interactive_output[0]\n        response = babi.candidate_answers[pred_cand_idx]\n\n        print(response)\n\n        if ""api_call"" in response:\n            db_results = issue_api_call(response, db, names_to_idxs, kb_text, babi)\n        else:\n            db_results = []\n\n\ndef print_memory(context):\n    if not context:\n        return\n\n    max_sent_len = max(map(len, map(lambda z: reduce(lambda x, y: x + "" "" + y, z), context)))\n\n    print(""-"" * max_sent_len)\n    for sent in context:\n        print("" "".join(sent))\n    print(""-"" * max_sent_len)\n\n\ndef print_human_vocab(babi):\n    if babi.task + 1 < 6:\n        print([x for x in babi.vocab if ""resto"" not in x])\n    else:\n        print(babi.vocab)\n\n\ndef print_help():\n    print(\n        ""Available Commands: \\n""\n        + "" >> help: Display this help menu\\n""\n        + "" >> exit / quit: Exit interactive mode\\n""\n        + "" >> restart / clear: Restart the conversation and erase the bot\'s memory\\n""\n        + "" >> vocab: Display usable vocabulary\\n""\n        + "" >> allow_oov: Allow out of vocab words to be replaced with <OOV> token\\n""\n        + "" >> show_memory: Display the current contents of the bot\'s memory\\n""\n    )\n\n\ndef build_kb_db(babi):\n    """"""\n    Build a searchable database from the kb files to be used in interactive mode\n    """"""\n    with open(babi.kb_file, ""r"") as f:\n        kb_text = f.readlines()\n\n    kb_text = [x.replace(""\\t"", "" "") for x in kb_text]\n\n    db = {}\n\n    property_types = set(x.split("" "")[2] for x in kb_text)\n\n    for ptype in property_types:\n        unique_props = set(\n            x.split("" "")[3].strip() for x in kb_text if ptype in x.strip().split("" "")\n        )\n\n        db[ptype] = {\n            prop: [x for idx, x in enumerate(kb_text) if prop in x.strip().split("" "")]\n            for prop in unique_props\n        }\n        db[ptype][ptype] = kb_text\n\n    resto_names = set(x.split("" "")[1] for x in kb_text)\n    names_to_idxs = {}\n    for name in resto_names:\n        names_to_idxs[name] = [idx for idx, x in enumerate(kb_text) if name in x.strip().split("" "")]\n\n    kb_text_clean = np.array(["" "".join(x.strip().split("" "")[1:]) for x in kb_text])\n\n    return db, names_to_idxs, kb_text_clean\n\n\ndef issue_api_call(api_call, db, names_to_idxs, kb_text, babi):\n    """"""\n    Parse the api call and use it to search the database\n    """"""\n    desired_properties = api_call.strip().split("" "")[1:]\n\n    if babi.task + 1 < 6:\n        properties_order = [""R_cuisine"", ""R_location"", ""R_number"", ""R_price""]\n    else:\n        properties_order = [""R_cuisine"", ""R_location"", ""R_price""]\n\n    assert len(properties_order) == len(desired_properties)\n\n    # Start result as all possible kb entries\n    returned_kb_idxs = set(itertools.chain.from_iterable(names_to_idxs.values()))\n\n    for ptype, prop in zip(properties_order, desired_properties):\n        kb_idxs = [names_to_idxs[x.split("" "")[1]] for x in db[ptype][prop]]\n        kb_idxs = list(itertools.chain.from_iterable(kb_idxs))\n        # iteratively perform intersection with subset that matches query\n        returned_kb_idxs = returned_kb_idxs.intersection(kb_idxs)\n\n    returned_kb_idxs = list(returned_kb_idxs)\n    # Return actual text kb entries\n    kb_results = list(kb_text[returned_kb_idxs])\n    return kb_results\n'"
examples/memn2n_dialogue/memn2n_dialogue.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\nfrom six.moves import range\n\n\ndef zero_nil_slot(t):\n    """"""\n    Overwrites the nil_slot (first row) of the input Tensor with zeros.\n\n    The nil_slot is a dummy slot and should not be trained and influence\n    the training algorithm.\n    """"""\n    t = tf.convert_to_tensor(t, name=""t"")\n    s = tf.shape(t)[1]\n    z = tf.zeros(tf.stack([1, s]))\n    return tf.concat(axis=0, values=[z, tf.slice(t, [1, 0], [-1, -1])])\n\n\nclass MemN2N_Dialog(object):\n    """"""End-To-End Memory Network.""""""\n\n    def __init__(\n        self,\n        batch_size,\n        vocab_size,\n        sentence_size,\n        memory_size,\n        embedding_size,\n        num_cands,\n        max_cand_len,\n        hops=3,\n        max_grad_norm=40.0,\n        nonlin=None,\n        initializer=tf.random_normal_initializer(stddev=0.1),\n        optimizer=tf.train.AdamOptimizer(learning_rate=0.001, epsilon=1e-8),\n        session=tf.Session(),\n        name=""MemN2N_Dialog"",\n    ):\n        """"""Creates an End-To-End Memory Network for Goal Oriented Dialog\n\n        Args:\n            cands: Encoded candidate answers\n\n            batch_size: The size of the batch.\n\n            vocab_size: The size of the vocabulary (should include the nil word). The nil word\n            one-hot encoding should be 0.\n\n            sentence_size: The max size of a sentence in the data. All sentences should be padded\n            to this length. If padding is required it should be done with nil one-hot encoding (0).\n\n            memory_size: The max size of the memory. Since Tensorflow currently does not support\n            jagged arrays all memories must be padded to this length. If padding is required, the\n            extra memories should be\n\n            empty memories; memories filled with the nil word ([0, 0, 0, ......, 0]).\n\n            embedding_size: The size of the word embedding.\n\n            hops: The number of hops. A hop consists of reading and addressing a memory slot.\n            Defaults to `3`.\n\n            max_grad_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n\n            nonlin: Non-linearity. Defaults to `None`.\n\n            initializer: Weight initializer. Defaults to\n            `tf.random_normal_initializer(stddev=0.1)`.\n\n            optimizer: Optimizer algorithm used for SGD. Defaults to\n            `tf.train.AdamOptimizer(learning_rate=1e-2)`.\n\n            session: Tensorflow Session the model is run with. Defaults to `tf.Session()`.\n\n            name: Name of the End-To-End Memory Network. Defaults to `MemN2N`.\n        """"""\n\n        self._batch_size = batch_size\n        self._vocab_size = vocab_size\n        self._sentence_size = sentence_size\n        self._memory_size = memory_size\n        self._embedding_size = embedding_size\n        self._max_cand_len = max_cand_len\n        self._num_cands = num_cands\n        self._hops = hops\n        self._max_grad_norm = max_grad_norm\n        self._nonlin = nonlin\n        self._init = initializer\n        self._name = name\n\n        self._build_inputs()\n        self._build_vars()\n\n        self._opt = optimizer\n\n        # cross entropy\n        logits = self._inference(self._stories, self._queries)  # (batch_size, vocab_size)\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n            logits=logits, labels=tf.cast(self._answers, tf.float32), name=""cross_entropy""\n        )\n        # loss op\n        cross_entropy_sum = tf.reduce_sum(cross_entropy, name=""cross_entropy_sum"")\n\n        # gradient pipeline\n        grads_and_vars = self._opt.compute_gradients(cross_entropy_sum)\n        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) for g, v in grads_and_vars]\n        nil_grads_and_vars = []\n        for g, v in grads_and_vars:\n            if v.name in self._nil_vars:\n                nil_grads_and_vars.append((zero_nil_slot(g), v))\n            else:\n                nil_grads_and_vars.append((g, v))\n        train_op = self._opt.apply_gradients(nil_grads_and_vars, name=""train_op"")\n\n        # predict ops\n        predict_op = tf.argmax(logits, 1, name=""predict_op"")\n        predict_proba_op = tf.nn.softmax(logits, name=""predict_proba_op"")\n        predict_log_proba_op = tf.log(predict_proba_op, name=""predict_log_proba_op"")\n\n        # assign ops\n        self.loss_op = cross_entropy_sum\n        self.predict_op = predict_op\n        self.predict_proba_op = predict_proba_op\n        self.predict_log_proba_op = predict_log_proba_op\n        self.train_op = train_op\n\n        init_op = tf.global_variables_initializer()\n        self._sess = session\n        self._sess.run(init_op)\n        self.saver = tf.train.Saver(max_to_keep=1)\n\n    def _build_inputs(self):\n        self._stories = tf.placeholder(tf.int32, [None, None, self._sentence_size], name=""stories"")\n        self._queries = tf.placeholder(tf.int32, [None, self._sentence_size], name=""queries"")\n        self._answers = tf.placeholder(tf.int32, [None, self._num_cands], name=""answers"")\n        self._cands = tf.placeholder(\n            tf.int32, [None, self._num_cands, self._max_cand_len], name=""candidate_answers""\n        )\n\n    def _build_vars(self):\n        with tf.variable_scope(self._name):\n            nil_word_slot = tf.zeros([1, self._embedding_size])\n            A = tf.concat(\n                axis=0,\n                values=[nil_word_slot, self._init([self._vocab_size - 1, self._embedding_size])],\n            )\n            W = tf.concat(\n                axis=0,\n                values=[nil_word_slot, self._init([self._vocab_size - 1, self._embedding_size])],\n            )\n\n            self.LUT_A = tf.Variable(A, name=""LUT_A"")\n            self.LUT_W = tf.Variable(W, name=""LUT_W"")\n\n            # Dont use projection for layerwise weight sharing\n            self.R_proj = tf.Variable(\n                self._init([self._embedding_size, self._embedding_size]), name=""R_proj""\n            )\n\n        self._nil_vars = set([self.LUT_A.name, self.LUT_W.name])\n\n    def _inference(self, stories, queries):\n        with tf.variable_scope(self._name):\n            q_emb = tf.nn.embedding_lookup(self.LUT_A, queries)\n            u_0 = tf.reduce_sum(q_emb, 1)\n            u = [u_0]\n\n            for _ in range(self._hops):\n                m_emb_A = tf.nn.embedding_lookup(self.LUT_A, stories)\n                m_A = tf.reduce_sum(m_emb_A, 2)\n\n                # hack to get around no reduce_dot\n                u_temp = tf.transpose(tf.expand_dims(u[-1], -1), [0, 2, 1])\n                dotted = tf.reduce_sum(m_A * u_temp, 2)\n\n                # Calculate probabilities\n                probs = tf.nn.softmax(dotted)\n\n                probs_temp = tf.transpose(tf.expand_dims(probs, -1), [0, 2, 1])\n\n                # Reuse A for the output memory encoding\n                c_temp = tf.transpose(m_A, [0, 2, 1])\n                o_k = tf.reduce_sum(c_temp * probs_temp, 2)\n\n                # Project hidden state, and add update\n                u_k = tf.matmul(u[-1], self.R_proj) + o_k\n\n                # nonlinearity\n                if self._nonlin:\n                    u_k = self._nonlin(u_k)\n\n                u.append(u_k)\n\n            cands_emb = tf.nn.embedding_lookup(self.LUT_W, self._cands)\n            cands_emb_sum = tf.reduce_sum(cands_emb, 2)\n\n            logits = tf.reshape(\n                tf.matmul(tf.expand_dims(u_k, 1), tf.transpose(cands_emb_sum, [0, 2, 1])),\n                (-1, cands_emb_sum.shape[1]),\n            )\n\n            return logits\n\n    def batch_fit(self, stories, queries, answers, cands):\n        """"""Runs the training algorithm over the passed batch\n\n        Args:\n            stories: Tensor (None, memory_size, sentence_size)\n            queries: Tensor (None, sentence_size)\n            answers: Tensor (None, vocab_size)\n\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n        """"""\n        feed_dict = {\n            self._stories: stories,\n            self._queries: queries,\n            self._answers: answers,\n            self._cands: cands,\n        }\n        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n\n        return loss\n\n    def predict(self, stories, queries, cands):\n        """"""Predicts answers as one-hot encoding.\n\n        Args:\n            stories: Tensor (None, memory_size, sentence_size)\n            queries: Tensor (None, sentence_size)\n\n        Returns:\n            answers: Tensor (None, vocab_size)\n        """"""\n        feed_dict = {self._stories: stories, self._queries: queries, self._cands: cands}\n        return self._sess.run(self.predict_op, feed_dict=feed_dict)\n'"
examples/memn2n_dialogue/train_model.py,0,"b'#!/usr/bin/env python\n# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nExample that trains an End-to-End Memory Network on the Facebook bAbI\ngoal-oriented dialog dataset.\n\nReference:\n    ""Learning End-to-End Goal Oriented Dialog""\n    https://arxiv.org/abs/1605.07683.\n\nUsage:\n\n    python train_memn2n.py --task 5\n\n    use --task to specify which bAbI-dialog task to run on\n        - Task 1: Issuing API Calls\n        - Task 2: Updating API Calls\n        - Task 3: Displaying Options\n        - Task 4: Providing Extra Information\n        - Task 5: Conducting Full Dialogs\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom interactive_utils import interactive_loop\nfrom tqdm import tqdm\n\nfrom .babi_dialog import BABI_Dialog\nfrom .memn2n_dialogue import MemN2N_Dialog\nfrom nlp_architect.utils.io import validate_parent_exists, validate\n\n# parse the command line arguments\ntf.flags.DEFINE_integer(""task"", 1, ""the task ID to train/test on from bAbI-dialog dataset (1-6)"")\ntf.flags.DEFINE_integer(""emb_size"", 20, ""Size of the word-embedding used in the model."")\ntf.flags.DEFINE_integer(""batch_size"", 32, ""Size of the batch for optimization."")\ntf.flags.DEFINE_integer(""nhops"", 3, ""Number of memory hops in the network"")\ntf.flags.DEFINE_boolean(""use_match_type"", False, ""use match type features"")\ntf.flags.DEFINE_boolean(""cache_match_type"", False, ""cache match type answers"")\ntf.flags.DEFINE_boolean(""cache_vectorized"", False, ""cache vectorized data"")\ntf.flags.DEFINE_boolean(""use_oov"", False, ""use OOV test set"")\ntf.flags.DEFINE_float(""lr"", 0.001, ""learning rate"")\ntf.flags.DEFINE_float(""grad_clip_norm"", 40.0, ""Clip gradients such that norm is below this value."")\ntf.flags.DEFINE_float(\n    ""eps"", 1e-8, ""epsilon used to avoid divide by zero in softmax renormalization.""\n)\ntf.flags.DEFINE_boolean(""save_log"", False, ""Save evaluation results to log file."")\ntf.flags.DEFINE_string(""data_dir"", ""data/"", ""File to save model weights to."")\ntf.flags.DEFINE_string(\n    ""log_file"", ""memn2n_dialgoue_results.txt"", ""File to write evaluation set results to.""\n)\ntf.flags.DEFINE_string(""weights_save_path"", ""saved_tf/"", ""File to save model weights to."")\ntf.flags.DEFINE_integer(""save_epochs"", 10, ""Number of epochs between saving model weights."")\ntf.flags.DEFINE_integer(""epochs"", 100, ""Number of epochs between saving model weights."")\ntf.flags.DEFINE_boolean(""restore"", False, ""Restore weights if found."")\ntf.flags.DEFINE_boolean(""interactive"", False, ""enable interactive mode at the end of training."")\ntf.flags.DEFINE_boolean(""test"", False, ""evaluate on the test set at the end of training."")\nFLAGS = tf.flags.FLAGS\n\n# Validate inputs\nvalidate(\n    (FLAGS.task, int, 1, 7),\n    (FLAGS.nhops, int, 1, 10),\n    (FLAGS.batch_size, int, 1, 32000),\n    (FLAGS.emb_size, int, 1, 10000),\n    (FLAGS.eps, float, 1e-15, 1e-2),\n    (FLAGS.lr, float, 1e-8, 10),\n    (FLAGS.grad_clip_norm, float, 1e-3, 1e5),\n    (FLAGS.epochs, int, 1, 1e10),\n    (FLAGS.save_epochs, int, 1, 1e10),\n)\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\nlog_file = os.path.join(current_dir, FLAGS.log_file)\nvalidate_parent_exists(log_file)\nweights_save_path = os.path.join(current_dir, FLAGS.weights_save_path)\nvalidate_parent_exists(weights_save_path)\ndata_dir = os.path.join(current_dir, FLAGS.data_dir)\nvalidate_parent_exists(data_dir)\nassert log_file.endswith("".txt"")\n\nbabi = BABI_Dialog(\n    path=data_dir,\n    task=FLAGS.task,\n    oov=FLAGS.use_oov,\n    use_match_type=FLAGS.use_match_type,\n    cache_match_type=FLAGS.cache_match_type,\n    cache_vectorized=FLAGS.cache_vectorized,\n)\n\ntrain_set = babi.data_dict[""train""]\ndev_set = babi.data_dict[""dev""]\ntest_set = babi.data_dict[""test""]\n\nn_train = train_set[""memory""][""data""].shape[0]\nn_val = dev_set[""memory""][""data""].shape[0]\nn_test = test_set[""memory""][""data""].shape[0]\n\ntrain_batches = zip(\n    range(0, n_train - FLAGS.batch_size, FLAGS.batch_size),\n    range(FLAGS.batch_size, n_train, FLAGS.batch_size),\n)\ntrain_batches = [(start, end) for start, end in train_batches]\n\nval_batches = zip(\n    range(0, n_val - FLAGS.batch_size, FLAGS.batch_size),\n    range(FLAGS.batch_size, n_val, FLAGS.batch_size),\n)\nval_batches = [(start, end) for start, end in val_batches]\n\ntest_batches = zip(\n    range(0, n_test - FLAGS.batch_size, FLAGS.batch_size),\n    range(FLAGS.batch_size, n_test, FLAGS.batch_size),\n)\ntest_batches = [(start, end) for start, end in test_batches]\n\nwith tf.Session() as sess:\n    memn2n = MemN2N_Dialog(\n        FLAGS.batch_size,\n        babi.vocab_size,\n        babi.max_utt_len,\n        babi.memory_size,\n        FLAGS.emb_size,\n        babi.num_cands,\n        babi.max_cand_len,\n        hops=FLAGS.nhops,\n        max_grad_norm=FLAGS.grad_clip_norm,\n        optimizer=tf.train.AdamOptimizer(learning_rate=FLAGS.lr, epsilon=FLAGS.eps),\n        session=sess,\n    )\n\n    if FLAGS.restore and os.path.exists(weights_save_path):\n        print(""Loading weights from {}"".format(weights_save_path))\n        memn2n.saver.restore(sess, weights_save_path)\n    elif FLAGS.restore and os.path.exists(weights_save_path) is False:\n        print(\n            ""Could not find weights at {}. "".format(weights_save_path)\n            + ""Running with random initialization.""\n        )\n\n    for e in range(FLAGS.epochs):\n        np.random.shuffle(train_batches)\n        train_cost = []\n\n        for start, end in tqdm(\n            train_batches, total=len(train_batches), unit=""minibatches"", desc=""Epoch {}"".format(e)\n        ):\n            s = train_set[""memory""][""data""][start:end]\n            q = train_set[""user_utt""][""data""][start:end]\n            a = train_set[""answer""][""data""][start:end]\n\n            if not FLAGS.use_match_type:\n                c = np.tile(np.expand_dims(babi.cands, 0), [FLAGS.batch_size, 1, 1])\n            else:\n                c = train_set[""cands_mat""][""data""][start:end]\n\n            cost = memn2n.batch_fit(s, q, a, c)\n            train_cost.append(cost)\n\n        train_cost_str = ""Epoch {}: train_cost {}"".format(e, np.mean(train_cost))\n        print(train_cost_str)\n\n        if FLAGS.save_log:\n            with open(log_file, ""a"") as f:\n                f.write(train_cost_str + ""\\n"")\n\n        if e % FLAGS.save_epochs == 0:\n            print(""Saving model to {}"".format(weights_save_path))\n            memn2n.saver.save(sess, weights_save_path)\n            print(""Saving complete"")\n\n            val_error = []\n            # Eval after each epoch\n            for start, end in tqdm(\n                val_batches, total=len(val_batches), unit=""minibatches"", desc=""Epoch {}"".format(e)\n            ):\n                s = dev_set[""memory""][""data""][start:end]\n                q = dev_set[""user_utt""][""data""][start:end]\n                a = dev_set[""answer""][""data""][start:end]\n\n                if not FLAGS.use_match_type:\n                    c = np.tile(np.expand_dims(babi.cands, 0), [FLAGS.batch_size, 1, 1])\n                else:\n                    c = dev_set[""cands_mat""][""data""][start:end]\n\n                a_pred = memn2n.predict(s, q, c)\n\n                error = np.mean(a.argmax(axis=1) != a_pred)\n                val_error.append(error)\n\n            val_err_str = ""Epoch {}: Validation Error: {}"".format(e, np.mean(val_error))\n            print(val_err_str)\n            if FLAGS.save_log:\n                with open(log_file, ""a"") as f:\n                    f.write(val_err_str + ""\\n"")\n\n    print(""Training Complete."")\n    print(""Saving model to {}"".format(weights_save_path))\n    memn2n.saver.save(sess, weights_save_path)\n    print(""Saving complete"")\n\n    if FLAGS.interactive:\n        interactive_loop(memn2n, babi)\n\n    if FLAGS.test:\n        # Final evaluation on test set\n        test_error = []\n        # Eval after each epoch\n        for start, end in tqdm(test_batches, total=len(test_batches), unit=""minibatches""):\n            s = test_set[""memory""][""data""][start:end]\n            q = test_set[""user_utt""][""data""][start:end]\n            a = test_set[""answer""][""data""][start:end]\n\n            if not FLAGS.use_match_type:\n                c = np.tile(np.expand_dims(babi.cands, 0), [FLAGS.batch_size, 1, 1])\n            else:\n                c = test_set[""cands_mat""][""data""][start:end]\n\n            a_pred = memn2n.predict(s, q, c)\n\n            error = np.mean(a.argmax(axis=1) != a_pred)\n            test_error.append(error)\n\n        test_err_str = ""Test Error: {}"".format(np.mean(test_error))\n        print(test_err_str)\n        if FLAGS.save_log:\n            with open(log_file, ""a"") as f:\n                f.write(test_err_str + ""\\n"")\n'"
examples/most_common_word_sense/__init__.py,0,b''
examples/most_common_word_sense/feature_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ****************************************************************************\nimport re\n\nimport nltk\nimport numpy\nfrom numpy import dot\nfrom numpy.linalg import norm\n\nfrom nlp_architect.utils.generic import license_prompt\n\ntry:\n    nltk.data.find(""taggers/averaged_perceptron_tagger"")\nexcept LookupError:\n    if license_prompt(""Averaged Perceptron Tagger"", ""http://www.nltk.org/nltk_data/"") is False:\n        raise Exception(\n            ""can\'t continue data prepare process "" ""without downloading averaged_perceptron_tagger""\n        )\n    nltk.download(""averaged_perceptron_tagger"")\n\ntry:\n    nltk.data.find(""tokenizers/punkt"")\nexcept LookupError:\n    if license_prompt(""Punkt model"", ""http://www.nltk.org/nltk_data/"") is False:\n        raise Exception(""can\'t continue data prepare process "" ""without downloading punkt"")\n    nltk.download(""punkt"")\n\n\ndef extract_features_envelope(target_word, definition, hyps_vec, model_w2v):\n    """"""\n    extract features\n\n    Args:\n        target_word (list(str)): target word for finding senses\n        definition (list(str)):  definition of target word\n        hyps_vec (list(str)):    hypernym list of of target word\n        model_w2v (list(str)):   word embedding\'s model\n\n    Returns:\n        valid_w2v_flag(bool):         true if target word has w2v entry, else false\n        definition_sim_cbow(float):   cosine similarity between target word embedding and\n        definition cbow sentence embedding\n        definition_sim(float):        cosine similarity between target word embedding and\n        definition sentence embedding\n        hyps_sim(float):              cosine similarity between target word embedding and\n        hypernyms embeddings\n        target_word_emb(numpy.array): word embedding of target word\n        definition_sentence_emb_cbow(numpy.array): definition sentence cbow embedding\n\n    """"""\n    valid_w2v_flag, target_word_emb = return_w2v(target_word, model_w2v)\n\n    # calculate target word to definition similarity and definition similarity CBOW\n    definition_words = extract_meaningful_words_from_sentence(definition)\n    definition_sim = calc_word_to_sentence_sim_w2v(target_word, definition_words, model_w2v, 0)\n    definition_sentence_emb_cbow, definition_sim_cbow = calc_word_to_sentence_dist_cbow(\n        target_word, definition_words, model_w2v\n    )\n\n    # calculate hypernyms similarity\n    hyps_vec = convert_string_to_list_of_words(hyps_vec)\n    hyps_sim = calc_word_to_sentence_sim_w2v(target_word, hyps_vec, model_w2v, 2)\n\n    return [\n        valid_w2v_flag,\n        definition_sim_cbow,\n        definition_sim,\n        hyps_sim,\n        target_word_emb,\n        definition_sentence_emb_cbow,\n    ]\n\n\ndef extract_meaningful_words_from_sentence(sentence):\n    """"""\n    extract meaningful (nouns and verbs) words from sentence\n\n    Args:\n        sentence(str): input sentence\n\n    Returns:\n        list(str): vector of meaningful words\n\n    """"""\n    sentence = re.sub(r""[-+.^:,\\[\\]()]"", """", str(sentence))\n\n    tokens = nltk.word_tokenize(sentence)\n    pos_tags = nltk.pos_tag(tokens)\n    words_vec = []\n    cntr = 0\n    for word, tag in pos_tags:\n        if tag.startswith(""NN"") | tag.startswith(""VB""):\n            if not bool(re.search(r""\\d"", word)):  # disqualify words does not contain digits\n                words_vec.insert(cntr, word)\n                cntr += 1\n\n    return words_vec\n\n\ndef convert_string_to_list_of_words(string_list_of_words):\n    """"""\n    convert string to list of words\n\n    Args:\n        string_list_of_words(str): input sentence\n\n    Returns:\n        list(str): vector of words\n\n    """"""\n    string_list_of_words = re.sub(r""[-+.^:,\\[\\]()]"", """", str(string_list_of_words))\n    tokens = nltk.word_tokenize(string_list_of_words)\n\n    words_vec = []\n    cntr = 0\n    for word in tokens:\n        words_vec.insert(cntr, word)\n        cntr += 1\n\n    return words_vec\n\n\ndef calc_word_to_sentence_dist_cbow(target_word, sentence, model):\n    """"""\n    calculate cosine similaity between word emb. and sentence cbow emb.\n\n    Args:\n        target_word(str): input word\n        sentence(list(str)): input sentence\n        model(gensim.models.Word2Vec): w2v model\n\n    Returns:\n        numpy.array: cbow_sentence_emb, sentence cbow embedding\n        float: cosine_sim, cosine similarity between target word embedding and cbow sentence\n         embedding\n\n    """"""\n    cbow_sentence_emb = calc_cbow_sentence(sentence, model)\n\n    try:\n        wv_target_word = model[target_word]\n        cosine_sim = cosine_similarity(wv_target_word, cbow_sentence_emb)\n    # if target word is not in embedding dictionary\n    except KeyError:\n        cosine_sim = 0\n\n    return cbow_sentence_emb, cosine_sim\n\n\ndef cosine_similarity(vec1, vec2):\n    """"""\n    calculate cosine similarity between 2 vecs\n\n    Args:\n        vec1(numpy.array): input vec 1\n        vec2(numpy.array): input vec 2\n\n    Returns:\n        float: cosine_sim, cosine similarity between 2 vecs\n\n\n    """"""\n\n    cosine_sim = 0\n    try:\n        norm_vec1 = norm(vec1)\n        norm_vec2 = norm(vec2)\n        den = norm_vec1 * norm_vec2\n        if den != 0:\n            cosine_sim = dot(vec1, vec2) / den\n    except ValueError:  # if target word is not in embedding dictionary\n        cosine_sim = 0\n\n    return cosine_sim\n\n\ndef calc_cbow_sentence(sentence, model):\n    """"""\n    calc cbow embedding of an input sentence\n\n    Args:\n        sentence(bytearray): vector of words\n        model(gensim.models.Word2Vec): w2v model\n\n    Returns:\n        numpy.array: cbow_sentence, cbow embedding of the input sentence\n\n    """"""\n    cbow_sentence = numpy.zeros(300)\n    i = 0\n    for word in sentence:\n        try:\n            wv = model[word]\n            cbow_sentence = cbow_sentence + wv\n            i += 1\n        except KeyError:  # if word is not in embedding dictionary\n            pass  # no-operation\n        except IndexError:  # if word is not in embedding dictionary\n            pass  # no-operation\n\n    if i > 0:\n        cbow_sentence = cbow_sentence / i\n\n    return cbow_sentence\n\n\ndef calc_word_to_sentence_sim_w2v(target_word, string_vec, model, max_items_to_test):\n    """"""\n\n    Args:\n        target_word(str): input target word\n        string_vec(list(str)): sentence\n        model(gensim.models.Word2Vec): w2v model\n        max_items_to_test(int): max number of words\n\n    Returns:\n        float: top_av, average embedding similarity betewwen target word and words in string vec\n        (sentence)\n\n    """"""\n    sim_score_vec = []\n    i = 0\n    for word in string_vec:\n        # remove leading white spaces\n        word = word.strip()\n        if target_word != word:\n            sim_score = w2v_similarity_envelope(target_word, word, model)\n            sim_score_vec.insert(i, sim_score)\n            i += 1\n        if max_items_to_test > 0:\n            if max_items_to_test == i:\n                break\n\n    top_av = calc_top_av(sim_score_vec)\n    return top_av\n\n\ndef w2v_similarity_envelope(word_a, phrase_b, model):\n    """"""\n    calculate cosine similarity between 2 words\n\n    Args:\n        word_a(str):   input word a\n        phrase_b(str): input word b\n        model(gensim.models.Word2Vec): w2v model\n\n    Returns:\n        float: similarity, mean cosine similarity between 2 words\n\n\n    """"""\n    try:\n        similarity = -1\n        sim_scores_vec = []\n        word_vec = phrase_b.split(""_"")  # in case wordB is a phrase\n        i = 0\n        for wordB in word_vec:\n            # if (wordA != wordB) & ( wordA not in stopwords.words(\'english\')):\n            if word_a != wordB:\n                sim = w2v_similarity(word_a, wordB, model)\n                if sim > -1:\n                    sim_scores_vec.insert(i, sim)\n                    i = i + 1\n        if sim_scores_vec:\n            similarity = numpy.mean(sim_scores_vec)\n\n        return similarity\n    except ValueError:\n        return -1\n\n\ndef w2v_similarity(word_a, word_b, model):\n    """"""\n\n    Args:\n        word_a(str):   input word a\n        word_b(str):   input word b (can be phrase)\n        model(gensim.models.Word2Vec): w2v model\n\n    Returns:\n        float: similarity, cosine similarity between 2 words\n\n    """"""\n    try:\n        similarity = model.similarity(word_a, word_b)\n        return similarity\n    except ValueError:\n        return -1\n    except IndexError:\n        return -1\n    except KeyError:\n        return -1\n\n\ndef return_w2v(word_a, model):\n    """"""\n    extract embedding vector of word_a\n\n    Args:\n         word_a(str):   input word\n        model(gensim.models.Word2Vec): w2v model\n\n    Returns:\n        numpy.array: w2v, embedding vector of word_a\n\n    """"""\n    w2v = None\n    try:\n        w2v = numpy.array\n        w2v = model[word_a]\n        return True, w2v\n    except ValueError:\n        return False, w2v\n    except IndexError:\n        return False, w2v\n    except KeyError:\n        return False, w2v\n\n\ndef calc_top_av(sim_score_vec):\n    """"""\n    calc top average of scores vector\n\n    Args:\n        sim_score_vec(list(float)): vector of similarity scores\n\n    Returns:\n        float: av_score, average of top similarity scores in sim_score_vec\n\n    """"""\n    av_number_th = 3\n    sim_score_vec_sorted = sorted(sim_score_vec, reverse=True)\n    cntr = 0\n    score_acc = 0\n    for score in sim_score_vec_sorted:\n        if score > -1:\n            score_acc = score_acc + score\n            cntr = cntr + 1\n            if cntr >= av_number_th:\n                break\n\n    if cntr > 0:\n        av_score = score_acc / cntr\n    else:\n        av_score = 0\n\n    return av_score\n\n\ndef get_inherited_hypernyms_list(synset, hyps_list):\n    """"""\n    get inherited hypernyms list of synset\n    Args:\n        synset(synset): synset\n        hyps_list: hypernym list\n\n    Returns:\n        list(str): hyps_list, hypernym list\n\n    """"""\n\n    for hypernym in synset.hypernyms():\n        hyp_string = hypernym.name()\n        hyp_string = hyp_string.split(""."")[0]\n        hyps_list.append(hyp_string)\n        set(get_inherited_hypernyms_list(hypernym, hyps_list))\n\n    return hyps_list\n\n\ndef get_synonyms(synset):\n    """"""\n    get synonyms list of synset\n    Args:\n        synset(synset): synset\n\n    Returns:\n        list(str): synonym_list, synonyms list\n\n    """"""\n    synonym_list = []\n    i = 0\n    for synonym in synset.lemma_names():\n        synonym_list.insert(i, synonym.replace(""_"", "" ""))\n        i = i + 1\n\n    return synonym_list\n\n\ndef extract_synset_data(synset):\n    """"""\n\n    Args:\n        synset(synset): synset\n\n    Returns:\n        definition(str): definition of synset\n        list(str): synonym_list, synonyms list\n        list(str): hyps_list, hypernym list\n\n    """"""\n    # a. get definition\n    definition = synset.definition()\n    # b. get inherited hypernyms\n    hyper_list = []\n    hyps_list = get_inherited_hypernyms_list(synset, hyper_list)\n\n    # c. get synonyms\n    synonym_list = get_synonyms(synset)\n\n    return definition, hyps_list, synonym_list\n'"
examples/most_common_word_sense/inference.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ****************************************************************************\n""""""\nMost Common Word Sense Inference module.\n""""""\nimport argparse\nimport logging\nimport gensim\nimport numpy as np\nfrom nltk.corpus import wordnet as wn\nfrom termcolor import colored\n\nfrom examples.most_common_word_sense.feature_extraction import (\n    extract_synset_data,\n    extract_features_envelope,\n)\nfrom examples.most_common_word_sense.prepare_data import read_inference_input_examples_file\nfrom nlp_architect.models.most_common_word_sense import MostCommonWordSense\nfrom nlp_architect.utils.io import validate_existing_filepath, check_size\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef wsd_classify(x_test, y_test=None):\n    """"""\n    classifiy target word. output all word senses ranked according to the most probable sense\n\n    Args:\n        x_test(numpy.ndarray): input x data for inference\n        y_test: input y data for inference\n\n    Returns:\n         str: predicted values by the model\n    """"""\n    # test set\n    x_test = np.array(x_test)\n    if y_test is not None:\n        y_test = np.array(y_test)\n    test_set = {""X"": x_test, ""y"": y_test}\n\n    mlp_clf = MostCommonWordSense(args.epochs, args.batch_size, None)\n    # load existing model\n    mlp_clf.load(args.model)\n\n    results = mlp_clf.get_outputs(test_set[""X""])\n\n    return results\n\n\nif __name__ == ""__main__"":\n    # parse the command line arguments\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        ""--max_num_of_senses_to_search"",\n        default=3,\n        type=int,\n        action=check_size(0, 100),\n        help=""maximum number of senses that are tests"",\n    )\n    parser.add_argument(\n        ""--input_inference_examples_file"",\n        type=validate_existing_filepath,\n        default=""data/input_inference_examples.csv"",\n        help=""input_data_file"",\n    )\n    parser.add_argument(\n        ""--model"",\n        default=""data/mcs_model.h5"",\n        type=validate_existing_filepath,\n        help=""path to the file where the trained model has been stored"",\n    )\n    parser.add_argument(\n        ""--word_embedding_model_file"",\n        type=validate_existing_filepath,\n        default=""pretrained_models/GoogleNews-vectors-negative300.bin"",\n        help=""path to the word embedding\'s model"",\n    )\n    parser.add_argument(\n        ""--epochs"", default=100, type=int, help=""number of epochs"", action=check_size(0, 200)\n    )\n    parser.add_argument(\n        ""--batch_size"", default=50, type=int, help=""batch_size"", action=check_size(0, 256)\n    )\n\n    args = parser.parse_args()\n\n    # 1. input data\n    target_word_vec = read_inference_input_examples_file(args.input_inference_examples_file)\n    logger.info(""finished reading inference input examples file"")\n\n    # 2. Load pre-trained word embeddings model.\n    word_embeddings_model = gensim.models.KeyedVectors.load_word2vec_format(\n        args.word_embedding_model_file, binary=True\n    )\n    logger.info(""finished loading word embeddings model"")\n\n    example_cntr = 0\n    for input_word in target_word_vec:\n        mtype = ""f4, S200""\n        sense_data_matrix = np.zeros(0, dtype=mtype)\n\n        i = 0\n        # 3. iterate over all synsets of the word\n        for synset in wn.synsets(input_word):\n            # extract all synset data\n            definition, hyps_list, synonym_list = extract_synset_data(synset)\n            # 4. feature extraction\n            [\n                valid_w2v_flag,\n                definition_sim_cbow,\n                definition_sim,\n                hyps_sim,\n                target_word_emb,\n                definition_sentence_emb_cbow,\n            ] = extract_features_envelope(input_word, definition, hyps_list, word_embeddings_model)\n\n            feature_vec = np.array([definition_sim_cbow, definition_sim, hyps_sim])\n            feature_vec = np.concatenate((feature_vec, target_word_emb), 0)\n            feature_vec = np.concatenate((feature_vec, definition_sentence_emb_cbow), 0)\n            featVecDim = feature_vec.shape[0]\n            # X_featureMatrix dim should be (1,featVecDim) but neon classifier gets a minimum of\n            # 10 samples not just 1\n            X_featureMatrix = np.zeros((10, featVecDim))\n            X_featureMatrix[0, :] = feature_vec\n\n            # 5. inference\n            classifierOutScore = wsd_classify(x_test=X_featureMatrix, y_test=None)\n            data_str = ""hyps: "" + str(hyps_list[0:2]) + "" definition: "" + definition\n            sense_data_matrix = np.append(\n                sense_data_matrix, np.array([(classifierOutScore[0, 1], data_str)], dtype=mtype)\n            )\n\n            i = i + 1\n\n            # max num of senses to check\n            if i == int(args.max_num_of_senses_to_search):\n                break\n        example_cntr = example_cntr + 1\n\n        # find sense with max score\n        if sense_data_matrix is not None:\n            max_val = max(\n                sense_data_matrix, key=lambda sense_data_matrix_entry: sense_data_matrix_entry[0]\n            )\n            max_val = max_val[0]\n            header_text = ""word: "" + input_word\n            print(colored(header_text, ""grey"", attrs=[""bold"", ""underline""]))\n\n            for data_sense in sense_data_matrix:\n                if data_sense[0] == max_val:\n                    print(colored(data_sense, ""green"", attrs=[""bold""]))\n                else:\n                    print(data_sense)\n\n            print()\n'"
examples/most_common_word_sense/prepare_data.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ****************************************************************************\n""""""\nPrepare the datasets for Most Common Word Sense training\n""""""\n\nimport argparse\nimport csv\nimport logging\nimport math\nimport pickle\n\nimport gensim\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom examples.most_common_word_sense.feature_extraction import extract_features_envelope\nfrom nlp_architect.utils.io import validate_existing_filepath, check_size, validate_parent_exists\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef read_gs_file(gs_file_name):\n    """"""\n    reads gold standard file\n\n    Args:\n        gs_file_name (str): the file path\n\n    Returns:\n        target_word_vec1 (list(str)): words vector\n        definition_vec1  (list(str)): words definitions vector - definition per target word\n        hypernym_vec1    (list(str)): words hypernyms vector\n        label_vec1       (list(str)): labels of binary class 0/1\n    """"""\n    with open(gs_file_name, ""rU"", encoding=""utf-8"") as file:\n        reader = csv.reader((line.replace(""\\0"", """") for line in file))\n\n        cntr1 = 0\n        # 1. read csv file\n        target_word_vec1 = []\n        definition_vec1 = []\n        hypernym_vec1 = []\n\n        label_vec1 = []\n\n        header_line_flag = True\n        for line in reader:\n            if line is not None:\n                if header_line_flag:  # skip header line\n                    header_line_flag = False\n                    continue\n\n                target_word_vec1.insert(cntr1, line[0].strip())\n                definition_vec1.insert(cntr1, line[1])\n                hypernym_vec1.insert(cntr1, line[2])\n                label_vec1.insert(cntr1, line[3])\n                cntr1 = cntr1 + 1\n\n    return target_word_vec1, definition_vec1, hypernym_vec1, label_vec1\n\n\ndef read_inference_input_examples_file(input_examples_file):\n    """"""\n    read inference input examples file\n\n    Args:\n        input_examples_file(str): inference input file containing a vector of target word\n\n    Returns:\n        list(str): target word vector\n\n    """"""\n    with open(input_examples_file, ""rU"", encoding=""utf-8"") as file:\n        reader = csv.reader((line.replace(""\\0"", """") for line in file))\n        cntr2 = 0\n        # 1. read csv file\n        target_word_vec1 = []\n        header_line_flag = True\n        for line in reader:\n            if line is not None:\n                if header_line_flag:  # skip header line\n                    header_line_flag = False\n                    continue\n                target_word_vec1.insert(cntr2, line[0])\n\n                cntr2 = cntr2 + 1\n\n    return target_word_vec1\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--gold_standard_file"",\n        default=""data/goldStd.csv"",\n        type=validate_existing_filepath,\n        help=""path to gold standard file"",\n    )\n    parser.add_argument(\n        ""--word_embedding_model_file"",\n        type=validate_existing_filepath,\n        default=""pretrained_models/GoogleNews-vectors-negative300.bin"",\n        help=""path to the word embedding\'s model"",\n    )\n    parser.add_argument(\n        ""--training_to_validation_size_ratio"",\n        default=0.8,\n        type=float,\n        action=check_size(0, 1),\n        help=""ratio between training and validation size"",\n    )\n    parser.add_argument(\n        ""--data_set_file"",\n        default=""data/data_set.pkl"",\n        type=validate_parent_exists,\n        help=""path the file where the train, valid and test sets will be stored"",\n    )\n\n    args = parser.parse_args()\n    # training set\n    X_train = []\n    y_train = []\n\n    # validation set\n    X_valid = []\n    y_valid = []\n\n    # 1. read GS file\n    [target_word_vec, definition_vec, hypernym_vec, label_vec] = read_gs_file(\n        args.gold_standard_file\n    )\n    logger.info(""finished reading GS file"")\n\n    # 2. Load pre-trained word embeddings model.\n    word_embeddings_model = gensim.models.KeyedVectors.load_word2vec_format(\n        args.word_embedding_model_file, binary=True\n    )\n    logger.info(""finished loading word embeddings model"")\n\n    feat_vec_dim = 603\n    num_samples = len(list(target_word_vec))\n\n    x_feature_matrix = np.zeros((num_samples, feat_vec_dim))\n    y_labels_vec = []\n\n    i = 0\n    cntr = 0\n    for target_word in target_word_vec:\n        InputWord = target_word_vec[i]\n        definition = definition_vec[i]\n        hypslist = hypernym_vec[i]\n        label = label_vec[i]\n\n        [\n            valid_w2v_flag,\n            definition_sim_cbow,\n            definition_sim,\n            hyps_sim,\n            target_word_emb,\n            definition_sentence_emb_cbow,\n        ] = extract_features_envelope(InputWord, definition, hypslist, word_embeddings_model)\n\n        if definition_sim_cbow != 0 and definition_sim != 0 and valid_w2v_flag is True:\n            feature_vec = np.array([definition_sim_cbow, definition_sim, hyps_sim])\n            feature_vec = np.concatenate((feature_vec, target_word_emb), 0)\n            feature_vec = np.concatenate((feature_vec, definition_sentence_emb_cbow), 0)\n            X_features = np.array(feature_vec)\n            x_feature_matrix[cntr, :] = X_features\n\n            #           binary classifier = 2 categories\n            y_vec = np.zeros(2, ""uint8"")\n            y_vec[int(label)] = 1\n            y_labels_vec.append(y_vec)\n\n            cntr = cntr + 1\n\n        i = i + 1\n\n    logger.info(""finished feature extraction"")\n    x_feature_matrix = x_feature_matrix[0 : len(y_labels_vec), 0:feat_vec_dim]\n\n    # split between train and valid sets\n    X_train1, X_valid1, y_train1, y_valid1 = train_test_split(\n        x_feature_matrix,\n        y_labels_vec,\n        train_size=math.ceil(num_samples * float(args.training_to_validation_size_ratio)),\n    )\n\n    X_train.extend(X_train1)\n    X_valid.extend(X_valid1)\n    y_train.extend(y_train1)\n    y_valid.extend(y_valid1)\n\n    logger.info(""training set size: %s"", str(len(y_train)))\n    logger.info(""validation set size: %s"", str(len(y_valid)))\n\n    # store data on file\n    data_out = dict()\n\n    data_out[""X_train""] = X_train\n    data_out[""X_valid""] = X_valid\n    data_out[""y_train""] = y_train\n    data_out[""y_valid""] = y_valid\n\n    with open(args.data_set_file, ""wb"") as fp:\n        pickle.dump(data_out, fp)\n'"
examples/most_common_word_sense/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ****************************************************************************\n""""""\nMost Common Word Sense - Train MLP classifier and evaluate it.\n""""""\nimport argparse\nimport logging\nimport pickle\n\nimport numpy as np\n\nfrom nlp_architect.models.most_common_word_sense import MostCommonWordSense\nfrom nlp_architect.utils.io import validate_existing_filepath, validate_parent_exists, check_size\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef most_common_word_train(x_train, y_train, x_valid, y_valid):\n    """"""\n    Train an MLP model, save it and evaluate it\n\n    Args:\n        x_train: x data for training\n        y_train: y data for training\n        x_valid: x data for validation\n        y_valid: x data for validation\n\n    Returns:\n        str: reslts, predicted values by the model\n\n    """"""\n\n    # train set\n    x_train = np.array(x_train)\n    y_train1 = np.array(y_train)\n    train_set = {""X"": x_train, ""y"": y_train1}\n\n    # validation set\n    x_valid = np.array(x_valid)\n    y_valid1 = np.array(y_valid)\n    valid_set = {""X"": x_valid, ""y"": y_valid1}\n\n    input_dim = train_set[""X""].shape[1]\n    mlp_model = MostCommonWordSense(args.epochs, args.batch_size, None)\n    # build model\n    mlp_model.build(input_dim)\n    # train\n    mlp_model.fit(train_set)\n    # save model\n    mlp_model.save(args.model)\n\n    # evaluation\n    error_rate = mlp_model.eval(valid_set)\n    logger.info(""Mis-classification error on validation set= %0.1f"", error_rate * 100)\n\n    reslts = mlp_model.get_outputs(valid_set[""X""])\n\n    return reslts\n\n\nif __name__ == ""__main__"":\n    # parse the command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--data_set_file"",\n        default=""data/data_set.pkl"",\n        type=validate_existing_filepath,\n        help=""train and validation sets path"",\n    )\n    parser.add_argument(\n        ""--model"",\n        default=""data/mcs_model.h5"",\n        type=validate_parent_exists,\n        help=""trained model full path"",\n    )\n    parser.add_argument(\n        ""--epochs"", default=100, type=int, help=""number of epochs"", action=check_size(0, 200)\n    )\n    parser.add_argument(\n        ""--batch_size"", default=50, type=int, help=""batch_size"", action=check_size(0, 256)\n    )\n\n    args = parser.parse_args()\n\n    # read training and validation data file\n    with open(args.data_set_file, ""rb"") as fp:\n        data_in = pickle.load(fp)\n\n    X_train = data_in[""X_train""]\n    X_valid = data_in[""X_valid""]\n    Y_train = data_in[""y_train""]\n    Y_valid = data_in[""y_valid""]\n\n    logger.info(""training set size: %s"", str(len(Y_train)))\n    logger.info(""validation set size: %s"", str(len(Y_valid)))\n\n    results = most_common_word_train(\n        x_train=X_train, y_train=Y_train, x_valid=X_valid, y_valid=Y_valid\n    )\n'"
examples/ner/__init__.py,0,b''
examples/ner/interactive.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import division, print_function, unicode_literals, absolute_import\n\nimport argparse\nimport pickle\n\nimport numpy as np\n\nfrom nlp_architect.models.ner_crf import NERCRF\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.io import validate_existing_filepath\nfrom nlp_architect.utils.text import SpacyInstance\n\nnlp = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n\n\ndef read_input_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--model_path"", type=validate_existing_filepath, required=True, help=""Path of model weights""\n    )\n    parser.add_argument(\n        ""--model_info_path"",\n        type=validate_existing_filepath,\n        required=True,\n        help=""Path of model topology"",\n    )\n    input_args = parser.parse_args()\n    return input_args\n\n\ndef load_saved_model():\n    ner_model = NERCRF()\n    ner_model.load(args.model_path)\n    return ner_model\n\n\ndef process_text(doc):\n    input_text = "" "".join(doc.strip().split())\n    return nlp.tokenize(input_text)\n\n\ndef vectorize(doc, w_vocab, c_vocab):\n    words = np.asarray([w_vocab[w.lower()] if w.lower() in w_vocab else 1 for w in doc]).reshape(\n        1, -1\n    )\n    sentence_chars = []\n    for w in doc:\n        word_chars = []\n        for c in w:\n            if c in c_vocab:\n                _cid = c_vocab[c]\n            else:\n                _cid = 1\n            word_chars.append(_cid)\n        sentence_chars.append(word_chars)\n    sentence_chars = np.expand_dims(pad_sentences(sentence_chars, model.word_length), axis=0)\n    return words, sentence_chars\n\n\nif __name__ == ""__main__"":\n    args = read_input_args()\n    with open(args.model_info_path, ""rb"") as fp:\n        model_info = pickle.load(fp)\n    assert model_info is not None, ""No model topology information loaded""\n    model = load_saved_model()\n    word_vocab = model_info[""word_vocab""]\n    y_vocab = {v: k for k, v in model_info[""y_vocab""].items()}\n    char_vocab = model_info[""char_vocab""]\n    while True:\n        text = input(""Enter sentence >> "")\n        text_arr = process_text(text)\n        doc_vec = vectorize(text_arr, word_vocab, char_vocab)\n        seq_len = np.array([len(text_arr)]).reshape(-1, 1)\n        inputs = list(doc_vec)\n        # pylint: disable=no-member\n        if model.crf_mode == ""pad"":\n            inputs = list(doc_vec) + [seq_len]\n        doc_ner = model.predict(inputs, batch_size=1).argmax(2).flatten()\n        ners = [y_vocab.get(n, None) for n in doc_ner]\n        for t, n in zip(text_arr, ners):\n            print(""{}\\t{}\\t"".format(t, n))\n        print()\n'"
examples/ner/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport argparse\nimport pickle\nfrom os import path\n\nimport numpy as np\nfrom tensorflow import keras\n\nfrom nlp_architect.nn.tensorflow.python.keras.callbacks import ConllCallback\nfrom nlp_architect.data.sequential_tagging import SequentialTaggingDataset\nfrom nlp_architect.models.ner_crf import NERCRF\nfrom nlp_architect.utils.embedding import get_embedding_matrix, load_word_embeddings\nfrom nlp_architect.utils.io import validate, validate_existing_filepath, validate_parent_exists\nfrom nlp_architect.utils.metrics import get_conll_scores\n\n\ndef read_input_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-b"", type=int, default=10, help=""Batch size"")\n    parser.add_argument(""-e"", type=int, default=10, help=""Number of epochs"")\n    parser.add_argument(\n        ""--train_file"",\n        type=validate_existing_filepath,\n        required=True,\n        help=""Train file (sequential tagging dataset format)"",\n    )\n    parser.add_argument(\n        ""--test_file"",\n        type=validate_existing_filepath,\n        required=True,\n        help=""Test file (sequential tagging dataset format)"",\n    )\n    parser.add_argument(\n        ""--tag_num"", type=int, default=2, help=""Entity labels tab number in train/test files""\n    )\n    parser.add_argument(""--sentence_length"", type=int, default=50, help=""Max sentence length"")\n    parser.add_argument(""--word_length"", type=int, default=12, help=""Max word length in characters"")\n    parser.add_argument(\n        ""--word_embedding_dims"",\n        type=int,\n        default=100,\n        help=""Word features embedding dimension size"",\n    )\n    parser.add_argument(\n        ""--character_embedding_dims"",\n        type=int,\n        default=25,\n        help=""Character features embedding dimension size"",\n    )\n    parser.add_argument(\n        ""--char_features_lstm_dims"",\n        type=int,\n        default=25,\n        help=""Character feature extractor LSTM dimension size"",\n    )\n    parser.add_argument(\n        ""--entity_tagger_lstm_dims"", type=int, default=100, help=""Entity tagger LSTM dimension size""\n    )\n    parser.add_argument(""--dropout"", type=float, default=0.2, help=""Dropout rate"")\n    parser.add_argument(\n        ""--embedding_model"",\n        type=validate_existing_filepath,\n        help=""Path to external word embedding model file"",\n    )\n    parser.add_argument(\n        ""--model_path"", type=str, default=""model.h5"", help=""Path for saving model weights""\n    )\n    parser.add_argument(\n        ""--model_info_path"",\n        type=str,\n        default=""model_info.dat"",\n        help=""Path for saving model topology"",\n    )\n    parser.add_argument(\n        ""--use_cudnn"", default=False, action=""store_true"", help=""use CUDNN based LSTM cells""\n    )\n    input_args = parser.parse_args()\n    validate_input_args(input_args)\n    return input_args\n\n\ndef validate_input_args(input_args):\n    validate((input_args.b, int, 1, 100000))\n    validate((input_args.e, int, 1, 100000))\n    validate((input_args.tag_num, int, 1, 1000))\n    validate((input_args.sentence_length, int, 1, 10000))\n    validate((input_args.word_length, int, 1, 100))\n    validate((input_args.word_embedding_dims, int, 1, 10000))\n    validate((input_args.character_embedding_dims, int, 1, 1000))\n    validate((input_args.char_features_lstm_dims, int, 1, 10000))\n    validate((input_args.entity_tagger_lstm_dims, int, 1, 10000))\n    validate((input_args.dropout, float, 0, 1))\n    model_path = path.join(path.dirname(path.realpath(__file__)), str(input_args.model_path))\n    validate_parent_exists(model_path)\n    model_info_path = path.join(\n        path.dirname(path.realpath(__file__)), str(input_args.model_info_path)\n    )\n    validate_parent_exists(model_info_path)\n\n\nif __name__ == ""__main__"":\n    # parse the input\n    args = read_input_args()\n\n    # load dataset and parameters\n    dataset = SequentialTaggingDataset(\n        args.train_file,\n        args.test_file,\n        max_sentence_length=args.sentence_length,\n        max_word_length=args.word_length,\n        tag_field_no=args.tag_num,\n    )\n\n    # get the train and test data sets\n    x_train, x_char_train, y_train = dataset.train_set\n    x_test, x_char_test, y_test = dataset.test_set\n\n    num_y_labels = len(dataset.y_labels) + 1\n    vocabulary_size = dataset.word_vocab_size\n    char_vocabulary_size = dataset.char_vocab_size\n\n    y_test = keras.utils.to_categorical(y_test, num_y_labels)\n    y_train = keras.utils.to_categorical(y_train, num_y_labels)\n\n    ner_model = NERCRF(use_cudnn=args.use_cudnn)\n    # pylint: disable=unexpected-keyword-arg\n    ner_model.build(\n        args.word_length,\n        num_y_labels,\n        vocabulary_size,\n        char_vocabulary_size,\n        word_embedding_dims=args.word_embedding_dims,\n        char_embedding_dims=args.character_embedding_dims,\n        tagger_lstm_dims=args.entity_tagger_lstm_dims,\n        dropout=args.dropout,\n    )\n\n    # initialize word embedding if external model selected\n    if args.embedding_model is not None:\n        embedding_model, _ = load_word_embeddings(args.embedding_model)\n        embedding_mat = get_embedding_matrix(embedding_model, dataset.word_vocab)\n        ner_model.load_embedding_weights(embedding_mat)\n\n    train_inputs = [x_train, x_char_train]\n    test_inputs = [x_test, x_char_test]\n\n    train_inputs.append(np.sum(np.not_equal(x_train, 0), axis=-1).reshape((-1, 1)))\n    test_inputs.append(np.sum(np.not_equal(x_test, 0), axis=-1).reshape((-1, 1)))\n\n    conll_cb = ConllCallback(test_inputs, y_test, dataset.y_labels.vocab, batch_size=args.b)\n    ner_model.fit(\n        x=train_inputs,\n        y=y_train,\n        batch_size=args.b,\n        epochs=args.e,\n        callbacks=[conll_cb],\n        validation=(test_inputs, y_test),\n    )\n\n    # saving model\n    ner_model.save(args.model_path)\n    with open(args.model_info_path, ""wb"") as fp:\n        info = {\n            ""y_vocab"": dataset.y_labels.vocab,\n            ""word_vocab"": dataset.word_vocab.vocab,\n            ""char_vocab"": dataset.char_vocab.vocab,\n        }\n        pickle.dump(info, fp)\n\n    # running predictions\n    predictions = ner_model.predict(x=test_inputs, batch_size=args.b)\n    eval = get_conll_scores(predictions, y_test, {v: k for k, v in dataset.y_labels.vocab.items()})\n    print(eval)\n'"
examples/np2vec/__init__.py,0,b''
examples/np2vec/inference.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport logging\n\nfrom nlp_architect.models.np2vec import NP2vec\nfrom nlp_architect.utils.io import validate_existing_filepath, check_size\n\nlogger = logging.getLogger(__name__)\n\nif __name__ == ""__main__"":\n    arg_parser = argparse.ArgumentParser()\n    arg_parser.add_argument(\n        ""--np2vec_model_file"",\n        default=""conll2000.train.model"",\n        help=""path to the file with the np2vec model to load."",\n        type=validate_existing_filepath,\n    )\n    arg_parser.add_argument(\n        ""--binary"",\n        help=""boolean indicating whether the model to load has been stored in binary "" ""format."",\n        action=""store_true"",\n    )\n    arg_parser.add_argument(\n        ""--word_ngrams"",\n        default=0,\n        type=int,\n        choices=[0, 1],\n        help=""If 0, the model to load stores word information. If 1, the model to load stores ""\n        ""subword (ngrams) information; note that subword information is relevant only to ""\n        ""fasttext models."",\n    )\n    arg_parser.add_argument(\n        ""--mark_char"",\n        default=""_"",\n        type=str,\n        action=check_size(1, 2),\n        help=""special character that marks word separator and NP suffix."",\n    )\n    arg_parser.add_argument(\n        ""--np"",\n        default=""Intel Corp."",\n        type=str,\n        action=check_size(min_size=1),\n        required=True,\n        help=""NP to print its word vector."",\n    )\n\n    args = arg_parser.parse_args()\n\n    np2vec_model = NP2vec.load(\n        args.np2vec_model_file, binary=args.binary, word_ngrams=args.word_ngrams\n    )\n\n    print(\n        ""word vector for the NP \'"" + args.np + ""\':"",\n        np2vec_model[args.mark_char.join(args.np.split()) + args.mark_char],\n    )\n'"
examples/np2vec/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport logging\n\nfrom nlp_architect.models.np2vec import NP2vec\nfrom nlp_architect.utils.io import check_size, validate_existing_filepath\n\nlogger = logging.getLogger(__name__)\n\n\nif __name__ == ""__main__"":\n    arg_parser = argparse.ArgumentParser()\n    arg_parser.add_argument(\n        ""--corpus"",\n        default=""train.txt"",\n        type=str,\n        action=check_size(min_size=1),\n        help=""path to the corpus. By default, ""\n        ""it is the training set of CONLL2000 shared task dataset."",\n    )\n    arg_parser.add_argument(\n        ""--corpus_format"",\n        default=""conll2000"",\n        type=str,\n        choices=[""json"", ""txt"", ""conll2000""],\n        help=""format of the input marked corpus; txt, conll2000 and json formats are supported. ""\n        ""For json format, the file should contain an iterable of sentences. ""\n        ""Each sentence is a list of terms (unicode strings) that will be used for training."",\n    )\n    arg_parser.add_argument(\n        ""--mark_char"",\n        default=""_"",\n        type=str,\n        action=check_size(1, 2),\n        help=""special character that marks word separator and NP suffix."",\n    )\n    arg_parser.add_argument(\n        ""--word_embedding_type"",\n        default=""word2vec"",\n        type=str,\n        choices=[""word2vec"", ""fasttext""],\n        help=""word embedding model type; word2vec and fasttext are supported."",\n    )\n    arg_parser.add_argument(\n        ""--np2vec_model_file"",\n        default=""conll2000.train.model"",\n        type=str,\n        action=check_size(min_size=1),\n        help=""path to the file where the trained np2vec model has to be stored."",\n    )\n    arg_parser.add_argument(\n        ""--binary"",\n        help=""boolean indicating whether the model is stored in binary format; if ""\n        ""word_embedding_type is fasttext and word_ngrams is 1, ""\n        ""binary should be set to True."",\n        action=""store_true"",\n    )\n\n    # word2vec and fasttext common hyperparameters\n    arg_parser.add_argument(\n        ""--sg"",\n        default=0,\n        type=int,\n        choices=[0, 1],\n        help=""model training hyperparameter, skip-gram. Defines the training algorithm. If 1, ""\n        ""CBOW is used, otherwise, skip-gram is employed."",\n    )\n    arg_parser.add_argument(\n        ""--size"",\n        default=100,\n        type=int,\n        action=check_size(min_size=1),\n        help=""model training hyperparameter, size of the feature vectors."",\n    )\n    arg_parser.add_argument(\n        ""--window"",\n        default=10,\n        type=int,\n        action=check_size(min_size=1),\n        help=""model training hyperparameter, maximum distance ""\n        ""between the current and predicted word within a ""\n        ""sentence."",\n    )\n    arg_parser.add_argument(\n        ""--alpha"",\n        default=0.025,\n        type=float,\n        action=check_size(min_size=0),\n        help=""model training hyperparameter. The initial learning rate."",\n    )\n    arg_parser.add_argument(\n        ""--min_alpha"",\n        default=0.0001,\n        type=float,\n        action=check_size(min_size=0),\n        help=""model training hyperparameter. Learning rate will linearly drop to `min_alpha` as ""\n        ""training progresses."",\n    )\n    arg_parser.add_argument(\n        ""--min_count"",\n        default=5,\n        type=int,\n        action=check_size(min_size=0),\n        help=""model training hyperparameter, ignore all words ""\n        ""with total frequency lower than this."",\n    )\n    arg_parser.add_argument(\n        ""--sample"",\n        default=1e-5,\n        type=float,\n        action=check_size(min_size=0),\n        help=""model training hyperparameter, threshold for ""\n        ""configuring which higher-frequency words are ""\n        ""randomly downsampled, useful range is (0, 1e-5)"",\n    )\n    arg_parser.add_argument(\n        ""--workers"",\n        default=20,\n        type=int,\n        action=check_size(min_size=1),\n        help=""model training hyperparameter, number of worker threads."",\n    )\n    arg_parser.add_argument(\n        ""--hs"",\n        default=0,\n        type=int,\n        choices=[0, 1],\n        help=""model training hyperparameter, hierarchical softmax. If set to 1, hierarchical ""\n        ""softmax will be used for model training. ""\n        ""If set to 0, and `negative` is non-zero, negative sampling will be used."",\n    )\n    arg_parser.add_argument(\n        ""--negative"",\n        default=25,\n        type=int,\n        action=check_size(min_size=0),\n        help=""model training hyperparameter, negative sampling. If > 0, negative sampling will be ""\n        \'used, the int for negative specifies how many ""noise words"" should be drawn (\'\n        ""usually between 5-20). If set to 0, no negative sampling is used."",\n    )\n    arg_parser.add_argument(\n        ""--cbow_mean"",\n        default=1,\n        type=int,\n        choices=[0, 1],\n        help=""model training hyperparameter.  If 0, use the sum of the context word vectors. ""\n        ""If 1, use the mean, only applies when cbow is used."",\n    )\n    arg_parser.add_argument(\n        ""--iter"",\n        default=15,\n        type=int,\n        action=check_size(min_size=1),\n        help=""model training hyperparameter, number of iterations."",\n    )\n\n    # fasttext hyperparameters\n    arg_parser.add_argument(\n        ""--min_n"",\n        default=3,\n        type=int,\n        action=check_size(min_size=1),\n        help=""fasttext training hyperparameter. Min length of char ngrams to be used for training ""\n        ""word representations."",\n    )\n    arg_parser.add_argument(\n        ""--max_n"",\n        default=6,\n        type=int,\n        action=check_size(min_size=0),\n        help=""fasttext training hyperparameter. Max length of char ngrams to be used for training ""\n        ""word representations. ""\n        ""Set `max_n` to be lesser than `min_n` to avoid char ngrams being used."",\n    )\n    arg_parser.add_argument(\n        ""--word_ngrams"",\n        default=1,\n        type=int,\n        choices=[0, 1],\n        help=""fasttext training hyperparameter. If 1, uses enrich word vectors with subword (""\n        ""ngrams) information. If 0, this is equivalent to word2vec training."",\n    )\n\n    args = arg_parser.parse_args()\n\n    if args.corpus_format != ""conll2000"":\n        validate_existing_filepath(args.corpus)\n\n    np2vec = NP2vec(\n        args.corpus,\n        args.corpus_format,\n        args.mark_char,\n        args.word_embedding_type,\n        args.sg,\n        args.size,\n        args.window,\n        args.alpha,\n        args.min_alpha,\n        args.min_count,\n        args.sample,\n        args.workers,\n        args.hs,\n        args.negative,\n        args.cbow_mean,\n        args.iter,\n        args.min_n,\n        args.max_n,\n        args.word_ngrams,\n    )\n\n    np2vec.save(args.np2vec_model_file, args.binary)\n'"
examples/np_semantic_segmentation/__init__.py,0,b''
examples/np_semantic_segmentation/data.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\n# pylint: disable=global-statement,redefined-outer-name\n\nimport argparse\nimport csv\nimport math\nimport os\nfrom multiprocessing import Pool\n\nimport feature_extraction as fe\nimport numpy\nfrom tqdm import tqdm\n\nfrom nlp_architect.utils.io import (\n    validate_existing_filepath,\n    validate_parent_exists,\n    validate_proxy_path,\n)\n\nwordnet = None\nwikidata = None\nword2vec = None\n\n\ndef build_feature_vector(np):\n    """"""\n    Build a feature vector for the given noun-phrase. the size of the\n    vector = (4 + (#WORDS X 300)) = 1506. ==> (the number of words in a noun-phrase X size\n    of the word2vec(300)) + 4 additional features from external sources\n\n    Args:\n        np (str): a noun-phrase\n\n    Returns:\n        :obj:`np.ndarray`: the feature vector of the np\n    """"""\n    feature_vector = []\n    # 1. find if np exist as an entity in WordNet\n    wordnet_feature = find_wordnet_entity(np)\n    feature_vector.append(wordnet_feature)\n    # 2. find if np exist as an entity in Wikidata\n    wikidata_feature = find_wikidata_entity(np)\n    feature_vector.append(wikidata_feature)\n    # 3. word2vec score: from\n    word2vec_distance = word2vec.get_similarity_score(np)\n    feature_vector.append(word2vec_distance)\n    for w in np.split("" ""):\n        feature_vector.extend(word2vec.get_word_embedding(w))\n    return numpy.array(feature_vector)\n\n\ndef find_wordnet_entity(np):\n    """"""\n    extract WordNet indicator-feature (1 if exist in WordNet, else 0)\n\n    Args:\n        np (str): a noun-phrase\n\n    Returns:\n        int: 1 if exist in WordNet, else 0\n    """"""\n    candidates = expand_np_candidates(np, True)\n    return wordnet.find_wordnet_existence(candidates)\n\n\ndef find_wikidata_entity(np):\n    """"""\n    extract Wikidata indicator-feature (1 if exist in Wikidata, else 0)\n\n    Args:\n        np (str): a noun-phrase\n\n    Returns:\n        int: 1 if exist in Wikidata, else 0\n    """"""\n    candidates = expand_np_candidates(np, True)\n    return wikidata.find_wikidata_existence(candidates)\n\n\ndef expand_np_candidates(np, stemming):\n    """"""\n    Create all case-combination of the noun-phrase (nyc to NYC, israel to Israel etc.)\n\n    Args:\n        np (str): a noun-phrase\n        stemming (bool): True if to add case-combinations of noun-phrases\'s stem\n\n    Returns:\n        list(str): All case-combination of the noun-phrase\n    """"""\n    candidates = []\n    # create all case-combinations of np-> nyc to NYC, israel to Israel etc.\n    candidates.extend(get_all_case_combinations(np))\n    if stemming:\n        # create all case-combinations of np\'s stem-> t-shirts to t-shirt etc.\n        # pylint: disable=no-member\n        candidates.extend(get_all_case_combinations(fe.stem(np)))\n    return candidates\n\n\ndef get_all_case_combinations(np):\n    """"""\n    Returns all case combinations for the noun-phrase (regular, upper, lower, title)\n    Args:\n        np (str): a noun-phrase\n    Returns:\n        list(str): List of all case combinations\n    """"""\n    candidates = [np, np.upper(), np.lower(), np.title()]\n    return candidates\n\n\ndef write_to_csv(output, np_feature_vectors, np_dic, np_list):\n    """"""\n    Write data to csv file\n\n    Args:\n        output (str): output file path\n        np_feature_vectors (:obj:`np.ndarray`): numpy vectors\n        np_dic (dict): dict, keys: the noun phrase, value: the features\n        np_list (list): features list\n    """"""\n    with open(output, ""w"", encoding=""utf-8"") as out_file:\n        writer = csv.writer(out_file, delimiter="","", quotechar=\'""\')\n        print(""prepared data CSV file is saved in {0}"".format(output))\n        for i, _ in enumerate(np_feature_vectors):\n            np_vector = np_feature_vectors[i]\n            np_vector = numpy.append(np_vector, np_dic[np_list[i]])\n            writer.writerow(np_vector)\n\n\ndef prepare_data(data_file, output_file, word2vec_file, http_proxy=None, https_proxy=None):\n    """"""\n    Extract for each noun-phrase a feature vector (W2V, WordNet, Wikidata, NPMI, UCI).\n    Write the feature vectors to --output specifies local path\n\n    Args:\n        data_file(str): file_path to input data\n        output_file(str): file_path to output processed data\n        word2vec_file(str): file_path to word2vec model\n        http_proxy(str): http_proxy\n        https_proxy(str): https_proxy\n    """"""\n    # init_resources:\n    global wordnet, wikidata, word2vec\n    # pylint: disable=no-member\n    wordnet = fe.Wordnet()\n    # pylint: disable=no-member\n    wikidata = fe.Wikidata(http_proxy, https_proxy)\n    print(""Start loading Word2Vec model (this might take a while...)"")\n    # pylint: disable=no-member\n    word2vec = fe.Word2Vec(word2vec_file)\n    print(""Finish loading feature extraction services"")\n    reader_list = read_csv_file_data(data_file)\n    np_dic = {}\n    np_list = []\n    for row in reader_list:\n        np_dic[row[0]] = row[1]\n        np_list.append(row[0])\n    p = Pool(10)\n    np_feature_vectors = list(\n        tqdm(p.imap(build_feature_vector, np_list), total=len(np_list))\n    )  # , desc=""np feature extraction status""))\n    write_to_csv(output_file, np_feature_vectors, np_dic, np_list)\n\n\ndef read_csv_file_data(input_path):\n    """"""\n    Read csv file to a list\n\n    Args:\n        input_path (str): read csv file from this local file path\n\n    Returns:\n        list(str): A list where each item is a row in the csv file\n    """"""\n    # 1. read csv file\n    if not os.path.isabs(input_path):\n        # handle case using default value\\relative paths\n        input_path = os.path.join(os.path.dirname(__file__), input_path)\n    with open(input_path, ""r"", encoding=""utf-8-sig"") as input_file:\n        reader = csv.reader((line.replace(""\\0"", """") for line in input_file))\n        reader_list = list(reader)\n    return reader_list\n\n\ndef extract_y_labels(input_path):\n    """"""\n    Extract only the Labels of the data\n\n    Args:\n        input_path (str): read csv file from this local file path\n\n    Returns:\n        :obj:`np.ndarray`: A numpy array of the labels, each item is the label of the row\n    """"""\n    reader_list = read_csv_file_data(input_path)\n    Y_labels_vec = []\n    cntr = 0\n    for line in reader_list:\n        Y_label = line[-1]\n        Y_labels_vec.insert(cntr, Y_label)\n        cntr += 1\n    y_train = Y_labels_vec\n    y_train = numpy.array(y_train, dtype=numpy.int32)\n    return y_train\n\n\nclass NpSemanticSegData:\n    """"""\n    Dataset for NP Semantic Segmentation Model\n\n        Args:\n            file_path (str): read data from this local file path\n\n            train_to_test_ratio (:obj:`float`): the train-to-test ration of the dataset\n\n            feature_vec_dim (:obj:`int`): the size of the feature vector for each noun-phrase\n    """"""\n\n    def __init__(self, file_path, train_to_test_ratio=0.8, feature_vec_dim=603):\n        self.file_path = file_path\n        self.feature_vec_dim = feature_vec_dim\n        self.train_to_test_ratio = train_to_test_ratio\n        self.is_y_labels = None\n        self.y_labels = None\n        self.data_set = self.load_data_to_array_iterator()\n\n    def load_data_from_file(self):\n        """"""\n        Loads data from file_path to X_train, y_train, X_test, y_test numpy arrays\n\n        Returns:\n            tuple(:obj:`np.ndarray`): X_train, y_train, X_test, y_test numpy arrays\n        """"""\n        reader_list = read_csv_file_data(self.file_path)\n        # count num of feature vectors\n        num_feats = len(reader_list)\n        # is_y_labels is for inference - if the inference data is labeled y_labels are extracted\n        self.is_y_labels = len(reader_list[0]) == self.feature_vec_dim + 1\n        X_feature_matrix = numpy.zeros((num_feats, self.feature_vec_dim))\n        Y_labels_vec = []\n        cntr = 0\n        for line in reader_list:\n            X_features = numpy.array(line[: self.feature_vec_dim])\n            X_feature_matrix[cntr, :] = X_features\n\n            if self.is_y_labels:\n                Y_label = line[self.feature_vec_dim]\n                Y_labels_vec.insert(cntr, Y_label)\n            cntr += 1\n\n        len_train = int(math.floor(num_feats * self.train_to_test_ratio))\n\n        X_train = X_feature_matrix[0 : len_train - 1]\n        y_train = None\n        if self.is_y_labels:\n            y_train = Y_labels_vec[0 : len_train - 1]\n            y_train = numpy.array(y_train, dtype=numpy.int32)\n\n        X_test = X_feature_matrix[len_train:]\n        y_test = None\n        if self.is_y_labels:\n            y_test = Y_labels_vec[len_train:]\n            y_test = numpy.array(y_test, dtype=numpy.int32)\n        return X_train, y_train, X_test, y_test\n\n    def load_data_to_array_iterator(self):\n        """"""\n        Load data into dict of \'train\' and \'test\' datasets\n\n        Returns:\n            dict: dict with train set & test_set (each is dict with X and y)\n        """"""\n        X_train, y_train, X_test, y_test = self.load_data_from_file()\n        data_set = {""train"": {""X"": X_train, ""y"": y_train}, ""test"": {""X"": X_test, ""y"": y_test}}\n        return data_set\n\n    @property\n    def train_set(self):\n        """"""dict(:obj:`numpy.ndarray`): train set (X & y)""""""\n        return self.data_set[""train""]\n\n    @property\n    def train_set_x(self):\n        """"""dict(:obj:`numpy.ndarray`): train set (X)""""""\n        return self.data_set[""train""][""X""]\n\n    @property\n    def train_set_y(self):\n        """"""dict(:obj:`numpy.ndarray`): train set (y)""""""\n        return self.data_set[""train""][""y""]\n\n    @property\n    def test_set(self):\n        """"""dict(:obj:`numpy.ndarray`): test set (X & y)""""""\n        return self.data_set[""test""]\n\n    @property\n    def test_set_x(self):\n        """"""dict(:obj:`numpy.ndarray`): test set (X)""""""\n        return self.data_set[""test""][""X""]\n\n    @property\n    def test_set_y(self):\n        """"""dict(:obj:`numpy.ndarray`): test set (y)""""""\n        return self.data_set[""test""][""y""]\n\n\ndef absolute_path(input_path):\n    """"""\n    Return input_path\'s absolute path\n\n    Args:\n        input_path(str): input_path\n\n    Returns:\n        str: absolute path\n    """"""\n    if isinstance(input_path, str):\n        if not os.path.isabs(input_path):\n            # handle case using default value\\relative paths\n            input_path = os.path.join(os.path.dirname(__file__), input_path)\n    return input_path\n\n\nif __name__ == ""__main__"":\n    # parse the command line arguments\n    parser = argparse.ArgumentParser(description=""Prepare data"")\n    parser.add_argument(\n        ""--data"",\n        type=validate_existing_filepath,\n        help=""path the CSV file where the raw dataset is saved"",\n    )\n    parser.add_argument(\n        ""--output"",\n        type=validate_parent_exists,\n        help=""path the CSV file where the prepared dataset will be saved"",\n    )\n    parser.add_argument(\n        ""--w2v_path"", type=validate_existing_filepath, help=""path to the word embedding\'s model""\n    )\n    parser.add_argument(\n        ""--http_proxy"", help=""system\'s http proxy"", type=validate_proxy_path, default=None\n    )\n    parser.add_argument(\n        ""--https_proxy"", help=""system\'s https proxy"", type=validate_proxy_path, default=None\n    )\n    args = parser.parse_args()\n    data_path = absolute_path(args.data)\n    word2vec_path = args.w2v_path\n    output_path = absolute_path(args.output)\n    http_proxy = args.http_proxy\n    https_proxy = args.https_proxy\n    prepare_data(data_path, output_path, word2vec_path, http_proxy, https_proxy)\n'"
examples/np_semantic_segmentation/feature_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport nltk\nimport nltk.corpus\nimport numpy as np\nimport requests\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom nlp_architect.utils.generic import license_prompt\n\nstemmer = SnowballStemmer(""english"")\nheaders = {""Accept"": ""application/json""}\n\n\ndef stem(w):\n    """"""\n    Stem input\n\n    Args:\n        w (str): word to extract stem\n\n    Returns:\n        str: stem of w\n    """"""\n    return stemmer.stem(w)\n\n\nclass Wikidata:\n    """"""\n    Wikidata service\n\n    Args:\n        http_proxy(str) : http proxy\n        https_proxy(str) : https proxy\n    """"""\n\n    def __init__(self, http_proxy=None, https_proxy=None):\n        self.headers = headers\n        proxies = {}\n        if http_proxy:\n            proxies[""http""] = http_proxy\n        if https_proxy:\n            proxies[""https""] = https_proxy\n        self.proxies = proxies\n\n    def find_wikidata_existence(self, candidates):\n        """"""\n        extract Wikidata indicator-feature (1 if exist in Wikidata, else 0)\n\n        Args:\n            candidates(list(str)): a list of all possible candidates to have Wikidata entry\n\n        Returns :\n            int: 1 if exist in Wikidata for any candidate in candidates, else 0\n        """"""\n        for candidate in set(candidates):\n            if self.has_item(candidate):\n                return 1\n        return 0\n\n    def has_item(self, phrase):\n        """"""\n        Send a SPARQL query to wikidata, and return response\n\n        Args:\n            phrase (str):  a noun-phrase\n\n        Returns:\n            bool: True if exist in Wikidata for phrase, else False\n        """"""\n        chr_url = (\n            """"""https://query.wikidata.org/sparql?query=\n                        SELECT ?item ?lable\n                        WHERE\n                        {\n                            ?item ?label \'""""""\n            + phrase\n            + """"""\'@en .\n            SERVICE wikibase:label { bd:serviceParam wikibase:language ""en"". }\n                        }\n                        &format = JSON""""""\n        )\n        r = requests.get(chr_url, headers=self.headers, proxies=self.proxies)\n        empty_result = (\n            b\'{\\n  ""head"" : {\\n    ""vars"" : [ ""item"",\'\n            b\' ""lable"" ]\\n  },\\n  \'\n            b\'""results"" : {\\n    ""bindings"" : [ ]\\n  }\\n}\'\n        )\n        if r.status_code == 200 and empty_result != r.content:\n            return True\n        return False\n\n\nclass Word2Vec:\n    """"""\n    Word2Vec service\n\n        Args:\n            word2vec_model_path (str): the local path to the Word2Vec pre-trained model\n    """"""\n\n    def __init__(self, word2vec_model_path):\n        self.word2vec_model_path = word2vec_model_path\n        self.model = self.load_word2vec_model_from_path()\n\n    def load_word2vec_model_from_path(self):\n        """"""\n        Load Word2Vec model\n\n        Returns:\n            the Word2Vec model\n        """"""\n        word_embeddings_model = KeyedVectors.load_word2vec_format(\n            self.word2vec_model_path, binary=True\n        )\n        if not word_embeddings_model:\n            return None\n        return word_embeddings_model\n\n    def get_word_embedding(self, word):\n        """"""\n        Get the pre-trained word embeddings\n\n        Args:\n            word (str): the word to extract from the models embedding\n\n        Returns:\n            :obj:`np.ndarray`: the word embeddings\n        """"""\n        if not self.model:\n            return np.full((300,), -1)\n        if word in self.model.vocab:\n            vec = np.array(self.model.word_vec(word))\n        else:\n            vec = np.full((300,), -1)\n        return vec\n\n    def get_similarity_score(self, noun_phrase):\n        """"""\n            Get the cosign similarity distance between the np words (only if 2)\n\n        Args:\n            noun_phrase (str): the noun-phrase\n\n        Returns:\n            float: cosign similarity distance between the np words (only if 2)\n        """"""\n        candidates = noun_phrase.split("" "")\n        if len(candidates) < 2:\n            candidates.extend(candidates[0])\n        if len(candidates) > 2:\n            return -1\n        try:\n            if not self.model:\n                return -1\n            return self.model.similarity(candidates[0], candidates[1])\n        except KeyError:\n            return -1\n\n\nclass Wordnet:\n    """"""\n    WordNet service\n    """"""\n\n    def __init__(self):\n        try:\n            nltk.data.find(""corpora/wordnet"")\n        except LookupError:\n            if license_prompt(""WordNet data set"", ""http://www.nltk.org/nltk_data/"") is False:\n                raise Exception(\n                    ""can\'t continue data prepare process "" ""without downloading WordNet dataset""\n                )\n            nltk.download(""wordnet"")\n        self.wordnet = wn\n\n    def find_wordnet_existence(self, candidates):\n        """"""\n        extract WordNet indicator-feature (1 if exist in WordNet, else 0)\n\n        Args:\n            candidates (list(str)): a list of all possible candidates to have WordNet entry\n\n        Returns:\n            int: 1 if exist in WordNet for any candidate in candidates, else 0\n        """"""\n        for candidate in candidates:\n            candidate = candidate.replace("" "", ""_"")\n            if self.wordnet.synsets(candidate):\n                return 1\n        return 0\n'"
examples/np_semantic_segmentation/inference.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport argparse\nimport csv\nimport os\n\nfrom .data import NpSemanticSegData, absolute_path\n\nfrom nlp_architect.models.np_semantic_segmentation import NpSemanticSegClassifier\nfrom nlp_architect.utils.io import validate_existing_filepath, validate_parent_exists\n\n\ndef classify_collocation(test_set, model_file_path, num_epochs, callback_args=None):\n    """"""\n    Classify the dataset by the given trained model\n\n    Args:\n        model_file_path (str): model path\n        num_epochs (int): number of epochs\n        callback_args (dict): callback_arg\n        test_set (:obj:`core_models.np_semantic_segmentation.data.NpSemanticSegData`):\n            NpSemanticSegData object containing the dataset\n\n    Returns:\n        the output of the final layer for the entire Dataset\n    """"""\n    # load existing model\n    if not os.path.isabs(model_file_path):\n        # handle case using default value\\relative paths\n        model_file_path = os.path.join(os.path.dirname(__file__), model_file_path)\n    loaded_model = NpSemanticSegClassifier(num_epochs, callback_args)\n    loaded_model.load(model_file_path)\n    print(""Model loaded"")\n    # arrange the data\n    return loaded_model.get_outputs(test_set[""X""])\n\n\ndef print_evaluation(y_test, predictions):\n    """"""\n    Print evaluation of the model\'s predictions comparing to the given y labels (if given)\n\n    Args:\n        y_test (list(str)): list of the labels given in the data\n        predictions(obj:`numpy.ndarray`): the model\'s predictions\n    """"""\n    tp = 0.0\n    fp = 0.0\n    tn = 0.0\n    fn = 0.0\n    for y_true, prediction in zip(y_test, [round(p[0]) for p in predictions.tolist()]):\n        if prediction == 1:\n            if y_true == 1:\n                tp = tp + 1\n            else:\n                fp = fp + 1\n        elif y_true == 0:\n            tn = tn + 1\n        else:\n            fn = fn + 1\n    if tp + fn == 0:\n        fn = 1\n    if tp == 0:\n        tp = 1\n    acc = 100 * ((tp + tn) / len(predictions))\n    prec = 100 * (tp / (tp + fp))\n    rec = 100 * (tp / (tp + fn))\n    print(\n        ""Model statistics:\\naccuracy: {0:.2f}\\nprecision: {1:.2f}""\n        ""\\nrecall: {2:.2f}\\n"".format(acc, prec, rec)\n    )\n\n\ndef write_results(predictions, output):\n    """"""\n    Write csv file of predication results to specified --output\n\n    Args:\n        output (str): output file path\n        predictions:\n            the model\'s predictions\n    """"""\n    results_list = [round(p[0]) for p in predictions.tolist()]\n    with open(output, ""w"", encoding=""utf-8"") as out_file:\n        writer = csv.writer(out_file, delimiter="","", quotechar=\'""\')\n        for result in results_list:\n            writer.writerow([result])\n    print(""Results of inference saved in {0}"".format(output))\n\n\nif __name__ == ""__main__"":\n    # parse the command line arguments\n    parser = argparse.ArgumentParser()\n    parser.set_defaults(epochs=200)\n    parser.add_argument(\n        ""--data"", help=""prepared data CSV file path"", type=validate_existing_filepath\n    )\n    parser.add_argument(\n        ""--model"", help=""path to the trained model file"", type=validate_existing_filepath\n    )\n    parser.add_argument(\n        ""--print_stats"",\n        action=""store_true"",\n        default=False,\n        help=""print evaluation stats for the model predictions - if "" ""your data has tagging"",\n    )\n    parser.add_argument(\n        ""--output"", help=""path to location for inference output file"", type=validate_parent_exists\n    )\n    args = parser.parse_args()\n    data_path = absolute_path(args.data)\n    model_path = absolute_path(args.model)\n    print_stats = args.print_stats\n    output_path = absolute_path(args.output)\n    data_set = NpSemanticSegData(data_path)\n    results = classify_collocation(data_set.test_set, model_path, args.epochs)\n    if print_stats and (data_set.is_y_labels is not None):\n        y_labels = data_set.test_set_y\n        print_evaluation(y_labels, results)\n    write_results(results, output_path)\n'"
examples/np_semantic_segmentation/preprocess_tratz2011.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import unicode_literals, print_function, division, absolute_import\n\nimport argparse\nimport csv\nimport os\n\nfrom .data import absolute_path\n\nfrom nlp_architect.utils.io import validate_existing_directory\n\ntratz2011_train_labeled_dict = {\n    False: [\n        55,\n        64,\n        67,\n        68,\n        100,\n        104,\n        121,\n        150,\n        444,\n        492,\n        782,\n        798,\n        878,\n        942,\n        952,\n        967,\n        990,\n        1012,\n        1031,\n        1036,\n        1371,\n        1658,\n        1679,\n        1717,\n        1719,\n        2845,\n        2904,\n        3454,\n        3921,\n        4040,\n        4059,\n        4123,\n        4334,\n        4435,\n        4512,\n        4834,\n        4890,\n        4902,\n        4919,\n        4923,\n        4925,\n        4926,\n        4932,\n        4988,\n        5004,\n        5078,\n        5081,\n        5083,\n        5094,\n        5197,\n        5208,\n        5246,\n        5315,\n        5342,\n        5409,\n        5440,\n        5446,\n        5656,\n        5678,\n        5684,\n        5688,\n        5713,\n        5719,\n        5723,\n        5764,\n        5776,\n        5779,\n        5848,\n        6038,\n        6049,\n        6115,\n        6116,\n        6136,\n        6140,\n        6209,\n        6236,\n        6297,\n        6331,\n        6334,\n        6471,\n        6478,\n        6490,\n        6731,\n        6734,\n        6736,\n        6737,\n        6740,\n        6798,\n        6811,\n        6836,\n        6837,\n        6841,\n        6847,\n        6848,\n        6853,\n        6899,\n        6900,\n        6940,\n        6947,\n        7003,\n        7011,\n        7101,\n        7107,\n        7125,\n        7189,\n        7330,\n        7393,\n        7400,\n        7500,\n        7504,\n        7512,\n        7591,\n        7594,\n        7615,\n        7742,\n        7765,\n        7842,\n        7935,\n        8546,\n        8613,\n        8621,\n        8642,\n        8658,\n        8665,\n        8741,\n        8792,\n        8823,\n        8863,\n        8876,\n        8878,\n        9169,\n        9210,\n        9277,\n        9280,\n        9341,\n        9403,\n        9435,\n        9483,\n        9517,\n        9600,\n        9697,\n        9749,\n        9807,\n        9818,\n        9842,\n        9906,\n        10098,\n        10161,\n        10194,\n        10273,\n        10315,\n        10350,\n        10351,\n        10396,\n        10441,\n        10463,\n        10468,\n        10490,\n        10492,\n        10497,\n        10509,\n        10510,\n        10537,\n        10581,\n        10589,\n        10616,\n        10737,\n        10751,\n        10922,\n        10944,\n        10960,\n        11007,\n        11013,\n        11021,\n        11181,\n        11188,\n        11255,\n        11297,\n        11322,\n        11444,\n        11464,\n        11466,\n        11469,\n        11474,\n        11499,\n        11608,\n        11629,\n        11636,\n        11679,\n        11692,\n        11747,\n        11792,\n        11865,\n        11894,\n        11898,\n        11908,\n        12032,\n        12045,\n        12046,\n        12067,\n        12109,\n        12173,\n        12207,\n        12222,\n        12385,\n        12386,\n        12398,\n        12408,\n        12472,\n        12556,\n        12669,\n        12677,\n        12679,\n        12755,\n        12788,\n        12809,\n        12818,\n        12822,\n        12844,\n        13031,\n        13041,\n        13120,\n        13122,\n        13127,\n        13147,\n        13160,\n        13161,\n        13186,\n        13188,\n        13189,\n        13194,\n        13214,\n        13236,\n        13318,\n        13368,\n        13421,\n        13453,\n        13456,\n        13505,\n        13535,\n        13583,\n        13584,\n        13611,\n        13653,\n        13691,\n        13692,\n        13701,\n        13737,\n        13797,\n        14007,\n        14029,\n        14047,\n        14066,\n        14069,\n        14092,\n    ],\n    True: [\n        60,\n        207,\n        247,\n        258,\n        268,\n        362,\n        456,\n        485,\n        641,\n        694,\n        1025,\n        1272,\n        1304,\n        1317,\n        1542,\n        1602,\n        1643,\n        1746,\n        1908,\n        1909,\n        2028,\n        2302,\n        3424,\n        3627,\n        4113,\n        4398,\n        4399,\n        4542,\n        4759,\n        4836,\n        4849,\n        4957,\n        5041,\n        5126,\n        5306,\n        5630,\n        5661,\n        5708,\n        5791,\n        5971,\n        5983,\n        6142,\n        6548,\n        7293,\n        7416,\n        7923,\n        8932,\n        9700,\n        10486,\n        10746,\n        10803,\n        11448,\n        11781,\n        12072,\n        12308,\n        12354,\n        12368,\n        12470,\n        12510,\n        12647,\n        12662,\n        12766,\n        12821,\n        12879,\n        13494,\n        14014,\n        14018,\n        14020,\n        14091,\n    ],\n}\n\ntratz2011_val_labeled_dict = {\n    False: [12, 16, 17, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 32, 40, 42, 43, 45, 47, 48],\n    True: [33, 121, 240, 365, 425],\n}\n\n\ndef rebuild_row(lst, is_collocation):\n    """"""\n    Re-construct csv row as expected by data.py `read_csv_file_data` method\n\n    Args:\n        lst (list(str)): list with a string containing \'\\t\'\n        is_collocation (bool): if collocation True, else False\n\n    Returns:\n        list(str): list of string where the first entry is the noun phrase and the second is 0\\1\n    """"""\n    split_list = lst[0].split(""\\t"")\n    if is_collocation:\n        return [split_list[0] + "" "" + split_list[1], ""1""]\n    return [split_list[0] + "" "" + split_list[1], ""0""]\n\n\ndef read_from_tratz_2011(file_full_path, labeled_dict):\n    """"""\n    Read tratz_2011 files and print re-formatted csv files\n\n    Args:\n        file_full_path (str): file path\n        labeled_dict (dict): dictionary with prepared labels\n    """"""\n    # 1. read the data\n    with open(file_full_path, ""r"", encoding=""utf-8-sig"") as input_file:\n        reader = csv.reader((line.replace(""\\0"", """") for line in input_file))\n        reader_list = list(reader)\n        csv_data = []\n        for index, row in enumerate(reader_list):\n            if index in labeled_dict[False]:\n                csv_data.append(rebuild_row(row, False))\n            if index in labeled_dict[True]:\n                csv_data.append(rebuild_row(row, True))\n        # 2. write to csv file\n        write_csv(csv_data, file_full_path)\n\n\ndef write_csv(data, output):\n    """"""\n    Write csv data\n\n    Args:\n        output (str): output file path\n        data (list(str)):\n            the csv formatted data\n    """"""\n    output_path = output[:-3] + ""csv""\n    with open(output_path, ""w"", encoding=""utf-8"") as out_file:\n        writer = csv.writer(out_file, delimiter="","", quotechar=\'""\')\n        print(""CSV file is saved in {0}"".format(output_path))\n        for result_row in data:\n            writer.writerow(result_row)\n\n\ndef preprocess_tratz_2011(folder_path):\n    """"""\n    Pre-process tratz_2011 dataset\n\n    Args:\n        folder_path (str): path to the unzipped tratz_2011 dataset\n    """"""\n    files = [\n        ""tratz2011_coarse_grained_random/train.tsv"",\n        ""tratz2011_coarse_grained_random/"" ""val.tsv"",\n    ]\n    dicts = [tratz2011_train_labeled_dict, tratz2011_val_labeled_dict]\n    # 1. get abs path\n    if not os.path.isabs(folder_path):\n        # handle case using default value\\relative paths\n        folder_path = os.path.join(os.path.dirname(__file__), folder_path)\n    # 2. add the location of the train file in the folder\n    for file, dic in zip(files, dicts):\n        file_full_path = os.path.join(folder_path, file)\n        read_from_tratz_2011(file_full_path, dic)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Pre-process Tratz 2011 data from tsv to csv"")\n    parser.add_argument(\n        ""--data"",\n        type=validate_existing_directory,\n        help=""path the Tratz_2011_dataset folder local path"",\n    )\n    args = parser.parse_args()\n    data_path = absolute_path(args.data)\n    preprocess_tratz_2011(data_path)\n'"
examples/np_semantic_segmentation/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport argparse\n\nfrom .data import NpSemanticSegData, absolute_path\n\nfrom nlp_architect.models.np_semantic_segmentation import NpSemanticSegClassifier\nfrom nlp_architect.utils.io import validate_existing_filepath, validate_parent_exists, validate\n\n\ndef train_mlp_classifier(dataset, model_file_path, epochs, callback_args=None):\n    """"""\n    Train the np_semantic_segmentation mlp classifier\n    Args:\n        model_file_path (str): model path\n        epochs (int): number of epochs\n        callback_args (dict): callback_arg\n        dataset: NpSemanticSegData object containing the dataset\n\n    Returns:\n        print error_rate, test_accuracy_rate and precision_recall_rate evaluation from the model\n\n    """"""\n    model = NpSemanticSegClassifier(epochs, callback_args)\n    input_dim = dataset.train_set_x.shape[1]\n    model.build(input_dim)\n    # run fit\n    model.fit(dataset.train_set)\n    # save model params\n    model.save(model_file_path)\n    # set evaluation error rates\n    loss, binary_accuracy, precision, recall, f1 = model.eval(dataset.test_set)\n    print(""loss = %.1f%%"" % (loss))\n    print(""Test binary_accuracy rate = %.1f%%"" % (binary_accuracy * 100))\n    print(""Test precision rate = %.1f%%"" % (precision * 100))\n    print(""Test recall rate = %.1f%%"" % (recall * 100))\n    print(""Test f1 rate = %.1f%%"" % (f1 * 100))\n\n\nif __name__ == ""__main__"":\n    # parse the command line arguments\n    parser = argparse.ArgumentParser()\n    parser.set_defaults(epochs=200)\n    parser.add_argument(\n        ""--data"",\n        type=validate_existing_filepath,\n        help=""Path to the CSV file where the prepared dataset is saved"",\n    )\n    parser.add_argument(""--model_path"", type=validate_parent_exists, help=""Path to save the model"")\n    args = parser.parse_args()\n    validate((args.epochs, int, 1, 100000))\n    data_path = absolute_path(args.data)\n    model_path = absolute_path(args.model_path)\n    num_epochs = args.epochs\n    # load data sets from file\n    data_set = NpSemanticSegData(data_path, train_to_test_ratio=0.8)\n    # train the mlp classifier\n    train_mlp_classifier(data_set, model_path, args.epochs)\n'"
examples/reading_comprehension/__init__.py,0,b''
examples/sparse_gnmt/__init__.py,0,b''
examples/sparse_gnmt/gnmt_model.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: skip-file\n""""""GNMT attention sequence-to-sequence model with dynamic RNN support.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom .gnmt import attention_model\nfrom .gnmt import model_helper\nfrom .gnmt.utils import misc_utils as utils\n\n__all__ = [""GNMTModel""]\n\n\nclass GNMTModel(attention_model.AttentionModel):\n    """"""Sequence-to-sequence dynamic model with GNMT attention architecture\n        with sparsity policy support.\n    """"""\n\n    def __init__(\n        self,\n        hparams,\n        mode,\n        iterator,\n        source_vocab_table,\n        target_vocab_table,\n        reverse_target_vocab_table=None,\n        scope=None,\n        extra_args=None,\n    ):\n        self.is_gnmt_attention = hparams.attention_architecture in [""gnmt"", ""gnmt_v2""]\n\n        super(GNMTModel, self).__init__(\n            hparams=hparams,\n            mode=mode,\n            iterator=iterator,\n            source_vocab_table=source_vocab_table,\n            target_vocab_table=target_vocab_table,\n            reverse_target_vocab_table=reverse_target_vocab_table,\n            scope=scope,\n            extra_args=extra_args,\n        )\n\n    def _build_encoder(self, hparams):\n        """"""Build a GNMT encoder.""""""\n        if hparams.encoder_type == ""uni"" or hparams.encoder_type == ""bi"":\n            return super(GNMTModel, self)._build_encoder(hparams)\n\n        if hparams.encoder_type != ""gnmt"":\n            raise ValueError(""Unknown encoder_type %s"" % hparams.encoder_type)\n\n        # Build GNMT encoder.\n        num_bi_layers = 1\n        num_uni_layers = self.num_encoder_layers - num_bi_layers\n        utils.print_out(""# Build a GNMT encoder"")\n        utils.print_out(""  num_bi_layers = %d"" % num_bi_layers)\n        utils.print_out(""  num_uni_layers = %d"" % num_uni_layers)\n\n        iterator = self.iterator\n        source = iterator.source\n        if self.time_major:\n            source = tf.transpose(source)\n\n        with tf.variable_scope(""encoder"") as scope:\n            dtype = scope.dtype\n\n            self.encoder_emb_inp = self.encoder_emb_lookup_fn(self.embedding_encoder, source)\n\n            # Execute _build_bidirectional_rnn from Model class\n            bi_encoder_outputs, bi_encoder_state = self._build_bidirectional_rnn(\n                inputs=self.encoder_emb_inp,\n                sequence_length=iterator.source_sequence_length,\n                dtype=dtype,\n                hparams=hparams,\n                num_bi_layers=num_bi_layers,\n                num_bi_residual_layers=0,  # no residual connection\n            )\n\n            # Build unidirectional layers\n            if self.extract_encoder_layers:\n                encoder_state, encoder_outputs = self._build_individual_encoder_layers(\n                    bi_encoder_outputs, num_uni_layers, dtype, hparams\n                )\n            else:\n                encoder_state, encoder_outputs = self._build_all_encoder_layers(\n                    bi_encoder_outputs, num_uni_layers, dtype, hparams\n                )\n\n            # Pass all encoder states to the decoder\n            #   except the first bi-directional layer\n            encoder_state = (bi_encoder_state[1],) + (\n                (encoder_state,) if num_uni_layers == 1 else encoder_state\n            )\n\n        return encoder_outputs, encoder_state\n\n    def _build_all_encoder_layers(self, bi_encoder_outputs, num_uni_layers, dtype, hparams):\n        """"""Build encoder layers all at once.""""""\n        uni_cell = model_helper.create_rnn_cell(\n            unit_type=hparams.unit_type,\n            num_units=hparams.num_units,\n            num_layers=num_uni_layers,\n            num_residual_layers=self.num_encoder_residual_layers,\n            forget_bias=hparams.forget_bias,\n            dropout=hparams.dropout,\n            num_gpus=self.num_gpus,\n            base_gpu=1,\n            mode=self.mode,\n            single_cell_fn=self.single_cell_fn,\n        )\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n            uni_cell,\n            bi_encoder_outputs,\n            dtype=dtype,\n            sequence_length=self.iterator.source_sequence_length,\n            time_major=self.time_major,\n        )\n\n        # Use the top layer for now\n        self.encoder_state_list = [encoder_outputs]\n\n        return encoder_state, encoder_outputs\n\n    def _build_individual_encoder_layers(self, bi_encoder_outputs, num_uni_layers, dtype, hparams):\n        """"""Run each of the encoder layer separately, not used in general seq2seq.""""""\n        uni_cell_lists = model_helper._cell_list(\n            unit_type=hparams.unit_type,\n            num_units=hparams.num_units,\n            num_layers=num_uni_layers,\n            num_residual_layers=self.num_encoder_residual_layers,\n            forget_bias=hparams.forget_bias,\n            dropout=hparams.dropout,\n            num_gpus=self.num_gpus,\n            base_gpu=1,\n            mode=self.mode,\n            single_cell_fn=self.single_cell_fn,\n        )\n\n        encoder_inp = bi_encoder_outputs\n        encoder_states = []\n        self.encoder_state_list = [\n            bi_encoder_outputs[:, :, : hparams.num_units],\n            bi_encoder_outputs[:, :, hparams.num_units :],\n        ]\n        with tf.variable_scope(""rnn/multi_rnn_cell""):\n            for i, cell in enumerate(uni_cell_lists):\n                with tf.variable_scope(""cell_%d"" % i) as scope:\n                    encoder_inp, encoder_state = tf.nn.dynamic_rnn(\n                        cell,\n                        encoder_inp,\n                        dtype=dtype,\n                        sequence_length=self.iterator.source_sequence_length,\n                        time_major=self.time_major,\n                        scope=scope,\n                    )\n                    encoder_states.append(encoder_state)\n                    self.encoder_state_list.append(encoder_inp)\n\n        encoder_state = tuple(encoder_states)\n        encoder_outputs = self.encoder_state_list[-1]\n        return encoder_state, encoder_outputs\n\n    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state, source_sequence_length):\n        """"""Build a RNN cell with GNMT attention architecture.""""""\n        # Standard attention\n        if not self.is_gnmt_attention:\n            return super(GNMTModel, self)._build_decoder_cell(\n                hparams, encoder_outputs, encoder_state, source_sequence_length\n            )\n\n        # GNMT attention\n        attention_option = hparams.attention\n        attention_architecture = hparams.attention_architecture\n        num_units = hparams.num_units\n        infer_mode = hparams.infer_mode\n\n        dtype = tf.float32\n\n        if self.time_major:\n            memory = tf.transpose(encoder_outputs, [1, 0, 2])\n        else:\n            memory = encoder_outputs\n\n        if self.mode == tf.contrib.learn.ModeKeys.INFER and infer_mode == ""beam_search"":\n            (\n                memory,\n                source_sequence_length,\n                encoder_state,\n                batch_size,\n            ) = self._prepare_beam_search_decoder_inputs(\n                hparams.beam_width, memory, source_sequence_length, encoder_state\n            )\n        else:\n            batch_size = self.batch_size\n\n        attention_mechanism = self.attention_mechanism_fn(\n            attention_option, num_units, memory, source_sequence_length, self.mode\n        )\n\n        cell_list = model_helper._cell_list(  # pylint: disable=protected-access\n            unit_type=hparams.unit_type,\n            num_units=num_units,\n            num_layers=self.num_decoder_layers,\n            num_residual_layers=self.num_decoder_residual_layers,\n            forget_bias=hparams.forget_bias,\n            dropout=hparams.dropout,\n            num_gpus=self.num_gpus,\n            mode=self.mode,\n            single_cell_fn=self.single_cell_fn,\n            residual_fn=gnmt_residual_fn,\n        )\n\n        # Only wrap the bottom layer with the attention mechanism.\n        attention_cell = cell_list.pop(0)\n\n        # Only generate alignment in greedy INFER mode.\n        alignment_history = (\n            self.mode == tf.contrib.learn.ModeKeys.INFER and infer_mode != ""beam_search""\n        )\n        attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n            attention_cell,\n            attention_mechanism,\n            attention_layer_size=None,  # don\'t use attention layer.\n            output_attention=False,\n            alignment_history=alignment_history,\n            name=""attention"",\n        )\n\n        if attention_architecture == ""gnmt"":\n            cell = GNMTAttentionMultiCell(attention_cell, cell_list)\n        elif attention_architecture == ""gnmt_v2"":\n            cell = GNMTAttentionMultiCell(attention_cell, cell_list, use_new_attention=True)\n        else:\n            raise ValueError(""Unknown attention_architecture %s"" % attention_architecture)\n\n        if hparams.pass_hidden_state:\n            decoder_initial_state = tuple(\n                zs.clone(cell_state=es)\n                if isinstance(zs, tf.contrib.seq2seq.AttentionWrapperState)\n                else es\n                for zs, es in zip(cell.zero_state(batch_size, dtype), encoder_state)\n            )\n        else:\n            decoder_initial_state = cell.zero_state(batch_size, dtype)\n\n        return cell, decoder_initial_state\n\n    def _get_infer_summary(self, hparams):\n        if hparams.infer_mode == ""beam_search"":\n            return tf.no_op()\n        elif self.is_gnmt_attention:\n            return attention_model._create_attention_images_summary(self.final_context_state[0])\n        else:\n            return super(GNMTModel, self)._get_infer_summary(hparams)\n\n\nclass GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\n    """"""A MultiCell with GNMT attention style.""""""\n\n    def __init__(self, attention_cell, cells, use_new_attention=False):\n        """"""Creates a GNMTAttentionMultiCell.\n\n        Args:\n          attention_cell: An instance of AttentionWrapper.\n          cells: A list of RNNCell wrapped with AttentionInputWrapper.\n          use_new_attention: Whether to use the attention generated from current\n            step bottom layer\'s output. Default is False.\n        """"""\n        cells = [attention_cell] + cells\n        self.use_new_attention = use_new_attention\n        super(GNMTAttentionMultiCell, self).__init__(cells, state_is_tuple=True)\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run the cell with bottom layer\'s attention copied to all upper layers.""""""\n        if not tf.contrib.framework.nest.is_sequence(state):\n            raise ValueError(\n                ""Expected state to be a tuple of length %d, but received: %s""\n                % (len(self.state_size), state)\n            )\n\n        with tf.variable_scope(scope or ""multi_rnn_cell""):\n            new_states = []\n\n            with tf.variable_scope(""cell_0_attention""):\n                attention_cell = self._cells[0]\n                attention_state = state[0]\n                cur_inp, new_attention_state = attention_cell(inputs, attention_state)\n                new_states.append(new_attention_state)\n\n            for i in range(1, len(self._cells)):\n                with tf.variable_scope(""cell_%d"" % i):\n\n                    cell = self._cells[i]\n                    cur_state = state[i]\n\n                    if self.use_new_attention:\n                        cur_inp = tf.concat([cur_inp, new_attention_state.attention], -1)\n                    else:\n                        cur_inp = tf.concat([cur_inp, attention_state.attention], -1)\n\n                    cur_inp, new_state = cell(cur_inp, cur_state)\n                    new_states.append(new_state)\n\n        return cur_inp, tuple(new_states)\n\n\ndef gnmt_residual_fn(inputs, outputs):\n    """"""Residual function that handles different inputs and outputs inner dims.\n\n    Args:\n      inputs: cell inputs, this is actual inputs concatenated with the attention\n        vector.\n      outputs: cell outputs\n\n    Returns:\n      outputs + actual inputs\n    """"""\n\n    def split_input(inp, out):\n        out_dim = out.get_shape().as_list()[-1]\n        inp_dim = inp.get_shape().as_list()[-1]\n        return tf.split(inp, [out_dim, inp_dim - out_dim], axis=-1)\n\n    actual_inputs, _ = tf.contrib.framework.nest.map_structure(split_input, inputs, outputs)\n\n    def assert_shape_match(inp, out):\n        inp.get_shape().assert_is_compatible_with(out.get_shape())\n\n    tf.contrib.framework.nest.assert_same_structure(actual_inputs, outputs)\n    tf.contrib.framework.nest.map_structure(assert_shape_match, actual_inputs, outputs)\n    return tf.contrib.framework.nest.map_structure(\n        lambda inp, out: inp + out, actual_inputs, outputs\n    )\n'"
examples/sparse_gnmt/inference.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n#   quantization interface\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""To perform inference on test set given a trained model.""""""\nfrom __future__ import print_function\n\nimport codecs\nimport os\nimport time\n\nimport tensorflow as tf\n\nfrom . import gnmt_model\nfrom .gnmt import attention_model, model as nmt_model\nfrom .gnmt import model_helper\nfrom .gnmt.utils import nmt_utils, misc_utils as utils\n\n__all__ = [""load_data"", ""inference"", ""single_worker_inference"", ""multi_worker_inference""]\n\n\ndef _decode_inference_indices(\n    model,\n    sess,\n    output_infer,\n    output_infer_summary_prefix,\n    inference_indices,\n    tgt_eos,\n    subword_option,\n):\n    """"""Decoding only a specific set of sentences.""""""\n    utils.print_out(\n        ""  decoding to output %s , num sents %d."" % (output_infer, len(inference_indices))\n    )\n    start_time = time.time()\n    with codecs.getwriter(""utf-8"")(tf.gfile.GFile(output_infer, mode=""wb"")) as trans_f:\n        trans_f.write("""")  # Write empty string to ensure file is created.\n        for decode_id in inference_indices:\n            nmt_outputs, infer_summary = model.decode(sess)\n\n            # get text translation\n            assert nmt_outputs.shape[0] == 1\n            translation = nmt_utils.get_translation(\n                nmt_outputs, sent_id=0, tgt_eos=tgt_eos, subword_option=subword_option\n            )\n\n            if infer_summary is not None:  # Attention models\n                image_file = output_infer_summary_prefix + str(decode_id) + "".png""\n                utils.print_out(""  save attention image to %s*"" % image_file)\n                image_summ = tf.Summary()\n                image_summ.ParseFromString(infer_summary)\n                with tf.gfile.GFile(image_file, mode=""w"") as img_f:\n                    # pylint: disable=no-member\n                    img_f.write(image_summ.value[0].image.encoded_image_string)\n\n            trans_f.write(""%s\\n"" % translation)\n            utils.print_out(translation + b""\\n"")\n    utils.print_time(""  done"", start_time)\n\n\ndef load_data(inference_input_file, hparams=None):\n    """"""Load inference data.""""""\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(inference_input_file, mode=""rb"")) as f:\n        inference_data = f.read().splitlines()\n\n    if hparams and hparams.inference_indices:\n        inference_data = [inference_data[i] for i in hparams.inference_indices]\n\n    return inference_data\n\n\ndef get_model_creator(hparams):\n    """"""Get the right model class depending on configuration.""""""\n    if hparams.encoder_type == ""gnmt"" or hparams.attention_architecture in [""gnmt"", ""gnmt_v2""]:\n        model_creator = gnmt_model.GNMTModel\n    elif hparams.attention_architecture == ""standard"":\n        model_creator = attention_model.AttentionModel\n    elif not hparams.attention:\n        model_creator = nmt_model.Model\n    else:\n        raise ValueError(""Unknown attention architecture %s"" % hparams.attention_architecture)\n    return model_creator\n\n\ndef inference(\n    ckpt_path,\n    inference_input_file,\n    inference_output_file,\n    hparams,\n    num_workers=1,\n    jobid=0,\n    scope=None,\n):\n    """"""Perform translation.""""""\n    if hparams.inference_indices:\n        assert num_workers == 1\n\n    model_creator = get_model_creator(hparams)\n    infer_model = model_helper.create_infer_model(model_creator, hparams, scope)\n\n    if hparams.quantize_ckpt or hparams.from_quantized_ckpt:\n        model_helper.add_quatization_variables(infer_model)\n\n    with tf.Session(graph=infer_model.graph, config=utils.get_config_proto()) as sess:\n        with infer_model.graph.as_default():\n            load_fn = (\n                model_helper.load_model\n                if not hparams.from_quantized_ckpt\n                else model_helper.load_quantized_model\n            )\n            if hparams.quantize_ckpt:\n                load_fn(infer_model.model, ckpt_path, sess, ""infer"")\n                load_fn = model_helper.load_quantized_model\n                ckpt_path = os.path.join(hparams.out_dir, ""quant_"" + os.path.basename(ckpt_path))\n                model_helper.quantize_checkpoint(sess, ckpt_path)\n            loaded_infer_model = load_fn(infer_model.model, ckpt_path, sess, ""infer"")\n\n        if num_workers == 1:\n            single_worker_inference(\n                sess,\n                infer_model,\n                loaded_infer_model,\n                inference_input_file,\n                inference_output_file,\n                hparams,\n            )\n        else:\n            multi_worker_inference(\n                sess,\n                infer_model,\n                loaded_infer_model,\n                inference_input_file,\n                inference_output_file,\n                hparams,\n                num_workers=num_workers,\n                jobid=jobid,\n            )\n\n\ndef single_worker_inference(\n    sess, infer_model, loaded_infer_model, inference_input_file, inference_output_file, hparams\n):\n    """"""Inference with a single worker.""""""\n    output_infer = inference_output_file\n\n    # Read data\n    infer_data = load_data(inference_input_file, hparams)\n\n    with infer_model.graph.as_default():\n        sess.run(\n            infer_model.iterator.initializer,\n            feed_dict={\n                infer_model.src_placeholder: infer_data,\n                infer_model.batch_size_placeholder: hparams.infer_batch_size,\n            },\n        )\n        # Decode\n        utils.print_out(""# Start decoding"")\n        if hparams.inference_indices:\n            _decode_inference_indices(\n                loaded_infer_model,\n                sess,\n                output_infer=output_infer,\n                output_infer_summary_prefix=output_infer,\n                inference_indices=hparams.inference_indices,\n                tgt_eos=hparams.eos,\n                subword_option=hparams.subword_option,\n            )\n        else:\n            nmt_utils.decode_and_evaluate(\n                ""infer"",\n                loaded_infer_model,\n                sess,\n                output_infer,\n                ref_file=None,\n                metrics=hparams.metrics,\n                subword_option=hparams.subword_option,\n                beam_width=hparams.beam_width,\n                tgt_eos=hparams.eos,\n                num_translations_per_input=hparams.num_translations_per_input,\n                infer_mode=hparams.infer_mode,\n            )\n\n\ndef multi_worker_inference(\n    sess,\n    infer_model,\n    loaded_infer_model,\n    inference_input_file,\n    inference_output_file,\n    hparams,\n    num_workers,\n    jobid,\n):\n    """"""Inference using multiple workers.""""""\n    assert num_workers > 1\n\n    final_output_infer = inference_output_file\n    output_infer = ""%s_%d"" % (inference_output_file, jobid)\n    output_infer_done = ""%s_done_%d"" % (inference_output_file, jobid)\n\n    # Read data\n    infer_data = load_data(inference_input_file, hparams)\n\n    # Split data to multiple workers\n    total_load = len(infer_data)\n    load_per_worker = int((total_load - 1) / num_workers) + 1\n    start_position = jobid * load_per_worker\n    end_position = min(start_position + load_per_worker, total_load)\n    infer_data = infer_data[start_position:end_position]\n\n    with infer_model.graph.as_default():\n        sess.run(\n            infer_model.iterator.initializer,\n            {\n                infer_model.src_placeholder: infer_data,\n                infer_model.batch_size_placeholder: hparams.infer_batch_size,\n            },\n        )\n        # Decode\n        utils.print_out(""# Start decoding"")\n        nmt_utils.decode_and_evaluate(\n            ""infer"",\n            loaded_infer_model,\n            sess,\n            output_infer,\n            ref_file=None,\n            metrics=hparams.metrics,\n            subword_option=hparams.subword_option,\n            beam_width=hparams.beam_width,\n            tgt_eos=hparams.eos,\n            num_translations_per_input=hparams.num_translations_per_input,\n            infer_mode=hparams.infer_mode,\n        )\n\n        # Change file name to indicate the file writing is completed.\n        tf.gfile.Rename(output_infer, output_infer_done, overwrite=True)\n\n        # Job 0 is responsible for the clean up.\n        if jobid != 0:\n            return\n\n        # Now write all translations\n        with codecs.getwriter(""utf-8"")(tf.gfile.GFile(final_output_infer, mode=""wb"")) as final_f:\n            for worker_id in range(num_workers):\n                worker_infer_done = ""%s_done_%d"" % (inference_output_file, worker_id)\n                while not tf.gfile.Exists(worker_infer_done):\n                    utils.print_out(""  waiting job %d to complete."" % worker_id)\n                    time.sleep(10)\n\n                with codecs.getreader(""utf-8"")(tf.gfile.GFile(worker_infer_done, mode=""rb"")) as f:\n                    for translation in f:\n                        final_f.write(""%s"" % translation)\n\n            for worker_id in range(num_workers):\n                worker_infer_done = ""%s_done_%d"" % (inference_output_file, worker_id)\n                tf.gfile.Remove(worker_infer_done)\n'"
examples/sparse_gnmt/nmt.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n#   modified arguments\n#   validating arguments\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""TensorFlow NMT model implementation.""""""\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nlp_architect.utils import io\nfrom . import inference\nfrom . import train\nfrom .gnmt.utils import misc_utils as utils, vocab_utils, evaluation_utils\n\nutils.check_tensorflow_version()\n\nFLAGS = None\n\nINFERENCE_KEYS = [\n    ""src_max_len_infer"",\n    ""tgt_max_len_infer"",\n    ""subword_option"",\n    ""infer_batch_size"",\n    ""beam_width"",\n    ""length_penalty_weight"",\n    ""sampling_temperature"",\n    ""num_translations_per_input"",\n    ""infer_mode"",\n]\n\n\n# pylint: disable=too-many-statements\ndef add_arguments(parser):\n    """"""Build ArgumentParser.""""""\n    parser.register(""type"", ""bool"", io.validate_boolean)\n\n    # network\n    parser.add_argument(""--num_units"", type=int, default=32, help=""Network size."")\n    parser.add_argument(""--num_layers"", type=int, default=2, help=""Network depth."")\n    parser.add_argument(\n        ""--num_encoder_layers"",\n        type=int,\n        default=None,\n        help=""Encoder depth, equal to num_layers if None."",\n    )\n    parser.add_argument(\n        ""--num_decoder_layers"",\n        type=int,\n        default=None,\n        help=""Decoder depth, equal to num_layers if None."",\n    )\n    parser.add_argument(\n        ""--encoder_type"",\n        type=str,\n        default=""uni"",\n        choices=[""uni"", ""bi"", ""gnmt""],\n        help=""""""\\\n      uni | bi | gnmt.\n      For bi, we build num_encoder_layers/2 bi-directional layers.\n      For gnmt, we build 1 bi-directional layer, and (num_encoder_layers - 1)\n        uni-directional layers.\\\n      """""",\n    )\n    parser.add_argument(\n        ""--residual"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""Whether to add residual connections."",\n    )\n    parser.add_argument(\n        ""--time_major"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=True,\n        help=""Whether to use time-major mode for dynamic RNN."",\n    )\n    parser.add_argument(\n        ""--num_embeddings_partitions"",\n        type=int,\n        default=0,\n        help=""Number of partitions for embedding vars."",\n    )\n\n    # attention mechanisms\n    parser.add_argument(\n        ""--attention"",\n        type=str,\n        default="""",\n        choices=["""", ""luong"", ""scaled_luong"", ""bahdanau"", ""normed_bahdanau""],\n        help=""""""\\\n      luong | scaled_luong | bahdanau | normed_bahdanau or set to """" for no\n      attention\\\n      """""",\n    )\n    parser.add_argument(\n        ""--attention_architecture"",\n        type=str,\n        default=""standard"",\n        choices=[""standard"", ""gnmt"", ""gnmt_v2""],\n        help=""""""\\\n      standard | gnmt | gnmt_v2.\n      standard: use top layer to compute attention.\n      gnmt: GNMT style of computing attention, use previous bottom layer to\n          compute attention.\n      gnmt_v2: similar to gnmt, but use current bottom layer to compute\n          attention.\\\n      """""",\n    )\n    parser.add_argument(\n        ""--output_attention"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=True,\n        help=""""""\\\n      Only used in standard attention_architecture. Whether use attention as\n      the cell output at each timestep.\n      .\\\n      """""",\n    )\n    parser.add_argument(\n        ""--pass_hidden_state"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=True,\n        help=""""""\\\n      Whether to pass encoder\'s hidden state to decoder when using an attention\n      based model.\\\n      """""",\n    )\n\n    # optimizer\n    parser.add_argument(\n        ""--optimizer"", type=str, default=""sgd"", choices=[""sgd"", ""adam""], help=""sgd | adam""\n    )\n    parser.add_argument(\n        ""--learning_rate"", type=float, default=1.0, help=""Learning rate. Adam: 0.001 | 0.0001""\n    )\n    parser.add_argument(\n        ""--warmup_steps"", type=int, default=0, help=""How many steps we inverse-decay learning.""\n    )\n    parser.add_argument(\n        ""--warmup_scheme"",\n        type=str,\n        default=""t2t"",\n        choices=[""t2t""],\n        help=""""""\\\n      How to warmup learning rates. Options include:\n        t2t: Tensor2Tensor\'s way, start with lr 100 times smaller, then\n             exponentiate until the specified lr.\\\n      """""",\n    )\n    parser.add_argument(\n        ""--decay_scheme"",\n        type=str,\n        default="""",\n        choices=["""", ""luong234"", ""luong5"", ""luong10""],\n        help=""""""\\\n      How we decay learning rate. Options include:\n        luong234: after 2/3 num train steps, we start halving the learning rate\n          for 4 times before finishing.\n        luong5: after 1/2 num train steps, we start halving the learning rate\n          for 5 times before finishing.\\\n        luong10: after 1/2 num train steps, we start halving the learning rate\n          for 10 times before finishing.\\\n      """""",\n    )\n\n    parser.add_argument(""--num_train_steps"", type=int, default=12000, help=""Num steps to train."")\n    parser.add_argument(\n        ""--colocate_gradients_with_ops"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=True,\n        help=(""Whether try colocating gradients with "" ""corresponding op""),\n    )\n\n    # initializer\n    parser.add_argument(\n        ""--init_op"",\n        type=str,\n        default=""uniform"",\n        choices=[""uniform"", ""glorot_normal"", ""glorot_uniform""],\n        help=""uniform | glorot_normal | glorot_uniform"",\n    )\n    parser.add_argument(\n        ""--init_weight"",\n        type=float,\n        default=0.1,\n        help=(""for uniform init_op, initialize weights "" ""between [-this, this].""),\n    )\n\n    # data\n    parser.add_argument(""--src"", type=str, default=None, help=""Source suffix, e.g., en."")\n    parser.add_argument(""--tgt"", type=str, default=None, help=""Target suffix, e.g., de."")\n    parser.add_argument(\n        ""--train_prefix"",\n        type=str,\n        default=None,\n        help=""Train prefix, expect files with src/tgt suffixes."",\n    )\n    parser.add_argument(\n        ""--dev_prefix"",\n        type=str,\n        default=None,\n        help=""Dev prefix, expect files with src/tgt suffixes."",\n    )\n    parser.add_argument(\n        ""--test_prefix"",\n        type=str,\n        default=None,\n        help=""Test prefix, expect files with src/tgt suffixes."",\n    )\n    parser.add_argument(""--out_dir"", type=str, default=None, help=""Store log/model files."")\n\n    # Vocab\n    parser.add_argument(\n        ""--vocab_prefix"",\n        type=str,\n        default=None,\n        help=""""""\\\n      Vocab prefix, expect files with src/tgt suffixes.\\\n      """""",\n    )\n    parser.add_argument(\n        ""--embed_prefix"",\n        type=str,\n        default=None,\n        help=""""""\\\n      Pretrained embedding prefix, expect files with src/tgt suffixes.\n      The embedding files should be Glove formated txt files.\\\n      """""",\n    )\n    parser.add_argument(""--sos"", type=str, default=""<s>"", help=""Start-of-sentence symbol."")\n    parser.add_argument(""--eos"", type=str, default=""</s>"", help=""End-of-sentence symbol."")\n    parser.add_argument(\n        ""--share_vocab"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""""""\\\n      Whether to use the source vocab and embeddings for both source and\n      target.\\\n      """""",\n    )\n    parser.add_argument(\n        ""--check_special_token"",\n        type=""bool"",\n        default=True,\n        help=""""""\\\n                      Whether check special sos, eos, unk tokens exist in the\n                      vocab files.\\\n                      """""",\n    )\n\n    # Sequence lengths\n    parser.add_argument(\n        ""--src_max_len"", type=int, default=50, help=""Max length of src sequences during training.""\n    )\n    parser.add_argument(\n        ""--tgt_max_len"", type=int, default=50, help=""Max length of tgt sequences during training.""\n    )\n    parser.add_argument(\n        ""--src_max_len_infer"",\n        type=int,\n        default=None,\n        help=""Max length of src sequences during inference."",\n    )\n    parser.add_argument(\n        ""--tgt_max_len_infer"",\n        type=int,\n        default=None,\n        help=""""""\\\n      Max length of tgt sequences during inference.  Also use to restrict the\n      maximum decoding length.\\\n      """""",\n    )\n\n    # Default settings works well (rarely need to change)\n    parser.add_argument(\n        ""--unit_type"",\n        type=str,\n        default=""lstm"",\n        choices=[""lstm"", ""gru"", ""layer_norm_lstm"", ""mlstm""],\n        help=""lstm | gru | layer_norm_lstm | nas | mlstm"",\n    )\n    parser.add_argument(\n        ""--projection_type"",\n        type=str,\n        default=""dense"",\n        choices=[""dense"", ""sparse""],\n        help=""dense | sparse"",\n    )\n    parser.add_argument(\n        ""--embedding_type"",\n        type=str,\n        default=""dense"",\n        choices=[""dense"", ""sparse""],\n        help=""dense | sparse"",\n    )\n    parser.add_argument(\n        ""--forget_bias"", type=float, default=1.0, help=""Forget bias for BasicLSTMCell.""\n    )\n    parser.add_argument(""--dropout"", type=float, default=0.2, help=""Dropout rate (not keep_prob)"")\n    parser.add_argument(\n        ""--max_gradient_norm"", type=float, default=5.0, help=""Clip gradients to this norm.""\n    )\n    parser.add_argument(""--batch_size"", type=int, default=128, help=""Batch size."")\n\n    parser.add_argument(\n        ""--steps_per_stats"",\n        type=int,\n        default=100,\n        help=(\n            ""How many training steps to do per stats logging.""\n            ""Save checkpoint every 10x steps_per_stats""\n        ),\n    )\n    parser.add_argument(\n        ""--max_train"", type=int, default=0, help=""Limit on the size of training data (0: no limit).""\n    )\n    parser.add_argument(\n        ""--num_buckets"", type=int, default=5, help=""Put data into similar-length buckets.""\n    )\n    parser.add_argument(\n        ""--num_sampled_softmax"",\n        type=int,\n        default=0,\n        help=(""Use sampled_softmax_loss if > 0."" ""Otherwise, use full softmax loss.""),\n    )\n\n    # SPM\n    parser.add_argument(\n        ""--subword_option"",\n        type=str,\n        default="""",\n        choices=["""", ""bpe"", ""spm""],\n        help=""""""\\\n                      Set to bpe or spm to activate subword desegmentation.\\\n                      """""",\n    )\n\n    # Experimental encoding feature.\n    parser.add_argument(\n        ""--use_char_encode"",\n        type=""bool"",\n        default=False,\n        help=""""""\\\n                      Whether to split each word or bpe into character, and then\n                      generate the word-level representation from the character\n                      reprentation.\n                      """""",\n    )\n\n    # Misc\n    parser.add_argument(""--num_gpus"", type=int, default=1, help=""Number of gpus in each worker."")\n    parser.add_argument(\n        ""--log_device_placement"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""Debug GPU allocation."",\n    )\n    parser.add_argument(\n        ""--metrics"",\n        type=str,\n        default=""bleu"",\n        help=(""Comma-separated list of evaluations "" ""metrics (bleu,rouge,accuracy)""),\n    )\n    parser.add_argument(\n        ""--steps_per_external_eval"",\n        type=int,\n        default=None,\n        help=""""""\\\n      How many training steps to do per external evaluation.  Automatically set\n      based on data if None.\\\n      """""",\n    )\n    parser.add_argument(""--scope"", type=str, default=None, help=""scope to put variables under"")\n    parser.add_argument(\n        ""--hparams_path"",\n        type=str,\n        default=None,\n        help=(""Path to standard hparams json file that overrides"" ""hparams values from FLAGS.""),\n    )\n    parser.add_argument(\n        ""--random_seed"", type=int, default=None, help=""Random seed (>0, set a specific seed).""\n    )\n    parser.add_argument(\n        ""--override_loaded_hparams"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""Override loaded hparams with values specified"",\n    )\n    parser.add_argument(\n        ""--num_keep_ckpts"", type=int, default=5, help=""Max number of checkpoints to keep.""\n    )\n    parser.add_argument(\n        ""--avg_ckpts"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=(\n            """"""\\\n                      Average the last N checkpoints for external evaluation.\n                      N can be controlled by setting --num_keep_ckpts.\\\n                      """"""\n        ),\n    )\n    parser.add_argument(\n        ""--language_model"",\n        type=""bool"",\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""True to train a language model, ignoring encoder"",\n    )\n\n    # Inference\n    parser.add_argument(\n        ""--ckpt"", type=str, default="""", help=""Checkpoint file to load a model for inference.""\n    )\n    parser.add_argument(\n        ""--quantize_ckpt"",\n        type=""bool"",\n        default=False,\n        help=""Set to True to produce a quantized checkpoint from existing "" ""checkpoint"",\n    )\n    parser.add_argument(\n        ""--from_quantized_ckpt"",\n        type=""bool"",\n        default=False,\n        help=""Set to True when the given checkpoint is quantized"",\n    )\n    parser.add_argument(\n        ""--inference_input_file"", type=str, default=None, help=""Set to the text to decode.""\n    )\n    parser.add_argument(\n        ""--inference_list"",\n        type=str,\n        default=None,\n        help=(""A comma-separated list of sentence indices "" ""(0-based) to decode.""),\n    )\n    parser.add_argument(\n        ""--infer_batch_size"", type=int, default=32, help=""Batch size for inference mode.""\n    )\n    parser.add_argument(\n        ""--inference_output_file"",\n        type=str,\n        default=None,\n        help=""Output file to store decoding results."",\n    )\n    parser.add_argument(\n        ""--inference_ref_file"",\n        type=str,\n        default=None,\n        help=(\n            """"""\\\n      Reference file to compute evaluation scores (if provided).\\\n      """"""\n        ),\n    )\n\n    # Advanced inference arguments\n    parser.add_argument(\n        ""--infer_mode"",\n        type=str,\n        default=""greedy"",\n        choices=[""greedy"", ""sample"", ""beam_search""],\n        help=""Which type of decoder to use during inference."",\n    )\n    parser.add_argument(\n        ""--beam_width"",\n        type=int,\n        default=0,\n        help=(\n            """"""\\\n      beam width when using beam search decoder. If 0 (default), use standard\n      decoder with greedy helper.\\\n      """"""\n        ),\n    )\n    parser.add_argument(\n        ""--length_penalty_weight"", type=float, default=0.0, help=""Length penalty for beam search.""\n    )\n    parser.add_argument(\n        ""--sampling_temperature"",\n        type=float,\n        default=0.0,\n        help=(\n            """"""\\\n      Softmax sampling temperature for inference decoding, 0.0 means greedy\n      decoding. This option is ignored when using beam search.\\\n      """"""\n        ),\n    )\n    parser.add_argument(\n        ""--num_translations_per_input"",\n        type=int,\n        default=1,\n        help=(\n            """"""\\\n      Number of translations generated for each sentence. This is only used for\n      inference.\\\n      """"""\n        ),\n    )\n\n    # Job info\n    parser.add_argument(""--jobid"", type=int, default=0, help=""Task id of the worker."")\n    parser.add_argument(\n        ""--num_workers"", type=int, default=1, help=""Number of workers (inference only).""\n    )\n    parser.add_argument(\n        ""--num_inter_threads"", type=int, default=0, help=""number of inter_op_parallelism_threads""\n    )\n    parser.add_argument(\n        ""--num_intra_threads"", type=int, default=0, help=""number of intra_op_parallelism_threads""\n    )\n    parser.add_argument(\n        ""--pruning_hparams"", type=str, default=None, help=""model pruning parameters""\n    )\n\n\ndef create_hparams(flags):\n    """"""Create training hparams.""""""\n    return tf.contrib.training.HParams(\n        # Data\n        src=flags.src,\n        tgt=flags.tgt,\n        train_prefix=flags.train_prefix,\n        dev_prefix=flags.dev_prefix,\n        test_prefix=flags.test_prefix,\n        vocab_prefix=flags.vocab_prefix,\n        embed_prefix=flags.embed_prefix,\n        out_dir=flags.out_dir,\n        # Networks\n        num_units=flags.num_units,\n        num_encoder_layers=(flags.num_encoder_layers or flags.num_layers),\n        num_decoder_layers=(flags.num_decoder_layers or flags.num_layers),\n        dropout=flags.dropout,\n        unit_type=flags.unit_type,\n        projection_type=flags.projection_type,\n        embedding_type=flags.embedding_type,\n        encoder_type=flags.encoder_type,\n        residual=flags.residual,\n        time_major=flags.time_major,\n        num_embeddings_partitions=flags.num_embeddings_partitions,\n        # Attention mechanisms\n        attention=flags.attention,\n        attention_architecture=flags.attention_architecture,\n        output_attention=flags.output_attention,\n        pass_hidden_state=flags.pass_hidden_state,\n        # Train\n        optimizer=flags.optimizer,\n        num_train_steps=flags.num_train_steps,\n        batch_size=flags.batch_size,\n        init_op=flags.init_op,\n        init_weight=flags.init_weight,\n        max_gradient_norm=flags.max_gradient_norm,\n        learning_rate=flags.learning_rate,\n        warmup_steps=flags.warmup_steps,\n        warmup_scheme=flags.warmup_scheme,\n        decay_scheme=flags.decay_scheme,\n        colocate_gradients_with_ops=flags.colocate_gradients_with_ops,\n        num_sampled_softmax=flags.num_sampled_softmax,\n        # Data constraints\n        num_buckets=flags.num_buckets,\n        max_train=flags.max_train,\n        src_max_len=flags.src_max_len,\n        tgt_max_len=flags.tgt_max_len,\n        # Inference\n        quantize_ckpt=flags.quantize_ckpt,\n        from_quantized_ckpt=flags.from_quantized_ckpt,\n        src_max_len_infer=flags.src_max_len_infer,\n        tgt_max_len_infer=flags.tgt_max_len_infer,\n        infer_batch_size=flags.infer_batch_size,\n        # Advanced inference arguments\n        infer_mode=flags.infer_mode,\n        beam_width=flags.beam_width,\n        length_penalty_weight=flags.length_penalty_weight,\n        sampling_temperature=flags.sampling_temperature,\n        num_translations_per_input=flags.num_translations_per_input,\n        # Vocab\n        sos=flags.sos if flags.sos else vocab_utils.SOS,\n        eos=flags.eos if flags.eos else vocab_utils.EOS,\n        subword_option=flags.subword_option,\n        check_special_token=flags.check_special_token,\n        use_char_encode=flags.use_char_encode,\n        # Misc\n        forget_bias=flags.forget_bias,\n        num_gpus=flags.num_gpus,\n        epoch_step=0,  # record where we were within an epoch.\n        steps_per_stats=flags.steps_per_stats,\n        steps_per_external_eval=flags.steps_per_external_eval,\n        share_vocab=flags.share_vocab,\n        metrics=flags.metrics.split("",""),\n        log_device_placement=flags.log_device_placement,\n        random_seed=flags.random_seed,\n        override_loaded_hparams=flags.override_loaded_hparams,\n        num_keep_ckpts=flags.num_keep_ckpts,\n        avg_ckpts=flags.avg_ckpts,\n        language_model=flags.language_model,\n        num_intra_threads=flags.num_intra_threads,\n        num_inter_threads=flags.num_inter_threads,\n        # Model pruning\n        pruning_hparams=flags.pruning_hparams,\n    )\n\n\ndef fix_path(file_path):\n    """"""Fix file path for validation function""""""\n    return os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), os.path.realpath(str(file_path))\n    )\n\n\ndef add_suffix(p, s):\n    """"""Stich suffix and prefix""""""\n    s = ""."" + str(s) if s is not None else """"\n    return str(p) + str(s)\n\n\ndef validate_existing_filepath(prefix, suffix=None):\n    """"""Validates existing file in the path constructed from prefix.suffix in case\n    prefix is not None""""""\n    if prefix is not None and prefix:\n        io.validate_existing_filepath(fix_path(add_suffix(prefix, suffix)))\n\n\ndef validate_parent_exists(file_path):\n    """"""Validates parent directory exists in case the file_path is not None""""""\n    if file_path is not None and file_path:\n        io.validate_parent_exists(fix_path(file_path))\n\n\ndef validate_arguments(args):\n    """"""Validate input arguments""""""\n    io.validate((args.num_units, int, 1, None))\n    io.validate((args.num_layers, int, 1, None))\n    io.validate((args.num_encoder_layers, (int, type(None)), 0, None))\n    io.validate((args.num_decoder_layers, (int, type(None)), 0, None))\n    io.validate((args.num_embeddings_partitions, int, 0, None))\n    io.validate((args.learning_rate, float, 0.0, None))\n    io.validate((args.num_train_steps, int, 1, None))\n    io.validate((args.warmup_steps, int, 0, args.num_train_steps))\n    io.validate((args.init_weight, float))\n    io.validate((args.src, (str, type(None)), 1, 256))\n    io.validate((args.tgt, (str, type(None)), 1, 256))\n    io.validate((args.sos, str, 1, 256))\n    io.validate((args.eos, str, 1, 256))\n    io.validate((args.src_max_len, int, 1, None))\n    io.validate((args.tgt_max_len, int, 1, None))\n    io.validate((args.src_max_len_infer, (int, type(None)), 1, None))\n    io.validate((args.tgt_max_len_infer, (int, type(None)), 1, None))\n    io.validate((args.forget_bias, float, 0.0, None))\n    io.validate((args.dropout, float, 0.0, 1.0))\n    io.validate((args.max_gradient_norm, float, 0.000000001, None))\n    io.validate((args.batch_size, int, 1, None))\n    io.validate((args.steps_per_stats, int, 1, None))\n    io.validate((args.max_train, int, 0, None))\n    io.validate((args.num_buckets, int, 1, None))\n    io.validate((args.num_sampled_softmax, int, 0, None))\n    io.validate((args.num_gpus, int, 0, None))\n    io.validate((args.metrics, str, 1, 256))\n    io.validate((args.inference_list, (str, type(None)), 0, 256))\n    io.validate((args.steps_per_external_eval, (int, type(None)), 1, None))\n    io.validate((args.scope, (str, type(None)), 1, 256))\n    io.validate((args.random_seed, (int, type(None))))\n    io.validate((args.num_keep_ckpts, int, 0, None))\n    io.validate((args.infer_batch_size, int, 1, None))\n    io.validate((args.beam_width, int, 0, None))\n    io.validate((args.length_penalty_weight, float, 0.0, None))\n    io.validate((args.sampling_temperature, float, 0.0, None))\n    io.validate((args.num_translations_per_input, int, 1, None))\n    io.validate((args.jobid, int, 0, None))\n    io.validate((args.num_workers, int, 1, None))\n    io.validate((args.num_inter_threads, int, 0, None))\n    io.validate((args.num_intra_threads, int, 0, None))\n    io.validate((args.pruning_hparams, (str, type(None)), 1, 256))\n\n    suffixes = [args.src]\n    if not args.language_model:\n        suffixes.append(args.tgt)\n\n    for suffix in suffixes:\n        validate_existing_filepath(args.train_prefix, suffix)\n        validate_existing_filepath(args.dev_prefix, suffix)\n        validate_existing_filepath(args.test_prefix, suffix)\n        validate_existing_filepath(args.vocab_prefix, suffix)\n        validate_existing_filepath(args.embed_prefix, suffix)\n    validate_existing_filepath(args.inference_ref_file)\n    validate_existing_filepath(args.inference_input_file)\n    validate_existing_filepath(args.hparams_path)\n    validate_parent_exists(args.ckpt)\n    validate_parent_exists(args.inference_output_file)\n    validate_parent_exists(args.out_dir)\n\n\ndef _add_argument(hparams, key, value, update=True):\n    """"""Add an argument to hparams; if exists, change the value if update==True.""""""\n    if hasattr(hparams, key):\n        if update:\n            setattr(hparams, key, value)\n    else:\n        hparams.add_hparam(key, value)\n\n\n# pylint: disable=too-many-statements\ndef extend_hparams(hparams):\n    """"""Add new arguments to hparams.""""""\n    # Sanity checks\n    if hparams.encoder_type == ""bi"" and hparams.num_encoder_layers % 2 != 0:\n        raise ValueError(\n            ""For bi, num_encoder_layers %d should be even"" % hparams.num_encoder_layers\n        )\n    if hparams.attention_architecture in [""gnmt""] and hparams.num_encoder_layers < 2:\n        raise ValueError(\n            ""For gnmt attention architecture, ""\n            ""num_encoder_layers %d should be >= 2"" % hparams.num_encoder_layers\n        )\n    if hparams.subword_option and hparams.subword_option not in [""spm"", ""bpe""]:\n        raise ValueError(""subword option must be either spm, or bpe"")\n    if hparams.infer_mode == ""beam_search"" and hparams.beam_width <= 0:\n        raise ValueError(""beam_width must greater than 0 when using beam_search"" ""decoder."")\n    if hparams.infer_mode == ""sample"" and hparams.sampling_temperature <= 0.0:\n        raise ValueError(""sampling_temperature must greater than 0.0 when using"" ""sample decoder."")\n\n    # Different number of encoder / decoder layers\n    assert hparams.num_encoder_layers and hparams.num_decoder_layers\n    if hparams.num_encoder_layers != hparams.num_decoder_layers:\n        hparams.pass_hidden_state = False\n        utils.print_out(\n            ""Num encoder layer %d is different from num decoder layer""\n            "" %d, so set pass_hidden_state to False""\n            % (hparams.num_encoder_layers, hparams.num_decoder_layers)\n        )\n\n    # Set residual layers\n    num_encoder_residual_layers = 0\n    num_decoder_residual_layers = 0\n    if hparams.residual:\n        if hparams.num_encoder_layers > 1:\n            num_encoder_residual_layers = hparams.num_encoder_layers - 1\n        if hparams.num_decoder_layers > 1:\n            num_decoder_residual_layers = hparams.num_decoder_layers - 1\n\n        if hparams.encoder_type == ""gnmt"":\n            # The first unidirectional layer (after the bi-directional layer) in\n            # the GNMT encoder can\'t have residual connection due to the input is\n            # the concatenation of fw_cell and bw_cell\'s outputs.\n            num_encoder_residual_layers = hparams.num_encoder_layers - 2\n\n            # Compatible for GNMT models\n            if hparams.num_encoder_layers == hparams.num_decoder_layers:\n                num_decoder_residual_layers = num_encoder_residual_layers\n    _add_argument(hparams, ""num_encoder_residual_layers"", num_encoder_residual_layers)\n    _add_argument(hparams, ""num_decoder_residual_layers"", num_decoder_residual_layers)\n\n    # Language modeling\n    if getattr(hparams, ""language_model"", None):\n        hparams.attention = """"\n        hparams.attention_architecture = """"\n        hparams.pass_hidden_state = False\n        hparams.share_vocab = True\n        hparams.src = hparams.tgt\n        utils.print_out(\n            ""For language modeling, we turn off attention and ""\n            ""pass_hidden_state; turn on share_vocab; set src to tgt.""\n        )\n\n    # Vocab\n    # Get vocab file names first\n    if hparams.vocab_prefix:\n        src_vocab_file = hparams.vocab_prefix + ""."" + hparams.src\n        tgt_vocab_file = hparams.vocab_prefix + ""."" + hparams.tgt\n    else:\n        raise ValueError(""hparams.vocab_prefix must be provided."")\n\n    # Source vocab\n    check_special_token = getattr(hparams, ""check_special_token"", True)\n    src_vocab_size, src_vocab_file = vocab_utils.check_vocab(\n        src_vocab_file,\n        hparams.out_dir,\n        check_special_token=check_special_token,\n        sos=hparams.sos,\n        eos=hparams.eos,\n        unk=vocab_utils.UNK,\n    )\n\n    # Target vocab\n    if hparams.share_vocab:\n        utils.print_out(""  using source vocab for target"")\n        tgt_vocab_file = src_vocab_file\n        tgt_vocab_size = src_vocab_size\n    else:\n        tgt_vocab_size, tgt_vocab_file = vocab_utils.check_vocab(\n            tgt_vocab_file,\n            hparams.out_dir,\n            check_special_token=check_special_token,\n            sos=hparams.sos,\n            eos=hparams.eos,\n            unk=vocab_utils.UNK,\n        )\n    _add_argument(hparams, ""src_vocab_size"", src_vocab_size)\n    _add_argument(hparams, ""tgt_vocab_size"", tgt_vocab_size)\n    _add_argument(hparams, ""src_vocab_file"", src_vocab_file)\n    _add_argument(hparams, ""tgt_vocab_file"", tgt_vocab_file)\n\n    # Num embedding partitions\n    num_embeddings_partitions = getattr(hparams, ""num_embeddings_partitions"", 0)\n    _add_argument(hparams, ""num_enc_emb_partitions"", num_embeddings_partitions)\n    _add_argument(hparams, ""num_dec_emb_partitions"", num_embeddings_partitions)\n\n    # Pretrained Embeddings\n    _add_argument(hparams, ""src_embed_file"", """")\n    _add_argument(hparams, ""tgt_embed_file"", """")\n    if getattr(hparams, ""embed_prefix"", None):\n        src_embed_file = hparams.embed_prefix + ""."" + hparams.src\n        tgt_embed_file = hparams.embed_prefix + ""."" + hparams.tgt\n\n        if tf.gfile.Exists(src_embed_file):\n            utils.print_out(""  src_embed_file %s exist"" % src_embed_file)\n            hparams.src_embed_file = src_embed_file\n\n            utils.print_out(""For pretrained embeddings, set num_enc_emb_partitions to 1"")\n            hparams.num_enc_emb_partitions = 1\n        else:\n            utils.print_out(""  src_embed_file %s doesn\'t exist"" % src_embed_file)\n\n        if tf.gfile.Exists(tgt_embed_file):\n            utils.print_out(""  tgt_embed_file %s exist"" % tgt_embed_file)\n            hparams.tgt_embed_file = tgt_embed_file\n\n            utils.print_out(""For pretrained embeddings, set num_dec_emb_partitions to 1"")\n            hparams.num_dec_emb_partitions = 1\n        else:\n            utils.print_out(""  tgt_embed_file %s doesn\'t exist"" % tgt_embed_file)\n\n    # Evaluation\n    for metric in hparams.metrics:\n        best_metric_dir = os.path.join(hparams.out_dir, ""best_"" + metric)\n        tf.gfile.MakeDirs(best_metric_dir)\n        _add_argument(hparams, ""best_"" + metric, 0, update=False)\n        _add_argument(hparams, ""best_"" + metric + ""_dir"", best_metric_dir)\n\n        if getattr(hparams, ""avg_ckpts"", None):\n            best_metric_dir = os.path.join(hparams.out_dir, ""avg_best_"" + metric)\n            tf.gfile.MakeDirs(best_metric_dir)\n            _add_argument(hparams, ""avg_best_"" + metric, 0, update=False)\n            _add_argument(hparams, ""avg_best_"" + metric + ""_dir"", best_metric_dir)\n\n    return hparams\n\n\ndef ensure_compatible_hparams(hparams, default_hparams, hparams_path=""""):\n    """"""Make sure the loaded hparams is compatible with new changes.""""""\n    default_hparams = utils.maybe_parse_standard_hparams(default_hparams, hparams_path)\n\n    # Set num encoder/decoder layers (for old checkpoints)\n    if hasattr(hparams, ""num_layers""):\n        if not hasattr(hparams, ""num_encoder_layers""):\n            hparams.add_hparam(""num_encoder_layers"", hparams.num_layers)\n        if not hasattr(hparams, ""num_decoder_layers""):\n            hparams.add_hparam(""num_decoder_layers"", hparams.num_layers)\n\n    # For compatible reason, if there are new fields in default_hparams,\n    #   we add them to the current hparams\n    default_config = default_hparams.values()\n    config = hparams.values()\n    for key in default_config:\n        if key not in config:\n            hparams.add_hparam(key, default_config[key])\n\n    # Update all hparams\' keys if override_loaded_hparams=True\n    if getattr(default_hparams, ""override_loaded_hparams"", None):\n        overwritten_keys = default_config.keys()\n    else:\n        # For inference\n        overwritten_keys = INFERENCE_KEYS\n\n    for key in overwritten_keys:\n        if getattr(hparams, key) != default_config[key]:\n            utils.print_out(\n                ""# Updating hparams.%s: %s -> %s""\n                % (key, str(getattr(hparams, key)), str(default_config[key]))\n            )\n            setattr(hparams, key, default_config[key])\n    return hparams\n\n\ndef create_or_load_hparams(out_dir, default_hparams, hparams_path, save_hparams=True):\n    """"""Create hparams or load hparams from out_dir.""""""\n    hparams = utils.load_hparams(out_dir)\n    if not hparams:\n        hparams = default_hparams\n        hparams = utils.maybe_parse_standard_hparams(hparams, hparams_path)\n    else:\n        hparams = ensure_compatible_hparams(hparams, default_hparams, hparams_path)\n    hparams = extend_hparams(hparams)\n\n    # Save HParams\n    if save_hparams:\n        utils.save_hparams(out_dir, hparams)\n        for metric in hparams.metrics:\n            utils.save_hparams(getattr(hparams, ""best_"" + metric + ""_dir""), hparams)\n\n    # Print HParams\n    utils.print_hparams(hparams)\n    return hparams\n\n\ndef run_main(flags, default_hparams, train_fn, inference_fn, target_session=""""):\n    """"""Run main.""""""\n    # Job\n    jobid = flags.jobid\n    num_workers = flags.num_workers\n    utils.print_out(""# Job id %d"" % jobid)\n\n    # GPU device\n    utils.print_out(""# Devices visible to TensorFlow: %s"" % repr(tf.Session().list_devices()))\n\n    # Random\n    random_seed = flags.random_seed\n    if random_seed is not None and random_seed > 0:\n        utils.print_out(""# Set random seed to %d"" % random_seed)\n        random.seed(random_seed + jobid)\n        np.random.seed(random_seed + jobid)\n\n    # Model output directory\n    out_dir = flags.out_dir\n    if out_dir and not tf.gfile.Exists(out_dir):\n        utils.print_out(""# Creating output directory %s ..."" % out_dir)\n        tf.gfile.MakeDirs(out_dir)\n\n    # Load hparams.\n    loaded_hparams = False\n    if flags.ckpt:  # Try to load hparams from the same directory as ckpt\n        ckpt_dir = os.path.dirname(flags.ckpt)\n        ckpt_hparams_file = os.path.join(ckpt_dir, ""hparams"")\n        if tf.gfile.Exists(ckpt_hparams_file) or flags.hparams_path:\n            hparams = create_or_load_hparams(\n                ckpt_dir, default_hparams, flags.hparams_path, save_hparams=False\n            )\n            loaded_hparams = True\n    if not loaded_hparams:  # Try to load from out_dir\n        assert out_dir\n        hparams = create_or_load_hparams(\n            out_dir, default_hparams, flags.hparams_path, save_hparams=(jobid == 0)\n        )\n\n    # Train / Decode\n    if flags.inference_input_file:\n        # Inference output directory\n        trans_file = flags.inference_output_file\n        assert trans_file\n        trans_dir = os.path.dirname(trans_file)\n        if not tf.gfile.Exists(trans_dir):\n            tf.gfile.MakeDirs(trans_dir)\n\n        # Inference indices\n        hparams.inference_indices = None\n        if flags.inference_list:\n            (hparams.inference_indices) = [int(token) for token in flags.inference_list.split("","")]\n\n        # Inference\n        ckpt = flags.ckpt\n        if not ckpt:\n            ckpt = tf.train.latest_checkpoint(out_dir)\n        inference_fn(ckpt, flags.inference_input_file, trans_file, hparams, num_workers, jobid)\n\n        # Evaluation\n        ref_file = flags.inference_ref_file\n        if ref_file and tf.gfile.Exists(trans_file):\n            for metric in hparams.metrics:\n                score = evaluation_utils.evaluate(\n                    ref_file, trans_file, metric, hparams.subword_option\n                )\n                utils.print_out(""  %s: %.1f"" % (metric, score))\n    else:\n        # Train\n        train_fn(hparams, target_session=target_session)\n\n\ndef main(_):\n    default_hparams = create_hparams(FLAGS)\n    train_fn = train.train\n    inference_fn = inference.inference\n    run_main(FLAGS, default_hparams, train_fn, inference_fn)\n\n\nif __name__ == ""__main__"":\n    nmt_parser = argparse.ArgumentParser()\n    add_arguments(nmt_parser)\n    FLAGS, unparsed = nmt_parser.parse_known_args()\n    validate_arguments(FLAGS)\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
examples/sparse_gnmt/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n#   added pruning ops\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""For training NMT models.""""""\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport time\n\nimport tensorflow as tf\n\nfrom . import inference, gnmt_model\nfrom .gnmt import attention_model, model as nmt_model\nfrom .gnmt import model_helper\nfrom .gnmt.utils import nmt_utils, misc_utils as utils\n\nutils.check_tensorflow_version()\n\n__all__ = [\n    ""run_sample_decode"",\n    ""run_internal_eval"",\n    ""run_external_eval"",\n    ""run_avg_external_eval"",\n    ""run_full_eval"",\n    ""init_stats"",\n    ""update_stats"",\n    ""print_step_info"",\n    ""process_stats"",\n    ""train"",\n    ""get_model_creator"",\n    ""add_info_summaries"",\n    ""get_best_results"",\n]\n\n\ndef run_sample_decode(\n    infer_model, infer_sess, model_dir, hparams, summary_writer, src_data, tgt_data\n):\n    """"""Sample decode a random sentence from src_data.""""""\n    with infer_model.graph.as_default():\n        loaded_infer_model, global_step = model_helper.create_or_load_model(\n            infer_model.model, model_dir, infer_sess, ""infer""\n        )\n\n    _sample_decode(\n        loaded_infer_model,\n        global_step,\n        infer_sess,\n        hparams,\n        infer_model.iterator,\n        src_data,\n        tgt_data,\n        infer_model.src_placeholder,\n        infer_model.batch_size_placeholder,\n        summary_writer,\n    )\n\n\ndef run_internal_eval(\n    eval_model,\n    eval_sess,\n    model_dir,\n    hparams,\n    summary_writer,\n    use_test_set=True,\n    dev_eval_iterator_feed_dict=None,\n    test_eval_iterator_feed_dict=None,\n):\n    """"""Compute internal evaluation (perplexity) for both dev / test.\n\n    Computes development and testing perplexities for given model.\n\n    Args:\n      eval_model: Evaluation model for which to compute perplexities.\n      eval_sess: Evaluation TensorFlow session.\n      model_dir: Directory from which to load evaluation model from.\n      hparams: Model hyper-parameters.\n      summary_writer: Summary writer for logging metrics to TensorBoard.\n      use_test_set: Computes testing perplexity if true; does not otherwise.\n        Note that the development perplexity is always computed regardless of\n        value of this parameter.\n      dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        development evaluation.\n      test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        testing evaluation.\n    Returns:\n      Pair containing development perplexity and testing perplexity, in this\n      order.\n    """"""\n    if dev_eval_iterator_feed_dict is None:\n        dev_eval_iterator_feed_dict = {}\n    if test_eval_iterator_feed_dict is None:\n        test_eval_iterator_feed_dict = {}\n    with eval_model.graph.as_default():\n        loaded_eval_model, global_step = model_helper.create_or_load_model(\n            eval_model.model, model_dir, eval_sess, ""eval""\n        )\n\n    dev_src_file = ""%s.%s"" % (hparams.dev_prefix, hparams.src)\n    dev_tgt_file = ""%s.%s"" % (hparams.dev_prefix, hparams.tgt)\n    dev_eval_iterator_feed_dict[eval_model.src_file_placeholder] = dev_src_file\n    dev_eval_iterator_feed_dict[eval_model.tgt_file_placeholder] = dev_tgt_file\n\n    dev_ppl = _internal_eval(\n        loaded_eval_model,\n        global_step,\n        eval_sess,\n        eval_model.iterator,\n        dev_eval_iterator_feed_dict,\n        summary_writer,\n        ""dev"",\n    )\n    test_ppl = None\n    if use_test_set and hparams.test_prefix:\n        test_src_file = ""%s.%s"" % (hparams.test_prefix, hparams.src)\n        test_tgt_file = ""%s.%s"" % (hparams.test_prefix, hparams.tgt)\n        test_eval_iterator_feed_dict[eval_model.src_file_placeholder] = test_src_file\n        test_eval_iterator_feed_dict[eval_model.tgt_file_placeholder] = test_tgt_file\n        test_ppl = _internal_eval(\n            loaded_eval_model,\n            global_step,\n            eval_sess,\n            eval_model.iterator,\n            test_eval_iterator_feed_dict,\n            summary_writer,\n            ""test"",\n        )\n    return dev_ppl, test_ppl\n\n\ndef run_external_eval(\n    infer_model,\n    infer_sess,\n    model_dir,\n    hparams,\n    summary_writer,\n    save_best_dev=True,\n    use_test_set=True,\n    avg_ckpts=False,\n    dev_infer_iterator_feed_dict=None,\n    test_infer_iterator_feed_dict=None,\n):\n    """"""Compute external evaluation for both dev / test.\n\n    Computes development and testing external evaluation (e.g. bleu, rouge) for\n    given model.\n\n    Args:\n      infer_model: Inference model for which to compute perplexities.\n      infer_sess: Inference TensorFlow session.\n      model_dir: Directory from which to load inference model from.\n      hparams: Model hyper-parameters.\n      summary_writer: Summary writer for logging metrics to TensorBoard.\n      use_test_set: Computes testing external evaluation if true; does not\n        otherwise. Note that the development external evaluation is always\n        computed regardless of value of this parameter.\n      dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        development external evaluation.\n      test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        testing external evaluation.\n    Returns:\n      Triple containing development scores, testing scores and the TensorFlow\n      Variable for the global step number, in this order.\n    """"""\n    if dev_infer_iterator_feed_dict is None:\n        dev_infer_iterator_feed_dict = {}\n    if test_infer_iterator_feed_dict is None:\n        test_infer_iterator_feed_dict = {}\n    with infer_model.graph.as_default():\n        loaded_infer_model, global_step = model_helper.create_or_load_model(\n            infer_model.model, model_dir, infer_sess, ""infer""\n        )\n\n    dev_src_file = ""%s.%s"" % (hparams.dev_prefix, hparams.src)\n    dev_tgt_file = ""%s.%s"" % (hparams.dev_prefix, hparams.tgt)\n    dev_infer_iterator_feed_dict[infer_model.src_placeholder] = inference.load_data(dev_src_file)\n    dev_infer_iterator_feed_dict[infer_model.batch_size_placeholder] = hparams.infer_batch_size\n    dev_scores = _external_eval(\n        loaded_infer_model,\n        global_step,\n        infer_sess,\n        hparams,\n        infer_model.iterator,\n        dev_infer_iterator_feed_dict,\n        dev_tgt_file,\n        ""dev"",\n        summary_writer,\n        save_on_best=save_best_dev,\n        avg_ckpts=avg_ckpts,\n    )\n\n    test_scores = None\n    if use_test_set and hparams.test_prefix:\n        test_src_file = ""%s.%s"" % (hparams.test_prefix, hparams.src)\n        test_tgt_file = ""%s.%s"" % (hparams.test_prefix, hparams.tgt)\n        test_infer_iterator_feed_dict[infer_model.src_placeholder] = inference.load_data(\n            test_src_file\n        )\n        test_infer_iterator_feed_dict[infer_model.batch_size_placeholder] = hparams.infer_batch_size\n        test_scores = _external_eval(\n            loaded_infer_model,\n            global_step,\n            infer_sess,\n            hparams,\n            infer_model.iterator,\n            test_infer_iterator_feed_dict,\n            test_tgt_file,\n            ""test"",\n            summary_writer,\n            save_on_best=False,\n            avg_ckpts=avg_ckpts,\n        )\n    return dev_scores, test_scores, global_step\n\n\ndef run_avg_external_eval(infer_model, infer_sess, model_dir, hparams, summary_writer, global_step):\n    """"""Creates an averaged checkpoint and run external eval with it.""""""\n    avg_dev_scores, avg_test_scores = None, None\n    if hparams.avg_ckpts:\n        # Convert VariableName:0 to VariableName.\n        global_step_name = infer_model.model.global_step.name.split("":"")[0]\n        avg_model_dir = model_helper.avg_checkpoints(\n            model_dir, hparams.num_keep_ckpts, global_step, global_step_name\n        )\n\n        if avg_model_dir:\n            avg_dev_scores, avg_test_scores, _ = run_external_eval(\n                infer_model, infer_sess, avg_model_dir, hparams, summary_writer, avg_ckpts=True\n            )\n\n    return avg_dev_scores, avg_test_scores\n\n\ndef run_internal_and_external_eval(\n    model_dir,\n    infer_model,\n    infer_sess,\n    eval_model,\n    eval_sess,\n    hparams,\n    summary_writer,\n    avg_ckpts=False,\n    dev_eval_iterator_feed_dict=None,\n    test_eval_iterator_feed_dict=None,\n    dev_infer_iterator_feed_dict=None,\n    test_infer_iterator_feed_dict=None,\n):\n    """"""Compute internal evaluation (perplexity) for both dev / test.\n\n    Computes development and testing perplexities for given model.\n\n    Args:\n      model_dir: Directory from which to load models from.\n      infer_model: Inference model for which to compute perplexities.\n      infer_sess: Inference TensorFlow session.\n      eval_model: Evaluation model for which to compute perplexities.\n      eval_sess: Evaluation TensorFlow session.\n      hparams: Model hyper-parameters.\n      summary_writer: Summary writer for logging metrics to TensorBoard.\n      avg_ckpts: Whether to compute average external evaluation scores.\n      dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        internal development evaluation.\n      test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        internal testing evaluation.\n      dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        external development evaluation.\n      test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.\n        Can be used to pass in additional inputs necessary for running the\n        external testing evaluation.\n    Returns:\n      Triple containing results summary, global step Tensorflow Variable and\n      metrics in this order.\n    """"""\n    dev_ppl, test_ppl = run_internal_eval(\n        eval_model,\n        eval_sess,\n        model_dir,\n        hparams,\n        summary_writer,\n        dev_eval_iterator_feed_dict=dev_eval_iterator_feed_dict,\n        test_eval_iterator_feed_dict=test_eval_iterator_feed_dict,\n    )\n    dev_scores, test_scores, global_step = run_external_eval(\n        infer_model,\n        infer_sess,\n        model_dir,\n        hparams,\n        summary_writer,\n        dev_infer_iterator_feed_dict=dev_infer_iterator_feed_dict,\n        test_infer_iterator_feed_dict=test_infer_iterator_feed_dict,\n    )\n\n    metrics = {\n        ""dev_ppl"": dev_ppl,\n        ""test_ppl"": test_ppl,\n        ""dev_scores"": dev_scores,\n        ""test_scores"": test_scores,\n    }\n\n    avg_dev_scores, avg_test_scores = None, None\n    if avg_ckpts:\n        avg_dev_scores, avg_test_scores = run_avg_external_eval(\n            infer_model, infer_sess, model_dir, hparams, summary_writer, global_step\n        )\n        metrics[""avg_dev_scores""] = avg_dev_scores\n        metrics[""avg_test_scores""] = avg_test_scores\n\n    result_summary = _format_results(""dev"", dev_ppl, dev_scores, hparams.metrics)\n    if avg_dev_scores:\n        result_summary += "", "" + _format_results(""avg_dev"", None, avg_dev_scores, hparams.metrics)\n    if hparams.test_prefix:\n        result_summary += "", "" + _format_results(""test"", test_ppl, test_scores, hparams.metrics)\n        if avg_test_scores:\n            result_summary += "", "" + _format_results(\n                ""avg_test"", None, avg_test_scores, hparams.metrics\n            )\n\n    return result_summary, global_step, metrics\n\n\ndef run_full_eval(\n    model_dir,\n    infer_model,\n    infer_sess,\n    eval_model,\n    eval_sess,\n    hparams,\n    summary_writer,\n    sample_src_data,\n    sample_tgt_data,\n    avg_ckpts=False,\n):\n    """"""Wrapper for running sample_decode, internal_eval and external_eval.\n\n    Args:\n      model_dir: Directory from which to load models from.\n      infer_model: Inference model for which to compute perplexities.\n      infer_sess: Inference TensorFlow session.\n      eval_model: Evaluation model for which to compute perplexities.\n      eval_sess: Evaluation TensorFlow session.\n      hparams: Model hyper-parameters.\n      summary_writer: Summary writer for logging metrics to TensorBoard.\n      sample_src_data: sample of source data for sample decoding.\n      sample_tgt_data: sample of target data for sample decoding.\n      avg_ckpts: Whether to compute average external evaluation scores.\n    Returns:\n      Triple containing results summary, global step Tensorflow Variable and\n      metrics in this order.\n    """"""\n    run_sample_decode(\n        infer_model,\n        infer_sess,\n        model_dir,\n        hparams,\n        summary_writer,\n        sample_src_data,\n        sample_tgt_data,\n    )\n    return run_internal_and_external_eval(\n        model_dir,\n        infer_model,\n        infer_sess,\n        eval_model,\n        eval_sess,\n        hparams,\n        summary_writer,\n        avg_ckpts,\n    )\n\n\ndef init_stats():\n    """"""Initialize statistics that we want to accumulate.""""""\n    return {\n        ""step_time"": 0.0,\n        ""train_loss"": 0.0,\n        ""predict_count"": 0.0,  # word count on the target side\n        ""word_count"": 0.0,  # word counts for both source and target\n        ""sequence_count"": 0.0,  # number of training examples processed\n        ""grad_norm"": 0.0,\n    }\n\n\ndef update_stats(stats, start_time, step_result):\n    """"""Update stats: write summary and accumulate statistics.""""""\n    _, output_tuple = step_result\n\n    # Update statistics\n    batch_size = output_tuple.batch_size\n    stats[""step_time""] += time.time() - start_time\n    stats[""train_loss""] += output_tuple.train_loss * batch_size\n    stats[""grad_norm""] += output_tuple.grad_norm\n    stats[""predict_count""] += output_tuple.predict_count\n    stats[""word_count""] += output_tuple.word_count\n    stats[""sequence_count""] += batch_size\n\n    return (\n        output_tuple.global_step,\n        output_tuple.learning_rate,\n        output_tuple.train_summary,\n        output_tuple.pruning_summary,\n    )\n\n\ndef print_step_info(prefix, global_step, info, result_summary, log_f):\n    """"""Print all info at the current global step.""""""\n    utils.print_out(\n        ""%sstep %d lr %g step-time %.2fs wps %.2fK ppl %.2f gN %.2f %s, %s""\n        % (\n            prefix,\n            global_step,\n            info[""learning_rate""],\n            info[""avg_step_time""],\n            info[""speed""],\n            info[""train_ppl""],\n            info[""avg_grad_norm""],\n            result_summary,\n            time.ctime(),\n        ),\n        log_f,\n    )\n\n\ndef add_info_summaries(summary_writer, global_step, info):\n    """"""Add stuffs in info to summaries.""""""\n    excluded_list = [""learning_rate""]\n    for key in info:\n        if key not in excluded_list:\n            utils.add_summary(summary_writer, global_step, key, info[key])\n\n\ndef process_stats(stats, info, global_step, steps_per_stats, log_f):\n    """"""Update info and check for overflow.""""""\n    # Per-step info\n    info[""avg_step_time""] = stats[""step_time""] / steps_per_stats\n    info[""avg_grad_norm""] = stats[""grad_norm""] / steps_per_stats\n    info[""avg_sequence_count""] = stats[""sequence_count""] / steps_per_stats\n    info[""speed""] = stats[""word_count""] / (1000 * stats[""step_time""])\n\n    # Per-predict info\n    info[""train_ppl""] = utils.safe_exp(stats[""train_loss""] / stats[""predict_count""])\n\n    # Check for overflow\n    is_overflow = False\n    train_ppl = info[""train_ppl""]\n    if math.isnan(train_ppl) or math.isinf(train_ppl) or train_ppl > 1e20:\n        utils.print_out(""  step %d overflow, stop early"" % global_step, log_f)\n        is_overflow = True\n\n    return is_overflow\n\n\ndef before_train(loaded_train_model, train_model, train_sess, global_step, hparams, log_f):\n    """"""Misc tasks to do before training.""""""\n    stats = init_stats()\n    info = {\n        ""train_ppl"": 0.0,\n        ""speed"": 0.0,\n        ""avg_step_time"": 0.0,\n        ""avg_grad_norm"": 0.0,\n        ""avg_sequence_count"": 0.0,\n        ""learning_rate"": loaded_train_model.learning_rate.eval(session=train_sess),\n    }\n    start_train_time = time.time()\n    utils.print_out(\n        ""# Start step %d, lr %g, %s"" % (global_step, info[""learning_rate""], time.ctime()), log_f\n    )\n\n    # Initialize all of the iterators\n    skip_count = hparams.batch_size * hparams.epoch_step\n    utils.print_out(""# Init train iterator, skipping %d elements"" % skip_count)\n    train_sess.run(\n        train_model.iterator.initializer, feed_dict={train_model.skip_count_placeholder: skip_count}\n    )\n\n    return stats, info, start_train_time\n\n\ndef get_model_creator(hparams):\n    """"""Get the right model class depending on configuration.""""""\n    if hparams.encoder_type == ""gnmt"" or hparams.attention_architecture in [""gnmt"", ""gnmt_v2""]:\n        model_creator = gnmt_model.GNMTModel\n    elif hparams.attention_architecture == ""standard"":\n        model_creator = attention_model.AttentionModel\n    elif not hparams.attention:\n        model_creator = nmt_model.Model\n    else:\n        raise ValueError(""Unknown attention architecture %s"" % hparams.attention_architecture)\n    return model_creator\n\n\n# pylint: disable=too-many-statements\ndef train(hparams, scope=None, target_session=""""):\n    """"""Train a translation model.""""""\n    log_device_placement = hparams.log_device_placement\n    out_dir = hparams.out_dir\n    num_train_steps = hparams.num_train_steps\n    steps_per_stats = hparams.steps_per_stats\n    steps_per_external_eval = hparams.steps_per_external_eval\n    steps_per_eval = 10 * steps_per_stats\n    avg_ckpts = hparams.avg_ckpts\n\n    if not steps_per_external_eval:\n        steps_per_external_eval = 5 * steps_per_eval\n\n    # Create model\n    model_creator = get_model_creator(hparams)\n    train_model = model_helper.create_train_model(model_creator, hparams, scope)\n    eval_model = model_helper.create_eval_model(model_creator, hparams, scope)\n    infer_model = model_helper.create_infer_model(model_creator, hparams, scope)\n\n    # Preload data for sample decoding.\n    dev_src_file = ""%s.%s"" % (hparams.dev_prefix, hparams.src)\n    dev_tgt_file = ""%s.%s"" % (hparams.dev_prefix, hparams.tgt)\n    sample_src_data = inference.load_data(dev_src_file)\n    sample_tgt_data = inference.load_data(dev_tgt_file)\n\n    summary_name = ""train_log""\n    model_dir = hparams.out_dir\n\n    # Log and output files\n    log_file = os.path.join(out_dir, ""log_%d"" % time.time())\n    log_f = tf.gfile.GFile(log_file, mode=""a"")\n    utils.print_out(""# log_file=%s"" % log_file, log_f)\n\n    # TensorFlow model\n    config_proto = utils.get_config_proto(\n        log_device_placement=log_device_placement,\n        num_intra_threads=hparams.num_intra_threads,\n        num_inter_threads=hparams.num_inter_threads,\n    )\n    train_sess = tf.Session(target=target_session, config=config_proto, graph=train_model.graph)\n    eval_sess = tf.Session(target=target_session, config=config_proto, graph=eval_model.graph)\n    infer_sess = tf.Session(target=target_session, config=config_proto, graph=infer_model.graph)\n\n    with train_model.graph.as_default():\n        loaded_train_model, global_step = model_helper.create_or_load_model(\n            train_model.model, model_dir, train_sess, ""train""\n        )\n\n    # Summary writer\n    summary_writer = tf.summary.FileWriter(os.path.join(out_dir, summary_name), train_model.graph)\n\n    # First evaluation\n    run_full_eval(\n        model_dir,\n        infer_model,\n        infer_sess,\n        eval_model,\n        eval_sess,\n        hparams,\n        summary_writer,\n        sample_src_data,\n        sample_tgt_data,\n        avg_ckpts,\n    )\n\n    last_stats_step = global_step\n    last_eval_step = global_step\n    last_external_eval_step = global_step\n\n    # This is the training loop.\n    stats, info, start_train_time = before_train(\n        loaded_train_model, train_model, train_sess, global_step, hparams, log_f\n    )\n    while global_step < num_train_steps:\n        # Run a step #\n        start_time = time.time()\n        try:\n            step_result = loaded_train_model.train(train_sess)\n            hparams.epoch_step += 1\n        except tf.errors.OutOfRangeError:\n            # Finished going through the training dataset.  Go to next epoch.\n            hparams.epoch_step = 0\n            utils.print_out(\n                ""# Finished an epoch, step %d. Perform external evaluation"" % global_step\n            )\n            run_sample_decode(\n                infer_model,\n                infer_sess,\n                model_dir,\n                hparams,\n                summary_writer,\n                sample_src_data,\n                sample_tgt_data,\n            )\n            run_external_eval(infer_model, infer_sess, model_dir, hparams, summary_writer)\n\n            if avg_ckpts:\n                run_avg_external_eval(\n                    infer_model, infer_sess, model_dir, hparams, summary_writer, global_step\n                )\n\n            train_sess.run(\n                train_model.iterator.initializer, feed_dict={train_model.skip_count_placeholder: 0}\n            )\n            continue\n\n        # Process step_result, accumulate stats, and write summary\n        global_step, info[""learning_rate""], step_summary, pruning_stats = update_stats(\n            stats, start_time, step_result\n        )\n        summary_writer.add_summary(step_summary, global_step)\n        if hparams.pruning_hparams is not None:\n            summary_writer.add_summary(pruning_stats, global_step)\n\n        # Once in a while, we print statistics.\n        if global_step - last_stats_step >= steps_per_stats:\n            last_stats_step = global_step\n            is_overflow = process_stats(stats, info, global_step, steps_per_stats, log_f)\n            print_step_info(""  "", global_step, info, get_best_results(hparams), log_f)\n            if is_overflow:\n                break\n\n            # Reset statistics\n            stats = init_stats()\n\n        if global_step - last_eval_step >= steps_per_eval:\n            last_eval_step = global_step\n            utils.print_out(""# Save eval, global step %d"" % global_step)\n            add_info_summaries(summary_writer, global_step, info)\n\n            # Save checkpoint\n            loaded_train_model.saver.save(\n                train_sess, os.path.join(out_dir, ""translate.ckpt""), global_step=global_step\n            )\n\n            # Evaluate on dev/test\n            run_sample_decode(\n                infer_model,\n                infer_sess,\n                model_dir,\n                hparams,\n                summary_writer,\n                sample_src_data,\n                sample_tgt_data,\n            )\n            run_internal_eval(eval_model, eval_sess, model_dir, hparams, summary_writer)\n\n        if global_step - last_external_eval_step >= steps_per_external_eval:\n            last_external_eval_step = global_step\n\n            # Save checkpoint\n            loaded_train_model.saver.save(\n                train_sess, os.path.join(out_dir, ""translate.ckpt""), global_step=global_step\n            )\n            run_sample_decode(\n                infer_model,\n                infer_sess,\n                model_dir,\n                hparams,\n                summary_writer,\n                sample_src_data,\n                sample_tgt_data,\n            )\n            run_external_eval(infer_model, infer_sess, model_dir, hparams, summary_writer)\n\n            if avg_ckpts:\n                run_avg_external_eval(\n                    infer_model, infer_sess, model_dir, hparams, summary_writer, global_step\n                )\n\n    # Done training\n    loaded_train_model.saver.save(\n        train_sess, os.path.join(out_dir, ""translate.ckpt""), global_step=global_step\n    )\n\n    (result_summary, _, final_eval_metrics) = run_full_eval(\n        model_dir,\n        infer_model,\n        infer_sess,\n        eval_model,\n        eval_sess,\n        hparams,\n        summary_writer,\n        sample_src_data,\n        sample_tgt_data,\n        avg_ckpts,\n    )\n    print_step_info(""# Final, "", global_step, info, result_summary, log_f)\n    utils.print_time(""# Done training!"", start_train_time)\n\n    summary_writer.close()\n\n    utils.print_out(""# Start evaluating saved best models."")\n    for metric in hparams.metrics:\n        best_model_dir = getattr(hparams, ""best_"" + metric + ""_dir"")\n        summary_writer = tf.summary.FileWriter(\n            os.path.join(best_model_dir, summary_name), infer_model.graph\n        )\n        result_summary, best_global_step, _ = run_full_eval(\n            best_model_dir,\n            infer_model,\n            infer_sess,\n            eval_model,\n            eval_sess,\n            hparams,\n            summary_writer,\n            sample_src_data,\n            sample_tgt_data,\n        )\n        print_step_info(""# Best %s, "" % metric, best_global_step, info, result_summary, log_f)\n        summary_writer.close()\n\n        if avg_ckpts:\n            best_model_dir = getattr(hparams, ""avg_best_"" + metric + ""_dir"")\n            with tf.summary.FileWriter(\n                os.path.join(best_model_dir, summary_name), infer_model.graph\n            ) as summary_writer:\n                result_summary, best_global_step, _ = run_full_eval(\n                    best_model_dir,\n                    infer_model,\n                    infer_sess,\n                    eval_model,\n                    eval_sess,\n                    hparams,\n                    summary_writer,\n                    sample_src_data,\n                    sample_tgt_data,\n                )\n                print_step_info(\n                    ""# Averaged Best %s, "" % metric, best_global_step, info, result_summary, log_f\n                )\n\n    return final_eval_metrics, global_step\n\n\ndef _format_results(name, ppl, scores, metrics):\n    """"""Format results.""""""\n    result_str = """"\n    if ppl:\n        result_str = ""%s ppl %.2f"" % (name, ppl)\n    if scores:\n        for metric in metrics:\n            if result_str:\n                result_str += "", %s %s %.1f"" % (name, metric, scores[metric])\n            else:\n                result_str = ""%s %s %.1f"" % (name, metric, scores[metric])\n    return result_str\n\n\ndef get_best_results(hparams):\n    """"""Summary of the current best results.""""""\n    tokens = []\n    for metric in hparams.metrics:\n        tokens.append(""%s %.2f"" % (metric, getattr(hparams, ""best_"" + metric)))\n    return "", "".join(tokens)\n\n\ndef _internal_eval(model, global_step, sess, iterator, iterator_feed_dict, summary_writer, label):\n    """"""Computing perplexity.""""""\n    sess.run(iterator.initializer, feed_dict=iterator_feed_dict)\n    ppl = model_helper.compute_perplexity(model, sess, label)\n    utils.add_summary(summary_writer, global_step, ""%s_ppl"" % label, ppl)\n    return ppl\n\n\ndef _sample_decode(\n    model,\n    global_step,\n    sess,\n    hparams,\n    iterator,\n    src_data,\n    tgt_data,\n    iterator_src_placeholder,\n    iterator_batch_size_placeholder,\n    summary_writer,\n):\n    """"""Pick a sentence and decode.""""""\n    decode_id = random.randint(0, len(src_data) - 1)\n    utils.print_out(""  # %d"" % decode_id)\n\n    iterator_feed_dict = {\n        iterator_src_placeholder: [src_data[decode_id]],\n        iterator_batch_size_placeholder: 1,\n    }\n    sess.run(iterator.initializer, feed_dict=iterator_feed_dict)\n\n    nmt_outputs, attention_summary = model.decode(sess)\n\n    if hparams.infer_mode == ""beam_search"":\n        # get the top translation.\n        nmt_outputs = nmt_outputs[0]\n\n    translation = nmt_utils.get_translation(\n        nmt_outputs, sent_id=0, tgt_eos=hparams.eos, subword_option=hparams.subword_option\n    )\n    utils.print_out(""    src: %s"" % src_data[decode_id])\n    utils.print_out(""    ref: %s"" % tgt_data[decode_id])\n    utils.print_out(b""    nmt: "" + translation)\n\n    # Summary\n    if attention_summary is not None:\n        summary_writer.add_summary(attention_summary, global_step)\n\n\ndef _external_eval(\n    model,\n    global_step,\n    sess,\n    hparams,\n    iterator,\n    iterator_feed_dict,\n    tgt_file,\n    label,\n    summary_writer,\n    save_on_best,\n    avg_ckpts=False,\n):\n    """"""External evaluation such as BLEU and ROUGE scores.""""""\n    out_dir = hparams.out_dir\n    decode = global_step > 0\n\n    if avg_ckpts:\n        label = ""avg_"" + label\n\n    if decode:\n        utils.print_out(""# External evaluation, global step %d"" % global_step)\n\n    sess.run(iterator.initializer, feed_dict=iterator_feed_dict)\n\n    output = os.path.join(out_dir, ""output_%s"" % label)\n    scores = nmt_utils.decode_and_evaluate(\n        label,\n        model,\n        sess,\n        output,\n        ref_file=tgt_file,\n        metrics=hparams.metrics,\n        subword_option=hparams.subword_option,\n        beam_width=hparams.beam_width,\n        tgt_eos=hparams.eos,\n        decode=decode,\n        infer_mode=hparams.infer_mode,\n    )\n    # Save on best metrics\n    if decode:\n        for metric in hparams.metrics:\n            if avg_ckpts:\n                best_metric_label = ""avg_best_"" + metric\n            else:\n                best_metric_label = ""best_"" + metric\n\n            utils.add_summary(\n                summary_writer, global_step, ""%s_%s"" % (label, metric), scores[metric]\n            )\n            # metric: larger is better\n            if save_on_best and scores[metric] > getattr(hparams, best_metric_label):\n                setattr(hparams, best_metric_label, scores[metric])\n                model.saver.save(\n                    sess,\n                    os.path.join(getattr(hparams, best_metric_label + ""_dir""), ""translate.ckpt""),\n                    global_step=model.global_step,\n                )\n        utils.save_hparams(out_dir, hparams)\n    return scores\n'"
examples/supervised_sentiment/__init__.py,0,b''
examples/supervised_sentiment/amazon_reviews.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\n# This dataset should be downloaded from http://jmcauley.ucsd.edu/data/amazon/\n# The terms and conditions of the data set license apply.\n# Intel does not grant any rights to the data files.\n# The Amazon Review Dataset was published in the following papers:\n#\n# Ups and downs:\n# Modeling the visual evolution of fashion trends with one-class collaborative filtering\n# R. He, J. McAuley\n# WWW, 2016\n# http://cseweb.ucsd.edu/~jmcauley/pdfs/www16a.pdf\n#\n# Image-based recommendations on styles and substitutes\n# J. McAuley, C. Targett, J. Shi, A. van den Hengel\n# SIGIR, 2015\n# http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf\n\nimport json\n\nimport pandas as pd\n\nfrom nlp_architect.utils.generic import normalize, balance\n\ngood_columns = [""overall"", ""reviewText"", ""summary""]\n\n\ndef review_to_sentiment(review):\n    # Review is coming in as overall (the rating, reviewText, and summary)\n    # this then cleans the summary and review and gives it a positive or negative value\n    norm_text = normalize(review[2] + "" "" + review[1])\n    review_sent = [""neutral"", norm_text]\n    if review[0] > 3:\n        review_sent = [""positive"", norm_text]\n    elif review[0] < 3:\n        review_sent = [""negative"", norm_text]\n\n    return review_sent\n\n\nclass Amazon_Reviews(object):\n    """"""\n    Take the *.json file of Amazon reviews as downloaded from\n    http://jmcauley.ucsd.edu/data/amazon/\n    Then does data cleaning and balancing, as well as transforms the reviews 1-5 to a sentiment\n    """"""\n\n    def __init__(self, review_file, run_balance=True):\n        self.run_balance = run_balance\n\n        print(""Parsing and processing json file"")\n        data = []\n\n        with open(review_file, ""r"") as f:\n            for line in f:\n                data_line = json.loads(line)\n                selected_row = []\n                for item in good_columns:\n                    selected_row.append(data_line[item])\n                # as we read in, clean\n                data.append(review_to_sentiment(selected_row))\n\n        # Not sure how to easily balance outside of pandas...but should replace eventually\n        self.amazon = pd.DataFrame(data, columns=[""Sentiment"", ""clean_text""])\n        self.all_text = self.amazon[""clean_text""]\n        self.labels_0 = pd.get_dummies(self.amazon[""Sentiment""])\n        self.labels = self.labels_0.values\n        self.text = self.amazon[""clean_text""].values\n\n    def process(self):\n        self.amazon = self.amazon[self.amazon[""Sentiment""].isin([""positive"", ""negative""])]\n\n        if self.run_balance:\n            # balance it out\n            self.amazon = balance(self.amazon)\n\n        print(""Sample Data"")\n        print(self.amazon[[""Sentiment"", ""clean_text""]].head())\n\n        # mapping of the labels with dummies (has headers)\n        self.labels_0 = pd.get_dummies(self.amazon[""Sentiment""])\n        self.labels = self.labels_0.values\n        self.text = self.amazon[""clean_text""].values\n'"
examples/supervised_sentiment/ensembler.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\n\ndef simple_ensembler(np_arrays, weights):\n    """"""\n    Simple ensembler takes a list of n by m numpy array predictions and a weight list\n    The predictions should be n by m. n is the number of elements and m is the number of classes\n\n\n    Modified from the default LookupTable implementation to support multiple axis lookups.\n\n    Args:\n        vocab_size (int): the vocabulary size\n        embed_dim (int): the size of embedding vector\n        init (Initializor): initialization function\n        update (bool): if the word vectors get updated through training\n        pad_idx (int): by knowing the pad value, the update will make sure always\n                       have the vector representing pad value to be 0s.\n    """"""\n    ensembled_matrix = np_arrays[0] * weights[0]\n    for i in range(1, len(np_arrays)):\n        ensembled_matrix = ensembled_matrix + np_arrays[i] * weights[i]\n    return ensembled_matrix\n'"
examples/supervised_sentiment/example_ensemble.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\n""""""\nThis example uses the Amazon reviews though additional datasets can easily be substituted.\nIt only requires text and a sentiment label\nIt then takes the dataset and trains two models (again can be expanded)\nThe labels for the test data is then predicted.\nThe same train and test data is used for both models\n\nThe ensembler takes the two prediction matrixes and weights (as defined by model accuracy)\nand determines the final prediction matrix.\n\nFinally, the full classification report is displayed.\n\nA similar pipeline could be utilized to train models on a dataset, predict on a second dataset\nand aquire a list of final predictions\n""""""\n\nimport argparse\n\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\n\nfrom examples.supervised_sentiment.amazon_reviews import Amazon_Reviews\nfrom examples.supervised_sentiment.ensembler import simple_ensembler\nfrom nlp_architect.utils.generic import to_one_hot\nfrom nlp_architect.utils.io import validate_existing_filepath, check_size\nfrom .supervised_sentiment import simple_lstm, one_hot_cnn\n\nmax_fatures = 2000\nmax_len = 300\nbatch_size = 32\nembed_dim = 256\nlstm_out = 140\n\n\ndef ensemble_models(data, args):\n    # split, train, test\n    data.process()\n    dense_out = len(data.labels[0])\n    # split for all models\n    X_train_, X_test_, Y_train, Y_test = train_test_split(\n        data.text, data.labels, test_size=0.20, random_state=42\n    )\n\n    # Prep data for the LSTM model\n    tokenizer = Tokenizer(num_words=max_fatures, split="" "")\n    tokenizer.fit_on_texts(X_train_)\n    X_train = tokenizer.texts_to_sequences(X_train_)\n    X_train = pad_sequences(X_train, maxlen=max_len)\n    X_test = tokenizer.texts_to_sequences(X_test_)\n    X_test = pad_sequences(X_test, maxlen=max_len)\n\n    # Train the LSTM model\n    lstm_model = simple_lstm(max_fatures, dense_out, X_train.shape[1], embed_dim, lstm_out)\n    model_hist = lstm_model.fit(\n        X_train,\n        Y_train,\n        epochs=args.epochs,\n        batch_size=batch_size,\n        verbose=1,\n        validation_data=(X_test, Y_test),\n    )\n    lstm_acc = model_hist.history[""acc""][-1]\n    print(""LSTM model accuracy "", lstm_acc)\n\n    # And make predictions using the LSTM model\n    lstm_predictions = lstm_model.predict(X_test)\n\n    # Now prep data for the one-hot CNN model\n    X_train_cnn = np.asarray([to_one_hot(x) for x in X_train_])\n    X_test_cnn = np.asarray([to_one_hot(x) for x in X_test_])\n\n    # And train the one-hot CNN classifier\n    model_cnn = one_hot_cnn(dense_out, max_len)\n    model_hist_cnn = model_cnn.fit(\n        X_train_cnn,\n        Y_train,\n        batch_size=batch_size,\n        epochs=args.epochs,\n        verbose=1,\n        validation_data=(X_test_cnn, Y_test),\n    )\n    cnn_acc = model_hist_cnn.history[""acc""][-1]\n    print(""CNN model accuracy: "", cnn_acc)\n\n    # And make predictions\n    one_hot_cnn_predictions = model_cnn.predict(X_test_cnn)\n\n    # Using the accuracies create an ensemble\n    accuracies = [lstm_acc, cnn_acc]\n    norm_accuracies = [a / sum(accuracies) for a in accuracies]\n\n    print(""Ensembling with weights: "")\n    for na in norm_accuracies:\n        print(na)\n    ensembled_predictions = simple_ensembler(\n        [lstm_predictions, one_hot_cnn_predictions], norm_accuracies\n    )\n    final_preds = np.argmax(ensembled_predictions, axis=1)\n\n    # Get the final accuracy\n    print(\n        classification_report(\n            np.argmax(Y_test, axis=1), final_preds, target_names=data.labels_0.columns.values\n        )\n    )\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--file_path"", type=str, default=""./"", help=""file_path where the files to parse are located""\n    )\n    parser.add_argument(\n        ""--data_type"", type=str, default=""amazon"", choices=[""amazon""], help=""dataset source""\n    )\n    parser.add_argument(\n        ""--epochs"",\n        type=int,\n        default=10,\n        help=""Number of epochs for both models"",\n        action=check_size(1, 20000),\n    )\n    args_in = parser.parse_args()\n\n    # Check file path\n    if args_in.file_path:\n        validate_existing_filepath(args_in.file_path)\n\n    if args_in.data_type == ""amazon"":\n        data_in = Amazon_Reviews(args_in.file_path)\n    ensemble_models(data_in, args_in)\n'"
examples/supervised_sentiment/optimize_example.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport argparse\nimport pickle\n\nfrom hyperopt import fmin, tpe, hp, Trials\nfrom sklearn.model_selection import train_test_split\n\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.keras.callbacks import EarlyStopping\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\n\nfrom examples.supervised_sentiment.amazon_reviews import Amazon_Reviews\nfrom nlp_architect.utils.io import validate_parent_exists, check_size, validate_existing_filepath\nfrom .supervised_sentiment import simple_lstm\n\nmax_len = 100\nbatch_size = 32\n\n\ndef run_loss(args):\n    data = args[""data""]\n\n    # For each run we want to get a new random balance\n    data.process()\n    # split, train, test\n    dense_out = len(data.labels[0])\n    # split for all models\n    X_train_, X_test_, Y_train, Y_test = train_test_split(\n        data.text, data.labels, test_size=0.20, random_state=42\n    )\n\n    print(args)\n\n    # Prep data for the LSTM model\n    # This currently will train the tokenizer on all text (unbalanced and train/test)\n    # It would be nice to replace this with a pretrained embedding on larger text\n\n    tokenizer = Tokenizer(num_words=int(args[""max_features""]), split="" "")\n    tokenizer.fit_on_texts(data.all_text)\n    X_train = tokenizer.texts_to_sequences(X_train_)\n    X_train = pad_sequences(X_train, maxlen=max_len)\n    X_test = tokenizer.texts_to_sequences(X_test_)\n    X_test = pad_sequences(X_test, maxlen=max_len)\n\n    # Train the LSTM model\n    lstm_model = simple_lstm(\n        int(args[""max_features""]),\n        dense_out,\n        X_train.shape[1],\n        int(args[""embed_dim""]),\n        int(args[""lstm_out""]),\n        args[""dropout""],\n    )\n\n    if args[""epochs""] == 0:\n        args[""epochs""] = 1\n\n    es = EarlyStopping(monitor=""val_acc"", min_delta=0, patience=6, verbose=0, mode=""max"")\n    model_hist = lstm_model.fit(\n        X_train,\n        Y_train,\n        epochs=args[""epochs""],\n        batch_size=batch_size,\n        verbose=1,\n        validation_data=(X_test, Y_test),\n        callbacks=[es],\n    )\n    lstm_acc = model_hist.history[""val_acc""][-1]\n    print(""LSTM model accuracy "", lstm_acc)\n    # This minimizes, so the maximize we have to take the inverse :)\n    return 1 - lstm_acc\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--file_path"",\n        type=validate_existing_filepath,\n        default=""./"",\n        help=""file_path where the files to parse are located"",\n    )\n    parser.add_argument(""--data_type"", type=str, default=""amazon"", choices=[""amazon""])\n    parser.add_argument(\n        ""--output_file"",\n        type=validate_parent_exists,\n        default=""./opt_trials.pkl"",\n        help=""file_path where the output of the trials will be located"",\n    )\n    parser.add_argument(""--new_trials"", type=int, default=20, action=check_size(1, 20000))\n    args_in = parser.parse_args()\n\n    # Check inputs\n    if args_in.file_path:\n        validate_existing_filepath(args_in.file_path)\n    if args_in.output_file:\n        validate_parent_exists(args_in.output_file)\n\n    if args_in.data_type == ""amazon"":\n        data_in = Amazon_Reviews(args_in.file_path)\n\n    try:\n        if args_in.output_file.endswith("".pkl""):\n            with open(args_in.output_file, ""rb"") as read_f:\n                trials_to_keep = pickle.load(read_f)\n            print(""Utilizing existing trial files"")\n        else:\n            trials_to_keep = Trials()\n    # If the file does not already exist we will start with a new set of trials\n    except FileNotFoundError:\n        trials_to_keep = Trials()\n\n    space = {\n        ""data"": data_in,\n        ""max_features"": hp.choice(""max_features"", [500, 1000, 2000, 3000]),\n        ""embed_dim"": hp.uniform(""embed_dim"", 100, 500),\n        ""lstm_out"": hp.uniform(""lstm_out"", 50, 300),\n        ""epochs"": hp.randint(""epochs"", 50),\n        ""dropout"": hp.uniform(""dropout"", 0, 0.1),\n    }\n\n    num_evals = len(trials_to_keep.trials) + args_in.new_trials\n    best = fmin(run_loss, space=space, algo=tpe.suggest, max_evals=num_evals, trials=trials_to_keep)\n    # Write out the trials\n    with open(args_in.output_file, ""wb"") as f:\n        pickle.dump(trials_to_keep, f)\n'"
examples/supervised_sentiment/supervised_sentiment.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport tensorflow as tf\n\n\ndef simple_lstm(max_features, dense_out, input_length, embed_dim=256, lstm_out=140, dropout=0.5):\n    """"""\n    Simple Bi-direction LSTM Model in Keras\n\n    Single layer bi-directional lstm with recurrent dropout and a fully connected layer\n\n    Args:\n        max_features (int): vocabulary size\n        dense_out (int): size out the output dense layer, this is the number of classes\n        input_length (int): length of the input text\n        embed_dim (int): internal embedding size used in the lstm\n        lstm_out (int): size of the bi-directional output layer\n        dropout (float): value for recurrent dropout, between 0 and 1\n\n    Returns:\n        model (model): LSTM model\n    """"""\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Embedding(max_features, embed_dim, input_length=input_length))\n    model.add(\n        tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(lstm_out, recurrent_dropout=dropout, activation=""tanh"")\n        )\n    )\n    model.add(tf.keras.layers.Dense(dense_out, activation=""softmax""))\n    model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])\n\n    return model\n\n\ndef one_hot_cnn(dense_out, max_len=300, frame=""small""):\n    """"""\n    Temporal CNN Model\n\n    As defined in ""Text Understanding from Scratch"" by Zhang, LeCun 2015\n    https://arxiv.org/pdf/1502.01710v4.pdf\n    This model is a series of 1D CNNs, with a maxpooling and fully connected layers.\n    The frame sizes may either be large or small.\n\n\n    Args:\n        dense_out (int): size out the output dense layer, this is the number of classes\n        max_len (int): length of the input text\n        frame (str): frame size, either large or small\n\n    Returns:\n        model (model): temporal CNN model\n    """"""\n\n    if frame == ""large"":\n        cnn_size = 1024\n        fully_connected = [2048, 2048, dense_out]\n    else:\n        cnn_size = 256\n        fully_connected = [1024, 1024, dense_out]\n\n    model = tf.keras.models.Sequential()\n\n    model.add(tf.keras.layers.Conv1D(cnn_size, 7, padding=""same"", input_shape=(68, max_len)))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n\n    print(model.output_shape)\n\n    # Input = 22 x 256\n    model.add(tf.keras.layers.Conv1D(cnn_size, 7, padding=""same""))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n\n    print(model.output_shape)\n    # Input = 7 x 256\n    model.add(tf.keras.layers.Conv1D(cnn_size, 3, padding=""same""))\n\n    # Input = 7 x 256\n    model.add(tf.keras.layers.Conv1D(cnn_size, 3, padding=""same""))\n\n    model.add(tf.keras.layers.Conv1D(cnn_size, 3, padding=""same""))\n\n    # Input = 7 x 256\n    model.add(tf.keras.layers.Conv1D(cnn_size, 3, padding=""same""))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n\n    model.add(tf.keras.layers.Flatten())\n\n    # Fully Connected Layers\n\n    # Input is 512 Output is 1024/2048\n    model.add(tf.keras.layers.Dense(fully_connected[0]))\n    model.add(tf.keras.layers.Dropout(0.75))\n    model.add(tf.keras.layers.Activation(""relu""))\n\n    # Input is 1024/2048 Output is 1024/2048\n    model.add(tf.keras.layers.Dense(fully_connected[1]))\n    model.add(tf.keras.layers.Dropout(0.75))\n    model.add(tf.keras.layers.Activation(""relu""))\n\n    # Input is 1024/2048 Output is dense_out size (number of classes)\n    model.add(tf.keras.layers.Dense(fully_connected[2]))\n    model.add(tf.keras.layers.Activation(""softmax""))\n\n    # Stochastic gradient parameters as set by paper\n    sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True)\n    model.compile(loss=""categorical_crossentropy"", optimizer=sgd, metrics=[""accuracy""])\n\n    return model\n'"
examples/supervised_sentiment/test_ensembler.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport numpy as np\nfrom .ensembler import simple_ensembler\n\n\ndef test_ensembler():\n    a = np.asarray([[1, 2, 3], [1, 2, 3], [1, 2, 3]])\n    b = np.asarray([[0, 0, 0], [1, 1, 1], [2, 2, 2]])\n    w = [0.25, 0.75]\n    correct_answer = np.asarray([[0.25, 0.5, 0.75], [1.0, 1.25, 1.5], [1.75, 2.0, 2.25]])\n\n    assert simple_ensembler([a, b], w).all() == correct_answer.all()\n'"
examples/word_language_model_with_tcn/__init__.py,0,b''
nlp_architect/api/__init__.py,0,b''
nlp_architect/api/abstract_api.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom abc import abstractmethod\n\n\nNot_Implemented = ""Not Implemented""\n\n\nclass AbstractApi:\n    """"""\n    Abstract class for API\'s to the server\n    """"""\n\n    @abstractmethod\n    def load_model(self):\n        raise Not_Implemented\n\n    @abstractmethod\n    def inference(self, doc):\n        raise Not_Implemented\n'"
nlp_architect/api/base.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nfrom typing import Union, List, Dict\n\nlogger = logging.getLogger(__name__)\n\n\nclass ModelAPI:\n    """""" Base class for a model API implementation\n        Implementing classes must provide a default model and/or a path to a model\n\n        Args:\n            model_path (str): path to a trained model\n\n        run method must return\n    """"""\n\n    default_model = None  # pre-trained model from library\n\n    def __init__(self, model_path: str = None):\n        if model_path is not None:\n            self.load_model(model_path)\n        elif self.default_model is not None:\n            # get default model and load it\n            # TODO: implement model integration\n            raise NotImplementedError\n        else:\n            logger.error(""Not model provided or not pre-trained model configured"")\n\n    def load_model(self, model_path: str):\n        raise NotImplementedError\n\n    def run(self, inputs: Union[str, List[str]]) -> Dict:\n        raise NotImplementedError\n\n    def __call__(self, inputs: Union[str, List[str]]):\n        return self.run(inputs)\n'"
nlp_architect/api/bist_parser_api.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom nlp_architect.api.abstract_api import AbstractApi\nfrom nlp_architect.pipelines.spacy_bist import SpacyBISTParser\n\n\nclass BistParserApi(AbstractApi):\n    """"""\n    Bist Parser API\n    """"""\n\n    def __init__(self):\n        self.model = None\n\n    def load_model(self):\n        """"""\n        Load SpacyBISTParser model\n        """"""\n        self.model = SpacyBISTParser()\n\n    def inference(self, doc):\n        """"""\n        Parse according to SpacyBISTParser\'s model\n\n        Args:\n            doc (str): the doc str\n\n        Returns:\n            CoreNLPDoc: the parser\'s response hosted in CoreNLPDoc object\n        """"""\n        return self.model.parse(doc)\n'"
nlp_architect/api/intent_extraction_api.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport numpy as np\nimport pickle\nfrom os import makedirs, path, sys\n\nfrom nlp_architect.api.abstract_api import AbstractApi\nfrom nlp_architect.models.intent_extraction import MultiTaskIntentModel, Seq2SeqIntentModel\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.io import download_unlicensed_file\nfrom nlp_architect.utils.text import SpacyInstance, bio_to_spans\n\n\nclass IntentExtractionApi(AbstractApi):\n    model_dir = str(LIBRARY_OUT / ""intent-pretrained"")\n    pretrained_model_info = path.join(model_dir, ""model_info.dat"")\n    pretrained_model = path.join(model_dir, ""model.h5"")\n\n    def __init__(self, prompt=False):\n        self.model = None\n        self.model_type = None\n        self.word_vocab = None\n        self.tags_vocab = None\n        self.char_vocab = None\n        self.intent_vocab = None\n        self._download_pretrained_model(prompt)\n        self.nlp = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n\n    def process_text(self, text):\n        input_text = "" "".join(text.strip().split())\n        return self.nlp.tokenize(input_text)\n\n    @staticmethod\n    def _prompt():\n        response = input(""\\nTo download \'{}\', please enter YES: "".format(""intent_extraction""))\n        res = response.lower().strip()\n        if res == ""yes"" or (len(res) == 1 and res == ""y""):\n            print(""Downloading {}..."".format(""ner""))\n            responded_yes = True\n        else:\n            print(""Download declined. Response received {} != YES|Y. "".format(res))\n            responded_yes = False\n        return responded_yes\n\n    @staticmethod\n    def _download_pretrained_model(prompt=True):\n        """"""Downloads the pre-trained BIST model if non-existent.""""""\n        model_info_exists = path.isfile(IntentExtractionApi.pretrained_model_info)\n        model_exists = path.isfile(IntentExtractionApi.pretrained_model)\n        if not model_exists or not model_info_exists:\n            print(\n                ""The pre-trained models to be downloaded for the intent extraction dataset ""\n                ""are licensed under Apache 2.0. By downloading, you accept the terms ""\n                ""and conditions provided by the license""\n            )\n            makedirs(IntentExtractionApi.model_dir, exist_ok=True)\n            if prompt is True:\n                agreed = IntentExtractionApi._prompt()\n                if agreed is False:\n                    sys.exit(0)\n            download_unlicensed_file(\n                ""https://s3-us-west-2.amazonaws.com/nlp-architect-data"" ""/models/intent/"",\n                ""model_info.dat"",\n                IntentExtractionApi.pretrained_model_info,\n            )\n            download_unlicensed_file(\n                ""https://s3-us-west-2.amazonaws.com/nlp-architect-data"" ""/models/intent/"",\n                ""model.h5"",\n                IntentExtractionApi.pretrained_model,\n            )\n            print(""Done."")\n\n    @staticmethod\n    def display_results(text_str, predictions, intent_type):\n        ret = {""annotation_set"": [], ""doc_text"": "" "".join([t for t in text_str])}\n        spans = []\n        available_tags = set()\n        for s, e, tag in bio_to_spans(text_str, predictions):\n            spans.append({""start"": s, ""end"": e, ""type"": tag})\n            available_tags.add(tag)\n        ret[""annotation_set""] = list(available_tags)\n        ret[""spans""] = spans\n        ret[""title""] = intent_type\n        return {""doc"": ret, ""type"": ""high_level""}\n\n    def vectorize(self, doc, vocab, char_vocab=None):\n        words = np.asarray([vocab[w.lower()] if w.lower() in vocab else 1 for w in doc]).reshape(\n            1, -1\n        )\n        if char_vocab is not None:\n            sentence_chars = []\n            for w in doc:\n                word_chars = []\n                for c in w:\n                    if c in char_vocab:\n                        _cid = char_vocab[c]\n                    else:\n                        _cid = 1\n                    word_chars.append(_cid)\n                sentence_chars.append(word_chars)\n            sentence_chars = np.expand_dims(\n                pad_sentences(sentence_chars, self.model.word_length), axis=0\n            )\n            return [words, sentence_chars]\n        return words\n\n    def inference(self, doc):\n        text_arr = self.process_text(doc)\n        intent_type = None\n        if self.model_type == ""mtl"":\n            doc_vec = self.vectorize(text_arr, self.word_vocab, self.char_vocab)\n            intent, tags = self.model.predict(doc_vec, batch_size=1)\n            intent = int(intent.argmax(1).flatten())\n            intent_type = self.intent_vocab.get(intent, None)\n            print(""Detected intent type: {}"".format(intent_type))\n        else:\n            doc_vec = self.vectorize(text_arr, self.word_vocab, None)\n            tags = self.model.predict(doc_vec, batch_size=1)\n        tags = tags.argmax(2).flatten()\n        tag_str = [self.tags_vocab.get(n, None) for n in tags]\n        for t, n in zip(text_arr, tag_str):\n            print(""{}\\t{}\\t"".format(t, n))\n        return self.display_results(text_arr, tag_str, intent_type)\n\n    def load_model(self):\n        with open(IntentExtractionApi.pretrained_model_info, ""rb"") as fp:\n            model_info = pickle.load(fp)\n        self.model_type = model_info[""type""]\n        self.word_vocab = model_info[""word_vocab""]\n        self.tags_vocab = {v: k for k, v in model_info[""tags_vocab""].items()}\n        if self.model_type == ""mtl"":\n            self.char_vocab = model_info[""char_vocab""]\n            self.intent_vocab = {v: k for k, v in model_info[""intent_vocab""].items()}\n            model = MultiTaskIntentModel()\n        else:\n            model = Seq2SeqIntentModel()\n        model.load(self.pretrained_model)\n        self.model = model\n'"
nlp_architect/api/ner_api.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport pickle\nfrom os import makedirs, path, sys\n\nimport numpy as np\n\nfrom nlp_architect.api.abstract_api import AbstractApi\nfrom nlp_architect.models.ner_crf import NERCRF\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.io import download_unlicensed_file\nfrom nlp_architect.utils.text import SpacyInstance, bio_to_spans\n\n\nclass NerApi(AbstractApi):\n    """"""\n    NER model API\n    """"""\n\n    model_dir = str(LIBRARY_OUT / ""ner-pretrained"")\n    pretrained_model = path.join(model_dir, ""model_v4.h5"")\n    pretrained_model_info = path.join(model_dir, ""model_info_v4.dat"")\n\n    def __init__(self, prompt=False):\n        self.model = None\n        self.model_info = None\n        self.word_vocab = None\n        self.y_vocab = None\n        self.char_vocab = None\n        self._download_pretrained_model(prompt)\n        self.nlp = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n\n    @staticmethod\n    def _prompt():\n        response = input(""\\nTo download \'{}\', please enter YES: "".format(""ner""))\n        res = response.lower().strip()\n        if res == ""yes"" or (len(res) == 1 and res == ""y""):\n            print(""Downloading {}..."".format(""ner""))\n            responded_yes = True\n        else:\n            print(""Download declined. Response received {} != YES|Y. "".format(res))\n            responded_yes = False\n        return responded_yes\n\n    def _download_pretrained_model(self, prompt=True):\n        """"""Downloads the pre-trained BIST model if non-existent.""""""\n        model_exists = path.isfile(self.pretrained_model)\n        model_info_exists = path.isfile(self.pretrained_model_info)\n        if not model_exists or not model_info_exists:\n            print(\n                ""The pre-trained models to be downloaded for the NER dataset ""\n                ""are licensed under Apache 2.0. By downloading, you accept the terms ""\n                ""and conditions provided by the license""\n            )\n            makedirs(self.model_dir, exist_ok=True)\n            if prompt is True:\n                agreed = NerApi._prompt()\n                if agreed is False:\n                    sys.exit(0)\n            download_unlicensed_file(\n                ""https://s3-us-west-2.amazonaws.com/nlp-architect-data"" ""/models/ner/"",\n                ""model_v4.h5"",\n                self.pretrained_model,\n            )\n            download_unlicensed_file(\n                ""https://s3-us-west-2.amazonaws.com/nlp-architect-data"" ""/models/ner/"",\n                ""model_info_v4.dat"",\n                self.pretrained_model_info,\n            )\n            print(""Done."")\n\n    def load_model(self):\n        self.model = NERCRF()\n        self.model.load(self.pretrained_model)\n        with open(self.pretrained_model_info, ""rb"") as fp:\n            model_info = pickle.load(fp)\n        self.word_vocab = model_info[""word_vocab""]\n        self.y_vocab = {v: k for k, v in model_info[""y_vocab""].items()}\n        self.char_vocab = model_info[""char_vocab""]\n\n    @staticmethod\n    def pretty_print(text, tags):\n        spans = []\n        for s, e, tag in bio_to_spans(text, tags):\n            spans.append({""start"": s, ""end"": e, ""type"": tag})\n        ents = dict((obj[""type""].lower(), obj) for obj in spans).keys()\n        ret = {\n            ""doc_text"": "" "".join(text),\n            ""annotation_set"": list(ents),\n            ""spans"": spans,\n            ""title"": ""None"",\n        }\n        print({""doc"": ret, ""type"": ""high_level""})\n        return {""doc"": ret, ""type"": ""high_level""}\n\n    def process_text(self, text):\n        input_text = "" "".join(text.strip().split())\n        return self.nlp.tokenize(input_text)\n\n    def vectorize(self, doc, vocab, char_vocab):\n        words = np.asarray([vocab[w.lower()] if w.lower() in vocab else 1 for w in doc]).reshape(\n            1, -1\n        )\n        sentence_chars = []\n        for w in doc:\n            word_chars = []\n            for c in w:\n                if c in char_vocab:\n                    _cid = char_vocab[c]\n                else:\n                    _cid = 1\n                word_chars.append(_cid)\n            sentence_chars.append(word_chars)\n        sentence_chars = np.expand_dims(\n            pad_sentences(sentence_chars, self.model.word_length), axis=0\n        )\n        return words, sentence_chars\n\n    def inference(self, doc):\n        text_arr = self.process_text(doc)\n        doc_vec = self.vectorize(text_arr, self.word_vocab, self.char_vocab)\n        seq_len = np.array([len(text_arr)]).reshape(-1, 1)\n        inputs = list(doc_vec)\n        # pylint: disable=no-member\n        inputs = list(doc_vec) + [seq_len]\n        doc_ner = self.model.predict(inputs, batch_size=1).argmax(2).flatten()\n        tags = [self.y_vocab.get(n, None) for n in doc_ner]\n        return self.pretty_print(text_arr, tags)\n'"
nlp_architect/cli/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport logging\n\n# register all procedures by importing\nimport nlp_architect.procedures  # noqa: F401\nfrom nlp_architect.cli.cmd_registry import CMD_REGISTRY\nfrom nlp_architect.version import NLP_ARCHITECT_VERSION\n\nlogging.basicConfig(format=""%(asctime)s : %(levelname)s : %(message)s"", level=logging.INFO)\n\n\ndef nlp_train_cli():\n    prog_name = ""nlp-train""\n    desc = ""NLP Architect Train CLI [{}]"".format(NLP_ARCHITECT_VERSION)\n    parser = argparse.ArgumentParser(description=desc, prog=prog_name)\n    parser.add_argument(\n        ""-v"", ""--version"", action=""version"", version=""%(prog)s v{}"".format(NLP_ARCHITECT_VERSION)\n    )\n    parser.set_defaults(func=lambda _: parser.print_help())\n    subparsers = parser.add_subparsers(title=""Models"", metavar="""")\n    for model in CMD_REGISTRY[""train""]:\n        sp = subparsers.add_parser(\n            model[""name""], description=model[""description""], help=model[""description""]\n        )\n        model[""arg_adder""](sp)\n        sp.set_defaults(func=model[""fn""])\n\n    args = parser.parse_args()\n    if hasattr(args, ""func""):\n        args.func(args)\n    else:\n        parser.print_help()\n\n\ndef nlp_inference_cli():\n    prog_name = ""nlp-inference""\n    desc = ""NLP Architect Inference CLI [{}]"".format(NLP_ARCHITECT_VERSION)\n    parser = argparse.ArgumentParser(description=desc, prog=prog_name)\n    parser.add_argument(\n        ""-v"", ""--version"", action=""version"", version=""%(prog)s v{}"".format(NLP_ARCHITECT_VERSION)\n    )\n    parser.set_defaults(func=lambda _: parser.print_help())\n    subparsers = parser.add_subparsers(title=""Models"", metavar="""")\n    for model in CMD_REGISTRY[""inference""]:\n        sp = subparsers.add_parser(\n            model[""name""], description=model[""description""], help=model[""description""]\n        )\n        model[""arg_adder""](sp)\n        sp.set_defaults(func=model[""fn""])\n\n    args = parser.parse_args()\n    if hasattr(args, ""func""):\n        args.func(args)\n    else:\n        parser.print_help()\n'"
nlp_architect/cli/cmd_registry.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nCMD_REGISTRY = {\n    ""train"": [],\n    ""inference"": [],\n}\n'"
nlp_architect/common/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# flake8: noqa\nfrom .config import Config\n'"
nlp_architect/common/config.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nGeneric config object:\n    load config from json file\n    load config from ordinary python dict\n    export config as dictionaty or json string\n    define in init default parameters\n""""""\n\nimport copy\nimport json\nimport abc\n\n\nclass Config(abc.ABC):\n    """"""Quantization Configuration Object""""""\n\n    ATTRIBUTES = {}\n\n    def __init__(self, **kwargs):\n        for entry in self.ATTRIBUTES:\n            setattr(self, entry, kwargs.pop(entry, self.ATTRIBUTES[entry]))\n        if kwargs:\n            raise TypeError(f""got an unexpected keyword argument: {list(kwargs.keys())}"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a config from a Python dictionary of parameters.""""""\n        config = cls()\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs Config from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=""utf-8"") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n'"
nlp_architect/common/core_nlp_doc.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport json\n\n\ndef merge_punct_tok(merged_punct_sentence, last_merged_punct_index, punct_text, is_traverse):\n    # merge the text of the punct tok\n    if is_traverse:\n        merged_punct_sentence[last_merged_punct_index][""text""] = (\n            punct_text + merged_punct_sentence[last_merged_punct_index][""text""]\n        )\n    else:\n        merged_punct_sentence[last_merged_punct_index][""text""] = (\n            merged_punct_sentence[last_merged_punct_index][""text""] + punct_text\n        )\n\n\ndef find_correct_index(orig_gov, merged_punct_sentence):\n    for tok_index, tok in enumerate(merged_punct_sentence):\n        if (\n            tok[""start""] == orig_gov[""start""]\n            and tok[""len""] == orig_gov[""len""]\n            and tok[""pos""] == orig_gov[""pos""]\n            and tok[""text""] == orig_gov[""text""]\n        ):\n            return tok_index\n    return None\n\n\ndef fix_gov_indexes(merged_punct_sentence, sentence):\n    for merged_token in merged_punct_sentence:\n        tok_gov = merged_token[""gov""]\n        if tok_gov == -1:  # gov is root\n            merged_token[""gov""] = -1\n        else:\n            orig_gov = sentence[tok_gov]\n            correct_index = find_correct_index(orig_gov, merged_punct_sentence)\n            merged_token[""gov""] = correct_index\n\n\ndef merge_punctuation(sentence):\n    merged_punct_sentence = []\n    tmp_punct_text = None\n    punct_text = None\n    last_merged_punct_index = -1\n    for tok_index, token in enumerate(sentence):\n        if token[""rel""] == ""punct"":\n            punct_text = token[""text""]\n            if tok_index < 1:  # this is the first tok - append to the next token\n                tmp_punct_text = punct_text\n            else:  # append to the previous token\n                merge_punct_tok(merged_punct_sentence, last_merged_punct_index, punct_text, False)\n        else:\n            merged_punct_sentence.append(token)\n            last_merged_punct_index = last_merged_punct_index + 1\n            if tmp_punct_text is not None:\n                merge_punct_tok(merged_punct_sentence, last_merged_punct_index, punct_text, True)\n                tmp_punct_text = None\n    return merged_punct_sentence\n\n\nclass CoreNLPDoc(object):\n    """"""Object for core-components (POS, Dependency Relations, etc).\n\n    Attributes:\n        _doc_text: the doc text\n        _sentences: list of sentences, each word in a sentence is\n            represented by a dictionary, structured as follows: {\'start\': (int), \'len\': (int),\n            \'pos\': (str), \'ner\': (str), \'lemma\': (str), \'gov\': (int), \'rel\': (str)}\n    """"""\n\n    def __init__(self, doc_text: str = """", sentences: list = None):\n        if sentences is None:\n            sentences = []\n        self._doc_text = doc_text\n        self._sentences = sentences\n\n    @property\n    def doc_text(self):\n        return self._doc_text\n\n    @doc_text.setter\n    def doc_text(self, val):\n        self._doc_text = val\n\n    @property\n    def sentences(self):\n        return self._sentences\n\n    @sentences.setter\n    def sentences(self, val):\n        self._sentences = val\n\n    @staticmethod\n    def decoder(obj):\n        if ""_doc_text"" in obj and ""_sentences"" in obj:\n            return CoreNLPDoc(obj[""_doc_text""], obj[""_sentences""])\n        return obj\n\n    def __repr__(self):\n        return self.pretty_json()\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __iter__(self):\n        return self.sentences.__iter__()\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def json(self):\n        """"""Returns json representations of the object.""""""\n        return json.dumps(self.__dict__)\n\n    def pretty_json(self):\n        """"""Returns pretty json representations of the object.""""""\n        return json.dumps(self.__dict__, indent=4)\n\n    def sent_text(self, i):\n        parsed_sent = self.sentences[i]\n        first_tok, last_tok = parsed_sent[0], parsed_sent[-1]\n        return self.doc_text[first_tok[""start""] : last_tok[""start""] + last_tok[""len""]]\n\n    def sent_iter(self):\n        for parsed_sent in self.sentences:\n            first_tok, last_tok = parsed_sent[0], parsed_sent[-1]\n            sent_text = self.doc_text[first_tok[""start""] : last_tok[""start""] + last_tok[""len""]]\n            yield sent_text, parsed_sent\n\n    def brat_doc(self):\n        """"""Returns doc adapted to BRAT expected input.""""""\n        doc = {""text"": """", ""entities"": [], ""relations"": []}\n        tok_count = 0\n        rel_count = 1\n        for sentence in self.sentences:\n            sentence_start = sentence[0][""start""]\n            sentence_end = sentence[-1][""start""] + sentence[-1][""len""]\n            doc[""text""] = doc[""text""] + ""\\n"" + self.doc_text[sentence_start:sentence_end]\n            token_offset = tok_count\n\n            for token in sentence:\n                start = token[""start""]\n                end = start + token[""len""]\n                doc[""entities""].append([""T"" + str(tok_count), token[""pos""], [[start, end]]])\n\n                if token[""gov""] != -1 and token[""rel""] != ""punct"":\n                    doc[""relations""].append(\n                        [\n                            rel_count,\n                            token[""rel""],\n                            [\n                                ["""", ""T"" + str(token_offset + token[""gov""])],\n                                ["""", ""T"" + str(tok_count)],\n                            ],\n                        ]\n                    )\n                    rel_count += 1\n                tok_count += 1\n        doc[""text""] = doc[""text""][1:]\n        return doc\n\n    def displacy_doc(self):\n        """"""Return doc adapted to displacyENT expected input.""""""\n        doc = []\n        for sentence in self.sentences:\n            sentence_doc = {""arcs"": [], ""words"": []}\n            # Merge punctuation:\n            merged_punct_sentence = merge_punctuation(sentence)\n            fix_gov_indexes(merged_punct_sentence, sentence)\n            for tok_index, token in enumerate(merged_punct_sentence):\n                sentence_doc[""words""].append({""text"": token[""text""], ""tag"": token[""pos""]})\n                dep_tok = tok_index\n                gov_tok = token[""gov""]\n                direction = ""left""\n                arc_start = dep_tok\n                arc_end = gov_tok\n                if dep_tok > gov_tok:\n                    direction = ""right""\n                    arc_start = gov_tok\n                    arc_end = dep_tok\n                if token[""gov""] != -1 and token[""rel""] != ""punct"":\n                    sentence_doc[""arcs""].append(\n                        {\n                            ""dir"": direction,\n                            ""label"": token[""rel""],\n                            ""start"": arc_start,\n                            ""end"": arc_end,\n                        }\n                    )\n            doc.append(sentence_doc)\n        return doc\n'"
nlp_architect/common/high_level_doc.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import unicode_literals, print_function, division, absolute_import\nimport json\n\n\nclass HighLevelDoc:\n    """"""\n    object for annotation documents\n\n    Args:\n        self.doc_text (str): document text\n        self.annotation_set (list(str)): list of all annotations in doc\n        self.spans (list(dict)): list of span dict, each span_dict is structured as follows:\n            { \'end\': (int), \'start\': (int), \'type\': (str) string of annotation }\n    """"""\n\n    def __init__(self):\n        self.doc_text = None\n        self.annotation_set = []\n        self.spans = []\n\n    def json(self):\n        """"""\n        Return json representations of the object\n\n        Returns:\n            :obj:`json`: json representations of the object\n        """"""\n        return json.dumps(self.__dict__)\n\n    def pretty_json(self):\n        """"""\n        Return pretty json representations of the object\n\n        Returns:\n            :obj:`json`: pretty json representations of the object\n        """"""\n        return json.dumps(self.__dict__, indent=4)\n\n    def displacy_doc(self):  # only change annotations to lowercase\n        """"""\n        Return doc adapted to displacyENT expected input\n        """"""\n        self.annotation_set = [annotation.lower() for annotation in self.annotation_set]\n        return self.__dict__\n'"
nlp_architect/data/__init__.py,0,b''
nlp_architect/data/conll.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport re\n\n\n# This file contains adapted open sourced code, publicly available at:\n# https://github.com/elikip/bist-parser/blob/master/bmstparser/src/utils.py\n\n# Things that were changed from the original:\n# 1) Added input validation\n# 2) Updated function and object names to dyNet 2.0.2 and Python 3\n# 3) Removed external embeddings option\n# 4) Reformatted code and variable names to conform with PEP8\n# 5) Added dict_to_obj()\n# 6) Added option for train() to get ConllEntry input\n# 7) Added legal header\n\nNUMBER_REGEX = re.compile(""[0-9]+|[0-9]+\\\\.[0-9]+|[0-9]+[0-9,]+"")\n\n\nclass ConllEntry:\n    def __init__(\n        self,\n        eid,\n        form,\n        lemma,\n        pos,\n        cpos,\n        feats=None,\n        parent_id=None,\n        relation=None,\n        deps=None,\n        misc=None,\n    ):\n        self.id = eid\n        self.form = form\n        self.norm = normalize(form)\n        self.cpos = cpos.upper()\n        self.pos = pos.upper()\n        self.parent_id = parent_id\n        self.relation = relation\n\n        self.lemma = lemma\n        self.feats = feats\n        self.deps = deps\n        self.misc = misc\n\n        self.pred_parent_id = None\n        self.pred_relation = None\n\n        self.vec = None\n        self.lstms = None\n\n    def __str__(self):\n        values = [\n            str(self.id),\n            self.form,\n            self.lemma,\n            self.cpos,\n            self.pos,\n            self.feats,\n            str(self.pred_parent_id) if self.pred_parent_id is not None else None,\n            self.pred_relation,\n            self.deps,\n            self.misc,\n        ]\n        return ""\\t"".join([""_"" if v is None else v for v in values])\n\n\ndef normalize(word):\n    return ""NUM"" if NUMBER_REGEX.match(word) else word.lower()\n'"
nlp_architect/data/fasttext_emb.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport os\nfrom six.moves import urllib\nimport numpy as np\nfrom nlp_architect.utils.generic import license_prompt\n\n\nclass FastTextEmb:\n    """"""\n    Downloads FastText Embeddings for a given language to the given path.\n    Arguments:\n        path(str): Local path to copy embeddings\n        language(str): Embeddings language\n        vocab_size(int): Size of vocabulary\n    Returns:\n        Returns a dictionary and reverse dictionary\n        Returns a numpy array with embeddings in emb_sizexvocab_size shape\n    """"""\n\n    def __init__(self, path, language, vocab_size, emb_dim=300):\n        self.path = path\n        self.language = language\n        self.vocab_size = vocab_size\n        self.emb_dim = emb_dim\n        self.url = ""https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki."" + language + "".vec""\n\n    def _maybe_download(self):\n        """"""\n        Download filename from url unless it\'s already in directory\n        """"""\n        # 1. Check if the file doesnt exist. Download and extract if it doesnt\n        filename = ""wiki."" + self.language + "".vec""\n        filepath = os.path.join(self.path, filename)\n        link = ""https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md""\n        if not os.path.exists(filepath):\n            if license_prompt(filepath, link, self.path):\n                print(""Downloading FastText embeddings for "" + self.language + "" to "" + filepath)\n                urllib.request.urlretrieve(self.url, filepath)\n                statinfo = os.stat(filepath)\n                print(""Sucessfully downloaded"", filename, statinfo.st_size, ""bytes"")\n            else:\n                exit()\n        else:\n            print(""Found FastText embeddings for "" + self.language + "" at "" + filepath)\n        return filepath\n\n    def read_embeddings(self, filepath):\n        word2id = {}\n        word_vec = []\n        with open(filepath) as emb_file:\n            for i, line in enumerate(emb_file):\n                # Line zero has total words, emb dimensions\n                if i == 0:\n                    split_line = line.split()\n                    assert len(split_line) == 2\n                    assert self.emb_dim == int(split_line[1])\n                # Rest of line are word, word_vec format\n                else:\n                    word, vector = line.rstrip().split("" "", 1)\n                    vector = np.fromstring(vector, sep="" "")\n                    # If norm is zero fill with 0.01\n                    if np.linalg.norm(vector) == 0:\n                        vector[0] = 0.01\n                    assert vector.shape == (self.emb_dim,), i\n                    # Assign a token\n                    word2id[word] = len(word2id)\n                    word_vec.append(vector[None])\n                # Check if your reached goal of vocab_size\n                if i >= self.vocab_size:\n                    break\n        # Reverse dictionary\n        id2word = {v: k for k, v in word2id.items()}\n        # Dictionary just combines both id2word and word2id into one dict\n        dico = Dictionary(id2word, word2id, self.language)\n        # All word_vectors\n        word_vec = np.concatenate(word_vec, 0)\n        # Normalize the embeddings\n        return dico, word_vec\n\n    def load_embeddings(self):\n        # Check if embeddings exist else download\n        filepath = self._maybe_download()\n        # Read embeddings\n        dico, word_vec = self.read_embeddings(filepath)\n        print(""Completed loading embeddings for "" + self.language)\n        word_vec = np.float32(word_vec)\n        return dico, word_vec\n\n\ndef get_eval_data(eval_path, src_lang, tgt_lang):\n    """"""\n    Downloads evaluation cross lingual dictionaries to the eval_path\n    Arguments:\n        eval_path: Path where cross-lingual dictionaries are downloaded\n        src_lang : Source Language\n        tgt_lang : Target Language\n    Returns:\n        Path to where cross lingual dictionaries are downloaded\n    """"""\n    eval_url = ""https://s3.amazonaws.com/arrival/dictionaries/""\n    link = ""https://github.com/facebookresearch/MUSE#ground-truth-bilingual-dictionaries""\n    src_path = os.path.join(eval_path, ""%s-%s.5000-6500.txt"" % (src_lang, tgt_lang))\n    filename = src_lang + ""-"" + tgt_lang + "".5000-6500.txt""\n    if not os.path.exists(src_path):\n        if license_prompt(src_path, link, src_path):\n            os.system(""mkdir -p "" + eval_path)\n            print(""Downloading cross-lingual dictionaries for "" + src_lang)\n            urllib.request.urlretrieve(eval_url + filename, src_path)\n            print(""Completed downloading to "" + eval_path)\n        else:\n            exit()\n    return src_path\n\n\nclass Dictionary:\n    """"""\n    Merges word2idx and idx2word dictionaries\n    Arguments:\n        id2word dictionary\n        word2id dictionary\n        language of the dictionary\n    Usage:\n        dico.index(word) - returns an index\n        dico[index] - returns the word\n    """"""\n\n    def __init__(self, id2word, word2id, lang):\n        assert len(id2word) == len(word2id)\n        self.id2word = id2word\n        self.word2id = word2id\n        self.lang = lang\n        self.check_valid()\n\n    def __len__(self):\n        """"""\n        Returns the number of words in the dictionary.\n        """"""\n        return len(self.id2word)\n\n    def __getitem__(self, i):\n        """"""\n        Returns the word of the specified index.\n        """"""\n        return self.id2word[i]\n\n    def __contains__(self, w):\n        """"""\n        Returns whether a word is in the dictionary.\n        """"""\n        return w in self.word2id\n\n    def __eq__(self, y):\n        """"""\n        Compare the dictionary with another one.\n        """"""\n        self.check_valid()\n        y.check_valid()\n        if len(self.id2word) != len(y):\n            return False\n        return self.lang == y.lang and all(self.id2word[i] == y[i] for i in range(len(y)))\n\n    def check_valid(self):\n        """"""\n        Check that the dictionary is valid.\n        """"""\n        assert len(self.id2word) == len(self.word2id)\n        for i in range(len(self.id2word)):\n            assert self.word2id[self.id2word[i]] == i\n\n    def index(self, word):\n        """"""\n        Returns the index of the specified word.\n        """"""\n        return self.word2id[word]\n'"
nlp_architect/data/glue_tasks.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nimport os\n\nfrom sklearn.metrics import matthews_corrcoef\n\nfrom nlp_architect.data.sequence_classification import SequenceClsInputExample\nfrom nlp_architect.data.utils import DataProcessor, Task, read_tsv\nfrom nlp_architect.utils.metrics import acc_and_f1, pearson_and_spearman, simple_accuracy\n\nlogger = logging.getLogger(__name__)\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n        self.valid_ids = valid_ids\n\n\nclass MrpcProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[3]\n            text_b = line[4]\n            if set_type in [""train"", ""dev""]:\n                label = line[0]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b, label=label)\n                )\n            else:\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b))\n        return examples\n\n\nclass MnliProcessor(DataProcessor):\n    """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")), ""dev_matched""\n        )\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            read_tsv(os.path.join(data_dir, ""test_matched.tsv"")), ""test_matched""\n        )\n\n    def get_labels(self):\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[8]\n            text_b = line[9]\n            if set_type in [""train"", ""dev_matched""]:\n                label = line[-1]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b, label=label)\n                )\n            else:\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b))\n        return examples\n\n\nclass MnliMismatchedProcessor(MnliProcessor):\n    """"""Processor for the MultiNLI Mismatched data set (GLUE version).""""""\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            read_tsv(os.path.join(data_dir, ""dev_mismatched.tsv"")), ""dev_matched""\n        )\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            read_tsv(os.path.join(data_dir, ""test_mismatched.tsv"")), ""test_mismatched""\n        )\n\n\nclass ColaProcessor(DataProcessor):\n    """"""Processor for the CoLA data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0 and set_type not in [""train"", ""dev""]:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            if set_type in [""train"", ""dev""]:\n                text_a = line[3]\n                label = line[1]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=None, label=label)\n                )\n            else:\n                text_a = line[1]\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a))\n        return examples\n\n\nclass Sst2Processor(DataProcessor):\n    """"""Processor for the SST-2 data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            if set_type in [""train"", ""dev""]:\n                text_a = line[0]\n                label = line[1]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=None, label=label)\n                )\n            else:\n                text_a = line[1]\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a))\n        return examples\n\n\nclass StsbProcessor(DataProcessor):\n    """"""Processor for the STS-B data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [None]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[7]\n            text_b = line[8]\n            if set_type in [""train"", ""dev""]:\n                label = line[-1]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b, label=label)\n                )\n            else:\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b))\n        return examples\n\n\nclass QqpProcessor(DataProcessor):\n    """"""Processor for the QQP data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            if set_type in [""train"", ""dev""]:\n                try:\n                    text_a = line[3]\n                    text_b = line[4]\n                    label = line[5]\n                except IndexError:\n                    continue\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b, label=label)\n                )\n            else:\n                try:\n                    text_a = line[1]\n                    text_b = line[2]\n                except IndexError:\n                    continue\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b))\n        return examples\n\n\nclass QnliProcessor(DataProcessor):\n    """"""Processor for the QNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [""entailment"", ""not_entailment""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            if set_type in [""train"", ""dev""]:\n                label = line[-1]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b, label=label)\n                )\n            else:\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b))\n        return examples\n\n\nclass RteProcessor(DataProcessor):\n    """"""Processor for the RTE data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [""entailment"", ""not_entailment""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            if set_type in [""train"", ""dev""]:\n                label = line[-1]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b, label=label)\n                )\n            else:\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b))\n        return examples\n\n\nclass WnliProcessor(DataProcessor):\n    """"""Processor for the WNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            if set_type in [""train"", ""dev""]:\n                label = line[-1]\n                examples.append(\n                    SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b, label=label)\n                )\n            else:\n                examples.append(SequenceClsInputExample(guid=guid, text=text_a, text_b=text_b))\n        return examples\n\n\ndef convert_examples_to_features(\n    examples,\n    label_list,\n    max_seq_length,\n    tokenizer,\n    output_mode,\n    cls_token_at_end=False,\n    pad_on_left=False,\n    cls_token=""[CLS]"",\n    sep_token=""[SEP]"",\n    pad_token=0,\n    sequence_a_segment_id=0,\n    sequence_b_segment_id=1,\n    cls_token_segment_id=1,\n    pad_token_segment_id=0,\n    mask_padding_with_zero=True,\n):\n    """""" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token\n        (0 for BERT, 2 for XLNet)\n    """"""\n\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(""Writing example {} of {}"".format(ex_index, len(examples)))\n\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[: (max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids:   0   0   0   0  0     0   0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = tokens_a + [sep_token]\n        segment_ids = [sequence_a_segment_id] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [sep_token]\n            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n\n        if cls_token_at_end:\n            tokens = tokens + [cls_token]\n            segment_ids = segment_ids + [cls_token_segment_id]\n        else:\n            tokens = [cls_token] + tokens\n            segment_ids = [cls_token_segment_id] + segment_ids\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding_length = max_seq_length - len(input_ids)\n        if pad_on_left:\n            input_ids = ([pad_token] * padding_length) + input_ids\n            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n        else:\n            input_ids = input_ids + ([pad_token] * padding_length)\n            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        if output_mode == ""classification"":\n            label_id = label_map[example.label]\n        elif output_mode == ""regression"":\n            label_id = float(example.label)\n        else:\n            raise KeyError(output_mode)\n\n        features.append(\n            InputFeatures(\n                input_ids=input_ids,\n                input_mask=input_mask,\n                segment_ids=segment_ids,\n                label_id=label_id,\n            )\n        )\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    """"""Truncates a sequence pair in place to the maximum length.""""""\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that\'s truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\nprocessors = {\n    ""cola"": ColaProcessor,\n    ""mnli"": MnliProcessor,\n    ""mnli-mm"": MnliMismatchedProcessor,\n    ""mrpc"": MrpcProcessor,\n    ""sst-2"": Sst2Processor,\n    ""sts-b"": StsbProcessor,\n    ""qqp"": QqpProcessor,\n    ""qnli"": QnliProcessor,\n    ""rte"": RteProcessor,\n    ""wnli"": WnliProcessor,\n}\n\noutput_modes = {\n    ""cola"": ""classification"",\n    ""mnli"": ""classification"",\n    ""mnli-mm"": ""classification"",\n    ""mrpc"": ""classification"",\n    ""sst-2"": ""classification"",\n    ""sts-b"": ""regression"",\n    ""qqp"": ""classification"",\n    ""qnli"": ""classification"",\n    ""rte"": ""classification"",\n    ""wnli"": ""classification"",\n}\n\nDEFAULT_FOLDER_NAMES = {\n    ""cola"": ""CoLA"",\n    ""sst"": ""SST-2"",\n    ""mrpc"": ""MRPC"",\n    ""stsb"": ""STS-B"",\n    ""qqp"": ""QQP"",\n    ""mnli"": ""MNLI"",\n    ""qnli"": ""QNLI"",\n    ""rte"": ""RTE"",\n    ""wnli"": ""WNLI"",\n    ""snli"": ""SNLI"",\n}\n\n\n# GLUE task metrics\ndef get_metric_fn(task_name):\n    if task_name == ""cola"":\n        return lambda p, l: {""mcc"": matthews_corrcoef(p, l)}\n    if task_name == ""sst-2"":\n        return lambda p, l: {""acc"": simple_accuracy(p, l)}\n    if task_name == ""mrpc"":\n        return acc_and_f1\n    if task_name == ""sts-b"":\n        return pearson_and_spearman\n    if task_name == ""qqp"":\n        return acc_and_f1\n    if task_name == ""mnli"":\n        return lambda p, l: {""acc"": simple_accuracy(p, l)}\n    if task_name == ""mnli-mm"":\n        return lambda p, l: {""acc"": simple_accuracy(p, l)}\n    if task_name == ""qnli"":\n        return lambda p, l: {""acc"": simple_accuracy(p, l)}\n    if task_name == ""rte"":\n        return lambda p, l: {""acc"": simple_accuracy(p, l)}\n    if task_name == ""wnli"":\n        return lambda p, l: {""acc"": simple_accuracy(p, l)}\n    raise KeyError(task_name)\n\n\ndef get_glue_task(task_name: str, data_dir: str = None):\n    """"""Return a GLUE task object\n    Args:\n        task_name (str): name of GLUE task\n        data_dir (str, optional): path to dataset, if not provided will be taken from\n            GLUE_DIR env. variable\n    """"""\n    task_name = task_name.lower()\n    if task_name not in processors:\n        raise ValueError(""Task not found: {}"".format(task_name))\n    task_processor = processors[task_name]()\n    if data_dir is None:\n        try:\n            data_dir = os.path.join(os.environ[""GLUE_DIR""], DEFAULT_FOLDER_NAMES[task_name])\n        except Exception:\n            data_dir = None\n    task_type = output_modes[task_name]\n    return Task(task_name, task_processor, data_dir, task_type)\n'"
nlp_architect/data/intent_datasets.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport os\nimport sys\n\nimport numpy as np\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.text import (\n    SpacyInstance,\n    Vocabulary,\n    character_vector_generator,\n    word_vector_generator,\n)\n\n\nclass IntentDataset(object):\n    """"""\n    Intent extraction dataset base class\n\n    Args:\n        sentence_length (int): max sentence length\n    """"""\n\n    def __init__(self, sentence_length=50, word_length=12):\n        self.data_dict = {}\n        self.vecs = {}\n        self.sentence_len = sentence_length\n        self.word_len = word_length\n\n        self._tokens_vocab = Vocabulary(2)\n        self._chars_vocab = Vocabulary(2)\n        self._tags_vocab = Vocabulary(1)\n        self._intents_vocab = Vocabulary()\n\n    def _load_data(self, train_set, test_set):\n        # vectorize\n        # add offset of 2 for PAD and OOV\n        train_size = len(train_set)\n        test_size = len(test_set)\n        texts, tags, intents = list(zip(*train_set + test_set))\n        text_vectors, self._tokens_vocab = word_vector_generator(texts, lower=True, start=1)\n        tag_vectors, self._tags_vocab = word_vector_generator(tags, lower=False, start=1)\n        chars_vectors, self._chars_vocab = character_vector_generator(texts, start=1)\n        i, self._intents_vocab = word_vector_generator([intents])\n        i = np.asarray(i[0])\n\n        text_vectors = pad_sentences(text_vectors, max_length=self.sentence_len)\n        tag_vectors = pad_sentences(tag_vectors, max_length=self.sentence_len)\n        chars_vectors = [pad_sentences(d, max_length=self.word_len) for d in chars_vectors]\n        zeros = np.zeros((len(chars_vectors), self.sentence_len, self.word_len))\n        for idx, d in enumerate(chars_vectors):\n            d = d[: self.sentence_len]\n            zeros[idx, : d.shape[0]] = d\n        chars_vectors = zeros.astype(dtype=np.int32)\n\n        self.vecs[""train""] = [\n            text_vectors[:train_size],\n            chars_vectors[:train_size],\n            i[:train_size],\n            tag_vectors[:train_size],\n        ]\n        self.vecs[""test""] = [\n            text_vectors[-test_size:],\n            chars_vectors[-test_size:],\n            i[-test_size:],\n            tag_vectors[-test_size:],\n        ]\n\n    @property\n    def word_vocab_size(self):\n        """"""int: vocabulary size""""""\n        return len(self._tokens_vocab) + 1\n\n    @property\n    def char_vocab_size(self):\n        """"""int: char vocabulary size""""""\n        return len(self._chars_vocab) + 1\n\n    @property\n    def label_vocab_size(self):\n        """"""int: label vocabulary size""""""\n        return len(self._tags_vocab) + 1\n\n    @property\n    def intent_size(self):\n        """"""int: intent label vocabulary size""""""\n        return len(self._intents_vocab)\n\n    @property\n    def word_vocab(self):\n        """"""dict: tokens vocabulary""""""\n        return self._tokens_vocab\n\n    @property\n    def char_vocab(self):\n        """"""dict: word character vocabulary""""""\n        return self._chars_vocab\n\n    @property\n    def tags_vocab(self):\n        """"""dict: labels vocabulary""""""\n        return self._tags_vocab\n\n    @property\n    def intents_vocab(self):\n        """"""dict: intent labels vocabulary""""""\n        return self._intents_vocab\n\n    @property\n    def train_set(self):\n        """""":obj:`tuple` of :obj:`numpy.ndarray`: train set""""""\n        return self.vecs[""train""]\n\n    @property\n    def test_set(self):\n        """""":obj:`tuple` of :obj:`numpy.ndarray`: test set""""""\n        return self.vecs[""test""]\n\n\nclass TabularIntentDataset(IntentDataset):\n    """"""\n    Tabular Intent/Slot tags dataset loader.\n    Compatible with many sequence tagging datasets (ATIS, CoNLL, etc..)\n    data format must be int tabular format where:\n    - one word per line with tag annotation and intent type separated\n    by tabs <token>\\t<tag_label>\\t<intent>\\n\n    - sentences are separated by an empty line\n\n    Args:\n        train_file (str): path to train set file\n        test_file (str): path to test set file\n        sentence_length (int): max sentence length\n        word_length (int): max word length\n    """"""\n\n    files = [""train"", ""test""]\n\n    def __init__(self, train_file, test_file, sentence_length=30, word_length=12):\n        train_set_raw, test_set_raw = self._load_dataset(train_file, test_file)\n        super(TabularIntentDataset, self).__init__(\n            sentence_length=sentence_length, word_length=word_length\n        )\n\n        self._load_data(train_set_raw, test_set_raw)\n\n    def _load_dataset(self, train_file, test_file):\n        """"""returns a tuple of train/test with 3-tuple of tokens, tags, intent_type""""""\n        train = self._parse_sentences(self._read_file(train_file))\n        test = self._parse_sentences(self._read_file(test_file))\n        return train, test\n\n    def _read_file(self, path):\n        with open(path, encoding=""utf-8"", errors=""ignore"") as fp:\n            data = fp.readlines()\n        return self._split_into_sentences(data)\n\n    @staticmethod\n    def _split_into_sentences(file_lines):\n        sents = []\n        s = []\n        for line in file_lines:\n            line = line.strip()\n            if not line:\n                sents.append(s)\n                s = []\n                continue\n            s.append(line)\n        if s:\n            sents.append(s)\n        return sents\n\n    @staticmethod\n    def _parse_sentences(sentences):\n        encoded_sentences = []\n        for sen in sentences:\n            tokens = []\n            tags = []\n            intent = None\n            for line in sen:\n                t, s, i = line.split()\n                tokens.append(t)\n                tags.append(s)\n                intent = i\n                if intent is None:\n                    intent = i\n            encoded_sentences.append((tokens, tags, intent))\n        return encoded_sentences\n\n\nclass SNIPS(IntentDataset):\n    """"""\n    SNIPS dataset class\n\n    Args:\n            path (str): dataset path\n            sentence_length (int, optional): max sentence length\n            word_length (int, optional): max word length\n    """"""\n\n    train_files = [\n        ""AddToPlaylist/train_AddToPlaylist_full.json"",\n        ""BookRestaurant/train_BookRestaurant_full.json"",\n        ""GetWeather/train_GetWeather_full.json"",\n        ""PlayMusic/train_PlayMusic_full.json"",\n        ""RateBook/train_RateBook_full.json"",\n        ""SearchCreativeWork/train_SearchCreativeWork_full.json"",\n        ""SearchScreeningEvent/train_SearchScreeningEvent_full.json"",\n    ]\n    test_files = [\n        ""AddToPlaylist/validate_AddToPlaylist.json"",\n        ""BookRestaurant/validate_BookRestaurant.json"",\n        ""GetWeather/validate_GetWeather.json"",\n        ""PlayMusic/validate_PlayMusic.json"",\n        ""RateBook/validate_RateBook.json"",\n        ""SearchCreativeWork/validate_SearchCreativeWork.json"",\n        ""SearchScreeningEvent/validate_SearchScreeningEvent.json"",\n    ]\n    files = [""train"", ""test""]\n\n    def __init__(self, path, sentence_length=30, word_length=12):\n        if path is None or not os.path.isdir(path):\n            print(""invalid path for SNIPS dataset loader"")\n            sys.exit(0)\n        self.dataset_root = path\n        train_set_raw, test_set_raw = self._load_dataset()\n        super(SNIPS, self).__init__(sentence_length=sentence_length, word_length=word_length)\n        self._load_data(train_set_raw, test_set_raw)\n\n    def _load_dataset(self):\n        """"""returns a tuple of train/test with 3-tuple of tokens, tags, intent_type""""""\n        train_data = self._load_intents(self.train_files)\n        test_data = self._load_intents(self.test_files)\n        train = [(t, l, i) for i in sorted(train_data) for t, l in train_data[i]]\n        test = [(t, l, i) for i in sorted(test_data) for t, l in test_data[i]]\n        return train, test\n\n    def _load_intents(self, files):\n        data = {}\n        for f in sorted(files):\n            fname = os.path.join(self.dataset_root, f)\n            intent = f.split(os.sep)[0]\n            with open(fname, encoding=""utf-8"", errors=""ignore"") as fp:\n                fdata = json.load(fp)\n            entries = self._parse_json([d[""data""] for d in fdata[intent]])\n            data[intent] = entries\n        return data\n\n    def _parse_json(self, data):\n        tok = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n        sentences = []\n        for s in data:\n            tokens = []\n            tags = []\n            for t in s:\n                new_tokens = tok.tokenize(t[""text""].strip())\n                tokens += new_tokens\n                ent = t.get(""entity"", None)\n                if ent is not None:\n                    tags += self._create_tags(ent, len(new_tokens))\n                else:\n                    tags += [""O""] * len(new_tokens)\n            sentences.append((tokens, tags))\n        return sentences\n\n    @staticmethod\n    def _create_tags(tag, length):\n        labels = [""B-"" + tag]\n        if length > 1:\n            for _ in range(length - 1):\n                labels.append(""I-"" + tag)\n        return labels\n'"
nlp_architect/data/ptb.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nData loader for penn tree bank dataset\n""""""\nimport os\nimport sys\nimport numpy as np\nimport urllib.request\n\nLICENSE_URL = {\n    ""PTB"": ""http://www.fit.vutbr.cz/~imikolov/rnnlm/"",\n    ""WikiText-103"": ""https://einstein.ai/research/the-wikitext-long-term-dependency-""\n    ""language-modeling-dataset"",\n}\n\nSOURCE_URL = {\n    ""PTB"": ""http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz"",\n    ""WikiText-103"": ""https://s3.amazonaws.com/research.metamind.io/wikitext/""\n    + ""wikitext-103-v1.zip"",\n}\nFILENAME = {""PTB"": ""simple-examples"", ""WikiText-103"": ""wikitext-103""}\nEXTENSION = {""PTB"": ""tgz"", ""WikiText-103"": ""zip""}\nFILES = {\n    ""PTB"": lambda x: ""data/ptb."" + x + "".txt"",\n    ""WikiText-103"": lambda x: ""wiki."" + x + "".tokens"",\n}\n\n\nclass PTBDictionary:\n    """"""\n    Class for generating a dictionary of all words in the PTB corpus\n    """"""\n\n    def __init__(self, data_dir=os.path.expanduser(""~/data""), dataset=""WikiText-103""):\n        """"""\n        Initialize class\n        Args:\n            data_dir: str, location of data\n            dataset: str, name of data corpus\n        """"""\n        self.data_dir = data_dir\n        self.dataset = dataset\n        self.filepath = os.path.join(data_dir, FILENAME[self.dataset])\n        self._maybe_download(data_dir)\n\n        self.word2idx = {}\n        self.idx2word = []\n\n        self.load_dictionary()\n        print(""Loaded dictionary of words of size {}"".format(len(self.idx2word)))\n        self.sos_symbol = self.word2idx[""<sos>""]\n        self.eos_symbol = self.word2idx[""<eos>""]\n        self.save_dictionary()\n\n    def add_word(self, word):\n        """"""\n        Method for adding a single word to the dictionary\n        Args:\n            word: str, word to be added\n\n        Returns:\n            None\n        """"""\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def load_dictionary(self):\n        """"""\n        Populate the corpus with words from train, test and valid splits of data\n        Returns:\n            None\n        """"""\n        for split_type in [""train"", ""test"", ""valid""]:\n            path = os.path.join(\n                self.data_dir, FILENAME[self.dataset], FILES[self.dataset](split_type)\n            )\n            # Add words to the dictionary\n            with open(path, ""r"") as fp:\n                tokens = 0\n                for line in fp:\n                    words = [""<sos>""] + line.split() + [""<eos>""]\n                    tokens += len(words)\n                    for word in words:\n                        self.add_word(word)\n\n    def save_dictionary(self):\n        """"""\n        Save dictionary to file\n        Returns:\n            None\n        """"""\n        with open(os.path.join(self.data_dir, ""dictionary.txt""), ""w"") as fp:\n            for k in self.word2idx:\n                fp.write(""%s,%d\\n"" % (k, self.word2idx[k]))\n\n    def _maybe_download(self, work_directory):\n        """"""\n        This function downloads the corpus if its not already present\n        Args:\n            work_directory: str, location to download data to\n        Returns:\n            None\n        """"""\n        if not os.path.exists(self.filepath):\n            print(\n                ""{} was not found in the directory: {}, looking for compressed version"".format(\n                    FILENAME[self.dataset], self.filepath\n                )\n            )\n            full_filepath = os.path.join(\n                work_directory, FILENAME[self.dataset] + ""."" + EXTENSION[self.dataset]\n            )\n            if not os.path.exists(full_filepath):\n                print(""Did not find data"")\n                print(\n                    ""PTB can be downloaded from http://www.fit.vutbr.cz/~imikolov/rnnlm/ \\n""\n                    ""wikitext can be downloaded from""\n                    "" https://einstein.ai/research/the-wikitext-long-term-dependency-language""\n                    ""-modeling-dataset""\n                )\n                print(\n                    ""\\nThe terms and conditions of the data set license apply. Intel does not ""\n                    ""grant any rights to the data files or database\\n""\n                )\n                response = input(\n                    ""\\nTo download data from {}, please enter YES: "".format(\n                        LICENSE_URL[self.dataset]\n                    )\n                )\n                res = response.lower().strip()\n                if res == ""yes"" or (len(res) == 1 and res == ""y""):\n                    print(""Downloading..."")\n                    self._download_data(work_directory)\n                    self._uncompress_data(work_directory)\n                else:\n                    print(""Download declined. Response received {} != YES|Y. "".format(res))\n                    print(\n                        ""Please download the model manually from the links above ""\n                        ""and place in directory: {}"".format(work_directory)\n                    )\n                    sys.exit()\n            else:\n                self._uncompress_data(work_directory)\n\n    def _download_data(self, work_directory):\n        """"""\n        This function downloads the corpus\n        Args:\n            work_directory: str, location to download data to\n        Returns:\n            None\n        """"""\n        work_directory = os.path.abspath(work_directory)\n        if not os.path.exists(work_directory):\n            os.mkdir(work_directory)\n\n        headers = {""User-Agent"": ""Mozilla/5.0""}\n\n        full_filepath = os.path.join(\n            work_directory, FILENAME[self.dataset] + ""."" + EXTENSION[self.dataset]\n        )\n        req = urllib.request.Request(SOURCE_URL[self.dataset], headers=headers)\n        data_handle = urllib.request.urlopen(req)\n        with open(full_filepath, ""wb"") as fp:\n            fp.write(data_handle.read())\n        print(""Successfully downloaded data to {}"".format(full_filepath))\n\n    def _uncompress_data(self, work_directory):\n        full_filepath = os.path.join(\n            work_directory, FILENAME[self.dataset] + ""."" + EXTENSION[self.dataset]\n        )\n        if EXTENSION[self.dataset] == ""tgz"":\n            import tarfile\n\n            with tarfile.open(full_filepath, ""r:gz"") as tar:\n                tar.extractall(path=work_directory)\n        if EXTENSION[self.dataset] == ""zip"":\n            import zipfile\n\n            with zipfile.ZipFile(full_filepath, ""r"") as zip_handle:\n                zip_handle.extractall(work_directory)\n\n        print(\n            ""Successfully unzipped data to {}"".format(\n                os.path.join(work_directory, FILENAME[self.dataset])\n            )\n        )\n\n\nclass PTBDataLoader:\n    """"""\n    Class that defines data loader\n    """"""\n\n    def __init__(\n        self,\n        word_dict,\n        seq_len=100,\n        data_dir=os.path.expanduser(""~/data""),\n        dataset=""WikiText-103"",\n        batch_size=32,\n        skip=30,\n        split_type=""train"",\n        loop=True,\n    ):\n        """"""\n        Initialize class\n        Args:\n            word_dict: PTBDictionary object\n            seq_len: int, sequence length of data\n            data_dir: str, location of corpus data\n            dataset: str, name of corpus\n            batch_size: int, batch size\n            skip: int, number of words to skip over while generating batches\n            split_type: str, train/test/valid\n            loop: boolean, whether or not to loop over data when it runs out\n        """"""\n        self.seq_len = seq_len\n        self.dataset = dataset\n\n        self.loop = loop\n        self.skip = skip\n\n        self.word2idx = word_dict.word2idx\n        self.idx2word = word_dict.idx2word\n\n        self.data = self.load_series(\n            os.path.join(data_dir, FILENAME[self.dataset], FILES[self.dataset](split_type))\n        )\n        self.random_index = np.random.permutation(\n            np.arange(0, self.data.shape[0] - self.seq_len, self.skip)\n        )\n        self.n_train = self.random_index.shape[0]\n\n        self.batch_size = batch_size\n        self.sample_count = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.get_batch()\n\n    def reset(self):\n        """"""\n        Resets the sample count to zero, re-shuffles data\n        Returns:\n            None\n        """"""\n        self.sample_count = 0\n        self.random_index = np.random.permutation(\n            np.arange(0, self.data.shape[0] - self.seq_len, self.skip)\n        )\n\n    def get_batch(self):\n        """"""\n        Get one batch of the data\n        Returns:\n            None\n        """"""\n        if self.sample_count + self.batch_size > self.n_train:\n            if self.loop:\n                self.reset()\n            else:\n                raise StopIteration(""Ran out of data"")\n\n        batch_x = []\n        batch_y = []\n        for _ in range(self.batch_size):\n            c_i = int(self.random_index[self.sample_count])\n            batch_x.append(self.data[c_i : c_i + self.seq_len])\n            batch_y.append(self.data[c_i + 1 : c_i + self.seq_len + 1])\n            self.sample_count += 1\n        batch = (np.array(batch_x), np.array(batch_y))\n\n        return batch\n\n    def load_series(self, path):\n        """"""\n        Load all the data into an array\n        Args:\n            path: str, location of the input data file\n\n        Returns:\n\n        """"""\n        # Tokenize file content\n        with open(path, ""r"") as fp:\n            ids = []\n            for line in fp:\n                words = line.split() + [""<eos>""]\n                for word in words:\n                    ids.append(self.word2idx[word])\n\n        data = np.array(ids)\n\n        return data\n\n    def decode_line(self, tokens):\n        """"""\n        Decode a given line from index to word\n        Args:\n            tokens: List of indexes\n\n        Returns:\n            str, a sentence\n        """"""\n        return "" "".join([self.idx2word[t] for t in tokens])\n'"
nlp_architect/data/sequence_classification.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import, division, print_function\n\nfrom nlp_architect.data.utils import InputExample\n\n\nclass SequenceClsInputExample(InputExample):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid: str, text: str, text_b: str = None, label: str = None):\n        """"""Constructs a SequenceClassInputExample.\n        Args:\n            guid: Unique id for the example.\n            text: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label/tags of the example.\n            This should be specified for train and dev examples, but not for test examples.\n        """"""\n        super(SequenceClsInputExample, self).__init__(guid, text, label)\n        self.text_b = text_b\n'"
nlp_architect/data/sequential_tagging.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nimport os\nfrom os import path\nfrom typing import List\n\nimport numpy as np\n\nfrom nlp_architect.data.utils import DataProcessor, InputExample, read_column_tagged_file\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.io import validate_existing_directory, validate_existing_filepath\nfrom nlp_architect.utils.text import (\n    character_vector_generator,\n    read_sequential_tagging_file,\n    word_vector_generator,\n)\nfrom nlp_architect.utils.text import Vocabulary\n\nlogger = logging.getLogger(__name__)\n\n\nclass SequentialTaggingDataset(object):\n    """"""\n    Sequential tagging dataset loader.\n    Loads train/test files with tabular separation.\n\n    Args:\n        train_file (str): path to train file\n        test_file (str): path to test file\n        max_sentence_length (int, optional): max sentence length\n        max_word_length (int, optional): max word length\n        tag_field_no (int, optional): index of column to use a y-samples\n    """"""\n\n    def __init__(\n        self, train_file, test_file, max_sentence_length=30, max_word_length=20, tag_field_no=2\n    ):\n        self.files = {""train"": train_file, ""test"": test_file}\n        self.max_sent_len = max_sentence_length\n        self.max_word_len = max_word_length\n        self.tf = tag_field_no\n\n        self.vocabs = {""token"": None, ""char"": None, ""tag"": None}  # 0=pad, 1=unk  # 0=pad\n\n        self.data = {}\n\n        sentences = self._read_file(self.files[""train""])\n        train_size = len(sentences)\n        sentences += self._read_file(self.files[""test""])\n        test_size = len(sentences) - train_size\n        texts, tags = list(zip(*sentences))\n\n        texts_mat, self.vocabs[""token""] = word_vector_generator(texts, lower=True, start=2)\n        tags_mat, self.vocabs[""tag""] = word_vector_generator(tags, start=1)\n        chars_mat, self.vocabs[""char""] = character_vector_generator(texts, start=2)\n\n        texts_mat = pad_sentences(texts_mat, max_length=self.max_sent_len)\n        tags_mat = pad_sentences(tags_mat, max_length=self.max_sent_len)\n\n        chars_mat = [pad_sentences(d, max_length=self.max_word_len) for d in chars_mat]\n        zeros = np.zeros((len(chars_mat), self.max_sent_len, self.max_word_len))\n        for idx, d in enumerate(chars_mat):\n            d = d[: self.max_sent_len]\n            zeros[idx, : d.shape[0]] = d\n        chars_mat = zeros.astype(dtype=np.int32)\n\n        self.data[""train""] = texts_mat[:train_size], chars_mat[:train_size], tags_mat[:train_size]\n        self.data[""test""] = texts_mat[-test_size:], chars_mat[-test_size:], tags_mat[-test_size:]\n\n    @property\n    def y_labels(self):\n        """"""return y labels""""""\n        return self.vocabs[""tag""]\n\n    @property\n    def word_vocab(self):\n        """"""words vocabulary""""""\n        return self.vocabs[""token""]\n\n    @property\n    def char_vocab(self):\n        """"""characters vocabulary""""""\n        return self.vocabs[""char""]\n\n    @property\n    def word_vocab_size(self):\n        """"""word vocabulary size""""""\n        return len(self.vocabs[""token""]) + 2\n\n    @property\n    def char_vocab_size(self):\n        """"""character vocabulary size""""""\n        return len(self.vocabs[""char""]) + 2\n\n    @property\n    def train_set(self):\n        """"""Get the train set""""""\n        return self.data[""train""]\n\n    @property\n    def test_set(self):\n        """"""Get the test set""""""\n        return self.data[""test""]\n\n    def _read_file(self, filepath):\n        with open(filepath, encoding=""utf-8"") as fp:\n            data = fp.readlines()\n            data = [d.strip() for d in data]\n            sentences = self._split_into_sentences(data)\n            parsed_sentences = [self._parse_sentence(s) for s in sentences if len(s) > 0]\n        return parsed_sentences\n\n    def _parse_sentence(self, sentence):\n        tokens = []\n        tags = []\n        for line in sentence:\n            fields = line.split()\n            assert len(fields) >= self.tf, ""tag field exceeds number of fields""\n            if ""CD"" in fields[1]:\n                tokens.append(""0"")\n            else:\n                tokens.append(fields[0])\n            tags.append(fields[self.tf - 1])\n        return tokens, tags\n\n    @staticmethod\n    def _split_into_sentences(file_lines):\n        sents = []\n        s = []\n        for line in file_lines:\n            line = line.strip()\n            if not line:\n                sents.append(s)\n                s = []\n                continue\n            s.append(line)\n        sents.append(s)\n        return sents\n\n\nclass CONLL2000(object):\n    """"""\n        CONLL 2000 POS/chunking task data set (numpy)\n\n        Arguments:\n            data_path (str): directory containing CONLL2000 files\n            sentence_length (int, optional): number of time steps to embed the data.\n                None value will not truncate vectors\n            max_word_length (int, optional): max word length in characters.\n                None value will not truncate vectors\n            extract_chars (boolean, optional): Yield Char RNN features.\n            lowercase (bool, optional): lower case sentence words\n        """"""\n\n    dataset_files = {""train"": ""train.txt"", ""test"": ""test.txt""}\n\n    def __init__(\n        self,\n        data_path,\n        sentence_length=None,\n        max_word_length=None,\n        extract_chars=False,\n        lowercase=True,\n    ):\n        self._validate_paths(data_path)\n        self.data_path = data_path\n        self.sentence_length = sentence_length\n        self.use_chars = extract_chars\n        self.max_word_length = max_word_length\n        self.lower = lowercase\n        self.vocabs = {""word"": None, ""char"": None, ""pos"": None, ""chunk"": None}\n        self._data_dict = {}\n\n    def _validate_paths(self, data_path):\n        validate_existing_directory(data_path)\n        for f in self.dataset_files:\n            _f_path = path.join(data_path, self.dataset_files[f])\n            validate_existing_filepath(_f_path)\n            self.dataset_files[f] = _f_path\n\n    def _load_data(self):\n        """"""\n        open files and parse\n        return format: list of 3-tuples (word list, POS list, chunk list)\n        """"""\n        train_set = read_sequential_tagging_file(self.dataset_files[""train""])\n        test_set = read_sequential_tagging_file(self.dataset_files[""test""])\n        train_data = [list(zip(*x)) for x in train_set]\n        test_data = [list(zip(*x)) for x in test_set]\n        return train_data, test_data\n\n    @property\n    def train_set(self):\n        """"""get the train set""""""\n        if self._data_dict.get(""train"", None) is None:\n            self._gen_data()\n        return self._data_dict.get(""train"")\n\n    @property\n    def test_set(self):\n        """"""get the test set""""""\n        if self._data_dict.get(""test"", None) is None:\n            self._gen_data()\n        return self._data_dict.get(""test"")\n\n    @staticmethod\n    def _extract(x, y, n):\n        return list(zip(*x))[n] + list(zip(*y))[n]\n\n    @property\n    def word_vocab(self):\n        """"""word Vocabulary""""""\n        return self.vocabs[""word""]\n\n    @property\n    def char_vocab(self):\n        """"""character Vocabulary""""""\n        return self.vocabs[""char""]\n\n    @property\n    def pos_vocab(self):\n        """"""pos label Vocabulary""""""\n        return self.vocabs[""pos""]\n\n    @property\n    def chunk_vocab(self):\n        """"""chunk label Vocabulary""""""\n        return self.vocabs[""chunk""]\n\n    def _gen_data(self):\n        train, test = self._load_data()\n        train_size = len(train)\n        test_size = len(test)\n        sentences = self._extract(train, test, 0)\n        pos_tags = self._extract(train, test, 1)\n        chunk_tags = self._extract(train, test, 2)\n        sentence_vecs, word_vocab = word_vector_generator(sentences, self.lower, 2)\n        pos_vecs, pos_vocab = word_vector_generator(pos_tags, start=1)\n        chunk_vecs, chunk_vocab = word_vector_generator(chunk_tags, start=1)\n        self.vocabs = {\n            ""word"": word_vocab,  # 0=pad, 1=unk\n            ""pos"": pos_vocab,  # 0=pad, 1=unk\n            ""chunk"": chunk_vocab,\n        }  # 0=pad\n        if self.sentence_length is not None:\n            sentence_vecs = pad_sentences(sentence_vecs, max_length=self.sentence_length)\n            chunk_vecs = pad_sentences(chunk_vecs, max_length=self.sentence_length)\n            pos_vecs = pad_sentences(pos_vecs, max_length=self.sentence_length)\n        self._data_dict[""train""] = (\n            sentence_vecs[:train_size],\n            pos_vecs[:train_size],\n            chunk_vecs[:train_size],\n        )\n        self._data_dict[""test""] = (\n            sentence_vecs[-test_size:],\n            pos_vecs[-test_size:],\n            chunk_vecs[-test_size:],\n        )\n        if self.use_chars:\n            chars_vecs, char_vocab = character_vector_generator(sentences, start=2)\n            self.vocabs.update({""char"": char_vocab})  # 0=pad, 1=unk\n            if self.max_word_length is not None:\n                chars_vecs = [pad_sentences(d, max_length=self.max_word_length) for d in chars_vecs]\n                zeros = np.zeros((len(chars_vecs), self.sentence_length, self.max_word_length))\n                for idx, d in enumerate(chars_vecs):\n                    d = d[: self.sentence_length]\n                    zeros[idx, -d.shape[0] :] = d\n                chars_vecs = zeros.astype(dtype=np.int32)\n            self._data_dict[""train""] += (chars_vecs[:train_size],)\n            self._data_dict[""test""] += (chars_vecs[-test_size:],)\n\n\nclass TokenClsInputExample(InputExample):\n    """"""A single training/test example for simple sequence token classification.""""""\n\n    def __init__(\n        self,\n        guid: str,\n        text: str,\n        tokens: List[str],\n        shapes: List[int] = None,\n        label: List[str] = None,\n    ):\n        """"""Constructs a SequenceClassInputExample.\n        Args:\n            guid: Unique id for the example.\n            text: string. The untokenized text of the sequence.\n            tokens (List[str]): The list of tokens.\n            shapes (List[str]): List of tokens shapes.\n            label (List[str], optional): The tags of the tokens.\n        """"""\n        super(TokenClsInputExample, self).__init__(guid, text, label)\n        self.tokens = tokens\n        self.shapes = shapes\n\n\nclass TokenClsProcessor(DataProcessor):\n    """"""Sequence token classification Processor dataset loader.\n    Loads a directory with train.txt/test.txt/dev.txt files in tab separeted\n    format (one token per line - conll style).\n    Label dictionary is given in labels.txt file.\n    """"""\n\n    def __init__(self, data_dir, tag_col: int = -1, ignore_token=None):\n        if not os.path.exists(data_dir):\n            raise FileNotFoundError\n        self.data_dir = data_dir\n        self.tag_col = tag_col\n        self.labels = None\n        self.ignore_token = ignore_token\n\n    def _read_examples(self, data_dir, file_name, set_name):\n        if not os.path.exists(data_dir + os.sep + file_name):\n            logger.error(\n                ""Requested file {} in path {} for TokenClsProcess not found"".format(\n                    file_name, data_dir\n                )\n            )\n            return None\n        return self._create_examples(\n            read_column_tagged_file(\n                os.path.join(data_dir, file_name),\n                tag_col=self.tag_col,\n                ignore_token=self.ignore_token,\n            ),\n            set_name,\n        )\n\n    def get_train_examples(self, filename=""train.txt""):\n        return self._read_examples(self.data_dir, filename, ""train"")\n\n    def get_dev_examples(self, filename=""dev.txt""):\n        return self._read_examples(self.data_dir, filename, ""dev"")\n\n    def get_test_examples(self, filename=""test.txt""):\n        return self._read_examples(self.data_dir, filename, ""test"")\n\n    # pylint: disable=arguments-differ\n    def get_labels(self):\n        if self.labels is not None:\n            return self.labels\n\n        f_path = self.data_dir + os.sep + ""labels.txt""\n        if not os.path.exists(f_path):\n            logger.error(""Labels file (labels.txt) not found in {}"".format(self.data_dir))\n            raise FileNotFoundError\n\n        self.labels = []\n        with open(f_path, encoding=""utf-8"") as fp:\n            self.labels = [line.strip() for line in fp.readlines()]\n\n        return self.labels\n\n    @staticmethod\n    def get_labels_filename():\n        return ""labels.txt""\n\n    @staticmethod\n    def _get_shape(string):\n        if all(c.isupper() for c in string):\n            return 1  # ""AA""\n        if string[0].isupper():\n            return 2  # ""Aa""\n        if any(c for c in string if c.isupper()):\n            return 3  # ""aAa""\n        return 4  # ""a""\n\n    @classmethod\n    def _create_examples(cls, lines, set_type):\n        """"""See base class.""""""\n        examples = []\n        for i, (sentence, labels) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text = "" "".join(sentence)\n            shapes = [cls._get_shape(w) for w in sentence]\n            examples.append(\n                TokenClsInputExample(\n                    guid=guid, text=text, tokens=sentence, label=labels, shapes=shapes\n                )\n            )\n        return examples\n\n    def get_vocabulary(self, examples: TokenClsInputExample = None):\n        vocab = Vocabulary(start=1)\n        for e in examples:\n            for t in e.tokens:\n                vocab.add(t)\n        return vocab\n'"
nlp_architect/data/utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\nimport random\nimport sys\nfrom abc import ABC\nfrom io import open\nfrom typing import List, Tuple\n\n\nclass InputExample(ABC):\n    """"""Base class for a single training/dev/test example """"""\n\n    def __init__(self, guid: str, text, label=None):\n        self.guid = guid\n        self.text = text\n        self.text_a = text  # for compatibility with trasformer library\n        self.label = label\n\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence/token classification data sets.""""""\n\n    def get_train_examples(self):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_test_examples(self):\n        """"""Gets a collection of `InputExample`s for the test set.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n\nclass Task:\n    """""" A task definition class\n    Args:\n        name (str): the name of the task\n        processor (DataProcessor): a DataProcessor class containing a dataset loader\n        data_dir (str): path to the data source\n        task_type (str): the task type (classification/regression/tagging)\n    """"""\n\n    def __init__(self, name: str, processor: DataProcessor, data_dir: str, task_type: str):\n        self.name = name\n        self.processor = processor\n        self.data_dir = data_dir\n        self.task_type = task_type\n\n    def get_train_examples(self):\n        return self.processor.get_train_examples(self.data_dir)\n\n    def get_dev_examples(self):\n        return self.processor.get_dev_examples(self.data_dir)\n\n    def get_test_examples(self):\n        return self.processor.get_test_examples(self.data_dir)\n\n    def get_labels(self):\n        return self.processor.get_labels()\n\n\ndef read_tsv(input_file, quotechar=None):\n    """"""Reads a tab separated value file.""""""\n    with open(input_file, ""r"", encoding=""utf-8-sig"") as f:\n        reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n        lines = []\n        for line in reader:\n            if sys.version_info[0] == 2:\n                line = list(str(cell, ""utf-8"") for cell in line)  # noqa: F821\n            lines.append(line)\n        return lines\n\n\ndef read_column_tagged_file(filename: str, tag_col: int = -1, ignore_token: str = None):\n    """"""Reads column tagged (CONLL) style file (tab separated and token per line)\n    tag_col is the column number to use as tag of the token (defualts to the last in line)\n    Args:\n        filename (str): input file path\n        tag_col (int): the column contains the labels\n        ignore_token (str): a str token to exclude\n    return format :\n    [ [\'token\', \'TAG\'], [\'token\', \'TAG2\'],... ]\n    """"""\n    data = []\n    sentence = []\n    labels = []\n    with open(filename) as fp:\n        for line in fp:\n            line = line.strip()\n            if len(line) == 0:\n                if len(sentence) > 0:\n                    data.append((sentence, labels))\n                    sentence = []\n                    labels = []\n                continue\n            splits = line.split()\n            token = splits[0]\n            if token != ignore_token:\n                sentence.append(token)\n                labels.append(splits[tag_col])\n\n    if len(sentence) > 0:\n        data.append((sentence, labels))\n    return data\n\n\ndef write_column_tagged_file(filename: str, data: List[Tuple]):\n    file_dir = ""{}"".format(os.sep).join(filename.split(os.sep)[:-1])\n    if not os.path.exists(file_dir):\n        raise FileNotFoundError\n    with open(filename, ""w"", encoding=""utf-8"") as fw:\n        for sen in data:\n            cols = len(sen)\n            items = len(sen[0])\n            for i in range(items):\n                line = ""\\t"".join([sen[c][i] for c in range(cols)]) + ""\\n""\n                fw.write(line)\n            fw.write(""\\n"")\n\n\ndef sample_label_unlabeled(samples: List[InputExample], no_labeled: int, no_unlabeled: int):\n    """"""\n    Randomly sample 2 sets of samples from a given collection of InputExamples\n    (used for semi-supervised models)\n    """"""\n    num_of_examples = len(samples)\n    assert no_labeled > 0 and no_unlabeled > 0, ""Must provide no_samples > 0""\n    assert (\n        num_of_examples >= no_labeled + no_unlabeled\n    ), ""num of total samples smaller than requested sub sets""\n    all_indices = list(range(num_of_examples))\n    labeled_indices = random.sample(all_indices, no_labeled)\n    remaining_indices = list(set(all_indices).difference(set(labeled_indices)))\n    unlabeled_indices = random.sample(remaining_indices, no_unlabeled)\n    label_samples = [samples[i] for i in labeled_indices]\n    unlabel_samples = [samples[i] for i in unlabeled_indices]\n    return label_samples, unlabel_samples\n\n\ndef split_column_dataset(\n    first_count: int,\n    second_count: int,\n    out_folder,\n    dataset,\n    first_filename,\n    second_filename,\n    tag_col=-1,\n):\n    """"""\n    Splits a single column tagged dataset into two files according to the amount of examples\n    requested to be included in each file.\n    first_count (int) : the amount of examples to include in the first split file\n    second_count (int) : the amount of examples to include in the second split file\n    out_folder (str) : the folder in which the result files will be stored\n    dataset (str) : the path to the original data file\n    first_filename (str) : the name of the first split file\n    second_filename (str) : the name of the second split file\n    tag_col (int) : the index of the tag column\n    """"""\n    lines = read_column_tagged_file(dataset, tag_col=tag_col)\n    num_of_examples = len(lines)\n    assert first_count + second_count <= num_of_examples and first_count > 0\n    selected_lines = random.sample(lines, first_count + second_count)\n    first_data = selected_lines[:first_count]\n    second_data = selected_lines[first_count:]\n    write_column_tagged_file(out_folder + os.sep + first_filename, first_data)\n    if second_count != 0:\n        write_column_tagged_file(out_folder + os.sep + second_filename, second_data)\n\n\ndef get_cached_filepath(data_dir, model_name, seq_length, task_name, set_type=""train""):\n    """"""get cached file name\n\n    Arguments:\n        data_dir {str} -- data directory string\n        model_name {str} -- model name\n        seq_length {int} -- max sequence length\n        task_name {str} -- name of task\n\n    Keyword Arguments:\n        set_type {str} -- set type (choose from train/dev/test) (default: {""train""})\n\n    Returns:\n        str -- cached filename\n    """"""\n    cached_features_file = os.path.join(\n        data_dir,\n        ""cached_{}_{}_{}_{}"".format(\n            set_type,\n            list(filter(None, model_name.split(""/""))).pop(),\n            str(seq_length),\n            str(task_name),\n        ),\n    )\n    return cached_features_file\n'"
nlp_architect/models/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nfrom abc import ABC\n\nlogger = logging.getLogger(__name__)\n\n\nclass TrainableModel(ABC):\n    """"""Base class for a trainable model\n    """"""\n\n    def convert_to_tensors(self, *args, **kwargs):\n        """"""convert any chosen input to valid model format of tensors\n        """"""\n        raise NotImplementedError\n\n    def get_logits(self, *args, **kwargs):\n        """"""get model logits from given input\n        """"""\n        raise NotImplementedError\n\n    def train(self, *args, **kwargs):\n        """"""train the model\n        """"""\n        raise NotImplementedError\n\n    def inference(self, *args, **kwargs):\n        """"""run inference\n        """"""\n        raise NotImplementedError\n\n    def save_model(self, *args, **kwargs):\n        """"""save the model\n        """"""\n        ...\n\n    def load_model(self, *args, **kwargs):\n        """"""load a model\n        """"""\n        ...\n'"
nlp_architect/models/bist_parser.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport json\nimport os\n\nfrom nlp_architect.models.bist import utils\nfrom nlp_architect.models.bist.utils import get_options_dict\nfrom nlp_architect.utils.io import validate, validate_existing_filepath\n\n\nclass BISTModel(object):\n    """"""\n    BIST parser model class.\n    This class handles training, prediction, loading and saving of a BIST parser model.\n    After the model is initialized, it accepts a CoNLL formatted dataset as input, and learns to\n    output dependencies for new input.\n\n    Args:\n        activation (str, optional): Activation function to use.\n        lstm_layers (int, optional): Number of LSTM layers to use.\n        lstm_dims (int, optional): Number of LSTM dimensions to use.\n        pos_dims (int, optional): Number of part-of-speech embedding dimensions to use.\n\n    Attributes:\n        model (MSTParserLSTM): The underlying LSTM model.\n        params (tuple): Additional parameters and resources for the model.\n        options (dict): User model options.\n    """"""\n\n    def __init__(self, activation=""tanh"", lstm_layers=2, lstm_dims=125, pos_dims=25):\n        validate(\n            (activation, str),\n            (lstm_layers, int, 0, None),\n            (lstm_dims, int, 0, 1000),\n            (pos_dims, int, 0, 1000),\n        )\n        self.options = get_options_dict(activation, lstm_dims, lstm_layers, pos_dims)\n        self.params = None\n        self.model = None\n\n    def fit(self, dataset, epochs=10, dev=None):\n        """"""\n        Trains a BIST model on an annotated dataset in CoNLL file format.\n\n        Args:\n            dataset (str): Path to input dataset for training, formatted in CoNLL/U format.\n            epochs (int, optional): Number of learning iterations.\n            dev (str, optional): Path to development dataset for conducting evaluations.\n        """"""\n        if dev:\n            dev = validate_existing_filepath(dev)\n        dataset = validate_existing_filepath(dataset)\n        validate((epochs, int, 0, None))\n\n        print(""\\nRunning fit on "" + dataset + ""...\\n"")\n        words, w2i, pos, rels = utils.vocab(dataset)\n        self.params = words, w2i, pos, rels, self.options\n\n        from nlp_architect.models.bist.mstlstm import MSTParserLSTM\n\n        self.model = MSTParserLSTM(*self.params)\n\n        for epoch in range(epochs):\n            print(""Starting epoch"", epoch + 1)\n            self.model.train(dataset)\n            if dev:\n                ext = dev.rindex(""."")\n                res_path = dev[:ext] + ""_epoch_"" + str(epoch + 1) + ""_pred"" + dev[ext:]\n                utils.write_conll(res_path, self.model.predict(dev))\n                utils.run_eval(dev, res_path)\n\n    def predict(self, dataset, evaluate=False):\n        """"""\n        Runs inference with the BIST model on a dataset in CoNLL file format.\n\n        Args:\n            dataset (str): Path to input CoNLL file.\n            evaluate (bool, optional): Write prediction and evaluation files to dataset\'s folder.\n        Returns:\n            res (list of list of ConllEntry): The list of input sentences with predicted\n            dependencies attached.\n        """"""\n        dataset = validate_existing_filepath(dataset)\n        validate((evaluate, bool))\n\n        print(""\\nRunning predict on "" + dataset + ""...\\n"")\n        res = list(self.model.predict(conll_path=dataset))\n        if evaluate:\n            ext = dataset.rindex(""."")\n            pred_path = dataset[:ext] + ""_pred"" + dataset[ext:]\n            utils.write_conll(pred_path, res)\n            utils.run_eval(dataset, pred_path)\n        return res\n\n    def predict_conll(self, dataset):\n        """"""\n        Runs inference with the BIST model on a dataset in CoNLL object format.\n\n        Args:\n            dataset (list of list of ConllEntry): Input in the form of ConllEntry objects.\n        Returns:\n            res (list of list of ConllEntry): The list of input sentences with predicted\n            dependencies attached.\n        """"""\n        res = None\n        if hasattr(dataset, ""__iter__""):\n            res = list(self.model.predict(conll=dataset))\n        return res\n\n    def load(self, path):\n        """"""Loads and initializes a BIST model from file.""""""\n        with open(path.parent / ""params.json"") as file:\n            self.params = json.load(file)\n\n        from nlp_architect.models.bist.mstlstm import MSTParserLSTM\n\n        self.model = MSTParserLSTM(*self.params)\n        self.model.model.populate(str(path))\n\n    def save(self, path):\n        """"""Saves the BIST model to file.""""""\n        print(""Saving"")\n        with open(os.path.join(os.path.dirname(path), ""params.json""), ""w"") as file:\n            json.dump(self.params, file)\n        self.model.model.save(path)\n'"
nlp_architect/models/chunker.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\nfrom nlp_architect.nn.tensorflow.python.keras.layers.crf import CRF\nfrom nlp_architect.nn.tensorflow.python.keras.utils import load_model, save_model\n\n\nclass SequenceTagger(object):\n    """"""\n    A sequence tagging model for POS and Chunks written in Tensorflow (and Keras) based on the\n    paper \'Deep multi-task learning with low level tasks supervised at lower layers\'.\n    The model has 3 Bi-LSTM layers and outputs POS and Chunk tags.\n\n    Args:\n        use_cudnn (bool, optional): use GPU based model (CUDNNA cells)\n    """"""\n\n    def __init__(self, use_cudnn=False):\n        self.vocabulary_size = None\n        self.num_pos_labels = None\n        self.num_chunk_labels = None\n        self.char_vocab_size = None\n        self.feature_size = None\n        self.dropout = None\n        self.max_word_len = None\n        self.classifier = None\n        self.optimizer = None\n        self.model = None\n        self.use_cudnn = use_cudnn\n\n    def build(\n        self,\n        vocabulary_size,\n        num_pos_labels,\n        num_chunk_labels,\n        char_vocab_size=None,\n        max_word_len=25,\n        feature_size=100,\n        dropout=0.5,\n        classifier=""softmax"",\n        optimizer=None,\n    ):\n        """"""\n        Build a chunker/POS model\n\n        Args:\n            vocabulary_size (int): the size of the input vocabulary\n            num_pos_labels (int): the size of of POS labels\n            num_chunk_labels (int): the sie of chunk labels\n            char_vocab_size (int, optional): character vocabulary size\n            max_word_len (int, optional): max characters in a word\n            feature_size (int, optional): feature size - determines the embedding/LSTM layer \\\n                hidden state size\n            dropout (float, optional): dropout rate\n            classifier (str, optional): classifier layer, \'softmax\' for softmax or \'crf\' for \\\n                conditional random fields classifier. default is \'softmax\'.\n            optimizer (tensorflow.python.training.optimizer.Optimizer, optional): optimizer, if \\\n                None will use default SGD (paper setup)\n        """"""\n        self.vocabulary_size = vocabulary_size\n        self.char_vocab_size = char_vocab_size\n        self.num_pos_labels = num_pos_labels\n        self.num_chunk_labels = num_chunk_labels\n        self.max_word_len = max_word_len\n        self.feature_size = feature_size\n        self.dropout = dropout\n        self.classifier = classifier\n\n        word_emb_layer = tf.keras.layers.Embedding(\n            self.vocabulary_size, self.feature_size, name=""embedding"", mask_zero=False\n        )\n        word_input = tf.keras.layers.Input(shape=(None,))\n        word_embedding = word_emb_layer(word_input)\n        input_src = word_input\n        features = word_embedding\n\n        # add char input if present\n        if self.char_vocab_size is not None:\n            char_input = tf.keras.layers.Input(shape=(None, self.max_word_len))\n            char_emb_layer = tf.keras.layers.Embedding(\n                self.char_vocab_size, 30, name=""char_embedding"", mask_zero=False\n            )\n            char_embedding = char_emb_layer(char_input)\n            char_embedding = tf.keras.layers.TimeDistributed(\n                tf.keras.layers.Conv1D(30, 3, padding=""same"")\n            )(char_embedding)\n            char_embedding = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalMaxPooling1D())(\n                char_embedding\n            )\n\n            input_src = [input_src, char_input]\n            features = tf.keras.layers.concatenate([word_embedding, char_embedding])\n\n        rnn_layer_1 = tf.keras.layers.Bidirectional(self._rnn_cell(return_sequences=True))(features)\n        rnn_layer_2 = tf.keras.layers.Bidirectional(self._rnn_cell(return_sequences=True))(\n            rnn_layer_1\n        )\n        rnn_layer_3 = tf.keras.layers.Bidirectional(self._rnn_cell(return_sequences=True))(\n            rnn_layer_2\n        )\n\n        # outputs\n        pos_out = tf.keras.layers.Dense(\n            self.num_pos_labels, activation=""softmax"", name=""pos_output""\n        )(rnn_layer_1)\n        losses = {""pos_output"": ""categorical_crossentropy""}\n        metrics = {""pos_output"": ""categorical_accuracy""}\n\n        if ""crf"" in self.classifier:\n            with tf.device(""/cpu:0""):\n                chunk_crf = CRF(self.num_chunk_labels, name=""chunk_crf"")\n                rnn_layer_3_dense = tf.keras.layers.Dense(self.num_chunk_labels)(\n                    tf.keras.layers.Dropout(self.dropout)(rnn_layer_3)\n                )\n                chunks_out = chunk_crf(rnn_layer_3_dense)\n                losses[""chunk_crf""] = chunk_crf.loss\n                metrics[""chunk_crf""] = chunk_crf.viterbi_accuracy\n        else:\n            chunks_out = tf.keras.layers.TimeDistributed(\n                tf.keras.layers.Dense(self.num_chunk_labels, activation=""softmax""), name=""chunk_out""\n            )(rnn_layer_3)\n            losses[""chunk_out""] = ""categorical_crossentropy""\n            metrics[""chunk_out""] = ""categorical_accuracy""\n\n        model = tf.keras.Model(input_src, [pos_out, chunks_out])\n        if optimizer is None:\n            self.optimizer = tf.keras.optimizers.Adam(0.001, clipnorm=5.0)\n        else:\n            self.optimizer = optimizer\n        model.compile(optimizer=self.optimizer, loss=losses, metrics=metrics)\n        self.model = model\n\n    def load_embedding_weights(self, weights):\n        """"""\n        Load word embedding weights into the model embedding layer\n\n        Args:\n            weights (numpy.ndarray): 2D matrix of word weights\n        """"""\n        assert self.model is not None, (\n            ""Cannot assign weights, apply build() before trying to "" ""loading embedding weights ""\n        )\n        emb_layer = self.model.get_layer(name=""embedding"")\n        assert emb_layer.output_dim == weights.shape[1], ""embedding vectors shape mismatch""\n        emb_layer.set_weights([weights])\n\n    def _rnn_cell(self, **kwargs):\n        if self.use_cudnn:\n            rnn_cell = tf.keras.layers.CuDNNLSTM(self.feature_size, **kwargs)\n        else:\n            rnn_cell = tf.keras.layers.LSTM(self.feature_size, **kwargs)\n        return rnn_cell\n\n    def fit(self, x, y, batch_size=1, epochs=1, validation_data=None, callbacks=None):\n        """"""\n        Fit provided X and Y on built model\n\n        Args:\n            x: x samples\n            y: y samples\n            batch_size (int, optional): batch size per sample\n            epochs (int, optional): number of epochs to run before ending training process\n            validation_data (optional): x and y samples to validate at the end of the epoch\n            callbacks (optional): additional callbacks to run with fitting\n        """"""\n        self.model.fit(\n            x=x,\n            y=y,\n            batch_size=batch_size,\n            epochs=epochs,\n            validation_data=validation_data,\n            callbacks=callbacks,\n        )\n\n    def predict(self, x, batch_size=1):\n        """"""\n        Predict labels given x.\n\n        Args:\n            x: samples for inference\n            batch_size (int, optional): forward pass batch size\n\n        Returns:\n            tuple of numpy arrays of pos and chunk labels\n        """"""\n        return self.model.predict(x=x, batch_size=batch_size)\n\n    def save(self, filepath):\n        """"""\n        Save the model to disk\n\n        Args:\n            filepath (str): file name to save model\n        """"""\n        topology = {k: v for k, v in self.__dict__.items()}\n        topology.pop(""model"")\n        topology.pop(""optimizer"")\n        topology.pop(""use_cudnn"")\n        save_model(self.model, topology, filepath)\n\n    def load(self, filepath):\n        """"""\n        Load model from disk\n\n        Args:\n            filepath (str): file name of model\n        """"""\n        load_model(filepath, self)\n\n\nclass SequenceChunker(SequenceTagger):\n    """"""\n    A sequence Chunker model written in Tensorflow (and Keras) based SequenceTagger model.\n    The model uses only the chunking output of the model.\n    """"""\n\n    def predict(self, x, batch_size=1):\n        """"""\n        Predict labels given x.\n\n        Args:\n            x: samples for inference\n            batch_size (int, optional): forward pass batch size\n\n        Returns:\n            tuple of numpy arrays of chunk labels\n        """"""\n        model = tf.keras.Model(self.model.input, self.model.output[-1])\n        return model.predict(x=x, batch_size=batch_size)\n\n\nclass SequencePOSTagger(SequenceTagger):\n    """"""\n        A sequence POS tagger model written in Tensorflow (and Keras) based SequenceTagger model.\n        The model uses only the chunking output of the model.\n        """"""\n\n    def predict(self, x, batch_size=1):\n        """"""\n        Predict labels given x.\n\n        Args:\n            x: samples for inference\n            batch_size (int, optional): forward pass batch size\n\n        Returns:\n            tuple of numpy arrays of POS labels\n        """"""\n        model = tf.keras.Model(self.model.input, self.model.output[0])\n        return model.predict(x=x, batch_size=batch_size)\n'"
nlp_architect/models/cross_doc_sieves.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nfrom typing import List\n\nfrom nlp_architect.common.cdc.cluster import Clusters\nfrom nlp_architect.common.cdc.topics import Topics\nfrom nlp_architect.models.cross_doc_coref.system.sieves.run_sieve_system import get_run_system\nfrom nlp_architect.models.cross_doc_coref.system.sieves_container_init import (\n    SievesContainerInitialization,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef run_event_coref(topics: Topics, resources: SievesContainerInitialization) -> List[Clusters]:\n    """"""\n    Running Cross Document Coref on event mentions\n    Args:\n        topics   : The Topics (with mentions) to evaluate\n        resources: resources for running the evaluation\n\n    Returns:\n        Clusters: List of clusters and mentions with predicted cross doc coref within each topic\n    """"""\n\n    return _run_coref(topics, resources, ""event"")\n\n\ndef run_entity_coref(topics: Topics, resources: SievesContainerInitialization) -> List[Clusters]:\n    """"""\n    Running Cross Document Coref on Entity mentions\n    Args:\n        topics   : The Topics (with mentions) to evaluate\n        resources: (SievesContainerInitialization) resources for running the evaluation\n\n    Returns:\n        Clusters: List of topics and mentions with predicted cross doc coref within each topic\n    """"""\n    return _run_coref(topics, resources, ""entity"")\n\n\ndef _run_coref(\n    topics: Topics, resources: SievesContainerInitialization, eval_type: str\n) -> List[Clusters]:\n    """"""\n    Running Cross Document Coref on Entity mentions\n    Args:\n        resources: (SievesContainerInitialization) resources for running the evaluation\n        topics   : The Topics (with mentions) to evaluate\n\n    Returns:\n        Clusters: List of topics and mentions with predicted cross doc coref within each topic\n    """"""\n    clusters_list = list()\n    for topic in topics.topics_list:\n        sieves_list = get_run_system(topic, resources, eval_type)\n        clusters = sieves_list.run_deterministic()\n        clusters.set_coref_chain_to_mentions()\n        clusters_list.append(clusters)\n\n    return clusters_list\n'"
nlp_architect/models/crossling_emb.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import print_function, division\n\nimport io\nimport os\nimport time\n\nimport numpy as np\nimport scipy\nimport tensorflow as tf\n\n\nclass Discriminator:\n    def __init__(self, input_data, Y, lr_ph):\n        self.input_data = input_data\n        self.lr_ph = lr_ph\n        self.do_ph = tf.placeholder(name=""dropout_ph"", dtype=tf.float32)\n        self.Y = Y\n        self.hid_dim = 2048\n        # Build Graph\n        self._build_network_graph()\n        self.disc_cost = None\n        self.disc_opt = None\n        self.map_opt = None\n        self.W = None\n\n    def _build_network_graph(self):\n        """"""\n        Builds the basic inference graph for discriminator\n        """"""\n        with tf.variable_scope(""Discriminator"", reuse=tf.AUTO_REUSE):\n            w_init = tf.contrib.layers.xavier_initializer()\n            noisy_input = tf.nn.dropout(self.input_data, self.do_ph, name=""DO1"")\n            fc1 = tf.layers.dense(\n                noisy_input,\n                self.hid_dim,\n                kernel_initializer=w_init,\n                activation=tf.nn.leaky_relu,\n                name=""Dense1"",\n            )\n            fc2 = tf.layers.dense(\n                fc1,\n                self.hid_dim,\n                kernel_initializer=w_init,\n                activation=tf.nn.leaky_relu,\n                name=""Dense2"",\n            )\n            self.prediction = tf.layers.dense(fc2, 1, kernel_initializer=w_init, name=""Dense_Sig"")\n\n    def build_train_graph(self, disc_pred):\n        """"""\n        Builds training graph for discriminator\n        Arguments:\n             disc_pred(object): Discriminator instance\n        """"""\n        # Variables in discrimnator scope\n        disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""Discriminator"")\n        # Binary Cross entropy\n        disc_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_pred, labels=self.Y)\n        # Cost\n        self.disc_cost = tf.reduce_mean(disc_entropy)\n        # Optimizer\n        disc_opt = tf.train.GradientDescentOptimizer(self.lr_ph)\n        self.disc_opt = disc_opt.minimize(self.disc_cost, var_list=disc_vars)\n\n\nclass Generator:\n    def __init__(self, src_ten, tgt_ten, emb_dim, batch_size, smooth_val, lr_ph, beta, vocab_size):\n        self.src_ten = src_ten\n        self.tgt_ten = tgt_ten\n        self.emb_dim = emb_dim\n        self.batch_size = batch_size\n        self.smooth_val = smooth_val\n        self.beta = beta\n        self.lr_ph = lr_ph\n        self.vocab_size = vocab_size\n\n        # Placeholders\n        self.src_ph = tf.placeholder(name=""src_ph"", shape=[None], dtype=tf.int32)\n        self.tgt_ph = tf.placeholder(name=""tgt_ph"", shape=[None], dtype=tf.int32)\n\n        # Build Graph\n        self._build_network_graph()\n        ortho_weight = self._build_ortho_graph(self.W)\n        self.assign_weight = self._assign_ortho_weight(ortho_weight)\n        self.map_opt = None\n        self.W = None\n\n    def _build_network_graph(self):\n        """"""\n        Builds basic inference graph for generator\n        """"""\n        with tf.variable_scope(""Generator"", reuse=tf.AUTO_REUSE):\n            # Look up tables\n            self.src_emb = tf.nn.embedding_lookup(self.src_ten, self.src_ph, name=""src_lut"")\n            self.tgt_emb = tf.nn.embedding_lookup(self.tgt_ten, self.tgt_ph, name=""tgt_lut"")\n            # Map them\n            self.mapWX = self._mapper(self.src_emb)\n            # Concatenate them\n            self.X = tf.concat([self.mapWX, self.tgt_emb], 0, name=""X"")\n            # Set target for discriminator\n            Y = np.zeros(shape=(2 * self.batch_size, 1), dtype=np.float32)\n            # Label smoothing\n            Y[: self.batch_size] = 1 - self.smooth_val\n            Y[self.batch_size :] = self.smooth_val\n            # Convert to tensor\n            self.Y = tf.convert_to_tensor(Y, name=""Y"")\n\n    def build_train_graph(self, disc_pred):\n        """"""\n        Builds training graph for generator\n        Arguments:\n            disc_pred(object): Discriminator instance\n\n        """"""\n        # Variables in Mapper scope\n        map_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""Generator/Mapper"")\n        # Binary Cross entropy\n        map_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_pred, labels=(1 - self.Y))\n        # Cost\n        map_cost = tf.reduce_mean(map_entropy)\n        map_opt = tf.train.GradientDescentOptimizer(self.lr_ph)\n        self.map_opt = map_opt.minimize(map_cost, var_list=map_vars)\n\n    def _build_ortho_graph(self, W):\n        """"""\n        Builds a graph to orthogonalize weight W\n        Arguments:\n            W (Tensor): Weight in the mapper\n        """"""\n        with tf.variable_scope(""Ortho"", reuse=tf.AUTO_REUSE):\n            a = tf.scalar_mul((1 + self.beta), W)  # (1+B)W\n            b = tf.matmul(tf.transpose(W), W)  # WWt\n            c = tf.matmul(W, b)  # W(W.Wt)\n            d = tf.scalar_mul(self.beta, c)  # B(W.Wt)W\n            ortho_weight = a - d\n            return ortho_weight\n\n    def _assign_ortho_weight(self, ortho_weight):\n        """"""\n        Builds a graph to assign weight W after it is orthogonalized\n        Arguments:\n             ortho_weight(Tensor): Weight after it is orthogonalized\n        """"""\n        return tf.assign(self.W, ortho_weight)\n\n    def _mapper(self, src_emb):\n        """"""\n        Learns WX mapping to make ||WX-Y|| smaller\n        Arguments:\n             src_emb(Tensor): Source embeddings after lookup\n        """"""\n        with tf.variable_scope(""Mapper"", reuse=tf.AUTO_REUSE):\n            # Initialize as an eye of emb_dim x emb_dim\n            self.W = tf.Variable(name=""W"", initial_value=tf.eye(self.emb_dim, self.emb_dim))\n            # Do Matrix Multiply\n            WX = tf.matmul(src_emb, self.W)\n            # Returns map and weight handles\n            return WX\n\n\nclass WordTranslator:\n    """"""\n    Main network which does cross-lingual embeddings training\n    """"""\n\n    def __init__(self, hparams, src_vec, tgt_vec, vocab_size):\n        # Hyperparameters\n        self.batch_size = hparams.batch_size\n        self.smooth_val = hparams.smooth_val\n        self.beta = hparams.beta\n        self.most_freq = hparams.most_freq\n        self.emb_dim = hparams.emb_dim\n        self.vocab_size = vocab_size\n        self.disc_runs = hparams.disc_runs\n        self.iters_epoch = hparams.iters_epoch\n        self.src_vec = src_vec\n        self.tgt_vec = tgt_vec\n        self.src_ten = tf.convert_to_tensor(src_vec)\n        self.tgt_ten = tf.convert_to_tensor(tgt_vec)\n        self.save_dir = hparams.weight_dir\n        self.slang = hparams.src_lang\n        self.tlang = hparams.tgt_lang\n\n        # Placeholders\n        self.lr_ph = tf.placeholder(tf.float32, name=""lrPh"")\n        # Build Graph\n        self._build_network_graph()\n        self._build_train_graph()\n\n    def _build_network_graph(self):\n        """"""\n        Builds inference graph for the GAN\n        """"""\n        self.generator = Generator(\n            self.src_ten,\n            self.tgt_ten,\n            self.emb_dim,\n            self.batch_size,\n            self.smooth_val,\n            self.lr_ph,\n            self.beta,\n            self.vocab_size,\n        )\n        self.discriminator = Discriminator(self.generator.X, self.generator.Y, self.lr_ph)\n\n    def _build_train_graph(self):\n        """"""\n        Builds training graph for the GAN\n        """"""\n        self.generator.build_train_graph(self.discriminator.prediction)\n        self.discriminator.build_train_graph(self.discriminator.prediction)\n\n    @staticmethod\n    def report_metrics(iters, n_words_proc, disc_cost_acc, tic):\n        """"""\n        Reports metrics of how training is going\n        """"""\n        if iters > 0 and iters % 500 == 0:\n            mean_cost = str(sum(disc_cost_acc) / len(disc_cost_acc))\n            print(\n                str(int(n_words_proc / (time.time() - tic)))\n                + "" Samples/Sec - Iter ""\n                + str(iters)\n                + "" Discriminator Cost: ""\n                + mean_cost\n            )\n            # Reset instrumentation\n            del disc_cost_acc\n            disc_cost_acc = []\n            n_words_proc = 0\n            tic = time.time()\n\n    def run_generator(self, sess, local_lr):\n        """"""\n        Runs generator part of GAN\n        Arguments:\n            sess(tf.session): Tensorflow Session\n            local_lr(float): Learning rate\n        Returns:\n            Returns number of words processed\n        """"""\n        # Generate random ids to look up\n        src_ids = np.random.choice(self.vocab_size, self.batch_size, replace=False)\n        tgt_ids = np.random.choice(self.vocab_size, self.batch_size, replace=False)\n        train_dict = {\n            self.generator.src_ph: src_ids,\n            self.generator.tgt_ph: tgt_ids,\n            self.discriminator.do_ph: 1.0,\n            self.lr_ph: local_lr,\n        }\n        sess.run(self.generator.map_opt, feed_dict=train_dict)\n        # Run orthogonalize\n        sess.run(self.generator.assign_weight)\n        return 2 * self.batch_size\n\n    def run_discriminator(self, sess, local_lr):\n        """"""\n        Runs discriminator part of GAN\n        Arguments:\n            sess(tf.session): Tensorflow Session\n            local_lr(float): Learning rate\n        """"""\n        # Generate random ids to look up\n        src_ids = np.random.choice(self.most_freq, self.batch_size, replace=False)\n        tgt_ids = np.random.choice(self.most_freq, self.batch_size, replace=False)\n        train_dict = {\n            self.generator.src_ph: src_ids,\n            self.generator.tgt_ph: tgt_ids,\n            self.discriminator.do_ph: 0.9,\n            self.lr_ph: local_lr,\n        }\n        return sess.run(\n            [self.discriminator.disc_cost, self.discriminator.disc_opt], feed_dict=train_dict\n        )\n\n    def run(self, sess, local_lr):\n        """"""\n        Runs whole GAN\n        Arguments:\n            sess(tf.session): Tensorflow Session\n            local_lr(float): Learning rate\n        """"""\n        disc_cost_acc = []\n        n_words_proc = 0\n        tic = time.time()\n        for iters in range(0, self.iters_epoch, self.batch_size):\n            # 1.Run the discriminator\n            for _ in range(self.disc_runs):\n                disc_result = self.run_discriminator(sess, local_lr)\n                disc_cost_acc.append(disc_result[0])\n            # 2.Run the Generator\n            n_words_proc += self.run_generator(sess, local_lr)\n            # 3.Report the metrics\n            self.report_metrics(iters, n_words_proc, disc_cost_acc, tic)\n\n    @staticmethod\n    def set_lr(local_lr, drop_lr):\n        """"""\n        Drops learning rate based on CSLS criterion\n        Arguments:\n            local_lr(float): Learning Rate\n            drop_lr(bool): Drop learning rate by 2 if True\n        """"""\n        new_lr = local_lr * 0.98\n        print(""Dropping learning rate to "" + str(new_lr) + "" from "" + str(local_lr))\n        if drop_lr:\n            new_lr = new_lr / 2.0\n            print(\n                ""Dividing learning rate by 2 as validation criterion\\\n                   decreased. New lr is ""\n                + str(new_lr)\n            )\n        return new_lr\n\n    def save_model(self, save_model, sess):\n        """"""\n        Saves W in mapper as numpy array based on CSLS criterion\n        Arguments:\n            save_model(bool): Save model if True\n            sess(tf.session): Tensorflow Session\n        """"""\n        if save_model:\n            print(""Saving model ...."")\n            model_W = sess.run(self.generator.W)\n            path = os.path.join(self.save_dir, ""W_best_mapping"")\n            np.save(path, model_W)\n\n    def apply_procrustes(self, sess, final_pairs):\n        """"""\n        Applies procrustes to W matrix for better mapping\n        Arguments:\n            sess(tf.session): Tensorflow Session\n            final_pairs(ndarray): Array of pairs which are mutual neighbors\n        """"""\n        print(""Applying solution of Procrustes problem to get better mapping..."")\n        proc_dict = {\n            self.generator.src_ph: final_pairs[:, 0],\n            self.generator.tgt_ph: final_pairs[:, 1],\n        }\n        A, B = sess.run([self.generator.src_emb, self.generator.tgt_emb], feed_dict=proc_dict)\n        # pylint: disable=no-member\n        R = scipy.linalg.orthogonal_procrustes(A, B)\n        sess.run(tf.assign(self.generator.W, R[0]))\n\n    def generate_xling_embed(self, sess, src_dict, tgt_dict, tgt_vec):\n        """"""\n        Generates cross lingual embeddings\n        Arguments:\n             sess(tf.session): Tensorflow session\n        """"""\n        print(""Generating Cross-lingual embeddings..."")\n        src_emb_x = []\n        batch_size = 512\n        for i in range(0, self.vocab_size, batch_size):\n            sids = [x for x in range(i, min(i + batch_size, self.vocab_size))]\n            src_emb_x.append(\n                sess.run(self.generator.mapWX, feed_dict={self.generator.src_ph: sids})\n            )\n        src_emb_x = np.concatenate(src_emb_x)\n        print(""Writing cross-lingual embeddings to file..."")\n        src_path = os.path.join(self.save_dir, ""vectors-%s.txt"" % self.slang)\n        tgt_path = os.path.join(self.save_dir, ""vectors-%s.txt"" % self.tlang)\n        with io.open(src_path, ""w"", encoding=""utf-8"") as f:\n            f.write(""%i %i\\n"" % src_emb_x.shape)\n            for i in range(len(src_dict)):\n                f.write(""%s %s\\n"" % (src_dict[i], "" "".join(""%.5f"" % x for x in src_emb_x[i])))\n\n        with io.open(tgt_path, ""w"", encoding=""utf-8"") as f:\n            f.write(""%i %i\\n"" % tgt_vec.shape)\n            for i in range(len(tgt_dict)):\n                f.write(""%s %s\\n"" % (tgt_dict[i], "" "".join(""%.5f"" % x for x in tgt_vec[i])))\n'"
nlp_architect/models/intent_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\nfrom nlp_architect.nn.tensorflow.python.keras.layers.crf import CRF\nfrom nlp_architect.nn.tensorflow.python.keras.utils import load_model, save_model\n\n\nclass IntentExtractionModel(object):\n    """"""\n    Intent Extraction model base class (using tf.keras)\n    """"""\n\n    def __init__(self):\n        self.model = None\n\n    def fit(self, x, y, epochs=1, batch_size=1, callbacks=None, validation=None):\n        """"""\n        Train a model given input samples and target labels.\n\n        Args:\n            x: input samples\n            y: input sample labels\n            epochs (:obj:`int`, optional): number of epochs to train\n            batch_size (:obj:`int`, optional): batch size\n            callbacks(:obj:`Callback`, optional): Keras compatible callbacks\n            validation(:obj:`list` of :obj:`numpy.ndarray`, optional): optional validation data\n                to be evaluated when training\n        """"""\n        assert self.model, ""Model was not initialized""\n        self.model.fit(\n            x,\n            y,\n            epochs=epochs,\n            batch_size=batch_size,\n            shuffle=True,\n            validation_data=validation,\n            callbacks=callbacks,\n        )\n\n    def predict(self, x, batch_size=1):\n        """"""\n        Get the prediction of the model on given input\n\n        Args:\n            x: samples to run through the model\n            batch_size (:obj:`int`, optional): batch size:\n\n        Returns:\n            numpy.ndarray: predicted values by the model\n        """"""\n        assert self.model, ""Model was not initialized""\n        return self.model.predict(x, batch_size=batch_size)\n\n    def save(self, path, exclude=None):\n        """"""\n        Save model to path\n\n        Args:\n            path (str): path to save model\n            exclude (list, optional): a list of object fields to exclude when saving\n        """"""\n        assert self.model, ""Model was not initialized""\n        topology = {k: v for k, v in self.__dict__.items()}\n        topology.pop(""model"")\n        if exclude and isinstance(exclude, list):\n            for x in exclude:\n                topology.pop(x)\n        save_model(self.model, topology=topology, filepath=path)\n\n    def load(self, path):\n        """"""\n        Load a trained model\n\n        Args:\n            path (str): path to model file\n        """"""\n        load_model(path, self)\n\n    @property\n    def input_shape(self):\n        """""":obj:`tuple`:Get input shape""""""\n        return self.model.layers[0].input_shape\n\n    @staticmethod\n    def _create_input_embed(sentence_len, is_extern_emb, token_emb_size, vocab_size):\n        if is_extern_emb:\n            in_layer = e_layer = tf.keras.layers.Input(\n                shape=(sentence_len, token_emb_size,), dtype=""float32"", name=""tokens_input""\n            )\n        else:\n            in_layer = tf.keras.layers.Input(\n                shape=(sentence_len,), dtype=""int32"", name=""tokens_input""\n            )\n            e_layer = tf.keras.layers.Embedding(\n                vocab_size, token_emb_size, input_length=sentence_len, name=""embedding_layer""\n            )(in_layer)\n        return in_layer, e_layer\n\n    def load_embedding_weights(self, weights):\n        """"""\n        Load word embedding weights into the model embedding layer\n\n        Args:\n            weights (numpy.ndarray): 2D matrix of word weights\n        """"""\n        assert self.model is not None, (\n            ""Cannot assign weights, apply build() before trying to "" ""loading embedding weights ""\n        )\n        emb_layer = self.model.get_layer(name=""word_embedding"")\n        assert emb_layer.output_dim == weights.shape[1], ""embedding vectors shape mismatch""\n        emb_layer.set_weights([weights])\n\n\nclass MultiTaskIntentModel(IntentExtractionModel):\n    """"""\n    Multi-Task Intent and Slot tagging model (using tf.keras)\n\n    Args:\n        use_cudnn (bool, optional): use GPU based model (CUDNNA cells)\n    """"""\n\n    def __init__(self, use_cudnn=False):\n        super().__init__()\n        self.model = None\n        self.word_length = None\n        self.num_labels = None\n        self.num_intent_labels = None\n        self.word_vocab_size = None\n        self.char_vocab_size = None\n        self.word_emb_dims = None\n        self.char_emb_dims = None\n        self.char_lstm_dims = None\n        self.tagger_lstm_dims = None\n        self.dropout = None\n        self.use_cudnn = use_cudnn\n\n    def build(\n        self,\n        word_length,\n        num_labels,\n        num_intent_labels,\n        word_vocab_size,\n        char_vocab_size,\n        word_emb_dims=100,\n        char_emb_dims=30,\n        char_lstm_dims=30,\n        tagger_lstm_dims=100,\n        dropout=0.2,\n    ):\n        """"""\n        Build a model\n\n        Args:\n            word_length (int): max word length (in characters)\n            num_labels (int): number of slot labels\n            num_intent_labels (int): number of intent classes\n            word_vocab_size (int): word vocabulary size\n            char_vocab_size (int): character vocabulary size\n            word_emb_dims (int, optional): word embedding dimensions\n            char_emb_dims (int, optional): character embedding dimensions\n            char_lstm_dims (int, optional): character feature LSTM hidden size\n            tagger_lstm_dims (int, optional): tagger LSTM hidden size\n            dropout (float, optional): dropout rate\n        """"""\n        self.word_length = word_length\n        self.num_labels = num_labels\n        self.num_intent_labels = num_intent_labels\n        self.word_vocab_size = word_vocab_size\n        self.char_vocab_size = char_vocab_size\n        self.word_emb_dims = word_emb_dims\n        self.char_emb_dims = char_emb_dims\n        self.char_lstm_dims = char_lstm_dims\n        self.tagger_lstm_dims = tagger_lstm_dims\n        self.dropout = dropout\n\n        words_input = tf.keras.layers.Input(shape=(None,), name=""words_input"")\n        embedding_layer = tf.keras.layers.Embedding(\n            self.word_vocab_size, self.word_emb_dims, name=""word_embedding""\n        )\n        word_embeddings = embedding_layer(words_input)\n        word_embeddings = tf.keras.layers.Dropout(self.dropout)(word_embeddings)\n\n        # create word character input and embeddings layer\n        word_chars_input = tf.keras.layers.Input(\n            shape=(None, self.word_length), name=""word_chars_input""\n        )\n        char_embedding_layer = tf.keras.layers.Embedding(\n            self.char_vocab_size,\n            self.char_emb_dims,\n            input_length=self.word_length,\n            name=""char_embedding"",\n        )\n        # apply embedding to each word\n        char_embeddings = char_embedding_layer(word_chars_input)\n        # feed dense char vectors into BiLSTM\n        char_embeddings = tf.keras.layers.TimeDistributed(\n            tf.keras.layers.Bidirectional(self._rnn_cell(self.char_lstm_dims))\n        )(char_embeddings)\n        char_embeddings = tf.keras.layers.Dropout(self.dropout)(char_embeddings)\n\n        # first BiLSTM layer (used for intent classification)\n        first_bilstm_layer = tf.keras.layers.Bidirectional(\n            self._rnn_cell(self.tagger_lstm_dims, return_sequences=True, return_state=True)\n        )\n        first_lstm_out = first_bilstm_layer(word_embeddings)\n\n        lstm_y_sequence = first_lstm_out[:1][0]  # save y states of the LSTM layer\n        states = first_lstm_out[1:]\n        hf, _, hb, _ = states  # extract last hidden states\n        h_state = tf.keras.layers.concatenate([hf, hb], axis=-1)\n        intents = tf.keras.layers.Dense(\n            self.num_intent_labels, activation=""softmax"", name=""intent_classifier_output""\n        )(h_state)\n\n        # create the 2nd feature vectors\n        combined_features = tf.keras.layers.concatenate([lstm_y_sequence, char_embeddings], axis=-1)\n\n        # 2nd BiLSTM layer for label classification\n        second_bilstm_layer = tf.keras.layers.Bidirectional(\n            self._rnn_cell(self.tagger_lstm_dims, return_sequences=True)\n        )(combined_features)\n        second_bilstm_layer = tf.keras.layers.Dropout(self.dropout)(second_bilstm_layer)\n        bilstm_out = tf.keras.layers.Dense(self.num_labels)(second_bilstm_layer)\n\n        # feed BiLSTM vectors into CRF\n        with tf.device(""/cpu:0""):\n            crf = CRF(self.num_labels, name=""intent_slot_crf"")\n            labels = crf(bilstm_out)\n\n        # compile the model\n        model = tf.keras.Model(inputs=[words_input, word_chars_input], outputs=[intents, labels])\n\n        # define losses and metrics\n        loss_f = {\n            ""intent_classifier_output"": ""categorical_crossentropy"",\n            ""intent_slot_crf"": crf.loss,\n        }\n        metrics = {\n            ""intent_classifier_output"": ""categorical_accuracy"",\n            ""intent_slot_crf"": crf.viterbi_accuracy,\n        }\n\n        model.compile(loss=loss_f, optimizer=tf.train.AdamOptimizer(), metrics=metrics)\n        self.model = model\n\n    def _rnn_cell(self, units, **kwargs):\n        if self.use_cudnn:\n            rnn_cell = tf.keras.layers.CuDNNLSTM(units, **kwargs)\n        else:\n            rnn_cell = tf.keras.layers.LSTM(units, **kwargs)\n        return rnn_cell\n\n    # pylint: disable=arguments-differ\n    def save(self, path):\n        """"""\n        Save model to path\n\n        Args:\n            path (str): path to save model\n        """"""\n        super().save(path, [""use_cudnn""])\n\n\nclass Seq2SeqIntentModel(IntentExtractionModel):\n    """"""\n    Encoder Decoder Deep LSTM Tagger Model (using tf.keras)\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.model = None\n        self.vocab_size = None\n        self.tag_labels = None\n        self.token_emb_size = None\n        self.encoder_depth = None\n        self.decoder_depth = None\n        self.lstm_hidden_size = None\n        self.encoder_dropout = None\n        self.decoder_dropout = None\n\n    def build(\n        self,\n        vocab_size,\n        tag_labels,\n        token_emb_size=100,\n        encoder_depth=1,\n        decoder_depth=1,\n        lstm_hidden_size=100,\n        encoder_dropout=0.5,\n        decoder_dropout=0.5,\n    ):\n        """"""\n        Build the model\n\n        Args:\n            vocab_size (int): vocabulary size\n            tag_labels (int): number of tag labels\n            token_emb_size (int, optional): token embedding vector size\n            encoder_depth (int, optional): number of encoder LSTM layers\n            decoder_depth (int, optional): number of decoder LSTM layers\n            lstm_hidden_size (int, optional): LSTM layers hidden size\n            encoder_dropout (float, optional): encoder dropout\n            decoder_dropout (float, optional): decoder dropout\n        """"""\n        self.vocab_size = vocab_size\n        self.tag_labels = tag_labels\n        self.token_emb_size = token_emb_size\n        self.encoder_depth = encoder_depth\n        self.decoder_depth = decoder_depth\n        self.lstm_hidden_size = lstm_hidden_size\n        self.encoder_dropout = encoder_dropout\n        self.decoder_dropout = decoder_dropout\n\n        words_input = tf.keras.layers.Input(shape=(None,), name=""words_input"")\n        emb_layer = tf.keras.layers.Embedding(\n            self.vocab_size, self.token_emb_size, name=""word_embedding""\n        )\n        benc_in = emb_layer(words_input)\n\n        assert self.encoder_depth > 0, ""Encoder depth must be > 0""\n        for i in range(self.encoder_depth):\n            bencoder = tf.keras.layers.LSTM(\n                self.lstm_hidden_size,\n                return_sequences=True,\n                return_state=True,\n                go_backwards=True,\n                dropout=self.encoder_dropout,\n                name=""encoder_blstm_{}"".format(i),\n            )(benc_in)\n            benc_in = bencoder[0]\n        b_states = bencoder[1:]\n        benc_h, bene_c = b_states\n\n        decoder_inputs = benc_in\n        assert self.decoder_depth > 0, ""Decoder depth must be > 0""\n        for i in range(self.decoder_depth):\n            decoder = tf.keras.layers.LSTM(\n                self.lstm_hidden_size, return_sequences=True, name=""decoder_lstm_{}"".format(i)\n            )(decoder_inputs, initial_state=[benc_h, bene_c])\n            decoder_inputs = decoder\n        decoder_outputs = tf.keras.layers.Dropout(self.decoder_dropout)(decoder)\n        decoder_predictions = tf.keras.layers.TimeDistributed(\n            tf.keras.layers.Dense(self.tag_labels, activation=""softmax""), name=""decoder_classifier""\n        )(decoder_outputs)\n\n        self.model = tf.keras.Model(words_input, decoder_predictions)\n        self.model.compile(\n            optimizer=tf.train.AdamOptimizer(),\n            loss=""categorical_crossentropy"",\n            metrics=[""categorical_accuracy""],\n        )\n'"
nlp_architect/models/most_common_word_sense.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ****************************************************************************\nimport tensorflow as tf\n\n\nclass MostCommonWordSense(object):\n    def __init__(self, epochs, batch_size, callback_args=None):\n        self.optimizer = tf.keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n        self.loss = ""mean_squared_error""\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.model = None\n        self.callback_args = callback_args\n\n    def build(self, input_dim):\n        # setup model layers\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Dense(100, activation=""relu"", input_dim=input_dim))\n        model.add(tf.keras.layers.Dropout(0.5))\n        model.add(tf.keras.layers.Dense(2, activation=""softmax""))\n        model.compile(loss=self.loss, optimizer=self.optimizer)\n        self.model = model\n\n    def fit(self, train_set):\n        self.model.fit(\n            train_set[""X""], train_set[""y""], epochs=self.epochs, batch_size=self.batch_size\n        )\n\n    def save(self, save_path):\n        self.model.save(save_path)\n\n    def load(self, model_path):\n        self.model = tf.keras.models.load_model(model_path)\n\n    def eval(self, valid_set):\n        eval_rate = self.model.evaluate(valid_set[""X""], valid_set[""y""], batch_size=self.batch_size)\n        return eval_rate\n\n    def get_outputs(self, valid_set):\n        return self.model.predict(valid_set)\n'"
nlp_architect/models/ner_crf.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\nfrom nlp_architect.nn.tensorflow.python.keras.layers.crf import CRF\nfrom nlp_architect.nn.tensorflow.python.keras.utils import load_model, save_model\n\n\nclass NERCRF(object):\n    """"""\n    Bi-LSTM NER model with CRF classification layer (tf.keras model)\n\n    Args:\n        use_cudnn (bool, optional): use cudnn LSTM cells\n    """"""\n\n    def __init__(self, use_cudnn=False):\n        self.model = None\n        self.word_length = None\n        self.target_label_dims = None\n        self.word_vocab_size = None\n        self.char_vocab_size = None\n        self.word_embedding_dims = None\n        self.char_embedding_dims = None\n        self.tagger_lstm_dims = None\n        self.dropout = None\n        self.use_cudnn = use_cudnn\n\n    def build(\n        self,\n        word_length,\n        target_label_dims,\n        word_vocab_size,\n        char_vocab_size,\n        word_embedding_dims=100,\n        char_embedding_dims=16,\n        tagger_lstm_dims=200,\n        dropout=0.5,\n    ):\n        """"""\n        Build a NERCRF model\n\n        Args:\n            word_length (int): max word length in characters\n            target_label_dims (int): number of entity labels (for classification)\n            word_vocab_size (int): word vocabulary size\n            char_vocab_size (int): character vocabulary size\n            word_embedding_dims (int): word embedding dimensions\n            char_embedding_dims (int): character embedding dimensions\n            tagger_lstm_dims (int): word tagger LSTM output dimensions\n            dropout (float): dropout rate\n        """"""\n        self.word_length = word_length\n        self.target_label_dims = target_label_dims\n        self.word_vocab_size = word_vocab_size\n        self.char_vocab_size = char_vocab_size\n        self.word_embedding_dims = word_embedding_dims\n        self.char_embedding_dims = char_embedding_dims\n        self.tagger_lstm_dims = tagger_lstm_dims\n        self.dropout = dropout\n\n        # build word input\n        words_input = tf.keras.layers.Input(shape=(None,), name=""words_input"")\n        embedding_layer = tf.keras.layers.Embedding(\n            self.word_vocab_size, self.word_embedding_dims, name=""word_embedding""\n        )\n        word_embeddings = embedding_layer(words_input)\n\n        # create word character embeddings\n        word_chars_input = tf.keras.layers.Input(\n            shape=(None, self.word_length), name=""word_chars_input""\n        )\n        char_embedding_layer = tf.keras.layers.Embedding(\n            self.char_vocab_size, self.char_embedding_dims, name=""char_embedding""\n        )(word_chars_input)\n        char_embeddings = tf.keras.layers.TimeDistributed(\n            tf.keras.layers.Conv1D(128, 3, padding=""same"", activation=""relu"")\n        )(char_embedding_layer)\n        char_embeddings = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalMaxPooling1D())(\n            char_embeddings\n        )\n\n        # create the final feature vectors\n        features = tf.keras.layers.concatenate([word_embeddings, char_embeddings], axis=-1)\n\n        # encode using a bi-LSTM\n        features = tf.keras.layers.Dropout(self.dropout)(features)\n        bilstm = tf.keras.layers.Bidirectional(\n            self._rnn_cell(self.tagger_lstm_dims, return_sequences=True)\n        )(features)\n        bilstm = tf.keras.layers.Bidirectional(\n            self._rnn_cell(self.tagger_lstm_dims, return_sequences=True)\n        )(bilstm)\n        bilstm = tf.keras.layers.Dropout(self.dropout)(bilstm)\n        bilstm = tf.keras.layers.Dense(self.target_label_dims)(bilstm)\n\n        inputs = [words_input, word_chars_input]\n\n        sequence_lengths = tf.keras.layers.Input(shape=(1,), dtype=""int32"", name=""seq_lens"")\n        inputs.append(sequence_lengths)\n        crf = CRF(self.target_label_dims, name=""ner_crf"")\n        predictions = crf(inputs=bilstm, sequence_lengths=sequence_lengths)\n\n        # compile the model\n        model = tf.keras.Model(inputs=inputs, outputs=predictions)\n        model.compile(\n            loss={""ner_crf"": crf.loss}, optimizer=tf.keras.optimizers.Adam(0.001, clipnorm=5.0)\n        )\n\n        self.model = model\n\n    def _rnn_cell(self, units, **kwargs):\n        if self.use_cudnn:\n            rnn_cell = tf.keras.layers.CuDNNLSTM(units, **kwargs)\n        else:\n            rnn_cell = tf.keras.layers.LSTM(units, **kwargs)\n        return rnn_cell\n\n    def load_embedding_weights(self, weights):\n        """"""\n        Load word embedding weights into the model embedding layer\n\n        Args:\n            weights (numpy.ndarray): 2D matrix of word weights\n        """"""\n        assert self.model is not None, (\n            ""Cannot assign weights, apply build() before trying to "" ""loading embedding weights ""\n        )\n        emb_layer = self.model.get_layer(name=""word_embedding"")\n        assert emb_layer.output_dim == weights.shape[1], ""embedding vectors shape mismatch""\n        emb_layer.set_weights([weights])\n\n    def fit(self, x, y, epochs=1, batch_size=1, callbacks=None, validation=None):\n        """"""\n        Train a model given input samples and target labels.\n\n        Args:\n            x (numpy.ndarray or :obj:`numpy.ndarray`): input samples\n            y (numpy.ndarray): input sample labels\n            epochs (:obj:`int`, optional): number of epochs to train\n            batch_size (:obj:`int`, optional): batch size\n            callbacks(:obj:`Callback`, optional): Keras compatible callbacks\n            validation(:obj:`list` of :obj:`numpy.ndarray`, optional): optional validation data\n                to be evaluated when training\n        """"""\n        assert self.model, ""Model was not initialized""\n        self.model.fit(\n            x,\n            y,\n            epochs=epochs,\n            batch_size=batch_size,\n            shuffle=True,\n            validation_data=validation,\n            callbacks=callbacks,\n        )\n\n    def predict(self, x, batch_size=1):\n        """"""\n        Get the prediction of the model on given input\n\n        Args:\n            x (numpy.ndarray or :obj:`numpy.ndarray`): input samples\n            batch_size (:obj:`int`, optional): batch size\n\n        Returns:\n            numpy.ndarray: predicted values by the model\n        """"""\n        assert self.model, ""Model was not initialized""\n        return self.model.predict(x, batch_size=batch_size)\n\n    def save(self, path):\n        """"""\n        Save model to path\n\n        Args:\n            path (str): path to save model weights\n        """"""\n        topology = {k: v for k, v in self.__dict__.items()}\n        topology.pop(""model"")\n        topology.pop(""use_cudnn"")\n        save_model(self.model, topology, path)\n\n    def load(self, path):\n        """"""\n        Load model weights\n\n        Args:\n            path (str): path to load model from\n        """"""\n        load_model(path, self)\n'"
nlp_architect/models/np2vec.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport json\nimport logging\nimport sys\n\nfrom gensim.models import FastText, Word2Vec, KeyedVectors\nfrom gensim.models.word2vec import LineSentence\nfrom gensim import utils\nimport nltk\nfrom nltk.corpus import conll2000\nfrom six import iteritems\nfrom smart_open import open as smart_open\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable-msg=too-many-instance-attributes\nclass NP2vec:\n    """"""\n    Initialize the np2vec model, train it, save it and load it.\n    """"""\n\n    def is_marked(self, s):\n        """"""\n        Check if a string is marked.\n\n        Args:\n            s (str): string to check\n        """"""\n        return len(s) > 0 and s[-1] == self.mark_char\n\n    # pylint: disable-msg=too-many-arguments\n    # pylint: disable-msg=too-many-locals\n    # pylint: disable-msg=too-many-branches\n    def __init__(  # noqa: C901\n        self,\n        corpus,\n        corpus_format=""txt"",\n        mark_char=""_"",\n        word_embedding_type=""word2vec"",\n        sg=0,\n        size=100,\n        window=10,\n        alpha=0.025,\n        min_alpha=0.0001,\n        min_count=5,\n        sample=1e-5,\n        workers=20,\n        hs=0,\n        negative=25,\n        cbow_mean=1,\n        iterations=15,\n        min_n=3,\n        max_n=6,\n        word_ngrams=1,\n        prune_non_np=True,\n    ):\n        """"""\n        Initialize np2vec model and train it.\n\n        Args:\n          corpus (str): path to the corpus.\n          corpus_format (str {json,txt,conll2000}): format of the input marked corpus; txt and json\n          formats are supported. For json format, the file should contain an iterable of\n          sentences. Each sentence is a list of terms (unicode strings) that will be used for\n          training.\n          mark_char (char): special character that marks NP\'s suffix.\n          word_embedding_type (str {word2vec,fasttext}): word embedding model type; word2vec and\n          fasttext are supported.\n          np2vec_model_file (str): path to the file where the trained np2vec model has to be\n          stored.\n          binary (bool): boolean indicating whether the model is stored in binary format; if\n          word_embedding_type is fasttext and word_ngrams is 1, binary should be set to True.\n          sg (int {0,1}): model training hyperparameter, skip-gram. Defines the training\n          algorithm. If 1, CBOW is used,otherwise, skip-gram is employed.\n          size (int): model training hyperparameter, size of the feature vectors.\n          window (int): model training hyperparameter, maximum distance between the current and\n          predicted word within a sentence.\n          alpha (float): model training hyperparameter. The initial learning rate.\n          min_alpha (float): model training hyperparameter. Learning rate will linearly drop to\n          `min_alpha` as training progresses.\n          min_count (int): model training hyperparameter, ignore all words with total frequency\n          lower than this.\n          sample (float): model training hyperparameter, threshold for configuring which\n          higher-frequency words are randomly downsampled, useful range is (0, 1e-5)\n          workers (int): model training hyperparameter, number of worker threads.\n          hs (int {0,1}): model training hyperparameter, hierarchical softmax. If set to 1,\n          hierarchical softmax will be used for model training. If set to 0, and `negative` is non-\n                        zero, negative sampling will be used.\n          negative (int): model training hyperparameter, negative sampling. If > 0, negative\n          sampling will be used, the int for negative specifies how many ""noise words"" should be\n          drawn (usually between 5-20). If set to 0, no negative sampling is used.\n          cbow_mean (int {0,1}): model training hyperparameter. If 0, use the sum of the context\n          word vectors. If 1, use the mean, only applies when cbow is used.\n          iterations (int): model training hyperparameter, number of iterations.\n          min_n (int): fasttext training hyperparameter. Min length of char ngrams to be used\n          for training word representations.\n          max_n (int): fasttext training hyperparameter. Max length of char ngrams to be used for\n          training word representations. Set `max_n` to be lesser than `min_n` to avoid char\n          ngrams being used.\n          word_ngrams (int {0,1}): fasttext training hyperparameter. If 1, uses enrich word\n          vectors with subword (ngrams) information. If 0, this is equivalent to word2vec training.\n          prune_non_np (bool): indicates whether to prune non-NP\'s after training process.\n\n        """"""\n\n        self.mark_char = mark_char\n        self.word_embedding_type = word_embedding_type\n        self.sg = sg\n        self.size = size\n        self.window = window\n        self.alpha = alpha\n        self.min_alpha = min_alpha\n        self.min_count = min_count\n        self.sample = sample\n        self.workers = workers\n        self.hs = hs\n        self.negative = negative\n        self.cbow_mean = cbow_mean\n        self.iter = iterations\n        self.min_n = min_n\n        self.max_n = max_n\n        self.word_ngrams = word_ngrams\n        self.prune_non_np = prune_non_np\n\n        if corpus_format == ""txt"":\n            self._sentences = LineSentence(corpus)\n        elif corpus_format == ""json"":\n            with open(corpus) as json_data:\n                self._sentences = json.load(json_data)\n        # pylint: disable-msg=too-many-nested-blocks\n        elif corpus_format == ""conll2000"":\n            try:\n                self._sentences = list()\n                for chunked_sent in conll2000.chunked_sents(corpus):\n                    tokens = list()\n                    for chunk in chunked_sent:\n                        # pylint: disable-msg=protected-access\n                        if hasattr(chunk, ""_label"") and chunk._label == ""NP"":\n                            s = """"\n                            for w in chunk:\n                                s += w[0] + self.mark_char\n                            tokens.append(s)\n                        else:\n                            if isinstance(chunk, nltk.Tree):\n                                for w in chunk:\n                                    tokens.append(w[0])\n                            else:\n                                tokens.append(chunk[0])\n                        self._sentences.append(tokens)\n            # pylint: disable-msg=broad-except\n            except Exception:\n                print(""Conll2000 dataset is missing. See downloading details in the "" ""README file"")\n        else:\n            logger.error(""invalid corpus format: %s"", corpus_format)\n            sys.exit(0)\n\n        if word_embedding_type == ""fasttext"" and word_ngrams == 1:\n            # remove the marking character at the end for subword fasttext model training\n            self._sentences = [\n                [w[:-1] if self.is_marked(w) else w for w in sentence]\n                for sentence in self._sentences\n            ]\n\n        logger.info(""training np2vec model"")\n        self._train()\n\n    def _train(self):\n        """"""\n        Train the np2vec model.\n        """"""\n        if self.word_embedding_type == ""word2vec"":\n            self.model = Word2Vec(\n                self._sentences,\n                sg=self.sg,\n                size=self.size,\n                window=self.window,\n                alpha=self.alpha,\n                min_alpha=self.min_alpha,\n                min_count=self.min_count,\n                sample=self.sample,\n                workers=self.workers,\n                hs=self.hs,\n                negative=self.negative,\n                cbow_mean=self.cbow_mean,\n                iter=self.iter,\n            )\n\n        elif self.word_embedding_type == ""fasttext"":\n            self.model = FastText(\n                self._sentences,\n                sg=self.sg,\n                size=self.size,\n                window=self.window,\n                alpha=self.alpha,\n                min_alpha=self.min_alpha,\n                min_count=self.min_count,\n                sample=self.sample,\n                workers=self.workers,\n                hs=self.hs,\n                negative=self.negative,\n                cbow_mean=self.cbow_mean,\n                iter=self.iter,\n                min_n=self.min_n,\n                max_n=self.max_n,\n                word_ngrams=self.word_ngrams,\n            )\n        else:\n            logger.error(""invalid word embedding type: %s"", self.word_embedding_type)\n            sys.exit(0)\n\n    def save(self, np2vec_model_file=""np2vec.model"", binary=False, word2vec_format=True):\n        """"""\n        Save the np2vec model.\n\n        Args:\n            np2vec_model_file (str): the file containing the np2vec model to load\n            binary (bool): boolean indicating whether the np2vec model to load is in binary format\n            word2vec_format(bool): boolean indicating whether to save the model in original\n            word2vec format.\n        """"""\n        if self.word_embedding_type == ""fasttext"" and self.word_ngrams == 1:\n            if not binary:\n                logger.error(\n                    ""if word_embedding_type is fasttext and word_ngrams is 1, ""\n                    ""binary should be set to True.""\n                )\n                sys.exit(0)\n            # not relevant to prune fasttext subword model\n            self.model.save(np2vec_model_file)\n        else:\n            # prune non NP terms\n            if self.prune_non_np:\n                logger.info(""pruning np2vec model"")\n                total_vec = 0\n                vector_size = self.model.vector_size\n                for word in self.model.wv.vocab.keys():\n                    if self.is_marked(word) and len(word) > 1:\n                        total_vec += 1\n                logger.info(\n                    ""storing %sx%s projection weights for NP\'s into %s"",\n                    total_vec,\n                    vector_size,\n                    np2vec_model_file,\n                )\n                with smart_open(np2vec_model_file, ""wb"") as fout:\n                    fout.write(utils.to_utf8(""%s %s\\n"" % (total_vec, vector_size)))\n                    # store NP vectors in sorted order: most frequent NP\'s at the top\n                    for word, vocab in sorted(\n                        iteritems(self.model.wv.vocab), key=lambda item: -item[1].count\n                    ):\n                        if self.is_marked(word) and len(word) > 1:  # discard empty marked np\'s\n                            embedding_vec = self.model.wv.syn0[vocab.index]\n                            if binary:\n                                fout.write(utils.to_utf8(word) + b"" "" + embedding_vec.tostring())\n                            else:\n                                fout.write(\n                                    utils.to_utf8(\n                                        ""%s %s\\n""\n                                        % (word, "" "".join(""%f"" % val for val in embedding_vec))\n                                    )\n                                )\n                if not word2vec_format:\n                    # pylint: disable=attribute-defined-outside-init\n                    self.model = KeyedVectors.load_word2vec_format(np2vec_model_file, binary=binary)\n            if not word2vec_format:\n                self.model.save(np2vec_model_file)\n\n    @classmethod\n    def load(cls, np2vec_model_file, binary=False, word_ngrams=0, word2vec_format=True):\n        """"""\n        Load the np2vec model.\n\n        Args:\n            np2vec_model_file (str): the file containing the np2vec model to load\n            binary (bool): boolean indicating whether the np2vec model to load is in binary format\n            word_ngrams (int {1,0}): If 1, np2vec model to load uses word vectors with subword (\n            ngrams) information.\n            word2vec_format(bool): boolean indicating whether the model to load has been stored in\n            original word2vec format.\n\n        Returns:\n            np2vec model to load\n        """"""\n        if word_ngrams == 0:\n            if word2vec_format:\n                return KeyedVectors.load_word2vec_format(np2vec_model_file, binary=binary)\n            return KeyedVectors.load(np2vec_model_file, mmap=""r"")\n        if word_ngrams == 1:\n            return FastText.load(np2vec_model_file)\n        logger.error(""invalid value for \'word_ngrams\'"")\n        return None\n'"
nlp_architect/models/np_semantic_segmentation.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport tensorflow as tf\n\n\n# taken from keras previous versions: https://github.com/keras-team/keras/issues/5400\ndef precision_score(y_true, y_pred):\n    """"""Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    """"""\n    K = tf.keras.backend\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n\ndef recall_score(y_true, y_pred):\n    """"""Recall metric.\n\n    Only computes a batch-wise average of recall.\n\n    Computes the recall, a metric for multi-label classification of\n    how many relevant items are selected.\n    """"""\n    K = tf.keras.backend\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n\ndef f1(y_true, y_pred):\n    """"""\n\n    Args:\n        y_true:\n        y_pred:\n\n    Returns:\n\n    """"""\n    K = tf.keras.backend\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n\nclass NpSemanticSegClassifier:\n    """"""\n    NP Semantic Segmentation classifier model (based on tf.Keras framework).\n\n    Args:\n        num_epochs(int): number of epochs to train the model\n        **callback_args (dict): callback args keyword arguments to init a Callback for the model\n        loss: the model\'s cost function. Default is \'tf.keras.losses.binary_crossentropy\' loss\n        optimizer (:obj:`tf.keras.optimizers`): the model\'s optimizer. Default is \'adam\'\n    """"""\n\n    def __init__(\n        self,\n        num_epochs,\n        callback_args,\n        loss=""binary_crossentropy"",\n        optimizer=""adam"",\n        batch_size=128,\n    ):\n        """"""\n        Args:\n            num_epochs(int): number of epochs to train the model\n            callback_args (dict): callback args keyword arguments to init Callback for the model\n            loss: the model\'s loss function. Default is \'tf.keras.losses.binary_crossentropy\' loss\n            optimizer (:obj:`tf.keras.optimizers`): the model\'s optimizer. Default is `adam`\n            batch_size (int):  batch size\n        """"""\n        self.model = None\n        self.loss = loss\n        self.optimizer = optimizer\n        self.epochs = num_epochs\n        self.callback_args = callback_args\n        self.batch_size = batch_size\n\n    def build(self, input_dim):\n        """"""\n        Build the model\'s layers\n        Args:\n            input_dim (int): the first layer\'s input_dim\n        """"""\n        first_layer_dens = 64\n        second_layer_dens = 64\n        output_layer_dens = 1\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Dense(first_layer_dens, activation=""relu"", input_dim=input_dim))\n        model.add(tf.keras.layers.Dropout(0.5))\n        model.add(tf.keras.layers.Dense(second_layer_dens, activation=""relu""))\n        model.add(tf.keras.layers.Dropout(0.5))\n        model.add(tf.keras.layers.Dense(output_layer_dens, activation=""sigmoid""))\n        metrics = [""binary_accuracy"", precision_score, recall_score, f1]\n        # Compile model\n        model.compile(loss=self.loss, optimizer=self.optimizer, metrics=metrics)\n        self.model = model\n\n    def fit(self, train_set):\n        """"""\n        Train and fit the model on the datasets\n\n        Args:\n            train_set (:obj:`numpy.ndarray`): The train set\n            args: callback_args and epochs from ArgParser input\n        """"""\n        self.model.fit(\n            train_set[""X""],\n            train_set[""y""],\n            epochs=self.epochs,\n            batch_size=self.batch_size,\n            verbose=2,\n        )\n\n    def save(self, model_path):\n        """"""\n        Save the model\'s prm file in model_path location\n\n        Args:\n            model_path(str): local path for saving the model\n        """"""\n        # serialize model to JSON\n        model_json = self.model.to_json()\n        with open(model_path[:-2] + ""json"", ""w"") as json_file:\n            json_file.write(model_json)\n        # serialize weights to HDF5\n        self.model.save_weights(model_path)\n        print(""Saved model to disk"")\n\n    def load(self, model_path):\n        """"""\n        Load pre-trained model\'s .h5 file to NpSemanticSegClassifier object\n\n        Args:\n            model_path(str): local path for loading the model\n        """"""\n        # load json and create model\n        with open(model_path[:-2] + ""json"", ""r"") as json_file:\n            loaded_model_json = json_file.read()\n        loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n        # load weights into new model\n        loaded_model.load_weights(model_path)\n        print(""Loaded model from disk"")\n        self.model = loaded_model\n\n    def eval(self, test_set):\n        """"""\n        Evaluate the model\'s test_set on error_rate, test_accuracy_rate and precision_recall_rate\n\n        Args:\n            test_set (:obj:`numpy.ndarray`): The test set\n\n        Returns:\n            tuple(float): loss, binary_accuracy, precision, recall and f1 measures\n        """"""\n        return self.model.evaluate(test_set[""X""], test_set[""y""], batch_size=128, verbose=2)\n\n    def get_outputs(self, test_set):\n        """"""\n        Classify the dataset on the model\n\n        Args:\n            test_set (:obj:`numpy.ndarray`): The test set\n\n        Returns:\n            list(:obj:`numpy.ndarray`): model\'s predictions\n        """"""\n        return self.model.predict(test_set)\n'"
nlp_architect/models/pretrained_models.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom nlp_architect.utils.io import uncompress_file, zipfile_list\nfrom nlp_architect.utils.file_cache import cached_path\n\nfrom nlp_architect import LIBRARY_OUT\n\nS3_PREFIX = ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/""\n\n\nclass PretrainedModel:\n\n    """""" Generic class to download the pre-trained models\n\n    Usage Example:\n\n    chunker = ChunkerModel.get_instance()\n    chunker2 = ChunkerModel.get_instance()\n    print(chunker, chunker2)\n    print(""Local File path = "", chunker.get_file_path())\n    files_models = chunker2.get_model_files()\n    for idx, file_name in enumerate(files_models):\n        print(str(idx) + "": "" + file_name)\n\n    """"""\n\n    def __init__(self, model_name, sub_path, files):\n        if isinstance(self, (BistModel, ChunkerModel, MrcModel, IntentModel, AbsaModel, NerModel)):\n            if self._instance is not None:  # pylint: disable=no-member\n                raise Exception(""This class is a singleton!"")\n        self.model_name = model_name\n        self.base_path = S3_PREFIX + sub_path\n        self.files = files\n        self.download_path = LIBRARY_OUT / ""pretrained_models"" / self.model_name\n        self.model_files = []\n\n    @classmethod\n    # pylint: disable=no-member\n    def get_instance(cls):\n        """"""\n        Static instance access method\n        Args:\n            cls (Class name): Calling class\n        """"""\n        if cls._instance is None:\n            cls()  # pylint: disable=no-value-for-parameter\n        return cls._instance\n\n    def get_file_path(self):\n        """"""\n        Return local file path of downloaded model files\n        """"""\n        for filename in self.files:\n            cached_file_path, need_downloading = cached_path(\n                self.base_path + filename, self.download_path\n            )\n            if filename.endswith(""zip""):\n                if need_downloading:\n                    print(""Unzipping..."")\n                    uncompress_file(cached_file_path, outpath=self.download_path)\n                    print(""Done."")\n        return self.download_path\n\n    def get_model_files(self):\n        """"""\n        Return individual file names of downloaded models\n        """"""\n        for fileName in self.files:\n            cached_file_path, need_downloading = cached_path(\n                self.base_path + fileName, self.download_path\n            )\n            if fileName.endswith(""zip""):\n                if need_downloading:\n                    print(""Unzipping..."")\n                    uncompress_file(cached_file_path, outpath=self.download_path)\n                    print(""Done."")\n                self.model_files.extend(zipfile_list(cached_file_path))\n            else:\n                self.model_files.extend([fileName])\n        return self.model_files\n\n\n# Model-specific classes developers instantiate where model has to be used\n\n\nclass BistModel(PretrainedModel):\n    """"""\n    Download and process (unzip) pre-trained BIST model\n    """"""\n\n    _instance = None\n    sub_path = ""models/dep_parse/""\n    files = [""bist-pretrained.zip""]\n\n    def __init__(self):\n        super().__init__(""bist"", self.sub_path, self.files)\n        BistModel._instance = self\n\n\nclass IntentModel(PretrainedModel):\n    """"""\n    Download and process (unzip) pre-trained Intent model\n    """"""\n\n    _instance = None\n    sub_path = ""models/intent/""\n    files = [""model_info.dat"", ""model.h5""]\n\n    def __init__(self):\n        super().__init__(""intent"", self.sub_path, self.files)\n        IntentModel._instance = self\n\n\nclass MrcModel(PretrainedModel):\n    """"""\n    Download and process (unzip) pre-trained MRC model\n    """"""\n\n    _instance = None\n    sub_path = ""models/mrc/""\n    files = [""mrc_data.zip"", ""mrc_model.zip""]\n\n    def __init__(self):\n        super().__init__(""mrc"", self.sub_path, self.files)\n        MrcModel._instance = self\n\n\nclass NerModel(PretrainedModel):\n    """"""\n    Download and process (unzip) pre-trained NER model\n    """"""\n\n    _instance = None\n    sub_path = ""models/ner/""\n    files = [""model_v4.h5"", ""model_info_v4.dat""]\n\n    def __init__(self):\n        super().__init__(""ner"", self.sub_path, self.files)\n        NerModel._instance = self\n\n\nclass AbsaModel(PretrainedModel):\n    """"""\n    Download and process (unzip) pre-trained ABSA model\n    """"""\n\n    _instance = None\n    sub_path = ""models/absa/""\n    files = [""rerank_model.h5""]\n\n    def __init__(self):\n        super().__init__(""absa"", self.sub_path, self.files)\n        AbsaModel._instance = self\n\n\nclass ChunkerModel(PretrainedModel):\n    """"""\n    Download and process (unzip) pre-trained Chunker model\n    """"""\n\n    _instance = None\n    sub_path = ""models/chunker/""\n    files = [""model.h5"", ""model_info.dat.params""]\n\n    def __init__(self):\n        super().__init__(""chunker"", self.sub_path, self.files)\n        ChunkerModel._instance = self\n'"
nlp_architect/models/tagging.py,31,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport io\nimport logging\nimport os\nimport pickle\nfrom typing import List\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nfrom tqdm import tqdm, trange\n\nfrom nlp_architect.data.sequential_tagging import TokenClsInputExample\nfrom nlp_architect.models import TrainableModel\nfrom nlp_architect.nn.torch.layers import CRF\nfrom nlp_architect.nn.torch.distillation import TeacherStudentDistill\nfrom nlp_architect.nn.torch.modules.embedders import IDCNN\nfrom nlp_architect.utils.metrics import tagging\nfrom nlp_architect.utils.text import Vocabulary, char_to_id\n\nlogger = logging.getLogger(__name__)\n\n\nclass NeuralTagger(TrainableModel):\n    """"""\n    Simple neural tagging model\n    Supports pytorch embedder models, multi-gpu training, KD from teacher models\n\n    Args:\n        embedder_model: pytorch embedder model (valid nn.Module model)\n        word_vocab (Vocabulary): word vocabulary\n        labels (List, optional): list of labels. Defaults to None\n        use_crf (bool, optional): use CRF a the classifier (instead of Softmax). Defaults to False.\n        device (str, optional): device backend. Defatuls to \'cpu\'.\n        n_gpus (int, optional): number of gpus. Default to 0.\n    """"""\n\n    def __init__(\n        self,\n        embedder_model,\n        word_vocab: Vocabulary,\n        labels: List[str] = None,\n        use_crf: bool = False,\n        device: str = ""cpu"",\n        n_gpus=0,\n    ):\n        super(NeuralTagger, self).__init__()\n        self.model = embedder_model\n        self.labels = labels\n        self.num_labels = len(labels) + 1  # +1 for padding\n        self.label_str_id = {l: i for i, l in enumerate(self.labels, 1)}\n        self.label_id_str = {v: k for k, v in self.label_str_id.items()}\n        self.word_vocab = word_vocab\n        self.use_crf = use_crf\n        if self.use_crf:\n            self.crf = CRF(self.num_labels, batch_first=True)\n        self.device = device\n        self.n_gpus = n_gpus\n        self.to(self.device, self.n_gpus)\n\n    def convert_to_tensors(\n        self,\n        examples: List[TokenClsInputExample],\n        max_seq_length: int = 128,\n        max_word_length: int = 12,\n        pad_id: int = 0,\n        labels_pad_id: int = 0,\n        include_labels: bool = True,\n    ) -> TensorDataset:\n        """"""\n        Convert examples to valid tagger dataset\n\n        Args:\n            examples (List[TokenClsInputExample]): List of examples\n            max_seq_length (int, optional): max words per sentence. Defaults to 128.\n            max_word_length (int, optional): max characters in a word. Defaults to 12.\n            pad_id (int, optional): padding int id. Defaults to 0.\n            labels_pad_id (int, optional): labels padding id. Defaults to 0.\n            include_labels (bool, optional): include labels in dataset. Defaults to True.\n\n        Returns:\n            TensorDataset: TensorDataset for given examples\n        """"""\n\n        features = []\n        for example in examples:\n            word_tokens = [self.word_vocab[t] for t in example.tokens]\n            labels = []\n            if include_labels:\n                labels = [self.label_str_id.get(label) for label in example.label]\n            word_chars = []\n            for word in example.tokens:\n                word_chars.append([char_to_id(c) for c in word])\n            word_shapes = example.shapes\n\n            # cut up to max length\n            word_tokens = word_tokens[:max_seq_length]\n            word_shapes = word_shapes[:max_seq_length]\n            if include_labels:\n                labels = labels[:max_seq_length]\n            word_chars = word_chars[:max_seq_length]\n            for i in range(len(word_chars)):\n                word_chars[i] = word_chars[i][:max_word_length]\n            mask = [1] * len(word_tokens)\n\n            # Zero-pad up to the sequence length.\n            padding_length = max_seq_length - len(word_tokens)\n            input_ids = word_tokens + ([pad_id] * padding_length)\n            shape_ids = word_shapes + ([pad_id] * padding_length)\n            mask = mask + ([0] * padding_length)\n            if include_labels:\n                label_ids = labels + ([labels_pad_id] * padding_length)\n\n            word_char_ids = []\n            # pad word vectors\n            for i in range(len(word_chars)):\n                word_char_ids.append(\n                    word_chars[i] + ([pad_id] * (max_word_length - len(word_chars[i])))\n                )\n\n            # pad word vectors with remaining zero vectors\n            for _ in range(padding_length):\n                word_char_ids.append(([pad_id] * max_word_length))\n\n            assert len(input_ids) == max_seq_length\n            assert len(shape_ids) == max_seq_length\n            if include_labels:\n                assert len(label_ids) == max_seq_length\n            assert len(word_char_ids) == max_seq_length\n            for i in range(len(word_char_ids)):\n                assert len(word_char_ids[i]) == max_word_length\n\n            features.append(\n                InputFeatures(\n                    input_ids,\n                    word_char_ids,\n                    shape_ids,\n                    mask=mask,\n                    label_id=label_ids if include_labels else None,\n                )\n            )\n\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_char_ids = torch.tensor([f.char_ids for f in features], dtype=torch.long)\n        all_shape_ids = torch.tensor([f.shape_ids for f in features], dtype=torch.long)\n        masks = torch.tensor([f.mask for f in features], dtype=torch.long)\n\n        if include_labels:\n            is_labeled = torch.tensor([True for f in features], dtype=torch.bool)\n            all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n            dataset = TensorDataset(\n                all_input_ids, all_char_ids, all_shape_ids, masks, is_labeled, all_label_ids\n            )\n        else:\n            is_labeled = torch.tensor([False for f in features], dtype=torch.bool)\n            dataset = TensorDataset(all_input_ids, all_char_ids, all_shape_ids, masks, is_labeled)\n        return dataset\n\n    def get_optimizer(self, opt_fn=None, lr: int = 0.001):\n        """"""\n        Get default optimizer\n\n        Args:\n            lr (int, optional): learning rate. Defaults to 0.001.\n\n        Returns:\n            torch.optim.Optimizer: optimizer\n        """"""\n        params = self.model.parameters()\n        if self.use_crf:\n            params = list(params) + list(self.crf.parameters())\n        if opt_fn is None:\n            opt_fn = optim.Adam\n        return opt_fn(params, lr=lr)\n\n    @staticmethod\n    def batch_mapper(batch):\n        """"""\n        Map batch to correct input names\n        """"""\n        mapping = {\n            ""words"": batch[0],\n            ""word_chars"": batch[1],\n            ""shapes"": batch[2],\n            ""mask"": batch[3],\n            ""is_labeled"": batch[4],\n        }\n        if len(batch) == 6:\n            mapping.update({""labels"": batch[5]})\n        return mapping\n\n    def train(\n        self,\n        train_data_set: DataLoader,\n        dev_data_set: DataLoader = None,\n        test_data_set: DataLoader = None,\n        epochs: int = 3,\n        batch_size: int = 8,\n        optimizer=None,\n        max_grad_norm: float = 5.0,\n        logging_steps: int = 50,\n        save_steps: int = 100,\n        save_path: str = None,\n        distiller: TeacherStudentDistill = None,\n        best_result_file: str = None,\n        word_dropout: float = 0,\n    ):\n        """"""\n        Train a tagging model\n\n        Args:\n            train_data_set (DataLoader): train examples dataloader.\n                - If distiller object is provided train examples should contain a tuple of\n                  student/teacher data examples.\n            dev_data_set (DataLoader, optional): dev examples dataloader. Defaults to None.\n            test_data_set (DataLoader, optional): test examples dataloader. Defaults to None.\n            epochs (int, optional): num of epochs to train. Defaults to 3.\n            batch_size (int, optional): batch size. Defaults to 8.\n            optimizer (fn, optional): optimizer function. Defaults to default model optimizer.\n            max_grad_norm (float, optional): max gradient norm. Defaults to 5.0.\n            logging_steps (int, optional): number of steps between logging. Defaults to 50.\n            save_steps (int, optional): number of steps between model saves. Defaults to 100.\n            save_path (str, optional): model output path. Defaults to None.\n            distiller (TeacherStudentDistill, optional): KD model for training the model using\n            a teacher model. Defaults to None.\n            best_result_file (str, optional): path to save best dev results when it\'s updated.\n            word_dropout (float, optional): whole-word (-> oov) dropout rate. Defaults to 0.\n        """"""\n        if optimizer is None:\n            optimizer = self.get_optimizer()\n        train_batch_size = batch_size * max(1, self.n_gpus)\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_data_set.dataset))\n        logger.info(""  Num Epochs = %d"", epochs)\n        logger.info(""  Instantaneous batch size per GPU/CPU = %d"", batch_size)\n        logger.info(""  Total batch size = %d"", train_batch_size)\n        global_step = 0\n        best_dev = 0\n        dev_test = 0\n        self.model.zero_grad()\n        epoch_it = trange(epochs, desc=""Epoch"")\n        for epoch in epoch_it:\n            step_it = tqdm(train_data_set, desc=""Train iteration"")\n            avg_loss = 0\n\n            for step, batches in enumerate(step_it):\n                self.model.train()\n\n                batch, t_batch = (batches, []) if not distiller else (batches[:2])\n                batch = tuple(t.to(self.device) for t in batch)\n                inputs = self.batch_mapper(batch)\n                logits = self.model(**inputs)\n\n                if distiller:\n                    t_batch = tuple(t.to(self.device) for t in t_batch)\n                    t_logits = distiller.get_teacher_logits(t_batch)\n                    valid_positions = (\n                        t_batch[3] != 0.0\n                    )  # TODO: implement method to get only valid logits from the model itself\n                    valid_t_logits = {}\n                    max_seq_len = logits.shape[1]\n                    for i in range(len(logits)):  # each example in batch\n                        valid_logit_i = t_logits[i][valid_positions[i]]\n                        valid_t_logits[i] = (\n                            valid_logit_i\n                            if valid_logit_i.shape[0] <= max_seq_len\n                            else valid_logit_i[:][:max_seq_len]\n                        )  # cut to max len\n\n                    # prepare teacher labels for non-labeled examples\n                    t_labels_dict = {}\n                    for i in range(len(valid_t_logits.keys())):\n                        t_labels_dict[i] = torch.argmax(\n                            F.log_softmax(valid_t_logits[i], dim=-1), dim=-1\n                        )\n\n                # pseudo labeling\n                for i, is_labeled in enumerate(inputs[""is_labeled""]):\n                    if not is_labeled:\n                        t_labels_i = t_labels_dict[i]\n                        # add the padded teacher label:\n                        inputs[""labels""][i] = torch.cat(\n                            (\n                                t_labels_i,\n                                torch.zeros([max_seq_len - len(t_labels_i)], dtype=torch.long).to(\n                                    self.device\n                                ),\n                            ),\n                            0,\n                        )\n\n                # apply word dropout to the input\n                if word_dropout != 0:\n                    tokens = inputs[""words""]\n                    tokens = np.array(tokens.detach().cpu())\n                    word_probs = np.random.random(tokens.shape)\n                    drop_indices = np.where(\n                        (word_probs > word_dropout) & (tokens != 0)\n                    )  # ignore padding indices\n                    inputs[""words""][drop_indices[0], drop_indices[1]] = self.word_vocab.oov_id\n\n                # loss\n                if self.use_crf:\n                    loss = -1.0 * self.crf(logits, inputs[""labels""], mask=inputs[""mask""] != 0.0)\n                else:\n                    loss_fn = CrossEntropyLoss(ignore_index=0)\n                    loss = loss_fn(logits.view(-1, self.num_labels), inputs[""labels""].view(-1))\n\n                # for idcnn training - add dropout penalty loss\n                module = self.model.module if self.n_gpus > 1 else self.model\n                if isinstance(module, IDCNN) and module.drop_penalty != 0:\n                    logits_no_drop = self.model(**inputs, no_dropout=True)\n                    sub = logits.sub(logits_no_drop)\n                    drop_loss = torch.div(torch.sum(torch.pow(sub, 2)), 2)\n                    loss += module.drop_penalty * drop_loss\n\n                if self.n_gpus > 1:\n                    loss = loss.mean()\n\n                # add distillation loss if activated\n                if distiller:\n                    # filter masked student logits (no padding)\n                    valid_s_logits = {}\n                    valid_s_positions = inputs[""mask""] != 0.0\n                    for i in range(len(logits)):\n                        valid_s_logit_i = logits[i][valid_s_positions[i]]\n                        valid_s_logits[i] = valid_s_logit_i\n                    loss = distiller.distill_loss_dict(loss, valid_s_logits, valid_t_logits)\n\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n\n                optimizer.step()\n                optimizer.zero_grad()\n                global_step += 1\n                avg_loss += loss.item()\n                if global_step % logging_steps == 0:\n                    if step != 0:\n                        logger.info(\n                            "" global_step = %s, average loss = %s"", global_step, avg_loss / step\n                        )\n                        best_dev, dev_test = self.update_best_model(\n                            dev_data_set,\n                            test_data_set,\n                            best_dev,\n                            dev_test,\n                            best_result_file,\n                            avg_loss / step,\n                            epoch,\n                            save_path=None,\n                        )\n                if save_steps != 0 and save_path is not None and global_step % save_steps == 0:\n                    self.save_model(save_path)\n        self.update_best_model(\n            dev_data_set,\n            test_data_set,\n            best_dev,\n            dev_test,\n            best_result_file,\n            ""end_training"",\n            ""end_training"",\n            save_path=save_path + ""/best_dev"",\n        )\n\n    def _get_eval(self, ds, set_name):\n        if ds is not None:\n            logits, out_label_ids = self.evaluate(ds)\n            res = self.evaluate_predictions(logits, out_label_ids)\n            logger.info("" {} set F1 = {}"".format(set_name, res[""f1""]))\n            return res[""f1""]\n        return None\n\n    def to(self, device=""cpu"", n_gpus=0):\n        """"""\n        Put model on given device\n\n        Args:\n            device (str, optional): device backend. Defaults to \'cpu\'.\n            n_gpus (int, optional): number of gpus. Defaults to 0.\n        """"""\n        if self.model is not None:\n            self.model.to(device)\n            if self.use_crf:\n                self.crf.to(device)\n            if n_gpus > 1:\n                self.model = torch.nn.DataParallel(self.model)\n                if self.use_crf:\n                    self.crf = torch.nn.DataParallel(self.crf)\n        self.device = device\n        self.n_gpus = n_gpus\n\n    def evaluate(self, data_set: DataLoader):\n        """"""\n        Run evaluation on given dataloader\n\n        Args:\n            data_set (DataLoader): a data loader to run evaluation on\n\n        Returns:\n            logits, labels (if labels are given)\n        """"""\n        logger.info(""***** Running inference *****"")\n        logger.info("" Batch size: {}"".format(data_set.batch_size))\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(data_set, desc=""Inference iteration""):\n            self.model.eval()\n            batch = tuple(t.to(self.device) for t in batch)\n            with torch.no_grad():\n                inputs = self.batch_mapper(batch)\n                logits = self.model(**inputs)\n            model_output = logits.detach().cpu()\n            model_out_label_ids = inputs[""labels""].detach().cpu() if ""labels"" in inputs else None\n            if preds is None:\n                preds = model_output\n                out_label_ids = model_out_label_ids\n            else:\n                preds = torch.cat((preds, model_output), dim=0)\n                out_label_ids = (\n                    torch.cat((out_label_ids, model_out_label_ids), dim=0)\n                    if out_label_ids is not None\n                    else None\n                )\n        output = (preds,)\n        if out_label_ids is not None:\n            output = output + (out_label_ids,)\n        return output\n\n    def evaluate_predictions(self, logits, label_ids):\n        """"""\n        Evaluate given logits on truth labels\n\n        Args:\n            logits: logits of model\n            label_ids: truth label ids\n\n        Returns:\n            dict: dictionary containing P/R/F1 metrics\n        """"""\n        active_positions = label_ids.view(-1) != 0.0\n        active_labels = label_ids.view(-1)[active_positions]\n        if self.use_crf:\n            logits_shape = logits.size()\n            decode_ap = active_positions.view(logits_shape[0], logits_shape[1]) != 0.0\n            if self.n_gpus > 1:\n                decode_fn = self.crf.module.decode\n            else:\n                decode_fn = self.crf.decode\n            logits = decode_fn(logits.to(self.device), mask=decode_ap.to(self.device))\n            logits = [lll for ll in logits for lll in ll]\n        else:\n            active_logits = logits.view(-1, len(self.label_id_str) + 1)[active_positions]\n            logits = torch.argmax(F.log_softmax(active_logits, dim=1), dim=1)\n            logits = logits.detach().cpu().numpy()\n        out_label_ids = active_labels.detach().cpu().numpy()\n        y_true, y_pred = self.extract_labels(out_label_ids, logits)\n        p, r, f1 = tagging(y_pred, y_true)\n        return {""p"": p, ""r"": r, ""f1"": f1}\n\n    def update_best_model(\n        self,\n        dev_data_set,\n        test_data_set,\n        best_dev,\n        best_dev_test,\n        best_result_file,\n        loss,\n        epoch,\n        save_path=None,\n    ):\n        new_best_dev = best_dev\n        new_test = best_dev_test\n        dev = self._get_eval(dev_data_set, ""dev"")\n        test = self._get_eval(test_data_set, ""test"")\n        if dev > best_dev:\n            new_best_dev = dev\n            new_test = test\n            if best_result_file is not None:\n                with open(best_result_file, ""a+"") as f:\n                    f.write(\n                        ""best dev= ""\n                        + str(new_best_dev)\n                        + "", test= ""\n                        + str(new_test)\n                        + "", loss= ""\n                        + str(loss)\n                        + "", epoch= ""\n                        + str(epoch)\n                        + ""\\n""\n                    )\n        logger.info(""Best result: Dev=%s, Test=%s"", str(new_best_dev), str(new_test))\n        if save_path is not None:\n            self.save_model(save_path)\n        return new_best_dev, new_test\n\n    def extract_labels(self, label_ids, logits):\n        label_map = self.label_id_str\n        y_true = []\n        y_pred = []\n        for p, y in zip(logits, label_ids):\n            y_pred.append(label_map.get(p, ""O""))\n            y_true.append(label_map.get(y, ""O""))\n        assert len(y_true) == len(y_pred)\n        return (y_true, y_pred)\n\n    def inference(self, examples: List[TokenClsInputExample], batch_size: int = 64):\n        """"""\n        Do inference on given examples\n\n        Args:\n            examples (List[TokenClsInputExample]): examples\n            batch_size (int, optional): batch size. Defaults to 64.\n\n        Returns:\n            List(tuple): a list of tuples of tokens, tags predicted by model\n        """"""\n        data_set = self.convert_to_tensors(examples, include_labels=False)\n        inf_sampler = SequentialSampler(data_set)\n        inf_dataloader = DataLoader(data_set, sampler=inf_sampler, batch_size=batch_size)\n        logits = self.evaluate(inf_dataloader)\n        active_positions = data_set.tensors[-1].view(len(data_set), -1) != 0.0\n        logits = torch.argmax(F.log_softmax(logits[0], dim=2), dim=2)\n        res_ids = []\n        for i in range(logits.size()[0]):\n            res_ids.append(logits[i][active_positions[i]].detach().cpu().numpy())\n        output = []\n        for tag_ids, ex in zip(res_ids, examples):\n            tokens = ex.tokens\n            tags = [self.label_id_str.get(t, ""O"") for t in tag_ids]\n            output.append((tokens, tags))\n        return output\n\n    def save_model(self, output_dir: str):\n        """"""\n        Save model to path\n\n        Args:\n            output_dir (str): output directory\n        """"""\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        torch.save(self.model, os.path.join(output_dir, ""model.bin""))\n        if self.use_crf:\n            torch.save(self.crf, os.path.join(output_dir, ""crf.bin""))\n        with io.open(output_dir + os.sep + ""labels.txt"", ""w"", encoding=""utf-8"") as fw:\n            for lbl in self.labels:\n                fw.write(""{}\\n"".format(lbl))\n        with io.open(output_dir + os.sep + ""w_vocab.dat"", ""wb"") as fw:\n            pickle.dump(self.word_vocab, fw)\n\n    @classmethod\n    def load_model(cls, model_path: str):\n        """"""\n        Load a tagger model from given path\n\n        Args:\n            model_path (str): model path\n\n            NeuralTagger: tagger model loaded from path\n        """"""\n        # Load a trained model and vocabulary from given path\n        if not os.path.exists(model_path):\n            raise FileNotFoundError\n        with io.open(model_path + os.sep + ""labels.txt"") as fp:\n            labels = [lines.strip() for lines in fp.readlines()]\n\n        with io.open(model_path + os.sep + ""w_vocab.dat"", ""rb"") as fp:\n            w_vocab = pickle.load(fp)\n        # load model.bin into\n        model_file_path = model_path + os.sep + ""model.bin""\n        if not os.path.exists(model_file_path):\n            raise FileNotFoundError\n        model = torch.load(model_file_path)\n        new_class = cls(model, w_vocab, labels)\n        crf_file_path = model_path + os.sep + ""crf.bin""\n        if os.path.exists(crf_file_path):\n            new_class.use_crf = True\n            new_class.crf = torch.load(crf_file_path)\n        else:\n            new_class.use_crf = False\n        return new_class\n\n    def get_logits(self, batch):\n        self.model.eval()\n        inputs = self.batch_mapper(batch)\n        outputs = self.model(**inputs)\n        return outputs[-1]\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, char_ids, shape_ids, mask=None, label_id=None):\n        self.input_ids = input_ids\n        self.char_ids = char_ids\n        self.shape_ids = shape_ids\n        self.mask = mask\n        self.label_id = label_id\n'"
nlp_architect/models/temporal_convolutional_network.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport tensorflow as tf\n\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras.layers import Wrapper\nfrom tensorflow.python.layers.convolutional import Conv1D\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.keras.engine.base_layer import Layer\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.keras.engine.base_layer import InputSpec\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.framework import ops\n\n\n# ***NOTE***: The WeightNorm Class is copied from this PR:\n# https://github.com/tensorflow/tensorflow/issues/14070\n# Once this becomes part of the official TF release, it will be removed\nclass WeightNorm(Wrapper):\n    """""" This wrapper reparameterizes a layer by decoupling the weight\'s\n    magnitude and direction. This speeds up convergence by improving the\n    conditioning of the optimization problem.\n\n    Weight Normalization: A Simple Reparameterization to Accelerate\n    Training of Deep Neural Networks: https://arxiv.org/abs/1602.07868\n    Tim Salimans, Diederik P. Kingma (2016)\n\n    WeightNorm wrapper works for keras and tf layers.\n\n    ```python\n      net = WeightNorm(tf.keras.layers.Conv2D(2, 2, activation=\'relu\'),\n             input_shape=(32, 32, 3), data_init=True)(x)\n      net = WeightNorm(tf.keras.layers.Conv2D(16, 5, activation=\'relu\'),\n                       data_init=True)\n      net = WeightNorm(tf.keras.layers.Dense(120, activation=\'relu\'),\n                       data_init=True)(net)\n      net = WeightNorm(tf.keras.layers.Dense(n_classes),\n                       data_init=True)(net)\n    ```\n\n    Arguments:\n      layer: a layer instance.\n      data_init: If `True` use data dependent variable initialization\n\n    Raises:\n      ValueError: If not initialized with a `Layer` instance.\n      ValueError: If `Layer` does not contain a `kernel` of weights\n      NotImplementedError: If `data_init` is True and running graph execution\n    """"""\n\n    def __init__(self, layer, data_init=False, **kwargs):\n        if not isinstance(layer, Layer):\n            raise ValueError(\n                ""Please initialize `WeightNorm` layer with a ""\n                ""`Layer` instance. You passed: {input}"".format(input=layer)\n            )\n\n        if not context.executing_eagerly() and data_init:\n            raise NotImplementedError(\n                ""Data dependent variable initialization is not available for "" ""graph execution""\n            )\n\n        self.initialized = True\n        if data_init:\n            self.initialized = False\n\n        self.layer_depth = None\n        self.norm_axes = None\n        super(WeightNorm, self).__init__(layer, **kwargs)\n        self._track_trackable(layer, name=""layer"")\n\n    def _compute_weights(self):\n        """"""Generate weights by combining the direction of weight vector\n         with it\'s norm """"""\n        with variable_scope.variable_scope(""compute_weights""):\n            self.layer.kernel = (\n                nn_impl.l2_normalize(self.layer.v, axis=self.norm_axes) * self.layer.g\n            )\n\n    def _init_norm(self, weights):\n        """"""Set the norm of the weight vector""""""\n        from tensorflow.python.ops.linalg_ops import norm\n\n        with variable_scope.variable_scope(""init_norm""):\n            # pylint: disable=no-member\n            flat = array_ops.reshape(weights, [-1, self.layer_depth])\n            # pylint: disable=no-member\n            return array_ops.reshape(norm(flat, axis=0), (self.layer_depth,))\n\n    def _data_dep_init(self, inputs):\n        """"""Data dependent initialization for eager execution""""""\n        from tensorflow.python.ops.nn import moments\n        from tensorflow.python.ops.math_ops import sqrt\n\n        with variable_scope.variable_scope(""data_dep_init""):\n            # Generate data dependent init values\n            activation = self.layer.activation\n            self.layer.activation = None\n            x_init = self.layer.call(inputs)\n            m_init, v_init = moments(x_init, self.norm_axes)\n            scale_init = 1.0 / sqrt(v_init + 1e-10)\n\n        # Assign data dependent init values\n        self.layer.g = self.layer.g * scale_init\n        self.layer.bias = -1 * m_init * scale_init\n        self.layer.activation = activation\n        self.initialized = True\n\n    # pylint: disable=signature-differs\n    def build(self, input_shape):\n        """"""Build `Layer`""""""\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        self.input_spec = InputSpec(shape=input_shape)\n\n        if not self.layer.built:\n            self.layer.build(input_shape)\n            self.layer.built = False\n\n            if not hasattr(self.layer, ""kernel""):\n                raise ValueError(\n                    ""`WeightNorm` must wrap a layer that"" "" contains a `kernel` for weights""\n                )\n\n            # The kernel\'s filter or unit dimension is -1\n            self.layer_depth = int(self.layer.kernel.shape[-1])\n            self.norm_axes = list(range(self.layer.kernel.shape.ndims - 1))\n\n            self.layer.v = self.layer.kernel\n            self.layer.g = self.layer.add_variable(\n                name=""g"",\n                shape=(self.layer_depth,),\n                initializer=initializers.get(""ones""),\n                dtype=self.layer.kernel.dtype,\n                trainable=True,\n            )\n\n            with ops.control_dependencies([self.layer.g.assign(self._init_norm(self.layer.v))]):\n                self._compute_weights()\n\n            self.layer.built = True\n\n        super(WeightNorm, self).build()\n        self.built = True\n\n    # pylint: disable=arguments-differ\n    def call(self, inputs):\n        """"""Call `Layer`""""""\n        if context.executing_eagerly():\n            if not self.initialized:\n                self._data_dep_init(inputs)\n            self._compute_weights()  # Recompute weights for each forward pass\n\n        output = self.layer.call(inputs)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return tensor_shape.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n\n\nclass TCN:\n    """"""\n    This class defines core TCN architecture.\n    This is only the base class, training strategy is not implemented.\n    """"""\n\n    def __init__(self, max_len, n_features_in, hidden_sizes, kernel_size=7, dropout=0.2):\n        """"""\n        To use this class,\n            1. Inherit this class\n            2. Define the training losses in build_train_graph()\n            3. Define the training strategy in run()\n            4. After the inherited class object is initialized,\n               call build_train_graph followed by run\n\n        Args:\n            max_len: Maximum length of sequence\n            n_features_in: Number of input features (dimensions)\n            hidden_sizes: Number of hidden sizes in each layer of TCN (same for all layers)\n            kernel_size: Kernel size of convolution filter (same for all layers)\n            dropout: Dropout, fraction of activations to drop\n        """"""\n        self.max_len = max_len\n        self.n_features_in = n_features_in\n        self.hidden_sizes = hidden_sizes\n        self.kernel_size = kernel_size\n        self.dropout = dropout\n        self.n_hidden_layers = len(self.hidden_sizes)\n        receptive_field_len = self.calculate_receptive_field()\n        if receptive_field_len < self.max_len:\n            print(\n                ""Warning! receptive field of the TCN: ""\n                ""%d is less than the input sequence length: %d.""\n                % (receptive_field_len, self.max_len)\n            )\n        else:\n            print(\n                ""Receptive field of the TCN: %d, input sequence length: %d.""\n                % (receptive_field_len, self.max_len)\n            )\n        self.layer_activations = []\n\n        # toggle this for train/inference mode\n        self.training_mode = tf.placeholder(tf.bool, name=""training_mode"")\n\n        self.sequence_output = None\n\n    def calculate_receptive_field(self):\n        """"""\n\n        Returns:\n\n        """"""\n        return 1 + 2 * (self.kernel_size - 1) * (2 ** self.n_hidden_layers - 1)\n\n    def build_network_graph(self, x, last_timepoint=False):\n        """"""\n        Given the input placeholder x, build the entire TCN graph\n        Args:\n            x: Input placeholder\n            last_timepoint: Whether or not to select only the last timepoint to output\n\n        Returns:\n            output of the TCN\n        """"""\n        # loop and define multiple residual blocks\n        with tf.variable_scope(""tcn""):\n            for i in range(self.n_hidden_layers):\n                dilation_size = 2 ** i\n                in_channels = self.n_features_in if i == 0 else self.hidden_sizes[i - 1]\n                out_channels = self.hidden_sizes[i]\n                with tf.variable_scope(""residual_block_"" + str(i)):\n                    x = self._residual_block(\n                        x,\n                        in_channels,\n                        out_channels,\n                        dilation_size,\n                        (self.kernel_size - 1) * dilation_size,\n                    )\n                    x = tf.nn.relu(x)\n                self.layer_activations.append(x)\n            self.sequence_output = x\n\n            # get outputs\n            if not last_timepoint:\n                prediction = self.sequence_output\n            else:\n                # last time point size (batch_size, hidden_sizes_encoder)\n                width = self.sequence_output.shape[1].value\n                lt = tf.squeeze(\n                    tf.slice(self.sequence_output, [0, width - 1, 0], [-1, 1, -1]), axis=1\n                )\n                prediction = tf.layers.Dense(\n                    1,\n                    kernel_initializer=tf.initializers.random_normal(0, 0.01),\n                    bias_initializer=tf.initializers.random_normal(0, 0.01),\n                )(lt)\n\n        return prediction\n\n    def _residual_block(self, x, in_channels, out_channels, dilation, padding):\n        """"""\n        Defines the residual block\n        Args:\n            x: Input tensor to residual block\n            in_channels: Number of input features (dimensions)\n            out_channels: Number of output features (dimensions)\n            dilation: Dilation rate\n            padding: Padding value\n\n        Returns:\n            Output of residual path\n        """"""\n        xin = x\n        # define two temporal blocks\n        for i in range(2):\n            with tf.variable_scope(""temporal_block_"" + str(i)):\n                x = self._temporal_block(x, out_channels, dilation, padding)\n\n        # sidepath\n        if in_channels != out_channels:\n            x_side = tf.layers.Conv1D(\n                filters=out_channels,\n                kernel_size=1,\n                padding=""same"",\n                strides=1,\n                activation=None,\n                dilation_rate=1,\n                kernel_initializer=tf.initializers.random_normal(0, 0.01),\n                bias_initializer=tf.initializers.random_normal(0, 0.01),\n            )(xin)\n        else:\n            x_side = xin\n\n        # combine both\n        return tf.add(x, x_side)\n\n    def _temporal_block(self, x, out_channels, dilation, padding):\n        """"""\n        Defines the temporal block, which is a dilated causual conv layer,\n        followed by relu and dropout\n        Args:\n            x: Input to temporal block\n            out_channels: Number of conv filters\n            dilation: dilation rate\n            padding: padding value\n\n        Returns:\n            Tensor output of temporal block\n        """"""\n        # conv layer\n        x = self._dilated_causal_conv(x, out_channels, dilation, padding)\n\n        x = tf.nn.relu(x)\n\n        # dropout\n        batch_size = tf.shape(x)[0]\n        x = tf.layers.dropout(\n            x,\n            rate=self.dropout,\n            noise_shape=[batch_size, 1, out_channels],\n            training=self.training_mode,\n        )\n\n        return x\n\n    # define model\n    def _dilated_causal_conv(self, x, n_filters, dilation, padding):\n        """"""\n        Defines dilated causal convolution\n        Args:\n            x: Input activation\n            n_filters: Number of convolution filters\n            dilation: Dilation rate\n            padding: padding value\n\n        Returns:\n            Tensor output of convolution\n        """"""\n        input_width = x.shape[1].value\n        with tf.variable_scope(""dilated_causal_conv""):\n            # define dilated convolution layer with left side padding\n            x = tf.pad(x, tf.constant([[0, 0], [padding, 0], [0, 0]]), ""CONSTANT"")\n            x = WeightNorm(\n                Conv1D(\n                    filters=n_filters,\n                    kernel_size=self.kernel_size,\n                    padding=""valid"",\n                    strides=1,\n                    activation=None,\n                    dilation_rate=dilation,\n                    kernel_initializer=tf.initializers.random_normal(0, 0.01),\n                    bias_initializer=tf.initializers.random_normal(0, 0.01),\n                )\n            )(x)\n\n        assert x.shape[1].value == input_width\n\n        return x\n\n    def build_train_graph(self, *args, **kwargs):\n        """"""\n        Placeholder for defining training losses and metrics\n        """"""\n        raise NotImplementedError(""Error! losses for training must be defined"")\n\n    def run(self, *args, **kwargs):\n        """"""\n        Placeholder for defining training strategy\n        """"""\n        raise NotImplementedError(""Error! training routine must be defined"")\n\n\nclass CommonLayers:\n    """"""\n    Class that contains the common layers for language modeling -\n            word embeddings and projection layer\n    """"""\n\n    def __init__(self):\n        """"""\n        Initialize class\n        """"""\n        self.word_embeddings_tf = None\n        self.num_words = None\n        self.n_features_in = None\n\n    def define_input_layer(\n        self, input_placeholder_tokens, word_embeddings, embeddings_trainable=True\n    ):\n        """"""\n        Define the input word embedding layer\n        Args:\n            input_placeholder_tokens: tf.placeholder, input to the model\n            word_embeddings: numpy array (optional), to initialize the embeddings with\n            embeddings_trainable: boolean, whether or not to train the embedding table\n\n        Returns:\n            Embeddings corresponding to the data in input placeholder\n        """"""\n        with tf.device(""/cpu:0""):\n            with tf.variable_scope(""embedding_layer"", reuse=False):\n                if word_embeddings is None:\n                    initializer = tf.initializers.random_normal(0, 0.01)\n                else:\n                    initializer = tf.constant_initializer(word_embeddings)\n                self.word_embeddings_tf = tf.get_variable(\n                    ""embedding_table"",\n                    shape=[self.num_words, self.n_features_in],\n                    initializer=initializer,\n                    trainable=embeddings_trainable,\n                )\n\n                input_embeddings = tf.nn.embedding_lookup(\n                    self.word_embeddings_tf, input_placeholder_tokens\n                )\n        return input_embeddings\n\n    def define_projection_layer(self, prediction, tied_weights=True):\n        """"""\n        Define the output word embedding layer\n        Args:\n            prediction: tf.tensor, the prediction from the model\n            tied_weights: boolean, whether or not to tie weights from the input embedding layer\n\n        Returns:\n            Probability distribution over vocabulary\n        """"""\n        with tf.device(""/cpu:0""):\n            if tied_weights:\n                # tie projection layer and embedding layer\n                with tf.variable_scope(""embedding_layer"", reuse=tf.AUTO_REUSE):\n                    softmax_w = tf.matrix_transpose(self.word_embeddings_tf)\n                    softmax_b = tf.get_variable(""softmax_b"", [self.num_words])\n                    _, l, k = prediction.shape.as_list()\n                    prediction_reshaped = tf.reshape(prediction, [-1, k])\n                    mult_out = tf.nn.bias_add(tf.matmul(prediction_reshaped, softmax_w), softmax_b)\n                    projection_out = tf.reshape(mult_out, [-1, l, self.num_words])\n            else:\n                with tf.variable_scope(""projection_layer"", reuse=False):\n                    projection_out = tf.layers.Dense(self.num_words)(prediction)\n        return projection_out\n'"
nlp_architect/nlp/__init__.py,0,b''
nlp_architect/nn/__init__.py,0,b''
nlp_architect/pipelines/__init__.py,0,b''
nlp_architect/pipelines/spacy_bist.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom os import path, remove, makedirs\n\nfrom nlp_architect.common.core_nlp_doc import CoreNLPDoc\nfrom nlp_architect.data.conll import ConllEntry\nfrom nlp_architect.models.bist_parser import BISTModel\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.utils.io import download_unlicensed_file, uncompress_file\nfrom nlp_architect.utils.io import validate\nfrom nlp_architect.utils.text import SpacyInstance\n\n\nclass SpacyBISTParser(object):\n    """"""Main class which handles parsing with Spacy-BIST parser.\n\n    Args:\n        verbose (bool, optional): Controls output verbosity.\n        spacy_model (str, optional): Spacy model to use\n        (see https://spacy.io/api/top-level#spacy.load).\n        bist_model (str, optional): Path to a .model file to load. Defaults pre-trained model\'.\n    """"""\n\n    dir = LIBRARY_OUT / ""bist-pretrained""\n    _pretrained = dir / ""bist.model""\n\n    def __init__(self, verbose=False, spacy_model=""en"", bist_model=None):\n        validate(\n            (verbose, bool), (spacy_model, str, 0, 1000), (bist_model, (type(None), str), 0, 1000)\n        )\n        if not bist_model:\n            print(""Using pre-trained BIST model."")\n            _download_pretrained_model()\n            bist_model = SpacyBISTParser._pretrained\n\n        self.verbose = verbose\n        self.bist_parser = BISTModel()\n        self.bist_parser.load(bist_model if bist_model else SpacyBISTParser._pretrained)\n        self.spacy_parser = SpacyInstance(spacy_model, disable=[""ner"", ""vectors"", ""textcat""]).parser\n\n    def to_conll(self, doc_text):\n        """"""Converts a document to CoNLL format with spacy POS tags.\n\n        Args:\n            doc_text (str): raw document text.\n\n        Yields:\n            list of ConllEntry: The next sentence in the document in CoNLL format.\n        """"""\n        validate((doc_text, str))\n        for sentence in self.spacy_parser(doc_text).sents:\n            sentence_conll = [\n                ConllEntry(\n                    0, ""*root*"", ""*root*"", ""ROOT-POS"", ""ROOT-CPOS"", ""_"", -1, ""rroot"", ""_"", ""_""\n                )\n            ]\n            i_tok = 0\n            for tok in sentence:\n                if self.verbose:\n                    print(tok.text + ""\\t"" + tok.tag_)\n\n                if not tok.is_space:\n                    pos = tok.tag_\n                    text = tok.text\n\n                    if text != ""-"" or pos != ""HYPH"":\n                        pos = _spacy_pos_to_ptb(pos, text)\n                        token_conll = ConllEntry(\n                            i_tok + 1,\n                            text,\n                            tok.lemma_,\n                            pos,\n                            pos,\n                            tok.ent_type_,\n                            -1,\n                            ""_"",\n                            ""_"",\n                            tok.idx,\n                        )\n                        sentence_conll.append(token_conll)\n                        i_tok += 1\n\n            if self.verbose:\n                print(""-----------------------\\ninput conll form:"")\n                for entry in sentence_conll:\n                    print(str(entry.id) + ""\\t"" + entry.form + ""\\t"" + entry.pos + ""\\t"")\n            yield sentence_conll\n\n    def parse(self, doc_text, show_tok=True, show_doc=True):\n        """"""Parse a raw text document.\n\n        Args:\n            doc_text (str)\n            show_tok (bool, optional): Specifies whether to include token text in output.\n            show_doc (bool, optional): Specifies whether to include document text in output.\n\n        Returns:\n            CoreNLPDoc: The annotated document.\n        """"""\n        validate((doc_text, str), (show_tok, bool), (show_doc, bool))\n        doc_conll = self.to_conll(doc_text)\n        parsed_doc = CoreNLPDoc()\n\n        if show_doc:\n            parsed_doc.doc_text = doc_text\n\n        for sent_conll in self.bist_parser.predict_conll(doc_conll):\n            parsed_sent = []\n            conj_governors = {""and"": set(), ""or"": set()}\n\n            for tok in sent_conll:\n                gov_id = int(tok.pred_parent_id)\n                rel = tok.pred_relation\n\n                if tok.form != ""*root*"":\n                    if tok.form.lower() == ""and"":\n                        conj_governors[""and""].add(gov_id)\n                    if tok.form.lower() == ""or"":\n                        conj_governors[""or""].add(gov_id)\n\n                    if rel == ""conj"":\n                        if gov_id in conj_governors[""and""]:\n                            rel += ""_and""\n                        if gov_id in conj_governors[""or""]:\n                            rel += ""_or""\n\n                    parsed_tok = {\n                        ""start"": tok.misc,\n                        ""len"": len(tok.form),\n                        ""pos"": tok.pos,\n                        ""ner"": tok.feats,\n                        ""lemma"": tok.lemma,\n                        ""gov"": gov_id - 1,\n                        ""rel"": rel,\n                    }\n\n                    if show_tok:\n                        parsed_tok[""text""] = tok.form\n                    parsed_sent.append(parsed_tok)\n            if parsed_sent:\n                parsed_doc.sentences.append(parsed_sent)\n        return parsed_doc\n\n\ndef _download_pretrained_model():\n    """"""Downloads the pre-trained BIST model if non-existent.""""""\n    if not path.isfile(SpacyBISTParser.dir / ""bist.model""):\n        print(""Downloading pre-trained BIST model..."")\n        zip_path = SpacyBISTParser.dir / ""bist-pretrained.zip""\n        makedirs(SpacyBISTParser.dir, exist_ok=True)\n        download_unlicensed_file(\n            ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/models/dep_parse/"",\n            ""bist-pretrained.zip"",\n            zip_path,\n        )\n        print(""Unzipping..."")\n        uncompress_file(zip_path, outpath=str(SpacyBISTParser.dir))\n        remove(zip_path)\n        print(""Done."")\n\n\ndef _spacy_pos_to_ptb(pos, text):\n    """"""\n    Converts a Spacy part-of-speech tag to a Penn Treebank part-of-speech tag.\n\n    Args:\n        pos (str): Spacy POS tag.\n        text (str): The token text.\n\n    Returns:\n        ptb_tag (str): Standard PTB POS tag.\n    """"""\n    validate((pos, str, 0, 30), (text, str, 0, 1000))\n    ptb_tag = pos\n    if text in [""..."", ""\xe2\x80\x94""]:\n        ptb_tag = "":""\n    elif text == ""*"":\n        ptb_tag = ""SYM""\n    elif pos == ""AFX"":\n        ptb_tag = ""JJ""\n    elif pos == ""ADD"":\n        ptb_tag = ""NN""\n    elif text != pos and text in ["","", ""."", "":"", ""``"", ""-RRB-"", ""-LRB-""]:\n        ptb_tag = text\n    elif pos in [""NFP"", ""HYPH"", ""XX""]:\n        ptb_tag = ""SYM""\n    return ptb_tag\n'"
nlp_architect/pipelines/spacy_np_annotator.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport pickle\nfrom os import path\n\nimport numpy as np\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy.tokens import Span\n\nfrom nlp_architect.models.chunker import SequenceChunker\nfrom nlp_architect.utils.generic import pad_sentences\nfrom nlp_architect.utils.io import validate_existing_filepath\nfrom nlp_architect.utils.text import extract_nps, Stopwords\n\n\nclass NPAnnotator(object):\n    """"""\n    Spacy based NP annotator - uses models.SequenceChunker model for annotation\n\n    Args:\n        model (SequenceChunker): a chunker model\n        word_vocab (Vocabulary): word-id vocabulary of the model\n        char_vocab (Vocabulary): char id vocabulary of words of the model\n        chunk_vocab (Vocabulary): chunk tag vocabulary of the model\n        batch_size (int, optional): inference batch size\n    """"""\n\n    def __init__(self, model, word_vocab, char_vocab, chunk_vocab, batch_size: int = 32):\n        self.model = model\n        self.bs = batch_size\n        self.word_vocab = word_vocab\n        self.char_vocab = char_vocab\n        self.chunk_vocab = chunk_vocab\n        Doc.set_extension(""noun_phrases"", default=[], force=True)\n\n    @classmethod\n    def load(\n        cls, model_path: str, parameter_path: str, batch_size: int = 32, use_cudnn: bool = False\n    ):\n        """"""\n        Load a NPAnnotator annotator\n\n        Args:\n            model_path (str): path to trained model\n            parameter_path (str): path to model parameters\n            batch_size (int, optional): inference batch_size\n            use_cudnn (bool, optional): use gpu for inference (cudnn cells)\n\n        Returns:\n            NPAnnotator class with loaded model\n        """"""\n\n        _model_path = path.join(path.dirname(path.realpath(__file__)), model_path)\n        validate_existing_filepath(_model_path)\n        _parameter_path = path.join(path.dirname(path.realpath(__file__)), parameter_path)\n        validate_existing_filepath(_parameter_path)\n\n        model = SequenceChunker(use_cudnn=use_cudnn)\n        model.load(_model_path)\n        with open(_parameter_path, ""rb"") as fp:\n            model_params = pickle.load(fp)\n            word_vocab = model_params[""word_vocab""]\n            chunk_vocab = model_params[""chunk_vocab""]\n            char_vocab = model_params.get(""char_vocab"", None)\n        return cls(model, word_vocab, char_vocab, chunk_vocab, batch_size)\n\n    def _infer_chunks(self, input_vec, doc_lengths):\n        tagged_sents = self.model.predict(input_vec, batch_size=self.bs).argmax(2)\n        sentence = []\n        for c, l in zip(tagged_sents, doc_lengths):\n            sentence.append(c[-l:])\n        doc = np.concatenate(sentence)\n        chunk_tags = [self.chunk_vocab.id_to_word(w) for w in doc]\n        return extract_nps(chunk_tags)\n\n    def _feature_extractor(self, doc):\n        features = np.asarray(\n            [self.word_vocab[w] if self.word_vocab[w] is not None else 1 for w in doc]\n        )\n        if self.char_vocab:\n            sentence_chars = []\n            for w in doc:\n                word_chars = []\n                for c in w:\n                    _cid = self.char_vocab[c]\n                    word_chars.append(_cid if _cid is not None else 1)\n                sentence_chars.append(word_chars)\n            sentence_chars = pad_sentences(sentence_chars, self.model.max_word_len)\n            features = (features, sentence_chars)\n        return features\n\n    def __call__(self, doc: Doc) -> Doc:\n        """"""\n        Annotate the document with noun phrase spans\n        """"""\n        spans = []\n        doc_vecs = []\n        doc_chars = []\n        doc_lens = []\n        if len(doc) < 1:\n            return doc\n        for sentence in doc.sents:\n            features = self._feature_extractor([t.text for t in sentence])\n            if isinstance(features, tuple):\n                doc_vec = features[0]\n                doc_chars.append(features[1])\n            else:\n                doc_vec = features\n            doc_vecs.append(doc_vec)\n            doc_lens.append(len(doc_vec))\n        doc_vectors = pad_sentences(np.asarray(doc_vecs))\n        inputs = doc_vectors\n        if self.char_vocab:\n            max_len = doc_vectors.shape[1]\n            padded_chars = np.zeros((len(doc_chars), max_len, self.model.max_word_len))\n            for idx, d in enumerate(doc_chars):\n                d = d[:max_len]\n                padded_chars[idx, -d.shape[0] :] = d\n            inputs = [inputs, padded_chars]\n        np_indexes = self._infer_chunks(inputs, doc_lens)\n        for s, e in np_indexes:\n            np_span = Span(doc, s, e)\n            spans.append(np_span)\n        spans = _NPPostprocessor.process(spans)\n        set_noun_phrases(doc, spans)\n        return doc\n\n\ndef get_noun_phrases(doc: Doc) -> [Span]:\n    """"""\n    Get noun phrase tags from a spacy annotated document.\n\n    Args:\n        doc (Doc): a spacy type document\n\n    Returns:\n        a list of noun phrase Span objects\n    """"""\n    assert hasattr(doc._, ""noun_phrases""), ""no noun_phrase attributes in document""\n    return doc._.noun_phrases\n\n\ndef set_noun_phrases(doc: Doc, nps: [Span]) -> None:\n    """"""\n    Set noun phrase tags\n\n    Args:\n        doc (Doc): a spacy type document\n        nps ([Span]): a list of Spans\n    """"""\n    assert hasattr(doc._, ""noun_phrases""), ""no noun_phrase attributes in document""\n    doc._.set(""noun_phrases"", nps)\n\n\nclass _NPPostprocessor:\n    @classmethod\n    def process(cls, noun_phrases: [Span]) -> [Span]:\n        new_phrases = []\n        for phrase in noun_phrases:\n            p = _NPPostprocessor._phrase_process(phrase)\n            if p is not None and len(p) > 0:\n                new_phrases.append(p)\n        return new_phrases\n\n    @classmethod\n    def _phrase_process(cls, phrase: Span) -> Span:\n        last_phrase = None\n        while phrase != last_phrase:\n            last_phrase = phrase\n            for func_args in post_processing_rules:\n                pf = func_args[0]\n                args = func_args[1:]\n                if len(args) > 0:\n                    phrase = pf(phrase, *args)\n                else:\n                    phrase = pf(phrase)\n                if phrase is None:\n                    break\n        return phrase\n\n\ndef _filter_repeating_nonalnum(phrase, length):\n    """"""\n    Check if a given phrase has non repeating alphanumeric chars\n    of given length.\n    Example: \'phrase $$$\' with length=3 will return False\n    """"""\n    if len(phrase) > 0:\n        alnum_len = length\n        for t in phrase:\n            if not t.is_alpha:\n                alnum_len -= 1\n            else:\n                alnum_len = length\n            if alnum_len == 0:\n                return None\n    return phrase\n\n\ndef _filter_long_phrases(phrase, word_length, phrase_length):\n    if (\n        len(phrase) > 0\n        and max([len(t) for t in phrase]) > word_length\n        and len(phrase) > phrase_length\n    ):\n        return None\n    return phrase\n\n\ndef _remove_non_alphanum_from_start(phrase):\n    if len(phrase) > 1 and not phrase[0].is_alpha:\n        phrase = phrase[1:]\n    return phrase\n\n\ndef _remove_non_alphanum_from_end(phrase):\n    if len(phrase) > 1 and not phrase[-1].is_alpha:\n        phrase = phrase[:-1]\n    return phrase\n\n\ndef _remove_stop_words(phrase):\n    while len(phrase) > 0 and (\n        phrase[0].is_stop or str(phrase[0]).strip().lower() in Stopwords.get_words()\n    ):\n        phrase = phrase[1:]\n    while len(phrase) > 0 and (\n        phrase[-1].is_stop or str(phrase[-1]).strip().lower() in Stopwords.get_words()\n    ):\n        phrase = phrase[:-1]\n    return phrase\n\n\ndef _remove_char_at_start(phrase):\n    chars = [""@"", ""-"", ""="", ""."", "":"", ""+"", ""?"", ""nt"", \'""\', ""\'"", ""\'S"", ""\'s"", "",""]\n    if phrase and len(phrase) > 0:\n        while len(phrase) > 0 and phrase[0].text in chars:\n            phrase = phrase[1:]\n    return phrase\n\n\ndef _remove_char_at_end(phrase):\n    chars = ["","", ""("", "")"", "" "", ""-""]\n    if phrase:\n        while len(phrase) > 0 and phrase[-1].text in chars:\n            phrase = phrase[:-1]\n    return phrase\n\n\ndef _remove_pos_from_start(phrase):\n    tag_list = [""WDT"", ""PRP$"", "":""]\n    pos_list = [""PUNCT"", ""INTJ"", ""NUM"", ""PART"", ""ADV"", ""DET"", ""PRON"", ""VERB""]\n    if phrase:\n        while len(phrase) > 0 and (phrase[0].pos_ in pos_list or phrase[0].tag_ in tag_list):\n            phrase = phrase[1:]\n    return phrase\n\n\ndef _remove_pos_from_end(phrase):\n    tag_list = [""WDT"", "":""]\n    pos_list = [""DET"", ""PUNCT"", ""CONJ""]\n    if phrase:\n        while len(phrase) > 0 and (phrase[-1].pos_ in pos_list or phrase[-1].tag_ in tag_list):\n            phrase = phrase[:-1]\n    return phrase\n\n\ndef _filter_single_pos(phrase):\n    pos_list = [""VERB"", ""ADJ"", ""ADV""]\n    if phrase and len(phrase) == 1 and phrase[0].pos_ in pos_list:\n        return None\n    return phrase\n\n\ndef _filter_fp_nums(phrase):\n    if len(phrase) > 0:\n        try:\n            # check for float number\n            float(phrase.text.replace("","", """"))\n            return None\n        except ValueError:\n            return phrase\n    return phrase\n\n\ndef _filter_single_char(phrase):\n    if phrase and len(phrase) == 1 and len(phrase[0]) == 1:\n        return None\n    return phrase\n\n\ndef _filter_empty(phrase):\n    if (\n        phrase is None\n        or len(phrase) == 0\n        or len(phrase.text) == 0\n        or len(str(phrase.text).strip()) == 0\n    ):\n        return None\n    return phrase\n\n\npost_processing_rules = [\n    (_filter_single_char,),\n    (_filter_single_pos,),\n    (_remove_pos_from_start,),\n    (_remove_pos_from_end,),\n    (_remove_stop_words,),\n    (_remove_non_alphanum_from_start,),\n    (_remove_non_alphanum_from_end,),\n    (_filter_repeating_nonalnum, 5),\n    (_filter_long_phrases, 5, 75),\n    (_remove_char_at_start,),\n    (_remove_char_at_end,),\n    (_filter_fp_nums,),\n    (_filter_empty,),\n]\n\n\nclass SpacyNPAnnotator(object):\n    """"""\n    Simple Spacy pipe with NP extraction annotations\n    """"""\n\n    def __init__(self, model_path, settings_path, spacy_model=""en"", batch_size=32, use_cudnn=False):\n        _model_path = path.join(path.dirname(path.realpath(__file__)), model_path)\n        validate_existing_filepath(_model_path)\n        _settings_path = path.join(path.dirname(path.realpath(__file__)), settings_path)\n        validate_existing_filepath(_settings_path)\n\n        nlp = spacy.load(spacy_model)\n        for p in nlp.pipe_names:\n            if p not in [""tagger""]:\n                nlp.remove_pipe(p)\n        nlp.add_pipe(nlp.create_pipe(""sentencizer""), first=True)\n        nlp.add_pipe(\n            NPAnnotator.load(\n                _model_path, settings_path, batch_size=batch_size, use_cudnn=use_cudnn\n            ),\n            last=True,\n        )\n        self.nlp = nlp\n\n    def __call__(self, text: str) -> [str]:\n        """"""\n        Parse a given text and return a list of noun phrases found\n\n        Args:\n            text (str): a text string\n\n        Returns:\n            list of noun phrases as strings\n        """"""\n        return [np.text for np in get_noun_phrases(self.nlp(text))]\n'"
nlp_architect/procedures/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# flake8: noqa\nfrom nlp_architect.procedures.transformers.glue import TransformerGlueRun, TransformerGlueTrain\nfrom nlp_architect.procedures.transformers.seq_tag import (\n    TransformerTokenClsRun,\n    TransformerTokenClsTrain,\n)\nfrom nlp_architect.procedures.token_tagging import TrainTagger, TrainTaggerKD, RunTagger\n'"
nlp_architect/procedures/procedure.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport abc\n\n\nclass Procedure:\n    def __init__(self, parser=None):\n        if parser is None:\n            parser = argparse.ArgumentParser()\n        self.parser = parser\n\n    @staticmethod\n    @abc.abstractmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        raise NotImplementedError\n\n    @staticmethod\n    @abc.abstractmethod\n    def run_procedure(args):\n        raise NotImplementedError\n\n    @classmethod\n    def run(cls):\n        parser = argparse.ArgumentParser()\n        cls.add_arguments(parser)\n        cls.run_procedure(parser.parse_args())\n'"
nlp_architect/procedures/registry.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Register procedures to be used by CLI\nfrom nlp_architect.cli.cmd_registry import CMD_REGISTRY\nfrom nlp_architect.procedures.procedure import Procedure\n\n\ndef register_cmd(registry: dict, name: str, description: str):\n    def register_cmd_fn(cls):\n        if not issubclass(cls, Procedure):\n            raise ValueError(""Registered class must be subclassed from Procedure"")\n        if name in registry:\n            raise ValueError(""Cannot register duplicate model {}"".format(name))\n        run_fn = cls.run_procedure\n        arg_adder_fn = cls.add_arguments\n        new_cmd = {\n            ""name"": name,\n            ""description"": description,\n            ""fn"": run_fn,\n            ""arg_adder"": arg_adder_fn,\n        }\n        registry.append(new_cmd)\n        return cls\n\n    return register_cmd_fn\n\n\ndef register_train_cmd(name: str, description: str):\n    return register_cmd(CMD_REGISTRY[""train""], name, description)\n\n\ndef register_inference_cmd(name: str, description: str):\n    return register_cmd(CMD_REGISTRY[""inference""], name, description)\n'"
nlp_architect/procedures/token_tagging.py,9,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport io\nimport os\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nfrom nlp_architect.data.sequential_tagging import TokenClsInputExample, TokenClsProcessor\nfrom nlp_architect.data.utils import write_column_tagged_file\nfrom nlp_architect.models.tagging import NeuralTagger\nfrom nlp_architect.nn.torch.modules.embedders import IDCNN, CNNLSTM\nfrom nlp_architect.nn.torch import setup_backend, set_seed\nfrom nlp_architect.nn.torch.distillation import TeacherStudentDistill\nfrom nlp_architect.procedures.procedure import Procedure\nfrom nlp_architect.procedures.registry import register_train_cmd, register_inference_cmd\nfrom nlp_architect.utils.embedding import get_embedding_matrix, load_embedding_file\nfrom nlp_architect.utils.io import prepare_output_path\nfrom nlp_architect.utils.text import SpacyInstance\nfrom nlp_architect.nn.torch.data.dataset import (\n    ParallelDataset,\n    ConcatTensorDataset,\n    CombinedTensorDataset,\n)\nfrom nlp_architect.models.transformers import TransformerTokenClassifier\n\n\n@register_train_cmd(name=""tagger"", description=""Train a neural tagger"")\nclass TrainTagger(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        add_parse_args(parser)\n\n    @staticmethod\n    def run_procedure(args):\n        do_training(args)\n\n\n@register_train_cmd(\n    name=""tagger_kd"",\n    description=""Train a neural tagger using Knowledge Distillation""\n    "" and a Transformer teacher model"",\n)\nclass TrainTaggerKD(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        add_parse_args(parser)\n        TeacherStudentDistill.add_args(parser)\n        parser.add_argument(\n            ""--teacher_max_seq_len"",\n            type=int,\n            default=128,\n            help=""Max sentence \\\n                             length for teacher data loading"",\n        )\n\n    @staticmethod\n    def run_procedure(args):\n        do_kd_training(args)\n\n\n@register_train_cmd(\n    name=""tagger_kd_pseudo"",\n    description=""Train a neural tagger using Knowledge Distillation""\n    "" and a Transformer teacher model + pseudo-labeling"",\n)\nclass TrainTaggerKDPseudo(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        add_parse_args(parser)\n        TeacherStudentDistill.add_args(parser)\n        parser.add_argument(\n            ""--unlabeled_filename"",\n            default=""unlabeled.txt"",\n            type=str,\n            help=""The file name containing the unlabeled training examples"",\n        )\n        parser.add_argument(\n            ""--parallel_batching"",\n            action=""store_true"",\n            help=""sample labeled/unlabeled batch in parallel"",\n        )\n        parser.add_argument(\n            ""--teacher_max_seq_len"",\n            type=int,\n            default=128,\n            help=""Max sentence \\\n                             length for teacher data loading"",\n        )\n\n    @staticmethod\n    def run_procedure(args):\n        do_kd_pseudo_training(args)\n\n\n@register_inference_cmd(name=""tagger"", description=""Run a neural tagger model"")\nclass RunTagger(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        parser.add_argument(\n            ""--data_file"",\n            default=None,\n            type=str,\n            required=True,\n            help=""The data file containing data for inference"",\n        )\n        parser.add_argument(\n            ""--model_dir"", type=str, required=True, help=""Path to trained model directory""\n        )\n        parser.add_argument(\n            ""--output_dir"",\n            type=str,\n            required=True,\n            help=""Output directory where the model will be saved"",\n        )\n        parser.add_argument(\n            ""--overwrite_output_dir"",\n            action=""store_true"",\n            help=""Overwrite the content of the output directory"",\n        )\n        parser.add_argument(\n            ""--no_cuda"", action=""store_true"", help=""Avoid using CUDA when available""\n        )\n        parser.add_argument(""-b"", type=int, default=100, help=""Batch size"")\n\n    @staticmethod\n    def run_procedure(args):\n        do_inference(args)\n\n\ndef add_parse_args(parser: argparse.ArgumentParser):\n    parser.add_argument(\n        ""--model_type"",\n        default=""cnn-lstm"",\n        type=str,\n        choices=list(MODEL_TYPE.keys()),\n        help=""model type to use for this tagger"",\n    )\n    parser.add_argument(""--config_file"", type=str, help=""Embedder model configuration file"")\n    parser.add_argument(""-b"", type=int, default=10, help=""Batch size"")\n    parser.add_argument(""-e"", type=int, default=155, help=""Number of epochs"")\n    parser.add_argument(\n        ""--data_dir"",\n        default=None,\n        type=str,\n        required=True,\n        help=""The input data dir. Should contain dataset files to be parsed by "" ""the dataloaders."",\n    )\n    parser.add_argument(\n        ""--tag_col"", type=int, default=-1, help=""Entity labels tab number in train/test files""\n    )\n    parser.add_argument(""--max_sentence_length"", type=int, default=50, help=""Max sentence length"")\n    parser.add_argument(\n        ""--max_word_length"", type=int, default=12, help=""Max word length in characters""\n    )\n    parser.add_argument(\n        ""--use_crf"", action=""store_true"", help=""Use CRF classifier instead of Softmax""\n    )\n    parser.add_argument(\n        ""--lr"", type=float, default=0.001, help=""Learning rate for optimizer (Adam)""\n    )\n    parser.add_argument(""--embedding_file"", help=""Path to external word embedding model file"")\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        required=True,\n        help=""Output directory where the model will be saved"",\n    )\n    parser.add_argument(""--seed"", type=int, default=42, help=""random seed for initialization"")\n    parser.add_argument(""--logging_steps"", type=int, default=50, help=""Log every X updates steps."")\n    parser.add_argument(\n        ""--save_steps"", type=int, default=500, help=""Save model every X updates steps.""\n    )\n    parser.add_argument(\n        ""--overwrite_output_dir"",\n        action=""store_true"",\n        help=""Overwrite the content of the output directory"",\n    )\n    parser.add_argument(""--no_cuda"", action=""store_true"", help=""Avoid using CUDA when available"")\n    parser.add_argument(\n        ""--best_result_file"",\n        type=str,\n        default=""best_dev.txt"",\n        help=""file path for best evaluation output"",\n    )\n    parser.add_argument(\n        ""--word_dropout"", type=float, default=0, help=""word dropout rate for input tokens""\n    )\n    parser.add_argument(\n        ""--ignore_token"", type=str, default="""", help=""a token to ignore when processing the data""\n    )\n    parser.add_argument(\n        ""--train_filename"", type=str, default=""train.txt"", help=""filename of training dataset""\n    )\n    parser.add_argument(\n        ""--dev_filename"", type=str, default=""dev.txt"", help=""filename of development dataset""\n    )\n    parser.add_argument(\n        ""--test_filename"", type=str, default=""test.txt"", help=""filename of test dataset""\n    )\n\n\nMODEL_TYPE = {""cnn-lstm"": CNNLSTM, ""id-cnn"": IDCNN}\n\n\ndef do_training(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    # Set seed\n    args.seed = set_seed(args.seed, n_gpus)\n    # prepare data\n    processor = TokenClsProcessor(\n        args.data_dir, tag_col=args.tag_col, ignore_token=args.ignore_token\n    )\n    train_ex = processor.get_train_examples(filename=args.train_filename)\n    dev_ex = processor.get_dev_examples(filename=args.dev_filename)\n    test_ex = processor.get_test_examples(filename=args.test_filename)\n    vocab = processor.get_vocabulary(train_ex + dev_ex + test_ex)\n    vocab_size = len(vocab) + 1\n    num_labels = len(processor.get_labels()) + 1\n    # create an embedder\n    embedder_cls = MODEL_TYPE[args.model_type]\n    if args.config_file is not None:\n        embedder_model = embedder_cls.from_config(vocab_size, num_labels, args.config_file)\n    else:\n        embedder_model = embedder_cls(vocab_size, num_labels)\n\n    # load external word embeddings if present\n    if args.embedding_file is not None:\n        emb_dict = load_embedding_file(args.embedding_file, dim=embedder_model.word_embedding_dim)\n        emb_mat = get_embedding_matrix(emb_dict, vocab)\n        emb_mat = torch.tensor(emb_mat, dtype=torch.float)\n        embedder_model.load_embeddings(emb_mat)\n\n    classifier = NeuralTagger(\n        embedder_model,\n        word_vocab=vocab,\n        labels=processor.get_labels(),\n        use_crf=args.use_crf,\n        device=device,\n        n_gpus=n_gpus,\n    )\n\n    train_batch_size = args.b * max(1, n_gpus)\n\n    train_dataset = classifier.convert_to_tensors(\n        train_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n    )\n    train_sampler = RandomSampler(train_dataset)\n    train_dl = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n    if dev_ex is not None:\n        dev_dataset = classifier.convert_to_tensors(\n            dev_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n        )\n        dev_sampler = SequentialSampler(dev_dataset)\n        dev_dl = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=args.b)\n\n    if test_ex is not None:\n        test_dataset = classifier.convert_to_tensors(\n            test_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n        )\n        test_sampler = SequentialSampler(test_dataset)\n        test_dl = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.b)\n    if args.lr is not None:\n        opt = classifier.get_optimizer(lr=args.lr)\n    classifier.train(\n        train_dl,\n        dev_dl,\n        test_dl,\n        epochs=args.e,\n        batch_size=args.b,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        save_path=args.output_dir,\n        optimizer=opt if opt is not None else None,\n        best_result_file=args.best_result_file,\n        word_dropout=args.word_dropout,\n    )\n    classifier.save_model(args.output_dir)\n\n\ndef do_kd_training(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    # Set seed\n    args.seed = set_seed(args.seed, n_gpus)\n    # prepare data\n    processor = TokenClsProcessor(\n        args.data_dir, tag_col=args.tag_col, ignore_token=args.ignore_token\n    )\n    train_ex = processor.get_train_examples(filename=args.train_filename)\n    dev_ex = processor.get_dev_examples(filename=args.dev_filename)\n    test_ex = processor.get_test_examples(filename=args.test_filename)\n    vocab = processor.get_vocabulary(train_ex + dev_ex + test_ex)\n    vocab_size = len(vocab) + 1\n    num_labels = len(processor.get_labels()) + 1\n    # create an embedder\n    embedder_cls = MODEL_TYPE[args.model_type]\n    if args.config_file is not None:\n        embedder_model = embedder_cls.from_config(vocab_size, num_labels, args.config_file)\n    else:\n        embedder_model = embedder_cls(vocab_size, num_labels)\n\n    # load external word embeddings if present\n    if args.embedding_file is not None:\n        emb_dict = load_embedding_file(args.embedding_file, dim=embedder_model.word_embedding_dim)\n        emb_mat = get_embedding_matrix(emb_dict, vocab)\n        emb_mat = torch.tensor(emb_mat, dtype=torch.float)\n        embedder_model.load_embeddings(emb_mat)\n\n    classifier = NeuralTagger(\n        embedder_model,\n        word_vocab=vocab,\n        labels=processor.get_labels(),\n        use_crf=args.use_crf,\n        device=device,\n        n_gpus=n_gpus,\n    )\n\n    train_batch_size = args.b * max(1, n_gpus)\n    train_dataset = classifier.convert_to_tensors(\n        train_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n    )\n\n    # load saved teacher args if exist\n    if os.path.exists(args.teacher_model_path + os.sep + ""training_args.bin""):\n        t_args = torch.load(args.teacher_model_path + os.sep + ""training_args.bin"")\n        t_device, t_n_gpus = setup_backend(t_args.no_cuda)\n        teacher = TransformerTokenClassifier.load_model(\n            model_path=args.teacher_model_path,\n            model_type=args.teacher_model_type,\n            config_name=t_args.config_name,\n            tokenizer_name=t_args.tokenizer_name,\n            do_lower_case=t_args.do_lower_case,\n            output_path=t_args.output_dir,\n            device=t_device,\n            n_gpus=t_n_gpus,\n        )\n    else:\n        teacher = TransformerTokenClassifier.load_model(\n            model_path=args.teacher_model_path, model_type=args.teacher_model_type\n        )\n        teacher.to(device, n_gpus)\n    teacher_dataset = teacher.convert_to_tensors(\n        train_ex, max_seq_length=args.teacher_max_seq_len, include_labels=False\n    )\n\n    train_dataset = ParallelDataset(train_dataset, teacher_dataset)\n\n    train_sampler = RandomSampler(train_dataset)\n    train_dl = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n\n    if dev_ex is not None:\n        dev_dataset = classifier.convert_to_tensors(\n            dev_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n        )\n        dev_sampler = SequentialSampler(dev_dataset)\n        dev_dl = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=args.b)\n\n    if test_ex is not None:\n        test_dataset = classifier.convert_to_tensors(\n            test_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n        )\n        test_sampler = SequentialSampler(test_dataset)\n        test_dl = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.b)\n    if args.lr is not None:\n        opt = classifier.get_optimizer(lr=args.lr)\n\n    distiller = TeacherStudentDistill(\n        teacher, args.kd_temp, args.kd_dist_w, args.kd_student_w, args.kd_loss_fn\n    )\n    classifier.train(\n        train_dl,\n        dev_dl,\n        test_dl,\n        epochs=args.e,\n        batch_size=args.b,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        save_path=args.output_dir,\n        optimizer=opt if opt is not None else None,\n        distiller=distiller,\n        best_result_file=args.best_result_file,\n        word_dropout=args.word_dropout,\n    )\n    classifier.save_model(args.output_dir)\n\n\ndef do_kd_pseudo_training(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    # Set seed\n    args.seed = set_seed(args.seed, n_gpus)\n    # prepare data\n    processor = TokenClsProcessor(\n        args.data_dir, tag_col=args.tag_col, ignore_token=args.ignore_token\n    )\n    train_labeled_ex = processor.get_train_examples(filename=args.train_filename)\n    train_unlabeled_ex = processor.get_train_examples(filename=args.unlabeled_filename)\n    dev_ex = processor.get_dev_examples(filename=args.dev_filename)\n    test_ex = processor.get_test_examples(filename=args.test_filename)\n    vocab = processor.get_vocabulary(train_labeled_ex + train_unlabeled_ex + dev_ex + test_ex)\n    vocab_size = len(vocab) + 1\n    num_labels = len(processor.get_labels()) + 1\n    # create an embedder\n    embedder_cls = MODEL_TYPE[args.model_type]\n    if args.config_file is not None:\n        embedder_model = embedder_cls.from_config(vocab_size, num_labels, args.config_file)\n    else:\n        embedder_model = embedder_cls(vocab_size, num_labels)\n\n    # load external word embeddings if present\n    if args.embedding_file is not None:\n        emb_dict = load_embedding_file(args.embedding_file, dim=embedder_model.word_embedding_dim)\n        emb_mat = get_embedding_matrix(emb_dict, vocab)\n        emb_mat = torch.tensor(emb_mat, dtype=torch.float)\n        embedder_model.load_embeddings(emb_mat)\n\n    classifier = NeuralTagger(\n        embedder_model,\n        word_vocab=vocab,\n        labels=processor.get_labels(),\n        use_crf=args.use_crf,\n        device=device,\n        n_gpus=n_gpus,\n    )\n\n    train_batch_size = args.b * max(1, n_gpus)\n    train_labeled_dataset = classifier.convert_to_tensors(\n        train_labeled_ex,\n        max_seq_length=args.max_sentence_length,\n        max_word_length=args.max_word_length,\n    )\n    train_unlabeled_dataset = classifier.convert_to_tensors(\n        train_unlabeled_ex,\n        max_seq_length=args.max_sentence_length,\n        max_word_length=args.max_word_length,\n        include_labels=False,\n    )\n\n    if args.parallel_batching:\n        # # concat labeled+unlabeled dataset\n        # train_dataset = ConcatTensorDataset(train_labeled_dataset, [train_unlabeled_dataset])\n        # match sizes of labeled/unlabeled train data for parallel batching\n        larger_ds, smaller_ds = (\n            (train_labeled_dataset, train_unlabeled_dataset)\n            if len(train_labeled_dataset) > len(train_unlabeled_dataset)\n            else (train_unlabeled_dataset, train_labeled_dataset)\n        )\n        concat_smaller_ds = smaller_ds\n        while len(concat_smaller_ds) < len(larger_ds):\n            concat_smaller_ds = ConcatTensorDataset(concat_smaller_ds, [smaller_ds])\n        if len(concat_smaller_ds[0]) == 4:\n            train_unlabeled_dataset = concat_smaller_ds\n        else:\n            train_labeled_dataset = concat_smaller_ds\n    else:\n        train_dataset = CombinedTensorDataset([train_labeled_dataset, train_unlabeled_dataset])\n\n    # load saved teacher args if exist\n    if os.path.exists(args.teacher_model_path + os.sep + ""training_args.bin""):\n        t_args = torch.load(args.teacher_model_path + os.sep + ""training_args.bin"")\n        t_device, t_n_gpus = setup_backend(t_args.no_cuda)\n        teacher = TransformerTokenClassifier.load_model(\n            model_path=args.teacher_model_path,\n            model_type=args.teacher_model_type,\n            config_name=t_args.config_name,\n            tokenizer_name=t_args.tokenizer_name,\n            do_lower_case=t_args.do_lower_case,\n            output_path=t_args.output_dir,\n            device=t_device,\n            n_gpus=t_n_gpus,\n        )\n    else:\n        teacher = TransformerTokenClassifier.load_model(\n            model_path=args.teacher_model_path, model_type=args.teacher_model_type\n        )\n        teacher.to(device, n_gpus)\n\n    teacher_labeled_dataset = teacher.convert_to_tensors(train_labeled_ex, args.teacher_max_seq_len)\n    teacher_unlabeled_dataset = teacher.convert_to_tensors(\n        train_unlabeled_ex, args.teacher_max_seq_len, False\n    )\n\n    if args.parallel_batching:\n        # # concat teacher labeled+unlabeled dataset\n        # teacher_dataset = ConcatTensorDataset(teacher_labeled_dataset, [teacher_unlabeled_dataset])\n        # match sizes of labeled/unlabeled teacher train data for parallel batching\n        larger_ds, smaller_ds = (\n            (teacher_labeled_dataset, teacher_unlabeled_dataset)\n            if len(teacher_labeled_dataset) > len(teacher_unlabeled_dataset)\n            else (teacher_unlabeled_dataset, teacher_labeled_dataset)\n        )\n        concat_smaller_ds = smaller_ds\n        while len(concat_smaller_ds) < len(larger_ds):\n            concat_smaller_ds = ConcatTensorDataset(concat_smaller_ds, [smaller_ds])\n        if len(concat_smaller_ds[0]) == 4:\n            teacher_unlabeled_dataset = concat_smaller_ds\n        else:\n            teacher_labeled_dataset = concat_smaller_ds\n\n        train_all_dataset = ParallelDataset(\n            train_labeled_dataset,\n            teacher_labeled_dataset,\n            train_unlabeled_dataset,\n            teacher_unlabeled_dataset,\n        )\n\n        train_all_sampler = RandomSampler(train_all_dataset)\n        # this way must use same batch size for both labeled/unlabeled sets\n        train_dl = DataLoader(\n            train_all_dataset, sampler=train_all_sampler, batch_size=train_batch_size\n        )\n\n    else:\n        teacher_dataset = CombinedTensorDataset(\n            [teacher_labeled_dataset, teacher_unlabeled_dataset]\n        )\n\n        train_dataset = ParallelDataset(train_dataset, teacher_dataset)\n        train_sampler = RandomSampler(train_dataset)\n        train_dl = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n\n    if dev_ex is not None:\n        dev_dataset = classifier.convert_to_tensors(\n            dev_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n        )\n        dev_sampler = SequentialSampler(dev_dataset)\n        dev_dl = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=args.b)\n\n    if test_ex is not None:\n        test_dataset = classifier.convert_to_tensors(\n            test_ex, max_seq_length=args.max_sentence_length, max_word_length=args.max_word_length\n        )\n        test_sampler = SequentialSampler(test_dataset)\n        test_dl = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.b)\n    if args.lr is not None:\n        opt = classifier.get_optimizer(lr=args.lr)\n\n    distiller = TeacherStudentDistill(\n        teacher, args.kd_temp, args.kd_dist_w, args.kd_student_w, args.kd_loss_fn\n    )\n\n    classifier.train(\n        train_dl,\n        dev_dl,\n        test_dl,\n        epochs=args.e,\n        batch_size=args.b,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        save_path=args.output_dir,\n        optimizer=opt if opt is not None else None,\n        best_result_file=args.best_result_file,\n        distiller=distiller,\n        word_dropout=args.word_dropout,\n    )\n\n    classifier.save_model(args.output_dir)\n\n\ndef do_inference(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    args.batch_size = args.b * max(1, n_gpus)\n    inference_examples = process_inference_input(args.data_file)\n    classifier = NeuralTagger.load_model(model_path=args.model_dir)\n    classifier.to(device, n_gpus)\n    output = classifier.inference(inference_examples, args.b)\n    write_column_tagged_file(args.output_dir + os.sep + ""output.txt"", output)\n\n\ndef process_inference_input(input_file):\n    with io.open(input_file) as fp:\n        texts = [line.strip() for line in fp.readlines()]\n    tokenizer = SpacyInstance(disable=[""tagger"", ""parser"", ""ner""])\n    examples = []\n    for i, t in enumerate(texts):\n        examples.append(TokenClsInputExample(str(i), t, tokenizer.tokenize(t)))\n    return examples\n'"
nlp_architect/utils/__init__.py,0,b''
nlp_architect/utils/ansi2html.py,0,"b'#!/usr/bin/env python\n\n# This file was taken from:\n# https://github.com/Kronuz/ansi2html/blob/master/ansi2html.py\n# Copyright \xc2\xa9 2013 German M. Bravo (Kronuz)\n# License: https://github.com/Kronuz/ansi2html/blob/master/LICENSE\n# Changes made to original file: removed main, added run() function.\nfrom __future__ import print_function\n\nimport re\nimport sys\n\n_ANSI2HTML_STYLES = {}\n# pylint: disable=anomalous-backslash-in-string\nANSI2HTML_CODES_RE = re.compile(""(?:\\033\\\\[(\\\\d+(?:;\\\\d+)*)?([cnRhlABCDfsurgKJipm]))"")\nANSI2HTML_PALETTE = {\n    # See http://ethanschoonover.com/solarized\n    ""solarized"": [\n        ""#073642"",\n        ""#D30102"",\n        ""#859900"",\n        ""#B58900"",\n        ""#268BD2"",\n        ""#D33682"",\n        ""#2AA198"",\n        ""#EEE8D5"",\n        ""#002B36"",\n        ""#CB4B16"",\n        ""#586E75"",\n        ""#657B83"",\n        ""#839496"",\n        ""#6C71C4"",\n        ""#93A1A1"",\n        ""#FDF6E3"",\n    ],\n    # Above mapped onto the xterm 256 color palette\n    ""solarized-xterm"": [\n        ""#262626"",\n        ""#AF0000"",\n        ""#5F8700"",\n        ""#AF8700"",\n        ""#0087FF"",\n        ""#AF005F"",\n        ""#00AFAF"",\n        ""#E4E4E4"",\n        ""#1C1C1C"",\n        ""#D75F00"",\n        ""#585858"",\n        ""#626262"",\n        ""#808080"",\n        ""#5F5FAF"",\n        ""#8A8A8A"",\n        ""#FFFFD7"",\n    ],\n    # Gnome default:\n    ""tango"": [\n        ""#000000"",\n        ""#CC0000"",\n        ""#4E9A06"",\n        ""#C4A000"",\n        ""#3465A4"",\n        ""#75507B"",\n        ""#06989A"",\n        ""#D3D7CF"",\n        ""#555753"",\n        ""#EF2929"",\n        ""#8AE234"",\n        ""#FCE94F"",\n        ""#729FCF"",\n        ""#AD7FA8"",\n        ""#34E2E2"",\n        ""#EEEEEC"",\n    ],\n    # xterm:\n    ""xterm"": [\n        ""#000000"",\n        ""#CD0000"",\n        ""#00CD00"",\n        ""#CDCD00"",\n        ""#0000EE"",\n        ""#CD00CD"",\n        ""#00CDCD"",\n        ""#E5E5E5"",\n        ""#7F7F7F"",\n        ""#FF0000"",\n        ""#00FF00"",\n        ""#FFFF00"",\n        ""#5C5CFF"",\n        ""#FF00FF"",\n        ""#00FFFF"",\n        ""#FFFFFF"",\n    ],\n    ""console"": [\n        ""#000000"",\n        ""#AA0000"",\n        ""#00AA00"",\n        ""#AA5500"",\n        ""#0000AA"",\n        ""#AA00AA"",\n        ""#00AAAA"",\n        ""#AAAAAA"",\n        ""#555555"",\n        ""#FF5555"",\n        ""#55FF55"",\n        ""#FFFF55"",\n        ""#5555FF"",\n        ""#FF55FF"",\n        ""#55FFFF"",\n        ""#FFFFFF"",\n    ],\n}\n\n\ndef _ansi2html_get_styles(palette):\n    if palette not in _ANSI2HTML_STYLES:\n        p = ANSI2HTML_PALETTE.get(palette, ANSI2HTML_PALETTE[""console""])\n\n        regular_style = {\n            ""1"": """",  # bold\n            ""2"": ""opacity:0.5"",\n            ""4"": ""text-decoration:underline"",\n            ""5"": ""font-weight:bold"",\n            ""7"": """",\n            ""8"": ""display:none"",\n        }\n        bold_style = regular_style.copy()\n        for i in range(8):\n            regular_style[""3%s"" % i] = ""color:%s"" % p[i]\n            regular_style[""4%s"" % i] = ""background-color:%s"" % p[i]\n\n            bold_style[""3%s"" % i] = ""color:%s"" % p[i + 8]\n            bold_style[""4%s"" % i] = ""background-color:%s"" % p[i + 8]\n\n        # The default xterm 256 colour p:\n        indexed_style = {}\n        for i in range(16):\n            indexed_style[""%s"" % i] = p[i]\n\n        for rr in range(6):\n            for gg in range(6):\n                for bb in range(6):\n                    i = 16 + rr * 36 + gg * 6 + bb\n                    r = (rr * 40 + 55) if rr else 0\n                    g = (gg * 40 + 55) if gg else 0\n                    b = (bb * 40 + 55) if bb else 0\n                    indexed_style[""%s"" % i] = """".join(\n                        ""%02X"" % c if 0 <= c <= 255 else None for c in (r, g, b)\n                    )\n\n        for g in range(24):\n            i = g + 232\n            j = g * 10 + 8\n            indexed_style[""%s"" % i] = """".join(\n                ""%02X"" % c if 0 <= c <= 255 else None for c in (j, j, j)\n            )\n\n        _ANSI2HTML_STYLES[palette] = (regular_style, bold_style, indexed_style)\n    return _ANSI2HTML_STYLES[palette]\n\n\ndef ansi2html(text, palette=""solarized""):\n    # pylint: disable=too-many-branches, too-many-statements\n    def _ansi2html(m):\n        if m.group(2) != ""m"":\n            return """"\n        state = None\n        sub = """"\n        cs = m.group(1)\n        cs = cs.strip() if cs else """"\n        for c in cs.split("";""):\n            c = c.strip().lstrip(""0"") or ""0""\n            if c == ""0"":\n                while stack:\n                    sub += ""</span>""\n                    stack.pop()\n            elif c in (""38"", ""48""):\n                extra = [c]\n                state = ""extra""\n            elif state == ""extra"":\n                if c == ""5"":\n                    state = ""idx""\n                elif c == ""2"":\n                    state = ""r""\n            elif state:\n                if state == ""idx"":\n                    extra.append(c)\n                    state = None\n                    color = indexed_style.get(c)  # 256 colors\n                    if color is not None:\n                        sub += \'<span style=""%s:%s"">\' % (\n                            ""color"" if extra[0] == ""38"" else ""background-color"",\n                            color,\n                        )\n                        stack.append(extra)\n                elif state in (""r"", ""g"", ""b""):\n                    extra.append(c)\n                    if state == ""r"":\n                        state = ""g""\n                    elif state == ""g"":\n                        state = ""b""\n                    else:\n                        state = None\n                        try:\n                            color = ""#"" + """".join(\n                                ""%02X"" % c if 0 <= c <= 255 else None\n                                for x in extra\n                                for c in [int(x)]\n                            )\n                        except (ValueError, TypeError):\n                            pass\n                        else:\n                            sub += \'<span style=""%s:%s"">\' % (\n                                ""color"" if extra[0] == ""38"" else ""background-color"",\n                                color,\n                            )\n                            stack.append(extra)\n            else:\n                if ""1"" in stack:\n                    style = bold_style.get(c)\n                else:\n                    style = regular_style.get(c)\n                if style is not None:\n                    sub += \'<span style=""%s"">\' % style\n                    stack.append(c)  # Still needs to be added to the stack even if style is empty\n                    #  (so it can check \'1\' in stack above, for example)\n        return sub\n\n    stack = []\n    regular_style, bold_style, indexed_style = _ansi2html_get_styles(palette)\n    sub = ANSI2HTML_CODES_RE.sub(_ansi2html, text)\n    while stack:\n        sub += ""</span>""\n        stack.pop()\n    return sub\n\n\ndef run(file, out):\n    try:\n        html = """"\n        try:\n            with open(file) as fp:\n                html += ""<pre>"" + ansi2html(fp.read()) + ""</pre>""\n        except IOError:\n            print(""File not found: %r"" % ""pylint.txt"", file=sys.stderr)\n\n        with open(out, ""w"") as out_fp:\n            out_fp.write(html)\n    except KeyboardInterrupt:\n        pass\n'"
nlp_architect/utils/embedding.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nimport os\nimport sys\nfrom typing import List\n\nimport numpy as np\n\nfrom gensim.models import FastText\n\nfrom nlp_architect.utils.text import Vocabulary\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_word_embeddings(file_path, vocab=None):\n    """"""\n    Loads a word embedding model text file into a word(str) to numpy vector dictionary\n\n    Args:\n        file_path (str): path to model file\n        vocab (list of str): optional - vocabulary\n\n    Returns:\n        list: a dictionary of numpy.ndarray vectors\n        int: detected word embedding vector size\n    """"""\n    with open(file_path, encoding=""utf-8"") as fp:\n        word_vectors = {}\n        size = None\n        for line in fp:\n            line_fields = line.split()\n            if len(line_fields) < 5:\n                continue\n            if line[0] == "" "":\n                word_vectors["" ""] = np.asarray(line_fields, dtype=""float32"")\n            elif vocab is None or line_fields[0] in vocab:\n                word_vectors[line_fields[0]] = np.asarray(line_fields[1:], dtype=""float32"")\n                if size is None:\n                    size = len(line_fields[1:])\n    return word_vectors, size\n\n\ndef fill_embedding_mat(src_mat, src_lex, emb_lex, emb_size):\n    """"""\n    Creates a new matrix from given matrix of int words using the embedding\n    model provided.\n\n    Args:\n        src_mat (numpy.ndarray): source matrix\n        src_lex (dict): source matrix lexicon\n        emb_lex (dict): embedding lexicon\n        emb_size (int): embedding vector size\n    """"""\n    emb_mat = np.zeros((src_mat.shape[0], src_mat.shape[1], emb_size))\n    for i, sen in enumerate(src_mat):\n        for j, w in enumerate(sen):\n            if w > 0:\n                w_emb = emb_lex.get(str(src_lex.get(w)).lower())\n                if w_emb is not None:\n                    emb_mat[i][j] = w_emb\n    return emb_mat\n\n\ndef get_embedding_matrix(\n    embeddings: dict, vocab: Vocabulary, embedding_size: int = None, lowercase_only: bool = False\n) -> np.ndarray:\n    """"""\n    Generate a matrix of word embeddings given a vocabulary\n\n    Args:\n        embeddings (dict): a dictionary of embedding vectors\n        vocab (Vocabulary): a Vocabulary\n        embedding_size (int): custom embedding matrix size\n\n    Returns:\n        a 2D numpy matrix of lexicon embeddings\n    """"""\n    emb_size = len(next(iter(embeddings.values())))\n    if embedding_size:\n        mat = np.zeros((embedding_size, emb_size))\n    else:\n        mat = np.zeros((len(vocab), emb_size))\n    if lowercase_only:\n        for word, wid in vocab.vocab.items():\n            vec = embeddings.get(word.lower(), None)\n            if vec is not None:\n                mat[wid] = vec\n    else:\n        for word, wid in vocab.vocab.items():\n            vec = embeddings.get(word, None)\n            if vec is None:\n                vec = embeddings.get(word.lower(), None)\n            if vec is not None:\n                mat[wid] = vec\n    return mat\n\n\ndef load_embedding_file(filename: str, dim: int = None) -> dict:\n    """"""Load a word embedding file\n\n    Args:\n        filename (str): path to embedding file\n\n    Returns:\n        dict: dictionary with embedding vectors\n    """"""\n    if filename is not None and os.path.exists(filename):\n        logger.info(""Loading external word embeddings from {}"".format(filename))\n    embedding_dict = {}\n    with open(filename, encoding=""utf-8"") as fp:\n        for line in fp:\n            split_line = line.split()\n            word = split_line[0]\n            vec = np.array([float(val) for val in split_line[1:]])\n            embedding_dict[word] = vec\n    return embedding_dict\n\n\n# pylint: disable=not-context-manager\nclass ELMoEmbedderTFHUB(object):\n    def __init__(self):\n        try:\n            import tensorflow as tf\n            import tensorflow_hub as hub\n        except (AttributeError, ImportError):\n            logger.error(\n                ""tensorflow_hub is not installed, ""\n                + ""please install nlp_architect with [all] package. ""\n                + ""for example: pip install nlp_architect[all]""\n            )\n            sys.exit()\n\n        self.g = tf.Graph()\n\n        with self.g.as_default():\n            text_input = tf.compat.v1.placeholder(dtype=tf.string)\n            text_input_size = tf.compat.v1.placeholder(dtype=tf.int32)\n            print(\n                ""Loading Tensorflow hub ELMo model, ""\n                ""might take a while on first load (model is downloaded from web)""\n            )\n            self.elmo = hub.Module(""https://tfhub.dev/google/elmo/3"", trainable=False)\n            self.inputs = {""tokens"": text_input, ""sequence_len"": text_input_size}\n            self.embedding = self.elmo(inputs=self.inputs, signature=""tokens"", as_dict=True)[""elmo""]\n\n            sess = tf.compat.v1.Session(graph=self.g)\n            sess.run(tf.compat.v1.global_variables_initializer())\n            sess.run(tf.compat.v1.tables_initializer())\n            self.s = sess\n\n    def get_vector(self, tokens):\n        vec = self.s.run(\n            self.embedding,\n            feed_dict={self.inputs[""tokens""]: [tokens], self.inputs[""sequence_len""]: [len(tokens)]},\n        )\n        return np.squeeze(vec, axis=0)\n\n\nclass FasttextEmbeddingsModel(object):\n    """"""Fasttext embedding trainer class\n\n    Args:\n        texts (List[List[str]]): list of tokenized sentences\n        size (int): embedding size\n        epochs (int, optional): number of epochs to train\n        window (int, optional): The maximum distance between\n        the current and predicted word within a sentence\n\n    """"""\n\n    def __init__(self, size: int = 5, window: int = 3, min_count: int = 1, skipgram: bool = True):\n        model = FastText(size=size, window=window, min_count=min_count, sg=skipgram)\n        self.model = model\n\n    def train(self, texts: List[List[str]], epochs: int = 100):\n        self.model.build_vocab(texts)\n        self.model.train(sentences=texts, total_examples=len(texts), epochs=epochs)\n\n    def vec(self, word: str) -> np.ndarray:\n        """"""return vector corresponding given word\n        """"""\n        return self.model.wv[word]\n\n    def __getitem__(self, item):\n        return self.vec(item)\n\n    def save(self, path) -> None:\n        """"""save model to path\n        """"""\n        self.model.save(path)\n\n    @classmethod\n    def load(cls, path):\n        """"""load model from path\n        """"""\n        loaded_model = FastText.load(path)\n        new_model = cls()\n        new_model.model = loaded_model\n        return new_model\n'"
nlp_architect/utils/file_cache.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nUtilities for working with the local dataset cache.\n""""""\nimport os\nimport logging\nimport shutil\nimport tempfile\nimport json\nfrom urllib.parse import urlparse\nfrom pathlib import Path\nfrom typing import Tuple, Union, IO\nfrom hashlib import sha256\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.utils.io import load_json_file\n\nimport requests\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nMODEL_CACHE = LIBRARY_OUT / ""pretrained_models""\n\n\ndef cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = MODEL_CACHE\n    else:\n        cache_dir = cache_dir\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (""http"", ""https""):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    if os.path.exists(url_or_filename):\n        # File, and it exists.\n        print(""File already exists. No further processing needed."")\n        return url_or_filename\n    if parsed.scheme == """":\n        # File, but it doesn\'t exist.\n        raise FileNotFoundError(""file {} not found"".format(url_or_filename))\n\n    # Something unknown\n    raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef url_to_filename(url: str, etag: str = None) -> str:\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    if url.split(""/"")[-1].endswith(""zip""):\n        url_bytes = url.encode(""utf-8"")\n        url_hash = sha256(url_bytes)\n        filename = url_hash.hexdigest()\n        if etag:\n            etag_bytes = etag.encode(""utf-8"")\n            etag_hash = sha256(etag_bytes)\n            filename += ""."" + etag_hash.hexdigest()\n    else:\n        filename = url.split(""/"")[-1]\n\n    return filename\n\n\ndef filename_to_url(filename: str, cache_dir: str = None) -> Tuple[str, str]:\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = MODEL_CACHE\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise FileNotFoundError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + "".json""\n    if not os.path.exists(meta_path):\n        raise FileNotFoundError(""file {} not found"".format(meta_path))\n\n    with open(meta_path) as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[""url""]\n    etag = metadata[""etag""]\n\n    return url, etag\n\n\ndef http_get(url: str, temp_file: IO) -> None:\n    req = requests.get(url, stream=True)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            temp_file.write(chunk)\n\n\ndef get_from_cache(url: str, cache_dir: str = None) -> str:\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = MODEL_CACHE\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    response = requests.head(url, allow_redirects=True)\n    if response.status_code != 200:\n        raise IOError(\n            ""HEAD request failed for url {} with status code {}"".format(url, response.status_code)\n        )\n    etag = response.headers.get(""ETag"")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    need_downloading = True\n\n    if os.path.exists(cache_path):\n        # check if etag has changed comparing with the metadata\n        if url.split(""/"")[-1].endswith(""zip""):\n            meta_path = cache_path + "".json""\n        else:\n            meta_path = cache_path + ""_meta_"" + "".json""\n        meta = load_json_file(meta_path)\n        if meta[""etag""] == etag:\n            print(""file already present"")\n            need_downloading = False\n\n    if need_downloading:\n        print(""File not present or etag changed"")\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, ""wb"") as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {""url"": url, ""etag"": etag}\n            if url.split(""/"")[-1].endswith(""zip""):\n                meta_path = cache_path + "".json""\n            else:\n                meta_path = cache_path + ""_meta_"" + "".json""\n            with open(meta_path, ""w"") as meta_file:\n                json.dump(meta, meta_file)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path, need_downloading\n'"
nlp_architect/utils/generic.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport re\n\nimport numpy as np\n\n\n# pylint: disable=invalid-unary-operand-type\ndef pad_sentences(\n    sequences: np.ndarray, max_length: int = None, padding_value: int = 0, padding_style=""post""\n) -> np.ndarray:\n    """"""\n    Pad input sequences up to max_length\n    values are aligned to the right\n\n    Args:\n        sequences (iter): a 2D matrix (np.array) to pad\n        max_length (int, optional): max length of resulting sequences\n        padding_value (int, optional): padding value\n        padding_style (str, optional): add padding values as prefix (use with \'pre\')\n            or postfix (use with \'post\')\n\n    Returns:\n        input sequences padded to size \'max_length\'\n    """"""\n    if isinstance(sequences, list) and len(sequences) > 0:\n        try:\n            sequences = np.asarray(sequences)\n        except ValueError:\n            print(""cannot convert sequences into numpy array"")\n    assert hasattr(sequences, ""shape"")\n    if len(sequences) < 1:\n        return sequences\n    if max_length is None:\n        max_length = np.max([len(s) for s in sequences])\n    elif max_length < 1:\n        raise ValueError(""max sequence length must be > 0"")\n    if max_length < 1:\n        return sequences\n    padded_sequences = np.ones((len(sequences), max_length), dtype=np.int32) * padding_value\n    for i, sent in enumerate(sequences):\n        if padding_style == ""post"":\n            trunc = sent[-max_length:]\n            padded_sequences[i, : len(trunc)] = trunc\n        elif padding_style == ""pre"":\n            trunc = sent[:max_length]\n            padded_sequences[i, -trunc:] = trunc\n    return padded_sequences.astype(dtype=np.int32)\n\n\ndef one_hot(mat: np.ndarray, num_classes: int) -> np.ndarray:\n    """"""\n    Convert a 1D matrix of ints into one-hot encoded vectors.\n\n    Arguments:\n        mat (numpy.ndarray): A 1D matrix of labels (int)\n        num_classes (int): Number of all possible classes\n\n    Returns:\n        numpy.ndarray: A 2D matrix\n    """"""\n    assert len(mat.shape) < 2 or isinstance(mat.shape, int)\n    vec = np.zeros((mat.shape[0], num_classes))\n    for i, v in enumerate(mat):\n        vec[i][v] = 1.0\n    return vec\n\n\ndef one_hot_sentence(mat: np.ndarray, num_classes: int) -> np.ndarray:\n    """"""\n    Convert a 2D matrix of ints into one-hot encoded 3D matrix\n\n    Arguments:\n        mat (numpy.ndarray): A 2D matrix of labels (int)\n        num_classes (int): Number of all possible classes\n\n    Returns:\n        numpy.ndarray: A 3D matrix\n    """"""\n    new_mat = []\n    for i in range(mat.shape[0]):\n        new_mat.append(one_hot(mat[i], num_classes))\n    return np.asarray(new_mat)\n\n\ndef add_offset(mat: np.ndarray, offset: int = 1) -> np.ndarray:\n    """"""\n    Add +1 to all values in matrix mat\n\n    Arguments:\n        mat (numpy.ndarray): A 2D matrix with int values\n        offset (int): offset to add\n\n    Returns:\n        numpy.ndarray: input matrix\n    """"""\n    for i, vec in enumerate(mat):\n        offset_arr = np.array(vec.shape)\n        offset_arr.fill(offset)\n        mat[i] = vec + offset_arr\n    return mat\n\n\ndef license_prompt(model_name, model_website, dataset_dir=None):\n    if dataset_dir:\n        print(""\\n\\n***\\n{} was not found in the directory: {}"".format(model_name, dataset_dir))\n    else:\n        print(""\\n\\n***\\n\\n{} was not found on local installation"".format(model_name))\n    print(""{} can be downloaded from {}"".format(model_name, model_website))\n    print(\n        ""The terms and conditions of the data set license apply. Intel does not ""\n        ""grant any rights to the data files or database\\n""\n    )\n    response = input(\n        ""To download \'{}\' from {}, please enter YES: "".format(model_name, model_website)\n    )\n    res = response.lower().strip()\n    if res == ""yes"" or (len(res) == 1 and res == ""y""):\n        print(""Downloading {}..."".format(model_name))\n        responded_yes = True\n    else:\n        print(""Download declined. Response received {} != YES|Y. "".format(res))\n        if dataset_dir:\n            print(\n                ""Please download the model manually from {} and place in the directory: {}"".format(\n                    model_website, dataset_dir\n                )\n            )\n        else:\n            print(""Please download the model manually from {}"".format(model_website))\n        responded_yes = False\n    return responded_yes\n\n\n# character vocab\nzhang_lecun_vocab = list(""abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\xe2\x80\x99/\\\\|_@#$%\xcb\x86&*\xcb\x9c\xe2\x80\x98+=<>()[]{}"")\nvocab_hash = {b: a for a, b in enumerate(zhang_lecun_vocab)}\n\n\ndef normalize(\n    txt,\n    vocab=None,\n    replace_char="" "",\n    max_length=300,\n    pad_out=True,\n    to_lower=True,\n    reverse=False,\n    truncate_left=False,\n    encoding=None,\n):\n\n    # remove html\n    # This will keep characters and other symbols\n    txt = txt.split()\n    # Remove HTML\n    txt = [re.sub(r""http:.*"", """", r) for r in txt]\n    txt = [re.sub(r""https:.*"", """", r) for r in txt]\n\n    txt = "" "".join(txt)\n\n    # Remove punctuation\n    txt = re.sub(""[.,!]"", "" "", txt)\n    txt = "" "".join(txt.split())\n\n    # store length for multiple comparisons\n    txt_len = len(txt)\n\n    if truncate_left:\n        txt = txt[-max_length:]\n    else:\n        txt = txt[:max_length]\n    # change case\n    if to_lower:\n        txt = txt.lower()\n    # Reverse order\n    if reverse:\n        txt = txt[::-1]\n    # replace chars\n    if vocab is not None:\n        txt = """".join([c if c in vocab else replace_char for c in txt])\n    # re-encode text\n    if encoding is not None:\n        txt = txt.encode(encoding, errors=""ignore"")\n    # pad out if needed\n    if pad_out and max_length > txt_len:\n        txt = txt + replace_char * (max_length - txt_len)\n    return txt\n\n\ndef to_one_hot(txt, vocab=vocab_hash):\n    vocab_size = len(vocab.keys())\n    one_hot_vec = np.zeros((vocab_size + 1, len(txt)), dtype=np.float32)\n    # run through txt and ""switch on"" relevant positions in one-hot vector\n    for idx, char in enumerate(txt):\n        if char in vocab_hash:\n            vocab_idx = vocab_hash[char]\n            one_hot_vec[vocab_idx, idx] = 1\n        # raised if character is out of vocabulary\n        else:\n            pass\n    return one_hot_vec\n\n\ndef balance(df):\n    print(""Balancing the classes"")\n    type_counts = df[""Sentiment""].value_counts()\n    min_count = min(type_counts.values)\n\n    balanced_df = None\n    for key in type_counts.keys():\n        df_sub = df[df[""Sentiment""] == key].sample(n=min_count, replace=False)\n        if balanced_df is not None:\n            balanced_df = balanced_df.append(df_sub)\n        else:\n            balanced_df = df_sub\n    return balanced_df\n'"
nlp_architect/utils/io.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport gzip\nimport io\nimport json\nimport os\nimport posixpath\nimport re\nimport sys\nimport zipfile\nfrom os import PathLike, makedirs, remove\nfrom pathlib import Path\nfrom urllib.parse import urlparse\n\nimport requests\nfrom tqdm import tqdm\n\nfrom nlp_architect.utils.generic import license_prompt\n\n\ndef download_unlicensed_file(url, sourcefile, destfile, totalsz=None):\n    """"""\n    Download the file specified by the given URL.\n\n    Args:\n        url (str): url to download from\n        sourcefile (str): file to download from url\n        destfile (str): save path\n        totalsz (:obj:`int`, optional): total size of file\n    """"""\n    req = requests.get(posixpath.join(url, sourcefile), stream=True)\n\n    chunksz = 1024 ** 2\n    if totalsz is None:\n        if ""Content-length"" in req.headers:\n            totalsz = int(req.headers[""Content-length""])\n            nchunks = totalsz // chunksz\n        else:\n            print(""Unable to determine total file size."")\n            nchunks = None\n    else:\n        nchunks = totalsz // chunksz\n\n    print(""Downloading file to: {}"".format(destfile))\n    with open(destfile, ""wb"") as f:\n        for data in tqdm(req.iter_content(chunksz), total=nchunks, unit=""MB"", file=sys.stdout):\n            f.write(data)\n    print(""Download Complete"")\n\n\ndef uncompress_file(filepath: str or os.PathLike, outpath="".""):\n    """"""\n    Unzip a file to the same location of filepath\n    uses decompressing algorithm by file extension\n\n    Args:\n        filepath (str): path to file\n        outpath (str): path to extract to\n    """"""\n    filepath = str(filepath)\n    if filepath.endswith("".gz""):\n        if os.path.isdir(outpath):\n            raise ValueError(""output path for gzip must be a file"")\n        with gzip.open(filepath, ""rb"") as fp:\n            file_content = fp.read()\n        with open(outpath, ""wb"") as fp:\n            fp.write(file_content)\n        return None\n    # To unzip zipped model files having SHA-encoded etag and url as filename\n    # raise ValueError(\'Unsupported archive provided. Method supports only .zip/.gz files.\')\n    with zipfile.ZipFile(filepath) as z:\n        z.extractall(outpath)\n        return [x for x in z.namelist() if not (x.startswith(""__MACOSX"") or x.endswith(""/""))]\n\n\ndef zipfile_list(filepath: str or os.PathLike):\n    """"""\n    List the files inside a given zip file\n\n    Args:\n        filepath (str): path to file\n\n    Returns:\n        String list of filenames\n    """"""\n    with zipfile.ZipFile(filepath) as z:\n        return [x for x in z.namelist() if not (x.startswith(""__MACOSX"") or x.endswith(""/""))]\n\n\ndef gzip_str(g_str):\n    """"""\n    Transform string to GZIP coding\n\n    Args:\n        g_str (str): string of data\n\n    Returns:\n        GZIP bytes data\n    """"""\n    compressed_str = io.BytesIO()\n    with gzip.GzipFile(fileobj=compressed_str, mode=""w"") as file_out:\n        file_out.write((json.dumps(g_str).encode()))\n    bytes_obj = compressed_str.getvalue()\n    return bytes_obj\n\n\ndef check_directory_and_create(dir_path):\n    """"""\n    Check if given directory exists, create if not.\n\n    Args:\n        dir_path (str): path to directory\n    """"""\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n\ndef walk_directory(directory, verbose=False):\n    """"""Iterates a directory\'s text files and their contents.""""""\n    for dir_path, _, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(dir_path, filename)\n            if os.path.isfile(file_path) and not filename.startswith("".""):\n                with io.open(file_path, ""r"", encoding=""utf-8"") as file:\n                    if verbose:\n                        print(""Reading {}"".format(filename))\n                    doc_text = file.read()\n                    yield filename, doc_text\n\n\ndef validate(*args):\n    """"""\n    Validate all arguments are of correct type and in correct range.\n    Args:\n        *args (tuple of tuples): Each tuple represents an argument validation like so:\n        Option 1 - With range check:\n            (arg, class, min_val, max_val)\n        Option 2 - Without range check:\n            (arg, class)\n        If class is a tuple of type objects check if arg is an instance of any of the types.\n        To allow a None valued argument, include type(None) in class.\n        To disable lower or upper bound check, set min_val or max_val to None, respectively.\n        If arg has the len attribute (such as string), range checks are performed on its length.\n    """"""\n    for arg in args:\n        arg_val = arg[0]\n        arg_type = (arg[1],) if isinstance(arg[1], type) else arg[1]\n        if not isinstance(arg_val, arg_type):\n            raise TypeError(""Expected type {}"".format("" or "".join([t.__name__ for t in arg_type])))\n        if arg_val is not None and len(arg) >= 4:\n            name = ""of "" + arg[4] if len(arg) == 5 else """"\n            arg_min = arg[2]\n            arg_max = arg[3]\n            if hasattr(arg_val, ""__len__""):\n                val = ""Length""\n                num = len(arg_val)\n            else:\n                val = ""Value""\n                num = arg_val\n            if arg_min is not None and num < arg_min:\n                raise ValueError(""{} {} must be greater or equal to {}"".format(val, name, arg_min))\n            if arg_max is not None and num >= arg_max:\n                raise ValueError(""{} {} must be less than {}"".format(val, name, arg_max))\n\n\ndef validate_existing_filepath(arg):\n    """"""Validates an input argument is a path string to an existing file.""""""\n    validate((arg, str, 0, 255))\n    if not os.path.isfile(arg):\n        raise ValueError(""{0} does not exist."".format(arg))\n    return arg\n\n\ndef validate_existing_directory(arg):\n    """"""Validates an input argument is a path string to an existing directory.""""""\n    arg = os.path.abspath(arg)\n    validate((arg, str, 0, 255))\n    if not os.path.isdir(arg):\n        raise ValueError(""{0} does not exist"".format(arg))\n    return arg\n\n\ndef validate_existing_path(arg):\n    """"""Validates an input argument is a path string to an existing file or directory.""""""\n    arg = os.path.abspath(arg)\n    validate((arg, str, 0, 255))\n    if not os.path.exists(arg):\n        raise ValueError(""{0} does not exist"".format(arg))\n    return arg\n\n\ndef validate_parent_exists(arg):\n    """"""Validates an input argument is a path string, and its parent directory exists.""""""\n    arg = os.path.abspath(arg)\n    dir_arg = os.path.dirname(os.path.abspath(arg))\n    if validate_existing_directory(dir_arg):\n        return arg\n    return None\n\n\ndef valid_path_append(path, *args):\n    """"""\n    Helper to validate passed path directory and append any subsequent\n    filename arguments.\n\n    Arguments:\n        path (str): Initial filesystem path.  Should expand to a valid\n                    directory.\n        *args (list, optional): Any filename or path suffices to append to path\n                                for returning.\n        Returns:\n            (list, str): path prepended list of files from args, or path alone if\n                     no args specified.\n    Raises:\n        ValueError: if path is not a valid directory on this filesystem.\n    """"""\n    full_path = os.path.expanduser(path)\n    res = []\n    if not os.path.exists(full_path):\n        os.makedirs(full_path)\n    if not os.path.isdir(full_path):\n        raise ValueError(""path: {0} is not a valid directory"".format(path))\n    for suffix_path in args:\n        res.append(os.path.join(full_path, suffix_path))\n    if len(res) == 0:\n        return path\n    if len(res) == 1:\n        return res[0]\n    return res\n\n\ndef sanitize_path(path):\n    s_path = os.path.normpath(""/"" + path).lstrip(""/"")\n    assert len(s_path) < 255\n    return s_path\n\n\ndef check(validator):\n    class CustomAction(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            validator(values)\n            setattr(namespace, self.dest, values)\n\n    return CustomAction\n\n\ndef check_size(min_size=None, max_size=None):\n    class CustomAction(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            validate((values, self.type, min_size, max_size, self.dest))\n            setattr(namespace, self.dest, values)\n\n    return CustomAction\n\n\ndef validate_proxy_path(arg):\n    """"""Validates an input argument is a valid proxy path or None""""""\n    proxy_validation_regex = re.compile(\n        r""^(?:http|ftp)s?://""  # http:// or https://\n        r""(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|""\n        r""localhost|""  # localhost...\n        r""\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})""  # ...or ip\n        r""(?::\\d+)?""  # optional port\n        r""(?:/?|[/?]\\S+)$"",\n        re.IGNORECASE,\n    )\n    if arg is not None and re.match(proxy_validation_regex, arg) is None:\n        raise ValueError(""{0} is not a valid proxy path"".format(arg))\n    return arg\n\n\ndef validate_boolean(arg):\n    """"""Validates an input argument of type boolean""""""\n    if arg.lower() not in [""true"", ""false""]:\n        raise argparse.ArgumentTypeError(""expected true | false argument"")\n    return arg.lower() == ""true""\n\n\ndef load_json_file(file_path):\n    """"""load a file into a json object""""""\n    try:\n        with open(file_path) as small_file:\n            return json.load(small_file)\n    except OSError as e:\n        print(e)\n        print(""trying to read file in blocks"")\n        with open(file_path) as big_file:\n            json_string = """"\n            while True:\n                block = big_file.read(64 * (1 << 20))  # Read 64 MB at a time;\n                json_string = json_string + block\n                if not block:  # Reached EOF\n                    break\n            return json.loads(json_string)\n\n\ndef json_dumper(obj):\n    """"""for objects that have members that cant be serialized and implement toJson() method""""""\n    try:\n        return obj.toJson()\n    except Exception:\n        return obj.__dict__\n\n\ndef load_files_from_path(dir_path, extension=""txt""):\n    """"""load all files from given directory (with given extension)""""""\n    files = [\n        os.path.join(dir_path, f)\n        for f in os.listdir(dir_path)\n        if os.path.isfile(os.path.join(dir_path, f)) and f.endswith(extension)\n    ]\n    files_data = []\n    for f in files:\n        with open(f) as fp:\n            files_data.append("" "".join(map(str.strip, fp.readlines())))\n    return files_data\n\n\ndef create_folder(path):\n    if path:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n\ndef download_unzip(\n    url: str, sourcefile: str, unzipped_path: str or PathLike, license_msg: str = None\n):\n    """"""Downloads a zip file, extracts it to destination, deletes the zip file. If license_msg is\n    supplied, user is prompted for download confirmation.""""""\n    dest_parent = Path(unzipped_path).parent\n\n    if not os.path.exists(unzipped_path):\n        if license_msg is None or license_prompt(license_msg, urlparse(url).netloc):\n            zip_path = dest_parent / sourcefile\n            makedirs(dest_parent, exist_ok=True)\n            download_unlicensed_file(url, sourcefile, zip_path)\n            print(""Unzipping..."")\n            uncompress_file(zip_path, dest_parent)\n            remove(zip_path)\n    return unzipped_path\n\n\ndef line_count(file):\n    """"""Utility function for getting number of lines in a text file.""""""\n    count = 0\n    with open(file, encoding=""utf-8"") as f:\n        for _ in f:\n            count += 1\n    return count\n\n\ndef prepare_output_path(output_dir: str, overwrite_output_dir: str):\n    """"""Create output directory or throw error if exists and overwrite_output_dir is false\n    """"""\n    if os.path.exists(output_dir) and os.listdir(output_dir) and not overwrite_output_dir:\n        raise ValueError(\n            ""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir ""\n            ""to overcome."".format(output_dir)\n        )\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n'"
nlp_architect/utils/metrics.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n##\n# This file contains code from\n#  (https://github.com/chakki-works/seqeval/tree/master/seqeval/metrics)\n#\n# The code was changed to support BILOU format evaluation\n#\n#  MIT License\n#\n# Copyright (c) 2018 chakki\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import f1_score as classification_f1_score\nfrom collections import defaultdict\nimport numpy as np\n\n\ndef get_conll_scores(predictions, y, y_lex, unk=""O""):\n    """"""Get Conll style scores (precision, recall, f1)\n    """"""\n    if isinstance(predictions, list):\n        predictions = predictions[-1]\n    test_p = predictions\n    if len(test_p.shape) > 2:\n        test_p = test_p.argmax(2)\n    test_y = y\n    if len(test_y.shape) > 2:\n        test_y = test_y.argmax(2)\n\n    prediction_data = []\n    for n in range(test_y.shape[0]):\n        test_yval = []\n        for i in list(test_y[n]):\n            try:\n                test_yval.append(y_lex[i])\n            except KeyError:\n                pass\n        test_pval = [unk] * len(test_yval)\n        for e, i in enumerate(list(test_p[n])[: len(test_pval)]):\n            try:\n                test_pval[e] = y_lex[i]\n            except KeyError:\n                pass\n        prediction_data.append((test_yval, test_pval))\n    y_true, y_pred = list(zip(*prediction_data))\n    return classification_report(y_true, y_pred, digits=3)\n\n\ndef simple_accuracy(preds, labels):\n    """"""return simple accuracy\n    """"""\n    return (preds == labels).mean()\n\n\ndef accuracy(preds, labels):\n    """"""return simple accuracy in expected dict format\n    """"""\n    acc = simple_accuracy(preds, labels)\n    return {""acc"": acc}\n\n\ndef acc_and_f1(preds, labels):\n    """"""return accuracy and f1 score\n    """"""\n    acc = simple_accuracy(preds, labels)\n    f1 = classification_f1_score(y_true=labels, y_pred=preds)\n    return {\n        ""acc"": acc,\n        ""f1"": f1,\n        ""acc_and_f1"": (acc + f1) / 2,\n    }\n\n\ndef pearson_and_spearman(preds, labels):\n    """"""get pearson and spearman correlation\n    """"""\n    pearson_corr = pearsonr(preds, labels)[0]\n    spearman_corr = spearmanr(preds, labels)[0]\n    return {\n        ""pearson"": pearson_corr,\n        ""spearmanr"": spearman_corr,\n        ""corr"": (pearson_corr + spearman_corr) / 2,\n    }\n\n\ndef tagging(preds, labels):\n    p = sequence_precision_score(labels, preds)\n    r = sequence_recall_score(labels, preds)\n    f1 = sequence_f1_score(labels, preds)\n    return p, r, f1\n\n\n##\n# The below code is taken and changed from package chakki-works/seqeval\n# (seqeval/metrics/sequence_labeling.py) The code was changed to support\n# BILOU format evaluation\n##\n\n\n""""""Metrics to assess performance on sequence labeling task given prediction\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n""""""\n\n\ndef get_entities(seq, suffix=False):\n    """"""Gets entities from sequence.\n\n    Args:\n        seq (list): sequence of labels.\n\n    Returns:\n        list: list of (chunk_type, chunk_start, chunk_end).\n\n    Example:\n        >>> from seqeval.metrics.sequence_labeling import get_entities\n        >>> seq = [\'B-PER\', \'I-PER\', \'O\', \'B-LOC\']\n        >>> get_entities(seq)\n        [(\'PER\', 0, 1), (\'LOC\', 3, 3)]\n    """"""\n    # for nested list\n    if any(isinstance(s, list) for s in seq):\n        seq = [item for sublist in seq for item in sublist + [""O""]]\n\n    prev_tag = ""O""\n    prev_type = """"\n    begin_offset = 0\n    chunks = []\n    for i, chunk in enumerate(seq + [""O""]):\n        if suffix:\n            tag = chunk[-1]\n            type_ = chunk.split(""-"")[0]\n        else:\n            tag = chunk[0]\n            type_ = chunk.split(""-"")[-1]\n\n        if end_of_chunk(prev_tag, tag, prev_type, type_):\n            chunks.append((prev_type, begin_offset, i - 1))\n        if start_of_chunk(prev_tag, tag, prev_type, type_):\n            begin_offset = i\n        prev_tag = tag\n        prev_type = type_\n\n    return chunks\n\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    """"""Checks if a chunk ended between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n\n    Returns:\n        chunk_end: boolean.\n    """"""\n    chunk_end = False\n\n    end_tag = (""L"", ""E"")\n    start_tag = (""U"", ""S"")\n\n    if prev_tag in end_tag:\n        chunk_end = True\n    if prev_tag in start_tag:\n        chunk_end = True\n\n    if prev_tag == ""B"" and tag == ""B"":\n        chunk_end = True\n    if prev_tag == ""B"" and tag in start_tag:\n        chunk_end = True\n    if prev_tag == ""B"" and tag == ""O"":\n        chunk_end = True\n    if prev_tag == ""I"" and tag == ""B"":\n        chunk_end = True\n    if prev_tag == ""I"" and tag in start_tag:\n        chunk_end = True\n    if prev_tag == ""I"" and tag == ""O"":\n        chunk_end = True\n\n    if prev_tag != ""O"" and prev_tag != ""."" and prev_type != type_:\n        chunk_end = True\n\n    return chunk_end\n\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    """"""Checks if a chunk started between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n\n    Returns:\n        chunk_start: boolean.\n    """"""\n    chunk_start = False\n\n    end_tag = (""L"", ""E"")\n    start_tag = (""U"", ""S"")\n\n    if tag == ""B"":\n        chunk_start = True\n    if tag in start_tag:\n        chunk_start = True\n\n    if prev_tag in end_tag and tag in end_tag:\n        chunk_start = True\n    if prev_tag in end_tag and tag == ""I"":\n        chunk_start = True\n    if prev_tag in start_tag and tag in end_tag:\n        chunk_start = True\n    if prev_tag in start_tag and tag == ""I"":\n        chunk_start = True\n    if prev_tag == ""O"" and tag in end_tag:\n        chunk_start = True\n    if prev_tag == ""O"" and tag == ""I"":\n        chunk_start = True\n\n    if tag != ""O"" and tag != ""."" and prev_type != type_:\n        chunk_start = True\n\n    return chunk_start\n\n\ndef sequence_f1_score(y_true, y_pred, suffix=False):\n    """"""Compute the F1 score.\n\n    The F1 score can be interpreted as a weighted average of the precision and\n    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n    The relative contribution of precision and recall to the F1 score are\n    equal. The formula for the F1 score is::\n\n        F1 = 2 * (precision * recall) / (precision + recall)\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import f1_score\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'],\n        >>> [\'B-PER\', \'I-PER\', \'O\']]\n        >>> f1_score(y_true, y_pred)\n        0.50\n    """"""\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n    nb_true = len(true_entities)\n\n    p = nb_correct / nb_pred if nb_pred > 0 else 0\n    r = nb_correct / nb_true if nb_true > 0 else 0\n    score = 2 * p * r / (p + r) if p + r > 0 else 0\n\n    return score\n\n\ndef sequence_accuracy_score(y_true, y_pred):\n    """"""Accuracy classification score.\n\n    In multilabel classification, this function computes subset accuracy:\n    the set of labels predicted for a sample must *exactly* match the\n    corresponding set of labels in y_true.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import accuracy_score\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'],\n        >>> [\'B-PER\', \'I-PER\', \'O\']]\n        >>> accuracy_score(y_true, y_pred)\n        0.80\n    """"""\n    if any(isinstance(s, list) for s in y_true):\n        y_true = [item for sublist in y_true for item in sublist]\n        y_pred = [item for sublist in y_pred for item in sublist]\n\n    nb_correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))\n    nb_true = len(y_true)\n\n    score = nb_correct / nb_true\n\n    return score\n\n\ndef sequence_precision_score(y_true, y_pred, suffix=False):\n    """"""Compute the precision.\n\n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample.\n\n    The best value is 1 and the worst value is 0.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import precision_score\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'],\n        >>> [\'B-PER\', \'I-PER\', \'O\']]\n        >>> precision_score(y_true, y_pred)\n        0.50\n    """"""\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n\n    score = nb_correct / nb_pred if nb_pred > 0 else 0\n\n    return score\n\n\ndef sequence_recall_score(y_true, y_pred, suffix=False):\n    """"""Compute the recall.\n\n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n\n    The best value is 1 and the worst value is 0.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import recall_score\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'],\n        >>> [\'B-PER\', \'I-PER\', \'O\']]\n        >>> recall_score(y_true, y_pred)\n        0.50\n    """"""\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_true = len(true_entities)\n\n    score = nb_correct / nb_true if nb_true > 0 else 0\n\n    return score\n\n\ndef sequence_performance_measure(y_true, y_pred):\n    """"""\n    Compute the performance metrics: TP, FP, FN, TN\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        performance_dict : dict\n\n    Example:\n        >>> from seqeval.metrics import performance_measure\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'O\', \'B-ORG\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> performance_measure(y_true, y_pred)\n        (3, 3, 1, 4)\n    """"""\n    performace_dict = dict()\n    if any(isinstance(s, list) for s in y_true):\n        y_true = [item for sublist in y_true for item in sublist]\n        y_pred = [item for sublist in y_pred for item in sublist]\n    performace_dict[""TP""] = sum(\n        y_t == y_p for y_t, y_p in zip(y_true, y_pred) if ((y_t != ""O"") or (y_p != ""O""))\n    )\n    performace_dict[""FP""] = sum(y_t != y_p for y_t, y_p in zip(y_true, y_pred))\n    performace_dict[""FN""] = sum(((y_t != ""O"") and (y_p == ""O"")) for y_t, y_p in zip(y_true, y_pred))\n    performace_dict[""TN""] = sum((y_t == y_p == ""O"") for y_t, y_p in zip(y_true, y_pred))\n\n    return performace_dict\n\n\ndef classification_report(y_true, y_pred, digits=2, suffix=False):\n    """"""Build a text report showing the main classification metrics.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a classifier.\n        digits : int. Number of digits for formatting output floating point values.\n\n    Returns:\n        report : string. Text summary of the precision, recall, F1 score for each class.\n\n    Examples:\n        >>> from seqeval.metrics import classification_report\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'],\n        >>> [\'B-PER\', \'I-PER\', \'O\']]\n        >>> print(classification_report(y_true, y_pred))\n                     precision    recall  f1-score   support\n        <BLANKLINE>\n               MISC       0.00      0.00      0.00         1\n                PER       1.00      1.00      1.00         1\n        <BLANKLINE>\n          micro avg       0.50      0.50      0.50         2\n          macro avg       0.50      0.50      0.50         2\n        <BLANKLINE>\n    """"""\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    name_width = 0\n    d1 = defaultdict(set)\n    d2 = defaultdict(set)\n    for e in true_entities:\n        d1[e[0]].add((e[1], e[2]))\n        name_width = max(name_width, len(e[0]))\n    for e in pred_entities:\n        d2[e[0]].add((e[1], e[2]))\n\n    last_line_heading = ""macro avg""\n    width = max(name_width, len(last_line_heading), digits)\n\n    headers = [""precision"", ""recall"", ""f1-score"", ""support""]\n    head_fmt = ""{:>{width}s} "" + "" {:>9}"" * len(headers)\n    report = head_fmt.format("""", *headers, width=width)\n    report += ""\\n\\n""\n\n    row_fmt = ""{:>{width}s} "" + "" {:>9.{digits}f}"" * 3 + "" {:>9}\\n""\n\n    ps, rs, f1s, s = [], [], [], []\n    for type_name, true_entities in d1.items():\n        pred_entities = d2[type_name]\n        nb_correct = len(true_entities & pred_entities)\n        nb_pred = len(pred_entities)\n        nb_true = len(true_entities)\n\n        p = nb_correct / nb_pred if nb_pred > 0 else 0\n        r = nb_correct / nb_true if nb_true > 0 else 0\n        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n\n        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n\n        ps.append(p)\n        rs.append(r)\n        f1s.append(f1)\n        s.append(nb_true)\n\n    report += ""\\n""\n\n    # compute averages\n    report += row_fmt.format(\n        ""micro avg"",\n        sequence_precision_score(y_true, y_pred, suffix=suffix),\n        sequence_recall_score(y_true, y_pred, suffix=suffix),\n        sequence_f1_score(y_true, y_pred, suffix=suffix),\n        np.sum(s),\n        width=width,\n        digits=digits,\n    )\n    report += row_fmt.format(\n        last_line_heading,\n        np.average(ps, weights=s),\n        np.average(rs, weights=s),\n        np.average(f1s, weights=s),\n        np.sum(s),\n        width=width,\n        digits=digits,\n    )\n\n    return report\n\n\n# up to here code from seqeval/metrics/sequence_labeling.py\n# (https://github.com/chakki-works/seqeval/tree/master/seqeval/metrics)\n'"
nlp_architect/utils/string_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport os\nimport re\nimport string\nfrom typing import List\n\nfrom nlp_architect.utils.io import load_json_file\nfrom nlp_architect.utils.text import SpacyInstance\n\nCURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\n\nSTOP_WORDS_FILE = os.path.join(CURRENT_DIR, ""resources/stop_words_en.json"")\nPRONOUN_FILE = os.path.join(CURRENT_DIR, ""resources/pronoun_en.json"")\nPREPOSITION_FILE = os.path.join(CURRENT_DIR, ""resources/preposition_en.json"")\nDETERMINERS_FILE = os.path.join(CURRENT_DIR, ""resources/determiners_en.json"")\n\nDISAMBIGUATION_CATEGORY = [""disambig"", ""disambiguation""]\n\n\nclass StringUtils:\n    spacy_no_parser = SpacyInstance(disable=[""parser""])\n    spacy_parser = SpacyInstance()\n    stop_words = []\n    pronouns = []\n    preposition = []\n    determiners = []\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def is_stop(token: str) -> bool:\n        if not StringUtils.stop_words:\n            StringUtils.stop_words = load_json_file(STOP_WORDS_FILE)\n            StringUtils.stop_words.extend(DISAMBIGUATION_CATEGORY)\n        if token not in StringUtils.stop_words:\n            return False\n        return True\n\n    @staticmethod\n    def normalize_str(in_str: str) -> str:\n        str_clean = (\n            re.sub(""["" + string.punctuation + string.whitespace + ""]"", "" "", in_str).strip().lower()\n        )\n        if isinstance(str_clean, str):\n            str_clean = str(str_clean)\n\n        parser = StringUtils.spacy_no_parser.parser\n        doc = parser(str_clean)\n        ret_clean = []\n        for token in doc:\n            lemma = token.lemma_.strip()\n            if not StringUtils.is_pronoun(lemma) and not StringUtils.is_stop(lemma):\n                ret_clean.append(token.lemma_)\n\n        return "" "".join(ret_clean)\n\n    @staticmethod\n    def is_pronoun(in_str: str) -> bool:\n        if not StringUtils.pronouns:\n            StringUtils.pronouns = load_json_file(PRONOUN_FILE)\n\n        tokens = in_str.split()\n        if len(tokens) == 1:\n            if tokens[0] in StringUtils.pronouns:\n                return True\n        return False\n\n    @staticmethod\n    def is_determiner(in_str: str) -> bool:\n        if not StringUtils.determiners:\n            StringUtils.determiners = load_json_file(DETERMINERS_FILE)\n\n        tokens = in_str.split()\n        if len(tokens) == 1:\n            if tokens[0] in StringUtils.determiners:\n                return True\n        return False\n\n    @staticmethod\n    def is_preposition(in_str: str) -> bool:\n        if not StringUtils.preposition:\n            StringUtils.preposition = load_json_file(PREPOSITION_FILE)\n\n        tokens = in_str.split()\n        if len(tokens) == 1:\n            if tokens[0] in StringUtils.preposition:\n                return True\n        return False\n\n    @staticmethod\n    def normalize_string_list(str_list: str) -> List[str]:\n        ret_list = []\n        for _str in str_list:\n            normalize_str = StringUtils.normalize_str(_str)\n            if normalize_str != """":\n                ret_list.append(normalize_str)\n        return ret_list\n\n    @staticmethod\n    def find_head_lemma_pos_ner(x: str):\n        """"""""\n\n        :param x: mention\n        :return: the head word and the head word lemma of the mention\n        """"""\n        head = ""UNK""\n        lemma = ""UNK""\n        pos = ""UNK""\n        ner = ""UNK""\n\n        # pylint: disable=not-callable\n        doc = StringUtils.spacy_parser.parser(x)\n        for tok in doc:\n            if tok.head == tok:\n                head = tok.text\n                lemma = tok.lemma_\n                pos = tok.pos_\n\n        for ent in doc.ents:\n            if ent.root.text == head:\n                ner = ent.label_\n\n        return head, lemma, pos, ner\n'"
nlp_architect/utils/testing.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport os\nimport pathlib\nimport shutil\nimport tempfile\nfrom unittest import TestCase\n\nTEST_DIR = tempfile.mkdtemp(prefix=""nlp_architect_tests"")\n\n\nclass NLPArchitectTestCase(TestCase):\n    def setUp(self):\n        self.TEST_DIR = pathlib.Path(TEST_DIR)\n        os.makedirs(self.TEST_DIR, exist_ok=True)\n\n    def tearDown(self):\n        shutil.rmtree(self.TEST_DIR)\n'"
nlp_architect/utils/text.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport re\nimport string\nimport sys\nfrom os import path\nfrom typing import List, Tuple\n\nimport spacy\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem.snowball import EnglishStemmer\nfrom spacy.cli.download import download as spacy_download\nfrom spacy.lang.en import LEMMA_EXC, LEMMA_INDEX, LEMMA_RULES\nfrom spacy.lemmatizer import Lemmatizer\n\nfrom nlp_architect.utils.generic import license_prompt\n\n\nclass Vocabulary:\n    """"""\n    A vocabulary that maps words to ints (storing a vocabulary)\n    """"""\n\n    def __init__(self, start=0, include_oov=True):\n\n        self._vocab = {}\n        self._rev_vocab = {}\n        self.include_oov = include_oov\n        if include_oov:\n            self._vocab[""<UNK>""] = start\n            self.oov_id = start\n            self._rev_vocab[start] = ""<UNK>""\n            self.next = start + 1\n        else:\n            self.next = start\n\n    def add(self, word):\n        """"""\n        Add word to vocabulary\n\n        Args:\n            word (str): word to add\n\n        Returns:\n            int: id of added word\n        """"""\n        if word not in self._vocab.keys():\n            self._vocab[word] = self.next\n            self._rev_vocab[self.next] = word\n            self.next += 1\n        return self._vocab.get(word)\n\n    def word_id(self, word):\n        """"""\n        Get the word_id of given word\n\n        Args:\n            word (str): word from vocabulary\n\n        Returns:\n            int: int id of word\n        """"""\n        if hasattr(self, ""oov_id""):\n            return self._vocab.get(word, self.oov_id)\n        return self._vocab.get(word, None)\n\n    def __getitem__(self, item):\n        """"""\n        Get the word_id of given word (same as `word_id`)\n        """"""\n        return self.word_id(item)\n\n    def __len__(self):\n        vocab_size = len(self._vocab)\n        if hasattr(self, ""include_oov"") and self.include_oov:\n            vocab_size += 1\n        return vocab_size\n\n    def __iter__(self):\n        for word in self.vocab.keys():\n            yield word\n\n    @property\n    def max(self):\n        return self.next\n\n    def id_to_word(self, wid):\n        """"""\n        Word-id to word (string)\n\n        Args:\n            wid (int): word id\n\n        Returns:\n            str: string of given word id\n        """"""\n        return self._rev_vocab.get(wid)\n\n    @property\n    def vocab(self):\n        """"""\n        dict: get the dict object of the vocabulary\n        """"""\n        return self._vocab\n\n    def add_vocab_offset(self, offset):\n        """"""\n        Adds an offset to the ints of the vocabulary\n\n        Args:\n            offset (int): an int offset\n        """"""\n        new_vocab = {}\n        for k, v in self.vocab.items():\n            new_vocab[k] = v + offset\n        self.next += offset\n        self._vocab = new_vocab\n        self._rev_vocab = {v: k for k, v in new_vocab.items()}\n\n    def reverse_vocab(self):\n        """"""\n        Return the vocabulary as a reversed dict object\n\n        Returns:\n            dict: reversed vocabulary object\n        """"""\n        return self._rev_vocab\n\n\nall_letters = string.ascii_letters + "" .,;\'""\nn_letters = len(all_letters)\n\n\ndef char_to_id(c):\n    """"""return int id of given character\n        OOV char = len(all_letter) + 1\n\n    Args:\n        c (str): string character\n\n    Returns:\n        int: int value of given char\n    """"""\n    char_idx = all_letters.find(c)\n    if char_idx == -1:\n        char_idx = n_letters\n    return char_idx\n\n\ndef id_to_char(c_id):\n    """"""return character of given char id\n    """"""\n    if c_id < n_letters:\n        return all_letters[c_id]\n    return None\n\n\ndef try_to_load_spacy(model_name):\n    try:\n        spacy.load(model_name)\n        return True\n    except OSError:\n        return False\n\n\nclass SpacyInstance:\n    """"""\n    Spacy pipeline wrapper which prompts user for model download authorization.\n\n    Args:\n        model (str, optional): spacy model name (default: english small model)\n        disable (list of string, optional): pipeline annotators to disable\n            (default: [])\n        display_prompt (bool, optional): flag to display/skip license prompt\n    """"""\n\n    def __init__(self, model=""en"", disable=None, display_prompt=True):\n        if disable is None:\n            disable = []\n        try:\n            self._parser = spacy.load(model, disable=disable)\n        except OSError:\n            url = ""https://spacy.io/models""\n            if display_prompt and license_prompt(""Spacy {} model"".format(model), url) is False:\n                sys.exit(0)\n            spacy_download(model)\n            self._parser = spacy.load(model, disable=disable)\n\n    @property\n    def parser(self):\n        """"""return Spacy\'s instance parser""""""\n        return self._parser\n\n    def tokenize(self, text: str) -> List[str]:\n        """"""\n        Tokenize a sentence into tokens\n        Args:\n            text (str): text to tokenize\n\n        Returns:\n            list: a list of str tokens of input\n        """"""\n        # pylint: disable=not-callable\n\n        return [t.text for t in self.parser(text)]\n\n\nstemmer = EnglishStemmer()\nlemmatizer = WordNetLemmatizer()\nspacy_lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\np = re.compile(r""[ \\-,;.@&_]"")\n\n\nclass Stopwords(object):\n    """"""\n    Stop words list class.\n    """"""\n\n    stop_words = []\n\n    @staticmethod\n    def get_words():\n        if not Stopwords.stop_words:\n            sw_path = path.join(path.dirname(path.realpath(__file__)), ""resources"", ""stopwords.txt"")\n            with open(sw_path) as fp:\n                stop_words = []\n                for w in fp:\n                    stop_words.append(w.strip().lower())\n            Stopwords.stop_words = stop_words\n        return Stopwords.stop_words\n\n\ndef simple_normalizer(text):\n    """"""\n    Simple text normalizer. Runs each token of a phrase thru wordnet lemmatizer\n    and a stemmer.\n    """"""\n    if not str(text).isupper() or not str(text).endswith(""S"") or not len(text.split()) == 1:\n        tokens = list(filter(lambda x: len(x) != 0, p.split(text.strip())))\n        text = "" "".join([stemmer.stem(lemmatizer.lemmatize(t)) for t in tokens])\n    return text\n\n\ndef spacy_normalizer(text, lemma=None):\n    """"""\n    Simple text normalizer using spacy lemmatizer. Runs each token of a phrase\n    thru a lemmatizer and a stemmer.\n    Arguments:\n        text(string): the text to normalize.\n        lemma(string): lemma of the given text. in this case only stemmer will\n        run.\n    """"""\n    if not str(text).isupper() or not str(text).endswith(""S"") or not len(text.split()) == 1:\n        tokens = list(filter(lambda x: len(x) != 0, p.split(text.strip())))\n        if lemma:\n            lemma = lemma.split("" "")\n            text = "" "".join([stemmer.stem(lem) for lem in lemma])\n        else:\n            text = "" "".join([stemmer.stem(spacy_lemmatizer(t, ""NOUN"")[0]) for t in tokens])\n    return text\n\n\ndef read_sequential_tagging_file(file_path, ignore_line_patterns=None):\n    """"""\n    Read a tab separated sequential tagging file.\n    Returns a list of list of tuple of tags (sentences, words)\n\n    Args:\n        file_path (str): input file path\n        ignore_line_patterns (list, optional): list of string patterns to ignore\n\n    Returns:\n        list of list of tuples\n    """"""\n    if ignore_line_patterns:\n        assert isinstance(ignore_line_patterns, list), ""ignore_line_patterns must be a list""\n\n    def _split_into_sentences(file_lines):\n        sentences = []\n        s = []\n        for line in file_lines:\n            if len(line) == 0:\n                sentences.append(s)\n                s = []\n                continue\n            s.append(line)\n        if len(s) > 0:\n            sentences.append(s)\n        return sentences\n\n    with open(file_path, encoding=""utf-8"") as fp:\n        data = fp.readlines()\n        data = [d.strip() for d in data]\n        if ignore_line_patterns:\n            for s in ignore_line_patterns:\n                data = [d for d in data if s not in d]\n        data = [tuple(d.split()) for d in data]\n    return _split_into_sentences(data)\n\n\ndef word_vector_generator(data, lower=False, start=0):\n    """"""\n    Word vector generator util.\n    Transforms a list of sentences into numpy int vectors and returns the\n    constructed vocabulary\n\n    Arguments:\n        data (list): list of list of strings\n        lower (bool, optional): transform strings into lower case\n        start (int, optional): vocabulary index start integer\n\n    Returns:\n        2D numpy array and Vocabulary of the detected words\n    """"""\n    vocab = Vocabulary(start)\n    data_vec = []\n    for sentence in data:\n        sentence_vec = []\n        for w in sentence:\n            word = w\n            if lower:\n                word = word.lower()\n            wid = vocab.add(word)\n            sentence_vec.append(wid)\n        data_vec.append(sentence_vec)\n    return data_vec, vocab\n\n\ndef character_vector_generator(data, start=0):\n    """"""\n    Character word vector generator util.\n    Transforms a list of sentences into numpy int vectors of the characters\n    of the words of the sentence, and returns the constructed vocabulary\n\n    Arguments:\n        data (list): list of list of strings\n        start (int, optional): vocabulary index start integer\n\n    Returns:\n        np.array: a 2D numpy array\n        Vocabulary: constructed vocabulary\n    """"""\n    vocab = Vocabulary(start)\n    data_vec = []\n    for sentence in data:\n        sentence_vec = []\n        for w in sentence:\n            word_vec = []\n            for char in w:\n                cid = vocab.add(char)\n                word_vec.append(cid)\n            sentence_vec.append(word_vec)\n        data_vec.append(sentence_vec)\n    return data_vec, vocab\n\n\ndef extract_nps(annotation_list, text=None):\n    """"""\n    Extract Noun Phrases from given text tokens and phrase annotations.\n    Returns a list of tuples with start/end indexes.\n\n    Args:\n        annotation_list (list): a list of annotation tags in str\n        text (list, optional): a list of token texts in str\n\n    Returns:\n        list of start/end markers of noun phrases, if text is provided a list of noun phrase texts\n    """"""\n    np_starts = [i for i in range(len(annotation_list)) if annotation_list[i] == ""B-NP""]\n    np_markers = []\n    for s in np_starts:\n        i = 1\n        while s + i < len(annotation_list) and annotation_list[s + i] == ""I-NP"":\n            i += 1\n        np_markers.append((s, s + i))\n    return_markers = np_markers\n    if text:\n        assert len(text) == len(annotation_list), ""annotations/text length mismatch""\n        return_markers = ["" "".join(text[s:e]) for s, e in np_markers]\n    return return_markers\n\n\ndef bio_to_spans(text: List[str], tags: List[str]) -> List[Tuple[int, int, str]]:\n    """"""\n    Convert BIO tagged list of strings into span starts and ends\n    Args:\n        text: list of words\n        tags: list of tags\n\n    Returns:\n        tuple: list of start, end and tag of detected spans\n    """"""\n    pointer = 0\n    starts = []\n    for i, t, in enumerate(tags):\n        if t.startswith(""B-""):\n            starts.append((i, pointer))\n        pointer += len(text[i]) + 1\n\n    spans = []\n    for s_i, s_char in starts:\n        label_str = tags[s_i][2:]\n        e = 0\n        e_char = len(text[s_i + e])\n        while len(tags) > s_i + e + 1 and tags[s_i + e + 1].startswith(""I-""):\n            e += 1\n            e_char += 1 + len(text[s_i + e])\n        spans.append((s_char, s_char + e_char, label_str))\n    return spans\n'"
server/angular-ui/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n'"
solutions/absa_solution/__init__.py,0,b''
solutions/absa_solution/sentiment_solution.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport json\nfrom os import PathLike\nfrom os.path import isdir\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport csv\nimport re\nfrom abc import abstractmethod\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.common.core_nlp_doc import CoreNLPDoc\nfrom nlp_architect.models.absa.inference.data_types import (\n    TermType,\n    SentimentDocEncoder,\n    SentimentDoc,\n    SentimentSentence,\n)\nfrom nlp_architect.models.absa.inference.inference import SentimentInference\nfrom nlp_architect.models.absa.utils import load_opinion_lex\nfrom nlp_architect.utils.io import (\n    walk_directory,\n    validate_existing_filepath,\n    validate_existing_directory,\n    validate_existing_path,\n    line_count,\n)\n\nSENTIMENT_OUT = LIBRARY_OUT / ""absa_solution""\n\n\nclass Anonymiser(object):\n    """"""Abstract class for anonymiser algorithm, intended for privacy keeping.""""""\n\n    @abstractmethod\n    def run(self, text):\n        pass\n\n\nclass TweetAnonymiser(Anonymiser):\n    """"""Anonymiser for tweets which uses lexicon for simple string replacements.""""""\n\n    def __init__(self, lexicon_path):\n        self.entity_dict = self._init_entity_dict(lexicon_path)\n\n    @staticmethod\n    def _init_entity_dict(lexicon_path):\n        ret = {}\n        with open(lexicon_path, encoding=""utf-8"") as f:\n            for row in csv.reader(f):\n                ret[row[0]] = [_ for _ in row[1:] if _]\n        return ret\n\n    def run(self, text):\n        for anonymised, entities in self.entity_dict.items():\n            for entity in entities:\n                text = re.sub(entity, anonymised, text, flags=re.IGNORECASE)\n        text = "" "".join(\n            [\n                ""@other_entity""\n                if (word.startswith(""@"") and word[1:] not in self.entity_dict.keys())\n                else word\n                for word in text.split()\n            ]\n        )\n        return text\n\n\ndef _ui_format(sent: SentimentSentence, doc: SentimentDoc) -> str:\n    """"""Get sentence as HTML with 4 classes: aspects, opinions, negations and intensifiers.""""""\n    text = doc.doc_text[sent.start : sent.end + 1]\n    seen = set()\n    for term in sorted([t for e in sent.events for t in e], key=lambda t: t.start)[::-1]:\n        if term.start not in seen:\n            seen.add(term.start)\n            start = term.start - sent.start\n            end = start + term.len\n            label = term.type.value + ""_"" + term.polarity.value\n            text = """".join(\n                (text[:start], \'<span class=""\', label, \'"">\', text[start:end], ""</span>"", text[end:])\n            )\n    return text\n\n\nclass SentimentSolution(object):\n    """"""Main class for executing Sentiment Solution pipeline.\n\n    Args:\n        anonymiser (Anonymiser, optional): Method to anonymise events\' text.\n        max_events (int, optional): Maximum number of events to show for each aspect-polarity pair.\n    """"""\n\n    def __init__(self, anonymiser: Anonymiser = None, max_events: int = 400):\n        self.anonymiser = anonymiser\n        self.max_events = max_events\n        SENTIMENT_OUT.mkdir(parents=True, exist_ok=True)\n\n    def run(\n        self,\n        aspect_lex: PathLike = None,\n        opinion_lex: PathLike = None,\n        data: PathLike = None,\n        parsed_data: PathLike = None,\n        inference_results: PathLike = None,\n    ) -> Optional[pd.DataFrame]:\n\n        opinions = load_opinion_lex(opinion_lex)\n        if not opinions:\n            raise ValueError(""Empty opinion lexicon!"")\n        aspects = pd.read_csv(aspect_lex, header=None, encoding=""utf-8"")[0]\n        if aspects.empty:\n            raise ValueError(""Empty aspect lexicon!"")\n        if inference_results:\n            with open(inference_results, encoding=""utf-8"") as f:\n                results = json.loads(f.read(), object_hook=SentimentDoc.decoder)\n        elif data or parsed_data:\n            inference = SentimentInference(aspect_lex, opinions, parse=False)\n            parse = None\n            if not parsed_data:  # source data is raw text, need to parse\n                from nlp_architect.pipelines.spacy_bist import SpacyBISTParser\n\n                parse = SpacyBISTParser().parse\n\n            results = {}\n            print(""Running inference on data files... (Iterating data files)"")\n            data_source = parsed_data if parsed_data else data\n            for file, doc in self._iterate_docs(data_source):\n                parsed_doc = (\n                    parse(doc) if parse else json.loads(doc, object_hook=CoreNLPDoc.decoder)\n                )\n                sentiment_doc = inference.run(parsed_doc=parsed_doc)\n                if sentiment_doc:\n                    results[file] = sentiment_doc\n            with open(SENTIMENT_OUT / ""inference_results.json"", ""w"", encoding=""utf-8"") as f:\n                json.dump(results, f, cls=SentimentDocEncoder, indent=4, sort_keys=True)\n        else:\n            print(\n                ""No input given. Please supply one of: ""\n                ""data directory, parsed data directory, or inference results.""\n            )\n            return None\n\n        print(""\\nComputing statistics..."")\n        stats = self._compute_stats(results, aspects, opinions)\n        print(""Done."")\n        return stats\n\n    @staticmethod\n    def _iterate_docs(data: PathLike) -> tuple:\n        if isdir(data):\n            for file, doc_text in tqdm(list(walk_directory(data))):\n                yield file, doc_text\n        else:\n            with open(data, encoding=""utf-8"") as f:\n                for i, doc_text in tqdm(enumerate(f), total=line_count(data)):\n                    yield str(i + 1), doc_text\n\n    def _compute_stats(self, results: dict, aspects: list, opinion_lex: dict) -> pd.DataFrame:\n        """"""Aggregates counts for each aspect-polarity pairs, with separate counts for in-domain\n         only events.\n        """"""\n        index = pd.MultiIndex.from_product(\n            [aspects, [""POS"", ""NEG""], [False, True]], names=[""Aspect"", ""Polarity"", ""inDomain""]\n        )\n        stats = pd.DataFrame(columns=[""Quantity"", ""Score""], index=index)\n        stats[[""Quantity"", ""Score""]] = stats[[""Quantity"", ""Score""]].fillna(0)\n        stats = stats.sort_index()\n        scores = stats.copy()\n\n        for doc in tqdm(results.values()):\n            for sent in doc.sentences:\n                for event in sent.events:\n                    aspect = [t for t in event if t.type == TermType.ASPECT][0]\n                    opinion = [t for t in event if t.type == TermType.OPINION][0]\n                    score = aspect.score\n                    key = aspect.text, aspect.polarity.name\n                    count = self._add_event(stats, key, False, score)\n                    in_domain = opinion_lex[opinion.text.lower()].is_acquired\n                    count_dom = self._add_event(stats, key, True, score) if in_domain else -1\n\n                    if count <= self.max_events:\n                        sent_ui = _ui_format(sent, doc)\n                        self._add_sentence(sent_ui, stats, scores, key, False, count, score)\n                        if in_domain:\n                            self._add_sentence(sent_ui, stats, scores, key, True, count_dom, score)\n        for key in index:  # sort sentences according to their scores\n            stats.loc[key, 2:] = stats.loc[key][2:][np.argsort(scores.loc[key][2:])].tolist()\n        return stats\n\n    def _add_sentence(\n        self,\n        sent_ui: str,\n        stats: pd.DataFrame,\n        scores: pd.DataFrame,\n        key: tuple,\n        in_domain: bool,\n        count: int,\n        score: int,\n    ) -> int:\n        """"""Utility function for adding event sentence to output.""""""\n        sent_ui = self.anonymiser.run(sent_ui) if self.anonymiser else sent_ui\n        sent_key = key + (in_domain,), ""Sent_"" + str(count)\n        stats.at[sent_key] = sent_ui\n        scores.at[sent_key] = -abs(score)\n        return count\n\n    @staticmethod\n    def _add_event(df: pd.DataFrame, key: tuple, in_domain: bool, score: int) -> int:\n        """"""Utility function for incrementing event counts.""""""\n        key = key + (in_domain,)\n        count = int(df.loc[key, ""Quantity""]) + 1\n        df.loc[key, ""Quantity""] = count\n        df.loc[key, ""Score""] += score\n        return count\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=""Aspect-Based Sentiment Analysis"")\n    parser.add_argument(""--data"", type=validate_existing_path, help=""Path to data"")\n    parser.add_argument(\n        ""--aspects"", type=validate_existing_filepath, help=""Path to aspect lexicon"", required=True\n    )\n    parser.add_argument(\n        ""--opinions"", type=validate_existing_filepath, help=""Path to opinion lexicon"", required=True\n    )\n    parser.add_argument(""--parsed"", type=validate_existing_directory, help=""Path to parsed data"")\n    parser.add_argument(""--res"", type=validate_existing_filepath, help=""Path to inference results"")\n    args = parser.parse_args()\n\n    solution = SentimentSolution()\n    solution.run(\n        data=args.data,\n        parsed_data=args.parsed,\n        inference_results=args.res,\n        aspect_lex=args.aspects,\n        opinion_lex=args.opinions,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
solutions/absa_solution/test_absa_solution.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom nlp_architect import LIBRARY_ROOT\nfrom nlp_architect.utils.io import download_unzip\nfrom .sentiment_solution import SentimentSolution, SENTIMENT_OUT\n\n\ndef test_solution(generate_new=False):\n    lexicons_dir = Path(LIBRARY_ROOT) / ""examples"" / ""absa""\n    expected_dir = Path(LIBRARY_ROOT) / ""tests"" / ""fixtures"" / ""data"" / ""absa_solution""\n    data_url = ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/tests/""\n    parsed_data = download_unzip(\n        data_url, ""tripadvisor_test_parsed.zip"", SENTIMENT_OUT / ""test"" / ""tripadvisor_test_parsed""\n    )\n\n    predicted_stats = SentimentSolution().run(\n        parsed_data=parsed_data,\n        aspect_lex=lexicons_dir / ""aspects.csv"",\n        opinion_lex=lexicons_dir / ""opinions.csv"",\n    )\n\n    predicted_stats.to_csv(""predicted.csv"", encoding=""utf-8"")\n    predicted_trimmed = pd.read_csv(""predicted.csv"", encoding=""utf-8"").loc[:, ""Aspect"":""Score""]\n    predicted_trimmed.loc[:, ""Score""] = np.around(predicted_trimmed.loc[:, ""Score""], 2)\n    os.remove(""predicted.csv"")\n\n    if generate_new:\n        with open(""expected.csv"", ""w"", encoding=""utf-8"", newline="""") as f:\n            predicted_trimmed.to_csv(f)\n        assert False\n\n    else:\n        with open(expected_dir / ""expected.csv"", encoding=""utf-8"") as expected_fp:\n            assert predicted_trimmed.to_csv() == expected_fp.read()\n'"
solutions/absa_solution/ui.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport base64\nimport io\nimport os\nimport json\nfrom os.path import join\n\nimport pandas as pd\nimport numpy as np\nfrom bokeh import layouts\nfrom bokeh.document import Document\nfrom bokeh.layouts import widgetbox, row, column, layout\nfrom bokeh.models import Div, CustomJS, HoverTool, Title, ColumnDataSource\nfrom bokeh.models.widgets import DataTable, TableColumn, TextInput\nfrom bokeh.models.widgets import HTMLTemplateFormatter, RadioButtonGroup\nfrom bokeh.models.widgets import Dropdown\nfrom bokeh.models.widgets import Tabs, Panel\nfrom bokeh.plotting import Figure, figure\nfrom bokeh.server.server import Server\nfrom bokeh.transform import dodge\nfrom bokeh.core.properties import value\nfrom tornado.web import StaticFileHandler\n\nfrom nlp_architect import LIBRARY_PATH\nfrom nlp_architect.models.absa import LEXICONS_OUT\nfrom nlp_architect.models.absa.train.acquire_terms import AcquireTerms\nfrom nlp_architect.models.absa.train.train import TrainSentiment\nfrom nlp_architect.models.absa.inference.data_types import SentimentDoc, SentimentSentence\nfrom sentiment_solution import SENTIMENT_OUT, SentimentSolution\n\nSOLUTION_DIR = join(LIBRARY_PATH, ""solutions/absa_solution/"")\nPOLARITIES = (""POS"", ""NEG"")\n\n\n# pylint: disable=global-variable-undefined\ndef serve_absa_ui() -> None:\n    """"""Main function for serving UI application.\n    """"""\n\n    def _doc_modifier(doc: Document) -> None:\n        grid = _create_ui_components()\n        doc.add_root(grid)\n\n    print(""Opening Bokeh application on http://localhost:5006/"")\n    server = Server(\n        {""/"": _doc_modifier},\n        websocket_max_message_size=5000 * 1024 * 1024,\n        extra_patterns=[\n            (\n                ""/style/(.*)"",\n                StaticFileHandler,\n                {""path"": os.path.normpath(join(SOLUTION_DIR, ""/style""))},\n            )\n        ],\n    )\n    server.start()\n    server.io_loop.add_callback(server.show, ""/"")\n    server.io_loop.start()\n\n\ndef _create_header(train_dropdown, inference_dropdown, text_status) -> layouts.Row:\n    """"""Utility function for creating and styling the header row in the UI layout.""""""\n\n    architect_logo = Div(\n        text=\'<a href=""http://nlp_architect.nervanasys.com""> <img border=""0"" \'\n        \'src=""style/nlp_architect.jpg"" width=""200""></a> by Intel\xc2\xae AI Lab\',\n        style={\n            ""margin-left"": ""500px"",\n            ""margin-top"": ""20px"",\n            ""font-size"": ""110%"",\n            ""text-align"": ""center"",\n        },\n    )\n    css_link = Div(\n        text=""<link rel=\'stylesheet\' type=\'text/css\' href=\'style/lexicon_manager.css\'>"",\n        style={""font-size"": ""0%""},\n    )\n\n    js_script = Div(text=""<input type=\'file\' id=\'inputOS\' hidden=\'true\'>"")\n\n    title = Div(\n        text=""ABSApp"",\n        style={\n            ""font-size"": ""300%"",\n            ""color"": ""royalblue"",\n            ""font-weight"": ""bold"",\n            ""margin-left"": ""500px"",\n        },\n    )\n\n    return row(\n        column(\n            row(children=[train_dropdown, lexicons_dropdown, inference_dropdown], width=500),\n            row(text_status),\n        ),\n        css_link,\n        js_script,\n        widgetbox(title, width=900, height=84),\n        widgetbox(architect_logo, width=400, height=84),\n    )\n\n\ndef empty_table(*headers):\n    return ColumnDataSource(data={header: 19 * [""""] for header in headers})\n\n\ndef _create_ui_components() -> (Figure, ColumnDataSource):  # pylint: disable=too-many-statements\n    global asp_table_source, asp_filter_src, op_table_source, op_filter_src\n    global stats, aspects, tabs, lexicons_dropdown\n    stats = pd.DataFrame(columns=[""Quantity"", ""Score""])\n    aspects = pd.Series([])\n\n    def new_col_data_src():\n        return ColumnDataSource({""file_contents"": [], ""file_name"": []})\n\n    large_text = HTMLTemplateFormatter(template=""""""<div><%= value %></div>"""""")\n\n    def data_column(title):\n        return TableColumn(\n            field=title, title=\'<span class=""header"">\' + title + ""</span>"", formatter=large_text\n        )\n\n    asp_table_columns = [\n        data_column(""Term""),\n        data_column(""Alias1""),\n        data_column(""Alias2""),\n        data_column(""Alias3""),\n    ]\n    op_table_columns = [data_column(""Term""), data_column(""Score""), data_column(""Polarity"")]\n\n    asp_table_source = empty_table(""Term"", ""Alias1"", ""Alias2"", ""Alias3"")\n    asp_filter_src = empty_table(""Term"", ""Alias1"", ""Alias2"", ""Alias3"")\n    asp_src = new_col_data_src()\n\n    op_table_source = empty_table(""Term"", ""Score"", ""Polarity"", ""Polarity"")\n    op_filter_src = empty_table(""Term"", ""Score"", ""Polarity"", ""Polarity"")\n    op_src = new_col_data_src()\n\n    asp_table = DataTable(\n        source=asp_table_source,\n        selectable=""checkbox"",\n        columns=asp_table_columns,\n        editable=True,\n        width=600,\n        height=500,\n    )\n    op_table = DataTable(\n        source=op_table_source,\n        selectable=""checkbox"",\n        columns=op_table_columns,\n        editable=True,\n        width=600,\n        height=500,\n    )\n\n    asp_examples_box = _create_examples_table()\n    op_examples_box = _create_examples_table()\n    asp_layout = layout([[asp_table, asp_examples_box]])\n    op_layout = layout([[op_table, op_examples_box]])\n    asp_tab = Panel(child=asp_layout, title=""Aspect Lexicon"")\n    op_tab = Panel(child=op_layout, title=""Opinion Lexicon"")\n    tabs = Tabs(tabs=[asp_tab, op_tab], width=700, css_classes=[""mytab""])\n\n    lexicons_menu = [(""Open"", ""open""), (""Save"", ""save"")]\n    lexicons_dropdown = Dropdown(\n        label=""Edit Lexicons"",\n        button_type=""success"",\n        menu=lexicons_menu,\n        width=140,\n        height=31,\n        css_classes=[""mybutton""],\n    )\n\n    train_menu = [(""Parsed Data"", ""parsed""), (""Raw Data"", ""raw"")]\n    train_dropdown = Dropdown(\n        label=""Extract Lexicons"",\n        button_type=""success"",\n        menu=train_menu,\n        width=162,\n        height=31,\n        css_classes=[""mybutton""],\n    )\n\n    inference_menu = [(""Parsed Data"", ""parsed""), (""Raw Data"", ""raw"")]\n    inference_dropdown = Dropdown(\n        label=""Classify"",\n        button_type=""success"",\n        menu=inference_menu,\n        width=140,\n        height=31,\n        css_classes=[""mybutton""],\n    )\n\n    text_status = TextInput(\n        value=""Select training data"", title=""Train Run Status:"", css_classes=[""statusText""]\n    )\n    text_status.visible = False\n\n    train_src = new_col_data_src()\n    infer_src = new_col_data_src()\n\n    with open(join(SOLUTION_DIR, ""dropdown.js"")) as f:\n        args = dict(\n            clicked=lexicons_dropdown,\n            asp_filter=asp_filter_src,\n            op_filter=op_filter_src,\n            asp_src=asp_src,\n            op_src=op_src,\n            tabs=tabs,\n            text_status=text_status,\n            train_src=train_src,\n            infer_src=infer_src,\n            train_clicked=train_dropdown,\n            infer_clicked=inference_dropdown,\n            opinion_lex_generic="""",\n        )\n        code = f.read()\n\n    args[""train_clicked""] = train_dropdown\n    train_dropdown.js_on_change(""value"", CustomJS(args=args, code=code))\n\n    args[""train_clicked""] = inference_dropdown\n    inference_dropdown.js_on_change(""value"", CustomJS(args=args, code=code))\n\n    args[""clicked""] = lexicons_dropdown\n    lexicons_dropdown.js_on_change(""value"", CustomJS(args=args, code=code))\n\n    def update_filter_source(table_source, filter_source):\n        df = table_source.to_df()\n        sel_inx = sorted(table_source.selected.indices)\n        df = df.iloc[sel_inx, 1:]\n        new_source = ColumnDataSource(df)\n        filter_source.data = new_source.data\n\n    def update_examples_box(data, examples_box, old, new):\n        examples_box.source.data = {""Examples"": []}\n        unselected = list(set(old) - set(new))\n        selected = list(set(new) - set(old))\n        if len(selected) <= 1 and len(unselected) <= 1:\n            examples_box.source.data.update(\n                {\n                    ""Examples"": [str(data.iloc[unselected[0], i]) for i in range(4, 24)]\n                    if len(unselected) != 0\n                    else [str(data.iloc[selected[0], i]) for i in range(4, 24)]\n                }\n            )\n\n    def asp_selected_change(_, old, new):\n        global asp_filter_src, asp_table_source, aspects_data\n        update_filter_source(asp_table_source, asp_filter_src)\n        update_examples_box(aspects_data, asp_examples_box, old, new)\n\n    def op_selected_change(_, old, new):\n        global op_filter_src, op_table_source, opinions_data\n        update_filter_source(op_table_source, op_filter_src)\n        update_examples_box(opinions_data, op_examples_box, old, new)\n\n    def read_csv(file_src, headers=False, index_cols=False, readCSV=True):\n        if readCSV:\n            raw_contents = file_src.data[""file_contents""][0]\n\n            if len(raw_contents.split("","")) == 1:\n                b64_contents = raw_contents\n            else:\n                # remove the prefix that JS adds\n                b64_contents = raw_contents.split("","", 1)[1]\n            file_contents = base64.b64decode(b64_contents)\n            return pd.read_csv(\n                io.BytesIO(file_contents),\n                encoding=""ISO-8859-1"",\n                keep_default_na=False,\n                na_values={None},\n                engine=""python"",\n                index_col=index_cols,\n                header=0 if headers else None,\n            )\n        return file_src\n\n    def read_parsed_files(file_content, file_name):\n        try:\n            # remove the prefix that JS adds\n            b64_contents = file_content.split("","", 1)[1]\n            file_content = base64.b64decode(b64_contents)\n            with open(SENTIMENT_OUT / file_name, ""w"") as json_file:\n                data_dict = json.loads(file_content.decode(""utf-8""))\n                json.dump(data_dict, json_file)\n        except Exception as e:\n            print(str(e))\n\n    # pylint: disable=unused-argument\n    def train_file_callback(attr, old, new):\n        global train_data\n        SENTIMENT_OUT.mkdir(parents=True, exist_ok=True)\n        train = TrainSentiment(parse=True, rerank_model=None)\n        if len(train_src.data[""file_contents""]) == 1:\n            train_data = read_csv(train_src, index_cols=0)\n            file_name = train_src.data[""file_name""][0]\n            raw_data_path = SENTIMENT_OUT / file_name\n            train_data.to_csv(raw_data_path, header=False)\n            print(""Running_SentimentTraining on data..."")\n            train.run(data=raw_data_path)\n        else:\n            f_contents = train_src.data[""file_contents""]\n            f_names = train_src.data[""file_name""]\n            raw_data_path = SENTIMENT_OUT / train_src.data[""file_name""][0].split(""/"")[0]\n            if not os.path.exists(raw_data_path):\n                os.makedirs(raw_data_path)\n            for f_content, f_name in zip(f_contents, f_names):\n                read_parsed_files(f_content, f_name)\n            print(""Running_SentimentTraining on data..."")\n            train.run(parsed_data=raw_data_path)\n\n        text_status.value = ""Lexicon extraction completed""\n\n        with io.open(AcquireTerms.acquired_aspect_terms_path, ""r"") as fp:\n            aspect_data_csv = fp.read()\n        file_data = base64.b64encode(str.encode(aspect_data_csv))\n        file_data = file_data.decode(""utf-8"")\n        asp_src.data = {""file_contents"": [file_data], ""file_name"": [""nameFile.csv""]}\n\n        out_path = LEXICONS_OUT / ""generated_opinion_lex_reranked.csv""\n        with io.open(out_path, ""r"") as fp:\n            opinion_data_csv = fp.read()\n        file_data = base64.b64encode(str.encode(opinion_data_csv))\n        file_data = file_data.decode(""utf-8"")\n        op_src.data = {""file_contents"": [file_data], ""file_name"": [""nameFile.csv""]}\n\n    def show_analysis() -> None:\n        global stats, aspects, plot, source, tabs\n        plot, source = _create_plot()\n        events_table = _create_events_table()\n\n        # pylint: disable=unused-argument\n        def _events_handler(attr, old, new):\n            _update_events(events_table, events_type.active)\n\n        # Toggle display of in-domain / All aspect mentions\n        events_type = RadioButtonGroup(labels=[""All Events"", ""In-Domain Events""], active=0)\n\n        analysis_layout = layout([[plot], [events_table]])\n\n        # events_type display toggle disabled\n        # analysis_layout = layout([[plot],[events_type],[events_table]])\n\n        analysis_tab = Panel(child=analysis_layout, title=""Analysis"")\n        tabs.tabs.insert(2, analysis_tab)\n        tabs.active = 2\n        events_type.on_change(""active"", _events_handler)\n        source.selected.on_change(""indices"", _events_handler)  # pylint: disable=no-member\n\n    # pylint: disable=unused-argument\n    def infer_file_callback(attr, old, new):\n\n        # run inference on input data and current aspect/opinion lexicons in view\n        global infer_data, stats, aspects\n\n        SENTIMENT_OUT.mkdir(parents=True, exist_ok=True)\n\n        df_aspect = pd.DataFrame.from_dict(asp_filter_src.data)\n        aspect_col_list = [""Term"", ""Alias1"", ""Alias2"", ""Alias3""]\n        df_aspect = df_aspect[aspect_col_list]\n        df_aspect.to_csv(SENTIMENT_OUT / ""aspects.csv"", index=False, na_rep=""NaN"")\n\n        df_opinion = pd.DataFrame.from_dict(op_filter_src.data)\n        opinion_col_list = [""Term"", ""Score"", ""Polarity"", ""isAcquired""]\n        df_opinion = df_opinion[opinion_col_list]\n        df_opinion.to_csv(SENTIMENT_OUT / ""opinions.csv"", index=False, na_rep=""NaN"")\n\n        solution = SentimentSolution()\n\n        if len(infer_src.data[""file_contents""]) == 1:\n            infer_data = read_csv(infer_src, index_cols=0)\n            file_name = infer_src.data[""file_name""][0]\n            raw_data_path = SENTIMENT_OUT / file_name\n            infer_data.to_csv(raw_data_path, header=False)\n            print(""Running_SentimentInference on data..."")\n            text_status.value = ""Running classification on data...""\n            stats = solution.run(\n                data=raw_data_path,\n                aspect_lex=SENTIMENT_OUT / ""aspects.csv"",\n                opinion_lex=SENTIMENT_OUT / ""opinions.csv"",\n            )\n        else:\n            f_contents = infer_src.data[""file_contents""]\n            f_names = infer_src.data[""file_name""]\n            raw_data_path = SENTIMENT_OUT / infer_src.data[""file_name""][0].split(""/"")[0]\n            if not os.path.exists(raw_data_path):\n                os.makedirs(raw_data_path)\n            for f_content, f_name in zip(f_contents, f_names):\n                read_parsed_files(f_content, f_name)\n            print(""Running_SentimentInference on data..."")\n            text_status.value = ""Running classification on data...""\n            stats = solution.run(\n                parsed_data=raw_data_path,\n                aspect_lex=SENTIMENT_OUT / ""aspects.csv"",\n                opinion_lex=SENTIMENT_OUT / ""opinions.csv"",\n            )\n\n        aspects = pd.read_csv(SENTIMENT_OUT / ""aspects.csv"", encoding=""utf-8"")[""Term""]\n        text_status.value = ""Classification completed""\n        show_analysis()\n\n    # pylint: disable=unused-argument\n    def asp_file_callback(attr, old, new):\n        global aspects_data, asp_table_source\n        aspects_data = read_csv(asp_src, headers=True)\n        # Replaces None values by empty string\n        aspects_data = aspects_data.fillna("""")\n        new_source = ColumnDataSource(aspects_data)\n        asp_table_source.data = new_source.data\n        asp_table_source.selected.indices = list(range(len(aspects_data)))\n\n    # pylint: disable=unused-argument\n    def op_file_callback(attr, old, new):\n        global opinions_data, op_table_source, lexicons_dropdown, df_opinion_generic\n        df = read_csv(op_src, headers=True)\n        # Replaces None values by empty string\n        df = df.fillna("""")\n        # Placeholder for generic opinion lexicons from the given csv file\n        df_opinion_generic = df[df[""isAcquired""] == ""N""]\n        # Update the argument value for the callback customJS\n        lexicons_dropdown.js_property_callbacks.get(""change:value"")[0].args[\n            ""opinion_lex_generic""\n        ] = df_opinion_generic.to_dict(orient=""list"")\n        opinions_data = df[df[""isAcquired""] == ""Y""]\n        new_source = ColumnDataSource(opinions_data)\n        op_table_source.data = new_source.data\n        op_table_source.selected.indices = list(range(len(opinions_data)))\n\n    # pylint: disable=unused-argument\n    def txt_status_callback(attr, old, new):\n        print(""Previous label: "" + old)\n        print(""Updated label: "" + new)\n\n    text_status.on_change(""value"", txt_status_callback)\n\n    asp_src.on_change(""data"", asp_file_callback)\n    # pylint: disable=no-member\n    asp_table_source.selected.on_change(""indices"", asp_selected_change)\n\n    op_src.on_change(""data"", op_file_callback)\n    op_table_source.selected.on_change(""indices"", op_selected_change)  # pylint: disable=no-member\n\n    train_src.on_change(""data"", train_file_callback)\n    infer_src.on_change(""data"", infer_file_callback)\n\n    return layout([[_create_header(train_dropdown, inference_dropdown, text_status)], [tabs]])\n\n\ndef _create_events_table() -> DataTable:\n    """"""Utility function for creating and styling the events table.""""""\n    formatter = HTMLTemplateFormatter(\n        template=""""""\n    <style>\n        .AS_POS {color: #0000FF; font-weight: bold;}\n        .AS_NEG {color: #0000FF; font-weight: bold;}\n        .OP_POS {color: #1aaa0d; font-style: bold;}\n        .OP_NEG {color: #f40000;font-style: bold;}\n        .NEG_POS {font-style: italic;}\n        .NEG_NEG {color: #f40000; font-style: italic;}\n        .INT_POS {color: #1aaa0d; font-style: italic;}\n        .INT_NEG {color: #f40000; font-style: italic;}\n    </style>\n    <%= value %>""""""\n    )\n    columns = [\n        TableColumn(field=""POS_events"", title=""Positive Examples"", formatter=formatter),\n        TableColumn(field=""NEG_events"", title=""Negative Examples"", formatter=formatter),\n    ]\n    return DataTable(\n        source=ColumnDataSource(),\n        columns=columns,\n        height=400,\n        index_position=None,\n        width=2110,\n        sortable=False,\n        editable=True,\n        reorderable=False,\n    )\n\n\ndef _create_plot() -> (Figure, ColumnDataSource):\n    """"""Utility function for creating and styling the bar plot.""""""\n    global source, aspects, stats\n    pos_counts, neg_counts = (\n        [stats.loc[(asp, pol, False), ""Quantity""] for asp in aspects] for pol in POLARITIES\n    )\n    np.seterr(divide=""ignore"")\n    source = ColumnDataSource(\n        data={\n            ""aspects"": aspects,\n            ""POS"": pos_counts,\n            ""NEG"": neg_counts,\n            ""log-POS"": np.log2(pos_counts),\n            ""log-NEG"": np.log2(neg_counts),\n        }\n    )\n    np.seterr(divide=""warn"")\n    p = figure(\n        plot_height=145,\n        sizing_mode=""scale_width"",\n        x_range=aspects,\n        toolbar_location=""right"",\n        tools=""save, tap"",\n    )\n    rs = [\n        p.vbar(\n            x=dodge(""aspects"", -0.207, range=p.x_range),\n            top=""log-POS"",\n            width=0.4,\n            source=source,\n            color=""limegreen"",\n            legend=value(""POS""),\n            name=""POS"",\n        ),\n        p.vbar(\n            x=dodge(""aspects"", 0.207, range=p.x_range),\n            top=""log-NEG"",\n            width=0.4,\n            source=source,\n            color=""orangered"",\n            legend=value(""NEG""),\n            name=""NEG"",\n        ),\n    ]\n    for r in rs:\n        p.add_tools(\n            HoverTool(tooltips=[(""Aspect"", ""@aspects""), (r.name, ""@"" + r.name)], renderers=[r])\n        )\n    p.add_layout(\n        Title(text="" "" * 7 + ""Sentiment Count (log scale)"", align=""left"", text_font_size=""23px""),\n        ""left"",\n    )\n    p.yaxis.ticker = []\n    p.y_range.start = 0\n    p.xgrid.grid_line_color = None\n    p.xaxis.major_label_text_font_size = ""20pt""\n    p.legend.label_text_font_size = ""20pt""\n    return p, source\n\n\ndef _update_events(events: DataTable, in_domain: bool) -> None:\n    """"""Utility function for updating the content of the events table.""""""\n    i = source.selected.indices\n    events.source.data.update(\n        {\n            pol + ""_events"": stats.loc[aspects[i[0]], pol, in_domain][""Sent_1"":].replace(np.nan, """")\n            if i\n            else []\n            for pol in POLARITIES\n        }\n    )\n\n\ndef _ui_format(sent: SentimentSentence, doc: SentimentDoc) -> str:\n    """"""Get sentence as HTML with 4 classes: aspects, opinions, negations and intensifiers.""""""\n    text = doc.doc_text[sent.start : sent.end + 1]\n    seen = set()\n    for term in sorted([t for e in sent.events for t in e], key=lambda t: t.start)[::-1]:\n        if term.start not in seen:\n            seen.add(term.start)\n            start = term.start - sent.start\n            end = start + term.len\n            label = term.type.value + ""_"" + term.polarity.value\n            text = """".join(\n                (text[:start], \'<span class=""\', label, \'"">\', text[start:end], ""</span>"", text[end:])\n            )\n    return text\n\n\ndef _create_examples_table() -> DataTable:\n    """"""Utility function for creating and styling the events table.""""""\n\n    formatter = HTMLTemplateFormatter(\n        template=""""""\n    <style>\n        .AS {color: #0000FF; font-weight: bold;}\n        .OP {color: #0000FF; font-weight: bold;}\n    </style>\n    <div><%= value %></div>""""""\n    )\n    columns = [\n        TableColumn(\n            field=""Examples"", title=\'<span class=""header"">Examples</span>\', formatter=formatter\n        )\n    ]\n    empty_source = ColumnDataSource()\n    empty_source.data = {""Examples"": []}\n    return DataTable(\n        source=empty_source,\n        columns=columns,\n        height=500,\n        index_position=None,\n        width=1500,\n        sortable=False,\n        editable=False,\n        reorderable=False,\n        header_row=True,\n    )\n\n\nif __name__ == ""__main__"":\n    serve_absa_ui()\n'"
solutions/set_expansion/__init__.py,0,b''
solutions/set_expansion/expand_server.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport socketserver\nimport argparse\nimport pickle\nimport logging\nimport re\n\nfrom nlp_architect.utils.io import validate_existing_filepath, check_size\nfrom prepare_data import extract_noun_phrases, load_parser\nfrom set_expand import SetExpand\n\nlogger = logging.getLogger(__name__)\n\n\nclass MyTCPHandler(socketserver.BaseRequestHandler):\n    """"""\n    A simple server to load the w2v model and handle expand requests from the\n    ui\n    """"""\n\n    def handle(self):\n        logger.info(""handling expand request"")\n        res = """"\n        self.data = pickle.loads(self.request.recv(10240))\n        logger.info(""request data: %s"", self.data)\n        req = self.data[0]\n        if req == ""get_vocab"":\n            logger.info(""getting vocabulary"")\n            res = se.get_vocab()\n        elif req == ""in_vocab"":\n            term = self.data[1]\n            res = se.in_vocab(term)\n        elif req == ""get_group"":\n            term = self.data[1]\n            res = se.get_group(term)\n        elif req == ""annotate"":\n            seed = self.data[1]\n            text = self.data[2]\n            res = self.annotate(text, seed)\n            logger.info(""res:%s"", str(res))\n        elif req == ""expand"":\n            logger.info(""expanding"")\n            data = [x.strip() for x in self.data[1].split("","")]\n            res = se.expand(data)\n        logger.info(""compressing response"")\n        packet = pickle.dumps(res)\n        logger.info(""response length= %s"", str(len(packet)))\n        logger.info(""sending response"")\n        self.request.sendall(packet)\n        logger.info(""done"")\n\n    @staticmethod\n    def annotate(text, seed):\n        # remove extra spaces from text\n        text = re.sub(r""\\s\\s+"", "" "", text)\n        np_list = []\n        docs = [text]\n        spans = extract_noun_phrases(docs, nlp, args.chunker)\n        for x in spans:\n            np = x.text\n            if np not in np_list:\n                np_list.append(np)\n        logger.info(""np_list=%s"", str(np_list))\n        return se.similarity(np_list, seed, args.similarity)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(prog=""expand_server.py"")\n    parser.add_argument(\n        ""model_path"",\n        metavar=""model_path"",\n        type=validate_existing_filepath,\n        help=""a path to the w2v model file"",\n    )\n    parser.add_argument(\n        ""--host"",\n        type=str,\n        default=""localhost"",\n        help=""set port for the server"",\n        action=check_size(1, 20),\n    )\n    parser.add_argument(\n        ""--port"",\n        type=int,\n        default=1234,\n        help=""set port for the server"",\n        action=check_size(0, 65535),\n    )\n    parser.add_argument(""--grouping"", action=""store_true"", default=False, help=""grouping mode"")\n    parser.add_argument(\n        ""--similarity"",\n        default=0.5,\n        type=float,\n        action=check_size(0, 1),\n        help=""similarity threshold"",\n    )\n    parser.add_argument(\n        ""--chunker"",\n        type=str,\n        choices=[""spacy"", ""nlp_arch""],\n        help=""spacy chunker or \'nlp_arch\' for NLP Architect NP Extractor"",\n    )\n    args = parser.parse_args()\n\n    port = args.port\n    model_path = args.model_path\n    logger.info(""loading model"")\n    se = SetExpand(model_path, grouping=args.grouping)\n    logger.info(""loading chunker"")\n    nlp = load_parser(args.chunker)\n    logger.info(""loading server"")\n    HOST, PORT = args.host, port\n    server = socketserver.TCPServer((HOST, PORT), MyTCPHandler)\n    logger.info(""server loaded"")\n    server.serve_forever()\n'"
solutions/set_expansion/prepare_data.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\n""""""\nScript that prepares the input corpus for np2vec training: it runs NP extractor on the corpus and\nmarks extracted NP\'s.\n""""""\nimport gzip\nimport json\nimport logging\nfrom argparse import ArgumentParser\nfrom os import path, makedirs\n\nfrom tqdm import tqdm\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.pipelines.spacy_np_annotator import NPAnnotator, get_noun_phrases\nfrom nlp_architect.utils.io import check_size, download_unlicensed_file, validate_parent_exists\nfrom nlp_architect.utils.text import spacy_normalizer, SpacyInstance\n\nlogger = logging.getLogger(__name__)\n\nnp2id = {}\nid2group = {}\nid2rep = {}\nnp2count = {}\nnlp_chunker_url = ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/models/chunker/""\nchunker_path = str(LIBRARY_OUT / ""chunker-pretrained"")\nchunker_model_dat_file = ""model_info.dat.params""\nchunker_model_file = ""model.h5""\n\n\ndef get_group_norm(spacy_span):\n    """"""\n    Give a span, determine the its group and return the normalized text representing the group\n\n    Args:\n            spacy_span (spacy.tokens.Span)\n    """"""\n    np = spacy_span.text\n    norm = spacy_normalizer(np, spacy_span.lemma_)\n    if args.mark_char in norm:\n        norm = norm.replace(args.mark_char, "" "")\n    if np not in np2count:  # new np\n        np2count[np] = 1\n        np2id[np] = norm\n        if norm in id2group:  # norm already exist\n            id2group[norm].append(np)\n        else:\n            id2group[norm] = [np]\n            id2rep[norm] = np\n    else:  # another occurrence of this np. norm must exist and be consistent\n        np2count[np] += 1\n        if np2id[np] != norm:  # new norm to the same np - merge groups.\n            #  no need to update np2id[np]\n            norm = merge_groups(np, np2id[np], norm)  # set to the already exist\n            #  norm so I know this norm is already in id2group/id2rep\n        else:  # update rep\n            if np2count[np] > np2count[id2rep[norm]]:\n                id2rep[norm] = np  # replace rep\n\n    return norm\n\n\ndef load_parser(chunker):\n    # load spacy parser\n    logger.info(""loading spacy. chunker=%s"", chunker)\n    if ""nlp_arch"" in chunker:\n        parser = SpacyInstance(model=""en_core_web_sm"", disable=[""textcat"", ""ner"", ""parser""]).parser\n        parser.add_pipe(parser.create_pipe(""sentencizer""), first=True)\n        _path_to_model = path.join(chunker_path, chunker_model_file)\n        _path_to_params = path.join(chunker_path, chunker_model_dat_file)\n        if not path.exists(chunker_path):\n            makedirs(chunker_path)\n        if not path.exists(_path_to_model):\n            logger.info(\n                ""The pre-trained model to be downloaded for NLP Architect""\n                "" word chunker model is licensed under Apache 2.0""\n            )\n            download_unlicensed_file(nlp_chunker_url, chunker_model_file, _path_to_model)\n        if not path.exists(_path_to_params):\n            download_unlicensed_file(nlp_chunker_url, chunker_model_dat_file, _path_to_params)\n        parser.add_pipe(NPAnnotator.load(_path_to_model, _path_to_params), last=True)\n    else:\n        parser = SpacyInstance(model=""en_core_web_sm"", disable=[""textcat"", ""ner""]).parser\n    logger.info(""spacy loaded"")\n    return parser\n\n\ndef extract_noun_phrases(docs, nlp_parser, chunker):\n    logger.info(""extract nps from: %s"", docs)\n    spans = []\n    for doc in nlp_parser.pipe(docs, n_threads=-1):\n        if ""nlp_arch"" in chunker:\n            spans.extend(get_noun_phrases(doc))\n        else:\n            nps = list(doc.noun_chunks)\n            spans.extend(nps)\n    logger.info(""nps= %s"", str(spans))\n    return spans\n\n\n# pylint: disable-msg=too-many-nested-blocks,too-many-branches\ndef mark_noun_phrases(\n    corpus_file, marked_corpus_file, nlp_parser, lines_count, chunker, mark_char=""_"", grouping=False\n):\n    i = 0\n    with tqdm(total=lines_count) as pbar:\n        for doc in nlp_parser.pipe(corpus_file, n_threads=-1):\n            if ""nlp_arch"" in chunker:\n                spans = get_noun_phrases(doc)\n            else:\n                spans = list(doc.noun_chunks)\n            i += 1\n            if len(spans) > 0:\n                span = spans.pop(0)\n            else:\n                span = None\n            span_written = False\n            for token in doc:\n                if span is None:\n                    if len(token.text.strip()) > 0:\n                        marked_corpus_file.write(token.text + "" "")\n                else:\n                    if token.idx < span.start_char or token.idx >= span.end_char:  # outside a\n                        # span\n                        if len(token.text.strip()) > 0:\n                            marked_corpus_file.write(token.text + "" "")\n                    else:\n                        if not span_written:\n                            # mark NP\'s\n                            if len(span.text) > 1 and span.lemma_ != ""-PRON-"":\n                                if grouping:\n                                    text = get_group_norm(span)\n                                else:\n                                    text = span.text\n                                # mark NP\'s\n                                text = text.replace("" "", mark_char) + mark_char\n                                marked_corpus_file.write(text + "" "")\n                            else:\n                                marked_corpus_file.write(span.text + "" "")\n                            span_written = True\n                        if token.idx + len(token.text) == span.end_char:\n                            if len(spans) > 0:\n                                span = spans.pop(0)\n                            else:\n                                span = None\n                            span_written = False\n            marked_corpus_file.write(""\\n"")\n            pbar.update(1)\n\n\ndef merge_groups(np, old_id, diff_id):\n    if diff_id in id2group:\n        for term in id2group[diff_id]:  # for each term update dicts\n            np2id[term] = old_id\n            id2group[old_id].append(term)\n            if np2count[term] > np2count[id2rep[old_id]]:\n                id2rep[old_id] = term\n        id2rep.pop(diff_id)\n        id2group.pop(diff_id)\n    else:\n        if np2count[np] > np2count[id2rep[old_id]]:\n            id2rep[old_id] = np\n    return old_id\n\n\nif __name__ == ""__main__"":\n    arg_parser = ArgumentParser(__doc__)\n    arg_parser.add_argument(\n        ""--corpus"",\n        help=""path to the input corpus. Compressed files (gz) are also supported. By default, ""\n        ""it is a subset of English Wikipedia. ""\n        ""get subset of English wikipedia from ""\n        ""https://github.com/NervanaSystems/nlp-architect/raw/""\n        ""master/datasets/wikipedia/enwiki-20171201_subset.txt.gz"",\n    )\n    arg_parser.add_argument(\n        ""--marked_corpus"",\n        default=""enwiki-20171201_subset_marked.txt"",\n        type=validate_parent_exists,\n        help=""path to the marked corpus corpus."",\n    )\n    arg_parser.add_argument(\n        ""--mark_char"",\n        default=""_"",\n        type=str,\n        action=check_size(1, 2),\n        help=""special character that marks NP\'s in the corpus (word separator and NP suffix). ""\n        ""Default value is _."",\n    )\n    arg_parser.add_argument(\n        ""--grouping"", action=""store_true"", default=False, help=""perform noun-phrase grouping""\n    )\n    arg_parser.add_argument(\n        ""--chunker"",\n        type=str,\n        choices=[""spacy"", ""nlp_arch""],\n        default=""spacy"",\n        help=""chunker to use for detecting noun phrases. \'spacy\' for using spacy built-in ""\n        ""chunker or \'nlp_arch\' for NLP Architect NP Extractor"",\n    )\n\n    args = arg_parser.parse_args()\n    if args.corpus.endswith(""gz""):\n        open_func = gzip.open\n        mode = ""rt""\n    else:\n        open_func = open\n        mode = ""r""\n\n    with open_func(args.corpus, mode, encoding=""utf8"", errors=""ignore"") as my_corpus_file:\n        with open(args.marked_corpus, ""w"", encoding=""utf8"") as my_marked_corpus_file:\n            nlp = load_parser(args.chunker)\n            num_lines = sum(1 for line in my_corpus_file)\n            my_corpus_file.seek(0)\n            logger.info(""%i lines in corpus"", num_lines)\n            mark_noun_phrases(\n                my_corpus_file,\n                my_marked_corpus_file,\n                nlp,\n                num_lines,\n                mark_char=args.mark_char,\n                grouping=args.grouping,\n                chunker=args.chunker,\n            )\n\n        # write grouping data :\n        if args.grouping:\n            corpus_dir = path.dirname(args.marked_corpus)\n            with open(path.join(corpus_dir, ""id2group""), ""w"", encoding=""utf8"") as id2group_file:\n                id2group_file.write(json.dumps(id2group))\n\n            with open(path.join(corpus_dir, ""id2rep""), ""w"", encoding=""utf8"") as id2rep_file:\n                id2rep_file.write(json.dumps(id2rep))\n\n            with open(path.join(corpus_dir, ""np2id""), ""w"", encoding=""utf8"") as np2id_file:\n                np2id_file.write(json.dumps(np2id))\n'"
solutions/set_expansion/set_expand.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nimport sys\nfrom argparse import ArgumentParser\nfrom os import path\n\nfrom nlp_architect.models.np2vec import NP2vec\nfrom nlp_architect.utils.io import validate_existing_filepath, check_size, load_json_file\n\nlogger = logging.getLogger(__name__)\n\n\nclass SetExpand(object):\n    """"""\n        Set expansion module, given a trained np2vec model.\n    """"""\n\n    def __init__(\n        self,\n        np2vec_model_file,\n        binary=False,\n        word_ngrams=False,\n        grouping=False,\n        light_grouping=False,\n        grouping_map_dir=None,\n    ):\n        """"""\n        Load the np2vec model for set expansion.\n\n        Args:\n            np2vec_model_file (str): the file containing the np2vec model to load\n            binary (bool): boolean indicating whether the np2vec model to load is in binary format\n            word_ngrams (int {1,0}): If 1, np2vec model to load uses word vectors with subword (\n            ngrams) information.\n            light_grouping (bool): boolean indicating whether to load all maps for grouping.\n            grouping_map_dir (str): path to the directory containing maps for grouping.\n        Returns:\n            np2vec model to load\n        """"""\n        self.grouping = grouping\n        if grouping:\n            # load grouping info\n            logger.info(""loading grouping data"")\n            if not grouping_map_dir:\n                grouping_map_dir = path.dirname(np2vec_model_file)\n            self.np2id = load_json_file(path.join(grouping_map_dir, ""np2id""))\n            if not light_grouping:\n                self.id2rep = load_json_file(path.join(grouping_map_dir, ""id2rep""))\n                self.id2group = load_json_file(path.join(grouping_map_dir, ""id2group""))\n        logger.info(""loadind model..."")\n        self.np2vec_model = NP2vec.load(np2vec_model_file, binary=binary, word_ngrams=word_ngrams)\n        # extract the first term of the model in order to get the marking character\n        logger.info(""compute L2 norm"")\n        first_term = next(iter(self.np2vec_model.vocab.keys()))\n        self.mark_char = first_term[-1]\n        # Precompute L2-normalized vectors.\n        self.np2vec_model.init_sims()\n        logger.info(""done init"")\n\n    def term2id(self, term, suffix=True):\n        """"""\n        Given an term, return its id.\n\n        Args:\n            term(str): term (noun phrase)\n\n        Returns:\n            its id (if is part of the model)\n        """"""\n        if self.grouping:\n            if term not in self.np2id.keys():\n                return None\n            term = self.np2id[term]\n        id = term.replace("" "", self.mark_char)\n        if suffix:\n            id += self.mark_char\n        if id not in self.np2vec_model.vocab:\n            return None\n        return id\n\n    def __id2term(self, id):\n        """"""\n        Given the id of a noun phrase, return the noun phrase string.\n\n        Args:\n            id(str): id\n\n         Returns:\n            term (noun phrase)\n        """"""\n        norm = id.replace(self.mark_char, "" "")[:-1]\n        if self.grouping:\n            if norm in self.id2rep:\n                return self.id2rep[norm]\n            logger.warning(""id:#%s#, norm:#%s# is not in id2rep"")\n            return """"\n        return norm\n\n    def get_vocab(self):\n        """"""\n        Return the vocabulary as the list of terms.\n\n        Returns:\n            the list of terms.\n        """"""\n        vocab = []\n        for id in self.np2vec_model.vocab:\n            term = self.__id2term(id)\n            if term is not None:\n                vocab.append(term)\n        return vocab\n\n    def in_vocab(self, term):\n        id = self.term2id(term)\n        if id is None:\n            return False\n        return True\n\n    def get_group(self, term):\n        logger.info(""get group of: %s"", term)\n        group = []\n        if term in self.np2id:\n            id = self.np2id[term]\n            group = self.id2group[id]\n        return group\n\n    def similarity(self, terms, seed, threshold):\n\n        similar = []\n        seed_id = self.get_seed_id(seed)\n        for term in terms:\n            term_id = self.term2id(term)\n            if term_id is not None:\n                if self.seed2term_similarity(seed_id, [term_id]) > threshold:\n                    similar.append(term)\n            else:\n                logger.info(""term: %s is not in vocab"", term)\n        return similar\n\n    # pylint: disable-msg=too-many-branches\n    def expand(self, seed, topn=500):\n        """"""\n        Given a seed of terms, return the expanded set of terms.\n\n        Args:\n            seed: seed terms\n            topn: maximal number of expanded terms to return\n\n        Returns:\n            up to topn expanded terms and their probabilities\n        """"""\n        seed_ids = list()\n        upper = True\n        lower = True\n        for np in seed:\n            np = np.strip()\n            if not self.grouping and (upper or lower):\n                # case feature is relevant only if we don\'t have grouping\n                if np[0].islower():\n                    upper = False\n                else:\n                    lower = False\n            id = self.term2id(np)\n            if id is not None:\n                seed_ids.append(id)\n            else:\n                logger.warning(""The term: \'%s\' is out-of-vocabulary."", np)\n        if len(seed_ids) > 0:\n            if not self.grouping and (upper or lower):\n                res_id = self.np2vec_model.most_similar(seed_ids, topn=2 * topn)\n            else:\n                res_id = self.np2vec_model.most_similar(seed_ids, topn=topn)\n            res = list()\n            for r in res_id:\n                if len(res) == topn:\n                    break\n                # pylint: disable=R0916\n                if (\n                    self.grouping\n                    or (not lower and not upper)\n                    or (upper and r[0][0].isupper())\n                    or (lower and r[0][0].islower())\n                ):\n                    term = self.__id2term(r[0])\n                    if term is not None:\n                        res.append((self.__id2term(r[0]), r[1]))\n            ret_val = res\n        else:\n            logger.info(""All the seed terms are out-of-vocabulary."")\n            ret_val = None\n        return ret_val\n\n    def get_seed_id(self, seed):\n        seed_ids = list()\n        for np in seed:\n            np = np.strip()\n            id = self.term2id(np)\n            if id is not None:\n                seed_ids.append(id)\n            else:\n                logger.warning(""The term: \'%s\' is out-of-vocabulary."", np)\n        return seed_ids\n\n    def term2term_similarity(self, term_id_1, term_id_2):\n        """"""\n        Compute cosine similarity between two term id\'s.\n        Args:\n            term_id_1: first term id\n            term_id_2: second term id\n\n        Returns:\n            Similarity between the first and second term id\'s\n\n        """"""\n        logger.info(""calculate similarity for: %s , %s"", term_id_1, term_id_2)\n        res = self.np2vec_model.similarity(term_id_1, term_id_2)\n        logger.info(""similarity result: %s"", str(res))\n        return res\n\n    def seed2term_similarity(self, seed_id, term_id):\n        """"""\n        Compute cosine similarity between a seed terms and a term.\n        Args:\n            seed_id: seed term id\'s\n            term_id: the term id\n\n        Returns:\n            Similarity between the seed terms and the term\n\n        """"""\n        logger.info(""calculate similarity for: %s , %s"", str(seed_id), term_id)\n        res = self.np2vec_model.n_similarity(seed_id, list(term_id))\n        logger.info(""similarity result: %s"", str(res))\n        return res\n\n\nif __name__ == ""__main__"":\n    arg_parser = ArgumentParser(__doc__)\n    arg_parser.add_argument(\n        ""--np2vec_model_file"",\n        help=""path to the file with the np2vec model to load."",\n        type=validate_existing_filepath,\n    )\n    arg_parser.add_argument(\n        ""--binary"",\n        help=""boolean indicating whether the model to load has been stored in binary format."",\n        action=""store_true"",\n    )\n    arg_parser.add_argument(\n        ""--word_ngrams"",\n        default=0,\n        type=int,\n        choices=[0, 1],\n        help=""If 0, the model to load stores word information. If 1, the model to load stores ""\n        ""subword (ngrams) information; note that subword information is relevant only to ""\n        ""fasttext models."",\n    )\n    arg_parser.add_argument(\n        ""--topn"",\n        default=500,\n        type=int,\n        action=check_size(min_size=1),\n        help=""maximal number of expanded terms to return"",\n    )\n    arg_parser.add_argument(""--grouping"", action=""store_true"", default=False, help=""grouping mode"")\n\n    args = arg_parser.parse_args()\n\n    se = SetExpand(\n        np2vec_model_file=args.np2vec_model_file,\n        binary=args.binary,\n        word_ngrams=args.word_ngrams,\n        grouping=args.grouping,\n    )\n    enter_seed_str = ""Enter the seed (comma-separated seed terms):""\n    logger.info(enter_seed_str)\n    for seed_str in sys.stdin:\n        seed_list = seed_str.strip().split("","")\n        exp = se.expand(seed_list, args.topn)\n        logger.info(""Expanded results:"")\n        logger.info(exp)\n        logger.info(enter_seed_str)\n'"
solutions/trend_analysis/__init__.py,0,b''
solutions/trend_analysis/np_scorer.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nfrom os import path, makedirs\n\nfrom tqdm import tqdm\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.pipelines.spacy_np_annotator import NPAnnotator, get_noun_phrases\nfrom nlp_architect.utils.io import download_unlicensed_file\nfrom nlp_architect.utils.text import SpacyInstance\nfrom .scoring_utils import TextSpanScoring\n\nnlp_chunker_url = ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/models/chunker/""\nchunker_model_dat_file = ""model_info.dat.params""\nchunker_model_file = ""model.h5""\nchunker_local_path = str(LIBRARY_OUT / ""chunker-pretrained"")\nlogger = logging.getLogger(__name__)\n\n\nclass NPScorer(object):\n    def __init__(self, parser=None):\n        if parser is None:\n            self.nlp = SpacyInstance(disable=[""ner"", ""parser"", ""vectors"", ""textcat""]).parser\n        else:\n            self.nlp = parser\n\n        self.nlp.add_pipe(self.nlp.create_pipe(""sentencizer""), first=True)\n        _path_to_model = path.join(chunker_local_path, chunker_model_file)\n        if not path.exists(chunker_local_path):\n            makedirs(chunker_local_path)\n        if not path.exists(_path_to_model):\n            logger.info(\n                ""The pre-trained model to be downloaded for NLP Architect word""\n                "" chunker model is licensed under Apache 2.0""\n            )\n            download_unlicensed_file(nlp_chunker_url, chunker_model_file, _path_to_model)\n        _path_to_params = path.join(chunker_local_path, chunker_model_dat_file)\n        if not path.exists(_path_to_params):\n            download_unlicensed_file(nlp_chunker_url, chunker_model_dat_file, _path_to_params)\n        self.nlp.add_pipe(NPAnnotator.load(_path_to_model, _path_to_params), last=True)\n\n    def score_documents(self, texts: list, limit=-1, return_all=False, min_tf=5):\n        documents = []\n        assert len(texts) > 0, ""texts should contain at least 1 document""\n        assert min_tf > 0, ""min_tf should be at least 1""\n        with tqdm(total=len(texts), desc=""documents scoring progress"", unit=""docs"") as pbar:\n            for doc in self.nlp.pipe(texts, n_threads=-1):\n                if len(doc) > 0:\n                    documents.append(doc)\n                pbar.update(1)\n\n        corpus = []\n        for doc in documents:\n            spans = get_noun_phrases(doc)\n            if len(spans) > 0:\n                corpus.append((doc, spans))\n\n        if len(corpus) < 1:\n            return []\n\n        documents, doc_phrases = list(zip(*corpus))\n        scorer = TextSpanScoring(documents=documents, spans=doc_phrases, min_tf=min_tf)\n        tfidf_scored_list = scorer.get_tfidf_scores()\n        if len(tfidf_scored_list) < 1:\n            return []\n        cvalue_scored_list = scorer.get_cvalue_scores()\n        freq_scored_list = scorer.get_freq_scores()\n\n        if limit > 0:\n            tf = {tuple(k[0]): k[1] for k in tfidf_scored_list}\n            cv = {tuple(k[0]): k[1] for k in cvalue_scored_list}\n            fr = {tuple(k[0]): k[1] for k in freq_scored_list}\n            tfidf_scored_list_limit = []\n            cvalue_scored_list_limit = []\n            freq_scored_list_limit = []\n            for phrase in list(zip(*tfidf_scored_list))[0][:limit]:\n                tfidf_scored_list_limit.append((phrase, tf[tuple(phrase)]))\n                cvalue_scored_list_limit.append((phrase, cv[tuple(phrase)]))\n                freq_scored_list_limit.append((phrase, fr[tuple(phrase)]))\n            tfidf_scored_list = tfidf_scored_list_limit\n            cvalue_scored_list = cvalue_scored_list_limit\n            freq_scored_list = freq_scored_list_limit\n\n        tfidf_scored_list = scorer.normalize_l2(tfidf_scored_list)\n        cvalue_scored_list = scorer.normalize_l2(cvalue_scored_list)\n        freq_scored_list = scorer.normalize_minmax(freq_scored_list, invert=True)\n        tfidf_scored_list = scorer.normalize_minmax(tfidf_scored_list)\n        cvalue_scored_list = scorer.normalize_minmax(cvalue_scored_list)\n        if return_all:\n            tf = {tuple(k[0]): k[1] for k in tfidf_scored_list}\n            cv = {tuple(k[0]): k[1] for k in cvalue_scored_list}\n            fr = {tuple(k[0]): k[1] for k in freq_scored_list}\n            final_list = []\n            for phrases in tf.keys():\n                final_list.append(([p for p in phrases], tf[phrases], cv[phrases], fr[phrases]))\n            return final_list\n        merged_list = scorer.interpolate_scores([tfidf_scored_list, cvalue_scored_list], [0.5, 0.5])\n        merged_list = scorer.multiply_scores([merged_list, freq_scored_list])\n        merged_list = scorer.normalize_minmax(merged_list)\n        final_list = []\n        for phrases, score in merged_list:\n            if any([len(p) > 1 for p in phrases]):\n                final_list.append(([p for p in phrases], score))\n        return final_list\n'"
solutions/trend_analysis/scoring_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=no-name-in-module\nimport itertools\nimport math\n\nimport numpy as np\nimport logging\nfrom spacy.tokens.doc import Doc\nfrom spacy.tokens.span import Span\nfrom spacy.tokens.token import Token\n\nlogger = logging.getLogger(__name__)\n\n\n# try:\n#     from wordfreq import zipf_frequency\n\n#     _has_wordfreq = True\n\n# except (AttributeError, ImportError):\n#     logger.warn(""wordfreq package is required to be installed before using this module. "" +\n#                 ""Install requirements.txt from solution directory"")\n#     _has_wordfreq = False\n\n# if _has_wordfreq:\n\n\nclass TextSpanScoring:\n    """"""\n    Text spans scoring class.\n    Contains misc scoring algorithms for scoring text fragments extracted\n    from a corpus.\n\n    Arguments:\n        documents(list): List of spaCy documents.\n        spans(list[list]): List of spaCy spans representing noun phrases of documents\n            document.\n    """"""\n\n    def __init__(self, documents, spans, min_tf=1):\n        assert len(documents) == len(spans)\n        self._documents = documents\n        self._doc_text_spans = spans\n        self.index = CorpusIndex(documents, spans)\n        assert min_tf > 0, ""min_tf must be > 0""\n        if min_tf > 1:\n            self.re_index_min_tf(min_tf)\n\n    def re_index_min_tf(self, tf):\n        filtered_doc_text_spans = []\n        for d in self.doc_text_spans:\n            filtered_doc_phrases = [p for p in d if self.index.tf(p) >= tf]\n            filtered_doc_text_spans.append(filtered_doc_phrases)\n        self._doc_text_spans = filtered_doc_text_spans\n        self.index = CorpusIndex(self.documents, self.doc_text_spans)\n\n    @property\n    def documents(self):\n        return self._documents\n\n    @property\n    def doc_text_spans(self):\n        return self._doc_text_spans\n\n    def get_tfidf_scores(self, group_similar_spans=True):\n        """"""\n        Get TF-IDF scores of spans\n        return a list of spans sorted by desc order of importance\n        span score = TF (global) * (1 + log_n(DF/N))\n        """"""\n        phrases_and_scores = {}\n        num_of_docs = len(self.documents)\n        for _, noun_phrases in zip(self.documents, self.doc_text_spans):\n            for p in noun_phrases:\n                if p not in phrases_and_scores:\n                    tf = self.index.tf(p)\n                    df = self.index.df(p)\n                    phrases_and_scores[p] = (tf + 1) * math.log(1 + num_of_docs / df)\n        if len(phrases_and_scores) > 0:\n            return self._maybe_group_and_sort(group_similar_spans, phrases_and_scores)\n        return []\n\n    def get_freq_scores(self, group_similar_spans=True):\n        try:\n            from wordfreq import zipf_frequency\n\n            _has_wordfreq = True\n        except (AttributeError, ImportError):\n            logger.error(\n                ""wordfreq is not installed, please install nlp_architect with [all] package. ""\n                + ""for example: pip install nlp_architect[all]""\n            )\n            _has_wordfreq = False\n\n        if _has_wordfreq:\n            phrases_and_scores = {}\n            for _, noun_phrases in zip(self.documents, self.doc_text_spans):\n                for p in noun_phrases:\n                    if p not in phrases_and_scores:\n                        phrases_and_scores[p] = zipf_frequency(p.text, ""en"")\n            return self._maybe_group_and_sort(group_similar_spans, phrases_and_scores)\n        return None\n\n    def group_spans(self, phrases):\n        pid_phrase_scores = [{""k"": self.index.get_pid(p), ""v"": (p, s)} for p, s in phrases.items()]\n        phrase_groups = []\n        for _, group in itertools.groupby(\n            sorted(pid_phrase_scores, key=lambda x: x[""k""]), lambda x: x[""k""]\n        ):\n            _group = list(group)\n            phrases = {g[""v""][0].text for g in _group}\n            score = _group[0][""v""][1]\n            phrase_groups.append((sorted(phrases), score))\n        return phrase_groups\n\n    def _maybe_group_and_sort(self, is_group, phrases_dict):\n        phrase_groups = phrases_dict.items()\n        if is_group:\n            phrase_groups = self.group_spans(phrases_dict)\n        return sorted(phrase_groups, key=lambda x: x[1], reverse=True)\n\n    @staticmethod\n    def normalize_minmax(phrases_list, invert=False):\n        _, scores = list(zip(*phrases_list))\n        max_score = max(scores)\n        min_score = min(scores)\n        norm_list = []\n        for p, s in phrases_list:\n            if max_score - min_score > 0:\n                new_score = (s - min_score) / (max_score - min_score)\n            else:\n                new_score = 0\n            norm_list.append([p, new_score])\n        if invert:\n            for e in norm_list:\n                e[1] = 1.0 - e[1]\n        return norm_list\n\n    @staticmethod\n    def normalize_l2(phrases_list):\n        phrases, scores = list(zip(*phrases_list))\n        scores = scores / np.linalg.norm(scores)\n        return list(zip(phrases, scores.tolist()))\n\n    @staticmethod\n    def interpolate_scores(phrase_lists, weights=None):\n        if weights is None:\n            weights = [1.0 / len(phrase_lists)]\n        else:\n            assert len(weights) == len(phrase_lists)\n\n        list_sizes = [len(lst) for lst in phrase_lists]\n        for lsize in list_sizes:\n            assert len(phrase_lists[0]) == lsize, ""list sizes not equal""\n\n        phrase_list_dicts = []\n        for lst in phrase_lists:\n            phrase_list_dicts.append({tuple(k): v for k, v in lst})\n\n        phrases = phrase_list_dicts[0].keys()\n        interp_scores = {}\n        for p in phrases:\n            interp_scores[p] = 0.0\n            for ref, w in zip(phrase_list_dicts, weights):\n                interp_scores[p] += ref[p] * w\n        return sorted(interp_scores.items(), key=lambda x: x[1], reverse=True)\n\n    def get_cvalue_scores(self, group_similar_spans=True):\n        phrases_text = [p for ps in self.index.pid_to_spans.values() for p in ps]\n        phrase_scores = {}\n        for phrase in phrases_text:\n            sub_phrase_pids = None\n            for w in phrase:\n                sub_pids = self.index.get_subphrases_of_word(w)\n                if sub_phrase_pids is None:\n                    sub_phrase_pids = sub_pids\n                else:\n                    sub_phrase_pids = sub_phrase_pids.intersection(sub_pids)\n            sub_phrase_pids.discard(self.index.get_pid(phrase))\n\n            if len(sub_phrase_pids) > 0:\n                score = math.log2(1 + len(phrase)) * (\n                    self.index.tf(phrase)\n                    - 1.0\n                    / len(sub_phrase_pids)\n                    * sum(\n                        [self.index.tf(list(self.index.get_phrase(p))[0]) for p in sub_phrase_pids]\n                    )\n                )\n            else:\n                score = math.log2(1 + len(phrase)) * self.index.tf(phrase)\n            phrase_scores[phrase] = score\n        for phrase in phrase_scores:\n            score = [phrase_scores[p] for p in self.index.get_phrase(self.index.get_pid(phrase))]\n            phrase_scores[phrase] = sum(score) / len(score)\n        return self._maybe_group_and_sort(group_similar_spans, phrase_scores)\n\n    @staticmethod\n    def multiply_scores(phrase_lists):\n        phrase_list_dicts = []\n        for lst in phrase_lists:\n            phrase_list_dicts.append({tuple(k): v for k, v in lst})\n        phrases = phrase_list_dicts[0].keys()\n        interp_scores = {}\n        for p in phrases:\n            interp_scores[p] = 1.0\n            for ref in phrase_list_dicts:\n                interp_scores[p] *= ref[p]\n        return sorted(interp_scores.items(), key=lambda x: x[1], reverse=True)\n\n\nclass CorpusIndex:\n    """"""\n    Text span index class.\n    Holds TF and DF values per span. Text spans are normalized and similar\n    spans are mapped to the same TF DF values.\n    """"""\n\n    def __init__(self, documents: list, spans: list):\n        self.df_index = {}  # span to DF\n        self.tf_index = {}  # span, doc to TF\n        self.pid_to_spans = {}  # id to spans (normalized types)\n        self.word_to_phrase_index = {}\n        self.documents = set()\n        for d, phrases in zip(documents, spans):\n            doc_id = CorpusIndex.get_docid(d)\n            self.documents.add(doc_id)\n            for phrase in phrases:\n                pid = CorpusIndex.get_pid(phrase)\n                # add to norm phrases dict\n                if pid not in self.pid_to_spans:\n                    self.pid_to_spans[pid] = set()\n                self.pid_to_spans[pid].add(phrase)\n\n                # add to df index\n                if self.df_index.get(pid, None) is None:\n                    self.df_index[pid] = {doc_id}\n                else:\n                    self.df_index[pid].add(doc_id)\n\n                # add to tf index\n                if self.tf_index.get(pid, None) is None:\n                    self.tf_index[pid] = 1\n                else:\n                    self.tf_index[pid] += 1\n\n                for word in phrase:\n                    wid = self.get_wid(word)\n                    if wid not in self.word_to_phrase_index:\n                        self.word_to_phrase_index[wid] = set()\n                    self.word_to_phrase_index[wid].add(pid)\n\n    def get_phrase(self, pid):\n        return self.pid_to_spans.get(pid)\n\n    def get_subphrases_of_word(self, w):\n        wid = self.get_wid(w)\n        return self.word_to_phrase_index.get(wid)\n\n    @staticmethod\n    def get_wid(w: Token):\n        """"""\n        get word id\n        """"""\n        return CorpusIndex.hash_func(w.text)\n\n    @staticmethod\n    def get_pid(p: Span):\n        """"""\n        get phrase id\n        """"""\n        return CorpusIndex.hash_func(p.lemma_)\n\n    @staticmethod\n    def get_docid(d: Doc):\n        """"""\n        get doc id\n        """"""\n        return CorpusIndex.hash_func(d)\n\n    @staticmethod\n    def hash_func(x):\n        return hash(x)\n\n    def tf(self, phrase):\n        """"""\n        Get TF of phrase in doc\n        """"""\n        pid = CorpusIndex.get_pid(phrase)\n        if self.tf_index.get(pid, None) is not None:\n            return self.tf_index.get(pid)\n        return 0\n\n    def df(self, phrase):\n        """"""\n        Get DF of phrase in corpus\n        """"""\n        pid = CorpusIndex.get_pid(phrase)\n        if self.df_index.get(pid, None) is not None:\n            return len(self.df_index[pid])\n        return 0\n'"
solutions/trend_analysis/test_np_scorer.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom nlp_architect.utils.io import load_files_from_path\nfrom .np_scorer import NPScorer\n\n\ndef test_np_scorer():\n    doc1 = (\n        ""It is based on representing any term of a training corpus using word embeddings in ""\n        ""order to estimate the similarity between the seed terms and any candidate term.""\n        "" Noun phrases provide good approximation for candidate terms and are extracted in""\n        ""our system using a noun phrase chunker. At expansion time, given a seed of terms, ""\n        ""the most similar terms are returned.""\n    )\n    doc2 = (\n        ""The expand server gets requests containing seed terms, and expands them based on""\n        "" the given word embedding model. You can use the model you trained yourself in the ""\n        ""previous step, or to provide a pre-trained model you own. Important note: ""\n        ""default server will listen on localhost:1234. If you set the host/port you ""\n        ""should also set it in the ui/settings.py file.""\n    )\n    docs = [doc1, doc2]\n    np = NPScorer()\n    phrase_scores = np.score_documents(docs, limit=3, return_all=True, min_tf=1)\n    assert len(phrase_scores) == 3\n    phrase_scores = np.score_documents(docs, return_all=True, min_tf=100)\n    assert len(phrase_scores) == 0\n\n\ndef test_extract_and_score(file_path=None):\n    if file_path:\n        documents = load_files_from_path(file_path)\n        np = NPScorer()\n        phrases = np.score_documents(documents, return_all=True, min_tf=1)\n        assert phrases\n'"
solutions/trend_analysis/test_trend_analysis.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=redefined-outer-name\nimport os\nfrom os import path\n\nimport pytest\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.utils.text import try_to_load_spacy\nfrom .topic_extraction import main as topic_ex_main\nfrom .trend_analysis import analyze as trend_analyze\n\npytestmark = pytest.mark.skipif(True, reason=""Test fails on Linux"")\n\nif not try_to_load_spacy(""en""):\n    pytest.skip(\n        ""\\n\\nSkipping test_spacy_np_annotator.py. Reason: \'spacy en\' model not installed.""\n        ""Please see https://spacy.io/models/ for installation instructions.\\n""\n        ""The terms and conditions of the data set and/or model license apply.\\n""\n        ""Intel does not grant any rights to the data and/or model files.\\n"",\n        allow_module_level=True,\n    )\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\nta_path = str(LIBRARY_OUT / ""trend-analysis-data"")\ntarget_corpus_path = path.join(ta_path, ""target_corpus.csv"")\nreference_corpus_path = path.join(ta_path, ""reference_corpus.csv"")\n\n\n@pytest.fixture\ndef input_data_path():\n    return os.path.join(current_dir, ""fixtures/data/trend_analysis"")\n\n\n@pytest.fixture\ndef output_folder_path():\n    return ta_path\n\n\n@pytest.fixture\ndef filter_data_path(output_folder_path):\n    return os.path.join(output_folder_path, ""filter_phrases.csv"")\n\n\n@pytest.fixture\ndef graph_data_path(output_folder_path):\n    return os.path.join(output_folder_path, ""graph_data.csv"")\n\n\n@pytest.fixture\ndef model_folder_path(output_folder_path):\n    return os.path.join(output_folder_path, ""W2V_Models"")\n\n\n@pytest.fixture\ndef unified_corpus_path(output_folder_path):\n    return os.path.join(output_folder_path, ""corpus.txt"")\n\n\ndef test_topic_extraction(input_data_path, output_folder_path):\n    tar_corpus_path = os.path.join(input_data_path, ""target_corpus"")\n    ref_corpus_path = os.path.join(input_data_path, ""reference_corpus"")\n    topic_ex_main(tar_corpus_path, ref_corpus_path, single_thread=True, no_train=False, url=False)\n    assert os.path.isfile(path.join(ta_path, ""reference_corpus.csv""))\n    assert os.path.isfile(path.join(ta_path, ""target_corpus.csv""))\n    assert os.path.isfile(os.path.join(output_folder_path, ""W2V_Models/model.bin""))\n\n\ndef test_trend_analysis(filter_data_path, graph_data_path):\n    target_corpus_path = path.join(ta_path, ""target_corpus.csv"")\n    reference_corpus_path = path.join(ta_path, ""reference_corpus.csv"")\n    trend_analyze(\n        target_corpus_path, reference_corpus_path, target_corpus_path, reference_corpus_path,\n    )\n    assert os.path.isfile(filter_data_path)\n    assert os.path.isfile(graph_data_path)\n'"
solutions/trend_analysis/topic_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\nimport csv\nimport logging\nimport os\nimport sys\nfrom multiprocessing import Pool\nfrom os import makedirs, path\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.utils.embedding import FasttextEmbeddingsModel\nfrom nlp_architect.utils.io import validate_existing_directory\nfrom nlp_architect.utils.text import SpacyInstance\n\nfrom .np_scorer import NPScorer\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    from newspaper import Article\nexcept (AttributeError, ImportError):\n    logger.error(\n        ""newspaper3k is not installed, please install nlp_architect with [all] package. ""\n        + ""for example: pip install nlp_architect[all]""\n    )\n    sys.exit()\n\n\nlogger = logging.getLogger(__name__)\ndata_dir = str(LIBRARY_OUT / ""trend-analysis-data"")\n\n\ndef noun_phrase_extraction(docs, parser):\n    """"""\n    Extract noun-phrases from a textual corpus\n\n    Args:\n        docs (List[String]): A list of documents\n        parser (SpacyInstance): Spacy NLP parser\n    Returns:\n        List of topics with their tf_idf, c_value, language-model scores\n    """"""\n    logger.info(""extracting NPs"")\n    np_app = NPScorer(parser=parser)\n    np_result = np_app.score_documents(docs, return_all=True)\n    logger.info(""got NP extractor result. %s phrases were extracted"", str(len(np_result)))\n    return np_result\n\n\ndef load_text_from_folder(folder):\n    """"""\n    Load files content into a list of docs (texts)\n\n    Args:\n        folder: A path to a folder containing text files\n\n    Returns:\n        A list of documents (List[String])\n    """"""\n    files_content = []\n    try:\n        for filename in os.listdir(folder):\n            with open(folder + os.sep + filename, ""rb"") as f:\n                try:\n                    files_content.append(\n                        ((f.read()).decode(""UTF-8"", errors=""replace""))\n                        .replace(""\\n"", "" "")\n                        .replace(""\\r"", "" "")\n                        .replace(""\\b"", "" "")\n                    )\n                except Exception as e:\n                    logger.error(str(e))\n    except Exception as e:\n        logger.error(""Error in load_text: %s"", str(e))\n\n    return files_content\n\n\ndef load_url_content(url_list):\n    """"""\n    Load articles content into a list of docs (texts)\n\n    Args:\n        url_list (List[String]): A list of urls\n\n    Returns:\n        A list of documents (List[String])\n    """"""\n    files_content = []\n    url = """"\n    try:\n        for url in url_list:\n            try:\n                url = str(url)\n                logger.info(""loading %s"", url)\n                article = Article(url)\n                article.download()\n                article.parse()\n                files_content.append(article.title + "" "" + article.text)\n            except Exception as e:\n                logger.error(str(e))\n    except Exception as e:\n        logger.error(""Error in load_text: %s, for url: %s"", str(e), str(url))\n\n    return files_content\n\n\ndef save_scores(np_result, file_path):\n    """"""\n    Save the result of a topic extraction into a file\n\n    Args:\n        np_result: A list of topics with different score types (tfidf, cvalue, freq)\n        file_path: The output file path\n    """"""\n    logger.info(""saving multi-scores np extraction results to: %s"", file_path)\n    with open(file_path, ""wt"", encoding=""utf-8"", newline="""") as csv_file:\n        writer = csv.writer(csv_file, delimiter="","")\n        ctr = 0\n        for group, tfidf, cvalue, freq in np_result:\n            try:\n                group_str = """"\n                for p in group:\n                    group_str += p + "";""\n                row = (group_str[0:-1], str(tfidf), str(cvalue), str(freq))\n                writer.writerow(row)\n                ctr += 1\n            except Exception as e:\n                logger.error(\n                    ""Error while writing np results. iteration #: %s. Error: %s"", str(ctr), str(e)\n                )\n\n\ndef unify_corpora_from_texts(text_list_t, text_list_r):\n    """"""\n    Merge two corpora into a single text file\n\n    Args:\n        text_list_t: A list of documents - target corpus (List[String])\n        text_list_r: A list of documents - reference corpus (List[String])\n    Returns:\n        The path of the unified corpus\n    """"""\n    logger.info(""prepare data for w2v training"")\n    out_file = str(path.join(data_dir, ""corpus.txt""))\n    try:\n        with open(out_file, ""w+"", encoding=""utf-8"") as writer:\n            write_text_list_to_file(text_list_t, writer)\n            write_text_list_to_file(text_list_r, writer)\n    except Exception as e:\n        logger.error(""Error: %s"", str(e))\n    return out_file\n\n\ndef unify_corpora_from_folders(corpus_a, corpus_b):\n    """"""\n    Merge two corpora into a single text file\n\n    Args:\n        corpus_a: A folder containing text files (String)\n        corpus_b: A folder containing text files (String)\n    Returns:\n        The path of the unified corpus\n    """"""\n    logger.info(""prepare data for w2v training"")\n    out_file = str(path.join(data_dir, ""corpus.txt""))\n    try:\n        with open(out_file, ""w+"", encoding=""utf-8"") as writer:\n            write_folder_corpus_to_file(corpus_a, writer)\n            write_folder_corpus_to_file(corpus_b, writer)\n    except Exception as e:\n        logger.error(""Error: %s"", str(e))\n    return out_file\n\n\ndef write_folder_corpus_to_file(corpus, writer):\n    """"""\n    Merge content of a folder into a single text file\n\n    Args:\n        corpus: A folder containing text files (String)\n        writer: A file writer\n    """"""\n    for filename in os.listdir(corpus):\n        try:\n            file_path = str(path.join(corpus, filename))\n            with open(file_path, ""r"", encoding=""utf-8"") as f:\n                lines = f.readlines()\n                writer.writelines(lines)\n        except Exception as e:\n            logger.error(""Error: %s. skipping file: %s"", str(e), str(filename))\n\n\ndef write_text_list_to_file(text_list, writer):\n    """"""\n    Merge a list of texts into a single text file\n\n    Args:\n        text_list: A list of texts (List[String])\n        writer: A file writer\n    """"""\n    try:\n        for text in text_list:\n            writer.writelines(text)\n    except Exception as e:\n        logger.error(""Error: %s"", str(e))\n\n\ndef train_w2v_model(data):\n    """"""\n    Train a w2v (skipgram) model using fasttext package\n\n    Args:\n        data: A path to the training data (String)\n    """"""\n    with open(data, encoding=""utf-8"") as fp:\n        texts = [line.split() for line in fp.readlines()]\n    logger.info(""Fasttext embeddings training..."")\n    try:\n        model = FasttextEmbeddingsModel(size=100, min_count=1, skipgram=True)\n        model.train(texts, epochs=100)\n        model.save(str(path.join(data_dir, ""W2V_Models/model.bin"")))\n    except Exception as e:\n        logger.error(""Error: %s"", str(e))\n\n\ndef create_w2v_model(text_list_t, text_list_r):\n    """"""\n     Create a w2v model on the given corpora\n\n     Args:\n         text_list_t: A list of documents - target corpus (List[String])\n         text_list_r: A list of documents - reference corpus (List[String])\n     """"""\n    unified_data = unify_corpora_from_texts(text_list_t, text_list_r)\n    train_w2v_model(unified_data)\n\n\ndef get_urls_from_file(file):\n    """"""\n     Merge two corpora into a single text file\n\n     Args:\n         corpus_a: A folder containing text files (String)\n         corpus_b: A folder containing text files (String)\n     Returns:\n         The path of the unified corpus\n     """"""\n    with open(file) as url_file:\n        url_list = url_file.readlines()\n        url_list = [x.strip() for x in url_list]\n    return url_list\n\n\ndef initiate_parser():\n    return SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""]).parser\n\n\ndef main(corpus_t, corpus_r, single_thread, no_train, url):\n    try:\n        if not path.exists(data_dir):\n            makedirs(data_dir)\n        if not path.exists(path.join(data_dir, ""W2V_Models"")):\n            os.makedirs(path.join(data_dir, ""W2V_Models""))\n    except Exception:\n        logger.error(""failed to create output folder."")\n        sys.exit()\n    if url:\n        if not path.isfile(corpus_t) or not path.isfile(corpus_r):\n            logger.error(""Please provide a valid csv file with urls"")\n            sys.exit()\n        else:\n            text_t = load_url_content(get_urls_from_file(corpus_t))\n            text_r = load_url_content(get_urls_from_file(corpus_r))\n    else:\n        if not path.isdir(corpus_t) or not path.isdir(corpus_r):\n            logger.error(""Please provide valid directories for target_corpus and"" "" for ref_corpus"")\n            sys.exit()\n\n        else:\n            text_t = load_text_from_folder(corpus_t)\n            text_r = load_text_from_folder(corpus_r)\n\n    # extract noun phrases\n    nlp_parser_t = initiate_parser()\n    nlp_parser_r = initiate_parser()\n    if single_thread:\n        result_t = noun_phrase_extraction(text_t, nlp_parser_t)\n        result_r = noun_phrase_extraction(text_r, nlp_parser_r)\n    else:\n        with Pool(processes=2) as pool:\n            run_np_t = pool.apply_async(noun_phrase_extraction, [text_t, nlp_parser_t])\n            run_np_r = pool.apply_async(noun_phrase_extraction, [text_r, nlp_parser_r])\n            result_t = run_np_t.get()\n            result_r = run_np_r.get()\n\n    # save results to csv\n\n    # save_scores(result_t, os.path.splitext(os.path.basename(corpus_t))[0] + \'.csv\')\n    # save_scores(result_r, os.path.splitext(os.path.basename(corpus_r))[0] + \'.csv\')\n    save_scores(result_t, str(path.join(data_dir, os.path.basename(corpus_t))) + "".csv"")\n    save_scores(result_r, str(path.join(data_dir, os.path.basename(corpus_r))) + "".csv"")\n\n    # create w2v model\n    if not no_train:\n        create_w2v_model(text_t, text_r)\n\n\nif __name__ == ""__main__"":\n    argparser = argparse.ArgumentParser(prog=""topic_extraction.py"")\n    argparser.add_argument(\n        ""target_corpus"",\n        metavar=""target_corpus"",\n        type=validate_existing_directory,\n        help=""a path to a folder containing text files"",\n    )\n    argparser.add_argument(\n        ""ref_corpus"",\n        metavar=""ref_corpus"",\n        type=validate_existing_directory,\n        help=""a path to a folder containing text files"",\n    )\n    argparser.add_argument(""--no_train"", action=""store_true"", help=""skip the creation of w2v model"")\n    argparser.add_argument(\n        ""--url"", action=""store_true"", help=""corpus provided as csv file with urls""\n    )\n    argparser.add_argument(\n        ""--single_thread"", action=""store_true"", help=""analyse corpora sequentially""\n    )\n    args = argparser.parse_args()\n\n    main(args.target_corpus, args.ref_corpus, args.single_thread, args.no_train, args.url)\n'"
solutions/trend_analysis/trend_analysis.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport csv\nimport logging\nimport operator\nimport os\nimport sys\nfrom os import path\nfrom pathlib import Path\nfrom shutil import copyfile\n\nimport numpy\nimport pandas as pd\nfrom sklearn.manifold import TSNE\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.utils.embedding import FasttextEmbeddingsModel\nfrom nlp_architect.utils.io import check_size, validate_existing_filepath\nfrom nlp_architect.utils.text import simple_normalizer\n\ndirectory = str(LIBRARY_OUT / ""trend-analysis-data"")\nlogger = logging.getLogger(__name__)\ntarget_topics_path = path.join(directory, ""target_topics.csv"")\nref_topics_path = path.join(directory, ""ref_topics.csv"")\ntarget_scores_path = path.join(directory, ""target_scores.csv"")\nref_scores_path = path.join(directory, ""ref_scores.csv"")\n\n\ndef analyze(\n    target_data,\n    ref_data,\n    tar_header,\n    ref_header,\n    top_n=10000,\n    top_n_vectors=500,\n    re_analysis=False,\n    tfidf_w=0.5,\n    cval_w=0.5,\n    lm_w=0,\n):\n    """"""\n    Compare a topics list of a target data to a topics list of a reference data\n    and extract hot topics, trends and clusters. Topic lists can be generated\n    by running topic_extraction.py\n\n    Args:\n        target_data: A list of topics with importance scores extracted from the tagret corpus\n        ref_data: A list of topics with importance scores extracted from the reference corpus\n        tar_header: The header to appear for the target topics graphs\n        ref_header: The header to appear for the reference topics graphs\n        top_n (int): Limit the analysis to only the top N phrases of each list\n        top_n_vectors (int): The number of vectors to include in the scatter\n        re_analysis (Boolean): whether a first analysis has already been made or not\n        tfidf_w (Float): the TF_IDF weight for the final score calculation\n        cval_w (Float): the C_Value weight for the final score calculation\n        lm_w (Float): the Language-Model weight for the final score calculation\n    """"""\n    hash2group = {}\n    rep2rank = {}  # the merged list of groups extracted from both corpora.\n    #  Will be sorted by their rank.\n    in_model_count = 0\n    create_clusters = False\n    try:\n        if not re_analysis:  # first analysis, not through ui\n            copyfile(target_data, target_topics_path)  # copying the initial\n            # scores file to have a const filename for the ui to recognize\n            #  when reanalyzing\n            copyfile(ref_data, ref_topics_path)\n        calc_scores(target_data, tfidf_w, cval_w, lm_w, target_scores_path)\n        calc_scores(ref_data, tfidf_w, cval_w, lm_w, ref_scores_path)\n        # unify all topics:\n        with open(ref_data, encoding=""utf-8"", errors=""ignore"") as f:\n            topics1 = sum(1 for _ in f)\n        with open(target_data, encoding=""utf-8"", errors=""ignore"") as f:\n            topics2 = sum(1 for _ in f)\n        sum_topics = topics1 + topics2\n        logger.info(""sum of all topics= %s"", str(sum_topics))\n        merge_phrases(ref_scores_path, True, hash2group, rep2rank, top_n, sum_topics)\n        merge_phrases(target_scores_path, False, hash2group, rep2rank, top_n, sum_topics)\n        logger.info(""Total number of evaluated topics: %s"", str(len(rep2rank)))\n        all_topics_sorted = sorted(rep2rank, key=rep2rank.get)\n        top_n_scatter = len(all_topics_sorted) if top_n_vectors is None else top_n_vectors\n        # compute 2D space clusters if model exists:\n        w2v_loc = path.join(directory, ""W2V_Models/model.bin"")\n        if os.path.isfile(w2v_loc):\n            scatter_group = all_topics_sorted[0:top_n_scatter]\n            np_scat, x_scat, y_scat, in_model_count = compute_scatter_subwords(\n                scatter_group, w2v_loc\n            )\n            if np_scat is not None and x_scat is not None:\n                create_clusters = True\n                for j in range(len(np_scat)):\n                    hash2group[simple_normalizer(np_scat[j])] += (x_scat[j], y_scat[j])\n        # prepare reports data:\n        groups_r = list(filter(lambda x: x[2] == 0 or x[2] == 2, hash2group.values()))\n        groups_t = list(filter(lambda x: x[2] == 1 or x[2] == 2, hash2group.values()))\n        trends = list(filter(lambda x: x[2] == 2, hash2group.values()))  # all trends\n        groups_r_sorted = sorted(groups_r, key=operator.itemgetter(1), reverse=True)\n        groups_t_sorted = sorted(groups_t, key=operator.itemgetter(3), reverse=True)\n        trends_sorted = sorted(trends, key=operator.itemgetter(6), reverse=True)  # sort by t_score\n\n        # save results:\n        save_report_data(\n            hash2group,\n            groups_r_sorted,\n            groups_t_sorted,\n            trends_sorted,\n            all_topics_sorted,\n            create_clusters,\n            tar_header,\n            ref_header,\n            tfidf_w,\n            cval_w,\n            lm_w,\n            in_model_count,\n            top_n_scatter,\n        )\n        logger.info(""Done analysis."")\n    except Exception as e:\n        logger.error(str(e))\n\n\ndef save_report_data(\n    hash2group,\n    groups_r_sorted,\n    groups_t_sorted,\n    trends_sorted,\n    all_topics_sorted,\n    create_clusters,\n    target_header,\n    ref_header,\n    tfidf_w,\n    cval_w,\n    freq_w,\n    in_model_count,\n    top_n_scatter,\n):\n    filter_output = path.join(directory, ""filter_phrases.csv"")\n    data_output = path.join(directory, ""graph_data.csv"")\n    logger.info(""writing results to: %s and: %s "", str(data_output), str(filter_output))\n\n    reports_column = [\n        ""Top Topics ("" + ref_header + "")"",\n        ""Top Topics ("" + target_header + "")"",\n        ""Hot Trends"",\n        ""Cold Trends"",\n        ""Custom Trends"",\n    ]\n    if create_clusters:\n        reports_column.extend(\n            [\n                ""Trend Clustering"",\n                ""Topic Clustering ("" + ref_header + "")"",\n                ""Topic Clustering ("" + target_header + "")"",\n            ]\n        )\n    headers = [\n        ""reports"",\n        ""ref_topic"",\n        ""ref_imp"",\n        ""x_ref"",\n        ""y_ref"",\n        ""tar_topic"",\n        ""tar_imp"",\n        ""x_tar"",\n        ""y_tar"",\n        ""trends"",\n        ""change"",\n        ""t_score"",\n        ""x_tre"",\n        ""y_tre"",\n        ""weights"",\n        ""in_w2v_model_count"",\n        ""top_n_scatter"",\n    ]\n    filter_headers = [""topics"", ""valid"", ""custom""]\n    weights = [tfidf_w, cval_w, freq_w]\n\n    with open(data_output, ""wt"", encoding=""utf-8"") as data_file:\n        with open(filter_output, ""wt"", encoding=""utf-8"") as filter_file:\n            data_writer = csv.writer(data_file, delimiter="","")\n            filter_writer = csv.writer(filter_file, delimiter="","")\n            data_writer.writerow(headers)\n            filter_writer.writerow(filter_headers)\n            for i in range(len(hash2group.keys())):\n                try:\n                    new_row = ()\n                    new_row += (reports_column[i],) if i < len(reports_column) else ("""",)\n                    if i < len(groups_r_sorted):\n                        new_row += (groups_r_sorted[i][0], groups_r_sorted[i][1])\n                        # \'only b\' type tuple\n                        if groups_r_sorted[i][2] == 0:\n                            new_row += (\n                                (groups_r_sorted[i][-2], groups_r_sorted[i][-1])\n                                if len(groups_r_sorted[i]) > 4\n                                else (-1, -1)\n                            )\n                        else:  # \'trend\' type tuple\n                            new_row += (\n                                (groups_r_sorted[i][-2], groups_r_sorted[i][-1])\n                                if len(groups_r_sorted[i]) > 7\n                                else (-1, -1)\n                            )\n                    else:\n                        new_row += ("""", """", """", """")\n                    if len(groups_t_sorted) > i:\n                        new_row += (groups_t_sorted[i][0], groups_t_sorted[i][3])\n                        # \'only a\' type tuple\n                        if groups_t_sorted[i][2] == 1:\n                            new_row += (\n                                (groups_t_sorted[i][-2], groups_t_sorted[i][-1])\n                                if len(groups_t_sorted[i]) > 5\n                                else (-1, -1)\n                            )\n                        else:\n                            new_row += (\n                                (groups_t_sorted[i][-2], groups_t_sorted[i][-1])\n                                if len(groups_t_sorted[i]) > 7\n                                else (-1, -1)\n                            )\n                    else:\n                        new_row += ("""", """", """", """")\n                    if len(trends_sorted) > i:\n                        new_row += (trends_sorted[i][0], trends_sorted[i][4], trends_sorted[i][6])\n                        new_row += (\n                            (trends_sorted[i][-2], trends_sorted[i][-1])\n                            if len(trends_sorted[i]) > 7\n                            else (-1, -1)\n                        )\n                    else:\n                        new_row += ("""", """", """", """")\n                    new_row += (weights[i],) if i < len(weights) else ("""",)\n                    new_row += (in_model_count, top_n_scatter) if i == 0 else ("""",)\n                    data_writer.writerow(new_row)\n                    filter_row = (all_topics_sorted[i], 1, 0)\n                    filter_writer.writerow(filter_row)\n                except Exception as e:\n                    logger.error(\n                        ""Error while writing analysis results. "" ""iteration #: %s. Error: %s"",\n                        str(i),\n                        str(e),\n                    )\n            if len(hash2group.keys()) < len(reports_column):\n                for i in range(len(hash2group.keys()), len(reports_column)):\n                    new_row = ()\n                    new_row += (\n                        reports_column[i],\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                        """",\n                    )\n                    data_writer.writerow(new_row)\n\n\ndef compute_scatter_subwords(top_groups, w2v_loc):\n    """"""\n    Compute 2D vectors of the provided phrases groups\n\n    Args:\n        top_groups: A list of group-representative phrases (List(String))\n        w2v_loc: A path to a w2v model (String)\n\n    Returns:\n        A tuple (phrases, x, y, n)\n        WHERE:\n        phrases: A list of phrases that are part of the model\n        x: A DataFrame column as the x values of the phrase vector\n        y: A DataFrame column as the y values of the phrase vector\n        n: The number of computed vectors\n    """"""\n    x_feature, np_indices = [], []\n    in_ctr = 0\n\n    try:\n        if path.isfile(w2v_loc):\n            logger.info(""computing scatter on %s groups"", str(len(top_groups)))\n            model = FasttextEmbeddingsModel.load(w2v_loc)\n            for a in top_groups:\n                x_feature.append(model[a])\n                np_indices.append(a)\n                in_ctr += 1\n            logger.info(""topics found in model out of %s: %s."", str(len(top_groups)), str(in_ctr))\n            if len(np_indices) == 0:\n                logger.error(""no vectors extracted"")\n                return None, None, None, 0\n            logger.info(""computing TSNE embedding..."")\n            tsne = TSNE(n_components=2, random_state=0, method=""exact"")\n            numpy.set_printoptions(suppress=True)\n            x_tsne = tsne.fit_transform(x_feature)\n            df = pd.DataFrame(x_tsne, index=np_indices, columns=[""x"", ""y""])\n            return np_indices, df[""x""], df[""y""], in_ctr\n        logger.error(""no model found for cumputing scatter, skipping step."")\n        return None, None, None, 0\n    except Exception as e:\n        logger.error(str(e))\n\n\ndef calc_scores(scores_file, tfidf_w, cval_w, lm_w, output_path):\n    """"""\n    Given a topic list with tf_idf,c_value,language_model scores, compute\n     a final score for each phrases group according to the given weights\n\n    Args:\n        scores_file (String): A path to the file with groups and raw scores\n        tfidf_w (Float): the TF_IDF weight for the final score calculation\n        cval_w (Float): the C_Value weight for the final score calculation\n        lm_w (Float): the Language-Model weight for the final score calculation\n        output_path: A path for the output file of final scores (String)\n    """"""\n    logger.info(\n        ""calculating scores for file: %s with: tfidf_w=%s, cval_w=%s,"" "" freq_w=%s"",\n        scores_file,\n        str(tfidf_w),\n        str(cval_w),\n        str(lm_w),\n    )\n    with open(scores_file, encoding=""utf-8"", errors=""ignore"") as csv_file:\n        lines = csv.reader((x.replace(""\\0"", """") for x in csv_file), delimiter="","")\n        final_list = []\n        for group, tfidf, cval, freq in lines:\n            score = (float(tfidf) * tfidf_w) + (float(cval) * cval_w) + (float(freq) * lm_w)\n            final_list.append((group, score))\n        sorted_list = sorted(final_list, key=lambda tup: tup[1], reverse=True)\n    save_scores_list(sorted_list, output_path)\n\n\ndef save_scores_list(scores, file_path):\n    """"""\n    Save the list of topics-scores into a file\n\n    Args:\n        scores: A list of topics (groups) with scores\n        file_path: The output file path\n    """"""\n    logger.info(""saving np extraction results to: %s"", file_path)\n    with open(file_path, ""wt"", encoding=""utf-8"", newline="""") as csv_file:\n        writer = csv.writer(csv_file, delimiter="","")\n        ctr = 0\n        for topics, imp in scores:\n            try:\n                row = (topics, str(imp))\n                writer.writerow(row)\n                ctr += 1\n            except Exception as e:\n                logger.error(\n                    ""Error while writing scores to file. iteration #: %s. Error: %s"",\n                    str(ctr),\n                    str(e),\n                )\n\n\ndef merge_phrases(data, is_ref_data, hash2group, rep2rank, top_n, topics_count):\n    """"""\n    Analyze the provided topics data and detect trends (changes in importance)\n\n    Args:\n        data: A list of topics with importance scores\n        is_ref_data (Boolean): Was the data extracted from the target/reference corpus\n        hash2group: A dictionary storing the data of each topic\n        rep2rank: A dict of all groups representatives and their ranks\n        top_n (int): Limit the analysis to only the top N phrases of each list\n        topics_count (int): The total sum of all topics extracted from both corpora\n    """"""\n    logger.info(""merge and compare groups for data: %s"", str(data))\n    ctr = 0\n    if not Path(data).exists():\n        logger.error(""invalid csv file: %s"", str(data))\n        sys.exit()\n    try:\n        with open(data, encoding=""utf-8"", errors=""ignore"") as csv_file:\n            topics = csv.reader(csv_file, delimiter="","")\n            for group, imp in topics:\n                if ctr == top_n:\n                    break\n                try:\n                    rep = clean_group(group).strip()\n                    imp = float(imp) * 100.0\n                    rank = ctr + 1\n                    hash_id = simple_normalizer(rep)\n                    if hash_id not in hash2group:\n                        rep2rank[rep] = rank\n                        if is_ref_data:\n                            hash2group[hash_id] = (rep, imp, 0, rank)\n                        else:\n                            hash2group[hash_id] = (rep, 0, 1, imp, rank)\n                    elif not is_ref_data:\n                        data_b = hash2group[hash_id]\n                        if data_b[2] == 0:  # create a trend only in comparison to the\n                            #  ref topics, ignore cases of different topics have the\n                            # same hash or same topic was extracted twice from the\n                            # same data\n                            old_rep = data_b[0]\n                            old_rank = data_b[3]\n                            rep2rank[old_rep] = int((rank + old_rank) / 2)  # rank\n                            #  of topic that appear in both corpora calculated as\n                            #  the avarage of ranks\n                            change = float(imp) - float(data_b[1])\n                            t_score = (topics_count - (old_rank + rank)) * abs(change)\n                            hash2group[hash_id] = (\n                                old_rep,\n                                float(data_b[1]),\n                                2,\n                                imp,\n                                change,\n                                abs(change),\n                                t_score,\n                            )  # trend phrase\n                    ctr += 1\n                except Exception as e:\n                    logger.error(""bad line: %s. Error: %s"", str(ctr), str(e))\n    except Exception as e:\n        logger.error(""Error: %s. Is %s a valid csv file?"", str(e), str(data))\n        sys.exit()\n\n\ndef clean_group(phrase_group):\n    """"""\n   Returns the shortest element in a group of phrases\n\n    Args:\n        phrase_group (String): a group of phrases separated by \';\'\n\n    Returns:\n        The shortest phrase in the group (String)\n    """"""\n    text = [x.lstrip() for x in phrase_group.split("";"")]\n    return min(text, key=len)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(prog=""trend_analysis.py"")\n    parser.add_argument(\n        ""target_topics"",\n        metavar=""target_topics"",\n        type=validate_existing_filepath,\n        help=""a path to a csv topic-list extracted from the "" ""target corpus"",\n    )\n    parser.add_argument(\n        ""ref_topics"",\n        metavar=""ref_topics"",\n        type=validate_existing_filepath,\n        help=""a path to a csv topic-list extracted from the "" ""reference corpus"",\n    )\n    parser.add_argument(\n        ""--top_n"",\n        type=int,\n        action=check_size(0, 100000),\n        default=10000,\n        help=""compare only top N topics (default: 10000)"",\n    )\n    parser.add_argument(\n        ""--top_vectors"",\n        type=int,\n        action=check_size(0, 100000),\n        default=500,\n        help=""include only top N vectors in the scatter graph (default: 500)"",\n    )\n    args = parser.parse_args()\n    analyze(\n        args.target_topics,\n        args.ref_topics,\n        args.target_topics,\n        args.ref_topics,\n        args.top_n,\n        args.top_vectors,\n    )\n'"
solutions/trend_analysis/ui_main.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=global-statement\nimport csv\nimport logging\nimport time\nfrom os import path\nfrom os.path import isfile\n\nimport pandas as pd\nfrom bokeh.io import curdoc\nfrom bokeh.layouts import column, layout, WidgetBox\nfrom bokeh.models import ColumnDataSource, Div, Row, ranges, LabelSet, CDSView, IndexFilter\nfrom bokeh.models.widgets import (\n    DataTable,\n    TableColumn,\n    RadioGroup,\n    Dropdown,\n    Tabs,\n    Panel,\n    TextInput,\n    Button,\n    Slider,\n)\nfrom bokeh.plotting import figure\n\nfrom trend_analysis import analyze\nfrom nlp_architect import LIBRARY_OUT\n\nlogger = logging.getLogger(__name__)\ndir = str(LIBRARY_OUT / ""trend-analysis-data"")\ngraph_data_path = path.join(dir, ""graph_data.csv"")\nfilter_data_path = path.join(dir, ""filter_phrases.csv"")\ntarget_scores_path = path.join(dir, ""target_topics.csv"")\nref_scores_path = path.join(dir, ""ref_topics.csv"")\nmax_len = 50\ngd = None\ntop_before = None\ntop_after = None\nhot_trends = None\ncold_trends = None\ntrend_clustering = None\ncustom_trends = None\ntopic_clustering_before = None\ntopic_clustering_after = None\nwhite_list = []\ncustom_nps = []\nref_data_name = """"\ntarget_data_name = """"\napply_filter = False\ndont_regenerate = False\nfilter_item = -1\nall_topics = []\nfilter_rows = []\ntable_columns = [TableColumn(field=""topics"", title=""Topics"")]\n\n\n# create ui components\n\nfilter_topics_table_source = ColumnDataSource(data={})\nview1 = CDSView(source=filter_topics_table_source, filters=[IndexFilter()])\nfilter_topics_table = DataTable(\n    source=filter_topics_table_source,\n    view=view1,\n    columns=table_columns,\n    width=500,\n    selectable=True,\n    scroll_to_selection=True,\n    css_classes=[""filter_topics_table""],\n)\nfilter_custom_table_source = ColumnDataSource(data={})\nfilter_custom_table = DataTable(\n    source=filter_custom_table_source,\n    columns=table_columns,\n    width=500,\n    selectable=True,\n    css_classes=[""filter_custom_table""],\n)\nradio_group_area = RadioGroup(\n    labels=[\n        ""All"",\n        ""Top Topics"",\n        ""Trends"",\n        ""Trend Clustering"",\n        ""Custom Trends"",\n        ""Topic Clustering"",\n        ""Filter"",\n    ],\n    active=0,\n    orientation=""vertical"",\n    height=150,\n)\nn_menu = [(""5"", ""5""), (""10"", ""10""), (""20"", ""20""), (""30"", ""30"")]\nn_clusters_menu = [(""20"", ""20""), (""50"", ""50""), (""100"", ""100""), (""300"", ""300""), (""500"", ""500"")]\nscores = [(""0"", ""0""), (""0.1"", ""0.1""), (""0.25"", ""0.25""), (""0.5"", ""0.5""), (""1"", ""1"")]\ntop_n_dropdown = Dropdown(\n    label=""Show Top 10 Phrases"", button_type=""warning"", menu=n_menu, value=""10""\n)\ntop_n_clusters_dropdown = Dropdown(\n    label=""Show Top 100 Clusters"", button_type=""warning"", menu=n_clusters_menu, value=""100""\n)\ntfidf_slider = Slider(start=0, end=1, value=0, step=0.05, title=""Informativeness"")\ncval_slider = Slider(start=0, end=1, value=0, step=0.05, title=""Completeness"")\nfreq_slider = Slider(start=0, end=1, value=0, step=0.05, title=""Linguistical"")\nfilter_topics_tab = Panel(child=filter_topics_table, title=""Filter Topics"")\nfilter_custom_tab = Panel(child=filter_custom_table, title=""Custom Trends"")\nfilter_tabs = Tabs(tabs=[filter_topics_tab, filter_custom_tab])\nsearch_input_box = TextInput(title=""Search:"", value="""", width=300)\nsearch_button = Button(label=""Go"", button_type=""success"", width=50, css_classes=[""search_button""])\nclear_button = Button(label=""Clear"", button_type=""success"", width=50, css_classes=[""clear_button""])\nbuttons_layout = column(Div(height=0), Row(search_button, clear_button))\nanalyse_button = Button(label=""re-analyze"", button_type=""success"")\nfilter_label = Div(text="""", style={""color"": ""red"", ""padding-bottom"": ""0px"", ""font-size"": ""15px""})\nlabel = Div(text="""", style={""color"": ""red"", ""padding-bottom"": ""0px"", ""font-size"": ""15px""})\nconfig_box = WidgetBox(tfidf_slider, cval_slider, freq_slider)\ngraphs_area = column(children=[])\n\n\n# define page layout\n\nconfig_area = layout(\n    [\n        [radio_group_area],\n        [top_n_dropdown],\n        [top_n_clusters_dropdown],\n        [config_box],\n        [analyse_button],\n    ]\n)\ngrid = layout(\n    [\n        [Div(width=500), Div(text=""<H1>AIPG Trend Analysis</H1>"")],\n        [label],\n        [config_area, graphs_area],\n    ]\n)\n\n\n# define functions\n\n\ndef reset_filters():\n    """"""\n    Resets the filter tables (e.g. after clearing search)\n    """"""\n    logger.info(""reset filters"")\n    global filter_item\n    filter_item = -1\n    filter_topics_table.view.filters = [IndexFilter()]\n    filter_custom_table.view.filters = [IndexFilter()]\n    filter_label.text = """"\n\n\ndef refresh_filter_area():\n    graphs_area.children = [Row(search_input_box, buttons_layout), filter_label, filter_tabs]\n\n\ndef get_valid_phrases():\n    """"""\n    Get a list of all whitelist/valid phrases (which were not filtered-out by the user)\n    """"""\n    return [x[0] for x in all_topics if x[1] == ""1""]\n\n\ndef get_custom_phrases():\n    """"""\n    Get a list of all phrases selected by the user for the custom trends graph\n    """"""\n    return [x[0] for x in all_topics if x[2] == ""1""]\n\n\ndef get_valid_indices():\n    """"""\n    Get the filter-table indices of all whitelist/valid phrases\n    """"""\n    return [i for i, val in enumerate(all_topics) if val[1] == ""1""]\n\n\ndef is_valid_topic(index):\n    """"""\n    Check if a certain phrase should be presented\n    """"""\n    return all_topics[index][1] == ""1""\n\n\ndef get_custom_indices():\n    """"""\n    Get the custom-table indices of all custom selected phrases\n    """"""\n    return [i for i, val in enumerate(all_topics) if val[2] == ""1""]\n\n\ndef search(item):\n    """"""\n    Search an item in the filter tables\n    """"""\n    row = -1\n    try:\n        row = filter_rows.index(item)\n    except Exception:\n        logger.warning(""%s was not found"", str(item))\n    return row\n\n\ndef cut_phrase(phrase):\n    """"""\n    Cut phrases which exceeds phrase max length to present\n    """"""\n    return phrase if len(phrase) < max_len else phrase[:max_len] + ""...""\n\n\ndef read_filter_graph_data():\n    global all_topics, filter_rows\n    all_topics = []\n    filter_rows = []\n    with open(filter_data_path, errors=""ignore"") as f:\n        reader = csv.reader(f)\n        next(reader, None)\n        for line in enumerate(reader):\n            all_topics.append(line[1])\n            filter_rows.append(line[1][0])\n\n\ndef create_topic_clustering_before_graph(top_n):\n    if ""Trend Clustering"" not in gd[""reports""].values:  # no clustering data\n        return None\n    header = gd[""reports""][6]\n    scatter_series = gd[(gd[""x_ref""] != -1) & (gd[""x_ref""].notna())]\n    phrases = scatter_series[""ref_topic""].values\n    x_b = scatter_series[""x_ref""].values\n    y_b = scatter_series[""y_ref""].values\n    valid_phrases, valid_x, valid_y = [], [], []\n    ctr = 0\n    for i in range(len(phrases)):\n        if ctr == top_n:\n            break\n        phrase = phrases[i]\n        if phrase in white_list:\n            valid_phrases.append(phrase)\n            valid_x.append(x_b[i])\n            valid_y.append(y_b[i])\n            ctr += 1\n\n    source = ColumnDataSource(data=dict(x=valid_x, y=valid_y, topics=valid_phrases))\n\n    p = figure(title=header, plot_width=600, plot_height=400, tooltips=""@topics"")\n    p.circle(x=""x"", y=""y"", size=13, color=""#335fa5"", alpha=1, source=source)\n    return p\n\n\ndef create_topic_clustering_after_graph(top_n):\n    if ""Trend Clustering"" not in gd[""reports""].values:  # no clustering data\n        return None\n    header = gd[""reports""][7]\n    scatter_series = gd[(gd[""x_tar""] != -1) & (gd[""x_tar""].notna())]\n    phrases = scatter_series[""tar_topic""].values\n    x_a = scatter_series[""x_tar""].values\n    y_a = scatter_series[""y_tar""].values\n    valid_phrases, valid_x, valid_y = [], [], []\n    ctr = 0\n    for i in range(len(phrases)):\n        if ctr == top_n:\n            break\n        phrase = phrases[i]\n        if phrase in white_list:\n            valid_phrases.append(phrase)\n            valid_x.append(x_a[i])\n            valid_y.append(y_a[i])\n            ctr += 1\n\n    source = ColumnDataSource(data=dict(x=valid_x, y=valid_y, topics=valid_phrases))\n\n    p = figure(title=header, plot_width=600, plot_height=400, tooltips=""@topics"")\n    p.circle(x=""x"", y=""y"", size=13, color=""#335fa5"", alpha=1, source=source)\n\n    return p\n\n\ndef regenerate_graphs(top_n_phrases, top_n_clusters):\n    logger.info(""regenerate graphs"")\n    global top_before, top_after, hot_trends, cold_trends, trend_clustering, custom_trends, white_list, custom_nps, topic_clustering_before, topic_clustering_after\n    white_list = get_valid_phrases()\n    custom_nps = get_custom_phrases()\n    top_before = create_top_before_graph(int(top_n_phrases))\n    top_after = create_top_after_graph(int(top_n_phrases))\n    hot_trends = create_trend_graphs(int(top_n_phrases))[0]\n    cold_trends = create_trend_graphs(int(top_n_phrases))[1]\n    if top_n_clusters != 0:  # no w2v model\n        trend_clustering = create_trend_clustering_graph(int(top_n_clusters))\n        topic_clustering_before = create_topic_clustering_before_graph(int(top_n_clusters))\n        topic_clustering_after = create_topic_clustering_after_graph(int(top_n_clusters))\n    custom_trends = create_custom_trends_graph()\n\n\ndef create_graphs(top_n_phrases, top_n_clusters):\n    logger.info(""create graphs"")\n    global topic_clustering_before, topic_clustering_after, gd\n    if gd is None:  # initialization\n        gd = pd.read_csv(graph_data_path)\n        gd = gd.fillna(0)\n        read_filter_graph_data()  # read only once- then update using the filter table\n        global white_list, custom_nps\n        white_list = get_valid_phrases()\n        custom_nps = get_custom_phrases()\n        set_weights()\n    regenerate_graphs(top_n_phrases, top_n_clusters)\n\n\ndef set_weights():\n    weights_series = gd[""weights""]\n    tfidf_slider.value = weights_series[0]\n    cval_slider.value = weights_series[1]\n    freq_slider.value = weights_series[2]\n\n\ndef re_analyze():\n    logger.info(""re-analysing scores"")\n    label.text = ""Re-Analysing... Please Wait...""\n    global gd\n    if isfile(target_scores_path) and isfile(ref_scores_path):\n        analyze(\n            target_scores_path,\n            ref_scores_path,\n            tar_header=target_data_name,\n            ref_header=ref_data_name,\n            re_analysis=True,\n            tfidf_w=float(tfidf_slider.value),\n            cval_w=float(cval_slider.value),\n            lm_w=float(freq_slider.value),\n        )\n        gd = None\n        create_graphs(top_n_dropdown.value, int(top_n_clusters_dropdown.value))\n        if radio_group_area.active == 0:\n            handle_selected_graph(0)\n        else:\n            radio_group_area.active = 0\n        label.text = ""Done!""\n        time.sleep(2)\n        label.text = """"\n    else:\n        logger.info(""scores files are missing"")\n        label.text = ""Error: scores files are missing!""\n        time.sleep(2)\n    logger.info(""re-analysing done"")\n\n\ndef create_custom_trends_graph():\n    header = gd[""reports""][4]\n    phrases = gd.values[:, 9]\n    imp_change = gd.values[:, 10]\n    phrases_h, changes_h = [], []\n    phrases_c, changes_c = [], []\n    text_h, text_c = [], []\n    custom_count = 0\n    for i in range(len(phrases)):\n        phrase = phrases[i]\n        change = float(imp_change[i])\n        if phrase in custom_nps:\n            if custom_count > 30:\n                logger.warning(""too many custom phrases (>30). Showing partial list"")\n                break\n            phrase = cut_phrase(phrase)\n            if change > 0:\n                phrases_h.append(phrase)\n                changes_h.append(change)\n                text_h.append(""+"" + str((""%.1f"" % change)) + ""%"")\n            else:\n                phrases_c.append(phrase)\n                changes_c.append(change)\n                text_c.append(str((""%.1f"" % change)) + ""%"")\n            custom_count += 1\n\n    changes = changes_h + changes_c\n    text = text_h + text_c\n    trends = phrases_h + phrases_c\n    colors = []\n\n    if len(changes_h) > 0:\n        for i in range(len(changes_h)):\n            colors.append(""#1d6d34"")\n    if len(changes_c) > 0:\n        for i in range(len(changes_c)):\n            colors.append(""#912605"")\n    if len(changes) < 10:  # pad with 10 blanks\n        changes += [0] * (10 - len(changes))\n        text += "" "" * (10 - len(text))\n        trends += [str((10 - len(trends)) - i) for i in range(0, (10 - len(trends)))]\n        colors += [""white""] * (10 - len(colors))\n\n    source = ColumnDataSource(\n        dict(y=trends[::-1], x=changes[::-1], colors=colors[::-1], text=text[::-1])\n    )\n\n    plot = figure(\n        title=header,\n        plot_width=600,\n        plot_height=400,\n        tools=""save"",\n        x_range=ranges.Range1d(start=min(changes) - 10, end=max(changes) + 20),\n        y_range=source.data[""y""],\n        tooltips=""@y<br>change: @x"",\n    )\n    labels = LabelSet(\n        x=max(changes) + 5,\n        y=""y"",\n        text=""text"",\n        level=""glyph"",\n        x_offset=0,\n        y_offset=-10,\n        source=source,\n        render_mode=""canvas"",\n    )\n\n    plot.hbar(source=source, right=""x"", y=""y"", left=0, height=0.5, color=""colors"")\n    plot.add_layout(labels)\n\n    return plot\n\n\ndef create_trend_graphs(top_n):\n    header_h = gd[""reports""][2]\n    header_c = gd[""reports""][3]\n    phrases = gd.values[:, 9]  # all trends\n    imp_change = gd.values[:, 10]\n    data_h, data_c = {}, {}\n    text_h, text_c = [], []\n    hot, cold = 0, 0  # counters for each list\n    plot_h = figure(plot_width=600, plot_height=400, title=header_h)\n    plot_c = figure(plot_width=600, plot_height=400, title=header_c)\n    for i in range(len(phrases)):\n        if hot == top_n and cold == top_n:\n            break\n        phrase = phrases[i]\n        change = float(imp_change[i])\n        if phrase in white_list:\n            phrase = cut_phrase(phrase)\n            if change > 0:\n                if hot < top_n:\n                    data_h[phrase] = change\n                    text_h.append(""+"" + str((""%.1f"" % change)) + ""%"")\n                    hot += 1\n            elif cold < top_n:\n                data_c[phrase] = change\n                text_c.append(str((""%.1f"" % change)) + ""%"")\n                cold += 1\n    if len(data_h.keys()) > 0:\n        phrases_h = sorted(data_h, key=data_h.get)\n        changes_h = sorted(data_h.values())\n        source = ColumnDataSource(dict(y=phrases_h, x=changes_h, text=text_h[::-1]))\n        title = header_h\n        plot_h = figure(\n            plot_width=600,\n            plot_height=400,\n            tools=""save"",\n            title=title,\n            x_range=ranges.Range1d(start=0, end=max(changes_h) + 20),\n            y_range=source.data[""y""],\n            tooltips=""@y<br>change: @x"",\n        )\n        labels = LabelSet(\n            x=""x"",\n            y=""y"",\n            text=""text"",\n            level=""glyph"",\n            x_offset=5,\n            y_offset=0,\n            source=source,\n            render_mode=""canvas"",\n            text_color=""#1d6d34"",\n        )\n        plot_h.hbar(source=source, right=""x"", y=""y"", left=0, height=0.5, color=""#1d6d34"")\n        plot_h.add_layout(labels)\n        plot_h.xaxis.visible = False\n    if len(data_c.keys()) > 0:\n        phrases_c = sorted(data_c, key=data_c.get)\n        changes_c = sorted(data_c.values())\n        source = ColumnDataSource(dict(y=phrases_c[::-1], x=changes_c[::-1], text=text_c[::-1]))\n        plot_c = figure(\n            plot_width=600,\n            plot_height=400,\n            tools=""save"",\n            title=header_c,\n            x_range=ranges.Range1d(start=min(changes_c) - 10, end=20),\n            y_range=source.data[""y""],\n            tooltips=""@y<br>change: @x"",\n        )\n        labels = LabelSet(\n            x=0,\n            y=""y"",\n            text=""text"",\n            level=""glyph"",\n            x_offset=5,\n            y_offset=0,\n            source=source,\n            render_mode=""canvas"",\n            text_color=""#912605"",\n        )\n        plot_c.hbar(source=source, right=""x"", y=""y"", left=0, height=0.5, color=""#912605"")\n        plot_c.add_layout(labels)\n        plot_c.xaxis.visible = False\n\n    return [plot_h, plot_c]\n\n\ndef create_trend_clustering_graph(top_n):\n    if ""Trend Clustering"" not in gd[""reports""].values:  # no clustering data\n        return None\n    header = gd[""reports""][5]\n    scatter_series_pos = gd[(gd[""x_tre""] != -1) & (gd[""x_tre""].notna()) & (gd[""change""] > 0)]\n    scatter_series_neg = gd[(gd[""x_tre""] != -1) & (gd[""x_tre""].notna()) & (gd[""change""] < 0)]\n    hot_phrases = scatter_series_pos[""trends""].values\n    cold_phrases = scatter_series_neg[""trends""].values\n    hot_change = scatter_series_pos[""change""].values\n    cold_change = scatter_series_neg[""change""].values\n    hot_x_t = scatter_series_pos[""x_tre""].values\n    cold_x_t = scatter_series_neg[""x_tre""].values\n    hot_y_t = scatter_series_pos[""y_tre""].values\n    cold_y_t = scatter_series_neg[""y_tre""].values\n    phrases_h, changes_h, x_h, y_h, hover_data_h = [], [], [], [], []\n    phrases_c, changes_c, x_c, y_c, hover_data_c = [], [], [], [], []\n    opacity_h, opacity_c = [], []\n    hot, cold = 0, 0  # counters for each list\n    opacity_norm = 1\n    if len(hot_change) > 0 and len(cold_change) > 0:\n        highest_change = max(hot_change[0], abs(cold_change[0]))\n        opacity_norm = 1 / (highest_change) if highest_change != 0 else 0.05\n\n    for i in range(len(hot_phrases)):\n        if hot == top_n:\n            break\n        phrase = hot_phrases[i]\n        change = float(hot_change[i])\n        if phrase in white_list:\n            phrases_h.append(phrase)\n            changes_h.append(change)\n            x_h.append(hot_x_t[i])\n            y_h.append(hot_y_t[i])\n            hover_data_h.append(phrase)\n            opacity = opacity_norm * abs(change)\n            opacity_h.append(opacity if opacity > 0.05 else 0.05)\n            hot += 1\n\n    for j in range(len(cold_phrases)):\n        if cold == top_n:\n            break\n        phrase = cold_phrases[j]\n        change = float(cold_change[j])\n        if phrase in white_list:\n            phrases_c.append(phrase)\n            changes_c.append(change)\n            x_c.append(cold_x_t[j])\n            y_c.append(cold_y_t[j])\n            hover_data_c.append(phrase)\n            opacity = opacity_norm * abs(change)\n            opacity_c.append(opacity if opacity > 0.05 else 0.05)\n            cold += 1\n\n    opacities = []\n    colors = []\n    # hot:\n    for i in range(len(x_h)):\n        opacities.append(opacity_h[i])\n        colors.append(""#4c823d"")\n    # cold:\n    for i in range(len(x_c)):\n        opacities.append(opacity_c[i])\n        colors.append(""#d6400a"")\n\n    source = ColumnDataSource(\n        data=dict(\n            x=x_h + x_c,\n            y=y_h + y_c,\n            topic=hover_data_h + hover_data_c,\n            opacities=opacities,\n            colors=colors,\n        )\n    )\n    p = figure(title=header, plot_width=600, plot_height=400, tooltips=""@topic"")\n    p.circle(x=""x"", y=""y"", size=13, color=""colors"", alpha=""opacities"", source=source)\n    return p\n\n\ndef create_top_before_graph(top_n):\n    global ref_data_name\n    header = gd[""reports""][0]\n    if ref_data_name == """":\n        ref_data_name = header.split(""("")[1][:-1]\n    phrases = gd.values[:, 1]\n    importance = gd.values[:, 2]\n    valid_phrases, valid_imp = [], []\n    ctr = 0\n    for i in range(len(phrases)):\n        if ctr == top_n:\n            break\n        p = phrases[i]\n        if p in white_list:\n            p = cut_phrase(p)\n            valid_phrases.append(p)\n            valid_imp.append(importance[i])\n            ctr += 1\n\n    source = ColumnDataSource(dict(y=valid_phrases[::-1], x=valid_imp[::-1]))\n    x_label = ""Importance""\n    plot = figure(\n        plot_width=600,\n        plot_height=400,\n        tools=""save"",\n        x_axis_label=x_label,\n        title=header,\n        y_range=source.data[""y""],\n        tooltips=""@y<br>@x"",\n    )\n    plot.hbar(source=source, right=""x"", y=""y"", left=0, height=0.5, color=""#335fa5"")\n    return plot\n\n\ndef create_top_after_graph(topN):\n    global target_data_name\n    header = gd[""reports""][1]\n    if target_data_name == """":\n        target_data_name = header.split(""("")[1][:-1]\n    phrases = gd.values[:, 5]\n    importance = gd.values[:, 6]\n    valid_phrases, valid_imp = [], []\n    ctr = 0\n    for i in range(len(phrases)):\n        if ctr == topN:\n            break\n        p = phrases[i]\n        if p in white_list:\n            p = cut_phrase(p)\n            valid_phrases.append(p)\n            valid_imp.append(importance[i])\n            ctr += 1\n\n    source = ColumnDataSource(dict(y=valid_phrases[::-1], x=valid_imp[::-1]))\n    x_label = ""Importance""\n    plot = figure(\n        plot_width=600,\n        plot_height=400,\n        tools=""save"",\n        x_axis_label=x_label,\n        title=header,\n        y_range=source.data[""y""],\n        tooltips=""@y<br>@x"",\n    )\n    plot.hbar(source=source, right=""x"", y=""y"", left=0, height=0.5, color=""#335fa5"")\n    return plot\n\n\ndef handle_selected_graph(selected):\n    logger.info(""handle selected graph. selected = %s"", str(selected))\n    label.text = ""Please Wait...""\n    global apply_filter\n    if top_before is not None:\n        if apply_filter:\n            regenerate_graphs(int(top_n_dropdown.value), int(top_n_clusters_dropdown.value))\n            apply_filter = False\n        if selected == 0:\n            if trend_clustering is None:\n                graphs_area.children = [\n                    Row(top_before, top_after),\n                    Row(hot_trends, cold_trends),\n                    Row(custom_trends),\n                ]\n            else:\n                graphs_area.children = [\n                    Row(top_before, top_after),\n                    Row(hot_trends, cold_trends),\n                    Row(trend_clustering, custom_trends),\n                    Row(topic_clustering_before, topic_clustering_after),\n                ]\n        if selected == 1:\n            graphs_area.children = [Row(top_before, top_after)]\n        if selected == 2:\n            graphs_area.children = [Row(hot_trends, cold_trends)]\n        if selected == 3:\n            if trend_clustering is None:\n                graphs_area.children = [Div(text=""no word embedding data"")]\n            else:\n                graphs_area.children = [trend_clustering]\n        if selected == 4:\n            graphs_area.children = [custom_trends]\n        if selected == 5:\n            if topic_clustering_before is None:\n                graphs_area.children = [Div(text=""no word embedding data"")]\n            else:\n                graphs_area.children = [Row(topic_clustering_before, topic_clustering_after)]\n        if selected == 6:\n            filter_topics_table_source.data = {""topics"": filter_rows}\n            filter_topics_table_source.selected.indices = get_valid_indices()\n            filter_custom_table_source.data = {""topics"": filter_rows}\n            filter_custom_table_source.selected.indices = get_custom_indices()\n            refresh_filter_area()\n            apply_filter = False\n    label.text = """"\n\n\ndef draw_ui(top_n_phrases, top_n_clusters, active_area):\n    logger.info(\n        ""draw ui. top_n_phrases=%s, top_n_clusters=%s"", str(top_n_phrases), str(top_n_clusters)\n    )\n    create_graphs(top_n_phrases, top_n_clusters)\n    handle_selected_graph(active_area)\n\n\n# define callbacks\n# pylint: disable=unused-argument\ndef top_n_phrases_changed_callback(value, old, new):\n    logger.info(""top n changed to: %s"", str(new))\n    top_n_dropdown.label = ""Show Top "" + str(new) + "" Phrases""\n    draw_ui(new, top_n_clusters_dropdown.value, radio_group_area.active)\n    # create_graphs(new)\n    # handle_selected_graph(radio_group_area.active)\n\n\n# pylint: disable=unused-argument\ndef top_n_clusters_changed_callback(value, old, new):\n    global dont_regenerate\n    logger.info(""top n clusters changed to: %s"", str(new))\n    top_n_clusters_dropdown.label = ""Show Top "" + str(new) + "" Clusters""\n    if not dont_regenerate:\n        draw_ui(top_n_dropdown.value, new, radio_group_area.active)\n    else:\n        logger.info(""skip draw_ui"")\n        dont_regenerate = False\n\n\n# pylint: disable=unused-argument\ndef selected_graph_changed_callback(active, old, new):\n    global filter_item\n    logger.info(""selected graph changed to: %s"", str(new))\n    reset_filters()\n    handle_selected_graph(new)\n\n\n# pylint: disable=unused-argument\ndef filter_topic_selected_callback(indices, old, new):\n    # update all_topics according to new selected items\n    logger.info(""filter topic callback"")\n    filter_label.text = ""Please Wait...""\n    global all_topics, apply_filter\n    if filter_item != -1:  # on filter/search state there are None values so need\n        # to separate this case\n        apply_filter = True\n        if filter_item in new:\n            all_topics[filter_item][filter_tabs.active + 1] = ""1""\n        else:\n            all_topics[filter_item][filter_tabs.active + 1] = ""0""\n    elif new != [-1]:\n        apply_filter = True\n        selected_topics = [filter_topics_table_source.data[""topics""][x] for x in new]\n        for i, line in enumerate(all_topics):\n            if line[0] in selected_topics:\n                all_topics[i][1] = ""1""\n            else:\n                all_topics[i][1] = ""0""\n    filter_label.text = """"\n\n\n# pylint: disable=unused-argument\ndef filter_custom_selected_callback(indices, old, new):\n    logger.info(""filter custom callback"")\n    filter_label.text = ""Please Wait...""\n    global all_topics, apply_filter\n    if new != [-1]:\n        apply_filter = True\n        selected_topics = [filter_custom_table_source.data[""topics""][x] for x in new]\n        for i, line in enumerate(all_topics):\n            if line[0] in selected_topics:\n                all_topics[i][2] = ""1""\n            else:\n                all_topics[i][2] = ""0""\n    filter_label.text = """"\n\n\ndef search_topic_callback():\n    global filter_item\n    logger.info(""search topic callback"")\n    filter_label.text = """"\n    text_to_search = search_input_box.value\n    if len(text_to_search) > 0:\n        x = search(text_to_search)\n        logger.info(""x=%s"", str(x))\n        if x != -1:\n            reset_filters()\n            filter_item = x\n            if filter_tabs.active == 0:\n                logger.info(""is index %s a valid topic: %s"", str(x), str(is_valid_topic(x)))\n                filter_topics_table.view.filters[0].indices = [x]\n                filter_topics_table_source.data = {""topics"": filter_rows}  # just for\n                # refreshing the ui\n            else:\n                filter_custom_table.view.filters[0].indices = [x]\n                filter_custom_table_source.data = {""topics"": filter_rows}\n        else:\n            filter_label.text = ""no results""\n            logger.info(""no results"")\n    refresh_filter_area()\n\n\ndef tab_changed_callback(active, old, new):\n    reset_filters()\n\n\ndef clear_search_callback():\n    reset_filters()\n    search_input_box.value = """"\n    refresh_filter_area()\n\n\ndef re_analyze_callback():\n    re_analyze()\n\n\n# set callbacks\n\ntop_n_dropdown.on_change(""value"", top_n_phrases_changed_callback)\ntop_n_clusters_dropdown.on_change(""value"", top_n_clusters_changed_callback)\nradio_group_area.on_change(""active"", selected_graph_changed_callback)\n# pylint: disable=no-member\nfilter_topics_table_source.selected.on_change(""indices"", filter_topic_selected_callback)\nfilter_custom_table_source.selected.on_change(""indices"", filter_custom_selected_callback)\nsearch_button.on_click(search_topic_callback)\nclear_button.on_click(clear_search_callback)\nfilter_tabs.on_change(""active"", tab_changed_callback)\nanalyse_button.on_click(re_analyze_callback)\n\n\n# start app\n\ndraw_ui(top_n_dropdown.value, top_n_clusters_dropdown.value, radio_group_area.active)\n\ndoc = curdoc()\nmain_title = ""Trend Analysis""\ndoc.title = main_title\ndoc.add_root(grid)\n'"
tests/cdc/__init__.py,0,b''
tests/cdc/test_utils.py,0,"b'from nlp_architect.common.cdc.mention_data import MentionData\n\n\ndef get_embedd_mentions():\n    mentions_json = [\n        {\n            ""coref_chain"": ""HUM16236184328979740"",\n            ""doc_id"": ""0"",\n            ""mention_context"": [\n                ""Perennial"",\n                ""party"",\n                ""girl"",\n                ""Tara"",\n                ""Reid"",\n                ""checked"",\n                ""herself"",\n                ""into"",\n                ""Promises"",\n                ""Treatment"",\n                ""Center"",\n                "","",\n                ""her"",\n                ""rep"",\n                ""told"",\n                ""People"",\n                ""."",\n            ],\n            ""mention_head"": ""Reid"",\n            ""mention_head_lemma"": ""reid"",\n            ""mention_head_pos"": ""PROPN"",\n            ""mention_id"": ""0"",\n            ""mention_index"": -1,\n            ""mention_ner"": ""PERSON"",\n            ""mention_type"": ""HUM"",\n            ""predicted_coref_chain"": None,\n            ""score"": -1.0,\n            ""sent_id"": 0,\n            ""tokens_number"": [3, 4],\n            ""tokens_str"": ""Tara Reid"",\n            ""topic_id"": ""1ecb"",\n        },\n        {\n            ""coref_chain"": ""HUM16236184328979740"",\n            ""doc_id"": ""1_12ecb.xml"",\n            ""mention_context"": [\n                ""Tara"",\n                ""Reid"",\n                ""has"",\n                ""checked"",\n                ""into"",\n                ""Promises"",\n                ""Treatment"",\n                ""Center"",\n                "","",\n                ""a"",\n                ""prominent"",\n                ""rehab"",\n                ""clinic"",\n                ""in"",\n                ""Los"",\n                ""Angeles"",\n                ""."",\n            ],\n            ""mention_head"": ""Reid"",\n            ""mention_head_lemma"": ""reid"",\n            ""mention_head_pos"": ""PROPN"",\n            ""mention_id"": ""1"",\n            ""mention_index"": -1,\n            ""mention_ner"": ""PERSON"",\n            ""mention_type"": ""HUM"",\n            ""predicted_coref_chain"": None,\n            ""score"": -1.0,\n            ""sent_id"": 1,\n            ""tokens_number"": [0, 1],\n            ""tokens_str"": ""Tara Reid"",\n            ""topic_id"": ""1ecb"",\n        },\n        {\n            ""coref_chain"": ""Singleton_LOC_8_1_12ecb"",\n            ""doc_id"": ""1_12ecb.xml"",\n            ""mention_context"": [\n                ""Tara"",\n                ""Reid"",\n                ""has"",\n                ""checked"",\n                ""into"",\n                ""Promises"",\n                ""Treatment"",\n                ""Center"",\n                "","",\n                ""a"",\n                ""prominent"",\n                ""rehab"",\n                ""clinic"",\n                ""in"",\n                ""Los"",\n                ""Angeles"",\n                ""."",\n            ],\n            ""mention_head"": ""in"",\n            ""mention_head_lemma"": ""in"",\n            ""mention_head_pos"": ""ADP"",\n            ""mention_id"": ""2"",\n            ""mention_index"": -1,\n            ""mention_ner"": None,\n            ""mention_type"": ""LOC"",\n            ""predicted_coref_chain"": None,\n            ""score"": -1.0,\n            ""sent_id"": 1,\n            ""tokens_number"": [13, 14, 15],\n            ""tokens_str"": ""in Los Angeles"",\n            ""topic_id"": ""1ecb"",\n        },\n        {\n            ""coref_chain"": ""HUM16236184328979740"",\n            ""doc_id"": ""0"",\n            ""mention_context"": None,\n            ""mention_head"": ""Reid"",\n            ""mention_head_lemma"": ""reid"",\n            ""mention_head_pos"": ""PROPN"",\n            ""mention_id"": ""3"",\n            ""mention_ner"": ""PERSON"",\n            ""mention_type"": ""HUM"",\n            ""predicted_coref_chain"": None,\n            ""score"": -1.0,\n            ""sent_id"": 0,\n            ""tokens_number"": [3, 4],\n            ""tokens_str"": ""Tara Reid"",\n            ""topic_id"": ""1ecb"",\n        },\n    ]\n\n    mentions = list()\n    for json in mentions_json:\n        mentions.append(MentionData.read_json_mention_data_line(json))\n\n    return mentions\n\n\ndef get_wiki_mentions():\n    mentions_json = [\n        {""mention_id"": ""0"", ""tokens_str"": ""Ellen DeGeneres"", ""topic_id"": ""1ecb""},\n        {""mention_id"": ""1"", ""tokens_str"": ""television host"", ""topic_id"": ""1ecb""},\n        {""mention_id"": ""2"", ""tokens_str"": ""Los Angeles"", ""topic_id"": ""1ecb""},\n    ]\n\n    mentions = list()\n    for json in mentions_json:\n        mentions.append(MentionData.read_json_mention_data_line(json))\n\n    return mentions\n\n\ndef get_compute_mentions():\n    mentions_json = [\n        {""mention_id"": ""0"", ""tokens_str"": ""Exact String"", ""topic_id"": ""1ecb""},\n        {""mention_id"": ""1"", ""tokens_str"": ""Exact Same Head String"", ""topic_id"": ""1ecb""},\n        {""mention_id"": ""2"", ""tokens_str"": ""Nothing"", ""topic_id"": ""1ecb""},\n    ]\n\n    mentions = list()\n    for json in mentions_json:\n        mentions.append(MentionData.read_json_mention_data_line(json))\n\n    return mentions\n\n\ndef get_wordnet_mentions():\n    mentions_json = [\n        {""mention_id"": ""0"", ""tokens_str"": ""play"", ""topic_id"": ""1ecb""},\n        {""mention_id"": ""1"", ""tokens_str"": ""game"", ""topic_id"": ""1ecb""},\n        {""mention_id"": ""2"", ""tokens_str"": ""Chair"", ""topic_id"": ""1ecb""},\n    ]\n\n    mentions = list()\n    for json in mentions_json:\n        mentions.append(MentionData.read_json_mention_data_line(json))\n\n    return mentions\n'"
examples/absa/inference/__init__.py,0,b''
examples/absa/inference/inference.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom os import path\nfrom pathlib import Path\n\nfrom nlp_architect.models.absa.inference.inference import SentimentInference\n\n\ndef main() -> list:\n    parent_dir = Path(path.realpath(__file__)).parent.parent\n    inference = SentimentInference(parent_dir / ""aspects.csv"", parent_dir / ""opinions.csv"")\n\n    print(""\\n"" + ""="" * 40 + ""\\n"" + ""Running inference on examples from sample test set:\\n"")\n\n    docs = [\n        ""The food was very fresh and flavoursome the service was very attentive. Would go back""\n        "" to this restaurant if visiting London again."",\n        ""The food was wonderful and fresh, I really enjoyed it and will definitely go back. ""\n        ""Staff were friendly."",\n        ""The ambiance is charming. Uncharacteristically, the service was DREADFUL. When we""\n        "" wanted to pay our bill at the end of the evening, our waitress was nowhere to be ""\n        ""found..."",\n    ]\n\n    sentiment_docs = []\n\n    for doc_raw in docs:\n        print(""Raw Document: \\n{}"".format(doc_raw))\n        sentiment_doc = inference.run(doc=doc_raw)\n        sentiment_docs.append(sentiment_doc)\n        print(""SentimentDocument: \\n{}\\n"".format(sentiment_doc) + ""="" * 40 + ""\\n"")\n    return sentiment_docs\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/absa/inference/interactive.py,0,"b'import argparse\n\nfrom nlp_architect.models.absa.inference.inference import SentimentInference\nfrom nlp_architect.utils.io import validate_existing_path\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=""ABSA Inference"")\n    parser.add_argument(\n        ""--aspects"", type=validate_existing_path, help=""Path to aspect lexicon (csv)"", required=True\n    )\n    parser.add_argument(\n        ""--opinions"",\n        type=validate_existing_path,\n        required=True,\n        help=""Path to opinion lexicon (csv)"",\n    )\n    args = parser.parse_args()\n\n    inference = SentimentInference(aspect_lex=args.aspects, opinion_lex=args.opinions)\n\n    while True:\n        doc = input(""\\nEnter sentence >> "")\n        print(inference.run(doc))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/absa/solution/__init__.py,0,b''
examples/absa/solution/absa_solution.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport base64\nimport io\nimport os\nimport json\nfrom os.path import join\n\nimport pandas as pd\nimport numpy as np\nfrom bokeh import layouts\nfrom bokeh.document import Document\nfrom bokeh.layouts import widgetbox, row, column, layout\nfrom bokeh.models import Div, CustomJS, HoverTool, Title, ColumnDataSource\nfrom bokeh.models.widgets import DataTable, TableColumn, TextInput\nfrom bokeh.models.widgets import HTMLTemplateFormatter, RadioButtonGroup\nfrom bokeh.models.widgets import Dropdown\nfrom bokeh.models.widgets import Tabs, Panel\nfrom bokeh.plotting import Figure, figure\nfrom bokeh.server.server import Server\nfrom bokeh.transform import dodge\nfrom bokeh.core.properties import value\nfrom tornado.web import StaticFileHandler\n\nfrom nlp_architect import LIBRARY_PATH\nfrom nlp_architect.models.absa import LEXICONS_OUT\nfrom nlp_architect.models.absa.train.acquire_terms import AcquireTerms\nfrom nlp_architect.models.absa.train.train import TrainSentiment\nfrom nlp_architect.models.absa.inference.data_types import SentimentDoc, SentimentSentence\nfrom sentiment_solution import SENTIMENT_OUT, SentimentSolution\n\nSOLUTION_DIR = join(LIBRARY_PATH, ""solutions/absa_solution/"")\nPOLARITIES = (""POS"", ""NEG"")\n\n\n# pylint: disable=global-variable-undefined\ndef serve_absa_ui() -> None:\n    """"""Main function for serving UI application.\n    """"""\n\n    def _doc_modifier(doc: Document) -> None:\n        grid = _create_ui_components()\n        doc.add_root(grid)\n\n    print(""Opening Bokeh application on http://localhost:5006/"")\n    server = Server(\n        {""/"": _doc_modifier},\n        websocket_max_message_size=5000 * 1024 * 1024,\n        extra_patterns=[\n            (\n                ""/style/(.*)"",\n                StaticFileHandler,\n                {""path"": os.path.normpath(join(SOLUTION_DIR, ""/style""))},\n            )\n        ],\n    )\n    server.start()\n    server.io_loop.add_callback(server.show, ""/"")\n    server.io_loop.start()\n\n\ndef _create_header(train_dropdown, inference_dropdown, text_status) -> layouts.Row:\n    """"""Utility function for creating and styling the header row in the UI layout.""""""\n\n    architect_logo = Div(\n        text=\'<a href=""http://nlp_architect.nervanasys.com""> <img border=""0"" \'\n        \'src=""style/nlp_architect.jpg"" width=""200""></a> by Intel\xc2\xae AI Lab\',\n        style={\n            ""margin-left"": ""500px"",\n            ""margin-top"": ""20px"",\n            ""font-size"": ""110%"",\n            ""text-align"": ""center"",\n        },\n    )\n    css_link = Div(\n        text=""<link rel=\'stylesheet\' type=\'text/css\' href=\'style/lexicon_manager.css\'>"",\n        style={""font-size"": ""0%""},\n    )\n\n    js_script = Div(text=""<input type=\'file\' id=\'inputOS\' hidden=\'true\'>"")\n\n    title = Div(\n        text=""ABSApp"",\n        style={\n            ""font-size"": ""300%"",\n            ""color"": ""royalblue"",\n            ""font-weight"": ""bold"",\n            ""margin-left"": ""500px"",\n        },\n    )\n\n    return row(\n        column(\n            row(children=[train_dropdown, lexicons_dropdown, inference_dropdown], width=500),\n            row(text_status),\n        ),\n        css_link,\n        js_script,\n        widgetbox(title, width=900, height=84),\n        widgetbox(architect_logo, width=400, height=84),\n    )\n\n\ndef empty_table(*headers):\n    return ColumnDataSource(data={header: 19 * [""""] for header in headers})\n\n\ndef _create_ui_components() -> (Figure, ColumnDataSource):  # pylint: disable=too-many-statements\n    global asp_table_source, asp_filter_src, op_table_source, op_filter_src\n    global stats, aspects, tabs, lexicons_dropdown\n    stats = pd.DataFrame(columns=[""Quantity"", ""Score""])\n    aspects = pd.Series([])\n\n    def new_col_data_src():\n        return ColumnDataSource({""file_contents"": [], ""file_name"": []})\n\n    large_text = HTMLTemplateFormatter(template=""""""<div><%= value %></div>"""""")\n\n    def data_column(title):\n        return TableColumn(\n            field=title, title=\'<span class=""header"">\' + title + ""</span>"", formatter=large_text\n        )\n\n    asp_table_columns = [\n        data_column(""Term""),\n        data_column(""Alias1""),\n        data_column(""Alias2""),\n        data_column(""Alias3""),\n    ]\n    op_table_columns = [data_column(""Term""), data_column(""Score""), data_column(""Polarity"")]\n\n    asp_table_source = empty_table(""Term"", ""Alias1"", ""Alias2"", ""Alias3"")\n    asp_filter_src = empty_table(""Term"", ""Alias1"", ""Alias2"", ""Alias3"")\n    asp_src = new_col_data_src()\n\n    op_table_source = empty_table(""Term"", ""Score"", ""Polarity"", ""Polarity"")\n    op_filter_src = empty_table(""Term"", ""Score"", ""Polarity"", ""Polarity"")\n    op_src = new_col_data_src()\n\n    asp_table = DataTable(\n        source=asp_table_source,\n        selectable=""checkbox"",\n        columns=asp_table_columns,\n        editable=True,\n        width=600,\n        height=500,\n    )\n    op_table = DataTable(\n        source=op_table_source,\n        selectable=""checkbox"",\n        columns=op_table_columns,\n        editable=True,\n        width=600,\n        height=500,\n    )\n\n    asp_examples_box = _create_examples_table()\n    op_examples_box = _create_examples_table()\n    asp_layout = layout([[asp_table, asp_examples_box]])\n    op_layout = layout([[op_table, op_examples_box]])\n    asp_tab = Panel(child=asp_layout, title=""Aspect Lexicon"")\n    op_tab = Panel(child=op_layout, title=""Opinion Lexicon"")\n    tabs = Tabs(tabs=[asp_tab, op_tab], width=700, css_classes=[""mytab""])\n\n    lexicons_menu = [(""Open"", ""open""), (""Save"", ""save"")]\n    lexicons_dropdown = Dropdown(\n        label=""Edit Lexicons"",\n        button_type=""success"",\n        menu=lexicons_menu,\n        width=140,\n        height=31,\n        css_classes=[""mybutton""],\n    )\n\n    train_menu = [(""Parsed Data"", ""parsed""), (""Raw Data"", ""raw"")]\n    train_dropdown = Dropdown(\n        label=""Extract Lexicons"",\n        button_type=""success"",\n        menu=train_menu,\n        width=162,\n        height=31,\n        css_classes=[""mybutton""],\n    )\n\n    inference_menu = [(""Parsed Data"", ""parsed""), (""Raw Data"", ""raw"")]\n    inference_dropdown = Dropdown(\n        label=""Classify"",\n        button_type=""success"",\n        menu=inference_menu,\n        width=140,\n        height=31,\n        css_classes=[""mybutton""],\n    )\n\n    text_status = TextInput(\n        value=""Select training data"", title=""Train Run Status:"", css_classes=[""statusText""]\n    )\n    text_status.visible = False\n\n    train_src = new_col_data_src()\n    infer_src = new_col_data_src()\n\n    with open(join(SOLUTION_DIR, ""dropdown.js"")) as f:\n        args = dict(\n            clicked=lexicons_dropdown,\n            asp_filter=asp_filter_src,\n            op_filter=op_filter_src,\n            asp_src=asp_src,\n            op_src=op_src,\n            tabs=tabs,\n            text_status=text_status,\n            train_src=train_src,\n            infer_src=infer_src,\n            train_clicked=train_dropdown,\n            infer_clicked=inference_dropdown,\n            opinion_lex_generic="""",\n        )\n        code = f.read()\n\n    args[""train_clicked""] = train_dropdown\n    train_dropdown.js_on_change(""value"", CustomJS(args=args, code=code))\n\n    args[""train_clicked""] = inference_dropdown\n    inference_dropdown.js_on_change(""value"", CustomJS(args=args, code=code))\n\n    args[""clicked""] = lexicons_dropdown\n    lexicons_dropdown.js_on_change(""value"", CustomJS(args=args, code=code))\n\n    def update_filter_source(table_source, filter_source):\n        df = table_source.to_df()\n        sel_inx = sorted(table_source.selected.indices)\n        df = df.iloc[sel_inx, 1:]\n        new_source = ColumnDataSource(df)\n        filter_source.data = new_source.data\n\n    def update_examples_box(data, examples_box, old, new):\n        examples_box.source.data = {""Examples"": []}\n        unselected = list(set(old) - set(new))\n        selected = list(set(new) - set(old))\n        if len(selected) <= 1 and len(unselected) <= 1:\n            examples_box.source.data.update(\n                {\n                    ""Examples"": [str(data.iloc[unselected[0], i]) for i in range(4, 24)]\n                    if len(unselected) != 0\n                    else [str(data.iloc[selected[0], i]) for i in range(4, 24)]\n                }\n            )\n\n    def asp_selected_change(_, old, new):\n        global asp_filter_src, asp_table_source, aspects_data\n        update_filter_source(asp_table_source, asp_filter_src)\n        update_examples_box(aspects_data, asp_examples_box, old, new)\n\n    def op_selected_change(_, old, new):\n        global op_filter_src, op_table_source, opinions_data\n        update_filter_source(op_table_source, op_filter_src)\n        update_examples_box(opinions_data, op_examples_box, old, new)\n\n    def read_csv(file_src, headers=False, index_cols=False, readCSV=True):\n        if readCSV:\n            raw_contents = file_src.data[""file_contents""][0]\n\n            if len(raw_contents.split("","")) == 1:\n                b64_contents = raw_contents\n            else:\n                # remove the prefix that JS adds\n                b64_contents = raw_contents.split("","", 1)[1]\n            file_contents = base64.b64decode(b64_contents)\n            return pd.read_csv(\n                io.BytesIO(file_contents),\n                encoding=""ISO-8859-1"",\n                keep_default_na=False,\n                na_values={None},\n                engine=""python"",\n                index_col=index_cols,\n                header=0 if headers else None,\n            )\n        return file_src\n\n    def read_parsed_files(file_content, file_name):\n        try:\n            # remove the prefix that JS adds\n            b64_contents = file_content.split("","", 1)[1]\n            file_content = base64.b64decode(b64_contents)\n            with open(SENTIMENT_OUT / file_name, ""w"") as json_file:\n                data_dict = json.loads(file_content.decode(""utf-8""))\n                json.dump(data_dict, json_file)\n        except Exception as e:\n            print(str(e))\n\n    # pylint: disable=unused-argument\n    def train_file_callback(attr, old, new):\n        global train_data\n        SENTIMENT_OUT.mkdir(parents=True, exist_ok=True)\n        train = TrainSentiment(parse=True, rerank_model=None)\n        if len(train_src.data[""file_contents""]) == 1:\n            train_data = read_csv(train_src, index_cols=0)\n            file_name = train_src.data[""file_name""][0]\n            raw_data_path = SENTIMENT_OUT / file_name\n            train_data.to_csv(raw_data_path, header=False)\n            print(""Running_SentimentTraining on data..."")\n            train.run(data=raw_data_path)\n        else:\n            f_contents = train_src.data[""file_contents""]\n            f_names = train_src.data[""file_name""]\n            raw_data_path = SENTIMENT_OUT / train_src.data[""file_name""][0].split(""/"")[0]\n            if not os.path.exists(raw_data_path):\n                os.makedirs(raw_data_path)\n            for f_content, f_name in zip(f_contents, f_names):\n                read_parsed_files(f_content, f_name)\n            print(""Running_SentimentTraining on data..."")\n            train.run(parsed_data=raw_data_path)\n\n        text_status.value = ""Lexicon extraction completed""\n\n        with io.open(AcquireTerms.acquired_aspect_terms_path, ""r"") as fp:\n            aspect_data_csv = fp.read()\n        file_data = base64.b64encode(str.encode(aspect_data_csv))\n        file_data = file_data.decode(""utf-8"")\n        asp_src.data = {""file_contents"": [file_data], ""file_name"": [""nameFile.csv""]}\n\n        out_path = LEXICONS_OUT / ""generated_opinion_lex_reranked.csv""\n        with io.open(out_path, ""r"") as fp:\n            opinion_data_csv = fp.read()\n        file_data = base64.b64encode(str.encode(opinion_data_csv))\n        file_data = file_data.decode(""utf-8"")\n        op_src.data = {""file_contents"": [file_data], ""file_name"": [""nameFile.csv""]}\n\n    def show_analysis() -> None:\n        global stats, aspects, plot, source, tabs\n        plot, source = _create_plot()\n        events_table = _create_events_table()\n\n        # pylint: disable=unused-argument\n        def _events_handler(attr, old, new):\n            _update_events(events_table, events_type.active)\n\n        # Toggle display of in-domain / All aspect mentions\n        events_type = RadioButtonGroup(labels=[""All Events"", ""In-Domain Events""], active=0)\n\n        analysis_layout = layout([[plot], [events_table]])\n\n        # events_type display toggle disabled\n        # analysis_layout = layout([[plot],[events_type],[events_table]])\n\n        analysis_tab = Panel(child=analysis_layout, title=""Analysis"")\n        tabs.tabs.insert(2, analysis_tab)\n        tabs.active = 2\n        events_type.on_change(""active"", _events_handler)\n        source.selected.on_change(""indices"", _events_handler)  # pylint: disable=no-member\n\n    # pylint: disable=unused-argument\n    def infer_file_callback(attr, old, new):\n\n        # run inference on input data and current aspect/opinion lexicons in view\n        global infer_data, stats, aspects\n\n        SENTIMENT_OUT.mkdir(parents=True, exist_ok=True)\n\n        df_aspect = pd.DataFrame.from_dict(asp_filter_src.data)\n        aspect_col_list = [""Term"", ""Alias1"", ""Alias2"", ""Alias3""]\n        df_aspect = df_aspect[aspect_col_list]\n        df_aspect.to_csv(SENTIMENT_OUT / ""aspects.csv"", index=False, na_rep=""NaN"")\n\n        df_opinion = pd.DataFrame.from_dict(op_filter_src.data)\n        opinion_col_list = [""Term"", ""Score"", ""Polarity"", ""isAcquired""]\n        df_opinion = df_opinion[opinion_col_list]\n        df_opinion.to_csv(SENTIMENT_OUT / ""opinions.csv"", index=False, na_rep=""NaN"")\n\n        solution = SentimentSolution()\n\n        if len(infer_src.data[""file_contents""]) == 1:\n            infer_data = read_csv(infer_src, index_cols=0)\n            file_name = infer_src.data[""file_name""][0]\n            raw_data_path = SENTIMENT_OUT / file_name\n            infer_data.to_csv(raw_data_path, header=False)\n            print(""Running_SentimentInference on data..."")\n            text_status.value = ""Running classification on data...""\n            stats = solution.run(\n                data=raw_data_path,\n                aspect_lex=SENTIMENT_OUT / ""aspects.csv"",\n                opinion_lex=SENTIMENT_OUT / ""opinions.csv"",\n            )\n        else:\n            f_contents = infer_src.data[""file_contents""]\n            f_names = infer_src.data[""file_name""]\n            raw_data_path = SENTIMENT_OUT / infer_src.data[""file_name""][0].split(""/"")[0]\n            if not os.path.exists(raw_data_path):\n                os.makedirs(raw_data_path)\n            for f_content, f_name in zip(f_contents, f_names):\n                read_parsed_files(f_content, f_name)\n            print(""Running_SentimentInference on data..."")\n            text_status.value = ""Running classification on data...""\n            stats = solution.run(\n                parsed_data=raw_data_path,\n                aspect_lex=SENTIMENT_OUT / ""aspects.csv"",\n                opinion_lex=SENTIMENT_OUT / ""opinions.csv"",\n            )\n\n        aspects = pd.read_csv(SENTIMENT_OUT / ""aspects.csv"", encoding=""utf-8"")[""Term""]\n        text_status.value = ""Classification completed""\n        show_analysis()\n\n    # pylint: disable=unused-argument\n    def asp_file_callback(attr, old, new):\n        global aspects_data, asp_table_source\n        aspects_data = read_csv(asp_src, headers=True)\n        # Replaces None values by empty string\n        aspects_data = aspects_data.fillna("""")\n        new_source = ColumnDataSource(aspects_data)\n        asp_table_source.data = new_source.data\n        asp_table_source.selected.indices = list(range(len(aspects_data)))\n\n    # pylint: disable=unused-argument\n    def op_file_callback(attr, old, new):\n        global opinions_data, op_table_source, lexicons_dropdown, df_opinion_generic\n        df = read_csv(op_src, headers=True)\n        # Replaces None values by empty string\n        df = df.fillna("""")\n        # Placeholder for generic opinion lexicons from the given csv file\n        df_opinion_generic = df[df[""isAcquired""] == ""N""]\n        # Update the argument value for the callback customJS\n        lexicons_dropdown.js_property_callbacks.get(""change:value"")[0].args[\n            ""opinion_lex_generic""\n        ] = df_opinion_generic.to_dict(orient=""list"")\n        opinions_data = df[df[""isAcquired""] == ""Y""]\n        new_source = ColumnDataSource(opinions_data)\n        op_table_source.data = new_source.data\n        op_table_source.selected.indices = list(range(len(opinions_data)))\n\n    # pylint: disable=unused-argument\n    def txt_status_callback(attr, old, new):\n        print(""Previous label: "" + old)\n        print(""Updated label: "" + new)\n\n    text_status.on_change(""value"", txt_status_callback)\n\n    asp_src.on_change(""data"", asp_file_callback)\n    # pylint: disable=no-member\n    asp_table_source.selected.on_change(""indices"", asp_selected_change)\n\n    op_src.on_change(""data"", op_file_callback)\n    op_table_source.selected.on_change(""indices"", op_selected_change)  # pylint: disable=no-member\n\n    train_src.on_change(""data"", train_file_callback)\n    infer_src.on_change(""data"", infer_file_callback)\n\n    return layout([[_create_header(train_dropdown, inference_dropdown, text_status)], [tabs]])\n\n\ndef _create_events_table() -> DataTable:\n    """"""Utility function for creating and styling the events table.""""""\n    formatter = HTMLTemplateFormatter(\n        template=""""""\n    <style>\n        .AS_POS {color: #0000FF; font-weight: bold;}\n        .AS_NEG {color: #0000FF; font-weight: bold;}\n        .OP_POS {color: #1aaa0d; font-style: bold;}\n        .OP_NEG {color: #f40000;font-style: bold;}\n        .NEG_POS {font-style: italic;}\n        .NEG_NEG {color: #f40000; font-style: italic;}\n        .INT_POS {color: #1aaa0d; font-style: italic;}\n        .INT_NEG {color: #f40000; font-style: italic;}\n    </style>\n    <%= value %>""""""\n    )\n    columns = [\n        TableColumn(field=""POS_events"", title=""Positive Examples"", formatter=formatter),\n        TableColumn(field=""NEG_events"", title=""Negative Examples"", formatter=formatter),\n    ]\n    return DataTable(\n        source=ColumnDataSource(),\n        columns=columns,\n        height=400,\n        index_position=None,\n        width=2110,\n        sortable=False,\n        editable=True,\n        reorderable=False,\n    )\n\n\ndef _create_plot() -> (Figure, ColumnDataSource):\n    """"""Utility function for creating and styling the bar plot.""""""\n    global source, aspects, stats\n    pos_counts, neg_counts = (\n        [stats.loc[(asp, pol, False), ""Quantity""] for asp in aspects] for pol in POLARITIES\n    )\n    np.seterr(divide=""ignore"")\n    source = ColumnDataSource(\n        data={\n            ""aspects"": aspects,\n            ""POS"": pos_counts,\n            ""NEG"": neg_counts,\n            ""log-POS"": np.log2(pos_counts),\n            ""log-NEG"": np.log2(neg_counts),\n        }\n    )\n    np.seterr(divide=""warn"")\n    p = figure(\n        plot_height=145,\n        sizing_mode=""scale_width"",\n        x_range=aspects,\n        toolbar_location=""right"",\n        tools=""save, tap"",\n    )\n    rs = [\n        p.vbar(\n            x=dodge(""aspects"", -0.207, range=p.x_range),\n            top=""log-POS"",\n            width=0.4,\n            source=source,\n            color=""limegreen"",\n            legend=value(""POS""),\n            name=""POS"",\n        ),\n        p.vbar(\n            x=dodge(""aspects"", 0.207, range=p.x_range),\n            top=""log-NEG"",\n            width=0.4,\n            source=source,\n            color=""orangered"",\n            legend=value(""NEG""),\n            name=""NEG"",\n        ),\n    ]\n    for r in rs:\n        p.add_tools(\n            HoverTool(tooltips=[(""Aspect"", ""@aspects""), (r.name, ""@"" + r.name)], renderers=[r])\n        )\n    p.add_layout(\n        Title(text="" "" * 7 + ""Sentiment Count (log scale)"", align=""left"", text_font_size=""23px""),\n        ""left"",\n    )\n    p.yaxis.ticker = []\n    p.y_range.start = 0\n    p.xgrid.grid_line_color = None\n    p.xaxis.major_label_text_font_size = ""20pt""\n    p.legend.label_text_font_size = ""20pt""\n    return p, source\n\n\ndef _update_events(events: DataTable, in_domain: bool) -> None:\n    """"""Utility function for updating the content of the events table.""""""\n    i = source.selected.indices\n    events.source.data.update(\n        {\n            pol + ""_events"": stats.loc[aspects[i[0]], pol, in_domain][""Sent_1"":].replace(np.nan, """")\n            if i\n            else []\n            for pol in POLARITIES\n        }\n    )\n\n\ndef _ui_format(sent: SentimentSentence, doc: SentimentDoc) -> str:\n    """"""Get sentence as HTML with 4 classes: aspects, opinions, negations and intensifiers.""""""\n    text = doc.doc_text[sent.start : sent.end + 1]\n    seen = set()\n    for term in sorted([t for e in sent.events for t in e], key=lambda t: t.start)[::-1]:\n        if term.start not in seen:\n            seen.add(term.start)\n            start = term.start - sent.start\n            end = start + term.len\n            label = term.type.value + ""_"" + term.polarity.value\n            text = """".join(\n                (text[:start], \'<span class=""\', label, \'"">\', text[start:end], ""</span>"", text[end:])\n            )\n    return text\n\n\ndef _create_examples_table() -> DataTable:\n    """"""Utility function for creating and styling the events table.""""""\n\n    formatter = HTMLTemplateFormatter(\n        template=""""""\n    <style>\n        .AS {color: #0000FF; font-weight: bold;}\n        .OP {color: #0000FF; font-weight: bold;}\n    </style>\n    <div><%= value %></div>""""""\n    )\n    columns = [\n        TableColumn(\n            field=""Examples"", title=\'<span class=""header"">Examples</span>\', formatter=formatter\n        )\n    ]\n    empty_source = ColumnDataSource()\n    empty_source.data = {""Examples"": []}\n    return DataTable(\n        source=empty_source,\n        columns=columns,\n        height=500,\n        index_position=None,\n        width=1500,\n        sortable=False,\n        editable=False,\n        reorderable=False,\n        header_row=True,\n    )\n\n\nif __name__ == ""__main__"":\n    serve_absa_ui()\n'"
examples/absa/train/__init__.py,0,b''
examples/absa/train/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nfrom os import path\nfrom pathlib import Path\n\nfrom nlp_architect.models.absa.train.train import TrainSentiment\nfrom nlp_architect.utils.io import (\n    validate_existing_filepath,\n    validate_existing_path,\n    validate_existing_directory,\n)\n\n\ndef main() -> None:\n    lib_root = Path(path.realpath(__file__)).parent.parent.parent.parent\n    tripadvisor_train = (\n        lib_root\n        / ""datasets""\n        / ""absa""\n        / ""tripadvisor_co_uk-travel_restaurant_reviews_sample_2000_train.csv""\n    )\n\n    parser = argparse.ArgumentParser(description=""ABSA Train"")\n    parser.add_argument(\n        ""--rerank-model"",\n        type=validate_existing_filepath,\n        default=None,\n        help=""Path to rerank model .h5 file"",\n    )\n\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\n        ""--data"",\n        type=validate_existing_path,\n        default=tripadvisor_train,\n        help=""Path to raw data (directory or txt/csv file)"",\n    )\n    group.add_argument(\n        ""--parsed-data"",\n        type=validate_existing_directory,\n        default=None,\n        help=""Path to parsed data directory"",\n    )\n    args = parser.parse_args()\n\n    train = TrainSentiment(parse=not args.parsed_data, rerank_model=args.rerank_model)\n    opinion_lex, aspect_lex = train.run(data=args.data, parsed_data=args.parsed_data)\n\n    print(""Aspect Lexicon: {}\\n"".format(aspect_lex) + ""="" * 40 + ""\\n"")\n    print(""Opinion Lexicon: {}"".format(opinion_lex))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/reading_comprehension/match_lstm_mrc/__init__.py,0,b''
examples/reading_comprehension/match_lstm_mrc/machine_comprehension_api.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport zipfile\nfrom os import makedirs\nfrom random import shuffle\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nlp_architect import LIBRARY_OUT\nfrom nlp_architect.api.abstract_api import AbstractApi\nfrom nlp_architect.utils.generic import license_prompt\nfrom nlp_architect.utils.io import download_unlicensed_file\nfrom .matchlstm_ansptr import MatchLSTMAnswerPointer\nfrom .mrc_utils import create_squad_training, max_values_squad, get_data_array_squad\n\n\nclass MachineComprehensionApi(AbstractApi):\n    """"""\n    Machine Comprehension API\n    """"""\n\n    dir = str(LIBRARY_OUT / ""mrc-pretrained"")\n    data_path = os.path.join(dir, ""mrc_data"", ""data"")\n    data_dir = os.path.join(dir, ""mrc_data"")\n    model_dir = os.path.join(dir, ""mrc_trained_model"")\n    model_path = os.path.join(dir, ""mrc_trained_model"", ""trained_model"")\n\n    def __init__(self, prompt=True):\n        self.prompt = None\n        self.vocab_dict = None\n        self.vocab_rev = None\n        self.model = None\n        self.dev = None\n        self.sess = None\n        self.prompt = prompt\n        self.params_dict = {\n            ""batch_size"": 1,\n            ""hidden_size"": 150,\n            ""max_para"": 300,\n            ""epoch_no"": 15,\n            ""inference_only"": True,\n        }\n        self.file_name_dict = {\n            ""train_para_ids"": ""train.ids.context"",\n            ""train_ques_ids"": ""train.ids.question"",\n            ""train_answer"": ""train.span"",\n            ""val_para_ids"": ""dev.ids.context"",\n            ""val_ques_ids"": ""dev.ids.question"",\n            ""val_ans"": ""dev.span"",\n            ""vocab_file"": ""vocab.dat"",\n            ""embedding"": ""glove.trimmed.300.npz"",\n        }\n\n    def download_model(self):\n        # Validate contents of data_path folder:\n        data_path = self.data_path\n        download = False\n        for file_name in self.file_name_dict.values():\n            if not os.path.exists(os.path.join(data_path, file_name)):\n                # prompt\n                download = True\n                print(""The following required file is missing :"", file_name)\n\n        if download is True:\n            if self.prompt is True:\n                license_prompt(\n                    ""mrc_data"",\n                    ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/models/mrc""\n                    ""/mrc_data.zip"",\n                    self.data_dir,\n                )\n                license_prompt(\n                    ""mrc_model"",\n                    ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/models/mrc""\n                    ""/mrc_model.zip"",\n                    self.model_dir,\n                )\n            data_zipfile = os.path.join(self.data_dir, ""mrc_data.zip"")\n            model_zipfile = os.path.join(self.model_dir, ""mrc_model.zip"")\n            makedirs(self.data_dir, exist_ok=True)\n            makedirs(self.model_dir, exist_ok=True)\n            download_unlicensed_file(\n                ""https://s3-us-west-2.amazonaws.com/nlp-architect-data"" ""/models/mrc/"",\n                ""mrc_data.zip"",\n                data_zipfile,\n            )\n            download_unlicensed_file(\n                ""https://s3-us-west-2.amazonaws.com/nlp-architect-data"" ""/models/mrc/"",\n                ""mrc_model.zip"",\n                model_zipfile,\n            )\n            with zipfile.ZipFile(data_zipfile) as data_zip_ref:\n                data_zip_ref.extractall(self.data_dir)\n            with zipfile.ZipFile(model_zipfile) as model_zip_ref:\n                model_zip_ref.extractall(self.model_dir)\n\n    def load_model(self):\n        select_device = ""GPU""\n        restore_model = True\n        # Create dictionary of filenames\n        self.download_model()\n\n        data_path = self.data_path\n        # Paths for preprcessed files\n        path_gen = data_path  # data is actually in mrc_data/data not, mrc_data\n        train_para_ids = os.path.join(path_gen, self.file_name_dict[""train_para_ids""])\n        train_ques_ids = os.path.join(path_gen, self.file_name_dict[""train_ques_ids""])\n        answer_file = os.path.join(path_gen, self.file_name_dict[""train_answer""])\n        val_paras_ids = os.path.join(path_gen, self.file_name_dict[""val_para_ids""])\n        val_ques_ids = os.path.join(path_gen, self.file_name_dict[""val_ques_ids""])\n        val_ans_file = os.path.join(path_gen, self.file_name_dict[""val_ans""])\n        vocab_file = os.path.join(path_gen, self.file_name_dict[""vocab_file""])\n\n        model_dir = self.model_path\n        # Create model dir if it doesn\'t exist\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n\n        model_path = model_dir\n\n        # Create lists for train and validation sets\n        data_train = create_squad_training(train_para_ids, train_ques_ids, answer_file)\n        data_dev = create_squad_training(val_paras_ids, val_ques_ids, val_ans_file)\n        with open(vocab_file, encoding=""UTF-8"") as fp:\n            vocab_list = fp.readlines()\n        self.vocab_dict = {}\n        self.vocab_rev = {}\n\n        for i in range(len(vocab_list)):\n            self.vocab_dict[i] = vocab_list[i].strip()\n            self.vocab_rev[vocab_list[i].strip()] = i\n\n            self.params_dict[""train_set_size""] = len(data_train)\n\n        # Combine train and dev data\n        data_total = data_train + data_dev\n\n        # obtain maximum length of question\n        _, max_question = max_values_squad(data_total)\n        self.params_dict[""max_question""] = max_question\n\n        # Load embeddings for vocab\n        print(""Loading Embeddings"")\n        embeddingz = np.load(os.path.join(path_gen, self.file_name_dict[""embedding""]))\n        embeddings = embeddingz[""glove""]\n\n        # Create train and dev sets\n        print(""Creating training and development sets"")\n        self.dev = get_data_array_squad(self.params_dict, data_dev, set_val=""val"")\n\n        # Define Reading Comprehension model\n        with tf.device(""/device:"" + select_device + "":0""):\n            self.model = MatchLSTMAnswerPointer(self.params_dict, embeddings)\n\n        # Define Configs for training\n        run_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\n        # Create session run training\n        self.sess = tf.Session(config=run_config)\n        init = tf.global_variables_initializer()\n\n        # Model Saver\n        # pylint: disable=no-member\n        model_saver = tf.train.Saver()\n        model_ckpt = tf.train.get_checkpoint_state(model_path)\n        idx_path = model_ckpt.model_checkpoint_path + "".index"" if model_ckpt else """"\n\n        # Initialize with random or pretrained weights\n        # pylint: disable=no-member\n        if (\n            model_ckpt\n            and restore_model\n            and (tf.gfile.Exists(model_ckpt.model_checkpoint_path) or tf.gfile.Exists(idx_path))\n        ):\n            model_saver.restore(self.sess, model_ckpt.model_checkpoint_path)\n            print(""Loading from previously stored session"")\n        else:\n            self.sess.run(init)\n\n        shuffle(self.dev)\n\n    @staticmethod\n    def paragraphs(valid, vocab_tuple, num_examples):\n        paragraphs = []\n        vocab_forward = vocab_tuple[0]\n        for idx in range(num_examples):\n            test_paragraph = [vocab_forward[ele] for ele in valid[idx][0] if ele != 0]\n            para_string = "" "".join(map(str, test_paragraph))\n            paragraphs.append(re.sub(r\'\\s([?.!,""](?:\\s|$))\', r""\\1"", para_string))  # (?:\\s|$))\n        return paragraphs\n\n    @staticmethod\n    def questions(valid, vocab_tuple, num_examples):\n        vocab_forward = vocab_tuple[0]\n        questions = []\n        for idx in range(num_examples):\n            test_question = [vocab_forward[ele] for ele in valid[idx][1] if ele != 0]\n            ques_string = "" "".join(map(str, test_question))\n            questions.append(re.sub(r\'\\s([?.!"""",])\', r""\\1"", ques_string))\n        return questions\n\n    def inference(self, doc):\n        body = doc\n        print(""Begin Inference Mode"")\n        question = body[""question""]\n        paragraph_id = body[""paragraph""]\n        return self.model.inference_mode(\n            self.sess,\n            self.dev,\n            [self.vocab_dict, self.vocab_rev],\n            dynamic_question_mode=True,\n            num_examples=1,\n            dropout=1.0,\n            dynamic_usr_question=question,\n            dynamic_question_index=paragraph_id,\n        )\n\n    def get_paragraphs(self):\n        ret = {\n            ""paragraphs"": self.paragraphs(\n                self.dev, [self.vocab_dict, self.vocab_rev], num_examples=5\n            ),\n            ""questions"": self.questions(\n                self.dev, [self.vocab_dict, self.vocab_rev], num_examples=5\n            ),\n        }\n        return ret\n'"
examples/reading_comprehension/match_lstm_mrc/matchlstm_ansptr.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nfrom collections import Counter\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nlp_architect.utils.text import SpacyInstance\n\n\nclass MatchLSTMAnswerPointer(object):\n    """"""\n    Defines end to end MatchLSTM and Answer_Pointer network for Reading Comprehension\n    """"""\n\n    def __init__(self, params_dict, embeddings):\n        """"""\n        Args:\n            params_dict: Dictionary containing the following keys-\n                         \'max_question\' : max length of all questions in the dataset\n                         \'max_para\' :  max length of all paragraphs in the dataset\n                         \'hidden_size\': number of hidden units in the network\n                         \'batch_size\' : batch size defined by user\n\n            embeddings: Glove pretrained embedding matrix\n        """"""\n\n        # Assign Variables:\n        self.max_question = params_dict[""max_question""]\n        self.max_para = params_dict[""max_para""]\n        self.hidden_size = params_dict[""hidden_size""]\n        self.batch_size = params_dict[""batch_size""]\n        self.embeddings = embeddings\n        self.inference_only = params_dict[""inference_only""]\n        self.G_i = None\n        self.attn = None\n        self.stacked_lists_forward = None\n        self.stacked_lists_reverse = None\n        self.logits_withsf = None\n\n        # init tokenizer\n        self.tokenizer = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n\n        # Create Placeholders\n        # Question ids\n        self.question_ids = tf.placeholder(\n            tf.int32, shape=[None, self.max_question], name=""question_ids""\n        )\n        # Paragraph ids\n        self.para_ids = tf.placeholder(tf.int32, shape=[None, self.max_para], name=""para_ids"")\n        # Length of question\n        self.question_length = tf.placeholder(tf.int32, shape=[None], name=""question_len"")\n        # Length of paragraph\n        self.para_length = tf.placeholder(tf.int32, shape=[None], name=""para_len"")\n        # Mask for paragraph\n        self.para_mask = tf.placeholder(tf.float32, shape=[None, self.max_para], name=""para_mask"")\n        # Mask for question\n        self.ques_mask = tf.placeholder(\n            tf.float32, shape=[None, self.max_question], name=""ques_mask""\n        )\n        # Answer spans\n        if self.inference_only is False:\n            self.labels = tf.placeholder(tf.int32, shape=[None, 2], name=""labels"")\n        # Dropout value\n        self.dropout = tf.placeholder(tf.float32, shape=[], name=""dropout"")\n        self.global_step = tf.Variable(0, name=""global"")\n\n        # Get variables\n        self.create_variables()\n\n        # Define model\n        self.create_model()\n\n    def create_variables(self):\n        """"""\n        Function to create variables used for training\n\n        """"""\n        # define all variables required for training\n        self.W_p = tf.get_variable(""W_p"", [1, self.hidden_size, self.hidden_size])\n        self.W_r = tf.get_variable(""W_r"", [1, self.hidden_size, self.hidden_size])\n        self.W_q = tf.get_variable(""W_q"", [1, self.hidden_size, self.hidden_size])\n        self.w_lr = tf.get_variable(""w_lr"", [1, 1, self.hidden_size])\n        self.b_p = tf.get_variable(""b_p"", [1, self.hidden_size, 1])\n        self.c_p = tf.get_variable(""c_p"", [1])\n        self.ones_vector = tf.constant(np.ones([1, self.max_question]), dtype=tf.float32)\n\n        self.ones_vector_exp = tf.tile(tf.expand_dims(self.ones_vector, 0), [self.batch_size, 1, 1])\n\n        self.ones_vector_para = tf.constant(np.ones([1, self.max_para]), dtype=tf.float32)\n\n        self.ones_para_exp = tf.tile(\n            tf.expand_dims(self.ones_vector_para, 0), [self.batch_size, 1, 1]\n        )\n\n        self.ones_embed = tf.tile(\n            tf.expand_dims(tf.constant(np.ones([1, self.hidden_size]), dtype=tf.float32), 0),\n            [self.batch_size, 1, 1],\n        )\n\n        self.V_r = tf.get_variable(""V_r"", [1, self.hidden_size, 2 * self.hidden_size])\n        self.W_a = tf.get_variable(""W_a"", [1, self.hidden_size, self.hidden_size])\n        self.b_a = tf.get_variable(""b_a"", [1, self.hidden_size, 1])\n        self.v_a_pointer = tf.get_variable(""v_a_pointer"", [1, 1, self.hidden_size])\n        self.c_pointer = tf.get_variable(""c_pointer"", [1, 1, 1])\n        self.Wans_q = tf.get_variable(""Wans_q"", [1, self.hidden_size, self.hidden_size])\n        self.Wans_v = tf.get_variable(""Wans_v"", [1, self.hidden_size, self.hidden_size])\n        self.Vans_r = tf.get_variable(""Vans_r"", [1, self.hidden_size, self.max_question])\n\n        self.mask_ques_mul = tf.matmul(\n            tf.transpose(self.ones_embed, [0, 2, 1]), tf.expand_dims(self.ques_mask, 1)\n        )\n\n    def create_model(self):\n        """"""\n        Function to set up the end 2 end reading comprehension model\n\n        """"""\n        # Embedding Layer\n        embedding_lookup = tf.Variable(\n            self.embeddings, name=""word_embeddings"", dtype=tf.float32, trainable=False\n        )\n        # Embedding Lookups\n        self.question_emb = tf.nn.embedding_lookup(\n            embedding_lookup, self.question_ids, name=""question_embed""\n        )\n\n        self.para_emb = tf.nn.embedding_lookup(embedding_lookup, self.para_ids, name=""para_embed"")\n\n        # Apply dropout after embeddings\n        self.question = tf.nn.dropout(self.question_emb, self.dropout)\n        self.para = tf.nn.dropout(self.para_emb, self.dropout)\n\n        # Encoding Layer\n        # Share weights of pre-processing LSTM layer with both para and\n        # question\n        with tf.variable_scope(""encoded_question""):\n            self.lstm_cell_question = tf.nn.rnn_cell.BasicLSTMCell(\n                self.hidden_size, state_is_tuple=True\n            )\n\n            self.encoded_question, _ = tf.nn.dynamic_rnn(\n                self.lstm_cell_question, self.question, self.question_length, dtype=tf.float32\n            )\n\n        with tf.variable_scope(""encoded_para""):\n            self.encoded_para, _ = tf.nn.dynamic_rnn(\n                self.lstm_cell_question, self.para, self.para_length, dtype=tf.float32\n            )\n\n        # Define Match LSTM and Answer Pointer Cells\n        with tf.variable_scope(""match_lstm_cell""):\n            self.match_lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(\n                self.hidden_size, state_is_tuple=True\n            )\n\n        with tf.variable_scope(""answer_pointer_cell""):\n            self.lstm_pointer_cell = tf.nn.rnn_cell.BasicLSTMCell(\n                self.hidden_size, state_is_tuple=True\n            )\n\n        print(""Match LSTM Pass"")\n        # Match LSTM Pass in forward direction\n        self.unroll_with_attention(reverse=False)\n        self.encoded_para_reverse = tf.reverse(self.encoded_para, axis=[1])\n        # Match LSTM Pass in reverse direction\n        self.unroll_with_attention(reverse=True)\n        # Apply dropout\n        self.stacked_lists = tf.concat(\n            [\n                tf.nn.dropout(self.stacked_lists_forward, tf.maximum(self.dropout, 0.8)),\n                tf.nn.dropout(self.stacked_lists_reverse, tf.maximum(self.dropout, 0.8)),\n            ],\n            1,\n        )\n\n        # Answer pointer pass\n        self.logits = self.answer_pointer_pass()\n        if self.inference_only is False:\n            print(""Settting up Loss"")\n            # Compute Losses\n            loss_1 = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=self.logits[0], labels=self.labels[:, 0]\n            )\n\n            loss_2 = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=self.logits[1], labels=self.labels[:, 1]\n            )\n            # Total Loss\n            self.loss = tf.reduce_mean(loss_1 + loss_2)\n            self.learning_rate = tf.constant(0.002)\n\n            print(""Set up optmizer"")\n            # Optmizer\n            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(\n                self.loss, global_step=self.global_step\n            )\n\n    def unroll_with_attention(self, reverse=False):\n        """"""\n        Function to run the match_lstm pass in both forward and reverse directions\n\n        Args:\n        reverse: Boolean indicating whether to unroll in reverse directions\n\n        """"""\n        # Intitialze first hidden_state with zeros\n        h_r_old = tf.constant(np.zeros([self.batch_size, self.hidden_size, 1]), dtype=tf.float32)\n        final_state_list = []\n\n        for i in range(self.max_para):\n\n            if not reverse:\n                encoded_paraslice = tf.gather(self.encoded_para, indices=i, axis=1)\n            else:\n                encoded_paraslice = tf.gather(self.encoded_para_reverse, indices=i, axis=1)\n\n            W_p_expanded = tf.tile(self.W_p, [self.batch_size, 1, 1])\n            W_q_expanded = tf.tile(self.W_q, [self.batch_size, 1, 1])\n            W_r_expanded = tf.tile(self.W_r, [self.batch_size, 1, 1])\n            w_lr_expanded = tf.tile(self.w_lr, [self.batch_size, 1, 1])\n            b_p_expanded = tf.tile(self.b_p, [self.batch_size, 1, 1])\n\n            int_sum = (\n                tf.matmul(W_p_expanded, tf.expand_dims(encoded_paraslice, 2))\n                + tf.matmul(W_r_expanded, h_r_old)\n                + b_p_expanded\n            )\n\n            int_sum_new = tf.matmul(int_sum, tf.expand_dims(self.ques_mask, 1))\n\n            int_sum1 = tf.matmul(W_q_expanded, tf.transpose(self.encoded_question, [0, 2, 1]))\n\n            self.G_i = tf.nn.tanh(int_sum_new + int_sum1) + tf.expand_dims(\n                self.c_p * self.ques_mask, 1\n            )\n\n            # Attention Vector\n            self.attn = tf.nn.softmax(tf.matmul(w_lr_expanded, self.G_i))\n\n            z1 = encoded_paraslice\n\n            z2 = tf.squeeze(\n                tf.matmul(\n                    tf.transpose(self.encoded_question, [0, 2, 1]),\n                    tf.transpose(self.attn, [0, 2, 1]),\n                ),\n                axis=2,\n            )\n\n            z_i_stacked = tf.concat([z1, z2], 1)\n            if i == 0:\n                h_r_old, cell_state_old = self.match_lstm_cell(\n                    z_i_stacked,\n                    state=self.match_lstm_cell.zero_state(self.batch_size, dtype=tf.float32),\n                )\n            else:\n                h_r_old, cell_state_old = self.match_lstm_cell(z_i_stacked, state=cell_state_old)\n\n            final_state_list.append(h_r_old)\n            h_r_old = tf.expand_dims(h_r_old, 2)\n            stacked_lists = tf.stack(final_state_list, 1)\n\n        if not reverse:\n            # Mask Output\n            mask_mult_lstm_forward = tf.matmul(\n                tf.transpose(self.ones_embed, [0, 2, 1]), tf.expand_dims(self.para_mask, 1)\n            )\n\n            self.stacked_lists_forward = tf.multiply(\n                tf.transpose(stacked_lists, [0, 2, 1]), mask_mult_lstm_forward\n            )\n        else:\n            # Mask Output\n            mask_mult_lstm_reverse = tf.matmul(\n                tf.transpose(self.ones_embed, [0, 2, 1]),\n                tf.expand_dims(tf.reverse(self.para_mask, axis=[1]), 1),\n            )\n\n            self.stacked_lists_reverse = tf.reverse(\n                tf.multiply(tf.transpose(stacked_lists, [0, 2, 1]), mask_mult_lstm_reverse),\n                axis=[2],\n            )\n\n    def answer_pointer_pass(self):\n        """"""\n        Function to run the answer pointer pass:\n\n        Args:\n            None\n\n        Returns:\n            List of logits for start and end indices of the answer\n        """"""\n\n        V_r_expanded = tf.tile(self.V_r, [self.batch_size, 1, 1])\n        W_a_expanded = tf.tile(self.W_a, [self.batch_size, 1, 1])\n        b_a_expanded = tf.tile(self.b_a, [self.batch_size, 1, 1])\n        mask_multiplier_1 = tf.expand_dims(self.para_mask, 1)\n        mask_multiplier = self.ones_para_exp\n\n        v_apointer_exp = tf.tile(self.v_a_pointer, [self.batch_size, 1, 1])\n\n        # Zero initialization\n        h_k_old = tf.constant(np.zeros([self.batch_size, self.hidden_size, 1]), dtype=tf.float32)\n\n        b_k_lists = []\n\n        print(""Answer Pointer Pass"")\n\n        for i in range(0, 2):\n            sum1 = tf.matmul(V_r_expanded, self.stacked_lists)\n            sum2 = tf.matmul(W_a_expanded, h_k_old) + b_a_expanded\n            F_k = tf.nn.tanh(sum1 + tf.matmul(sum2, mask_multiplier))\n\n            b_k_withoutsf = tf.matmul(v_apointer_exp, F_k)\n\n            b_k = tf.nn.softmax(b_k_withoutsf + tf.log(mask_multiplier_1))\n            lstm_cell_inp = tf.squeeze(\n                tf.matmul(self.stacked_lists, tf.transpose(b_k, [0, 2, 1])), axis=2\n            )\n\n            with tf.variable_scope(""lstm_pointer""):\n                if i == 0:\n                    h_k_old, cell_state_pointer = self.lstm_pointer_cell(\n                        lstm_cell_inp,\n                        state=self.lstm_pointer_cell.zero_state(self.batch_size, dtype=tf.float32),\n                    )\n                else:\n                    h_k_old, cell_state_pointer = self.lstm_pointer_cell(\n                        lstm_cell_inp, state=cell_state_pointer\n                    )\n\n            h_k_old = tf.expand_dims(h_k_old, 2)\n            b_k_lists.append(b_k_withoutsf + tf.log(mask_multiplier_1))\n\n        self.logits_withsf = [\n            tf.nn.softmax(tf.squeeze(b_k_lists[0], axis=1)),\n            tf.nn.softmax(tf.squeeze(b_k_lists[1], axis=1)),\n        ]\n\n        return [tf.squeeze(b_k_lists[0], axis=1), tf.squeeze(b_k_lists[1], axis=1)]\n\n    @staticmethod\n    def obtain_indices(preds_start, preds_end):\n        """"""\n        Function to get answer indices given the predictions\n\n        Args:\n            preds_start: predicted start indices\n            predictions: predicted end indices\n\n        Returns:\n            final start and end indices for the answer\n        """"""\n        ans_start = []\n        ans_end = []\n        for i in range(preds_start.shape[0]):\n            max_ans_id = -100000000\n            st_idx = 0\n            en_idx = 0\n            ele1 = preds_start[i]\n            ele2 = preds_end[i]\n            len_para = len(ele1)\n            for j in range(len_para):\n                for k in range(15):\n                    if j + k >= len_para:\n                        break\n                    ans_start_int = ele1[j]\n                    ans_end_int = ele2[j + k]\n                    if (ans_start_int + ans_end_int) > max_ans_id:\n                        max_ans_id = ans_start_int + ans_end_int\n                        st_idx = j\n                        en_idx = j + k\n\n            ans_start.append(st_idx)\n            ans_end.append(en_idx)\n\n        return (np.array(ans_start), np.array(ans_end))\n\n    def cal_f1_score(self, ground_truths, predictions):\n        """"""\n        Function to calculate F-1 and EM scores\n\n        Args:\n            ground_truths: labels given in the dataset\n            predictions: logits predicted by the network\n\n        Returns:\n            F1 score and Exact-Match score\n        """"""\n        start_idx, end_idx = self.obtain_indices(predictions[0], predictions[1])\n        f1 = 0\n        exact_match = 0\n        for i in range(self.batch_size):\n            ele1 = start_idx[i]\n            ele2 = end_idx[i]\n            preds = np.linspace(ele1, ele2, abs(ele2 - ele1 + 1))\n            length_gts = abs(ground_truths[i][1] - ground_truths[i][0] + 1)\n            gts = np.linspace(ground_truths[i][0], ground_truths[i][1], length_gts)\n            common = Counter(preds) & Counter(gts)\n            num_same = sum(common.values())\n\n            exact_match += int(np.array_equal(preds, gts))\n            if num_same == 0:\n                f1 += 0\n            else:\n                precision = 1.0 * num_same / len(preds)\n                recall = 1.0 * num_same / len(gts)\n                f1 += (2 * precision * recall) / (precision + recall)\n\n        return 100 * (f1 / self.batch_size), 100 * (exact_match / self.batch_size)\n\n    def get_dynamic_feed_params(self, question_str, vocab_reverse):\n        """"""\n        Function to get required feed_dict format for user entered questions.\n        Used mainly in the demo mode.\n\n        Args:\n           question_str: question string\n           vocab_reverse: vocab dictionary with words as keys and indices as values\n\n        Returns:\n           question_idx: list of indicies represnting the question padded to max length\n           question_len: actual length of the question\n           ques_mask: mask for question_idx\n\n        """"""\n\n        question_words = [\n            word.replace(""``"", \'""\').replace(""\'\'"", \'""\')\n            for word in self.tokenizer.tokenize(question_str)\n        ]\n\n        question_ids = [vocab_reverse[ele] for ele in question_words]\n\n        if len(question_ids) < self.max_question:\n            pad_length = self.max_question - len(question_ids)\n            question_idx = question_ids + [0] * pad_length\n            question_len = len(question_ids)\n            ques_mask = np.zeros([1, self.max_question])\n            ques_mask[0, 0:question_len] = 1\n            ques_mask = ques_mask.tolist()[0]\n\n        return question_idx, question_len, ques_mask\n\n    def run_loop(self, session, train, mode=""train"", dropout=0.6):\n        """"""\n        Function to run training/validation loop and display training loss, F1 & EM scores\n\n        Args:\n            session: tensorflow session\n            train:   data dictionary for training/validation\n            dropout: float value\n            mode: \'train\'/\'val\'\n        """"""\n\n        nbatches = int((len(train[""para""]) / self.batch_size))\n        f1_score = 0\n        em_score = 0\n        for idx in range(nbatches):\n            # Train for all batches\n            start_batch = self.batch_size * idx\n            end_batch = self.batch_size * (idx + 1)\n            if end_batch > len(train[""para""]):\n                break\n\n            # Create feed dictionary\n            feed_dict_qa = {\n                self.para_ids: np.asarray(train[""para""][start_batch:end_batch]),\n                self.question_ids: np.asarray(train[""question""][start_batch:end_batch]),\n                self.para_length: np.asarray(train[""para_len""][start_batch:end_batch]),\n                self.question_length: np.asarray(train[""question_len""][start_batch:end_batch]),\n                self.labels: np.asarray(train[""answer""][start_batch:end_batch]),\n                self.para_mask: np.asarray(train[""para_mask""][start_batch:end_batch]),\n                self.ques_mask: np.asarray(train[""question_mask""][start_batch:end_batch]),\n                self.dropout: dropout,\n            }\n            # Training Phase\n            if mode == ""train"":\n                _, train_loss, _, logits, labels = session.run(\n                    [\n                        self.optimizer,\n                        self.loss,\n                        self.learning_rate,\n                        self.logits_withsf,\n                        self.labels,\n                    ],\n                    feed_dict=feed_dict_qa,\n                )\n\n                if idx % 20 == 0:\n                    print(""iteration = {}, train loss = {}"".format(idx, train_loss))\n                    f1_score, em_score = self.cal_f1_score(labels, logits)\n                    print(""F-1 and EM Scores are"", f1_score, em_score)\n\n                self.global_step.assign(self.global_step + 1)\n\n            else:\n                logits, labels = session.run(\n                    [self.logits_withsf, self.labels], feed_dict=feed_dict_qa\n                )\n\n                f1_score_int, em_score_int = self.cal_f1_score(labels, logits)\n                f1_score += f1_score_int\n                em_score += em_score_int\n\n        # Validation Phase\n        if mode == ""val"":\n            print(""Validation F1 and EM scores are"", f1_score / nbatches, em_score / nbatches)\n\n    # pylint: disable=inconsistent-return-statements\n    def inference_mode(\n        self,\n        session,\n        valid,\n        vocab_tuple,\n        num_examples,\n        dropout=1.0,\n        dynamic_question_mode=False,\n        dynamic_usr_question="""",\n        dynamic_question_index=0,\n    ):\n        """"""\n          Function to run inference_mode for reading comprehension\n\n          Args:\n              session: tensorflow session\n              valid: data dictionary for validation set\n              vocab_tuple: a tuple containing voacab dictionaries in forward and reverse directions\n              num_examples : specify the number of samples to run for inference\n              dropout : Float value which is always 1.0 for inference\n              dynamic_question_mode : boolean to enable whether or not accept\n                                      questions from the user(used in the demo mode)\n\n        """"""\n        vocab_forward = vocab_tuple[0]\n        vocab_reverse = vocab_tuple[1]\n\n        for idx in range(num_examples):\n            if dynamic_question_mode is True:\n                idx = dynamic_question_index\n                required_params = self.get_dynamic_feed_params(dynamic_usr_question, vocab_reverse)\n                question_ids = required_params[0]\n                question_length = required_params[1]\n                ques_mask = required_params[2]\n                test_paragraph = [vocab_forward[ele] for ele in valid[idx][0] if ele != 0]\n                para_string = "" "".join(map(str, test_paragraph))\n            else:\n                # Print Paragraph\n                print(""\\n"")\n                print(""Paragraph Number AA:"", idx)\n                test_paragraph = [\n                    vocab_forward[ele].replace("" "", """") for ele in valid[idx][0] if ele != 0\n                ]\n                para_string = "" "".join(map(str, test_paragraph))\n                print(para_string)\n\n                # Print corresponding Question\n                test_question = [\n                    vocab_forward[ele].replace("" "", """") for ele in valid[idx][1] if ele != 0\n                ]\n                ques_string = "" "".join(map(str, test_question))\n                print(""Question:"", ques_string)\n                question_ids = valid[idx][1]\n                question_length = valid[idx][3]\n                ques_mask = valid[idx][6]\n\n            # Create a feed dictionary\n            feed_dict_qa = {\n                self.para_ids: np.expand_dims(valid[idx][0], 0),\n                self.question_ids: np.expand_dims(question_ids, 0),\n                self.para_length: np.expand_dims(valid[idx][2], 0),\n                self.question_length: np.expand_dims(question_length, 0),\n                self.para_mask: np.expand_dims(valid[idx][5], 0),\n                self.ques_mask: np.expand_dims(ques_mask, 0),\n                self.dropout: dropout,\n            }\n\n            # Run session and obtain indices\n            predictions = session.run([self.logits_withsf], feed_dict=feed_dict_qa)\n\n            # Get the start and end indices of the answer\n            start_idx, end_idx = self.obtain_indices(predictions[0][0], predictions[0][1])\n            answer_ind = valid[idx][0][start_idx[0] : end_idx[0] + 1]\n\n            # Print answer\n            req_ans = [vocab_forward[ele].replace("" "", """") for ele in answer_ind if ele != 0]\n            ans_string = "" "".join(map(str, req_ans))\n            answer = re.sub(r\'\\s([?.!"",])\', r""\\1"", ans_string)\n            print(""Answer:"", answer)\n            if dynamic_question_mode is True:\n                return {""answer"": answer}\n'"
examples/reading_comprehension/match_lstm_mrc/mrc_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport numpy as np\n\n\ndef max_values_squad(data_train):\n    """"""\n    Function to compute the maximum length of sentences in\n    paragraphs and questions\n\n    Args:\n    -----------\n    data_train: list containing the entire dataset\n\n    Returns:\n    --------\n    maximum length of question and paragraph\n    """"""\n    max_hypothesis = len(data_train[0][1])\n    max_premise = len(data_train[0][0])\n\n    for ele in data_train:\n        len1 = len(ele[0])\n        len2 = len(ele[1])\n        if max_premise < len1:\n            max_premise = len1\n        if max_hypothesis < len2:\n            max_hypothesis = len2\n\n    return max_premise, max_hypothesis\n\n\ndef get_qids(args, q_id_path, data_dev):\n\n    """"""\n    Function to create a list of question_ids in dev set\n\n    Args:\n    -----------\n    q_id_path: path to question_ids file\n    data_dev: development set\n\n    Returns:\n    --------\n    list of question ids\n    """"""\n    qids_list = []\n    with open(q_id_path) as q_ids:\n        for ele in q_ids:\n            qids_list.append(ele.strip().replace("" "", """"))\n\n    final_qidlist = []\n    count = 0\n    for ele in data_dev:\n        para_idx = ele[0]\n        if len(para_idx) < args.max_para:\n            final_qidlist.append(qids_list[count])\n        count += 1\n\n    return final_qidlist\n\n\ndef create_squad_training(paras_file, ques_file, answer_file, data_train_len=None):\n\n    """"""\n    Function to read data from preprocessed files and return\n    data in the form of a list\n\n    Args:\n    -----------\n    paras_file: File name for preprocessed paragraphs\n    ques_file: File name for preprocessed questions\n    answer_file: File name for preprocessed answer spans\n    vocab_file:  File name for preprocessed vocab\n    data_train_len= length of train dataset to use\n\n    Returns:\n    --------\n    appended list for train/dev dataset\n    """"""\n\n    para_list = []\n    ques_list = []\n    ans_list = []\n    with open(paras_file) as f_para:\n        for ele in f_para:\n            para_list.append(list(map(int, ele.strip().split())))\n\n    with open(ques_file) as f_ques:\n        for ele in f_ques:\n            ques_list.append(list(map(int, ele.strip().split())))\n\n    with open(answer_file) as f_ans:\n        for ele in f_ans:\n            ans_list.append(list(map(int, ele.strip().split())))\n\n    data_train = []\n    if data_train_len is None:\n        data_train_len = len(para_list)\n\n    for idx in range(data_train_len):\n        data_train.append([para_list[idx], ques_list[idx], ans_list[idx]])\n\n    return data_train\n\n\ndef get_data_array_squad(params_dict, data_train, set_val=""train""):\n    """"""\n    Function to pad all sentences and restrict to max length defined by user\n\n    Args:\n    ---------\n    params_dict: dictionary containing all input parameters\n    data_train: list containing the training/dev data_train\n    set_val: indicates id its a training set or dev set\n\n    Returns:\n    ----------\n    Returns a list of tuples with padded sentences and masks\n    """"""\n\n    max_para = params_dict[""max_para""]\n    max_question = params_dict[""max_question""]\n    train_set = []\n    count = 0\n    for ele in data_train:\n\n        para = ele[0]\n        para_idx = ele[0]\n\n        if len(para_idx) < max_para:\n            pad_length = max_para - len(para_idx)\n            para_idx = para_idx + [0] * pad_length\n            para_len = len(para)\n            para_mask = np.zeros([1, max_para])\n            para_mask[0, 0 : len(para)] = 1\n            para_mask = para_mask.tolist()[0]\n\n            question_idx = ele[1]\n            question = ele[1]\n\n            if len(question) < max_question:\n                pad_length = max_question - len(question)\n                question_idx = question_idx + [0] * pad_length\n                question_len = len(question)\n                ques_mask = np.zeros([1, max_question])\n                ques_mask[0, 0:question_len] = 1\n                ques_mask = ques_mask.tolist()[0]\n\n            train_set.append(\n                (para_idx, question_idx, para_len, question_len, ele[2], para_mask, ques_mask)\n            )\n\n            if set_val == ""train"":\n                count += 1\n                if count >= params_dict[""train_set_size""]:\n                    break\n    return train_set\n\n\ndef create_data_dict(data):\n\n    """"""\n    Function to convert data to dictionary format\n\n    Args:\n    -----------\n    data: train/dev data as a list\n\n    Returns:\n    --------\n    a dictionary containing dev/train data\n    """"""\n\n    train = {}\n    train[""para""] = []\n    train[""answer""] = []\n    train[""question""] = []\n    train[""question_len""] = []\n    train[""para_len""] = []\n    train[""para_mask""] = []\n    train[""question_mask""] = []\n\n    for (para_idx, question_idx, para_len, question_len, answer, para_mask, ques_mask) in data:\n\n        train[""para""].append(para_idx)\n        train[""question""].append(question_idx)\n        train[""para_len""].append(para_len)\n        train[""question_len""].append(question_len)\n        train[""answer""].append(answer)\n        train[""para_mask""].append(para_mask)\n        train[""question_mask""].append(ques_mask)\n\n    return train\n'"
examples/reading_comprehension/match_lstm_mrc/prepare_data.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import print_function\nimport numpy as np\nimport argparse\nimport json\nimport os\nfrom tqdm import tqdm\nfrom nlp_architect.utils.io import validate_existing_directory\nfrom nlp_architect.utils.text import SpacyInstance\n\nsep = os.sep\nPAD = ""<pad>""\nSOS = ""<sos>""\nUNK = ""<unk>""\nSTART_VOCAB = [PAD, SOS, UNK]\ntokenizer = SpacyInstance(disable=[""tagger"", ""ner"", ""parser"", ""vectors"", ""textcat""])\n\n\ndef create_vocabulary(data_list):\n    """"""\n    Function to generate vocabulary for both training and development datasets\n    """"""\n    vocab = {}\n    for list_element in data_list:\n        for sentence in list_element:\n            for word in sentence:\n                if word in vocab:\n                    vocab[word] += 1\n                else:\n                    vocab[word] = 1\n\n    _vocab_list = START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n    _vocab_dict = dict((vocab_ele, i) for i, vocab_ele in enumerate(_vocab_list))\n\n    return _vocab_list, _vocab_dict\n\n\ndef create_token_map(para, para_tokens):\n    """"""\n    Function to generate mapping between tokens and indices\n    """"""\n    token_map = {}\n    char_append = """"\n    para_token_idx = 0\n    for char_idx, char in enumerate(para):\n        if char != "" "":\n            current_para_token = para_tokens[para_token_idx]\n            char_append += char\n            if char_append == current_para_token:\n                token_map[char_idx - len(char_append) + 1] = [char_append, para_token_idx]\n                para_token_idx += 1\n                char_append = """"\n\n    return token_map\n\n\ndef get_glove_matrix(vocabulary_list, download_path):\n    """"""\n    Function to obtain preprocessed glove embeddings matrix\n    """"""\n    save_file_name = download_path + sep + ""glove.trimmed.300""\n    if not os.path.exists(save_file_name + "".npz""):\n        vocab_len = len(vocabulary_list)\n        glove_path = os.path.join(download_path + sep + ""glove.6B.300d.txt"")\n        glove_matrix = np.zeros((vocab_len, 300))\n        count = 0\n        with open(glove_path) as f:\n            for line in tqdm(f):\n                split_line = line.lstrip().rstrip().split("" "")\n                word = split_line[0]\n                word_vec = list(map(float, split_line[1:]))\n                if word in vocabulary_list:\n                    word_index = vocabulary_list.index(word)\n                    glove_matrix[word_index, :] = word_vec\n                    count += 1\n\n                if word.upper() in vocabulary_list:\n                    word_index = vocabulary_list.index(word.upper())\n                    glove_matrix[word_index, :] = word_vec\n                    count += 1\n\n                if word.capitalize() in vocabulary_list:\n                    word_index = vocabulary_list.index(word.capitalize())\n                    glove_matrix[word_index, :] = word_vec\n                    count += 1\n\n        print(""Saving the embeddings .npz file"")\n        np.savez_compressed(save_file_name, glove=glove_matrix)\n        print(""Percentage of words in vocab found in glove embeddings %f"" % (count / vocab_len))\n\n\n# pylint: disable=unnecessary-lambda\ndef tokenize_sentence(line):\n    """"""\n    Function to tokenize  sentence\n    """"""\n    tokenized_words = [\n        word.replace(""``"", \'""\').replace(""\'\'"", \'""\') for word in tokenizer.tokenize(line)\n    ]\n    return list(map(lambda x: str(x), tokenized_words))\n\n\ndef extract_data_from_files(json_data):\n    """"""\n    Function to read and extract data from raw input json files\n    """"""\n    data_para = []\n    data_ques = []\n    data_answer = []\n    line_skipped = 0\n    for article_id in range(len(json_data[""data""])):\n        sub_paragraphs = json_data[""data""][article_id][""paragraphs""]\n        for para_id in range(len(sub_paragraphs)):\n\n            req_para = sub_paragraphs[para_id][""context""]\n            req_para = req_para.replace(""\'\'"", \'"" \').replace(""``"", \'"" \')\n            para_tokens = tokenize_sentence(req_para)\n            answer_map = create_token_map(req_para, para_tokens)\n\n            questions = sub_paragraphs[para_id][""qas""]\n            for ques_id in range(len(questions)):\n\n                req_question = questions[ques_id][""question""]\n                question_tokens = tokenize_sentence(req_question)\n\n                for ans_id in range(1):\n                    answer_text = questions[ques_id][""answers""][ans_id][""text""]\n                    answer_start = questions[ques_id][""answers""][ans_id][""answer_start""]\n                    answer_end = answer_start + len(answer_text)\n                    text_tokens = tokenize_sentence(answer_text)\n                    last_word_answer = len(text_tokens[-1])\n                    try:\n                        a_start_idx = answer_map[answer_start][1]\n\n                        a_end_idx = answer_map[answer_end - last_word_answer][1]\n\n                        data_para.append(para_tokens)\n                        data_ques.append(question_tokens)\n                        data_answer.append((a_start_idx, a_end_idx))\n\n                    except KeyError:\n                        line_skipped += 1\n\n    return data_para, data_ques, data_answer\n\n\ndef write_to_file(file_dict, path_to_save):\n    """"""\n    Function to write data to files\n    """"""\n\n    for f_name in file_dict:\n        if f_name == ""vocab.dat"":\n            with open(os.path.join(path_to_save + f_name), ""w"") as target_file:\n                for word in file_dict[f_name]:\n                    target_file.write(str(word) + ""\\n"")\n        else:\n            with open(os.path.join(path_to_save + f_name), ""w"") as target_file:\n                for line in file_dict[f_name]:\n                    target_file.write("" "".join([str(tok) for tok in line]) + ""\\n"")\n\n\ndef get_ids_list(data_list, vocab):\n    """"""\n    Function to obtain indices from vocabulary\n    """"""\n    ids_list = []\n    for line in data_list:\n        curr_line_idx = []\n        for word in line:\n            try:\n                curr_line_idx.append(vocab[word])\n            except ValueError:\n                curr_line_idx.append(vocab[UNK])\n\n        ids_list.append(curr_line_idx)\n    return ids_list\n\n\nif __name__ == ""__main__"":\n\n    # parse the command line arguments\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        ""--data_path"",\n        help=""enter path where training data and the \\\n                        glove embeddings were downloaded"",\n        type=validate_existing_directory,\n    )\n\n    parser.add_argument(\n        ""--no_preprocess_glove"",\n        action=""store_true"",\n        help=""Chose whether or not to preprocess glove embeddings"",\n    )\n\n    parser.set_defaults()\n    args = parser.parse_args()\n\n    glove_flag = not args.no_preprocess_glove\n\n    # Validate files in the folder:\n    missing_flag = 0\n    files_list = [""train-v1.1.json"", ""dev-v1.1.json""]\n    for file_name in files_list:\n        if not os.path.exists(os.path.join(args.data_path, file_name)):\n            print(""The following required file is missing :"", file_name)\n            missing_flag = 1\n\n    if missing_flag:\n        print(""Please ensure that required datasets are downloaded"")\n        exit()\n\n    data_path = args.data_path\n    # Load Train and Dev Data\n    train_filename = os.path.join(data_path, ""train-v1.1.json"")\n    dev_filename = os.path.join(data_path, ""dev-v1.1.json"")\n    with open(train_filename) as train_file:\n        train_data = json.load(train_file)\n\n    with open(dev_filename) as dev_file:\n        dev_data = json.load(dev_file)\n\n    print(""Extracting data from json files"")\n    # Extract training data from raw files\n    train_para, train_question, train_ans = extract_data_from_files(train_data)\n    # Extract dev data from raw dataset\n    dev_para, dev_question, dev_ans = extract_data_from_files(dev_data)\n\n    data_lists = [train_para, train_question, dev_para, dev_question]\n    # Obtain vocab list\n    print(""Creating Vocabulary"")\n    vocab_list, vocab_dict = create_vocabulary(data_lists)\n\n    # Obtain embedding matrix from pre-trained glove vectors:\n    if glove_flag:\n        print(""Preprocessing glove"")\n        get_glove_matrix(vocab_list, data_path)\n\n    # Get vocab ids file\n    train_para_ids = get_ids_list(train_para, vocab_dict)\n    train_question_ids = get_ids_list(train_question, vocab_dict)\n    dev_para_ids = get_ids_list(dev_para, vocab_dict)\n    dev_question_ids = get_ids_list(dev_question, vocab_dict)\n\n    final_data_dict = {\n        sep + ""train.ids.context"": train_para_ids,\n        sep + ""train.ids.question"": train_question_ids,\n        sep + ""dev.ids.context"": dev_para_ids,\n        sep + ""dev.ids.question"": dev_question_ids,\n        sep + ""vocab.dat"": vocab_list,\n        sep + ""train.span"": train_ans,\n        sep + ""dev.span"": dev_ans,\n    }\n\n    print(""writing data to files"")\n    write_to_file(final_data_dict, data_path)\n'"
examples/reading_comprehension/match_lstm_mrc/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nfrom random import shuffle\n\nimport numpy as np\nimport tensorflow as tf\nfrom .mrc_utils import (\n    create_squad_training,\n    max_values_squad,\n    get_data_array_squad,\n    create_data_dict,\n)\n\nfrom .matchlstm_ansptr import MatchLSTMAnswerPointer\nfrom nlp_architect.utils.io import validate_existing_directory, check_size, validate_parent_exists\n\n# Parse the command line arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--data_path"",\n    default=""data"",\n    type=validate_existing_directory,\n    help=""enter path for training data"",\n)\n\nparser.add_argument(""--gpu_id"", default=""0"", type=str, help=""enter gpu id"", action=check_size(0, 8))\n\nparser.add_argument(\n    ""--max_para_req"",\n    default=300,\n    type=int,\n    help=""enter the max length of paragraph"",\n    action=check_size(30, 300),\n)\n\nparser.add_argument(\n    ""--epochs"", default=15, type=int, help=""enter the number of epochs"", action=check_size(1, 30)\n)\n\nparser.add_argument(\n    ""--select_device"",\n    default=""GPU"",\n    type=str,\n    help=""enter the device to execute on"",\n    action=check_size(3, 9),\n)\n\nparser.add_argument(\n    ""--train_set_size"",\n    default=None,\n    type=int,\n    help=""enter the size of the training set"",\n    action=check_size(200, 90000),\n)\n\nparser.add_argument(\n    ""--hidden_size"",\n    default=150,\n    type=int,\n    help=""enter the number of hidden units"",\n    action=check_size(30, 300),\n)\n\nparser.add_argument(\n    ""--model_dir"",\n    default=""trained_model"",\n    type=validate_parent_exists,\n    help=""enter path to save model"",\n)\n\nparser.add_argument(\n    ""--restore_model"",\n    default=False,\n    type=bool,\n    help=""Choose whether to restore training from a previously saved model"",\n)\n\nparser.add_argument(\n    ""--inference_mode"", default=False, type=bool, help=""Choose whether to run inference only""\n)\n\n\nparser.add_argument(\n    ""--batch_size"", default=64, type=int, help=""enter the batch size"", action=check_size(1, 256)\n)\n\nparser.add_argument(\n    ""--num_examples"",\n    default=50,\n    type=int,\n    help=""enter the number of examples to run inference"",\n    action=check_size(1, 10000),\n)\n\nparser.set_defaults()\nargs = parser.parse_args()\n# Set GPU\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n\n# Create a dictionary of all parameters\nparams_dict = {}\nparams_dict[""batch_size""] = args.batch_size\nparams_dict[""hidden_size""] = args.hidden_size\nparams_dict[""max_para""] = args.max_para_req\nparams_dict[""epoch_no""] = args.epochs\nparams_dict[""inference_only""] = args.inference_mode\n\n# Validate select_device\nif args.select_device not in [""CPU"", ""GPU""]:\n    print(""Please enter a valid device name"")\n    exit()\n\n# Create dictionary of filenames\nfile_name_dict = {}\nfile_name_dict[""train_para_ids""] = ""train.ids.context""\nfile_name_dict[""train_ques_ids""] = ""train.ids.question""\nfile_name_dict[""train_answer""] = ""train.span""\nfile_name_dict[""val_para_ids""] = ""dev.ids.context""\nfile_name_dict[""val_ques_ids""] = ""dev.ids.question""\nfile_name_dict[""val_ans""] = ""dev.span""\nfile_name_dict[""vocab_file""] = ""vocab.dat""\nfile_name_dict[""embedding""] = ""glove.trimmed.300.npz""\n\n# Validate contents of data_path folder:\nmissing_flag = 0\nfor file_name in file_name_dict.values():\n    if not os.path.exists(os.path.join(args.data_path, file_name)):\n        print(""The following required file is missing :"", file_name)\n        missing_flag = 1\n\nif missing_flag:\n    print(""Please rereun prepare_data.py to generate missing files"")\n    exit()\n\n# Paths for preprcessed files\npath_gen = args.data_path\ntrain_para_ids = os.path.join(path_gen, file_name_dict[""train_para_ids""])\ntrain_ques_ids = os.path.join(path_gen, file_name_dict[""train_ques_ids""])\nanswer_file = os.path.join(path_gen, file_name_dict[""train_answer""])\nval_paras_ids = os.path.join(path_gen, file_name_dict[""val_para_ids""])\nval_ques_ids = os.path.join(path_gen, file_name_dict[""val_ques_ids""])\nval_ans_file = os.path.join(path_gen, file_name_dict[""val_ans""])\nvocab_file = os.path.join(path_gen, file_name_dict[""vocab_file""])\n\n# Create model dir if it doesn\'t exist\nif not os.path.exists(args.model_dir):\n    os.makedirs(args.model_dir)\n\nmodel_path = args.model_dir\n\n# Create lists for train and validation sets\ndata_train = create_squad_training(train_para_ids, train_ques_ids, answer_file)\ndata_dev = create_squad_training(val_paras_ids, val_ques_ids, val_ans_file)\nvocab_list = []\nwith open(vocab_file) as f:\n    for ele in f:\n        vocab_list.append(ele)\nvocab_dict = {}\nvocab_rev = {}\n\nfor i in range(len(vocab_list)):\n    vocab_dict[i] = vocab_list[i].strip()\n    vocab_rev[vocab_list[i].strip()] = i\n\nif args.train_set_size is None:\n    params_dict[""train_set_size""] = len(data_train)\nelse:\n    params_dict[""train_set_size""] = args.train_set_size\n\n# Combine train and dev data\ndata_total = data_train + data_dev\n\n# obtain maximum length of question\n_, max_question = max_values_squad(data_total)\nparams_dict[""max_question""] = max_question\n\n# Load embeddings for vocab\nprint(""Loading Embeddings"")\nembeddingz = np.load(os.path.join(path_gen, file_name_dict[""embedding""]))\nembeddings = embeddingz[""glove""]\n\n# Create train and dev sets\nprint(""Creating training and development sets"")\ntrain = get_data_array_squad(params_dict, data_train, set_val=""train"")\ndev = get_data_array_squad(params_dict, data_dev, set_val=""val"")\n\n# Define Reading Comprehension model\nwith tf.device(""/device:"" + args.select_device + "":0""):\n    model = MatchLSTMAnswerPointer(params_dict, embeddings)\n\n# Define Configs for training\nrun_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\n# Create session run training\nwith tf.Session(config=run_config) as sess:\n    # pylint: disable=no-member\n\n    init = tf.global_variables_initializer()\n\n    # Model Saver\n    model_saver = tf.train.Saver()\n    model_ckpt = tf.train.get_checkpoint_state(model_path)\n    idx_path = model_ckpt.model_checkpoint_path + "".index"" if model_ckpt else """"\n\n    # Intitialze with random or pretrained weights\n    if (\n        model_ckpt\n        and args.restore_model\n        and (tf.gfile.Exists(model_ckpt.model_checkpoint_path) or tf.gfile.Exists(idx_path))\n    ):\n        model_saver.restore(sess, model_ckpt.model_checkpoint_path)\n        print(""Loading from previously stored session"")\n    else:\n        sess.run(init)\n\n    dev_dict = create_data_dict(dev)\n    if args.inference_mode is False:\n        print(""Begin Training"")\n\n        for epoch in range(params_dict[""epoch_no""]):\n            print(""Epoch Number: "", epoch)\n\n            # Shuffle Datset and create train data dictionary\n            shuffle(train)\n            train_dict = create_data_dict(train)\n\n            # Run training for 1 epoch\n            model.run_loop(sess, train_dict, mode=""train"", dropout=0.6)\n\n            # Save Weights after 1 epoch\n            print(""Saving Weights"")\n            model_saver.save(sess, ""%s/trained_model.ckpt"" % model_path)\n\n            # Start validation phase at end of each epoch\n            print(""Begin Validation"")\n            model.run_loop(sess, dev_dict, mode=""val"", dropout=1)\n\n    else:\n        print(""Begin Inference Mode"")\n        # Shuffle Validation Set\n        shuffle(dev)\n        # Run Inference Mode\n        model.inference_mode(\n            sess, dev, [vocab_dict, vocab_rev], num_examples=args.num_examples, dropout=1.0\n        )\n'"
examples/sparse_gnmt/gnmt/__init__.py,0,b''
examples/sparse_gnmt/gnmt/attention_model.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: skip-file\n""""""Attention-based sequence-to-sequence model with dynamic RNN support.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom . import model\nfrom . import model_helper\n\n__all__ = [""AttentionModel""]\n\n\nclass AttentionModel(model.Model):\n    """"""Sequence-to-sequence dynamic model with attention.\n\n    This class implements a multi-layer recurrent neural network as encoder,\n    and an attention-based decoder. This is the same as the model described in\n    (Luong et al., EMNLP\'2015) paper: https://arxiv.org/pdf/1508.04025v5.pdf.\n    This class also allows to use GRU cells in addition to LSTM cells with\n    support for dropout.\n    """"""\n\n    def __init__(\n        self,\n        hparams,\n        mode,\n        iterator,\n        source_vocab_table,\n        target_vocab_table,\n        reverse_target_vocab_table=None,\n        scope=None,\n        extra_args=None,\n    ):\n        self.has_attention = hparams.attention_architecture and hparams.attention\n\n        # Set attention_mechanism_fn\n        if self.has_attention:\n            if extra_args and extra_args.attention_mechanism_fn:\n                self.attention_mechanism_fn = extra_args.attention_mechanism_fn\n            else:\n                self.attention_mechanism_fn = create_attention_mechanism\n\n        super(AttentionModel, self).__init__(\n            hparams=hparams,\n            mode=mode,\n            iterator=iterator,\n            source_vocab_table=source_vocab_table,\n            target_vocab_table=target_vocab_table,\n            reverse_target_vocab_table=reverse_target_vocab_table,\n            scope=scope,\n            extra_args=extra_args,\n        )\n\n    def _prepare_beam_search_decoder_inputs(\n        self, beam_width, memory, source_sequence_length, encoder_state\n    ):\n        memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=beam_width)\n        source_sequence_length = tf.contrib.seq2seq.tile_batch(\n            source_sequence_length, multiplier=beam_width\n        )\n        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n        batch_size = self.batch_size * beam_width\n        return memory, source_sequence_length, encoder_state, batch_size\n\n    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state, source_sequence_length):\n        """"""Build a RNN cell with attention mechanism that can be used by decoder.""""""\n        # No Attention\n        if not self.has_attention:\n            return super(AttentionModel, self)._build_decoder_cell(\n                hparams, encoder_outputs, encoder_state, source_sequence_length\n            )\n        elif hparams.attention_architecture != ""standard"":\n            raise ValueError(""Unknown attention architecture %s"" % hparams.attention_architecture)\n\n        num_units = hparams.num_units\n        num_layers = self.num_decoder_layers\n        num_residual_layers = self.num_decoder_residual_layers\n        infer_mode = hparams.infer_mode\n\n        dtype = tf.float32\n\n        # Ensure memory is batch-major\n        if self.time_major:\n            memory = tf.transpose(encoder_outputs, [1, 0, 2])\n        else:\n            memory = encoder_outputs\n\n        if self.mode == tf.contrib.learn.ModeKeys.INFER and infer_mode == ""beam_search"":\n            (\n                memory,\n                source_sequence_length,\n                encoder_state,\n                batch_size,\n            ) = self._prepare_beam_search_decoder_inputs(\n                hparams.beam_width, memory, source_sequence_length, encoder_state\n            )\n        else:\n            batch_size = self.batch_size\n\n        # Attention\n        attention_mechanism = self.attention_mechanism_fn(\n            hparams.attention, num_units, memory, source_sequence_length, self.mode\n        )\n\n        cell = model_helper.create_rnn_cell(\n            unit_type=hparams.unit_type,\n            num_units=num_units,\n            num_layers=num_layers,\n            num_residual_layers=num_residual_layers,\n            forget_bias=hparams.forget_bias,\n            dropout=hparams.dropout,\n            num_gpus=self.num_gpus,\n            mode=self.mode,\n            single_cell_fn=self.single_cell_fn,\n        )\n\n        # Only generate alignment in greedy INFER mode.\n        alignment_history = (\n            self.mode == tf.contrib.learn.ModeKeys.INFER and infer_mode != ""beam_search""\n        )\n        cell = tf.contrib.seq2seq.AttentionWrapper(\n            cell,\n            attention_mechanism,\n            attention_layer_size=num_units,\n            alignment_history=alignment_history,\n            output_attention=hparams.output_attention,\n            name=""attention"",\n        )\n\n        # TODO(thangluong): do we need num_layers, num_gpus?\n        cell = tf.contrib.rnn.DeviceWrapper(\n            cell, model_helper.get_device_str(num_layers - 1, self.num_gpus)\n        )\n\n        if hparams.pass_hidden_state:\n            decoder_initial_state = cell.zero_state(batch_size, dtype).clone(\n                cell_state=encoder_state\n            )\n        else:\n            decoder_initial_state = cell.zero_state(batch_size, dtype)\n\n        return cell, decoder_initial_state\n\n    def _get_infer_summary(self, hparams):\n        if not self.has_attention or hparams.infer_mode == ""beam_search"":\n            return tf.no_op()\n        return _create_attention_images_summary(self.final_context_state)\n\n\ndef create_attention_mechanism(attention_option, num_units, memory, source_sequence_length, mode):\n    """"""Create attention mechanism based on the attention_option.""""""\n    del mode  # unused\n\n    # Mechanism\n    if attention_option == ""luong"":\n        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n            num_units, memory, memory_sequence_length=source_sequence_length\n        )\n    elif attention_option == ""scaled_luong"":\n        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n            num_units, memory, memory_sequence_length=source_sequence_length, scale=True\n        )\n    elif attention_option == ""bahdanau"":\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n            num_units, memory, memory_sequence_length=source_sequence_length\n        )\n    elif attention_option == ""normed_bahdanau"":\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n            num_units, memory, memory_sequence_length=source_sequence_length, normalize=True\n        )\n    else:\n        raise ValueError(""Unknown attention option %s"" % attention_option)\n\n    return attention_mechanism\n\n\ndef _create_attention_images_summary(final_context_state):\n    """"""create attention image and attention summary.""""""\n    attention_images = final_context_state.alignment_history.stack()\n    # Reshape to (batch, src_seq_len, tgt_seq_len,1)\n    attention_images = tf.expand_dims(tf.transpose(attention_images, [1, 2, 0]), -1)\n    # Scale to range [0, 255]\n    attention_images *= 255\n    attention_summary = tf.summary.image(""attention_images"", attention_images)\n    return attention_summary\n'"
examples/sparse_gnmt/gnmt/model.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n#   pruning operations\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: skip-file\n""""""Basic sequence-to-sequence model with dynamic RNN support.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\n\nimport tensorflow as tf\nfrom tensorflow.contrib.model_pruning import get_masks, get_thresholds\nfrom tensorflow.contrib.model_pruning.python import pruning\nfrom tensorflow.contrib.model_pruning.python.layers import core_layers\n\nfrom . import model_helper\nfrom .utils import misc_utils as utils, iterator_utils\n\nutils.check_tensorflow_version()\n\n__all__ = [""BaseModel"", ""Model""]\n\n\nclass TrainOutputTuple(\n    collections.namedtuple(\n        ""TrainOutputTuple"",\n        (\n            ""train_summary"",\n            ""train_loss"",\n            ""predict_count"",\n            ""global_step"",\n            ""word_count"",\n            ""batch_size"",\n            ""grad_norm"",\n            ""learning_rate"",\n            ""mask_update_op"",\n            ""pruning_summary"",\n        ),\n    )\n):\n    """"""To allow for flexibily in returing different outputs.""""""\n\n    pass\n\n\nclass EvalOutputTuple(\n    collections.namedtuple(""EvalOutputTuple"", (""eval_loss"", ""predict_count"", ""batch_size""))\n):\n    """"""To allow for flexibily in returing different outputs.""""""\n\n    pass\n\n\nclass InferOutputTuple(\n    collections.namedtuple(\n        ""InferOutputTuple"", (""infer_logits"", ""infer_summary"", ""sample_id"", ""sample_words"")\n    )\n):\n    """"""To allow for flexibily in returing different outputs.""""""\n\n    pass\n\n\nclass BaseModel(object):\n    """"""Sequence-to-sequence base class.\n    """"""\n\n    def __init__(\n        self,\n        hparams,\n        mode,\n        iterator,\n        source_vocab_table,\n        target_vocab_table,\n        reverse_target_vocab_table=None,\n        scope=None,\n        extra_args=None,\n    ):\n        """"""Create the model.\n\n        Args:\n          hparams: Hyperparameter configurations.\n          mode: TRAIN | EVAL | INFER\n          iterator: Dataset Iterator that feeds data.\n          source_vocab_table: Lookup table mapping source words to ids.\n          target_vocab_table: Lookup table mapping target words to ids.\n          reverse_target_vocab_table: Lookup table mapping ids to target words. Only\n            required in INFER mode. Defaults to None.\n          scope: scope of the model.\n          extra_args: model_helper.ExtraArgs, for passing customizable functions.\n\n        """"""\n        # Set params\n        self._set_params_initializer(\n            hparams, mode, iterator, source_vocab_table, target_vocab_table, scope, extra_args\n        )\n\n        # Not used in general seq2seq models; when True, ignore decoder & training\n        self.extract_encoder_layers = (\n            hasattr(hparams, ""extract_encoder_layers"") and hparams.extract_encoder_layers\n        )\n\n        # Train graph\n        res = self.build_graph(hparams, scope=scope)\n        if not self.extract_encoder_layers:\n            self._set_train_or_infer(res, reverse_target_vocab_table, hparams)\n\n        # Saver\n        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=hparams.num_keep_ckpts)\n\n    def _set_params_initializer(\n        self,\n        hparams,\n        mode,\n        iterator,\n        source_vocab_table,\n        target_vocab_table,\n        scope,\n        extra_args=None,\n    ):\n        """"""Set various params for self and initialize.""""""\n        assert isinstance(iterator, iterator_utils.BatchedInput)\n        self.iterator = iterator\n        self.mode = mode\n        self.src_vocab_table = source_vocab_table\n        self.tgt_vocab_table = target_vocab_table\n\n        self.src_vocab_size = hparams.src_vocab_size\n        self.tgt_vocab_size = hparams.tgt_vocab_size\n        self.num_gpus = hparams.num_gpus\n        self.time_major = hparams.time_major\n\n        if hparams.use_char_encode:\n            assert not self.time_major, ""Can\'t use time major for"" "" char-level inputs.""\n\n        self.dtype = tf.float32\n        self.num_sampled_softmax = hparams.num_sampled_softmax\n\n        # extra_args: to make it flexible for adding external customizable code\n        self.single_cell_fn = None\n        if extra_args:\n            self.single_cell_fn = extra_args.single_cell_fn\n\n        # Set num units\n        self.num_units = hparams.num_units\n\n        # Set num layers\n        self.num_encoder_layers = hparams.num_encoder_layers\n        self.num_decoder_layers = hparams.num_decoder_layers\n        assert self.num_encoder_layers\n        assert self.num_decoder_layers\n\n        # Set num residual layers\n        if hasattr(hparams, ""num_residual_layers""):  # compatible common_test_utils\n            self.num_encoder_residual_layers = hparams.num_residual_layers\n            self.num_decoder_residual_layers = hparams.num_residual_layers\n        else:\n            self.num_encoder_residual_layers = hparams.num_encoder_residual_layers\n            self.num_decoder_residual_layers = hparams.num_decoder_residual_layers\n\n        # Batch size\n        self.batch_size = tf.size(self.iterator.source_sequence_length)\n\n        # Global step\n        self.global_step = tf.Variable(0, trainable=False)\n\n        # Initializer\n        self.random_seed = hparams.random_seed\n        initializer = model_helper.get_initializer(\n            hparams.init_op, self.random_seed, hparams.init_weight\n        )\n        tf.get_variable_scope().set_initializer(initializer)\n\n        # Embeddings\n        if extra_args and extra_args.encoder_emb_lookup_fn:\n            self.encoder_emb_lookup_fn = extra_args.encoder_emb_lookup_fn\n        else:\n            self.encoder_emb_lookup_fn = tf.nn.embedding_lookup\n        self.init_embeddings(hparams, scope)\n\n    def _set_train_or_infer(self, res, reverse_target_vocab_table, hparams):\n        """"""Set up training and inference.""""""\n        if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n            self.train_loss = res[1]\n            self.word_count = tf.reduce_sum(self.iterator.source_sequence_length) + tf.reduce_sum(\n                self.iterator.target_sequence_length\n            )\n        elif self.mode == tf.contrib.learn.ModeKeys.EVAL:\n            self.eval_loss = res[1]\n        elif self.mode == tf.contrib.learn.ModeKeys.INFER:\n            self.infer_logits, _, self.final_context_state, self.sample_id = res\n            self.sample_words = reverse_target_vocab_table.lookup(tf.to_int64(self.sample_id))\n\n        if self.mode != tf.contrib.learn.ModeKeys.INFER:\n            # Count the number of predicted words for compute ppl.\n            self.predict_count = tf.reduce_sum(self.iterator.target_sequence_length)\n\n        params = tf.trainable_variables()\n\n        # Gradients and SGD update operation for training the model.\n        # Arrange for the embedding vars to appear at the beginning.\n        if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n            self.learning_rate = tf.constant(hparams.learning_rate)\n            # warm-up\n            self.learning_rate = self._get_learning_rate_warmup(hparams)\n            # decay\n            self.learning_rate = self._get_learning_rate_decay(hparams)\n\n            # Optimizer\n            if hparams.optimizer == ""sgd"":\n                opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n            elif hparams.optimizer == ""adam"":\n                opt = tf.train.AdamOptimizer(self.learning_rate)\n            else:\n                raise ValueError(""Unknown optimizer type %s"" % hparams.optimizer)\n\n            # Gradients\n            gradients = tf.gradients(\n                self.train_loss,\n                params,\n                colocate_gradients_with_ops=hparams.colocate_gradients_with_ops,\n            )\n\n            clipped_grads, grad_norm_summary, grad_norm = model_helper.gradient_clip(\n                gradients, max_gradient_norm=hparams.max_gradient_norm\n            )\n            self.grad_norm_summary = grad_norm_summary\n            self.grad_norm = grad_norm\n\n            self.update = opt.apply_gradients(\n                zip(clipped_grads, params), global_step=self.global_step\n            )\n\n            # Summary\n            self.train_summary = self._get_train_summary()\n        elif self.mode == tf.contrib.learn.ModeKeys.INFER:\n            self.infer_summary = self._get_infer_summary(hparams)\n\n        # Print trainable variables\n        utils.print_out(""# Trainable variables"")\n        utils.print_out(""Format: <name>, <shape>, <(soft) device placement>"")\n        for param in params:\n            utils.print_out(""  %s, %s, %s"" % (param.name, str(param.get_shape()), param.op.device))\n\n    def _get_learning_rate_warmup(self, hparams):\n        """"""Get learning rate warmup.""""""\n        warmup_steps = hparams.warmup_steps\n        warmup_scheme = hparams.warmup_scheme\n        utils.print_out(\n            ""  learning_rate=%g, warmup_steps=%d, warmup_scheme=%s""\n            % (hparams.learning_rate, warmup_steps, warmup_scheme)\n        )\n\n        # Apply inverse decay if global steps less than warmup steps.\n        # Inspired by https://arxiv.org/pdf/1706.03762.pdf (Section 5.3)\n        # When step < warmup_steps,\n        #   learing_rate *= warmup_factor ** (warmup_steps - step)\n        if warmup_scheme == ""t2t"":\n            # 0.01^(1/warmup_steps): we start with a lr, 100 times smaller\n            warmup_factor = tf.exp(tf.log(0.01) / warmup_steps)\n            inv_decay = warmup_factor ** (tf.to_float(warmup_steps - self.global_step))\n        else:\n            raise ValueError(""Unknown warmup scheme %s"" % warmup_scheme)\n\n        return tf.cond(\n            self.global_step < hparams.warmup_steps,\n            lambda: inv_decay * self.learning_rate,\n            lambda: self.learning_rate,\n            name=""learning_rate_warump_cond"",\n        )\n\n    def _get_decay_info(self, hparams):\n        """"""Return decay info based on decay_scheme.""""""\n        if hparams.decay_scheme in [""luong5"", ""luong10"", ""luong234""]:\n            decay_factor = 0.5\n            if hparams.decay_scheme == ""luong5"":\n                start_decay_step = int(hparams.num_train_steps / 2)\n                decay_times = 5\n            elif hparams.decay_scheme == ""luong10"":\n                start_decay_step = int(hparams.num_train_steps / 2)\n                decay_times = 10\n            elif hparams.decay_scheme == ""luong234"":\n                start_decay_step = int(hparams.num_train_steps * 2 / 3)\n                decay_times = 4\n            remain_steps = hparams.num_train_steps - start_decay_step\n            decay_steps = int(remain_steps / decay_times)\n        elif not hparams.decay_scheme:  # no decay\n            start_decay_step = hparams.num_train_steps\n            decay_steps = 0\n            decay_factor = 1.0\n        elif hparams.decay_scheme:\n            raise ValueError(""Unknown decay scheme %s"" % hparams.decay_scheme)\n        return start_decay_step, decay_steps, decay_factor\n\n    def _get_learning_rate_decay(self, hparams):\n        """"""Get learning rate decay.""""""\n        start_decay_step, decay_steps, decay_factor = self._get_decay_info(hparams)\n        utils.print_out(\n            ""  decay_scheme=%s, start_decay_step=%d, decay_steps %d, ""\n            ""decay_factor %g"" % (hparams.decay_scheme, start_decay_step, decay_steps, decay_factor)\n        )\n\n        return tf.cond(\n            self.global_step < start_decay_step,\n            lambda: self.learning_rate,\n            lambda: tf.train.exponential_decay(\n                self.learning_rate,\n                (self.global_step - start_decay_step),\n                decay_steps,\n                decay_factor,\n                staircase=True,\n            ),\n            name=""learning_rate_decay_cond"",\n        )\n\n    def init_embeddings(self, hparams, scope):\n        """"""Init embeddings.""""""\n        (\n            self.embedding_encoder,\n            self.embedding_decoder,\n        ) = model_helper.create_emb_for_encoder_and_decoder(\n            share_vocab=hparams.share_vocab,\n            src_vocab_size=self.src_vocab_size,\n            tgt_vocab_size=self.tgt_vocab_size,\n            src_embed_size=self.num_units,\n            tgt_embed_size=self.num_units,\n            num_enc_partitions=hparams.num_enc_emb_partitions,\n            num_dec_partitions=hparams.num_dec_emb_partitions,\n            src_vocab_file=hparams.src_vocab_file,\n            tgt_vocab_file=hparams.tgt_vocab_file,\n            src_embed_file=hparams.src_embed_file,\n            tgt_embed_file=hparams.tgt_embed_file,\n            use_char_encode=hparams.use_char_encode,\n            scope=scope,\n            embed_type=hparams.embedding_type,\n        )\n\n    def _get_train_summary(self):\n        """"""Get train summary.""""""\n        train_summary = tf.summary.merge(\n            [\n                tf.summary.scalar(""lr"", self.learning_rate),\n                tf.summary.scalar(""train_loss"", self.train_loss),\n            ]\n            + self.grad_norm_summary\n        )\n        return train_summary\n\n    def train(self, sess):\n        """"""Execute train graph.""""""\n        assert self.mode == tf.contrib.learn.ModeKeys.TRAIN\n        output_tuple = TrainOutputTuple(\n            train_summary=self.train_summary,\n            train_loss=self.train_loss,\n            predict_count=self.predict_count,\n            global_step=self.global_step,\n            word_count=self.word_count,\n            batch_size=self.batch_size,\n            grad_norm=self.grad_norm,\n            learning_rate=self.learning_rate,\n            mask_update_op=self.mask_update_op,\n            pruning_summary=self.pruning_summary,\n        )\n        return sess.run([self.update, output_tuple])\n\n    def eval(self, sess):\n        """"""Execute eval graph.""""""\n        assert self.mode == tf.contrib.learn.ModeKeys.EVAL\n        output_tuple = EvalOutputTuple(\n            eval_loss=self.eval_loss, predict_count=self.predict_count, batch_size=self.batch_size\n        )\n        return sess.run(output_tuple)\n\n    def build_graph(self, hparams, scope=None):\n        """"""Subclass must implement this method.\n\n        Creates a sequence-to-sequence model with dynamic RNN decoder API.\n        Args:\n          hparams: Hyperparameter configurations.\n          scope: VariableScope for the created subgraph; default ""dynamic_seq2seq"".\n\n        Returns:\n          A tuple of the form (logits, loss_tuple, final_context_state, sample_id),\n          where:\n            logits: float32 Tensor [batch_size x num_decoder_symbols].\n            loss: loss = the total loss / batch_size.\n            final_context_state: the final state of decoder RNN.\n            sample_id: sampling indices.\n\n        Raises:\n          ValueError: if encoder_type differs from mono and bi, or\n            attention_option is not (luong | scaled_luong |\n            bahdanau | normed_bahdanau).\n        """"""\n        utils.print_out(""# Creating %s graph ..."" % self.mode)\n\n        # Projection\n        if not self.extract_encoder_layers:\n            with tf.variable_scope(scope or ""build_network""):\n                with tf.variable_scope(""decoder/output_projection""):\n                    if hparams.projection_type == ""sparse"":\n                        self.output_layer = core_layers.MaskedFullyConnected(\n                            hparams.tgt_vocab_size, use_bias=False, name=""output_projection""\n                        )\n                    elif hparams.projection_type == ""dense"":\n                        self.output_layer = tf.layers.Dense(\n                            hparams.tgt_vocab_size, use_bias=False, name=""output_projection""\n                        )\n                    else:\n                        raise ValueError(""Unknown projection type %s!"" % hparams.projection_type)\n\n        with tf.variable_scope(scope or ""dynamic_seq2seq"", dtype=self.dtype):\n            # Encoder\n            if hparams.language_model:  # no encoder for language modeling\n                utils.print_out(""  language modeling: no encoder"")\n                self.encoder_outputs = None\n                encoder_state = None\n            else:\n                self.encoder_outputs, encoder_state = self._build_encoder(hparams)\n\n            # Skip decoder if extracting only encoder layers\n            if self.extract_encoder_layers:\n                return\n\n            # Decoder\n            logits, decoder_cell_outputs, sample_id, final_context_state = self._build_decoder(\n                self.encoder_outputs, encoder_state, hparams\n            )\n\n            # Loss\n            if self.mode != tf.contrib.learn.ModeKeys.INFER:\n                with tf.device(\n                    model_helper.get_device_str(self.num_encoder_layers - 1, self.num_gpus)\n                ):\n                    loss = self._compute_loss(logits, decoder_cell_outputs)\n            else:\n                loss = tf.constant(0.0)\n\n            # model pruning\n            if hparams.pruning_hparams is not None:\n                pruning_hparams = pruning.get_pruning_hparams().parse(hparams.pruning_hparams)\n                self.p = pruning.Pruning(pruning_hparams, global_step=self.global_step)\n                self.mask_update_op = self.p.conditional_mask_update_op()\n                masks = get_masks()\n                thresholds = get_thresholds()\n                masks_s = []\n                for index, mask in enumerate(masks):\n                    masks_s.append(\n                        tf.summary.scalar(mask.name + ""/sparsity"", tf.nn.zero_fraction(mask))\n                    )\n                    masks_s.append(\n                        tf.summary.scalar(\n                            thresholds[index].op.name + ""/threshold"", thresholds[index]\n                        )\n                    )\n                    masks_s.append(tf.summary.histogram(mask.name + ""/mask_tensor"", mask))\n                self.pruning_summary = tf.summary.merge(\n                    [\n                        tf.summary.scalar(""sparsity"", self.p._sparsity),\n                        tf.summary.scalar(""last_mask_update_step"", self.p._last_update_step),\n                    ]\n                    + masks_s\n                )\n            else:\n                self.mask_update_op = tf.no_op()\n                self.pruning_summary = tf.no_op()\n\n            return logits, loss, final_context_state, sample_id\n\n    @abc.abstractmethod\n    def _build_encoder(self, hparams):\n        """"""Subclass must implement this.\n\n        Build and run an RNN encoder.\n\n        Args:\n          hparams: Hyperparameters configurations.\n\n        Returns:\n          A tuple of encoder_outputs and encoder_state.\n        """"""\n        pass\n\n    def _build_encoder_cell(self, hparams, num_layers, num_residual_layers, base_gpu=0):\n        """"""Build a multi-layer RNN cell that can be used by encoder.""""""\n\n        return model_helper.create_rnn_cell(\n            unit_type=hparams.unit_type,\n            num_units=self.num_units,\n            num_layers=num_layers,\n            num_residual_layers=num_residual_layers,\n            forget_bias=hparams.forget_bias,\n            dropout=hparams.dropout,\n            num_gpus=hparams.num_gpus,\n            mode=self.mode,\n            base_gpu=base_gpu,\n            single_cell_fn=self.single_cell_fn,\n        )\n\n    def _get_infer_maximum_iterations(self, hparams, source_sequence_length):\n        """"""Maximum decoding steps at inference time.""""""\n        if hparams.tgt_max_len_infer:\n            maximum_iterations = hparams.tgt_max_len_infer\n            utils.print_out(""  decoding maximum_iterations %d"" % maximum_iterations)\n        else:\n            # TODO(thangluong): add decoding_length_factor flag\n            decoding_length_factor = 2.0\n            max_encoder_length = tf.reduce_max(source_sequence_length)\n            maximum_iterations = tf.to_int32(\n                tf.round(tf.to_float(max_encoder_length) * decoding_length_factor)\n            )\n        return maximum_iterations\n\n    def _build_decoder(self, encoder_outputs, encoder_state, hparams):\n        """"""Build and run a RNN decoder with a final projection layer.\n\n        Args:\n          encoder_outputs: The outputs of encoder for every time step.\n          encoder_state: The final state of the encoder.\n          hparams: The Hyperparameters configurations.\n\n        Returns:\n          A tuple of final logits and final decoder state:\n            logits: size [time, batch_size, vocab_size] when time_major=True.\n        """"""\n        tgt_sos_id = tf.cast(self.tgt_vocab_table.lookup(tf.constant(hparams.sos)), tf.int32)\n        tgt_eos_id = tf.cast(self.tgt_vocab_table.lookup(tf.constant(hparams.eos)), tf.int32)\n        iterator = self.iterator\n\n        # maximum_iteration: The maximum decoding steps.\n        maximum_iterations = self._get_infer_maximum_iterations(\n            hparams, iterator.source_sequence_length\n        )\n\n        # Decoder.\n        with tf.variable_scope(""decoder"") as decoder_scope:\n            cell, decoder_initial_state = self._build_decoder_cell(\n                hparams, encoder_outputs, encoder_state, iterator.source_sequence_length\n            )\n\n            # Optional ops depends on which mode we are in and which loss function we\n            # are using.\n            logits = tf.no_op()\n            decoder_cell_outputs = None\n\n            # Train or eval\n            if self.mode != tf.contrib.learn.ModeKeys.INFER:\n                # decoder_emp_inp: [max_time, batch_size, num_units]\n                target_input = iterator.target_input\n                if self.time_major:\n                    target_input = tf.transpose(target_input)\n                decoder_emb_inp = tf.nn.embedding_lookup(self.embedding_decoder, target_input)\n\n                # Helper\n                helper = tf.contrib.seq2seq.TrainingHelper(\n                    decoder_emb_inp, iterator.target_sequence_length, time_major=self.time_major\n                )\n\n                # Decoder\n                my_decoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, decoder_initial_state,)\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    output_time_major=self.time_major,\n                    swap_memory=True,\n                    scope=decoder_scope,\n                )\n\n                sample_id = outputs.sample_id\n\n                if self.num_sampled_softmax > 0:\n                    # Note: this is required when using sampled_softmax_loss.\n                    decoder_cell_outputs = outputs.rnn_output\n\n                # Note: there\'s a subtle difference here between train and inference.\n                # We could have set output_layer when create my_decoder\n                #   and shared more code between train and inference.\n                # We chose to apply the output_layer to all timesteps for speed:\n                #   10% improvements for small models & 20% for larger ones.\n                # If memory is a concern, we should apply output_layer per timestep.\n                num_layers = self.num_decoder_layers\n                num_gpus = self.num_gpus\n                device_id = num_layers if num_layers < num_gpus else (num_layers - 1)\n                # Colocate output layer with the last RNN cell if there is no extra GPU\n                # available. Otherwise, put last layer on a separate GPU.\n                with tf.device(model_helper.get_device_str(device_id, num_gpus)):\n                    logits = self.output_layer(outputs.rnn_output)\n\n                if self.num_sampled_softmax > 0:\n                    logits = tf.no_op()  # unused when using sampled softmax loss.\n\n            # Inference\n            else:\n                infer_mode = hparams.infer_mode\n                start_tokens = tf.fill([self.batch_size], tgt_sos_id)\n                end_token = tgt_eos_id\n                utils.print_out(\n                    ""  decoder: infer_mode=%sbeam_width=%d, length_penalty=%f""\n                    % (infer_mode, hparams.beam_width, hparams.length_penalty_weight)\n                )\n\n                if infer_mode == ""beam_search"":\n                    beam_width = hparams.beam_width\n                    length_penalty_weight = hparams.length_penalty_weight\n\n                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                        cell=cell,\n                        embedding=self.embedding_decoder,\n                        start_tokens=start_tokens,\n                        end_token=end_token,\n                        initial_state=decoder_initial_state,\n                        beam_width=beam_width,\n                        output_layer=self.output_layer,\n                        length_penalty_weight=length_penalty_weight,\n                    )\n                elif infer_mode == ""sample"":\n                    # Helper\n                    sampling_temperature = hparams.sampling_temperature\n                    assert sampling_temperature > 0.0, (\n                        ""sampling_temperature must greater than 0.0 when using sample"" "" decoder.""\n                    )\n                    helper = tf.contrib.seq2seq.SampleEmbeddingHelper(\n                        self.embedding_decoder,\n                        start_tokens,\n                        end_token,\n                        softmax_temperature=sampling_temperature,\n                        seed=self.random_seed,\n                    )\n                elif infer_mode == ""greedy"":\n                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                        self.embedding_decoder, start_tokens, end_token\n                    )\n                else:\n                    raise ValueError(""Unknown infer_mode \'%s\'"", infer_mode)\n\n                if infer_mode != ""beam_search"":\n                    my_decoder = tf.contrib.seq2seq.BasicDecoder(\n                        cell,\n                        helper,\n                        decoder_initial_state,\n                        output_layer=self.output_layer,  # applied per timestep\n                    )\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    maximum_iterations=maximum_iterations,\n                    output_time_major=self.time_major,\n                    swap_memory=True,\n                    scope=decoder_scope,\n                )\n\n                if infer_mode == ""beam_search"":\n                    sample_id = outputs.predicted_ids\n                else:\n                    logits = outputs.rnn_output\n                    sample_id = outputs.sample_id\n\n        return logits, decoder_cell_outputs, sample_id, final_context_state\n\n    def get_max_time(self, tensor):\n        time_axis = 0 if self.time_major else 1\n        return tensor.shape[time_axis].value or tf.shape(tensor)[time_axis]\n\n    @abc.abstractmethod\n    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state, source_sequence_length):\n        """"""Subclass must implement this.\n\n        Args:\n          hparams: Hyperparameters configurations.\n          encoder_outputs: The outputs of encoder for every time step.\n          encoder_state: The final state of the encoder.\n          source_sequence_length: sequence length of encoder_outputs.\n\n        Returns:\n          A tuple of a multi-layer RNN cell used by decoder and the intial state of\n          the decoder RNN.\n        """"""\n        pass\n\n    def _softmax_cross_entropy_loss(self, logits, decoder_cell_outputs, labels):\n        """"""Compute softmax loss or sampled softmax loss.""""""\n        if self.num_sampled_softmax > 0:\n\n            is_sequence = decoder_cell_outputs.shape.ndims == 3\n\n            if is_sequence:\n                labels = tf.reshape(labels, [-1, 1])\n                inputs = tf.reshape(decoder_cell_outputs, [-1, self.num_units])\n\n            crossent = tf.nn.sampled_softmax_loss(\n                weights=tf.transpose(self.output_layer.kernel),\n                biases=self.output_layer.bias or tf.zeros([self.tgt_vocab_size]),\n                labels=labels,\n                inputs=inputs,\n                num_sampled=self.num_sampled_softmax,\n                num_classes=self.tgt_vocab_size,\n                partition_strategy=""div"",\n                seed=self.random_seed,\n            )\n\n            if is_sequence:\n                if self.time_major:\n                    crossent = tf.reshape(crossent, [-1, self.batch_size])\n                else:\n                    crossent = tf.reshape(crossent, [self.batch_size, -1])\n\n        else:\n            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n\n        return crossent\n\n    def _compute_loss(self, logits, decoder_cell_outputs):\n        """"""Compute optimization loss.""""""\n        target_output = self.iterator.target_output\n        if self.time_major:\n            target_output = tf.transpose(target_output)\n        max_time = self.get_max_time(target_output)\n\n        crossent = self._softmax_cross_entropy_loss(logits, decoder_cell_outputs, target_output)\n\n        target_weights = tf.sequence_mask(\n            self.iterator.target_sequence_length, max_time, dtype=self.dtype\n        )\n        if self.time_major:\n            target_weights = tf.transpose(target_weights)\n\n        loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(self.batch_size)\n        return loss\n\n    def _get_infer_summary(self, hparams):\n        del hparams\n        return tf.no_op()\n\n    def infer(self, sess):\n        assert self.mode == tf.contrib.learn.ModeKeys.INFER\n        output_tuple = InferOutputTuple(\n            infer_logits=self.infer_logits,\n            infer_summary=self.infer_summary,\n            sample_id=self.sample_id,\n            sample_words=self.sample_words,\n        )\n        return sess.run(output_tuple)\n\n    def decode(self, sess):\n        """"""Decode a batch.\n\n        Args:\n          sess: tensorflow session to use.\n\n        Returns:\n          A tuple consiting of outputs, infer_summary.\n            outputs: of size [batch_size, time]\n        """"""\n        output_tuple = self.infer(sess)\n        sample_words = output_tuple.sample_words\n        infer_summary = output_tuple.infer_summary\n\n        # make sure outputs is of shape [batch_size, time] or [beam_width,\n        # batch_size, time] when using beam search.\n        if self.time_major:\n            sample_words = sample_words.transpose()\n        elif sample_words.ndim == 3:\n            # beam search output in [batch_size, time, beam_width] shape.\n            sample_words = sample_words.transpose([2, 0, 1])\n        return sample_words, infer_summary\n\n    def build_encoder_states(self, include_embeddings=False):\n        """"""Stack encoder states and return tensor [batch, length, layer, size].""""""\n        assert self.mode == tf.contrib.learn.ModeKeys.INFER\n        if include_embeddings:\n            stack_state_list = tf.stack([self.encoder_emb_inp] + self.encoder_state_list, 2)\n        else:\n            stack_state_list = tf.stack(self.encoder_state_list, 2)\n\n        # transform from [length, batch, ...] -> [batch, length, ...]\n        if self.time_major:\n            stack_state_list = tf.transpose(stack_state_list, [1, 0, 2, 3])\n\n        return stack_state_list\n\n\nclass Model(BaseModel):\n    """"""Sequence-to-sequence dynamic model.\n\n    This class implements a multi-layer recurrent neural network as encoder,\n    and a multi-layer recurrent neural network decoder.\n    """"""\n\n    def _build_encoder_from_sequence(self, hparams, sequence, sequence_length):\n        """"""Build an encoder from a sequence.\n\n        Args:\n          hparams: hyperparameters.\n          sequence: tensor with input sequence data.\n          sequence_length: tensor with length of the input sequence.\n\n        Returns:\n          encoder_outputs: RNN encoder outputs.\n          encoder_state: RNN encoder state.\n\n        Raises:\n          ValueError: if encoder_type is neither ""uni"" nor ""bi"".\n        """"""\n        num_layers = self.num_encoder_layers\n        num_residual_layers = self.num_encoder_residual_layers\n\n        if self.time_major:\n            sequence = tf.transpose(sequence)\n\n        with tf.variable_scope(""encoder"") as scope:\n            dtype = scope.dtype\n\n            self.encoder_emb_inp = self.encoder_emb_lookup_fn(self.embedding_encoder, sequence)\n\n            # Encoder_outputs: [max_time, batch_size, num_units]\n            if hparams.encoder_type == ""uni"":\n                utils.print_out(\n                    ""  num_layers = %d, num_residual_layers=%d"" % (num_layers, num_residual_layers)\n                )\n                cell = self._build_encoder_cell(hparams, num_layers, num_residual_layers)\n\n                encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n                    cell,\n                    self.encoder_emb_inp,\n                    dtype=dtype,\n                    sequence_length=sequence_length,\n                    time_major=self.time_major,\n                    swap_memory=True,\n                )\n            elif hparams.encoder_type == ""bi"":\n                num_bi_layers = int(num_layers / 2)\n                num_bi_residual_layers = int(num_residual_layers / 2)\n                utils.print_out(\n                    ""  num_bi_layers = %d, num_bi_residual_layers=%d""\n                    % (num_bi_layers, num_bi_residual_layers)\n                )\n\n                encoder_outputs, bi_encoder_state = self._build_bidirectional_rnn(\n                    inputs=self.encoder_emb_inp,\n                    sequence_length=sequence_length,\n                    dtype=dtype,\n                    hparams=hparams,\n                    num_bi_layers=num_bi_layers,\n                    num_bi_residual_layers=num_bi_residual_layers,\n                )\n\n                if num_bi_layers == 1:\n                    encoder_state = bi_encoder_state\n                else:\n                    # alternatively concat forward and backward states\n                    encoder_state = []\n                    for layer_id in range(num_bi_layers):\n                        encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n                        encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n                    encoder_state = tuple(encoder_state)\n            else:\n                raise ValueError(""Unknown encoder_type %s"" % hparams.encoder_type)\n\n        # Use the top layer for now\n        self.encoder_state_list = [encoder_outputs]\n\n        return encoder_outputs, encoder_state\n\n    def _build_encoder(self, hparams):\n        """"""Build encoder from source.""""""\n        utils.print_out(""# Build a basic encoder"")\n        return self._build_encoder_from_sequence(\n            hparams, self.iterator.source, self.iterator.source_sequence_length\n        )\n\n    def _build_bidirectional_rnn(\n        self,\n        inputs,\n        sequence_length,\n        dtype,\n        hparams,\n        num_bi_layers,\n        num_bi_residual_layers,\n        base_gpu=0,\n    ):\n        """"""Create and call biddirectional RNN cells.\n\n        Args:\n          num_residual_layers: Number of residual layers from top to bottom. For\n            example, if `num_bi_layers=4` and `num_residual_layers=2`, the last 2 RNN\n            layers in each RNN cell will be wrapped with `ResidualWrapper`.\n          base_gpu: The gpu device id to use for the first forward RNN layer. The\n            i-th forward RNN layer will use `(base_gpu + i) % num_gpus` as its\n            device id. The `base_gpu` for backward RNN cell is `(base_gpu +\n            num_bi_layers)`.\n\n        Returns:\n          The concatenated bidirectional output and the bidirectional RNN cell""s\n          state.\n        """"""\n        # Construct forward and backward cells\n        fw_cell = self._build_encoder_cell(\n            hparams, num_bi_layers, num_bi_residual_layers, base_gpu=base_gpu\n        )\n        bw_cell = self._build_encoder_cell(\n            hparams, num_bi_layers, num_bi_residual_layers, base_gpu=(base_gpu + num_bi_layers)\n        )\n\n        bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n            fw_cell,\n            bw_cell,\n            inputs,\n            dtype=dtype,\n            sequence_length=sequence_length,\n            time_major=self.time_major,\n            swap_memory=True,\n        )\n\n        return tf.concat(bi_outputs, -1), bi_state\n\n    def _build_decoder_cell(\n        self, hparams, encoder_outputs, encoder_state, source_sequence_length, base_gpu=0\n    ):\n        """"""Build an RNN cell that can be used by decoder.""""""\n        # We only make use of encoder_outputs in attention-based models\n        if hparams.attention:\n            raise ValueError(""BasicModel doesn\'t support attention."")\n\n        cell = model_helper.create_rnn_cell(\n            unit_type=hparams.unit_type,\n            num_units=self.num_units,\n            num_layers=self.num_decoder_layers,\n            num_residual_layers=self.num_decoder_residual_layers,\n            forget_bias=hparams.forget_bias,\n            dropout=hparams.dropout,\n            num_gpus=self.num_gpus,\n            mode=self.mode,\n            single_cell_fn=self.single_cell_fn,\n            base_gpu=base_gpu,\n        )\n\n        if hparams.language_model:\n            encoder_state = cell.zero_state(self.batch_size, self.dtype)\n        elif not hparams.pass_hidden_state:\n            raise ValueError(\n                ""For non-attentional model, "" ""pass_hidden_state needs to be set to True""\n            )\n\n        # For beam search, we need to replicate encoder infos beam_width times\n        if self.mode == tf.contrib.learn.ModeKeys.INFER and hparams.infer_mode == ""beam_search"":\n            decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n                encoder_state, multiplier=hparams.beam_width\n            )\n        else:\n            decoder_initial_state = encoder_state\n\n        return cell, decoder_initial_state\n'"
examples/sparse_gnmt/gnmt/model_helper.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n#   quantization operations\n#   pruning operations\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: skip-file\n""""""Utility functions for building models.""""""\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport time\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import lookup_ops\nfrom .utils import misc_utils as utils, vocab_utils, iterator_utils\n\n__all__ = [\n    ""get_initializer"",\n    ""get_device_str"",\n    ""create_train_model"",\n    ""create_eval_model"",\n    ""create_infer_model"",\n    ""create_emb_for_encoder_and_decoder"",\n    ""create_rnn_cell"",\n    ""gradient_clip"",\n    ""create_or_load_model"",\n    ""load_model"",\n    ""avg_checkpoints"",\n    ""compute_perplexity"",\n]\n\n# If a vocab size is greater than this value, put the embedding on cpu instead\nVOCAB_SIZE_THRESHOLD_CPU = 50000\n\n# Collection for all the tensors involved in the quantization process\n_QUANTIZATION_COLLECTION = ""qunatization""\n\n\ndef get_initializer(init_op, seed=None, init_weight=None):\n    """"""Create an initializer. init_weight is only for uniform.""""""\n    if init_op == ""uniform"":\n        assert init_weight\n        return tf.random_uniform_initializer(-init_weight, init_weight, seed=seed)\n    elif init_op == ""glorot_normal"":\n        return tf.keras.initializers.glorot_normal(seed=seed)\n    elif init_op == ""glorot_uniform"":\n        return tf.keras.initializers.glorot_uniform(seed=seed)\n    else:\n        raise ValueError(""Unknown init_op %s"" % init_op)\n\n\ndef get_device_str(device_id, num_gpus):\n    """"""Return a device string for multi-GPU setup.""""""\n    if num_gpus == 0:\n        return ""/cpu:0""\n    device_str_output = ""/gpu:%d"" % (device_id % num_gpus)\n    return device_str_output\n\n\nclass ExtraArgs(\n    collections.namedtuple(\n        ""ExtraArgs"",\n        (""single_cell_fn"", ""model_device_fn"", ""attention_mechanism_fn"", ""encoder_emb_lookup_fn""),\n    )\n):\n    pass\n\n\nclass TrainModel(\n    collections.namedtuple(""TrainModel"", (""graph"", ""model"", ""iterator"", ""skip_count_placeholder""))\n):\n    pass\n\n\ndef create_train_model(model_creator, hparams, scope=None, num_workers=1, jobid=0, extra_args=None):\n    """"""Create train graph, model, and iterator.""""""\n    src_file = ""%s.%s"" % (hparams.train_prefix, hparams.src)\n    tgt_file = ""%s.%s"" % (hparams.train_prefix, hparams.tgt)\n    src_vocab_file = hparams.src_vocab_file\n    tgt_vocab_file = hparams.tgt_vocab_file\n\n    graph = tf.Graph()\n\n    with graph.as_default(), tf.container(scope or ""train""):\n        src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(\n            src_vocab_file, tgt_vocab_file, hparams.share_vocab\n        )\n\n        src_dataset = tf.data.TextLineDataset(tf.gfile.Glob(src_file))\n        tgt_dataset = tf.data.TextLineDataset(tf.gfile.Glob(tgt_file))\n        skip_count_placeholder = tf.placeholder(shape=(), dtype=tf.int64)\n\n        iterator = iterator_utils.get_iterator(\n            src_dataset,\n            tgt_dataset,\n            src_vocab_table,\n            tgt_vocab_table,\n            batch_size=hparams.batch_size,\n            sos=hparams.sos,\n            eos=hparams.eos,\n            random_seed=hparams.random_seed,\n            num_buckets=hparams.num_buckets,\n            src_max_len=hparams.src_max_len,\n            tgt_max_len=hparams.tgt_max_len,\n            skip_count=skip_count_placeholder,\n            num_shards=num_workers,\n            shard_index=jobid,\n            use_char_encode=hparams.use_char_encode,\n        )\n\n        # Note: One can set model_device_fn to\n        # `tf.train.replica_device_setter(ps_tasks)` for distributed training.\n        model_device_fn = None\n        if extra_args:\n            model_device_fn = extra_args.model_device_fn\n        with tf.device(model_device_fn):\n            model = model_creator(\n                hparams,\n                iterator=iterator,\n                mode=tf.contrib.learn.ModeKeys.TRAIN,\n                source_vocab_table=src_vocab_table,\n                target_vocab_table=tgt_vocab_table,\n                scope=scope,\n                extra_args=extra_args,\n            )\n\n    return TrainModel(\n        graph=graph, model=model, iterator=iterator, skip_count_placeholder=skip_count_placeholder\n    )\n\n\nclass EvalModel(\n    collections.namedtuple(\n        ""EvalModel"", (""graph"", ""model"", ""src_file_placeholder"", ""tgt_file_placeholder"", ""iterator"")\n    )\n):\n    pass\n\n\ndef create_eval_model(model_creator, hparams, scope=None, extra_args=None):\n    """"""Create train graph, model, src/tgt file holders, and iterator.""""""\n    src_vocab_file = hparams.src_vocab_file\n    tgt_vocab_file = hparams.tgt_vocab_file\n    graph = tf.Graph()\n\n    with graph.as_default(), tf.container(scope or ""eval""):\n        src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(\n            src_vocab_file, tgt_vocab_file, hparams.share_vocab\n        )\n        reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_file(\n            tgt_vocab_file, default_value=vocab_utils.UNK\n        )\n\n        src_file_placeholder = tf.placeholder(shape=(), dtype=tf.string)\n        tgt_file_placeholder = tf.placeholder(shape=(), dtype=tf.string)\n        src_dataset = tf.data.TextLineDataset(src_file_placeholder)\n        tgt_dataset = tf.data.TextLineDataset(tgt_file_placeholder)\n        iterator = iterator_utils.get_iterator(\n            src_dataset,\n            tgt_dataset,\n            src_vocab_table,\n            tgt_vocab_table,\n            hparams.batch_size,\n            sos=hparams.sos,\n            eos=hparams.eos,\n            random_seed=hparams.random_seed,\n            num_buckets=hparams.num_buckets,\n            src_max_len=hparams.src_max_len_infer,\n            tgt_max_len=hparams.tgt_max_len_infer,\n            use_char_encode=hparams.use_char_encode,\n        )\n        model = model_creator(\n            hparams,\n            iterator=iterator,\n            mode=tf.contrib.learn.ModeKeys.EVAL,\n            source_vocab_table=src_vocab_table,\n            target_vocab_table=tgt_vocab_table,\n            reverse_target_vocab_table=reverse_tgt_vocab_table,\n            scope=scope,\n            extra_args=extra_args,\n        )\n    return EvalModel(\n        graph=graph,\n        model=model,\n        src_file_placeholder=src_file_placeholder,\n        tgt_file_placeholder=tgt_file_placeholder,\n        iterator=iterator,\n    )\n\n\nclass InferModel(\n    collections.namedtuple(\n        ""InferModel"", (""graph"", ""model"", ""src_placeholder"", ""batch_size_placeholder"", ""iterator"")\n    )\n):\n    pass\n\n\ndef create_infer_model(model_creator, hparams, scope=None, extra_args=None):\n    """"""Create inference model.""""""\n    graph = tf.Graph()\n    src_vocab_file = hparams.src_vocab_file\n    tgt_vocab_file = hparams.tgt_vocab_file\n\n    with graph.as_default(), tf.container(scope or ""infer""):\n        src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(\n            src_vocab_file, tgt_vocab_file, hparams.share_vocab\n        )\n        reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_file(\n            tgt_vocab_file, default_value=vocab_utils.UNK\n        )\n\n        src_placeholder = tf.placeholder(shape=[None], dtype=tf.string)\n        batch_size_placeholder = tf.placeholder(shape=[], dtype=tf.int64)\n\n        src_dataset = tf.data.Dataset.from_tensor_slices(src_placeholder)\n        iterator = iterator_utils.get_infer_iterator(\n            src_dataset,\n            src_vocab_table,\n            batch_size=batch_size_placeholder,\n            eos=hparams.eos,\n            src_max_len=hparams.src_max_len_infer,\n            use_char_encode=hparams.use_char_encode,\n        )\n        model = model_creator(\n            hparams,\n            iterator=iterator,\n            mode=tf.contrib.learn.ModeKeys.INFER,\n            source_vocab_table=src_vocab_table,\n            target_vocab_table=tgt_vocab_table,\n            reverse_target_vocab_table=reverse_tgt_vocab_table,\n            scope=scope,\n            extra_args=extra_args,\n        )\n    return InferModel(\n        graph=graph,\n        model=model,\n        src_placeholder=src_placeholder,\n        batch_size_placeholder=batch_size_placeholder,\n        iterator=iterator,\n    )\n\n\ndef _get_embed_device(vocab_size):\n    """"""Decide on which device to place an embed matrix given its vocab size.""""""\n    if vocab_size > VOCAB_SIZE_THRESHOLD_CPU:\n        return ""/cpu:0""\n    else:\n        return ""/gpu:0""\n\n\ndef _create_pretrained_emb_from_txt(\n    vocab_file, embed_file, num_trainable_tokens=3, dtype=tf.float32, scope=None\n):\n    """"""Load pretrain embeding from embed_file, and return an embedding matrix.\n\n    Args:\n      embed_file: Path to a Glove formated embedding txt file.\n      num_trainable_tokens: Make the first n tokens in the vocab file as trainable\n        variables. Default is 3, which is ""<unk>"", ""<s>"" and ""</s>"".\n    """"""\n    vocab, _ = vocab_utils.load_vocab(vocab_file)\n    trainable_tokens = vocab[:num_trainable_tokens]\n\n    utils.print_out(""# Using pretrained embedding: %s."" % embed_file)\n    utils.print_out(""  with trainable tokens: "")\n\n    emb_dict, emb_size = vocab_utils.load_embed_txt(embed_file)\n    for token in trainable_tokens:\n        utils.print_out(""    %s"" % token)\n        if token not in emb_dict:\n            emb_dict[token] = [0.0] * emb_size\n\n    emb_mat = np.array([emb_dict[token] for token in vocab], dtype=dtype.as_numpy_dtype())\n    emb_mat = tf.constant(emb_mat)\n    emb_mat_const = tf.slice(emb_mat, [num_trainable_tokens, 0], [-1, -1])\n    with tf.variable_scope(scope or ""pretrain_embeddings"", dtype=dtype) as scope:\n        with tf.device(_get_embed_device(num_trainable_tokens)):\n            emb_mat_var = tf.get_variable(""emb_mat_var"", [num_trainable_tokens, emb_size])\n    return tf.concat([emb_mat_var, emb_mat_const], 0)\n\n\ndef _create_or_load_embed(\n    embed_name, vocab_file, embed_file, vocab_size, embed_size, dtype, embed_type=""dense""\n):\n    """"""Create a new or load an existing embedding matrix.""""""\n    if vocab_file and embed_file:\n        embedding = _create_pretrained_emb_from_txt(vocab_file, embed_file)\n    else:\n        with tf.device(_get_embed_device(vocab_size)):\n            if embed_type == ""dense"":\n                embedding = tf.get_variable(embed_name, [vocab_size, embed_size], dtype)\n            elif embed_type == ""sparse"":\n                embedding = tf.get_variable(embed_name, [vocab_size, embed_size], dtype)\n                embedding = tf.contrib.model_pruning.apply_mask(embedding, embed_name)\n            else:\n                raise ValueError(""Unknown embedding type %s!"" % embed_type)\n    return embedding\n\n\ndef create_emb_for_encoder_and_decoder(\n    share_vocab,\n    src_vocab_size,\n    tgt_vocab_size,\n    src_embed_size,\n    tgt_embed_size,\n    embed_type=""dense"",\n    dtype=tf.float32,\n    num_enc_partitions=0,\n    num_dec_partitions=0,\n    src_vocab_file=None,\n    tgt_vocab_file=None,\n    src_embed_file=None,\n    tgt_embed_file=None,\n    use_char_encode=False,\n    scope=None,\n):\n    """"""Create embedding matrix for both encoder and decoder.\n\n    Args:\n      share_vocab: A boolean. Whether to share embedding matrix for both\n        encoder and decoder.\n      src_vocab_size: An integer. The source vocab size.\n      tgt_vocab_size: An integer. The target vocab size.\n      src_embed_size: An integer. The embedding dimension for the encoder\'s\n        embedding.\n      tgt_embed_size: An integer. The embedding dimension for the decoder\'s\n        embedding.\n      dtype: dtype of the embedding matrix. Default to float32.\n      num_enc_partitions: number of partitions used for the encoder\'s embedding\n        vars.\n      num_dec_partitions: number of partitions used for the decoder\'s embedding\n        vars.\n      scope: VariableScope for the created subgraph. Default to ""embedding"".\n\n    Returns:\n      embedding_encoder: Encoder\'s embedding matrix.\n      embedding_decoder: Decoder\'s embedding matrix.\n\n    Raises:\n      ValueError: if use share_vocab but source and target have different vocab\n        size.\n    """"""\n    if num_enc_partitions <= 1:\n        enc_partitioner = None\n    else:\n        # Note: num_partitions > 1 is required for distributed training due to\n        # embedding_lookup tries to colocate single partition-ed embedding variable\n        # with lookup ops. This may cause embedding variables being placed on worker\n        # jobs.\n        enc_partitioner = tf.fixed_size_partitioner(num_enc_partitions)\n\n    if num_dec_partitions <= 1:\n        dec_partitioner = None\n    else:\n        # Note: num_partitions > 1 is required for distributed training due to\n        # embedding_lookup tries to colocate single partition-ed embedding variable\n        # with lookup ops. This may cause embedding variables being placed on worker\n        # jobs.\n        dec_partitioner = tf.fixed_size_partitioner(num_dec_partitions)\n\n    if src_embed_file and enc_partitioner:\n        raise ValueError(\n            ""Can\'t set num_enc_partitions > 1 when using pretrained encoder "" ""embedding""\n        )\n\n    if tgt_embed_file and dec_partitioner:\n        raise ValueError(\n            ""Can\'t set num_dec_partitions > 1 when using pretrained decdoer "" ""embedding""\n        )\n\n    with tf.variable_scope(scope or ""embeddings"", dtype=dtype, partitioner=enc_partitioner):\n        # Share embedding\n        if share_vocab:\n            if src_vocab_size != tgt_vocab_size:\n                raise ValueError(\n                    ""Share embedding but different src/tgt vocab sizes""\n                    "" %d vs. %d"" % (src_vocab_size, tgt_vocab_size)\n                )\n            assert src_embed_size == tgt_embed_size\n            utils.print_out(""# Use the same embedding for source and target"")\n            vocab_file = src_vocab_file or tgt_vocab_file\n            embed_file = src_embed_file or tgt_embed_file\n\n            embedding_encoder = _create_or_load_embed(\n                ""embedding_share"",\n                vocab_file,\n                embed_file,\n                src_vocab_size,\n                src_embed_size,\n                dtype,\n                embed_type=embed_type,\n            )\n            embedding_decoder = embedding_encoder\n        else:\n            if not use_char_encode:\n                with tf.variable_scope(""encoder"", partitioner=enc_partitioner):\n                    embedding_encoder = _create_or_load_embed(\n                        ""embedding_encoder"",\n                        src_vocab_file,\n                        src_embed_file,\n                        src_vocab_size,\n                        src_embed_size,\n                        dtype,\n                        embed_type=embed_type,\n                    )\n            else:\n                embedding_encoder = None\n\n            with tf.variable_scope(""decoder"", partitioner=dec_partitioner):\n                embedding_decoder = _create_or_load_embed(\n                    ""embedding_decoder"",\n                    tgt_vocab_file,\n                    tgt_embed_file,\n                    tgt_vocab_size,\n                    tgt_embed_size,\n                    dtype,\n                    embed_type=embed_type,\n                )\n\n    return embedding_encoder, embedding_decoder\n\n\ndef _single_cell(\n    unit_type,\n    num_units,\n    forget_bias,\n    dropout,\n    mode,\n    residual_connection=False,\n    device_str=None,\n    residual_fn=None,\n):\n    """"""Create an instance of a single RNN cell.""""""\n    # dropout (= 1 - keep_prob) is set to 0 during eval and infer\n    dropout = dropout if mode == tf.contrib.learn.ModeKeys.TRAIN else 0.0\n\n    # Cell Type\n    if unit_type == ""lstm"":\n        utils.print_out(""  LSTM, forget_bias=%g"" % forget_bias, new_line=False)\n        single_cell = tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=forget_bias)\n    elif unit_type == ""gru"":\n        utils.print_out(""  GRU"", new_line=False)\n        single_cell = tf.contrib.rnn.GRUCell(num_units)\n    elif unit_type == ""layer_norm_lstm"":\n        utils.print_out(""  Layer Normalized LSTM, forget_bias=%g"" % forget_bias, new_line=False)\n        single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n            num_units, forget_bias=forget_bias, layer_norm=True\n        )\n    elif unit_type == ""nas"":\n        utils.print_out(""  NASCell"", new_line=False)\n        single_cell = tf.contrib.rnn.NASCell(num_units)\n    elif unit_type == ""mlstm"":\n        utils.print_out(""  Masked_LSTM, forget_bias=%g"" % forget_bias, new_line=False)\n        single_cell = tf.contrib.model_pruning.MaskedBasicLSTMCell(\n            num_units, forget_bias=forget_bias\n        )\n    else:\n        raise ValueError(""Unknown unit type %s!"" % unit_type)\n\n    # Dropout (= 1 - keep_prob)\n    if dropout > 0.0:\n        single_cell = tf.contrib.rnn.DropoutWrapper(\n            cell=single_cell, input_keep_prob=(1.0 - dropout)\n        )\n        utils.print_out(""  %s, dropout=%g "" % (type(single_cell).__name__, dropout), new_line=False)\n\n    # Residual\n    if residual_connection:\n        single_cell = tf.contrib.rnn.ResidualWrapper(single_cell, residual_fn=residual_fn)\n        utils.print_out(""  %s"" % type(single_cell).__name__, new_line=False)\n\n    # Device Wrapper\n    if device_str:\n        single_cell = tf.contrib.rnn.DeviceWrapper(single_cell, device_str)\n        utils.print_out(\n            ""  %s, device=%s"" % (type(single_cell).__name__, device_str), new_line=False\n        )\n\n    return single_cell\n\n\ndef _cell_list(\n    unit_type,\n    num_units,\n    num_layers,\n    num_residual_layers,\n    forget_bias,\n    dropout,\n    mode,\n    num_gpus,\n    base_gpu=0,\n    single_cell_fn=None,\n    residual_fn=None,\n):\n    """"""Create a list of RNN cells.""""""\n    if not single_cell_fn:\n        single_cell_fn = _single_cell\n\n    # Multi-GPU\n    cell_list = []\n    for i in range(num_layers):\n        utils.print_out(""  cell %d"" % i, new_line=False)\n        single_cell = single_cell_fn(\n            unit_type=unit_type,\n            num_units=num_units,\n            forget_bias=forget_bias,\n            dropout=dropout,\n            mode=mode,\n            residual_connection=(i >= num_layers - num_residual_layers),\n            device_str=get_device_str(i + base_gpu, num_gpus),\n            residual_fn=residual_fn,\n        )\n        utils.print_out("""")\n        cell_list.append(single_cell)\n\n    return cell_list\n\n\ndef create_rnn_cell(\n    unit_type,\n    num_units,\n    num_layers,\n    num_residual_layers,\n    forget_bias,\n    dropout,\n    mode,\n    num_gpus,\n    base_gpu=0,\n    single_cell_fn=None,\n):\n    """"""Create multi-layer RNN cell.\n\n    Args:\n      unit_type: string representing the unit type, i.e. ""lstm"".\n      num_units: the depth of each unit.\n      num_layers: number of cells.\n      num_residual_layers: Number of residual layers from top to bottom. For\n        example, if `num_layers=4` and `num_residual_layers=2`, the last 2 RNN\n        cells in the returned list will be wrapped with `ResidualWrapper`.\n      forget_bias: the initial forget bias of the RNNCell(s).\n      dropout: floating point value between 0.0 and 1.0:\n        the probability of dropout.  this is ignored if `mode != TRAIN`.\n      mode: either tf.contrib.learn.TRAIN/EVAL/INFER\n      num_gpus: The number of gpus to use when performing round-robin\n        placement of layers.\n      base_gpu: The gpu device id to use for the first RNN cell in the\n        returned list. The i-th RNN cell will use `(base_gpu + i) % num_gpus`\n        as its device id.\n      single_cell_fn: allow for adding customized cell.\n        When not specified, we default to model_helper._single_cell\n    Returns:\n      An `RNNCell` instance.\n    """"""\n    cell_list = _cell_list(\n        unit_type=unit_type,\n        num_units=num_units,\n        num_layers=num_layers,\n        num_residual_layers=num_residual_layers,\n        forget_bias=forget_bias,\n        dropout=dropout,\n        mode=mode,\n        num_gpus=num_gpus,\n        base_gpu=base_gpu,\n        single_cell_fn=single_cell_fn,\n    )\n\n    if len(cell_list) == 1:  # Single layer.\n        return cell_list[0]\n    else:  # Multi layers\n        return tf.contrib.rnn.MultiRNNCell(cell_list)\n\n\ndef gradient_clip(gradients, max_gradient_norm):\n    """"""Clipping gradients of a model.""""""\n    clipped_gradients, gradient_norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n    gradient_norm_summary = [tf.summary.scalar(""grad_norm"", gradient_norm)]\n    gradient_norm_summary.append(\n        tf.summary.scalar(""clipped_gradient"", tf.global_norm(clipped_gradients))\n    )\n\n    return clipped_gradients, gradient_norm_summary, gradient_norm\n\n\ndef print_variables_in_ckpt(ckpt_path):\n    """"""Print a list of variables in a checkpoint together with their shapes.""""""\n    utils.print_out(""# Variables in ckpt %s"" % ckpt_path)\n    reader = tf.train.NewCheckpointReader(ckpt_path)\n    variable_map = reader.get_variable_to_shape_map()\n    for key in sorted(variable_map.keys()):\n        utils.print_out(""  %s: %s"" % (key, variable_map[key]))\n\n\ndef load_model(model, ckpt_path, session, name):\n    """"""Load model from a checkpoint.""""""\n    start_time = time.time()\n    try:\n        model.saver.restore(session, ckpt_path)\n    except tf.errors.NotFoundError as e:\n        utils.print_out(""Can\'t load checkpoint"")\n        print_variables_in_ckpt(ckpt_path)\n        utils.print_out(""%s"" % str(e))\n    session.run(tf.tables_initializer())\n    utils.print_out(\n        ""  loaded %s model parameters from %s, time %.2fs""\n        % (name, ckpt_path, time.time() - start_time)\n    )\n    return model\n\n\ndef load_quantized_model(model, ckpt_path, session, name):\n    """"""Loads quantized model and dequantizes variables""""""\n    start_time = time.time()\n    dequant_ops = []\n    for tsr in tf.trainable_variables():\n        with tf.variable_scope(tsr.name.split("":"")[0], reuse=True):\n            quant_tsr = tf.get_variable(""quantized"", dtype=tf.qint8)\n            min_range = tf.get_variable(""min_range"")\n            max_range = tf.get_variable(""max_range"")\n            dequant_ops.append(tsr.assign(tf.dequantize(quant_tsr, min_range, max_range, ""SCALED"")))\n    restore_list = [tsr for tsr in tf.global_variables() if tsr not in tf.trainable_variables()]\n\n    saver = tf.train.Saver(restore_list)\n    try:\n        saver.restore(session, ckpt_path)\n    except tf.errors.NotFoundError as e:\n        utils.print_out(""Can\'t load checkpoint"")\n        print_variables_in_ckpt(ckpt_path)\n        utils.print_out(""%s"" % str(e))\n    session.run(tf.tables_initializer())\n    session.run(dequant_ops)\n    utils.print_out(\n        ""  loaded %s model parameters from %s, time %.2fs""\n        % (name, ckpt_path, time.time() - start_time)\n    )\n    return model\n\n\ndef add_quatization_variables(model):\n    """"""Add to graph quantization variables""""""\n    with model.graph.as_default():\n        for tsr in tf.trainable_variables():\n            with tf.variable_scope(tsr.name.split("":"")[0]):\n                output, min_range, max_range = tf.quantize(\n                    tsr, tf.reduce_min(tsr), tf.reduce_max(tsr), tf.qint8, mode=""SCALED""\n                )\n                tf.get_variable(\n                    ""quantized"",\n                    initializer=output,\n                    trainable=False,\n                    collections=[_QUANTIZATION_COLLECTION],\n                )\n                tf.get_variable(\n                    ""min_range"",\n                    initializer=min_range,\n                    trainable=False,\n                    collections=[_QUANTIZATION_COLLECTION],\n                )\n                tf.get_variable(\n                    ""max_range"",\n                    initializer=max_range,\n                    trainable=False,\n                    collections=[_QUANTIZATION_COLLECTION],\n                )\n\n\ndef quantize_checkpoint(session, ckpt_path):\n    """"""Quantize current loaded model and saves checkpoint in ckpt_path""""""\n    save_list = [tsr for tsr in tf.global_variables() if tsr not in tf.trainable_variables()]\n    saver = tf.train.Saver(save_list)\n    session.run(tf.variables_initializer(tf.get_collection(_QUANTIZATION_COLLECTION)))\n    saver.save(session, ckpt_path)\n    utils.print_out(""Saved quantized checkpoint as %s"" % ckpt_path)\n\n\ndef avg_checkpoints(model_dir, num_last_checkpoints, global_step, global_step_name):\n    """"""Average the last N checkpoints in the model_dir.""""""\n    checkpoint_state = tf.train.get_checkpoint_state(model_dir)\n    if not checkpoint_state:\n        utils.print_out(""# No checkpoint file found in directory: %s"" % model_dir)\n        return None\n\n    # Checkpoints are ordered from oldest to newest.\n    checkpoints = checkpoint_state.all_model_checkpoint_paths[-num_last_checkpoints:]\n\n    if len(checkpoints) < num_last_checkpoints:\n        utils.print_out(\n            ""# Skipping averaging checkpoints because not enough checkpoints is "" ""avaliable.""\n        )\n        return None\n\n    avg_model_dir = os.path.join(model_dir, ""avg_checkpoints"")\n    if not tf.gfile.Exists(avg_model_dir):\n        utils.print_out(\n            ""# Creating new directory %s for saving averaged checkpoints."" % avg_model_dir\n        )\n        tf.gfile.MakeDirs(avg_model_dir)\n\n    utils.print_out(""# Reading and averaging variables in checkpoints:"")\n    var_list = tf.contrib.framework.list_variables(checkpoints[0])\n    var_values, var_dtypes = {}, {}\n    for (name, shape) in var_list:\n        if name != global_step_name:\n            var_values[name] = np.zeros(shape)\n\n    for checkpoint in checkpoints:\n        utils.print_out(""    %s"" % checkpoint)\n        reader = tf.contrib.framework.load_checkpoint(checkpoint)\n        for name in var_values:\n            tensor = reader.get_tensor(name)\n            var_dtypes[name] = tensor.dtype\n            var_values[name] += tensor\n\n    for name in var_values:\n        var_values[name] /= len(checkpoints)\n\n    # Build a graph with same variables in the checkpoints, and save the averaged\n    # variables into the avg_model_dir.\n    with tf.Graph().as_default():\n        tf_vars = [\n            tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[name])\n            for v in var_values\n        ]\n\n        placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n        assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n        tf.Variable(global_step, name=global_step_name, trainable=False)\n        saver = tf.train.Saver(tf.all_variables())\n\n        with tf.Session() as sess:\n            sess.run(tf.initialize_all_variables())\n            for p, assign_op, (name, value) in zip(\n                placeholders, assign_ops, six.iteritems(var_values)\n            ):\n                sess.run(assign_op, {p: value})\n\n            # Use the built saver to save the averaged checkpoint. Only keep 1\n            # checkpoint and the best checkpoint will be moved to avg_best_metric_dir.\n            saver.save(sess, os.path.join(avg_model_dir, ""translate.ckpt""))\n\n    return avg_model_dir\n\n\ndef create_or_load_model(model, model_dir, session, name):\n    """"""Create translation model and initialize or load parameters in session.""""""\n    latest_ckpt = tf.train.latest_checkpoint(model_dir)\n    if latest_ckpt:\n        model = load_model(model, latest_ckpt, session, name)\n    else:\n        start_time = time.time()\n        session.run(tf.global_variables_initializer())\n        session.run(tf.tables_initializer())\n        utils.print_out(\n            ""  created %s model with fresh parameters, time %.2fs""\n            % (name, time.time() - start_time)\n        )\n\n    global_step = model.global_step.eval(session=session)\n    return model, global_step\n\n\ndef compute_perplexity(model, sess, name):\n    """"""Compute perplexity of the output of the model.\n\n    Args:\n      model: model for compute perplexity.\n      sess: tensorflow session to use.\n      name: name of the batch.\n\n    Returns:\n      The perplexity of the eval outputs.\n    """"""\n    total_loss = 0\n    total_predict_count = 0\n    start_time = time.time()\n\n    while True:\n        try:\n            output_tuple = model.eval(sess)\n            total_loss += output_tuple.eval_loss * output_tuple.batch_size\n            total_predict_count += output_tuple.predict_count\n        except tf.errors.OutOfRangeError:\n            break\n\n    perplexity = utils.safe_exp(total_loss / total_predict_count)\n    utils.print_time(""  eval %s: perplexity %.2f"" % (name, perplexity), start_time)\n    return perplexity\n'"
examples/word_language_model_with_tcn/adding_problem/__init__.py,0,b''
examples/word_language_model_with_tcn/adding_problem/adding_model.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport os\n\nimport tensorflow as tf\n\nfrom nlp_architect.models.temporal_convolutional_network import TCN\nfrom tqdm import tqdm\n\n\nclass TCNForAdding(TCN):\n    """"""\n    Main class that defines training graph and defines training run method for the adding problem\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(TCNForAdding, self).__init__(*args, **kwargs)\n        self.input_placeholder = None\n        self.label_placeholder = None\n        self.prediction = None\n        self.training_loss = None\n        self.merged_summary_op_train = None\n        self.merged_summary_op_val = None\n        self.training_update_step = None\n\n    # pylint: disable = arguments-differ\n    def run(self, data_loader, num_iterations=1000, log_interval=100, result_dir=""./""):\n        """"""\n        Runs training\n        Args:\n            data_loader: iterator, Data loader for adding problem\n            num_iterations: int, number of iterations to run\n            log_interval: int, number of iterations after which to run validation and log\n            result_dir: str, path to results directory\n\n        Returns:\n            float, Training loss of last iteration\n        """"""\n        summary_writer = tf.summary.FileWriter(\n            os.path.join(result_dir, ""tfboard""), tf.get_default_graph()\n        )\n        saver = tf.train.Saver(max_to_keep=None)\n\n        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n        sess.run(init)\n\n        for i in tqdm(range(num_iterations)):\n\n            x_data, y_data = next(data_loader)\n\n            feed_dict = {\n                self.input_placeholder: x_data,\n                self.label_placeholder: y_data,\n                self.training_mode: True,\n            }\n            _, summary_train, total_loss_i = sess.run(\n                [self.training_update_step, self.merged_summary_op_train, self.training_loss],\n                feed_dict=feed_dict,\n            )\n            summary_writer.add_summary(summary_train, i)\n\n            if i % log_interval == 0:\n                print(""Step {}: Total: {}"".format(i, total_loss_i))\n                saver.save(sess, result_dir, global_step=i)\n\n                feed_dict = {\n                    self.input_placeholder: data_loader.test[0],\n                    self.label_placeholder: data_loader.test[1],\n                    self.training_mode: False,\n                }\n                val_loss, summary_val = sess.run(\n                    [self.training_loss, self.merged_summary_op_val], feed_dict=feed_dict\n                )\n\n                summary_writer.add_summary(summary_val, i)\n\n                print(""Validation loss: {}"".format(val_loss))\n\n        return total_loss_i\n\n    # pylint: disable = arguments-differ\n    def build_train_graph(self, lr, max_gradient_norm=None):\n        """"""\n        Method that builds the graph for training\n        Args:\n            lr: float, learning rate\n            max_gradient_norm: float, maximum gradient norm value for clipping\n\n        Returns:\n            None\n        """"""\n        with tf.variable_scope(""input"", reuse=True):\n            self.input_placeholder = tf.placeholder(\n                tf.float32, [None, self.max_len, self.n_features_in], name=""input""\n            )\n            self.label_placeholder = tf.placeholder(tf.float32, [None, 1], name=""labels"")\n\n        self.prediction = self.build_network_graph(self.input_placeholder, last_timepoint=True)\n\n        with tf.variable_scope(""training""):\n            self.training_loss = tf.losses.mean_squared_error(\n                self.label_placeholder, self.prediction\n            )\n\n            summary_ops_train = [tf.summary.scalar(""Training Loss"", self.training_loss)]\n            self.merged_summary_op_train = tf.summary.merge(summary_ops_train)\n\n            summary_ops_val = [tf.summary.scalar(""Validation Loss"", self.training_loss)]\n            self.merged_summary_op_val = tf.summary.merge(summary_ops_val)\n\n            # Calculate and clip gradients\n            params = tf.trainable_variables()\n            gradients = tf.gradients(self.training_loss, params)\n            if max_gradient_norm is not None:\n                clipped_gradients = [\n                    t if t is None else tf.clip_by_norm(t, max_gradient_norm) for t in gradients\n                ]\n            else:\n                clipped_gradients = gradients\n\n            # Optimization\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            optimizer = tf.train.AdamOptimizer(lr)\n            with tf.control_dependencies(update_ops):\n                self.training_update_step = optimizer.apply_gradients(\n                    zip(clipped_gradients, params)\n                )\n'"
examples/word_language_model_with_tcn/adding_problem/adding_with_tcn.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nThis script replicates the experiments in the following paper for the synthetic ""adding"" data:\nBai, Shaojie, J. Zico Kolter, and Vladlen Koltun. ""An Empirical Evaluation of Generic Convolutional\nand Recurrent Networks for Sequence Modeling."" arXiv preprint arXiv:1803.01271 (2018).\n\nTo compare with the original implementation, run\npython ./adding_with_tcn.py --batch_size 32 --dropout 0.0 --epochs 20 --ksize 6 --levels 7\n--seq_len 200 --log_interval 100 --nhid 27 --lr 0.002 --results_dir ./\n\npython ./adding_with_tcn.py --batch_size 32 --dropout 0.0 --epochs 20 --ksize 7 --levels 7\n--seq_len 400 --log_interval 100 --nhid 27 --lr 0.002 --results_dir ./\n\npython ./adding_with_tcn.py --batch_size 32 --dropout 0.0 --epochs 20 --ksize 8 --levels 8\n--seq_len 600 --log_interval 100 --nhid 24 --lr 0.002 --results_dir ./\n""""""\nimport argparse\nimport os\n\nfrom examples.word_language_model_with_tcn.adding_problem.adding_model import TCNForAdding\nfrom examples.word_language_model_with_tcn.toy_data.adding import Adding\nfrom nlp_architect.utils.io import validate_parent_exists, check_size\n\n\ndef main(args):\n    """"""\n    Main function\n    Args:\n        args: output of argparse with all input arguments\n\n    Returns:\n        None\n    """"""\n    n_features = 2\n    hidden_sizes = [args.nhid] * args.levels\n    kernel_size = args.ksize\n    dropout = args.dropout\n    seq_len = args.seq_len\n    n_train = 50000\n    n_val = 1000\n    batch_size = args.batch_size\n    n_epochs = args.epochs\n    num_iterations = int(n_train * n_epochs * 1.0 / batch_size)\n    results_dir = os.path.abspath(args.results_dir)\n\n    adding_dataset = Adding(seq_len=seq_len, n_train=n_train, n_test=n_val)\n\n    model = TCNForAdding(\n        seq_len, n_features, hidden_sizes, kernel_size=kernel_size, dropout=dropout\n    )\n\n    model.build_train_graph(args.lr, max_gradient_norm=args.grad_clip_value)\n\n    model.run(\n        adding_dataset,\n        num_iterations=num_iterations,\n        log_interval=args.log_interval,\n        result_dir=results_dir,\n    )\n\n\nPARSER = argparse.ArgumentParser()\nPARSER.add_argument(\n    ""--seq_len"",\n    type=int,\n    action=check_size(0, 1000),\n    help=""Number of time points in each input sequence"",\n    default=200,\n)\nPARSER.add_argument(\n    ""--log_interval"",\n    type=int,\n    default=100,\n    action=check_size(0, 10000),\n    help=""frequency, in number of iterations, after which loss is evaluated"",\n)\nPARSER.add_argument(\n    ""--results_dir"",\n    type=validate_parent_exists,\n    help=""Directory to write results to"",\n    default=os.path.expanduser(""~/results""),\n)\nPARSER.add_argument(\n    ""--dropout"",\n    type=float,\n    default=0.0,\n    action=check_size(0, 1),\n    help=""dropout applied to layers, between 0 and 1 (default: 0.0)"",\n)\nPARSER.add_argument(\n    ""--ksize"", type=int, default=6, action=check_size(0, 10), help=""kernel size (default: 6)""\n)\nPARSER.add_argument(\n    ""--levels"", type=int, default=7, action=check_size(0, 10), help=""# of levels (default: 7)""\n)\nPARSER.add_argument(\n    ""--lr"",\n    type=float,\n    default=2e-3,\n    action=check_size(0, 1),\n    help=""initial learning rate (default: 2e-3)"",\n)\nPARSER.add_argument(\n    ""--nhid"",\n    type=int,\n    default=27,\n    action=check_size(0, 1000),\n    help=""number of hidden units per layer (default: 27)"",\n)\nPARSER.add_argument(\n    ""--grad_clip_value"",\n    type=float,\n    default=10,\n    action=check_size(0, 10),\n    help=""value to clip each element of gradient"",\n)\nPARSER.add_argument(\n    ""--batch_size"", type=int, default=32, action=check_size(0, 512), help=""Batch size""\n)\nPARSER.add_argument(\n    ""--epochs"", type=int, default=20, action=check_size(0, 1000), help=""Number of epochs""\n)\nPARSER.set_defaults()\nARGS = PARSER.parse_args()\nmain(ARGS)\n'"
examples/word_language_model_with_tcn/mle_language_model/__init__.py,0,b''
examples/word_language_model_with_tcn/mle_language_model/language_modeling_with_tcn.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nThis script replicates the experiments run in the following paper for music data.\nBai, Shaojie, J. Zico Kolter, and Vladlen Koltun. ""An Empirical Evaluation of Generic Convolutional\nand Recurrent Networks for Sequence Modeling."" arXiv preprint arXiv:1803.01271 (2018).\n\nTo compare with the original implementation, run\npython ./language_modeling_with_tcn.py --batch_size 16 --dropout 0.45 --epochs 100 --ksize 3\n--levels 4 --seq_len 60 --nhid 600 --em_len 600 --em_dropout 0.25 --lr 4 --grad_clip_value 0.35\n--results_dir ./ --dataset PTB\n\npython ./language_modeling_with_tcn.py --batch_size 16 --dropout 0.5 --epochs 100 --ksize 3\n--levels 5 --seq_len 60 --nhid 1000 --em_len 400 --em_dropout 0.25 --lr 4 --grad_clip_value 0.35\n--results_dir ./ --dataset WikiText-103\n\n""""""\nimport argparse\nimport os\n\nfrom examples.word_language_model_with_tcn.mle_language_model.lm_model import TCNForLM\nfrom nlp_architect.data.ptb import PTBDataLoader, PTBDictionary\nfrom nlp_architect.utils.io import (\n    validate_existing_directory,\n    validate_existing_filepath,\n    validate_parent_exists,\n    check_size,\n)\n\n\ndef main(args):\n    """"""\n    Main function\n    Args:\n        args: output of argparse with all input arguments\n\n    Returns:\n        None\n    """"""\n    hidden_sizes = [args.nhid] * args.levels\n    kernel_size = args.ksize\n    dropout = args.dropout\n    em_dropout = args.em_dropout\n    seq_len = args.seq_len\n    batch_size = args.batch_size\n    n_epochs = args.epochs\n    embedding_size = args.em_len\n    datadir = os.path.abspath(args.datadir)\n    results_dir = os.path.abspath(args.results_dir)\n\n    ptb_dict = PTBDictionary(data_dir=datadir, dataset=args.dataset)\n    ptb_dataset_train = PTBDataLoader(\n        ptb_dict,\n        data_dir=datadir,\n        seq_len=seq_len,\n        split_type=""train"",\n        skip=seq_len / 2,\n        dataset=args.dataset,\n    )\n    ptb_dataset_valid = PTBDataLoader(\n        ptb_dict,\n        data_dir=datadir,\n        seq_len=seq_len,\n        split_type=""valid"",\n        loop=False,\n        skip=seq_len / 2,\n        dataset=args.dataset,\n    )\n    ptb_dataset_test = PTBDataLoader(\n        ptb_dict,\n        data_dir=datadir,\n        seq_len=seq_len,\n        split_type=""test"",\n        loop=False,\n        skip=seq_len / 2,\n        dataset=args.dataset,\n    )\n\n    n_train = ptb_dataset_train.n_train\n    num_words = len(ptb_dataset_train.idx2word)\n    n_per_epoch = int(n_train * 1.0 / batch_size)\n    if not args.inference:\n        num_iterations = int(n_train * n_epochs * 1.0 / batch_size)\n        print(\n            ""Training examples per epoch: {}, num iterations per epoch: {}"".format(\n                n_train, num_iterations // n_epochs\n            )\n        )\n    else:\n        num_iterations = 0\n\n    model = TCNForLM(\n        seq_len, embedding_size, hidden_sizes, kernel_size=kernel_size, dropout=dropout\n    )\n\n    model.build_train_graph(\n        num_words=num_words, max_gradient_norm=args.grad_clip_value, em_dropout=em_dropout\n    )\n\n    if not args.inference:\n        model.run(\n            {""train"": ptb_dataset_train, ""valid"": ptb_dataset_valid, ""test"": ptb_dataset_test},\n            args.lr,\n            num_iterations=num_iterations,\n            log_interval=n_per_epoch,\n            result_dir=results_dir,\n            ckpt=None,\n        )\n    else:\n        sequences = model.run_inference(\n            args.ckpt,\n            num_samples=args.num_samples,\n            sos=ptb_dict.sos_symbol,\n            eos=ptb_dict.eos_symbol,\n        )\n        for seq in sequences:\n            sentence = []\n            for idx in seq:\n                while idx == ptb_dict.sos_symbol:\n                    continue\n                sentence.append(ptb_dict.idx2word[idx])\n            print("" "".join(sentence) + ""\\n"")\n\n\nPARSER = argparse.ArgumentParser()\nPARSER.add_argument(\n    ""--seq_len"",\n    type=int,\n    action=check_size(0, 1000),\n    help=""Number of time points in each input sequence"",\n    default=60,\n)\nPARSER.add_argument(\n    ""--results_dir"",\n    type=validate_parent_exists,\n    help=""Directory to write results to"",\n    default=os.path.expanduser(""~/results""),\n)\nPARSER.add_argument(\n    ""--dropout"",\n    type=float,\n    default=0.45,\n    action=check_size(0, 1),\n    help=""dropout applied to layers, value in [0, 1] (default: 0.45)"",\n)\nPARSER.add_argument(\n    ""--ksize"", type=int, default=3, action=check_size(0, 10), help=""kernel size (default: 3)""\n)\nPARSER.add_argument(\n    ""--levels"", type=int, default=4, action=check_size(0, 10), help=""# of levels (default: 4)""\n)\nPARSER.add_argument(\n    ""--lr"",\n    type=float,\n    default=4,\n    action=check_size(0, 100),\n    help=""initial learning rate (default: 4)"",\n)\nPARSER.add_argument(\n    ""--nhid"",\n    type=int,\n    default=600,\n    action=check_size(0, 10000),\n    help=""number of hidden units per layer (default: 600)"",\n)\nPARSER.add_argument(\n    ""--em_len"",\n    type=int,\n    default=600,\n    action=check_size(0, 10000),\n    help=""Length of embedding (default: 600)"",\n)\nPARSER.add_argument(\n    ""--em_dropout"",\n    type=float,\n    default=0.25,\n    action=check_size(0, 1),\n    help=""dropout applied to layers, value in [0, 1] (default: 0.25)"",\n)\nPARSER.add_argument(\n    ""--grad_clip_value"",\n    type=float,\n    default=0.35,\n    action=check_size(0, 10),\n    help=""value to clip each element of gradient"",\n)\nPARSER.add_argument(\n    ""--batch_size"", type=int, default=16, action=check_size(0, 512), help=""Batch size""\n)\nPARSER.add_argument(\n    ""--epochs"", type=int, default=100, action=check_size(0, 1000), help=""Number of epochs""\n)\nPARSER.add_argument(\n    ""--datadir"",\n    type=validate_existing_directory,\n    default=os.path.expanduser(""~/data""),\n    help=""dir to download data if not already present"",\n)\nPARSER.add_argument(\n    ""--dataset"", type=str, default=""PTB"", choices=[""PTB"", ""WikiText-103""], help=""dataset name""\n)\nPARSER.add_argument(""--inference"", action=""store_true"", help=""whether to run in inference mode"")\nPARSER.add_argument(""--ckpt"", type=validate_existing_filepath, help=""checkpoint file"")\nPARSER.add_argument(\n    ""--num_samples"",\n    type=int,\n    default=10,\n    action=check_size(0, 10000),\n    help=""number of samples to generate during inference"",\n)\nPARSER.set_defaults()\nARGS = PARSER.parse_args()\nmain(ARGS)\n'"
examples/word_language_model_with_tcn/mle_language_model/lm_model.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom nlp_architect.models.temporal_convolutional_network import TCN, CommonLayers\n\n\nclass TCNForLM(TCN, CommonLayers):\n    """"""\n    Main class that defines training graph and defines training run method for language modeling\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(TCNForLM, self).__init__(*args, **kwargs)\n        self.num_words = None\n        self.input_placeholder_tokens = None\n        self.label_placeholder_tokens = None\n        self.learning_rate = None\n        self.input_embeddings = None\n        self.prediction = None\n        self.projection_out = None\n        self.gen_seq_prob = None\n        self.training_loss = None\n        self.validation_loss = None\n        self.test_loss = None\n        self.merged_summary_op_train = None\n        self.merged_summary_op_test = None\n        self.merged_summary_op_val = None\n        self.training_update_step = None\n\n    # pylint: disable=arguments-differ\n    def run(\n        self, data_loaders, lr, num_iterations=100, log_interval=100, result_dir=""./"", ckpt=None\n    ):\n        """"""\n        Args:\n            data_loaders: dict, keys are ""train"", ""valid"", ""test"",\n                          values are corresponding iterator dataloaders\n            lr: float, learning rate\n            num_iterations: int, number of iterations to run\n            log_interval: int, number of iterations after which to run validation and log\n            result_dir: str, path to results directory\n            ckpt: str, location of checkpoint file\n\n        Returns:\n            None\n        """"""\n\n        summary_writer = tf.summary.FileWriter(\n            os.path.join(result_dir, ""tfboard""), tf.get_default_graph()\n        )\n        saver = tf.train.Saver(max_to_keep=None)\n\n        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n        sess.run(init)\n\n        if ckpt is not None:\n            saver.restore(sess, ckpt)\n\n        all_vloss = []\n        for i in range(num_iterations):\n\n            x_data, y_data = next(data_loaders[""train""])\n\n            feed_dict = {\n                self.input_placeholder_tokens: x_data,\n                self.label_placeholder_tokens: y_data,\n                self.training_mode: True,\n                self.learning_rate: lr,\n            }\n            _, summary_train, total_loss_i = sess.run(\n                [self.training_update_step, self.merged_summary_op_train, self.training_loss],\n                feed_dict=feed_dict,\n            )\n\n            summary_writer.add_summary(summary_train, i)\n\n            if i % log_interval == 0:\n                print(""Step {}: Total: {}"".format(i, total_loss_i))\n                saver.save(sess, result_dir, global_step=i)\n\n                val_loss = {}\n                for split_type in [""valid"", ""test""]:\n                    val_loss[split_type] = 0\n                    data_loaders[split_type].reset()\n                    count = 0\n                    for x_data_test, y_data_test in data_loaders[split_type]:\n                        feed_dict = {\n                            self.input_placeholder_tokens: x_data_test,\n                            self.label_placeholder_tokens: y_data_test,\n                            self.training_mode: False,\n                        }\n                        val_loss[split_type] += sess.run(self.training_loss, feed_dict=feed_dict)\n                        count += 1\n\n                    val_loss[split_type] = val_loss[split_type] / count\n\n                summary_val = sess.run(\n                    self.merged_summary_op_val, feed_dict={self.validation_loss: val_loss[""valid""]}\n                )\n                summary_test = sess.run(\n                    self.merged_summary_op_test, feed_dict={self.test_loss: val_loss[""test""]}\n                )\n\n                summary_writer.add_summary(summary_val, i)\n                summary_writer.add_summary(summary_test, i)\n\n                print(""Validation loss: {}"".format(val_loss[""valid""]))\n                print(""Test loss: {}"".format(val_loss[""test""]))\n                all_vloss.append(val_loss[""valid""])\n\n                if i > 3 * log_interval and val_loss[""valid""] >= max(all_vloss[-5:]):\n                    lr = lr / 2.0\n\n    def run_inference(self, ckpt, num_samples=10, sos=0, eos=1):\n        """"""\n        Method for running inference for generating sequences\n        Args:\n            ckpt: Location of checkpoint file with trained model\n            num_samples: int, number of samples to generate\n            sos: int, start of sequence symbol\n            eos: int, end of sequence symbol\n\n        Returns:\n            List of sequences\n        """"""\n        saver = tf.train.Saver(max_to_keep=None)\n        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        if ckpt is not None:\n            saver.restore(sess, ckpt)\n\n        results = self.sample_sequence(sess, num_samples, sos=sos, eos=eos)\n        return results\n\n    # pylint: disable=arguments-differ\n    def build_train_graph(\n        self, num_words=20000, word_embeddings=None, max_gradient_norm=None, em_dropout=0.4\n    ):\n        """"""\n        Method that builds the graph for training\n        Args:\n            num_words: int, number of words in the vocabulary\n            word_embeddings: numpy array, optional numpy array to initialize embeddings\n            max_gradient_norm: float, maximum gradient norm value for clipping\n            em_dropout: float, dropout rate for embeddings\n\n        Returns:\n            None\n        """"""\n        self.num_words = num_words\n        with tf.variable_scope(""input"", reuse=True):\n            self.input_placeholder_tokens = tf.placeholder(\n                tf.int32, [None, self.max_len], name=""input_tokens""\n            )\n            self.label_placeholder_tokens = tf.placeholder(\n                tf.int32, [None, self.max_len], name=""input_tokens_shifted""\n            )\n            self.learning_rate = tf.placeholder(tf.float32, shape=(), name=""learning_rate"")\n\n        self.input_embeddings = self.define_input_layer(\n            self.input_placeholder_tokens, word_embeddings, embeddings_trainable=True\n        )\n\n        input_embeddings_dropped = tf.layers.dropout(\n            self.input_embeddings, rate=em_dropout, training=self.training_mode\n        )\n        self.prediction = self.build_network_graph(input_embeddings_dropped, last_timepoint=False)\n\n        if self.prediction.shape[-1] != self.n_features_in:\n            print(""Not tying weights"")\n            tied_weights = False\n        else:\n            print(""Tying weights"")\n            tied_weights = True\n        self.projection_out = self.define_projection_layer(\n            self.prediction, tied_weights=tied_weights\n        )\n        self.gen_seq_prob = tf.nn.softmax(self.projection_out)\n\n        with tf.variable_scope(""training""):\n            params = tf.trainable_variables()\n\n            soft_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=self.label_placeholder_tokens, logits=self.projection_out\n            )\n            ce_last_tokens = tf.slice(\n                soft_ce, [0, int(self.max_len / 2)], [-1, int(self.max_len / 2)]\n            )\n            self.training_loss = tf.reduce_mean(ce_last_tokens)\n\n            summary_ops_train = [\n                tf.summary.scalar(""Training Loss"", self.training_loss),\n                tf.summary.scalar(""Training perplexity"", tf.exp(self.training_loss)),\n            ]\n            self.merged_summary_op_train = tf.summary.merge(summary_ops_train)\n\n            self.validation_loss = tf.placeholder(tf.float32, shape=())\n            summary_ops_val = [\n                tf.summary.scalar(""Validation Loss"", self.validation_loss),\n                tf.summary.scalar(""Validation perplexity"", tf.exp(self.validation_loss)),\n            ]\n            self.merged_summary_op_val = tf.summary.merge(summary_ops_val)\n\n            self.test_loss = tf.placeholder(tf.float32, shape=())\n            summary_ops_test = [\n                tf.summary.scalar(""Test Loss"", self.test_loss),\n                tf.summary.scalar(""Test perplexity"", tf.exp(self.test_loss)),\n            ]\n            self.merged_summary_op_test = tf.summary.merge(summary_ops_test)\n\n            # Calculate and clip gradients\n            gradients = tf.gradients(self.training_loss, params)\n\n            if max_gradient_norm is not None:\n                clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n            else:\n                clipped_gradients = gradients\n\n            grad_norm = tf.global_norm(clipped_gradients)\n            summary_ops_train.append(tf.summary.scalar(""Grad Norm"", grad_norm))\n\n            # Optimization\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            summary_ops_train.append(tf.summary.scalar(""Learning rate"", self.learning_rate))\n            self.merged_summary_op_train = tf.summary.merge(summary_ops_train)\n\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n            with tf.control_dependencies(update_ops):\n                self.training_update_step = optimizer.apply_gradients(\n                    zip(clipped_gradients, params)\n                )\n\n    def sample_sequence(self, sess, num_samples=10, sos=0, eos=1):\n        """"""\n        Method for sampling a sequence (repeatedly one symbol at a time)\n        Args:\n            sess: tensorflow session\n            num_samples: int, number of samples to generate\n            sos: int, start of sequence symbol\n            eos: int, end of sequence symbol\n\n        Returns:\n            List of sequences\n        """"""\n        all_sequences = []\n        for _ in tqdm(range(num_samples)):\n            sampled_sequence = []\n            input_sequence = sos * np.ones((1, self.max_len))\n            count = 0\n            elem = sos\n            while (elem != eos) and (count <= self.max_len * 10):\n                feed_dict = {\n                    self.input_placeholder_tokens: input_sequence,\n                    self.training_mode: False,\n                }\n                gen_seq_prob_value = sess.run(self.gen_seq_prob, feed_dict=feed_dict)\n                prob = gen_seq_prob_value[0, -1, :].astype(np.float64)\n                prob = prob / sum(prob)\n                elem = np.where(np.random.multinomial(1, prob))[0][0]\n                input_sequence = np.roll(input_sequence, -1, axis=-1)\n                input_sequence[:, -1] = elem\n                count += 1\n                sampled_sequence.append(elem)\n            all_sequences.append(sampled_sequence)\n        return all_sequences\n'"
examples/word_language_model_with_tcn/toy_data/adding.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n""""""\nData generation and data loader for the synthetic adding dataset\n""""""\nimport numpy as np\n\n\nclass Adding:\n    """"""\n    Iterator class that generates data and provides batches for training\n    """"""\n\n    def __init__(self, seq_len=200, n_train=50000, n_test=1000, batch_size=32):\n        """"""\n        Initialize class\n        Args:\n            seq_len: int, sequence length of data\n            n_train: int, number of training samples\n            n_test: int, number of test samples\n            batch_size: int, number of samples per batch\n        """"""\n        self.seq_len = seq_len\n        self.n_train = n_train\n        self.n_test = n_test\n\n        x_train, y_train = self.load_data(n_train)\n        x_val, y_val = self.load_data(n_test)\n\n        self.train = (x_train, y_train)\n\n        self.test = (x_val, y_val)\n\n        self.sample_count = 0\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.get_batch()\n\n    def get_batch(self):\n        """"""\n        Return one batch at a time\n        Returns:\n            Tuple of two numpy arrays, first is input data sequence, second is the addition output\n        """"""\n        if self.sample_count + self.batch_size > self.n_train:\n            self.sample_count = 0\n\n        batch = (\n            self.train[0][self.sample_count : self.sample_count + self.batch_size],\n            self.train[1][self.sample_count : self.sample_count + self.batch_size],\n        )\n        self.sample_count += self.batch_size\n        return batch\n\n    def load_data(self, num_samples):\n        """"""\n        Generate and load the data into numpy arrays\n        Args:\n            num_samples: # of data in the set\n        Returns:\n            Tuple of two numpy arrays, first is input data sequence, second is the addition output\n        """"""\n        x_num = np.random.rand(num_samples, self.seq_len, 1)\n        x_mask = np.zeros((num_samples, self.seq_len, 1))\n        y_data = np.zeros((num_samples, 1))\n        for i in range(num_samples):\n            positions = np.random.choice(self.seq_len, size=2, replace=False)\n            x_mask[i, positions[0], 0] = 1\n            x_mask[i, positions[1], 0] = 1\n            y_data[i, 0] = x_num[i, positions[0], 0] + x_num[i, positions[1], 0]\n        x_data = np.concatenate((x_num, x_mask), axis=2)\n        return x_data, y_data\n'"
nlp_architect/common/cdc/__init__.py,0,b''
nlp_architect/common/cdc/cluster.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom typing import List\n\nfrom nlp_architect.common.cdc.mention_data import MentionData\n\n\nclass Cluster(object):\n    def __init__(self, coref_chain: int = -1) -> None:\n        """"""\n        Object represent a set of mentions with same coref chain id\n\n        Args:\n            coref_chain (int): the cluster id/coref_chain value\n        """"""\n        self.mentions = []\n        self.cluster_strings = []\n        self.merged = False\n        self.coref_chain = coref_chain\n        self.mentions_corefs = set()\n\n    def get_mentions(self):\n        return self.mentions\n\n    def add_mention(self, mention: MentionData) -> None:\n        if mention is not None:\n            mention.predicted_coref_chain = self.coref_chain\n            self.mentions.append(mention)\n            self.cluster_strings.append(mention.tokens_str)\n            self.mentions_corefs.add(mention.coref_chain)\n\n    def merge_clusters(self, cluster) -> None:\n        """"""\n        Args:\n            cluster: cluster to merge this cluster with\n        """"""\n        for mention in cluster.mentions:\n            mention.predicted_coref_chain = self.coref_chain\n\n        self.mentions.extend(cluster.mentions)\n        self.cluster_strings.extend(cluster.cluster_strings)\n        self.mentions_corefs.update(cluster.mentions_corefs)\n\n    def get_cluster_id(self) -> str:\n        """"""\n        Returns:\n            A generated cluster unique Id created from cluster mentions ids\n        """"""\n        return ""$"".join([mention.mention_id for mention in self.mentions])\n\n\nclass Clusters(object):\n    cluster_coref_chain = 1000\n\n    def __init__(self, topic_id: str, mentions: List[MentionData] = None) -> None:\n        """"""\n\n        Args:\n            mentions: ``list[MentionData]``, required\n                The initial mentions to create the clusters from\n        """"""\n        self.clusters_list = []\n        self.topic_id = topic_id\n        self.set_initial_clusters(mentions)\n\n    def set_initial_clusters(self, mentions: List[MentionData]) -> None:\n        """"""\n\n        Args:\n            mentions: ``list[MentionData]``, required\n                The initial mentions to create the clusters from\n\n        """"""\n        if mentions:\n            for mention in mentions:\n                cluster = Cluster(Clusters.cluster_coref_chain)\n                cluster.add_mention(mention)\n                self.clusters_list.append(cluster)\n                Clusters.cluster_coref_chain += 1\n\n    def clean_clusters(self) -> None:\n        """"""\n        Remove all clusters that were already merged with other clusters\n        """"""\n\n        self.clusters_list = [cluster for cluster in self.clusters_list if not cluster.merged]\n\n    def set_coref_chain_to_mentions(self) -> None:\n        """"""\n        Give all cluster mentions the same coref ID as cluster coref chain ID\n\n        """"""\n        for cluster in self.clusters_list:\n            for mention in cluster.mentions:\n                mention.predicted_coref_chain = str(cluster.coref_chain)\n\n    def add_cluster(self, cluster: Cluster) -> None:\n        self.clusters_list.append(cluster)\n\n    def add_clusters(self, clusters) -> None:\n        for cluster in clusters.clusters_list:\n            self.clusters_list.append(cluster)\n'"
nlp_architect/common/cdc/mention_data.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport sys\nfrom typing import List\n\nfrom nlp_architect.utils.io import load_json_file\nfrom nlp_architect.utils.string_utils import StringUtils\n\n\nclass MentionDataLight(object):\n    def __init__(\n        self,\n        tokens_str: str,\n        mention_context: str = None,\n        mention_head: str = None,\n        mention_head_lemma: str = None,\n        mention_pos: str = None,\n        mention_ner: str = None,\n    ):\n        """"""\n        Object represent a mention with only text values\n        Args:\n            tokens_str: str the tokens combine text (join with space)\n            mention_head: str\n            mention_head_lemma: str\n        """"""\n        self.tokens_str = tokens_str\n        self.mention_context = mention_context\n        if not mention_head and not mention_head_lemma:\n            (\n                self.mention_head,\n                self.mention_head_lemma,\n                self.mention_head_pos,\n                self.mention_ner,\n            ) = StringUtils.find_head_lemma_pos_ner(str(tokens_str))\n        else:\n            self.mention_head = mention_head\n            self.mention_head_lemma = mention_head_lemma\n            self.mention_head_pos = mention_pos\n            self.mention_ner = mention_ner\n\n\nclass MentionData(MentionDataLight):\n    def __init__(\n        self,\n        topic_id: str,\n        doc_id: str,\n        sent_id: int,\n        tokens_numbers: List[int],\n        tokens_str: str,\n        mention_context: List[str],\n        mention_head: str,\n        mention_head_lemma: str,\n        coref_chain: str,\n        mention_type: str = ""NA"",\n        is_continuous: bool = True,\n        is_singleton: bool = False,\n        score: float = float(-1),\n        predicted_coref_chain: str = None,\n        mention_pos: str = None,\n        mention_ner: str = None,\n        mention_index: int = -1,\n    ) -> None:\n        """"""\n        Object represent a mention\n\n        Args:\n            topic_id: str topic ID\n            doc_id: str document ID\n            sent_id: int sentence number\n            tokens_numbers: List[int] - tokens numbers\n            mention_context: List[str] - list of tokens strings\n            coref_chain: str\n            mention_type: str one of (HUM/NON/TIM/LOC/ACT/NEG)\n            is_continuous: bool\n            is_singleton: bool\n            score: float\n            predicted_coref_chain: str (should be field while evaluated)\n            mention_pos: str\n            mention_ner: str\n            mention_index: in case order is of value (default = -1)\n        """"""\n        super(MentionData, self).__init__(\n            tokens_str, mention_context, mention_head, mention_head_lemma, mention_pos, mention_ner\n        )\n        self.topic_id = topic_id\n        self.doc_id = doc_id\n        self.sent_id = sent_id\n        self.tokens_number = tokens_numbers\n        self.mention_type = mention_type\n        self.coref_chain = coref_chain\n        self.is_continuous = is_continuous\n        self.is_singleton = is_singleton\n        self.score = score\n        self.predicted_coref_chain = predicted_coref_chain\n        self.mention_id = self.gen_mention_id()\n        self.mention_index = mention_index\n\n    @staticmethod\n    def read_json_mention_data_line(mention_line: str):\n        """"""\n        Args:\n            mention_line: a Json representation of a single mention\n\n        Returns:\n            MentionData object\n        """"""\n        # pylint: disable=too-many-branches\n\n        try:\n            topic_id = None\n            coref_chain = None\n            doc_id = None\n            sent_id = None\n            tokens_numbers = None\n            score = -1\n            mention_type = None\n            predicted_coref_chain = None\n            mention_context = None\n            is_continue = False\n            is_singleton = False\n            mention_pos = None\n            mention_ner = None\n            mention_index = -1\n\n            mention_text = mention_line[""tokens_str""]\n\n            if ""topic_id"" in mention_line:\n                topic_id = mention_line[""topic_id""]\n\n            if ""coref_chain"" in mention_line:\n                coref_chain = mention_line[""coref_chain""]\n\n            if ""doc_id"" in mention_line:\n                doc_id = mention_line[""doc_id""]\n                if "".xml"" not in doc_id:\n                    doc_id = doc_id + "".xml""\n\n            if ""sent_id"" in mention_line:\n                sent_id = mention_line[""sent_id""]\n\n            if ""tokens_number"" in mention_line:\n                tokens_numbers = mention_line[""tokens_number""]\n\n            if ""mention_context"" in mention_line:\n                mention_context = mention_line[""mention_context""]\n\n            if ""mention_head"" in mention_line and ""mention_head_lemma"" in mention_line:\n                mention_head = mention_line[""mention_head""]\n                mention_head_lemma = mention_line[""mention_head_lemma""]\n                if ""mention_head_pos"" in mention_line:\n                    mention_pos = mention_line[""mention_head_pos""]\n                if ""mention_ner"" in mention_line:\n                    mention_ner = mention_line[""mention_ner""]\n            else:\n                (\n                    mention_head,\n                    mention_head_lemma,\n                    mention_pos,\n                    mention_ner,\n                ) = StringUtils.find_head_lemma_pos_ner(str(mention_text))\n\n            if ""mention_type"" in mention_line:\n                mention_type = mention_line[""mention_type""]\n            if ""score"" in mention_line:\n                score = mention_line[""score""]\n\n            if ""is_continuous"" in mention_line:\n                is_continue = mention_line[""is_continuous""]\n\n            if ""is_singleton"" in mention_line:\n                is_singleton = mention_line[""is_singleton""]\n\n            if ""predicted_coref_chain"" in mention_line:\n                predicted_coref_chain = mention_line[""predicted_coref_chain""]\n\n            if ""mention_index"" in mention_line:\n                mention_index = mention_line[""mention_index""]\n\n            mention_data = MentionData(\n                topic_id,\n                doc_id,\n                sent_id,\n                tokens_numbers,\n                mention_text,\n                mention_context,\n                mention_head,\n                mention_head_lemma,\n                coref_chain,\n                mention_type,\n                is_continue,\n                is_singleton,\n                score,\n                predicted_coref_chain,\n                mention_pos,\n                mention_ner,\n                mention_index,\n            )\n        except Exception:\n            print(""Unexpected error:"", sys.exc_info()[0])\n            raise Exception(""failed reading json line-"" + str(mention_line))\n\n        return mention_data\n\n    @staticmethod\n    def read_mentions_json_to_mentions_data_list(mentions_json_file: str):\n        """"""\n\n        Args:\n            mentions_json_file: the path of the mentions json file to read\n\n        Returns:\n            List[MentionData]\n        """"""\n        all_mentions_only = load_json_file(mentions_json_file)\n\n        mentions = []\n        for mention_line in all_mentions_only:\n            mention_data = MentionData.read_json_mention_data_line(mention_line)\n            mentions.append(mention_data)\n\n        return mentions\n\n    def get_tokens(self):\n        return self.tokens_number\n\n    def gen_mention_id(self) -> str:\n        if self.doc_id and self.sent_id is not None and self.tokens_number:\n            tokens_ids = [str(self.doc_id), str(self.sent_id)]\n            tokens_ids.extend([str(token_id) for token_id in self.tokens_number])\n            return ""_"".join(tokens_ids)\n\n        return ""_"".join(self.tokens_str.split())\n\n    def get_mention_id(self) -> str:\n        if not self.mention_id:\n            self.mention_id = self.gen_mention_id()\n        return self.mention_id\n\n    @staticmethod\n    def static_gen_token_unique_id(doc_id: int, sent_id: int, token_id: int) -> str:\n        return ""_"".join([str(doc_id), str(sent_id), str(token_id)])\n'"
nlp_architect/common/cdc/topics.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport time\nfrom typing import List\n\nfrom nlp_architect.common.cdc.mention_data import MentionData\nfrom nlp_architect.utils.io import load_json_file\n\nlogger = logging.getLogger(__name__)\n\n\nclass Topic(object):\n    def __init__(self, topic_id):\n        self.topic_id = topic_id\n        self.mentions = []\n\n\nclass Topics(object):\n    def __init__(self):\n        self.topics_list = []\n        self.keep_order = False\n\n    def create_from_file(self, mentions_file_path: str, keep_order: bool = False) -> None:\n        """"""\n\n        Args:\n            keep_order: whether to keep original mentions order or not (default = False)\n            mentions_file_path: this topic mentions json file\n        """"""\n        self.keep_order = keep_order\n        self.topics_list = self.load_mentions_from_file(mentions_file_path)\n\n    def load_mentions_from_file(self, mentions_file_path: str) -> List[Topic]:\n        start_data_load = time.time()\n        logger.info(""Loading mentions from-%s"", mentions_file_path)\n        mentions = load_json_file(mentions_file_path)\n        topics = self.order_mentions_by_topics(mentions)\n        end_data_load = time.time()\n        took_load = end_data_load - start_data_load\n        logger.info(""Mentions file-%s, took:%.4f sec to load"", mentions_file_path, took_load)\n        return topics\n\n    def order_mentions_by_topics(self, mentions: str) -> List[Topic]:\n        """"""\n        Order mentions to documents topics\n        Args:\n            mentions: json mentions file\n\n        Returns:\n            List[Topic] of the mentions separated by their documents topics\n        """"""\n        running_index = 0\n        topics = []\n        current_topic_ref = None\n        for mention_line in mentions:\n            mention = MentionData.read_json_mention_data_line(mention_line)\n\n            if self.keep_order:\n                if mention.mention_index == -1:\n                    mention.mention_index = running_index\n                    running_index += 1\n\n            topic_id = mention.topic_id\n\n            if not current_topic_ref or len(topics) > 0 and topic_id != topics[-1].topic_id:\n                current_topic_ref = Topic(topic_id)\n                topics.append(current_topic_ref)\n\n            current_topic_ref.mentions.append(mention)\n\n        return topics\n'"
nlp_architect/data/cdc_resources/__init__.py,0,b''
nlp_architect/models/absa/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom os import path\nfrom pathlib import Path\n\nfrom nlp_architect import LIBRARY_OUT\n\nABSA_ROOT = Path(path.realpath(__file__)).parent\n\nTRAIN_LEXICONS = ABSA_ROOT / ""train"" / ""lexicons""\n\nTRAIN_CONF = ABSA_ROOT / ""train"" / ""config.ini""\n\nTRAIN_OUT = LIBRARY_OUT / ""absa"" / ""train""\n\nLEXICONS_OUT = TRAIN_OUT / ""lexicons""\n\nINFERENCE_LEXICONS = ABSA_ROOT / ""inference"" / ""lexicons""\n\nINFERENCE_OUT = LIBRARY_OUT / ""absa"" / ""inference""\n\nGENERIC_OP_LEX = ABSA_ROOT / ""train"" / ""lexicons"" / ""GenericOpinionLex.csv""\n'"
nlp_architect/models/absa/utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport csv\nimport json\nimport sys\nfrom os import walk, path, makedirs, PathLike, listdir\nfrom os.path import join, isfile, isdir\nfrom pathlib import Path\nfrom typing import Union\nfrom tqdm import tqdm\nimport numpy as np\n\nfrom nlp_architect.common.core_nlp_doc import CoreNLPDoc\nfrom nlp_architect.models.absa import INFERENCE_LEXICONS\nfrom nlp_architect.models.absa.inference.data_types import LexiconElement, Polarity\nfrom nlp_architect.models.absa.train.data_types import OpinionTerm\nfrom nlp_architect.pipelines.spacy_bist import SpacyBISTParser\nfrom nlp_architect.utils.io import download_unlicensed_file, line_count\n\n\ndef _download_pretrained_rerank_model(rerank_model_full_path):\n    rerank_model_dir = path.dirname(rerank_model_full_path)\n    if not path.isfile(rerank_model_full_path):\n        makedirs(rerank_model_dir, exist_ok=True)\n        print(""dowloading pre-trained reranking model.."")\n        download_unlicensed_file(\n            ""https://s3-us-west-2.amazonaws.com/nlp-architect-data/models/"" ""absa/"",\n            ""rerank_model.h5"",\n            rerank_model_full_path,\n        )\n    return rerank_model_full_path\n\n\ndef _walk_directory(directory: Union[str, PathLike]):\n    """"""Iterates a directory\'s text files and their contents.""""""\n    for dir_path, _, filenames in walk(directory):\n        for filename in filenames:\n            file_path = join(dir_path, filename)\n            if isfile(file_path) and not filename.startswith("".""):\n                with open(file_path, encoding=""utf-8"") as file:\n                    doc_text = file.read()\n                    yield filename, doc_text\n\n\ndef parse_docs(\n    parser: SpacyBISTParser,\n    docs: Union[str, PathLike],\n    out_dir: Union[str, PathLike] = None,\n    show_tok=True,\n    show_doc=True,\n):\n    """"""Parse raw documents in the form of text files in a directory or lines in a text file.\n\n    Args:\n        parser (SpacyBISTParser)\n        docs (str or PathLike)\n        out_dir (str or PathLike): If specified, the output will also be written to this path.\n        show_tok (bool, optional): Specifies whether to include token text in output.\n        show_doc (bool, optional): Specifies whether to include document text in output.\n\n    Returns:\n        (list of CoreNLPDoc)\n    """"""\n    if out_dir:\n        Path(out_dir).mkdir(parents=True, exist_ok=True)\n\n    params = parser, Path(docs), out_dir, show_tok, show_doc\n    parsed_docs = list(parse_dir(*params) if isdir(docs) else parse_txt(*params))\n    total_parsed = np.sum([len(doc) for doc in parsed_docs])\n    return parsed_docs, total_parsed\n\n\ndef parse_txt(\n    parser: SpacyBISTParser,\n    txt_path: Union[str, PathLike],\n    out_dir: Union[str, PathLike] = None,\n    show_tok=True,\n    show_doc=True,\n):\n    """"""Parse raw documents in the form of lines in a text file.\n\n    Args:\n        parser (SpacyBISTParser)\n        txt_path (str or PathLike)\n        out_dir (str or PathLike): If specified, the output will also be written to this path.\n        show_tok (bool, optional): Specifies whether to include token text in output.\n        show_doc (bool, optional): Specifies whether to include document text in output.\n\n    Yields:\n        CoreNLPDoc: the annotated document.\n    """"""\n    with open(txt_path, encoding=""utf-8"") as f:\n        if out_dir:\n            print(""Writing parsed documents to {}"".format(out_dir))\n        for i, doc_text in enumerate(tqdm(f, total=line_count(txt_path), file=sys.stdout)):\n            parsed_doc = parser.parse(doc_text.rstrip(""\\n""), show_tok, show_doc)\n\n            if out_dir:\n                out_path = Path(out_dir) / (str(i + 1) + "".json"")\n                with open(out_path, ""w"", encoding=""utf-8"") as doc_file:\n                    doc_file.write(parsed_doc.pretty_json())\n            yield parsed_doc\n\n\ndef parse_dir(\n    parser,\n    input_dir: Union[str, PathLike],\n    out_dir: Union[str, PathLike] = None,\n    show_tok=True,\n    show_doc=True,\n):\n    """"""Parse a directory of raw text documents, one by one.\n\n    Args:\n        parser (SpacyBISTParser)\n        input_dir (str or PathLike)\n        out_dir (str or PathLike): If specified, the output will also be written to this path.\n        show_tok (bool, optional): Specifies whether to include token text in output.\n        show_doc (bool, optional): Specifies whether to include document text in output.\n\n    Yields:\n        CoreNLPDoc: the annotated document.\n    """"""\n    if out_dir:\n        print(""Writing parsed documents to {}"".format(out_dir))\n    for filename, file_contents in tqdm(list(_walk_directory(input_dir)), file=sys.stdout):\n        parsed_doc = parser.parse(file_contents, show_tok, show_doc)\n\n        if out_dir:\n            out_path = Path(out_dir) / (filename + "".json"")\n            with open(out_path, ""w"", encoding=""utf-8"") as file:\n                file.write(parsed_doc.pretty_json())\n        yield parsed_doc\n\n\ndef _read_lexicon_from_csv(lexicon_path: Union[str, PathLike]) -> dict:\n    """"""Read a lexicon from a CSV file.\n\n    Returns:\n        Dictionary of LexiconElements, each LexiconElement presents a row.\n    """"""\n    lexicon = {}\n    with open(INFERENCE_LEXICONS / lexicon_path, newline="""", encoding=""utf-8"") as csv_file:\n        reader = csv.reader(csv_file, delimiter="","", quotechar=""|"")\n        for row in reader:\n            try:\n                lexicon[row[0]] = LexiconElement(\n                    term=row[0], score=row[1], polarity=None, is_acquired=None, position=row[2]\n                )\n            except Exception:\n                lexicon[row[0]] = LexiconElement(\n                    term=row[0], score=row[1], polarity=None, is_acquired=None, position=None\n                )\n    return lexicon\n\n\ndef load_opinion_lex(file_name: Union[str, PathLike]) -> dict:\n    """"""Read opinion lexicon from CSV file.\n\n    Returns:\n        Dictionary of LexiconElements, each LexiconElement presents a row.\n    """"""\n    lexicon = {}\n    with open(file_name, newline="""", encoding=""utf-8"") as csvfile:\n        reader = csv.reader(csvfile, delimiter="","", quotechar=""|"")\n        next(reader)\n        for row in reader:\n            term, score, polarity, is_acquired = row[0], row[1], row[2], row[3]\n            score = float(score)\n            # ignore terms with low score\n            if score >= 0.5 and polarity in (Polarity.POS.value, Polarity.NEG.value):\n                lexicon[term] = LexiconElement(\n                    term.lower(),\n                    score if polarity == Polarity.POS.value else -score,\n                    polarity,\n                    is_acquired,\n                )\n    return lexicon\n\n\ndef _load_aspect_lexicon(file_name: Union[str, PathLike]):\n    """"""Read aspect lexicon from CSV file.\n\n    Returns: Dictionary of LexiconElements, each LexiconElement presents a row.\n    """"""\n    lexicon = []\n    with open(file_name, newline="""", encoding=""utf-8-sig"") as csv_file:\n        reader = csv.reader(csv_file, delimiter="","", quotechar=""|"")\n        next(reader)\n        for row in reader:\n            lexicon.append(LexiconElement(row))\n    return lexicon\n\n\ndef _load_parsed_docs_from_dir(directory: Union[str, PathLike]):\n    """"""Read all file in directory.\n\n    Args:\n        directory (PathLike): path\n    """"""\n    res = {}\n    for file_name in listdir(directory):\n        if file_name.endswith("".txt"") or file_name.endswith("".json""):\n            with open(Path(directory) / file_name, encoding=""utf-8"") as f:\n                content = f.read()\n                res[file_name] = json.loads(content, object_hook=CoreNLPDoc.decoder)\n    return res\n\n\ndef _write_table(table: list, filename: Union[str, PathLike]):\n    """"""Write table as csv to file system.\n\n    Args:\n        table (list): table to be printed, as list of lists\n        filename (str or Pathlike): file name\n    """"""\n    with open(filename, ""w"", newline="""", encoding=""utf-8"") as f:\n        writer = csv.writer(f, delimiter="","")\n        for row in table:\n            writer.writerow(row)\n\n\ndef _write_final_opinion_lex(dictionary: list, file_name: Union[str, PathLike]):\n    """"""Write generated opinion lex as csv to file.\n\n    Args:\n        dictionary (list): list of filtered terms\n        file_name (str): file name\n    """"""\n    candidate_terms = [[""#"", ""CandidateTerm"", ""Frequency"", ""Polarity""]]\n    term_num = 1\n    for candidate_term in dictionary:\n        term_row = [int(term_num)] + candidate_term.as_string_list()\n        candidate_terms.append(term_row)\n        term_num += 1\n    _write_table(candidate_terms, file_name)\n\n\ndef _write_final_aspect_lex(dictionary: list, file_name: Union[str, PathLike]):\n    """"""Write generated aspect lex as csv to file.\n\n    Args:\n        dictionary (list): list of filtered terms\n        file_name (str or PathLike): file name\n    """"""\n    candidate_terms = [[""Term""]]\n    candidate_terms_debug = [[""Frequency"", ""Term"", ""Lemma""]]\n    for candidate_term in dictionary:\n        term_row_debug = candidate_term.as_string_list_aspect_debug()\n        term_row = candidate_term.as_string_list_aspect()\n        candidate_terms_debug.append(term_row_debug)\n        candidate_terms.append(term_row)\n    _write_table(candidate_terms, file_name)\n    _write_table(candidate_terms_debug, str(path.splitext(file_name)[0]) + ""_debug.csv"")\n\n\ndef _write_generic_sentiment_terms(dictionary: dict, file_name: Union[str, PathLike]):\n    """"""Write generic sentiment terms as csv to file system.""""""\n    generic_terms = [[""Term"", ""Score"", ""Polarity""]]\n    for generic_term in dictionary.values():\n        generic_terms.append([str(generic_term), ""1.0"", generic_term.polarity.name])\n    _write_table(generic_terms, file_name)\n\n\ndef _load_lex_as_list_from_csv(file_name: Union[str, PathLike]):\n    """"""Load lexicon as list.\n\n    Args:\n        file_name (str or PathLike): input csv file name\n    """"""\n    lexicon_table = []\n    with open(file_name, ""r"", encoding=""utf-8-sig"") as f:\n        reader = csv.DictReader(f, skipinitialspace=True)\n\n        if reader is None:\n            print(""file name is None"")\n            return lexicon_table\n\n        for row in reader:\n            term = row[""Term""].strip()\n            lexicon_table.append(term)\n\n        return lexicon_table\n\n\ndef read_generic_lex_from_file(file_name: Union[str, PathLike]):\n    """"""Read generic opinion lex for term acquisition.\n\n    Args:\n        file_name (str or PathLike): name of csv file\n    """"""\n    with open(file_name, encoding=""utf-8-sig"") as f:\n        reader = csv.DictReader(f)\n        dict_list = {}\n        for row in reader:\n            if row[""UsedForAcquisition""] == ""Y"":\n                key_term = row[""Term""] if len(row) > 0 else """"\n                terms = []\n                if len(row) > 0:\n                    terms = key_term.split()\n                polarity = Polarity[row[""Polarity""]]\n                dict_list[key_term] = OpinionTerm(terms, polarity)\n    return dict_list\n\n\ndef _read_generic_lex_for_similarity(file_name: Union[str, PathLike]):\n    """"""Read generic opinion terms for similarity calc from csv file.\n\n    Args:\n        file_name (str or PathLike): name of csv file\n    """"""\n    with open(file_name, encoding=""utf-8-sig"") as f:\n        reader = csv.DictReader(f)\n        dict_list = {}\n        for row in reader:\n            if row[""UsedForReranking""] == ""Y"":\n                key_term = row[""Term""] if len(row) > 0 else """"\n                polarity = row[""Polarity""]\n                dict_list[key_term] = polarity\n    return dict_list\n\n\ndef _write_aspect_lex(parsed_data: Union[str, PathLike], generated_aspect_lex: dict, out_dir: Path):\n    parsed_docs = _load_parsed_docs_from_dir(parsed_data)\n    aspect_dict = {}\n    max_examples = 20\n    label = ""AS""\n    for doc in parsed_docs.values():\n        for sent_text, _ in doc.sent_iter():\n\n            for term, lemma in generated_aspect_lex.items():\n                if term in sent_text.lower():\n                    _find_aspect_in_sentence(\n                        term, lemma, sent_text, aspect_dict, label, max_examples, False\n                    )\n                if lemma != """" and lemma in sent_text.lower():\n                    _find_aspect_in_sentence(\n                        term, lemma, sent_text, aspect_dict, label, max_examples, True\n                    )\n\n    # write aspect lex to file\n    header_row = [""Term"", ""Alias1"", ""Alias2"", ""Alias3""]\n    for k in range(1, max_examples + 1):\n        header_row.append(""Example"" + str(k))\n    aspect_table = [header_row]\n\n    for [term, lemma], sentences in aspect_dict.items():\n        term_row = [term, lemma, """", """"]\n        for sent in sentences:\n            term_row.append(sent)\n        aspect_table.append(term_row)\n\n    out_dir.mkdir(parents=True, exist_ok=True)\n    out_file_path = out_dir / ""generated_aspect_lex.csv""\n    _write_table(aspect_table, out_file_path)\n    print(""Aspect lexicon written to {}"".format(out_file_path))\n\n\ndef _find_aspect_in_sentence(term, lemma, sent_text, aspect_dict, label, max_examples, found_lemma):\n    search_term = term\n    if found_lemma:\n        search_term = lemma\n\n    start_idx = sent_text.lower().find(search_term)\n    end_idx = start_idx + len(search_term)\n    if (start_idx - 1 > 0 and sent_text[start_idx - 1] == "" "") and (\n        end_idx < len(sent_text) and sent_text[end_idx] == "" ""\n    ):\n\n        sent_text_html = """".join(\n            (\n                sent_text[:start_idx],\n                \'<span class=""\',\n                label,\n                \'"">\',\n                sent_text[start_idx:end_idx],\n                ""</span>"",\n                sent_text[end_idx:],\n            )\n        )\n\n        if (term, lemma) in aspect_dict:\n            if len(aspect_dict[term, lemma]) < max_examples:\n                aspect_dict[term, lemma].append(str(sent_text_html))\n        else:\n            aspect_dict[term, lemma] = [sent_text_html]\n\n\ndef _write_opinion_lex(parsed_data, generated_opinion_lex_reranked, out_dir):\n\n    parsed_docs = _load_parsed_docs_from_dir(parsed_data)\n    opinion_dict = {}\n    max_examples = 20\n    label = ""OP""\n    for doc in parsed_docs.values():\n        for sent_text, _ in doc.sent_iter():\n\n            for term, terms_params in generated_opinion_lex_reranked.items():\n                is_acquired = terms_params[2]\n                if is_acquired == ""Y"":\n                    if term in sent_text.lower():\n                        _find_opinion_in_sentence(\n                            term, terms_params, sent_text, opinion_dict, label, max_examples\n                        )\n                else:\n                    opinion_dict[term] = list(terms_params)\n\n    # write opinion lex to file\n    header_row = [""Term"", ""Score"", ""Polarity"", ""isAcquired""]\n    for k in range(1, max_examples + 1):\n        header_row.append(""Example"" + str(k))\n    opinion_table = [header_row]\n\n    for term, value in opinion_dict.items():\n        term_row = [term, value[0], value[1], value[2]]\n        for k in range(3, len(value)):\n            term_row.append(value[k])\n        opinion_table.append(term_row)\n\n    out_dir.mkdir(parents=True, exist_ok=True)\n    out_file_path = out_dir / ""generated_opinion_lex_reranked.csv""\n    _write_table(opinion_table, out_file_path)\n    print(""Reranked opinion lexicon written to {}"".format(out_file_path))\n\n\ndef _find_opinion_in_sentence(term, terms_params, sent_text, opinion_dict, label, max_examples):\n\n    start_idx = sent_text.lower().find(term)\n    end_idx = start_idx + len(term)\n\n    if (start_idx - 1 > 0 and sent_text[start_idx - 1] == "" "") and (\n        end_idx < len(sent_text) and sent_text[end_idx] == "" ""\n    ):\n\n        sent_text_html = """".join(\n            (\n                sent_text[:start_idx],\n                \'<span class=""\',\n                label,\n                \'"">\',\n                sent_text[start_idx:end_idx],\n                ""</span>"",\n                sent_text[end_idx:],\n            )\n        )\n\n        if term in opinion_dict:\n            if len(opinion_dict[term]) < max_examples:\n                opinion_dict[term].append(str(sent_text_html))\n        else:\n            vals = list(terms_params)\n            vals.append(str(sent_text_html))\n            opinion_dict[term] = vals\n'"
nlp_architect/models/bist/__init__.py,0,b''
nlp_architect/models/bist/decoder.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=invalid-name\nimport numpy as np\n\n\n# Things that were changed from the original:\n# - Reformatted code and variable names to conform with PEP8\n# - Added legal header\n\n\n# This file contains routines from Lisbon Machine Learning summer school.\n# The code is freely distributed under a MIT license. https://github.com/LxMLS/lxmls-toolkit/\n\n\ndef parse_proj(scores, gold=None):\n    # pylint: disable=too-many-locals\n    """"""\n    Parse using Eisner\'s algorithm.\n    """"""\n    nr, nc = np.shape(scores)\n    if nr != nc:\n        raise ValueError(""scores must be a squared matrix with nw+1 rows"")\n\n    N = nr - 1  # Number of words (excluding root).\n\n    # Initialize CKY table.\n    complete = np.zeros([N + 1, N + 1, 2])  # s, t, direction (right=1).\n    incomplete = np.zeros([N + 1, N + 1, 2])  # s, t, direction (right=1).\n    complete_backtrack = -np.ones([N + 1, N + 1, 2], dtype=int)  # s, t, direction (right=1).\n    incomplete_backtrack = -np.ones([N + 1, N + 1, 2], dtype=int)  # s, t, direction (right=1).\n\n    incomplete[0, :, 0] -= np.inf\n\n    # Loop from smaller items to larger items.\n    for k in range(1, N + 1):\n        for s in range(N - k + 1):\n            t = s + k\n\n            # First, create incomplete items.\n            # left tree\n            incomplete_vals0 = (\n                complete[s, s:t, 1]\n                + complete[(s + 1) : (t + 1), t, 0]\n                + scores[t, s]\n                + (0.0 if gold is not None and gold[s] == t else 1.0)\n            )\n            incomplete[s, t, 0] = np.max(incomplete_vals0)\n            incomplete_backtrack[s, t, 0] = s + np.argmax(incomplete_vals0)\n            # right tree\n            incomplete_vals1 = (\n                complete[s, s:t, 1]\n                + complete[(s + 1) : (t + 1), t, 0]\n                + scores[s, t]\n                + (0.0 if gold is not None and gold[t] == s else 1.0)\n            )\n            incomplete[s, t, 1] = np.max(incomplete_vals1)\n            incomplete_backtrack[s, t, 1] = s + np.argmax(incomplete_vals1)\n\n            # Second, create complete items.\n            # left tree\n            complete_vals0 = complete[s, s:t, 0] + incomplete[s:t, t, 0]\n            complete[s, t, 0] = np.max(complete_vals0)\n            complete_backtrack[s, t, 0] = s + np.argmax(complete_vals0)\n            # right tree\n            complete_vals1 = incomplete[s, (s + 1) : (t + 1), 1] + complete[(s + 1) : (t + 1), t, 1]\n            complete[s, t, 1] = np.max(complete_vals1)\n            complete_backtrack[s, t, 1] = s + 1 + np.argmax(complete_vals1)\n\n    # value = complete[0][N][1]\n    heads = [-1 for _ in range(N + 1)]  # -np.ones(N+1, dtype=int)\n    _backtrack_eisner(incomplete_backtrack, complete_backtrack, 0, N, 1, 1, heads)\n    value_proj = 0.0\n    for m in range(1, N + 1):\n        h = heads[m]\n        value_proj += scores[h, m]\n    return heads\n\n\n# pylint: disable=too-many-arguments\ndef _backtrack_eisner(incomplete_backtrack, complete_backtrack, s, t, direction, complete, heads):\n    """"""\n    Backtracking step in Eisner\'s algorithm.\n    - incomplete_backtrack is a (NW+1)-by-(NW+1) numpy array indexed by a start\n     position, an end position, and a direction flag (0 means left, 1 means\n     right). This array contains the arg-maxes of each step in the Eisner\n     algorithm when building *incomplete* spans.\n    - complete_backtrack is a (NW+1)-by-(NW+1) numpy array indexed by a start\n     position, an end position, and a direction flag (0 means left, 1 means\n     right). This array contains the arg-maxes of each step in the Eisner\n     algorithm when building *complete* spans.\n    - s is the current start of the span\n    - t is the current end of the span\n    - direction is 0 (left attachment) or 1 (right attachment)\n    - complete is 1 if the current span is complete, and 0 otherwise\n    - heads is a (NW+1)-sized numpy array of integers which is a placeholder\n      for storing the head of each word.\n    """"""\n    if s == t:\n        return\n    if complete:\n        r = complete_backtrack[s][t][direction]\n        if direction == 0:\n            _backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 0, 1, heads)\n            _backtrack_eisner(incomplete_backtrack, complete_backtrack, r, t, 0, 0, heads)\n            return\n        _backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 1, 0, heads)\n        _backtrack_eisner(incomplete_backtrack, complete_backtrack, r, t, 1, 1, heads)\n        return\n\n    r = incomplete_backtrack[s][t][direction]\n    if direction == 0:\n        heads[s] = t\n        _backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 1, 1, heads)\n        _backtrack_eisner(incomplete_backtrack, complete_backtrack, r + 1, t, 0, 1, heads)\n        return\n    heads[t] = s\n    _backtrack_eisner(incomplete_backtrack, complete_backtrack, s, r, 1, 1, heads)\n    _backtrack_eisner(incomplete_backtrack, complete_backtrack, r + 1, t, 0, 1, heads)\n    return\n'"
nlp_architect/models/bist/mstlstm.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=no-name-in-module\n# pylint: disable=too-many-instance-attributes\n# pylint: disable=too-many-arguments\n# pylint: disable=too-many-statements\n# pylint: disable=too-many-locals\n# pylint: disable=too-many-branches\n\nimport random\nimport time\nfrom collections import namedtuple\nfrom operator import itemgetter\n\nfrom dynet import (\n    ParameterCollection,\n    AdamTrainer,\n    LSTMBuilder,\n    tanh,\n    logistic,\n    rectify,\n    cmult,\n    SimpleRNNBuilder,\n    concatenate,\n    np,\n    renew_cg,\n    esum,\n)\n\nfrom nlp_architect.data.conll import ConllEntry\nfrom nlp_architect.models.bist import decoder\nfrom nlp_architect.models.bist.utils import read_conll\n\n\n# Things that were changed from the original:\n# - Added input validation\n# - Updated function and object names to dyNet 2.0.2 and Python 3\n# - Removed external embeddings option\n# - Reformatted code and variable names to conform with PEP8\n# - Added dict_to_obj()\n# - Added option for train() to get ConllEntry input\n# - Added legal header\n# - Disabled some style checks\n\n\nclass MSTParserLSTM(object):\n    """"""Underlying LSTM model for MSTParser used by BIST parser.""""""\n\n    def __init__(self, vocab, w2i, pos, rels, options):\n        if isinstance(options, dict):\n            options = _dict_to_obj(options, ""Values"")\n\n        self.model = ParameterCollection()\n        random.seed(1)\n        self.trainer = AdamTrainer(self.model)\n\n        self.activations = {\n            ""tanh"": tanh,\n            ""sigmoid"": logistic,\n            ""relu"": rectify,\n            ""tanh3"": (lambda x: tanh(cmult(cmult(x, x), x))),\n        }\n        self.activation = self.activations[options.activation]\n\n        self.blstm_flag = options.blstmFlag\n        self.labels_flag = options.labelsFlag\n        self.costaug_flag = options.costaugFlag\n        self.bibi_flag = options.bibiFlag\n\n        self.ldims = options.lstm_dims\n        self.wdims = options.wembedding_dims\n        self.pdims = options.pembedding_dims\n        self.rdims = options.rembedding_dims\n        self.layers = options.lstm_layers\n        self.words_count = vocab\n        self.vocab = {word: ind + 3 for word, ind in list(w2i.items())}\n        self.pos = {word: ind + 3 for ind, word in enumerate(pos)}\n        self.rels = {word: ind for ind, word in enumerate(rels)}\n        self.irels = rels\n\n        if self.bibi_flag:\n            self.builders = [\n                LSTMBuilder(1, self.wdims + self.pdims, self.ldims, self.model),\n                LSTMBuilder(1, self.wdims + self.pdims, self.ldims, self.model),\n            ]\n            self.bbuilders = [\n                LSTMBuilder(1, self.ldims * 2, self.ldims, self.model),\n                LSTMBuilder(1, self.ldims * 2, self.ldims, self.model),\n            ]\n        elif self.layers > 0:\n            self.builders = [\n                LSTMBuilder(self.layers, self.wdims + self.pdims, self.ldims, self.model),\n                LSTMBuilder(self.layers, self.wdims + self.pdims, self.ldims, self.model),\n            ]\n        else:\n            self.builders = [\n                SimpleRNNBuilder(1, self.wdims + self.pdims, self.ldims, self.model),\n                SimpleRNNBuilder(1, self.wdims + self.pdims, self.ldims, self.model),\n            ]\n\n        self.hidden_units = options.hidden_units\n        self.hidden2_units = options.hidden2_units\n\n        self.vocab[""*PAD*""] = 1\n        self.pos[""*PAD*""] = 1\n\n        self.vocab[""*INITIAL*""] = 2\n        self.pos[""*INITIAL*""] = 2\n\n        self.wlookup = self.model.add_lookup_parameters((len(vocab) + 3, self.wdims))\n        self.plookup = self.model.add_lookup_parameters((len(pos) + 3, self.pdims))\n        self.rlookup = self.model.add_lookup_parameters((len(rels), self.rdims))\n\n        self.hid_layer_foh = self.model.add_parameters((self.hidden_units, self.ldims * 2))\n        self.hid_layer_fom = self.model.add_parameters((self.hidden_units, self.ldims * 2))\n        self.hid_bias = self.model.add_parameters((self.hidden_units))\n\n        self.hid2_layer = self.model.add_parameters((self.hidden2_units, self.hidden_units))\n        self.hid2_bias = self.model.add_parameters((self.hidden2_units))\n\n        self.out_layer = self.model.add_parameters(\n            (1, self.hidden2_units if self.hidden2_units > 0 else self.hidden_units)\n        )\n\n        if self.labels_flag:\n            self.rhid_layer_foh = self.model.add_parameters((self.hidden_units, 2 * self.ldims))\n            self.rhid_layer_fom = self.model.add_parameters((self.hidden_units, 2 * self.ldims))\n            self.rhid_bias = self.model.add_parameters((self.hidden_units))\n            self.rhid2_layer = self.model.add_parameters((self.hidden2_units, self.hidden_units))\n            self.rhid2_bias = self.model.add_parameters((self.hidden2_units))\n            self.rout_layer = self.model.add_parameters(\n                (\n                    len(self.irels),\n                    self.hidden2_units if self.hidden2_units > 0 else self.hidden_units,\n                )\n            )\n            self.rout_bias = self.model.add_parameters((len(self.irels)))\n\n    def _get_expr(self, sentence, i, j):\n        # pylint: disable=missing-docstring\n        if sentence[i].headfov is None:\n            sentence[i].headfov = self.hid_layer_foh.expr() * concatenate(\n                [sentence[i].lstms[0], sentence[i].lstms[1]]\n            )\n        if sentence[j].modfov is None:\n            sentence[j].modfov = self.hid_layer_fom.expr() * concatenate(\n                [sentence[j].lstms[0], sentence[j].lstms[1]]\n            )\n\n        if self.hidden2_units > 0:\n            output = self.out_layer.expr() * self.activation(\n                self.hid2_bias.expr()\n                + self.hid2_layer.expr()\n                * self.activation(sentence[i].headfov + sentence[j].modfov + self.hid_bias.expr())\n            )  # + self.outBias\n        else:\n            output = self.out_layer.expr() * self.activation(\n                sentence[i].headfov + sentence[j].modfov + self.hid_bias.expr()\n            )  # + self.outBias\n        return output\n\n    def _evaluate(self, sentence):\n        # pylint: disable=missing-docstring\n        exprs = [\n            [self._get_expr(sentence, i, j) for j in range(len(sentence))]\n            for i in range(len(sentence))\n        ]\n        scores = np.array([[output.scalar_value() for output in exprsRow] for exprsRow in exprs])\n        return scores, exprs\n\n    def _evaluate_label(self, sentence, i, j):\n        # pylint: disable=missing-docstring\n        if sentence[i].rheadfov is None:\n            sentence[i].rheadfov = self.rhid_layer_foh.expr() * concatenate(\n                [sentence[i].lstms[0], sentence[i].lstms[1]]\n            )\n        if sentence[j].rmodfov is None:\n            sentence[j].rmodfov = self.rhid_layer_fom.expr() * concatenate(\n                [sentence[j].lstms[0], sentence[j].lstms[1]]\n            )\n\n        if self.hidden2_units > 0:\n            output = (\n                self.rout_layer.expr()\n                * self.activation(\n                    self.rhid2_bias.expr()\n                    + self.rhid2_layer.expr()\n                    * self.activation(\n                        sentence[i].rheadfov + sentence[j].rmodfov + self.rhid_bias.expr()\n                    )\n                )\n                + self.rout_bias.expr()\n            )\n        else:\n            output = (\n                self.rout_layer.expr()\n                * self.activation(\n                    sentence[i].rheadfov + sentence[j].rmodfov + self.rhid_bias.expr()\n                )\n                + self.rout_bias.expr()\n            )\n        return output.value(), output\n\n    def predict(self, conll_path=None, conll=None):\n        # pylint: disable=missing-docstring\n        if conll is None:\n            conll = read_conll(conll_path)\n\n        for sentence in conll:\n            conll_sentence = [entry for entry in sentence if isinstance(entry, ConllEntry)]\n\n            for entry in conll_sentence:\n                wordvec = (\n                    self.wlookup[int(self.vocab.get(entry.norm, 0))] if self.wdims > 0 else None\n                )\n                posvec = self.plookup[int(self.pos[entry.pos])] if self.pdims > 0 else None\n                entry.vec = concatenate([_f for _f in [wordvec, posvec, None] if _f])\n\n                entry.lstms = [entry.vec, entry.vec]\n                entry.headfov = None\n                entry.modfov = None\n\n                entry.rheadfov = None\n                entry.rmodfov = None\n\n            if self.blstm_flag:\n                lstm_forward = self.builders[0].initial_state()\n                lstm_backward = self.builders[1].initial_state()\n\n                for entry, rentry in zip(conll_sentence, reversed(conll_sentence)):\n                    lstm_forward = lstm_forward.add_input(entry.vec)\n                    lstm_backward = lstm_backward.add_input(rentry.vec)\n\n                    entry.lstms[1] = lstm_forward.output()\n                    rentry.lstms[0] = lstm_backward.output()\n\n                if self.bibi_flag:\n                    for entry in conll_sentence:\n                        entry.vec = concatenate(entry.lstms)\n\n                    blstm_forward = self.bbuilders[0].initial_state()\n                    blstm_backward = self.bbuilders[1].initial_state()\n\n                    for entry, rentry in zip(conll_sentence, reversed(conll_sentence)):\n                        blstm_forward = blstm_forward.add_input(entry.vec)\n                        blstm_backward = blstm_backward.add_input(rentry.vec)\n\n                        entry.lstms[1] = blstm_forward.output()\n                        rentry.lstms[0] = blstm_backward.output()\n\n            scores, _ = self._evaluate(conll_sentence)\n            heads = decoder.parse_proj(scores)\n\n            for entry, head in zip(conll_sentence, heads):\n                entry.pred_parent_id = head\n                entry.pred_relation = ""_""\n\n            dump = False\n\n            if self.labels_flag:\n                for modifier, head in enumerate(heads[1:]):\n                    scores, _ = self._evaluate_label(conll_sentence, head, modifier + 1)\n                    conll_sentence[modifier + 1].pred_relation = self.irels[\n                        max(enumerate(scores), key=itemgetter(1))[0]\n                    ]\n\n            renew_cg()\n            if not dump:\n                yield sentence\n\n    def train(self, conll_path):\n        # pylint: disable=invalid-name\n        # pylint: disable=missing-docstring\n        eloss = 0.0\n        mloss = 0.0\n        eerrors = 0\n        etotal = 0\n        start = time.time()\n\n        shuffled_data = list(read_conll(conll_path))\n        random.shuffle(shuffled_data)\n        errs = []\n        lerrs = []\n        i_sentence = 0\n\n        for sentence in shuffled_data:\n            if i_sentence % 100 == 0 and i_sentence != 0:\n                print(\n                    ""Processing sentence number:"",\n                    i_sentence,\n                    ""Loss:"",\n                    eloss / etotal,\n                    ""Errors:"",\n                    (float(eerrors)) / etotal,\n                    ""Time"",\n                    time.time() - start,\n                )\n                start = time.time()\n                eerrors = 0\n                eloss = 0.0\n                etotal = 0\n\n            conll_sentence = [entry for entry in sentence if isinstance(entry, ConllEntry)]\n\n            for entry in conll_sentence:\n                c = float(self.words_count.get(entry.norm, 0))\n                drop_flag = random.random() < (c / (0.25 + c))\n                wordvec = (\n                    self.wlookup[int(self.vocab.get(entry.norm, 0)) if drop_flag else 0]\n                    if self.wdims > 0\n                    else None\n                )\n                posvec = self.plookup[int(self.pos[entry.pos])] if self.pdims > 0 else None\n\n                entry.vec = concatenate([_f for _f in [wordvec, posvec, None] if _f])\n\n                entry.lstms = [entry.vec, entry.vec]\n                entry.headfov = None\n                entry.modfov = None\n\n                entry.rheadfov = None\n                entry.rmodfov = None\n\n            if self.blstm_flag:\n                lstm_forward = self.builders[0].initial_state()\n                lstm_backward = self.builders[1].initial_state()\n\n                for entry, rentry in zip(conll_sentence, reversed(conll_sentence)):\n                    lstm_forward = lstm_forward.add_input(entry.vec)\n                    lstm_backward = lstm_backward.add_input(rentry.vec)\n\n                    entry.lstms[1] = lstm_forward.output()\n                    rentry.lstms[0] = lstm_backward.output()\n\n                if self.bibi_flag:\n                    for entry in conll_sentence:\n                        entry.vec = concatenate(entry.lstms)\n\n                    blstm_forward = self.bbuilders[0].initial_state()\n                    blstm_backward = self.bbuilders[1].initial_state()\n\n                    for entry, rentry in zip(conll_sentence, reversed(conll_sentence)):\n                        blstm_forward = blstm_forward.add_input(entry.vec)\n                        blstm_backward = blstm_backward.add_input(rentry.vec)\n\n                        entry.lstms[1] = blstm_forward.output()\n                        rentry.lstms[0] = blstm_backward.output()\n\n            scores, exprs = self._evaluate(conll_sentence)\n            gold = [entry.parent_id for entry in conll_sentence]\n            heads = decoder.parse_proj(scores, gold if self.costaug_flag else None)\n\n            if self.labels_flag:\n                for modifier, head in enumerate(gold[1:]):\n                    rscores, rexprs = self._evaluate_label(conll_sentence, head, modifier + 1)\n                    gold_label_ind = self.rels[conll_sentence[modifier + 1].relation]\n                    wrong_label_ind = max(\n                        (\n                            (label, scr)\n                            for label, scr in enumerate(rscores)\n                            if label != gold_label_ind\n                        ),\n                        key=itemgetter(1),\n                    )[0]\n                    if rscores[gold_label_ind] < rscores[wrong_label_ind] + 1:\n                        lerrs.append(rexprs[wrong_label_ind] - rexprs[gold_label_ind])\n\n            e = sum([1 for h, g in zip(heads[1:], gold[1:]) if h != g])\n            eerrors += e\n            if e > 0:\n                loss = [\n                    (exprs[h][i] - exprs[g][i])\n                    for i, (h, g) in enumerate(zip(heads, gold))\n                    if h != g\n                ]  # * (1.0/float(e))\n                eloss += e\n                mloss += e\n                errs.extend(loss)\n\n            etotal += len(conll_sentence)\n\n            if i_sentence % 1 == 0 or errs > 0 or lerrs:\n                if errs or lerrs:\n                    eerrs = esum(errs + lerrs)  # * (1.0/(float(len(errs))))\n                    eerrs.scalar_value()\n                    eerrs.backward()\n                    self.trainer.update()\n                    errs = []\n                    lerrs = []\n\n                renew_cg()\n\n            i_sentence += 1\n\n        if errs:\n            eerrs = esum(errs + lerrs)  # * (1.0/(float(len(errs))))\n            eerrs.scalar_value()\n            eerrs.backward()\n            self.trainer.update()\n\n            renew_cg()\n\n        self.trainer.update()\n        print(""Loss: "", mloss / i_sentence)\n\n    def save(self, filename):\n        self.model.save(filename)\n\n    def load(self, filename):\n        self.model.populate(filename)\n\n\ndef _dict_to_obj(dic, name=""Object""):\n    """"""Return an object form of a dictionary.""""""\n    return namedtuple(name, dic.keys())(*dic.values())\n'"
nlp_architect/models/bist/utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=deprecated-module\nimport os\nimport subprocess\nfrom collections import Counter\n\nfrom nlp_architect.data.conll import ConllEntry\nfrom nlp_architect.models.bist.eval.conllu.conll17_ud_eval import run_conllu_eval\n\n\n# Things that were changed from the original:\n# - Removed ConllEntry class, normalize()\n# - Changed read_conll() and write_conll() input from file to path\n# - Added run_eval(), get_options_dict() and is_conllu()\n# - Reformatted code and variable names to conform with PEP8\n# - Added legal header\n\n\ndef vocab(conll_path):\n    # pylint: disable=missing-docstring\n    words_count = Counter()\n    pos_count = Counter()\n    rel_count = Counter()\n\n    for sentence in read_conll(conll_path):\n        words_count.update([node.norm for node in sentence if isinstance(node, ConllEntry)])\n        pos_count.update([node.pos for node in sentence if isinstance(node, ConllEntry)])\n        rel_count.update([node.relation for node in sentence if isinstance(node, ConllEntry)])\n\n    return (\n        words_count,\n        {w: i for i, w in enumerate(words_count.keys())},\n        list(pos_count.keys()),\n        list(rel_count.keys()),\n    )\n\n\ndef read_conll(path):\n    """"""Yields CoNLL sentences read from CoNLL formatted file..""""""\n    with open(path, ""r"") as conll_fp:\n        root = ConllEntry(\n            0, ""*root*"", ""*root*"", ""ROOT-POS"", ""ROOT-CPOS"", ""_"", -1, ""rroot"", ""_"", ""_""\n        )\n        tokens = [root]\n        for line in conll_fp:\n            stripped_line = line.strip()\n            tok = stripped_line.split(""\\t"")\n            if not tok or line.strip() == """":\n                if len(tokens) > 1:\n                    yield tokens\n                tokens = [root]\n            else:\n                if line[0] == ""#"" or ""-"" in tok[0] or ""."" in tok[0]:\n                    # noinspection PyTypeChecker\n                    tokens.append(stripped_line)\n                else:\n                    tokens.append(\n                        ConllEntry(\n                            int(tok[0]),\n                            tok[1],\n                            tok[2],\n                            tok[4],\n                            tok[3],\n                            tok[5],\n                            int(tok[6]) if tok[6] != ""_"" else -1,\n                            tok[7],\n                            tok[8],\n                            tok[9],\n                        )\n                    )\n        if len(tokens) > 1:\n            yield tokens\n\n\ndef write_conll(path, conll_gen):\n    """"""Writes CoNLL sentences to CoNLL formatted file.""""""\n    with open(path, ""w"") as file:\n        for sentence in conll_gen:\n            for entry in sentence[1:]:\n                file.write(str(entry) + ""\\n"")\n            file.write(""\\n"")\n\n\ndef run_eval(gold, test):\n    """"""Evaluates a set of predictions using the appropriate script.""""""\n    if is_conllu(gold):\n        run_conllu_eval(gold_file=gold, test_file=test)\n    else:\n        eval_script = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""eval"", ""eval.pl"")\n        with open(test[: test.rindex(""."")] + ""_eval.txt"", ""w"") as out_file:\n            subprocess.run([""perl"", eval_script, ""-g"", gold, ""-s"", test], stdout=out_file)\n\n\ndef is_conllu(path):\n    """"""Determines if the file is in CoNLL-U format.""""""\n    return os.path.splitext(path.lower())[1] == "".conllu""\n\n\ndef get_options_dict(activation, lstm_dims, lstm_layers, pos_dims):\n    """"""Generates dictionary with all parser options.""""""\n    return {\n        ""activation"": activation,\n        ""lstm_dims"": lstm_dims,\n        ""lstm_layers"": lstm_layers,\n        ""pembedding_dims"": pos_dims,\n        ""wembedding_dims"": 100,\n        ""rembedding_dims"": 25,\n        ""hidden_units"": 100,\n        ""hidden2_units"": 0,\n        ""learning_rate"": 0.1,\n        ""blstmFlag"": True,\n        ""labelsFlag"": True,\n        ""bibiFlag"": True,\n        ""costaugFlag"": True,\n        ""seed"": 0,\n        ""mem"": 0,\n    }\n'"
nlp_architect/models/cross_doc_coref/__init__.py,0,b''
nlp_architect/models/cross_doc_coref/sieves_config.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom typing import List, Tuple\n\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import RelationType\n\n\nclass SievesConfiguration(object):\n    def __init__(self):\n        """"""Cross document co-reference event and entity evaluation configuration settings""""""\n\n        self.__sieves_order = None\n        self.__run_evaluation = False\n\n    @property\n    def sieves_order(self):\n        """"""\n        Sieve definition and Sieve running order\n\n        Tuple[SieveType, RelationType, Threshold(float)] - define sieves to run, were\n\n        Strict- Merge clusters only in case all mentions has current relation between them,\n        Relax- Merge clusters in case (matched mentions) / len(cluster_1.mentions)) >= thresh,\n        Very_Relax- Merge clusters in case (matched mentions) / (all possible pairs) >= thresh\n\n        RelationType represent the type of sieve to run.\n\n        """"""\n        return self.__sieves_order\n\n    @sieves_order.setter\n    def sieves_order(self, sieves_order: List[Tuple[RelationType, float]]):\n        self.__sieves_order = sieves_order\n\n    @property\n    def run_evaluation(self):\n        """"""Should run evaluation (True/False)""""""\n        return self.__run_evaluation\n\n    @run_evaluation.setter\n    def run_evaluation(self, run_evaluation: bool):\n        self.__run_evaluation = run_evaluation\n\n\nclass EventSievesConfiguration(SievesConfiguration):\n    def __init__(self):\n        super(EventSievesConfiguration, self).__init__()\n\n        self.run_evaluation = True\n\n        self.sieves_order = [\n            (RelationType.SAME_HEAD_LEMMA, 1.0),\n            (RelationType.EXACT_STRING, 1.0),\n            (RelationType.WIKIPEDIA_DISAMBIGUATION, 0.1),\n            (RelationType.WORD_EMBEDDING_MATCH, 0.7),\n            (RelationType.WIKIPEDIA_REDIRECT_LINK, 0.1),\n            (RelationType.FUZZY_HEAD_FIT, 0.5),\n            (RelationType.FUZZY_FIT, 1.0),\n            (RelationType.WITHIN_DOC_COREF, 1.0),\n            (RelationType.WIKIPEDIA_TITLE_PARENTHESIS, 0.1),\n            (RelationType.WIKIPEDIA_BE_COMP, 0.1),\n            (RelationType.WIKIPEDIA_CATEGORY, 0.1),\n            (RelationType.VERBOCEAN_MATCH, 0.1),\n            (RelationType.WORDNET_DERIVATIONALLY, 1.0),\n        ]\n\n\nclass EntitySievesConfiguration(SievesConfiguration):\n    def __init__(self):\n        super(EntitySievesConfiguration, self).__init__()\n\n        self.run_evaluation = True\n\n        self.sieves_order = [\n            (RelationType.SAME_HEAD_LEMMA, 1.0),\n            (RelationType.EXACT_STRING, 1.0),\n            (RelationType.FUZZY_FIT, 1.0),\n            (RelationType.WIKIPEDIA_REDIRECT_LINK, 0.1),\n            (RelationType.WIKIPEDIA_DISAMBIGUATION, 0.1),\n            (RelationType.WORD_EMBEDDING_MATCH, 0.7),\n            (RelationType.WORDNET_PARTIAL_SYNSET_MATCH, 0.1),\n            (RelationType.FUZZY_HEAD_FIT, 0.5),\n            (RelationType.WIKIPEDIA_CATEGORY, 0.1),\n            (RelationType.WITHIN_DOC_COREF, 1.0),\n            (RelationType.WIKIPEDIA_BE_COMP, 0.1),\n            (RelationType.WIKIPEDIA_TITLE_PARENTHESIS, 0.1),\n            (RelationType.WORDNET_SAME_SYNSET, 1.0),\n            (RelationType.REFERENT_DICT, 0.5),\n        ]\n'"
nlp_architect/models/cross_doc_coref/sieves_resource.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom nlp_architect import LIBRARY_ROOT\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import (\n    WikipediaSearchMethod,\n    OnlineOROfflineMethod,\n    EmbeddingMethod,\n)\n\n\nclass SievesResources(object):\n    def __init__(self):\n        """"""Cross Document co-reference sieve system resources configuration class""""""\n\n        self.__eval_output_dir = str(LIBRARY_ROOT / ""datasets"" / ""cdc"" / ""test_predict"")\n\n        self.__elastic_index = ""enwiki_v2""\n        self.__elastic_host = ""localhost""\n        self.__elastic_port = 9200\n        self.__wiki_folder = str(LIBRARY_ROOT / ""dumps"" ""wikipedia"")\n        self.__wd_file = str(LIBRARY_ROOT / ""dump"" / ""within_doc_core"" / ""ecb_wd_coref_proc.json"")\n        self.__wn_folder = str(LIBRARY_ROOT / ""dump"" / ""wordnet"")\n        self.__elmo_file = str(LIBRARY_ROOT / ""dump"" / ""embedde"" / ""ecb_all_with_stop_elmo.pickle"")\n        self.__glove_file = str(LIBRARY_ROOT / ""dump"" / ""embedde"" / ""ecb_all_embed_glove.pickle"")\n        self.__referent_dict_file = str(LIBRARY_ROOT / ""dataset"" / ""coref.dict1.tsv"")\n        self.__vo_dict_file = str(LIBRARY_ROOT / ""dataset"" / ""verbocean.unrefined.2004-05-20.txt"")\n\n        self.__wiki_search_method = WikipediaSearchMethod.ONLINE\n        self.__wn_search_method = OnlineOROfflineMethod.ONLINE\n        self.__embed_search_method = EmbeddingMethod.ELMO\n        self.__referent_dict_method = OnlineOROfflineMethod.ONLINE\n        self.__vo_search_method = OnlineOROfflineMethod.ONLINE\n\n    @property\n    def eval_output_dir(self) -> str:\n        """"""\n        The output dir of the evaluation files, here scorer file for cross doc coref spans will\n        be saved\n        """"""\n        return self.__eval_output_dir\n\n    @eval_output_dir.setter\n    def eval_output_dir(self, eval_output_dir: str):\n        self.__eval_output_dir = eval_output_dir\n\n    @property\n    def wiki_folder(self):\n        """"""\n        Location of Wikipedia mini data set file, #Required mini data set file location for Offline\n        evaluation using Wikipedia sieve\n        """"""\n        return self.__wiki_folder\n\n    @wiki_folder.setter\n    def wiki_folder(self, wiki_folder: str):\n        self.__wiki_folder = wiki_folder\n\n    @property\n    def wd_file(self):\n        """"""\n        Location of Within doc data set file, #Required when using Within doc sieve\n        """"""\n        return self.__wd_file\n\n    @wd_file.setter\n    def wd_file(self, wd_file: str):\n        self.__wd_file = wd_file\n\n    @property\n    def wn_folder(self):\n        """"""\n        Location of WordNet mini data set file, #Required mini data set file location for Offline\n        evaluation using WordNet sieve\n        """"""\n        return self.__wn_folder\n\n    @wn_folder.setter\n    def wn_folder(self, wn_folder: str):\n        self.__wn_folder = wn_folder\n\n    @property\n    def glove_file(self):\n        """"""\n        Location of GloVe mini data set file, #Required mini data set file location for Offline\n        evaluation using GloVe sieve\n        """"""\n        return self.__glove_file\n\n    @glove_file.setter\n    def glove_file(self, glove_file: str):\n        self.__glove_file = glove_file\n\n    @property\n    def elmo_file(self):\n        """"""\n        Location of Elmo mini data set file, #Required mini data set file location for Offline\n        evaluation using GloVe sieve\n        """"""\n        return self.__elmo_file\n\n    @elmo_file.setter\n    def elmo_file(self, elmo_file: str):\n        self.__elmo_file = elmo_file\n\n    @property\n    def referent_dict_file(self):\n        """"""\n        Location of Referent dic data set file, #Required mini data set file for Offline\n        evaluation or original file for Online evaluation using Referent Dict sieve\n        """"""\n        return self.__referent_dict_file\n\n    @referent_dict_file.setter\n    def referent_dict_file(self, referent_dict_file: str):\n        self.__referent_dict_file = referent_dict_file\n\n    @property\n    def vo_dict_file(self):\n        """"""\n        Location of VerbOcean data set file, #Required mini data set file for Offline evaluation or\n        original file for Online evaluation using VerbOcean sieve\n        """"""\n        return self.__vo_dict_file\n\n    @vo_dict_file.setter\n    def vo_dict_file(self, vo_dict_file: str):\n        self.__vo_dict_file = vo_dict_file\n\n    @property\n    def elastic_index(self):\n        """"""\n        Elastic index name, #Required when using Elastic evaluation using Wikipedia sieve\n        """"""\n        return self.__elastic_index\n\n    @elastic_index.setter\n    def elastic_index(self, elastic_index: str):\n        self.__elastic_index = elastic_index\n\n    @property\n    def elastic_host(self):\n        """"""\n        Elastic host, #Required when using Elastic evaluation using Wikipedia sieve\n        """"""\n        return self.__elastic_host\n\n    @elastic_host.setter\n    def elastic_host(self, elastic_host: str):\n        self.__elastic_host = elastic_host\n\n    @property\n    def elastic_port(self):\n        """"""\n        Elastic port number, #Required when using Elastic evaluation using Wikipedia sieve\n        """"""\n        return self.__elastic_port\n\n    @elastic_port.setter\n    def elastic_port(self, elastic_port: str):\n        self.__elastic_port = elastic_port\n\n    @property\n    def wiki_search_method(self):\n        """"""\n        Wikipedia search method type, one of: WikipediaSearchMethod.ONLINE,\n        WikipediaSearchMethod.OFFLINE, WikipediaSearchMethod.ELASTIC\n        """"""\n        return self.__wiki_search_method\n\n    @wiki_search_method.setter\n    def wiki_search_method(self, wiki_search_method: str):\n        self.__wiki_search_method = wiki_search_method\n\n    @property\n    def wn_search_method(self):\n        """"""\n        Wordnet search method type, one of: OnlineOROfflineMethod.ONLINE,\n        OnlineOROfflineMethod.OFFLINE\n        """"""\n        return self.__wn_search_method\n\n    @wn_search_method.setter\n    def wn_search_method(self, wn_search_method: str):\n        self.__wn_search_method = wn_search_method\n\n    @property\n    def embed_search_method(self):\n        """"""\n        Wordnet search method type, one of: EmbeddingMethod.GLOVE, EmbeddingMethod.GLOVE_OFFLINE,\n        EmbeddingMethod.ELMO, EmbeddingMethod.ELMO_OFFLINE\n        """"""\n        return self.__embed_search_method\n\n    @embed_search_method.setter\n    def embed_search_method(self, embed_search_method: str):\n        self.__embed_search_method = embed_search_method\n\n    @property\n    def referent_dict_method(self):\n        """"""\n        Referent Dict search method type, one of: OnlineOROfflineMethod.ONLINE,\n        OnlineOROfflineMethod.OFFLINE\n        """"""\n        return self.__referent_dict_method\n\n    @referent_dict_method.setter\n    def referent_dict_method(self, referent_dict_method: str):\n        self.__referent_dict_method = referent_dict_method\n\n    @property\n    def vo_search_method(self):\n        """"""\n        VerbOcean search method type, one of: OnlineOROfflineMethod.ONLINE,\n        OnlineOROfflineMethod.OFFLINE\n        """"""\n        return self.__vo_search_method\n\n    @vo_search_method.setter\n    def vo_search_method(self, vo_search_method: str):\n        self.__vo_search_method = vo_search_method\n'"
nlp_architect/models/transformers/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# flake8: noqa\nfrom nlp_architect.models.transformers.sequence_classification import TransformerSequenceClassifier\nfrom nlp_architect.models.transformers.token_classification import TransformerTokenClassifier\n'"
nlp_architect/models/transformers/base_model.py,7,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport io\nimport logging\nimport os\nfrom typing import List, Union\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm, trange\nfrom transformers import (\n    AdamW,\n    BertConfig,\n    BertTokenizer,\n    RobertaConfig,\n    RobertaTokenizer,\n    XLMConfig,\n    XLMTokenizer,\n    XLNetConfig,\n    XLNetTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\nfrom nlp_architect.models import TrainableModel\nfrom nlp_architect.models.transformers.quantized_bert import QuantizedBertConfig\n\nlogger = logging.getLogger(__name__)\n\n\nALL_MODELS = sum(\n    (\n        tuple(conf.pretrained_config_archive_map.keys())\n        for conf in (BertConfig, XLNetConfig, XLMConfig)\n    ),\n    (),\n)\n\n\ndef get_models(models: List[str]):\n    if models is not None:\n        return [m for m in ALL_MODELS if m.split(""-"")[0] in models]\n    return ALL_MODELS\n\n\nclass TransformerBase(TrainableModel):\n    """"""\n    Transformers base model (for working with pytorch-transformers models)\n    """"""\n\n    MODEL_CONFIGURATIONS = {\n        ""bert"": (BertConfig, BertTokenizer),\n        ""quant_bert"": (QuantizedBertConfig, BertTokenizer),\n        ""xlnet"": (XLNetConfig, XLNetTokenizer),\n        ""xlm"": (XLMConfig, XLMTokenizer),\n        ""roberta"": (RobertaConfig, RobertaTokenizer),\n    }\n\n    def __init__(\n        self,\n        model_type: str,\n        model_name_or_path: str,\n        labels: List[str] = None,\n        num_labels: int = None,\n        config_name=None,\n        tokenizer_name=None,\n        do_lower_case=False,\n        output_path=None,\n        device=""cpu"",\n        n_gpus=0,\n    ):\n        """"""\n        Transformers base model (for working with pytorch-transformers models)\n\n        Args:\n            model_type (str): transformer model type\n            model_name_or_path (str): model name or path to model\n            labels (List[str], optional): list of labels. Defaults to None.\n            num_labels (int, optional): number of labels. Defaults to None.\n            config_name ([type], optional): configuration name. Defaults to None.\n            tokenizer_name ([type], optional): tokenizer name. Defaults to None.\n            do_lower_case (bool, optional): lower case input words. Defaults to False.\n            output_path ([type], optional): model output path. Defaults to None.\n            device (str, optional): backend device. Defaults to \'cpu\'.\n            n_gpus (int, optional): num of gpus. Defaults to 0.\n\n        Raises:\n            FileNotFoundError: [description]\n        """"""\n        assert model_type in self.MODEL_CONFIGURATIONS.keys(), ""unsupported model_type""\n        self.model_type = model_type\n        self.model_name_or_path = model_name_or_path\n        self.labels = labels\n        self.num_labels = num_labels\n        self.do_lower_case = do_lower_case\n        if output_path is not None and not os.path.exists(output_path):\n            raise FileNotFoundError(""output_path is not found"")\n        self.output_path = output_path\n\n        self.model_class = None\n        config_class, tokenizer_class = self.MODEL_CONFIGURATIONS[model_type]\n        self.config_class = config_class\n        self.tokenizer_class = tokenizer_class\n\n        self.tokenizer_name = tokenizer_name\n        self.tokenizer = self._load_tokenizer(self.tokenizer_name)\n        self.config_name = config_name\n        self.config = self._load_config(config_name)\n\n        self.model = None\n        self.device = device\n        self.n_gpus = n_gpus\n\n        self._optimizer = None\n        self._scheduler = None\n\n        self.training_args = None\n\n    def to(self, device=""cpu"", n_gpus=0):\n        if self.model is not None:\n            self.model.to(device)\n            if n_gpus > 1:\n                self.model = torch.nn.DataParallel(self.model)\n        self.device = device\n        self.n_gpus = n_gpus\n\n    @property\n    def optimizer(self):\n        return self._optimizer\n\n    @optimizer.setter\n    def optimizer(self, opt):\n        self._optimizer = opt\n\n    @property\n    def scheduler(self):\n        return self._scheduler\n\n    @scheduler.setter\n    def scheduler(self, sch):\n        self._scheduler = sch\n\n    def setup_default_optimizer(\n        self,\n        weight_decay: float = 0.0,\n        learning_rate: float = 5e-5,\n        adam_epsilon: float = 1e-8,\n        warmup_steps: int = 0,\n        total_steps: int = 0,\n    ):\n        # Prepare optimizer and schedule (linear warmup and decay)\n        no_decay = [""bias"", ""LayerNorm.weight""]\n        optimizer_grouped_parameters = [\n            {\n                ""params"": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if not any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": weight_decay,\n            },\n            {\n                ""params"": [\n                    p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.0,\n            },\n        ]\n        self.optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n        self.scheduler = get_linear_schedule_with_warmup(\n            self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n        )\n\n    def _load_config(self, config_name=None):\n        config = self.config_class.from_pretrained(\n            config_name if config_name else self.model_name_or_path, num_labels=self.num_labels\n        )\n        return config\n\n    def _load_tokenizer(self, tokenizer_name=None):\n        tokenizer = self.tokenizer_class.from_pretrained(\n            tokenizer_name if tokenizer_name else self.model_name_or_path,\n            do_lower_case=self.do_lower_case,\n        )\n        return tokenizer\n\n    def save_model(self, output_dir: str, save_checkpoint: bool = False, args=None):\n        """"""\n        Save model/tokenizer/arguments to given output directory\n\n        Args:\n            output_dir (str): path to output directory\n            save_checkpoint (bool, optional): save as checkpoint. Defaults to False.\n            args ([type], optional): arguments object to save. Defaults to None.\n        """"""\n        # Create output directory if needed\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        logger.info(""Saving model checkpoint to %s"", output_dir)\n        model_to_save = self.model.module if hasattr(self.model, ""module"") else self.model\n        model_to_save.save_pretrained(output_dir)\n        if not save_checkpoint:\n            if self.tokenizer is not None:\n                self.tokenizer.save_pretrained(output_dir)\n            with io.open(output_dir + os.sep + ""labels.txt"", ""w"", encoding=""utf-8"") as fw:\n                for label in self.labels:\n                    fw.write(""{}\\n"".format(label))\n            if args is not None:\n                torch.save(args, os.path.join(output_dir, ""training_args.bin""))\n\n    @classmethod\n    def load_model(cls, model_path: str, model_type: str, *args, **kwargs):\n        """"""\n        Create a TranformerBase deom from given path\n\n        Args:\n            model_path (str): path to model\n            model_type (str): model type\n\n        Returns:\n            TransformerBase: model\n        """"""\n        # Load a trained model and vocabulary from given path\n        if not os.path.exists(model_path):\n            raise FileNotFoundError\n        with io.open(model_path + os.sep + ""labels.txt"") as fp:\n            labels = [line.strip() for line in fp.readlines()]\n        return cls(\n            model_type=model_type, model_name_or_path=model_path, labels=labels, *args, **kwargs\n        )\n\n    @staticmethod\n    def get_train_steps_epochs(\n        max_steps: int, num_train_epochs: int, gradient_accumulation_steps: int, num_samples: int\n    ):\n        """"""\n        get train steps and epochs\n\n        Args:\n            max_steps (int): max steps\n            num_train_epochs (int): num epochs\n            gradient_accumulation_steps (int): gradient accumulation steps\n            num_samples (int): number of samples\n\n        Returns:\n            Tuple: total steps, number of epochs\n        """"""\n        if max_steps > 0:\n            t_total = max_steps\n            num_train_epochs = max_steps // (num_samples // gradient_accumulation_steps) + 1\n        else:\n            t_total = num_samples // gradient_accumulation_steps * num_train_epochs\n        return t_total, num_train_epochs\n\n    def get_logits(self, batch):\n        self.model.eval()\n        inputs = self._batch_mapper(batch)\n        outputs = self.model(**inputs)\n        return outputs[-1]\n\n    def _train(\n        self,\n        data_set: DataLoader,\n        dev_data_set: Union[DataLoader, List[DataLoader]] = None,\n        test_data_set: Union[DataLoader, List[DataLoader]] = None,\n        gradient_accumulation_steps: int = 1,\n        per_gpu_train_batch_size: int = 8,\n        max_steps: int = -1,\n        num_train_epochs: int = 3,\n        max_grad_norm: float = 1.0,\n        logging_steps: int = 50,\n        save_steps: int = 100,\n        best_result_file: str = None,\n    ):\n        """"""Run model training\n            batch_mapper: a function that maps a batch into parameters that the model\n                          expects in the forward method (for use with custom heads and models).\n                          If None it will default to the basic models input structure.\n            logging_callback_fn: a function that is called in each evaluation step\n                          with the model as a parameter.\n\n        """"""\n        t_total, num_train_epochs = self.get_train_steps_epochs(\n            max_steps, num_train_epochs, gradient_accumulation_steps, len(data_set)\n        )\n        if self.optimizer is None and self.scheduler is None:\n            logger.info(""Loading default optimizer and scheduler"")\n            self.setup_default_optimizer(total_steps=t_total)\n\n        train_batch_size = per_gpu_train_batch_size * max(1, self.n_gpus)\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(data_set.dataset))\n        logger.info(""  Num Epochs = %d"", num_train_epochs)\n        logger.info(""  Instantaneous batch size per GPU/CPU = %d"", per_gpu_train_batch_size)\n        logger.info(\n            ""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n            train_batch_size * gradient_accumulation_steps,\n        )\n        logger.info(""  Gradient Accumulation steps = %d"", gradient_accumulation_steps)\n        logger.info(""  Total optimization steps = %d"", t_total)\n\n        global_step = 0\n        best_dev = 0\n        dev_test = 0\n        best_model_path = os.path.join(self.output_path, ""best_dev"")\n        tr_loss, logging_loss = 0.0, 0.0\n        self.model.zero_grad()\n        train_iterator = trange(num_train_epochs, desc=""Epoch"")\n\n        for epoch, _ in enumerate(train_iterator):\n            print(""****** Epoch: "" + str(epoch))\n            epoch_iterator = tqdm(data_set, desc=""Train iteration"")\n            for step, batch in enumerate(epoch_iterator):\n                self.model.train()\n                batch = tuple(t.to(self.device) for t in batch)\n                inputs = self._batch_mapper(batch)\n                outputs = self.model(**inputs)\n                loss = outputs[0]  # get loss\n\n                if self.n_gpus > 1:\n                    loss = loss.mean()  # mean() to average on multi-gpu parallel training\n                if gradient_accumulation_steps > 1:\n                    loss = loss / gradient_accumulation_steps\n\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n\n                tr_loss += loss.item()\n                if (step + 1) % gradient_accumulation_steps == 0:\n                    self.optimizer.step()\n                    self.scheduler.step()\n                    self.model.zero_grad()\n                    global_step += 1\n\n                    if logging_steps > 0 and global_step % logging_steps == 0:\n                        # Log metrics and run evaluation on dev/test\n                        best_dev, dev_test = self.update_best_model(\n                            dev_data_set,\n                            test_data_set,\n                            best_dev,\n                            dev_test,\n                            best_result_file,\n                            save_path=best_model_path,\n                        )\n                        logger.info(""lr = {}"".format(self.scheduler.get_lr()[0]))\n                        logger.info(""loss = {}"".format((tr_loss - logging_loss) / logging_steps))\n                        logging_loss = tr_loss\n\n                    if save_steps > 0 and global_step % save_steps == 0:\n                        # Save model checkpoint\n                        self.save_model_checkpoint(\n                            output_path=self.output_path, name=""checkpoint-{}"".format(global_step)\n                        )\n\n                if 0 < max_steps < global_step:\n                    epoch_iterator.close()\n                    break\n            if 0 < max_steps < global_step:\n                train_iterator.close()\n                break\n\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n        logger.info(""lr = {}"".format(self.scheduler.get_lr()[0]))\n        logger.info(""loss = {}"".format((tr_loss - logging_loss) / logging_steps))\n        # final evaluation:\n        self.update_best_model(\n            dev_data_set,\n            test_data_set,\n            best_dev,\n            dev_test,\n            best_result_file,\n            save_path=best_model_path,\n        )\n\n    def update_best_model(\n        self,\n        dev_data_set,\n        test_data_set,\n        best_dev,\n        best_dev_test,\n        best_result_file,\n        save_path=None,\n    ):\n        new_best_dev = best_dev\n        new_test_dev = best_dev_test\n        set_test = False\n\n        for i, ds in enumerate([dev_data_set, test_data_set]):\n            if ds is None:  # got no data loader\n                continue\n            if isinstance(ds, DataLoader):\n                ds = [ds]\n            for d in ds:\n                logits, label_ids = self._evaluate(d)\n                f1 = self.evaluate_predictions(logits, label_ids)\n                if i == 0 and f1 > best_dev:  # dev set\n                    new_best_dev = f1\n                    set_test = True\n                    if save_path is not None:\n                        self.save_model(save_path, args=self.training_args)\n                elif set_test:\n                    new_test_dev = f1\n                    set_test = False\n                    if best_result_file is not None:\n                        with open(best_result_file, ""a+"") as f:\n                            f.write(\n                                ""best dev= "" + str(new_best_dev) + "", test= "" + str(new_test_dev)\n                            )\n        logger.info(""\\n\\nBest dev=%s. test=%s\\n"", str(new_best_dev), str(new_test_dev))\n        return new_best_dev, new_test_dev\n\n    def _evaluate(self, data_set: DataLoader):\n        logger.info(""***** Running inference *****"")\n        logger.info("" Batch size: {}"".format(data_set.batch_size))\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(data_set, desc=""Inference iteration""):\n            self.model.eval()\n            batch = tuple(t.to(self.device) for t in batch)\n\n            with torch.no_grad():\n                inputs = self._batch_mapper(batch)\n                outputs = self.model(**inputs)\n                if ""labels"" in inputs:\n                    tmp_eval_loss, logits = outputs[:2]\n                    eval_loss += tmp_eval_loss.mean().item()\n                else:\n                    logits = outputs[0]\n            nb_eval_steps += 1\n            model_output = logits.detach().cpu()\n            model_out_label_ids = inputs[""labels""].detach().cpu() if ""labels"" in inputs else None\n            if preds is None:\n                preds = model_output\n                out_label_ids = model_out_label_ids\n            else:\n                preds = torch.cat((preds, model_output), dim=0)\n                out_label_ids = (\n                    torch.cat((out_label_ids, model_out_label_ids), dim=0)\n                    if out_label_ids is not None\n                    else None\n                )\n        if out_label_ids is None:\n            return preds\n        return preds, out_label_ids\n\n    def _batch_mapper(self, batch):\n        mapping = {\n            ""input_ids"": batch[0],\n            ""attention_mask"": batch[1],\n            # XLM don\'t use segment_ids\n            ""token_type_ids"": batch[2]\n            if self.model_type in [""bert"", ""quant_bert"", ""xlnet""]\n            else None,\n        }\n        if len(batch) == 4:\n            mapping.update({""labels"": batch[3]})\n        return mapping\n\n    def evaluate_predictions(self, logits, label_ids):\n        raise NotImplementedError(\n            ""evaluate_predictions method must be implemented in order to""\n            ""be used for dev/test set evaluation""\n        )\n\n    def save_model_checkpoint(self, output_path: str, name: str):\n        """"""\n        save model checkpoint\n\n        Args:\n            output_path (str): output path\n            name (str): name of checkpoint\n        """"""\n        output_dir_path = os.path.join(output_path, name)\n        self.save_model(output_dir_path, save_checkpoint=True)\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id=None, valid_ids=None):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n        self.valid_ids = valid_ids\n'"
nlp_architect/models/transformers/quantized_bert.py,3,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=bad-super-call\n""""""\nQuantized BERT layers and model\n""""""\n\nimport logging\nimport os\nimport sys\n\nimport torch\nfrom torch import nn\nfrom transformers.modeling_bert import (\n    ACT2FN,\n    BertAttention,\n    BertConfig,\n    BertEmbeddings,\n    BertEncoder,\n    BertForQuestionAnswering,\n    BertForSequenceClassification,\n    BertForTokenClassification,\n    BertIntermediate,\n    BertLayer,\n    BertLayerNorm,\n    BertModel,\n    BertOutput,\n    BertPooler,\n    BertPreTrainedModel,\n    BertSelfAttention,\n    BertSelfOutput,\n)\n\nfrom nlp_architect.nn.torch.quantization import (\n    QuantizationConfig,\n    QuantizedEmbedding,\n    QuantizedLayer,\n    QuantizedLinear,\n)\n\nlogger = logging.getLogger(__name__)\n\nQUANT_WEIGHTS_NAME = ""quant_pytorch_model.bin""\n\nQUANT_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    ""bert-base-uncased"": ""https://nlp-architect-data.s3-us-west-2.amazonaws.com/models/transformers/bert-base-uncased.json"",  # noqa: E501\n    ""bert-large-uncased"": ""https://nlp-architect-data.s3-us-west-2.amazonaws.com/models/transformers/bert-large-uncased.json"",  # noqa: E501\n}\n\n\ndef quantized_linear_setup(config, name, *args, **kwargs):\n    """"""\n    Get QuantizedLinear layer according to config params\n    """"""\n    try:\n        quant_config = QuantizationConfig.from_dict(getattr(config, name))\n        linear = QuantizedLinear.from_config(*args, **kwargs, config=quant_config)\n    except AttributeError:\n        linear = nn.Linear(*args, **kwargs)\n    return linear\n\n\ndef quantized_embedding_setup(config, name, *args, **kwargs):\n    """"""\n    Get QuantizedEmbedding layer according to config params\n    """"""\n    try:\n        quant_config = QuantizationConfig.from_dict(getattr(config, name))\n        embedding = QuantizedEmbedding.from_config(*args, **kwargs, config=quant_config)\n    except AttributeError:\n        embedding = nn.Embedding(*args, **kwargs)\n    return embedding\n\n\nclass QuantizedBertConfig(BertConfig):\n    pretrained_config_archive_map = QUANT_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n\nclass QuantizedBertEmbeddings(BertEmbeddings):\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = quantized_embedding_setup(\n            config, ""word_embeddings"", config.vocab_size, config.hidden_size, padding_idx=0\n        )\n        self.position_embeddings = quantized_embedding_setup(\n            config, ""position_embeddings"", config.max_position_embeddings, config.hidden_size\n        )\n        self.token_type_embeddings = quantized_embedding_setup(\n            config, ""token_type_embeddings"", config.type_vocab_size, config.hidden_size\n        )\n\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n\nclass QuantizedBertSelfAttention(BertSelfAttention):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = quantized_linear_setup(\n            config, ""attention_query"", config.hidden_size, self.all_head_size\n        )\n        self.key = quantized_linear_setup(\n            config, ""attention_key"", config.hidden_size, self.all_head_size\n        )\n        self.value = quantized_linear_setup(\n            config, ""attention_value"", config.hidden_size, self.all_head_size\n        )\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n\nclass QuantizedBertSelfOutput(BertSelfOutput):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = quantized_linear_setup(\n            config, ""attention_output"", config.hidden_size, config.hidden_size\n        )\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n\nclass QuantizedBertAttention(BertAttention):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = QuantizedBertSelfAttention(config)\n        self.output = QuantizedBertSelfOutput(config)\n\n    def prune_heads(self, heads):\n        raise NotImplementedError(""pruning heads is not implemented for Quantized BERT"")\n\n\nclass QuantizedBertIntermediate(BertIntermediate):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = quantized_linear_setup(\n            config, ""ffn_intermediate"", config.hidden_size, config.intermediate_size\n        )\n        if isinstance(config.hidden_act, str) or (\n            sys.version_info[0] == 2 and isinstance(config.hidden_act, str)\n        ):  # noqa: F821\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n\nclass QuantizedBertOutput(BertOutput):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = quantized_linear_setup(\n            config, ""ffn_output"", config.intermediate_size, config.hidden_size\n        )\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n\nclass QuantizedBertLayer(BertLayer):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = QuantizedBertAttention(config)\n        self.is_decoder = config.is_decoder\n        if self.is_decoder:\n            logger.warning(""Using QuantizedBertLayer as decoder was not tested."")\n            self.crossattention = QuantizedBertAttention(config)\n        self.intermediate = QuantizedBertIntermediate(config)\n        self.output = QuantizedBertOutput(config)\n\n\nclass QuantizedBertEncoder(BertEncoder):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.layer = nn.ModuleList(\n            [QuantizedBertLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n\n\nclass QuantizedBertPooler(BertPooler):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = quantized_linear_setup(\n            config, ""pooler"", config.hidden_size, config.hidden_size\n        )\n        self.activation = nn.Tanh()\n\n\nclass QuantizedBertPreTrainedModel(BertPreTrainedModel):\n    config_class = QuantizedBertConfig\n    base_model_prefix = ""quant_bert""\n\n    def init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding, QuantizedLinear, QuantizedEmbedding)):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *args, from_8bit=False, **kwargs):\n        """"""load trained model from 8bit model""""""\n        if not from_8bit:\n            return super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n        config = kwargs.pop(""config"", None)\n        output_loading_info = kwargs.pop(""output_loading_info"", False)\n\n        # Load config\n        if config is None:\n            config = cls.config_class.from_pretrained(\n                pretrained_model_name_or_path, *args, **kwargs\n            )\n\n        # Load model\n        model_file = os.path.join(pretrained_model_name_or_path, QUANT_WEIGHTS_NAME)\n\n        # Instantiate model.\n        model = cls(config)\n        # Set model to initialize variables to be loaded from quantized\n        # checkpoint which are None by Default\n        model.eval()\n        # Get state dict of model\n        state_dict = torch.load(model_file, map_location=""cpu"")\n        logger.info(""loading weights file {}"".format(model_file))\n\n        # Load from a PyTorch state_dict\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, ""_metadata"", None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=""""):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n            )\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + ""."")\n\n        # Make sure we are able to load base models as well as derived models (with heads)\n        start_prefix = """"\n        model_to_load = model\n        if not hasattr(model, cls.base_model_prefix) and any(\n            s.startswith(cls.base_model_prefix) for s in state_dict.keys()\n        ):\n            start_prefix = cls.base_model_prefix + "".""\n        if hasattr(model, cls.base_model_prefix) and not any(\n            s.startswith(cls.base_model_prefix) for s in state_dict.keys()\n        ):\n            model_to_load = getattr(model, cls.base_model_prefix)\n\n        load(model_to_load, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(\n                ""Weights of {} not initialized from pretrained model: {}"".format(\n                    model.__class__.__name__, missing_keys\n                )\n            )\n        if len(unexpected_keys) > 0:\n            logger.info(\n                ""Weights from pretrained model not used in {}: {}"".format(\n                    model.__class__.__name__, unexpected_keys\n                )\n            )\n        if len(error_msgs) > 0:\n            raise RuntimeError(\n                ""Error(s) in loading state_dict for {}:\\n\\t{}"".format(\n                    model.__class__.__name__, ""\\n\\t"".join(error_msgs)\n                )\n            )\n\n        if hasattr(model, ""tie_weights""):\n            model.tie_weights()  # make sure word embedding weights are still tied\n\n        if output_loading_info:\n            loading_info = {\n                ""missing_keys"": missing_keys,\n                ""unexpected_keys"": unexpected_keys,\n                ""error_msgs"": error_msgs,\n            }\n            return model, loading_info\n\n        return model\n\n    def save_pretrained(self, save_directory):\n        """"""save trained model in 8bit""""""\n        super().save_pretrained(save_directory)\n        # Only save the model it-self if we are using distributed training\n        model_to_save = self.module if hasattr(self, ""module"") else self\n        model_to_save.toggle_8bit(True)\n        output_model_file = os.path.join(save_directory, QUANT_WEIGHTS_NAME)\n        torch.save(model_to_save.state_dict(), output_model_file)\n        model_to_save.toggle_8bit(False)\n\n    def toggle_8bit(self, mode: bool):\n        def _toggle_8bit(module):\n            if isinstance(module, QuantizedLayer):\n                module.mode_8bit = mode\n\n        self.apply(_toggle_8bit)\n        if mode:\n            training = self.training\n            self.eval()\n            self.train(training)\n\n\nclass QuantizedBertModel(QuantizedBertPreTrainedModel, BertModel):\n    def __init__(self, config):\n        # we only want BertForQuestionAnswering init to run to avoid unnecessary\n        # initializations\n        super(BertModel, self).__init__(config)\n\n        self.embeddings = QuantizedBertEmbeddings(config)\n        self.encoder = QuantizedBertEncoder(config)\n        self.pooler = QuantizedBertPooler(config)\n\n        self.apply(self.init_weights)\n\n\nclass QuantizedBertForSequenceClassification(\n    QuantizedBertPreTrainedModel, BertForSequenceClassification\n):\n    def __init__(self, config):\n        # we only want BertForQuestionAnswering init to run to avoid unnecessary\n        # initializations\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = QuantizedBertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = quantized_linear_setup(\n            config, ""head"", config.hidden_size, self.config.num_labels\n        )\n\n        self.apply(self.init_weights)\n\n\nclass QuantizedBertForQuestionAnswering(QuantizedBertPreTrainedModel, BertForQuestionAnswering):\n    def __init__(self, config):\n        # we only want BertForQuestionAnswering init to run to avoid unnecessary\n        # initializations\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = QuantizedBertModel(config)\n        self.qa_outputs = quantized_linear_setup(\n            config, ""head"", config.hidden_size, config.num_labels\n        )\n\n        self.apply(self.init_weights)\n\n\nclass QuantizedBertForTokenClassification(QuantizedBertPreTrainedModel, BertForTokenClassification):\n    def __init__(self, config):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = QuantizedBertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = quantized_linear_setup(\n            config, ""head"", config.hidden_size, config.num_labels\n        )\n\n        self.apply(self.init_weights)\n'"
nlp_architect/models/transformers/sequence_classification.py,6,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport os\nfrom typing import List, Union\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nfrom transformers import (\n    BertForSequenceClassification,\n    RobertaForSequenceClassification,\n    XLMForSequenceClassification,\n    XLNetForSequenceClassification,\n)\n\nfrom nlp_architect.data.sequence_classification import SequenceClsInputExample\nfrom nlp_architect.models.transformers.base_model import InputFeatures, TransformerBase\nfrom nlp_architect.models.transformers.quantized_bert import QuantizedBertForSequenceClassification\nfrom nlp_architect.utils.metrics import accuracy\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformerSequenceClassifier(TransformerBase):\n    """"""\n    Transformer sequence classifier\n\n    Args:\n        model_type (str): transformer base model type\n        labels (List[str], optional): list of labels. Defaults to None.\n        task_type (str, optional): task type (classification/regression). Defaults to\n        classification.\n        metric_fn ([type], optional): metric to use for evaluation. Defaults to\n        simple_accuracy.\n    """"""\n\n    MODEL_CLASS = {\n        ""bert"": BertForSequenceClassification,\n        ""quant_bert"": QuantizedBertForSequenceClassification,\n        ""xlnet"": XLNetForSequenceClassification,\n        ""xlm"": XLMForSequenceClassification,\n        ""roberta"": RobertaForSequenceClassification,\n    }\n\n    def __init__(\n        self,\n        model_type: str,\n        labels: List[str] = None,\n        task_type=""classification"",\n        metric_fn=accuracy,\n        load_quantized=False,\n        *args,\n        **kwargs,\n    ):\n        assert model_type in self.MODEL_CLASS.keys(), ""unsupported model type""\n        self.labels = labels\n        self.num_labels = len(labels)\n        super(TransformerSequenceClassifier, self).__init__(\n            model_type, labels=labels, num_labels=self.num_labels, *args, **kwargs\n        )\n        self.model_class = self.MODEL_CLASS[model_type]\n        if model_type == ""quant_bert"" and load_quantized:\n            self.model = self.model_class.from_pretrained(\n                self.model_name_or_path,\n                from_tf=bool("".ckpt"" in self.model_name_or_path),\n                config=self.config,\n                from_8bit=load_quantized,\n            )\n        else:\n            self.model = self.model_class.from_pretrained(\n                self.model_name_or_path,\n                from_tf=bool("".ckpt"" in self.model_name_or_path),\n                config=self.config,\n            )\n        self.task_type = task_type\n        self.metric_fn = metric_fn\n        self.to(self.device, self.n_gpus)\n\n    def train(\n        self,\n        train_data_set: DataLoader,\n        dev_data_set: Union[DataLoader, List[DataLoader]] = None,\n        test_data_set: Union[DataLoader, List[DataLoader]] = None,\n        gradient_accumulation_steps: int = 1,\n        per_gpu_train_batch_size: int = 8,\n        max_steps: int = -1,\n        num_train_epochs: int = 3,\n        max_grad_norm: float = 1.0,\n        logging_steps: int = 50,\n        save_steps: int = 100,\n    ):\n        """"""\n        Train a model\n\n        Args:\n            train_data_set (DataLoader): training data set\n            dev_data_set (Union[DataLoader, List[DataLoader]], optional): development set.\n            Defaults to None.\n            test_data_set (Union[DataLoader, List[DataLoader]], optional): test set.\n            Defaults to None.\n            gradient_accumulation_steps (int, optional): num of gradient accumulation steps.\n            Defaults to 1.\n            per_gpu_train_batch_size (int, optional): per GPU train batch size. Defaults to 8.\n            max_steps (int, optional): max steps. Defaults to -1.\n            num_train_epochs (int, optional): number of train epochs. Defaults to 3.\n            max_grad_norm (float, optional): max gradient normalization. Defaults to 1.0.\n            logging_steps (int, optional): number of steps between logging. Defaults to 50.\n            save_steps (int, optional): number of steps between model save. Defaults to 100.\n        """"""\n        self._train(\n            train_data_set,\n            dev_data_set,\n            test_data_set,\n            gradient_accumulation_steps,\n            per_gpu_train_batch_size,\n            max_steps,\n            num_train_epochs,\n            max_grad_norm,\n            logging_steps=logging_steps,\n            save_steps=save_steps,\n        )\n\n    def evaluate_predictions(self, logits, label_ids):\n        """"""\n        Run evaluation of given logits and truth labels\n\n        Args:\n            logits: model logits\n            label_ids: truth label ids\n        """"""\n        preds = self._postprocess_logits(logits)\n        label_ids = label_ids.numpy()\n        result = self.metric_fn(preds, label_ids)\n        try:\n            output_eval_file = os.path.join(self.output_path, ""eval_results.txt"")\n        except TypeError:\n            output_eval_file = os.devnull\n        with open(output_eval_file, ""w"") as writer:\n            logger.info(""***** Evaluation results *****"")\n            for key in sorted(result.keys()):\n                logger.info(""  %s = %s"", key, str(result[key]))\n                writer.write(""%s = %s\\n"" % (key, str(result[key])))\n\n    def convert_to_tensors(\n        self,\n        examples: List[SequenceClsInputExample],\n        max_seq_length: int = 128,\n        include_labels: bool = True,\n    ) -> TensorDataset:\n        """"""\n        Convert examples to tensor dataset\n\n        Args:\n            examples (List[SequenceClsInputExample]): examples\n            max_seq_length (int, optional): max sequence length. Defaults to 128.\n            include_labels (bool, optional): include labels. Defaults to True.\n\n        Returns:\n            TensorDataset:\n        """"""\n        features = self._convert_examples_to_features(\n            examples,\n            max_seq_length,\n            self.tokenizer,\n            self.task_type,\n            include_labels,\n            pad_on_left=bool(self.model_type in [""xlnet""]),\n            pad_token=self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0],\n            pad_token_segment_id=4 if self.model_type in [""xlnet""] else 0,\n        )\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n        if include_labels:\n            if self.task_type == ""classification"":\n                all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n            elif self.task_type == ""regression"":\n                all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n            return TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n        return TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n\n    def inference(\n        self,\n        examples: List[SequenceClsInputExample],\n        max_seq_length: int,\n        batch_size: int = 64,\n        evaluate=False,\n    ):\n        """"""\n        Run inference on given examples\n\n        Args:\n            examples (List[SequenceClsInputExample]): examples\n            batch_size (int, optional): batch size. Defaults to 64.\n\n        Returns:\n            logits\n        """"""\n        data_set = self.convert_to_tensors(\n            examples, max_seq_length=max_seq_length, include_labels=evaluate\n        )\n        inf_sampler = SequentialSampler(data_set)\n        inf_dataloader = DataLoader(data_set, sampler=inf_sampler, batch_size=batch_size)\n        logits = self._evaluate(inf_dataloader)\n        if not evaluate:\n            preds = self._postprocess_logits(logits)\n        else:\n            logits, label_ids = logits\n            preds = self._postprocess_logits(logits)\n            self.evaluate_predictions(logits, label_ids)\n        return preds\n\n    def _postprocess_logits(self, logits):\n        preds = logits.numpy()\n        if self.task_type == ""classification"":\n            preds = np.argmax(preds, axis=1)\n        elif self.task_type == ""regression"":\n            preds = np.squeeze(preds)\n        return preds\n\n    def _convert_examples_to_features(\n        self,\n        examples,\n        max_seq_length,\n        tokenizer,\n        task_type,\n        include_labels=True,\n        pad_on_left=False,\n        pad_token=0,\n        pad_token_segment_id=0,\n        mask_padding_with_zero=True,\n    ):\n        """""" Loads a data file into a list of `InputBatch`s\n            `cls_token_at_end` define the location of the CLS token:\n                - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n                - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n            `cls_token_segment_id` define the segment id associated to the CLS token\n            (0 for BERT, 2 for XLNet)\n        """"""\n\n        if include_labels:\n            label_map = {label: i for i, label in enumerate(self.labels)}\n\n        features = []\n        for (ex_index, example) in enumerate(examples):\n            if ex_index % 10000 == 0:\n                logger.info(""Writing example %d of %d"", ex_index, len(examples))\n\n            inputs = tokenizer.encode_plus(\n                example.text, example.text_b, add_special_tokens=True, max_length=max_seq_length,\n            )\n            input_ids, token_type_ids = inputs[""input_ids""], inputs[""token_type_ids""]\n\n            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n            padding_length = max_seq_length - len(input_ids)\n            if pad_on_left:\n                input_ids = ([pad_token] * padding_length) + input_ids\n                attention_mask = (\n                    [0 if mask_padding_with_zero else 1] * padding_length\n                ) + attention_mask\n                token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n            else:\n                input_ids = input_ids + ([pad_token] * padding_length)\n                attention_mask = attention_mask + (\n                    [0 if mask_padding_with_zero else 1] * padding_length\n                )\n                token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n\n            assert len(input_ids) == max_seq_length\n            assert len(attention_mask) == max_seq_length\n            assert len(token_type_ids) == max_seq_length\n\n            if include_labels:\n                if task_type == ""classification"":\n                    label_id = label_map[example.label]\n                elif task_type == ""regression"":\n                    label_id = float(example.label)\n                else:\n                    raise KeyError(task_type)\n            else:\n                label_id = None\n\n            features.append(\n                InputFeatures(\n                    input_ids=input_ids,\n                    input_mask=attention_mask,\n                    segment_ids=token_type_ids,\n                    label_id=label_id,\n                )\n            )\n        return features\n'"
nlp_architect/models/transformers/token_classification.py,12,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nfrom typing import List, Union\n\nimport torch\nfrom torch.nn import CrossEntropyLoss, Dropout, Linear\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nfrom transformers import (\n    ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP,\n    BertForTokenClassification,\n    BertPreTrainedModel,\n    RobertaConfig,\n    RobertaModel,\n    XLNetModel,\n    XLNetPreTrainedModel,\n)\n\nfrom nlp_architect.data.sequential_tagging import TokenClsInputExample\nfrom nlp_architect.models.transformers.base_model import InputFeatures, TransformerBase\nfrom nlp_architect.models.transformers.quantized_bert import QuantizedBertForTokenClassification\nfrom nlp_architect.utils.metrics import tagging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n\ndef _bert_token_tagging_head_fw(\n    bert,\n    input_ids,\n    token_type_ids=None,\n    attention_mask=None,\n    labels=None,\n    head_mask=None,\n    valid_ids=None,\n):\n    outputs = bert.bert(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask\n    )\n    sequence_output = outputs[0]\n    sequence_output = bert.dropout(sequence_output)\n    logits = bert.classifier(sequence_output)\n\n    if labels is not None:\n        loss_fct = CrossEntropyLoss(ignore_index=0)\n        active_positions = valid_ids.view(-1) != 0.0\n        active_labels = labels.view(-1)[active_positions]\n        active_logits = logits.view(-1, bert.num_labels)[active_positions]\n        loss = loss_fct(active_logits, active_labels)\n        return (\n            loss,\n            logits,\n        )\n    return (logits,)\n\n\nclass BertTokenClassificationHead(BertForTokenClassification):\n    """"""BERT token classification head with linear classifier.\n       This head\'s forward ignores word piece tokens in its linear layer.\n\n       The forward requires an additional \'valid_ids\' map that maps the tensors\n       for valid tokens (e.g., ignores additional word piece tokens generated by\n       the tokenizer, as in NER task the \'X\' label).\n    """"""\n\n    def forward(\n        self,\n        input_ids,\n        token_type_ids=None,\n        attention_mask=None,\n        labels=None,\n        position_ids=None,\n        head_mask=None,\n        valid_ids=None,\n    ):\n        return _bert_token_tagging_head_fw(\n            self,\n            input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            head_mask=head_mask,\n            valid_ids=valid_ids,\n        )\n\n\nclass QuantizedBertForTokenClassificationHead(QuantizedBertForTokenClassification):\n    """"""Quantized BERT token classification head with linear classifier.\n       This head\'s forward ignores word piece tokens in its linear layer.\n\n       The forward requires an additional \'valid_ids\' map that maps the tensors\n       for valid tokens (e.g., ignores additional word piece tokens generated by\n       the tokenizer, as in NER task the \'X\' label).\n    """"""\n\n    def forward(\n        self,\n        input_ids,\n        token_type_ids=None,\n        attention_mask=None,\n        labels=None,\n        position_ids=None,\n        head_mask=None,\n        valid_ids=None,\n    ):\n        return _bert_token_tagging_head_fw(\n            self,\n            input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            head_mask=head_mask,\n            valid_ids=valid_ids,\n        )\n\n\nclass XLNetTokenClassificationHead(XLNetPreTrainedModel):\n    """"""XLNet token classification head with linear classifier.\n       This head\'s forward ignores word piece tokens in its linear layer.\n\n       The forward requires an additional \'valid_ids\' map that maps the tensors\n       for valid tokens (e.g., ignores additional word piece tokens generated by\n       the tokenizer, as in NER task the \'X\' label).\n    """"""\n\n    def __init__(self, config):\n        super(XLNetTokenClassificationHead, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = XLNetModel(config)\n        self.logits_proj = torch.nn.Linear(config.d_model, config.num_labels)\n        self.dropout = torch.nn.Dropout(config.dropout)\n\n        self.apply(self.init_weights)\n\n    def forward(\n        self,\n        input_ids,\n        token_type_ids=None,\n        input_mask=None,\n        attention_mask=None,\n        mems=None,\n        perm_mask=None,\n        target_mapping=None,\n        labels=None,\n        head_mask=None,\n        valid_ids=None,\n    ):\n        transformer_outputs = self.transformer(\n            input_ids,\n            token_type_ids=token_type_ids,\n            input_mask=input_mask,\n            attention_mask=attention_mask,\n            mems=mems,\n            perm_mask=perm_mask,\n            target_mapping=target_mapping,\n            head_mask=head_mask,\n        )\n        sequence_output = transformer_outputs[0]\n        output = self.dropout(sequence_output)\n        logits = self.logits_proj(output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=0)\n            active_positions = valid_ids.view(-1) != 0.0\n            active_labels = labels.view(-1)[active_positions]\n            active_logits = logits.view(-1, self.num_labels)[active_positions]\n            loss = loss_fct(active_logits, active_labels)\n            return (\n                loss,\n                logits,\n            )\n        return (logits,)\n\n\nclass RobertaForTokenClassificationHead(BertPreTrainedModel):\n    """"""RoBERTa token classification head with linear classifier.\n       This head\'s forward ignores word piece tokens in its linear layer.\n\n       The forward requires an additional \'valid_ids\' map that maps the tensors\n       for valid tokens (e.g., ignores additional word piece tokens generated by\n       the tokenizer, as in NER task the \'X\' label).\n    """"""\n\n    config_class = RobertaConfig\n    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = ""roberta""\n\n    def __init__(self, config):\n        super(RobertaForTokenClassificationHead, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config)\n        self.dropout = Dropout(config.hidden_dropout_prob)\n        self.classifier = Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        labels=None,\n        valid_ids=None,\n    ):\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=0)\n            active_positions = valid_ids.view(-1) != 0.0\n            active_labels = labels.view(-1)[active_positions]\n            active_logits = logits.view(-1, self.num_labels)[active_positions]\n            loss = loss_fct(active_logits, active_labels)\n            return (\n                loss,\n                logits,\n            )\n        return (logits,)\n\n\nclass TransformerTokenClassifier(TransformerBase):\n    """"""\n    Transformer word tagging classifier\n\n    Args:\n        model_type(str): model family (classifier head), choose between bert/quant_bert/xlnet\n        labels (List[str], optional): list of tag labels\n    """"""\n\n    MODEL_CLASS = {\n        ""bert"": BertTokenClassificationHead,\n        ""quant_bert"": QuantizedBertForTokenClassificationHead,\n        ""xlnet"": XLNetTokenClassificationHead,\n        ""roberta"": RobertaForTokenClassificationHead,\n    }\n\n    def __init__(\n        self,\n        model_type: str,\n        labels: List[str] = None,\n        training_args: bool = None,\n        *args,\n        load_quantized=False,\n        **kwargs,\n    ):\n        assert model_type in self.MODEL_CLASS.keys(), ""unsupported model type""\n        self.labels = labels\n        self.num_labels = len(labels) + 1  # +1 for padding label\n        self.labels_id_map = {k: v for k, v in enumerate(self.labels, 1)}\n\n        super(TransformerTokenClassifier, self).__init__(\n            model_type, labels=self.labels, num_labels=self.num_labels, *args, **kwargs\n        )\n\n        self.model_class = self.MODEL_CLASS[model_type]\n        if model_type == ""quant_bert"" and load_quantized:\n            self.model = self.model_class.from_pretrained(\n                self.model_name_or_path,\n                from_tf=bool("".ckpt"" in self.model_name_or_path),\n                config=self.config,\n                from_8bit=load_quantized,\n            )\n        else:\n            self.model = self.model_class.from_pretrained(\n                self.model_name_or_path,\n                from_tf=bool("".ckpt"" in self.model_name_or_path),\n                config=self.config,\n            )\n        self.training_args = training_args\n        self.to(self.device, self.n_gpus)\n\n    def train(\n        self,\n        train_data_set: DataLoader,\n        dev_data_set: Union[DataLoader, List[DataLoader]] = None,\n        test_data_set: Union[DataLoader, List[DataLoader]] = None,\n        gradient_accumulation_steps: int = 1,\n        per_gpu_train_batch_size: int = 8,\n        max_steps: int = -1,\n        num_train_epochs: int = 3,\n        max_grad_norm: float = 1.0,\n        logging_steps: int = 50,\n        save_steps: int = 100,\n        best_result_file: str = None,\n    ):\n        """"""\n        Run model training\n\n        Args:\n            train_data_set (DataLoader): training dataset\n            dev_data_set (Union[DataLoader, List[DataLoader]], optional): development data set\n            (can be list). Defaults to None.\n            test_data_set (Union[DataLoader, List[DataLoader]], optional): test data set\n            (can be list). Defaults to None.\n            gradient_accumulation_steps (int, optional): gradient accumulation steps.\n            Defaults to 1.\n            per_gpu_train_batch_size (int, optional): per GPU train batch size (or GPU).\n            Defaults to 8.\n            max_steps (int, optional): max steps for training. Defaults to -1.\n            num_train_epochs (int, optional): number of training epochs. Defaults to 3.\n            max_grad_norm (float, optional): max gradient norm. Defaults to 1.0.\n            logging_steps (int, optional): number of steps between logging. Defaults to 50.\n            save_steps (int, optional): number of steps between model save. Defaults to 100.\n            best_result_file (str, optional): path to save best dev results when it\'s updated.\n        """"""\n        self._train(\n            train_data_set,\n            dev_data_set,\n            test_data_set,\n            gradient_accumulation_steps,\n            per_gpu_train_batch_size,\n            max_steps,\n            num_train_epochs,\n            max_grad_norm,\n            logging_steps=logging_steps,\n            save_steps=save_steps,\n            best_result_file=best_result_file,\n        )\n\n    def _batch_mapper(self, batch):\n        mapping = {\n            ""input_ids"": batch[0],\n            ""attention_mask"": batch[1],\n            # XLM don\'t use segment_ids\n            ""token_type_ids"": batch[2],\n            ""valid_ids"": batch[3],\n        }\n        if len(batch) > 4:\n            mapping.update({""labels"": batch[4]})\n        return mapping\n\n    def evaluate_predictions(self, logits, label_ids):\n        """"""\n        Run evaluation of given logist and truth labels\n\n        Args:\n            logits: model logits\n            label_ids: truth label ids\n        """"""\n        active_positions = label_ids.view(-1) != 0.0\n        active_labels = label_ids.view(-1)[active_positions]\n        active_logits = logits.view(-1, len(self.labels_id_map) + 1)[active_positions]\n        logits = torch.argmax(F.log_softmax(active_logits, dim=1), dim=1)\n        logits = logits.detach().cpu().numpy()\n        out_label_ids = active_labels.detach().cpu().numpy()\n        _, _, f1 = self.extract_labels(out_label_ids, self.labels_id_map, logits)\n        logger.info(""Results on evaluation set: F1 = {}"".format(f1))\n        return f1\n\n    @staticmethod\n    def extract_labels(label_ids, label_map, logits):\n        y_true = []\n        y_pred = []\n        for p, y in zip(logits, label_ids):\n            y_pred.append(label_map.get(p, ""O""))\n            y_true.append(label_map.get(y, ""O""))\n        assert len(y_true) == len(y_pred)\n        return tagging(y_pred, y_true)\n\n    def convert_to_tensors(\n        self,\n        examples: List[TokenClsInputExample],\n        max_seq_length: int = 128,\n        include_labels: bool = True,\n    ) -> TensorDataset:\n        """"""\n        Convert examples to tensor dataset\n\n        Args:\n            examples (List[SequenceClsInputExample]): examples\n            max_seq_length (int, optional): max sequence length. Defaults to 128.\n            include_labels (bool, optional): include labels. Defaults to True.\n\n        Returns:\n            TensorDataset:\n        """"""\n        features = self._convert_examples_to_features(\n            examples,\n            max_seq_length,\n            self.tokenizer,\n            include_labels,\n            # xlnet has a cls token at the end\n            cls_token_at_end=bool(self.model_type in [""xlnet""]),\n            cls_token=self.tokenizer.cls_token,\n            cls_token_segment_id=2 if self.model_type in [""xlnet""] else 0,\n            sep_token=self.tokenizer.sep_token,\n            sep_token_extra=bool(self.model_type in [""roberta""]),\n            # pad on the left for xlnet\n            pad_on_left=bool(self.model_type in [""xlnet""]),\n            pad_token=self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0],\n            pad_token_segment_id=4 if self.model_type in [""xlnet""] else 0,\n        )\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n        all_valid_ids = torch.tensor([f.valid_ids for f in features], dtype=torch.long)\n\n        if include_labels:\n            all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n            dataset = TensorDataset(\n                all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids\n            )\n        else:\n            dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_valid_ids)\n        return dataset\n\n    def _convert_examples_to_features(\n        self,\n        examples: List[TokenClsInputExample],\n        max_seq_length,\n        tokenizer,\n        include_labels=True,\n        cls_token_at_end=False,\n        pad_on_left=False,\n        cls_token=""[CLS]"",\n        sep_token=""[SEP]"",\n        pad_token=0,\n        sequence_segment_id=0,\n        sep_token_extra=0,\n        cls_token_segment_id=1,\n        pad_token_segment_id=0,\n        mask_padding_with_zero=True,\n    ):\n        """""" Loads a data file into a list of `InputBatch`s\n            `cls_token_at_end` define the location of the CLS token:\n                - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n                - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n            `cls_token_segment_id` define the segment id associated to the CLS token\n            (0 for BERT, 2 for XLNet)\n        """"""\n\n        if include_labels:\n            label_map = {v: k for k, v in self.labels_id_map.items()}\n            label_pad = 0\n\n        features = []\n        for (ex_index, example) in enumerate(examples):\n            if ex_index % 10000 == 0:\n                logger.info(""Processing example %d of %d"", ex_index, len(examples))\n\n            tokens = []\n            labels = []\n            valid_tokens = []\n            for i, token in enumerate(example.tokens):\n                new_tokens = tokenizer.tokenize(token)\n                tokens.extend(new_tokens)\n                v_tok = [0] * (len(new_tokens))\n                v_tok[0] = 1\n                valid_tokens.extend(v_tok)\n                if include_labels:\n                    v_lbl = [label_pad] * (len(new_tokens))\n                    v_lbl[0] = label_map.get(example.label[i])\n                    labels.extend(v_lbl)\n\n            # truncate by max_seq_length\n            special_tokens_count = 3 if sep_token_extra else 2\n            tokens = tokens[: (max_seq_length - special_tokens_count)]\n            valid_tokens = valid_tokens[: (max_seq_length - special_tokens_count)]\n            if include_labels:\n                labels = labels[: (max_seq_length - special_tokens_count)]\n\n            tokens += [sep_token]\n            if include_labels:\n                labels += [label_pad]\n            valid_tokens += [0]\n            if sep_token_extra:  # roberta special case\n                tokens += [sep_token]\n                valid_tokens += [0]\n                if include_labels:\n                    labels += [label_pad]\n            segment_ids = [sequence_segment_id] * len(tokens)\n\n            if cls_token_at_end:\n                tokens = tokens + [cls_token]\n                segment_ids = segment_ids + [cls_token_segment_id]\n                if include_labels:\n                    labels = labels + [label_pad]\n                valid_tokens = valid_tokens + [0]\n            else:\n                tokens = [cls_token] + tokens\n                segment_ids = [cls_token_segment_id] + segment_ids\n                if include_labels:\n                    labels = [label_pad] + labels\n                valid_tokens = [0] + valid_tokens\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            padding_length = max_seq_length - len(input_ids)\n            if pad_on_left:\n                input_ids = ([pad_token] * padding_length) + input_ids\n                input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n                segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n                if include_labels:\n                    labels = ([label_pad] * padding_length) + labels\n                valid_tokens = ([0] * padding_length) + valid_tokens\n            else:\n                input_ids = input_ids + ([pad_token] * padding_length)\n                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n                segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n                if include_labels:\n                    labels = labels + ([label_pad] * padding_length)\n                valid_tokens = valid_tokens + ([0] * padding_length)\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n            assert len(valid_tokens) == max_seq_length\n            if include_labels:\n                assert len(labels) == max_seq_length\n\n            features.append(\n                InputFeatures(\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    segment_ids=segment_ids,\n                    label_id=labels,\n                    valid_ids=valid_tokens,\n                )\n            )\n        return features\n\n    def inference(\n        self, examples: List[TokenClsInputExample], max_seq_length: int, batch_size: int = 64\n    ):\n        """"""\n        Run inference on given examples\n\n        Args:\n            examples (List[SequenceClsInputExample]): examples\n            batch_size (int, optional): batch size. Defaults to 64.\n\n        Returns:\n            logits\n        """"""\n        data_set = self.convert_to_tensors(\n            examples, max_seq_length=max_seq_length, include_labels=False\n        )\n        inf_sampler = SequentialSampler(data_set)\n        inf_dataloader = DataLoader(data_set, sampler=inf_sampler, batch_size=batch_size)\n        logits = self._evaluate(inf_dataloader)\n        active_positions = data_set.tensors[-1].view(len(data_set), -1) != 0.0\n        logits = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n        res_ids = []\n        for i in range(logits.size()[0]):\n            res_ids.append(logits[i][active_positions[i]].detach().cpu().numpy())\n        output = []\n        for tag_ids, ex in zip(res_ids, examples):\n            tokens = ex.tokens\n            tags = [self.labels_id_map.get(t, ""O"") for t in tag_ids]\n            output.append((tokens, tags))\n        return output\n'"
nlp_architect/nn/tensorflow/__init__.py,0,b''
nlp_architect/nn/torch/__init__.py,7,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport random\nimport time\n\nimport numpy as np\nimport torch\n\n\ndef setup_backend(no_cuda):\n    """"""Setup backend according to selected backend and detected configuration\n    """"""\n    device = torch.device(""cuda"" if torch.cuda.is_available() and not no_cuda else ""cpu"")\n    if torch.cuda.is_available() and not no_cuda:\n        device = torch.device(""cuda"")\n        n_gpu = torch.cuda.device_count()\n    else:\n        device = torch.device(""cpu"")\n        n_gpu = 0\n    return device, n_gpu\n\n\ndef set_seed(seed, n_gpus=None):\n    """"""set and return seed\n    """"""\n    if seed == -1:\n        seed = int(time.time())\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if n_gpus is not None and n_gpus > 0:\n        torch.cuda.manual_seed_all(seed)\n    return seed\n'"
nlp_architect/nn/torch/distillation.py,3,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nlp_architect.models import TrainableModel\n\nlogger = logging.getLogger(__name__)\n\n\nMSE_loss = nn.MSELoss(reduction=""mean"")\nKL_loss = nn.KLDivLoss(reduction=""batchmean"")\n\nlosses = {\n    ""kl"": KL_loss,\n    ""mse"": MSE_loss,\n}\n\nTEACHER_TYPES = [""bert""]\n\n\nclass TeacherStudentDistill:\n    """"""\n        Teacher-Student knowledge distillation helper.\n        Use this object when training a model with KD and a teacher model.\n\n        Args:\n            teacher_model (TrainableModel): teacher model\n            temperature (float, optional): KD temperature. Defaults to 1.0.\n            dist_w (float, optional): distillation loss weight. Defaults to 0.1.\n            loss_w (float, optional): student loss weight. Defaults to 1.0.\n            loss_function (str, optional): loss function to use (kl for KLDivLoss,\n                mse for MSELoss)\n        """"""\n\n    def __init__(\n        self,\n        teacher_model: TrainableModel,\n        temperature: float = 1.0,\n        dist_w: float = 0.1,\n        loss_w: float = 1.0,\n        loss_function=""kl"",\n    ):\n        self.teacher = teacher_model\n        self.t = temperature\n        self.dist_w = dist_w\n        self.loss_w = loss_w\n        self.loss_fn = losses.get(loss_function, KL_loss)\n\n    def get_teacher_logits(self, inputs):\n        """"""\n        Get teacher logits\n\n        Args:\n            inputs: input\n\n        Returns:\n            teachr logits\n        """"""\n        return self.teacher.get_logits(inputs)\n\n    @staticmethod\n    def add_args(parser: argparse.ArgumentParser):\n        """"""\n        Add KD arguments to parser\n\n        Args:\n            parser (argparse.ArgumentParser): parser\n        """"""\n        parser.add_argument(\n            ""--teacher_model_path"", type=str, required=True, help=""Path to teacher model""\n        )\n        parser.add_argument(\n            ""--teacher_model_type"",\n            type=str,\n            required=True,\n            choices=TEACHER_TYPES,\n            help=""Teacher model class type"",\n        )\n        parser.add_argument(""--kd_temp"", type=float, default=1.0, help=""KD temperature value"")\n        parser.add_argument(\n            ""--kd_loss_fn"", type=str, choices=[""kl"", ""mse""], default=""mse"", help=""KD loss function""\n        )\n        parser.add_argument(""--kd_dist_w"", type=float, default=0.1, help=""KD weight on loss"")\n        parser.add_argument(\n            ""--kd_student_w"", type=float, default=1.0, help=""KD student weight on loss""\n        )\n\n    def distill_loss(self, loss, student_logits, teacher_logits):\n        """"""\n        Add KD loss\n\n        Args:\n            loss: student loss\n            student_logits: student model logits\n            teacher_logits: teacher model logits\n\n        Returns:\n            KD loss\n        """"""\n        student_log_sm = F.log_softmax(student_logits / self.t, dim=-1)\n        teacher_log_sm = F.softmax(teacher_logits / self.t, dim=-1)\n        distill_loss = self.loss_fn(input=student_log_sm, target=teacher_log_sm)\n        return (self.loss_w * loss) + (distill_loss * self.dist_w * (self.t ** 2))\n\n    def distill_loss_dict(self, loss, student_logits_dict, teacher_logits_dict):\n        """"""\n        Add KD loss\n\n        Args:\n            loss: student loss\n            student_logits: student model logits\n            teacher_logits: teacher model logits\n\n        Returns:\n            KD loss\n        """"""\n\n        student_sm_dict = {}\n        for i in range(len(student_logits_dict.keys())):\n            student_sm_dict[i] = F.log_softmax(student_logits_dict[i] / self.t, dim=-1)\n\n        teacher_sm_dict = {}\n        for i in range(len(teacher_logits_dict.keys())):\n            teacher_sm_dict[i] = F.softmax(teacher_logits_dict[i] / self.t, dim=-1)\n\n        distill_losses = [\n            self.loss_fn(input=student_sm_dict[i], target=teacher_sm_dict[i])\n            for i in range(len(student_sm_dict.keys()))\n        ]\n        distill_loss = torch.mean(torch.stack(distill_losses))\n\n        return (self.loss_w * loss) + (distill_loss * self.dist_w * (self.t ** 2))\n'"
nlp_architect/nn/torch/quantization.py,6,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# pylint: disable=no-member\n""""""\nQuantization ops\n""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nfrom enum import Enum, auto\nimport logging\nfrom abc import ABC, abstractmethod\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom nlp_architect.common import Config\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_dynamic_scale(x, bits, with_grad=False):\n    """"""Calculate dynamic scale for quantization from input by taking the\n    maximum absolute value from x and number of bits""""""\n    with torch.set_grad_enabled(with_grad):\n        threshold = x.abs().max()\n    return get_scale(bits, threshold)\n\n\ndef get_scale(bits, threshold):\n    """"""Calculate scale for quantization according to some constant and number of bits""""""\n    return calc_max_quant_value(bits) / threshold\n\n\ndef calc_max_quant_value(bits):\n    """"""Calculate the maximum symmetric quantized value according to number of bits""""""\n    return 2 ** (bits - 1) - 1\n\n\ndef quantize(input, scale, bits):\n    """"""Do linear quantization to input according to a scale and number of bits""""""\n    thresh = calc_max_quant_value(bits)\n    return input.mul(scale).round().clamp(-thresh, thresh)\n\n\ndef dequantize(input, scale):\n    """"""linear dequantization according to some scale""""""\n    return input.div(scale)\n\n\n# TODO(ofir) future work, implement a layer that uses this function that gives a more comfortable\nclass FakeLinearQuantizationWithSTE(torch.autograd.Function):\n    """"""Simulates error caused by quantization. Uses Straight-Through Estimator for Back prop""""""\n\n    @staticmethod\n    def forward(ctx, input, scale, bits=8):\n        """"""fake quantize input according to scale and number of bits, dequantize\n        quantize(input))""""""\n        return dequantize(quantize(input, scale, bits), scale)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        """"""Calculate estimated gradients for fake quantization using\n        Straight-Through Estimator (STE) according to:\n        https://openreview.net/pdf?id=B1ae1lZRb""""""\n        return grad_output, None, None\n\n\nclass QuantizationMode(Enum):\n    NONE = auto()\n    DYNAMIC = auto()\n    EMA = auto()\n\n\n_fake_quantize = FakeLinearQuantizationWithSTE.apply\n\n\nclass QuantizedLayer(ABC):\n    """"""Quantized Layer interface""""""\n\n    CONFIG_ATTRIBUTES = [""weight_bits"", ""start_step"", ""mode""]\n    REPR_ATTRIBUTES = [""mode"", ""weight_bits""]\n\n    def __init__(self, *args, weight_bits=8, start_step=0, mode=""none"", **kwargs):\n        if weight_bits < 2:\n            raise ValueError(f""weight_bits={weight_bits} must be higher than 1 "")\n        super().__init__(*args, **kwargs)\n        self.weight_bits = weight_bits\n        self.mode = QuantizationMode[mode.upper()]\n        self.start_step = start_step\n        self.register_buffer(""_step"", torch.zeros(1))\n        # buffers for inference\n        self.register_buffer(""quantized_weight"", None)\n        self.register_buffer(""_weight_scale"", None)\n        # handle import and export in 8bit\n        self.mode_8bit = False\n        self._imported_from_quantized = False\n        # register saving hook\n        self._register_state_dict_hook(self._state_dict_hook)\n\n    def forward(self, input):\n        if self.mode == QuantizationMode.NONE:\n            return super().forward(input)\n        if self.training:\n            if self._step >= self.start_step:\n                out = self.training_quantized_forward(input)\n            else:\n                out = super().forward(input)\n            self._step += 1\n        else:\n            out = self.inference_quantized_forward(input)\n        return out\n\n    @abstractmethod\n    def training_quantized_forward(self, input):\n        """"""Implement forward method to be used while training""""""\n\n    @abstractmethod\n    def inference_quantized_forward(self, input):\n        """"""Implement forward method to be used while evaluating""""""\n\n    @classmethod\n    def from_config(cls, *args, config=None, **kwargs):\n        """"""Initialize quantized layer from config""""""\n        return cls(*args, **kwargs, **{k: getattr(config, k) for k in cls.CONFIG_ATTRIBUTES})\n\n    @property\n    def fake_quantized_weight(self):\n        return _fake_quantize(self.weight, self.weight_scale, self.weight_bits)\n\n    @property\n    def weight_scale(self):\n        return (\n            get_dynamic_scale(self.weight, self.weight_bits)\n            if self.training\n            else self._weight_scale\n        )\n\n    def train(self, mode=True):\n        """"""handle transition between quantized model and simulated quantization""""""\n        if self.training != mode:\n            if mode:\n                if self._imported_from_quantized:\n                    raise RuntimeError(\n                        ""Model imported from quantized checkpoint cannot be moved to \\\n                            training mode""\n                    )\n                self._train()\n            else:\n                self._eval()\n        super().train(mode)\n\n    def _train(self):\n        """"""function to be called by self.train(mode=True) which modifies modules attributes\\\n             according to the model""""""\n\n    def _eval(self):\n        """"""function to be called by self.train(mode=False), or eval() which modifies modules\\\n             attributes according to the model""""""\n        self._weight_scale = self.weight_scale\n        self.quantized_weight = quantize(self.weight, self.weight_scale, self.weight_bits)\n\n    def _load_from_state_dict(\n        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n    ):\n        """"""check if model is loaded from quantized checkpoint or regular checkpoint""""""\n        super()._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n        )\n        if state_dict.get(prefix + ""quantized_weight"", None) is not None:\n            if self.training:\n                raise RuntimeError(\n                    ""Can\'t load quantized model in training mode, first change model\'s \\\n                         to evaluation and then load the saved model""\n                )\n            self._imported_from_quantized = True\n\n    @staticmethod\n    def _state_dict_hook(module, state_dict, prefix, local_metadata):\n        """"""hook to be registered to module when exporting the model to 8bit, can be overrided\\\n             to customize to layer behaviour""""""\n        if module.mode_8bit and module.mode != QuantizationMode.NONE:\n            state_dict.pop(prefix + ""weight"", None)\n            state_dict.pop(prefix + ""_step"", None)\n            state_dict[prefix + ""quantized_weight""] = state_dict[prefix + ""quantized_weight""].char()\n        else:\n            state_dict.pop(prefix + ""quantized_weight"", None)\n            state_dict.pop(prefix + ""_weight_scale"", None)\n\n    def extra_repr(self):\n        s = """"\n        for entry in self.REPR_ATTRIBUTES:\n            s += f"", {entry}={getattr(self, entry)}""\n        return super().extra_repr() + s\n\n\nclass QuantizedLinear(QuantizedLayer, nn.Linear):\n    """"""Linear layer with quantization aware training capability""""""\n\n    CONFIG_ATTRIBUTES = QuantizedLayer.CONFIG_ATTRIBUTES + [\n        ""activation_bits"",\n        ""requantize_output"",\n        ""ema_decay"",\n    ]\n    REPR_ATTRIBUTES = QuantizedLayer.REPR_ATTRIBUTES + [\n        ""activation_bits"",\n        ""accumulation_bits"",\n        ""ema_decay"",\n        ""requantize_output"",\n    ]\n\n    def __init__(\n        self, *args, activation_bits=8, requantize_output=True, ema_decay=0.9999, **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        if activation_bits < 2:\n            raise ValueError(f""activation_bits={activation_bits} must be higher than 1 "")\n        self.activation_bits = activation_bits\n        self.accumulation_bits = 32\n        self.ema_decay = ema_decay\n        self.requantize_output = requantize_output\n        self.register_buffer(""input_thresh"", torch.zeros(1))\n        if self.requantize_output:\n            self.register_buffer(""output_thresh"", torch.zeros(1))\n        # real quantization\n        if kwargs.get(""bias"", True):\n            self.register_buffer(""_quantized_bias"", None)\n            self.register_buffer(""bias_scale"", None)\n\n    def training_quantized_forward(self, input):\n        """"""fake quantized forward, fake quantizes weights and activations,\n        learn quantization ranges if quantization mode is EMA.\n        This function should only be used while training""""""\n        assert self.training, ""should only be called when training""\n        if self.mode == QuantizationMode.EMA:\n            self._update_ema(self.input_thresh, input.detach())\n        input_scale = self._get_input_scale(input)\n        out = F.linear(\n            _fake_quantize(input, input_scale, self.activation_bits),\n            self.fake_quantized_weight,\n            self.bias,\n        )\n        if self.requantize_output:\n            if self.mode == QuantizationMode.EMA:\n                self._update_ema(self.output_thresh, out.detach())\n            out = _fake_quantize(out, self._get_output_scale(out), self.activation_bits)\n        return out\n\n    def inference_quantized_forward(self, input):\n        """"""Simulate quantized inference. quantize input and perform calculation with only integer numbers.\n        This function should only be used while doing inference""""""\n        assert not self.training, ""should only be called when not training""\n        input_scale = self._get_input_scale(input)\n        self.bias_scale = self.weight_scale * input_scale\n        quantized_input = quantize(input, input_scale, self.activation_bits)\n        out = F.linear(quantized_input, self.quantized_weight, self.quantized_bias)\n        # TODO(ofir) fuse the operation of requantization with dequantiz\n        out = dequantize(out, self.bias_scale)\n        if self.requantize_output:\n            output_scale = self._get_output_scale(out)\n            out = dequantize(quantize(out, output_scale, self.activation_bits), output_scale)\n        return out\n\n    def _eval(self):\n        super()._eval()\n        if self.mode == QuantizationMode.EMA and self.bias is not None:\n            self.bias_scale = self._get_input_scale() * self.weight_scale\n            self.quantized_bias = quantize(self.bias, self.bias_scale, self.accumulation_bits)\n\n    @staticmethod\n    def _state_dict_hook(module, state_dict, prefix, local_metadata):\n        """"""hook to be registered to module when exporting the model to 8bit,\\\n             can be overrided to customize to layer behaviour""""""\n        super()._state_dict_hook(module, state_dict, prefix, local_metadata)\n        if module.mode_8bit:\n            if module.mode == QuantizationMode.EMA:\n                state_dict.pop(prefix + ""bias"", None)\n                try:\n                    state_dict[prefix + ""_quantized_bias""] = state_dict[\n                        prefix + ""_quantized_bias""\n                    ].int()\n                except KeyError:\n                    # in case there is no bias dont do anything\n                    pass\n        else:\n            state_dict.pop(prefix + ""_quantized_bias"", None)\n            state_dict.pop(prefix + ""bias_scale"", None)\n\n    @property\n    def quantized_bias(self):\n        try:\n            if self.mode == QuantizationMode.EMA:\n                bias = self._quantized_bias\n            elif self.mode == QuantizationMode.DYNAMIC:\n                bias = quantize(self.bias, self.bias_scale, self.accumulation_bits)\n            else:\n                raise RuntimeError(f""Unknown quantization mode: {self.mode}"")\n        except AttributeError:\n            bias = None\n        return bias\n\n    @quantized_bias.setter\n    def quantized_bias(self, value):\n        self._quantized_bias = value\n\n    def _get_input_scale(self, input=None):\n        return self._get_activation_scale(input, self.input_thresh)\n\n    def _get_output_scale(self, output=None):\n        return self._get_activation_scale(output, self.output_thresh)\n\n    def _get_activation_scale(self, activation, threshold):\n        if self.mode == QuantizationMode.DYNAMIC:\n            scale = get_dynamic_scale(activation, self.activation_bits)\n        elif self.mode == QuantizationMode.EMA:\n            scale = get_scale(self.activation_bits, threshold)\n        return scale\n\n    def _update_ema(self, ema, input, reduce_fn=lambda x: x.abs().max()):\n        """"""Update exponential moving average (EMA) of activations thresholds.\n        the reduce_fn calculates the current threshold from the input tensor""""""\n        assert self._step >= self.start_step\n        if self._step == self.start_step:\n            ema.fill_(reduce_fn(input))\n        else:\n            ema.sub_((1 - self.ema_decay) * (ema - reduce_fn(input)))\n\n\nclass QuantizedEmbedding(QuantizedLayer, nn.Embedding):\n    """"""Embedding layer with quantization aware training capability""""""\n\n    def training_quantized_forward(self, input):\n        """"""Return quantized embeddings""""""\n        assert self.training, ""should only be called when training""\n        return F.embedding(\n            input,\n            self.fake_quantized_weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n\n    def inference_quantized_forward(self, input):\n        """"""forward to be used during inference""""""\n        assert not self.training, ""should only be called when not training""\n        q_embeddings = F.embedding(\n            input,\n            self.quantized_weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n        return dequantize(q_embeddings, self.weight_scale)\n\n\nclass QuantizationConfig(Config):\n    """"""Quantization Configuration Object""""""\n\n    ATTRIBUTES = {\n        ""activation_bits"": 8,\n        ""weight_bits"": 8,\n        ""mode"": ""none"",\n        ""start_step"": 0,\n        ""ema_decay"": 0.9999,\n        ""requantize_output"": True,\n    }\n'"
nlp_architect/procedures/transformers/__init__.py,0,b''
nlp_architect/procedures/transformers/base.py,0,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\n\nfrom nlp_architect.models.transformers.base_model import get_models\n\n\ndef create_base_args(parser: argparse.ArgumentParser, model_types=None):\n    """"""Add base arguments for Transformers based models\n    """"""\n    # Required parameters\n    if model_types is not None and len(model_types) > 1:\n        parser.add_argument(\n            ""--model_type"",\n            default=None,\n            type=str,\n            choices=model_types,\n            required=True,\n            help=""Model type selected in the list: "" + "", "".join(model_types),\n        )\n    parser.add_argument(\n        ""--output_dir"",\n        default=None,\n        type=str,\n        required=True,\n        help=""The output directory where the model predictions and checkpoints "" ""will be written."",\n    )\n\n    # Other parameters\n    parser.add_argument(\n        ""--tokenizer_name"",\n        default="""",\n        type=str,\n        help=""Pretrained tokenizer name or path if not the same as model_name"",\n    )\n    parser.add_argument(\n        ""--max_seq_length"",\n        default=128,\n        type=int,\n        help=""The maximum total input sequence length after tokenization. ""\n        ""Sequences longer than this will be truncated, sequences shorter ""\n        ""will be padded."",\n    )\n    parser.add_argument(\n        ""--cache_dir"",\n        default="""",\n        type=str,\n        help=""Where do you want to store the pre-trained models downloaded "" ""from s3"",\n    )\n    parser.add_argument(\n        ""--do_lower_case"",\n        action=""store_true"",\n        help=""Set this flag if you are using an uncased model."",\n    )\n    parser.add_argument(\n        ""--per_gpu_eval_batch_size"",\n        default=8,\n        type=int,\n        help=""Batch size per GPU/CPU for evaluation."",\n    )\n    parser.add_argument(""--no_cuda"", action=""store_true"", help=""Avoid using CUDA when available"")\n    parser.add_argument(\n        ""--overwrite_output_dir"",\n        action=""store_true"",\n        help=""Overwrite the content of the output directory"",\n    )\n    parser.add_argument(\n        ""--overwrite_cache"",\n        action=""store_true"",\n        help=""Overwrite the cached training and evaluation sets"",\n    )\n\n\ndef inference_args(parser: argparse.ArgumentParser):\n    """""" Add inference specific arguments for Transoformer based models\n    """"""\n    parser.add_argument(\n        ""--model_path"", default=None, type=str, required=True, help=""Path to pre-trained model""\n    )\n    parser.add_argument(\n        ""--load_quantized_model"",\n        action=""store_true"",\n        help=""Load and perform Inference from saved quantized model,\\\n                        \'quant_pytorch_model.bin\' file must exist in directory and model\\\n                             type must be \'quant_<model>\'"",\n    )\n\n\ndef train_args(parser: argparse.ArgumentParser, models_family=None):\n    """""" Add training specific arguments for Transformer based models\n    """"""\n    parser.add_argument(\n        ""--model_name_or_path"",\n        default=None,\n        type=str,\n        required=True,\n        help=""Path to pre-trained model or shortcut name selected in the list: ""\n        + "", "".join(get_models(models_family)),\n    )\n    parser.add_argument(\n        ""--config_name"",\n        default="""",\n        type=str,\n        help=""Pretrained config name or path if not the same as model_name"",\n    )\n    parser.add_argument(\n        ""--evaluate_during_training"",\n        action=""store_true"",\n        help=""Run evaluation during training at each logging step."",\n    )\n    parser.add_argument(\n        ""--per_gpu_train_batch_size"",\n        default=8,\n        type=int,\n        help=""Batch size per GPU/CPU for training."",\n    )\n    parser.add_argument(\n        ""--gradient_accumulation_steps"",\n        type=int,\n        default=1,\n        help=""Number of updates steps to accumulate before performing a "" ""backward/update pass."",\n    )\n    parser.add_argument(\n        ""--learning_rate"", default=5e-5, type=float, help=""The initial learning rate for Adam.""\n    )\n    parser.add_argument(\n        ""--weight_decay"", default=0.0, type=float, help=""Weight deay if we apply some.""\n    )\n    parser.add_argument(\n        ""--adam_epsilon"", default=1e-8, type=float, help=""Epsilon for Adam optimizer.""\n    )\n    parser.add_argument(""--max_grad_norm"", default=1.0, type=float, help=""Max gradient norm."")\n    parser.add_argument(\n        ""--num_train_epochs"",\n        default=3,\n        type=int,\n        help=""Total number of training epochs to perform."",\n    )\n    parser.add_argument(\n        ""--max_steps"",\n        default=-1,\n        type=int,\n        help=""If > 0: set total number of training steps to perform. "" ""Override num_train_epochs."",\n    )\n    parser.add_argument(\n        ""--warmup_steps"", default=0, type=int, help=""Linear warmup over warmup_steps.""\n    )\n    parser.add_argument(""--logging_steps"", type=int, default=50, help=""Log every X updates steps."")\n    parser.add_argument(\n        ""--save_steps"", type=int, default=500, help=""Save checkpoint every X updates steps.""\n    )\n    parser.add_argument(\n        ""--eval_all_checkpoints"",\n        action=""store_true"",\n        help=""Evaluate all checkpoints starting with the same prefix as ""\n        + ""model_name ending and ending with step number"",\n    )\n    parser.add_argument(""--seed"", type=int, default=42, help=""random seed for initialization"")\n'"
nlp_architect/procedures/transformers/glue.py,1,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport io\nimport logging\nimport os\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nfrom nlp_architect.data.glue_tasks import get_glue_task, get_metric_fn, processors\nfrom nlp_architect.models.transformers import TransformerSequenceClassifier\nfrom nlp_architect.nn.torch import set_seed, setup_backend\nfrom nlp_architect.procedures.procedure import Procedure\nfrom nlp_architect.procedures.registry import register_inference_cmd, register_train_cmd\nfrom nlp_architect.procedures.transformers.base import create_base_args, inference_args, train_args\nfrom nlp_architect.utils.io import prepare_output_path\n\nlogger = logging.getLogger(__name__)\n\n\n@register_train_cmd(\n    name=""transformer_glue"", description=""Train (finetune) a BERT/XLNet/XLM model on a GLUE task""\n)\nclass TransformerGlueTrain(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        add_glue_args(parser)\n        create_base_args(parser, model_types=TransformerSequenceClassifier.MODEL_CLASS.keys())\n        train_args(parser, models_family=TransformerSequenceClassifier.MODEL_CLASS.keys())\n\n    @staticmethod\n    def run_procedure(args):\n        do_training(args)\n\n\n@register_inference_cmd(\n    name=""transformer_glue"", description=""Run a BERT/XLNet/XLM model on a GLUE task""\n)\nclass TransformerGlueRun(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        add_glue_args(parser)\n        add_glue_inference_args(parser)\n        inference_args(parser)\n        create_base_args(parser, model_types=TransformerSequenceClassifier.MODEL_CLASS.keys())\n\n    @staticmethod\n    def run_procedure(args):\n        do_inference(args)\n\n\ndef add_glue_args(parser: argparse.ArgumentParser):\n    parser.add_argument(\n        ""--task_name"",\n        default=None,\n        type=str,\n        required=True,\n        help=""The name of the task to train selected in the list: "" + "", "".join(processors.keys()),\n    )\n    parser.add_argument(\n        ""--data_dir"",\n        default=None,\n        type=str,\n        required=True,\n        help=""The input data dir. Should contain dataset files to be parsed ""\n        + ""by the dataloaders."",\n    )\n\n\ndef add_glue_inference_args(parser: argparse.ArgumentParser):\n    parser.add_argument(\n        ""--evaluate"", action=""store_true"", help=""Evaluate the model on the task\'s development set""\n    )\n\n\ndef do_training(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    # Set seed\n    args.seed = set_seed(args.seed, n_gpus)\n    # Prepare GLUE task\n    args.task_name = args.task_name.lower()\n    task = get_glue_task(args.task_name, data_dir=args.data_dir)\n    classifier = TransformerSequenceClassifier(\n        model_type=args.model_type,\n        model_name_or_path=args.model_name_or_path,\n        labels=task.get_labels(),\n        task_type=task.task_type,\n        metric_fn=get_metric_fn(task.name),\n        config_name=args.config_name,\n        tokenizer_name=args.tokenizer_name,\n        do_lower_case=args.do_lower_case,\n        output_path=args.output_dir,\n        device=device,\n        n_gpus=n_gpus,\n    )\n\n    train_batch_size = args.per_gpu_train_batch_size * max(1, n_gpus)\n\n    train_ex = task.get_train_examples()\n    dev_ex = task.get_dev_examples()\n    train_dataset = classifier.convert_to_tensors(train_ex, args.max_seq_length)\n    dev_dataset = classifier.convert_to_tensors(dev_ex, args.max_seq_length)\n    train_sampler = RandomSampler(train_dataset)\n    dev_sampler = SequentialSampler(dev_dataset)\n    train_dl = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n    dev_dl = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=args.per_gpu_eval_batch_size)\n\n    total_steps, _ = classifier.get_train_steps_epochs(\n        args.max_steps, args.num_train_epochs, args.per_gpu_train_batch_size, len(train_dataset)\n    )\n    classifier.setup_default_optimizer(\n        weight_decay=args.weight_decay,\n        learning_rate=args.learning_rate,\n        adam_epsilon=args.adam_epsilon,\n        warmup_steps=args.warmup_steps,\n        total_steps=total_steps,\n    )\n    classifier.train(\n        train_dl,\n        dev_dl,\n        None,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        per_gpu_train_batch_size=args.per_gpu_train_batch_size,\n        max_steps=args.max_steps,\n        num_train_epochs=args.num_train_epochs,\n        max_grad_norm=args.max_grad_norm,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n    )\n    classifier.save_model(args.output_dir, args=args)\n\n\ndef do_inference(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    args.task_name = args.task_name.lower()\n    task = get_glue_task(args.task_name, data_dir=args.data_dir)\n    args.batch_size = args.per_gpu_eval_batch_size * max(1, n_gpus)\n    classifier = TransformerSequenceClassifier.load_model(\n        model_path=args.model_path,\n        model_type=args.model_type,\n        task_type=task.task_type,\n        metric_fn=get_metric_fn(task.name),\n        do_lower_case=args.do_lower_case,\n        load_quantized=args.load_quantized_model,\n    )\n    classifier.to(device, n_gpus)\n    examples = task.get_dev_examples() if args.evaluate else task.get_test_examples()\n    preds = classifier.inference(\n        examples, args.max_seq_length, args.batch_size, evaluate=args.evaluate\n    )\n    with io.open(os.path.join(args.output_dir, ""output.txt""), ""w"", encoding=""utf-8"") as fw:\n        for p in preds:\n            fw.write(""{}\\n"".format(p))\n'"
nlp_architect/procedures/transformers/seq_tag.py,1,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport io\nimport logging\nimport os\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nfrom nlp_architect.data.sequential_tagging import TokenClsInputExample, TokenClsProcessor\nfrom nlp_architect.data.utils import write_column_tagged_file\nfrom nlp_architect.models.transformers import TransformerTokenClassifier\nfrom nlp_architect.nn.torch import setup_backend, set_seed\nfrom nlp_architect.procedures.procedure import Procedure\nfrom nlp_architect.procedures.registry import register_inference_cmd, register_train_cmd\nfrom nlp_architect.procedures.transformers.base import create_base_args, inference_args, train_args\nfrom nlp_architect.utils.io import prepare_output_path\nfrom nlp_architect.utils.text import SpacyInstance\n\nlogger = logging.getLogger(__name__)\n\n\n@register_train_cmd(\n    name=""transformer_token"", description=""Train a BERT/XLNet model with token classification head""\n)\nclass TransformerTokenClsTrain(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        parser.add_argument(\n            ""--data_dir"",\n            default=None,\n            type=str,\n            required=True,\n            help=""The input data dir. Should contain dataset files to be parsed ""\n            + ""by the dataloaders."",\n        )\n        train_args(parser, models_family=TransformerTokenClassifier.MODEL_CLASS.keys())\n        create_base_args(parser, model_types=TransformerTokenClassifier.MODEL_CLASS.keys())\n        parser.add_argument(\n            ""--train_file_name"",\n            type=str,\n            default=""train.txt"",\n            help=""File name of the training dataset"",\n        )\n        parser.add_argument(\n            ""--ignore_token"",\n            type=str,\n            default="""",\n            help=""a token to ignore when processing the data"",\n        )\n        parser.add_argument(\n            ""--best_result_file"",\n            type=str,\n            default=""best_result.txt"",\n            help=""file path for best evaluation output"",\n        )\n\n    @staticmethod\n    def run_procedure(args):\n        do_training(args)\n\n\n@register_inference_cmd(\n    name=""transformer_token"", description=""Run a BERT/XLNet model with token classification head""\n)\nclass TransformerTokenClsRun(Procedure):\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser):\n        parser.add_argument(\n            ""--data_file"",\n            default=None,\n            type=str,\n            required=True,\n            help=""The data file containing data for inference"",\n        )\n        inference_args(parser)\n        create_base_args(parser, model_types=TransformerTokenClassifier.MODEL_CLASS.keys())\n\n    @staticmethod\n    def run_procedure(args):\n        do_inference(args)\n\n\ndef do_training(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    # Set seed\n    args.seed = set_seed(args.seed, n_gpus)\n    # prepare data\n    processor = TokenClsProcessor(args.data_dir, ignore_token=args.ignore_token)\n\n    classifier = TransformerTokenClassifier(\n        model_type=args.model_type,\n        model_name_or_path=args.model_name_or_path,\n        labels=processor.get_labels(),\n        config_name=args.config_name,\n        tokenizer_name=args.tokenizer_name,\n        do_lower_case=args.do_lower_case,\n        output_path=args.output_dir,\n        device=device,\n        n_gpus=n_gpus,\n        training_args=args,\n    )\n\n    train_ex = processor.get_train_examples(filename=args.train_file_name)\n    if train_ex is None:\n        raise Exception(""No train examples found, quitting."")\n    dev_ex = processor.get_dev_examples()\n    test_ex = processor.get_test_examples()\n\n    train_batch_size = args.per_gpu_train_batch_size * max(1, n_gpus)\n\n    train_dataset = classifier.convert_to_tensors(train_ex, max_seq_length=args.max_seq_length)\n    train_sampler = RandomSampler(train_dataset)\n    train_dl = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n    dev_dl = None\n    test_dl = None\n    if dev_ex is not None:\n        dev_dataset = classifier.convert_to_tensors(dev_ex, max_seq_length=args.max_seq_length)\n        dev_sampler = SequentialSampler(dev_dataset)\n        dev_dl = DataLoader(\n            dev_dataset, sampler=dev_sampler, batch_size=args.per_gpu_eval_batch_size\n        )\n\n    if test_ex is not None:\n        test_dataset = classifier.convert_to_tensors(test_ex, max_seq_length=args.max_seq_length)\n        test_sampler = SequentialSampler(test_dataset)\n        test_dl = DataLoader(\n            test_dataset, sampler=test_sampler, batch_size=args.per_gpu_eval_batch_size\n        )\n\n    total_steps, _ = classifier.get_train_steps_epochs(\n        args.max_steps, args.num_train_epochs, args.per_gpu_train_batch_size, len(train_dataset)\n    )\n\n    classifier.setup_default_optimizer(\n        weight_decay=args.weight_decay,\n        learning_rate=args.learning_rate,\n        adam_epsilon=args.adam_epsilon,\n        warmup_steps=args.warmup_steps,\n        total_steps=total_steps,\n    )\n    classifier.train(\n        train_dl,\n        dev_dl,\n        test_dl,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        per_gpu_train_batch_size=args.per_gpu_train_batch_size,\n        max_steps=args.max_steps,\n        num_train_epochs=args.num_train_epochs,\n        max_grad_norm=args.max_grad_norm,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        best_result_file=args.best_result_file,\n    )\n    classifier.save_model(args.output_dir, args=args)\n\n\ndef do_inference(args):\n    prepare_output_path(args.output_dir, args.overwrite_output_dir)\n    device, n_gpus = setup_backend(args.no_cuda)\n    args.batch_size = args.per_gpu_eval_batch_size * max(1, n_gpus)\n    inference_examples = process_inference_input(args.data_file)\n    classifier = TransformerTokenClassifier.load_model(\n        model_path=args.model_path,\n        model_type=args.model_type,\n        do_lower_case=args.do_lower_case,\n        load_quantized=args.load_quantized_model,\n    )\n    classifier.to(device, n_gpus)\n    output = classifier.inference(inference_examples, args.max_seq_length, args.batch_size)\n    write_column_tagged_file(args.output_dir + os.sep + ""output.txt"", output)\n\n\ndef process_inference_input(input_file):\n    with io.open(input_file) as fp:\n        texts = [line.strip() for line in fp.readlines()]\n    tokenizer = SpacyInstance(disable=[""tagger"", ""parser"", ""ner""])\n    examples = []\n    for i, t in enumerate(texts):\n        examples.append(TokenClsInputExample(str(i), t, tokenizer.tokenize(t)))\n    return examples\n'"
nlp_architect/utils/resources/__init__.py,0,b''
solutions/set_expansion/ui/__init__.py,0,b''
solutions/set_expansion/ui/main.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport socket\nimport pickle\nimport logging\nimport re\nfrom os.path import dirname, join\n\nfrom bokeh.layouts import column, layout\nfrom bokeh.models import ColumnDataSource, Div, Row, CustomJS\nfrom bokeh.models.widgets import Button, DataTable, TableColumn, CheckboxGroup, MultiSelect\nfrom bokeh.models.widgets.inputs import TextInput\nfrom bokeh.io import curdoc\n\nfrom settings import grouping, expand_host, expand_port\n\n# pylint: skip-file\nlogger = logging.getLogger(__name__)\n\nvocab = None\nvocab_dict = {}\ncut_vocab_dict = {}\nmax_visible_phrases = 5000\nworking_text = ""please wait...""\nfetching_text = ""Fetching vocabulary from server (one time only), this can take few minutes...""\nseed_check_text = """"\nall_selected_phrases = []\nsearch_flag = False\nmax_phrase_length = 40\nclear_flag = False\nexpand_columns = [\n    TableColumn(field=""res"", title=""Results""),\n    TableColumn(field=""score"", title=""Score""),\n]\nempty_table = {""res"": 15 * [""""], ""score"": 15 * [""""]}\ncheckbox_label = ""Show extracted term groups"" if grouping else ""Show extracted phrases""\n\n# create ui components\n\nseed_input_title = ""Please enter a comma separated seed list of terms:""\nseed_input_box = TextInput(title=seed_input_title, value="""", width=450, css_classes=[""seed-input""])\nannotation_input = TextInput(\n    title=""Please enter text to annotate:"",\n    value="""",\n    width=400,\n    height=80,\n    css_classes=[""annotation-input""],\n)\nannotation_output = Div(text="""", height=30, width=500, style={""padding-left"": ""35px""})\nannotate_button = Button(\n    label=""Annotate"", button_type=""success"", width=150, css_classes=[""annotation-button""]\n)\ngroup_info_box = Div(text="""", height=30, css_classes=[""group-div""])\nsearch_input_box = TextInput(title=""Search:"", value="""", width=300)\nexpand_button = Button(\n    label=""Expand"", button_type=""success"", width=150, css_classes=[""expand-button""]\n)\nclear_seed_button = Button(\n    label=""Clear"", button_type=""success"", css_classes=[""clear_button""], width=50\n)\nexport_button = Button(\n    label=""Export"", button_type=""success"", css_classes=[""export_button""], width=100\n)\nexpand_table_source = ColumnDataSource(data=empty_table)\nexpand_table = DataTable(\n    source=expand_table_source, columns=expand_columns, width=500, css_classes=[""expand_table""]\n)\nphrases_list = MultiSelect(\n    title="""", value=[], options=[], width=300, size=27, css_classes=[""phrases_list""]\n)\ncheckbox_group = CheckboxGroup(\n    labels=[""Text annotation"", checkbox_label], active=[], width=400, css_classes=[""checkbox_group""]\n)\nannotate_checkbox = CheckboxGroup(\n    labels=[""Text annotation""], active=[], width=400, css_classes=[""annotate_checkbox""]\n)\nsearch_box_area = column(children=[Div(height=10, width=200)])\nworking_label = Div(text="""", style={""color"": ""blue"", ""font-size"": ""15px""})\nsearch_working_label = Div(\n    text="""", style={""color"": ""blue"", ""padding-bottom"": ""0px"", ""font-size"": ""15px""}\n)\nseed_check_label = Div(text="""", style={""font-size"": ""15px""}, height=20, width=500)\ntable_layout = Row(expand_table)\ntable_area = column(children=[table_layout])\nseed_layout = column(\n    Row(seed_input_box, column(Div(height=14, width=0), clear_seed_button)),\n    expand_button,\n    table_area,\n)\nannotation_layout = column(children=[])\n\nphrases_area = column(children=[search_working_label, Div(width=300)])\ncheckbox_layout = column(children=[checkbox_group, phrases_area])\ngrid = layout(\n    [\n        [working_label, Div(width=250), Div(text=""<h1>Set Expansion Demo</h1>"")],\n        [\n            checkbox_layout,\n            seed_layout,\n            Div(width=50),\n            column(Div(height=0, width=0), annotation_layout),\n        ],\n        [group_info_box, Div(width=500), export_button],\n    ]\n)\n\n\n# define callbacks\n\n\ndef get_vocab():\n    """"""\n    Get vocabulary of the np2vec model from the server\n    """"""\n    global vocab\n    logger.info(""sending get_vocab request to server..."")\n    received = send_request_to_server([""get_vocab""])\n    vocab = received\n    for p in vocab:\n        if len(p) < max_phrase_length:\n            vocab_dict[p] = p\n            cut_vocab_dict[p] = p\n        else:\n            vocab_dict[p] = p[: max_phrase_length - 1] + ""...""\n            cut_vocab_dict[p[: max_phrase_length - 1] + ""...""] = p\n\n\ndef send_request_to_server(request):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        # Connect to server and send data\n        sock.connect((expand_host, expand_port))\n        logger.info(""sending request"")\n        req_packet = pickle.dumps(request)\n        # sock.sendall(bytes(request + ""\\n"", ""utf-8""))\n        sock.sendall(req_packet)\n        # Receive data from the server and shut down\n        data = b""""\n        ctr = 0\n        while True:\n            packet = sock.recv(134217728)\n            logger.info(""%s. received: %s"", str(ctr), str(len(packet)))\n            ctr += 1\n            if not packet:\n                break\n            data += packet\n            logger.info(""got response, uncompressing"")\n        received = pickle.loads(data)\n        # logger.info(""Received: {}"".format(received))\n        return received\n    except EOFError:\n        logger.info(""No data received"")\n    finally:\n        sock.close()\n\n\ndef row_selected_callback(indices, old, new):\n    logger.info(""row selected callback"")\n    global clear_flag, all_selected_phrases\n    if not clear_flag and expand_table_source.data != empty_table:\n        logger.info(""row selected callback. old indices=%s. new indices=%s"", str(old), str(new))\n        # sync phrases lists:\n        old_phrases = [expand_table_source.data[""res""][p] for p in old]\n        new_phrases = [expand_table_source.data[""res""][p] for p in new]\n        logger.info(\n            ""selected_expand was updated: old=%s ,new=%s"", str(old_phrases), str(new_phrases)\n        )\n        # phrase was de-selected from expand list:\n        for o in old_phrases:\n            if o not in new_phrases and (vocab is not None and vocab_dict[o] in phrases_list.value):\n                logger.info(""removing %s from vocab selected"", o)\n                phrases_list.value.remove(vocab_dict[o])\n                break\n        # new phrase was selected from expand list:\n        for n in new_phrases:\n            if n not in old_phrases and (\n                vocab is not None\n                and vocab_dict[n] in phrases_list.options\n                and vocab_dict[n] not in phrases_list.value\n            ):\n                phrases_list.value.append(vocab_dict[n])\n                break\n        update_all_selected_phrases()\n        seed_input_box.value = get_selected_phrases_for_seed()\n\n\ndef update_all_selected_phrases():\n    """"""\n    Sync selected values from both the expand-table and the vocabulary list\n    """"""\n    logger.info(""update selected phrases"")\n    global all_selected_phrases\n    updated_selected_phrases = all_selected_phrases[:]\n    selected_expand = [\n        expand_table_source.data[""res""][i]\n        for i in expand_table_source.selected.indices\n        if expand_table_source.data[""res""][i] != """"\n    ]\n    selected_vocab = phrases_list.value\n    logger.info(""selected expand= %s"", str(selected_expand))\n    logger.info(""selected vocab= %s"", str(selected_vocab))\n    logger.info(""current all_selected_phrases= %s"", str(all_selected_phrases))\n    for x in all_selected_phrases:\n        logger.info(""x= %s"", x)\n        if (x in expand_table_source.data[""res""] and x not in selected_expand) or (\n            vocab is not None\n            and (vocab_dict[x] in phrases_list.options)\n            and (vocab_dict[x] not in selected_vocab)\n        ):\n            logger.info(""removing %s"", x)\n            updated_selected_phrases.remove(x)\n    for e in selected_expand:\n        if e not in updated_selected_phrases:\n            logger.info(""adding %s"", e)\n            updated_selected_phrases.append(e)\n    for v in selected_vocab:\n        full_v = cut_vocab_dict[v]\n        if full_v not in updated_selected_phrases:\n            logger.info(""adding %s"", full_v)\n            updated_selected_phrases.append(full_v)\n    all_selected_phrases = updated_selected_phrases[:]\n    logger.info(""all_selected_phrases list was updated: %s"", str(all_selected_phrases))\n\n\ndef checkbox_callback(checked_value):\n    global search_box_area, phrases_area\n    group_info_box.text = """"\n    if 0 in checked_value:\n        annotation_layout.children = [annotation_input, annotate_button, annotation_output]\n    else:\n        annotation_layout.children = []\n        annotation_output.text = """"\n    if 1 in checked_value:\n        if vocab is None:\n            working_label.text = fetching_text\n            get_vocab()\n        if not phrases_list.options:\n            working_label.text = working_text\n            phrases_list.options = list(cut_vocab_dict.keys())[\n                0:max_visible_phrases\n            ]  # show the cut representation\n        # search_box_area.children = [search_input_box]\n        phrases_area.children = [search_input_box, search_working_label, phrases_list]\n        working_label.text = """"\n    else:\n        # search_box_area.children = []\n        phrases_area.children = []\n        group_info_box.text = """"\n\n\ndef get_expand_results_callback():\n    """"""\n    Send to the server the seed to expand and set the results in the expand\n    table.\n    """"""\n    logger.info(""### new expand request"")\n    working_label.text = working_text\n    global seed_check_text, table_area\n    try:\n        seed_check_label.text = """"\n        table_area.children = [table_layout]\n        seed = seed_input_box.value\n        logger.info(""input seed: %s"", seed)\n        if seed == """":\n            expand_table_source.data = empty_table\n            return\n        seed_words = [x.strip() for x in seed.split("","")]\n        bad_words = """"\n        for w in seed_words:\n            res = send_request_to_server([""in_vocab"", w])\n            if res is False:\n                bad_words += ""\'"" + w + ""\',""\n        if bad_words != """":\n            seed_check_label.text = (\n                \'the words: <span class=""bad-word"">\'\n                + bad_words[:-1]\n                + ""</span> are not in the vocabulary and will be ignored""\n            )\n            logger.info(""setting table area"")\n            table_area.children = [seed_check_label, table_layout]\n        logger.info(""sending expand request to server with seed= %s"", seed)\n        received = send_request_to_server([""expand"", seed])\n        if received is not None:\n            res = [x[0] for x in received]\n            scores = [""{0:.5f}"".format(y[1]) for y in received]\n            logger.info(""setting table data"")\n            expand_table_source.data = {""res"": res, ""score"": scores}\n        else:\n            logger.info(""Nothing received from server"")\n    except Exception as e:\n        logger.info(""Exception: %s"", str(e))\n    finally:\n        working_label.text = """"\n\n\ndef search_callback(value, old, new):\n    group_info_box.text = """"\n    search_working_label.text = working_text\n    logger.info(""search vocab"")\n    global vocab, phrases_list, all_selected_phrases, search_flag\n    search_flag = True\n    phrases_list.value = []\n    if new == """":\n        new_phrases = list(cut_vocab_dict.keys())\n    else:\n        new_phrases = []\n        for x in vocab:\n            if x.lower().startswith(new.lower()) and vocab_dict[x] not in new_phrases:\n                new_phrases.append(vocab_dict[x])\n    phrases_list.options = new_phrases[0:max_visible_phrases]\n    if new != """":\n        phrases_list.options.sort()\n    phrases_list.value = [\n        vocab_dict[x] for x in all_selected_phrases if vocab_dict[x] in phrases_list.options\n    ]\n    logger.info(""selected vocab after search= %s"", str(phrases_list.value))\n    search_working_label.text = """"\n    search_flag = False\n\n\ndef vocab_phrase_selected_callback(attr, old_selected, new_selected):\n    logger.info(""vocab selected"")\n    if grouping:\n        # show group info\n        if len(new_selected) == 1:\n            res = send_request_to_server([""get_group"", new_selected[0]])\n            if res is not None:\n                group_info_box.text = str(res)\n    global clear_flag\n    if not clear_flag:\n        global all_selected_phrases, search_flag\n        if search_flag:\n            return\n        logger.info(\n            ""selected_vocab was updated: old= %s, new= %s"", str(old_selected), str(new_selected)\n        )\n        # sync expand table:\n        # phrase was de-selected from vocab list:\n        expand_selected = [\n            expand_table_source.data[""res""][p] for p in expand_table_source.selected.indices\n        ]\n        for o in old_selected:\n            full_o = cut_vocab_dict[o]\n            if o not in new_selected and full_o in expand_selected:\n                logger.info(""%s removed from vocab selected and exists in expand selected"", full_o)\n                logger.info(\n                    ""removing %s from expand selected indices. index=%s"",\n                    full_o,\n                    str(expand_table_source.data[""res""].index(full_o)),\n                )\n                logger.info(""current expand indices: %s"", str(expand_table_source.selected.indices))\n                expand_table_source.selected.indices.remove(\n                    expand_table_source.data[""res""].index(full_o)\n                )\n                logger.info(""new expand indices: %s"", str(expand_table_source.selected.indices))\n                break\n        # new phrase was selected from vocab list:\n        for n in new_selected:\n            full_n = cut_vocab_dict[n]\n            logger.info(""selected phrase="" + n + "", full phrase="" + full_n)\n            if (\n                n not in old_selected\n                and full_n in expand_table_source.data[""res""]\n                and full_n not in expand_selected\n            ):\n                expand_table_source.selected.indices.append(\n                    expand_table_source.data[""res""].index(full_n)\n                )\n                break\n        update_all_selected_phrases()\n        seed_input_box.value = get_selected_phrases_for_seed()\n\n\ndef clear_seed_callback():\n    logger.info(""clear"")\n    global all_selected_phrases, table_area, clear_flag\n    # table_area.children = []  # needed for refreshing the selections\n    clear_flag = True\n    seed_input_box.value = """"\n    seed_check_label.text = """"\n    expand_table_source.selected.indices = []\n    phrases_list.value = []\n    all_selected_phrases = []\n    table_area.children = [table_layout]\n    clear_flag = False\n\n\ndef get_selected_phrases_for_seed():\n    """"""\n     create the seed string to send to the server\n    """"""\n    global all_selected_phrases\n    phrases = """"\n    for x in all_selected_phrases:\n        phrases += x + "", ""\n    phrases = phrases[:-2]\n    return phrases\n\n\ndef expand_data_changed_callback(data, old, new):\n    """"""\n    remove the selected indices when table is empty\n    """"""\n    if old == empty_table:\n        expand_table_source.selected.indices = []\n\n\ndef annotate_callback():\n    try:\n        annotation_output.text = working_text\n        user_text = annotation_input.value\n        # if len(user_text) == 0 :\n        #    annotation_output.text = ""Please provaide valid text to annotate""\n        if len(seed_input_box.value) == 0:\n            out_text = ""No seed to compare to""\n        else:\n            out_text = user_text\n            seed = [x.strip() for x in seed_input_box.value.split("","")]\n            res = send_request_to_server([""annotate"", seed, user_text])\n            logger.info(""res:%s"", str(res))\n            if len(res) == 0:\n                out_text = ""No results found""\n            for np in res:\n                pattern = re.compile(r""\\b"" + np + r""\\b"")\n                out_text = re.sub(pattern, mark_phrase_tag(np), out_text)\n        annotation_output.text = out_text\n    except Exception as e:\n        annotation_output.text = ""An error occured""\n        logger.error(""Error: %s"", e)\n\n\ndef mark_phrase_tag(text):\n    return ""<phrase>"" + text + ""</phrase>""\n\n\n# set callbacks\n\nexpand_button.on_click(get_expand_results_callback)\nexpand_table_source.selected.on_change(""indices"", row_selected_callback)\nexpand_table_source.on_change(""data"", expand_data_changed_callback)\ncheckbox_group.on_click(checkbox_callback)\nsearch_input_box.on_change(""value"", search_callback)\nphrases_list.on_change(""value"", vocab_phrase_selected_callback)\nclear_seed_button.on_click(clear_seed_callback)\nwith open(join(dirname(__file__), ""download.js"")) as f:\n    code = f.read()\nexport_button.callback = CustomJS(args=dict(source=expand_table_source), code=code)\nannotate_button.on_click(annotate_callback)\n# table_area.on_change(\'children\', table_area_change_callback)\n\n# arrange components in page\n\ndoc = curdoc()\nmain_title = ""Set Expansion Demo""\ndoc.title = main_title\ndoc.add_root(grid)\n'"
solutions/set_expansion/ui/settings.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nexpand_host = ""localhost""\nexpand_port = 1234\ngrouping = False\n'"
solutions/trend_analysis/ui/__init__.py,0,b''
examples/sparse_gnmt/gnmt/utils/__init__.py,0,b''
examples/sparse_gnmt/gnmt/utils/bleu.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   no changes\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Python implementation of BLEU and smooth-BLEU.\n\nThis module provides a Python implementation of BLEU and smooth-BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\nChin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\nevaluation metrics for machine translation. COLING 2004.\n""""""\n\nimport collections\nimport math\n\n\ndef _get_ngrams(segment, max_order):\n    """"""Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  """"""\n    ngram_counts = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i : i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):\n    """"""Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  """"""\n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    reference_length = 0\n    translation_length = 0\n    for (references, translation) in zip(reference_corpus, translation_corpus):\n        reference_length += min(len(r) for r in references)\n        translation_length += len(translation)\n\n        merged_ref_ngram_counts = collections.Counter()\n        for reference in references:\n            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n        translation_ngram_counts = _get_ngrams(translation, max_order)\n        overlap = translation_ngram_counts & merged_ref_ngram_counts\n        for ngram in overlap:\n            matches_by_order[len(ngram) - 1] += overlap[ngram]\n        for order in range(1, max_order + 1):\n            possible_matches = len(translation) - order + 1\n            if possible_matches > 0:\n                possible_matches_by_order[order - 1] += possible_matches\n\n    precisions = [0] * max_order\n    for i in range(0, max_order):\n        if smooth:\n            precisions[i] = (matches_by_order[i] + 1.0) / (possible_matches_by_order[i] + 1.0)\n        else:\n            if possible_matches_by_order[i] > 0:\n                precisions[i] = float(matches_by_order[i]) / possible_matches_by_order[i]\n            else:\n                precisions[i] = 0.0\n\n    if min(precisions) > 0:\n        p_log_sum = sum((1.0 / max_order) * math.log(p) for p in precisions)\n        geo_mean = math.exp(p_log_sum)\n    else:\n        geo_mean = 0\n\n    ratio = float(translation_length) / reference_length\n\n    if ratio > 1.0:\n        bp = 1.0\n    else:\n        bp = math.exp(1 - 1.0 / ratio)\n\n    bleu = geo_mean * bp\n\n    return (bleu, precisions, bp, ratio, translation_length, reference_length)\n'"
examples/sparse_gnmt/gnmt/utils/evaluation_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ************************nlp_architect.models.gnmt******************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: skip-file\n""""""Utility for evaluating various tasks, e.g., translation & summarization.""""""\nimport codecs\nimport os\nimport re\nimport subprocess\n\nimport tensorflow as tf\n\nfrom . import rouge, bleu\n\n__all__ = [""evaluate""]\n\n\ndef evaluate(ref_file, trans_file, metric, subword_option=None):\n    """"""Pick a metric and evaluate depending on task.""""""\n    # BLEU scores for translation task\n    if metric.lower() == ""bleu"":\n        evaluation_score = _bleu(ref_file, trans_file, subword_option=subword_option)\n    # ROUGE scores for summarization tasks\n    elif metric.lower() == ""rouge"":\n        evaluation_score = _rouge(ref_file, trans_file, subword_option=subword_option)\n    elif metric.lower() == ""accuracy"":\n        evaluation_score = _accuracy(ref_file, trans_file)\n    elif metric.lower() == ""word_accuracy"":\n        evaluation_score = _word_accuracy(ref_file, trans_file)\n    else:\n        raise ValueError(""Unknown metric %s"" % metric)\n\n    return evaluation_score\n\n\ndef _clean(sentence, subword_option):\n    """"""Clean and handle BPE or SPM outputs.""""""\n    sentence = sentence.strip()\n\n    # BPE\n    if subword_option == ""bpe"":\n        sentence = re.sub(""@@ "", """", sentence)\n\n    # SPM\n    elif subword_option == ""spm"":\n        sentence = """".join(sentence.split()).replace(""\\u2581"", "" "").lstrip()\n\n    return sentence\n\n\n# Follow //transconsole/localization/machine_translation/metrics/bleu_calc.py\ndef _bleu(ref_file, trans_file, subword_option=None):\n    """"""Compute BLEU scores and handling BPE.""""""\n    max_order = 4\n    smooth = False\n\n    ref_files = [ref_file]\n    reference_text = []\n    for reference_filename in ref_files:\n        with codecs.getreader(""utf-8"")(tf.gfile.GFile(reference_filename, ""rb"")) as fh:\n            reference_text.append(fh.readlines())\n\n    per_segment_references = []\n    for references in zip(*reference_text):\n        reference_list = []\n        for reference in references:\n            reference = _clean(reference, subword_option)\n            reference_list.append(reference.split("" ""))\n        per_segment_references.append(reference_list)\n\n    translations = []\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(trans_file, ""rb"")) as fh:\n        for line in fh:\n            line = _clean(line, subword_option=None)\n            translations.append(line.split("" ""))\n\n    # bleu_score, precisions, bp, ratio, translation_length, reference_length\n    bleu_score, _, _, _, _, _ = bleu.compute_bleu(\n        per_segment_references, translations, max_order, smooth\n    )\n    return 100 * bleu_score\n\n\ndef _rouge(ref_file, summarization_file, subword_option=None):\n    """"""Compute ROUGE scores and handling BPE.""""""\n\n    references = []\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(ref_file, ""rb"")) as fh:\n        for line in fh:\n            references.append(_clean(line, subword_option))\n\n    hypotheses = []\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(summarization_file, ""rb"")) as fh:\n        for line in fh:\n            hypotheses.append(_clean(line, subword_option=None))\n\n    rouge_score_map = rouge.rouge(hypotheses, references)\n    return 100 * rouge_score_map[""rouge_l/f_score""]\n\n\ndef _accuracy(label_file, pred_file):\n    """"""Compute accuracy, each line contains a label.""""""\n\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(label_file, ""rb"")) as label_fh:\n        with codecs.getreader(""utf-8"")(tf.gfile.GFile(pred_file, ""rb"")) as pred_fh:\n            count = 0.0\n            match = 0.0\n            for label in label_fh:\n                label = label.strip()\n                pred = pred_fh.readline().strip()\n                if label == pred:\n                    match += 1\n                count += 1\n    return 100 * match / count\n\n\ndef _word_accuracy(label_file, pred_file):\n    """"""Compute accuracy on per word basis.""""""\n\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(label_file, ""r"")) as label_fh:\n        with codecs.getreader(""utf-8"")(tf.gfile.GFile(pred_file, ""r"")) as pred_fh:\n            total_acc, total_count = 0.0, 0.0\n            for sentence in label_fh:\n                labels = sentence.strip().split("" "")\n                preds = pred_fh.readline().strip().split("" "")\n                match = 0.0\n                for pos in range(min(len(labels), len(preds))):\n                    label = labels[pos]\n                    pred = preds[pos]\n                    if label == pred:\n                        match += 1\n                total_acc += 100 * match / max(len(labels), len(preds))\n                total_count += 1\n    return total_acc / total_count\n\n\ndef _moses_bleu(multi_bleu_script, tgt_test, trans_file, subword_option=None):\n    """"""Compute BLEU scores using Moses multi-bleu.perl script.""""""\n\n    # TODO(thangluong): perform rewrite using python\n    # BPE\n    if subword_option == ""bpe"":\n        debpe_tgt_test = tgt_test + "".debpe""\n        if not os.path.exists(debpe_tgt_test):\n            # TODO(thangluong): not use shell=True, can be a security hazard\n            cp_cmd = ""cp %s %s"" % (tgt_test, debpe_tgt_test)\n            subprocess.call(cp_cmd.split(), shell=False)\n            sed_cmd = ""sed s/@@ //g %s"" % (debpe_tgt_test)\n            subprocess.call(sed_cmd.split(), shell=False)\n        tgt_test = debpe_tgt_test\n    elif subword_option == ""spm"":\n        despm_tgt_test = tgt_test + "".despm""\n        if not os.path.exists(despm_tgt_test):\n            subprocess.call(""cp %s %s"" % (tgt_test, despm_tgt_test))\n            subprocess.call(""sed s/ //g %s"" % (despm_tgt_test))\n            subprocess.call(""sed s/^\\u2581/g %s"" % (despm_tgt_test))\n            subprocess.call(""sed s/\\u2581/ /g %s"" % (despm_tgt_test))\n        tgt_test = despm_tgt_test\n    cmd = ""%s %s < %s"" % (multi_bleu_script, tgt_test, trans_file)\n\n    # subprocess\n    # TODO(thangluong): not use shell=True, can be a security hazard\n    bleu_output = subprocess.check_output(cmd.split(), shell=False)\n\n    # extract BLEU score\n    m = re.search(""BLEU = (.+?),"", bleu_output)\n    bleu_score = float(m.group(1))\n\n    return bleu_score\n'"
examples/sparse_gnmt/gnmt/utils/iterator_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: skip-file\n""""""For loading data into NMT models.""""""\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nfrom . import vocab_utils\n\n__all__ = [""BatchedInput"", ""get_iterator"", ""get_infer_iterator""]\n\n\n# NOTE(ebrevdo): When we subclass this, instances\' __dict__ becomes empty.\nclass BatchedInput(\n    collections.namedtuple(\n        ""BatchedInput"",\n        (\n            ""initializer"",\n            ""source"",\n            ""target_input"",\n            ""target_output"",\n            ""source_sequence_length"",\n            ""target_sequence_length"",\n        ),\n    )\n):\n    pass\n\n\ndef get_infer_iterator(\n    src_dataset, src_vocab_table, batch_size, eos, src_max_len=None, use_char_encode=False\n):\n    if use_char_encode:\n        src_eos_id = vocab_utils.EOS_CHAR_ID\n    else:\n        src_eos_id = tf.cast(src_vocab_table.lookup(tf.constant(eos)), tf.int32)\n    src_dataset = src_dataset.map(lambda src: tf.string_split([src]).values)\n\n    if src_max_len:\n        src_dataset = src_dataset.map(lambda src: src[:src_max_len])\n\n    if use_char_encode:\n        # Convert the word strings to character ids\n        src_dataset = src_dataset.map(\n            lambda src: tf.reshape(vocab_utils.tokens_to_bytes(src), [-1])\n        )\n    else:\n        # Convert the word strings to ids\n        src_dataset = src_dataset.map(lambda src: tf.cast(src_vocab_table.lookup(src), tf.int32))\n\n    # Add in the word counts.\n    if use_char_encode:\n        src_dataset = src_dataset.map(\n            lambda src: (src, tf.to_int32(tf.size(src) / vocab_utils.DEFAULT_CHAR_MAXLEN))\n        )\n    else:\n        src_dataset = src_dataset.map(lambda src: (src, tf.size(src)))\n\n    def batching_func(x):\n        return x.padded_batch(\n            batch_size,\n            # The entry is the source line rows;\n            # this has unknown-length vectors.  The last entry is\n            # the source row size; this is a scalar.\n            padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])),  # src  # src_len\n            # Pad the source sequences with eos tokens.\n            # (Though notice we don\'t generally need to do this since\n            # later on we will be masking out calculations past the true sequence.\n            padding_values=(src_eos_id, 0),  # src\n        )  # src_len -- unused\n\n    batched_dataset = batching_func(src_dataset)\n    batched_iter = batched_dataset.make_initializable_iterator()\n    (src_ids, src_seq_len) = batched_iter.get_next()\n    return BatchedInput(\n        initializer=batched_iter.initializer,\n        source=src_ids,\n        target_input=None,\n        target_output=None,\n        source_sequence_length=src_seq_len,\n        target_sequence_length=None,\n    )\n\n\ndef get_iterator(\n    src_dataset,\n    tgt_dataset,\n    src_vocab_table,\n    tgt_vocab_table,\n    batch_size,\n    sos,\n    eos,\n    random_seed,\n    num_buckets,\n    src_max_len=None,\n    tgt_max_len=None,\n    num_parallel_calls=4,\n    output_buffer_size=None,\n    skip_count=None,\n    num_shards=1,\n    shard_index=0,\n    reshuffle_each_iteration=True,\n    use_char_encode=False,\n):\n    if not output_buffer_size:\n        output_buffer_size = batch_size * 1000\n\n    if use_char_encode:\n        src_eos_id = vocab_utils.EOS_CHAR_ID\n    else:\n        src_eos_id = tf.cast(src_vocab_table.lookup(tf.constant(eos)), tf.int32)\n\n    tgt_sos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(sos)), tf.int32)\n    tgt_eos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(eos)), tf.int32)\n\n    src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))\n\n    src_tgt_dataset = src_tgt_dataset.shard(num_shards, shard_index)\n    if skip_count is not None:\n        src_tgt_dataset = src_tgt_dataset.skip(skip_count)\n\n    src_tgt_dataset = src_tgt_dataset.shuffle(\n        output_buffer_size, random_seed, reshuffle_each_iteration\n    )\n\n    src_tgt_dataset = src_tgt_dataset.map(\n        lambda src, tgt: (tf.string_split([src]).values, tf.string_split([tgt]).values),\n        num_parallel_calls=num_parallel_calls,\n    ).prefetch(output_buffer_size)\n\n    # Filter zero length input sequences.\n    src_tgt_dataset = src_tgt_dataset.filter(\n        lambda src, tgt: tf.logical_and(tf.size(src) > 0, tf.size(tgt) > 0)\n    )\n\n    if src_max_len:\n        src_tgt_dataset = src_tgt_dataset.map(\n            lambda src, tgt: (src[:src_max_len], tgt), num_parallel_calls=num_parallel_calls\n        ).prefetch(output_buffer_size)\n    if tgt_max_len:\n        src_tgt_dataset = src_tgt_dataset.map(\n            lambda src, tgt: (src, tgt[:tgt_max_len]), num_parallel_calls=num_parallel_calls\n        ).prefetch(output_buffer_size)\n\n    # Convert the word strings to ids.  Word strings that are not in the\n    # vocab get the lookup table\'s default_value integer.\n    if use_char_encode:\n        src_tgt_dataset = src_tgt_dataset.map(\n            lambda src, tgt: (\n                tf.reshape(vocab_utils.tokens_to_bytes(src), [-1]),\n                tf.cast(tgt_vocab_table.lookup(tgt), tf.int32),\n            ),\n            num_parallel_calls=num_parallel_calls,\n        )\n    else:\n        src_tgt_dataset = src_tgt_dataset.map(\n            lambda src, tgt: (\n                tf.cast(src_vocab_table.lookup(src), tf.int32),\n                tf.cast(tgt_vocab_table.lookup(tgt), tf.int32),\n            ),\n            num_parallel_calls=num_parallel_calls,\n        )\n\n    src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size)\n    # Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.\n    src_tgt_dataset = src_tgt_dataset.map(\n        lambda src, tgt: (\n            src,\n            tf.concat(([tgt_sos_id], tgt), 0),\n            tf.concat((tgt, [tgt_eos_id]), 0),\n        ),\n        num_parallel_calls=num_parallel_calls,\n    ).prefetch(output_buffer_size)\n    # Add in sequence lengths.\n    if use_char_encode:\n        src_tgt_dataset = src_tgt_dataset.map(\n            lambda src, tgt_in, tgt_out: (\n                src,\n                tgt_in,\n                tgt_out,\n                tf.to_int32(tf.size(src) / vocab_utils.DEFAULT_CHAR_MAXLEN),\n                tf.size(tgt_in),\n            ),\n            num_parallel_calls=num_parallel_calls,\n        )\n    else:\n        src_tgt_dataset = src_tgt_dataset.map(\n            lambda src, tgt_in, tgt_out: (src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in)),\n            num_parallel_calls=num_parallel_calls,\n        )\n\n    src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size)\n\n    # Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)\n    def batching_func(x):\n        return x.padded_batch(\n            batch_size,\n            # The first three entries are the source and target line rows;\n            # these have unknown-length vectors.  The last two entries are\n            # the source and target row sizes; these are scalars.\n            padded_shapes=(\n                tf.TensorShape([None]),  # src\n                tf.TensorShape([None]),  # tgt_input\n                tf.TensorShape([None]),  # tgt_output\n                tf.TensorShape([]),  # src_len\n                tf.TensorShape([]),\n            ),  # tgt_len\n            # Pad the source and target sequences with eos tokens.\n            # (Though notice we don\'t generally need to do this since\n            # later on we will be masking out calculations past the true sequence.\n            padding_values=(\n                src_eos_id,  # src\n                tgt_eos_id,  # tgt_input\n                tgt_eos_id,  # tgt_output\n                0,  # src_len -- unused\n                0,\n            ),\n        )  # tgt_len -- unused\n\n    if num_buckets > 1:\n\n        def key_func(unused_1, unused_2, unused_3, src_len, tgt_len):\n            # Calculate bucket_width by maximum source sequence length.\n            # Pairs with length [0, bucket_width) go to bucket 0, length\n            # [bucket_width, 2 * bucket_width) go to bucket 1, etc.  Pairs with length\n            # over ((num_bucket-1) * bucket_width) words all go into the last bucket.\n            if src_max_len:\n                bucket_width = (src_max_len + num_buckets - 1) // num_buckets\n            else:\n                bucket_width = 10\n\n            # Bucket sentence pairs by the length of their source sentence and target\n            # sentence.\n            bucket_id = tf.maximum(src_len // bucket_width, tgt_len // bucket_width)\n            return tf.to_int64(tf.minimum(num_buckets, bucket_id))\n\n        def reduce_func(unused_key, windowed_data):\n            return batching_func(windowed_data)\n\n        batched_dataset = src_tgt_dataset.apply(\n            tf.contrib.data.group_by_window(\n                key_func=key_func, reduce_func=reduce_func, window_size=batch_size\n            )\n        )\n\n    else:\n        batched_dataset = batching_func(src_tgt_dataset)\n    batched_iter = batched_dataset.make_initializable_iterator()\n    (src_ids, tgt_input_ids, tgt_output_ids, src_seq_len, tgt_seq_len) = batched_iter.get_next()\n    return BatchedInput(\n        initializer=batched_iter.initializer,\n        source=src_ids,\n        target_input=tgt_input_ids,\n        target_output=tgt_output_ids,\n        source_sequence_length=src_seq_len,\n        target_sequence_length=tgt_seq_len,\n    )\n'"
examples/sparse_gnmt/gnmt/utils/misc_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: skip-file\n""""""Generally useful utility functions.""""""\nfrom __future__ import print_function\n\nimport codecs\nimport collections\nimport json\nimport math\nimport os\nimport sys\nimport time\nfrom distutils import version\n\nimport tensorflow as tf\n\n\ndef check_tensorflow_version():\n    min_tf_version = ""1.4.0-dev20171024""\n    if version.LooseVersion(tf.__version__) < version.LooseVersion(min_tf_version):\n        raise EnvironmentError(""Tensorflow version must >= %s"" % min_tf_version)\n\n\ndef safe_exp(value):\n    """"""Exponentiation with catching of overflow error.""""""\n    try:\n        ans = math.exp(value)\n    except OverflowError:\n        ans = float(""inf"")\n    return ans\n\n\ndef print_time(s, start_time):\n    """"""Take a start time, print elapsed duration, and return a new time.""""""\n    print(""%s, time %ds, %s."" % (s, (time.time() - start_time), time.ctime()))\n    sys.stdout.flush()\n    return time.time()\n\n\ndef print_out(s, f=None, new_line=True):\n    """"""Similar to print but with support to flush and output to a file.""""""\n    if isinstance(s, bytes):\n        s = s.decode(""utf-8"")\n\n    if f:\n        f.write(s.encode(""utf-8""))\n        if new_line:\n            f.write(b""\\n"")\n\n    # stdout\n    out_s = s.encode(""utf-8"")\n    if not isinstance(out_s, str):\n        out_s = out_s.decode(""utf-8"")\n    print(out_s, end="""", file=sys.stdout)\n\n    if new_line:\n        sys.stdout.write(""\\n"")\n    sys.stdout.flush()\n\n\ndef print_hparams(hparams, skip_patterns=None, header=None):\n    """"""Print hparams, can skip keys based on pattern.""""""\n    if header:\n        print_out(""%s"" % header)\n    values = hparams.values()\n    for key in sorted(values.keys()):\n        if not skip_patterns or all([skip_pattern not in key for skip_pattern in skip_patterns]):\n            print_out(""  %s=%s"" % (key, str(values[key])))\n\n\ndef load_hparams(model_dir):\n    """"""Load hparams from an existing model directory.""""""\n    hparams_file = os.path.join(model_dir, ""hparams"")\n    if tf.gfile.Exists(hparams_file):\n        print_out(""# Loading hparams from %s"" % hparams_file)\n        with codecs.getreader(""utf-8"")(tf.gfile.GFile(hparams_file, ""rb"")) as f:\n            try:\n                hparams_values = json.load(f)\n                hparams = tf.contrib.training.HParams(**hparams_values)\n            except ValueError:\n                print_out(""  can\'t load hparams file"")\n                return None\n        return hparams\n    else:\n        return None\n\n\ndef maybe_parse_standard_hparams(hparams, hparams_path):\n    """"""Override hparams values with existing standard hparams config.""""""\n    if hparams_path and tf.gfile.Exists(hparams_path):\n        print_out(""# Loading standard hparams from %s"" % hparams_path)\n        with codecs.getreader(""utf-8"")(tf.gfile.GFile(hparams_path, ""rb"")) as f:\n            hparams.parse_json(f.read())\n    return hparams\n\n\ndef save_hparams(out_dir, hparams):\n    """"""Save hparams.""""""\n    hparams_file = os.path.join(out_dir, ""hparams"")\n    print_out(""  saving hparams to %s"" % hparams_file)\n    with codecs.getwriter(""utf-8"")(tf.gfile.GFile(hparams_file, ""wb"")) as f:\n        f.write(hparams.to_json(indent=4, sort_keys=True))\n\n\ndef debug_tensor(s, msg=None, summarize=10):\n    """"""Print the shape and value of a tensor at test time. Return a new tensor.""""""\n    if not msg:\n        msg = s.name\n    return tf.Print(s, [tf.shape(s), s], msg + "" "", summarize=summarize)\n\n\ndef add_summary(summary_writer, global_step, tag, value):\n    """"""Add a new summary to the current summary_writer.\n  Useful to log things that are not part of the training graph, e.g., tag=BLEU.\n  """"""\n    summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n    summary_writer.add_summary(summary, global_step)\n\n\ndef get_config_proto(\n    log_device_placement=False, allow_soft_placement=True, num_intra_threads=0, num_inter_threads=0\n):\n    # GPU options:\n    # https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html\n    config_proto = tf.ConfigProto(\n        log_device_placement=log_device_placement, allow_soft_placement=allow_soft_placement\n    )\n    config_proto.gpu_options.allow_growth = True\n\n    # CPU threads options\n    if num_intra_threads:\n        config_proto.intra_op_parallelism_threads = num_intra_threads\n    if num_inter_threads:\n        config_proto.inter_op_parallelism_threads = num_inter_threads\n\n    return config_proto\n\n\ndef format_text(words):\n    """"""Convert a sequence words into sentence.""""""\n    if not hasattr(words, ""__len__"") and not isinstance(  # for numpy array\n        words, collections.Iterable\n    ):\n        words = [words]\n    return b"" "".join(words)\n\n\ndef format_bpe_text(symbols, delimiter=b""@@""):\n    """"""Convert a sequence of bpe words into sentence.""""""\n    words = []\n    word = b""""\n    if isinstance(symbols, str):\n        symbols = symbols.encode()\n    delimiter_len = len(delimiter)\n    for symbol in symbols:\n        if len(symbol) >= delimiter_len and symbol[-delimiter_len:] == delimiter:\n            word += symbol[:-delimiter_len]\n        else:  # end of a word\n            word += symbol\n            words.append(word)\n            word = b""""\n    return b"" "".join(words)\n\n\ndef format_spm_text(symbols):\n    """"""Decode a text in SPM (https://github.com/google/sentencepiece) format.""""""\n    return (\n        """".join(format_text(symbols).decode(""utf-8"").split())\n        .replace(""\\u2581"", "" "")\n        .strip()\n        .encode(""utf-8"")\n    )\n'"
examples/sparse_gnmt/gnmt/utils/nmt_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility functions specifically for NMT.""""""\nfrom __future__ import print_function\n\nimport codecs\nimport time\nimport numpy as np\nimport tensorflow as tf\n\nfrom . import misc_utils as utils, evaluation_utils\n\n__all__ = [""decode_and_evaluate"", ""get_translation""]\n\n\ndef decode_and_evaluate(\n    name,\n    model,\n    sess,\n    trans_file,\n    ref_file,\n    metrics,\n    subword_option,\n    beam_width,\n    tgt_eos,\n    num_translations_per_input=1,\n    decode=True,\n    infer_mode=""greedy"",\n):\n    """"""Decode a test set and compute a score according to the evaluation task.""""""\n    # Decode\n    if decode:\n        utils.print_out(""  decoding to output %s"" % trans_file)\n\n        start_time = time.time()\n        num_sentences = 0\n        with codecs.getwriter(""utf-8"")(tf.gfile.GFile(trans_file, mode=""wb"")) as trans_f:\n            trans_f.write("""")  # Write empty string to ensure file is created.\n\n            if infer_mode == ""greedy"":\n                num_translations_per_input = 1\n            elif infer_mode == ""beam_search"":\n                num_translations_per_input = min(num_translations_per_input, beam_width)\n\n            while True:\n                try:\n                    nmt_outputs, _ = model.decode(sess)\n                    if infer_mode != ""beam_search"":\n                        nmt_outputs = np.expand_dims(nmt_outputs, 0)\n\n                    batch_size = nmt_outputs.shape[1]\n                    num_sentences += batch_size\n\n                    for sent_id in range(batch_size):\n                        for beam_id in range(num_translations_per_input):\n                            translation = get_translation(\n                                nmt_outputs[beam_id],\n                                sent_id,\n                                tgt_eos=tgt_eos,\n                                subword_option=subword_option,\n                            )\n                            trans_f.write((translation + b""\\n"").decode(""utf-8""))\n                except tf.errors.OutOfRangeError:\n                    utils.print_time(\n                        ""  done, num sentences %d, num translations per input %d""\n                        % (num_sentences, num_translations_per_input),\n                        start_time,\n                    )\n                    break\n\n    # Evaluation\n    evaluation_scores = {}\n    if ref_file and tf.gfile.Exists(trans_file):\n        for metric in metrics:\n            score = evaluation_utils.evaluate(\n                ref_file, trans_file, metric, subword_option=subword_option\n            )\n            evaluation_scores[metric] = score\n            utils.print_out(""  %s %s: %.1f"" % (metric, name, score))\n\n    return evaluation_scores\n\n\ndef get_translation(nmt_outputs, sent_id, tgt_eos, subword_option):\n    """"""Given batch decoding outputs, select a sentence and turn to text.""""""\n    if tgt_eos:\n        tgt_eos = tgt_eos.encode(""utf-8"")\n    # Select a sentence\n    output = nmt_outputs[sent_id, :].tolist()\n\n    # If there is an eos symbol in outputs, cut them at that point.\n    if tgt_eos and tgt_eos in output:\n        output = output[: output.index(tgt_eos)]\n\n    if subword_option == ""bpe"":  # BPE\n        translation = utils.format_bpe_text(output)\n    elif subword_option == ""spm"":  # SPM\n        translation = utils.format_spm_text(output)\n    else:\n        translation = utils.format_text(output)\n\n    return translation\n'"
examples/sparse_gnmt/gnmt/utils/rouge.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   no changes\n# ******************************************************************************\n# pylint: skip-file\n""""""ROUGE metric implementation.\n\nCopy from tf_seq2seq/seq2seq/metrics/rouge.py.\nThis is a modified and slightly extended verison of\nhttps://github.com/miso-belica/sumy/blob/dev/sumy/evaluation/rouge.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport itertools\nimport numpy as np\n\n\ndef _get_ngrams(n, text):\n    """"""Calcualtes n-grams.\n\n  Args:\n    n: which n-grams to calculate\n    text: An array of tokens\n\n  Returns:\n    A set of n-grams\n  """"""\n    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i : i + n]))\n    return ngram_set\n\n\ndef _split_into_words(sentences):\n    """"""Splits multiple sentences into words and flattens the result""""""\n    return list(itertools.chain(*[_.split("" "") for _ in sentences]))\n\n\ndef _get_word_ngrams(n, sentences):\n    """"""Calculates word n-grams for multiple sentences.\n  """"""\n    assert len(sentences) > 0\n    assert n > 0\n\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)\n\n\ndef _len_lcs(x, y):\n    """"""\n  Returns the length of the Longest Common Subsequence between sequences x\n  and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns\n    integer: Length of LCS between x and y\n  """"""\n    table = _lcs(x, y)\n    n, m = len(x), len(y)\n    return table[n, m]\n\n\ndef _lcs(x, y):\n    """"""\n  Computes the length of the longest common subsequence (lcs) between two\n  strings. The implementation below uses a DP programming algorithm and runs\n  in O(nm) time where n = len(x) and m = len(y).\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: collection of words\n    y: collection of words\n\n  Returns:\n    Table of dictionary of coord and len lcs\n  """"""\n    n, m = len(x), len(y)\n    table = dict()\n    for i in range(n + 1):\n        for j in range(m + 1):\n            if i == 0 or j == 0:\n                table[i, j] = 0\n            elif x[i - 1] == y[j - 1]:\n                table[i, j] = table[i - 1, j - 1] + 1\n            else:\n                table[i, j] = max(table[i - 1, j], table[i, j - 1])\n    return table\n\n\ndef _recon_lcs(x, y):\n    """"""\n  Returns the Longest Subsequence between x and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns:\n    sequence: LCS of x and y\n  """"""\n    i, j = len(x), len(y)\n    table = _lcs(x, y)\n\n    def _recon(i, j):\n        """"""private recon calculation""""""\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n    return recon_tuple\n\n\ndef rouge_n(evaluated_sentences, reference_sentences, n=2):\n    """"""\n  Computes ROUGE-N of two text collections of sentences.\n  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n  papers/rouge-working-note-v1.3.1.pdf\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n    n: Size of ngram.  Defaults to 2.\n\n  Returns:\n    A tuple (f1, precision, recall) for ROUGE-N\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  """"""\n    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n        raise ValueError(""Collections must contain at least 1 sentence."")\n\n    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n    reference_count = len(reference_ngrams)\n    evaluated_count = len(evaluated_ngrams)\n\n    # Gets the overlapping ngrams between evaluated and reference\n    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n    overlapping_count = len(overlapping_ngrams)\n\n    # Handle edge case. This isn\'t mathematically correct, but it\'s good enough\n    if evaluated_count == 0:\n        precision = 0.0\n    else:\n        precision = overlapping_count / evaluated_count\n\n    if reference_count == 0:\n        recall = 0.0\n    else:\n        recall = overlapping_count / reference_count\n\n    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n\n    # return overlapping_count / reference_count\n    return f1_score, precision, recall\n\n\ndef _f_p_r_lcs(llcs, m, n):\n    """"""\n  Computes the LCS-based F-measure score\n  Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Args:\n    llcs: Length of LCS\n    m: number of words in reference summary\n    n: number of words in candidate summary\n\n  Returns:\n    Float. LCS-based F-measure score\n  """"""\n    r_lcs = llcs / m\n    p_lcs = llcs / n\n    beta = p_lcs / (r_lcs + 1e-12)\n    num = (1 + (beta ** 2)) * r_lcs * p_lcs\n    denom = r_lcs + ((beta ** 2) * p_lcs)\n    f_lcs = num / (denom + 1e-12)\n    return f_lcs, p_lcs, r_lcs\n\n\ndef rouge_l_sentence_level(evaluated_sentences, reference_sentences):\n    """"""\n  Computes ROUGE-L (sentence level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = LCS(X,Y)/m\n  P_lcs = LCS(X,Y)/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  X = reference summary\n  Y = Candidate summary\n  m = length of reference summary\n  n = length of candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  """"""\n    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n        raise ValueError(""Collections must contain at least 1 sentence."")\n    reference_words = _split_into_words(reference_sentences)\n    evaluated_words = _split_into_words(evaluated_sentences)\n    m = len(reference_words)\n    n = len(evaluated_words)\n    lcs = _len_lcs(evaluated_words, reference_words)\n    return _f_p_r_lcs(lcs, m, n)\n\n\ndef _union_lcs(evaluated_sentences, reference_sentence):\n    """"""\n  Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n  subsequence between reference sentence ri and candidate summary C. For example\n  if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and\n  c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is\n  ""w1 w2"" and the longest common subsequence of r_i and c2 is ""w1 w3 w5"". The\n  union longest common subsequence of r_i, c1, and c2 is ""w1 w2 w3 w5"" and\n  LCS_u(r_i, C) = 4/5.\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    float: LCS_u(r_i, C)\n\n  ValueError:\n    Raises exception if a param has len <= 0\n  """"""\n    if len(evaluated_sentences) <= 0:\n        raise ValueError(""Collections must contain at least 1 sentence."")\n\n    lcs_union = set()\n    reference_words = _split_into_words([reference_sentence])\n    combined_lcs_length = 0\n    for eval_s in evaluated_sentences:\n        evaluated_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(reference_words, evaluated_words))\n        combined_lcs_length += len(lcs)\n        lcs_union = lcs_union.union(lcs)\n\n    union_lcs_count = len(lcs_union)\n    union_lcs_value = union_lcs_count / combined_lcs_length\n    return union_lcs_value\n\n\ndef rouge_l_summary_level(evaluated_sentences, reference_sentences):\n    """"""\n  Computes ROUGE-L (summary level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n  P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  SUM(i,u) = SUM from i through u\n  u = number of sentences in reference summary\n  C = Candidate summary made up of v sentences\n  m = number of words in reference summary\n  n = number of words in candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  """"""\n    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n        raise ValueError(""Collections must contain at least 1 sentence."")\n\n    # total number of words in reference sentences\n    m = len(_split_into_words(reference_sentences))\n\n    # total number of words in evaluated sentences\n    n = len(_split_into_words(evaluated_sentences))\n\n    union_lcs_sum_across_all_references = 0\n    for ref_s in reference_sentences:\n        union_lcs_sum_across_all_references += _union_lcs(evaluated_sentences, ref_s)\n    return _f_p_r_lcs(union_lcs_sum_across_all_references, m, n)\n\n\ndef rouge(hypotheses, references):\n    """"""Calculates average rouge scores for a list of hypotheses and\n  references""""""\n\n    # Filter out hyps that are of 0 length\n    # hyps_and_refs = zip(hypotheses, references)\n    # hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]\n    # hypotheses, references = zip(*hyps_and_refs)\n\n    # Calculate ROUGE-1 F1, precision, recall scores\n    rouge_1 = [rouge_n([hyp], [ref], 1) for hyp, ref in zip(hypotheses, references)]\n    rouge_1_f, rouge_1_p, rouge_1_r = map(np.mean, zip(*rouge_1))\n\n    # Calculate ROUGE-2 F1, precision, recall scores\n    rouge_2 = [rouge_n([hyp], [ref], 2) for hyp, ref in zip(hypotheses, references)]\n    rouge_2_f, rouge_2_p, rouge_2_r = map(np.mean, zip(*rouge_2))\n\n    # Calculate ROUGE-L F1, precision, recall scores\n    rouge_l = [rouge_l_sentence_level([hyp], [ref]) for hyp, ref in zip(hypotheses, references)]\n    rouge_l_f, rouge_l_p, rouge_l_r = map(np.mean, zip(*rouge_l))\n\n    return {\n        ""rouge_1/f_score"": rouge_1_f,\n        ""rouge_1/r_score"": rouge_1_r,\n        ""rouge_1/p_score"": rouge_1_p,\n        ""rouge_2/f_score"": rouge_2_f,\n        ""rouge_2/r_score"": rouge_2_r,\n        ""rouge_2/p_score"": rouge_2_p,\n        ""rouge_l/f_score"": rouge_l_f,\n        ""rouge_l/r_score"": rouge_l_r,\n        ""rouge_l/p_score"": rouge_l_p,\n    }\n'"
examples/sparse_gnmt/gnmt/utils/standard_hparams_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""standard hparams utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef create_standard_hparams():\n    return tf.contrib.training.HParams(\n        # Data\n        src="""",\n        tgt="""",\n        train_prefix="""",\n        dev_prefix="""",\n        test_prefix="""",\n        vocab_prefix="""",\n        embed_prefix="""",\n        out_dir="""",\n        # Networks\n        num_units=512,\n        num_encoder_layers=2,\n        num_decoder_layers=2,\n        dropout=0.2,\n        unit_type=""lstm"",\n        encoder_type=""bi"",\n        residual=False,\n        time_major=True,\n        num_embeddings_partitions=0,\n        num_enc_emb_partitions=0,\n        num_dec_emb_partitions=0,\n        # Attention mechanisms\n        attention=""scaled_luong"",\n        attention_architecture=""standard"",\n        output_attention=True,\n        pass_hidden_state=True,\n        # Train\n        optimizer=""sgd"",\n        batch_size=128,\n        init_op=""uniform"",\n        init_weight=0.1,\n        max_gradient_norm=5.0,\n        learning_rate=1.0,\n        warmup_steps=0,\n        warmup_scheme=""t2t"",\n        decay_scheme=""luong234"",\n        colocate_gradients_with_ops=True,\n        num_train_steps=12000,\n        num_sampled_softmax=0,\n        # Data constraints\n        num_buckets=5,\n        max_train=0,\n        src_max_len=50,\n        tgt_max_len=50,\n        src_max_len_infer=0,\n        tgt_max_len_infer=0,\n        # Data format\n        sos=""<s>"",\n        eos=""</s>"",\n        subword_option="""",\n        use_char_encode=False,\n        check_special_token=True,\n        # Misc\n        forget_bias=1.0,\n        num_gpus=1,\n        epoch_step=0,  # record where we were within an epoch.\n        steps_per_stats=100,\n        steps_per_external_eval=0,\n        share_vocab=False,\n        metrics=[""bleu""],\n        log_device_placement=False,\n        random_seed=None,\n        # only enable beam search during inference when beam_width > 0.\n        beam_width=0,\n        length_penalty_weight=0.0,\n        override_loaded_hparams=True,\n        num_keep_ckpts=5,\n        avg_ckpts=False,\n        # For inference\n        inference_indices=None,\n        infer_batch_size=32,\n        sampling_temperature=0.0,\n        num_translations_per_input=1,\n        infer_mode=""greedy"",\n        # Language model\n        language_model=False,\n    )\n'"
examples/sparse_gnmt/gnmt/utils/vocab_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Changes Made from original:\n#   import paths\n# ******************************************************************************\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility to handle vocabularies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport os\nimport tensorflow as tf\n\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.ops import lookup_ops\n\nfrom . import misc_utils as utils\n\n# word level special token\nUNK = ""<unk>""\nSOS = ""<s>""\nEOS = ""</s>""\nUNK_ID = 0\n\n# char ids 0-255 come from utf-8 encoding bytes\n# assign 256-300 to special chars\nBOS_CHAR_ID = 256  # <begin sentence>\nEOS_CHAR_ID = 257  # <end sentence>\nBOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>\n\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\n\n\ndef _string_to_bytes(text, max_length):\n    """"""Given string and length, convert to byte seq of at most max_length.\n\n  This process mimics docqa/elmo\'s preprocessing:\n  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py\n\n  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py &\n  our usage differs from docqa/elmo.\n\n  Args:\n    text: tf.string tensor of shape []\n    max_length: max number of chars for each word.\n\n  Returns:\n    A tf.int32 tensor of the byte encoded text.\n  """"""\n    byte_ids = tf.to_int32(tf.decode_raw(text, tf.uint8))\n    byte_ids = byte_ids[: max_length - 2]\n    padding = tf.fill([max_length - tf.shape(byte_ids)[0] - 2], PAD_CHAR_ID)\n    byte_ids = tf.concat([[BOW_CHAR_ID], byte_ids, [EOW_CHAR_ID], padding], axis=0)\n    tf.logging.info(byte_ids)\n\n    byte_ids = tf.reshape(byte_ids, [max_length])\n    tf.logging.info(byte_ids.get_shape().as_list())\n    return byte_ids + 1\n\n\ndef tokens_to_bytes(tokens):\n    """"""Given a sequence of strings, map to sequence of bytes.\n\n  Args:\n    tokens: A tf.string tensor\n\n  Returns:\n    A tensor of shape words.shape + [bytes_per_word] containing byte versions\n    of each word.\n  """"""\n    bytes_per_word = DEFAULT_CHAR_MAXLEN\n    with tf.device(""/cpu:0""):\n        tf.assert_rank(tokens, 1)\n        shape = tf.shape(tokens)\n        tf.logging.info(tokens)\n        tokens_flat = tf.reshape(tokens, [-1])\n        as_bytes_flat = tf.map_fn(\n            fn=lambda x: _string_to_bytes(x, max_length=bytes_per_word),\n            elems=tokens_flat,\n            dtype=tf.int32,\n            back_prop=False,\n        )\n        tf.logging.info(as_bytes_flat)\n        as_bytes = tf.reshape(as_bytes_flat, [shape[0], bytes_per_word])\n    return as_bytes\n\n\ndef load_vocab(vocab_file):\n    vocab = []\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(vocab_file, ""rb"")) as f:\n        vocab_size = 0\n        for word in f:\n            vocab_size += 1\n            vocab.append(word.strip())\n    return vocab, vocab_size\n\n\ndef check_vocab(vocab_file, out_dir, check_special_token=True, sos=None, eos=None, unk=None):\n    """"""Check if vocab_file doesn\'t exist, create from corpus_file.""""""\n    if tf.gfile.Exists(vocab_file):\n        utils.print_out(""# Vocab file %s exists"" % vocab_file)\n        vocab, vocab_size = load_vocab(vocab_file)\n        if check_special_token:\n            # Verify if the vocab starts with unk, sos, eos\n            # If not, prepend those tokens & generate a new vocab file\n            if not unk:\n                unk = UNK\n            if not sos:\n                sos = SOS\n            if not eos:\n                eos = EOS\n            assert len(vocab) >= 3\n            if vocab[0] != unk or vocab[1] != sos or vocab[2] != eos:\n                utils.print_out(\n                    ""The first 3 vocab words [%s, %s, %s]""\n                    "" are not [%s, %s, %s]"" % (vocab[0], vocab[1], vocab[2], unk, sos, eos)\n                )\n                vocab = [unk, sos, eos] + vocab\n                vocab_size += 3\n                new_vocab_file = os.path.join(out_dir, os.path.basename(vocab_file))\n                with codecs.getwriter(""utf-8"")(tf.gfile.GFile(new_vocab_file, ""wb"")) as f:\n                    for word in vocab:\n                        f.write(""%s\\n"" % word)\n                vocab_file = new_vocab_file\n    else:\n        raise ValueError(""vocab_file \'%s\' does not exist."" % vocab_file)\n\n    vocab_size = len(vocab)\n    return vocab_size, vocab_file\n\n\ndef create_vocab_tables(src_vocab_file, tgt_vocab_file, share_vocab):\n    """"""Creates vocab tables for src_vocab_file and tgt_vocab_file.""""""\n    src_vocab_table = lookup_ops.index_table_from_file(src_vocab_file, default_value=UNK_ID)\n    if share_vocab:\n        tgt_vocab_table = src_vocab_table\n    else:\n        tgt_vocab_table = lookup_ops.index_table_from_file(tgt_vocab_file, default_value=UNK_ID)\n    return src_vocab_table, tgt_vocab_table\n\n\ndef load_embed_txt(embed_file):\n    """"""Load embed_file into a python dictionary.\n\n  Note: the embed_file should be a Glove/word2vec formatted txt file. Assuming\n  Here is an exampe assuming embed_size=5:\n\n  the -0.071549 0.093459 0.023738 -0.090339 0.056123\n  to 0.57346 0.5417 -0.23477 -0.3624 0.4037\n  and 0.20327 0.47348 0.050877 0.002103 0.060547\n\n  For word2vec format, the first line will be: <num_words> <emb_size>.\n\n  Args:\n    embed_file: file path to the embedding file.\n  Returns:\n    a dictionary that maps word to vector, and the size of embedding dimensions.\n  """"""\n    emb_dict = dict()\n    emb_size = None\n\n    is_first_line = True\n    with codecs.getreader(""utf-8"")(tf.gfile.GFile(embed_file, ""rb"")) as f:\n        for line in f:\n            tokens = line.rstrip().split("" "")\n            if is_first_line:\n                is_first_line = False\n                if len(tokens) == 2:  # header line\n                    emb_size = int(tokens[1])\n                    continue\n            word = tokens[0]\n            vec = list(map(float, tokens[1:]))\n            emb_dict[word] = vec\n            if emb_size:\n                if emb_size != len(vec):\n                    utils.print_out(""Ignoring %s since embeding size is inconsistent."" % word)\n                    del emb_dict[word]\n            else:\n                emb_size = len(vec)\n    return emb_dict, emb_size\n'"
nlp_architect/data/cdc_resources/data_types/__init__.py,0,b''
nlp_architect/data/cdc_resources/embedding/__init__.py,0,b''
nlp_architect/data/cdc_resources/embedding/embed_elmo.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport pickle\nfrom typing import List\n\nimport numpy as np\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.utils.embedding import ELMoEmbedderTFHUB\n\nlogger = logging.getLogger(__name__)\n\n\nclass ElmoEmbedding(object):\n    def __init__(self):\n        logger.info(""Loading Elmo Embedding module"")\n        self.embeder = ELMoEmbedderTFHUB()\n        self.cache = dict()\n        logger.info(""Elmo Embedding module lead successfully"")\n\n    def get_head_feature_vector(self, mention: MentionDataLight):\n        if mention.mention_context is not None and mention.mention_context:\n            sentence = "" "".join(mention.mention_context)\n            return self.apply_get_from_cache(sentence, True, mention.tokens_number)\n\n        sentence = mention.tokens_str\n        return self.apply_get_from_cache(sentence, False, [])\n\n    def apply_get_from_cache(self, sentence: str, context: bool = False, indexs: List[int] = None):\n        if context and indexs is not None:\n            if sentence in self.cache:\n                elmo_full_vec = self.cache[sentence]\n            else:\n                elmo_full_vec = self.embeder.get_vector(sentence.split())\n                self.cache[sentence] = elmo_full_vec\n\n            elmo_ret_vec = self.get_mention_vec_from_sent(elmo_full_vec, indexs)\n        else:\n            if sentence in self.cache:\n                elmo_ret_vec = self.cache[sentence]\n            else:\n                elmo_ret_vec = self.get_elmo_avg(sentence.split())\n                self.cache[sentence] = elmo_ret_vec\n\n        return elmo_ret_vec\n\n    def get_avrg_feature_vector(self, tokens_str):\n        if tokens_str is not None:\n            return self.apply_get_from_cache(tokens_str)\n        return None\n\n    def get_elmo_avg(self, sentence):\n        sentence_embedding = self.embeder.get_vector(sentence)\n        return np.mean(sentence_embedding, axis=0)\n\n    @staticmethod\n    def get_mention_vec_from_sent(sent_vec, indexs):\n        if len(indexs) > 1:\n            elmo_ret_vec = np.mean(sent_vec[indexs[0] : indexs[-1] + 1], axis=0)\n        else:\n            elmo_ret_vec = sent_vec[indexs[0]]\n\n        return elmo_ret_vec\n\n\nclass ElmoEmbeddingOffline(object):\n    def __init__(self, dump_file):\n        logger.info(""Loading Elmo Offline Embedding module"")\n\n        if dump_file is not None:\n            with open(dump_file, ""rb"") as out:\n                self.embeder = pickle.load(out)\n        else:\n            logger.warning(""Elmo Offline without loaded embeder!"")\n\n        logger.info(""Elmo Offline Embedding module lead successfully"")\n\n    def get_head_feature_vector(self, mention: MentionDataLight):\n        embed = None\n        if mention.mention_context is not None and mention.mention_context:\n            sentence = "" "".join(mention.mention_context)\n            if sentence in self.embeder:\n                elmo_full_vec = self.embeder[sentence]\n                return ElmoEmbedding.get_mention_vec_from_sent(elmo_full_vec, mention.tokens_number)\n\n        sentence = mention.tokens_str\n        if sentence in self.embeder:\n            embed = self.embeder[sentence]\n\n        return embed\n\n    def get_avrg_feature_vector(self, tokens_str):\n        embed = None\n        if tokens_str in self.embeder:\n            embed = self.embeder[tokens_str]\n\n        return embed\n'"
nlp_architect/data/cdc_resources/embedding/embed_glove.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport pickle\n\nimport numpy as np\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\n\nlogger = logging.getLogger(__name__)\n\n\nclass GloveEmbedding(object):\n    def __init__(self, glove_file):\n        logger.info(""Loading Glove Online Embedding module, This my take a while..."")\n        self.word_to_ix, self.word_embeddings = self.load_glove_for_vocab(glove_file)\n        logger.info(""Glove Offline Embedding module lead successfully"")\n\n    @staticmethod\n    def load_glove_for_vocab(glove_filename):\n        vocab = []\n        embd = []\n        with open(glove_filename) as glove_file:\n            for line in glove_file:\n                row = line.strip().split("" "")\n                word = row[0]\n                vocab.append(word)\n                embd.append(row[1:])\n\n        embeddings = np.asarray(embd, dtype=float)\n        word_to_ix = {word: i for i, word in enumerate(vocab)}\n        return word_to_ix, embeddings\n\n\nclass GloveEmbeddingOffline(object):\n    def __init__(self, embed_resources):\n        logger.info(""Loading Glove Offline Embedding module"")\n        with open(embed_resources, ""rb"") as out:\n            self.word_to_ix, self.word_embeddings = pickle.load(out, encoding=""latin1"")\n        logger.info(""Glove Offline Embedding module lead successfully"")\n\n    def get_feature_vector(self, mention: MentionDataLight):\n        embed = None\n        head = mention.mention_head\n        lemma = mention.mention_head_lemma\n        if head in self.word_to_ix:\n            embed = self.word_embeddings[self.word_to_ix[head]]\n        elif lemma in self.word_to_ix:\n            embed = self.word_embeddings[self.word_to_ix[lemma]]\n\n        return embed\n\n    def get_avrg_feature_vector(self, tokens_str):\n        embed = np.zeros(300, dtype=np.float64)\n        mention_size = 0\n        for token in tokens_str.split():\n            if token in self.word_to_ix:\n                token_embed = self.word_embeddings[self.word_to_ix[token]]\n                embed = np.add(embed, token_embed)\n                mention_size += 1\n\n        if mention_size == 0:\n            mention_size = 1\n\n        return np.true_divide(embed, mention_size)\n'"
nlp_architect/data/cdc_resources/gen_scripts/__init__.py,0,b''
nlp_architect/data/cdc_resources/gen_scripts/create_reference_dict_dump.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport json\nimport logging\n\nfrom nlp_architect.data.cdc_resources.relations.referent_dict_relation_extraction import (\n    ReferentDictRelationExtraction,\n)\nfrom nlp_architect.models.cross_doc_coref.system.cdc_utils import load_mentions_vocab_from_files\nfrom nlp_architect.utils import io\n\nlogger = logging.getLogger(__name__)\n\nparser = argparse.ArgumentParser(description=""Create Referent dictionary dataset only dump"")\n\nparser.add_argument(""--ref_dict"", type=str, help=""referent dictionary file"", required=True)\n\nparser.add_argument(""--mentions"", type=str, help=""dataset mentions"", required=True)\n\nparser.add_argument(""--output"", type=str, help=""location were to create dump file"", required=True)\n\nargs = parser.parse_args()\n\n\ndef ref_dict_dump():\n    logger.info(""Extracting referent dict dump, this may take a while..."")\n    ref_dict_file = args.ref_dict\n    out_file = args.output\n    mentions_entity_gold_file = [args.mentions]\n    vocab = load_mentions_vocab_from_files(mentions_entity_gold_file, True)\n\n    ref_dict = ReferentDictRelationExtraction.load_reference_dict(ref_dict_file)\n\n    ref_dict_for_vocab = {}\n    for word in vocab:\n        if word in ref_dict:\n            ref_dict_for_vocab[word] = ref_dict[word]\n\n    logger.info(""Found %d words from vocabulary"", len(ref_dict_for_vocab.keys()))\n    logger.info(""Preparing to save refDict output file"")\n    with open(out_file, ""w"") as f:\n        json.dump(ref_dict_for_vocab, f)\n    logger.info(""Done saved to-%s"", out_file)\n\n\nif __name__ == ""__main__"":\n    io.validate_existing_filepath(args.mentions)\n    io.validate_existing_filepath(args.output)\n    io.validate_existing_filepath(args.ref_dict)\n    ref_dict_dump()\n'"
nlp_architect/data/cdc_resources/gen_scripts/create_verbocean_dump.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport json\nimport logging\n\nfrom nlp_architect.data.cdc_resources.relations.verbocean_relation_extraction import (\n    VerboceanRelationExtraction,\n)\nfrom nlp_architect.models.cross_doc_coref.system.cdc_utils import load_mentions_vocab_from_files\nfrom nlp_architect.utils import io\n\nlogger = logging.getLogger(__name__)\n\nparser = argparse.ArgumentParser(description=""Create Verb-Ocean dataset only dump"")\n\nparser.add_argument(""--vo"", type=str, help=""Verb Ocean file"", required=True)\n\nparser.add_argument(""--mentions"", type=str, help=""dataset mentions"", required=True)\n\nparser.add_argument(""--output"", type=str, help=""location were to create dump file"", required=True)\n\nargs = parser.parse_args()\n\n\ndef vo_dump():\n    vo_file = args.vo\n    out_file = args.output\n    mentions_event_gold_file = [args.mentions]\n    vocab = load_mentions_vocab_from_files(mentions_event_gold_file, True)\n    vo = VerboceanRelationExtraction.load_verbocean_file(vo_file)\n    vo_for_vocab = {}\n    for word in vocab:\n        if word in vo:\n            vo_for_vocab[word] = vo[word]\n\n    logger.info(""Found %d words from vocabulary"", len(vo_for_vocab.keys()))\n    logger.info(""Preparing to save refDict output file"")\n\n    with open(out_file, ""w"") as f:\n        json.dump(vo_for_vocab, f)\n    logger.info(""Done saved to-%s"", out_file)\n\n\nif __name__ == ""__main__"":\n    io.validate_existing_filepath(args.mentions)\n    io.validate_existing_filepath(args.output)\n    io.validate_existing_filepath(args.vo)\n    vo_dump()\n'"
nlp_architect/data/cdc_resources/gen_scripts/create_wiki_dump.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport json\nimport logging\n\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import WikipediaSearchMethod\nfrom nlp_architect.data.cdc_resources.relations.wikipedia_relation_extraction import (\n    WikipediaRelationExtraction,\n)\nfrom nlp_architect.models.cross_doc_coref.system.cdc_utils import load_mentions_vocab_from_files\nfrom nlp_architect.utils import io\nfrom nlp_architect.utils.io import json_dumper\n\nlogger = logging.getLogger(__name__)\n\nresult_dump = {}\n\nparser = argparse.ArgumentParser(description=""Create Wikipedia dataset only dump"")\nparser.add_argument(""--mentions"", type=str, help=""mentions_file file"", required=True)\nparser.add_argument(""--host"", type=str, help=""elastic host"")\nparser.add_argument(""--port"", type=int, help=""elastic port"")\nparser.add_argument(""--index"", type=str, help=""elastic index"")\nparser.add_argument(""--output"", type=str, help=""location were to create dump file"", required=True)\n\nargs = parser.parse_args()\n\n\ndef wiki_dump_from_gs():\n    logger.info(""Starting, process will connect with ElasticSearch and online wikipedia site..."")\n    mentions_files = [args.mentions]\n    dump_file = args.output\n    vocab = load_mentions_vocab_from_files(mentions_files)\n\n    if args.host and args.port and args.index:\n        wiki_elastic = WikipediaRelationExtraction(\n            WikipediaSearchMethod.ELASTIC, host=args.host, port=args.port, index=args.index\n        )\n    else:\n        logger.info(\n            ""Running without Wikipedia elastic search, Note that this will ""\n            ""take much longer to process only using online service""\n        )\n        wiki_elastic = None\n\n    wiki_online = WikipediaRelationExtraction(WikipediaSearchMethod.ONLINE)\n\n    for phrase in vocab:\n        phrase = phrase.replace(""\'"", """").replace(\'""\', """").replace(""\\\\"", """").strip()\n        logger.info(""Try to retrieve \'%s\' from elastic search"", phrase)\n        pages = None\n        if wiki_elastic:\n            pages = wiki_elastic.get_phrase_related_pages(phrase)\n        if not pages or not pages.get_pages() or len(pages.get_pages()) == 0:\n            logger.info(""Not on elastic, retrieve \'%s\' from wiki online site"", phrase)\n            pages = wiki_online.get_phrase_related_pages(phrase)\n        for search_page in pages.get_pages():\n            add_page(search_page, phrase)\n\n    with open(dump_file, ""w"") as myfile:\n        json.dump(result_dump, myfile, default=json_dumper)\n\n    logger.info(""Saving dump to file-%s"", dump_file)\n\n\ndef add_page(search_page, phrase):\n    try:\n        if search_page is not None:\n            if phrase not in result_dump:\n                result_dump[phrase] = []\n                result_dump[phrase].append(search_page)\n            else:\n                pages = result_dump[phrase]\n                for page in pages:\n                    if page.pageid == search_page.pageid:\n                        return\n\n                result_dump[phrase].append(search_page)\n\n            logger.info(""page-%s added"", str(search_page))\n    except Exception:\n        logger.error(""could not extract wiki info from phrase-%s"", search_page.orig_phrase)\n\n\nif __name__ == ""__main__"":\n    io.validate_existing_filepath(args.mentions)\n    io.validate_existing_filepath(args.output)\n    if args.host:\n        io.validate((args.host, str, 1, 1000))\n    if args.port:\n        io.validate((args.port, int, 1, 65536))\n    if args.index:\n        io.validate((args.index, str, 1, 10000))\n\n    wiki_dump_from_gs()\n'"
nlp_architect/data/cdc_resources/gen_scripts/create_word_embed_elmo_dump.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport logging\nimport os\nimport pickle\nfrom os.path import join\n\nfrom nlp_architect.common.cdc.mention_data import MentionData\nfrom nlp_architect.data.cdc_resources.embedding.embed_elmo import ElmoEmbedding\nfrom nlp_architect.utils import io\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_elmo_for_vocab(mentions):\n    """"""\n    Create the embedding using the cache logic in the embedding class\n    Args:\n        mentions:\n\n    Returns:\n\n    """"""\n    elmo_embeddings = ElmoEmbedding()\n\n    for mention in mentions:\n        elmo_embeddings.get_head_feature_vector(mention)\n\n    logger.info(""Total words/contexts in vocabulary %d"", len(elmo_embeddings.cache))\n    return elmo_embeddings.cache\n\n\ndef elmo_dump():\n    out_file = args.output\n    mention_files = list()\n    if os.path.isdir(args.mentions):\n        for (dirpath, _, files) in os.walk(args.mentions):\n            for file in files:\n                if file == "".DS_Store"":\n                    continue\n\n                mention_files.append(join(dirpath, file))\n    else:\n        mention_files.append(args.mentions)\n\n    mentions = []\n    for _file in mention_files:\n        mentions.extend(MentionData.read_mentions_json_to_mentions_data_list(_file))\n\n    elmo_ecb_embeddings = load_elmo_for_vocab(mentions)\n\n    with open(out_file, ""wb"") as f:\n        pickle.dump(elmo_ecb_embeddings, f)\n\n    logger.info(""Saving dump to file-%s"", out_file)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Create Elmo Embedding dataset only dump"")\n    parser.add_argument(""--mentions"", type=str, help=""mentions_file file"", required=True)\n    parser.add_argument(\n        ""--output"", type=str, help=""location were to create dump file"", required=True\n    )\n\n    args = parser.parse_args()\n\n    if os.path.isdir(args.mentions):\n        io.validate_existing_directory(args.mentions)\n    else:\n        io.validate_existing_filepath(args.mentions)\n\n    elmo_dump()\n    print(""Done!"")\n'"
nlp_architect/data/cdc_resources/gen_scripts/create_word_embed_glove_dump.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport logging\nimport pickle\n\nimport numpy as np\n\nfrom nlp_architect.models.cross_doc_coref.system.cdc_utils import load_mentions_vocab_from_files\nfrom nlp_architect.utils import io\n\nlogger = logging.getLogger(__name__)\n\nparser = argparse.ArgumentParser(description=""Create GloVe dataset only dump"")\n\nparser.add_argument(""--glove"", type=str, help=""glove db file"", required=True)\n\nparser.add_argument(""--mentions"", type=str, help=""mentions file"", required=True)\n\nparser.add_argument(""--output"", type=str, help=""location were to create dump file"", required=True)\n\nargs = parser.parse_args()\n\n\ndef load_glove_for_vocab(glove_filename, vocabulary):\n    vocab = []\n    embd = []\n    with open(glove_filename) as glove_file:\n        for line in glove_file:\n            row = line.strip().split("" "")\n            word = row[0]\n            if word in vocabulary:\n                vocab.append(word)\n                embd.append(row[1:])\n    logger.info(""Loaded GloVe!"")\n\n    embeddings = np.asarray(embd, dtype=float)\n    word_to_ix = {word: i for i, word in enumerate(vocab)}\n    return word_to_ix, embeddings\n\n\ndef glove_dump():\n    filter_stop_words = False\n    glove_file = args.glove\n    out_file = args.output\n    mention_files = [args.mentions]\n    vocab = load_mentions_vocab_from_files(mention_files, filter_stop_words)\n    word_to_ix, embeddings = load_glove_for_vocab(glove_file, vocab)\n\n    logger.info(""Words in vocabulary %d"", len(vocab))\n    logger.info(""Found %d words from vocabulary"", len(word_to_ix.keys()))\n    with open(out_file, ""wb"") as f:\n        pickle.dump([word_to_ix, embeddings], f)\n    logger.info(""Saving dump to file-%s"", out_file)\n\n\nif __name__ == ""__main__"":\n    io.validate_existing_filepath(args.mentions)\n    io.validate_existing_filepath(args.output)\n    io.validate_existing_filepath(args.glove)\n    glove_dump()\n'"
nlp_architect/data/cdc_resources/gen_scripts/create_wordnet_dump.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport argparse\nimport json\nimport logging\n\nfrom nlp_architect.common.cdc.mention_data import MentionData\nfrom nlp_architect.data.cdc_resources.wordnet.wordnet_online import WordnetOnline\nfrom nlp_architect.utils import io\nfrom nlp_architect.utils.io import json_dumper\n\nlogger = logging.getLogger(__name__)\n\nparser = argparse.ArgumentParser(description=""Create WordNet dataset only dump"")\n\nparser.add_argument(""--mentions"", type=str, help=""mentions file"", required=True)\n\nparser.add_argument(""--output"", type=str, help=""location were to create dump file"", required=True)\n\nargs = parser.parse_args()\n\n\ndef wordnet_dump():\n    out_file = args.output\n    mentions_file = args.mentions\n    logger.info(""Loading mentions files..."")\n    mentions = MentionData.read_mentions_json_to_mentions_data_list(mentions_file)\n    logger.info(""Done loading mentions files, starting local dump creation..."")\n    result_dump = dict()\n    wordnet = WordnetOnline()\n    for mention in mentions:\n        page = wordnet.get_pages(mention)\n        result_dump[page.orig_phrase] = page\n\n    with open(out_file, ""w"") as out:\n        json.dump(result_dump, out, default=json_dumper)\n\n    logger.info(\n        ""Wordnet Dump Created Successfully, "" ""extracted total of %d wn pages"", len(result_dump)\n    )\n    logger.info(""Saving dump to file-%s"", out_file)\n\n\nif __name__ == ""__main__"":\n    io.validate_existing_filepath(args.mentions)\n    io.validate_existing_filepath(args.output)\n    wordnet_dump()\n'"
nlp_architect/data/cdc_resources/relations/__init__.py,0,b''
nlp_architect/data/cdc_resources/relations/computed_relation_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport difflib\nimport logging\nimport sys\nfrom typing import List, Set\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import RelationType\nfrom nlp_architect.utils.string_utils import StringUtils\n\nlogger = logging.getLogger(__name__)\n\n\nclass ComputedRelationExtraction(RelationExtraction):\n    """"""\n    Extract Relation between two mentions according to computation and rule based algorithms\n    """"""\n\n    def extract_all_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> Set[RelationType]:\n        """"""\n        Try to find if mentions has anyone or more of the relations this class support\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            Set[RelationType]: One or more of: RelationType.EXACT_STRING, RelationType.FUZZY_FIT,\n                RelationType.FUZZY_HEAD_FIT, RelationType.SAME_HEAD_LEMMA,\n                RelationType.SAME_HEAD_LEMMA_RELAX\n        """"""\n        relations = set()\n        mention_x_str = mention_x.tokens_str\n        mention_y_str = mention_y.tokens_str\n\n        if StringUtils.is_pronoun(mention_x_str.lower()) or StringUtils.is_pronoun(\n            mention_y_str.lower()\n        ):\n            relations.add(RelationType.NO_RELATION_FOUND)\n            return relations\n\n        exact_rel = self.extract_exact_string(mention_x, mention_y)\n        fuzzy_rel = self.extract_fuzzy_fit(mention_x, mention_y)\n        fuzzy_head_rel = self.extract_fuzzy_head_fit(mention_x, mention_y)\n        same_head_rel = self.extract_same_head_lemma(mention_x, mention_y)\n        if exact_rel != RelationType.NO_RELATION_FOUND:\n            relations.add(exact_rel)\n        if fuzzy_rel != RelationType.NO_RELATION_FOUND:\n            relations.add(fuzzy_rel)\n        if fuzzy_head_rel != RelationType.NO_RELATION_FOUND:\n            relations.add(fuzzy_head_rel)\n        if same_head_rel != RelationType.NO_RELATION_FOUND:\n            relations.add(same_head_rel)\n\n        if len(relations) == 0:\n            relations.add(RelationType.NO_RELATION_FOUND)\n\n        return relations\n\n    def extract_sub_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Check if input mentions has the given relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation or\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        mention_x_str = mention_x.tokens_str\n        mention_y_str = mention_y.tokens_str\n\n        if StringUtils.is_pronoun(mention_x_str.lower()) or StringUtils.is_pronoun(\n            mention_y_str.lower()\n        ):\n            return RelationType.NO_RELATION_FOUND\n\n        if relation == RelationType.EXACT_STRING:\n            return self.extract_exact_string(mention_x, mention_y)\n        if relation == RelationType.FUZZY_FIT:\n            return self.extract_fuzzy_fit(mention_x, mention_y)\n        if relation == RelationType.FUZZY_HEAD_FIT:\n            return self.extract_fuzzy_head_fit(mention_x, mention_y)\n        if relation == RelationType.SAME_HEAD_LEMMA:\n            is_same_lemma = self.extract_same_head_lemma(mention_x, mention_y)\n            if is_same_lemma != RelationType.NO_RELATION_FOUND:\n                return relation\n\n        return RelationType.NO_RELATION_FOUND\n\n    @staticmethod\n    def extract_same_head_lemma(\n        mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> RelationType:\n        """"""\n        Check if input mentions has same head lemma relation\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            RelationType.SAME_HEAD_LEMMA or RelationType.NO_RELATION_FOUND\n        """"""\n        if (\n            StringUtils.is_preposition(mention_x.mention_head_lemma)\n            or StringUtils.is_preposition(mention_y.mention_head_lemma)\n            or StringUtils.is_determiner(mention_x.mention_head_lemma)\n            or StringUtils.is_determiner(mention_y.mention_head_lemma)\n        ):\n            return RelationType.NO_RELATION_FOUND\n        if mention_x.mention_head_lemma == mention_y.mention_head_lemma:\n            return RelationType.SAME_HEAD_LEMMA\n        return RelationType.NO_RELATION_FOUND\n\n    @staticmethod\n    def extract_fuzzy_head_fit(\n        mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> RelationType:\n        """"""\n        Check if input mentions has fuzzy head fit relation\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            RelationType.FUZZY_HEAD_FIT or RelationType.NO_RELATION_FOUND\n        """"""\n        if StringUtils.is_preposition(\n            mention_x.mention_head_lemma.lower()\n        ) or StringUtils.is_preposition(mention_y.mention_head_lemma.lower()):\n            return RelationType.NO_RELATION_FOUND\n\n        mention_y_tokens = mention_y.tokens_str.split()\n        mention_x_tokens = mention_x.tokens_str.split()\n        if mention_x.mention_head in mention_y_tokens or mention_y.mention_head in mention_x_tokens:\n            return RelationType.FUZZY_HEAD_FIT\n        return RelationType.NO_RELATION_FOUND\n\n    @staticmethod\n    def extract_fuzzy_fit(mention_x: MentionDataLight, mention_y: MentionDataLight) -> RelationType:\n        """"""\n        Check if input mentions has fuzzy fit relation\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            RelationType.FUZZY_FIT or RelationType.NO_RELATION_FOUND\n        """"""\n        try:\n            from num2words import num2words\n        except (AttributeError, ImportError):\n            logger.error(\n                ""num2words is not installed, please install nlp_architect with [all] package. ""\n                + ""for example: pip install nlp_architect[all]""\n            )\n            sys.exit()\n\n        relation = RelationType.NO_RELATION_FOUND\n        mention1_str = mention_x.tokens_str\n        mention2_str = mention_y.tokens_str\n        if difflib.SequenceMatcher(None, mention1_str, mention2_str).ratio() * 100 >= 90:\n            relation = RelationType.FUZZY_FIT\n            return relation\n\n        # Convert numbers to words\n        x_words = [\n            num2words(int(w)).replace(""-"", "" "") if w.isdigit() else w for w in mention1_str.split()\n        ]\n        y_words = [\n            num2words(int(w)).replace(""-"", "" "") if w.isdigit() else w for w in mention2_str.split()\n        ]\n\n        fuzzy_result = (\n            difflib.SequenceMatcher(None, "" "".join(x_words), "" "".join(y_words)).ratio() * 100 >= 85\n        )\n        if fuzzy_result:\n            relation = RelationType.FUZZY_FIT\n        return relation\n\n    @staticmethod\n    def extract_exact_string(\n        mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> RelationType:\n        """"""\n        Check if input mentions has exact string relation\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            RelationType.EXACT_STRING or RelationType.NO_RELATION_FOUND\n        """"""\n        relation = RelationType.NO_RELATION_FOUND\n        mention1_str = mention_x.tokens_str\n        mention2_str = mention_y.tokens_str\n        if StringUtils.is_preposition(mention1_str.lower()) or StringUtils.is_preposition(\n            mention2_str.lower()\n        ):\n            return relation\n\n        if mention1_str.lower() == mention2_str.lower():\n            relation = RelationType.EXACT_STRING\n\n        return relation\n\n    @staticmethod\n    def get_supported_relations() -> List[RelationType]:\n        """"""\n        Return all supported relations by this class\n\n        Returns:\n            List[RelationType]\n        """"""\n        return [\n            RelationType.EXACT_STRING,\n            RelationType.FUZZY_FIT,\n            RelationType.FUZZY_HEAD_FIT,\n            RelationType.SAME_HEAD_LEMMA,\n        ]\n'"
nlp_architect/data/cdc_resources/relations/referent_dict_relation_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport os\nfrom typing import Dict, List, Set\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import (\n    OnlineOROfflineMethod,\n    RelationType,\n)\nfrom nlp_architect.utils.io import load_json_file\nfrom nlp_architect.utils.string_utils import StringUtils\n\nlogger = logging.getLogger(__name__)\n\n\nclass ReferentDictRelationExtraction(RelationExtraction):\n    def __init__(\n        self, method: OnlineOROfflineMethod = OnlineOROfflineMethod.ONLINE, ref_dict: str = None\n    ):\n        """"""\n        Extract Relation between two mentions according to Referent Dictionary knowledge\n\n        Args:\n            method (optional): OnlineOROfflineMethod.{ONLINE/OFFLINE} run against full referent\n                dictionary or a sub-set of (default = ONLINE)\n            ref_dict (required): str Location of referent dictionary file to work with\n        """"""\n        logger.info(""Loading ReferentDict module"")\n        if ref_dict is not None and os.path.isfile(ref_dict):\n            if method == OnlineOROfflineMethod.OFFLINE:\n                self.ref_dict = load_json_file(ref_dict)\n            elif method == OnlineOROfflineMethod.ONLINE:\n                self.ref_dict = self.load_reference_dict(ref_dict)\n            logger.info(""ReferentDict module lead successfully"")\n        else:\n            raise FileNotFoundError(""Referent Dict file not found or not in path:"" + ref_dict)\n\n        super(ReferentDictRelationExtraction, self).__init__()\n\n    def extract_all_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> Set[RelationType]:\n        ret_ = set()\n        ret_.add(self.extract_sub_relations(mention_x, mention_y, RelationType.REFERENT_DICT))\n        return ret_\n\n    def extract_sub_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Check if input mentions has the given relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation or\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        if relation is not RelationType.REFERENT_DICT:\n            return RelationType.NO_RELATION_FOUND\n\n        mention_x_str = mention_x.tokens_str\n        mention_y_str = mention_y.tokens_str\n        if StringUtils.is_pronoun(mention_x_str.lower()) or StringUtils.is_pronoun(\n            mention_y_str.lower()\n        ):\n            return RelationType.NO_RELATION_FOUND\n\n        if self.is_referent_dict(mention_x, mention_y):\n            return RelationType.REFERENT_DICT\n\n        return RelationType.NO_RELATION_FOUND\n\n    def is_referent_dict(self, mention_x: MentionDataLight, mention_y: MentionDataLight) -> bool:\n        """"""\n        Check if input mentions has referent dictionary relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            bool\n        """"""\n        match_result = False\n        x_head = mention_x.mention_head\n        y_head = mention_y.mention_head\n        x_head_lemma = mention_x.mention_head_lemma\n        y_head_lemma = mention_y.mention_head_lemma\n\n        if (x_head in self.ref_dict and y_head in self.ref_dict[x_head]) or (\n            y_head in self.ref_dict and x_head in self.ref_dict[y_head]\n        ):\n            match_result = True\n        if (x_head_lemma in self.ref_dict and y_head_lemma in self.ref_dict[x_head_lemma]) or (\n            y_head_lemma in self.ref_dict and x_head_lemma in self.ref_dict[y_head_lemma]\n        ):\n            match_result = True\n        if (x_head_lemma in self.ref_dict and y_head in self.ref_dict[x_head_lemma]) or (\n            y_head_lemma in self.ref_dict and x_head in self.ref_dict[y_head_lemma]\n        ):\n            match_result = True\n        if (y_head in self.ref_dict and x_head_lemma in self.ref_dict[y_head]) or (\n            x_head in self.ref_dict and y_head_lemma in self.ref_dict[x_head]\n        ):\n            match_result = True\n\n        return match_result\n\n    @staticmethod\n    def get_supported_relations():\n        """"""\n        Return all supported relations by this class\n\n        Returns:\n            List[RelationType]\n        """"""\n        return [RelationType.REFERENT_DICT]\n\n    @staticmethod\n    def load_reference_dict(dict_fname: str) -> Dict[str, List[str]]:\n        """"""\n        Method to load referent dictionary to memory\n\n        Returns:\n            List[RelationType]\n        """"""\n        word_dict = {}\n        first = True\n        with open(dict_fname, ""r"", encoding=""utf-8"") as f:\n            for line in f:\n                if first:\n                    first = False\n                    continue\n                word1, word2, _, npmi = line.strip().split(""\\t"")\n                npmi = float(npmi)\n                if npmi >= 0.2:\n                    if word1 not in word_dict:\n                        word_dict[word1] = []\n                    word_dict[word1].append(word2)\n        return word_dict\n'"
nlp_architect/data/cdc_resources/relations/relation_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom typing import List\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import RelationType\n\n\nclass RelationExtraction(object):\n    def __init__(self):\n        pass\n\n    def extract_relation(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Base Class Check if Sub class support given relation before executing the sub class\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation and\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        ret_relation = RelationType.NO_RELATION_FOUND\n        if relation in self.get_supported_relations():\n            ret_relation = self.extract_sub_relations(mention_x, mention_y, relation)\n        return ret_relation\n\n    def extract_sub_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        raise NotImplementedError\n\n    @staticmethod\n    def get_supported_relations() -> List[RelationType]:\n        raise NotImplementedError\n'"
nlp_architect/data/cdc_resources/relations/relation_types_enums.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom enum import Enum\n\n\nclass EmbeddingMethod(Enum):\n    GLOVE = ""glove""\n    GLOVE_OFFLINE = ""glove_offline""\n    ELMO = ""elmo""\n    ELMO_OFFLINE = ""elmo_offline""\n\n\nclass WikipediaSearchMethod(Enum):\n    ONLINE = ""online""\n    OFFLINE = ""offline""\n    ELASTIC = ""elastic""\n\n\nclass OnlineOROfflineMethod(Enum):\n    ONLINE = ""online""\n    OFFLINE = ""offline""\n\n\nclass RelationType(Enum):\n    NO_RELATION_FOUND = 0\n    WIKIPEDIA_REDIRECT_LINK = 1\n    WIKIPEDIA_ALIASES = 2\n    WIKIPEDIA_DISAMBIGUATION = 3\n    WIKIPEDIA_PART_OF_SAME_NAME = 4\n    WIKIPEDIA_CATEGORY = 5\n    WIKIPEDIA_TITLE_PARENTHESIS = 6\n    WIKIPEDIA_BE_COMP = 7\n    EXACT_STRING = 8\n    FUZZY_FIT = 9\n    FUZZY_HEAD_FIT = 10\n    SAME_HEAD_LEMMA = 11\n    VERBOCEAN_MATCH = 13\n    WORDNET_DERIVATIONALLY = 14\n    WORDNET_PARTIAL_SYNSET_MATCH = 15\n    WORDNET_SAME_SYNSET = 17\n    REFERENT_DICT = 18\n    WORD_EMBEDDING_MATCH = 19\n    WITHIN_DOC_COREF = 20\n    OTHER = 21\n'"
nlp_architect/data/cdc_resources/relations/verbocean_relation_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport os\nfrom typing import Dict, Set\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import (\n    RelationType,\n    OnlineOROfflineMethod,\n)\nfrom nlp_architect.utils.io import load_json_file\nfrom nlp_architect.utils.string_utils import StringUtils\n\nlogger = logging.getLogger(__name__)\n\n\nclass VerboceanRelationExtraction(RelationExtraction):\n    def __init__(\n        self, method: OnlineOROfflineMethod = OnlineOROfflineMethod.ONLINE, vo_file: str = None\n    ):\n        """"""\n        Extract Relation between two mentions according to VerbOcean knowledge\n\n        Args:\n            method (optional): OnlineOROfflineMethod.{ONLINE/OFFLINE} run against full VerbOcean or\n                a sub-set of it (default = ONLINE)\n            vo_file (required): str Location of VerbOcean file to work with\n        """"""\n        logger.info(""Loading Verb Ocean module"")\n        if vo_file is not None and os.path.isfile(vo_file):\n            if method == OnlineOROfflineMethod.OFFLINE:\n                self.vo = load_json_file(vo_file)\n            elif method == OnlineOROfflineMethod.ONLINE:\n                self.vo = self.load_verbocean_file(vo_file)\n            logger.info(""Verb Ocean module lead successfully"")\n        else:\n            raise FileNotFoundError(""VerbOcean file not found or not in path.."")\n        super(VerboceanRelationExtraction, self).__init__()\n\n    def extract_all_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> Set[RelationType]:\n        ret_ = set()\n        ret_.add(self.extract_sub_relations(mention_x, mention_y, RelationType.VERBOCEAN_MATCH))\n        return ret_\n\n    def extract_sub_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Check if input mentions has the given relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation or\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        if relation is not RelationType.VERBOCEAN_MATCH:\n            return RelationType.NO_RELATION_FOUND\n\n        mention_x_str = mention_x.tokens_str\n        mention_y_str = mention_y.tokens_str\n        if StringUtils.is_pronoun(mention_x_str.lower()) or StringUtils.is_pronoun(\n            mention_y_str.lower()\n        ):\n            return RelationType.NO_RELATION_FOUND\n\n        if self.is_verbocean_relation(mention_x, mention_y):\n            return RelationType.VERBOCEAN_MATCH\n\n        return RelationType.NO_RELATION_FOUND\n\n    def is_verbocean_relation(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> bool:\n        """"""\n        Check if input mentions has VerbOcean relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            bool\n        """"""\n        x_head = mention_x.mention_head\n\n        y_head = mention_y.mention_head\n\n        rel = None\n\n        if x_head in self.vo and y_head in self.vo[x_head]:\n            rel = self.vo[x_head][y_head]\n        elif y_head in self.vo and x_head in self.vo[y_head]:\n            rel = self.vo[y_head][x_head]\n\n        match_result = False\n        if rel is not None and rel != ""[unk]"" and rel != ""[low-vol]"":\n            match_result = True\n\n        return match_result\n\n    @staticmethod\n    def get_supported_relations():\n        """"""\n        Return all supported relations by this class\n\n        Returns:\n            List[RelationType]\n        """"""\n        return [RelationType.VERBOCEAN_MATCH]\n\n    @staticmethod\n    def load_verbocean_file(fname: str) -> Dict[str, Dict[str, str]]:\n        """"""\n        Method to load referent dictionary to memory\n\n        Returns:\n            List[RelationType]\n        """"""\n        word_dict = {}\n        with open(fname) as f:\n            for line in f:\n                word1, rel, word2, _, _ = line.strip().split()\n                if word1 not in word_dict:\n                    word_dict[word1] = {}\n                word_dict[word1][word2] = rel\n        return word_dict\n'"
nlp_architect/data/cdc_resources/relations/wikipedia_relation_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import division\n\nimport logging\nimport os\nfrom typing import Set, List\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_pages import WikipediaPages\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import (\n    RelationType,\n    WikipediaSearchMethod,\n)\nfrom nlp_architect.data.cdc_resources.wikipedia.wiki_elastic import WikiElastic\nfrom nlp_architect.data.cdc_resources.wikipedia.wiki_offline import WikiOffline\nfrom nlp_architect.data.cdc_resources.wikipedia.wiki_online import WikiOnline\nfrom nlp_architect.utils.string_utils import StringUtils\n\nlogger = logging.getLogger(__name__)\n\n\nclass WikipediaRelationExtraction(RelationExtraction):\n    def __init__(\n        self,\n        method: WikipediaSearchMethod = WikipediaSearchMethod.ONLINE,\n        wiki_file: str = None,\n        host: str = None,\n        port: int = None,\n        index: str = None,\n        filter_pronouns: bool = True,\n        filter_time_data: bool = True,\n    ) -> None:\n        """"""\n        Extract Relation between two mentions according to Wikipedia knowledge\n\n        Args:\n            method (optional): WikipediaSearchMethod.{ONLINE/OFFLINE/ELASTIC} run against wiki\n                site a sub-set of wiki or on a local elastic database (default = ONLINE)\n            wiki_file (required on OFFLINE mode): str Location of Wikipedia file to work with\n            host (required on Elastic mode): str the Elastic search host name\n            port (required on Elastic mode): int the Elastic search port number\n            index (required on Elastic mode): int the Elastic search index name\n        """"""\n        logger.info(""Loading Wikipedia module"")\n        self.filter_pronouns = filter_pronouns\n        self.filter_time_data = filter_time_data\n        connectivity = method\n        if connectivity == WikipediaSearchMethod.ONLINE:\n            self.pywiki_impl = WikiOnline()\n        elif connectivity == WikipediaSearchMethod.OFFLINE:\n            if wiki_file is not None and os.path.isdir(wiki_file):\n                self.pywiki_impl = WikiOffline(wiki_file)\n            else:\n                raise FileNotFoundError(\n                    ""Wikipedia resource file not found or not in path, ""\n                    ""create it or change the initialization method""\n                )\n        elif connectivity == WikipediaSearchMethod.ELASTIC:\n            self.pywiki_impl = WikiElastic(host, port, index)\n\n        logger.info(""Wikipedia module lead successfully"")\n        super(WikipediaRelationExtraction, self).__init__()\n\n    def get_phrase_related_pages(self, mention_str: str) -> WikipediaPages:\n        """"""\n        Get all WikipediaPages pages related with this mention string\n\n        Args:\n            mention_str: str\n\n        Returns:\n            WikipediaPages\n        """"""\n        pages = self.pywiki_impl.get_pages(mention_str.strip())\n        ret_pages = WikipediaPages()\n        if pages:\n            for search_page in pages:\n                if search_page.page_result.pageid != 0:\n                    ret_pages.add_page(search_page.page_result)\n\n        return ret_pages\n\n    def extract_all_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> Set[RelationType]:\n        """"""\n        Try to find if mentions has anyone or more of the relations this class support\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            Set[RelationType]: One or more of: RelationType.WIKIPEDIA_BE_COMP,\n                RelationType.WIKIPEDIA_TITLE_PARENTHESIS,\n                RelationType.WIKIPEDIA_DISAMBIGUATION, RelationType.WIKIPEDIA_CATEGORY,\n                RelationType.WIKIPEDIA_REDIRECT_LINK, RelationType.WIKIPEDIA_ALIASES,\n                RelationType.WIKIPEDIA_PART_OF_SAME_NAME\n        """"""\n        relations = set()\n        mention1_str = mention_x.tokens_str.strip()\n        mention2_str = mention_y.tokens_str.strip()\n\n        if self.filter_pronouns:\n            if self.is_both_opposite_personal_pronouns(mention1_str, mention2_str):\n                relations.add(RelationType.NO_RELATION_FOUND)\n                return relations\n\n        if self.filter_time_data:\n            if self.is_both_data_or_time(mention_x, mention_y):\n                relations.add(RelationType.NO_RELATION_FOUND)\n                return relations\n\n        pages1 = self.get_phrase_related_pages(mention1_str)\n        pages2 = self.get_phrase_related_pages(mention2_str)\n\n        # check if search phrase is empty meaning it is probably a stop word\n        if pages1.is_empty_norm_phrase or pages2.is_empty_norm_phrase:\n            relations.add(RelationType.NO_RELATION_FOUND)\n            return relations\n\n        if self.is_redirect_same(pages1, pages2):\n            relations.add(RelationType.WIKIPEDIA_REDIRECT_LINK)\n\n        titles1 = pages1.get_and_set_titles()\n        titles1.add(mention1_str + "" "" + mention2_str)\n        titles1.add(mention2_str + "" "" + mention1_str)\n\n        titles2 = pages2.get_and_set_titles()\n        titles2.add(mention1_str + "" "" + mention2_str)\n        titles2.add(mention2_str + "" "" + mention1_str)\n\n        relation_alias = self.extract_aliases(pages1, pages2, titles1, titles2)\n        if relation_alias is not RelationType.NO_RELATION_FOUND:\n            relations.add(relation_alias)\n        relation_dis = self.extract_disambig(pages1, pages2, titles1, titles2)\n        if relation_dis is not RelationType.NO_RELATION_FOUND:\n            relations.add(relation_dis)\n        relation_cat = self.extract_category(pages1, pages2, titles1, titles2)\n        if relation_cat is not RelationType.NO_RELATION_FOUND:\n            relations.add(relation_cat)\n        relation_par = self.extract_parenthesis(pages1, pages2, titles1, titles2)\n        if relation_par is not RelationType.NO_RELATION_FOUND:\n            relations.add(relation_par)\n        relation_be = self.extract_be_comp(pages1, pages2, titles1, titles2)\n        if relation_be is not RelationType.NO_RELATION_FOUND:\n            relations.add(relation_be)\n\n        if len(relations) == 0:\n            relations.add(RelationType.NO_RELATION_FOUND)\n\n        return relations\n\n    def extract_sub_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Check if input mentions has the given relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation or\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        mention1_str = mention_x.tokens_str.strip()\n        mention2_str = mention_y.tokens_str.strip()\n\n        if self.filter_pronouns:\n            if self.is_both_opposite_personal_pronouns(mention1_str, mention2_str):\n                return RelationType.NO_RELATION_FOUND\n\n        if self.filter_time_data:\n            if self.is_both_data_or_time(mention_x, mention_y):\n                return RelationType.NO_RELATION_FOUND\n\n        pages1 = self.get_phrase_related_pages(mention1_str)\n        pages2 = self.get_phrase_related_pages(mention2_str)\n\n        # check if search phrase is empty meaning it is probably a stop word\n        if pages1.is_empty_norm_phrase or pages2.is_empty_norm_phrase:\n            return RelationType.NO_RELATION_FOUND\n\n        if relation == RelationType.WIKIPEDIA_REDIRECT_LINK:\n            if self.is_redirect_same(pages1, pages2):\n                return RelationType.WIKIPEDIA_REDIRECT_LINK\n\n            return RelationType.NO_RELATION_FOUND\n\n        titles1 = pages1.get_and_set_titles()\n        titles1.add(mention1_str + "" "" + mention2_str)\n        titles1.add(mention2_str + "" "" + mention1_str)\n\n        titles2 = pages2.get_and_set_titles()\n        titles2.add(mention1_str + "" "" + mention2_str)\n        titles2.add(mention2_str + "" "" + mention1_str)\n\n        if relation == RelationType.WIKIPEDIA_ALIASES:\n            return self.extract_aliases(pages1, pages2, titles1, titles2)\n        if relation == RelationType.WIKIPEDIA_DISAMBIGUATION:\n            return self.extract_disambig(pages1, pages2, titles1, titles2)\n        if relation == RelationType.WIKIPEDIA_CATEGORY:\n            return self.extract_category(pages1, pages2, titles1, titles2)\n        if relation == RelationType.WIKIPEDIA_TITLE_PARENTHESIS:\n            return self.extract_parenthesis(pages1, pages2, titles1, titles2)\n        if relation == RelationType.WIKIPEDIA_BE_COMP:\n            return self.extract_be_comp(pages1, pages2, titles1, titles2)\n\n        return RelationType.NO_RELATION_FOUND\n\n    @staticmethod\n    def extract_be_comp(\n        pages1: WikipediaPages, pages2: WikipediaPages, titles1: Set[str], titles2: Set[str]\n    ) -> RelationType:\n        """"""\n        Check if input mentions has be-comp/is-a relation\n\n        Args:\n            pages1: WikipediaPages\n            pages2: WikipediaPage\n            titles1: Set[str]\n            titles2: Set[str]\n\n        Returns:\n            RelationType.WIKIPEDIA_BE_COMP or RelationType.NO_RELATION_FOUND\n        """"""\n        relation = RelationType.NO_RELATION_FOUND\n        if bool(pages1.get_and_set_be_comp() & titles2):\n            relation = RelationType.WIKIPEDIA_BE_COMP\n        elif bool(pages2.get_and_set_be_comp() & titles1):\n            relation = RelationType.WIKIPEDIA_BE_COMP\n        return relation\n\n    @staticmethod\n    def extract_parenthesis(\n        pages1: WikipediaPages, pages2: WikipediaPages, titles1: Set[str], titles2: Set[str]\n    ) -> RelationType:\n        """"""\n        Check if input mentions has parenthesis relation\n\n        Args:\n            pages1: WikipediaPages\n            pages2: WikipediaPage\n            titles1: Set[str]\n            titles2: Set[str]\n\n        Returns:\n            RelationType.WIKIPEDIA_TITLE_PARENTHESIS or RelationType.NO_RELATION_FOUND\n        """"""\n        relation = RelationType.NO_RELATION_FOUND\n        if bool(pages1.get_and_set_parenthesis() & titles2):\n            relation = RelationType.WIKIPEDIA_TITLE_PARENTHESIS\n        elif bool(pages2.get_and_set_parenthesis() & titles1):\n            relation = RelationType.WIKIPEDIA_TITLE_PARENTHESIS\n        return relation\n\n    @staticmethod\n    def extract_category(\n        pages1: WikipediaPages, pages2: WikipediaPages, titles1: Set[str], titles2: Set[str]\n    ) -> RelationType:\n        """"""\n        Check if input mentions has category relation\n\n        Args:\n            pages1: WikipediaPages\n            pages2: WikipediaPage\n            titles1: Set[str]\n            titles2: Set[str]\n\n        Returns:\n            RelationType.WIKIPEDIA_CATEGORY or RelationType.NO_RELATION_FOUND\n        """"""\n        relation = RelationType.NO_RELATION_FOUND\n        if bool(pages1.get_and_set_all_categories() & titles2):\n            relation = RelationType.WIKIPEDIA_CATEGORY\n        elif bool(pages2.get_and_set_all_categories() & titles1):\n            relation = RelationType.WIKIPEDIA_CATEGORY\n        return relation\n\n    @staticmethod\n    def extract_disambig(\n        pages1: WikipediaPages, pages2: WikipediaPages, titles1: Set[str], titles2: Set[str]\n    ) -> RelationType:\n        """"""\n        Check if input mentions has disambiguation relation\n\n        Args:\n            pages1: WikipediaPages\n            pages2: WikipediaPage\n            titles1: Set[str]\n            titles2: Set[str]\n\n        Returns:\n            RelationType.WIKIPEDIA_DISAMBIGUATION or RelationType.NO_RELATION_FOUND\n        """"""\n        relation = RelationType.NO_RELATION_FOUND\n        if bool(pages1.get_and_set_all_disambiguation() & titles2):\n            relation = RelationType.WIKIPEDIA_DISAMBIGUATION\n        elif bool(pages2.get_and_set_all_disambiguation() & titles1):\n            relation = RelationType.WIKIPEDIA_DISAMBIGUATION\n        return relation\n\n    @staticmethod\n    def extract_aliases(\n        pages1: WikipediaPages, pages2: WikipediaPages, titles1: Set[str], titles2: Set[str]\n    ) -> RelationType:\n        """"""\n        Check if input mentions has aliases relation\n\n        Args:\n            pages1: WikipediaPages\n            pages2: WikipediaPage\n            titles1: Set[str]\n            titles2: Set[str]\n        Returns:\n            RelationType.WIKIPEDIA_ALIASES or RelationType.NO_RELATION_FOUND\n        """"""\n        relation = RelationType.NO_RELATION_FOUND\n        if bool(pages1.get_and_set_all_aliases() & titles2):\n            relation = RelationType.WIKIPEDIA_ALIASES\n        elif bool(pages2.get_and_set_all_aliases() & titles1):\n            relation = RelationType.WIKIPEDIA_ALIASES\n        return relation\n\n    def is_part_of_same_name(self, pages1: WikipediaPages, pages2: WikipediaPages) -> bool:\n        """"""\n        Check if input mentions has part of same name relation (eg: page1=John, page2=Smith)\n\n        Args:\n            pages1: WikipediaPages\n            pages2: WikipediaPage\n\n        Returns:\n            bool\n        """"""\n        for page1 in pages1.pages:\n            for page2 in pages2.pages:\n                if page1.relations.is_part_name and page2.relations.is_part_name:\n                    pages = self.pywiki_impl.get_pages(page1.orig_phrase + "" "" + page2.orig_phrase)\n                    for page in pages:\n                        if page.page_result.pageid != 0:\n                            return True\n        return False\n\n    @staticmethod\n    def is_redirect_same(pages1: WikipediaPages, pages2: WikipediaPages) -> bool:\n        """"""\n        Check if input mentions has same wikipedia redirect page\n\n        Args:\n            pages1: WikipediaPages\n            pages2: WikipediaPage\n\n        Returns:\n            bool\n        """"""\n        for page1 in pages1.get_pages():\n            for page2 in pages2.get_pages():\n                if page1.pageid > 0 and page2.pageid > 0:\n                    if page1.pageid == page2.pageid:\n                        return True\n        return False\n\n    @staticmethod\n    def get_supported_relations() -> List[RelationType]:\n        """"""\n        Return all supported relations by this class\n\n        Returns:\n            List[RelationType]\n        """"""\n        return [\n            RelationType.WIKIPEDIA_BE_COMP,\n            RelationType.WIKIPEDIA_TITLE_PARENTHESIS,\n            RelationType.WIKIPEDIA_DISAMBIGUATION,\n            RelationType.WIKIPEDIA_CATEGORY,\n            RelationType.WIKIPEDIA_REDIRECT_LINK,\n            RelationType.WIKIPEDIA_ALIASES,\n            RelationType.WIKIPEDIA_PART_OF_SAME_NAME,\n        ]\n\n    @staticmethod\n    def is_both_opposite_personal_pronouns(phrase1: str, phrase2: str) -> bool:\n        """"""\n        check if both phrases refers to pronouns\n\n        Returns:\n            bool\n        """"""\n        result = False\n        if StringUtils.is_pronoun(phrase1.lower()) and StringUtils.is_pronoun(phrase2.lower()):\n            result = True\n\n        return result\n\n    @staticmethod\n    def is_both_data_or_time(mention1: MentionDataLight, mention2: MentionDataLight) -> bool:\n        """"""\n        check if both phrases refers to time or date\n\n        Returns:\n            bool\n        """"""\n        mention1_ner = mention1.mention_ner\n        mention2_ner = mention2.mention_ner\n\n        if mention1_ner is None:\n            _, _, _, mention1_ner = StringUtils.find_head_lemma_pos_ner(mention1.tokens_str)\n        if mention2_ner is None:\n            _, _, _, mention2_ner = StringUtils.find_head_lemma_pos_ner(mention2.tokens_str)\n\n        is1_time_or_data = ""DATE"" in mention1_ner or ""TIME"" in mention1_ner\n        is2_time_or_data = ""DATE"" in mention2_ner or ""TIME"" in mention2_ner\n\n        result = False\n        if is1_time_or_data and is2_time_or_data:\n            result = True\n\n        return result\n'"
nlp_architect/data/cdc_resources/relations/within_doc_coref_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nimport os\nfrom typing import List, Set\n\nfrom nlp_architect.common.cdc.mention_data import MentionData\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import RelationType\nfrom nlp_architect.utils.io import load_json_file\n\nlogger = logging.getLogger(__name__)\n\n\nclass WithinDocCoref(RelationExtraction):\n    def __init__(self, wd_file: str):\n        """"""\n        Extract Relation between two mentions according to Within document co-reference\n\n        Args:\n            wd_file (required): str Location of within doc co-reference mentions file\n        """"""\n        logger.info(""Loading Within doc resource"")\n        if wd_file is not None and os.path.isfile(wd_file):\n            wd_mentions_json = load_json_file(wd_file)\n            self.within_doc_coref_chain = self.arrange_resource(wd_mentions_json)\n        else:\n            raise FileNotFoundError(""Within-doc resource file not found or not in path"")\n        super(WithinDocCoref, self).__init__()\n\n    @staticmethod\n    def arrange_resource(wd_mentions_json):\n        document_tokens_dict = dict()\n        for mention_json in wd_mentions_json:\n            mention_data = MentionData.read_json_mention_data_line(mention_json)\n            mention_tokens = mention_data.tokens_number\n            for i in range(0, len(mention_tokens)):\n                doc_id = mention_data.doc_id\n                sent_id = mention_data.sent_id\n                token_map_key = MentionData.static_gen_token_unique_id(\n                    doc_id, sent_id, mention_tokens[i]\n                )\n                document_tokens_dict[token_map_key] = mention_data.coref_chain\n        return document_tokens_dict\n\n    def extract_all_relations(\n        self, mention_x: MentionData, mention_y: MentionData\n    ) -> Set[RelationType]:\n        ret_ = set()\n        ret_.add(self.extract_sub_relations(mention_x, mention_y, RelationType.WITHIN_DOC_COREF))\n        return ret_\n\n    def extract_sub_relations(\n        self, mention_x: MentionData, mention_y: MentionData, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Check if input mentions has the given relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation or\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        if relation is not RelationType.WITHIN_DOC_COREF:\n            return RelationType.NO_RELATION_FOUND\n\n        if mention_x.doc_id == mention_y.doc_id:\n            ment_x_coref_chain = self.extract_within_coref(mention_x)\n            ment_y_coref_chain = self.extract_within_coref(mention_y)\n\n            if not ment_x_coref_chain or not ment_y_coref_chain:\n                return RelationType.NO_RELATION_FOUND\n\n            if ""-"" in ment_x_coref_chain or ""-"" in ment_y_coref_chain:\n                return RelationType.NO_RELATION_FOUND\n\n            if set(ment_x_coref_chain) == set(ment_y_coref_chain):\n                return RelationType.WITHIN_DOC_COREF\n\n        return RelationType.NO_RELATION_FOUND\n\n    def extract_within_coref(self, mention: MentionData) -> List[str]:\n        tokens = mention.tokens_number\n        within_coref_token = []\n        for token_id in tokens:\n            token_x_id = MentionData.static_gen_token_unique_id(\n                str(mention.doc_id), str(mention.sent_id), str(token_id)\n            )\n            if token_x_id in self.within_doc_coref_chain:\n                token_coref_chain = self.within_doc_coref_chain[token_x_id]\n                if token_coref_chain:\n                    within_coref_token.append(token_coref_chain)\n            else:\n                within_coref_token.append(""-"")\n                break\n\n        return within_coref_token\n\n    def get_within_doc_coref_chain(self):\n        return self.within_doc_coref_chain\n\n    @staticmethod\n    def create_ment_id(mention_x: MentionData, mention_y: MentionData) -> str:\n        return ""_"".join([mention_x.get_mention_id(), mention_y.get_mention_id()])\n\n    @staticmethod\n    def get_supported_relations() -> List[RelationType]:\n        """"""\n        Return all supported relations by this class\n\n        Returns:\n            List[RelationType]\n        """"""\n        return [RelationType.WITHIN_DOC_COREF]\n'"
nlp_architect/data/cdc_resources/relations/word_embedding_relation_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport math\nfrom typing import List, Set\n\nfrom scipy.spatial.distance import cosine as cos\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.embedding.embed_elmo import (\n    ElmoEmbedding,\n    ElmoEmbeddingOffline,\n)\nfrom nlp_architect.data.cdc_resources.embedding.embed_glove import (\n    GloveEmbedding,\n    GloveEmbeddingOffline,\n)\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import (\n    EmbeddingMethod,\n    RelationType,\n)\nfrom nlp_architect.utils.string_utils import StringUtils\n\nlogger = logging.getLogger(__name__)\n\n\nclass WordEmbeddingRelationExtraction(RelationExtraction):\n    def __init__(\n        self,\n        method: EmbeddingMethod = EmbeddingMethod.GLOVE,\n        glove_file: str = None,\n        elmo_file: str = None,\n        cos_accepted_dist: float = 0.7,\n    ):\n        """"""\n        Extract Relation between two mentions according to Word Embedding cosine distance\n\n        Args:\n            method (optional): EmbeddingMethod.{GLOVE/GLOVE_OFFLINE/ELMO/ELMO_OFFLINE}\n                (default = GLOVE)\n            glove_file (required on GLOVE/GLOVE_OFFLINE mode): str Location of Glove file\n            elmo_file (required on ELMO_OFFLINE mode): str Location of Elmo file\n        """"""\n        if method == EmbeddingMethod.GLOVE:\n            self.embedding = GloveEmbedding(glove_file)\n            self.contextual = False\n        elif method == EmbeddingMethod.GLOVE_OFFLINE:\n            self.embedding = GloveEmbeddingOffline(glove_file)\n            self.contextual = False\n        elif method == EmbeddingMethod.ELMO:\n            self.embedding = ElmoEmbedding()\n            self.contextual = True\n        elif method == EmbeddingMethod.ELMO_OFFLINE:\n            self.embedding = ElmoEmbeddingOffline(elmo_file)\n            self.contextual = True\n\n        self.accepted_dist = cos_accepted_dist\n        super(WordEmbeddingRelationExtraction, self).__init__()\n\n    def extract_all_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> Set[RelationType]:\n        ret_ = set()\n        ret_.add(\n            self.extract_sub_relations(mention_x, mention_y, RelationType.WORD_EMBEDDING_MATCH)\n        )\n        return ret_\n\n    def extract_sub_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Check if input mentions has the given relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation or\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        if relation is not RelationType.WORD_EMBEDDING_MATCH:\n            return RelationType.NO_RELATION_FOUND\n\n        mention_x_str = mention_x.tokens_str\n        mention_y_str = mention_y.tokens_str\n        if StringUtils.is_pronoun(mention_x_str.lower()) or StringUtils.is_pronoun(\n            mention_y_str.lower()\n        ):\n            if not self.contextual:\n                return RelationType.NO_RELATION_FOUND\n\n            if mention_x.mention_context is None or mention_y.mention_context is None:\n                return RelationType.NO_RELATION_FOUND\n\n        if self.is_word_embed_match(mention_x, mention_y):\n            return RelationType.WORD_EMBEDDING_MATCH\n\n        return RelationType.NO_RELATION_FOUND\n\n    def is_word_embed_match(self, mention_x: MentionDataLight, mention_y: MentionDataLight):\n        """"""\n        Check if input mentions Word Embedding cosine distance below above 0.65\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            bool\n        """"""\n        match_result = False\n        x_embed = self.embedding.get_head_feature_vector(mention_x)\n        y_embed = self.embedding.get_head_feature_vector(mention_y)\n        # make sure words are not \'unk/None/0\'\n        if x_embed is not None and y_embed is not None:\n            dist = cos(x_embed, y_embed)\n            if not math.isnan(dist):\n                sim = 1 - dist\n                if sim >= self.accepted_dist:\n                    match_result = True\n\n        return match_result\n\n    @staticmethod\n    def get_supported_relations() -> List[RelationType]:\n        """"""\n        Return all supported relations by this class\n\n        Returns:\n            List[RelationType]\n        """"""\n        return [RelationType.WORD_EMBEDDING_MATCH]\n'"
nlp_architect/data/cdc_resources/relations/wordnet_relation_extraction.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport os\nfrom typing import Set, List\n\nfrom nlp_architect.common.cdc.mention_data import MentionDataLight\nfrom nlp_architect.data.cdc_resources.data_types.wn.wordnet_page import WordnetPage\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import (\n    RelationType,\n    OnlineOROfflineMethod,\n)\nfrom nlp_architect.data.cdc_resources.wordnet.wordnet_offline import WordnetOffline\nfrom nlp_architect.data.cdc_resources.wordnet.wordnet_online import WordnetOnline\nfrom nlp_architect.utils.string_utils import StringUtils\n\nlogger = logging.getLogger(__name__)\n\n\nclass WordnetRelationExtraction(RelationExtraction):\n    def __init__(\n        self, method: OnlineOROfflineMethod = OnlineOROfflineMethod.ONLINE, wn_file: str = None\n    ):\n        """"""\n        Extract Relation between two mentions according to Word Embedding cosine distance\n\n        Args:\n            method (required): OnlineOROfflineMethod.{ONLINE/OFFLINE} run against full wordnet or\n                a sub-set of it (default = ONLINE)\n            wn_file (required on OFFLINE mode): str Location of wordnet subset file to work with\n        """"""\n        logger.info(""Loading Wordnet module"")\n        self.connectivity = method\n        if self.connectivity == OnlineOROfflineMethod.ONLINE:\n            self.wordnet_impl = WordnetOnline()\n        elif self.connectivity == OnlineOROfflineMethod.OFFLINE:\n            if wn_file is not None and os.path.isdir(wn_file):\n                self.wordnet_impl = WordnetOffline(wn_file)\n            else:\n                raise FileNotFoundError(""WordNet resource directory not found or not in path"")\n\n        logger.info(""Wordnet module lead successfully"")\n        super(WordnetRelationExtraction, self).__init__()\n\n    def extract_all_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight\n    ) -> Set[RelationType]:\n        """"""\n        Try to find if mentions has anyone or more of the relations this class support\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n\n        Returns:\n            Set[RelationType]: One or more of: RelationType.WORDNET_SAME_SYNSET_ENTITY,\n                RelationType.WORDNET_SAME_SYNSET_EVENT, RelationType.WORDNET_PARTIAL_SYNSET_MATCH,\n                RelationType.WORDNET_DERIVATIONALLY\n        """"""\n        relations = set()\n        mention_x_str = mention_x.tokens_str\n        mention_y_str = mention_y.tokens_str\n        if StringUtils.is_pronoun(mention_x_str.lower()) or StringUtils.is_pronoun(\n            mention_y_str.lower()\n        ):\n            relations.add(RelationType.NO_RELATION_FOUND)\n            return relations\n\n        page_x = self.wordnet_impl.get_pages(mention_x)\n        page_y = self.wordnet_impl.get_pages(mention_y)\n\n        if page_x and page_y:\n            deriv_rel = self.extract_derivation(page_x, page_y)\n            part_syn_rel = self.extract_partial_synset_match(page_x, page_y)\n            same_syn_rel = self.extract_same_synset_entity(page_x, page_y)\n            if deriv_rel != RelationType.NO_RELATION_FOUND:\n                relations.add(deriv_rel)\n            if part_syn_rel != RelationType.NO_RELATION_FOUND:\n                relations.add(part_syn_rel)\n            if same_syn_rel != RelationType.NO_RELATION_FOUND:\n                relations.add(same_syn_rel)\n\n        if len(relations) == 0:\n            relations.add(RelationType.NO_RELATION_FOUND)\n\n        return relations\n\n    def extract_sub_relations(\n        self, mention_x: MentionDataLight, mention_y: MentionDataLight, relation: RelationType\n    ) -> RelationType:\n        """"""\n        Check if input mentions has the given relation between them\n\n        Args:\n            mention_x: MentionDataLight\n            mention_y: MentionDataLight\n            relation: RelationType\n\n        Returns:\n            RelationType: relation in case mentions has given relation or\n                RelationType.NO_RELATION_FOUND otherwise\n        """"""\n        mention_x_str = mention_x.tokens_str\n        mention_y_str = mention_y.tokens_str\n        if StringUtils.is_pronoun(mention_x_str.lower()) or StringUtils.is_pronoun(\n            mention_y_str.lower()\n        ):\n            return RelationType.NO_RELATION_FOUND\n\n        page_x = self.wordnet_impl.get_pages(mention_x)\n        page_y = self.wordnet_impl.get_pages(mention_y)\n\n        if page_x and page_y:\n            if relation == RelationType.WORDNET_DERIVATIONALLY:\n                return self.extract_derivation(page_x, page_y)\n            if relation == RelationType.WORDNET_PARTIAL_SYNSET_MATCH:\n                return self.extract_partial_synset_match(page_x, page_y)\n            if relation == RelationType.WORDNET_SAME_SYNSET:\n                return self.extract_same_synset_entity(page_x, page_y)\n\n        return RelationType.NO_RELATION_FOUND\n\n    @staticmethod\n    def extract_derivation(page_x: WordnetPage, page_y: WordnetPage) -> RelationType:\n        """"""\n        Check if input mentions has derivation relation\n\n        Args:\n            page_x:WordnetPage\n            page_y:WordnetPage\n\n        Returns:\n            RelationType.WORDNET_DERIVATIONALLY or RelationType.NO_RELATION_FOUND\n        """"""\n        x_head = page_x.head\n        x_head_lemma = page_x.head_lemma\n        y_head = page_y.head\n        y_head_lemma = page_y.head_lemma\n\n        x_set = set()\n        x_set.update(page_x.head_derivationally)\n        x_set.update(page_x.head_lemma_derivationally)\n\n        y_set = set()\n        y_set.update(page_y.head_derivationally)\n        y_set.update(page_y.head_lemma_derivationally)\n\n        relation = RelationType.NO_RELATION_FOUND\n\n        if (\n            y_head in x_set\n            or y_head_lemma in x_set\n            or x_head in y_set\n            or x_head_lemma in y_set\n            or len(x_set & y_set) > 0\n        ):\n            relation = RelationType.WORDNET_DERIVATIONALLY\n            # print \'matched by derivation - \' + str(x_head)+ \' , \' + str(y_head)\n\n        return relation\n\n    @staticmethod\n    def extract_partial_synset_match(page_x: WordnetPage, page_y: WordnetPage) -> RelationType:\n        """"""\n        Check if input mentions has partial synset relation\n\n        Args:\n            page_x:WordnetPage\n            page_y:WordnetPage\n\n        Returns:\n            RelationType.WORDNET_PARTIAL_SYNSET_MATCH or RelationType.NO_RELATION_FOUND\n        """"""\n        x_words = page_x.clean_phrase.split()\n        y_words = page_y.clean_phrase.split()\n\n        if len(x_words) == 0 or len(y_words) == 0:\n            return RelationType.NO_RELATION_FOUND\n\n        x_synonyms = page_x.all_clean_words_synonyms\n        y_synonyms = page_y.all_clean_words_synonyms\n\n        # One word - check whether there is intersection between synsets\n        if (\n            len(x_synonyms) == 1\n            and len(y_synonyms) == 1\n            and len([w for w in (x_synonyms[0] & y_synonyms[0])]) > 0\n        ):\n            # print \'matched by partial - \' + str(y) + \' , \' + str(x)\n            return RelationType.WORDNET_PARTIAL_SYNSET_MATCH\n\n        return RelationType.NO_RELATION_FOUND\n\n    @staticmethod\n    def extract_same_synset_entity(page_x: WordnetPage, page_y: WordnetPage) -> RelationType:\n        """"""\n        Check if input mentions has same synset relation for entity mentions\n\n        Args:\n            page_x:WordnetPage\n            page_y:WordnetPage\n\n        Returns:\n            RelationType.WORDNET_SAME_SYNSET_ENTITY or RelationType.NO_RELATION_FOUND\n        """"""\n        match_result = RelationType.NO_RELATION_FOUND\n        th = 0\n        if len([w for w in (page_x.head_synonyms & page_y.head_synonyms)]) > th:\n            match_result = RelationType.WORDNET_SAME_SYNSET\n\n        return match_result\n\n    @staticmethod\n    def get_supported_relations() -> List[RelationType]:\n        """"""\n        Return all supported relations by this class\n\n        Returns:\n            List[RelationType]\n        """"""\n        return [\n            RelationType.WORDNET_SAME_SYNSET,\n            RelationType.WORDNET_PARTIAL_SYNSET_MATCH,\n            RelationType.WORDNET_DERIVATIONALLY,\n        ]\n'"
nlp_architect/data/cdc_resources/wikipedia/__init__.py,0,b''
nlp_architect/data/cdc_resources/wikipedia/wiki_elastic.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport sys\nimport traceback\n\nimport requests\n\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page import WikipediaPage\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page_extracted_relations import (\n    WikipediaPageExtractedRelations,\n)\nfrom nlp_architect.data.cdc_resources.wikipedia.wiki_search_page_result import (\n    WikipediaSearchPageResult,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass WikiElastic(object):\n    def __init__(self, host: str, port: int, index: str):\n        try:\n            from elasticsearch import Elasticsearch\n        except (AttributeError, ImportError):\n            logger.error(\n                ""elasticsearch is not installed, please install nlp_architect with [all] package. ""\n                + ""for example: pip install nlp_architect[all]""\n            )\n            sys.exit()\n        # connect to our cluster\n        self.cache = dict()\n        if self.is_connected(host, port):\n            self.es_index = index\n            self.es = Elasticsearch([{""host"": host, ""port"": port}])\n        else:\n            traceback.print_exc()\n            raise IOError(""Cannot connect to ElasticSearch node"")\n\n    @staticmethod\n    def is_connected(elastic_host, elastic_port):\n        elastic_search_url = ""http://"" + elastic_host + "":"" + str(elastic_port)\n        res = requests.get(elastic_search_url)\n        if res.content:\n            return True\n        return False\n\n    def get_pages(self, phrase):\n        if phrase in self.cache:\n            return self.cache[phrase]\n        try:\n            phrase_strip = "" "".join(phrase.replace(""-"", "" "").split())\n            pages = set()\n            best_results = self.get_best_elastic_results(phrase_strip)\n            for result in best_results:\n                _id = result[""_id""]\n                if _id != 0:\n                    result_source = result[""_source""]\n                    if ""redirectTitle"" in result_source:\n                        redirect_title = result_source[""redirectTitle""]\n                        red_result = None\n                        while redirect_title and result_source[""title""] != redirect_title:\n                            red_result = self.get_redirect_result(redirect_title)\n                            if red_result is None or len(red_result) == 0:\n                                print(\n                                    ""could not find redirect title=""\n                                    + redirect_title\n                                    + "", does not exist in data""\n                                )\n                                redirect_title = None\n                            elif ""redirectTitle"" in red_result[0][""_source""]:\n                                redirect_title = red_result[0][""_source""][""redirectTitle""]\n                            else:\n                                redirect_title = None\n\n                        if red_result is not None and len(red_result) > 0:\n                            result = red_result[0]\n                            _id = result[""_id""]\n\n                    elastic_page_result = self.get_page_from_result_v1(phrase_strip, result, _id)\n                    pages.add(WikipediaSearchPageResult(phrase, elastic_page_result))\n\n            self.cache[phrase] = pages\n            return pages\n        except Exception:\n            traceback.print_exc()\n\n    def get_best_elastic_results(self, phrase):\n        best_results = []\n        best_results.extend(self.get_redirect_result(phrase))\n\n        search_result_near_match = self.es.search(\n            index=self.es_index,\n            body={""size"": 5, ""query"": {""match_phrase"": {""title.near_match"": phrase}}},\n        )\n        best_results.extend(self.extract_from_elastic_results(search_result_near_match))\n\n        return best_results\n\n    @staticmethod\n    def extract_from_elastic_results(search_result):\n        best_results = []\n        if search_result is not None and search_result[""hits""][""total""] > 0:\n            if search_result[""hits""][""total""] > 0:\n                best_results = search_result[""hits""][""hits""]\n        return best_results\n\n    def get_redirect_result(self, phrase):\n        search_result = self.es.search(\n            index=self.es_index,\n            body={""size"": 5, ""query"": {""match_phrase"": {""title.keyword"": phrase}}},\n        )\n        results = self.extract_from_elastic_results(search_result)\n        return results\n\n    def get_page_from_result_v1(self, phrase, result, result_id):\n        if result_id != 0 and result is not None:\n            relations = None\n            result_source = result[""_source""]\n            result_score = result[""_score""]\n            if result_source is not None:\n                title = result_source[""title""]\n                relations_source = result_source[""relations""]\n\n                if relations_source is not None:\n                    is_part = relations_source[""isPartName""]\n                    is_disambig = relations_source[""isDisambiguation""]\n\n                    disambig_links = self.safe_extract_field_from_dict(\n                        ""disambiguationLinks"", relations_source\n                    )\n                    disambig_links_norm = self.safe_extract_field_from_dict(\n                        ""disambiguationLinksNorm"", relations_source\n                    )\n                    categories = self.safe_extract_field_from_dict(""categories"", relations_source)\n                    categories_norm = self.safe_extract_field_from_dict(\n                        ""categoriesNorm"", relations_source\n                    )\n                    title_parent = self.safe_extract_field_from_dict(\n                        ""titleParenthesis"", relations_source\n                    )\n                    title_parent_norm = self.safe_extract_field_from_dict(\n                        ""titleParenthesisNorm"", relations_source\n                    )\n                    be_comp = self.safe_extract_field_from_dict(""beCompRelations"", relations_source)\n                    be_comp_norm = self.safe_extract_field_from_dict(\n                        ""beCompRelationsNorm"", relations_source\n                    )\n\n                    relations = WikipediaPageExtractedRelations(\n                        is_part,\n                        is_disambig,\n                        title_parent,\n                        disambig_links,\n                        categories,\n                        None,\n                        be_comp,\n                        disambig_links_norm,\n                        categories_norm,\n                        None,\n                        title_parent_norm,\n                        be_comp_norm,\n                    )\n\n            return WikipediaPage(\n                phrase, None, title, None, result_score, result_id, None, relations\n            )\n\n        return WikipediaPage()\n\n    @staticmethod\n    def safe_extract_field_from_dict(field_name, _dict):\n        if field_name in _dict:\n            return _dict[field_name]\n        return None\n'"
nlp_architect/data/cdc_resources/wikipedia/wiki_offline.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nfrom os import listdir\nfrom os.path import join, isfile\n\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page import WikipediaPage\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page_extracted_relations import (\n    WikipediaPageExtractedRelations,\n)\nfrom nlp_architect.data.cdc_resources.wikipedia.wiki_search_page_result import (\n    WikipediaSearchPageResult,\n)\nfrom nlp_architect.utils.io import load_json_file\n\nlogger = logging.getLogger(__name__)\n\n\nclass WikiOffline(object):\n    def __init__(self, wikidump):\n        if wikidump:\n            self.dump = self.load_dump(wikidump)\n            logger.info(""Wikipedia dump loaded successfully!"")\n\n    def get_pages(self, phrase):\n        if phrase and phrase in self.dump:\n            pages = self.dump[phrase]\n            if pages:\n                return pages\n\n        return set()\n\n    @staticmethod\n    def extract_json_values(json_pages):\n        pages = set()\n        for json_page in json_pages:\n            description = json_page.get(""description"", None)\n            pageid = int(json_page.get(""pageid"", 0))\n            orig_phrase = json_page.get(""orig_phrase"", None)\n            orig_phrase_norm = json_page.get(""orig_phrase_norm"", None)\n            wiki_title = json_page.get(""wiki_title"", None)\n            wiki_title_norm = json_page.get(""wiki_title_norm"", None)\n\n            relations_json = json_page.get(""relations"", None)\n            rel_is_part_name = relations_json.get(""isPartName"", None)\n            rel_is_disambiguation = relations_json.get(""isDisambiguation"", None)\n            rel_disambiguation = relations_json.get(""disambiguationLinks"", None)\n            rel_disambiguation_norm = relations_json.get(""disambiguationLinksNorm"", None)\n            rel_parenthesis = relations_json.get(""titleParenthesis"", None)\n            rel_parenthesis_norm = relations_json.get(""titleParenthesisNorm"", None)\n            rel_categories = relations_json.get(""categories"", None)\n            rel_categories_norm = relations_json.get(""categoriesNorm"", None)\n            rel_be_comp = relations_json.get(""beCompRelations"", None)\n            rel_be_comp_norm = relations_json.get(""beCompRelationsNorm"", None)\n            rel_aliases = relations_json.get(""aliases"", None)\n            rel_aliases_norm = relations_json.get(""aliasesNorm"", None)\n\n            relations = WikipediaPageExtractedRelations(\n                rel_is_part_name,\n                rel_is_disambiguation,\n                rel_parenthesis,\n                rel_disambiguation,\n                rel_categories,\n                rel_aliases,\n                rel_be_comp,\n                rel_disambiguation_norm,\n                rel_categories_norm,\n                rel_aliases_norm,\n                rel_parenthesis_norm,\n                rel_be_comp_norm,\n            )\n\n            page = WikipediaPage(\n                orig_phrase,\n                orig_phrase_norm,\n                wiki_title,\n                wiki_title_norm,\n                0,\n                pageid,\n                description,\n                relations,\n            )\n            pages.add(WikipediaSearchPageResult(orig_phrase, page))\n\n        return pages\n\n    def load_dump(self, wiki_dump):\n        onlyfiles = []\n        for _file in listdir(wiki_dump):\n            file_path = join(wiki_dump, _file)\n            if isfile(file_path):\n                onlyfiles.append(file_path)\n\n        json_dump_list = {}\n        for _file in onlyfiles:\n            json_dump_list.update(load_json_file(_file))\n\n        dump_final = {}\n        for key, value in json_dump_list.items():\n            dump_final[key] = self.extract_json_values(value)\n\n        return dump_final\n\n    class NoPage(object):\n        """""" Attribute not found. """"""\n\n        def __init__(self, *args, **kwargs):  # real signature unknown\n            pass\n\n        @staticmethod  # known case of __new__\n        def __new__(S, *more):  # real signature unknown; restored from __doc__\n            """""" T.__new__(S, ...) -> a new object with type S, a subtype of T """"""\n'"
nlp_architect/data/cdc_resources/wikipedia/wiki_online.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport os\nimport re\nimport sys\n\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page import WikipediaPage\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page_extracted_relations import (\n    WikipediaPageExtractedRelations,\n)\nfrom nlp_architect.data.cdc_resources.wikipedia.wiki_search_page_result import (\n    WikipediaSearchPageResult,\n)\nfrom nlp_architect.utils.text import SpacyInstance\n\nos.environ[""PYWIKIBOT_NO_USER_CONFIG""] = ""1""\n\nDISAMBIGUATE_PAGE = [""wikimedia disambiguation page"", ""wikipedia disambiguation page""]\nNAME_DESCRIPTIONS = [""given name"", ""first name"", ""family name""]\n\nlogger = logging.getLogger(__name__)\n\n\nclass WikiOnline(object):\n    def __init__(self):\n        try:\n            import pywikibot\n        except (AttributeError, ImportError):\n            logger.error(\n                ""pywikibot is not installed, please install nlp_architect with [all] package. ""\n                + ""for example: pip install nlp_architect[all]""\n            )\n            sys.exit()\n        self.spacy = SpacyInstance()\n        self.pywikibot = pywikibot\n        self.cache = dict()\n        self.site = pywikibot.Site(""en"", ""wikipedia"")  # The site we want to run our bot on\n\n    def get_pages(self, phrase):\n        if phrase in self.cache:\n            return self.cache[phrase]\n\n        ret_pages = set()\n        word_clean = phrase.replace(""-"", "" "")\n        word_lower = word_clean.lower()\n        word_upper = word_clean.upper()\n        word_title = word_clean.title()\n        words_set = {phrase, word_clean, word_lower, word_upper, word_title}\n        for appr in words_set:\n            try:\n                page_result = self.get_page_redirect(appr)\n                if page_result.pageid != 0:\n                    full_page = self.get_wiki_page_with_items(phrase, page_result)\n                    ret_pages.add(WikipediaSearchPageResult(appr, full_page))\n            except Exception as e:\n                logger.error(e)\n\n        self.cache[phrase] = ret_pages\n        return ret_pages\n\n    # pylint: disable=protected-access\n    def get_wiki_page_with_items(self, phrase, page):\n        item = self.get_wiki_page_item(page)\n        pageid = page.pageid\n        aliases = self.get_aliases(item)\n        description = self.get_description(item)\n        text = page.text\n        page_title = page._link._title\n\n        relations = WikipediaPageExtractedRelations()\n        relations.is_disambiguation = self.is_disambiguation_page(item)\n        relations.is_part_name = self.is_name_description(text, item, relations.is_disambiguation)\n        relations.aliases = aliases\n        relations.be_comp, relations.be_comp_norm = self.extract_be_comp(text)\n        relations.extract_relations_from_text_v0(text)\n\n        ret_page = WikipediaPage(phrase, None, page_title, None, 0, pageid, description, relations)\n\n        logger.debug(""Page: {}. Extracted successfully"".format(ret_page))\n\n        return ret_page\n\n    def get_wiki_page_item(self, page):\n        if page is not None:\n            try:\n                item = self.pywikibot.ItemPage.fromPage(\n                    page\n                )  # this can be used for any page object\n                item.get()  # need to call it to access any data.\n                return item\n            except (self.pywikibot.NoPage, AttributeError, TypeError, NameError):\n                pass\n        return None\n\n    def get_page_redirect(self, word):\n        page = self.pywikibot.Page(self.site, word)\n        if page.pageid != 0 and page.isRedirectPage():\n            return page.getRedirectTarget()\n        return page\n\n    @staticmethod\n    def get_aliases(item):\n        if item is not None and item.aliases is not None:\n            if ""en"" in item.aliases:\n                aliases = item.aliases[""en""]\n                return aliases\n\n        return None\n\n    @staticmethod\n    def get_description(item):\n        description = {}\n        if item is not None:\n            item_desc = item.get()\n            if ""desctiptions"" in item_desc and ""en"" in item_desc[""descriptions""]:\n                dict([(""age"", 25)])\n                description[""descriptions""] = dict([(""en"", item_desc[""descriptions""][""en""])])\n\n        return description\n\n    @staticmethod\n    def is_disambiguation_page(item):\n        if item is not None:\n            dic = item.get()\n            if dic is not None and ""descriptions"" in dic:\n                desc = dic[""descriptions""]\n                if desc is not None and ""en"" in desc:\n                    return desc[""en""].lower() in DISAMBIGUATE_PAGE\n\n        return False\n\n    @staticmethod\n    def is_name_description(text, item, is_disambiguation):\n        if item is not None:\n            if is_disambiguation:\n                if WikipediaPageExtractedRelations.is_name_part(text):\n                    return True\n            else:\n                dic = item.get()\n                if dic is not None and ""descriptions"" in dic:\n                    desc = dic[""descriptions""]\n                    if desc is not None and ""en"" in desc:\n                        if [s for s in NAME_DESCRIPTIONS if s in desc[""en""].lower()]:\n                            return True\n        return False\n\n    # pylint: disable=no-else-return\n    def extract_be_comp(self, text):\n        first_sentence_start_index = text.index(""\'\'\'"")\n        if first_sentence_start_index >= 0:\n            last_temp_index = text.find(""\\n"", first_sentence_start_index)\n        if last_temp_index == -1:\n            last_temp_index = len(text)\n\n        first_paragraph = text[first_sentence_start_index:last_temp_index]\n        if WikiOnline.extract_be_a_index(first_paragraph) == -1 and last_temp_index != len(text):\n            return self.extract_be_comp(text[last_temp_index:])\n        elif last_temp_index == len(text):\n            return None, None\n\n        first_paragraph_clean = re.sub(r""\\([^)]*\\)"", """", first_paragraph)\n        first_paragraph_clean = re.sub(r""<[^>]*>"", """", first_paragraph_clean)\n        first_paragraph_clean = re.sub(r""{[^}]*}"", """", first_paragraph_clean)\n        first_paragraph_clean = re.sub(r""\\[\\[[^]]*\\]\\]"", """", first_paragraph_clean)\n        first_paragraph_clean = re.sub(r""[\\\']"", """", first_paragraph_clean)\n        first_paragraph_clean = re.sub(r""&nbsp;"", "" "", first_paragraph_clean)\n\n        return self.extract_be_comp_relations(first_paragraph_clean)\n\n    # pylint: disable=not-callable\n    def extract_be_comp_relations(self, first_paragraph):\n        be_comp = set()\n        be_comp_norm = set()\n        if first_paragraph:\n            doc = self.spacy.parser(first_paragraph)\n            for token in doc:\n                target = token.text\n                target_lemma = token.lemma_\n                relation = token.dep_\n                governor = token.head.text\n                governor_lemma = token.head.lemma_\n                if relation == ""acl"":\n                    break\n                if relation == ""punct"" and target == ""."":\n                    break\n                elif relation == ""cop"":\n                    be_comp.add(governor)\n                    be_comp_norm.add(governor_lemma)\n                elif relation == ""nsubj"":\n                    be_comp.add(target)\n                    be_comp_norm.add(target_lemma)\n                elif relation == ""dep"":\n                    be_comp.add(governor)\n                    be_comp_norm.add(governor_lemma)\n                elif relation == ""compound"":\n                    be_comp.add(target + "" "" + governor)\n                    be_comp_norm.add(target_lemma + "" "" + governor_lemma)\n                elif relation == ""amod"":\n                    be_comp.add(target + "" "" + governor)\n                    be_comp_norm.add(target_lemma + "" "" + governor_lemma)\n                elif relation in [""conj"", ""appos""]:\n                    be_comp.add(target)\n                    be_comp_norm.add(target_lemma)\n\n        return be_comp, be_comp_norm\n\n    @staticmethod\n    def extract_be_a_index(sentence):\n        result = None\n        if ""is a"" in sentence:\n            result = sentence.index(""is a"")\n        elif ""are a"" in sentence:\n            result = sentence.index(""are a"")\n        elif ""was a"" in sentence:\n            result = sentence.index(""was a"")\n        elif ""were a"" in sentence:\n            result = sentence.index(""were a"")\n        elif ""be a"" in sentence:\n            result = sentence.index(""be a"")\n        elif ""is the"" in sentence:\n            result = sentence.index(""is the"")\n        elif ""are the"" in sentence:\n            result = sentence.index(""are the"")\n        elif ""was the"" in sentence:\n            result = sentence.index(""was the"")\n        elif ""were the"" in sentence:\n            result = sentence.index(""were the"")\n        elif ""be the"" in sentence:\n            result = sentence.index(""be the"")\n\n        return result\n'"
nlp_architect/data/cdc_resources/wikipedia/wiki_search_page_result.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page import WikipediaPage\n\n\nclass WikipediaSearchPageResult(object):\n    def __init__(self, search_phrase: str, page_result: WikipediaPage):\n        """"""\n        Args:\n            search_phrase: the search phrase that yield this page result\n            page_result: page result for search phrase\n        """"""\n        self.search_phrase = search_phrase\n        self.page_result = page_result\n'"
nlp_architect/data/cdc_resources/wordnet/__init__.py,0,b''
nlp_architect/data/cdc_resources/wordnet/wordnet_offline.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nfrom os import listdir\nfrom os.path import join, isfile\n\nfrom nlp_architect.data.cdc_resources.data_types.wn.wordnet_page import WordnetPage\nfrom nlp_architect.utils.io import load_json_file\n\nlogger = logging.getLogger(__name__)\n\n\nclass WordnetOffline(object):\n    def __init__(self, wordnet_dump):\n        if wordnet_dump:\n            self.dump = self.load_dump(wordnet_dump)\n            logger.info(""Wikipedia dump loaded successfully!"")\n\n    def get_pages(self, mention):\n        page = None\n        if mention.tokens_str is not None and mention.tokens_str in self.dump:\n            page = self.dump[mention.tokens_str]\n\n        return page\n\n    def load_dump(self, wn_dump):\n        onlyfiles = []\n        for _file in listdir(wn_dump):\n            file_path = join(wn_dump, _file)\n            if isfile(file_path):\n                onlyfiles.append(file_path)\n\n        json_dump_list = {}\n        for _file in onlyfiles:\n            json_dump_list.update(load_json_file(_file))\n\n        dump_final = {}\n        for key, value in json_dump_list.items():\n            dump_final[key] = self.extract_json_values(value)\n\n        return dump_final\n\n    @staticmethod\n    def extract_json_values(json_page):\n        if json_page is not None:\n            orig_phrase = json_page.get(""orig_phrase"", None)\n            clean_phrase = json_page.get(""clean_phrase"", None)\n            head = json_page.get(""head"", None)\n            head_lemma = json_page.get(""head_lemma"", None)\n            head_synonyms = set(json_page.get(""head_synonyms"", None))\n            head_lemma_synonyms = set(json_page.get(""head_lemma_synonyms"", None))\n            head_derivationally = set(json_page.get(""head_derivationally"", None))\n            head_lemma_derivationally = set(json_page.get(""head_lemma_derivationally"", None))\n\n            all_clean_words_synonyms = json_page.get(""all_clean_words_synonyms"", None)\n            all_as_set_list = []\n            for list_ in all_clean_words_synonyms:\n                all_as_set_list.append(set(list_))\n\n            wordnet_page = WordnetPage(\n                orig_phrase,\n                clean_phrase,\n                head,\n                head_lemma,\n                head_synonyms,\n                head_lemma_synonyms,\n                head_derivationally,\n                head_lemma_derivationally,\n                all_as_set_list,\n            )\n            return wordnet_page\n\n        return None\n'"
nlp_architect/data/cdc_resources/wordnet/wordnet_online.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport nltk\nfrom nltk.corpus import wordnet as wn\n\nfrom nlp_architect.data.cdc_resources.data_types.wn.wordnet_page import WordnetPage\nfrom nlp_architect.utils.string_utils import StringUtils\n\n\nclass WordnetOnline(object):\n    def __init__(self):\n        self.cache = dict()\n        nltk.download(""wordnet"")\n\n    def get_pages(self, mention):\n        if mention.tokens_str in self.cache:\n            return self.cache[mention.tokens_str]\n\n        head_synonyms, head_names_derivationally = self.extract_synonyms_and_derivation(\n            mention.mention_head\n        )\n        head_lemma_synonyms, head_lemma_derivationally = self.extract_synonyms_and_derivation(\n            mention.mention_head_lemma\n        )\n        clean_phrase = StringUtils.normalize_str(mention.tokens_str)\n        all_clean_words_synonyms = self.all_clean_words_synonyms(clean_phrase)\n\n        wordnet_page = WordnetPage(\n            mention.tokens_str,\n            clean_phrase,\n            mention.mention_head,\n            mention.mention_head_lemma,\n            head_synonyms,\n            head_lemma_synonyms,\n            head_names_derivationally,\n            head_lemma_derivationally,\n            all_clean_words_synonyms,\n        )\n\n        self.cache[mention.tokens_str] = wordnet_page\n        return wordnet_page\n\n    @staticmethod\n    def extract_synonyms_and_derivation(word):\n        lemma_names = set()\n        derivationally_related_forms = set()\n        for synset in wn.synsets(word):\n            for lemma in synset.lemmas():\n                lemma_name = lemma.name().replace(""_"", "" "")\n                if not StringUtils.is_stop(lemma_name.lower()):\n                    lemma_names.add(lemma_name)\n\n                derivationally_related_forms.update(\n                    [\n                        lem.name().replace(""_"", "" "")\n                        for lem in lemma.derivationally_related_forms()\n                        if not StringUtils.is_stop(lem.name().lower())\n                    ]\n                )\n\n        return lemma_names, derivationally_related_forms\n\n    @staticmethod\n    def all_clean_words_synonyms(clean_phrase):\n        words = clean_phrase.split()\n        return [\n            set(\n                [\n                    lemma.lower().replace(""_"", "" "")\n                    for synset in wn.synsets(w)\n                    for lemma in synset.lemma_names()\n                    if not StringUtils.is_stop(lemma.lower())\n                ]\n            )\n            for w in words\n        ]\n'"
nlp_architect/models/absa/inference/__init__.py,0,b''
nlp_architect/models/absa/inference/data_types.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport json\nfrom enum import Enum\nfrom json import JSONEncoder\n\n\nclass LexiconElement(object):\n    def __init__(\n        self,\n        term: list,\n        score: str or float = None,\n        polarity: str = None,\n        is_acquired: str = None,\n        position: str = None,\n    ):\n        self.term = term\n        self.polarity = polarity\n        try:\n            self.score = float(score)\n        except TypeError:\n            self.score = 0\n        self.position = position\n        if is_acquired == ""N"":\n            self.is_acquired = False\n        elif is_acquired == ""Y"":\n            self.is_acquired = True\n        else:\n            self.is_acquired = None\n\n    def __lt__(self, other):\n        return self.term[0] < other.term[0]\n\n    def __le__(self, other):\n        return self.term[0] <= other.term[0]\n\n    def __eq__(self, other):\n        return self.term[0] == other.term[0]\n\n    def __ne__(self, other):\n        return self.term[0] != other.term[0]\n\n    def __gt__(self, other):\n        return self.term[0] > other.term[0]\n\n    def __ge__(self, other):\n        return self.term[0] >= other.term[0]\n\n\nclass TermType(Enum):\n    OPINION = ""OP""\n    ASPECT = ""AS""\n    NEGATION = ""NEG""\n    INTENSIFIER = ""INT""\n\n\nclass Polarity(Enum):\n    POS = ""POS""\n    NEG = ""NEG""\n    UNK = ""UNK""\n\n\nclass Term(object):\n    def __init__(\n        self, text: str, kind: TermType, polarity: Polarity, score: float, start: int, length: int\n    ):\n        self._text = text\n        self._type = kind\n        self._polarity = polarity\n        self._score = score\n        self._start = start\n        self._len = length\n\n    def __eq__(self, other):\n        if type(other) is type(self):\n            return self.__dict__ == other.__dict__\n        return False\n\n    @property\n    def text(self):\n        return self._text\n\n    @property\n    def type(self):\n        return self._type\n\n    @property\n    def polarity(self):\n        return self._polarity\n\n    @property\n    def score(self):\n        return self._score\n\n    @property\n    def start(self):\n        return self._start\n\n    @property\n    def len(self):\n        return self._len\n\n    @text.setter\n    def text(self, val):\n        self._text = val\n\n    @score.setter\n    def score(self, val):\n        self._score = val\n\n    @polarity.setter\n    def polarity(self, val):\n        self._polarity = val\n\n    def __str__(self):\n        return (\n            ""text: ""\n            + self._text\n            + "" type: ""\n            + str(self._type)\n            + "" pol: ""\n            + str(self._polarity)\n            + "" score: ""\n            + str(self._score)\n            + "" start: ""\n            + str(self._start)\n            + "" len: ""\n            + str(self._len)\n        )\n\n\nclass SentimentDoc(object):\n    def __init__(self, doc_text: str = None, sentences: list = None):\n        if sentences is None:\n            sentences = []\n        self._doc_text = doc_text\n        self._sentences = sentences\n\n    def __eq__(self, other):\n        if type(other) is type(self):\n            return self.__dict__ == other.__dict__\n        return False\n\n    @property\n    def doc_text(self):\n        return self._doc_text\n\n    @doc_text.setter\n    def doc_text(self, val):\n        self._doc_text = val\n\n    @property\n    def sentences(self):\n        return self._sentences\n\n    @sentences.setter\n    def sentences(self, val):\n        self._sentences = val\n\n    @staticmethod\n    def decoder(obj):\n        """"""\n        :param obj: object to be decoded\n        :return: decoded Sentence object\n        """"""\n        # SentimentDoc\n        if ""_doc_text"" in obj and ""_sentences"" in obj:\n            return SentimentDoc(obj[""_doc_text""], obj[""_sentences""])\n\n        # SentimentSentence\n        if all((attr in obj for attr in (""_start"", ""_end"", ""_events""))):\n            return SentimentSentence(obj[""_start""], obj[""_end""], obj[""_events""])\n\n        # Term\n        if all(attr in obj for attr in (""_text"", ""_type"", ""_score"", ""_polarity"", ""_start"", ""_len"")):\n            return Term(\n                obj[""_text""],\n                TermType[obj[""_type""]],\n                Polarity[obj[""_polarity""]],\n                obj[""_score""],\n                obj[""_start""],\n                obj[""_len""],\n            )\n        return obj\n\n    def __repr__(self):\n        return self.pretty_json()\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __iter__(self):\n        return self.sentences.__iter__()\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def json(self):\n        """"""\n        Return json representations of the object\n\n        Returns:\n            :obj:`json`: json representations of the object\n        """"""\n        return json.dumps(self, cls=SentimentDocEncoder)\n\n    def pretty_json(self):\n        """"""\n        Return pretty json representations of the object\n\n        Returns:\n            :obj:`json`: pretty json representations of the object\n        """"""\n        return json.dumps(self, cls=SentimentDocEncoder, indent=4)\n\n\nclass SentimentSentence(object):\n    def __init__(self, start: int, end: int, events: list):\n        self._start = start\n        self._end = end\n        self._events = events\n\n    def __eq__(self, other):\n        if type(other) is type(self):\n            return self.__dict__ == other.__dict__\n        return False\n\n    @property\n    def start(self):\n        return self._start\n\n    @start.setter\n    def start(self, val):\n        self._start = val\n\n    @property\n    def end(self):\n        return self._end\n\n    @end.setter\n    def end(self, val):\n        self._end = val\n\n    @property\n    def events(self):\n        return self._events\n\n    @events.setter\n    def events(self, val):\n        self._events = val\n\n\nclass SentimentDocEncoder(JSONEncoder):\n    def default(self, o):  # pylint: disable=E0202\n        try:\n            if isinstance(o, Enum):\n                return getattr(o, ""_name_"")\n            if hasattr(o, ""__dict__""):\n                return vars(o)\n            if hasattr(o, ""__slots__""):\n                ret = {slot: getattr(o, slot) for slot in o.__slots__}\n                for cls in type(o).mro():\n                    spr = super(cls, o)\n                    if not hasattr(spr, ""__slots__""):\n                        break\n                    for slot in spr.__slots__:\n                        ret[slot] = getattr(o, slot)\n                return ret\n        except Exception as e:\n            print(e)\n        return None\n'"
nlp_architect/models/absa/inference/inference.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport math\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Union\n\nfrom nlp_architect.common.core_nlp_doc import CoreNLPDoc\nfrom nlp_architect.models.absa import INFERENCE_OUT\nfrom nlp_architect.models.absa.inference.data_types import (\n    Term,\n    TermType,\n    Polarity,\n    SentimentDoc,\n    SentimentSentence,\n    LexiconElement,\n)\nfrom nlp_architect.models.absa.utils import (\n    _read_lexicon_from_csv,\n    load_opinion_lex,\n    _load_aspect_lexicon,\n)\n\nINTENSIFIER_FACTOR = 0.3\nVERB_POS = {""VB"", ""VBD"", ""VBG"", ""VBN"", ""VBP"", ""VBZ""}\n\n\nclass SentimentInference(object):\n    """"""Main class for sentiment inference execution.\n\n    Attributes:\n        opinion_lex: Opinion lexicon as outputted by TrainSentiment module.\n        aspect_lex: Aspect lexicon as outputted by TrainSentiment module.\n        intensifier_lex (dict): Pre-defined intensifier lexicon.\n        negation_lex (dict): Pre-defined negation lexicon.\n    """"""\n\n    def __init__(\n        self,\n        aspect_lex: Union[str, PathLike],\n        opinion_lex: Union[str, PathLike, dict],\n        parse: bool = True,\n    ):\n        """"""Inits SentimentInference with given aspect and opinion lexicons.""""""\n        INFERENCE_OUT.mkdir(parents=True, exist_ok=True)\n        self.opinion_lex = (\n            opinion_lex if type(opinion_lex) is dict else load_opinion_lex(Path(opinion_lex))\n        )\n        self.aspect_lex = _load_aspect_lexicon(Path(aspect_lex))\n        self.intensifier_lex = _read_lexicon_from_csv(""IntensifiersLex.csv"")\n        self.negation_lex = _read_lexicon_from_csv(""NegationSentLex.csv"")\n\n        if parse:\n            from nlp_architect.pipelines.spacy_bist import SpacyBISTParser\n\n            self.parser = SpacyBISTParser(spacy_model=""en"")\n        else:\n            self.parser = None\n\n    def run(self, doc: str = None, parsed_doc: CoreNLPDoc = None) -> SentimentDoc:\n        """"""Run SentimentInference on a single document.\n\n        Returns:\n            The sentiment annotated document, which contains the detected events per sentence.\n        """"""\n        if not parsed_doc:\n            if not self.parser:\n                raise RuntimeError(""Parser not initialized (try parse=True at init )"")\n            parsed_doc = self.parser.parse(doc)\n\n        sentiment_doc = None\n        for sentence in parsed_doc.sentences:\n            events = []\n            scores = []\n            for aspect_row in self.aspect_lex:\n                _, asp_events = self._extract_event(aspect_row, sentence)\n                for asp_event in asp_events:\n                    events.append(asp_event)\n                    scores += [term.score for term in asp_event if term.type == TermType.ASPECT]\n\n            if events:\n                if not sentiment_doc:\n                    sentiment_doc = SentimentDoc(parsed_doc.doc_text)\n                sentiment_doc.sentences.append(\n                    SentimentSentence(\n                        sentence[0][""start""],\n                        sentence[-1][""start""] + sentence[-1][""len""] - 1,\n                        events,\n                    )\n                )\n        return sentiment_doc\n\n    def _extract_intensifier_terms(self, toks, sentiment_index, polarity, sentence):\n        """"""Extract intensifier events from sentence.""""""\n        count = 0\n        terms = []\n        for intens_i, intens in [(i, x) for i, x in enumerate(toks) if x in self.intensifier_lex]:\n            if math.fabs(sentiment_index - intens_i) == 1:\n                score = self.intensifier_lex[intens].score\n                terms.append(\n                    Term(\n                        intens,\n                        TermType.INTENSIFIER,\n                        polarity,\n                        score,\n                        sentence[intens_i][""start""],\n                        sentence[intens_i][""len""],\n                    )\n                )\n                count += abs(score + float(INTENSIFIER_FACTOR))\n        return count if count != 0 else 1, terms\n\n    def _extract_neg_terms(self, toks: list, op_i: int, sentence: list) -> tuple:\n        """"""Extract negation terms from sentence.\n\n        Args:\n            toks: Sentence text broken down to tokens (words).\n            op_i: Index of opinion term in sentence.\n            sentence: parsed sentence\n\n        Returns:\n            List of negation terms and its aggregated sign (positive or negative).\n        """"""\n        sign = 1\n        terms = []\n        gov_op_i = sentence[op_i][""gov""]\n        dep_op_indices = [sentence.index(x) for x in sentence if x[""gov""] == op_i]\n        for neg_i, negation in [(i, x) for i, x in enumerate(toks) if x in self.negation_lex]:\n            position = self.negation_lex[negation].position\n            dist = op_i - neg_i\n            before = position == ""before"" and (dist == 1 or neg_i in dep_op_indices)\n            after = position == ""after"" and (dist == -1 or neg_i == gov_op_i)\n            both = position == ""both"" and dist in (1, -1)\n            if before or after or both:\n                terms.append(\n                    Term(\n                        negation,\n                        TermType.NEGATION,\n                        Polarity.NEG,\n                        self.negation_lex[negation].score,\n                        sentence[toks.index(negation)][""start""],\n                        sentence[toks.index(negation)][""len""],\n                    )\n                )\n                sign *= self.negation_lex[negation].score\n        return terms, sign\n\n    def _extract_event(self, aspect_row: LexiconElement, parsed_sentence: list) -> tuple:\n        """"""Extract opinion and aspect terms from sentence.""""""\n        event = []\n        sent_aspect_pair = None\n        real_aspect_indices = _consolidate_aspects(aspect_row.term, parsed_sentence)\n        aspect_key = aspect_row.term[0]\n        for aspect_index_range in real_aspect_indices:\n            for word_index in aspect_index_range:\n                sent_aspect_pair, event = self._detect_opinion_aspect_events(\n                    word_index, parsed_sentence, aspect_key, aspect_index_range\n                )\n                if sent_aspect_pair:\n                    break\n        return sent_aspect_pair, event\n\n    @staticmethod\n    def _modify_for_multiple_word(cur_tkn, parsed_sentence, index_range):\n        """"""Modify multiple-word aspect tkn length and start index.\n\n        Args:\n            index_range: The index range of the multi-word aspect.\n        Returns:\n            The modified aspect token.\n        """"""\n        if len(index_range) >= 2:\n            cur_tkn[""start""] = parsed_sentence[index_range[0]][""start""]\n            cur_tkn[""len""] = len(parsed_sentence[index_range[0]][""text""])\n            for i in index_range[1:]:\n                cur_tkn[""len""] = int(cur_tkn[""len""]) + len(parsed_sentence[i][""text""]) + 1\n        return cur_tkn\n\n    def _detect_opinion_aspect_events(self, aspect_index, parsed_sent, aspect_key, index_range):\n        """"""Extract opinion-aspect events from sentence.\n\n        Args:\n            aspect_index: index of aspect in sentence.\n            parsed_sent: current sentence parse tree.\n            aspect_key: main aspect term serves as key in aspect dict.\n            index_range: The index range of the multi word aspect.\n\n        Returns:\n            List of aspect sentiment pair, and list of events extracted.\n        """"""\n        all_pairs, events = [], []\n        sentence_text_list = [x[""text""] for x in parsed_sent]\n        sentence_text = "" "".join(sentence_text_list)\n        for tok_i, tok in enumerate(parsed_sent):\n            aspect_op_pair = []\n            terms = []\n            gov_i = tok[""gov""]\n            gov = parsed_sent[gov_i]\n            gov_text = gov[""text""]\n            tok_text = tok[""text""]\n\n            # 1st order rules\n            # Is cur_tkn an aspect and gov an opinion?\n            if tok_i == aspect_index:\n                if gov_text.lower() in self.opinion_lex:\n                    aspect_op_pair.append(\n                        (self._modify_for_multiple_word(tok, parsed_sent, index_range), gov)\n                    )\n\n            # Is gov an aspect and cur_tkn an opinion?\n            if gov_i == aspect_index and tok_text.lower() in self.opinion_lex:\n                aspect_op_pair.append(\n                    (self._modify_for_multiple_word(gov, parsed_sent, index_range), tok)\n                )\n\n            # If not found, try 2nd order rules\n            if not aspect_op_pair and tok_i == aspect_index:\n                # 2nd order rule #1\n                for op_t in parsed_sent:\n                    if op_t[""gov""] == gov_i and op_t[""text""].lower() in self.opinion_lex:\n                        aspect_op_pair.append(\n                            (self._modify_for_multiple_word(tok, parsed_sent, index_range), op_t)\n                        )\n\n                # 2nd order rule #2\n                gov_gov = parsed_sent[parsed_sent[gov_i][""gov""]]\n                if gov_gov[""text""].lower() in self.opinion_lex:\n                    aspect_op_pair.append(\n                        (self._modify_for_multiple_word(tok, parsed_sent, index_range), gov_gov)\n                    )\n\n            # if aspect_tok found\n            for aspect, opinion in aspect_op_pair:\n                op_tok_i = parsed_sent.index(opinion)\n                score = self.opinion_lex[opinion[""text""].lower()].score\n                neg_terms, sign = self._extract_neg_terms(sentence_text_list, op_tok_i, parsed_sent)\n                polarity = Polarity.POS if score * sign > 0 else Polarity.NEG\n                intensifier_score, intensifier_terms = self._extract_intensifier_terms(\n                    sentence_text_list, op_tok_i, polarity, parsed_sent\n                )\n                over_all_score = score * sign * intensifier_score\n                terms.append(\n                    Term(\n                        aspect_key,\n                        TermType.ASPECT,\n                        polarity,\n                        over_all_score,\n                        aspect[""start""],\n                        aspect[""len""],\n                    )\n                )\n                terms.append(\n                    Term(\n                        opinion[""text""],\n                        TermType.OPINION,\n                        polarity,\n                        over_all_score,\n                        opinion[""start""],\n                        opinion[""len""],\n                    )\n                )\n                if len(neg_terms) > 0:\n                    terms = terms + neg_terms\n                if len(intensifier_terms) > 0:\n                    terms = terms + intensifier_terms\n                all_pairs.append(\n                    [aspect_key, opinion[""text""], over_all_score, polarity, sentence_text]\n                )\n                events.append(terms)\n        return all_pairs, events\n\n\ndef _sentence_contains_after(sentence, index, phrase):\n    """"""Returns sentence contains phrase after given index.""""""\n    for i in range(len(phrase)):\n        if len(sentence) <= index + i or phrase[i].lower() not in {\n            sentence[index + i][field].lower() for field in (""text"", ""lemma"")\n        }:\n            return False\n    return True\n\n\ndef _consolidate_aspects(aspect_row, sentence):\n    """"""Returns consolidated indices of aspect terms in sentence.\n\n    Args:\n        aspect_row: List of aspect terms which belong to the same aspect-group.\n    """"""\n    indices = []\n    aspect_phrases: list = sorted(\n        [phrase.split("" "") for phrase in aspect_row], key=len, reverse=True\n    )\n    appeared = set()\n    for tok_i in range(len(sentence)):\n        for aspect_phrase in aspect_phrases:\n            if _sentence_contains_after(sentence, tok_i, aspect_phrase):\n                span = range(tok_i, tok_i + len(aspect_phrase))\n                if not appeared & set(span):\n                    appeared |= set(span)\n                    indices.append(list(span))\n    return indices\n'"
nlp_architect/models/absa/train/__init__.py,0,b''
nlp_architect/models/absa/train/acquire_terms.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport copy\nimport re\nimport sys\nfrom os import PathLike\n\nfrom tqdm import tqdm\n\nfrom nlp_architect.models.absa import TRAIN_LEXICONS, LEXICONS_OUT\nfrom nlp_architect.models.absa import GENERIC_OP_LEX\nfrom nlp_architect.models.absa.inference.data_types import Polarity\nfrom nlp_architect.models.absa.train.data_types import (\n    AspectTerm,\n    DepRelation,\n    DepRelationTerm,\n    LoadOpinionStopLists,\n    LoadAspectStopLists,\n    OpinionTerm,\n    QualifiedTerm,\n)\nfrom nlp_architect.models.absa.train.rules import rule_1, rule_2, rule_3, rule_4, rule_5, rule_6\nfrom nlp_architect.models.absa.utils import (\n    _load_parsed_docs_from_dir,\n    _write_final_opinion_lex,\n    _load_lex_as_list_from_csv,\n    read_generic_lex_from_file,\n)\n\n\nclass AcquireTerms(object):\n    """"""\n    Lexicon acquisition. produce opinion lexicon and an aspect lexicon based\n    on input dataset.\n\n    Attributes:\n        opinion_candidate_list_curr_iter (dict): candidate opinion terms in the current iteration\n        opinion_candidate_list_prev_iter (dict): opinion candidates list of previous iteration\n        opinion_candidate_list (dict): opinion terms learned across all iterations\n        opinion_candidates_list_final (list): final opinion candidates list\n        opinion_candidate_list_raw (dict): all instances of candidate opinion terms\n                                           across all iterations\n        aspect_candidate_list_curr_iter (dict): candidate terms in the current iteration\n        aspects_candidate_list_prev_iter(list): Aspect candidates list of previous iteration\n        aspect_candidate_list (list):  aspect terms learned across all iterations\n        aspect_candidates_list_final (list): final aspect candidates list\n        aspect_candidate_list_raw (dict): all instances of candidate aspect terms\n                                          across all iterations\n        """"""\n\n    generic_opinion_lex_path = GENERIC_OP_LEX\n    acquired_opinion_terms_path = LEXICONS_OUT / ""generated_opinion_lex.csv""\n    acquired_aspect_terms_path = LEXICONS_OUT / ""generated_aspect_lex.csv""\n\n    GENERIC_OPINION_LEX = _load_lex_as_list_from_csv(GENERIC_OP_LEX)\n    GENERAL_ADJECTIVES_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""GeneralAdjectivesLex.csv"")\n    GENERIC_QUANTIFIERS_LEX = _load_lex_as_list_from_csv(\n        TRAIN_LEXICONS / ""GenericQuantifiersLex.csv""\n    )\n    GEOGRAPHICAL_ADJECTIVES_LEX = _load_lex_as_list_from_csv(\n        TRAIN_LEXICONS / ""GeographicalAdjectivesLex.csv""\n    )\n    INTENSIFIERS_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""IntensifiersLex.csv"")\n    TIME_ADJECTIVE_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""TimeAdjectiveLex.csv"")\n    ORDINAL_NUMBERS_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""OrdinalNumbersLex.csv"")\n    PREPOSITIONS_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""PrepositionsLex.csv"")\n    PRONOUNS_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""PronounsLex.csv"")\n    COLORS_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""ColorsLex.csv"")\n    DETERMINERS_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""DeterminersLex.csv"")\n    NEGATION_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""NegationLex.csv"")\n    AUXILIARIES_LEX = _load_lex_as_list_from_csv(TRAIN_LEXICONS / ""AuxiliariesLex.csv"")\n\n    OPINION_STOP_LIST = LoadOpinionStopLists(\n        DETERMINERS_LEX,\n        GENERAL_ADJECTIVES_LEX,\n        GENERIC_QUANTIFIERS_LEX,\n        GEOGRAPHICAL_ADJECTIVES_LEX,\n        INTENSIFIERS_LEX,\n        TIME_ADJECTIVE_LEX,\n        ORDINAL_NUMBERS_LEX,\n        PREPOSITIONS_LEX,\n        COLORS_LEX,\n        NEGATION_LEX,\n    )\n\n    ASPECT_STOP_LIST = LoadAspectStopLists(\n        GENERIC_OPINION_LEX,\n        DETERMINERS_LEX,\n        GENERAL_ADJECTIVES_LEX,\n        GENERIC_QUANTIFIERS_LEX,\n        GEOGRAPHICAL_ADJECTIVES_LEX,\n        INTENSIFIERS_LEX,\n        TIME_ADJECTIVE_LEX,\n        ORDINAL_NUMBERS_LEX,\n        PREPOSITIONS_LEX,\n        PRONOUNS_LEX,\n        COLORS_LEX,\n        NEGATION_LEX,\n        AUXILIARIES_LEX,\n    )\n\n    FILTER_PATTERNS = [re.compile(r"".*\\d+.*"")]\n    FLOAT_FORMAT = ""{0:.3g}""\n    # maximum number of iterations\n    NUM_OF_SENTENCES_PER_OPINION_AND_ASPECT_TERM_INC = 35000\n\n    def __init__(self, asp_thresh=3, op_thresh=2, max_iter=1):\n        self.opinion_candidate_list_prev_iter = read_generic_lex_from_file(\n            AcquireTerms.generic_opinion_lex_path\n        )\n        self.generic_sent_dict = copy.deepcopy(self.opinion_candidate_list_prev_iter)\n        self.opinion_candidate_list = {}\n        self.opinion_candidate_list_raw = {}\n        self.opinion_candidate_list_curr_iter = {}\n        self.opinion_candidates_list_final = []\n        self.aspect_candidate_list_raw = {}\n        self.aspect_candidate_list = list()\n        self.aspect_candidate_list_curr_iter = {}\n        self.aspect_candidates_list_final = []\n        self.init_aspect_dict = list()\n        self.aspects_candidate_list_prev_iter = list()\n        self.min_freq_aspect_candidate = asp_thresh\n        self.min_freq_opinion_candidate = op_thresh\n        self.max_num_of_iterations = max_iter\n\n    def extract_terms_from_doc(self, parsed_doc):\n        """"""Extract candidate terms for sentences in parsed document.\n\n        Args:\n            parsed_doc (ParsedDocument): Input parsed document.\n        """"""\n        for text, parsed_sent in parsed_doc.sent_iter():\n            relations = _get_rel_list(parsed_sent)\n\n            for rel_entry in relations:\n                if rel_entry.rel != ""root"":\n                    gov_seen = self.opinion_candidate_list_prev_iter.get(rel_entry.gov.text)\n                    dep_seen = self.opinion_candidate_list_prev_iter.get(rel_entry.dep.text)\n                    opinions = []\n                    aspects = []\n\n                    # =========================== acquisition rules ==============================\n\n                    if bool(gov_seen) ^ bool(dep_seen):\n                        opinions.append(rule_1(rel_entry, gov_seen, dep_seen, text))\n\n                    if not gov_seen and dep_seen:\n                        opinions.append(rule_2(rel_entry, relations, dep_seen, text))\n\n                        aspects.append(rule_3(rel_entry, relations, text))\n\n                        aspects.append(rule_4(rel_entry, relations, text))\n\n                    if (\n                        self.aspects_candidate_list_prev_iter\n                        and AspectTerm.from_token(rel_entry.gov)\n                        in self.aspects_candidate_list_prev_iter\n                        and AspectTerm.from_token(rel_entry.dep)\n                        not in self.aspects_candidate_list_prev_iter\n                    ):\n                        opinions.append(rule_5(rel_entry, text))\n                        aspects.append(rule_6(rel_entry, relations, text))\n\n                    self._add_opinion_term(opinions)\n                    self._add_aspect_term(aspects)\n\n    def extract_opinion_and_aspect_terms(self, parsed_document_iter, num_of_docs):\n        """"""Extract candidate terms from parsed document iterator.\n\n        Args:\n            parsed_document_iter (Iterator): Parsed document iterator.\n            num_of_docs (int): number of documents on iterator.\n        """"""\n\n        for parsed_document in tqdm(parsed_document_iter, total=num_of_docs, file=sys.stdout):\n            self.extract_terms_from_doc(parsed_document)\n\n    def _is_valid_term(self, cand_term):\n        """"""Validates a candidate term.\n\n        Args:\n            cand_term (CandidateTerm): candidate terms list.\n        """"""\n        term = str(cand_term)\n        for pattern in self.FILTER_PATTERNS:\n            if pattern.match(term):\n                return False\n        if self.OPINION_STOP_LIST.is_in_stop_list(term):\n            return False\n        if term.lower() != term and term.upper() != term:\n            return False\n        return True\n\n    def _add_aspect_term(self, terms):\n        """"""\n        add new aspect term to table.\n        Args:\n            terms (list of CandidateTerm): candidate terms list\n        """"""\n        for term in terms:\n            if term:\n                term_entry = AspectTerm(term.term, term.pos, term.lemma)\n                if (\n                    term_entry not in self.init_aspect_dict\n                    and term_entry not in self.aspect_candidate_list\n                    and not self.ASPECT_STOP_LIST.is_in_stop_list(term.term[0])\n                    and len(term.term[0]) > 1\n                ):\n                    _insert_new_term_to_table(term, self.aspect_candidate_list_curr_iter)\n\n        return True\n\n    def _add_opinion_term(self, terms):\n        """"""\n        Add new opinion term to table\n        Args:\n            terms (list of CandidateTerm): candidate term\n        """"""\n        for term in terms:\n            if term and self._is_valid_term(term):\n                if str(term.term[0]) not in self.generic_sent_dict.keys():\n                    if str(term.term[0]) not in self.opinion_candidate_list:\n                        if len(str(term.term[0])) > 1:\n                            if any(c.isalnum() for c in str(term.term[0])):\n                                _insert_new_term_to_table(\n                                    term, self.opinion_candidate_list_curr_iter\n                                )\n\n    def _insert_new_terms_to_tables(self):\n        """"""\n        Insert new terms to tables\n        clear candidates lists from previous iteration\n\n        """"""\n        self.opinion_candidate_list_prev_iter = {}\n        self.opinion_candidate_list_raw = _merge_tables(\n            self.opinion_candidate_list_raw, self.opinion_candidate_list_curr_iter\n        )\n        for cand_term_list in self.opinion_candidate_list_curr_iter.values():\n            if len(cand_term_list) >= self.min_freq_opinion_candidate:\n                new_opinion_term = _set_opinion_term_polarity(cand_term_list)\n                self.opinion_candidate_list_prev_iter[str(new_opinion_term)] = new_opinion_term\n        self.opinion_candidate_list_curr_iter = {}\n        self.opinion_candidate_list = {\n            **self.opinion_candidate_list,\n            **self.opinion_candidate_list_prev_iter,\n        }\n        self.aspects_candidate_list_prev_iter = list()\n        self.aspect_candidate_list_raw = _merge_tables(\n            self.aspect_candidate_list_raw, self.aspect_candidate_list_curr_iter\n        )\n        for extracted_aspect_list in self.aspect_candidate_list_curr_iter.values():\n            if len(extracted_aspect_list) >= self.min_freq_aspect_candidate:\n                first = extracted_aspect_list[0]\n                new_aspect_entry = AspectTerm(first.term, first.pos, first.lemma)\n                if new_aspect_entry not in self.aspects_candidate_list_prev_iter:\n                    self.aspects_candidate_list_prev_iter.append(new_aspect_entry)\n        self.aspect_candidate_list_curr_iter = {}\n        self.aspect_candidate_list = (\n            self.aspect_candidate_list + self.aspects_candidate_list_prev_iter\n        )\n\n    def _write_candidate_opinion_lex(self):\n        """"""\n        write generated lexicons to csv files\n        """"""\n        LEXICONS_OUT.mkdir(parents=True, exist_ok=True)\n\n        _write_final_opinion_lex(\n            self.opinion_candidates_list_final, self.acquired_opinion_terms_path\n        )\n\n    def acquire_lexicons(self, parsed_dir: str or PathLike):\n        """"""Acquire new opinion and aspect lexicons.\n\n        Args:\n            parsed_dir (PathLike): Path to parsed documents folder.\n        """"""\n        parsed_docs = _load_parsed_docs_from_dir(parsed_dir)\n        dataset_sentence_len = 0\n        for parsed_doc in parsed_docs.values():\n            dataset_sentence_len += len(parsed_doc.sentences)\n\n        add_to_thresholds = int(\n            dataset_sentence_len / self.NUM_OF_SENTENCES_PER_OPINION_AND_ASPECT_TERM_INC\n        )\n        self.min_freq_opinion_candidate += add_to_thresholds\n        self.min_freq_aspect_candidate += add_to_thresholds\n\n        for iteration_num in range(self.max_num_of_iterations):\n            if (\n                len(self.opinion_candidate_list_prev_iter) == 0\n                and len(self.aspects_candidate_list_prev_iter) == 0\n            ):\n                break\n\n            print(""\\n#Iteration: {}"".format(iteration_num + 1))\n\n            self.extract_opinion_and_aspect_terms(iter(parsed_docs.values()), len(parsed_docs))\n\n            self._insert_new_terms_to_tables()\n\n        self.opinion_candidates_list_final = generate_final_opinion_candidates_list(\n            self.opinion_candidate_list_raw,\n            self.opinion_candidates_list_final,\n            self.min_freq_opinion_candidate,\n        )\n        self.aspect_candidates_list_final = _generate_final_aspect_candidates_list(\n            self.aspect_candidate_list_raw,\n            self.aspect_candidates_list_final,\n            self.min_freq_aspect_candidate,\n        )\n\n        self._write_candidate_opinion_lex()\n\n        aspect_dict = _add_lemmas_aspect_lex(self.aspect_candidates_list_final)\n\n        return aspect_dict\n\n\ndef _add_lemmas_aspect_lex(aspect_candidates_list_final):\n\n    aspect_dict = {}\n    for cand_term in aspect_candidates_list_final:\n        lemma = """"\n        if cand_term.term[0] != cand_term.lemma[0]:\n            lemma = cand_term.lemma[0]\n        aspect_dict[cand_term.term[0]] = lemma\n\n    # unify aspect with aspect lemmas\n    lemma_to_erase = []\n    for _, lemma in aspect_dict.items():\n        if lemma != """" and lemma in aspect_dict:\n            lemma_to_erase.append(lemma)\n\n    # delete all duplicates (aspects that are lemmas of other aspects)\n    for lemma in lemma_to_erase:\n        if lemma in aspect_dict:\n            del aspect_dict[lemma]\n    return aspect_dict\n\n\ndef _get_rel_list(parsed_sentence):\n    res = []\n    gen_toks = []\n    for tok in parsed_sentence:\n        gen_toks.append(\n            DepRelationTerm(tok[""text""], tok[""lemma""], tok[""pos""], tok[""ner""], tok[""start""])\n        )\n\n    for gen_tok, tok in zip(gen_toks, parsed_sentence):\n        gov_idx = tok[""gov""]\n        if gov_idx != -1:\n            res.append(DepRelation(gen_toks[gov_idx], gen_tok, tok[""rel""]))\n    return res\n\n\ndef _merge_tables(d1, d2):\n    """"""\n    Merge dictionaries\n    Args:\n        d1 (dict): first dict to merge\n        d2 (dict): second dict to merge\n    """"""\n    for key, l in d2.items():\n        if key in d1:\n            for item in l:\n                if item not in d1[key]:\n                    d1[key].append(item)\n        else:\n            d1[key] = l\n    return d1\n\n\ndef _insert_new_term_to_table(term, curr_table):\n    """"""\n    Insert term to table of lists.\n    Args:\n        term (term): term to be inserted\n        curr_table (dict): input table\n    """"""\n    table_key_word = str(term)\n    if table_key_word:\n        if table_key_word in curr_table and term not in curr_table[table_key_word]:\n            curr_table[table_key_word].append(term)\n        else:\n            curr_table[table_key_word] = [term]\n\n\ndef _set_opinion_term_polarity(terms_list):\n    """"""Set opinion term polarity.\n\n    Args:\n        terms_list (list): list of opinion terms\n    """"""\n    first = terms_list[0]\n    new_term = first.term\n\n    positive_pol = 0\n    negative_pol = 0\n    pol = None\n    for term in terms_list:\n        try:\n            pol = term.term_polarity\n        except Exception as e:\n            print(""extracted_term missing term_polarity: "" + str(e))\n        if pol is not None:\n            if pol == Polarity.POS:\n                positive_pol = positive_pol + 1\n            if pol == Polarity.NEG:\n                negative_pol = negative_pol + 1\n    new_term_polarity = Polarity.UNK\n    if positive_pol >= negative_pol and positive_pol > 0:\n        new_term_polarity = Polarity.POS\n    elif negative_pol >= positive_pol and negative_pol > 0:\n        new_term_polarity = Polarity.NEG\n\n    return OpinionTerm(new_term, new_term_polarity)\n\n\ndef _generate_final_aspect_candidates_list(\n    aspect_candidate_list_raw, final_aspect_candidates_list, frequency_threshold\n):\n    """"""\n    generate final aspect candidates list from map\n    Args:\n        aspect_candidate_list_raw (dict): key = term, value =\n        lists of candidate terms.\n        final_aspect_candidates_list (list): list of final aspect candidates\n        frequency_threshold (int): minimum freq. for qualifying term\n    """"""\n    term_polarity = Polarity.UNK\n    for extracted_term_list in aspect_candidate_list_raw.values():\n        if len(extracted_term_list) >= frequency_threshold:\n            term = extracted_term_list[0]\n            qualified_term = QualifiedTerm(\n                term.term, term.lemma, term.pos, len(extracted_term_list), term_polarity\n            )\n            final_aspect_candidates_list.append(qualified_term)\n\n    return final_aspect_candidates_list\n\n\ndef generate_final_opinion_candidates_list(\n    opinion_candidate_list_raw, final_opinion_candidates_list, frequency_threshold\n):\n    """"""\n    generate final opinion candidates list from raw opinion candidate list\n    Args:\n        opinion_candidate_list_raw (dict): key = term, value =\n        lists of extracted terms.\n        final_opinion_candidates_list (list): list of final opinion candidates\n        frequency_threshold (int): minimum freq. for qualifying term\n    """"""\n    for candidate_list in opinion_candidate_list_raw.values():\n        positive_pol = 0\n        negative_pol = 0\n        if len(candidate_list) >= frequency_threshold:\n            for candidate in candidate_list:\n                pol = candidate.term_polarity\n                if pol is not None:\n                    if pol == Polarity.POS:\n                        positive_pol = positive_pol + 1\n                    if pol == Polarity.NEG:\n                        negative_pol = negative_pol + 1\n\n            term_polarity = Polarity.UNK\n            if positive_pol > negative_pol and positive_pol > 0:\n                term_polarity = Polarity.POS\n            elif negative_pol >= positive_pol and negative_pol > 0:\n                term_polarity = Polarity.NEG\n\n            term = candidate_list[0]\n\n            qualified_term = QualifiedTerm(\n                term.term, term.term, term.pos, len(candidate_list), term_polarity\n            )\n            final_opinion_candidates_list.append(qualified_term)\n\n    return final_opinion_candidates_list\n'"
nlp_architect/models/absa/train/data_types.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport csv\nfrom enum import Enum\nfrom os import PathLike\n\nfrom nlp_architect.models.absa import TRAIN_LEXICONS\n\n\nclass OpinionTerm:\n    """"""Opinion term.\n\n    Attributes:\n       terms (list): list of opinion term\n        polarity (Polarity): polarity of the sentiment\n    """"""\n\n    def __init__(self, terms, polarity):\n        self.terms = terms\n        self.polarity = polarity\n\n    def __str__(self):\n        return "" "".join(self.terms)\n\n\nclass AspectTerm(object):\n    """"""Aspect term.\n\n    Attributes:\n        terms (list): list of terms\n        pos (list): list of pos\n    """"""\n\n    def __init__(self, terms, pos, lemmas):\n        """"""\n        Args:\n            terms (list): list of terms\n            pos (list): list of pos\n        """"""\n        self.terms = terms\n        self.lemmas = lemmas\n        self.pos = pos\n\n    def __str__(self):\n        return "" "".join(self.terms)\n\n    def __eq__(self, other):\n        """"""\n        Override the default equals behavior.\n        """"""\n        return self.terms == other.terms and self.pos == other.pos\n\n    @staticmethod\n    def from_token(token):\n        return AspectTerm([token.text], [token.norm_pos], [token.lemma])\n\n\nclass CandidateTerm(object):\n    """"""Candidate opinion term or aspect term.\n\n    Attributes:\n        term (list): list of terms\n        pos (list): list of pos\n        source_term (list): list of related anchor terms\n        sentence (str): sentence text this term\n        term_polarity (int): term polarity\n    """"""\n\n    def __init__(self, term_a, term_b, sent_text, candidate_term_polarity):\n        """"""\n        Args:\n            term_a (DepRelationTerm): first term\n            term_b (DepRelationTerm): second term\n            sent_text (str): sentence text\n            candidate_term_polarity (Polarity): term polarity\n        """"""\n        self.term = [term_a.text]\n        self.pos = [term_a.norm_pos]\n        self.lemma = [term_a.lemma]\n        self.source_term = [term_b.text]\n        self.sentence = sent_text\n        self.term_polarity = candidate_term_polarity\n\n    def __str__(self):\n        return "" "".join(self.term)\n\n    def __eq__(self, other):\n        if other is None or self.__class__ != other.__class__:\n            return False\n        if self.term != other.term if self.term is not None else other.term is not None:\n            return False\n        if (\n            self.source_term != other.source_term\n            if self.source_term is not None\n            else other.source_term is not None\n        ):\n            return False\n        return (\n            self.sentence == other.sentence if self.sentence is not None else other.sentence is None\n        )\n\n    def __ne__(self, other):\n        return not self == other\n\n\nclass DepRelation(object):\n    """"""Generic Relation Entry contains the governor, it\'s dependent and the relation between them.\n\n    Attributes:\n        gov (DepRelationTerm): governor\n        dep (DepRelationTerm): dependent\n        rel (str): relation type between governor and dependent\n    """"""\n\n    def __init__(self, gov=None, dep=None, rel=None):\n        self.gov = gov\n        self.dep = dep\n        rel_split = rel.split("":"")\n        self.rel = rel_split[0]\n        self.subtype = rel_split[1] if len(rel_split) > 1 else None\n\n\nclass RelCategory(Enum):\n    SUBJ = {""nsubj"", ""nsubjpass"", ""csubj"", ""csubjpass""}\n    MOD = {""amod"", ""acl"", ""advcl"", ""appos"", ""neg"", ""nmod""}\n    OBJ = {""dobj"", ""iobj""}\n\n\nclass DepRelationTerm(object):\n    """"""\n    Attributes:\n        text (str, optional): token text\n        lemma (str, optional): token lemma\n        pos (str, optional): token pos\n        ner (str, optional): token ner\n        idx (int, optional): token start index (within the sentence)\n    """"""\n\n    def __init__(self, text=None, lemma=None, pos=None, ner=None, idx=None):\n        self.text = text\n        self.lemma = lemma\n        self.pos = pos\n        self.ner = ner\n        self.idx = idx\n        self.dep_rel_list = []\n        self.gov = None\n\n    @property\n    def norm_pos(self):\n        return normalize_pos(self.text, self.pos)\n\n\nclass QualifiedTerm(object):\n    """"""Qualified term - term that is accepted to generated lexicon.\n\n    Attributes:\n        term (list): list of terms\n        pos (list): list of pos.\n        frequency (int): frequency of filtered term in corpus.\n        term_polarity (Polarity): term polarity.\n\n    """"""\n\n    def __init__(self, term, lemma, pos, frequency, term_polarity):\n        self.term = term\n        self.lemma = lemma\n        self.pos = pos\n        self.frequency = frequency\n        self.term_polarity = term_polarity\n\n    def as_string_list(self):\n        return ["" "".join(self.term), str(self.frequency), self.term_polarity.name]\n\n    def as_string_list_aspect(self):\n        return ["" "".join(self.term)]\n\n    def as_string_list_aspect_debug(self):\n        return [str(self.frequency), "" "".join(self.term), "" "".join(self.lemma)]\n\n\ndef load_lex_as_dict_from_csv(file_name: str or PathLike):\n    """"""Read lexicon as dictionary, key = term, value = pos.\n\n    Args:\n        file_name: the csv file name\n    """"""\n    lexicon_map = {}\n    with open(file_name, encoding=""utf-8"") as f:\n        reader = csv.DictReader(f, skipinitialspace=True)\n        if reader is None:\n            print(""file name is None"")\n            return lexicon_map\n        next(reader)\n        for row in reader:\n            term = row[""Term""]\n            pos = row[""POS subtype""]\n\n            lexicon_map[term] = pos\n    return lexicon_map\n\n\nclass POS(Enum):\n    """"""Part-of-speech labels.""""""\n\n    ADJ = 1\n    ADV = 2\n    AUX = 3\n    AUX_PAST = 3\n    CONJ = 4\n    NUM = 5\n    DET = 6\n    EX = 7\n    FW = 8\n    IN = 9\n    PREP = 10\n    LS = 11\n    MD = 12\n    MD_CERTAIN = 13\n    NN = 14\n    PROPER_NAME = 15\n    POS = 16\n    PRON = 17\n    PRON_1_S = 18\n    PRON_1_P = 19\n    PRON_2_S = 20\n    PRON_3_S = 21\n    PRON_3_P = 22\n    PRON_4_S = 23\n    POSSPRON_1_S = 24\n    POSSPRON_1_P = 25\n    POSSPRON_2_S = 26\n    POSSPRON_2_P = 27\n    POSSPRON_3_S = 28\n    POSSPRON_3_P = 29\n    POSSPRON_4_S = 30\n    POSSPRON_4_P = 31\n    RP = 32\n    SYM = 33\n    TO = 34\n    INTERJ = 35\n    VB = 36\n    VB_PAST = 37\n    VB_PRESENT = 38\n    VBG = 39\n    VBN = 40\n    WH_DET = 41\n    WH_PROP = 42\n    WH_ADV = 43\n    PUNCT = 44\n    OTHER = 45\n\n\nPRONOUNS_LIST = load_lex_as_dict_from_csv(TRAIN_LEXICONS / ""PronounsLex.csv"")\n\n\ndef normalize_pos(word, in_pos):\n    if in_pos is None:\n        return POS.OTHER\n    if word.lower() in PRONOUNS_LIST and in_pos.startswith(""PR""):\n        return POS[PRONOUNS_LIST[word.lower()]]\n    if in_pos == ""CC"":\n        return POS.CONJ\n    if in_pos == ""CD"":\n        return POS.NUM\n    if in_pos == ""DT"":\n        return POS.DET\n    if in_pos == ""EX"":\n        return POS.EX\n    if in_pos == ""FW"":\n        return POS.FW\n    if in_pos == ""IN"":\n        return POS.PREP\n    if in_pos == ""TO"":\n        return POS.PREP\n    if in_pos.startswith(""JJ""):\n        return POS.ADJ\n    if in_pos == ""LS"":\n        return POS.LS\n    if in_pos == ""MD"":\n        return POS.MD\n    if in_pos.startswith(""NN""):\n        return POS.NN\n    if in_pos == ""PDT"":\n        return POS.DET\n    if in_pos == ""POS"":\n        return POS.POS\n    if in_pos.startswith(""PR""):\n        return POS.PRON\n    if in_pos.startswith(""RB""):\n        return POS.ADV\n    if in_pos == ""RP"":\n        return POS.RP\n    if in_pos == ""SYM"":\n        return POS.SYM\n    if in_pos == ""UH"":\n        return POS.INTERJ\n    if in_pos.startswith(""VB""):\n        return POS.VB\n    if in_pos == ""WDT"":\n        return POS.WH_DET\n    if in_pos.startswith(""WP""):\n        return POS.WH_PROP\n    if in_pos == ""WRB"":\n        return POS.WH_ADV\n    return POS.OTHER\n\n\nclass LoadAspectStopLists(object):\n    """"""A Filter holding all generic and general lexicons, can verify if a given term is contained\n     in one of the lexicons - hence belongs to one of the generic / general lexicons or is a valid\n     term.\n\n    Attributes:\n        generic_opinion_lex (dict): generic opinion lexicon\n        determiners_lex (dict): determiners lexicon\n        general_adjectives_lex (dict): general adjectives lexicon\n        generic_quantifiers_lex (dict): generic quantifiers lexicon\n        geographical_adjectives_lex (dict): geographical adjectives lexicon\n        intensifiers_lex (dict): intensifiers lexicon\n        time_adjective_lex (dict): time adjective lexicon\n        ordinal_numbers_lex (dict): ordinal numbers lexicon\n        prepositions_lex (dict): prepositions lexicon\n        pronouns_lex (dict): pronouns lexicon\n        colors_lex (dict): colors lexicon\n        negation_lex (dict): negation terms lexicon\n    """"""\n\n    def __init__(\n        self,\n        generic_opinion_lex,\n        determiners_lex,\n        general_adjectives_lex,\n        generic_quantifiers_lex,\n        geographical_adjectives_lex,\n        intensifiers_lex,\n        time_adjective_lex,\n        ordinal_numbers_lex,\n        prepositions_lex,\n        pronouns_lex,\n        colors_lex,\n        negation_lex,\n        auxiliaries_lex,\n    ):\n        self.generic_opinion_lex = generic_opinion_lex\n        self.determiners_lex = determiners_lex\n        self.general_adjectives_lex = general_adjectives_lex\n        self.generic_quantifiers_lex = generic_quantifiers_lex\n        self.geographical_adjectives_lex = geographical_adjectives_lex\n        self.intensifiers_lex = intensifiers_lex\n        self.time_adjective_lex = time_adjective_lex\n        self.ordinal_numbers_lex = ordinal_numbers_lex\n        self.prepositions_lex = prepositions_lex\n        self.pronouns_lex = pronouns_lex\n        self.colors_lex = colors_lex\n        self.negation_lex = negation_lex\n        self.auxiliaries_lex = auxiliaries_lex\n\n    def is_in_stop_list(self, term):\n        return any(term in lexicon for lexicon in self.__dict__.values())\n\n\nclass LoadOpinionStopLists(object):\n    """"""A Filter holding all generic and general lexicons, can verify if a given term is contained\n     in one of the lexicons - hence belongs to one of the generic / general lexicons or is a valid\n     term.\n\n    Attributes:\n        determiners_lex (dict): determiners lexicon\n        general_adjectives_lex (dict): general adjectives lexicon\n        generic_quantifiers_lex (dict): generic quantifiers lexicon\n        geographical_adjectives_lex (dict): geographical adjectives lexicon\n        intensifiers_lex (dict): intensifiers lexicon\n        time_adjective_lex (dict): time adjective lexicon\n        ordinal_numbers_lex (dict): ordinal numbers lexicon\n        prepositions_lex (dict): prepositions lexicon\n        colors_lex (dict): colors lexicon\n        negation_lex (dict): negation terms lexicon\n    """"""\n\n    def __init__(\n        self,\n        determiners_lex,\n        general_adjectives_lex,\n        generic_quantifiers_lex,\n        geographical_adjectives_lex,\n        intensifiers_lex,\n        time_adjective_lex,\n        ordinal_numbers_lex,\n        prepositions_lex,\n        colors_lex,\n        negation_lex,\n    ):\n        self.determiners_lex = determiners_lex\n        self.general_adjectives_lex = general_adjectives_lex\n        self.generic_quantifiers_lex = generic_quantifiers_lex\n        self.geographical_adjectives_lex = geographical_adjectives_lex\n        self.intensifiers_lex = intensifiers_lex\n        self.time_adjective_lex = time_adjective_lex\n        self.ordinal_numbers_lex = ordinal_numbers_lex\n        self.prepositions_lex = prepositions_lex\n        self.colors_lex = colors_lex\n        self.negation_lex = negation_lex\n\n    def is_in_stop_list(self, term):\n        return any(term in lexicon for lexicon in self.__dict__.values())\n'"
nlp_architect/models/absa/train/generate_lexicons.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom nlp_architect.models.absa.inference.data_types import Polarity\nfrom nlp_architect.models.absa.train.data_types import OpinionTerm, QualifiedTerm\n\n\ndef set_opinion_term_polarity(terms_list):\n    """"""Set opinion term polarity.\n\n    Args:\n        terms_list (list): list of opinion terms\n    """"""\n    first = terms_list[0]\n    new_term = first.term\n\n    positive_pol = 0\n    negative_pol = 0\n    curr_polarity = None\n    for term in terms_list:\n        try:\n            curr_polarity = term.term_polarity\n        except Exception as e:\n            print(""extracted_term missing term_polarity: "" + str(e))\n        if curr_polarity is not None:\n            if curr_polarity == Polarity.POS:\n                positive_pol = positive_pol + 1\n            if curr_polarity == Polarity.NEG:\n                negative_pol = negative_pol + 1\n    new_term_polarity = Polarity.UNK\n    if positive_pol >= negative_pol and positive_pol > 0:\n        new_term_polarity = Polarity.POS\n    elif negative_pol >= positive_pol and negative_pol > 0:\n        new_term_polarity = Polarity.NEG\n\n    return OpinionTerm(new_term, new_term_polarity)\n\n\ndef generate_final_aspect_candidates_list(\n    aspect_candidate_list_raw, final_aspect_candidates_list, frequency_threshold\n):\n    """"""Generate final aspect candidates list from map.\n\n    Args:\n        aspect_candidate_list_raw (dict): key = term, value =\n        lists of candidate terms.\n        final_aspect_candidates_list (list): list of final aspect candidates\n        frequency_threshold (int): minimum freq. for qualifying term\n    """"""\n    term_polarity = Polarity.UNK\n    for extracted_term_list in aspect_candidate_list_raw.values():\n        if len(extracted_term_list) >= frequency_threshold:\n            term = extracted_term_list[0]\n            qualified_term = QualifiedTerm(\n                term.term, term.lemma, term.pos, len(extracted_term_list), term_polarity\n            )\n            final_aspect_candidates_list.append(qualified_term)\n    return final_aspect_candidates_list\n\n\ndef generate_final_opinion_candidates_list(\n    opinion_candidate_list_raw, final_opinion_candidates_list, frequency_threshold\n):\n    """"""Generate final opinion candidates list from raw opinion candidate list.\n\n    Args:\n        opinion_candidate_list_raw (dict): key = term, value =\n        lists of extracted terms.\n        final_opinion_candidates_list (list): list of final opinion candidates\n        frequency_threshold (int): minimum freq. for qualifying term\n    """"""\n    for extracted_term_list in opinion_candidate_list_raw.values():\n        positive_pol = 0\n        negative_pol = 0\n        if len(extracted_term_list) >= frequency_threshold:\n            for ex_term in extracted_term_list:\n                curr_polarity = ex_term.term_polarity\n                if curr_polarity is not None:\n                    if curr_polarity == Polarity.POS:\n                        positive_pol = positive_pol + 1\n                    if curr_polarity == Polarity.NEG:\n                        negative_pol = negative_pol + 1\n\n            # set polarity according majority vote\n            term_polarity = Polarity.UNK\n            if positive_pol > negative_pol and positive_pol > 0:\n                term_polarity = Polarity.POS\n            elif negative_pol >= positive_pol and negative_pol > 0:\n                term_polarity = Polarity.NEG\n\n            term = extracted_term_list[0]\n            qualified_term = QualifiedTerm(\n                term.term, term.term, term.pos, len(extracted_term_list), term_polarity\n            )\n            final_opinion_candidates_list.append(qualified_term)\n    return final_opinion_candidates_list\n'"
nlp_architect/models/absa/train/rerank_terms.py,0,"b'# ******************************************************************************\r\n# Copyright 2017-2018 Intel Corporation\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ******************************************************************************\r\nimport csv\r\nimport pickle\r\nimport numpy as np\r\nimport tensorflow\r\nfrom os import PathLike\r\nfrom pathlib import Path\r\n\r\nfrom nlp_architect.models.absa.utils import _read_generic_lex_for_similarity\r\nfrom nlp_architect.models.absa import TRAIN_OUT, TRAIN_LEXICONS, GENERIC_OP_LEX, LEXICONS_OUT\r\n\r\nfrom scipy.spatial.distance import cosine\r\nfrom sklearn.model_selection import StratifiedKFold\r\n\r\n# pylint: disable=import-error\r\nfrom tensorflow.keras.layers import Dense, Dropout\r\nfrom tensorflow.keras.models import Sequential, load_model\r\n\r\nfrom nlp_architect.utils.embedding import load_word_embeddings\r\n\r\n\r\nclass RerankTerms(object):\r\n    model_dir = TRAIN_OUT / ""reranking_model""\r\n    train_rerank_data_path = TRAIN_LEXICONS / ""RerankTrainingData.csv""\r\n    PREDICTION_THRESHOLD = 0.7\r\n\r\n    def __init__(\r\n        self, vector_cache=True, rerank_model: PathLike = None, emb_model_path: PathLike = None\r\n    ):\r\n        # model and training params\r\n        self.embeddings_len = 300\r\n        self.activation_1 = ""relu""\r\n        self.activation_2 = ""relu""\r\n        self.activation_3 = ""sigmoid""\r\n        self.loss = ""binary_crossentropy""\r\n        self.optimizer = ""rmsprop""\r\n\r\n        self.epochs_and_batch_size = [(10, 2)]\r\n        self.seeds = [3]\r\n        self.threshold = 0.5\r\n\r\n        self.sim_lexicon = TRAIN_LEXICONS / ""RerankSentSimLex.csv""\r\n        self.generic_lexicon = GENERIC_OP_LEX\r\n\r\n        self.vector_cache = vector_cache\r\n        self.word_vectors_dict = {}\r\n        self.vectors_sim_dict = {}\r\n\r\n        self.rerank_model_path = rerank_model\r\n        self.emb_model_path = emb_model_path\r\n\r\n        LEXICONS_OUT.mkdir(parents=True, exist_ok=True)\r\n\r\n        tensorflow.logging.set_verbosity(tensorflow.logging.ERROR)\r\n\r\n    def calc_cosine_similarity(self, word_1, word_2, embedding_dict):\r\n        """"""\r\n        calculate cosine similarity scores between 2 terms\r\n\r\n        Args:\r\n            word_1 (str): 1st input word\r\n            word_2 (str): 2nd input word\r\n            embedding_dict (dict): embedding dictionary\r\n\r\n        Returns:\r\n            vectors_sim_dict[key] (float): similarity scores between the 2 input words\r\n\r\n        """"""\r\n        key = str(sorted([word_1, word_2]))\r\n\r\n        if not self.vector_cache or key not in self.vectors_sim_dict:\r\n            vector_1 = embedding_dict.get(word_1)\r\n            vector_2 = embedding_dict.get(word_2)\r\n\r\n            # check if both words have vectors\r\n            if np.count_nonzero(vector_1) > 0 and np.count_nonzero(vector_2) > 0:\r\n                sim_score = cosine(vector_1, vector_2)\r\n            else:\r\n                sim_score = None\r\n            self.vectors_sim_dict[key] = sim_score\r\n\r\n        return self.vectors_sim_dict[key]\r\n\r\n    def calc_similarity_scores_for_all_terms(self, terms, generic_terms, embedding_dict):\r\n        """"""\r\n        calculate similarity scores between each term and each off the generic terms\r\n\r\n        Args:\r\n            terms: candidate terms\r\n            generic_terms: generic opinion terms\r\n            embedding_dict: embedding dictionary\r\n\r\n        Returns:\r\n            neg_all: similarity scores between each cand term and neg generic term\r\n            pos_all: similarity scores between each cand term and pos generic term\r\n\r\n        """"""\r\n        print(""\\nComputing similarity scores...\\n"")\r\n\r\n        neg_all = []\r\n        pos_all = []\r\n\r\n        for term in terms:\r\n            polarity_sim_dic = {""NEG"": [], ""POS"": []}\r\n            for generic_term, polarity in generic_terms.items():\r\n\r\n                sim_score = self.calc_cosine_similarity(term, generic_term, embedding_dict)\r\n\r\n                if sim_score is not None:\r\n                    polarity_sim_dic[polarity].append(sim_score)\r\n                else:\r\n                    polarity_sim_dic[polarity].append(float(0))\r\n\r\n            neg_all.append(polarity_sim_dic[""NEG""])\r\n            pos_all.append(polarity_sim_dic[""POS""])\r\n\r\n        return neg_all, pos_all\r\n\r\n    @staticmethod\r\n    def load_terms_and_polarities(filename):\r\n        """"""\r\n        load terms and polarities from file\r\n\r\n        Args:\r\n            filename: feature table file full path\r\n\r\n        Returns:\r\n            terms: candidate terms\r\n            polarities: opinion polarity per term\r\n\r\n        """"""\r\n        print(""Loading training data from {} ..."".format(filename))\r\n\r\n        table = np.genfromtxt(filename, delimiter="","", skip_header=1, dtype=str)\r\n        if table.size == 0:\r\n            raise ValueError(""Error: Term file is empty, no terms to re-rank."")\r\n\r\n        try:\r\n            terms = table[:, 1]\r\n        except Exception as e:\r\n            print(""\\n\\nError converting str to float in training table: {}"".format(e))\r\n\r\n        polarities = table[:, 3].astype(str)\r\n\r\n        if len(terms) != len(polarities):\r\n            raise ValueError(\r\n                ""Count of opinion terms is different than the count of loaded polarities.""\r\n            )\r\n        polarities = {terms[i]: polarities[i] for i in range(len(terms))}\r\n\r\n        print(str(terms.shape[0]) + "" features loaded from CSV file"")\r\n        return terms, polarities\r\n\r\n    @staticmethod\r\n    def load_terms_and_y_labels(filename):\r\n        """"""Load terms and Y labels from feature file.\r\n\r\n        Args:\r\n            filename: feature table file full path\r\n\r\n        Returns:\r\n            x: feature vector\r\n            y: labels vector\r\n            terms: candidate terms\r\n            polarities: opinion polarity per term\r\n        """"""\r\n        print(""Loading basic features from {} ..."".format(filename))\r\n\r\n        table = np.genfromtxt(filename, delimiter="","", skip_header=1, dtype=str)\r\n        if table.size == 0:\r\n            raise ValueError(""Error: Terms file is empty, no terms to re-rank."")\r\n\r\n        try:\r\n            terms = table[:, 1]\r\n        except Exception as e:\r\n            print(""\\n\\nError converting str to float in training table: {}"".format(e))\r\n\r\n        y = table[:, 0].astype(int)\r\n        polarities = None\r\n\r\n        print(str(terms.shape[0]) + "" features loaded from CSV file"")\r\n        return y, terms, polarities\r\n\r\n    @staticmethod\r\n    def concat_sim_scores_and_features(x, neg_sim, pos_sim):\r\n        """"""\r\n        concatenate similarity scores to features\r\n\r\n        Args:\r\n            x: feature vector\r\n            neg_sim: similarity scores between cand terms and neg opinion terms\r\n            pos_sim: similarity scores between cand terms and pos opinion terms\r\n\r\n        Returns:\r\n            x: concatenated features and similarity scores\r\n        """"""\r\n        neg = np.array(neg_sim)\r\n        pos = np.array(pos_sim)\r\n\r\n        neg_avg = np.mean(neg, axis=1, keepdims=True)\r\n        neg_std = np.std(neg, axis=1, keepdims=True)\r\n        neg_min = np.min(neg, axis=1, keepdims=True)\r\n        neg_max = np.max(neg, axis=1, keepdims=True)\r\n\r\n        pos_avg = np.mean(pos, axis=1, keepdims=True)\r\n        pos_std = np.std(pos, axis=1, keepdims=True)\r\n        pos_min = np.min(pos, axis=1, keepdims=True)\r\n        pos_max = np.max(pos, axis=1, keepdims=True)\r\n\r\n        print(""\\nAdding polarity similarity features..."")\r\n\r\n        res_x = np.concatenate(\r\n            (neg_avg, neg_std, neg_min, neg_max, pos_avg, pos_std, pos_min, pos_max, x), 1\r\n        )\r\n\r\n        return res_x\r\n\r\n    def generate_embbeding_features(self, terms, embedding_dict):\r\n        """"""\r\n        concatenate word embedding to features\r\n\r\n        Args:\r\n            terms: candidate terms\r\n            embedding_dict: embedding dictionary\r\n            word_to_emb_idx: index to embedding dictionary\r\n        Returns:\r\n            x: concatenated features and word embs\r\n        """"""\r\n        print(""\\nAdding word vector features...\\n"")\r\n        vec_matrix = np.zeros((len(terms), self.embeddings_len))\r\n\r\n        j = 0\r\n        for term in terms:\r\n            word_vector = embedding_dict.get(term)\r\n            vec_matrix[j, :] = word_vector\r\n            j += 1\r\n\r\n        x = vec_matrix[:j]\r\n\r\n        return x\r\n\r\n    def load_terms_and_y_labels_and_generate_features(self, filename):\r\n        """"""\r\n       load candidate terms with their basic features, Y labels and polarities from feature file\r\n\r\n       Args:\r\n           filename: feature table file path\r\n       Returns:\r\n           x: feature vector\r\n           y: labels vector\r\n           terms: candidate terms\r\n           polarities: opinion polarity per term\r\n       """"""\r\n        print(""\\nLoading feature table...\\n"")\r\n\r\n        y, terms, polarities = self.load_terms_and_y_labels(filename)\r\n\r\n        x, terms, polarities = self.generate_features(terms, polarities)\r\n\r\n        y_vector = None\r\n        if y is not None:\r\n            y_vector = np.reshape(y, (y.shape[0], 1))\r\n\r\n        return x, y, y_vector, terms, polarities\r\n\r\n    def load_terms_and_generate_features(self, filename):\r\n        """"""\r\n       load candidate terms with their basic features, Y labels and polarities from feature file\r\n\r\n       Args:\r\n           filename: feature table file path\r\n       Returns:\r\n           x: feature vector\r\n           terms: candidate terms\r\n           polarities: opinion polarity per term\r\n       """"""\r\n        print(""\\nLoading feature table...\\n"")\r\n\r\n        terms, polarities = self.load_terms_and_polarities(filename)\r\n\r\n        x, terms, polarities = self.generate_features(terms, polarities)\r\n\r\n        return x, terms, polarities\r\n\r\n    @staticmethod\r\n    def _determine_unk_polarities(terms, polarities, neg, pos):\r\n\r\n        for i, term in enumerate(terms):\r\n            if np.average(pos[i]) <= np.average(neg[i]):\r\n                polarities[term] = ""POS""\r\n            else:\r\n                polarities[term] = ""NEG""\r\n\r\n        return polarities\r\n\r\n    def generate_features(self, terms, polarities):\r\n\r\n        generic_terms = _read_generic_lex_for_similarity(self.generic_lexicon)\r\n        # generate unified list of candidate terms and generic terms\r\n        terms_list = [term for term in terms]\r\n        for term in generic_terms.keys():\r\n            terms_list.append(term.strip(""\'\\""""))\r\n\r\n        print(""\\nLoading embedding model...\\n"")\r\n        embedding_dict, _ = load_word_embeddings(self.emb_model_path, terms_list)\r\n        x = self.generate_embbeding_features(terms, embedding_dict)\r\n        neg, pos = self.calc_similarity_scores_for_all_terms(terms, generic_terms, embedding_dict)\r\n        x = self.concat_sim_scores_and_features(x, neg, pos)\r\n        polarities = self._determine_unk_polarities(terms, polarities, neg, pos)\r\n        print(""\\nDimensions of X: "" + str(x.shape))\r\n        return x, terms, polarities\r\n\r\n    def evaluate(self, model, x_test, y_test, terms):\r\n        report = {}\r\n        predictions = model.predict(x_test, verbose=0)\r\n        tp = 0\r\n        fp = 0\r\n        tn = 0\r\n        fn = 0\r\n\r\n        for i, prediction in enumerate(predictions):\r\n            y_true = y_test[i][0]\r\n\r\n            if prediction[0] > self.threshold:\r\n                y_pred = 1\r\n            else:\r\n                y_pred = 0\r\n\r\n            report[terms[i]] = (prediction[0], y_pred, y_true)\r\n\r\n            if y_pred == 1:\r\n                if y_true == 1:\r\n                    tp = tp + 1\r\n                else:\r\n                    fp = fp + 1\r\n            elif y_true == 0:\r\n                tn = tn + 1\r\n            else:\r\n                fn = fn + 1\r\n\r\n        prec = 100 * tp / (tp + fp)\r\n        rec = 100 * tp / (tp + fn)\r\n        f1 = 2 * (prec * rec) / (prec + rec)\r\n\r\n        return (prec, rec, f1), report\r\n\r\n    def generate_model(self, input_vector_dimension):\r\n        """"""Generate MLP model.\r\n\r\n        Args:\r\n           input_vector_dimension (int): word emb vec length\r\n\r\n        Returns:\r\n        """"""\r\n        mlp_model = Sequential()\r\n\r\n        mlp_model.add(Dense(128, activation=self.activation_1, input_dim=input_vector_dimension))\r\n        mlp_model.add(Dropout(0.5))\r\n        mlp_model.add(Dense(64, activation=self.activation_2))\r\n        mlp_model.add(Dropout(0.5))\r\n        mlp_model.add(Dense(1, activation=self.activation_3))\r\n        mlp_model.compile(metrics=[""accuracy""], loss=self.loss, optimizer=self.optimizer)\r\n\r\n        return mlp_model\r\n\r\n    def predict(self, input_table_file, generic_opinion_terms):\r\n        """"""Predict classification class according to model.\r\n\r\n        Args:\r\n           input_table_file: feature(X) and labels(Y) table file\r\n           generic_opinion_terms: generic opinion terms file name\r\n\r\n        Returns:\r\n            final_concat_opinion_lex: reranked_lex conctenated with generic lex\r\n        """"""\r\n        x, terms, polarities = self.load_terms_and_generate_features(input_table_file)\r\n\r\n        model = load_model(self.rerank_model_path)\r\n        reranked_lexicon = model.predict(x, verbose=0)\r\n\r\n        reranked_lex = {}\r\n        for i, prediction in enumerate(reranked_lexicon):\r\n            if not np.isnan(prediction[0]) and prediction[0] > self.PREDICTION_THRESHOLD:\r\n                reranked_lex[terms[i]] = (prediction[0], polarities[terms[i]])\r\n\r\n        final_concat_opinion_lex = self._generate_concat_reranked_lex(\r\n            reranked_lex, generic_opinion_terms\r\n        )\r\n        return final_concat_opinion_lex\r\n\r\n    def rerank_train(self):\r\n        """"""Class for training a reranking model.""""""\r\n        x, y, _, _, _ = self.load_terms_and_y_labels_and_generate_features(\r\n            self.train_rerank_data_path\r\n        )\r\n\r\n        try:\r\n            print(""\\nModel training..."")\r\n            model = self.generate_model(x.shape[1])\r\n            e = self.epochs_and_batch_size[0][0]\r\n            b = self.epochs_and_batch_size[0][1]\r\n\r\n            model.fit(x, y, epochs=e, batch_size=b, verbose=0)\r\n            RerankTerms.model_dir.mkdir(parents=True, exist_ok=True)\r\n\r\n            model.save(str(RerankTerms.model_dir) + ""/rerank_model.h5"")\r\n            print(""\\nSaved model to: "" + str(RerankTerms.model_dir) + ""/rerank_model.h5"")\r\n\r\n        except ZeroDivisionError:\r\n            print(""Division by zero, skipping test"")\r\n\r\n    def cross_validation_training(self, verbose=False):\r\n        """"""Perform k fold cross validation and evaluate the results.""""""\r\n        final_report = {}\r\n        x, y, y_vector, terms, _ = self.load_terms_and_y_labels_and_generate_features(\r\n            self.train_rerank_data_path\r\n        )\r\n\r\n        for seed in self.seeds:\r\n            np.random.seed(seed)\r\n\r\n            for epochs, batch_size in self.epochs_and_batch_size:\r\n                self.print_params(batch_size, epochs, seed)\r\n                k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\r\n                f1_scores = []\r\n                precision_scores = []\r\n                recall_scores = []\r\n\r\n                try:\r\n\r\n                    for i, (train, test) in enumerate(k_fold.split(x, y)):\r\n                        model = self.generate_model(x.shape[1])\r\n                        model.fit(\r\n                            x[train],\r\n                            y_vector[train],\r\n                            epochs=epochs,\r\n                            batch_size=batch_size,\r\n                            verbose=0,\r\n                        )\r\n\r\n                        measures, report = self.evaluate(\r\n                            model, x[test], y_vector[test], terms[test]\r\n                        )\r\n                        final_report.update(report)\r\n\r\n                        precision, recall, f1 = measures\r\n                        f1_scores.append(f1)\r\n                        precision_scores.append(precision)\r\n                        recall_scores.append(recall)\r\n\r\n                        if verbose:\r\n                            print(""Fold "" + str(i + 1) + "":"")\r\n                            self.print_evaluation_results(precision, recall, f1)\r\n\r\n                    print(""\\nSummary:"")\r\n                    self.print_evaluation_results(precision_scores, recall_scores, f1_scores)\r\n\r\n                except ZeroDivisionError:\r\n                    print(""Division by zero, skipping test"")\r\n\r\n        self.write_evaluation_report(final_report)\r\n\r\n    def print_params(self, batch_size, epochs, seed):\r\n        """"""Print training params.\r\n\r\n        Args:\r\n            batch_size(int): batch size\r\n            epochs(int): num of epochs\r\n            seed(int): seed\r\n        """"""\r\n        print(\r\n            ""\\nModel Parameters: act_1= ""\r\n            + self.activation_1\r\n            + "", act_2= ""\r\n            + self.activation_2\r\n            + "", act_3= ""\r\n            + self.activation_3\r\n            + "", loss= ""\r\n            + self.loss\r\n            + "", optimizer= ""\r\n            + self.optimizer\r\n            + ""\\nseed= ""\r\n            + str(seed)\r\n            + "", epochs= ""\r\n            + str(epochs)\r\n            + "", batch_size= ""\r\n            + str(batch_size)\r\n            + "", threshold= ""\r\n            + str(self.threshold)\r\n            + "", use_complete_w2v= ""\r\n            + "", sim_lexicon= ""\r\n            + str(self.sim_lexicon)\r\n            + ""\\n""\r\n        )\r\n\r\n    def print_evaluation_results(self, precision, recall, f1):\r\n        """"""Print evaluation results.\r\n\r\n        Args:\r\n            precision(list of float): precision\r\n            recall(list of float): recall\r\n            f1(list of float): f measure\r\n        """"""\r\n        print()\r\n        self.print_measure(""Precision"", precision)\r\n        self.print_measure(""Recall"", recall)\r\n        self.print_measure(""F-measure"", f1)\r\n        print(\r\n            ""-------------------------------------------------------------------------""\r\n            ""------------------------------""\r\n        )\r\n\r\n    @staticmethod\r\n    def print_measure(measure, value):\r\n        """"""Print single measure.\r\n\r\n        Args:\r\n            measure(str): measure type\r\n            value(list of float): value\r\n        """"""\r\n        print(measure + "": {:.2f}%"".format(np.mean(value)), end="""")\r\n        if not np.isscalar(value):\r\n            print("" (+/- {:.2f}%)"".format(np.std(value)), end="""")\r\n        print()\r\n\r\n    @staticmethod\r\n    def _generate_concat_reranked_lex(acquired_opinion_lex, generic_opinion_lex_file):\r\n        print(""Loading generic sentiment terms from {}..."".format(generic_opinion_lex_file))\r\n        generics_table = np.genfromtxt(\r\n            generic_opinion_lex_file, delimiter="","", skip_header=1, dtype=str\r\n        )\r\n        print(str(generics_table.shape[0]) + "" generic sentiment terms loaded"")\r\n\r\n        concat_opinion_dict = {}\r\n\r\n        for key, value in acquired_opinion_lex.items():\r\n            concat_opinion_dict[key] = (value[0], value[1], ""Y"")\r\n        for row in generics_table:\r\n            concat_opinion_dict[row[0]] = (row[2], row[1], ""N"")\r\n\r\n        return concat_opinion_dict\r\n\r\n    @staticmethod\r\n    def _write_prediction_results(concat_opinion_dict, out_override):\r\n        out_dir = Path(out_override) if out_override else LEXICONS_OUT\r\n        out_path = out_dir / ""generated_opinion_lex_reranked.csv""\r\n        with open(out_path, ""w"") as csv_file:\r\n            writer = csv.writer(csv_file)\r\n            writer.writerow([""Term"", ""Score"", ""Polarity"", ""isAcquired""])\r\n            for key, value in concat_opinion_dict.items():\r\n                writer.writerow([key, value[0], value[1], value[2]])\r\n        print(""Reranked opinion lexicon written to {}"".format(out_path))\r\n\r\n    @staticmethod\r\n    def write_evaluation_report(report_dic):\r\n        RerankTerms.model_dir.mkdir(parents=True, exist_ok=True)\r\n        out_path = RerankTerms.model_dir / ""rerank_classifier_results.csv""\r\n        with open(out_path, ""w"", encoding=""utf-8"") as csv_file:\r\n            writer = csv.writer(csv_file)\r\n            writer.writerow([""term"", ""score"", ""y_pred"", ""y_true""])\r\n            for key, value in report_dic.items():\r\n                writer.writerow([key, value[0], value[1], value[2]])\r\n        print(""Report written to {}"" + str(out_path))\r\n\r\n    @staticmethod\r\n    def load_word_vectors_dict():\r\n        try:\r\n            with open(RerankTerms.model_dir / ""word_vectors_dict.pickle"", ""rb"") as f:\r\n                ret = pickle.load(f)\r\n        except OSError:\r\n            ret = {}\r\n        return ret\r\n'"
nlp_architect/models/absa/train/rules.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom nlp_architect.models.absa.inference.data_types import Polarity\nfrom nlp_architect.models.absa.train.data_types import (\n    CandidateTerm,\n    RelCategory,\n    DepRelationTerm,\n    POS,\n)\n\n\ndef rule_1(dep_rel, gov_entry, dep_entry, text):\n    """"""Extract term if rule 1 applies.\n\n    Args:\n        dep_rel (DepRelation): Dependency relation.\n        gov_entry (DicEntrySentiment): Governor opinion entry.\n        dep_entry (DicEntrySentiment): Dependant opinion entry.\n        text (str): Sentence text.\n    """"""\n    candidate = None\n    (anchor_entry, anchor, related) = (\n        (gov_entry, dep_rel.gov, dep_rel.dep)\n        if gov_entry\n        else (dep_entry, dep_rel.dep, dep_rel.gov)\n    )\n\n    if related.norm_pos == POS.ADJ and dep_rel.rel.startswith(""conj""):\n        polarity = anchor_entry.polarity\n        candidate = CandidateTerm(related, anchor, text, polarity)\n    return candidate\n\n\ndef rule_2(dep_rel, relation_list, dep_entry, text):\n    """"""Extract term if rule 2 applies.\n\n    Args:\n        dep_rel (DepRelation): Dependency relation.\n        relation_list (list of DepRelation): Generic relations between all tokens.\n        dep_entry (OpinionTerm): Dependent token.\n        text (str): Sentence text.\n    """"""\n    candidate = None\n    for curr_rt in relation_list:\n        if (curr_rt.gov, curr_rt.rel, curr_rt.dep.norm_pos) == (\n            dep_rel.gov,\n            dep_rel.rel,\n            POS.ADJ,\n        ) and curr_rt.dep != dep_rel.dep:\n            candidate = CandidateTerm(curr_rt.dep, dep_rel.dep, text, dep_entry.polarity)\n    return candidate\n\n\ndef rule_3(dep_rel, relation_list, text):\n    """"""Extract term if rule 3 applies.\n\n    Args:\n        dep_rel (DepRelation): Dependency relation.\n        relation_list (list of DepRelation): Generic relations between all tokens.\n        text (str): Sentence text.\n    """"""\n    candidate = None\n    if dep_rel.gov.norm_pos == POS.NN and is_subj_obj_or_mod(dep_rel):\n        aspect = expand_aspect(dep_rel.gov, relation_list)\n        candidate = CandidateTerm(aspect, dep_rel.dep, text, Polarity.UNK)\n    return candidate\n\n\ndef rule_4(dep_rel, relation_list, text):\n    """"""Extract term if rule 4 applies.\n\n    Args:\n        dep_rel (DepRelation): Dependency relation.\n        relation_list (list of DepRelation): Generic relations between all tokens.\n        relation between tokens\n        text (str): Sentence text.\n    """"""\n    candidate = None\n    for curr_rt in relation_list:\n        if (\n            curr_rt.gov == dep_rel.gov\n            and curr_rt.dep != dep_rel.dep\n            and curr_rt.dep.norm_pos == POS.NN\n            and is_subj_obj_or_mod(curr_rt)\n            and is_subj_obj_or_mod(dep_rel)\n        ):\n            aspect = expand_aspect(curr_rt.dep, relation_list)\n            candidate = CandidateTerm(aspect, dep_rel.dep, text, Polarity.UNK)\n    return candidate\n\n\ndef rule_5(dep_rel, text):\n    """"""Extract term if rule 5 applies.\n\n    Args:\n        dep_rel (DepRelation): Dependency relation.\n        text (str): Sentence text.\n    """"""\n    candidate = None\n    if is_subj_obj_or_mod(dep_rel) and dep_rel.dep.norm_pos == POS.ADJ:\n        return CandidateTerm(dep_rel.dep, dep_rel.gov, text, Polarity.UNK)\n    return candidate\n\n\ndef rule_6(dep_rel, relation_list, text):\n    """"""Extract term if rule 6 applies.\n\n    Args:\n        dep_rel (DepRelation): Dependency relation.\n        relation_list (list of DepRelation): Generic relations between all tokens.\n        text (str): Sentence text.\n    """"""\n    candidate = None\n    if dep_rel.rel in (""conj_and"", ""conj_but""):\n        aspect = expand_aspect(dep_rel.dep, relation_list)\n        candidate = CandidateTerm(aspect, dep_rel.gov, text, Polarity.UNK)\n    return candidate\n\n\ndef is_subj_obj_or_mod(rt):\n    return any(rt.rel in cat.value for cat in (RelCategory.SUBJ, RelCategory.OBJ, RelCategory.MOD))\n\n\ndef expand_aspect(in_aspect_token, relation_list):\n    """"""Expand aspect by Looking for a noun word that it\'s gov is the aspect. if it has (noun)\n    compound relation add it to aspect.""""""\n    aspect = DepRelationTerm(\n        text=in_aspect_token.text,\n        lemma=in_aspect_token.lemma,\n        pos=in_aspect_token.pos,\n        ner=in_aspect_token.ner,\n        idx=in_aspect_token.idx,\n    )\n    for rel in relation_list:\n        if (rel.rel == ""compound"") and (rel.gov.idx == aspect.idx):\n            diff_positive = aspect.idx - len(rel.dep.text) - 1 - rel.dep.idx\n            diff_negative = rel.dep.idx - len(aspect.text) - 1 - aspect.idx\n            if diff_positive == 0:\n                aspect.text = rel.dep.text + "" "" + aspect.text\n                aspect.lemma = rel.dep.text + "" "" + aspect.lemma\n                aspect.idx = rel.dep.idx\n            if diff_negative == 0:\n                aspect.text = aspect.text + "" "" + rel.dep.text\n                aspect.lemma = aspect.lemma + "" "" + rel.dep.lemma\n\n    aspect.text = aspect.text.lower()\n    aspect.lemma = aspect.lemma.lower()\n\n    return aspect\n'"
nlp_architect/models/absa/train/train.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom pathlib import Path, PosixPath\nfrom os import PathLike\nfrom typing import Union\n\nfrom nlp_architect.models.absa import TRAIN_OUT, LEXICONS_OUT\nfrom nlp_architect.models.absa.train.acquire_terms import AcquireTerms\nfrom nlp_architect.models.absa.train.rerank_terms import RerankTerms\nfrom nlp_architect.models.absa.utils import (\n    parse_docs,\n    _download_pretrained_rerank_model,\n    _write_aspect_lex,\n    _write_opinion_lex,\n)\nfrom nlp_architect.utils.io import download_unzip\n\nEMBEDDING_URL = ""http://nlp.stanford.edu/data"", ""glove.840B.300d.zip""\nEMBEDDING_PATH = TRAIN_OUT / ""word_emb_unzipped"" / ""glove.840B.300d.txt""\nRERANK_MODEL_DEFAULT_PATH = rerank_model_dir = TRAIN_OUT / ""reranking_model"" / ""rerank_model.h5""\n\n\nclass TrainSentiment:\n    def __init__(\n        self,\n        parse: bool = True,\n        rerank_model: PathLike = None,\n        asp_thresh: int = 3,\n        op_thresh: int = 2,\n        max_iter: int = 3,\n    ):\n        self.acquire_lexicon = AcquireTerms(asp_thresh, op_thresh, max_iter)\n        if parse:\n            from nlp_architect.pipelines.spacy_bist import SpacyBISTParser\n\n            self.parser = SpacyBISTParser()\n        else:\n            self.parser = None\n\n        if not rerank_model:\n            print(""using pre-trained reranking model"")\n            rerank_model = _download_pretrained_rerank_model(RERANK_MODEL_DEFAULT_PATH)\n\n        download_unzip(*EMBEDDING_URL, EMBEDDING_PATH, license_msg=""Glove word embeddings."")\n        self.rerank = RerankTerms(\n            vector_cache=True, rerank_model=rerank_model, emb_model_path=EMBEDDING_PATH\n        )\n\n    def run(\n        self,\n        data: Union[str, PathLike] = None,\n        parsed_data: Union[str, PathLike] = None,\n        out_dir: Union[str, PathLike] = TRAIN_OUT,\n    ):\n\n        if not parsed_data:\n            if not self.parser:\n                raise RuntimeError(""Parser not initialized (try parse=True at init)"")\n            parsed_dir = Path(out_dir) / ""parsed"" / Path(data).stem\n            parsed_data = self.parse_data(data, parsed_dir)\n\n        generated_aspect_lex = self.acquire_lexicon.acquire_lexicons(parsed_data)\n        _write_aspect_lex(parsed_data, generated_aspect_lex, LEXICONS_OUT)\n\n        generated_opinion_lex_reranked = self.rerank.predict(\n            AcquireTerms.acquired_opinion_terms_path, AcquireTerms.generic_opinion_lex_path\n        )\n        _write_opinion_lex(parsed_data, generated_opinion_lex_reranked, LEXICONS_OUT)\n\n        return generated_opinion_lex_reranked, generated_aspect_lex\n\n    def parse_data(self, data: PathLike or PosixPath, parsed_dir: PathLike or PosixPath):\n        _, data_size = parse_docs(self.parser, data, out_dir=parsed_dir)\n        if data_size < 1000:\n            raise ValueError(\n                ""The data contains only {0} sentences. A minimum of 1000 ""\n                ""sentences is required for training."".format(data_size)\n            )\n        return parsed_dir\n'"
nlp_architect/models/bist/eval/__init__.py,0,b''
nlp_architect/models/cross_doc_coref/system/__init__.py,0,b''
nlp_architect/models/cross_doc_coref/system/cdc_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport os\nfrom typing import List\n\nfrom nlp_architect.common.cdc.cluster import Clusters\nfrom nlp_architect.common.cdc.mention_data import MentionData\nfrom nlp_architect.common.cdc.topics import Topic\nfrom nlp_architect.utils.string_utils import StringUtils\n\nlogger = logging.getLogger(__name__)\n\n\ndef write_clusters_to_file(clusters: Clusters, topic_id: str, file_obj) -> None:\n    """"""\n    Write the clusters to a text file (for experiments or evaluation using\n        coreference scorer (v8.01))\n    Args:\n        clusters: the cluster to write\n        topic_id:\n        file_obj: file object\n    """"""\n    i = 0\n    file_obj.write(""Topic - "" + topic_id + ""\\n"")\n    for cluster in clusters.clusters_list:\n        i += 1\n        file_obj.write(""cluster #"" + str(i) + ""\\n"")\n        mentions_list = []\n        for mention in cluster.mentions:\n            mentions_list.append((mention.tokens_str, mention.predicted_coref_chain))\n        file_obj.write(str(mentions_list) + ""\\n"")\n\n\ndef extract_vocab(mentions: List[MentionData], filter_stop_words: bool) -> List[str]:\n    """"""\n    Extract Head, Lemma and mention string from all mentions to create a list of string vocabulary\n    Args:\n        mentions:\n        filter_stop_words:\n\n    Returns:\n\n    """"""\n    vocab = set()\n    for mention in mentions:\n        head = mention.mention_head\n        head_lemma = mention.mention_head_lemma\n        tokens_str = mention.tokens_str\n        if not filter_stop_words:\n            vocab.add(head)\n            vocab.add(head_lemma)\n            vocab.add(tokens_str)\n        else:\n            if not StringUtils.is_stop(head):\n                vocab.add(head)\n            if not StringUtils.is_stop(head_lemma):\n                vocab.add(head_lemma)\n            if not StringUtils.is_stop(tokens_str):\n                vocab.add(tokens_str)\n    vocab_set = list(vocab)\n    return vocab_set\n\n\ndef load_mentions_vocab_from_files(mentions_files, filter_stop_words=False):\n    logger.info(""Loading mentions files..."")\n    mentions = []\n    for _file in mentions_files:\n        mentions.extend(MentionData.read_mentions_json_to_mentions_data_list(_file))\n\n    return load_mentions_vocab(mentions, filter_stop_words)\n\n\ndef load_mentions_vocab(mentions, filter_stop_words=False):\n    vocab = extract_vocab(mentions, filter_stop_words)\n    logger.info(""Done loading mentions files..."")\n    return vocab\n\n\ndef write_event_coref_scorer_results(topics_list: List[Topic], output_file: str) -> None:\n    with open(os.path.join(output_file, ""cd_event_pred_clusters_spans.txt""), ""w"") as output:\n        write_topics(topics_list, output)\n\n\ndef write_entity_coref_scorer_results(topics_list: List[Topic], output_file: str) -> None:\n    with open(os.path.join(output_file, ""cd_entity_pred_clusters_spans.txt""), ""w"") as output:\n        write_topics(topics_list, output)\n\n\ndef write_topics(topics_list: List[Topic], output) -> None:\n    output.write(""#begin document (ECB+/ecbplus_all); part 000\\n"")\n    for topic in topics_list:\n        for mention in topic.mentions:\n            output.write(""ECB+/ecbplus_all\\t"" + ""("" + str(mention.predicted_coref_chain) + "")\\n"")\n    output.write(""#end document"")\n'"
nlp_architect/models/cross_doc_coref/system/sieves_container_init.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nfrom typing import List\n\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.models.cross_doc_coref.sieves_config import (\n    EventSievesConfiguration,\n    EntitySievesConfiguration,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass SievesContainerInitialization(object):\n    def __init__(\n        self,\n        event_coref_config: EventSievesConfiguration,\n        entity_coref_config: EntitySievesConfiguration,\n        sieves_model_list: List[RelationExtraction],\n    ):\n        self.sieves_model_list = sieves_model_list\n        self.event_config = event_coref_config\n        self.entity_config = entity_coref_config\n\n    def get_module_from_relation(self, relation_type):\n        for model in self.sieves_model_list:\n            if relation_type in model.get_supported_relations():\n                return model\n\n        raise Exception(""No model found that Support RelationType-"" + str(relation_type))\n'"
nlp_architect/nn/tensorflow/python/__init__.py,0,b''
nlp_architect/nn/torch/data/__init__.py,0,b''
nlp_architect/nn/torch/data/dataset.py,9,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport torch\nfrom typing import List\n\n\nclass ParallelDataset(torch.utils.data.Dataset):\n    def __init__(self, *datasets):\n        self.datasets = datasets\n\n    def __getitem__(self, i):\n        return tuple(d[i] for d in self.datasets)\n\n    def __len__(self):\n        return min(len(d) for d in self.datasets)\n\n\nclass ConcatTensorDataset(torch.utils.data.Dataset):\n    r""""""Dataset as a concatenation of multiple TensorDataset datasets with same number of tensors.\n\n    Each sample will be retrieved by indexing tensors along the first dimension.\n\n    Arguments:\n        dataset (TensorDataset): dataset to which rest datasets will be concatinated.\n        datasets (List[TensorDataset]): datasets to concat to the dataset.\n    """"""\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.TensorDataset,\n        datasets: List[torch.utils.data.TensorDataset],\n    ):\n        tensors = dataset.tensors\n        for ds in datasets:\n            concat_tensors = []\n            for i, t in enumerate(ds.tensors):\n                concat_tensors.append(torch.cat((tensors[i], t), 0))\n            tensors = concat_tensors\n        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n        self.tensors = tensors\n\n    def __getitem__(self, index):\n        return tuple(tensor[index] for tensor in self.tensors)\n\n    def __len__(self):\n        return self.tensors[0].size(0)\n\n\nclass CombinedTensorDataset(torch.utils.data.Dataset):\n    r""""""Dataset as a concatenation of tensor datasets with different number of\n        tensors (labeled dataset/ unlabeled dataset). Labels of unlabeled dataset will\n        be represented as a tensor of zeros.\n\n        Each sample will be retrieved by indexing tensors along the first dimension.\n\n        Arguments:\n            datasets (List[TensorDataset]): datasets to concat.\n    """"""\n\n    def __init__(self, datasets: List[torch.utils.data.TensorDataset]):\n        max_ds_len = max([len(ds.tensors) for ds in datasets])\n        tensors = ()\n        # match tensors count\n        for ds in datasets:\n            if len(ds.tensors) < max_ds_len:  # no labels\n                ds.tensors += (torch.tensor(torch.zeros(ds.tensors[0].shape), dtype=int),)\n        # concat\n        for i in range(max_ds_len):\n            tensors += (torch.cat([ds.tensors[i] for ds in datasets], dim=0),)\n        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n        self.tensors = tensors\n\n    def __getitem__(self, index):\n        return tuple(tensor[index] for tensor in self.tensors)\n\n    def __len__(self):\n        return self.tensors[0].size(0)\n'"
nlp_architect/nn/torch/layers/__init__.py,1,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# flake8: noqa\nfrom nlp_architect.nn.torch.layers.crf import CRF\n'"
nlp_architect/nn/torch/layers/crf.py,34,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# Module adapted from https://github.com/kmkurn/pytorch-crf\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\n\n\nclass CRF(nn.Module):\n    """"""Conditional random field.\n    This module implements a conditional random field [LMP01]_. The forward computation\n    of this class computes the log likelihood of the given sequence of tags and\n    emission score tensor. This class also has `~CRF.decode` method which finds\n    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n    Args:\n        num_tags: Number of tags.\n        batch_first: Whether the first dimension corresponds to the size of a minibatch.\n    Attributes:\n        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size\n            ``(num_tags,)``.\n        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size\n            ``(num_tags,)``.\n        transitions (`~torch.nn.Parameter`): Transition score tensor of size\n            ``(num_tags, num_tags)``.\n    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).\n       ""Conditional random fields: Probabilistic models for segmenting and\n       labeling sequence data"". *Proc. 18th International Conf. on Machine\n       Learning*. Morgan Kaufmann. pp. 282\xe2\x80\x93289.\n    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n    """"""\n\n    def __init__(self, num_tags: int, batch_first: bool = False) -> None:\n        if num_tags <= 0:\n            raise ValueError(f""invalid number of tags: {num_tags}"")\n        super().__init__()\n        self.num_tags = num_tags\n        self.batch_first = batch_first\n        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        """"""Initialize the transition parameters.\n        The parameters will be initialized randomly from a uniform distribution\n        between -0.1 and 0.1.\n        """"""\n        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n        nn.init.uniform_(self.transitions, -0.1, 0.1)\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}(num_tags={self.num_tags})""\n\n    def forward(\n        self,\n        emissions: torch.Tensor,\n        tags: torch.LongTensor,\n        mask: Optional[torch.ByteTensor] = None,\n        reduction: str = ""sum"",\n    ) -> torch.Tensor:\n        """"""Compute the conditional log likelihood of a sequence of tags given emission scores.\n        Args:\n            emissions (`~torch.Tensor`): Emission score tensor of size\n                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length, num_tags)`` otherwise.\n            tags (`~torch.LongTensor`): Sequence of tags tensor of size\n                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length)`` otherwise.\n            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n            reduction: Specifies  the reduction to apply to the output:\n                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n                ``sum``: the output will be summed over batches. ``mean``: the output will be\n                averaged over batches. ``token_mean``: the output will be averaged over tokens.\n        Returns:\n            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if\n            reduction is ``none``, ``()`` otherwise.\n        """"""\n        self._validate(emissions, tags=tags, mask=mask)\n        if reduction not in (""none"", ""sum"", ""mean"", ""token_mean""):\n            raise ValueError(f""invalid reduction: {reduction}"")\n        if mask is None:\n            mask = torch.ones_like(tags, dtype=torch.uint8)\n\n        if self.batch_first:\n            emissions = emissions.transpose(0, 1)\n            tags = tags.transpose(0, 1)\n            mask = mask.transpose(0, 1)\n\n        # shape: (batch_size,)\n        numerator = self._compute_score(emissions, tags, mask)\n        # shape: (batch_size,)\n        denominator = self._compute_normalizer(emissions, mask)\n        # shape: (batch_size,)\n        llh = numerator - denominator\n\n        if reduction == ""none"":\n            return llh\n        if reduction == ""sum"":\n            return llh.sum()\n        if reduction == ""mean"":\n            return llh.mean()\n        assert reduction == ""token_mean""\n        return llh.sum() / mask.float().sum()\n\n    def decode(\n        self, emissions: torch.Tensor, mask: Optional[torch.ByteTensor] = None\n    ) -> List[List[int]]:\n        """"""Find the most likely tag sequence using Viterbi algorithm.\n        Args:\n            emissions (`~torch.Tensor`): Emission score tensor of size\n                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length, num_tags)`` otherwise.\n            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n        Returns:\n            List of list containing the best tag sequence for each batch.\n        """"""\n        self._validate(emissions, mask=mask)\n        if mask is None:\n            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n\n        if self.batch_first:\n            emissions = emissions.transpose(0, 1)\n            mask = mask.transpose(0, 1)\n\n        return self._viterbi_decode(emissions, mask)\n\n    def _validate(\n        self,\n        emissions: torch.Tensor,\n        tags: Optional[torch.LongTensor] = None,\n        mask: Optional[torch.ByteTensor] = None,\n    ) -> None:\n        if emissions.dim() != 3:\n            raise ValueError(f""emissions must have dimension of 3, got {emissions.dim()}"")\n        if emissions.size(2) != self.num_tags:\n            raise ValueError(\n                f""expected last dimension of emissions is {self.num_tags}, ""\n                f""got {emissions.size(2)}""\n            )\n\n        if tags is not None:\n            if emissions.shape[:2] != tags.shape:\n                raise ValueError(\n                    ""the first two dimensions of emissions and tags must match, ""\n                    f""got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}""\n                )\n\n        if mask is not None:\n            if emissions.shape[:2] != mask.shape:\n                raise ValueError(\n                    ""the first two dimensions of emissions and mask must match, ""\n                    f""got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}""\n                )\n            no_empty_seq = not self.batch_first and mask[0].all()\n            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n            if not no_empty_seq and not no_empty_seq_bf:\n                raise ValueError(""mask of the first timestep must all be on"")\n\n    def _compute_score(\n        self, emissions: torch.Tensor, tags: torch.LongTensor, mask: torch.ByteTensor\n    ) -> torch.Tensor:\n        # emissions: (seq_length, batch_size, num_tags)\n        # tags: (seq_length, batch_size)\n        # mask: (seq_length, batch_size)\n        assert emissions.dim() == 3 and tags.dim() == 2\n        assert emissions.shape[:2] == tags.shape\n        assert emissions.size(2) == self.num_tags\n        assert mask.shape == tags.shape\n        assert mask[0].all()\n\n        seq_length, batch_size = tags.shape\n        mask = mask.float()\n\n        # Start transition score and first emission\n        # shape: (batch_size,)\n        score = self.start_transitions[tags[0]]\n        score += emissions[0, torch.arange(batch_size), tags[0]]\n\n        for i in range(1, seq_length):\n            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n            # shape: (batch_size,)\n            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n\n            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n            # shape: (batch_size,)\n            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n\n        # End transition score\n        # shape: (batch_size,)\n        seq_ends = mask.long().sum(dim=0) - 1\n        # shape: (batch_size,)\n        last_tags = tags[seq_ends, torch.arange(batch_size)]\n        # shape: (batch_size,)\n        score += self.end_transitions[last_tags]\n\n        return score\n\n    def _compute_normalizer(self, emissions: torch.Tensor, mask: torch.ByteTensor) -> torch.Tensor:\n        # emissions: (seq_length, batch_size, num_tags)\n        # mask: (seq_length, batch_size)\n        assert emissions.dim() == 3 and mask.dim() == 2\n        assert emissions.shape[:2] == mask.shape\n        assert emissions.size(2) == self.num_tags\n        assert mask[0].all()\n\n        seq_length = emissions.size(0)\n\n        # Start transition score and first emission; score has size of\n        # (batch_size, num_tags) where for each batch, the j-th column stores\n        # the score that the first timestep has tag j\n        # shape: (batch_size, num_tags)\n        score = self.start_transitions + emissions[0]\n\n        for i in range(1, seq_length):\n            # Broadcast score for every possible next tag\n            # shape: (batch_size, num_tags, 1)\n            broadcast_score = score.unsqueeze(2)\n\n            # Broadcast emission score for every possible current tag\n            # shape: (batch_size, 1, num_tags)\n            broadcast_emissions = emissions[i].unsqueeze(1)\n\n            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n            # for each sample, entry at row i and column j stores the sum of scores of all\n            # possible tag sequences so far that end with transitioning from tag i to tag j\n            # and emitting\n            # shape: (batch_size, num_tags, num_tags)\n            next_score = broadcast_score + self.transitions + broadcast_emissions\n\n            # Sum over all possible current tags, but we\'re in score space, so a sum\n            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n            # all possible tag sequences so far, that end in tag i\n            # shape: (batch_size, num_tags)\n            next_score = torch.logsumexp(next_score, dim=1)\n\n            # Set score to the next score if this timestep is valid (mask == 1)\n            # shape: (batch_size, num_tags)\n            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n\n        # End transition score\n        # shape: (batch_size, num_tags)\n        score += self.end_transitions\n\n        # Sum (log-sum-exp) over all possible tags\n        # shape: (batch_size,)\n        return torch.logsumexp(score, dim=1)\n\n    def _viterbi_decode(\n        self, emissions: torch.FloatTensor, mask: torch.ByteTensor\n    ) -> List[List[int]]:\n        # emissions: (seq_length, batch_size, num_tags)\n        # mask: (seq_length, batch_size)\n        assert emissions.dim() == 3 and mask.dim() == 2\n        assert emissions.shape[:2] == mask.shape\n        assert emissions.size(2) == self.num_tags\n        assert mask[0].all()\n\n        seq_length, batch_size = mask.shape\n\n        # Start transition and first emission\n        # shape: (batch_size, num_tags)\n        score = self.start_transitions + emissions[0]\n        history = []\n\n        # score is a tensor of size (batch_size, num_tags) where for every batch,\n        # value at column j stores the score of the best tag sequence so far that ends\n        # with tag j\n        # history saves where the best tags candidate transitioned from; this is used\n        # when we trace back the best tag sequence\n\n        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n        # for every possible next tag\n        for i in range(1, seq_length):\n            # Broadcast viterbi score for every possible next tag\n            # shape: (batch_size, num_tags, 1)\n            broadcast_score = score.unsqueeze(2)\n\n            # Broadcast emission score for every possible current tag\n            # shape: (batch_size, 1, num_tags)\n            broadcast_emission = emissions[i].unsqueeze(1)\n\n            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n            # for each sample, entry at row i and column j stores the score of the best\n            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n            # shape: (batch_size, num_tags, num_tags)\n            next_score = broadcast_score + self.transitions + broadcast_emission\n\n            # Find the maximum score over all possible current tag\n            # shape: (batch_size, num_tags)\n            next_score, indices = next_score.max(dim=1)\n\n            # Set score to the next score if this timestep is valid (mask == 1)\n            # and save the index that produces the next score\n            # shape: (batch_size, num_tags)\n            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n            history.append(indices)\n\n        # End transition score\n        # shape: (batch_size, num_tags)\n        score += self.end_transitions\n\n        # Now, compute the best path for each sample\n\n        # shape: (batch_size,)\n        seq_ends = mask.long().sum(dim=0) - 1\n        best_tags_list = []\n\n        for idx in range(batch_size):\n            # Find the tag which maximizes the score at the last timestep; this is our best tag\n            # for the last timestep\n            _, best_last_tag = score[idx].max(dim=0)\n            best_tags = [best_last_tag.item()]\n\n            # We trace back where the best last tag comes from, append that to our best tag\n            # sequence, and trace it back again, and so on\n            for hist in reversed(history[: seq_ends[idx]]):\n                best_last_tag = hist[idx][best_tags[-1]]\n                best_tags.append(best_last_tag.item())\n\n            # Reverse the order because we start from the last timestep\n            best_tags.reverse()\n            best_tags_list.append(best_tags)\n\n        return best_tags_list\n'"
nlp_architect/nn/torch/modules/__init__.py,0,b''
nlp_architect/nn/torch/modules/embedders.py,15,"b'# ******************************************************************************\n# Copyright 2017-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport os\nfrom typing import List\n\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom nlp_architect.utils.io import load_json_file\nfrom nlp_architect.utils.text import n_letters\n\n\nclass CNNLSTM(nn.Module):\n    """"""CNN-LSTM embedder (based on Ma and Hovy. 2016)\n\n    Args:\n        word_vocab_size (int): word vocabulary size\n        num_labels (int): number of labels (classifier)\n        word_embedding_dims (int, optional): word embedding dims\n        char_embedding_dims (int, optional): character embedding dims\n        cnn_kernel_size (int, optional): character CNN kernel size\n        cnn_num_filters (int, optional): character CNN number of filters\n        lstm_hidden_size (int, optional): LSTM embedder hidden size\n        lstm_layers (int, optional): num of LSTM layers\n        bidir (bool, optional): apply bi-directional LSTM\n        dropout (float, optional): dropout rate\n        padding_idx (int, optinal): padding number for embedding layers\n\n    """"""\n\n    def __init__(\n        self,\n        word_vocab_size: int,\n        num_labels: int,\n        word_embedding_dims: int = 100,\n        char_embedding_dims: int = 16,\n        cnn_kernel_size: int = 3,\n        cnn_num_filters: int = 128,\n        lstm_hidden_size: int = 100,\n        lstm_layers: int = 2,\n        bidir: bool = True,\n        dropout: float = 0.5,\n        padding_idx: int = 0,\n    ):\n        super(CNNLSTM, self).__init__()\n        self.word_embedding_dim = word_embedding_dims\n        self.word_embeddings = nn.Embedding(\n            word_vocab_size, word_embedding_dims, padding_idx=padding_idx\n        )\n        self.char_embeddings = nn.Embedding(\n            n_letters + 1, char_embedding_dims, padding_idx=padding_idx\n        )\n        self.conv1 = nn.Conv1d(\n            in_channels=char_embedding_dims,\n            out_channels=cnn_num_filters,\n            kernel_size=cnn_kernel_size,\n            padding=int(cnn_kernel_size / 2),\n        )\n        self.relu = nn.ReLU()\n\n        self.lstm = nn.LSTM(\n            input_size=word_embedding_dims + cnn_num_filters,\n            hidden_size=lstm_hidden_size,\n            bidirectional=bidir,\n            batch_first=True,\n            num_layers=lstm_layers,\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.dense = nn.Linear(\n            in_features=lstm_hidden_size * 2 if bidir else lstm_hidden_size, out_features=num_labels\n        )\n        self.num_labels = num_labels\n        self.padding_idx = padding_idx\n\n    def load_embeddings(self, embeddings):\n        """"""\n        Load pre-defined word embeddings\n\n        Args:\n            embeddings (torch.tensor): word embedding tensor\n        """"""\n        self.word_embeddings = nn.Embedding.from_pretrained(\n            embeddings, freeze=False, padding_idx=self.padding_idx\n        )\n\n    def forward(self, words, word_chars, **kwargs):\n        """"""\n        CNN-LSTM forward step\n\n        Args:\n            words (torch.tensor): words\n            word_chars (torch.tensor): word character tensors\n\n        Returns:\n            torch.tensor: logits of model\n        """"""\n        word_embeds = self.word_embeddings(words)\n        char_embeds = self.char_embeddings(word_chars)\n        saved_char_size = char_embeds.size()[:2]\n        char_embeds = char_embeds.permute(0, 1, 3, 2)\n        input_size = char_embeds.size()\n        squashed_shape = [-1] + list(input_size[2:])\n        char_embeds_reshape = char_embeds.contiguous().view(\n            *squashed_shape\n        )  # (samples * timesteps, input_size)\n        char_embeds = self.conv1(char_embeds_reshape)\n        char_embeds = char_embeds.permute(0, 2, 1)\n        char_embeds = self.relu(char_embeds)\n        char_embeds, _ = torch.max(char_embeds, 1)  # global max pooling\n        new_size = saved_char_size + char_embeds.size()[1:]\n        char_features = char_embeds.contiguous().view(new_size)\n\n        features = torch.cat((word_embeds, char_features), -1)\n        features = self.dropout(features)\n\n        self.lstm.flatten_parameters()\n        lstm_out, _ = self.lstm(features)\n        lstm_out = self.dropout(lstm_out)\n        logits = self.dense(lstm_out)\n\n        return logits\n\n    @classmethod\n    def from_config(cls, word_vocab_size: int, num_labels: int, config: str):\n        """"""\n        Load a model from a configuration file\n        A valid configuration file is a JSON file with fields as in class `__init__`\n\n        Args:\n            word_vocab_size (int): word vocabulary size\n            num_labels (int): number of labels (classifier)\n            config (str): path to configuration file\n\n        Returns:\n            CNNLSTM: CNNLSTM module pre-configured\n        """"""\n        if not os.path.exists(config):\n            raise FileNotFoundError\n        cfg = load_json_file(config)\n        return cls(word_vocab_size=word_vocab_size, num_labels=num_labels, **cfg)\n\n\nclass IDCNN(nn.Module):\n    """"""\n    ID-CNN (iterated dilated) tagging model (based on Strubell et al 2017) with word character\n    embedding (using CNN feature extractors)\n\n    Args:\n        word_vocab_size (int): word vocabulary size\n        num_labels (int): number of labels (classifier)\n        word_embedding_dims (int, optional): word embedding dims\n        shape_vocab_size (int, optional): shape vocabulary size\n        shape_embedding_dims (int, optional): shape embedding dims\n        char_embedding_dims (int, optional): character embedding dims\n        char_cnn_filters (int, optional): character CNN kernel size\n        char_cnn_kernel_size (int, optional): character CNN number of filters\n        cnn_kernel_size (int, optional): CNN embedder kernel size\n        cnn_num_filters (int, optional): CNN embedder number of filters\n        input_dropout (float, optional): input layer (embedding) dropout rate\n        middle_dropout (float, optional): middle layer dropout rate\n        hidden_dropout (float, optional): hidden layer dropout rate\n        blocks (int, optinal): number of blocks\n        dilations (List, optinal): List of dilations per CNN layer\n        embedding_pad_idx (int, optional): padding number for embedding layers\n        use_chars (bool, optional): whether to use char embedding, defaults to False\n        drop_penalty (float, optional): penalty for dropout regularization\n\n    """"""\n\n    def __init__(\n        self,\n        word_vocab_size: int,\n        num_labels: int,\n        word_embedding_dims: int = 100,\n        shape_vocab_size: int = 4,\n        shape_embedding_dims: int = 5,\n        char_embedding_dims: int = 16,\n        char_cnn_filters: int = 128,\n        char_cnn_kernel_size: int = 3,\n        cnn_kernel_size: int = 3,\n        cnn_num_filters: int = 128,\n        input_dropout: float = 0.35,\n        middle_dropout: float = 0,\n        hidden_dropout: float = 0.15,\n        blocks: int = 1,\n        dilations: List = None,\n        embedding_pad_idx: int = 0,\n        use_chars: bool = False,\n        drop_penalty: float = 1e-4,\n    ):\n        super(IDCNN, self).__init__()\n        if dilations is None:\n            dilations = [1, 2, 1]\n        self.num_blocks = blocks\n        self.dilation = dilations\n        self.use_chars = use_chars\n        self.drop_penalty = drop_penalty\n        self.num_labels = num_labels\n        self.padding_idx = embedding_pad_idx\n        self.word_embedding_dim = word_embedding_dims\n        self.word_embeddings = nn.Embedding(\n            word_vocab_size, self.word_embedding_dim, padding_idx=self.padding_idx\n        )\n        self.shape_embeddings = nn.Embedding(\n            shape_vocab_size + 1, shape_embedding_dims, padding_idx=self.padding_idx\n        )\n        padding_word = int(cnn_kernel_size / 2)\n        self.char_filters = char_cnn_filters if use_chars else 0\n        self.conv0 = nn.Conv1d(\n            in_channels=word_embedding_dims + shape_embedding_dims + self.char_filters,\n            out_channels=cnn_num_filters,\n            kernel_size=cnn_kernel_size,\n            padding=padding_word,\n        )\n        self.cnv_layers = []\n        for i in range(len(self.dilation)):\n            self.cnv_layers.append(\n                nn.Conv1d(\n                    in_channels=cnn_num_filters,\n                    out_channels=cnn_num_filters,\n                    kernel_size=cnn_kernel_size,\n                    padding=padding_word * self.dilation[i],\n                    dilation=self.dilation[i],\n                )\n            )\n        self.cnv_layers = nn.ModuleList(self.cnv_layers)\n        self.dense = nn.Linear(\n            in_features=(cnn_num_filters * self.num_blocks), out_features=num_labels\n        )\n\n        if use_chars:\n            padding_char = int(char_cnn_kernel_size / 2)\n            self.char_embeddings = nn.Embedding(\n                n_letters + 1, char_embedding_dims, padding_idx=self.padding_idx\n            )\n            self.char_conv = nn.Conv1d(\n                in_channels=char_embedding_dims,\n                out_channels=self.char_filters,\n                kernel_size=char_cnn_kernel_size,\n                padding=padding_char,\n            )\n        self.i_drop = nn.Dropout(input_dropout)\n        self.m_drop = nn.Dropout(middle_dropout)\n        self.h_drop = nn.Dropout(hidden_dropout)\n\n    def forward(self, words, word_chars, shapes, no_dropout=False, **kwargs):\n        """"""\n        IDCNN forward step\n\n        Args:\n            words (torch.tensor): words\n            word_chars (torch.tensor): word character tensors\n            shapes (torch.tensor): words shapes\n\n        Returns:\n            torch.tensor: logits of model\n        """"""\n        block_scores = []\n        input_features = []\n        word_embeds = self.word_embeddings(words)\n        shape_embeds = self.shape_embeddings(shapes)\n        input_features.extend([word_embeds, shape_embeds])\n\n        if self.use_chars:\n            char_embeds = self.char_embeddings(word_chars)\n            saved_char_size = char_embeds.size()[:2]\n            char_embeds = char_embeds.permute(0, 1, 3, 2)\n            input_size = char_embeds.size()\n            squashed_shape = [-1] + list(input_size[2:])\n            char_embeds_reshape = char_embeds.contiguous().view(*squashed_shape)\n            char_embeds = self.char_conv(char_embeds_reshape)\n            char_embeds = char_embeds.permute(0, 2, 1)\n            char_embeds = F.relu(char_embeds)\n            char_embeds, _ = torch.max(char_embeds, 1)  # global max pooling\n            new_size = saved_char_size + char_embeds.size()[1:]\n            char_features = char_embeds.contiguous().view(new_size)\n            input_features.append(char_features)\n\n        features = torch.cat(input_features, 2)\n        if not no_dropout:\n            features = self.i_drop(features)\n\n        features = features.permute(0, 2, 1)\n        conv0 = self.conv0(features)\n        conv0 = F.relu(conv0)\n        conv_layer = conv0\n        for _ in range(self.num_blocks):\n            conv_outputs = []\n            for j in range(len(self.cnv_layers)):\n                conv_layer = self.cnv_layers[j](conv_layer)\n                conv_layer = F.relu(conv_layer)\n                if j == len(self.cnv_layers) - 1:  # currently use only last layer\n                    conv_outputs.append(conv_layer)\n            layers_concat = torch.cat(conv_outputs, 1)\n            if not no_dropout:\n                conv_layer = self.m_drop(layers_concat)  # for next block iteration\n            else:\n                conv_layer = layers_concat\n\n            layers_concat = layers_concat.squeeze(2).permute(0, 2, 1)  # for final block scores\n            if not no_dropout:\n                block_output = self.h_drop(layers_concat)\n            else:\n                block_output = layers_concat\n            scores = self.dense(block_output)\n            block_scores.append(scores)\n            logits = block_scores[-1]  # currently use only last block\n\n        return logits\n\n    @classmethod\n    def from_config(cls, word_vocab_size: int, num_labels: int, config: str):\n        """"""\n        Load a model from a configuration file\n        A valid configuration file is a JSON file with fields as in class `__init__`\n\n        Args:\n            word_vocab_size (int): word vocabulary size\n            num_labels (int): number of labels (classifier)\n            config (str): path to configuration file\n\n        Returns:\n            IDCNN: IDCNNEmbedder module pre-configured\n        """"""\n        if not os.path.exists(config):\n            raise FileNotFoundError\n        cfg = load_json_file(config)\n        return cls(word_vocab_size=word_vocab_size, num_labels=num_labels, **cfg)\n\n    def load_embeddings(self, embeddings):\n        """"""\n        Load pre-defined word embeddings\n\n        Args:\n            embeddings (torch.tensor): word embedding tensor\n        """"""\n        self.word_embeddings = nn.Embedding.from_pretrained(\n            embeddings, freeze=False, padding_idx=self.padding_idx\n        )\n'"
nlp_architect/data/cdc_resources/data_types/wiki/__init__.py,0,b''
nlp_architect/data/cdc_resources/data_types/wiki/wikipedia_page.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport re\nfrom typing import Dict\n\nfrom nlp_architect.data.cdc_resources.data_types.wiki.wikipedia_page_extracted_relations import (\n    WikipediaPageExtractedRelations,\n    DISAMBIGUATION_TITLE,\n)\nfrom nlp_architect.utils.string_utils import StringUtils\n\n\nclass WikipediaPage(object):\n    def __init__(\n        self,\n        orig_phrase: str = None,\n        orig_phrase_norm: str = None,\n        wiki_title: str = None,\n        wiki_title_norm: str = None,\n        score: int = 0,\n        pageid: int = 0,\n        description: str = None,\n        relations: WikipediaPageExtractedRelations = None,\n    ) -> None:\n        """"""\n        Object represent a Wikipedia Page and extracted fields.\n\n        Args:\n            orig_phrase (str): original search phrase\n            orig_phrase_norm (str): original search phrase normalized\n            wiki_title (str): page title\n            wiki_title_norm (str): page title normalized\n            score (int): score for getting wiki_title from orig_phrase\n            pageid (int): the unique page identifier\n            description (str, optional): the page description\n            relations (WikipediaPageExtractedRelations): Object that represent all\n                                                         extracted Wikipedia relations\n        """"""\n        self.orig_phrase = orig_phrase\n        if orig_phrase_norm is None:\n            self.orig_phrase_norm = StringUtils.normalize_str(orig_phrase)\n        else:\n            self.orig_phrase_norm = orig_phrase_norm\n\n        self.wiki_title = wiki_title.replace(DISAMBIGUATION_TITLE, """")\n        if wiki_title_norm is None:\n            self.wiki_title_norm = StringUtils.normalize_str(wiki_title)\n        else:\n            self.wiki_title_norm = wiki_title_norm\n\n        self.score = score\n        self.pageid = int(pageid)\n        self.description = description\n        self.relations = relations\n\n    def toJson(self) -> Dict:\n        result_dict = {}\n        result_dict[""orig_phrase""] = self.orig_phrase\n        result_dict[""orig_phrase_norm""] = self.orig_phrase_norm\n        result_dict[""wiki_title""] = self.wiki_title\n        result_dict[""wiki_title_norm""] = self.wiki_title_norm\n        result_dict[""score""] = self.score\n        result_dict[""pageid""] = self.pageid\n        result_dict[""description""] = self.description\n        result_dict[""relations""] = self.relations.toJson()\n        return result_dict\n\n    def __eq__(self, other):\n        return (\n            self.orig_phrase == other.orig_phrase\n            and self.wiki_title == other.wiki_title\n            and self.pageid == other.pageid\n        )\n\n    def __hash__(self):\n        return hash(self.orig_phrase) + hash(self.pageid) + hash(self.wiki_title)\n\n    def __str__(self) -> str:\n        result_str = """"\n        try:\n            title_strip = re.sub(""(\\u2018|\\u2019)"", ""\'"", self.orig_phrase)\n            wiki_title_strip = re.sub(""(\\u2018|\\u2019)"", ""\'"", self.wiki_title)\n            result_str = (\n                str(title_strip)\n                + "", ""\n                + str(wiki_title_strip)\n                + "", ""\n                + str(self.score)\n                + "", ""\n                + str(self.pageid)\n                + "", ""\n                + str(self.description)\n                + "", ""\n                + str(self.relations)\n            )\n        except Exception:\n            result_str = ""error in to_string()""\n\n        return result_str\n'"
nlp_architect/data/cdc_resources/data_types/wiki/wikipedia_page_extracted_relations.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport re\nimport string\nfrom typing import Set, Dict\n\nfrom nlp_architect.utils.string_utils import StringUtils\n\nPART_NAME_CATEGORIES = [""name"", ""given name"", ""surname""]\nDISAMBIGUATION_TITLE = ""(disambiguation)""\nDISAMBIGUATION_CATEGORY = [""disambig"", ""disambiguation""]\n\n\nclass WikipediaPageExtractedRelations(object):\n    def __init__(\n        self,\n        is_part_name: bool = False,\n        is_disambiguation: bool = False,\n        parenthesis: Set[str] = None,\n        disambiguation_links: Set[str] = None,\n        categories: Set[str] = None,\n        aliases: Set[str] = None,\n        be_comp: Set[str] = None,\n        disambiguation_links_norm: Set[str] = None,\n        categories_norm: Set[str] = None,\n        aliases_norm: Set[str] = None,\n        title_parenthesis_norm: Set[str] = None,\n        be_comp_norm: Set[str] = None,\n    ) -> None:\n        """"""\n        Object represent a Wikipedia Relations Schema\n\n        Args:\n            is_part_name (bool): Weather page title is part of a Name (ie-family name/given name..)\n            is_disambiguation (bool): Weather page is a disambiguation page\n            parenthesis (set): a set of all parenthesis links/titles\n            disambiguation_links (set): a set of all disambiguation links/titles\n            categories (set): a set of all category links/titles\n            aliases (set): a set of all aliases links/titles\n            be_comp (set): a set of all ""is a"" links/titles\n            disambiguation_links_norm (set): same as disambiguation_link just normalized\n            categories_norm (set): same as categories just normalized, lower and clean\n            aliases_norm (set): same as aliases just normalized, lower and clean\n            title_parenthesis_norm (set): same as parenthesis just normalized, lower and clean\n            be_comp_norm (set): same as be_comp just normalized, lower and clean\n        """"""\n        self.is_part_name = is_part_name\n        self.is_disambiguation = is_disambiguation\n        self.disambiguation_links = disambiguation_links\n        self.title_parenthesis = parenthesis\n        self.categories = categories\n        self.aliases = aliases\n        self.be_comp = be_comp\n\n        self.disambiguation_links_norm = disambiguation_links_norm\n        self.categories_norm = categories_norm\n        self.aliases_norm = aliases_norm\n        self.title_parenthesis_norm = title_parenthesis_norm\n        self.be_comp_norm = be_comp_norm\n\n    def extract_relations_from_text_v0(self, text):\n        self.disambiguation_links = set()\n        self.categories = set()\n        self.title_parenthesis = set()\n\n        self.disambiguation_links_norm = set()\n        self.categories_norm = set()\n        self.title_parenthesis_norm = set()\n        self.be_comp_norm = set()\n\n        ext_links = set()\n        title_parenthesis = set()\n\n        text_lines = text.split(""\\n"")\n        for line in text_lines:\n            cat_links = self.extract_categories(line)\n            if not self.is_part_name:\n                self.is_part_name = self.is_name_part(line)\n                if not self.is_part_name and [s for s in PART_NAME_CATEGORIES if s in cat_links]:\n                    self.is_part_name = True\n\n            self.categories.update(cat_links)\n            self.categories_norm.update(StringUtils.normalize_string_list(cat_links))\n\n            links, parenthesis_links = self.extract_links_and_parenthesis(line)\n            ext_links.update(links)\n            title_parenthesis.update(parenthesis_links)\n\n        if self.is_disambiguation:\n            self.disambiguation_links = ext_links\n            self.disambiguation_links_norm = StringUtils.normalize_string_list(ext_links)\n            self.title_parenthesis = title_parenthesis\n            self.title_parenthesis_norm = StringUtils.normalize_string_list(title_parenthesis)\n\n    def __str__(self) -> str:\n        return (\n            str(self.is_disambiguation)\n            + "", ""\n            + str(self.is_part_name)\n            + "", ""\n            + str(self.disambiguation_links)\n            + "", ""\n            + str(self.be_comp)\n            + "", ""\n            + str(self.title_parenthesis)\n            + "", ""\n            + str(self.categories)\n        )\n\n    def toJson(self) -> Dict:\n        result_dict = dict()\n        result_dict[""isPartName""] = self.is_part_name\n        result_dict[""isDisambiguation""] = self.is_disambiguation\n\n        if self.disambiguation_links is not None:\n            result_dict[""disambiguationLinks""] = list(self.disambiguation_links)\n            result_dict[""disambiguationLinksNorm""] = list(self.disambiguation_links_norm)\n        if self.categories is not None:\n            result_dict[""categories""] = list(self.categories)\n            result_dict[""categoriesNorm""] = list(self.categories_norm)\n        if self.aliases is not None:\n            result_dict[""aliases""] = list(self.aliases)\n        if self.title_parenthesis is not None:\n            result_dict[""titleParenthesis""] = list(self.title_parenthesis)\n            result_dict[""titleParenthesisNorm""] = list(self.title_parenthesis_norm)\n        if self.be_comp_norm is not None:\n            result_dict[""beCompRelations""] = list(self.be_comp)\n            result_dict[""beCompRelationsNorm""] = list(self.be_comp_norm)\n\n        return result_dict\n\n    @staticmethod\n    def extract_categories(line: str) -> Set[str]:\n        categories = set()\n        category_form1 = re.findall(r""\\[\\[Category:(.*)\\]\\]"", line)\n        for cat in category_form1:\n            if DISAMBIGUATION_TITLE in cat:\n                cat = cat.replace(DISAMBIGUATION_TITLE, """")\n            categories.add(cat)\n\n        prog = re.search(""^{{(disambig.*|Disambig.*)}}$"", line)\n        if prog is not None:\n            category_form2 = prog.group(1)\n            cats = category_form2.split(""|"")\n            categories.update(cats)\n\n        return categories\n\n    @staticmethod\n    def extract_links_and_parenthesis(line: str):\n        links = set()\n        parenthesis_links = set()\n        ext_links = re.findall(r""\\[\\[(.*)\\]\\]"", line)\n        for link in ext_links:\n            split_link = link.split(""|"")\n            for s_link in split_link:\n                parenthesis_clean = None\n                matcher = re.match(r""(.*)\\s?\\((.*)\\)"", s_link)\n                if matcher:\n                    s_link = matcher.group(1)\n                    parenthesis_match = matcher.group(2)\n                    if parenthesis_match.lower() != ""disambiguation"":\n                        parenthesis_clean = re.sub(\n                            ""["" + string.punctuation + string.whitespace + ""]"",\n                            "" "",\n                            parenthesis_match,\n                        ).strip()\n\n                s_link_clean = re.sub(\n                    ""["" + string.punctuation + string.whitespace + ""]"", "" "", s_link\n                ).strip()\n                if parenthesis_clean is not None and DISAMBIGUATION_TITLE not in parenthesis_clean:\n                    parenthesis_links.add(parenthesis_clean)\n\n                links.add(s_link_clean)\n\n        return links, parenthesis_links\n\n    @staticmethod\n    def is_name_part(line: str) -> bool:\n        line = line.lower()\n        val = False\n        if WikipediaPageExtractedRelations.find_in_line(line, ""===as surname===""):\n            val = True\n        elif WikipediaPageExtractedRelations.find_in_line(line, ""===as given name===""):\n            val = True\n        elif WikipediaPageExtractedRelations.find_in_line(line, ""===given names===""):\n            val = True\n        elif WikipediaPageExtractedRelations.find_in_line(line, ""==as a surname==""):\n            val = True\n        elif WikipediaPageExtractedRelations.find_in_line(line, ""==people with the surname==""):\n            val = True\n        elif WikipediaPageExtractedRelations.find_in_line(line, ""==family name and surname==""):\n            val = True\n        elif WikipediaPageExtractedRelations.find_in_line(line, ""category:given names""):\n            val = True\n        elif WikipediaPageExtractedRelations.find_in_line(line, ""{{given name}}""):\n            val = True\n        return val\n\n    @staticmethod\n    def find_in_line(text: str, pattern: str) -> bool:\n        found = re.findall(pattern, text)\n        if found:\n            return True\n        return False\n'"
nlp_architect/data/cdc_resources/data_types/wiki/wikipedia_pages.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\n\nclass WikipediaPages(object):\n    def __init__(self):\n        """"""\n        Object represent a set of Wikipedia Pages\n        """"""\n        self.pages = set()\n        self.is_empty_norm_phrase = True\n\n    def get_pages(self):\n        return self.pages\n\n    def add_page(self, page):\n        self.pages.add(page)\n        if page.orig_phrase_norm is not None and page.orig_phrase_norm != """":\n            self.is_empty_norm_phrase = False\n\n    def get_and_set_all_disambiguation(self):\n        all_disambiguations = []\n        for page in self.pages:\n            if page.relations.disambiguation_links_norm is not None:\n                all_disambiguations.extend(page.relations.disambiguation_links_norm)\n            if page.relations.disambiguation_links is not None:\n                all_disambiguations.extend(page.relations.disambiguation_links)\n        return set(all_disambiguations)\n\n    def get_and_set_all_categories(self):\n        all_categories = []\n        for page in self.pages:\n            if page.relations.categories_norm is not None:\n                all_categories.extend(page.relations.categories_norm)\n            if page.relations.categories is not None:\n                all_categories.extend(page.relations.categories)\n        return set(all_categories)\n\n    def get_and_set_all_aliases(self):\n        all_aliases = []\n        for page in self.pages:\n            if page.relations.aliases_norm is not None:\n                all_aliases.extend(page.relations.aliases_norm)\n            if page.relations.aliases is not None:\n                all_aliases.extend(page.relations.aliases)\n        return set(all_aliases)\n\n    def get_and_set_parenthesis(self):\n        all_parenthesis = []\n        for page in self.pages:\n            if page.relations.title_parenthesis_norm is not None:\n                all_parenthesis.extend(page.relations.title_parenthesis_norm)\n            if page.relations.title_parenthesis is not None:\n                all_parenthesis.extend(page.relations.title_parenthesis)\n        return set(all_parenthesis)\n\n    def get_and_set_be_comp(self):\n        all_be_comp = []\n        for page in self.pages:\n            if page.relations.be_comp_norm is not None:\n                all_be_comp.extend(page.relations.be_comp_norm)\n            if page.relations.be_comp is not None:\n                all_be_comp.extend(page.relations.be_comp)\n        return set(all_be_comp)\n\n    def get_and_set_titles(self):\n        all_titles = []\n        for page in self.pages:\n            if page.orig_phrase != """":\n                all_titles.append(page.orig_phrase)\n                all_titles.append(page.orig_phrase_norm)\n            if page.wiki_title != """":\n                all_titles.append(page.wiki_title)\n                all_titles.append(page.wiki_title_norm)\n        return set(all_titles)\n\n    def toJson(self):\n        result_dict = {}\n        page_list = []\n        for page in self.pages:\n            page_list.append(page.toJson())\n\n        result_dict[""pages""] = page_list\n        return result_dict\n\n    def __str__(self) -> str:\n        result_str = """"\n        for page in self.pages:\n            result_str += str(page) + "", ""\n\n        return result_str.strip()\n'"
nlp_architect/data/cdc_resources/data_types/wn/__init__.py,0,b''
nlp_architect/data/cdc_resources/data_types/wn/wordnet_page.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nfrom typing import Set, Dict\n\n\nclass WordnetPage(object):\n    def __init__(\n        self,\n        orig_phrase: str,\n        clean_phrase: str,\n        head: str,\n        head_lemma: str,\n        head_synonyms: Set[str],\n        head_lemma_synonyms: Set[str],\n        head_derivationally: Set[str],\n        head_lemma_derivationally: Set[str],\n        all_clean_words_synonyms: Set[str],\n    ) -> None:\n        """"""\n        Object represent a Wikipedia Page and extracted fields.\n\n        Args:\n            orig_phrase (str): original search phrase\n            clean_phrase (str): original search phrase normalized\n            head (str): page title head\n            head_lemma (str): page title head lemma\n            head_synonyms (set): head synonyms words extracted from wordnet\n            head_lemma_synonyms (set): head lemma synonyms words extracted from wordnet\n            head_derivationally (set): wordnet head derivationally_related_forms()\n            head_lemma_derivationally (set): wordnet head lemma derivationally_related_forms()\n            all_clean_words_synonyms (set): clean_phrase wordnet synonyms\n        """"""\n        self.orig_phrase = orig_phrase\n        self.clean_phrase = clean_phrase\n        self.head = head\n        self.head_lemma = head_lemma\n        self.head_synonyms = head_synonyms\n        self.head_lemma_synonyms = head_lemma_synonyms\n        self.head_derivationally = head_derivationally\n        self.head_lemma_derivationally = head_lemma_derivationally\n        self.all_clean_words_synonyms = all_clean_words_synonyms\n\n    def __eq__(self, other):\n        return (\n            self.orig_phrase == other.orig_phrase\n            and self.head == other.head\n            and self.head_lemma == other.head_lemma\n        )\n\n    def __hash__(self):\n        return hash(self.orig_phrase) + hash(self.head) + hash(self.head_lemma)\n\n    def toJson(self) -> Dict:\n        result_dict = dict()\n        result_dict[""orig_phrase""] = self.orig_phrase\n        result_dict[""clean_phrase""] = self.clean_phrase\n        result_dict[""head""] = self.head\n        result_dict[""head_lemma""] = self.head_lemma\n        result_dict[""head_synonyms""] = list(self.head_synonyms)\n        result_dict[""head_lemma_synonyms""] = list(self.head_lemma_synonyms)\n        result_dict[""head_derivationally""] = list(self.head_derivationally)\n        result_dict[""head_lemma_derivationally""] = list(self.head_lemma_derivationally)\n        if self.all_clean_words_synonyms is not None:\n            all_as_list = []\n            for set_ in self.all_clean_words_synonyms:\n                all_as_list.append(list(set_))\n            result_dict[""all_clean_words_synonyms""] = all_as_list\n\n        return result_dict\n'"
nlp_architect/models/bist/eval/conllu/__init__.py,0,b''
nlp_architect/models/bist/eval/conllu/conll17_ud_eval.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport os\nimport sys\n\n# Things that were changed from the original:\n# - Added legal header\n# - Reformatted code and variable names to conform with PEP8\n# - Added pointer to \'weights.clas\' file\n# - Added run_conllu_eval()\n# - Removed tests and command-line usage option\n# - Removed unnecessary imports\n# - Add pylint check disable flags\n\n# !/usr/bin/env python\n# CoNLL 2017 UD Parsing evaluation script.\n#\n# Compatible with Python 2.7 and 3.2+, can be used either as a module\n# or a standalone executable.\n#\n# Copyright 2017 Institute of Formal and Applied Linguistics (UFAL),\n# Faculty of Mathematics and Physics, Charles University, Czech Republic.\n#\n# Changelog:\n# - [02 Jan 2017] Version 0.9: Initial release\n# - [25 Jan 2017] Version 0.9.1: Fix bug in LCS alignment computation\n# - [10 Mar 2017] Version 1.0: Add documentation and test\n#                              Compare HEADs correctly using aligned words\n#                              Allow evaluation with errorneous spaces in forms\n#                              Compare forms in LCS case insensitively\n#                              Detect cycles and multiple root nodes\n#                              Compute AlignedAccuracy\n# API usage\n# ---------\n# - load_conllu(file)\n#   - loads CoNLL-U file from given file object to an internal representation\n#   - the file object should return str on both Python 2 and Python 3\n#   - raises UDError exception if the given file cannot be loaded\n# - evaluate(gold_ud, system_ud)\n#   - evaluate the given gold and system CoNLL-U files (loaded with\n#     load_conllu)\n#   - raises UDError if the concatenated tokens of gold and system file do not\n#     match\n#   - returns a dictionary with the metrics described above, each metrics\n#     having three fields: precision, recall and f1\n#\n# Description of token matching\n# -----------------------------\n# In order to match tokens of gold file and system file, we consider the text\n# resulting from concatenation of gold tokens and text resulting from\n# concatenation of system tokens. These texts should match -- if they do not,\n# the evaluation fails.\n#\n# If the texts do match, every token is represented as a range in this original\n# text, and tokens are equal only if their range is the same.\n#\n# Description of word matching\n# ----------------------------\n# When matching words of gold file and system file, we first match the tokens.\n# The words which are also tokens are matched as tokens, but words in\n# multi-word tokens have to be handled differently.\n#\n# To handle multi-word tokens, we start by finding ""multi-word spans"".\n# Multi-word span is a span in the original text such that\n# - it contains at least one multi-word token\n# - all multi-word tokens in the span (considering both gold and system ones)\n#   are completely inside the span (i.e., they do not ""stick out"")\n# - the multi-word span is as small as possible\n#\n# For every multi-word span, we align the gold and system words completely\n# inside this span using LCS on their FORMs. The words not intersecting\n# (even partially) any multi-word span are then aligned as tokens.\n# pylint: disable=too-many-statements\n\n# CoNLL-U column names\nID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = list(range(10))\n\nWEIGHTS = os.path.join(os.path.abspath(os.path.dirname(__file__)), ""weights.clas"")\n\n\n# UD Error is used when raising exceptions in this module\nclass UDError(Exception):\n    pass\n\n\n# Load given CoNLL-U file into internal representation\ndef load_conllu(file):\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n\n    # Internal representation classes\n    class UDRepresentation:\n        # pylint: disable=too-few-public-methods\n        def __init__(self):\n            # Characters of all the tokens in the whole file.\n            # Whitespace between tokens is not included.\n            self.characters = []\n            # List of UDSpan instances with start&end indices into `characters`\n            self.tokens = []\n            # List of UDWord instances.\n            self.words = []\n            # List of UDSpan instances with start&end indices into `characters`\n            self.sentences = []\n\n    class UDSpan:\n        # pylint: disable=too-few-public-methods\n        def __init__(self, start, end):\n            self.start = start\n            # Note that self.end marks the first position **after the end** of\n            # span, so we can use characters[start:end] or range(start, end).\n            self.end = end\n\n    class UDWord:\n        # pylint: disable=too-few-public-methods\n        def __init__(self, span, columns, is_multiword):\n            # Span of this word (or MWT, see below) within\n            # ud_representation.characters.\n            self.span = span\n            # 10 columns of the CoNLL-U file: ID, FORM, LEMMA,...\n            self.columns = columns\n            # is_multiword==True means that this word is part of a multi-word\n            # token.\n            # In that case, self.span marks the span of the whole multi-word\n            # token.\n            self.is_multiword = is_multiword\n            # Reference to the UDWord instance representing the HEAD (or None\n            # if root).\n            self.parent = None\n            # Let\'s ignore language-specific deprel subtypes.\n            self.columns[DEPREL] = columns[DEPREL].split("":"")[0]\n\n    ud = UDRepresentation()\n\n    # Load the CoNLL-U file\n    index, sentence_start = 0, None\n    while True:\n        line = file.readline()\n        if not line:\n            break\n        line = line.rstrip(""\\r\\n"")\n\n        # Handle sentence start boundaries\n        if sentence_start is None:\n            # Skip comments\n            if line.startswith(""#""):\n                continue\n            # Start a new sentence\n            ud.sentences.append(UDSpan(index, 0))\n            sentence_start = len(ud.words)\n        if not line:\n            # Add parent UDWord links and check there are no cycles\n            def process_word(word):\n                if word.parent == ""remapping"":\n                    raise UDError(""There is a cycle in a sentence"")\n                if word.parent is None:\n                    head = int(word.columns[HEAD])\n                    if head > len(ud.words) - sentence_start:\n                        raise UDError(\n                            ""HEAD \'{}\' points outside of the sentence"".format(word.columns[HEAD])\n                        )\n                    if head:\n                        parent = ud.words[sentence_start + head - 1]\n                        word.parent = ""remapping""\n                        process_word(parent)\n                        word.parent = parent\n\n            for word in ud.words[sentence_start:]:\n                process_word(word)\n\n            # Check there is a single root node\n            if len([word for word in ud.words[sentence_start:] if word.parent is None]) != 1:\n                raise UDError(""There are multiple roots in a sentence"")\n\n            # End the sentence\n            ud.sentences[-1].end = index\n            sentence_start = None\n            continue\n\n        # Read next token/word\n        columns = line.split(""\\t"")\n        if len(columns) != 10:\n            raise UDError(\n                ""The CoNLL-U line does not contain 10 tab-separated columns: "" ""\'{}\'"".format(line)\n            )\n\n        # Skip empty nodes\n        if ""."" in columns[ID]:\n            continue\n\n        # Delete spaces from FORM  so gold.characters == system.characters\n        # even if one of them tokenizes the space.\n        columns[FORM] = columns[FORM].replace("" "", """")\n        if not columns[FORM]:\n            raise UDError(""There is an empty FORM in the CoNLL-U file"")\n\n        # Save token\n        ud.characters.extend(columns[FORM])\n        ud.tokens.append(UDSpan(index, index + len(columns[FORM])))\n        index += len(columns[FORM])\n\n        # Handle multi-word tokens to save word(s)\n        if ""-"" in columns[ID]:\n            try:\n                start, end = list(map(int, columns[ID].split(""-"")))\n            except Exception:\n                raise UDError(""Cannot parse multi-word token ID \'{}\'"".format(columns[ID]))\n\n            for _ in range(start, end + 1):\n                word_line = file.readline().rstrip(""\\r\\n"")\n                word_columns = word_line.split(""\\t"")\n                if len(word_columns) != 10:\n                    raise UDError(\n                        ""The CoNLL-U line does not contain 10 tab-separated ""\n                        ""columns: \'{}\'"".format(word_line)\n                    )\n                ud.words.append(UDWord(ud.tokens[-1], word_columns, is_multiword=True))\n        # Basic tokens/words\n        else:\n            try:\n                word_id = int(columns[ID])\n            except Exception:\n                raise UDError(""Cannot parse word ID \'{}\'"".format(columns[ID]))\n            if word_id != len(ud.words) - sentence_start + 1:\n                raise UDError(\n                    ""Incorrect word ID \'{}\' for word \'{}\', expected""\n                    "" \'{}\'"".format(columns[ID], columns[FORM], len(ud.words) - sentence_start + 1)\n                )\n\n            try:\n                head_id = int(columns[HEAD])\n            except Exception:\n                raise UDError(""Cannot parse HEAD \'{}\'"".format(columns[HEAD]))\n            if head_id < 0:\n                raise UDError(""HEAD cannot be negative"")\n\n            ud.words.append(UDWord(ud.tokens[-1], columns, is_multiword=False))\n\n    if sentence_start is not None:\n        raise UDError(""The CoNLL-U file does not end with empty line"")\n\n    return ud\n\n\n# Evaluate the gold and system treebanks (loaded using load_conllu).\ndef evaluate(gold_ud, system_ud, deprel_weights=None):\n    # pylint: disable=too-many-locals\n    class Score:\n        # pylint: disable=too-few-public-methods\n        def __init__(self, gold_total, system_total, correct, aligned_total=None):\n            self.precision = correct / system_total if system_total else 0.0\n            self.recall = correct / gold_total if gold_total else 0.0\n            self.f1 = (\n                2 * correct / (system_total + gold_total) if system_total + gold_total else 0.0\n            )\n            self.aligned_accuracy = correct / aligned_total if aligned_total else aligned_total\n\n    class AlignmentWord:\n        # pylint: disable=too-few-public-methods\n        def __init__(self, gold_word, system_word):\n            self.gold_word = gold_word\n            self.system_word = system_word\n            self.gold_parent = None\n            self.system_parent_gold_aligned = None\n\n    class Alignment:\n        def __init__(self, gold_words, system_words):\n            self.gold_words = gold_words\n            self.system_words = system_words\n            self.matched_words = []\n            self.matched_words_map = {}\n\n        def append_aligned_words(self, gold_word, system_word):\n            self.matched_words.append(AlignmentWord(gold_word, system_word))\n            self.matched_words_map[system_word] = gold_word\n\n        def fill_parents(self):\n            # We represent root parents in both gold and system data by \'0\'.\n            # For gold data, we represent non-root parent by corresponding gold\n            # word.\n            # For system data, we represent non-root parent by either gold word\n            # aligned\n            # to parent system nodes, or by None if no gold words is aligned to\n            # the parent.\n            for words in self.matched_words:\n                words.gold_parent = (\n                    words.gold_word.parent if words.gold_word.parent is not None else 0\n                )\n                words.system_parent_gold_aligned = (\n                    self.matched_words_map.get(words.system_word.parent, None)\n                    if words.system_word.parent is not None\n                    else 0\n                )\n\n    def lower(text):\n        if sys.version_info < (3, 0) and isinstance(text, str):\n            return text.decode(""utf-8"").lower()\n        return text.lower()\n\n    def spans_score(gold_spans, system_spans):\n        correct, gi, si = 0, 0, 0\n        while gi < len(gold_spans) and si < len(system_spans):\n            if system_spans[si].start < gold_spans[gi].start:\n                si += 1\n            elif gold_spans[gi].start < system_spans[si].start:\n                gi += 1\n            else:\n                correct += gold_spans[gi].end == system_spans[si].end\n                si += 1\n                gi += 1\n\n        return Score(len(gold_spans), len(system_spans), correct)\n\n    def alignment_score(alignment, key_fn, weight_fn=lambda w: 1):\n        gold, system, aligned, correct = 0, 0, 0, 0\n\n        for word in alignment.gold_words:\n            gold += weight_fn(word)\n\n        for word in alignment.system_words:\n            system += weight_fn(word)\n\n        for words in alignment.matched_words:\n            aligned += weight_fn(words.gold_word)\n\n        if key_fn is None:\n            # Return score for whole aligned words\n            return Score(gold, system, aligned)\n\n        for words in alignment.matched_words:\n            if key_fn(words.gold_word, words.gold_parent) == key_fn(\n                words.system_word, words.system_parent_gold_aligned\n            ):\n                correct += weight_fn(words.gold_word)\n\n        return Score(gold, system, correct, aligned)\n\n    def beyond_end(words, i, multiword_span_end):\n        if i >= len(words):\n            return True\n        if words[i].is_multiword:\n            return words[i].span.start >= multiword_span_end\n        return words[i].span.end > multiword_span_end\n\n    def extend_end(word, multiword_span_end):\n        if word.is_multiword and word.span.end > multiword_span_end:\n            return word.span.end\n        return multiword_span_end\n\n    def find_multiword_span(gold_words, system_words, gi, si):\n        # We know gold_words[gi].is_multiword or system_words[si].is_multiword.\n        # Find the start of the multiword span (gs, ss), so the multiword span\n        # is minimal.\n        # Initialize multiword_span_end characters index.\n        if gold_words[gi].is_multiword:\n            multiword_span_end = gold_words[gi].span.end\n            if (\n                not system_words[si].is_multiword\n                and system_words[si].span.start < gold_words[gi].span.start\n            ):\n                si += 1\n        else:  # if system_words[si].is_multiword\n            multiword_span_end = system_words[si].span.end\n            if (\n                not gold_words[gi].is_multiword\n                and gold_words[gi].span.start < system_words[si].span.start\n            ):\n                gi += 1\n        gs, ss = gi, si\n\n        # Find the end of the multiword span\n        # (so both gi and si are pointing to the word following the multiword\n        # span end).\n        while not beyond_end(gold_words, gi, multiword_span_end) or not beyond_end(\n            system_words, si, multiword_span_end\n        ):\n            gold_start = gold_words[gi].span.start\n            sys_start = system_words[si].span.start\n            if gi < len(gold_words) and (si >= len(system_words) or gold_start <= sys_start):\n                multiword_span_end = extend_end(gold_words[gi], multiword_span_end)\n                gi += 1\n            else:\n                multiword_span_end = extend_end(system_words[si], multiword_span_end)\n                si += 1\n        return gs, ss, gi, si\n\n    def compute_lcs(gold_words, system_words, gi, si, gs, ss):\n        # pylint: disable=too-many-arguments\n        lcs = [[0] * (si - ss) for _ in range(gi - gs)]\n        for g in reversed(list(range(gi - gs))):\n            for s in reversed(list(range(si - ss))):\n                if lower(gold_words[gs + g].columns[FORM]) == lower(\n                    system_words[ss + s].columns[FORM]\n                ):\n                    lcs[g][s] = 1 + (\n                        lcs[g + 1][s + 1] if g + 1 < gi - gs and s + 1 < si - ss else 0\n                    )\n                lcs[g][s] = max(lcs[g][s], lcs[g + 1][s] if g + 1 < gi - gs else 0)\n                lcs[g][s] = max(lcs[g][s], lcs[g][s + 1] if s + 1 < si - ss else 0)\n        return lcs\n\n    def align_words(gold_words, system_words):\n        alignment = Alignment(gold_words, system_words)\n\n        gi, si = 0, 0\n        while gi < len(gold_words) and si < len(system_words):\n            if gold_words[gi].is_multiword or system_words[si].is_multiword:\n                # A: Multi-word tokens => align via LCS within the whole\n                # ""multiword span"".\n                gs, ss, gi, si = find_multiword_span(gold_words, system_words, gi, si)\n\n                if si > ss and gi > gs:\n                    lcs = compute_lcs(gold_words, system_words, gi, si, gs, ss)\n\n                    # Store aligned words\n                    s, g = 0, 0\n                    while g < gi - gs and s < si - ss:\n                        if lower(gold_words[gs + g].columns[FORM]) == lower(\n                            system_words[ss + s].columns[FORM]\n                        ):\n                            alignment.append_aligned_words(gold_words[gs + g], system_words[ss + s])\n                            g += 1\n                            s += 1\n                        elif lcs[g][s] == (lcs[g + 1][s] if g + 1 < gi - gs else 0):\n                            g += 1\n                        else:\n                            s += 1\n            else:\n                # B: No multi-word token => align according to spans.\n                if (gold_words[gi].span.start, gold_words[gi].span.end) == (\n                    system_words[si].span.start,\n                    system_words[si].span.end,\n                ):\n                    alignment.append_aligned_words(gold_words[gi], system_words[si])\n                    gi += 1\n                    si += 1\n                elif gold_words[gi].span.start <= system_words[si].span.start:\n                    gi += 1\n                else:\n                    si += 1\n\n        alignment.fill_parents()\n\n        return alignment\n\n    # Check that underlying character sequences do match\n    if gold_ud.characters != system_ud.characters:\n        index = 0\n        while gold_ud.characters[index] == system_ud.characters[index]:\n            index += 1\n\n        raise UDError(\n            ""The concatenation of tokens in gold file and in system file ""\n            ""differ!\\n"" + ""First 20 differing characters in gold file: \'{}\' and system file:""\n            "" \'{}\'"".format(\n                """".join(gold_ud.characters[index : index + 20]),\n                """".join(system_ud.characters[index : index + 20]),\n            )\n        )\n\n    # Align words\n    alignment = align_words(gold_ud.words, system_ud.words)\n\n    # Compute the F1-scores\n    result = {\n        ""Tokens"": spans_score(gold_ud.tokens, system_ud.tokens),\n        ""Sentences"": spans_score(gold_ud.sentences, system_ud.sentences),\n        ""Words"": alignment_score(alignment, None),\n        ""UPOS"": alignment_score(alignment, lambda w, parent: w.columns[UPOS]),\n        ""XPOS"": alignment_score(alignment, lambda w, parent: w.columns[XPOS]),\n        ""Feats"": alignment_score(alignment, lambda w, parent: w.columns[FEATS]),\n        ""AllTags"": alignment_score(\n            alignment, lambda w, parent: (w.columns[UPOS], w.columns[XPOS], w.columns[FEATS])\n        ),\n        ""Lemmas"": alignment_score(alignment, lambda w, parent: w.columns[LEMMA]),\n        ""UAS"": alignment_score(alignment, lambda w, parent: parent),\n        ""LAS"": alignment_score(alignment, lambda w, parent: (parent, w.columns[DEPREL])),\n    }\n\n    # Add WeightedLAS if weights are given\n    if deprel_weights is not None:\n\n        def weighted_las(word):\n            return deprel_weights.get(word.columns[DEPREL], 1.0)\n\n        result[""WeightedLAS""] = alignment_score(\n            alignment, lambda w, parent: (parent, w.columns[DEPREL]), weighted_las\n        )\n\n    return result\n\n\ndef load_deprel_weights(weights_file):\n    if weights_file is None:\n        return None\n\n    deprel_weights = {}\n    with open(weights_file) as f:\n        for line in f:\n            # Ignore comments and empty lines\n            if line.startswith(""#"") or not line.strip():\n                continue\n\n            columns = line.rstrip(""\\r\\n"").split()\n            if len(columns) != 2:\n                raise ValueError(\n                    ""Expected two columns in the UD Relations weights file on line""\n                    "" \'{}\'"".format(line)\n                )\n\n            deprel_weights[columns[0]] = float(columns[1])\n\n    return deprel_weights\n\n\ndef load_conllu_file(path):\n    with open(\n        path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {})\n    ) as _file:\n        return load_conllu(_file)\n\n\ndef evaluate_wrapper(gold_file: str, system_file: str, weights_file: str):\n    # Load CoNLL-U files\n    gold_ud = load_conllu_file(gold_file)\n    system_ud = load_conllu_file(system_file)\n\n    # Load weights if requested\n    deprel_weights = load_deprel_weights(weights_file)\n\n    return evaluate(gold_ud, system_ud, deprel_weights)\n\n\ndef run_conllu_eval(gold_file, test_file, weights_file=WEIGHTS, verbose=True):\n    # Use verbose if weights are supplied\n    if weights_file is not None and not verbose:\n        verbose = True\n\n    # Evaluate\n    evaluation = evaluate_wrapper(gold_file, test_file, weights_file)\n\n    # Write the evaluation to file\n    with open(test_file[: test_file.rindex(""."")] + ""_eval.txt"", ""w"") as out_file:\n        if not verbose:\n            out_file.write(""LAS F1 Score: {:.2f}"".format(100 * evaluation[""LAS""].f1) + ""\\n"")\n        else:\n            metrics = [\n                ""Tokens"",\n                ""Sentences"",\n                ""Words"",\n                ""UPOS"",\n                ""XPOS"",\n                ""Feats"",\n                ""AllTags"",\n                ""Lemmas"",\n                ""UAS"",\n                ""LAS"",\n            ]\n            if weights_file is not None:\n                metrics.append(""WeightedLAS"")\n\n            out_file.write(""Metrics    | Precision |    Recall |  F1 Score | AligndAcc"" + ""\\n"")\n            out_file.write(""-----------+-----------+-----------+-----------+-----------"" + ""\\n"")\n            for metric in metrics:\n                out_file.write(\n                    ""{:11}|{:10.2f} |{:10.2f} |{:10.2f} |{}"".format(\n                        metric,\n                        100 * evaluation[metric].precision,\n                        100 * evaluation[metric].recall,\n                        100 * evaluation[metric].f1,\n                        ""{:10.2f}"".format(100 * evaluation[metric].aligned_accuracy)\n                        if evaluation[metric].aligned_accuracy is not None\n                        else """",\n                    )\n                    + ""\\n""\n                )\n'"
nlp_architect/models/cross_doc_coref/system/sieves/__init__.py,0,b''
nlp_architect/models/cross_doc_coref/system/sieves/run_sieve_system.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nimport logging\nimport time\n\nfrom nlp_architect.common.cdc.cluster import Clusters\nfrom nlp_architect.common.cdc.topics import Topic\nfrom nlp_architect.models.cross_doc_coref.system.sieves.sieves import SieveClusterMerger\nfrom nlp_architect.models.cross_doc_coref.system.sieves_container_init import (\n    SievesContainerInitialization,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass RunSystemsSuper(object):\n    def __init__(self, topic: Topic):\n        self.sieves = []\n        self.results_dict = dict()\n        self.results_ordered = []\n        logger.info(""loading topic %s, total mentions: %d"", topic.topic_id, len(topic.mentions))\n        self.clusters = Clusters(topic.topic_id, topic.mentions)\n\n    @staticmethod\n    def set_sieves_from_config(config, get_rel_extraction):\n        sieves = []\n        for _type_tup in config.sieves_order:\n            sieves.append(SieveClusterMerger(_type_tup, get_rel_extraction(_type_tup[0])))\n        return sieves\n\n    def run_deterministic(self):\n        # pylint: disable=too-many-nested-blocks\n        for sieve in self.sieves:\n            start = time.time()\n            clusters_changed = True\n            merge_count = 0\n            while clusters_changed:\n                clusters_changed = False\n                clusters_size = len(self.clusters.clusters_list)\n                for i in range(0, clusters_size):\n                    cluster_i = self.clusters.clusters_list[i]\n                    if cluster_i.merged:\n                        continue\n\n                    for j in range(i + 1, clusters_size):\n                        cluster_j = self.clusters.clusters_list[j]\n                        if cluster_j.merged:\n                            continue\n\n                        if cluster_i is not cluster_j:\n                            criterion = sieve.run_sieve(cluster_i, cluster_j)\n                            if criterion:\n                                merge_count += 1\n                                clusters_changed = True\n                                cluster_i.merge_clusters(cluster_j)\n                                cluster_j.merged = True\n\n                if clusters_changed:\n                    self.clusters.clean_clusters()\n\n            end = time.time()\n            took = end - start\n            logger.info(\n                ""Total of %d clusters merged using method: %s, took: %.4f sec"",\n                merge_count,\n                str(sieve.excepted_relation),\n                took,\n            )\n\n        return self.clusters\n\n    def get_results(self):\n        return self.results_ordered\n\n\nclass RunSystemsEntity(RunSystemsSuper):\n    def __init__(self, topic: Topic, resources):\n        super(RunSystemsEntity, self).__init__(topic)\n        self.sieves = self.set_sieves_from_config(\n            resources.entity_config, resources.get_module_from_relation\n        )\n\n\nclass RunSystemsEvent(RunSystemsSuper):\n    def __init__(self, topic, resources):\n        super(RunSystemsEvent, self).__init__(topic)\n        self.sieves = self.set_sieves_from_config(\n            resources.event_config, resources.get_module_from_relation\n        )\n\n\n# pylint: disable=no-else-return\ndef get_run_system(topic: Topic, resource: SievesContainerInitialization, eval_type: str):\n    if eval_type.lower() == ""entity"":\n        return RunSystemsEntity(topic, resource)\n    if eval_type.lower() == ""event"":\n        return RunSystemsEvent(topic, resource)\n    else:\n        raise AttributeError(eval_type + "" Not supported!"")\n'"
nlp_architect/models/cross_doc_coref/system/sieves/sieves.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport logging\nfrom typing import Tuple\n\nfrom nlp_architect.common.cdc.cluster import Cluster\nfrom nlp_architect.data.cdc_resources.relations.relation_extraction import RelationExtraction\nfrom nlp_architect.data.cdc_resources.relations.relation_types_enums import RelationType\n\nlogger = logging.getLogger(__name__)\n\n\nclass SieveClusterMerger(object):\n    def __init__(\n        self, excepted_relation: Tuple[RelationType, float], relation_extractor: RelationExtraction\n    ):\n        """"""\n        Args:\n            excepted_relation: tuple with relation to run in sieve,\n            threshold to merge clusters\n            relation_extractor:\n        """"""\n        self.excepted_relation = excepted_relation[0]\n        self.threshold = excepted_relation[1]\n        self.relation_extractor = relation_extractor\n\n        logger.info(\n            ""init Sieve, for relation-%s with threshold=%.1f"",\n            self.excepted_relation.name,\n            self.threshold,\n        )\n\n    def run_sieve(self, cluster_i: Cluster, cluster_j: Cluster) -> bool:\n        """"""\n        Args:\n            cluster_i:\n            cluster_j:\n\n        Returns:\n            bool -> indicating whether to merge clusters (True) or not (False)\n        """"""\n        matches = 0\n        for mention_i in cluster_i.mentions:\n            for mention_j in cluster_j.mentions:\n                match_result = self.relation_extractor.extract_sub_relations(\n                    mention_i, mention_j, self.excepted_relation\n                )\n                if match_result == self.excepted_relation:\n                    matches += 1\n\n        possible_pairs_len = float(len(cluster_i.mentions) * len(cluster_j.mentions))\n        matches_rate = matches / possible_pairs_len\n\n        result = False\n        if matches_rate >= self.threshold:\n            result = True\n\n        return result\n'"
nlp_architect/nn/tensorflow/python/keras/__init__.py,0,b''
nlp_architect/nn/tensorflow/python/keras/callbacks.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom nlp_architect.utils.metrics import get_conll_scores\n\n\nclass ConllCallback(tf.keras.callbacks.Callback):\n    """"""\n    A Tensorflow(Keras) Conlleval evaluator.\n    Runs the conlleval script for given x and y inputs.\n    Prints Conlleval F1 score on the end of each epoch.\n\n    Args:\n        x: features matrix\n        y: labels matrix\n        y_vocab (dict): int-to-str labels lexicon\n        batch_size (:obj:`int`, optional): batch size\n    """"""\n\n    def __init__(self, x, y, y_vocab, batch_size=1):\n        super(ConllCallback, self).__init__()\n        self.x = x\n        self.y = y\n        self.y_vocab = {v: k for k, v in y_vocab.items()}\n        self.bsz = batch_size\n\n    def on_epoch_end(self, epoch, logs=None):\n        predictions = self.model.predict(self.x, batch_size=self.bsz)\n        stats = get_conll_scores(predictions, self.y, self.y_vocab)\n        print()\n        print(""Conll eval: \\n{}"".format(stats))\n'"
nlp_architect/nn/tensorflow/python/keras/layers/__init__.py,0,b''
nlp_architect/nn/tensorflow/python/keras/layers/crf.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport tensorflow as tf\n\n\nclass CRF(tf.keras.layers.Layer):\n    """"""\n    Conditional Random Field layer (tf.keras)\n    `CRF` can be used as the last layer in a network (as a classifier). Input shape (features)\n    must be equal to the number of classes the CRF can predict (a linear layer is recommended).\n\n    Note: the loss and accuracy functions of networks using `CRF` must\n    use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy)\n    as the classification of sequences are used with the layers internal weights.\n\n    Args:\n        num_labels (int): the number of labels to tag each temporal input.\n\n    Input shape:\n        nD tensor with shape `(batch_size, sentence length, num_classes)`.\n\n    Output shape:\n        nD tensor with shape: `(batch_size, sentence length, num_classes)`.\n    """"""\n\n    def __init__(self, num_classes, **kwargs):\n        self.transitions = None\n        super(CRF, self).__init__(**kwargs)\n        # num of output labels\n        self.output_dim = int(num_classes)\n        self.input_spec = tf.keras.layers.InputSpec(min_ndim=3)\n        self.supports_masking = False\n        self.sequence_lengths = None\n\n    def get_config(self):\n        config = {\n            ""output_dim"": self.output_dim,\n            ""supports_masking"": self.supports_masking,\n            ""transitions"": tf.keras.backend.eval(self.transitions),\n        }\n        base_config = super(CRF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        f_shape = tf.TensorShape(input_shape)\n        input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n\n        if f_shape[-1] is None:\n            raise ValueError(\n                ""The last dimension of the inputs to `CRF` "" ""should be defined. Found `None`.""\n            )\n        if f_shape[-1] != self.output_dim:\n            raise ValueError(\n                ""The last dimension of the input shape must be equal to output""\n                "" shape. Use a linear layer if needed.""\n            )\n        self.input_spec = input_spec\n        self.transitions = self.add_weight(\n            name=""transitions"",\n            shape=[self.output_dim, self.output_dim],\n            initializer=""glorot_uniform"",\n            trainable=True,\n        )\n        self.built = True\n\n    # pylint: disable=arguments-differ\n    def call(self, inputs, sequence_lengths=None, **kwargs):\n        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n        if sequence_lengths is not None:\n            assert len(sequence_lengths.shape) == 2\n            assert tf.convert_to_tensor(sequence_lengths).dtype == ""int32""\n            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n            assert seq_len_shape[1] == 1\n            self.sequence_lengths = tf.keras.backend.flatten(sequence_lengths)\n        else:\n            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n                tf.shape(inputs)[1]\n            )\n\n        viterbi_sequence, _ = tf.contrib.crf.crf_decode(\n            sequences, self.transitions, self.sequence_lengths\n        )\n        output = tf.keras.backend.one_hot(viterbi_sequence, self.output_dim)\n        return tf.keras.backend.in_train_phase(sequences, output)\n\n    def loss(self, y_true, y_pred):\n        y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n        log_likelihood, self.transitions = tf.contrib.crf.crf_log_likelihood(\n            y_pred,\n            tf.cast(tf.keras.backend.argmax(y_true), dtype=tf.int32),\n            self.sequence_lengths,\n            transition_params=self.transitions,\n        )\n        return tf.reduce_mean(-log_likelihood)\n\n    def compute_output_shape(self, input_shape):\n        tf.TensorShape(input_shape).assert_has_rank(3)\n        return input_shape[:2] + (self.output_dim,)\n\n    @property\n    def viterbi_accuracy(self):\n        def accuracy(y_true, y_pred):\n            shape = tf.shape(y_pred)\n            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n            viterbi_sequence, _ = tf.contrib.crf.crf_decode(\n                y_pred, self.transitions, sequence_lengths\n            )\n            output = tf.keras.backend.one_hot(viterbi_sequence, self.output_dim)\n            return tf.keras.metrics.categorical_accuracy(y_true, output)\n\n        accuracy.func_name = ""viterbi_accuracy""\n        return accuracy\n'"
nlp_architect/nn/tensorflow/python/keras/utils/__init__.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\n# flake8: noqa\nfrom nlp_architect.nn.tensorflow.python.keras.utils.layer_utils import save_model, load_model\n'"
nlp_architect/nn/tensorflow/python/keras/utils/layer_utils.py,0,"b'# ******************************************************************************\n# Copyright 2017-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ******************************************************************************\nimport pickle\nimport tempfile\n\nfrom tensorflow import keras\n\n\ndef save_model(model: keras.models.Model, topology: dict, filepath: str) -> None:\n    """"""\n    Save a model to a file (tf.keras models only)\n    The method save the model topology, as given as a\n    Args:\n        model: model object\n        topology (dict): a dictionary of topology elements and their values\n        filepath (str): path to save model\n    """"""\n    with tempfile.NamedTemporaryFile(suffix="".h5"", delete=True) as fd:\n        model.save_weights(fd.name)\n        model_weights = fd.read()\n    data = {""model_weights"": model_weights, ""model_topology"": topology}\n    with open(filepath, ""wb"") as fp:\n        pickle.dump(data, fp)\n\n\ndef load_model(filepath, model) -> None:\n    """"""\n    Load a model (tf.keras) from disk, create topology from loaded values\n    and load weights.\n    Args:\n        filepath (str): path to model\n        model: model object to load\n    """"""\n    with open(filepath, ""rb"") as fp:\n        model_data = pickle.load(fp)\n    topology = model_data[""model_topology""]\n    model.build(**topology)\n    with tempfile.NamedTemporaryFile(suffix="".h5"", delete=True) as fd:\n        fd.write(model_data[""model_weights""])\n        fd.flush()\n        model.model.load_weights(fd.name)\n'"
