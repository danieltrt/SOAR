file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nfrom pathlib import Path\nfrom setuptools import setup, find_packages\n\nversion_file = Path(__file__).parent.joinpath(""adaptnlp"", ""VERSION.txt"")\nversion = version_file.read_text(encoding=""UTF-8"").strip()\n\nwith open(""requirements.txt"") as reqs_file:\n    install_requires = reqs_file.read().splitlines()\n\nwith open(""requirements_dev.txt"") as reqs_dev_file:\n    dev_requires = reqs_dev_file.read().splitlines()\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=""adaptnlp"",\n    version=version,\n    author=""Andrew Chang"",\n    author_email=""achang@novetta.com"",\n    packages=find_packages(),\n    keywords=[\n        ""NLP"",\n        ""flair"",\n        ""Natural Language Processing"",\n        ""Machine Learning"",\n        ""ML"",\n        ""torch"",\n        ""pytorch"",\n        ""NER"",\n    ],\n    install_requires=install_requires,\n    extras_require={\n        \'dev\': dev_requires\n    },\n    license=""Apache 2.0"",\n    description=""AdaptNLP: A Natural Language Processing Library and Framework"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    include_package_data=True,\n    zip_safe=True,\n)\n'"
adaptnlp/__init__.py,0,"b'import pkg_resources\nfrom pathlib import Path\n\n# Easy Modules\nfrom .embeddings import (\n    EasyWordEmbeddings,\n    EasyStackedEmbeddings,\n    EasyDocumentEmbeddings,\n)\nfrom .token_classification import EasyTokenTagger\nfrom .sequence_classification import (\n    EasySequenceClassifier,\n    TransformersSequenceClassifier,\n    FlairSequenceClassifier,\n)\nfrom .question_answering import EasyQuestionAnswering, TransformersQuestionAnswering\nfrom .summarization import EasySummarizer, TransformersSummarizer\nfrom .translation import EasyTranslator, TransformersTranslator\n\n# Training and Fine-tuning Modules\nfrom .training import SequenceClassifierTrainer\nfrom .transformers.finetuning import LMFineTuner\n\n# global variable like flair\'s: cache_root\ncache_root = Path(Path.home(), "".adaptnlp"")\n\n__version__ = (\n    pkg_resources.resource_string(""adaptnlp"", ""VERSION.txt"").decode(""UTF-8"").strip()\n)\n\n__all__ = [\n    ""__version__"",\n    ""EasyWordEmbeddings"",\n    ""EasyStackedEmbeddings"",\n    ""EasyDocumentEmbeddings"",\n    ""EasyTokenTagger"",\n    ""EasySequenceClassifier"",\n    ""FlairSequenceClassifier"",\n    ""TransformersSequenceClassifier"",\n    ""EasyQuestionAnswering"",\n    ""TransformersQuestionAnswering"",\n    ""EasySummarizer"",\n    ""TransformersSummarizer"",\n    ""EasyTranslator"",\n    ""TransformersTranslator"",\n    ""SequenceClassifierTrainer"",\n    ""LMFineTuner"",\n]\n'"
adaptnlp/__main__.py,0,"b'import click\n\nfrom . import __version__\n\n\n@click.group(\n    help=""AdaptNLP: an easy and flexible NLP library for SOTA of the art NLP tasks""\n)\n@click.version_option(version=__version__)\ndef _main() -> None:\n    pass\n\n\n@_main.command(name=""echo"", help=""Echos a message for testing"")\n@click.option(""-m"", ""--message"", required=True, type=str, help=""Message to echo"")\ndef _echo(message: str) -> None:\n    print(message)\n\n\nif __name__ == ""__main__"":\n    _main(prog_name=""adaptnlp"")\n'"
adaptnlp/embeddings.py,0,"b'import logging\nfrom typing import List, Dict, Union\nfrom collections import defaultdict\n\nfrom flair.data import Sentence\nfrom flair.embeddings import (\n    Embeddings,\n    WordEmbeddings,\n    StackedEmbeddings,\n    FlairEmbeddings,\n    BertEmbeddings,\n    DocumentPoolEmbeddings,\n    DocumentRNNEmbeddings,\n    OpenAIGPT2Embeddings,\n    XLNetEmbeddings,\n    XLMEmbeddings,\n    RoBERTaEmbeddings,\n    # MuseCrosslingualEmbeddings,\n)\n\nlogger = logging.getLogger(__name__)\n\nFLAIR_PRETRAINED_MODEL_NAMES = {\n    ""multi-forward"",\n    ""multi-backward"",\n    ""multi-v0-forward"",\n    ""multi-v0-backward"",\n    ""multi-v0-forward-fast"",\n    ""multi-v0-backward-fast"",\n    ""en-forward"",\n    ""en-backward"",\n    ""en-forward-fast"",\n    ""en-backward-fast"",\n    ""news-forward"",\n    ""news-backward"",\n    ""news-forward-fast"",\n    ""news-backward-fast"",\n    ""mix-forward"",\n    ""mix-backward"",\n    ""ar-forward"",\n    ""ar-backward"",\n    ""bg-forward-fast"",\n    ""bg-backward-fast"",\n    ""bg-forward"",\n    ""bg-backward"",\n    ""cs-forward"",\n    ""cs-backward"",\n    ""cs-v0-forward"",\n    ""cs-v0-backward"",\n    ""da-forward"",\n    ""da-backward"",\n    ""de-forward"",\n    ""de-backward"",\n    ""de-historic-ha-forward"",\n    ""de-historic-ha-backward"",\n    ""de-historic-wz-forward"",\n    ""de-historic-wz-backward"",\n    ""es-forward"",\n    ""es-backward"",\n    ""es-forward-fast"",\n    ""es-backward-fast"",\n    ""eu-forward"",\n    ""eu-backward"",\n    ""eu-v1-forward"",\n    ""eu-v1-backward"",\n    ""eu-v0-forward"",\n    ""eu-v0-backward"",\n    ""fa-forward"",\n    ""fa-backward"",\n    ""fi-forward"",\n    ""fi-backward"",\n    ""fr-forward"",\n    ""fr-backward"",\n    ""he-forward"",\n    ""he-backward"",\n    ""hi-forward"",\n    ""hi-backward"",\n    ""hr-forward"",\n    ""hr-backward"",\n    ""id-forward"",\n    ""id-backward"",\n    ""it-forward"",\n    ""it-backward"",\n    ""ja-forward"",\n    ""ja-backward"",\n    ""nl-forward"",\n    ""nl-backward"",\n    ""nl-v0-forward"",\n    ""nl-v0-backward"",\n    ""no-forward"",\n    ""no-backward"",\n    ""pl-forward"",\n    ""pl-backward"",\n    ""pl-opus-forward"",\n    ""pl-opus-backward"",\n    ""pt-forward"",\n    ""pt-backward"",\n    ""pubmed-forward"",\n    ""pubmed-backward"",\n    ""sl-forward"",\n    ""sl-backward"",\n    ""sl-v0-forward"",\n    ""sl-v0-backward"",\n    ""sv-forward"",\n    ""sv-backward"",\n    ""sv-v0-forward"",\n    ""sv-v0-backward"",\n    ""ta-forward"",\n    ""ta-backward"",\n}\n\n\nclass EasyWordEmbeddings:\n    """""" Word embeddings from the latest language models\n\n    Usage:\n\n    ```python\n    >>> embeddings = adaptnlp.EasyWordEmbeddings()\n    >>> embeddings.embed_text(""text you want embeddings for"", model_name_or_path=""bert-base-cased"")\n    ```\n    """"""\n\n    def __init__(self):\n        self.models: Dict[Embeddings] = defaultdict(bool)\n\n    def embed_text(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        model_name_or_path: str = ""bert-base-cased"",\n    ) -> List[Sentence]:\n        """""" Produces embeddings for text\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        * **model_name_or_path** - The hosted model name key or model path\n        **return** - A list of Flair\'s `Sentence`s\n        """"""\n        # Convert into sentences\n        if isinstance(text, str):\n            sentences = Sentence(text)\n        elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n            sentences = [Sentence(t) for t in text]\n        else:\n            sentences = text\n\n        # Load correct Embeddings module\n        if not self.models[model_name_or_path]:\n            if ""bert"" in model_name_or_path and ""roberta"" not in model_name_or_path:\n                self.models[model_name_or_path] = BertEmbeddings(model_name_or_path)\n            elif ""roberta"" in model_name_or_path:\n                self.models[model_name_or_path] = RoBERTaEmbeddings(model_name_or_path)\n            elif ""gpt2"" in model_name_or_path:\n                self.models[model_name_or_path] = OpenAIGPT2Embeddings(\n                    model_name_or_path\n                )\n            elif ""xlnet"" in model_name_or_path:\n                self.models[model_name_or_path] = XLNetEmbeddings(model_name_or_path)\n            elif ""xlm"" in model_name_or_path:\n                self.models[model_name_or_path] = XLMEmbeddings(model_name_or_path)\n            elif (\n                ""flair"" in model_name_or_path\n                or model_name_or_path in FLAIR_PRETRAINED_MODEL_NAMES\n            ):\n                self.models[model_name_or_path] = FlairEmbeddings(model_name_or_path)\n            else:\n                try:\n                    self.models[model_name_or_path] = WordEmbeddings(model_name_or_path)\n                except ValueError:\n                    raise ValueError(\n                        f""Embeddings not found for the model key: {model_name_or_path}, check documentation or custom model path to verify specified model""\n                    )\n                return Sentence("""")\n        embedding = self.models[model_name_or_path]\n        return embedding.embed(sentences)\n\n    def embed_all(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        *model_names_or_paths: str,\n    ) -> List[Sentence]:\n        """"""Embeds text with all embedding models loaded\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        * **model_names_or_paths** -  A variable input of model names or paths to embed\n        **return** - A list of Flair\'s `Sentence`s\n        """"""\n        # Convert into sentences\n        if isinstance(text, str):\n            sentences = Sentence(text)\n        elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n            sentences = [Sentence(t) for t in text]\n        else:\n            sentences = text\n\n        if model_names_or_paths:\n            for embedding_name in model_names_or_paths:\n                sentences = self.embed_text(\n                    sentences, model_name_or_path=embedding_name\n                )\n        else:\n            for embedding_name in self.models.keys():\n                sentences = self.embed_text(\n                    sentences, model_name_or_path=embedding_name\n                )\n        return sentences\n\n\nclass EasyStackedEmbeddings:\n    """""" Word Embeddings that have been concatenated and ""stacked"" as specified by flair\n\n    Usage:\n\n    ```python\n    >>> embeddings = adaptnlp.EasyStackedEmbeddings(""bert-base-cased"", ""gpt2"", ""xlnet-base-cased"")\n    ```\n\n    **Parameters:**\n\n    * **&ast;embeddings** - Non-keyword variable number of strings specifying the embeddings you want to stack\n    """"""\n\n    def __init__(self, *embeddings: str):\n        print(""May need a couple moments to instantiate..."")\n        self.embedding_stack = []\n\n        # Load correct Embeddings module\n        for model_name_or_path in embeddings:\n            if ""bert"" in model_name_or_path and ""roberta"" not in model_name_or_path:\n                self.embedding_stack.append(BertEmbeddings(model_name_or_path))\n            elif ""roberta"" in model_name_or_path:\n                self.embedding_stack.append(RoBERTaEmbeddings(model_name_or_path))\n            elif ""gpt2"" in model_name_or_path:\n                self.embedding_stack.append(OpenAIGPT2Embeddings(model_name_or_path))\n            elif ""xlnet"" in model_name_or_path:\n                self.embedding_stack.append(XLNetEmbeddings(model_name_or_path))\n            elif ""xlm"" in model_name_or_path:\n                self.embedding_stack.append(XLMEmbeddings(model_name_or_path))\n            elif (\n                ""flair"" in model_name_or_path\n                or model_name_or_path in FLAIR_PRETRAINED_MODEL_NAMES\n            ):\n                self.embedding_stack.append(FlairEmbeddings(model_name_or_path))\n            else:\n                try:\n                    self.embedding_stack.append(WordEmbeddings(model_name_or_path))\n                except ValueError:\n                    raise ValueError(\n                        f""Embeddings not found for the model key: {model_name_or_path}, check documentation or custom model path to verify specified model""\n                    )\n\n        assert len(self.embedding_stack) != 0\n        self.stacked_embeddings = StackedEmbeddings(embeddings=self.embedding_stack)\n\n    def embed_text(\n        self, text: Union[List[Sentence], Sentence, List[str], str],\n    ) -> List[Sentence]:\n        """""" Stacked embeddings\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        **return** A list of Flair\'s `Sentence`s\n        """"""\n        # Convert into sentences\n        if isinstance(text, str):\n            sentences = [Sentence(text)]\n        elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n            sentences = [Sentence(t) for t in text]\n        elif isinstance(text, Sentence):\n            sentences = [text]\n        else:\n            sentences = text\n\n        # Unlike flair embeddings modules, stacked embeddings do not return a list of sentences\n        self.stacked_embeddings.embed(sentences)\n        return sentences\n\n\nclass EasyDocumentEmbeddings:\n    """""" Document Embeddings generated by pool and rnn methods applied to the word embeddings of text\n\n    Usage:\n\n    ```python\n    >>> embeddings = adaptnlp.EasyDocumentEmbeddings(""bert-base-cased"", ""xlnet-base-cased"", methods[""rnn""])\n    ```\n\n    **Parameters:**\n\n    * **&ast;embeddings** - Non-keyword variable number of strings referring to model names or paths\n    * **methods** - A list of strings to specify which document embeddings to use i.e. [""rnn"", ""pool""] (avoids unncessary loading of models if only using one)\n    * **configs** - A dictionary of configurations for flair\'s rnn and pool document embeddings\n    ```python\n    >>>example_configs = {""pool_configs"": {""fine_tune_mode"": ""linear"", ""pooling"": ""mean"", },\n    ...                   ""rnn_configs"": {""hidden_size"": 512,\n    ...                                   ""rnn_layers"": 1,\n    ...                                   ""reproject_words"": True,\n    ...                                   ""reproject_words_dimension"": 256,\n    ...                                   ""bidirectional"": False,\n    ...                                   ""dropout"": 0.5,\n    ...                                   ""word_dropout"": 0.0,\n    ...                                   ""locked_dropout"": 0.0,\n    ...                                   ""rnn_type"": ""GRU"",\n    ...                                   ""fine_tune"": True, },\n    ...                  }\n    ```\n    """"""\n\n    __allowed_methods = [""rnn"", ""pool""]\n    __allowed_configs = (""pool_configs"", ""rnn_configs"")\n\n    def __init__(\n        self,\n        *embeddings: str,\n        methods: List[str] = [""rnn"", ""pool""],\n        configs: Dict = {\n            ""pool_configs"": {""fine_tune_mode"": ""linear"", ""pooling"": ""mean""},\n            ""rnn_configs"": {\n                ""hidden_size"": 512,\n                ""rnn_layers"": 1,\n                ""reproject_words"": True,\n                ""reproject_words_dimension"": 256,\n                ""bidirectional"": False,\n                ""dropout"": 0.5,\n                ""word_dropout"": 0.0,\n                ""locked_dropout"": 0.0,\n                ""rnn_type"": ""GRU"",\n                ""fine_tune"": True,\n            },\n        },\n    ):\n        print(""May need a couple moments to instantiate..."")\n        self.embedding_stack = []\n\n        # Check methods\n        for m in methods:\n            assert m in self.__class__.__allowed_methods\n\n        # Set configs for pooling and rnn parameters\n        for k, v in configs.items():\n            assert k in self.__class__.__allowed_configs\n            setattr(self, k, v)\n\n        # Load correct Embeddings module\n        for model_name_or_path in embeddings:\n            if ""bert"" in model_name_or_path and ""roberta"" not in model_name_or_path:\n                self.embedding_stack.append(BertEmbeddings(model_name_or_path))\n            elif ""roberta"" in model_name_or_path:\n                self.embedding_stack.append(RoBERTaEmbeddings(model_name_or_path))\n            elif ""gpt2"" in model_name_or_path:\n                self.embedding_stack.append(OpenAIGPT2Embeddings(model_name_or_path))\n            elif ""xlnet"" in model_name_or_path:\n                self.embedding_stack.append(XLNetEmbeddings(model_name_or_path))\n            elif ""xlm"" in model_name_or_path:\n                self.embedding_stack.append(XLMEmbeddings(model_name_or_path))\n            elif (\n                ""flair"" in model_name_or_path\n                or model_name_or_path in FLAIR_PRETRAINED_MODEL_NAMES\n            ):\n                self.embedding_stack.append(FlairEmbeddings(model_name_or_path))\n            else:\n                try:\n                    self.embedding_stack.append(WordEmbeddings(model_name_or_path))\n                except ValueError:\n                    raise ValueError(\n                        f""Embeddings not found for the model key: {model_name_or_path}, check documentation or custom model path to verify specified model""\n                    )\n\n        assert len(self.embedding_stack) != 0\n        if ""pool"" in methods:\n            self.pool_embeddings = DocumentPoolEmbeddings(\n                self.embedding_stack, **self.pool_configs\n            )\n            print(""Pooled embedding loaded"")\n        if ""rnn"" in methods:\n            self.rnn_embeddings = DocumentRNNEmbeddings(\n                self.embedding_stack, **self.rnn_configs\n            )\n            print(""RNN embeddings loaded"")\n\n    def embed_pool(\n        self, text: Union[List[Sentence], Sentence, List[str], str],\n    ) -> List[Sentence]:\n        """""" Stacked embeddings\n\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        **return** - A list of Flair\'s `Sentence`s\n        """"""\n        if isinstance(text, str):\n            sentences = [Sentence(text)]\n        elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n            sentences = [Sentence(t) for t in text]\n        elif isinstance(text, Sentence):\n            sentences = [text]\n        else:\n            sentences = text\n        self.pool_embeddings.embed(sentences)\n        return sentences\n\n    def embed_rnn(\n        self, text: Union[List[Sentence], Sentence, List[str], str],\n    ) -> List[Sentence]:\n        """""" Stacked embeddings\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        **return** - A list of Flair\'s `Sentence`s\n        """"""\n        if isinstance(text, str):\n            sentences = [Sentence(text)]\n        elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n            sentences = [Sentence(t) for t in text]\n        elif isinstance(text, Sentence):\n            sentences = [text]\n        else:\n            sentences = text\n        self.rnn_embeddings.embed(sentences)\n        return sentences\n'"
adaptnlp/file_utils.py,0,"b'""""""\nUtilities for working with the local dataset cache.  Adapted from AllenNLP.\n""""""\n\nimport os\nimport logging\nimport shutil\nimport tempfile\nimport json\nfrom urllib.parse import urlparse\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, IO, Callable, Set\nfrom hashlib import sha256\nfrom functools import wraps\n\nfrom tqdm import tqdm as _tqdm\nimport boto3\nimport botocore\nfrom botocore.exceptions import ClientError\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\nimport adaptnlp\n\nlogger = logging.getLogger(__name__)\n\nCACHE_ROOT = adaptnlp.cache_root\nCACHE_DIRECTORY = str(CACHE_ROOT / ""cache"")\n\n\ndef url_to_filename(url: str, etag: str = None) -> str:\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(""utf-8"")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(""utf-8"")\n        etag_hash = sha256(etag_bytes)\n        filename += ""."" + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename: str, cache_dir: str = None) -> Tuple[str, str]:\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise FileNotFoundError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + "".json""\n    if not os.path.exists(meta_path):\n        raise FileNotFoundError(""file {} not found"".format(meta_path))\n\n    with open(meta_path) as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[""url""]\n    etag = metadata[""etag""]\n\n    return url, etag\n\n\ndef cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n\n    url_or_filename = os.path.expanduser(url_or_filename)\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (""http"", ""https"", ""s3""):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == """":\n        # File, but it doesn\'t exist.\n        raise FileNotFoundError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(\n            ""unable to parse {} as a URL or as a local path"".format(url_or_filename)\n        )\n\n\ndef is_url_or_existing_file(url_or_filename: Union[str, Path, None]) -> bool:\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine check if it\'s url or an existing file path.\n    """"""\n    if url_or_filename is None:\n        return False\n    url_or_filename = os.path.expanduser(str(url_or_filename))\n    parsed = urlparse(url_or_filename)\n    return parsed.scheme in (""http"", ""https"", ""s3"") or os.path.exists(url_or_filename)\n\n\ndef split_s3_path(url: str) -> Tuple[str, str]:\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func: Callable):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url: str, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise FileNotFoundError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\ndef get_s3_resource():\n    session = boto3.session.Session()\n    if session.get_credentials() is None:\n        # Use unsigned requests.\n        s3_resource = session.resource(\n            ""s3"", config=botocore.client.Config(signature_version=botocore.UNSIGNED)\n        )\n    else:\n        s3_resource = session.resource(""s3"")\n    return s3_resource\n\n\n@s3_request\ndef s3_etag(url: str) -> Optional[str]:\n    """"""Check ETag on S3 object.""""""\n    s3_resource = get_s3_resource()\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url: str, temp_file: IO) -> None:\n    """"""Pull a file directly from S3.""""""\n    s3_resource = get_s3_resource()\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef session_with_backoff() -> requests.Session:\n    """"""\n    We ran into an issue where http requests to s3 were timing out,\n    possibly because we were making too many requests too quickly.\n    This helper function returns a requests session that has retry-with-backoff\n    built in.\n    see stackoverflow.com/questions/23267409/how-to-implement-retry-mechanism-into-python-requests-library\n    """"""\n    session = requests.Session()\n    retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n    session.mount(""http://"", HTTPAdapter(max_retries=retries))\n    session.mount(""https://"", HTTPAdapter(max_retries=retries))\n\n    return session\n\n\ndef http_get(url: str, temp_file: IO) -> None:\n    with session_with_backoff() as session:\n        req = session.get(url, stream=True)\n        content_length = req.headers.get(""Content-Length"")\n        total = int(content_length) if content_length is not None else None\n        progress = Tqdm.tqdm(unit=""B"", total=total)\n        for chunk in req.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                progress.update(len(chunk))\n                temp_file.write(chunk)\n        progress.close()\n\n\ndef get_from_cache(url: str, cache_dir: str = None) -> str:\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        with session_with_backoff() as session:\n            response = session.head(url, allow_redirects=True)\n        if response.status_code != 200:\n            raise IOError(\n                ""HEAD request failed for url {} with status code {}"".format(\n                    url, response.status_code\n                )\n            )\n        etag = response.headers.get(""ETag"")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, ""wb"") as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {""url"": url, ""etag"": etag}\n            meta_path = cache_path + "".json""\n            with open(meta_path, ""w"") as meta_file:\n                json.dump(meta, meta_file)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename: str) -> Set[str]:\n    """"""\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    """"""\n    collection = set()\n    with open(filename, ""r"") as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path: str, dot=True, lower: bool = True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n\n\nclass Tqdm:\n    # These defaults are the same as the argument defaults in tqdm.\n    default_mininterval: float = 0.1\n\n    @staticmethod\n    def set_default_mininterval(value: float) -> None:\n        Tqdm.default_mininterval = value\n\n    @staticmethod\n    def set_slower_interval(use_slower_interval: bool) -> None:\n        """"""\n        If ``use_slower_interval`` is ``True``, we will dramatically slow down ``tqdm\'s`` default\n        output rate.  ``tqdm\'s`` default output rate is great for interactively watching progress,\n        but it is not great for log files.  You might want to set this if you are primarily going\n        to be looking at output through log files, not the terminal.\n        """"""\n        if use_slower_interval:\n            Tqdm.default_mininterval = 10.0\n        else:\n            Tqdm.default_mininterval = 0.1\n\n    @staticmethod\n    def tqdm(*args, **kwargs):\n        new_kwargs = {""mininterval"": Tqdm.default_mininterval, **kwargs}\n\n        return _tqdm(*args, **new_kwargs)\n'"
adaptnlp/model.py,0,"b'from typing import Union, List\nfrom pathlib import Path\nfrom abc import ABC, abstractmethod\n\nfrom flair.data import Sentence\n\n\nclass AdaptiveModel(ABC):\n    @abstractmethod\n    def load(\n        self, model_name_or_path: Union[str, Path],\n    ):\n        """""" Load model into the `AdaptiveModel` object as alternative constructor """"""\n        pass\n\n    @abstractmethod\n    def predict(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        mini_batch_size: int = 32,\n        **kwargs,\n    ) -> List[Sentence]:\n        """""" Run inference on the model """"""\n        pass\n'"
adaptnlp/question_answering.py,5,"b'import logging\nfrom typing import Tuple, List, Union, Dict\nfrom collections import OrderedDict, defaultdict\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    XLNetForQuestionAnswering,\n    XLMForQuestionAnswering,\n    CamembertForQuestionAnswering,\n    DistilBertForQuestionAnswering,\n    RobertaForQuestionAnswering,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    SquadExample,\n    squad_convert_examples_to_features,\n)\nfrom transformers.data.processors.squad import SquadResult\n\nfrom adaptnlp.transformers import BertQuestionAnsweringModel\nfrom adaptnlp.model import AdaptiveModel\nfrom adaptnlp.transformers.squad_metrics import (\n    compute_predictions_log_probs,\n    compute_predictions_logits,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformersQuestionAnswering(AdaptiveModel):\n    """""" Adaptive Model for Transformers Question Answering Model\n\n    **Parameters**\n\n    * **tokenizer** - A tokenizer object from Huggingface\'s transformers (TODO)and tokenizers *\n    * **model** - A transformer Question Answering model\n\n    """"""\n\n    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n        # Load up model and tokenizer\n        self.tokenizer = tokenizer\n        self.model = model\n\n        # Setup cuda and automatic allocation of model\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        self.model.to(self.device)\n\n        self.xmodel_instances = (XLNetForQuestionAnswering, XLMForQuestionAnswering)\n\n    @classmethod\n    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n        """""" Class method for loading and constructing this model\n\n        * **model_name_or_path** - A key string of one of Transformer\'s pre-trained Question Answering (SQUAD) models\n        """"""\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n        qa_model = cls(tokenizer, model)\n        return qa_model\n\n    def predict(\n        self,\n        query: Union[List[str], str],\n        context: Union[List[str], str],\n        n_best_size: int = 5,\n        mini_batch_size: int = 32,\n        max_answer_length: int = 10,\n        do_lower_case: bool = False,\n        version_2_with_negative: bool = False,\n        verbose_logging: bool = False,\n        null_score_diff_threshold: float = 0.0,\n        max_seq_length: int = 512,\n        doc_stride: int = 128,\n        max_query_length: int = 64,\n        **kwargs,\n    ) -> Tuple[Tuple[str, List[OrderedDict]], Tuple[OrderedDict, OrderedDict]]:\n        """""" Predict method for running inference using the pre-trained question answering model\n\n        * **query** - String or list of strings that specify the ordered questions corresponding to `context`\n        * **context** - String or list of strings that specify the ordered contexts corresponding to `query`\n        * **n_best_size** - Number of top n results you want\n        * **mini_batch_size** - Mini batch size\n        * **max_answer_length** - Maximum token length for answers that are returned\n        * **do_lower_case** - Set as `True` if using uncased QA models\n        * **version_2_with_negative** - Set as True if using QA model with SQUAD2.0\n        * **verbose_logging** - Set True if you want prediction verbose loggings\n        * **null_score_diff_threshold** - Threshold for predicting null(no answer) in Squad 2.0 Model.  Default is 0.0.  Raise this if you want fewer null answers\n        * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n        * **doc_stride** - Number of token strides to take when splitting up conext into chunks of size `max_seq_length`\n        * **max_query_length** - Maximum token length for queries\n        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers model (mostly for saving evaluations)\n        """"""\n        # Make string input consistent as list\n        if isinstance(query, str):\n            query = [query]\n            context = [context]\n        assert len(query) == len(context)\n\n        examples = self._mini_squad_processor(query=query, context=context)\n        features, dataset = squad_convert_examples_to_features(\n            examples,\n            self.tokenizer,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n            is_training=False,\n            return_dataset=""pt"",\n            threads=1,\n        )\n        all_results = []\n\n        with torch.no_grad():\n\n            dataloader = DataLoader(dataset, batch_size=mini_batch_size)\n\n            for batch in tqdm(dataloader, desc=""Predicting answer""):\n                self.model.eval()\n                batch = tuple(t.to(self.device) for t in batch)\n\n                inputs = {\n                    ""input_ids"": batch[0],\n                    ""attention_mask"": batch[1],\n                    ""token_type_ids"": batch[2],\n                }\n                example_indices = batch[3]\n\n                if isinstance(\n                    self.model,\n                    (\n                        XLMForQuestionAnswering,\n                        RobertaForQuestionAnswering,\n                        DistilBertForQuestionAnswering,\n                        CamembertForQuestionAnswering,\n                    ),\n                ):\n                    del inputs[""token_type_ids""]\n\n                # XLNet and XLM use more arguments for their predictions\n                if isinstance(self.model, self.xmodel_instances):\n                    inputs.update({""cls_index"": batch[4], ""p_mask"": batch[5]})\n                    # for lang_id-sensitive xlm models\n                    if hasattr(self.model, ""config"") and hasattr(\n                        self.model.config, ""lang2id""\n                    ):\n                        # Set language id as 0 for now\n                        inputs.update(\n                            {\n                                ""langs"": (\n                                    torch.ones(batch[0].shape, dtype=torch.int64) * 0\n                                ).to(self.device)\n                            }\n                        )\n\n                outputs = self.model(**inputs)\n\n                # Iterate through and produce `SquadResults\n                for i, example_index in enumerate(example_indices):\n                    eval_feature = features[example_index.item()]\n                    unique_id = int(eval_feature.unique_id)\n\n                    output = [self.to_list(output[i]) for output in outputs]\n\n                    if isinstance(self.model, self.xmodel_instances):\n                        # Some models like the ones in `self.xmodel_instances` use 5 arguments for their predictions\n                        start_logits = output[0]\n                        start_top_index = output[1]\n                        end_logits = output[2]\n                        end_top_index = output[3]\n                        cls_logits = output[4]\n\n                        result = SquadResult(\n                            unique_id,\n                            start_logits,\n                            end_logits,\n                            start_top_index=start_top_index,\n                            end_top_index=end_top_index,\n                            cls_logits=cls_logits,\n                        )\n\n                    else:\n                        start_logits, end_logits = output\n                        result = SquadResult(unique_id, start_logits, end_logits)\n                    all_results.append(result)\n\n            if isinstance(self.model, self.xmodel_instances):\n                start_n_top = (\n                    self.model.config.start_n_top\n                    if hasattr(self.model, ""config"")\n                    else self.model.module.config.start_n_top\n                )\n                end_n_top = (\n                    self.model.config.end_n_top\n                    if hasattr(self.model, ""config"")\n                    else self.model.module.config.end_n_top\n                )\n\n                answers, n_best = compute_predictions_log_probs(\n                    examples,\n                    features,\n                    all_results,\n                    n_best_size,\n                    max_answer_length,\n                    start_n_top,\n                    end_n_top,\n                    version_2_with_negative,\n                    self.tokenizer,\n                    verbose_logging,\n                    **kwargs,\n                )\n\n            else:\n                answers, n_best = compute_predictions_logits(\n                    examples,\n                    features,\n                    all_results,\n                    n_best_size,\n                    max_answer_length,\n                    do_lower_case,\n                    verbose_logging,\n                    version_2_with_negative,\n                    null_score_diff_threshold,\n                    self.tokenizer,\n                    **kwargs,\n                )\n\n        return answers, n_best\n\n    def to_list(self, tensor: torch.Tensor):\n        return tensor.detach().cpu().tolist()\n\n    def _mini_squad_processor(\n        self, query: List[str], context: List[str]\n    ) -> List[SquadExample]:\n        """""" Squad data processor to create `SquadExamples`\n\n        * **query** - List of query strings, must be same length as `context`\n        * **context** - List of context strings, must be same length as `query`\n\n        """"""\n        assert len(query) == len(context)\n        examples = []\n        title = ""qa""\n        is_impossible = False\n        answer_text = None\n        start_position_character = None\n        answers = [""answer""]\n        for idx, (q, c) in enumerate(zip(query, context)):\n            example = SquadExample(\n                qas_id=str(idx),\n                question_text=q,\n                context_text=c,\n                answer_text=answer_text,\n                start_position_character=start_position_character,\n                title=title,\n                is_impossible=is_impossible,\n                answers=answers,\n            )\n            examples.append(example)\n        return examples\n\n\nclass EasyQuestionAnswering:\n    """"""Question Answering Module\n\n    Usage:\n\n    ```python\n    >>> qa = adaptnlp.EasyQuestionAnswering()\n    >>> qa.predict_qa(query=""What is life?"", context=""Life is NLP."", n_best_size=5, mini_batch_size=1)\n    ```\n    """"""\n\n    def __init__(self):\n        self.models: Dict[AdaptiveModel] = defaultdict(bool)\n\n        # Soon to deprecated\n        self.bert_qa = None\n\n    # Dynamic Loaders\n    def _load_bert_qa(self) -> None:\n        self.bert_qa = BertQuestionAnsweringModel()\n\n    # Soon to be deprecated\n    def predict_bert_qa(\n        self, query: str, context: str, n_best_size: int = 20\n    ) -> Tuple[str, List[OrderedDict]]:\n        """""" Predicts top_n answer spans of query in regards to context\n\n\n        * **query** - The question\n        * **context** - The context of which the question is asking\n        * **n_best_size** - The top n answers returned\n\n        **return** - Either a list of string answers or a dict of the results\n        """"""\n\n        self._load_bert_qa() if not self.bert_qa else None\n        return self.bert_qa.predict(\n            query=query, context=context, n_best_size=n_best_size\n        )\n\n    def predict_qa(\n        self,\n        query: Union[List[str], str],\n        context: Union[List[str], str],\n        n_best_size: int = 5,\n        mini_batch_size: int = 32,\n        model_name_or_path: str = ""bert-large-uncased-whole-word-masking-finetuned-squad"",\n        **kwargs,\n    ) -> Tuple[Tuple[str, List[OrderedDict]], Tuple[OrderedDict, OrderedDict]]:\n        """""" Predicts top_n answer spans of query in regards to context\n\n        * **query** - String or list of strings that specify the ordered questions corresponding to `context`\n        * **context** - String or list of strings that specify the ordered contexts corresponding to `query`\n        * **n_best_size** - The top n answers returned\n        * **mini_batch_size** - Mini batch size for inference\n        * **model_name_or_path** - Path to QA model or name of QA model at huggingface.co/models\n        * **kwargs**(Optional) - Keyword arguments for `AdaptiveModel`s like `TransformersQuestionAnswering`\n\n        **return** - Either a list of string answers or a dict of the results\n        """"""\n        try:\n            if not self.models[model_name_or_path]:\n                self.models[model_name_or_path] = TransformersQuestionAnswering.load(\n                    model_name_or_path\n                )\n        except OSError:\n            logger.info(\n                f""{model_name_or_path} not a valid Transformers pre-trained QA model...check path or huggingface.co/models""\n            )\n            raise ValueError(\n                f""{model_name_or_path} is not a valid path or model name from huggingface.co/models""\n            )\n            return OrderedDict(), [OrderedDict()]\n\n        model = self.models[model_name_or_path]\n\n        # If query and context is just one instance, get rid of nested OrderedDict\n        if isinstance(query, str):\n            top_answer, top_n_answers = model.predict(\n                query=query,\n                context=context,\n                n_best_size=n_best_size,\n                mini_batch_size=mini_batch_size,\n                **kwargs,\n            )\n            return top_answer[""0""], top_n_answers[""0""]\n\n        return model.predict(\n            query=query,\n            context=context,\n            n_best_size=n_best_size,\n            mini_batch_size=mini_batch_size,\n            **kwargs,\n        )\n'"
adaptnlp/sequence_classification.py,4,"b'import logging\nfrom typing import List, Dict, Union, Tuple\nfrom collections import defaultdict\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom flair.data import Sentence, DataPoint, Label\nfrom flair.models import TextClassifier\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    PreTrainedTokenizer,\n    PreTrainedModel,\n    BertForSequenceClassification,\n    XLNetForSequenceClassification,\n    AlbertForSequenceClassification,\n)\n\nfrom tqdm import tqdm\n\nfrom adaptnlp.model import AdaptiveModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformersSequenceClassifier(AdaptiveModel):\n    """""" Adaptive model for Transformer\'s Sequence Classification Model\n\n    Usage:\n    ```python\n    >>> classifier = TransformersSequenceClassifier.load(""transformers-sc-model"")\n    >>> classifier.predict(text=""Example text"", mini_batch_size=32)\n    ```\n\n    **Parameters:**\n\n    * **tokenizer** - A tokenizer object from Huggingface\'s transformers (TODO)and tokenizers\n    * **model** - A transformers Sequence Classsifciation model\n    """"""\n\n    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n        # Load up model and tokenizer\n        self.tokenizer = tokenizer\n        self.model = model\n\n        # Setup cuda and automatic allocation of model\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        self.model.to(self.device)\n\n    @classmethod\n    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n        """""" Class method for loading and constructing this classifier\n\n        * **model_name_or_path** - A key string of one of Transformer\'s pre-trained Sequence Classifier Model\n        """"""\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n        classifier = cls(tokenizer, model)\n        return classifier\n\n    def predict(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        mini_batch_size: int = 32,\n        use_tokenizer: bool = True,\n        **kwargs,\n    ) -> List[Sentence]:\n        """""" Predict method for running inference using the pre-trained sequence classifier model\n\n        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n        * **mini_batch_size** - Mini batch size\n        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers classifier\n        """"""\n        id2label = self.model.config.id2label\n        sentences = text\n        results: List[Sentence] = []\n\n        with torch.no_grad():\n            if not sentences:\n                return sentences\n\n            if isinstance(sentences, DataPoint) or isinstance(sentences, str):\n                sentences = [sentences]\n\n            # filter empty sentences\n            if isinstance(sentences[0], Sentence):\n                sentences = [sentence for sentence in sentences if len(sentence) > 0]\n            if len(sentences) == 0:\n                return sentences\n\n            # reverse sort all sequences by their length\n            rev_order_len_index = sorted(\n                range(len(sentences)), key=lambda k: len(sentences[k]), reverse=True\n            )\n            original_order_index = sorted(\n                range(len(rev_order_len_index)), key=lambda k: rev_order_len_index[k]\n            )\n\n            reordered_sentences: List[Union[DataPoint, str]] = [\n                sentences[index] for index in rev_order_len_index\n            ]\n            # Turn all Sentence objects into strings\n            if isinstance(reordered_sentences[0], Sentence):\n                str_reordered_sentences = [\n                    sentence.to_original_text() for sentence in sentences\n                ]\n            else:\n                str_reordered_sentences = reordered_sentences\n\n            # Tokenize and get dataset\n            dataset = self._tokenize(str_reordered_sentences)\n            dataloader = DataLoader(dataset, batch_size=mini_batch_size)\n            predictions: List[Tuple[str, float]] = []\n\n            logger.info(f""Running prediction on {len(dataset)} text sequences"")\n            logger.info(f""Batch size = {mini_batch_size}"")\n            for batch in tqdm(dataloader, desc=""Predicting text""):\n                self.model.eval()\n                batch = tuple(t.to(self.device) for t in batch)\n\n                if len(batch) == 3:\n                    inputs = {\n                        ""input_ids"": batch[0],\n                        ""attention_mask"": batch[1],\n                        ""token_type_ids"": batch[2],\n                    }\n                else:\n                    inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1]}\n                outputs = self.model(**inputs)\n                logits = outputs[0]\n                preds = torch.softmax(logits, dim=1).tolist()\n\n                predictions += preds\n\n            for text, pred in zip(str_reordered_sentences, predictions):\n                # Initialize and assign labels to each class in each datapoint prediction\n                text_sent = Sentence(text)\n                for k, v in id2label.items():\n                    label = Label(value=v, score=pred[k])\n                    text_sent.add_label(label)\n                results.append(text_sent)\n\n        # Order results back into original order\n        results = [results[index] for index in original_order_index]\n\n        return results\n\n    def _tokenize(\n        self, sentences: Union[List[Sentence], Sentence, List[str], str]\n    ) -> TensorDataset:\n        """""" Batch tokenizes text and produces a `TensorDataset` with them """"""\n\n        tokenized_text = self.tokenizer.batch_encode_plus(\n            sentences,\n            return_tensors=""pt"",\n            pad_to_max_length=True,\n            add_special_tokens=True,\n        )\n\n        # Bart, XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don\'t use token_type_ids\n        if isinstance(\n            self.model,\n            (\n                BertForSequenceClassification,\n                XLNetForSequenceClassification,\n                AlbertForSequenceClassification,\n            ),\n        ):\n            dataset = TensorDataset(\n                tokenized_text[""input_ids""],\n                tokenized_text[""attention_mask""],\n                tokenized_text[""token_type_ids""],\n            )\n        else:\n            dataset = TensorDataset(\n                tokenized_text[""input_ids""], tokenized_text[""attention_mask""]\n            )\n\n        return dataset\n\n\nclass FlairSequenceClassifier(AdaptiveModel):\n    """""" Adaptive Model for Flair\'s Sequence Classifier...very basic\n\n    Usage:\n    ```python\n    >>> classifier = FlairSequenceClassifier.load(""en-sentiment"")\n    >>> classifier.predict(text=""Example text"", mini_batch_size=32)\n    ```\n\n    **Parameters:**\n\n    * **model_name_or_path** - A key string of one of Flair\'s pre-trained Sequence Classifier Model\n    """"""\n\n    def __init__(self, model_name_or_path: str):\n        self.classifier = TextClassifier.load(model_name_or_path)\n\n    @classmethod\n    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n        """""" Class method for loading a constructing this classifier\n\n        * **model_name_or_path** - A key string of one of Flair\'s pre-trained Sequence Classifier Model\n        """"""\n        classifier = cls(model_name_or_path)\n        return classifier\n\n    def predict(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        mini_batch_size: int = 32,\n        use_tokenizer=True,\n        **kwargs,\n    ) -> List[Sentence]:\n        """""" Predict method for running inference using the pre-trained sequence classifier model\n\n        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n        * **mini_batch_size** - Mini batch size\n        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Flair classifier\n        """"""\n        return self.classifier.predict(\n            sentences=text,\n            mini_batch_size=mini_batch_size,\n            use_tokenizer=use_tokenizer,\n            **kwargs,\n        )\n\n\nclass EasySequenceClassifier:\n    """""" Sequence classification models\n\n    Usage:\n\n    ```python\n    >>> classifier = EasySequenceClassifier()\n    >>> classifier.tag_text(text=""text you want to label"", model_name_or_path=""en-sentiment"")\n    ```\n\n    """"""\n\n    def __init__(self):\n        self.sequence_classifiers: Dict[AdaptiveModel] = defaultdict(bool)\n\n    def tag_text(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        model_name_or_path: str = ""en-sentiment"",\n        mini_batch_size: int = 32,\n        **kwargs,\n    ) -> List[Sentence]:\n        """""" Tags a text sequence with labels the sequence classification models have been trained on\n\n        * **text** - String, list of strings, `Sentence`, or list of `Sentence`s to be classified\n        * **model_name_or_path** - The model name key or model path\n        * **mini_batch_size** - The mini batch size for running inference\n        * **&ast;&ast;kwargs** - (Optional) Keyword Arguments for Flair\'s `TextClassifier.predict()` method params\n        **return** A list of Flair\'s `Sentence`\'s\n        """"""\n        # Load Text Classifier Model and Pytorch Module into tagger dict\n        if not self.sequence_classifiers[model_name_or_path]:\n            """"""\n            self.sequence_classifiers[model_name_or_path] = TextClassifier.load(\n                model_name_or_path\n            )\n            """"""\n            # TODO: Find an alternative model-check method like an `is_available(model_name_or_path)\n            # Check whether this is a Transformers or Flair Sequence Classifier model we\'re loading\n            try:\n                self.sequence_classifiers[\n                    model_name_or_path\n                ] = FlairSequenceClassifier.load(model_name_or_path)\n            except FileNotFoundError:\n                logger.info(\n                    f""{model_name_or_path} not a valid Flair pre-trained model...checking transformers repo""\n                )\n                try:\n                    self.sequence_classifiers[\n                        model_name_or_path\n                    ] = TransformersSequenceClassifier.load(model_name_or_path)\n                except ValueError:\n                    logger.info(""Try transformers"")\n                    return [Sentence("""")]\n\n        classifier = self.sequence_classifiers[model_name_or_path]\n        return classifier.predict(text=text, mini_batch_size=mini_batch_size, **kwargs,)\n\n    def tag_all(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        mini_batch_size: int = 32,\n        **kwargs,\n    ) -> List[Sentence]:\n        """""" Tags text with all labels from all sequence classification models\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        * **mini_batch_size** - The mini batch size for running inference\n        * **&ast;&ast;kwargs** - (Optional) Keyword Arguments for Flair\'s `TextClassifier.predict()` method params\n        * **return** - A list of Flair\'s `Sentence`\'s\n        """"""\n        sentences = text\n        for tagger_name in self.sequence_classifiers.keys():\n            sentences = self.tag_text(\n                sentences,\n                model_name_or_path=tagger_name,\n                mini_batch_size=mini_batch_size,\n                **kwargs,\n            )\n        return sentences\n'"
adaptnlp/summarization.py,3,"b'import logging\nfrom typing import List, Dict, Union\nfrom collections import defaultdict\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelWithLMHead,\n    PreTrainedTokenizer,\n    PreTrainedModel,\n    T5ForConditionalGeneration,\n    BartForConditionalGeneration,\n)\n\nfrom tqdm import tqdm\n\nfrom adaptnlp.model import AdaptiveModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformersSummarizer(AdaptiveModel):\n    """""" Adaptive model for Transformer\'s Conditional Generation or Language Models (Transformer\'s T5 and Bart\n        conditiional generation models have a language modeling head)\n\n        Usage:\n        ```python\n        >>> summarizer = TransformersSummarizer.load(""transformers-summarizer-model"")\n        >>> summarizer.predict(text=""Example text"", mini_batch_size=32)\n        ```\n\n        **Parameters:**\n\n        * **tokenizer** - A tokenizer object from Huggingface\'s transformers (TODO)and tokenizers\n        * **model** - A transformers Conditional Generation (Bart or T5) or Language model\n        """"""\n\n    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n        # Load up model and tokenizer\n        self.tokenizer = tokenizer\n        self.model = model\n\n        # Setup cuda and automatic allocation of model\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.model.to(self.device)\n\n    @classmethod\n    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n        """""" Class method for loading and constructing this classifier\n\n         * **model_name_or_path** - A key string of one of Transformer\'s pre-trained Summarizer Model\n        """"""\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        model = AutoModelWithLMHead.from_pretrained(model_name_or_path)\n        summarizer = cls(tokenizer, model)\n        return summarizer\n\n    def predict(\n        self,\n        text: Union[List[str], str],\n        mini_batch_size: int = 32,\n        num_beams: int = 4,\n        min_length: int = 0,\n        max_length: int = 128,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> List[str]:\n        """""" Predict method for running inference using the pre-trained sequence classifier model\n\n        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n        * **mini_batch_size** - Mini batch size\n        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 4.\n        * **min_length** -  The min length of the sequence to be generated. Default to 0\n        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n        """"""\n        with torch.no_grad():\n\n            # Make all inputs lists\n            if isinstance(text, str):\n                text = [text]\n\n            # T5 requires ""summarize: "" precursor text for pre-trained summarizer\n            if isinstance(self.model, T5ForConditionalGeneration):\n                text = [f""summarize: {t}"" for t in text]\n\n            dataset = self._tokenize(text)\n            dataloader = DataLoader(dataset, batch_size=mini_batch_size)\n            summaries = []\n\n            logger.info(f""Running summarizer on {len(dataset)} text sequences"")\n            logger.info(f""Batch size = {mini_batch_size}"")\n            for batch in tqdm(dataloader, desc=""Summarizing""):\n                self.model.eval()\n                batch = tuple(t.to(self.device) for t in batch)\n\n                if len(batch) == 3:\n                    inputs = {\n                        ""input_ids"": batch[0],\n                        ""attention_mask"": batch[1],\n                        ""token_type_ids"": batch[2],\n                    }\n                else:\n                    inputs = {\n                        ""input_ids"": batch[0],\n                        ""attention_mask"": batch[1],\n                    }\n                outputs = self.model.generate(\n                    input_ids=inputs[""input_ids""],\n                    attention_mask=inputs[""attention_mask""],\n                    num_beams=num_beams,\n                    min_length=min_length,\n                    max_length=max_length,\n                    early_stopping=early_stopping,\n                    **kwargs,\n                )\n\n                \n                for o in outputs:\n                    summaries.append([\n                        self.tokenizer.decode(\n                            o, skip_special_tokens=True, clean_up_tokenization_spaces=False,\n                        )\n                    ].pop())\n\n        return summaries\n\n    def _tokenize(self, text: Union[List[str], str]) -> TensorDataset:\n        """""" Batch tokenizes text and produces a `TensorDataset` with text """"""\n\n        # Pre-trained Bart summarization model has a max length fo 1024 tokens for input\n        if isinstance(self.model, BartForConditionalGeneration):\n            tokenized_text = self.tokenizer.batch_encode_plus(\n                text,\n                return_tensors=""pt"",\n                max_length=1024,\n                add_special_tokens=True,\n                pad_to_max_length=True,\n            )\n        else:\n            tokenized_text = self.tokenizer.batch_encode_plus(\n                text,\n                return_tensors=""pt"",\n                pad_to_max_length=True,\n                add_special_tokens=True,\n            )\n\n        # Bart doesn\'t use `token_type_ids`\n        if isinstance(self.model, T5ForConditionalGeneration):\n            dataset = TensorDataset(\n                tokenized_text[""input_ids""],\n                tokenized_text[""attention_mask""],\n                tokenized_text[""token_type_ids""],\n            )\n        else:\n            dataset = TensorDataset(\n                tokenized_text[""input_ids""], tokenized_text[""attention_mask""],\n            )\n\n        return dataset\n\n\nclass EasySummarizer:\n    """""" Summarization Module\n\n    Usage:\n\n    ```python\n    >>> summarizer = EasySummarizer()\n    >>> summarizer.summarize(text=""Summarize this text"", model_name_or_path=""t5-small"")\n    ```\n\n    """"""\n\n    def __init__(self):\n        self.summarizers: Dict[AdaptiveModel] = defaultdict(bool)\n\n    def summarize(\n        self,\n        text: Union[List[str], str],\n        model_name_or_path: str = ""t5-small"",\n        mini_batch_size: int = 32,\n        num_beams: int = 4,\n        min_length: int = 0,\n        max_length: int = 128,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> List[str]:\n        """""" Predict method for running inference using the pre-trained sequence classifier model\n\n        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n        * **model_name_or_path** - A String model id or path to a pre-trained model repository or custom trained model directory \n        * **mini_batch_size** - Mini batch size\n        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 4.\n        * **min_length** -  The min length of the sequence to be generated. Default to 0\n        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n        """"""\n        if not self.summarizers[model_name_or_path]:\n            self.summarizers[model_name_or_path] = TransformersSummarizer.load(\n                model_name_or_path\n            )\n\n        summarizer = self.summarizers[model_name_or_path]\n        return summarizer.predict(\n            text=text,\n            mini_batch_size=mini_batch_size,\n            num_beams=num_beams,\n            min_length=min_length,\n            max_length=max_length,\n            early_stopping=early_stopping,\n            **kwargs,\n        )\n'"
adaptnlp/token_classification.py,0,"b'from typing import List, Dict, Union\nfrom collections import defaultdict\n\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n\nclass EasyTokenTagger:\n    """""" Token level classification models\n\n    Usage:\n\n    ```python\n    >>> tagger = adaptnlp.EasyTokenTagger()\n    >>> tagger.tag_text(text=""text you want to tag"", model_name_or_path=""ner-ontonotes"")\n    ```\n    """"""\n\n    def __init__(self):\n        self.token_taggers: Dict[SequenceTagger] = defaultdict(bool)\n\n    def tag_text(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        model_name_or_path: str = ""ner-ontonotes"",\n        mini_batch_size: int = 32,\n        **kwargs,\n    ) -> List[Sentence]:\n        """""" Tags tokens with labels the token classification models have been trained on\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        * **model_name_or_path** - The hosted model name key or model path\n        * **mini_batch_size** - The mini batch size for running inference\n        * **&ast;&ast;kwargs** - Keyword arguments for Flair\'s `SequenceTagger.predict()` method\n        **return** - A list of Flair\'s `Sentence`\'s\n        """"""\n        # Load Sequence Tagger Model and Pytorch Module into tagger dict\n        if not self.token_taggers[model_name_or_path]:\n            self.token_taggers[model_name_or_path] = SequenceTagger.load(\n                model_name_or_path\n            )\n\n        tagger = self.token_taggers[model_name_or_path]\n        return tagger.predict(\n            sentences=text,\n            mini_batch_size=mini_batch_size,\n            use_tokenizer=True,\n            **kwargs,\n        )\n\n    def tag_all(\n        self,\n        text: Union[List[Sentence], Sentence, List[str], str],\n        mini_batch_size: int = 32,\n        **kwargs,\n    ) -> List[Sentence]:\n        """""" Tags tokens with all labels from all token classification models\n\n        * **text** - Text input, it can be a string or any of Flair\'s `Sentence` input formats\n        * **mini_batch_size** - The mini batch size for running inference\n        * **&ast;&ast;kwargs** - Keyword arguments for Flair\'s `SequenceTagger.predict()` method\n        **return** A list of Flair\'s `Sentence`\'s\n        """"""\n        if len(self.token_taggers) == 0:\n            print(""No token classification models loaded..."")\n            return Sentence()\n        sentences = text\n        for tagger_name in self.token_taggers.keys():\n            sentences = self.tag_text(\n                sentences,\n                model_name_or_path=tagger_name,\n                mini_batch_size=mini_batch_size,\n                **kwargs,\n            )\n        return sentences\n'"
adaptnlp/training.py,0,"b'from typing import Union, Dict\nfrom pathlib import Path\nimport json\nimport csv\n\nimport numpy as np\n\nfrom adaptnlp import EasyDocumentEmbeddings\n\nfrom flair.datasets import CSVClassificationCorpus\nfrom flair.data import Corpus\nfrom flair.embeddings import DocumentRNNEmbeddings\nfrom flair.models import TextClassifier\nfrom flair.trainers import ModelTrainer\nfrom flair.visual.training_curves import Plotter\n\n\nclass SequenceClassifierTrainer:\n    """"""Sequence Classifier Trainer\n\n    Usage:\n\n    ```python\n    >>> sc_trainer = SequenceClassifierTrainer(corpus=""/Path/to/data/dir"")\n    ```\n\n    **Parameters:**\n\n    * **corpus** - A flair corpus data model or `Path`/string to a directory with train.csv/test.csv/dev.csv\n    * **encoder** - A `EasyDocumentEmbeddings` object if training with a flair prediction head or `Path`/string if training with Transformer\'s prediction models\n    * **column_name_map** - Required if corpus is not a `Corpus` object, it\'s a dictionary specifying the indices of the text and label columns of the csv i.e. {1:""text"",2:""label""}\n    * **corpus_in_memory** - Boolean for whether to store corpus embeddings in memory\n    * **predictive_head** - For now either ""flair"" or ""transformers"" for the prediction head\n    * **&ast;&ast;kwargs** - Keyword arguments for Flair\'s `TextClassifier` model class\n    """"""\n\n    def __init__(\n        self,\n        corpus: Union[Corpus, Path, str],\n        encoder: Union[EasyDocumentEmbeddings, Path, str],\n        column_name_map: None,\n        corpus_in_memory: bool = True,\n        predictive_head: str = ""flair"",\n        **kwargs,\n    ):\n        if isinstance(corpus, Corpus):\n            self.corpus = corpus\n        else:\n            if isinstance(corpus, str):\n                corpus = Path(corpus)\n            if not column_name_map:\n                raise ValueError(\n                    ""If not instantiating with `Corpus` object, must pass in `column_name_map` argument to specify text/label indices""\n                )\n            self.corpus = CSVClassificationCorpus(\n                corpus,\n                column_name_map,\n                skip_header=True,\n                delimiter="","",\n                in_memory=corpus_in_memory,\n            )\n\n        # Verify predictive head is within available heads\n        self.available_predictive_head = [""flair"", ""transformers""]\n        if predictive_head not in self.available_predictive_head:\n            raise ValueError(\n                f""predictive_head param must be one of the following: {self.available_predictive_head}""\n            )\n        self.predictive_head = predictive_head\n\n        # Verify correct corresponding encoder is used with predictive head (This can be structured with better design in the future)\n        if isinstance(encoder, EasyDocumentEmbeddings):\n            if predictive_head == ""transformers"":\n                raise ValueError(\n                    ""If using `transformers` predictive head, pass in the path to the transformer\'s model""\n                )\n            else:\n                self.encoder = encoder\n        else:\n            if isinstance(encoder, str):\n                encoder = Path(encoder)\n            self.encoder = encoder\n\n        # Create the label dictionary on init (store to keep from constantly generating label_dict) should we use dev/test set instead assuming all labels are provided?\n        self.label_dict = self.corpus.make_label_dictionary()\n\n        # Save trainer kwargs dict for reinitializations\n        self.trainer_kwargs = kwargs\n\n        # Load trainer with initial setup\n        self._initial_setup(self.label_dict, **kwargs)\n\n    def _initial_setup(self, label_dict: Dict, **kwargs):\n        if self.predictive_head == ""flair"":\n\n            # Get Document embeddings from `embeddings`\n            document_embeddings: DocumentRNNEmbeddings = self.encoder.rnn_embeddings\n\n            # Create the text classifier\n            classifier = TextClassifier(\n                document_embeddings, label_dictionary=label_dict, **kwargs,\n            )\n\n            # Initialize the text classifier trainer\n            self.trainer = ModelTrainer(classifier, self.corpus)\n\n        # TODO: In internal transformers package, create ****ForSequenceClassification adaptations\n        elif self.predictive_head == ""transformers"":\n            with open(self.encoder / ""config.json"") as config_f:\n                configs = json.load(config_f)\n                model_name = configs[""architectures""][-1]\n            if model_name == ""BertForMaskedLM"":\n                pass\n\n    def train(\n        self,\n        output_dir: Union[Path, str],\n        learning_rate: float = 0.07,\n        mini_batch_size: int = 32,\n        anneal_factor: float = 0.5,\n        patience: int = 5,\n        max_epochs: int = 150,\n        plot_weights: bool = False,\n        **kwargs,\n    ) -> None:\n        """"""\n        Train the Sequence Classifier\n\n        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n        * **learning_rate** - The initial learning rate\n        * **mini_batch_size** - Batch size for the dataloader\n        * **anneal_factor** - The factor by which the learning rate is annealed\n        * **patience** - Patience is the number of epochs with no improvement the Trainer waits until annealing the learning rate\n        * **max_epochs** - Maximum number of epochs to train. Terminates training if this number is surpassed.\n        * **plot_weights** - Bool to plot weights or not\n        * **kwargs** - Keyword arguments for the rest of Flair\'s `Trainer.train()` hyperparameters\n        """"""\n        if isinstance(output_dir, str):\n            output_dir = Path(output_dir)\n\n        # Start the training\n        self.trainer.train(\n            output_dir,\n            learning_rate=learning_rate,\n            mini_batch_size=mini_batch_size,\n            anneal_factor=anneal_factor,\n            patience=patience,\n            max_epochs=max_epochs,\n            **kwargs,\n        )\n\n        # Plot weight traces\n        if plot_weights:\n            plotter = Plotter()\n            plotter.plot_weights(output_dir / ""weights.txt"")\n\n    def find_learning_rate(\n        self,\n        output_dir: Union[Path, str],\n        file_name: str = ""learning_rate.tsv"",\n        start_learning_rate: float = 1e-8,\n        end_learning_rate: float = 10,\n        iterations: int = 100,\n        mini_batch_size: int = 32,\n        stop_early: bool = True,\n        smoothing_factor: float = 0.7,\n        plot_learning_rate: bool = True,\n        **kwargs,\n    ) -> float:\n        """"""\n        Uses Leslie\'s cyclical learning rate finding method to generate and save the loss x learning rate plot\n\n        This method returns a suggested learning rate using the static method `LMFineTuner.suggest_learning_rate()`\n        which is implicitly run in this method.\n\n        * **output_dir** - Path to dir for learning rate file to be saved\n        * **file_name** - Name of learning rate .tsv file\n        * **start_learning_rate** - Initial learning rate to start cyclical learning rate finder method\n        * **end_learning_rate** - End learning rate to stop exponential increase of the learning rate\n        * **iterations** - Number of optimizer iterations for the ExpAnnealLR scheduler\n        * **mini_batch_size** - Batch size for dataloader\n        * **stop_early** - Bool for stopping early once loss diverges\n        * **smoothing_factor** - Smoothing factor on moving average of losses\n        * **adam_epsilon** - Epsilon for Adam optimizer.\n        * **weight_decay** - Weight decay if we apply some.\n        * **kwargs** - Additional keyword arguments for the Adam optimizer\n        **return** - Learning rate as a float\n        """"""\n        # 7. find learning rate\n        learning_rate_tsv = self.trainer.find_learning_rate(\n            base_path=output_dir,\n            file_name=file_name,\n            start_learning_rate=start_learning_rate,\n            end_learning_rate=end_learning_rate,\n            iterations=iterations,\n            mini_batch_size=mini_batch_size,\n            stop_early=stop_early,\n            smoothing_factor=smoothing_factor,\n        )\n\n        # Reinitialize optimizer and parameters by reinitializing trainer\n        self._initial_setup(self.label_dict, **self.trainer_kwargs)\n\n        if plot_learning_rate:\n            plotter = Plotter()\n            plotter.plot_learning_rate(learning_rate_tsv)\n\n        # Use the automated learning rate finder\n        with open(learning_rate_tsv) as lr_f:\n            lr_tsv = list(csv.reader(lr_f, delimiter=""\\t""))\n        losses = np.array([float(row[-1]) for row in lr_tsv[1:]])\n        lrs = np.array([float(row[-2]) for row in lr_tsv[1:]])\n        lr_to_use = self.suggested_learning_rate(losses, lrs, **kwargs)\n        print(f""Recommended Learning Rate {lr_to_use}"")\n        return lr_to_use\n\n    @staticmethod\n    def suggested_learning_rate(\n        losses: np.array,\n        lrs: np.array,\n        lr_diff: int = 15,\n        loss_threshold: float = 0.2,\n        adjust_value: float = 1,\n    ) -> float:\n        # This seems redundant unless we can make this configured for each trainer/finetuner\n        """"""\n        Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method\n\n        * **losses** - Numpy array of losses\n        * **lrs** - Numpy array of exponentially increasing learning rates (must match dim of `losses`)\n        * **lr_diff** - Learning rate Interval of slide ruler\n        * **loss_threshold** - Threshold of loss difference on interval where the sliding stops\n        * **adjust_value** - Coefficient for adjustment\n        **return** - the optimal learning rate as a float\n        """"""\n        # Get loss values and their corresponding gradients, and get lr values\n        assert lr_diff < len(losses)\n        loss_grad = np.gradient(losses)\n\n        # Search for index in gradients where loss is lowest before the loss spike\n        # Initialize right and left idx using the lr_diff as a spacing unit\n        # Set the local min lr as -1 to signify if threshold is too low\n        r_idx = -1\n        l_idx = r_idx - lr_diff\n        local_min_lr = lrs[l_idx]\n        while (l_idx >= -len(losses)) and (\n            abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold\n        ):\n            local_min_lr = lrs[l_idx]\n            r_idx -= 1\n            l_idx -= 1\n\n        lr_to_use = local_min_lr * adjust_value\n\n        return lr_to_use\n'"
adaptnlp/translation.py,3,"b'import logging\nfrom typing import List, Dict, Union\nfrom collections import defaultdict\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelWithLMHead,\n    PreTrainedTokenizer,\n    PreTrainedModel,\n    T5ForConditionalGeneration,\n)\n\nfrom tqdm import tqdm\n\nfrom adaptnlp.model import AdaptiveModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformersTranslator(AdaptiveModel):\n    """""" Adaptive model for Transformer\'s Conditional Generation or Language Models (Transformer\'s T5 and Bart\n        conditional generation models have a language modeling head)\n\n        Usage:\n        ```python\n        >>> translator = TransformersTranslator.load(""transformers-translator-model"")\n        >>> translator.predict(text=""Example text"", mini_batch_size=32)\n        ```\n\n        **Parameters:**\n\n        * **tokenizer** - A tokenizer object from Huggingface\'s transformers (TODO)and tokenizers\n        * **model** - A transformers Conditional Generation (Bart or T5) or Language model\n        """"""\n\n    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n        # Load up model and tokenizer\n        self.tokenizer = tokenizer\n        self.model = model\n\n        # Setup cuda and automatic allocation of model\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        self.model.to(self.device)\n\n    @classmethod\n    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n        """""" Class method for loading and constructing this classifier\n\n         * **model_name_or_path** - A key string of one of Transformer\'s pre-trained translator Model\n        """"""\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        model = AutoModelWithLMHead.from_pretrained(model_name_or_path)\n        translator = cls(tokenizer, model)\n        return translator\n\n    def predict(\n        self,\n        text: Union[List[str], str],\n        t5_prefix: str = ""translate English to German"",\n        mini_batch_size: int = 32,\n        num_beams: int = 1,\n        min_length: int = 0,\n        max_length: int = 128,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> List[str]:\n        """""" Predict method for running inference using the pre-trained sequence classifier model.  Keyword arguments\n        for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n\n        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n        * **t5_prefix**(Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models.\n        * **mini_batch_size** - Mini batch size\n        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 1.\n        * **min_length** -  The min length of the sequence to be generated. Default to 0\n        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n        """"""\n        with torch.no_grad():\n\n            # Make all inputs lists\n            if isinstance(text, str):\n                text = [text]\n\n            # T5 requires ""translate: "" precursor text for pre-trained translator\n            if isinstance(self.model, T5ForConditionalGeneration):\n                text = [f""{t5_prefix}: {t}"" for t in text]\n\n            dataset = self._tokenize(text)\n            dataloader = DataLoader(dataset, batch_size=mini_batch_size)\n            translations = []\n\n            logger.info(f""Running translator on {len(dataset)} text sequences"")\n            logger.info(f""Batch size = {mini_batch_size}"")\n            for batch in tqdm(dataloader, desc=""Translating""):\n                self.model.eval()\n                batch = tuple(t.to(self.device) for t in batch)\n\n                if len(batch) == 3:\n                    inputs = {\n                        ""input_ids"": batch[0],\n                        ""attention_masks"": batch[1],\n                        ""token_type_ids"": batch[2],\n                    }\n                else:\n                    inputs = {\n                        ""input_ids"": batch[0],\n                        ""attention_masks"": batch[1],\n                    }\n                outputs = self.model.generate(\n                    inputs[""input_ids""],\n                    num_beams=num_beams,\n                    min_length=min_length,\n                    max_length=max_length,\n                    early_stopping=early_stopping,\n                    **kwargs,\n                )\n\n                for o in outputs:\n                    translations.append([\n                        self.tokenizer.decode(\n                            o, skip_special_tokens=True, clean_up_tokenization_spaces=False,\n                        )\n                    ].pop())\n\n        return translations\n\n    def _tokenize(self, text: Union[List[str], str]) -> TensorDataset:\n        """""" Batch tokenizes text and produces a `TensorDataset` with text """"""\n\n        tokenized_text = self.tokenizer.batch_encode_plus(\n            text,\n            return_tensors=""pt"",\n            max_length=512,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n        )\n\n        # Bart doesn\'t use `token_type_ids`\n        if isinstance(self.model, T5ForConditionalGeneration):\n            dataset = TensorDataset(\n                tokenized_text[""input_ids""],\n                tokenized_text[""attention_mask""],\n                tokenized_text[""token_type_ids""],\n            )\n        else:\n            dataset = TensorDataset(\n                tokenized_text[""input_ids""], tokenized_text[""attention_mask""],\n            )\n\n        return dataset\n\n\nclass EasyTranslator:\n    """""" Translation Module\n\n    Usage:\n\n    ```python\n    >>> translator = EasyTranslator()\n    >>> translator.translate(text=""translate this text"", model_name_or_path=""t5-small"")\n    ```\n\n    """"""\n\n    def __init__(self):\n        self.translators: Dict[AdaptiveModel] = defaultdict(bool)\n\n    def translate(\n        self,\n        text: Union[List[str], str],\n        model_name_or_path: str = ""t5-small"",\n        t5_prefix: str = ""translate English to German"",\n        mini_batch_size: int = 32,\n        num_beams: int = 1,\n        min_length: int = 0,\n        max_length: int = 128,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> List[str]:\n        """""" Predict method for running inference using the pre-trained sequence classifier model. Keyword arguments\n        for parameters of the method `Transformers.PreTrainedModel.generate()` can be used as well.\n\n        * **text** - String, list of strings, sentences, or list of sentences to run inference on\n        * **model_name_or_path** - A String model id or path to a pre-trained model repository or custom trained model directory \n        * **t5_prefix**(Optional) - The pre-appended prefix for the specificied task. Only in use for T5-type models.\n        * **mini_batch_size** - Mini batch size\n        * **num_beams** - Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search.  Default to 1.\n        * **min_length** -  The min length of the sequence to be generated. Default to 0\n        * **max_length** - The max length of the sequence to be generated. Between min_length and infinity. Default to 128\n        * **early_stopping** - if set to True beam search is stopped when at least num_beams sentences finished per batch.\n        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers `PreTrainedModel.generate()` method\n        """"""\n        if not self.translators[model_name_or_path]:\n            self.translators[model_name_or_path] = TransformersTranslator.load(\n                model_name_or_path\n            )\n\n        translator = self.translators[model_name_or_path]\n        return translator.predict(\n            text=text,\n            t5_prefix=t5_prefix,\n            mini_batch_size=mini_batch_size,\n            num_beams=num_beams,\n            min_length=min_length,\n            max_length=max_length,\n            early_stopping=early_stopping,\n            **kwargs,\n        )\n'"
tests/test_language_model_fine_tuner.py,0,"b'from adaptnlp import LMFineTuner\n\n\ndef test_language_model_fine_tuner():\n    train_data_file = ""Path/to/train.csv""\n    eval_data_file = ""Path/to/test.csv""\n\n    # Instantiate Finetuner with Desired Language Model\n    finetuner = LMFineTuner(\n        train_data_file=train_data_file,\n        eval_data_file=eval_data_file,\n        model_type=""bert"",\n        model_name_or_path=""bert-base-cased"",\n    )\n    finetuner\n'"
tests/test_question_answering.py,0,"b'from adaptnlp import EasyQuestionAnswering\n\n\ndef test_question_answering():\n    qa_model = EasyQuestionAnswering()\n    qa_model.predict_qa(query=""Test"", context=""Test"", n_best_size=1)\n'"
tests/test_sequence_classifier.py,0,"b'from adaptnlp import EasySequenceClassifier\n\n\ndef test_easy_sequence_classifier():\n    classifier = EasySequenceClassifier()\n    classifier.tag_text(text=""test"", model_name_or_path=""en-sentiment"")\n'"
tests/test_sequence_classifier_trainer.py,0,"b'from adaptnlp import SequenceClassifierTrainer, EasyDocumentEmbeddings\nfrom flair.datasets import TREC_6\n\n\ndef test_sequence_classifier_trainer():\n    corpus = TREC_6()\n\n    # Instantiate AdaptNLP easy document embeddings module, which can take in a variable number of embeddings to make `Stacked Embeddings`.\n    # You may also use custom Transformers LM models by specifying the path the the language model\n    doc_embeddings = EasyDocumentEmbeddings(""bert-base-cased"", methods=[""rnn""])\n\n    # Instantiate Sequence Classifier Trainer by loading in the data, data column map, and embeddings as an encoder\n    trainer = SequenceClassifierTrainer(\n        corpus=corpus, encoder=doc_embeddings, column_name_map={0: ""text"", 1: ""label""}\n    )\n\n    trainer\n'"
tests/test_summarizer.py,0,"b'from adaptnlp import EasySummarizer\n\n\ndef test_easy_summarizer():\n    summarizer = EasySummarizer()\n    summarizer.summarize(text=""Testing summarizer"")\n'"
tests/test_token_tagger.py,0,"b'from adaptnlp import EasyTokenTagger\nfrom flair.data import Sentence\nfrom flair.data import Token\n\n\ndef get_tokens():\n    s = Sentence()\n    s.tokens = [\n        Token(""The"", 1),\n        Token(""quick"", 2),\n        Token(""brown"", 3),\n        Token(""fox"", 4),\n        Token(""jumps"", 5),\n        Token(""over"", 6),\n        Token(""the"", 7),\n        Token(""lazy"", 8),\n        Token(""dog"", 9),\n        Token(""."", 10),\n    ]\n    s.tokenized = ""The quick brown fox jumps over the lazy dog.""\n    return [s]\n\n\ndef base_case_tag_text(model):\n    tagger = EasyTokenTagger()\n    example_text = ""The quick brown fox jumps over the lazy dog.""\n    sentences = tagger.tag_text(text=example_text, model_name_or_path=model)\n    return sentences\n\n\ndef base_case_tag_all(model_list):\n    tagger = EasyTokenTagger()\n    example_text = ""The quick brown fox jumps over the lazy dog.""\n    for model in model_list:\n        tagger.tag_text(text=example_text, model_name_or_path=model)\n    sentences = tagger.tag_all(text=example_text)\n    return sentences\n\n\n# tag_text method tests\ndef test_tag_text_type():\n    sentences = base_case_tag_text(""ner-fast"")\n    assert type(sentences) == type(get_tokens())\n\n\n# tag_all method tests\ndef test_tag_all_type():\n    sentences = base_case_tag_all([""ner-fast""])\n    assert type(sentences) == type(get_tokens())\n'"
tests/test_trained_embeddings.py,0,"b'from adaptnlp import EasyWordEmbeddings, EasyStackedEmbeddings, EasyDocumentEmbeddings\n\n\ndef test_easy_word_embeddings():\n    embeddings = EasyWordEmbeddings()\n    embeddings.embed_text(text=""Test"", model_name_or_path=""bert-base-cased"")\n\n\ndef test_easy_stacked_embeddings():\n    embeddings = EasyStackedEmbeddings(""bert-base-cased"", ""xlnet-base-cased"")\n    embeddings.embed_text(text=""Test"")\n\n\ndef test_easy_document_embeddings():\n    embeddings = EasyDocumentEmbeddings(""bert-base-cased"", ""xlnet-base-cased"")\n    embeddings.embed_pool(text=""Test"")\n'"
tests/test_translator.py,0,"b'from adaptnlp import EasyTranslator\n\n\ndef test_easy_Translator():\n    translator = EasyTranslator()\n    translator.translate(text=""Testing summarizer"")\n'"
adaptnlp/transformers/__init__.py,0,"b'import pkg_resources\n\nfrom .question_answering import BertQuestionAnsweringModel\n\n__version__ = (\n    pkg_resources.resource_string(""adaptnlp"", ""VERSION.txt"").decode(""UTF-8"").strip()\n)\n\n__all__ = [\n    ""__version__"",\n    ""BertQuestionAnsweringModel"",\n]\n'"
adaptnlp/transformers/finetuning.py,62,"b'# Contains code used/modified by AdaptNLP author from transformers\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Union, List\nimport datetime\nimport glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport csv\nimport copy\nfrom typing import Tuple\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom tqdm import trange\nfrom tqdm import tqdm as tqdm_base\n\nfrom flair.visual.training_curves import Plotter\n\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    BertConfig,\n    BertForMaskedLM,\n    BertTokenizer,\n    CamembertConfig,\n    CamembertForMaskedLM,\n    CamembertTokenizer,\n    DistilBertConfig,\n    DistilBertForMaskedLM,\n    DistilBertTokenizer,\n    GPT2Config,\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    OpenAIGPTConfig,\n    OpenAIGPTLMHeadModel,\n    OpenAIGPTTokenizer,\n    PreTrainedTokenizer,\n    RobertaConfig,\n    RobertaForMaskedLM,\n    RobertaTokenizer,\n    AlbertConfig,\n    AlbertForMaskedLM,\n    AlbertTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\n\ndef tqdm(*args, **kwargs):\n    if hasattr(tqdm_base, ""_instances""):\n        for instance in list(tqdm_base._instances):\n            tqdm_base._decr_instances(instance)\n    return tqdm_base(*args, **kwargs)\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TextDataset(Dataset):\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        model_type: str,\n        overwrite_cache: bool,\n        file_path: str = ""train"",\n        block_size: int = 512,\n    ):\n        assert os.path.isfile(file_path)\n\n        block_size = block_size - (\n            tokenizer.max_len - tokenizer.max_len_single_sentence\n        )\n\n        directory, filename = os.path.split(file_path)\n        cached_features_file = os.path.join(\n            directory, model_type + ""_cached_lm_"" + str(block_size) + ""_"" + filename\n        )\n\n        if os.path.exists(cached_features_file) and not overwrite_cache:\n            logger.info(""Loading features from cached file %s"", cached_features_file)\n            with open(cached_features_file, ""rb"") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            logger.info(""Creating features from dataset file at %s"", directory)\n\n            self.examples = []\n            print(""Opening file"")\n            with open(file_path, encoding=""utf-8"") as f:\n                print(f""Reading file {file_path}"")\n                if file_path.endswith("".txt"") or file_path.endswith("".raw""):\n                    text = f.read()\n                elif file_path.endswith("".csv""):\n                    reader = csv.reader(f)\n                    try:\n                        csv_idx = next(reader).index(""text"")\n                        text = "". "".join([row[csv_idx] for row in reader])\n                    except ValueError:\n                        logger.info(""No header row provided with \'text\' column"")\n                        text = """"\n                else:\n                    text = """"\n            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n            print(""iterating through tokenized text"")\n            for i in tqdm(\n                range(0, len(tokenized_text) - block_size + 1, block_size)\n            ):  # Truncate in block of block_size\n                self.examples.append(\n                    tokenizer.build_inputs_with_special_tokens(\n                        tokenized_text[i : i + block_size]\n                    )\n                )\n            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n            # If your dataset is small, first you should loook for a bigger one :-) and second you\n            # can change this behavior by adding (model specific) padding.\n\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            with open(cached_features_file, ""wb"") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item])\n\n\nclass LMFineTuner:\n    """"""\n     A Language Model Fine Tuner object you can set language model configurations and then train and evaluate\n\n    Usage:\n\n    ```python\n    >>> finetuner = adaptnlp.LMFineTuner()\n    >>> finetuner.train()\n    ```\n\n    **Parameters:**\n\n    * **train_data_file** - The input training data file (a text file).\n    * **eval_data_file** - An optional input evaluation data file to evaluate the perplexity on (a text file).\n    * **model_type** - The model architecture to be trained or fine-tuned.\n    * **model_name_or_path** - The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\n    * **mlm** - Train with masked-language modeling loss instead of language modeling.\n    * **mlm_probability** - Ratio of tokens to mask for masked language modeling loss\n    * **config_name** - Optional Transformers pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config.\n    * **tokenizer_name** - Optional Transformers pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer.\n    * **cache_dir** - Optional directory to store the pre-trained models downloaded from s3 (If None, will go to default dir)\n    * **block_size** - Optional input sequence length after tokenization.\n                        The training dataset will be truncated in block of this size for training.""\n                        `-1` will default to the model max input length for single sentence inputs (take into account special tokens).\n    * **no_cuda** - Avoid using CUDA when available\n    * **overwrite_cache** - Overwrite the cached training and evaluation sets\n    * **seed** - random seed for initialization\n    * **fp16** - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n    * **fp16_opt_level** - For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].\n    * **local_rank** - For distributed training: local_rank\n    """"""\n\n    def __init__(\n        self,\n        train_data_file: str,\n        eval_data_file: str = None,\n        model_type: str = ""bert"",\n        model_name_or_path: str = None,\n        mlm: bool = True,\n        mlm_probability: float = 0.15,\n        config_name: str = None,\n        tokenizer_name: str = None,\n        cache_dir: str = None,\n        block_size: int = -1,\n        no_cuda: bool = False,\n        overwrite_cache: bool = False,\n        seed: int = 42,\n        fp16: bool = False,\n        fp16_opt_level: str = ""O1"",\n        local_rank: int = -1,\n    ):\n\n        self.train_data_file = train_data_file\n        self.eval_data_file = eval_data_file\n        self.model_type = model_type\n        self.model_name_or_path = model_name_or_path\n        self.mlm = mlm\n        self.mlm_probability = mlm_probability\n        self.config_name = config_name\n        self.tokenizer_name = tokenizer_name\n        self.cache_dir = cache_dir\n        self.block_size = block_size\n        self.no_cuda = no_cuda\n        self.overwrite_cache = overwrite_cache\n        self.seed = seed\n        self.fp16 = fp16\n        self.fp16_opt_level = fp16_opt_level\n        self.local_rank = local_rank\n\n        self.MODEL_CLASSES = {\n            ""gpt2"": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n            ""openai-gpt"": (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n            ""bert"": (BertConfig, BertForMaskedLM, BertTokenizer),\n            ""roberta"": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n            ""distilbert"": (\n                DistilBertConfig,\n                DistilBertForMaskedLM,\n                DistilBertTokenizer,\n            ),\n            ""camembert"": (CamembertConfig, CamembertForMaskedLM, CamembertTokenizer),\n            ""albert"": (AlbertConfig, AlbertForMaskedLM, AlbertTokenizer),\n        }\n\n        self._initial_setup()\n\n    def _initial_setup(self):\n\n        #  Setup model type and output directory\n        if (\n            self.model_type in [""bert"", ""roberta"", ""distilbert"", ""camembert""]\n            and not self.mlm\n        ):\n            raise ValueError(\n                ""BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using with the mlm parameter set as `True`""\n                ""for (masked language modeling).""\n            )\n\n        if self.eval_data_file is None:\n            raise ValueError(\n                ""Cannot do evaluation without an evaluation data file. Either supply a file to eval_data_file parameter or continue without evaluating""\n            )\n\n        #  Setup CUDA, GPU, and distributed training\n        if self.local_rank == -1 or self.no_cuda:\n            device = torch.device(\n                ""cuda"" if torch.cuda.is_available() and not self.no_cuda else ""cpu""\n            )\n            self.n_gpu = 0 if self.no_cuda else torch.cuda.device_count()\n        else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n            torch.cuda.set_device(self.local_rank)\n            device = torch.device(""cuda"", self.local_rank)\n            torch.distributed.init_process_group(backend=""nccl"")\n            self.n_gpu = 1\n        self.device = device\n\n        # Setup logging and seed\n        logging.basicConfig(\n            format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n            datefmt=""%m/%d/%Y %H:%M:%S"",\n            level=logging.INFO if self.local_rank in [-1, 0] else logging.WARN,\n        )\n        logger.warning(\n            ""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n            self.local_rank,\n            device,\n            self.n_gpu,\n            bool(self.local_rank != -1),\n            self.fp16,\n        )\n        self._set_seed()\n\n        # Load pretrained model and tokenizer\n        if self.local_rank not in [-1, 0]:\n            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n\n        config_class, model_class, tokenizer_class = self.MODEL_CLASSES[self.model_type]\n        if self.config_name:\n            self.config = config_class.from_pretrained(\n                self.config_name, cache_dir=self.cache_dir\n            )\n        elif self.model_name_or_path:\n            self.config = config_class.from_pretrained(\n                self.model_name_or_path, cache_dir=self.cache_dir\n            )\n        else:\n            self.config = config_class()\n\n        if self.tokenizer_name:\n            self.tokenizer = tokenizer_class.from_pretrained(\n                self.tokenizer_name, cache_dir=self.cache_dir\n            )\n        elif self.model_name_or_path:\n            self.tokenizer = tokenizer_class.from_pretrained(\n                self.model_name_or_path, cache_dir=self.cache_dir\n            )\n        else:\n            raise ValueError(\n                f""You are instantiating a new {tokenizer_class.__name__} tokenizer from scratch. Are you sure this is what you meant to do? \\n To specifiy a pretrained tokenizer name, pass in a tokenizer_name argument""\n            )\n\n        if self.block_size <= 0:\n            self.block_size = self.tokenizer.max_len_single_sentence\n            # Our input block size will be the max possible for the model\n        else:\n            self.block_size = min(\n                self.block_size, self.tokenizer.max_len_single_sentence\n            )\n\n        if self.model_name_or_path:\n            self.model = model_class.from_pretrained(\n                self.model_name_or_path,\n                from_tf=bool("".ckpt"" in self.model_name_or_path),\n                config=self.config,\n                cache_dir=self.cache_dir,\n            )\n        else:\n            logger.info(""Training new model from scratch"")\n            self.model = model_class(config=self.config)\n\n        self.model.to(self.device)\n\n        if self.local_rank == 0:\n            torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n\n    def train_one_cycle(\n        self,\n        output_dir: str,\n        should_continue: bool = False,\n        overwrite_output_dir: bool = False,\n        evaluate_during_training: bool = False,\n        per_gpu_train_batch_size: int = 4,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 5e-5,\n        weight_decay: float = 0.0,\n        adam_epsilon: float = 1e-8,\n        max_grad_norm: float = 1.0,\n        num_train_epochs: float = 1.0,\n        max_steps: int = -1,\n        warmup_steps: int = 0,\n        logging_steps: int = 50,\n        save_steps: int = 50,\n        save_total_limit: int = 3,\n        use_tensorboard: bool = False,\n    ) -> None:\n        """"""\n\n        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n        * **should_continue** - Whether to continue training from latest checkpoint in `output_dir`\n        * **overwrite_output_dir** - Overwrite the content of output directory `output_dir`\n        * **evaluate_during_training** - Run evaluation during training at each `logging_step`.\n        * **per_gpu_train_batch_size** - Batch size per GPU/CPU for training. (If `evaluate_during_training` is True, this is also the eval batch size\n        * **gradient_accumulation_steps** - Number of updates steps to accumulate before performing a backward/update pass\n        * **learning_rate** - The initial learning rate for Adam optimizer.\n        * **weight_decay** - Weight decay if we apply some.\n        * **adam_epsilon** - Epsilon for Adam optimizer.\n        * **max_grad_norm** - Max gradient norm. Duh\n        * **num_train_epochs** - Total number of training epochs to perform.\n        * **max_steps** - If > 0: set total number of training steps to perform. Override `num_train_epochs`.\n        * **warmup_steps** - Linear warmup over warmup_steps.\n        * **logging_steps** - Number of steps until logging occurs.\n        * **save_steps** - Number of steps until checkpoint is saved in `output_dir`\n        * **save_total_limit** - Limit the total amount of checkpoints, delete the older checkpoints in the `output_dir`, does not delete by default\n        * **use_tensorboard** - Only useable if tensorboard is installed\n        **return** - None\n        """"""\n        # Check to overwrite\n        if (\n            os.path.exists(output_dir)\n            and os.listdir(output_dir)\n            and not overwrite_output_dir\n        ):\n            raise ValueError(\n                f""Output directory ({output_dir}) already exists and is not empty. Set overwrite_output_dir as `True` to overcome.""\n            )\n\n        # Check if continuing training from checkpoint\n        if should_continue:\n            sorted_checkpoints = self._sorted_checkpoints(\n                checkpoint_prefix=""checkpoint"", output_dir=output_dir\n            )\n            if len(sorted_checkpoints) == 0:\n                raise ValueError(\n                    f""Trying to continue training but no checkpoint was found in {output_dir}, set `should_continue` argument to False if training from scratch""\n                )\n            else:\n                self.model_name_or_path = sorted_checkpoints[-1]\n                self._initial_setup()\n\n        # Get locals for training args\n        init_locals = copy.deepcopy(locals())\n        init_locals.pop(""self"")\n\n        # Start logger\n        logger.info(""Training/evaluation parameters %s"", str(locals()))\n\n        ##############\n        ## Training ##\n        ##############\n        if self.local_rank not in [-1, 0]:\n            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n        # Load Dataset\n        train_dataset = self.load_and_cache_examples(evaluate=False)\n\n        if self.local_rank == 0:\n            torch.distributed.barrier()\n\n        if self.local_rank in [-1, 0] and use_tensorboard:\n            try:\n                from torch.utils.tensorboard import SummaryWriter\n\n                tb_writer = SummaryWriter()\n            except ImportError:\n                logger.warning(\n                    ""WARNING! Tensorboard is a required dependency...`use_tensorboard` is now set as False""\n                )\n                use_tensorboard = False\n                pass\n\n        # Train the model\n\n        train_batch_size = per_gpu_train_batch_size * max(1, self.n_gpu)\n\n        def collate(examples: List[torch.Tensor]):\n            if self.tokenizer._pad_token is None:\n                return pad_sequence(examples, batch_first=True)\n            return pad_sequence(\n                examples, batch_first=True, padding_value=self.tokenizer.pad_token_id\n            )\n\n        train_sampler = (\n            RandomSampler(train_dataset)\n            if self.local_rank == -1\n            else DistributedSampler(train_dataset)\n        )\n        train_dataloader = DataLoader(\n            train_dataset,\n            sampler=train_sampler,\n            batch_size=train_batch_size,\n            collate_fn=collate,\n        )\n\n        if max_steps > 0:\n            t_total = max_steps\n            num_train_epochs = (\n                max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n            )\n        else:\n            t_total = (\n                len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n            )\n\n        # Prepare optimizer and schedule (linear warmup and decay)\n        no_decay = [""bias"", ""LayerNorm.weight""]\n        optimizer_grouped_parameters = [\n            {\n                ""params"": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if not any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": weight_decay,\n            },\n            {\n                ""params"": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.0,\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon\n        )\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=learning_rate,\n            epochs=int(num_train_epochs),\n            steps_per_epoch=int(t_total),\n        )\n\n        # Check if saved optimizer or scheduler states exist\n        if (\n            self.model_name_or_path\n            and os.path.isfile(os.path.join(self.model_name_or_path, ""optimizer.pt""))\n            and os.path.isfile(os.path.join(self.model_name_or_path, ""scheduler.pt""))\n        ):\n            # Load in optimizer and scheduler states\n            optimizer.load_state_dict(\n                torch.load(os.path.join(self.model_name_or_path, ""optimizer.pt""))\n            )\n            scheduler.load_state_dict(\n                torch.load(os.path.join(self.model_name_or_path, ""scheduler.pt""))\n            )\n\n        if self.fp16:\n            try:\n                from apex import amp\n            except ImportError:\n                raise ImportError(\n                    ""Please install apex from https://www.github.com/nvidia/apex to use fp16 training.""\n                )\n            self.model, optimizer = amp.initialize(\n                self.model, optimizer, opt_level=self.fp16_opt_level\n            )\n\n        # multi-gpu training (should be after apex fp16 initialization)\n        if self.n_gpu > 1:\n            self.model = torch.nn.DataParallel(self.model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.local_rank != -1:\n            self.model = torch.nn.parallel.DistributedDataParallel(\n                self.model,\n                device_ids=[self.local_rank],\n                output_device=self.local_rank,\n                find_unused_parameters=True,\n            )\n\n        # Train!\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_dataset))\n        logger.info(""  Num Epochs = %d"", num_train_epochs)\n        logger.info(""  Instantaneous batch size per GPU = %d"", per_gpu_train_batch_size)\n        logger.info(\n            ""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n            train_batch_size\n            * gradient_accumulation_steps\n            * (torch.distributed.get_world_size() if self.local_rank != -1 else 1),\n        )\n        logger.info(""  Gradient Accumulation steps = %d"", gradient_accumulation_steps)\n        logger.info(""  Total optimization steps = %d"", t_total)\n\n        global_step = 0\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        # Check if continuing training from a checkpoint\n        if self.model_name_or_path and os.path.exists(self.model_name_or_path):\n            try:\n                # set global_step to gobal_step of last saved checkpoint from model path\n                checkpoint_suffix = self.model_name_or_path.split(""-"")[-1].split(""/"")[0]\n                global_step = int(checkpoint_suffix)\n                epochs_trained = global_step // (\n                    len(train_dataloader) // gradient_accumulation_steps\n                )\n                steps_trained_in_current_epoch = global_step % (\n                    len(train_dataloader) // gradient_accumulation_steps\n                )\n\n                logger.info(\n                    ""  Continuing training from checkpoint, will skip to saved global_step""\n                )\n                logger.info(""  Continuing training from epoch %d"", epochs_trained)\n                logger.info(""  Continuing training from global step %d"", global_step)\n                logger.info(\n                    ""  Will skip the first %d steps in the first epoch"",\n                    steps_trained_in_current_epoch,\n                )\n            except ValueError:\n                logger.info(""  Starting fine-tuning."")\n\n        tr_loss, logging_loss = 0.0, 0.0\n\n        model_to_resize = (\n            self.model.module if hasattr(self.model, ""module"") else self.model\n        )  # Take care of distributed/parallel training\n        model_to_resize.resize_token_embeddings(len(self.tokenizer))\n\n        self.model.zero_grad()\n        train_iterator = trange(\n            epochs_trained,\n            int(num_train_epochs),\n            desc=""Epoch"",\n            disable=self.local_rank not in [-1, 0],\n        )\n        self._set_seed()  # Added here for reproducibility\n        for _ in train_iterator:\n            epoch_iterator = tqdm(\n                train_dataloader,\n                desc=""Iteration"",\n                disable=self.local_rank not in [-1, 0],\n            )\n            for step, batch in enumerate(epoch_iterator):\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    continue\n\n                inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n                self.model.train()\n                outputs = (\n                    self.model(inputs, masked_lm_labels=labels)\n                    if self.mlm\n                    else self.model(inputs, labels=labels)\n                )\n                loss = outputs[\n                    0\n                ]  # model outputs are always tuple in transformers (see doc)\n\n                if self.n_gpu > 1:\n                    loss = (\n                        loss.mean()\n                    )  # mean() to average on multi-gpu parallel training\n                if gradient_accumulation_steps > 1:\n                    loss = loss / gradient_accumulation_steps\n\n                if self.fp16:\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n\n                tr_loss += loss.item()\n                if (step + 1) % gradient_accumulation_steps == 0:\n                    if self.fp16:\n                        torch.nn.utils.clip_grad_norm_(\n                            amp.master_params(optimizer), max_grad_norm\n                        )\n                    else:\n                        torch.nn.utils.clip_grad_norm_(\n                            self.model.parameters(), max_grad_norm\n                        )\n                    optimizer.step()\n                    scheduler.step()  # Update learning rate schedule\n                    self.model.zero_grad()\n                    global_step += 1\n\n                    if (\n                        self.local_rank in [-1, 0]\n                        and logging_steps > 0\n                        and global_step % logging_steps == 0\n                    ):\n                        # Log metrics\n                        if (\n                            self.local_rank == -1 and evaluate_during_training\n                        ):  # Only evaluate when single GPU otherwise metrics may not average well\n                            results = self.evaluate(\n                                output_dir=output_dir,\n                                per_gpu_eval_batch_size=per_gpu_train_batch_size,\n                            )\n                            if use_tensorboard:\n                                for key, value in results.items():\n                                    tb_writer.add_scalar(\n                                        f""eval_{key}"", value, global_step\n                                    )\n                        if use_tensorboard:\n                            tb_writer.add_scalar(\n                                ""lr"", scheduler.get_lr()[0], global_step\n                            )\n                            tb_writer.add_scalar(\n                                ""loss"",\n                                (tr_loss - logging_loss) / logging_steps,\n                                global_step,\n                            )\n                        logging_loss = tr_loss\n\n                    if (\n                        self.local_rank in [-1, 0]\n                        and save_steps > 0\n                        and global_step % save_steps == 0\n                    ):\n                        checkpoint_prefix = ""checkpoint""\n                        # Save model checkpoint\n                        # TODO: os.makedirs bug when output_dir exists\n                        ckpt_output_dir = os.path.join(\n                            output_dir, f""{checkpoint_prefix}-{global_step}""\n                        )\n                        os.makedirs(ckpt_output_dir, exist_ok=True)\n                        model_to_save = (\n                            self.model.module\n                            if hasattr(self.model, ""module"")\n                            else self.model\n                        )  # Take care of distributed/parallel training\n                        model_to_save.save_pretrained(ckpt_output_dir)\n                        self.tokenizer.save_pretrained(ckpt_output_dir)\n\n                        torch.save(\n                            init_locals,\n                            os.path.join(ckpt_output_dir, ""training_args.bin""),\n                        )\n                        logger.info(""Saving model checkpoint to %s"", ckpt_output_dir)\n\n                        self._rotate_checkpoints(\n                            checkpoint_prefix,\n                            save_total_limit=save_total_limit,\n                            output_dir=output_dir,\n                        )\n\n                        torch.save(\n                            optimizer.state_dict(),\n                            os.path.join(ckpt_output_dir, ""optimizer.pt""),\n                        )\n                        torch.save(\n                            scheduler.state_dict(),\n                            os.path.join(ckpt_output_dir, ""scheduler.pt""),\n                        )\n                        logger.info(\n                            ""Saving optimizer and scheduler states to %s"",\n                            ckpt_output_dir,\n                        )\n\n                if max_steps > 0 and global_step > max_steps:\n                    epoch_iterator.close()\n                    break\n            if max_steps > 0 and global_step > max_steps:\n                train_iterator.close()\n                break\n\n        if self.local_rank in [-1, 0] and use_tensorboard:\n            tb_writer.close()\n\n        tr_loss = tr_loss / global_step\n\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n\n        # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n        if self.local_rank == -1 or torch.distributed.get_rank() == 0:\n            # Create output directory if needed\n            if self.local_rank in [-1, 0]:\n                os.makedirs(output_dir, exist_ok=True)\n\n            logger.info(""Saving model checkpoint to %s"", output_dir)\n            # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n            # They can then be reloaded using `from_pretrained()`\n            model_to_save = (\n                self.model.module if hasattr(self.model, ""module"") else self.model\n            )  # Take care of distributed/parallel training\n            model_to_save.save_pretrained(output_dir)\n            self.tokenizer.save_pretrained(output_dir)\n\n            # Good practice: save your training arguments together with the trained model\n            torch.save(init_locals, os.path.join(output_dir, ""training_args.bin""))\n\n            # Load a trained model and vocabulary that you have fine-tuned\n            config_class, model_class, tokenizer_class = self.MODEL_CLASSES[\n                self.model_type\n            ]\n            self.model = model_class.from_pretrained(output_dir)\n            self.tokenizer = tokenizer_class.from_pretrained(output_dir)\n            self.model.to(self.device)\n\n    def train(\n        self,\n        output_dir: str,\n        should_continue: bool = False,\n        overwrite_output_dir: bool = False,\n        evaluate_during_training: bool = False,\n        per_gpu_train_batch_size: int = 4,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 5e-5,\n        weight_decay: float = 0.0,\n        adam_epsilon: float = 1e-8,\n        max_grad_norm: float = 1.0,\n        num_train_epochs: float = 1.0,\n        max_steps: int = -1,\n        warmup_steps: int = 0,\n        logging_steps: int = 50,\n        save_steps: int = 50,\n        save_total_limit: int = 3,\n        use_tensorboard: bool = False,\n    ) -> None:\n        """"""\n\n        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n        * **should_continue** - Whether to continue training from latest checkpoint in `output_dir`\n        * **overwrite_output_dir** - Overwrite the content of output directory `output_dir`\n        * **evaluate_during_training** - Run evaluation during training at each `logging_step`.\n        * **per_gpu_train_batch_size** - Batch size per GPU/CPU for training. (If `evaluate_during_training` is True, this is also the eval batch size\n        * **gradient_accumulation_steps** - Number of updates steps to accumulate before performing a backward/update pass\n        * **learning_rate** - The initial learning rate for Adam optimizer.\n        * **weight_decay** - Weight decay if we apply some.\n        * **adam_epsilon** - Epsilon for Adam optimizer.\n        * **max_grad_norm** - Max gradient norm. Duh\n        * **num_train_epochs** - Total number of training epochs to perform.\n        * **max_steps** - If > 0: set total number of training steps to perform. Override `num_train_epochs`.\n        * **warmup_steps** - Linear warmup over warmup_steps.\n        * **logging_steps** - Number of steps until logging occurs.\n        * **save_steps** - Number of steps until checkpoint is saved in `output_dir`\n        * **save_total_limit** - Limit the total amount of checkpoints, delete the older checkpoints in the `output_dir`, does not delete by default\n        * **use_tensorboard** - Only useable if tensorboard is installed\n        **return** - None\n        """"""\n        # Check to overwrite\n        if (\n            os.path.exists(output_dir)\n            and os.listdir(output_dir)\n            and not overwrite_output_dir\n        ):\n            raise ValueError(\n                f""Output directory ({output_dir}) already exists and is not empty. Set overwrite_output_dir as `True` to overcome.""\n            )\n\n        # Check if continuing training from checkpoint\n        if should_continue:\n            sorted_checkpoints = self._sorted_checkpoints(\n                checkpoint_prefix=""checkpoint"", output_dir=output_dir\n            )\n            if len(sorted_checkpoints) == 0:\n                raise ValueError(\n                    f""Trying to continue training but no checkpoint was found in {output_dir}, set `should_continue` argument to False if training from scratch""\n                )\n            else:\n                self.model_name_or_path = sorted_checkpoints[-1]\n                self._initial_setup()\n\n        # Get locals for training args\n        init_locals = copy.deepcopy(locals())\n        init_locals.pop(""self"")\n\n        # Start logger\n        logger.info(""Training/evaluation parameters %s"", str(locals()))\n\n        ##############\n        ## Training ##\n        ##############\n        if self.local_rank not in [-1, 0]:\n            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n        # Load Dataset\n        train_dataset = self.load_and_cache_examples(evaluate=False)\n\n        if self.local_rank == 0:\n            torch.distributed.barrier()\n\n        if self.local_rank in [-1, 0] and use_tensorboard:\n            try:\n                from torch.utils.tensorboard import SummaryWriter\n\n                tb_writer = SummaryWriter()\n            except ImportError:\n                logger.warning(\n                    ""WARNING! Tensorboard is a required dependency...`use_tensorboard` is now set as False""\n                )\n                use_tensorboard = False\n                pass\n\n        # Train the model\n\n        train_batch_size = per_gpu_train_batch_size * max(1, self.n_gpu)\n\n        def collate(examples: List[torch.Tensor]):\n            if self.tokenizer._pad_token is None:\n                return pad_sequence(examples, batch_first=True)\n            return pad_sequence(\n                examples, batch_first=True, padding_value=self.tokenizer.pad_token_id\n            )\n\n        train_sampler = (\n            RandomSampler(train_dataset)\n            if self.local_rank == -1\n            else DistributedSampler(train_dataset)\n        )\n        train_dataloader = DataLoader(\n            train_dataset,\n            sampler=train_sampler,\n            batch_size=train_batch_size,\n            collate_fn=collate,\n        )\n\n        if max_steps > 0:\n            t_total = max_steps\n            num_train_epochs = (\n                max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n            )\n        else:\n            t_total = (\n                len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n            )\n\n        # Prepare optimizer and schedule (linear warmup and decay)\n        no_decay = [""bias"", ""LayerNorm.weight""]\n        optimizer_grouped_parameters = [\n            {\n                ""params"": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if not any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": weight_decay,\n            },\n            {\n                ""params"": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.0,\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n        )\n\n        # Check if saved optimizer or scheduler states exist\n        if (\n            self.model_name_or_path\n            and os.path.isfile(os.path.join(self.model_name_or_path, ""optimizer.pt""))\n            and os.path.isfile(os.path.join(self.model_name_or_path, ""scheduler.pt""))\n        ):\n            # Load in optimizer and scheduler states\n            optimizer.load_state_dict(\n                torch.load(os.path.join(self.model_name_or_path, ""optimizer.pt""))\n            )\n            scheduler.load_state_dict(\n                torch.load(os.path.join(self.model_name_or_path, ""scheduler.pt""))\n            )\n\n        if self.fp16:\n            try:\n                from apex import amp\n            except ImportError:\n                raise ImportError(\n                    ""Please install apex from https://www.github.com/nvidia/apex to use fp16 training.""\n                )\n            self.model, optimizer = amp.initialize(\n                self.model, optimizer, opt_level=self.fp16_opt_level\n            )\n\n        # multi-gpu training (should be after apex fp16 initialization)\n        if self.n_gpu > 1:\n            self.model = torch.nn.DataParallel(self.model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.local_rank != -1:\n            self.model = torch.nn.parallel.DistributedDataParallel(\n                self.model,\n                device_ids=[self.local_rank],\n                output_device=self.local_rank,\n                find_unused_parameters=True,\n            )\n\n        # Train!\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_dataset))\n        logger.info(""  Num Epochs = %d"", num_train_epochs)\n        logger.info(""  Instantaneous batch size per GPU = %d"", per_gpu_train_batch_size)\n        logger.info(\n            ""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n            train_batch_size\n            * gradient_accumulation_steps\n            * (torch.distributed.get_world_size() if self.local_rank != -1 else 1),\n        )\n        logger.info(""  Gradient Accumulation steps = %d"", gradient_accumulation_steps)\n        logger.info(""  Total optimization steps = %d"", t_total)\n\n        global_step = 0\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        # Check if continuing training from a checkpoint\n        if self.model_name_or_path and os.path.exists(self.model_name_or_path):\n            try:\n                # set global_step to gobal_step of last saved checkpoint from model path\n                checkpoint_suffix = self.model_name_or_path.split(""-"")[-1].split(""/"")[0]\n                global_step = int(checkpoint_suffix)\n                epochs_trained = global_step // (\n                    len(train_dataloader) // gradient_accumulation_steps\n                )\n                steps_trained_in_current_epoch = global_step % (\n                    len(train_dataloader) // gradient_accumulation_steps\n                )\n\n                logger.info(\n                    ""  Continuing training from checkpoint, will skip to saved global_step""\n                )\n                logger.info(""  Continuing training from epoch %d"", epochs_trained)\n                logger.info(""  Continuing training from global step %d"", global_step)\n                logger.info(\n                    ""  Will skip the first %d steps in the first epoch"",\n                    steps_trained_in_current_epoch,\n                )\n            except ValueError:\n                logger.info(""  Starting fine-tuning."")\n\n        tr_loss, logging_loss = 0.0, 0.0\n\n        model_to_resize = (\n            self.model.module if hasattr(self.model, ""module"") else self.model\n        )  # Take care of distributed/parallel training\n        model_to_resize.resize_token_embeddings(len(self.tokenizer))\n\n        self.model.zero_grad()\n        train_iterator = trange(\n            epochs_trained,\n            int(num_train_epochs),\n            desc=""Epoch"",\n            disable=self.local_rank not in [-1, 0],\n        )\n        self._set_seed()  # Added here for reproducibility\n        for _ in train_iterator:\n            epoch_iterator = tqdm(\n                train_dataloader,\n                desc=""Iteration"",\n                disable=self.local_rank not in [-1, 0],\n            )\n            for step, batch in enumerate(epoch_iterator):\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    continue\n\n                inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n                self.model.train()\n                outputs = (\n                    self.model(inputs, masked_lm_labels=labels)\n                    if self.mlm\n                    else self.model(inputs, labels=labels)\n                )\n                loss = outputs[\n                    0\n                ]  # model outputs are always tuple in transformers (see doc)\n\n                if self.n_gpu > 1:\n                    loss = (\n                        loss.mean()\n                    )  # mean() to average on multi-gpu parallel training\n                if gradient_accumulation_steps > 1:\n                    loss = loss / gradient_accumulation_steps\n\n                if self.fp16:\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n\n                tr_loss += loss.item()\n                if (step + 1) % gradient_accumulation_steps == 0:\n                    if self.fp16:\n                        torch.nn.utils.clip_grad_norm_(\n                            amp.master_params(optimizer), max_grad_norm\n                        )\n                    else:\n                        torch.nn.utils.clip_grad_norm_(\n                            self.model.parameters(), max_grad_norm\n                        )\n                    optimizer.step()\n                    scheduler.step()  # Update learning rate schedule\n                    self.model.zero_grad()\n                    global_step += 1\n\n                    if (\n                        self.local_rank in [-1, 0]\n                        and logging_steps > 0\n                        and global_step % logging_steps == 0\n                    ):\n                        # Log metrics\n                        if (\n                            self.local_rank == -1 and evaluate_during_training\n                        ):  # Only evaluate when single GPU otherwise metrics may not average well\n                            results = self.evaluate(\n                                output_dir=output_dir,\n                                per_gpu_eval_batch_size=per_gpu_train_batch_size,\n                            )\n                            if use_tensorboard:\n                                for key, value in results.items():\n                                    tb_writer.add_scalar(\n                                        f""eval_{key}"", value, global_step\n                                    )\n                        if use_tensorboard:\n                            tb_writer.add_scalar(\n                                ""lr"", scheduler.get_lr()[0], global_step\n                            )\n                            tb_writer.add_scalar(\n                                ""loss"",\n                                (tr_loss - logging_loss) / logging_steps,\n                                global_step,\n                            )\n                        logging_loss = tr_loss\n\n                    if (\n                        self.local_rank in [-1, 0]\n                        and save_steps > 0\n                        and global_step % save_steps == 0\n                    ):\n                        checkpoint_prefix = ""checkpoint""\n                        # Save model checkpoint\n                        # TODO: os.makedirs bug when output_dir exists\n                        ckpt_output_dir = os.path.join(\n                            output_dir, f""{checkpoint_prefix}-{global_step}""\n                        )\n                        os.makedirs(ckpt_output_dir, exist_ok=True)\n                        model_to_save = (\n                            self.model.module\n                            if hasattr(self.model, ""module"")\n                            else self.model\n                        )  # Take care of distributed/parallel training\n                        model_to_save.save_pretrained(ckpt_output_dir)\n                        self.tokenizer.save_pretrained(ckpt_output_dir)\n\n                        torch.save(\n                            init_locals,\n                            os.path.join(ckpt_output_dir, ""training_args.bin""),\n                        )\n                        logger.info(""Saving model checkpoint to %s"", ckpt_output_dir)\n\n                        self._rotate_checkpoints(\n                            checkpoint_prefix,\n                            save_total_limit=save_total_limit,\n                            output_dir=output_dir,\n                        )\n\n                        torch.save(\n                            optimizer.state_dict(),\n                            os.path.join(ckpt_output_dir, ""optimizer.pt""),\n                        )\n                        torch.save(\n                            scheduler.state_dict(),\n                            os.path.join(ckpt_output_dir, ""scheduler.pt""),\n                        )\n                        logger.info(\n                            ""Saving optimizer and scheduler states to %s"",\n                            ckpt_output_dir,\n                        )\n\n                if max_steps > 0 and global_step > max_steps:\n                    epoch_iterator.close()\n                    break\n            if max_steps > 0 and global_step > max_steps:\n                train_iterator.close()\n                break\n\n        if self.local_rank in [-1, 0] and use_tensorboard:\n            tb_writer.close()\n\n        tr_loss = tr_loss / global_step\n\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n\n        # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n        if self.local_rank == -1 or torch.distributed.get_rank() == 0:\n            # Create output directory if needed\n            if self.local_rank in [-1, 0]:\n                os.makedirs(output_dir, exist_ok=True)\n\n            logger.info(""Saving model checkpoint to %s"", output_dir)\n            # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n            # They can then be reloaded using `from_pretrained()`\n            model_to_save = (\n                self.model.module if hasattr(self.model, ""module"") else self.model\n            )  # Take care of distributed/parallel training\n            model_to_save.save_pretrained(output_dir)\n            self.tokenizer.save_pretrained(output_dir)\n\n            # Good practice: save your training arguments together with the trained model\n            torch.save(init_locals, os.path.join(output_dir, ""training_args.bin""))\n\n            # Load a trained model and vocabulary that you have fine-tuned\n            config_class, model_class, tokenizer_class = self.MODEL_CLASSES[\n                self.model_type\n            ]\n            self.model = model_class.from_pretrained(output_dir)\n            self.tokenizer = tokenizer_class.from_pretrained(output_dir)\n            self.model.to(self.device)\n\n    def evaluate_all_checkpoints(\n        self, output_dir: str, per_gpu_eval_batch_size: int = 4,\n    ) -> dict:\n        """"""\n        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n        * **per_gpu_eval_batch_size** - Batch size per GPU/CPU for evaluation.\n        **return** - Results in a dictionary\n        """"""\n        # Evaluation\n        results = {}\n        if self.local_rank in [-1, 0]:\n            # checkpoints = [output_dir]\n            checkpoints = list(\n                os.path.dirname(c)\n                for c in sorted(\n                    glob.glob(output_dir + ""/**/"" + WEIGHTS_NAME, recursive=True)\n                )\n            )\n            print(checkpoints)\n            logging.getLogger(""transformers.modeling_utils"").setLevel(\n                logging.WARN\n            )  # Reduce logging\n            logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n            for checkpoint in checkpoints:\n                global_step = checkpoint.split(""-"")[-1] if len(checkpoints) > 1 else """"\n                prefix = (\n                    checkpoint.split(""/"")[-1]\n                    if checkpoint.find(""checkpoint"") != -1\n                    else """"\n                )\n\n                config_class, model_class, tokenizer_class = self.MODEL_CLASSES[\n                    self.model_type\n                ]\n                self.model = model_class.from_pretrained(checkpoint)\n                self.model.to(self.device)\n                result = self.evaluate(\n                    output_dir=output_dir,\n                    per_gpu_eval_batch_size=per_gpu_eval_batch_size,\n                    prefix=prefix,\n                )\n                result = dict((k + f""_{global_step}"", v) for k, v in result.items())\n                results.update(result)\n        return results\n\n    def evaluate(\n        self, output_dir: str, per_gpu_eval_batch_size: int = 4, prefix: str = """",\n    ) -> dict:\n        """"""\n        * **output_dir** - The output directory where the model predictions and checkpoints will be written.\n        * **per_gpu_eval_batch_size** - Batch size per GPU/CPU for evaluation.\n        * **prefix** - Prefix of checkpoint i.e. ""checkpoint-50""\n        **return** - Results in a dictionary\n        """"""\n        # Loop to handle MNLI double evaluation (matched, mis-matched)\n        eval_output_dir = output_dir\n\n        eval_dataset = self.load_and_cache_examples(evaluate=True)\n\n        if self.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir, exist_ok=True)\n\n        eval_batch_size = per_gpu_eval_batch_size * max(1, self.n_gpu)\n\n        # Note that DistributedSampler samples randomly\n        def collate(examples: List[torch.Tensor]):\n            if self.tokenizer._pad_token is None:\n                return pad_sequence(examples, batch_first=True)\n            return pad_sequence(\n                examples, batch_first=True, padding_value=self.tokenizer.pad_token_id\n            )\n\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(\n            eval_dataset,\n            sampler=eval_sampler,\n            batch_size=eval_batch_size,\n            collate_fn=collate,\n        )\n\n        # multi-gpu evaluate\n        if self.n_gpu > 1:\n            self.model = torch.nn.DataParallel(self.model)\n\n        # Eval!\n        logger.info(f""***** Running evaluation {prefix} *****"")\n        logger.info(""  Num examples = %d"", len(eval_dataset))\n        logger.info(""  Batch size = %d"", eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        self.model.eval()\n\n        for batch in tqdm(eval_dataloader, desc=""Evaluating""):\n            inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n            inputs = inputs.to(self.device)\n            labels = labels.to(self.device)\n\n            with torch.no_grad():\n                outputs = (\n                    self.model(inputs, masked_lm_labels=labels)\n                    if self.mlm\n                    else self.model(inputs, labels=labels)\n                )\n                lm_loss = outputs[0]\n                eval_loss += lm_loss.mean().item()\n            nb_eval_steps += 1\n\n        eval_loss = eval_loss / nb_eval_steps\n        perplexity = torch.exp(torch.tensor(eval_loss))\n\n        result = {""perplexity"": perplexity}\n\n        output_eval_file = os.path.join(eval_output_dir, prefix, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            logger.info(f""***** Eval results {prefix} *****"")\n            for key in sorted(result.keys()):\n                logger.info(""  %s = %s"", key, str(result[key]))\n                writer.write(""%s = %s\\n"" % (key, str(result[key])))\n\n        return result\n\n    def load_and_cache_examples(self, evaluate=False):\n        return TextDataset(\n            self.tokenizer,\n            self.model_type,\n            self.overwrite_cache,\n            file_path=self.eval_data_file if evaluate else self.train_data_file,\n            block_size=self.block_size,\n        )\n\n    def _set_seed(self):\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n        if self.n_gpu > 0:\n            torch.cuda.manual_seed_all(self.seed)\n\n    def _sorted_checkpoints(\n        self, checkpoint_prefix: str, output_dir: str, use_mtime=False\n    ) -> List[str]:\n        ordering_and_checkpoint_path = []\n\n        glob_checkpoints = glob.glob(os.path.join(output_dir, f""{checkpoint_prefix}-*""))\n\n        for path in glob_checkpoints:\n            if use_mtime:\n                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n            else:\n                regex_match = re.match("".*{}-([0-9]+)"".format(checkpoint_prefix), path)\n                if regex_match and regex_match.groups():\n                    ordering_and_checkpoint_path.append(\n                        (int(regex_match.groups()[0]), path)\n                    )\n\n        checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n        return checkpoints_sorted\n\n    def _rotate_checkpoints(\n        self,\n        checkpoint_prefix: str,\n        save_total_limit: int,\n        output_dir: str,\n        use_mtime: bool = False,\n    ) -> None:\n        if not save_total_limit:\n            return\n        if save_total_limit <= 0:\n            return\n\n        # Check if we should delete older checkpoint(s)\n        checkpoints_sorted = self._sorted_checkpoints(\n            checkpoint_prefix, output_dir, use_mtime\n        )\n        if len(checkpoints_sorted) <= save_total_limit:\n            return\n        number_of_checkpoints_to_delete = max(\n            0, len(checkpoints_sorted) - save_total_limit\n        )\n        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n        for checkpoint in checkpoints_to_be_deleted:\n            logger.info(\n                f""Deleting older checkpoint [{checkpoint}] due to save_total_limit""\n            )\n            shutil.rmtree(checkpoint)\n\n    def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """""" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. """"""\n\n        if self.tokenizer.mask_token is None:\n            raise ValueError(\n                ""This tokenizer does not have a mask token which is necessary for masked language modeling. Set `mlm` param as False""\n            )\n\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n        special_tokens_mask = [\n            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n            for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(\n            torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0\n        )\n        if self.tokenizer._pad_token is not None:\n            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n            probability_matrix.masked_fill_(padding_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = (\n            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n        )\n        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(\n            self.tokenizer.mask_token\n        )\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = (\n            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n            & masked_indices\n            & ~indices_replaced\n        )\n        random_words = torch.randint(\n            len(self.tokenizer), labels.shape, dtype=torch.long\n        )\n        inputs[indices_random] = random_words[indices_random]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels\n\n    def find_learning_rate(\n        self,\n        output_dir: Union[Path, str],\n        file_name: str = ""learning_rate.tsv"",\n        start_learning_rate: float = 1e-7,\n        end_learning_rate: float = 10,\n        iterations: int = 100,\n        mini_batch_size: int = 8,\n        stop_early: bool = True,\n        smoothing_factor: float = 0.7,\n        adam_epsilon: float = 1e-8,\n        weight_decay: float = 0.0,\n        **kwargs,\n    ) -> float:\n        """"""\n        Uses Leslie\'s cyclical learning rate finding method to generate and save the loss x learning rate plot\n\n        This method returns a suggested learning rate using the static method `LMFineTuner.suggest_learning_rate()`\n        which is implicitly run in this method.\n\n        * **output_dir** - Path to dir for learning rate file to be saved\n        * **file_name** - Name of learning rate .tsv file\n        * **start_learning_rate** - Initial learning rate to start cyclical learning rate finder method\n        * **end_learning_rate** - End learning rate to stop exponential increase of the learning rate\n        * **iterations** - Number of optimizer iterations for the ExpAnnealLR scheduler\n        * **mini_batch_size** - Batch size for dataloader\n        * **stop_early** - Bool for stopping early once loss diverges\n        * **smoothing_factor** - Smoothing factor on moving average of losses\n        * **adam_epsilon** - Epsilon for Adam optimizer.\n        * **weight_decay** - Weight decay if we apply some.\n        * **kwargs** - Additional keyword arguments for the Adam optimizer\n        **return** - Learning rate as a float\n        """"""\n        best_loss = None\n        moving_avg_loss = 0\n\n        # cast string to Path\n        if type(output_dir) is str:\n            output_dir = Path(output_dir)\n        from flair.training_utils import (\n            init_output_file,\n            log_line,\n        )\n\n        learning_rate_tsv = init_output_file(output_dir, file_name)\n\n        with open(learning_rate_tsv, ""a"") as f:\n            f.write(""ITERATION\\tTIMESTAMP\\tLEARNING_RATE\\tTRAIN_LOSS\\n"")\n\n        # Prepare optimizer and schedule (linear warmup and decay) for transformer\'s AdamW optimzer\n        no_decay = [""bias"", ""LayerNorm.weight""]\n        optimizer_grouped_parameters = [\n            {\n                ""params"": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if not any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": weight_decay,\n            },\n            {\n                ""params"": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.0,\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=start_learning_rate,\n            eps=adam_epsilon,\n            **kwargs,\n        )\n        ## Original SGD optimizer Flair uses commnted out\n        # optimizer = AdamW(self.model.parameters(), lr=start_learning_rate, **kwargs)\n        # from torch.optim.sgd import SGD\n        # optimizer = SGD(\n        #    self.model.parameters(), lr=start_learning_rate, **kwargs\n        # )\n\n        # Flair\'s original EXPAnnealLR scheduler\n        from flair.optim import ExpAnnealLR\n\n        scheduler = ExpAnnealLR(optimizer, end_learning_rate, iterations)\n\n        model_state = self.model.state_dict()\n        self.model.train()\n\n        train_dataset = self.load_and_cache_examples(evaluate=False)\n\n        step = 0\n        while step < iterations:\n            train_sampler = RandomSampler(train_dataset)\n            batch_loader = DataLoader(\n                train_dataset, sampler=train_sampler, batch_size=mini_batch_size\n            )\n            for batch in batch_loader:\n                step += 1\n\n                # forward pass\n                inputs, labels = self.mask_tokens(batch) if self.mlm else (batch, batch)\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n                self.model.train()\n                outputs = (\n                    self.model(inputs, masked_lm_labels=labels)\n                    if self.mlm\n                    else self.model(inputs, labels=labels)\n                )\n                loss = outputs[0]\n\n                # update optimizer and scheduler\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n                optimizer.step()\n                scheduler.step(step)\n\n                print(scheduler.get_lr())\n                learning_rate = scheduler.get_lr()[0]\n\n                loss_item = loss.item()\n                if step == 1:\n                    best_loss = loss_item\n                else:\n                    if smoothing_factor > 0:\n                        moving_avg_loss = (\n                            smoothing_factor * moving_avg_loss\n                            + (1 - smoothing_factor) * loss_item\n                        )\n                        loss_item = moving_avg_loss / (\n                            1 - smoothing_factor ** (step + 1)\n                        )\n                    if loss_item < best_loss:\n                        best_loss = loss\n\n                if step > iterations:\n                    break\n\n                if stop_early and (loss_item > 4 * best_loss or torch.isnan(loss)):\n                    log_line(logger)\n                    logger.info(""loss diverged - stopping early!"")\n                    step = iterations\n                    break\n\n                with open(str(learning_rate_tsv), ""a"") as f:\n                    f.write(\n                        f""{step}\\t{datetime.datetime.now():%H:%M:%S}\\t{learning_rate}\\t{loss_item}\\n""\n                    )\n\n            self.model.load_state_dict(model_state)\n            self.model.to(self.device)\n\n        log_line(logger)\n        logger.info(\n            f""learning rate finder finished - plot {learning_rate_tsv} \\n Reinitalizing model\'s parameters and optimizer""\n        )\n        log_line(logger)\n\n        # Reinitialize transformers model\'s parameters (This could be optimized)\n        self._initial_setup()\n\n        plotter = Plotter()\n        plotter.plot_learning_rate(Path(learning_rate_tsv))\n\n        # Use the automated learning rate finder\n        with open(learning_rate_tsv) as lr_f:\n            lr_tsv = list(csv.reader(lr_f, delimiter=""\\t""))\n        losses = np.array([float(row[-1]) for row in lr_tsv[1:]])\n        lrs = np.array([float(row[-2]) for row in lr_tsv[1:]])\n        lr_to_use = self.suggest_learning_rate(losses, lrs)\n        print(f""Recommended Learning Rate {lr_to_use}"")\n        return lr_to_use\n\n    @staticmethod\n    def suggest_learning_rate(\n        losses: np.array,\n        lrs: np.array,\n        lr_diff: int = 30,\n        loss_threshold: float = 0.05,\n        adjust_value: float = 1,\n    ) -> float:\n        """"""\n        Attempts to find the optimal learning rate using a interval slide rule approach with the cyclical learning rate method\n\n        * **losses** - Numpy array of losses\n        * **lrs** - Numpy array of exponentially increasing learning rates (must match dim of `losses`)\n        * **lr_diff** - Learning rate Interval of slide ruler\n        * **loss_threshold** - Threshold of loss difference on interval where the sliding stops\n        * **adjust_value** - Coefficient for adjustment\n        **return** - the optimal learning rate as a float\n        """"""\n        # Get loss values and their corresponding gradients, and get lr values\n        assert lr_diff < len(losses)\n        loss_grad = np.gradient(losses)\n\n        # Search for index in gradients where loss is lowest before the loss spike\n        # Initialize right and left idx using the lr_diff as a spacing unit\n        # Set the local min lr as -1 to signify if threshold is too low\n        r_idx = -1\n        l_idx = r_idx - lr_diff\n        local_min_lr = lrs[l_idx]\n        while (l_idx >= -len(losses)) and (\n            abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold\n        ):\n            local_min_lr = lrs[l_idx]\n            r_idx -= 1\n            l_idx -= 1\n\n        lr_to_use = local_min_lr * adjust_value\n\n        return lr_to_use\n\n    def freeze_to(self, n: int) -> None:\n        """"""Freeze first n layers of model\n\n        * **n** - Starting from initial layer, freeze all layers up to nth layer inclusively\n        """"""\n        layers = list(self.model.parameters())\n        # Freeze up to n layers\n        for param in layers[:n]:\n            param.requires_grad = False\n        for param in layers[n:]:\n            param.requires_grad = True\n\n    def freeze(self) -> None:\n        """"""Freeze last classification layer group only\n        """"""\n        layers_len = len(list(self.model.cls.parameters()))\n        self.freeze_to(-layers_len)\n\n    def unfreeze(self) -> None:\n        """"""Unfreeze all layers\n        """"""\n        self.freeze_to(0)\n'"
adaptnlp/transformers/question_answering.py,34,"b'# Contains code used/modified by AdaptNLP author from transformers\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union, Tuple\nimport collections\nfrom collections import OrderedDict\n\nimport torch\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n\nfrom transformers import (\n    BertConfig,\n    BertForQuestionAnswering,\n    BertTokenizer,\n    XLMConfig,\n    XLMForQuestionAnswering,\n    XLMTokenizer,\n    XLNetConfig,\n    XLNetForQuestionAnswering,\n    XLNetTokenizer,\n)\n\nfrom adaptnlp.transformers.utils_squad import (\n    SquadExample,\n    InputFeatures,\n    convert_examples_to_features,\n    RawResult,\n    RawResultExtended,\n    get_final_text,\n    _get_best_indexes,\n    _compute_softmax,\n)\n\n\nclass QuestionAnsweringModel(ABC):\n    @abstractmethod\n    def __init__(self):\n        super().__init__()\n        self.config\n        self.tokenizer\n        self.model\n\n    @abstractmethod\n    def _load(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, query, context, top_n, as_dict):\n        raise NotImplementedError\n\n\n# TODO To be deprecated in the near future for a better module design\nclass BertQuestionAnsweringModel(QuestionAnsweringModel):\n    def __init__(self):\n        self.config = BertConfig\n        self.tokenizer = BertTokenizer\n        self.model = BertForQuestionAnswering\n        self.model_names = list(self.config.pretrained_config_archive_map.keys())\n\n        # Post Load\n        self.pretrained_config = None\n        self.pretrained_tokenizer = None\n        self.pretrained_model = None\n\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    def _to_list(self, tensor: torch.Tensor) -> List[float]:\n        return tensor.detach().cpu().tolist()\n\n    def _load(self) -> None:\n        print(""Loading Pretrained Bert Question Answering Model..."")\n        model_name = ""bert-large-uncased-whole-word-masking-finetuned-squad""\n        self.pretrained_config = self.config.from_pretrained(\n            ""bert-large-uncased-whole-word-masking-finetuned-squad""\n        )\n        if ""uncased"" in model_name:\n            tokenizer = self.tokenizer.from_pretrained(\n                ""bert-large-uncased"", do_lower_case=True\n            )\n        else:\n            tokenizer = self.tokenizer.from_pretrained(\n                ""bert-large-cased"", do_lower_case=False\n            )\n        self.pretrained_tokenizer = tokenizer\n\n        model = self.model.from_pretrained(\n            model_name,\n            from_tf=bool("".ckpt"" in model_name),\n            config=self.pretrained_config,\n        )\n        self.pretrained_model = model\n        self.pretrained_model.to(self.device)\n\n    def _load_one_query(\n        self, query: str, context: str, output_examples=True\n    ) -> Union[TensorDataset, List[SquadExample], List[InputFeatures]]:\n        # Create doc_tokens for SquadExample with one query and context\n\n        def is_whitespace(c):\n            if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n                return True\n            return False\n\n        # Create doc_tokens\n        doc_tokens = []\n        prev_is_whitespace = True\n        for c in context:\n            if is_whitespace(c):\n                prev_is_whitespace = True\n            else:\n                if prev_is_whitespace:\n                    doc_tokens.append(c)\n                else:\n                    doc_tokens[-1] += c\n                prev_is_whitespace = False\n\n        # Create SquadExample\n        examples = []\n        example = SquadExample(\n            qas_id=None,\n            question_text=query,\n            doc_tokens=doc_tokens,\n            orig_answer_text=None,\n            start_position=None,\n            end_position=None,\n            is_impossible=False,\n        )\n        examples.append(example)\n\n        # Convert to features\n        features = convert_examples_to_features(\n            examples=examples,\n            tokenizer=self.pretrained_tokenizer,\n            max_seq_length=384,\n            doc_stride=128,\n            max_query_length=64,\n            is_training=False,\n        )\n\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor(\n            [f.input_mask for f in features], dtype=torch.long\n        )\n        all_segment_ids = torch.tensor(\n            [f.segment_ids for f in features], dtype=torch.long\n        )\n        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n\n        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n        dataset = TensorDataset(\n            all_input_ids,\n            all_input_mask,\n            all_segment_ids,\n            all_example_index,\n            all_cls_index,\n            all_p_mask,\n        )\n\n        if output_examples:\n            return dataset, examples, features\n        return dataset\n\n    def _produce_concrete_predictions(\n        self,\n        all_examples,\n        all_features,\n        all_results,\n        n_best_size=10,\n        max_answer_length=30,\n        do_lower_case=True,\n        verbose_logging=False,\n        version_2_with_negative=True,\n        null_score_diff_threshold=0.0,\n    ):\n\n        example_index_to_features = collections.defaultdict(list)\n        for feature in all_features:\n            example_index_to_features[feature.example_index].append(feature)\n\n        unique_id_to_result = {}\n        for result in all_results:\n            unique_id_to_result[result.unique_id] = result\n\n        _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""PrelimPrediction"",\n            [""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""],\n        )\n\n        all_predictions = collections.OrderedDict()\n        all_nbest_json = collections.OrderedDict()\n        scores_diff_json = collections.OrderedDict()\n\n        for (example_index, example) in enumerate(all_examples):\n            features = example_index_to_features[example_index]\n\n            prelim_predictions = []\n            # keep track of the minimum score of null start+end of position 0\n            score_null = 1000000  # large and positive\n            min_null_feature_index = 0  # the paragraph slice with min null score\n            null_start_logit = 0  # the start logit at the slice with min null score\n            null_end_logit = 0  # the end logit at the slice with min null score\n            for (feature_index, feature) in enumerate(features):\n                result = unique_id_to_result[feature.unique_id]\n                start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n                end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n                # if we could have irrelevant answers, get the min score of irrelevant\n                if version_2_with_negative:\n                    feature_null_score = result.start_logits[0] + result.end_logits[0]\n                    if feature_null_score < score_null:\n                        score_null = feature_null_score\n                        min_null_feature_index = feature_index\n                        null_start_logit = result.start_logits[0]\n                        null_end_logit = result.end_logits[0]\n                for start_index in start_indexes:\n                    for end_index in end_indexes:\n                        # We could hypothetically create invalid predictions, e.g., predict\n                        # that the start of the span is in the question. We throw out all\n                        # invalid predictions.\n                        if start_index >= len(feature.tokens):\n                            continue\n                        if end_index >= len(feature.tokens):\n                            continue\n                        if start_index not in feature.token_to_orig_map:\n                            continue\n                        if end_index not in feature.token_to_orig_map:\n                            continue\n                        if not feature.token_is_max_context.get(start_index, False):\n                            continue\n                        if end_index < start_index:\n                            continue\n                        length = end_index - start_index + 1\n                        if length > max_answer_length:\n                            continue\n                        prelim_predictions.append(\n                            _PrelimPrediction(\n                                feature_index=feature_index,\n                                start_index=start_index,\n                                end_index=end_index,\n                                start_logit=result.start_logits[start_index],\n                                end_logit=result.end_logits[end_index],\n                            )\n                        )\n            if version_2_with_negative:\n                prelim_predictions.append(\n                    _PrelimPrediction(\n                        feature_index=min_null_feature_index,\n                        start_index=0,\n                        end_index=0,\n                        start_logit=null_start_logit,\n                        end_logit=null_end_logit,\n                    )\n                )\n            prelim_predictions = sorted(\n                prelim_predictions,\n                key=lambda x: (x.start_logit + x.end_logit),\n                reverse=True,\n            )\n\n            _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n                ""NbestPrediction"",\n                [""text"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""],\n            )  # ### start_end_index\n\n            seen_predictions = {}\n            nbest = []\n            for pred in prelim_predictions:\n                if len(nbest) >= n_best_size:\n                    break\n                feature = features[pred.feature_index]\n                orig_doc_start = 0\n                orig_doc_end = 0\n                if pred.start_index > 0:  # this is a non-null prediction\n                    tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n                    orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                    orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                    orig_tokens = example.doc_tokens[\n                        orig_doc_start : (orig_doc_end + 1)\n                    ]\n                    tok_text = "" "".join(tok_tokens)\n\n                    # De-tokenize WordPieces that have been split off.\n                    tok_text = tok_text.replace("" ##"", """")\n                    tok_text = tok_text.replace(""##"", """")\n\n                    # Clean whitespace\n                    tok_text = tok_text.strip()\n                    tok_text = "" "".join(tok_text.split())\n                    orig_text = "" "".join(orig_tokens)\n\n                    final_text = get_final_text(\n                        tok_text, orig_text, do_lower_case, verbose_logging\n                    )\n                    if final_text in seen_predictions:\n                        continue\n\n                    seen_predictions[final_text] = True\n                else:\n                    final_text = """"\n                    seen_predictions[final_text] = True\n\n                nbest.append(\n                    _NbestPrediction(\n                        text=final_text,\n                        start_logit=pred.start_logit,\n                        end_logit=pred.end_logit,\n                        start_index=orig_doc_start,\n                        end_index=orig_doc_end,\n                    )\n                )  # ### start_end_index...Make span indices inclusive\n            # if we didn\'t include the empty option in the n-best, include it\n            if version_2_with_negative:\n                if """" not in seen_predictions:\n                    nbest.append(\n                        _NbestPrediction(\n                            text="""",\n                            start_logit=null_start_logit,\n                            end_logit=null_end_logit,\n                            start_index=0,\n                            end_index=0,\n                        )\n                    )  # ### start_end_index should this be pred.<index>\n\n                # In very rare edge cases we could only have single null prediction.\n                # So we just create a nonce prediction in this case to avoid failure.\n                if len(nbest) == 1:\n                    nbest.insert(\n                        0,\n                        _NbestPrediction(\n                            text=""empty"",\n                            start_logit=0.0,\n                            end_logit=0.0,\n                            start_index=0.0,\n                            end_index=0.0,\n                        ),\n                    )  # ### start_end_index\n\n            # In very rare edge cases we could have no valid predictions. So we\n            # just create a nonce prediction in this case to avoid failure.\n            if not nbest:\n                nbest.append(\n                    _NbestPrediction(\n                        text=""empty"",\n                        start_logit=0.0,\n                        end_logit=0.0,\n                        start_index=0.0,\n                        end_index=0.0,\n                    )\n                )  # ### start_end_index\n\n            assert len(nbest) >= 1\n\n            total_scores = []\n            best_non_null_entry = None\n            for entry in nbest:\n                total_scores.append(entry.start_logit + entry.end_logit)\n                if not best_non_null_entry:\n                    if entry.text:\n                        best_non_null_entry = entry\n\n            probs = _compute_softmax(total_scores)\n\n            nbest_json = []\n            for (i, entry) in enumerate(nbest):\n                output = collections.OrderedDict()\n                output[""text""] = entry.text\n                output[""probability""] = probs[i]\n                output[""start_logit""] = entry.start_logit\n                output[""end_logit""] = entry.end_logit\n                output[\n                    ""start_index""\n                ] = (\n                    entry.start_index\n                )  # ### start_end_index MAGIC NUMBERS for adjustment :/\n                output[""end_index""] = entry.end_index\n                nbest_json.append(output)\n\n            assert len(nbest_json) >= 1\n\n            if not version_2_with_negative:\n                all_predictions[example.qas_id] = nbest_json[0][""text""]\n            else:\n                # predict """" iff the null score - the score of best non-null > threshold\n                score_diff = (\n                    score_null\n                    - best_non_null_entry.start_logit\n                    - (best_non_null_entry.end_logit)\n                )\n                scores_diff_json[example.qas_id] = score_diff\n                if score_diff > null_score_diff_threshold:\n                    all_predictions[example.qas_id] = """"\n                else:\n                    all_predictions[example.qas_id] = best_non_null_entry.text\n            all_nbest_json[example.qas_id] = nbest_json\n\n            # All ids set as None so get rid of None Key\n            all_predictions = all_predictions[None]\n            all_nbest_json = all_nbest_json[None]\n\n        return all_predictions, all_nbest_json\n\n    def predict(\n        self, query: str, context: str, n_best_size: int = 20\n    ) -> Tuple[str, List[OrderedDict]]:\n        """""" Predicts top_n answer spans of query in regards to context\n\n        Args:\n            query: The question\n            context: The context of which the question is asking\n            top_n: The top n answers returned\n\n        Returns:\n            Either a list of string answers or a dict of the results\n        """"""\n        self._load() if not self.pretrained_model or not self.pretrained_tokenizer else None\n\n        # Load and Evaluate Context Queries\n        dataset, examples, features = self._load_one_query(query, context)\n        eval_sampler = SequentialSampler(dataset)\n        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=5)\n        all_results = []\n        for batch in eval_dataloader:\n            self.pretrained_model.eval()\n            batch = tuple(t.to(self.device) for t in batch)\n\n            with torch.no_grad():\n                # BERT XLM XLNET DIFFERENCE\n                inputs = {\n                    ""input_ids"": batch[0],\n                    ""attention_mask"": batch[1],\n                    ""token_type_ids"": batch[2],\n                }\n                example_indices = batch[3]\n                outputs = self.pretrained_model(**inputs)\n\n            for i, example_index in enumerate(example_indices):\n                eval_feature = features[example_index.item()]\n                unique_id = int(eval_feature.unique_id)\n                # BERT XLM XLNET DIFFERENCE\n                result = RawResult(\n                    unique_id=unique_id,\n                    start_logits=self._to_list(outputs[0][i]),\n                    end_logits=self._to_list(outputs[1][i]),\n                )\n                all_results.append(result)\n\n        # Obtain Concrete Predictions\n        all_predictions, all_nbest_json = self._produce_concrete_predictions(\n            examples, features, all_results, n_best_size=n_best_size\n        )\n        return all_predictions, all_nbest_json\n\n\nclass XLNetQuestionAnsweringModel(QuestionAnsweringModel):\n    def __init__(self):\n        self.config = XLNetConfig\n        self.tokenizer = XLNetTokenizer\n        self.model = XLNetForQuestionAnswering\n        self.model_names = list(self.config.pretrained_config_archive_map.keys())\n\n        # Post Load\n        self.pretrained_config = None\n        self.pretrained_tokenizer = None\n        self.pretrained_model = None\n\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    def _to_list(self, tensor: torch.Tensor) -> List[float]:\n        return tensor.detach().cpu().tolist()\n\n    def _load(self) -> None:\n        print(""Loading Pretrained XLNet Question Answering Model..."")\n        model_name = ""xlnet-large-cased""\n        self.pretrained_config = self.config.from_pretrained(""xlnet-large-cased"")\n        tokenizer = self.tokenizer.from_pretrained(\n            ""xlnet-large-cased"", do_lower_case=False\n        )\n        self.pretrained_tokenizer = tokenizer\n\n        model = self.model.from_pretrained(\n            model_name,\n            from_tf=bool("".ckpt"" in model_name),\n            config=self.pretrained_config,\n        )\n        self.pretrained_model = model\n        self.pretrained_model.to(self.device)\n\n    def _load_one_query(\n        self, query: str, context: str, output_examples=True\n    ) -> Union[TensorDataset, List[SquadExample], List[InputFeatures]]:\n        # Create doc_tokens for SquadExample with one query and context\n\n        def is_whitespace(c):\n            if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n                return True\n            return False\n\n        # Create doc_tokens\n        doc_tokens = []\n        prev_is_whitespace = True\n        for c in context:\n            if is_whitespace(c):\n                prev_is_whitespace = True\n            else:\n                if prev_is_whitespace:\n                    doc_tokens.append(c)\n                else:\n                    doc_tokens[-1] += c\n                prev_is_whitespace = False\n\n        # Create SquadExample\n        examples = []\n        example = SquadExample(\n            qas_id=None,\n            question_text=query,\n            doc_tokens=doc_tokens,\n            orig_answer_text=None,\n            start_position=None,\n            end_position=None,\n            is_impossible=False,\n        )\n        examples.append(example)\n\n        # Convert to features\n        features = convert_examples_to_features(\n            examples=examples,\n            tokenizer=self.pretrained_tokenizer,\n            max_seq_length=384,\n            doc_stride=128,\n            max_query_length=64,\n            is_training=False,\n        )\n\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor(\n            [f.input_mask for f in features], dtype=torch.long\n        )\n        all_segment_ids = torch.tensor(\n            [f.segment_ids for f in features], dtype=torch.long\n        )\n        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n\n        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n        dataset = TensorDataset(\n            all_input_ids,\n            all_input_mask,\n            all_segment_ids,\n            all_example_index,\n            all_cls_index,\n            all_p_mask,\n        )\n\n        if output_examples:\n            return dataset, examples, features\n        return dataset\n\n    def _produce_concrete_predictions(\n        self,\n        all_examples,\n        all_features,\n        all_results,\n        n_best_size=10,\n        max_answer_length=30,\n        verbose_logging=False,\n    ):\n\n        start_n_top = self.pretrained_model.config.start_n_top\n        end_n_top = self.pretrained_model.config.end_n_top\n        tokenizer = self.pretrained_tokenizer\n\n        _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""PrelimPrediction"",\n            [\n                ""feature_index"",\n                ""start_index"",\n                ""end_index"",\n                ""start_log_prob"",\n                ""end_log_prob"",\n            ],\n        )\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"", [""text"", ""start_log_prob"", ""end_log_prob""]\n        )\n\n        example_index_to_features = collections.defaultdict(list)\n        for feature in all_features:\n            example_index_to_features[feature.example_index].append(feature)\n\n        unique_id_to_result = {}\n        for result in all_results:\n            unique_id_to_result[result.unique_id] = result\n\n        all_predictions = collections.OrderedDict()\n        all_nbest_json = collections.OrderedDict()\n        scores_diff_json = collections.OrderedDict()\n\n        for (example_index, example) in enumerate(all_examples):\n            features = example_index_to_features[example_index]\n\n            prelim_predictions = []\n            # keep track of the minimum score of null start+end of position 0\n            score_null = 1000000  # large and positive\n\n            for (feature_index, feature) in enumerate(features):\n                result = unique_id_to_result[feature.unique_id]\n\n                cur_null_score = result.cls_logits\n\n                # if we could have irrelevant answers, get the min score of irrelevant\n                score_null = min(score_null, cur_null_score)\n\n                for i in range(start_n_top):\n                    for j in range(end_n_top):\n                        start_log_prob = result.start_top_log_probs[i]\n                        start_index = result.start_top_index[i]\n\n                        j_index = i * end_n_top + j\n\n                        end_log_prob = result.end_top_log_probs[j_index]\n                        end_index = result.end_top_index[j_index]\n\n                        # We could hypothetically create invalid predictions, e.g., predict\n                        # that the start of the span is in the question. We throw out all\n                        # invalid predictions.\n                        if start_index >= feature.paragraph_len - 1:\n                            continue\n                        if end_index >= feature.paragraph_len - 1:\n                            continue\n\n                        if not feature.token_is_max_context.get(start_index, False):\n                            continue\n                        if end_index < start_index:\n                            continue\n                        length = end_index - start_index + 1\n                        if length > max_answer_length:\n                            continue\n\n                        prelim_predictions.append(\n                            _PrelimPrediction(\n                                feature_index=feature_index,\n                                start_index=start_index,\n                                end_index=end_index,\n                                start_log_prob=start_log_prob,\n                                end_log_prob=end_log_prob,\n                            )\n                        )\n\n            prelim_predictions = sorted(\n                prelim_predictions,\n                key=lambda x: (x.start_log_prob + x.end_log_prob),\n                reverse=True,\n            )\n\n            seen_predictions = {}\n            nbest = []\n            for pred in prelim_predictions:\n                if len(nbest) >= n_best_size:\n                    break\n                feature = features[pred.feature_index]\n\n                # XLNet un-tokenizer\n                # Let\'s keep it simple for now and see if we need all this later.\n                #\n                # tok_start_to_orig_index = feature.tok_start_to_orig_index\n                # tok_end_to_orig_index = feature.tok_end_to_orig_index\n                # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n                # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n                # paragraph_text = example.paragraph_text\n                # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n\n                # Previously used Bert untokenizer\n                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = "" "".join(tok_text.split())\n                orig_text = "" "".join(orig_tokens)\n\n                final_text = get_final_text(\n                    tok_text, orig_text, tokenizer.do_lower_case, verbose_logging\n                )\n\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n\n                nbest.append(\n                    _NbestPrediction(\n                        text=final_text,\n                        start_log_prob=pred.start_log_prob,\n                        end_log_prob=pred.end_log_prob,\n                    )\n                )\n\n            # In very rare edge cases we could have no valid predictions. So we\n            # just create a nonce prediction in this case to avoid failure.\n            if not nbest:\n                nbest.append(\n                    _NbestPrediction(text="""", start_log_prob=-1e6, end_log_prob=-1e6)\n                )\n\n            total_scores = []\n            best_non_null_entry = None\n            for entry in nbest:\n                total_scores.append(entry.start_log_prob + entry.end_log_prob)\n                if not best_non_null_entry:\n                    best_non_null_entry = entry\n\n            probs = _compute_softmax(total_scores)\n\n            nbest_json = []\n            for (i, entry) in enumerate(nbest):\n                output = collections.OrderedDict()\n                output[""text""] = entry.text\n                output[""probability""] = probs[i]\n                output[""start_log_prob""] = entry.start_log_prob\n                output[""end_log_prob""] = entry.end_log_prob\n                nbest_json.append(output)\n\n            assert len(nbest_json) >= 1\n            assert best_non_null_entry is not None\n\n            score_diff = score_null\n            scores_diff_json[example.qas_id] = score_diff\n            # note(zhiliny): always predict best_non_null_entry\n            # and the evaluation script will search for the best threshold\n            all_predictions[example.qas_id] = best_non_null_entry.text\n\n            all_nbest_json[example.qas_id] = nbest_json\n\n        """"""\n        if version_2_with_negative:\n            with open(output_null_log_odds_file, ""w"") as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n        with open(orig_data_file, ""r"", encoding=\'utf-8\') as reader:\n            orig_data = json.load(reader)[""data""]\n        """"""\n\n        # All ids set as None so get rid of None Key\n        all_predictions = all_predictions[None]\n        all_nbest_json = all_nbest_json[None]\n\n        return all_predictions, all_nbest_json\n\n    def predict(\n        self, query: str, context: str, n_best_size: int = 20, as_dict: bool = False\n    ) -> Union[List[str], dict]:\n        """""" Predicts top_n answer spans of query in regards to context\n\n        Args:\n            query: The question\n            context: The context of which the question is asking\n            top_n: The top n answers returned\n            as_dict: Returns answer in dict format if True\n\n        Returns:\n            Either a list of string answers or a dict of the results\n        """"""\n        self._load() if not self.pretrained_model or not self.pretrained_tokenizer else None\n\n        # Load and Evaluate Context Queries\n        dataset, examples, features = self._load_one_query(query, context)\n        eval_sampler = SequentialSampler(dataset)\n        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=5)\n        all_results = []\n        for batch in eval_dataloader:\n            self.pretrained_model.eval()\n            batch = tuple(t.to(self.device) for t in batch)\n\n            with torch.no_grad():\n                # BERT XLM XLNET DIFFERENCE\n                inputs = {\n                    ""input_ids"": batch[0],\n                    ""attention_mask"": batch[1],\n                    ""token_type_ids"": batch[2],\n                    ""cls_index"": batch[4],\n                    ""p_mask"": batch[5],\n                }\n                example_indices = batch[3]\n                outputs = self.pretrained_model(**inputs)\n\n            for i, example_index in enumerate(example_indices):\n                eval_feature = features[example_index.item()]\n                unique_id = int(eval_feature.unique_id)\n                # BERT XLM XLNET DIFFERENCE\n                result = RawResultExtended(\n                    unique_id=unique_id,\n                    start_top_log_probs=self._to_list(outputs[0][i]),\n                    start_top_index=self._to_list(outputs[1][i]),\n                    end_top_log_probs=self._to_list(outputs[2][i]),\n                    end_top_index=self._to_list(outputs[3][i]),\n                    cls_logits=self._to_list(outputs[4][i]),\n                )\n                all_results.append(result)\n\n        # Obtain Concrete Predictions\n        all_predictions, all_nbest_json = self._produce_concrete_predictions(\n            examples, features, all_results, n_best_size=n_best_size\n        )\n        return all_predictions, all_nbest_json\n\n\nclass XLMQuestionAnsweringModel(QuestionAnsweringModel):\n    def __init__(self):\n        self.config = XLMConfig\n        self.tokenizer = XLMTokenizer\n        self.model = XLMForQuestionAnswering\n        self.model_names = list(self.config.pretrained_config_archive_map.keys())\n\n        # Post Load\n        self.pretrained_config = None\n        self.pretrained_tokenizer = None\n        self.pretrained_model = None\n\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    def _to_list(self, tensor: torch.Tensor) -> List[float]:\n        return tensor.detach().cpu().tolist()\n\n    def _load(self) -> None:\n        print(""Loading Pretrained XLNet Question Answering Model..."")\n        model_name = ""xlm-mlm-en-2048""\n        self.pretrained_config = self.config.from_pretrained(""xlm-mlm-en-2048"")\n        tokenizer = self.tokenizer.from_pretrained(\n            ""xlm-mlm-en-2048"", do_lower_case=False\n        )\n        self.pretrained_tokenizer = tokenizer\n\n        model = self.model.from_pretrained(\n            model_name,\n            from_tf=bool("".ckpt"" in model_name),\n            config=self.pretrained_config,\n        )\n        self.pretrained_model = model\n        self.pretrained_model.to(self.device)\n\n    def _load_one_query(\n        self, query: str, context: str, output_examples=True\n    ) -> Union[TensorDataset, List[SquadExample], List[InputFeatures]]:\n        # Create doc_tokens for SquadExample with one query and context\n\n        def is_whitespace(c):\n            if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n                return True\n            return False\n\n        # Create doc_tokens\n        doc_tokens = []\n        prev_is_whitespace = True\n        for c in context:\n            if is_whitespace(c):\n                prev_is_whitespace = True\n            else:\n                if prev_is_whitespace:\n                    doc_tokens.append(c)\n                else:\n                    doc_tokens[-1] += c\n                prev_is_whitespace = False\n\n        # Create SquadExample\n        examples = []\n        example = SquadExample(\n            qas_id=None,\n            question_text=query,\n            doc_tokens=doc_tokens,\n            orig_answer_text=None,\n            start_position=None,\n            end_position=None,\n            is_impossible=False,\n        )\n        examples.append(example)\n\n        # Convert to features\n        features = convert_examples_to_features(\n            examples=examples,\n            tokenizer=self.pretrained_tokenizer,\n            max_seq_length=384,\n            doc_stride=128,\n            max_query_length=64,\n            is_training=False,\n        )\n\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor(\n            [f.input_mask for f in features], dtype=torch.long\n        )\n        all_segment_ids = torch.tensor(\n            [f.segment_ids for f in features], dtype=torch.long\n        )\n        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n\n        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n        dataset = TensorDataset(\n            all_input_ids,\n            all_input_mask,\n            all_segment_ids,\n            all_example_index,\n            all_cls_index,\n            all_p_mask,\n        )\n\n        if output_examples:\n            return dataset, examples, features\n        return dataset\n\n    def _produce_concrete_predictions(\n        self,\n        all_examples,\n        all_features,\n        all_results,\n        n_best_size=10,\n        max_answer_length=30,\n        verbose_logging=False,\n    ):\n\n        start_n_top = self.pretrained_model.config.start_n_top\n        end_n_top = self.pretrained_model.config.end_n_top\n        tokenizer = self.pretrained_tokenizer\n\n        _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""PrelimPrediction"",\n            [\n                ""feature_index"",\n                ""start_index"",\n                ""end_index"",\n                ""start_log_prob"",\n                ""end_log_prob"",\n            ],\n        )\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"", [""text"", ""start_log_prob"", ""end_log_prob""]\n        )\n\n        example_index_to_features = collections.defaultdict(list)\n        for feature in all_features:\n            example_index_to_features[feature.example_index].append(feature)\n\n        unique_id_to_result = {}\n        for result in all_results:\n            unique_id_to_result[result.unique_id] = result\n\n        all_predictions = collections.OrderedDict()\n        all_nbest_json = collections.OrderedDict()\n        scores_diff_json = collections.OrderedDict()\n\n        for (example_index, example) in enumerate(all_examples):\n            features = example_index_to_features[example_index]\n\n            prelim_predictions = []\n            # keep track of the minimum score of null start+end of position 0\n            score_null = 1000000  # large and positive\n\n            for (feature_index, feature) in enumerate(features):\n                result = unique_id_to_result[feature.unique_id]\n\n                cur_null_score = result.cls_logits\n\n                # if we could have irrelevant answers, get the min score of irrelevant\n                score_null = min(score_null, cur_null_score)\n\n                for i in range(start_n_top):\n                    for j in range(end_n_top):\n                        start_log_prob = result.start_top_log_probs[i]\n                        start_index = result.start_top_index[i]\n\n                        j_index = i * end_n_top + j\n\n                        end_log_prob = result.end_top_log_probs[j_index]\n                        end_index = result.end_top_index[j_index]\n\n                        # We could hypothetically create invalid predictions, e.g., predict\n                        # that the start of the span is in the question. We throw out all\n                        # invalid predictions.\n                        if start_index >= feature.paragraph_len - 1:\n                            continue\n                        if end_index >= feature.paragraph_len - 1:\n                            continue\n\n                        if not feature.token_is_max_context.get(start_index, False):\n                            continue\n                        if end_index < start_index:\n                            continue\n                        length = end_index - start_index + 1\n                        if length > max_answer_length:\n                            continue\n\n                        prelim_predictions.append(\n                            _PrelimPrediction(\n                                feature_index=feature_index,\n                                start_index=start_index,\n                                end_index=end_index,\n                                start_log_prob=start_log_prob,\n                                end_log_prob=end_log_prob,\n                            )\n                        )\n\n            prelim_predictions = sorted(\n                prelim_predictions,\n                key=lambda x: (x.start_log_prob + x.end_log_prob),\n                reverse=True,\n            )\n\n            seen_predictions = {}\n            nbest = []\n            for pred in prelim_predictions:\n                if len(nbest) >= n_best_size:\n                    break\n                feature = features[pred.feature_index]\n\n                # XLNet un-tokenizer\n                # Let\'s keep it simple for now and see if we need all this later.\n                #\n                # tok_start_to_orig_index = feature.tok_start_to_orig_index\n                # tok_end_to_orig_index = feature.tok_end_to_orig_index\n                # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n                # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n                # paragraph_text = example.paragraph_text\n                # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n\n                # Previously used Bert untokenizer\n                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = "" "".join(tok_text.split())\n                orig_text = "" "".join(orig_tokens)\n\n                final_text = get_final_text(\n                    tok_text,\n                    orig_text,\n                    tokenizer,  # .do_lower_case, (a XLM problem?)\n                    verbose_logging,\n                )\n\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n\n                nbest.append(\n                    _NbestPrediction(\n                        text=final_text,\n                        start_log_prob=pred.start_log_prob,\n                        end_log_prob=pred.end_log_prob,\n                    )\n                )\n\n            # In very rare edge cases we could have no valid predictions. So we\n            # just create a nonce prediction in this case to avoid failure.\n            if not nbest:\n                nbest.append(\n                    _NbestPrediction(text="""", start_log_prob=-1e6, end_log_prob=-1e6)\n                )\n\n            total_scores = []\n            best_non_null_entry = None\n            for entry in nbest:\n                total_scores.append(entry.start_log_prob + entry.end_log_prob)\n                if not best_non_null_entry:\n                    best_non_null_entry = entry\n\n            probs = _compute_softmax(total_scores)\n\n            nbest_json = []\n            for (i, entry) in enumerate(nbest):\n                output = collections.OrderedDict()\n                output[""text""] = entry.text\n                output[""probability""] = probs[i]\n                output[""start_log_prob""] = entry.start_log_prob\n                output[""end_log_prob""] = entry.end_log_prob\n                nbest_json.append(output)\n\n            assert len(nbest_json) >= 1\n            assert best_non_null_entry is not None\n\n            score_diff = score_null\n            scores_diff_json[example.qas_id] = score_diff\n            # note(zhiliny): always predict best_non_null_entry\n            # and the evaluation script will search for the best threshold\n            all_predictions[example.qas_id] = best_non_null_entry.text\n\n            all_nbest_json[example.qas_id] = nbest_json\n\n        """"""\n        if version_2_with_negative:\n            with open(output_null_log_odds_file, ""w"") as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n        with open(orig_data_file, ""r"", encoding=\'utf-8\') as reader:\n            orig_data = json.load(reader)[""data""]\n        """"""\n\n        # All ids set as None so get rid of None Key\n        all_predictions = all_predictions[None]\n        all_nbest_json = all_nbest_json[None]\n\n        return all_predictions, all_nbest_json\n\n    def predict(\n        self, query: str, context: str, n_best_size: int = 20, as_dict: bool = False\n    ) -> Union[List[str], dict]:\n        """""" Predicts top_n answer spans of query in regards to context\n\n        Args:\n            query: The question\n            context: The context of which the question is asking\n            top_n: The top n answers returned\n            as_dict: Returns answer in dict format if True\n\n        Returns:\n            Either a list of string answers or a dict of the results\n        """"""\n        self._load() if not self.pretrained_model or not self.pretrained_tokenizer else None\n\n        # Load and Evaluate Context Queries\n        dataset, examples, features = self._load_one_query(query, context)\n        eval_sampler = SequentialSampler(dataset)\n        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=5)\n        all_results = []\n        for batch in eval_dataloader:\n            self.pretrained_model.eval()\n            batch = tuple(t.to(self.device) for t in batch)\n\n            with torch.no_grad():\n                # BERT XLM XLNET DIFFERENCE\n                inputs = {\n                    ""input_ids"": batch[0],\n                    ""attention_mask"": batch[1],\n                    ""token_type_ids"": batch[2],\n                    ""cls_index"": batch[4],\n                    ""p_mask"": batch[5],\n                }\n                example_indices = batch[3]\n                outputs = self.pretrained_model(**inputs)\n\n            for i, example_index in enumerate(example_indices):\n                eval_feature = features[example_index.item()]\n                unique_id = int(eval_feature.unique_id)\n                # BERT XLM XLNET DIFFERENCE\n                result = RawResultExtended(\n                    unique_id=unique_id,\n                    start_top_log_probs=self._to_list(outputs[0][i]),\n                    start_top_index=self._to_list(outputs[1][i]),\n                    end_top_log_probs=self._to_list(outputs[2][i]),\n                    end_top_index=self._to_list(outputs[3][i]),\n                    cls_logits=self._to_list(outputs[4][i]),\n                )\n                all_results.append(result)\n\n        # Obtain Concrete Predictions\n        all_predictions, all_nbest_json = self._produce_concrete_predictions(\n            examples, features, all_results, n_best_size=n_best_size\n        )\n        return all_predictions, all_nbest_json\n'"
adaptnlp/transformers/squad_metrics.py,0,"b'""""""Very heavily inspired by the official evaluation script for SQuAD version 2.0 which was\nmodified by XLNet authors to update `find_best_threshold` scripts for SQuAD V2.0\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID\'s to the model\'s predicted probability\nthat a question is unanswerable.\n\nAdditional modifications for adaptnlp include making prediction writes optional\n""""""\n\n\nimport collections\nimport json\nimport logging\nimport math\nimport re\nimport string\n\nfrom transformers.tokenization_bert import BasicTokenizer\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n\n    def remove_articles(text):\n        regex = re.compile(r""\\b(a|an|the)\\b"", re.UNICODE)\n        return re.sub(regex, "" "", text)\n\n    def white_space_fix(text):\n        return "" "".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return """".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef get_raw_scores(examples, preds):\n    """"""\n    Computes the exact and f1 scores from the examples and the model predictions\n    """"""\n    exact_scores = {}\n    f1_scores = {}\n\n    for example in examples:\n        qas_id = example.qas_id\n        gold_answers = [\n            answer[""text""]\n            for answer in example.answers\n            if normalize_answer(answer[""text""])\n        ]\n\n        if not gold_answers:\n            # For unanswerable questions, only correct answer is empty string\n            gold_answers = [""""]\n\n        if qas_id not in preds:\n            print(""Missing prediction for %s"" % qas_id)\n            continue\n\n        prediction = preds[qas_id]\n        exact_scores[qas_id] = max(compute_exact(a, prediction) for a in gold_answers)\n        f1_scores[qas_id] = max(compute_f1(a, prediction) for a in gold_answers)\n\n    return exact_scores, f1_scores\n\n\ndef apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n    new_scores = {}\n    for qid, s in scores.items():\n        pred_na = na_probs[qid] > na_prob_thresh\n        if pred_na:\n            new_scores[qid] = float(not qid_to_has_ans[qid])\n        else:\n            new_scores[qid] = s\n    return new_scores\n\n\ndef make_eval_dict(exact_scores, f1_scores, qid_list=None):\n    if not qid_list:\n        total = len(exact_scores)\n        return collections.OrderedDict(\n            [\n                (""exact"", 100.0 * sum(exact_scores.values()) / total),\n                (""f1"", 100.0 * sum(f1_scores.values()) / total),\n                (""total"", total),\n            ]\n        )\n    else:\n        total = len(qid_list)\n        return collections.OrderedDict(\n            [\n                (""exact"", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n                (""f1"", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n                (""total"", total),\n            ]\n        )\n\n\ndef merge_eval(main_eval, new_eval, prefix):\n    for k in new_eval:\n        main_eval[""%s_%s"" % (prefix, k)] = new_eval[k]\n\n\ndef find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n    cur_score = num_no_ans\n    best_score = cur_score\n    best_thresh = 0.0\n    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n    for i, qid in enumerate(qid_list):\n        if qid not in scores:\n            continue\n        if qid_to_has_ans[qid]:\n            diff = scores[qid]\n        else:\n            if preds[qid]:\n                diff = -1\n            else:\n                diff = 0\n        cur_score += diff\n        if cur_score > best_score:\n            best_score = cur_score\n            best_thresh = na_probs[qid]\n\n    has_ans_score, has_ans_cnt = 0, 0\n    for qid in qid_list:\n        if not qid_to_has_ans[qid]:\n            continue\n        has_ans_cnt += 1\n\n        if qid not in scores:\n            continue\n        has_ans_score += scores[qid]\n\n    return (\n        100.0 * best_score / len(scores),\n        best_thresh,\n        1.0 * has_ans_score / has_ans_cnt,\n    )\n\n\ndef find_all_best_thresh_v2(\n    main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans\n):\n    best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(\n        preds, exact_raw, na_probs, qid_to_has_ans\n    )\n    best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(\n        preds, f1_raw, na_probs, qid_to_has_ans\n    )\n    main_eval[""best_exact""] = best_exact\n    main_eval[""best_exact_thresh""] = exact_thresh\n    main_eval[""best_f1""] = best_f1\n    main_eval[""best_f1_thresh""] = f1_thresh\n    main_eval[""has_ans_exact""] = has_ans_exact\n    main_eval[""has_ans_f1""] = has_ans_f1\n\n\ndef find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n    cur_score = num_no_ans\n    best_score = cur_score\n    best_thresh = 0.0\n    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n    for _, qid in enumerate(qid_list):\n        if qid not in scores:\n            continue\n        if qid_to_has_ans[qid]:\n            diff = scores[qid]\n        else:\n            if preds[qid]:\n                diff = -1\n            else:\n                diff = 0\n        cur_score += diff\n        if cur_score > best_score:\n            best_score = cur_score\n            best_thresh = na_probs[qid]\n    return 100.0 * best_score / len(scores), best_thresh\n\n\ndef find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n    best_exact, exact_thresh = find_best_thresh(\n        preds, exact_raw, na_probs, qid_to_has_ans\n    )\n    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n\n    main_eval[""best_exact""] = best_exact\n    main_eval[""best_exact_thresh""] = exact_thresh\n    main_eval[""best_f1""] = best_f1\n    main_eval[""best_f1_thresh""] = f1_thresh\n\n\ndef squad_evaluate(\n    examples, preds, no_answer_probs=None, no_answer_probability_threshold=1.0\n):\n    qas_id_to_has_answer = {\n        example.qas_id: bool(example.answers) for example in examples\n    }\n    has_answer_qids = [\n        qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if has_answer\n    ]\n    no_answer_qids = [\n        qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer\n    ]\n\n    if no_answer_probs is None:\n        no_answer_probs = {k: 0.0 for k in preds}\n\n    exact, f1 = get_raw_scores(examples, preds)\n\n    exact_threshold = apply_no_ans_threshold(\n        exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold\n    )\n    f1_threshold = apply_no_ans_threshold(\n        f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold\n    )\n\n    evaluation = make_eval_dict(exact_threshold, f1_threshold)\n\n    if has_answer_qids:\n        has_ans_eval = make_eval_dict(\n            exact_threshold, f1_threshold, qid_list=has_answer_qids\n        )\n        merge_eval(evaluation, has_ans_eval, ""HasAns"")\n\n    if no_answer_qids:\n        no_ans_eval = make_eval_dict(\n            exact_threshold, f1_threshold, qid_list=no_answer_qids\n        )\n        merge_eval(evaluation, no_ans_eval, ""NoAns"")\n\n    if no_answer_probs:\n        find_all_best_thresh(\n            evaluation, preds, exact, f1, no_answer_probs, qas_id_to_has_answer\n        )\n\n    return evaluation\n\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    """"""Project the tokenized prediction back to the original text.""""""\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don\'t want in\n    # our prediction.\n    #\n    # For example, let\'s say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith\'s\n    #\n    # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n    #\n    # We don\'t want to return `pred_text` because it\'s already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is ""Steve Smith"".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heuristic between\n    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == "" "":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = """".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        if verbose_logging:\n            logger.info(""Unable to find text: \'%s\' in \'%s\'"" % (pred_text, orig_text))\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        if verbose_logging:\n            logger.info(\n                ""Length not equal after stripping spaces: \'%s\' vs \'%s\'"",\n                orig_ns_text,\n                tok_ns_text,\n            )\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map start position"")\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map end position"")\n        return orig_text\n\n    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n    return output_text\n\n\ndef _get_best_indexes(logits, n_best_size):\n    """"""Get the n-best logits from a list.""""""\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\n\ndef _compute_softmax(scores):\n    """"""Compute softmax probability over raw logits.""""""\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n\n\ndef compute_predictions_logits(\n    all_examples,\n    all_features,\n    all_results,\n    n_best_size,\n    max_answer_length,\n    do_lower_case,\n    verbose_logging,\n    version_2_with_negative,\n    null_score_diff_threshold,\n    tokenizer,\n    output_prediction_file=None,\n    output_nbest_file=None,\n    output_null_log_odds_file=None,\n):\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""],\n    )\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n        min_null_feature_index = 0  # the paragraph slice with min null score\n        null_start_logit = 0  # the start logit at the slice with min null score\n        null_end_logit = 0  # the end logit at the slice with min null score\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n            # if we could have irrelevant answers, get the min score of irrelevant\n            if version_2_with_negative:\n                feature_null_score = result.start_logits[0] + result.end_logits[0]\n                if feature_null_score < score_null:\n                    score_null = feature_null_score\n                    min_null_feature_index = feature_index\n                    null_start_logit = result.start_logits[0]\n                    null_end_logit = result.end_logits[0]\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index],\n                        )\n                    )\n        if version_2_with_negative:\n            prelim_predictions.append(\n                _PrelimPrediction(\n                    feature_index=min_null_feature_index,\n                    start_index=0,\n                    end_index=0,\n                    start_logit=null_start_logit,\n                    end_logit=null_end_logit,\n                )\n            )\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_logit + x.end_logit),\n            reverse=True,\n        )\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"",\n            [""text"", ""start_logit"", ""end_logit"", ""start_index"", ""end_index""],\n        )\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n            if pred.start_index > 0:  # this is a non-null prediction\n                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n\n                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n                # tok_text = "" "".join(tok_tokens)\n                #\n                # # De-tokenize WordPieces that have been split off.\n                # tok_text = tok_text.replace("" ##"", """")\n                # tok_text = tok_text.replace(""##"", """")\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = "" "".join(tok_text.split())\n                orig_text = "" "".join(orig_tokens)\n\n                final_text = get_final_text(\n                    tok_text, orig_text, do_lower_case, verbose_logging\n                )\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n            else:\n                final_text = """"\n                seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_logit=pred.start_logit,\n                    end_logit=pred.end_logit,\n                    start_index=orig_doc_start,\n                    end_index=orig_doc_end,\n                )\n            )\n        # if we didn\'t include the empty option in the n-best, include it\n        if version_2_with_negative:\n            if """" not in seen_predictions:\n                nbest.append(\n                    _NbestPrediction(\n                        text="""",\n                        start_logit=null_start_logit,\n                        end_logit=null_end_logit,\n                        start_index=0,\n                        end_index=0,\n                    )\n                )\n\n            # In very rare edge cases we could only have single null prediction.\n            # So we just create a nonce prediction in this case to avoid failure.\n            if len(nbest) == 1:\n                nbest.insert(\n                    0,\n                    _NbestPrediction(\n                        text=""empty"",\n                        start_logit=0.0,\n                        end_logit=0.0,\n                        start_index=0,\n                        end_index=0,\n                    ),\n                )\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(\n                    text=""empty"",\n                    start_logit=0.0,\n                    end_logit=0.0,\n                    start_index=0,\n                    end_index=0,\n                )\n            )\n\n        assert len(nbest) >= 1\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_logit + entry.end_logit)\n            if not best_non_null_entry:\n                if entry.text:\n                    best_non_null_entry = entry\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_logit""] = entry.start_logit\n            output[""end_logit""] = entry.end_logit\n            output[""start_index""] = entry.start_index\n            output[""end_index""] = entry.end_index\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n\n        if not version_2_with_negative:\n            all_predictions[example.qas_id] = nbest_json[0][""text""]\n        else:\n            # predict """" iff the null score - the score of best non-null > threshold\n            score_diff = (\n                score_null\n                - best_non_null_entry.start_logit\n                - (best_non_null_entry.end_logit)\n            )\n            scores_diff_json[example.qas_id] = score_diff\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example.qas_id] = """"\n            else:\n                all_predictions[example.qas_id] = best_non_null_entry.text\n        all_nbest_json[example.qas_id] = nbest_json\n\n    if output_prediction_file:\n        with open(output_prediction_file, ""w"") as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n\n    if output_nbest_file:\n        with open(output_nbest_file, ""w"") as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n    if output_null_log_odds_file and version_2_with_negative:\n        with open(output_null_log_odds_file, ""w"") as writer:\n            writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    return all_predictions, all_nbest_json\n\n\ndef compute_predictions_log_probs(\n    all_examples,\n    all_features,\n    all_results,\n    n_best_size,\n    max_answer_length,\n    start_n_top,\n    end_n_top,\n    version_2_with_negative,\n    tokenizer,\n    verbose_logging,\n    output_prediction_file=None,\n    output_nbest_file=None,\n    output_null_log_odds_file=None,\n):\n    """""" XLNet write prediction logic (more complex than Bert\'s).\n        Write final predictions to the json file and log-odds of null if needed.\n        Requires utils_squad_evaluate.py\n    """"""\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_log_prob"", ""end_log_prob""],\n    )\n\n    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""NbestPrediction"", [""text"", ""start_log_prob"", ""end_log_prob""]\n    )\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n\n            cur_null_score = result.cls_logits\n\n            # if we could have irrelevant answers, get the min score of irrelevant\n            score_null = min(score_null, cur_null_score)\n\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_log_prob = result.start_logits[i]\n                    start_index = result.start_top_index[i]\n\n                    j_index = i * end_n_top + j\n\n                    end_log_prob = result.end_logits[j_index]\n                    end_index = result.end_top_index[j_index]\n\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= feature.paragraph_len - 1:\n                        continue\n                    if end_index >= feature.paragraph_len - 1:\n                        continue\n\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_log_prob=start_log_prob,\n                            end_log_prob=end_log_prob,\n                        )\n                    )\n\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_log_prob + x.end_log_prob),\n            reverse=True,\n        )\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n\n            # XLNet un-tokenizer\n            # Let\'s keep it simple for now and see if we need all this later.\n            #\n            # tok_start_to_orig_index = feature.tok_start_to_orig_index\n            # tok_end_to_orig_index = feature.tok_end_to_orig_index\n            # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n            # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n            # paragraph_text = example.paragraph_text\n            # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n\n            # Previously used Bert untokenizer\n            tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n            orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n            # Clean whitespace\n            tok_text = tok_text.strip()\n            tok_text = "" "".join(tok_text.split())\n            orig_text = "" "".join(orig_tokens)\n\n            if hasattr(tokenizer, ""do_lower_case""):\n                do_lower_case = tokenizer.do_lower_case\n            else:\n                do_lower_case = tokenizer.do_lowercase_and_remove_accent\n\n            final_text = get_final_text(\n                tok_text, orig_text, do_lower_case, verbose_logging\n            )\n\n            if final_text in seen_predictions:\n                continue\n\n            seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_log_prob=pred.start_log_prob,\n                    end_log_prob=pred.end_log_prob,\n                )\n            )\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(text="""", start_log_prob=-1e6, end_log_prob=-1e6)\n            )\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_log_prob + entry.end_log_prob)\n            if not best_non_null_entry:\n                best_non_null_entry = entry\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_log_prob""] = entry.start_log_prob\n            output[""end_log_prob""] = entry.end_log_prob\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n        assert best_non_null_entry is not None\n\n        score_diff = score_null\n        scores_diff_json[example.qas_id] = score_diff\n        # note(zhiliny): always predict best_non_null_entry\n        # and the evaluation script will search for the best threshold\n        all_predictions[example.qas_id] = best_non_null_entry.text\n\n        all_nbest_json[example.qas_id] = nbest_json\n\n    if output_prediction_file:\n        with open(output_prediction_file, ""w"") as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n\n    if output_nbest_file:\n        with open(output_nbest_file, ""w"") as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n    if version_2_with_negative:\n        with open(output_null_log_odds_file, ""w"") as writer:\n            writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    return all_predictions, all_nbest_json\n'"
adaptnlp/transformers/utils_squad.py,0,"b'# coding=utf-8\n# Code used/modified by AdaptNLP author from transformers\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Load SQuAD dataset. """"""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport math\nimport collections\nfrom io import open\n\nfrom transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\n\n# Required by XLNet evaluation method to compute optimal threshold (see write_predictions_extended() method)\nfrom adaptnlp.transformers.utils_squad_evaluate import (\n    find_all_best_thresh_v2,\n    make_qid_to_has_ans,\n    get_raw_scores,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass SquadExample(object):\n    """"""\n    A single training/test example for the Squad dataset.\n    For examples without an answer, the start and end position are -1.\n    """"""\n\n    def __init__(\n        self,\n        qas_id,\n        question_text,\n        doc_tokens,\n        orig_answer_text=None,\n        start_position=None,\n        end_position=None,\n        is_impossible=None,\n    ):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = """"\n        s += ""qas_id: %s"" % (self.qas_id)\n        s += "", question_text: %s"" % (self.question_text)\n        s += "", doc_tokens: [%s]"" % ("" "".join(self.doc_tokens))\n        if self.start_position:\n            s += "", start_position: %d"" % (self.start_position)\n        if self.end_position:\n            s += "", end_position: %d"" % (self.end_position)\n        if self.is_impossible:\n            s += "", is_impossible: %r"" % (self.is_impossible)\n        return s\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(\n        self,\n        unique_id,\n        example_index,\n        doc_span_index,\n        tokens,\n        token_to_orig_map,\n        token_is_max_context,\n        input_ids,\n        input_mask,\n        segment_ids,\n        cls_index,\n        p_mask,\n        paragraph_len,\n        start_position=None,\n        end_position=None,\n        is_impossible=None,\n    ):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.cls_index = cls_index\n        self.p_mask = p_mask\n        self.paragraph_len = paragraph_len\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n\ndef read_squad_examples(input_file, is_training, version_2_with_negative):\n    """"""Read a SQuAD json file into a list of SquadExample.""""""\n    with open(input_file, ""r"", encoding=""utf-8"") as reader:\n        input_data = json.load(reader)[""data""]\n\n    def is_whitespace(c):\n        if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n            return True\n        return False\n\n    examples = []\n    for entry in input_data:\n        for paragraph in entry[""paragraphs""]:\n            paragraph_text = paragraph[""context""]\n            doc_tokens = []\n            char_to_word_offset = []\n            prev_is_whitespace = True\n            for c in paragraph_text:\n                if is_whitespace(c):\n                    prev_is_whitespace = True\n                else:\n                    if prev_is_whitespace:\n                        doc_tokens.append(c)\n                    else:\n                        doc_tokens[-1] += c\n                    prev_is_whitespace = False\n                char_to_word_offset.append(len(doc_tokens) - 1)\n\n            for qa in paragraph[""qas""]:\n                qas_id = qa[""id""]\n                question_text = qa[""question""]\n                start_position = None\n                end_position = None\n                orig_answer_text = None\n                is_impossible = False\n                if is_training:\n                    if version_2_with_negative:\n                        is_impossible = qa[""is_impossible""]\n                    if (len(qa[""answers""]) != 1) and (not is_impossible):\n                        raise ValueError(\n                            ""For training, each question should have exactly 1 answer.""\n                        )\n                    if not is_impossible:\n                        answer = qa[""answers""][0]\n                        orig_answer_text = answer[""text""]\n                        answer_offset = answer[""answer_start""]\n                        answer_length = len(orig_answer_text)\n                        start_position = char_to_word_offset[answer_offset]\n                        end_position = char_to_word_offset[\n                            answer_offset + answer_length - 1\n                        ]\n                        # Only add answers where the text can be exactly recovered from the\n                        # document. If this CAN\'T happen it\'s likely due to weird Unicode\n                        # stuff so we will just skip the example.\n                        #\n                        # Note that this means for training mode, every example is NOT\n                        # guaranteed to be preserved.\n                        actual_text = "" "".join(\n                            doc_tokens[start_position : (end_position + 1)]\n                        )\n                        cleaned_answer_text = "" "".join(\n                            whitespace_tokenize(orig_answer_text)\n                        )\n                        if actual_text.find(cleaned_answer_text) == -1:\n                            logger.warning(\n                                ""Could not find answer: \'%s\' vs. \'%s\'"",\n                                actual_text,\n                                cleaned_answer_text,\n                            )\n                            continue\n                    else:\n                        start_position = -1\n                        end_position = -1\n                        orig_answer_text = """"\n\n                example = SquadExample(\n                    qas_id=qas_id,\n                    question_text=question_text,\n                    doc_tokens=doc_tokens,\n                    orig_answer_text=orig_answer_text,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=is_impossible,\n                )\n                examples.append(example)\n    return examples\n\n\ndef convert_examples_to_features(\n    examples,\n    tokenizer,\n    max_seq_length,\n    doc_stride,\n    max_query_length,\n    is_training,\n    cls_token_at_end=False,\n    cls_token=""[CLS]"",\n    sep_token=""[SEP]"",\n    pad_token=0,\n    sequence_a_segment_id=0,\n    sequence_b_segment_id=1,\n    cls_token_segment_id=0,\n    pad_token_segment_id=0,\n    mask_padding_with_zero=True,\n):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    unique_id = 1000000000\n    # cnt_pos, cnt_neg = 0, 0\n    # max_N, max_M = 1024, 1024\n    # f = np.zeros((max_N, max_M), dtype=np.float32)\n\n    features = []\n    for (example_index, example) in enumerate(examples):\n\n        # if example_index % 100 == 0:\n        #     logger.info(\'Converting %s/%s pos %s neg %s\', example_index, len(examples), cnt_pos, cnt_neg)\n\n        query_tokens = tokenizer.tokenize(example.question_text)\n\n        if len(query_tokens) > max_query_length:\n            query_tokens = query_tokens[0:max_query_length]\n\n        tok_to_orig_index = []\n        orig_to_tok_index = []\n        all_doc_tokens = []\n        for (i, token) in enumerate(example.doc_tokens):\n            orig_to_tok_index.append(len(all_doc_tokens))\n            sub_tokens = tokenizer.tokenize(token)\n            for sub_token in sub_tokens:\n                tok_to_orig_index.append(i)\n                all_doc_tokens.append(sub_token)\n\n        tok_start_position = None\n        tok_end_position = None\n        if is_training and example.is_impossible:\n            tok_start_position = -1\n            tok_end_position = -1\n        if is_training and not example.is_impossible:\n            tok_start_position = orig_to_tok_index[example.start_position]\n            if example.end_position < len(example.doc_tokens) - 1:\n                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n            else:\n                tok_end_position = len(all_doc_tokens) - 1\n            (tok_start_position, tok_end_position) = _improve_answer_span(\n                all_doc_tokens,\n                tok_start_position,\n                tok_end_position,\n                tokenizer,\n                example.orig_answer_text,\n            )\n\n        # The -3 accounts for [CLS], [SEP] and [SEP]\n        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n        # We can have documents that are longer than the maximum sequence length.\n        # To deal with this we do a sliding window approach, where we take chunks\n        # of the up to our max length with a stride of `doc_stride`.\n        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n            ""DocSpan"", [""start"", ""length""]\n        )\n        doc_spans = []\n        start_offset = 0\n        while start_offset < len(all_doc_tokens):\n            length = len(all_doc_tokens) - start_offset\n            if length > max_tokens_for_doc:\n                length = max_tokens_for_doc\n            doc_spans.append(_DocSpan(start=start_offset, length=length))\n            if start_offset + length == len(all_doc_tokens):\n                break\n            start_offset += min(length, doc_stride)\n\n        for (doc_span_index, doc_span) in enumerate(doc_spans):\n            tokens = []\n            token_to_orig_map = {}\n            token_is_max_context = {}\n            segment_ids = []\n\n            # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n            # Original TF implem also keep the classification token (set to 0) (not sure why...)\n            p_mask = []\n\n            # CLS token at the beginning\n            if not cls_token_at_end:\n                tokens.append(cls_token)\n                segment_ids.append(cls_token_segment_id)\n                p_mask.append(0)\n                cls_index = 0\n\n            # Query\n            for token in query_tokens:\n                tokens.append(token)\n                segment_ids.append(sequence_a_segment_id)\n                p_mask.append(1)\n\n            # SEP token\n            tokens.append(sep_token)\n            segment_ids.append(sequence_a_segment_id)\n            p_mask.append(1)\n\n            # Paragraph\n            for i in range(doc_span.length):\n                split_token_index = doc_span.start + i\n                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n                is_max_context = _check_is_max_context(\n                    doc_spans, doc_span_index, split_token_index\n                )\n                token_is_max_context[len(tokens)] = is_max_context\n                tokens.append(all_doc_tokens[split_token_index])\n                segment_ids.append(sequence_b_segment_id)\n                p_mask.append(0)\n            paragraph_len = doc_span.length\n\n            # SEP token\n            tokens.append(sep_token)\n            segment_ids.append(sequence_b_segment_id)\n            p_mask.append(1)\n\n            # CLS token at the end\n            if cls_token_at_end:\n                tokens.append(cls_token)\n                segment_ids.append(cls_token_segment_id)\n                p_mask.append(0)\n                cls_index = len(tokens) - 1  # Index of classification token\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n            # tokens are attended to.\n            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            while len(input_ids) < max_seq_length:\n                input_ids.append(pad_token)\n                input_mask.append(0 if mask_padding_with_zero else 1)\n                segment_ids.append(pad_token_segment_id)\n                p_mask.append(1)\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n\n            span_is_impossible = example.is_impossible\n            start_position = None\n            end_position = None\n            if is_training and not span_is_impossible:\n                # For training, if our document chunk does not contain an annotation\n                # we throw it out, since there is nothing to predict.\n                doc_start = doc_span.start\n                doc_end = doc_span.start + doc_span.length - 1\n                out_of_span = False\n                if not (\n                    tok_start_position >= doc_start and tok_end_position <= doc_end\n                ):\n                    out_of_span = True\n                if out_of_span:\n                    start_position = 0\n                    end_position = 0\n                    span_is_impossible = True\n                else:\n                    doc_offset = len(query_tokens) + 2\n                    start_position = tok_start_position - doc_start + doc_offset\n                    end_position = tok_end_position - doc_start + doc_offset\n\n            if is_training and span_is_impossible:\n                start_position = cls_index\n                end_position = cls_index\n\n            if example_index < 20:\n                logger.info(""*** Example ***"")\n                logger.info(""unique_id: %s"" % (unique_id))\n                logger.info(""example_index: %s"" % (example_index))\n                logger.info(""doc_span_index: %s"" % (doc_span_index))\n                logger.info(""tokens: %s"" % "" "".join(tokens))\n                logger.info(\n                    ""token_to_orig_map: %s""\n                    % "" "".join(\n                        [""%d:%d"" % (x, y) for (x, y) in token_to_orig_map.items()]\n                    )\n                )\n                logger.info(\n                    ""token_is_max_context: %s""\n                    % "" "".join(\n                        [""%d:%s"" % (x, y) for (x, y) in token_is_max_context.items()]\n                    )\n                )\n                logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n                logger.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n                logger.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n                if is_training and span_is_impossible:\n                    logger.info(""impossible example"")\n                if is_training and not span_is_impossible:\n                    answer_text = "" "".join(tokens[start_position : (end_position + 1)])\n                    logger.info(""start_position: %d"" % (start_position))\n                    logger.info(""end_position: %d"" % (end_position))\n                    logger.info(""answer: %s"" % (answer_text))\n\n            features.append(\n                InputFeatures(\n                    unique_id=unique_id,\n                    example_index=example_index,\n                    doc_span_index=doc_span_index,\n                    tokens=tokens,\n                    token_to_orig_map=token_to_orig_map,\n                    token_is_max_context=token_is_max_context,\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    segment_ids=segment_ids,\n                    cls_index=cls_index,\n                    p_mask=p_mask,\n                    paragraph_len=paragraph_len,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=span_is_impossible,\n                )\n            )\n            unique_id += 1\n\n    return features\n\n\ndef _improve_answer_span(\n    doc_tokens, input_start, input_end, tokenizer, orig_answer_text\n):\n    """"""Returns tokenized answer spans that better match the annotated answer.""""""\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a ""better match"". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be ""(1895-1943)."". However\n    # after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose ""Japan"" as a character sub-span of\n    # the word ""Japanese"". Since our WordPiece tokenizer does not split\n    # ""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = "" "".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n    """"""Check if this is the \'max context\' doc span for the token.""""""\n\n    # Because of the sliding window approach taken to scoring documents, a single\n    # token can appear in multiple documents. E.g.\n    #  Doc: the man went to the store and bought a gallon of milk\n    #  Span A: the man went to the\n    #  Span B: to the store and bought\n    #  Span C: and bought a gallon of\n    #  ...\n    #\n    # Now the word \'bought\' will have two scores from spans B and C. We only\n    # want to consider the score with ""maximum context"", which we define as\n    # the *minimum* of its left and right context (the *sum* of left and\n    # right context will always be the same, of course).\n    #\n    # In the example the maximum context for \'bought\' would be span C since\n    # it has 1 left context and 3 right context, while span B has 4 left context\n    # and 0 right context.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n\nRawResult = collections.namedtuple(\n    ""RawResult"", [""unique_id"", ""start_logits"", ""end_logits""]\n)\n\n\ndef write_predictions(\n    all_examples,\n    all_features,\n    all_results,\n    n_best_size,\n    max_answer_length,\n    do_lower_case,\n    output_prediction_file,\n    output_nbest_file,\n    output_null_log_odds_file,\n    verbose_logging,\n    version_2_with_negative,\n    null_score_diff_threshold,\n):\n    """"""Write final predictions to the json file and log-odds of null if needed.""""""\n    logger.info(""Writing predictions to: %s"" % (output_prediction_file))\n    logger.info(""Writing nbest to: %s"" % (output_nbest_file))\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""],\n    )\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n        min_null_feature_index = 0  # the paragraph slice with min null score\n        null_start_logit = 0  # the start logit at the slice with min null score\n        null_end_logit = 0  # the end logit at the slice with min null score\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n            # if we could have irrelevant answers, get the min score of irrelevant\n            if version_2_with_negative:\n                feature_null_score = result.start_logits[0] + result.end_logits[0]\n                if feature_null_score < score_null:\n                    score_null = feature_null_score\n                    min_null_feature_index = feature_index\n                    null_start_logit = result.start_logits[0]\n                    null_end_logit = result.end_logits[0]\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index],\n                        )\n                    )\n        if version_2_with_negative:\n            prelim_predictions.append(\n                _PrelimPrediction(\n                    feature_index=min_null_feature_index,\n                    start_index=0,\n                    end_index=0,\n                    start_logit=null_start_logit,\n                    end_logit=null_end_logit,\n                )\n            )\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_logit + x.end_logit),\n            reverse=True,\n        )\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"", [""text"", ""start_logit"", ""end_logit""]\n        )\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n            if pred.start_index > 0:  # this is a non-null prediction\n                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n                tok_text = "" "".join(tok_tokens)\n\n                # De-tokenize WordPieces that have been split off.\n                tok_text = tok_text.replace("" ##"", """")\n                tok_text = tok_text.replace(""##"", """")\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = "" "".join(tok_text.split())\n                orig_text = "" "".join(orig_tokens)\n\n                final_text = get_final_text(\n                    tok_text, orig_text, do_lower_case, verbose_logging\n                )\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n            else:\n                final_text = """"\n                seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_logit=pred.start_logit,\n                    end_logit=pred.end_logit,\n                )\n            )\n        # if we didn\'t include the empty option in the n-best, include it\n        if version_2_with_negative:\n            if """" not in seen_predictions:\n                nbest.append(\n                    _NbestPrediction(\n                        text="""", start_logit=null_start_logit, end_logit=null_end_logit\n                    )\n                )\n\n            # In very rare edge cases we could only have single null prediction.\n            # So we just create a nonce prediction in this case to avoid failure.\n            if len(nbest) == 1:\n                nbest.insert(\n                    0, _NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0)\n                )\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(_NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0))\n\n        assert len(nbest) >= 1\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_logit + entry.end_logit)\n            if not best_non_null_entry:\n                if entry.text:\n                    best_non_null_entry = entry\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_logit""] = entry.start_logit\n            output[""end_logit""] = entry.end_logit\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n\n        if not version_2_with_negative:\n            all_predictions[example.qas_id] = nbest_json[0][""text""]\n        else:\n            # predict """" iff the null score - the score of best non-null > threshold\n            score_diff = (\n                score_null\n                - best_non_null_entry.start_logit\n                - (best_non_null_entry.end_logit)\n            )\n            scores_diff_json[example.qas_id] = score_diff\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example.qas_id] = """"\n            else:\n                all_predictions[example.qas_id] = best_non_null_entry.text\n        all_nbest_json[example.qas_id] = nbest_json\n\n    with open(output_prediction_file, ""w"") as writer:\n        writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n\n    with open(output_nbest_file, ""w"") as writer:\n        writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n    if version_2_with_negative:\n        with open(output_null_log_odds_file, ""w"") as writer:\n            writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    return all_predictions\n\n\n# For XLNet (and XLM which uses the same head)\nRawResultExtended = collections.namedtuple(\n    ""RawResultExtended"",\n    [\n        ""unique_id"",\n        ""start_top_log_probs"",\n        ""start_top_index"",\n        ""end_top_log_probs"",\n        ""end_top_index"",\n        ""cls_logits"",\n    ],\n)\n\n\ndef write_predictions_extended(\n    all_examples,\n    all_features,\n    all_results,\n    n_best_size,\n    max_answer_length,\n    output_prediction_file,\n    output_nbest_file,\n    output_null_log_odds_file,\n    orig_data_file,\n    start_n_top,\n    end_n_top,\n    version_2_with_negative,\n    tokenizer,\n    verbose_logging,\n):\n    """""" XLNet write prediction logic (more complex than Bert\'s).\n        Write final predictions to the json file and log-odds of null if needed.\n        Requires utils_squad_evaluate.py\n    """"""\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_log_prob"", ""end_log_prob""],\n    )\n\n    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""NbestPrediction"", [""text"", ""start_log_prob"", ""end_log_prob""]\n    )\n\n    logger.info(""Writing predictions to: %s"", output_prediction_file)\n    # logger.info(""Writing nbest to: %s"" % (output_nbest_file))\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n\n            cur_null_score = result.cls_logits\n\n            # if we could have irrelevant answers, get the min score of irrelevant\n            score_null = min(score_null, cur_null_score)\n\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_log_prob = result.start_top_log_probs[i]\n                    start_index = result.start_top_index[i]\n\n                    j_index = i * end_n_top + j\n\n                    end_log_prob = result.end_top_log_probs[j_index]\n                    end_index = result.end_top_index[j_index]\n\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= feature.paragraph_len - 1:\n                        continue\n                    if end_index >= feature.paragraph_len - 1:\n                        continue\n\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_log_prob=start_log_prob,\n                            end_log_prob=end_log_prob,\n                        )\n                    )\n\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_log_prob + x.end_log_prob),\n            reverse=True,\n        )\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n\n            # XLNet un-tokenizer\n            # Let\'s keep it simple for now and see if we need all this later.\n            #\n            # tok_start_to_orig_index = feature.tok_start_to_orig_index\n            # tok_end_to_orig_index = feature.tok_end_to_orig_index\n            # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n            # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n            # paragraph_text = example.paragraph_text\n            # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n\n            # Previously used Bert untokenizer\n            tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n            orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n            # Clean whitespace\n            tok_text = tok_text.strip()\n            tok_text = "" "".join(tok_text.split())\n            orig_text = "" "".join(orig_tokens)\n\n            final_text = get_final_text(\n                tok_text, orig_text, tokenizer.do_lower_case, verbose_logging\n            )\n\n            if final_text in seen_predictions:\n                continue\n\n            seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_log_prob=pred.start_log_prob,\n                    end_log_prob=pred.end_log_prob,\n                )\n            )\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(text="""", start_log_prob=-1e6, end_log_prob=-1e6)\n            )\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_log_prob + entry.end_log_prob)\n            if not best_non_null_entry:\n                best_non_null_entry = entry\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_log_prob""] = entry.start_log_prob\n            output[""end_log_prob""] = entry.end_log_prob\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n        assert best_non_null_entry is not None\n\n        score_diff = score_null\n        scores_diff_json[example.qas_id] = score_diff\n        # note(zhiliny): always predict best_non_null_entry\n        # and the evaluation script will search for the best threshold\n        all_predictions[example.qas_id] = best_non_null_entry.text\n\n        all_nbest_json[example.qas_id] = nbest_json\n\n    with open(output_prediction_file, ""w"") as writer:\n        writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n\n    with open(output_nbest_file, ""w"") as writer:\n        writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n    if version_2_with_negative:\n        with open(output_null_log_odds_file, ""w"") as writer:\n            writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    with open(orig_data_file, ""r"", encoding=""utf-8"") as reader:\n        orig_data = json.load(reader)[""data""]\n\n    qid_to_has_ans = make_qid_to_has_ans(orig_data)\n    # has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n    # no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n    exact_raw, f1_raw = get_raw_scores(orig_data, all_predictions)\n    out_eval = {}\n\n    find_all_best_thresh_v2(\n        out_eval, all_predictions, exact_raw, f1_raw, scores_diff_json, qid_to_has_ans\n    )\n\n    return out_eval\n\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    """"""Project the tokenized prediction back to the original text.""""""\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don\'t want in\n    # our prediction.\n    #\n    # For example, let\'s say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith\'s\n    #\n    # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n    #\n    # We don\'t want to return `pred_text` because it\'s already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is ""Steve Smith"".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heuristic between\n    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == "" "":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = """".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        if verbose_logging:\n            logger.info(""Unable to find text: \'%s\' in \'%s\'"" % (pred_text, orig_text))\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        if verbose_logging:\n            logger.info(\n                ""Length not equal after stripping spaces: \'%s\' vs \'%s\'"",\n                orig_ns_text,\n                tok_ns_text,\n            )\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map start position"")\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map end position"")\n        return orig_text\n\n    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n    return output_text\n\n\ndef _get_best_indexes(logits, n_best_size):\n    """"""Get the n-best logits from a list.""""""\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\n\ndef _compute_softmax(scores):\n    """"""Compute softmax probability over raw logits.""""""\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n'"
adaptnlp/transformers/utils_squad_evaluate.py,0,"b'# Code used/modified by AdaptNLP author from pytorch-transformers library\n#\n# Official evaluation script for SQuAD version 2.0.\n# Modified by XLNet authors to update `find_best_threshold` scripts for SQuAD V2.0\n# In addition to basic functionality, we also compute additional statistics and\n# plot precision-recall curves if an additional na_prob.json file is provided.\n# This file is expected to map question ID\'s to the model\'s predicted probability\n# that a question is unanswerable.\n\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\n\nclass EVAL_OPTS:\n    def __init__(\n        self,\n        data_file,\n        pred_file,\n        out_file="""",\n        na_prob_file=""na_prob.json"",\n        na_prob_thresh=1.0,\n        out_image_dir=None,\n        verbose=False,\n    ):\n        self.data_file = data_file\n        self.pred_file = pred_file\n        self.out_file = out_file\n        self.na_prob_file = na_prob_file\n        self.na_prob_thresh = na_prob_thresh\n        self.out_image_dir = out_image_dir\n        self.verbose = verbose\n\n\nOPTS = None\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        ""Official evaluation script for SQuAD version 2.0.""\n    )\n    parser.add_argument(""data_file"", metavar=""data.json"", help=""Input data JSON file."")\n    parser.add_argument(""pred_file"", metavar=""pred.json"", help=""Model predictions."")\n    parser.add_argument(\n        ""--out-file"",\n        ""-o"",\n        metavar=""eval.json"",\n        help=""Write accuracy metrics to file (default is stdout)."",\n    )\n    parser.add_argument(\n        ""--na-prob-file"",\n        ""-n"",\n        metavar=""na_prob.json"",\n        help=""Model estimates of probability of no answer."",\n    )\n    parser.add_argument(\n        ""--na-prob-thresh"",\n        ""-t"",\n        type=float,\n        default=1.0,\n        help=\'Predict """" if no-answer probability exceeds this (default = 1.0).\',\n    )\n    parser.add_argument(\n        ""--out-image-dir"",\n        ""-p"",\n        metavar=""out_images"",\n        default=None,\n        help=""Save precision-recall curves to directory."",\n    )\n    parser.add_argument(""--verbose"", ""-v"", action=""store_true"")\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()\n\n\ndef make_qid_to_has_ans(dataset):\n    qid_to_has_ans = {}\n    for article in dataset:\n        for p in article[""paragraphs""]:\n            for qa in p[""qas""]:\n                qid_to_has_ans[qa[""id""]] = bool(qa[""answers""])\n    return qid_to_has_ans\n\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n\n    def remove_articles(text):\n        regex = re.compile(r""\\b(a|an|the)\\b"", re.UNICODE)\n        return re.sub(regex, "" "", text)\n\n    def white_space_fix(text):\n        return "" "".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return """".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article[""paragraphs""]:\n            for qa in p[""qas""]:\n                qid = qa[""id""]\n                gold_answers = [\n                    a[""text""] for a in qa[""answers""] if normalize_answer(a[""text""])\n                ]\n                if not gold_answers:\n                    # For unanswerable questions, only correct answer is empty string\n                    gold_answers = [""""]\n                if qid not in preds:\n                    print(""Missing prediction for %s"" % qid)\n                    continue\n                a_pred = preds[qid]\n                # Take max over all gold answers\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\n\ndef apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n    new_scores = {}\n    for qid, s in scores.items():\n        pred_na = na_probs[qid] > na_prob_thresh\n        if pred_na:\n            new_scores[qid] = float(not qid_to_has_ans[qid])\n        else:\n            new_scores[qid] = s\n    return new_scores\n\n\ndef make_eval_dict(exact_scores, f1_scores, qid_list=None):\n    if not qid_list:\n        total = len(exact_scores)\n        return collections.OrderedDict(\n            [\n                (""exact"", 100.0 * sum(exact_scores.values()) / total),\n                (""f1"", 100.0 * sum(f1_scores.values()) / total),\n                (""total"", total),\n            ]\n        )\n    else:\n        total = len(qid_list)\n        return collections.OrderedDict(\n            [\n                (""exact"", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n                (""f1"", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n                (""total"", total),\n            ]\n        )\n\n\ndef merge_eval(main_eval, new_eval, prefix):\n    for k in new_eval:\n        main_eval[""%s_%s"" % (prefix, k)] = new_eval[k]\n\n\ndef plot_pr_curve(precisions, recalls, out_image, title):\n    plt.step(recalls, precisions, color=""b"", alpha=0.2, where=""post"")\n    plt.fill_between(recalls, precisions, step=""post"", alpha=0.2, color=""b"")\n    plt.xlabel(""Recall"")\n    plt.ylabel(""Precision"")\n    plt.xlim([0.0, 1.05])\n    plt.ylim([0.0, 1.05])\n    plt.title(title)\n    plt.savefig(out_image)\n    plt.clf()\n\n\ndef make_precision_recall_eval(\n    scores, na_probs, num_true_pos, qid_to_has_ans, out_image=None, title=None\n):\n    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n    true_pos = 0.0\n    cur_p = 1.0\n    cur_r = 0.0\n    precisions = [1.0]\n    recalls = [0.0]\n    avg_prec = 0.0\n    for i, qid in enumerate(qid_list):\n        if qid_to_has_ans[qid]:\n            true_pos += scores[qid]\n        cur_p = true_pos / float(i + 1)\n        cur_r = true_pos / float(num_true_pos)\n        if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i + 1]]:\n            # i.e., if we can put a threshold after this point\n            avg_prec += cur_p * (cur_r - recalls[-1])\n            precisions.append(cur_p)\n            recalls.append(cur_r)\n    if out_image:\n        plot_pr_curve(precisions, recalls, out_image, title)\n    return {""ap"": 100.0 * avg_prec}\n\n\ndef run_precision_recall_analysis(\n    main_eval, exact_raw, f1_raw, na_probs, qid_to_has_ans, out_image_dir\n):\n    if out_image_dir and not os.path.exists(out_image_dir):\n        os.makedirs(out_image_dir)\n    num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n    if num_true_pos == 0:\n        return\n    pr_exact = make_precision_recall_eval(\n        exact_raw,\n        na_probs,\n        num_true_pos,\n        qid_to_has_ans,\n        out_image=os.path.join(out_image_dir, ""pr_exact.png""),\n        title=""Precision-Recall curve for Exact Match score"",\n    )\n    pr_f1 = make_precision_recall_eval(\n        f1_raw,\n        na_probs,\n        num_true_pos,\n        qid_to_has_ans,\n        out_image=os.path.join(out_image_dir, ""pr_f1.png""),\n        title=""Precision-Recall curve for F1 score"",\n    )\n    oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n    pr_oracle = make_precision_recall_eval(\n        oracle_scores,\n        na_probs,\n        num_true_pos,\n        qid_to_has_ans,\n        out_image=os.path.join(out_image_dir, ""pr_oracle.png""),\n        title=""Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)"",\n    )\n    merge_eval(main_eval, pr_exact, ""pr_exact"")\n    merge_eval(main_eval, pr_f1, ""pr_f1"")\n    merge_eval(main_eval, pr_oracle, ""pr_oracle"")\n\n\ndef histogram_na_prob(na_probs, qid_list, image_dir, name):\n    if not qid_list:\n        return\n    x = [na_probs[k] for k in qid_list]\n    weights = np.ones_like(x) / float(len(x))\n    plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n    plt.xlabel(""Model probability of no-answer"")\n    plt.ylabel(""Proportion of dataset"")\n    plt.title(""Histogram of no-answer probability: %s"" % name)\n    plt.savefig(os.path.join(image_dir, ""na_prob_hist_%s.png"" % name))\n    plt.clf()\n\n\ndef find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n    cur_score = num_no_ans\n    best_score = cur_score\n    best_thresh = 0.0\n    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n    for i, qid in enumerate(qid_list):\n        if qid not in scores:\n            continue\n        if qid_to_has_ans[qid]:\n            diff = scores[qid]\n        else:\n            if preds[qid]:\n                diff = -1\n            else:\n                diff = 0\n        cur_score += diff\n        if cur_score > best_score:\n            best_score = cur_score\n            best_thresh = na_probs[qid]\n    return 100.0 * best_score / len(scores), best_thresh\n\n\ndef find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n    cur_score = num_no_ans\n    best_score = cur_score\n    best_thresh = 0.0\n    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n    for i, qid in enumerate(qid_list):\n        if qid not in scores:\n            continue\n        if qid_to_has_ans[qid]:\n            diff = scores[qid]\n        else:\n            if preds[qid]:\n                diff = -1\n            else:\n                diff = 0\n        cur_score += diff\n        if cur_score > best_score:\n            best_score = cur_score\n            best_thresh = na_probs[qid]\n\n    has_ans_score, has_ans_cnt = 0, 0\n    for qid in qid_list:\n        if not qid_to_has_ans[qid]:\n            continue\n        has_ans_cnt += 1\n\n        if qid not in scores:\n            continue\n        has_ans_score += scores[qid]\n\n    return (\n        100.0 * best_score / len(scores),\n        best_thresh,\n        1.0 * has_ans_score / has_ans_cnt,\n    )\n\n\ndef find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n    best_exact, exact_thresh = find_best_thresh(\n        preds, exact_raw, na_probs, qid_to_has_ans\n    )\n    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n    main_eval[""best_exact""] = best_exact\n    main_eval[""best_exact_thresh""] = exact_thresh\n    main_eval[""best_f1""] = best_f1\n    main_eval[""best_f1_thresh""] = f1_thresh\n\n\ndef find_all_best_thresh_v2(\n    main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans\n):\n    best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(\n        preds, exact_raw, na_probs, qid_to_has_ans\n    )\n    best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(\n        preds, f1_raw, na_probs, qid_to_has_ans\n    )\n    main_eval[""best_exact""] = best_exact\n    main_eval[""best_exact_thresh""] = exact_thresh\n    main_eval[""best_f1""] = best_f1\n    main_eval[""best_f1_thresh""] = f1_thresh\n    main_eval[""has_ans_exact""] = has_ans_exact\n    main_eval[""has_ans_f1""] = has_ans_f1\n\n\ndef main(OPTS):\n    with open(OPTS.data_file) as f:\n        dataset_json = json.load(f)\n        dataset = dataset_json[""data""]\n    with open(OPTS.pred_file) as f:\n        preds = json.load(f)\n    if OPTS.na_prob_file:\n        with open(OPTS.na_prob_file) as f:\n            na_probs = json.load(f)\n    else:\n        na_probs = {k: 0.0 for k in preds}\n    qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n    exact_raw, f1_raw = get_raw_scores(dataset, preds)\n    exact_thresh = apply_no_ans_threshold(\n        exact_raw, na_probs, qid_to_has_ans, OPTS.na_prob_thresh\n    )\n    f1_thresh = apply_no_ans_threshold(\n        f1_raw, na_probs, qid_to_has_ans, OPTS.na_prob_thresh\n    )\n    out_eval = make_eval_dict(exact_thresh, f1_thresh)\n    if has_ans_qids:\n        has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n        merge_eval(out_eval, has_ans_eval, ""HasAns"")\n    if no_ans_qids:\n        no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n        merge_eval(out_eval, no_ans_eval, ""NoAns"")\n    if OPTS.na_prob_file:\n        find_all_best_thresh(\n            out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans\n        )\n    if OPTS.na_prob_file and OPTS.out_image_dir:\n        run_precision_recall_analysis(\n            out_eval, exact_raw, f1_raw, na_probs, qid_to_has_ans, OPTS.out_image_dir\n        )\n        histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, ""hasAns"")\n        histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, ""noAns"")\n    if OPTS.out_file:\n        with open(OPTS.out_file, ""w"") as f:\n            json.dump(out_eval, f)\n    else:\n        print(json.dumps(out_eval, indent=2))\n    return out_eval\n\n\nif __name__ == ""__main__"":\n    OPTS = parse_args()\n    if OPTS.out_image_dir:\n        import matplotlib\n\n        matplotlib.use(""Agg"")\n        import matplotlib.pyplot as plt\n    main(OPTS)\n'"
rest/app/__init__.py,0,b''
rest/app/data_models.py,0,b'from typing import List\n\nfrom pydantic import BaseModel\n\n\n# General Data Models\nclass Labels(BaseModel):\n    value: str\n    confidence: float\n\n\nclass Entities(BaseModel):\n    text: str\n    start_pos: int\n    end_pos: int\n    type: str\n    confidence: float\n\n\nclass QASpanLabel(BaseModel):\n    text: str\n    probability: float\n    start_logit: float\n    end_logit: float\n    start_index: int\n    end_index: int\n\n\n# Token Tagging Data Model\nclass TokenTaggingRequest(BaseModel):\n    text: str\n\n\nclass TokenTaggingResponse(BaseModel):\n    text: str\n    labels: List[Labels] = []\n    entities: List[Entities] = []\n\n\n# Sequence Classification\nclass SequenceClassificationRequest(BaseModel):\n    text: str\n\n\nclass SequenceClassificationResponse(BaseModel):\n    text: str\n    labels: List[Labels] = []\n    entities: List[Entities] = []\n\n\n# Question Answering\nclass QuestionAnsweringRequest(BaseModel):\n    query: str\n    context: str\n    top_n: int = 10\n\n\nclass QuestionAnsweringResponse(BaseModel):\n    best_answer: str\n    best_n_answers: List[QASpanLabel]\n'
rest/app/main.py,0,"b'import os\nimport logging\nfrom typing import List\n\nimport adaptnlp\n\nimport uvicorn\nfrom fastapi import FastAPI\n\nfrom .data_models import (\n    TokenTaggingRequest,\n    TokenTaggingResponse,\n    SequenceClassificationRequest,\n    SequenceClassificationResponse,\n    QuestionAnsweringRequest,\n    QuestionAnsweringResponse,\n)\n\napp = FastAPI()\n\n#####################\n### Initialization###\n#####################\n\n# Initialize Logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=""INFO"",\n    format=""%(process)d-%(levelname)s-%(message)s"",\n    datefmt=""%Y-%m-%d %H:%M:%S"",\n)\n\n# Global Modules\n_TOKEN_TAGGER = adaptnlp.EasyTokenTagger()\n_SEQUENCE_CLASSIFIER = adaptnlp.EasySequenceClassifier()\n_QA_MODEL = adaptnlp.EasyQuestionAnswering()\n\n# Get Model Configurations From ENV VARS\n_TOKEN_TAGGING_MODE = os.environ[""TOKEN_TAGGING_MODE""]\n_TOKEN_TAGGING_MODEL = os.environ[""TOKEN_TAGGING_MODEL""]\n_SEQUENCE_CLASSIFICATION_MODEL = os.environ[""SEQUENCE_CLASSIFICATION_MODEL""]\n_QUESTION_ANSWERING_MODEL = os.environ[""QUESTION_ANSWERING_MODEL""]\n\n\n# Event Handling\n@app.on_event(""startup"")\nasync def initialize_nlp_task_modules():\n    _TOKEN_TAGGER.tag_text(text="""", model_name_or_path=_TOKEN_TAGGING_MODEL)\n    _SEQUENCE_CLASSIFIER.tag_text(\n        text="""", mini_batch_size=1, model_name_or_path=_SEQUENCE_CLASSIFICATION_MODEL\n    )\n    _QA_MODEL.predict_qa(\n        query=""-"",\n        context=""______________________________________________________________________________"",\n        n_best_size=1,\n        mini_batch_size=1,\n        model_name_or_path=_QUESTION_ANSWERING_MODEL,\n    )\n\n\n######################\n### AdaptNLP API ###\n######################\n@app.get(""/"")\nasync def root():\n    return {""message"": ""Welcome to AdaptNLP""}\n\n\n@app.post(""/api/token_tagger"", response_model=List[TokenTaggingResponse])\nasync def token_tagger(token_tagging_request: TokenTaggingRequest):\n    text = token_tagging_request.text\n    sentences = _TOKEN_TAGGER.tag_text(\n        text=text, model_name_or_path=_TOKEN_TAGGING_MODEL\n    )\n    payload = [sentence.to_dict(tag_type=_TOKEN_TAGGING_MODE) for sentence in sentences]\n    return payload\n\n\n@app.post(\n    ""/api/sequence-classifier"", response_model=List[SequenceClassificationResponse]\n)\nasync def sequence_classifier(\n    sequence_classification_request: SequenceClassificationRequest,\n):\n    text = sequence_classification_request.text\n    sentences = _SEQUENCE_CLASSIFIER.tag_text(\n        text=text, mini_batch_size=1, model_name_or_path=_SEQUENCE_CLASSIFICATION_MODEL\n    )\n    payload = [sentence.to_dict() for sentence in sentences]\n    return payload\n\n\n@app.post(""/api/question-answering"", response_model=QuestionAnsweringResponse)\nasync def question_answering(qa_request: QuestionAnsweringRequest):\n    query = qa_request.query\n    context = qa_request.context\n    top_n = qa_request.top_n\n    best_answer, best_n_answers = _QA_MODEL.predict_qa(\n        query=query,\n        context=context,\n        n_best_size=top_n,\n        mini_batch_size=1,\n        model_name_or_path=_QUESTION_ANSWERING_MODEL,\n    )\n    payload = QuestionAnsweringResponse(\n        best_answer=best_answer, best_n_answers=best_n_answers\n    )\n    return payload\n\n\nif __name__ == ""__main__"":\n    uvicorn.run(app, host=""0.0.0.0"", port=5000)\n'"
