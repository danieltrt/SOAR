file_path,api_count,code
image_augmentation.py,0,"b""import cv2\nimport numpy\n\nfrom umeyama import umeyama\n\nrandom_transform_args = {\n    'rotation_range': 10,\n    'zoom_range': 0.05,\n    'shift_range': 0.05,\n    'random_flip': 0.4,\n}\n\n\ndef random_transform(image, rotation_range, zoom_range, shift_range, random_flip):\n    h, w = image.shape[0:2]\n    rotation = numpy.random.uniform(-rotation_range, rotation_range)\n    scale = numpy.random.uniform(1 - zoom_range, 1 + zoom_range)\n    tx = numpy.random.uniform(-shift_range, shift_range) * w\n    ty = numpy.random.uniform(-shift_range, shift_range) * h\n    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n    mat[:, 2] += (tx, ty)\n    result = cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)\n    if numpy.random.random() < random_flip:\n        result = result[:, ::-1]\n    return result\n\n\n# get pair of random warped images from aligened face image\ndef random_warp(image):\n    assert image.shape == (256, 256, 3)\n    range_ = numpy.linspace(128 - 80, 128 + 80, 5)\n    mapx = numpy.broadcast_to(range_, (5, 5))\n    mapy = mapx.T\n\n    mapx = mapx + numpy.random.normal(size=(5, 5), scale=5)\n    mapy = mapy + numpy.random.normal(size=(5, 5), scale=5)\n\n    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')\n    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')\n\n    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n\n    src_points = numpy.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n    dst_points = numpy.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)\n    mat = umeyama(src_points, dst_points, True)[0:2]\n\n    target_image = cv2.warpAffine(image, mat, (64, 64))\n\n    return warped_image, target_image\n"""
models.py,2,"b""import torch\nimport torch.utils.data\nfrom torch import nn, optim\nfrom padding_same_conv import Conv2d\n\n\ndef toTensor(img):\n    img = torch.from_numpy(img.transpose((0, 3, 1, 2)))\n    return img\n\n\ndef var_to_np(img_var):\n    return img_var.data.cpu().numpy()\n\n\nclass _ConvLayer(nn.Sequential):\n    def __init__(self, input_features, output_features):\n        super(_ConvLayer, self).__init__()\n        self.add_module('conv2', Conv2d(input_features, output_features,\n                                        kernel_size=5, stride=2))\n        self.add_module('leakyrelu', nn.LeakyReLU(0.1, inplace=True))\n\n\nclass _UpScale(nn.Sequential):\n    def __init__(self, input_features, output_features):\n        super(_UpScale, self).__init__()\n        self.add_module('conv2_', Conv2d(input_features, output_features * 4,\n                                         kernel_size=3))\n        self.add_module('leakyrelu', nn.LeakyReLU(0.1, inplace=True))\n        self.add_module('pixelshuffler', _PixelShuffler())\n\n\nclass Flatten(nn.Module):\n\n    def forward(self, input):\n        output = input.view(input.size(0), -1)\n        return output\n\n\nclass Reshape(nn.Module):\n\n    def forward(self, input):\n        output = input.view(-1, 1024, 4, 4)  # channel * 4 * 4\n\n        return output\n\n\nclass _PixelShuffler(nn.Module):\n    def forward(self, input):\n        batch_size, c, h, w = input.size()\n        rh, rw = (2, 2)\n        oh, ow = h * rh, w * rw\n        oc = c // (rh * rw)\n        out = input.view(batch_size, rh, rw, oc, h, w)\n        out = out.permute(0, 3, 4, 1, 5, 2).contiguous()\n        out = out.view(batch_size, oc, oh, ow)  # channel first\n\n        return out\n\n\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n\n        self.encoder = nn.Sequential(\n            _ConvLayer(3, 128),\n            _ConvLayer(128, 256),\n            _ConvLayer(256, 512),\n            _ConvLayer(512, 1024),\n            Flatten(),\n            nn.Linear(1024 * 4 * 4, 1024),\n            nn.Linear(1024, 1024 * 4 * 4),\n            Reshape(),\n            _UpScale(1024, 512),\n        )\n\n        self.decoder_A = nn.Sequential(\n            _UpScale(512, 256),\n            _UpScale(256, 128),\n            _UpScale(128, 64),\n            Conv2d(64, 3, kernel_size=5, padding=1),\n            nn.Sigmoid(),\n        )\n\n        self.decoder_B = nn.Sequential(\n            _UpScale(512, 256),\n            _UpScale(256, 128),\n            _UpScale(128, 64),\n            Conv2d(64, 3, kernel_size=5, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, select='A'):\n        if select == 'A':\n            out = self.encoder(x)\n            out = self.decoder_A(out)\n        else:\n            out = self.encoder(x)\n            out = self.decoder_B(out)\n        return out\n"""
padding_same_conv.py,9,"b'# modify con2d function to use same padding\n# code referd to @famssa in \'https://github.com/pytorch/pytorch/issues/3867\'\n# and tensorflow source code\n\nimport torch.utils.data\nfrom torch.nn import functional as F\n\nimport math\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.functional import pad\nfrom torch.nn.modules import Module\nfrom torch.nn.modules.utils import _single, _pair, _triple\n\n\nclass _ConvNd(Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, transposed, output_padding, groups, bias):\n        super(_ConvNd, self).__init__()\n        if in_channels % groups != 0:\n            raise ValueError(\'in_channels must be divisible by groups\')\n        if out_channels % groups != 0:\n            raise ValueError(\'out_channels must be divisible by groups\')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.transposed = transposed\n        self.output_padding = output_padding\n        self.groups = groups\n        if transposed:\n            self.weight = Parameter(torch.Tensor(\n                in_channels, out_channels // groups, *kernel_size))\n        else:\n            self.weight = Parameter(torch.Tensor(\n                out_channels, in_channels // groups, *kernel_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def __repr__(self):\n        s = (\'{name}({in_channels}, {out_channels}, kernel_size={kernel_size}\'\n             \', stride={stride}\')\n        if self.padding != (0,) * len(self.padding):\n            s += \', padding={padding}\'\n        if self.dilation != (1,) * len(self.dilation):\n            s += \', dilation={dilation}\'\n        if self.output_padding != (0,) * len(self.output_padding):\n            s += \', output_padding={output_padding}\'\n        if self.groups != 1:\n            s += \', groups={groups}\'\n        if self.bias is None:\n            s += \', bias=False\'\n        s += \')\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass Conv2d(_ConvNd):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(Conv2d, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n\n    def forward(self, input):\n        return conv2d_same_padding(input, self.weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\n# custom con2d, because pytorch don\'t have ""padding=\'same\'"" option.\ndef conv2d_same_padding(input, weight, bias=None, stride=1, padding=1, dilation=1, groups=1):\n\n    input_rows = input.size(2)\n    filter_rows = weight.size(2)\n    effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n    out_rows = (input_rows + stride[0] - 1) // stride[0]\n    padding_needed = max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\n                  input_rows)\n    padding_rows = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    rows_odd = (padding_rows % 2 != 0)\n    padding_cols = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    cols_odd = (padding_rows % 2 != 0)\n\n    if rows_odd or cols_odd:\n        input = pad(input, [0, int(cols_odd), 0, int(rows_odd)])\n\n    return F.conv2d(input, weight, bias, stride,\n                  padding=(padding_rows // 2, padding_cols // 2),\n                  dilation=dilation, groups=groups)\n\n'"
train.py,9,"b'# author:oldpan\n# data:2018-4-16\n# Just for study and research\n\nfrom __future__ import print_function\nimport argparse\nimport os\n\nimport cv2\nimport numpy as np\nimport torch\n\nimport torch.utils.data\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nimport torch.backends.cudnn as cudnn\n\nfrom models import Autoencoder, toTensor, var_to_np\nfrom util import get_image_paths, load_images, stack_images\nfrom training_data import get_training_data\n\nparser = argparse.ArgumentParser(description=\'DeepFake-Pytorch\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--epochs\', type=int, default=100000, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10000)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif args.cuda is True:\n    print(\'===> Using GPU to train\')\n    device = torch.device(\'cuda:0\')\n    cudnn.benchmark = True\nelse:\n    print(\'===> Using CPU to train\')\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nprint(\'===> Loaing datasets\')\nimages_A = get_image_paths(""data/trump"")\nimages_B = get_image_paths(""data/cage"")\nimages_A = load_images(images_A) / 255.0\nimages_B = load_images(images_B) / 255.0\nimages_A += images_B.mean(axis=(0, 1, 2)) - images_A.mean(axis=(0, 1, 2))\n\nmodel = Autoencoder().to(device)\n\nprint(\'===> Try resume from checkpoint\')\nif os.path.isdir(\'checkpoint\'):\n    try:\n        checkpoint = torch.load(\'./checkpoint/autoencoder.t7\')\n        model.load_state_dict(checkpoint[\'state\'])\n        start_epoch = checkpoint[\'epoch\']\n        print(\'===> Load last checkpoint data\')\n    except FileNotFoundError:\n        print(\'Can\\\'t found autoencoder.t7\')\nelse:\n    start_epoch = 0\n    print(\'===> Start from scratch\')\n\n\ncriterion = nn.L1Loss()\noptimizer_1 = optim.Adam([{\'params\': model.encoder.parameters()},\n                          {\'params\': model.decoder_A.parameters()}]\n                         , lr=5e-5, betas=(0.5, 0.999))\noptimizer_2 = optim.Adam([{\'params\': model.encoder.parameters()},\n                          {\'params\': model.decoder_B.parameters()}]\n                         , lr=5e-5, betas=(0.5, 0.999))\n\n# print all the parameters im model\n# s = sum([np.prod(list(p.size())) for p in model.parameters()])\n# print(\'Number of params: %d\' % s)\n\nif __name__ == ""__main__"":\n\n    print(\'Start training, press \\\'q\\\' to stop\')\n\n    for epoch in range(start_epoch, args.epochs):\n        batch_size = args.batch_size\n\n        warped_A, target_A = get_training_data(images_A, batch_size)\n        warped_B, target_B = get_training_data(images_B, batch_size)\n\n        warped_A, target_A = toTensor(warped_A), toTensor(target_A)\n        warped_B, target_B = toTensor(warped_B), toTensor(target_B)\n\n        if args.cuda:\n            warped_A = warped_A.to(device).float()\n            target_A = target_A.to(device).float()\n            warped_B = warped_B.to(device).float()\n            target_B = target_B.to(device).float()\n\n        optimizer_1.zero_grad()\n        optimizer_2.zero_grad()\n\n        warped_A = model(warped_A, \'A\')\n        warped_B = model(warped_B, \'B\')\n\n        loss1 = criterion(warped_A, target_A)\n        loss2 = criterion(warped_B, target_B)\n        loss = loss1.item() + loss2.item()\n        loss1.backward()\n        loss2.backward()\n        optimizer_1.step()\n        optimizer_2.step()\n        print(\'epoch: {}, lossA:{}, lossB:{}\'.format(epoch, loss1.item(), loss2.item()))\n\n        if epoch % args.log_interval == 0:\n\n            test_A_ = target_A[0:14]\n            test_B_ = target_B[0:14]\n            test_A = var_to_np(target_A[0:14])\n            test_B = var_to_np(target_B[0:14])\n            print(\'===> Saving models...\')\n            state = {\n                \'state\': model.state_dict(),\n                \'epoch\': epoch\n            }\n            if not os.path.isdir(\'checkpoint\'):\n                os.mkdir(\'checkpoint\')\n            torch.save(state, \'./checkpoint/autoencoder.t7\')\n\n        figure_A = np.stack([\n            test_A,\n            var_to_np(model(test_A_, \'A\')),\n            var_to_np(model(test_A_, \'B\')),\n        ], axis=1)\n        figure_B = np.stack([\n            test_B,\n            var_to_np(model(test_B_, \'B\')),\n            var_to_np(model(test_B_, \'A\')),\n        ], axis=1)\n\n        figure = np.concatenate([figure_A, figure_B], axis=0)\n        figure = figure.transpose((0, 1, 3, 4, 2))\n        figure = figure.reshape((4, 7) + figure.shape[1:])\n        figure = stack_images(figure)\n\n        figure = np.clip(figure * 255, 0, 255).astype(\'uint8\')\n\n        cv2.imshow("""", figure)\n        key = cv2.waitKey(1)\n        if key == ord(\'q\'):\n            exit()\n'"
training_data.py,0,"b""import numpy\nfrom image_augmentation import random_transform\nfrom image_augmentation import random_warp\n\nrandom_transform_args = {\n    'rotation_range': 10,\n    'zoom_range': 0.05,\n    'shift_range': 0.05,\n    'random_flip': 0.4,\n}\n\n\ndef get_training_data(images, batch_size):\n    indices = numpy.random.randint(len(images), size=batch_size)\n    for i, index in enumerate(indices):\n        image = images[index]\n        image = random_transform(image, **random_transform_args)\n        warped_img, target_img = random_warp(image)\n\n        if i == 0:\n            warped_images = numpy.empty((batch_size,) + warped_img.shape, warped_img.dtype)\n            target_images = numpy.empty((batch_size,) + target_img.shape, warped_img.dtype)\n\n        warped_images[i] = warped_img\n        target_images[i] = target_img\n\n    return warped_images, target_images\n"""
umeyama.py,0,"b'# # License (Modified BSD) # Copyright (C) 2011, the scikit-image team All rights reserved. # # Redistribution and\n# use in source and binary forms, with or without modification, are permitted provided that the following conditions\n# are met: # # Redistributions of source code must retain the above copyright notice, this list of conditions and the\n#  following disclaimer. # Redistributions in binary form must reproduce the above copyright notice, this list of\n# conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n#  Neither the name of skimage nor the names of its contributors may be used to endorse or promote products derived\n# from this software without specific prior written permission. # THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS\'\'\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n#  FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE\n# GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# umeyama function from scikit-image/skimage/transform/_geometric.py\n\nimport numpy as np\n\n\ndef umeyama(src, dst, estimate_scale):\n    """"""Estimate N-D similarity transformation with or without scaling.\n    Parameters\n    ----------\n    src : (M, N) array\n        Source coordinates.\n    dst : (M, N) array\n        Destination coordinates.\n    estimate_scale : bool\n        Whether to estimate scaling factor.\n    Returns\n    -------\n    T : (N + 1, N + 1)\n        The homogeneous similarity transformation matrix. The matrix contains\n        NaN values only if the problem is not well-conditioned.\n    References\n    ----------\n    .. [1] ""Least-squares estimation of transformation parameters between two\n            point patterns"", Shinji Umeyama, PAMI 1991, DOI: 10.1109/34.88573\n    """"""\n\n    num = src.shape[0]\n    dim = src.shape[1]\n\n    # Compute mean of src and dst.\n    src_mean = src.mean(axis=0)\n    dst_mean = dst.mean(axis=0)\n\n    # Subtract mean from src and dst.\n    src_demean = src - src_mean\n    dst_demean = dst - dst_mean\n\n    # Eq. (38).\n    A = np.dot(dst_demean.T, src_demean) / num\n\n    # Eq. (39).\n    d = np.ones((dim,), dtype=np.double)\n    if np.linalg.det(A) < 0:\n        d[dim - 1] = -1\n\n    T = np.eye(dim + 1, dtype=np.double)\n\n    U, S, V = np.linalg.svd(A)\n\n    # Eq. (40) and (43).\n    rank = np.linalg.matrix_rank(A)\n    if rank == 0:\n        return np.nan * T\n    elif rank == dim - 1:\n        if np.linalg.det(U) * np.linalg.det(V) > 0:\n            T[:dim, :dim] = np.dot(U, V)\n        else:\n            s = d[dim - 1]\n            d[dim - 1] = -1\n            T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V))\n            d[dim - 1] = s\n    else:\n        T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V.T))\n\n    if estimate_scale:\n        # Eq. (41) and (42).\n        scale = 1.0 / src_demean.var(axis=0).sum() * np.dot(S, d)\n    else:\n        scale = 1.0\n\n    T[:dim, dim] = dst_mean - scale * np.dot(T[:dim, :dim], src_mean.T)\n    T[:dim, :dim] *= scale\n\n    return T\n'"
util.py,0,"b'import cv2\nimport numpy\nimport os\n\n\ndef get_image_paths(directory):\n    return [x.path for x in os.scandir(directory) if x.name.endswith("".jpg"") or x.name.endswith("".png"")\n            or x.name.endswith("".JPG"")]\n\n\ndef load_images(image_paths, convert=None):\n    iter_all_images = (cv2.resize(cv2.imread(fn), (256, 256)) for fn in image_paths)\n    if convert:\n        iter_all_images = (convert(img) for img in iter_all_images)\n    for i, image in enumerate(iter_all_images):\n        if i == 0:\n            all_images = numpy.empty((len(image_paths),) + image.shape, dtype=image.dtype)\n        all_images[i] = image\n    return all_images\n\n\ndef get_transpose_axes(n):\n    if n % 2 == 0:\n        y_axes = list(range(1, n - 1, 2))\n        x_axes = list(range(0, n - 1, 2))\n    else:\n        y_axes = list(range(0, n - 1, 2))\n        x_axes = list(range(1, n - 1, 2))\n    return y_axes, x_axes, [n - 1]\n\n\ndef stack_images(images):\n    images_shape = numpy.array(images.shape)\n    new_axes = get_transpose_axes(len(images_shape))\n    new_shape = [numpy.prod(images_shape[x]) for x in new_axes]\n    return numpy.transpose(\n        images,\n        axes=numpy.concatenate(new_axes)\n    ).reshape(new_shape)\n\n'"
