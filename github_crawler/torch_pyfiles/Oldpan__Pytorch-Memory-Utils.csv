file_path,api_count,code
gpu_mem_track.py,1,"b'import gc\nimport datetime\nimport pynvml\n\nimport torch\nimport numpy as np\n\n\nclass MemTracker(object):\n    """"""\n    Class used to track pytorch memory usage\n    Arguments:\n        frame: a frame to detect current py-file runtime\n        detail(bool, default True): whether the function shows the detail gpu memory usage\n        path(str): where to save log file\n        verbose(bool, default False): whether show the trivial exception\n        device(int): GPU number, default is 0\n    """"""\n    def __init__(self, frame, detail=True, path=\'\', verbose=False, device=0):\n        self.frame = frame\n        self.print_detail = detail\n        self.last_tensor_sizes = set()\n        self.gpu_profile_fn = path + f\'{datetime.datetime.now():%d-%b-%y-%H:%M:%S}-gpu_mem_track.txt\'\n        self.verbose = verbose\n        self.begin = True\n        self.device = device\n\n        self.func_name = frame.f_code.co_name\n        self.filename = frame.f_globals[""__file__""]\n        if (self.filename.endswith("".pyc"") or\n                self.filename.endswith("".pyo"")):\n            self.filename = self.filename[:-1]\n        self.module_name = self.frame.f_globals[""__name__""]\n        self.curr_line = self.frame.f_lineno\n\n    def get_tensors(self):\n        for obj in gc.get_objects():\n            try:\n                if torch.is_tensor(obj) or (hasattr(obj, \'data\') and torch.is_tensor(obj.data)):\n                    tensor = obj\n                else:\n                    continue\n                if tensor.is_cuda:\n                    yield tensor\n            except Exception as e:\n                if self.verbose:\n                    print(\'A trivial exception occured: {}\'.format(e))\n\n    def track(self):\n        """"""\n        Track the GPU memory usage\n        """"""\n        pynvml.nvmlInit()\n        handle = pynvml.nvmlDeviceGetHandleByIndex(self.device)\n        meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n        self.curr_line = self.frame.f_lineno\n        where_str = self.module_name + \' \' + self.func_name + \':\' + \' line \' + str(self.curr_line)\n\n        with open(self.gpu_profile_fn, \'a+\') as f:\n\n            if self.begin:\n                f.write(f""GPU Memory Track | {datetime.datetime.now():%d-%b-%y-%H:%M:%S} |""\n                        f"" Total Used Memory:{meminfo.used/1000**2:<7.1f}Mb\\n\\n"")\n                self.begin = False\n\n            if self.print_detail is True:\n                ts_list = [tensor.size() for tensor in self.get_tensors()]\n                new_tensor_sizes = {(type(x), tuple(x.size()), ts_list.count(x.size()), np.prod(np.array(x.size()))*4/1000**2)\n                                    for x in self.get_tensors()}\n                for t, s, n, m in new_tensor_sizes - self.last_tensor_sizes:\n                    f.write(f\'+ | {str(n)} * Size:{str(s):<20} | Memory: {str(m*n)[:6]} M | {str(t):<20}\\n\')\n                for t, s, n, m in self.last_tensor_sizes - new_tensor_sizes:\n                    f.write(f\'- | {str(n)} * Size:{str(s):<20} | Memory: {str(m*n)[:6]} M | {str(t):<20} \\n\')\n                self.last_tensor_sizes = new_tensor_sizes\n\n            f.write(f""\\nAt {where_str:<50}""\n                    f""Total Used Memory:{meminfo.used/1000**2:<7.1f}Mb\\n\\n"")\n\n        pynvml.nvmlShutdown()\n\n'"
modelsize_estimate.py,1,"b""import torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef modelsize(model, input, type_size=4):\n    para = sum([np.prod(list(p.size())) for p in model.parameters()])\n    # print('Model {} : Number of params: {}'.format(model._get_name(), para))\n    print('Model {} : params: {:4f}M'.format(model._get_name(), para * type_size / 1000 / 1000))\n\n    input_ = input.clone()\n    input_.requires_grad_(requires_grad=False)\n\n    mods = list(model.modules())\n    out_sizes = []\n\n    for i in range(1, len(mods)):\n        m = mods[i]\n        if isinstance(m, nn.ReLU):\n            if m.inplace:\n                continue\n        out = m(input_)\n        out_sizes.append(np.array(out.size()))\n        input_ = out\n\n    total_nums = 0\n    for i in range(len(out_sizes)):\n        s = out_sizes[i]\n        nums = np.prod(np.array(s))\n        total_nums += nums\n\n    # print('Model {} : Number of intermedite variables without backward: {}'.format(model._get_name(), total_nums))\n    # print('Model {} : Number of intermedite variables with backward: {}'.format(model._get_name(), total_nums*2))\n    print('Model {} : intermedite variables: {:3f} M (without backward)'\n          .format(model._get_name(), total_nums * type_size / 1000 / 1000))\n    print('Model {} : intermedite variables: {:3f} M (with backward)'\n          .format(model._get_name(), total_nums * type_size*2 / 1000 / 1000))\n\n"""
