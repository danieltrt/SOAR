file_path,api_count,code
data_loader.py,1,"b""import os\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\n\nclass KittiLoader(Dataset):\n    def __init__(self, root_dir, mode, transform=None):\n        left_dir = os.path.join(root_dir, 'image_02/data/')\n        self.left_paths = sorted([os.path.join(left_dir, fname) for fname\\\n                           in os.listdir(left_dir)])\n        if mode == 'train':\n            right_dir = os.path.join(root_dir, 'image_03/data/')\n            self.right_paths = sorted([os.path.join(right_dir, fname) for fname\\\n                                in os.listdir(right_dir)])\n            assert len(self.right_paths) == len(self.left_paths)\n        self.transform = transform\n        self.mode = mode\n\n\n    def __len__(self):\n        return len(self.left_paths)\n\n    def __getitem__(self, idx):\n        left_image = Image.open(self.left_paths[idx])\n        if self.mode == 'train':\n            right_image = Image.open(self.right_paths[idx])\n            sample = {'left_image': left_image, 'right_image': right_image}\n\n            if self.transform:\n                sample = self.transform(sample)\n                return sample\n            else:\n                return sample\n        else:\n            if self.transform:\n                left_image = self.transform(left_image)\n            return left_image\n"""
loss.py,17,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MonodepthLoss(nn.modules.Module):\n    def __init__(self, n=4, SSIM_w=0.85, disp_gradient_w=1.0, lr_w=1.0):\n        super(MonodepthLoss, self).__init__()\n        self.SSIM_w = SSIM_w\n        self.disp_gradient_w = disp_gradient_w\n        self.lr_w = lr_w\n        self.n = n\n\n    def scale_pyramid(self, img, num_scales):\n        scaled_imgs = [img]\n        s = img.size()\n        h = s[2]\n        w = s[3]\n        for i in range(num_scales - 1):\n            ratio = 2 ** (i + 1)\n            nh = h // ratio\n            nw = w // ratio\n            scaled_imgs.append(nn.functional.interpolate(img,\n                               size=[nh, nw], mode=\'bilinear\',\n                               align_corners=True))\n        return scaled_imgs\n\n    def gradient_x(self, img):\n        # Pad input to keep output size consistent\n        img = F.pad(img, (0, 1, 0, 0), mode=""replicate"")\n        gx = img[:, :, :, :-1] - img[:, :, :, 1:]  # NCHW\n        return gx\n\n    def gradient_y(self, img):\n        # Pad input to keep output size consistent\n        img = F.pad(img, (0, 0, 0, 1), mode=""replicate"")\n        gy = img[:, :, :-1, :] - img[:, :, 1:, :]  # NCHW\n        return gy\n\n    def apply_disparity(self, img, disp):\n        batch_size, _, height, width = img.size()\n\n        # Original coordinates of pixels\n        x_base = torch.linspace(0, 1, width).repeat(batch_size,\n                    height, 1).type_as(img)\n        y_base = torch.linspace(0, 1, height).repeat(batch_size,\n                    width, 1).transpose(1, 2).type_as(img)\n\n        # Apply shift in X direction\n        x_shifts = disp[:, 0, :, :]  # Disparity is passed in NCHW format with 1 channel\n        flow_field = torch.stack((x_base + x_shifts, y_base), dim=3)\n        # In grid_sample coordinates are assumed to be between -1 and 1\n        output = F.grid_sample(img, 2*flow_field - 1, mode=\'bilinear\',\n                               padding_mode=\'zeros\')\n\n        return output\n\n    def generate_image_left(self, img, disp):\n        return self.apply_disparity(img, -disp)\n\n    def generate_image_right(self, img, disp):\n        return self.apply_disparity(img, disp)\n\n    def SSIM(self, x, y):\n        C1 = 0.01 ** 2\n        C2 = 0.03 ** 2\n\n        mu_x = nn.AvgPool2d(3, 1)(x)\n        mu_y = nn.AvgPool2d(3, 1)(y)\n        mu_x_mu_y = mu_x * mu_y\n        mu_x_sq = mu_x.pow(2)\n        mu_y_sq = mu_y.pow(2)\n\n        sigma_x = nn.AvgPool2d(3, 1)(x * x) - mu_x_sq\n        sigma_y = nn.AvgPool2d(3, 1)(y * y) - mu_y_sq\n        sigma_xy = nn.AvgPool2d(3, 1)(x * y) - mu_x_mu_y\n\n        SSIM_n = (2 * mu_x_mu_y + C1) * (2 * sigma_xy + C2)\n        SSIM_d = (mu_x_sq + mu_y_sq + C1) * (sigma_x + sigma_y + C2)\n        SSIM = SSIM_n / SSIM_d\n\n        return torch.clamp((1 - SSIM) / 2, 0, 1)\n\n    def disp_smoothness(self, disp, pyramid):\n        disp_gradients_x = [self.gradient_x(d) for d in disp]\n        disp_gradients_y = [self.gradient_y(d) for d in disp]\n\n        image_gradients_x = [self.gradient_x(img) for img in pyramid]\n        image_gradients_y = [self.gradient_y(img) for img in pyramid]\n\n        weights_x = [torch.exp(-torch.mean(torch.abs(g), 1,\n                     keepdim=True)) for g in image_gradients_x]\n        weights_y = [torch.exp(-torch.mean(torch.abs(g), 1,\n                     keepdim=True)) for g in image_gradients_y]\n\n        smoothness_x = [disp_gradients_x[i] * weights_x[i]\n                        for i in range(self.n)]\n        smoothness_y = [disp_gradients_y[i] * weights_y[i]\n                        for i in range(self.n)]\n\n        return [torch.abs(smoothness_x[i]) + torch.abs(smoothness_y[i])\n                for i in range(self.n)]\n\n    def forward(self, input, target):\n        """"""\n        Args:\n            input [disp1, disp2, disp3, disp4]\n            target [left, right]\n\n        Return:\n            (float): The loss\n        """"""\n        left, right = target\n        left_pyramid = self.scale_pyramid(left, self.n)\n        right_pyramid = self.scale_pyramid(right, self.n)\n\n        # Prepare disparities\n        disp_left_est = [d[:, 0, :, :].unsqueeze(1) for d in input]\n        disp_right_est = [d[:, 1, :, :].unsqueeze(1) for d in input]\n\n        self.disp_left_est = disp_left_est\n        self.disp_right_est = disp_right_est\n        # Generate images\n        left_est = [self.generate_image_left(right_pyramid[i],\n                    disp_left_est[i]) for i in range(self.n)]\n        right_est = [self.generate_image_right(left_pyramid[i],\n                     disp_right_est[i]) for i in range(self.n)]\n        self.left_est = left_est\n        self.right_est = right_est\n\n        # L-R Consistency\n        right_left_disp = [self.generate_image_left(disp_right_est[i],\n                           disp_left_est[i]) for i in range(self.n)]\n        left_right_disp = [self.generate_image_right(disp_left_est[i],\n                           disp_right_est[i]) for i in range(self.n)]\n\n        # Disparities smoothness\n        disp_left_smoothness = self.disp_smoothness(disp_left_est,\n                                                    left_pyramid)\n        disp_right_smoothness = self.disp_smoothness(disp_right_est,\n                                                     right_pyramid)\n\n        # L1\n        l1_left = [torch.mean(torch.abs(left_est[i] - left_pyramid[i]))\n                   for i in range(self.n)]\n        l1_right = [torch.mean(torch.abs(right_est[i]\n                    - right_pyramid[i])) for i in range(self.n)]\n\n        # SSIM\n        ssim_left = [torch.mean(self.SSIM(left_est[i],\n                     left_pyramid[i])) for i in range(self.n)]\n        ssim_right = [torch.mean(self.SSIM(right_est[i],\n                      right_pyramid[i])) for i in range(self.n)]\n\n        image_loss_left = [self.SSIM_w * ssim_left[i]\n                           + (1 - self.SSIM_w) * l1_left[i]\n                           for i in range(self.n)]\n        image_loss_right = [self.SSIM_w * ssim_right[i]\n                            + (1 - self.SSIM_w) * l1_right[i]\n                            for i in range(self.n)]\n        image_loss = sum(image_loss_left + image_loss_right)\n\n        # L-R Consistency\n        lr_left_loss = [torch.mean(torch.abs(right_left_disp[i]\n                        - disp_left_est[i])) for i in range(self.n)]\n        lr_right_loss = [torch.mean(torch.abs(left_right_disp[i]\n                         - disp_right_est[i])) for i in range(self.n)]\n        lr_loss = sum(lr_left_loss + lr_right_loss)\n\n        # Disparities smoothness\n        disp_left_loss = [torch.mean(torch.abs(\n                          disp_left_smoothness[i])) / 2 ** i\n                          for i in range(self.n)]\n        disp_right_loss = [torch.mean(torch.abs(\n                           disp_right_smoothness[i])) / 2 ** i\n                           for i in range(self.n)]\n        disp_gradient_loss = sum(disp_left_loss + disp_right_loss)\n\n        loss = image_loss + self.disp_gradient_w * disp_gradient_loss\\\n               + self.lr_w * lr_loss\n        self.image_loss = image_loss\n        self.disp_gradient_loss = disp_gradient_loss\n        self.lr_loss = lr_loss\n        return loss\n'"
main_monodepth_pytorch.py,7,"b'import argparse\nimport time\nimport torch\nimport numpy as np\nimport torch.optim as optim\n\n# custom modules\n\nfrom loss import MonodepthLoss\nfrom utils import get_model, to_device, prepare_dataloader\n\n# plot params\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams[\'figure.figsize\'] = (15, 10)\n\n\ndef return_arguments():\n    parser = argparse.ArgumentParser(description=\'PyTorch Monodepth\')\n\n    parser.add_argument(\'data_dir\',\n                        help=\'path to the dataset folder. \\\n                        It should contain subfolders with following structure:\\\n                        ""image_02/data"" for left images and \\\n                        ""image_03/data"" for right images\'\n                        )\n    parser.add_argument(\'val_data_dir\',\n                        help=\'path to the validation dataset folder. \\\n                            It should contain subfolders with following structure:\\\n                            ""image_02/data"" for left images and \\\n                            ""image_03/data"" for right images\'\n                        )\n    parser.add_argument(\'model_path\', help=\'path to the trained model\')\n    parser.add_argument(\'output_directory\',\n                        help=\'where save dispairities\\\n                        for tested images\'\n                        )\n    parser.add_argument(\'--input_height\', type=int, help=\'input height\',\n                        default=256)\n    parser.add_argument(\'--input_width\', type=int, help=\'input width\',\n                        default=512)\n    parser.add_argument(\'--model\', default=\'resnet18_md\',\n                        help=\'encoder architecture: \' +\n                        \'resnet18_md or resnet50_md \' + \'(default: resnet18)\'\n                        + \'or torchvision version of any resnet model\'\n                        )\n    parser.add_argument(\'--pretrained\', default=False,\n                        help=\'Use weights of pretrained model\'\n                        )\n    parser.add_argument(\'--mode\', default=\'train\',\n                        help=\'mode: train or test (default: train)\')\n    parser.add_argument(\'--epochs\', default=50,\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'--learning_rate\', default=1e-4,\n                        help=\'initial learning rate (default: 1e-4)\')\n    parser.add_argument(\'--batch_size\', default=256,\n                        help=\'mini-batch size (default: 256)\')\n    parser.add_argument(\'--adjust_lr\', default=True,\n                        help=\'apply learning rate decay or not\\\n                        (default: True)\'\n                        )\n    parser.add_argument(\'--device\',\n                        default=\'cuda:0\',\n                        help=\'choose cpu or cuda:0 device""\'\n                        )\n    parser.add_argument(\'--do_augmentation\', default=True,\n                        help=\'do augmentation of images or not\')\n    parser.add_argument(\'--augment_parameters\', default=[\n        0.8,\n        1.2,\n        0.5,\n        2.0,\n        0.8,\n        1.2,\n        ],\n            help=\'lowest and highest values for gamma,\\\n                        brightness and color respectively\'\n            )\n    parser.add_argument(\'--print_images\', default=False,\n                        help=\'print disparity and image\\\n                        generated from disparity on every iteration\'\n                        )\n    parser.add_argument(\'--print_weights\', default=False,\n                        help=\'print weights of every layer\')\n    parser.add_argument(\'--input_channels\', default=3,\n                        help=\'Number of channels in input tensor\')\n    parser.add_argument(\'--num_workers\', default=4,\n                        help=\'Number of workers in dataloader\')\n    parser.add_argument(\'--use_multiple_gpu\', default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef adjust_learning_rate(optimizer, epoch, learning_rate):\n    """"""Sets the learning rate to the initial LR\\\n        decayed by 2 every 10 epochs after 30 epoches""""""\n\n    if epoch >= 30 and epoch < 40:\n        lr = learning_rate / 2\n    elif epoch >= 40:\n        lr = learning_rate / 4\n    else:\n        lr = learning_rate\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef post_process_disparity(disp):\n    (_, h, w) = disp.shape\n    l_disp = disp[0, :, :]\n    r_disp = np.fliplr(disp[1, :, :])\n    m_disp = 0.5 * (l_disp + r_disp)\n    (l, _) = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n    l_mask = 1.0 - np.clip(20 * (l - 0.05), 0, 1)\n    r_mask = np.fliplr(l_mask)\n    return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n\n\nclass Model:\n\n    def __init__(self, args):\n        self.args = args\n\n        # Set up model\n        self.device = args.device\n        self.model = get_model(args.model, input_channels=args.input_channels, pretrained=args.pretrained)\n        self.model = self.model.to(self.device)\n        if args.use_multiple_gpu:\n            self.model = torch.nn.DataParallel(self.model)\n\n        if args.mode == \'train\':\n            self.loss_function = MonodepthLoss(\n                n=4,\n                SSIM_w=0.85,\n                disp_gradient_w=0.1, lr_w=1).to(self.device)\n            self.optimizer = optim.Adam(self.model.parameters(),\n                                        lr=args.learning_rate)\n            self.val_n_img, self.val_loader = prepare_dataloader(args.val_data_dir, args.mode,\n                                                                 args.augment_parameters,\n                                                                 False, args.batch_size,\n                                                                 (args.input_height, args.input_width),\n                                                                 args.num_workers)\n        else:\n            self.model.load_state_dict(torch.load(args.model_path))\n            args.augment_parameters = None\n            args.do_augmentation = False\n            args.batch_size = 1\n\n        # Load data\n        self.output_directory = args.output_directory\n        self.input_height = args.input_height\n        self.input_width = args.input_width\n\n        self.n_img, self.loader = prepare_dataloader(args.data_dir, args.mode, args.augment_parameters,\n                                                     args.do_augmentation, args.batch_size,\n                                                     (args.input_height, args.input_width),\n                                                     args.num_workers)\n\n\n        if \'cuda\' in self.device:\n            torch.cuda.synchronize()\n\n\n    def train(self):\n        losses = []\n        val_losses = []\n        best_loss = float(\'Inf\')\n        best_val_loss = float(\'Inf\')\n\n        running_val_loss = 0.0\n        self.model.eval()\n        for data in self.val_loader:\n            data = to_device(data, self.device)\n            left = data[\'left_image\']\n            right = data[\'right_image\']\n            disps = self.model(left)\n            loss = self.loss_function(disps, [left, right])\n            val_losses.append(loss.item())\n            running_val_loss += loss.item()\n\n        running_val_loss /= self.val_n_img / self.args.batch_size\n        print(\'Val_loss:\', running_val_loss)\n\n        for epoch in range(self.args.epochs):\n            if self.args.adjust_lr:\n                adjust_learning_rate(self.optimizer, epoch,\n                                     self.args.learning_rate)\n            c_time = time.time()\n            running_loss = 0.0\n            self.model.train()\n            for data in self.loader:\n                # Load data\n                data = to_device(data, self.device)\n                left = data[\'left_image\']\n                right = data[\'right_image\']\n\n                # One optimization iteration\n                self.optimizer.zero_grad()\n                disps = self.model(left)\n                loss = self.loss_function(disps, [left, right])\n                loss.backward()\n                self.optimizer.step()\n                losses.append(loss.item())\n\n                # Print statistics\n                if self.args.print_weights:\n                    j = 1\n                    for (name, parameter) in self.model.named_parameters():\n                        if name.split(sep=\'.\')[-1] == \'weight\':\n                            plt.subplot(5, 9, j)\n                            plt.hist(parameter.data.view(-1))\n                            plt.xlim([-1, 1])\n                            plt.title(name.split(sep=\'.\')[0])\n                            j += 1\n                    plt.show()\n\n                if self.args.print_images:\n                    print(\'disp_left_est[0]\')\n                    plt.imshow(np.squeeze(\n                        np.transpose(self.loss_function.disp_left_est[0][0,\n                                     :, :, :].cpu().detach().numpy(),\n                                     (1, 2, 0))))\n                    plt.show()\n                    print(\'left_est[0]\')\n                    plt.imshow(np.transpose(self.loss_function\\\n                        .left_est[0][0, :, :, :].cpu().detach().numpy(),\n                        (1, 2, 0)))\n                    plt.show()\n                    print(\'disp_right_est[0]\')\n                    plt.imshow(np.squeeze(\n                        np.transpose(self.loss_function.disp_right_est[0][0,\n                                     :, :, :].cpu().detach().numpy(),\n                                     (1, 2, 0))))\n                    plt.show()\n                    print(\'right_est[0]\')\n                    plt.imshow(np.transpose(self.loss_function.right_est[0][0,\n                               :, :, :].cpu().detach().numpy(), (1, 2,\n                               0)))\n                    plt.show()\n                running_loss += loss.item()\n\n            running_val_loss = 0.0\n            self.model.eval()\n            for data in self.val_loader:\n                data = to_device(data, self.device)\n                left = data[\'left_image\']\n                right = data[\'right_image\']\n                disps = self.model(left)\n                loss = self.loss_function(disps, [left, right])\n                val_losses.append(loss.item())\n                running_val_loss += loss.item()\n\n            # Estimate loss per image\n            running_loss /= self.n_img / self.args.batch_size\n            running_val_loss /= self.val_n_img / self.args.batch_size\n            print (\n                \'Epoch:\',\n                epoch + 1,\n                \'train_loss:\',\n                running_loss,\n                \'val_loss:\',\n                running_val_loss,\n                \'time:\',\n                round(time.time() - c_time, 3),\n                \'s\',\n                )\n            self.save(self.args.model_path[:-4] + \'_last.pth\')\n            if running_val_loss < best_val_loss:\n                self.save(self.args.model_path[:-4] + \'_cpt.pth\')\n                best_val_loss = running_val_loss\n                print(\'Model_saved\')\n\n        print (\'Finished Training. Best loss:\', best_loss)\n        self.save(self.args.model_path)\n\n    def save(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load(self, path):\n        self.model.load_state_dict(torch.load(path))\n\n    def test(self):\n        self.model.eval()\n        disparities = np.zeros((self.n_img,\n                               self.input_height, self.input_width),\n                               dtype=np.float32)\n        disparities_pp = np.zeros((self.n_img,\n                                  self.input_height, self.input_width),\n                                  dtype=np.float32)\n        with torch.no_grad():\n            for (i, data) in enumerate(self.loader):\n                # Get the inputs\n                data = to_device(data, self.device)\n                left = data.squeeze()\n                # Do a forward pass\n                disps = self.model(left)\n                disp = disps[0][:, 0, :, :].unsqueeze(1)\n                disparities[i] = disp[0].squeeze().cpu().numpy()\n                disparities_pp[i] = \\\n                    post_process_disparity(disps[0][:, 0, :, :]\\\n                                           .cpu().numpy())\n\n        np.save(self.output_directory + \'/disparities.npy\', disparities)\n        np.save(self.output_directory + \'/disparities_pp.npy\',\n                disparities_pp)\n        print(\'Finished Testing\')\n\n\ndef main(args):\n    args = return_arguments()\n    if args.mode == \'train\':\n        model = Model(args)\n        model.train()\n    elif args.mode == \'test\':\n        model_test = Model(args)\n        model_test.test()\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
models_resnet.py,21,"b'from __future__ import absolute_import, division, print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport importlib\n\n\nclass conv(nn.Module):\n    def __init__(self, num_in_layers, num_out_layers, kernel_size, stride):\n        super(conv, self).__init__()\n        self.kernel_size = kernel_size\n        self.conv_base = nn.Conv2d(num_in_layers, num_out_layers, kernel_size=kernel_size, stride=stride)\n        self.normalize = nn.BatchNorm2d(num_out_layers)\n\n    def forward(self, x):\n        p = int(np.floor((self.kernel_size-1)/2))\n        p2d = (p, p, p, p)\n        x = self.conv_base(F.pad(x, p2d))\n        x = self.normalize(x)\n        return F.elu(x, inplace=True)\n\n\nclass convblock(nn.Module):\n    def __init__(self, num_in_layers, num_out_layers, kernel_size):\n        super(convblock, self).__init__()\n        self.conv1 = conv(num_in_layers, num_out_layers, kernel_size, 1)\n        self.conv2 = conv(num_out_layers, num_out_layers, kernel_size, 2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return self.conv2(x)\n\n\nclass maxpool(nn.Module):\n    def __init__(self, kernel_size):\n        super(maxpool, self).__init__()\n        self.kernel_size = kernel_size\n\n    def forward(self, x):\n        p = int(np.floor((self.kernel_size-1) / 2))\n        p2d = (p, p, p, p)\n        return F.max_pool2d(F.pad(x, p2d), self.kernel_size, stride=2)\n\n\nclass resconv(nn.Module):\n    def __init__(self, num_in_layers, num_out_layers, stride):\n        super(resconv, self).__init__()\n        self.num_out_layers = num_out_layers\n        self.stride = stride\n        self.conv1 = conv(num_in_layers, num_out_layers, 1, 1)\n        self.conv2 = conv(num_out_layers, num_out_layers, 3, stride)\n        self.conv3 = nn.Conv2d(num_out_layers, 4*num_out_layers, kernel_size=1, stride=1)\n        self.conv4 = nn.Conv2d(num_in_layers, 4*num_out_layers, kernel_size=1, stride=stride)\n        self.normalize = nn.BatchNorm2d(4*num_out_layers)\n\n    def forward(self, x):\n        # do_proj = x.size()[1] != self.num_out_layers or self.stride == 2\n        do_proj = True\n        shortcut = []\n        x_out = self.conv1(x)\n        x_out = self.conv2(x_out)\n        x_out = self.conv3(x_out)\n        if do_proj:\n            shortcut = self.conv4(x)\n        else:\n            shortcut = x\n        return F.elu(self.normalize(x_out + shortcut), inplace=True)\n\n\nclass resconv_basic(nn.Module):\n    # for resnet18\n    def __init__(self, num_in_layers, num_out_layers, stride):\n        super(resconv_basic, self).__init__()\n        self.num_out_layers = num_out_layers\n        self.stride = stride\n        self.conv1 = conv(num_in_layers, num_out_layers, 3, stride)\n        self.conv2 = conv(num_out_layers, num_out_layers, 3, 1)\n        self.conv3 = nn.Conv2d(num_in_layers, num_out_layers, kernel_size=1, stride=stride)\n        self.normalize = nn.BatchNorm2d(num_out_layers)\n\n    def forward(self, x):\n        #         do_proj = x.size()[1] != self.num_out_layers or self.stride == 2\n        do_proj = True\n        shortcut = []\n        x_out = self.conv1(x)\n        x_out = self.conv2(x_out)\n        if do_proj:\n            shortcut = self.conv3(x)\n        else:\n            shortcut = x\n        return F.elu(self.normalize(x_out + shortcut), inplace=True)\n\n\ndef resblock(num_in_layers, num_out_layers, num_blocks, stride):\n    layers = []\n    layers.append(resconv(num_in_layers, num_out_layers, stride))\n    for i in range(1, num_blocks - 1):\n        layers.append(resconv(4 * num_out_layers, num_out_layers, 1))\n    layers.append(resconv(4 * num_out_layers, num_out_layers, 1))\n    return nn.Sequential(*layers)\n\n\ndef resblock_basic(num_in_layers, num_out_layers, num_blocks, stride):\n    layers = []\n    layers.append(resconv_basic(num_in_layers, num_out_layers, stride))\n    for i in range(1, num_blocks):\n        layers.append(resconv_basic(num_out_layers, num_out_layers, 1))\n    return nn.Sequential(*layers)\n\n\nclass upconv(nn.Module):\n    def __init__(self, num_in_layers, num_out_layers, kernel_size, scale):\n        super(upconv, self).__init__()\n        self.scale = scale\n        self.conv1 = conv(num_in_layers, num_out_layers, kernel_size, 1)\n\n    def forward(self, x):\n        x = nn.functional.interpolate(x, scale_factor=self.scale, mode=\'bilinear\', align_corners=True)\n        return self.conv1(x)\n\n\nclass get_disp(nn.Module):\n    def __init__(self, num_in_layers):\n        super(get_disp, self).__init__()\n        self.conv1 = nn.Conv2d(num_in_layers, 2, kernel_size=3, stride=1)\n        self.normalize = nn.BatchNorm2d(2)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        p = 1\n        p2d = (p, p, p, p)\n        x = self.conv1(F.pad(x, p2d))\n        x = self.normalize(x)\n        return 0.3 * self.sigmoid(x)\n\n\nclass Resnet50_md(nn.Module):\n    def __init__(self, num_in_layers):\n        super(Resnet50_md, self).__init__()\n        # encoder\n        self.conv1 = conv(num_in_layers, 64, 7, 2)  # H/2  -   64D\n        self.pool1 = maxpool(3)  # H/4  -   64D\n        self.conv2 = resblock(64, 64, 3, 2)  # H/8  -  256D\n        self.conv3 = resblock(256, 128, 4, 2)  # H/16 -  512D\n        self.conv4 = resblock(512, 256, 6, 2)  # H/32 - 1024D\n        self.conv5 = resblock(1024, 512, 3, 2)  # H/64 - 2048D\n\n        # decoder\n        self.upconv6 = upconv(2048, 512, 3, 2)\n        self.iconv6 = conv(1024 + 512, 512, 3, 1)\n\n        self.upconv5 = upconv(512, 256, 3, 2)\n        self.iconv5 = conv(512+256, 256, 3, 1)\n\n        self.upconv4 = upconv(256, 128, 3, 2)\n        self.iconv4 = conv(256+128, 128, 3, 1)\n        self.disp4_layer = get_disp(128)\n\n        self.upconv3 = upconv(128, 64, 3, 2)\n        self.iconv3 = conv(64+64+2, 64, 3, 1)\n        self.disp3_layer = get_disp(64)\n\n        self.upconv2 = upconv(64, 32, 3, 2)\n        self.iconv2 = conv(32+64+2, 32, 3, 1)\n        self.disp2_layer = get_disp(32)\n\n        self.upconv1 = upconv(32, 16, 3, 2)\n        self.iconv1 = conv(16+2, 16, 3, 1)\n        self.disp1_layer = get_disp(16)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, x):\n        # encoder\n        x1 = self.conv1(x)\n        x_pool1 = self.pool1(x1)\n        x2 = self.conv2(x_pool1)\n        x3 = self.conv3(x2)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n\n        # skips\n        skip1 = x1\n        skip2 = x_pool1\n        skip3 = x2\n        skip4 = x3\n        skip5 = x4\n\n        # decoder\n        upconv6 = self.upconv6(x5)\n        concat6 = torch.cat((upconv6, skip5), 1)\n        iconv6 = self.iconv6(concat6)\n\n        upconv5 = self.upconv5(iconv6)\n        concat5 = torch.cat((upconv5, skip4), 1)\n        iconv5 = self.iconv5(concat5)\n\n        upconv4 = self.upconv4(iconv5)\n        concat4 = torch.cat((upconv4, skip3), 1)\n        iconv4 = self.iconv4(concat4)\n        self.disp4 = self.disp4_layer(iconv4)\n        self.udisp4 = nn.functional.interpolate(self.disp4, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv3 = self.upconv3(iconv4)\n        concat3 = torch.cat((upconv3, skip2, self.udisp4), 1)\n        iconv3 = self.iconv3(concat3)\n        self.disp3 = self.disp3_layer(iconv3)\n        self.udisp3 = nn.functional.interpolate(self.disp3, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv2 = self.upconv2(iconv3)\n        concat2 = torch.cat((upconv2, skip1, self.udisp3), 1)\n        iconv2 = self.iconv2(concat2)\n        self.disp2 = self.disp2_layer(iconv2)\n        self.udisp2 = nn.functional.interpolate(self.disp2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv1 = self.upconv1(iconv2)\n        concat1 = torch.cat((upconv1, self.udisp2), 1)\n        iconv1 = self.iconv1(concat1)\n        self.disp1 = self.disp1_layer(iconv1)\n        return self.disp1, self.disp2, self.disp3, self.disp4\n\n\nclass Resnet18_md(nn.Module):\n    def __init__(self, num_in_layers):\n        super(Resnet18_md, self).__init__()\n        # encoder\n        self.conv1 = conv(num_in_layers, 64, 7, 2)  # H/2  -   64D\n        self.pool1 = maxpool(3)  # H/4  -   64D\n        self.conv2 = resblock_basic(64, 64, 2, 2)  # H/8  -  64D\n        self.conv3 = resblock_basic(64, 128, 2, 2)  # H/16 -  128D\n        self.conv4 = resblock_basic(128, 256, 2, 2)  # H/32 - 256D\n        self.conv5 = resblock_basic(256, 512, 2, 2)  # H/64 - 512D\n\n        # decoder\n        self.upconv6 = upconv(512, 512, 3, 2)\n        self.iconv6 = conv(256+512, 512, 3, 1)\n\n        self.upconv5 = upconv(512, 256, 3, 2)\n        self.iconv5 = conv(128+256, 256, 3, 1)\n\n        self.upconv4 = upconv(256, 128, 3, 2)\n        self.iconv4 = conv(64+128, 128, 3, 1)\n        self.disp4_layer = get_disp(128)\n\n        self.upconv3 = upconv(128, 64, 3, 2)\n        self.iconv3 = conv(64+64 + 2, 64, 3, 1)\n        self.disp3_layer = get_disp(64)\n\n        self.upconv2 = upconv(64, 32, 3, 2)\n        self.iconv2 = conv(64+32 + 2, 32, 3, 1)\n        self.disp2_layer = get_disp(32)\n\n        self.upconv1 = upconv(32, 16, 3, 2)\n        self.iconv1 = conv(16+2, 16, 3, 1)\n        self.disp1_layer = get_disp(16)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, x):\n        # encoder\n        x1 = self.conv1(x)\n        x_pool1 = self.pool1(x1)\n        x2 = self.conv2(x_pool1)\n        x3 = self.conv3(x2)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n\n        # skips\n        skip1 = x1\n        skip2 = x_pool1\n        skip3 = x2\n        skip4 = x3\n        skip5 = x4\n\n        # decoder\n        upconv6 = self.upconv6(x5)\n        concat6 = torch.cat((upconv6, skip5), 1)\n        iconv6 = self.iconv6(concat6)\n\n        upconv5 = self.upconv5(iconv6)\n        concat5 = torch.cat((upconv5, skip4), 1)\n        iconv5 = self.iconv5(concat5)\n\n        upconv4 = self.upconv4(iconv5)\n        concat4 = torch.cat((upconv4, skip3), 1)\n        iconv4 = self.iconv4(concat4)\n        self.disp4 = self.disp4_layer(iconv4)\n        self.udisp4 = nn.functional.interpolate(self.disp4, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv3 = self.upconv3(iconv4)\n        concat3 = torch.cat((upconv3, skip2, self.udisp4), 1)\n        iconv3 = self.iconv3(concat3)\n        self.disp3 = self.disp3_layer(iconv3)\n        self.udisp3 = nn.functional.interpolate(self.disp3, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv2 = self.upconv2(iconv3)\n        concat2 = torch.cat((upconv2, skip1, self.udisp3), 1)\n        iconv2 = self.iconv2(concat2)\n        self.disp2 = self.disp2_layer(iconv2)\n        self.udisp2 = nn.functional.interpolate(self.disp2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv1 = self.upconv1(iconv2)\n        concat1 = torch.cat((upconv1, self.udisp2), 1)\n        iconv1 = self.iconv1(concat1)\n        self.disp1 = self.disp1_layer(iconv1)\n        return self.disp1, self.disp2, self.disp3, self.disp4\n\n\ndef class_for_name(module_name, class_name):\n    # load the module, will raise ImportError if module cannot be loaded\n    m = importlib.import_module(module_name)\n    # get the class, will raise AttributeError if class cannot be found\n    return getattr(m, class_name)\n\n\nclass ResnetModel(nn.Module):\n    def __init__(self, num_in_layers, encoder=\'resnet18\', pretrained=False):\n        super(ResnetModel, self).__init__()\n        assert encoder in [\'resnet18\', \'resnet34\', \'resnet50\',\\\n                           \'resnet101\', \'resnet152\'],\\\n                           ""Incorrect encoder type""\n        if encoder in [\'resnet18\', \'resnet34\']:\n            filters = [64, 128, 256, 512]\n        else:\n            filters = [256, 512, 1024, 2048]\n        resnet = class_for_name(""torchvision.models"", encoder)\\\n                                (pretrained=pretrained)\n        if num_in_layers != 3:  # Number of input channels\n            self.firstconv = nn.Conv2d(num_in_layers, 64,\n                              kernel_size=(7, 7), stride=(2, 2),\n                              padding=(3, 3), bias=False)\n        else:\n            self.firstconv = resnet.conv1 # H/2\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool # H/4\n\n        # encoder\n        self.encoder1 = resnet.layer1 # H/4\n        self.encoder2 = resnet.layer2 # H/8\n        self.encoder3 = resnet.layer3 # H/16\n        self.encoder4 = resnet.layer4 # H/32\n\n        # decoder\n        self.upconv6 = upconv(filters[3], 512, 3, 2)\n        self.iconv6 = conv(filters[2] + 512, 512, 3, 1)\n\n        self.upconv5 = upconv(512, 256, 3, 2)\n        self.iconv5 = conv(filters[1] + 256, 256, 3, 1)\n\n        self.upconv4 = upconv(256, 128, 3, 2)\n        self.iconv4 = conv(filters[0] + 128, 128, 3, 1)\n        self.disp4_layer = get_disp(128)\n\n        self.upconv3 = upconv(128, 64, 3, 1) #\n        self.iconv3 = conv(64 + 64 + 2, 64, 3, 1)\n        self.disp3_layer = get_disp(64)\n\n        self.upconv2 = upconv(64, 32, 3, 2)\n        self.iconv2 = conv(64 + 32 + 2, 32, 3, 1)\n        self.disp2_layer = get_disp(32)\n\n        self.upconv1 = upconv(32, 16, 3, 2)\n        self.iconv1 = conv(16 + 2, 16, 3, 1)\n        self.disp1_layer = get_disp(16)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, x):\n        # encoder\n        x_first_conv = self.firstconv(x)\n        x = self.firstbn(x_first_conv)\n        x = self.firstrelu(x)\n        x_pool1 = self.firstmaxpool(x)\n        x1 = self.encoder1(x_pool1)\n        x2 = self.encoder2(x1)\n        x3 = self.encoder3(x2)\n        x4 = self.encoder4(x3)\n        # skips\n        skip1 = x_first_conv\n        skip2 = x_pool1\n        skip3 = x1\n        skip4 = x2\n        skip5 = x3\n\n        # decoder\n        upconv6 = self.upconv6(x4)\n        concat6 = torch.cat((upconv6, skip5), 1)\n        iconv6 = self.iconv6(concat6)\n\n        upconv5 = self.upconv5(iconv6)\n        concat5 = torch.cat((upconv5, skip4), 1)\n        iconv5 = self.iconv5(concat5)\n\n        upconv4 = self.upconv4(iconv5)\n        concat4 = torch.cat((upconv4, skip3), 1)\n        iconv4 = self.iconv4(concat4)\n        self.disp4 = self.disp4_layer(iconv4)\n        self.udisp4 = nn.functional.interpolate(self.disp4, scale_factor=1, mode=\'bilinear\', align_corners=True)\n        self.disp4 = nn.functional.interpolate(self.disp4, scale_factor=0.5, mode=\'bilinear\', align_corners=True)\n\n        upconv3 = self.upconv3(iconv4)\n        concat3 = torch.cat((upconv3, skip2, self.udisp4), 1)\n        iconv3 = self.iconv3(concat3)\n        self.disp3 = self.disp3_layer(iconv3)\n        self.udisp3 = nn.functional.interpolate(self.disp3, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv2 = self.upconv2(iconv3)\n        concat2 = torch.cat((upconv2, skip1, self.udisp3), 1)\n        iconv2 = self.iconv2(concat2)\n        self.disp2 = self.disp2_layer(iconv2)\n        self.udisp2 = nn.functional.interpolate(self.disp2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        upconv1 = self.upconv1(iconv2)\n        concat1 = torch.cat((upconv1, self.udisp2), 1)\n        iconv1 = self.iconv1(concat1)\n        self.disp1 = self.disp1_layer(iconv1)\n        return self.disp1, self.disp2, self.disp3, self.disp4\n'"
transforms.py,3,"b""import torch\nimport torchvision.transforms as transforms\nimport numpy as np\n\n\n\ndef image_transforms(mode='train', augment_parameters=[0.8, 1.2, 0.5, 2.0, 0.8, 1.2],\n                     do_augmentation=True, transformations=None,  size=(256, 512)):\n    if mode == 'train':\n        data_transform = transforms.Compose([\n            ResizeImage(train=True, size=size),\n            RandomFlip(do_augmentation),\n            ToTensor(train=True),\n            AugmentImagePair(augment_parameters, do_augmentation)\n        ])\n        return data_transform\n    elif mode == 'test':\n        data_transform = transforms.Compose([\n            ResizeImage(train=False, size=size),\n            ToTensor(train=False),\n            DoTest(),\n        ])\n        return data_transform\n    elif mode == 'custom':\n        data_transform = transforms.Compose(transformations)\n        return data_transform\n    else:\n        print('Wrong mode')\n\n\nclass ResizeImage(object):\n    def __init__(self, train=True, size=(256, 512)):\n        self.train = train\n        self.transform = transforms.Resize(size)\n\n    def __call__(self, sample):\n        if self.train:\n            left_image = sample['left_image']\n            right_image = sample['right_image']\n            new_right_image = self.transform(right_image)\n            new_left_image = self.transform(left_image)\n            sample = {'left_image': new_left_image, 'right_image': new_right_image}\n        else:\n            left_image = sample\n            new_left_image = self.transform(left_image)\n            sample = new_left_image\n        return sample\n\n\nclass DoTest(object):\n    def __call__(self, sample):\n        new_sample = torch.stack((sample, torch.flip(sample, [2])))\n        return new_sample\n\n\nclass ToTensor(object):\n    def __init__(self, train):\n        self.train = train\n        self.transform = transforms.ToTensor()\n\n    def __call__(self, sample):\n        if self.train:\n            left_image = sample['left_image']\n            right_image = sample['right_image']\n            new_right_image = self.transform(right_image)\n            new_left_image = self.transform(left_image)\n            sample = {'left_image': new_left_image,\n                      'right_image': new_right_image}\n        else:\n            left_image = sample\n            sample = self.transform(left_image)\n        return sample\n\n\nclass RandomFlip(object):\n    def __init__(self, do_augmentation):\n        self.transform = transforms.RandomHorizontalFlip(p=1)\n        self.do_augmentation = do_augmentation\n\n    def __call__(self, sample):\n        left_image = sample['left_image']\n        right_image = sample['right_image']\n        k = np.random.uniform(0, 1, 1)\n        if self.do_augmentation:\n            if k > 0.5:\n                fliped_left = self.transform(right_image)\n                fliped_right = self.transform(left_image)\n                sample = {'left_image': fliped_left, 'right_image': fliped_right}\n        else:\n            sample = {'left_image': left_image, 'right_image': right_image}\n        return sample\n\n\nclass AugmentImagePair(object):\n    def __init__(self, augment_parameters, do_augmentation):\n        self.do_augmentation = do_augmentation\n        self.gamma_low = augment_parameters[0]  # 0.8\n        self.gamma_high = augment_parameters[1]  # 1.2\n        self.brightness_low = augment_parameters[2]  # 0.5\n        self.brightness_high = augment_parameters[3]  # 2.0\n        self.color_low = augment_parameters[4]  # 0.8\n        self.color_high = augment_parameters[5]  # 1.2\n\n    def __call__(self, sample):\n        left_image = sample['left_image']\n        right_image = sample['right_image']\n        p = np.random.uniform(0, 1, 1)\n        if self.do_augmentation:\n            if p > 0.5:\n                # randomly shift gamma\n                random_gamma = np.random.uniform(self.gamma_low, self.gamma_high)\n                left_image_aug = left_image ** random_gamma\n                right_image_aug = right_image ** random_gamma\n\n                # randomly shift brightness\n                random_brightness = np.random.uniform(self.brightness_low, self.brightness_high)\n                left_image_aug = left_image_aug * random_brightness\n                right_image_aug = right_image_aug * random_brightness\n\n                # randomly shift color\n                random_colors = np.random.uniform(self.color_low, self.color_high, 3)\n                for i in range(3):\n                    left_image_aug[i, :, :] *= random_colors[i]\n                    right_image_aug[i, :, :] *= random_colors[i]\n\n                # saturate\n                left_image_aug = torch.clamp(left_image_aug, 0, 1)\n                right_image_aug = torch.clamp(right_image_aug, 0, 1)\n\n                sample = {'left_image': left_image_aug, 'right_image': right_image_aug}\n\n        else:\n            sample = {'left_image': left_image, 'right_image': right_image}\n        return sample\n"""
utils.py,2,"b'import torch\nimport collections\nimport os\nfrom torch.utils.data import DataLoader, ConcatDataset\n\n\nfrom models_resnet import Resnet18_md, Resnet50_md, ResnetModel\nfrom data_loader import KittiLoader\nfrom transforms import image_transforms\n\ndef to_device(input, device):\n    if torch.is_tensor(input):\n        return input.to(device=device)\n    elif isinstance(input, str):\n        return input\n    elif isinstance(input, collections.Mapping):\n        return {k: to_device(sample, device=device) for k, sample in input.items()}\n    elif isinstance(input, collections.Sequence):\n        return [to_device(sample, device=device) for sample in input]\n    else:\n        raise TypeError(f""Input must contain tensor, dict or list, found {type(input)}"")\n\n\ndef get_model(model, input_channels=3, pretrained=False):\n    if model == \'resnet50_md\':\n        out_model = Resnet50_md(input_channels)\n    elif model == \'resnet18_md\':\n        out_model = Resnet18_md(input_channels)\n    else:\n        out_model = ResnetModel(input_channels, encoder=model, pretrained=pretrained)\n    return out_model\n\n\ndef prepare_dataloader(data_directory, mode, augment_parameters,\n                       do_augmentation, batch_size, size, num_workers):\n    data_dirs = os.listdir(data_directory)\n    data_transform = image_transforms(\n        mode=mode,\n        augment_parameters=augment_parameters,\n        do_augmentation=do_augmentation,\n        size = size)\n    datasets = [KittiLoader(os.path.join(data_directory,\n                            data_dir), mode, transform=data_transform)\n                            for data_dir in data_dirs]\n    dataset = ConcatDataset(datasets)\n    n_img = len(dataset)\n    print(\'Use a dataset with\', n_img, \'images\')\n    if mode == \'train\':\n        loader = DataLoader(dataset, batch_size=batch_size,\n                            shuffle=True, num_workers=num_workers,\n                            pin_memory=True)\n    else:\n        loader = DataLoader(dataset, batch_size=batch_size,\n                            shuffle=False, num_workers=num_workers,\n                            pin_memory=True)\n    return n_img, loader\n'"
