file_path,api_count,code
run_websocket_server.py,8,"b'from multiprocessing import Process\nimport argparse\nimport os\nimport logging\nimport syft as sy\nfrom syft.workers.websocket_server import WebsocketServerWorker\nimport torch\nimport numpy as np\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom syft.frameworks.torch.fl import utils\n\nKEEP_LABELS_DICT = {\n    ""alice"": [0, 1, 2, 3],\n    ""bob"": [4, 5, 6],\n    ""charlie"": [7, 8, 9],\n    ""testing"": list(range(10)),\n    None: list(range(10)),\n}\n\n\ndef start_websocket_server_worker(\n    id, host, port, hook, verbose, keep_labels=None, training=True, pytest_testing=False\n):\n    """"""Helper function for spinning up a websocket server and setting up the local datasets.""""""\n\n    server = WebsocketServerWorker(id=id, host=host, port=port, hook=hook, verbose=verbose)\n\n    # Setup toy data (mnist example)\n    mnist_dataset = datasets.MNIST(\n        root=""./data"",\n        train=training,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        ),\n    )\n\n    if training:\n        indices = np.isin(mnist_dataset.targets, keep_labels).astype(""uint8"")\n        logger.info(""number of true indices: %s"", indices.sum())\n        selected_data = (\n            torch.native_masked_select(mnist_dataset.data.transpose(0, 2), torch.tensor(indices))\n            .view(28, 28, -1)\n            .transpose(2, 0)\n        )\n        logger.info(""after selection: %s"", selected_data.shape)\n        selected_targets = torch.native_masked_select(mnist_dataset.targets, torch.tensor(indices))\n\n        dataset = sy.BaseDataset(\n            data=selected_data, targets=selected_targets, transform=mnist_dataset.transform\n        )\n        key = ""mnist""\n    else:\n        dataset = sy.BaseDataset(\n            data=mnist_dataset.data,\n            targets=mnist_dataset.targets,\n            transform=mnist_dataset.transform,\n        )\n        key = ""mnist_testing""\n\n    # Adding Dataset\n    server.add_dataset(dataset, key=key)\n    if pytest_testing:\n        # Setup toy data (vectors example)\n        data_vectors = torch.tensor([[-1, 2.0], [0, 1.1], [-1, 2.1], [0, 1.2]], requires_grad=True)\n        target_vectors = torch.tensor([[1], [0], [1], [0]])\n\n        server.add_dataset(sy.BaseDataset(data_vectors, target_vectors), key=""vectors"")\n\n        # Setup toy data (xor example)\n        data_xor = torch.tensor(\n            [[0.0, 1.0], [1.0, 0.0], [1.0, 1.0], [0.0, 0.0]], requires_grad=True\n        )\n        target_xor = torch.tensor([1.0, 1.0, 0.0, 0.0], requires_grad=False)\n\n        server.add_dataset(sy.BaseDataset(data_xor, target_xor), key=""xor"")\n\n        # Setup gaussian mixture dataset\n        data, target = utils.create_gaussian_mixture_toy_data(nr_samples=100)\n        server.add_dataset(sy.BaseDataset(data, target), key=""gaussian_mixture"")\n\n        # Setup partial iris dataset\n        data, target = utils.iris_data_partial()\n        dataset = sy.BaseDataset(data, target)\n        dataset_key = ""iris""\n        server.add_dataset(dataset, key=dataset_key)\n    else:\n        count = [0] * 10\n        logger.info(\n            ""MNIST dataset (%s set), available numbers on %s: "", ""train"" if training else ""test"", id\n        )\n        for i in range(10):\n            count[i] = (dataset.targets == i).sum().item()\n            logger.info(""      %s: %s"", i, count[i])\n\n    logger.info(""datasets: %s"", server.datasets)\n    if training:\n        logger.info(""len(datasets[mnist]): %s"", len(server.datasets[key]))\n\n    server.start()\n    return server\n\n\ndef start_proc(participant, kwargs):  # pragma: no cover\n    """""" helper function for spinning up a websocket participant """"""\n\n    def target():\n        server = participant(**kwargs)\n        server.start()\n\n    p = Process(target=target)\n    p.start()\n    return p\n\n\ndef start_proc_steal_data_over_sockets(participant, kwargs):  # pragma: no cover\n    """""" helper function for spinning up a websocket participant """"""\n\n    def target():\n        server = participant(**kwargs)\n        private_data = torch.tensor([1, 1, 1, 1, 1])\n        private_data.private = True\n        server._objects[1] = private_data\n        server.start()\n\n    p = Process(target=target)\n    p.start()\n    return p\n\n\nif __name__ == ""__main__"":\n\n    # Logging setup\n    FORMAT = ""%(asctime)s %(levelname)s %(filename)s(l:%(lineno)d, p:%(process)d) - %(message)s""\n    logging.basicConfig(format=FORMAT)\n    logger = logging.getLogger(""run_websocket_server"")\n    logger.setLevel(level=logging.DEBUG)\n\n    # Parse args\n    parser = argparse.ArgumentParser(description=""Run websocket server worker."")\n    parser.add_argument(\n        ""--port"",\n        ""-p"",\n        type=int,\n        help=""port number of the websocket server worker, e.g. --port 8777"",\n    )\n    parser.add_argument(""--host"", type=str, default=""localhost"", help=""host for the connection"")\n    parser.add_argument(\n        ""--id"", type=str, help=""name (id) of the websocket server worker, e.g. --id alice""\n    )\n    parser.add_argument(\n        ""--testing"",\n        action=""store_true"",\n        help=(\n            ""if set, websocket server worker will load ""\n            ""the test dataset instead of the training dataset""\n        ),\n    )\n    parser.add_argument(\n        ""--verbose"",\n        ""-v"",\n        action=""store_true"",\n        help=""""""if set, websocket server worker will be started in verbose mode"""""",\n    )\n    parser.add_argument(\n        ""--notebook"",\n        type=str,\n        default=""normal"",\n        help=(\n            ""can run websocket server for websockets examples of mnist/mnist-parallel or ""\n            ""pen_testing/steal_data_over_sockets. Type \'mnist\' for starting server ""\n            ""for websockets-example-MNIST, `mnist-parallel` for websockets-example-MNIST-parallel ""\n            ""and \'steal_data\' for pen_tesing stealing data over sockets""\n        ),\n    )\n    parser.add_argument(""--pytest_testing"", action=""store_true"", help=""""""Used for pytest testing"""""")\n    args = parser.parse_args()\n\n    # Hook and start server\n    hook = sy.TorchHook(torch)\n\n    # server = start_proc(WebsocketServerWorker, kwargs)\n    if args.notebook == ""normal"" or args.notebook == ""mnist"" or args.notebook == ""steal_data"":\n        kwargs = {\n            ""id"": args.id,\n            ""host"": args.host,\n            ""port"": args.port,\n            ""hook"": hook,\n            ""verbose"": args.verbose,\n        }\n        if os.name != ""nt"" and (args.notebook == ""normal"" or args.notebook == ""mnist""):\n            server = start_proc(WebsocketServerWorker, kwargs)\n        elif os.name != ""nt"" and args.notebook == ""steal_data"":\n            server = start_proc_steal_data_over_sockets(WebsocketServerWorker, kwargs)\n        else:\n            server = WebsocketServerWorker(**kwargs)\n            server.start()\n    elif args.notebook == ""mnist-parallel"" or args.pytest_testing:\n        server = start_websocket_server_worker(\n            id=args.id,\n            host=args.host,\n            port=args.port,\n            hook=hook,\n            verbose=args.verbose,\n            keep_labels=KEEP_LABELS_DICT[args.id]\n            if args.id in KEEP_LABELS_DICT\n            else list(range(10)),\n            training=not args.testing,\n        )\n'"
setup.py,0,"b'import os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\n# Utility function to read the README file.\n# Used for the long_description.  It\'s nice, because now 1) we have a top level\n# README file and 2) it\'s easier to type in the README file than to put a raw\n# string in below ...\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\n\ndef get_requirements(req_file):\n    """"""Read requirements file and return packages and git repos separately""""""\n    requirements = []\n    dependency_links = []\n    lines = read(req_file).split(""\\n"")\n    for line in lines:\n        if line.startswith(""git+""):\n            dependency_links.append(line)\n        else:\n            requirements.append(line)\n    return requirements, dependency_links\n\n\nREQ_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""pip-dep"")\ncore_reqs, core_dependency_links = get_requirements(os.path.join(REQ_DIR, ""requirements.txt""))\nudacity_extras = read(os.path.join(REQ_DIR, ""requirements_udacity.txt"")).split(""\\n"")\ntensorflow_extras = read(os.path.join(REQ_DIR, ""requirements_tensorflow.txt"")).split(""\\n"")\nnotebook_extras = read(os.path.join(REQ_DIR, ""requirements_notebooks.txt"")).split(""\\n"")\ndev_extras = read(os.path.join(REQ_DIR, ""requirements_dev.txt"")).split(""\\n"")\nsandbox_extras = [""scikit-learn>=0.21.0""]\ntests_require = [""pytest"", ""pytest-flake8""] + sandbox_extras + notebook_extras\n\n\nsetup(\n    name=""syft"",\n    version=""0.2.6"",\n    author=""Andrew Trask"",\n    author_email=""contact@openmined.org"",\n    description=(""A Library for Private, Secure Deep Learning""),\n    license=""Apache-2.0"",\n    keywords=(\n        ""deep learning artificial intelligence privacy secure ""\n        ""multi-party computation federated learning differential privacy""\n    ),\n    packages=find_packages(exclude=[""docs"", ""examples"", ""dist""]),\n    include_package_data=True,\n    long_description=read(""README.md""),\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/OpenMined/PySyft"",\n    install_requires=core_reqs,\n    extras_require={\n        ""udacity"": udacity_extras,\n        ""notebooks"": notebook_extras,\n        ""dev"": dev_extras,\n        ""sandbox"": sandbox_extras,\n        ""tensorflow"": tensorflow_extras,\n    },\n    dependency_links=core_dependency_links,\n    setup_requires=[""pytest-runner""],\n    tests_require=tests_require,\n    classifiers=[""Programming Language :: Python :: 3"", ""Operating System :: OS Independent""],\n)\n'"
examples/__init__.py,0,b''
syft/__init__.py,11,"b'r""""""\nPySyft is a Python library for secure, private Deep Learning.\nPySyft decouples private data from model training, using Federated Learning,\nDifferential Privacy, and Multi-Party Computation (MPC) within PyTorch.\n""""""\n# We load these modules first so that syft knows which are available\nfrom syft import dependency_check\nfrom syft import frameworks  # Triggers registration of any available frameworks # noqa:F401\n\n# Major imports\n\n# This import statement is strictly here to trigger registration of syft\n# tensor types inside hook_args.py.\nimport syft.frameworks.torch.hook.hook_args\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# The purpose of the following import section is to increase the convenience of using\n# PySyft by making it possible to import the most commonly used objects from syft\n# directly (i.e., syft.TorchHook or syft.VirtualWorker or syft.LoggingTensor)\n\n# Tensorflow / Keras dependencies\n# Import Hooks\n\n__all__ = []\nif dependency_check.tfe_available:\n    from syft.frameworks.keras import KerasHook  # noqa: F401\n    from syft.workers.tfe import TFECluster  # noqa: F401\n    from syft.workers.tfe import TFEWorker  # noqa: F401\n\n    __all__.extend([""KerasHook"", ""TFECluster"", ""TFEWorker""])\nelse:\n    logger.info(""TF Encrypted Keras not available."")\n\n# Pytorch dependencies\n# Import Hook\nfrom syft.frameworks.torch.hook.hook import TorchHook  # noqa: E402,F401\n\n# Import grids\nfrom syft.grid.private_grid import PrivateGridNetwork  # noqa: E402,F401\nfrom syft.grid.public_grid import PublicGridNetwork  # noqa: E402,F401\n\n\n# Import sandbox\nfrom syft.sandbox import create_sandbox, make_hook  # noqa: E402,F401\n\n# Import federate learning objects\nfrom syft.frameworks.torch.fl import (  # noqa: E402, F401\n    FederatedDataset,\n    FederatedDataLoader,\n    BaseDataset,\n)\n\n# Import messaging objects\nfrom syft.execution.protocol import Protocol  # noqa: E402, F401\nfrom syft.execution.protocol import func2protocol  # noqa: E402, F401\nfrom syft.execution.plan import Plan  # noqa: E402, F401\nfrom syft.execution.plan import func2plan  # noqa: E402, F401\n\n# Import Worker Types\nfrom syft.workers.virtual import VirtualWorker  # noqa: E402,F401\nfrom syft.workers.websocket_client import WebsocketClientWorker  # noqa: E402,F401\nfrom syft.workers.websocket_server import WebsocketServerWorker  # noqa: E402,F401\n\n# Import Syft\'s Public Tensor Types\nfrom syft.frameworks.torch.tensors.decorators.logging import LoggingTensor  # noqa: E402,F401\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import (  # noqa: E402,F401\n    AdditiveSharingTensor,\n)\nfrom syft.frameworks.torch.tensors.interpreters.autograd import AutogradTensor  # noqa: E402,F401\nfrom syft.frameworks.torch.tensors.interpreters.precision import (  # noqa: E402,F401\n    FixedPrecisionTensor,\n)\nfrom syft.frameworks.torch.tensors.interpreters.numpy import (  # noqa: E402,F401\n    create_numpy_tensor as NumpyTensor,\n)\n\nfrom syft.frameworks.torch.tensors.interpreters.private import PrivateTensor  # noqa: E402, F401\nfrom syft.execution.placeholder import PlaceHolder  # noqa: E402, F401\nfrom syft.generic.pointers.pointer_plan import PointerPlan  # noqa: E402, F401\nfrom syft.generic.pointers.pointer_tensor import PointerTensor  # noqa: E402, F401\nfrom syft.generic.pointers.multi_pointer import MultiPointerTensor  # noqa: E402, F401\n\n# Import serialization tools\nfrom syft import serde  # noqa: E402, F401\n\n# import functions\nfrom syft.frameworks.torch.functions import combine_pointers  # noqa: E402, F401\nfrom syft.frameworks.torch.he.paillier import keygen  # noqa: E402, F401\n\n# import common\nimport syft.common.util  # noqa: E402, F401\n\n\ndef pool():\n    if not hasattr(syft, ""_pool""):\n        import multiprocessing\n\n        syft._pool = multiprocessing.Pool()\n    return syft._pool\n\n\n__all__.extend(\n    [\n        ""frameworks"",\n        ""serde"",\n        ""TorchHook"",\n        ""VirtualWorker"",\n        ""WebsocketClientWorker"",\n        ""WebsocketServerWorker"",\n        ""Protocol"",\n        ""func2protocol"",\n        ""Plan"",\n        ""func2plan"",\n        ""make_plan"",\n        ""LoggingTensor"",\n        ""AdditiveSharingTensor"",\n        ""AutogradTensor"",\n        ""FixedPrecisionTensor"",\n        ""PointerTensor"",\n        ""MultiPointerTensor"",\n        ""PrivateGridNetwork"",\n        ""PublicGridNetwork"",\n        ""create_sandbox"",\n        ""hook"",\n        ""combine_pointers"",\n        ""FederatedDataset"",\n        ""FederatedDataLoader"",\n        ""BaseDataset"",\n    ]\n)\n\nlocal_worker = None\ntorch = None\nframework = None\n\nif ""ID_PROVIDER"" not in globals():\n    from syft.generic.id_provider import IdProvider\n\n    ID_PROVIDER = IdProvider()\n'"
syft/codes.py,0,"b'class PLAN_CMDS(object):  # noqa: N801\n    FETCH_PLAN = ""fetch_plan""\n    FETCH_PROTOCOL = ""fetch_protocol""\n\n\nclass TENSOR_SERIALIZATION(object):  # noqa: N801\n    TORCH = ""torch""\n    NUMPY = ""numpy""\n    TF = ""tf""\n    ALL = ""all""\n\n\nclass GATEWAY_ENDPOINTS(object):  # noqa: N801\n    SEARCH_TAGS = ""/search""\n    SEARCH_MODEL = ""/search-model""\n    SEARCH_ENCRYPTED_MODEL = ""/search-encrypted-model""\n    SELECT_MODEL_HOST = ""/choose-model-host""\n    SELECT_ENCRYPTED_MODEL_HOSTS = ""/choose-encrypted-model-host""\n\n\nclass REQUEST_MSG(object):  # noqa: N801\n    TYPE_FIELD = ""type""\n    GET_ID = ""get-id""\n    CONNECT_NODE = ""connect-node""\n    HOST_MODEL = ""host-model""\n    RUN_INFERENCE = ""run-inference""\n    LIST_MODELS = ""list-models""\n    DELETE_MODEL = ""delete-model""\n    RUN_INFERENCE = ""run-inference""\n    AUTHENTICATE = ""authentication""\n\n\nclass RESPONSE_MSG(object):  # noqa: N801\n    NODE_ID = ""id""\n    ERROR = ""error""\n    SUCCESS = ""success""\n    MODELS = ""models""\n    INFERENCE_RESULT = ""prediction""\n    SYFT_VERSION = ""syft_version""\n\n\nclass MSG_FIELD:\n    TYPE = ""type""\n    FROM = ""from""\n    DESTINATION = ""destination""\n    CONTENT = ""content""\n    NODE_ID = ""node_id""\n    PAYLOAD = ""payload""\n    NODES = ""nodes""\n    MODELS = ""models""\n    DATASETS = ""datasets""\n    CPU = ""cpu""\n    MEM_USAGE = ""mem_usage""\n\n\nclass NODE_EVENTS:\n    MONITOR = ""monitor""\n    WEBRTC_SCOPE = ""create-webrtc-scope""\n    WEBRTC_OFFER = ""webrtc-offer""\n    WEBRTC_ANSWER = ""webrtc-answer""\n\n\nclass GRID_EVENTS:\n    JOIN = ""join""\n    FORWARD = ""grid-forward""\n    FORWARD = ""forward""\n    MONITOR_ANSWER = ""monitor-answer""\n'"
syft/dependency_check.py,0,"b'from distutils.version import LooseVersion\nfrom importlib import util\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import tensorflow\n\n    if LooseVersion(tensorflow.__version__) < LooseVersion(""2.0.0""):\n        raise ImportError()\n    pstf_spec = util.find_spec(""syft_tensorflow"")\n    tensorflow_available = pstf_spec is not None\nexcept ImportError:\n    tensorflow_available = False\n\n\ntfe_spec = util.find_spec(""tf_encrypted"")\ntfe_available = tfe_spec is not None\n\n\ntorch_spec = util.find_spec(""torch"")\ntorch_available = torch_spec is not None\n'"
syft/exceptions.py,0,"b'""""""Specific Pysyft exceptions.""""""\nfrom tblib import Traceback\nimport traceback\nfrom six import reraise\nfrom typing import Tuple\n\nimport syft as sy\nfrom syft.generic.frameworks.types import FrameworkTensor\n\n\nclass DependencyError(Exception):\n    def __init__(self, package, pypi_alias=None):\n        if pypi_alias is None:\n            pypi_alias = package\n        message = (\n            f""The {package} dependency is not installed. If you intend""\n            "" to use it, please install it at your command line with ""\n            f""`pip install {pypi_alias}`.""\n        )\n        super().__init__(message)\n\n\nclass PureFrameworkTensorFoundError(BaseException):\n    """"""Exception raised for errors in the input.\n    This error is used in a recursive analysis of the args provided as an\n    input of a function, to break the recursion if a FrameworkTensor is found\n    as it means that _probably_ all the tensors are pure torch/tensorflow and\n    the function can be applied natively on this input.\n\n    Attributes:\n        expression -- input expression in which the error occurred\n        message -- explanation of the error\n    """"""\n\n    pass\n\n\nclass RemoteObjectFoundError(BaseException):\n    """"""Exception raised for errors in the input.\n    This error is used in a context similar to PureFrameworkTensorFoundError but\n    to indicate that a Pointer to a remote tensor was found  in the input\n    and thus that the command should be send elsewhere. The pointer retrieved\n    by the error gives the location where the command should be sent.\n\n    Attributes:\n        expression -- input expression in which the error occurred\n        message -- explanation of the error\n    """"""\n\n    def __init__(self, pointer):\n        self.pointer = pointer\n\n\nclass InvalidTensorForRemoteGet(Exception):\n    """"""Raised when a chain of pointer tensors is not provided for `remote_get`.""""""\n\n    def __init__(self, tensor: object):\n        message = (\n            ""Tensor does not have attribute child. You remote get should ""\n            f""be called on a chain of pointer tensors, instead you called it on {tensor}.""\n        )\n        super().__init__(message)\n\n\nclass WorkerNotFoundException(Exception):\n    """"""Raised when a non-existent worker is requested.""""""\n\n    pass\n\n\nclass CompressionNotFoundException(Exception):\n    """"""Raised when a non existent compression/decompression scheme is requested.""""""\n\n    pass\n\n\nclass CannotRequestObjectAttribute(Exception):\n    """"""Raised when .get() is called on a pointer which points to an attribute of\n    another object.""""""\n\n    pass\n\n\nclass TensorsNotCollocatedException(Exception):\n    """"""Raised when a command is executed on two tensors which are not\n    on the same machine. The goal is to provide as useful input as possible\n    to help the user identify which tensors are where so that they can debug\n    which one needs to be moved.""""""\n\n    def __init__(self, tensor_a, tensor_b, attr=""a method""):\n\n        if hasattr(tensor_a, ""child"") and tensor_a.is_wrapper:\n            tensor_a = tensor_a.child\n\n        if hasattr(tensor_b, ""child"") and tensor_b.is_wrapper:\n            tensor_b = tensor_b.child\n\n        if isinstance(tensor_a, sy.PointerTensor) and isinstance(tensor_b, sy.PointerTensor):\n            message = (\n                f""You tried to call {attr} involving two tensors which are not on the same machine!""\n                "" One tensor is on {tensor_a.location} while the other is on {tensor_b.location}.""\n                "" Use a combination of .move(), .get(), and/or .send()""\n                "" to co-locate them to the same machine.""\n            )\n        elif isinstance(tensor_a, sy.PointerTensor):\n            message = (\n                f""You tried to call {attr} involving two tensors where one tensor is actually""\n                "" located on another machine (is a PointerTensor). Call .get() on a""\n                "" the PointerTensor or .send({tensor_b.location.id}) on the other tensor.\\n""\n                ""Tensor A: {tensor_a}\\n""\n                ""Tensor B: {tensor_b}""\n            )\n        elif isinstance(tensor_b, sy.PointerTensor):\n            message = (\n                f""You tried to call {attr} involving two tensors where one tensor is actually""\n                "" located on another machine (is a PointerTensor). Call .get() on a""\n                "" the PointerTensor or .send({tensor_a.location.id}) on the other tensor.\\n""\n                ""Tensor A: {tensor_a}\\n""\n                ""Tensor B: {tensor_b}""\n            )\n        else:\n            message = (\n                f""You tried to call {attr} involving two tensors which are not""\n                "" on the same machine. Try calling .send(), .move(), and/or .get() on these""\n                "" tensors to get them to the same worker before calling methods that involve""\n                "" them working together.""\n            )\n\n        super().__init__(message)\n\n        self.tensor_a = tensor_a\n        self.tensor_b = tensor_b\n\n\nclass ResponseSignatureError(Exception):\n    """"""Raised when the return of a hooked function is not correctly predicted\n    (when defining in advance ids for results)\n    """"""\n\n    def __init__(self, ids_generated=None):\n        self.ids_generated = ids_generated\n\n    def get_attributes(self):\n        """"""\n        Specify all the attributes need to report an error correctly.\n        """"""\n        return {""ids_generated"": self.ids_generated}\n\n    @staticmethod\n    def simplify(worker: ""sy.workers.AbstractWorker"", e):\n        """"""\n        Serialize information about an Exception which was raised to forward it\n        """"""\n        # Get information about the exception: type of error,  traceback\n        tp = type(e)\n        tb = e.__traceback__\n        # Serialize the traceback\n        traceback_str = ""Traceback (most recent call last):\\n"" + """".join(traceback.format_tb(tb))\n        # Include special attributes if relevant\n        try:\n            attributes = e.get_attributes()\n        except AttributeError:\n            attributes = {}\n        return (\n            sy.serde.msgpack.serde._simplify(worker, tp.__name__),\n            sy.serde.msgpack.serde._simplify(worker, traceback_str),\n            sy.serde.msgpack.serde._simplify(worker, attributes),\n        )\n\n    @staticmethod\n    def detail(worker: ""sy.workers.AbstractWorker"", error_tuple: Tuple[str, str, dict]):\n        """"""\n        Detail and re-raise an Exception forwarded by another worker\n        """"""\n        error_name, traceback_str, attributes = error_tuple\n        error_name = sy.serde.msgpack.serde._detail(worker, error_name)\n        traceback_str = sy.serde.msgpack.serde._detail(worker, traceback_str)\n        attributes = sy.serde.msgpack.serde._detail(worker, attributes)\n        # De-serialize the traceback\n        tb = Traceback.from_string(traceback_str)\n        # Check that the error belongs to a valid set of Exceptions\n        if error_name in dir(sy.exceptions):\n            error_type = getattr(sy.exceptions, error_name)\n            error = error_type()\n            # Include special attributes if any\n            for attr_name, attr in attributes.items():\n                setattr(error, attr_name, attr)\n            reraise(error_type, error, tb.as_traceback())\n        else:\n            raise ValueError(f""Invalid Exception returned:\\n{traceback_str}"")\n\n\nclass SendNotPermittedError(Exception):\n    """"""Raised when calling send on a tensor which does not allow\n    send to be called on it. This can happen do to sensitivity being too high""""""\n\n    @staticmethod\n    def simplify(worker: ""sy.workers.AbstractWorker"", e):\n        """"""\n        Serialize information about an Exception which was raised to forward it\n        """"""\n        # Get information about the exception: type of error,  traceback\n        tp = type(e)\n        tb = e.__traceback__\n        # Serialize the traceback\n        traceback_str = ""Traceback (most recent call last):\\n"" + """".join(traceback.format_tb(tb))\n        # Include special attributes if relevant\n        try:\n            attributes = e.get_attributes()\n        except AttributeError:\n            attributes = {}\n        return tp.__name__, traceback_str, sy.serde.msgpack.serde._simplify(worker, attributes)\n\n    @staticmethod\n    def detail(worker: ""sy.workers.AbstractWorker"", error_tuple: Tuple[str, str, dict]):\n        """"""\n        Detail and re-raise an Exception forwarded by another worker\n        """"""\n        error_name, traceback_str, attributes = error_tuple\n        error_name, traceback_str = error_name.decode(""utf-8""), traceback_str.decode(""utf-8"")\n        attributes = sy.serde.msgpack.serde._detail(worker, attributes)\n        # De-serialize the traceback\n        tb = Traceback.from_string(traceback_str)\n        # Check that the error belongs to a valid set of Exceptions\n        if error_name in dir(sy.exceptions):\n            error_type = getattr(sy.exceptions, error_name)\n            error = error_type()\n            # Include special attributes if any\n            for attr_name, attr in attributes.items():\n                setattr(error, attr_name, attr)\n            reraise(error_type, error, tb.as_traceback())\n        else:\n            raise ValueError(f""Invalid Exception returned:\\n{traceback_str}"")\n\n\nclass GetNotPermittedError(Exception):\n    """"""Raised when calling get on a pointer to a tensor which does not allow\n    get to be called on it. This can happen do to sensitivity being too high""""""\n\n    @staticmethod\n    def simplify(worker: ""sy.workers.AbstractWorker"", e):\n        """"""\n        Serialize information about an Exception which was raised to forward it\n        """"""\n        # Get information about the exception: type of error,  traceback\n        tp = type(e)\n        tb = e.__traceback__\n        # Serialize the traceback\n        traceback_str = ""Traceback (most recent call last):\\n"" + """".join(traceback.format_tb(tb))\n        # Include special attributes if relevant\n        try:\n            attributes = e.get_attributes()\n        except AttributeError:\n            attributes = {}\n        return (\n            sy.serde.msgpack.serde._simplify(worker, tp.__name__),\n            sy.serde.msgpack.serde._simplify(worker, traceback_str),\n            sy.serde.msgpack.serde._simplify(worker, attributes),\n        )\n\n    @staticmethod\n    def detail(worker: ""sy.workers.AbstractWorker"", error_tuple: Tuple[str, str, dict]):\n        """"""\n        Detail and re-raise an Exception forwarded by another worker\n        """"""\n        error_name, traceback_str, attributes = error_tuple\n        error_name = sy.serde.msgpack.serde._detail(worker, error_name)\n        traceback_str = sy.serde.msgpack.serde._detail(worker, traceback_str)\n        attributes = sy.serde.msgpack.serde._detail(worker, attributes)\n        # De-serialize the traceback\n        tb = Traceback.from_string(traceback_str)\n        # Check that the error belongs to a valid set of Exceptions\n        if error_name in dir(sy.exceptions):\n            error_type = getattr(sy.exceptions, error_name)\n            error = error_type()\n            # Include special attributes if any\n            for attr_name, attr in attributes.items():\n                setattr(error, attr_name, attr)\n            reraise(error_type, error, tb.as_traceback())\n        else:\n            raise ValueError(f""Invalid Exception returned:\\n{traceback_str}"")\n\n\nclass IdNotUniqueError(Exception):\n    """"""Raised by the ID Provider when setting ids that have already been generated""""""\n\n    pass\n\n\nclass PlanCommandUnknownError(Exception):\n    """"""Raised when an unknown plan command execution is requested.""""""\n\n    def __init__(self, command_name: object):\n        message = f""Command {command_name} is not implemented.""\n        super().__init__(message)\n\n\nclass ObjectNotFoundError(Exception):\n    """"""Raised when object with given object id is not found on worker\n\n    Attributes:\n        obj_id -- id of the object with which the interaction is attempted\n        worker -- virtual worker on which the interaction is attempted\n    """"""\n\n    def __init__(self, obj_id, worker):\n        message = """"\n        message += \'Object ""\' + str(obj_id) + \'"" not found on worker! \'\n        message += (\n            f""You just tried to interact with an object ID: {obj_id} on {worker} ""\n            ""which does not exist!!!""\n        )\n        message += (\n            ""Use .send() and .get() on all your tensors to make sure they\'re ""\n            ""on the same machines. If you think this tensor does exist, check ""\n            ""the object_store._objects dict on the worker and see for yourself. ""\n            ""The most common reason this error happens is because someone calls ""\n            "".get() on the object\'s pointer without realizing it (which deletes ""\n            ""the remote object and sends it to the pointer). Check your code to ""\n            ""make sure you haven\'t already called .get() on this pointer!""\n        )\n        super().__init__(message)\n\n\nclass InvalidProtocolFileError(Exception):\n    """"""Raised when PySyft protocol file cannot be loaded.""""""\n\n    pass\n\n\nclass UndefinedProtocolTypeError(Exception):\n    """"""Raised when trying to serialize type that is not defined in protocol file.""""""\n\n    pass\n\n\nclass UndefinedProtocolTypePropertyError(Exception):\n    """"""Raised when trying to get protocol type property that is not defined in protocol file.""""""\n\n    pass\n\n\nclass EmptyCryptoPrimitiveStoreError(Exception):\n    """"""Raised when trying to get crypto primtives from an empty crypto store""""""\n\n    def __init__(self, crypto_store, crypto_type, available_instances, n_instances):\n        message = (\n            f""You tried to run a crypto protocol on worker {crypto_store._owner.id} ""\n            f""but its crypto_store doesn\'t have enough primitives left for the type ""\n            f""\'{crypto_type}\' ({n_instances} were requested while only {available_instances}""\n            f"" are available). Use your crypto_provider to `provide_primitives` to your ""\n            f""worker.""\n        )\n        super().__init__(message)\n\n\ndef route_method_exception(exception, self, args_, kwargs_):  # noqa: C901\n    try:\n        if self.is_wrapper:\n            if isinstance(self.child, sy.PointerTensor):\n                if len(args_) > 0:\n                    if not args_[0].is_wrapper:\n                        return TensorsNotCollocatedException(self, args_[0])\n                    elif isinstance(args_[0].child, sy.PointerTensor):\n                        if self.location != args_[0].child.location:\n                            return TensorsNotCollocatedException(self, args_[0])\n\n        # if self is a normal tensor\n        elif isinstance(self, FrameworkTensor):\n            if len(args_) > 0:\n                if args_[0].is_wrapper:\n                    if isinstance(args_[0].child, sy.PointerTensor):\n                        return TensorsNotCollocatedException(self, args_[0])\n                elif isinstance(args_[0], sy.PointerTensor):\n                    return TensorsNotCollocatedException(self, args_[0])\n    except:  # noqa: E722\n        """"\n    return exception\n'"
syft/sandbox.py,3,"b'import importlib\n\nfrom syft.frameworks.torch.hook.hook import TorchHook\nfrom syft.workers.virtual import VirtualWorker\nfrom syft.grid.private_grid import PrivateGridNetwork\n\nfrom syft.exceptions import DependencyError\n\n\ndef create_sandbox(gbs, verbose=True, download_data=True):  # noqa: C901\n    """"""There\'s some boilerplate stuff that most people who are\n    just playing around would like to have. This will create\n    that for you""""""\n\n    try:\n        torch = gbs[""torch""]\n    except KeyError:\n        torch = gbs[""th""]\n\n    global hook\n    global bob\n    global theo\n    global alice\n    global andy\n    global jason\n    global jon\n\n    if download_data and importlib.util.find_spec(""sklearn"") is None:\n        raise DependencyError(""sklearn"", ""scikit-learn"")\n\n    if download_data:  # pragma: no cover\n        from sklearn.datasets import load_boston\n        from sklearn.datasets import load_breast_cancer\n        from sklearn.datasets import load_digits\n        from sklearn.datasets import load_diabetes\n        from sklearn.datasets import load_iris\n        from sklearn.datasets import load_wine\n        from sklearn.datasets import load_linnerud\n\n        def load_sklearn(func, *tags):\n            dataset = func()\n            data = (\n                torch.tensor(dataset[""data""])\n                .float()\n                .tag(*(list(tags) + [""#data""] + dataset[""DESCR""].split(""\\n"")[0].lower().split("" "")))\n                .describe(dataset[""DESCR""])\n            )\n            target = (\n                torch.tensor(dataset[""target""])\n                .float()\n                .tag(\n                    *(list(tags) + [""#target""] + dataset[""DESCR""].split(""\\n"")[0].lower().split("" ""))\n                )\n                .describe(dataset[""DESCR""])\n            )\n\n            return data, target\n\n        def distribute_dataset(data, workers):\n            batch_size = int(data.shape[0] / len(workers))\n            n_batches = len(workers)\n            for batch_i in range(n_batches - 1):\n                batch = data[batch_i * batch_size : (batch_i + 1) * batch_size]\n                batch.tags = data.tags\n                batch.description = data.description\n                ptr = batch.send(workers[batch_i])\n                ptr.child.garbage_collect_data = False\n\n            batch = data[(n_batches - 1) * batch_size :]\n            batch.tags = data.tags\n            batch.description = data.description\n            ptr = batch.send(workers[n_batches - 1])\n            ptr.child.garbage_collect_data = False\n\n    print(""Setting up Sandbox..."")\n\n    if verbose:\n        print(""\\t- Hooking PyTorch"")\n    hook = TorchHook(torch)\n\n    if verbose:\n        print(""\\t- Creating Virtual Workers:"")\n        print(""\\t\\t- bob"")\n    bob = VirtualWorker(hook, id=""bob"")\n    if verbose:\n        print(""\\t\\t- theo"")\n    theo = VirtualWorker(hook, id=""theo"")\n    if verbose:\n        print(""\\t\\t- jason"")\n    jason = VirtualWorker(hook, id=""jason"")\n    if verbose:\n        print(""\\t\\t- alice"")\n    alice = VirtualWorker(hook, id=""alice"")\n    if verbose:\n        print(""\\t\\t- andy"")\n    andy = VirtualWorker(hook, id=""andy"")\n    if verbose:\n        print(""\\t\\t- jon"")\n    jon = VirtualWorker(hook, id=""jon"")\n\n    if verbose:\n        print(""\\tStoring hook and workers as global variables..."")\n    gbs[""hook""] = hook\n    gbs[""bob""] = bob\n    gbs[""theo""] = theo\n    gbs[""jason""] = jason\n    gbs[""alice""] = alice\n    gbs[""andy""] = andy\n    gbs[""jon""] = jon\n\n    gbs[""workers""] = [bob, theo, jason, alice, andy, jon]\n\n    if download_data:  # pragma: no cover\n\n        if verbose:\n            print(""\\tLoading datasets from SciKit Learn..."")\n            print(""\\t\\t- Boston Housing Dataset"")\n        boston = load_sklearn(load_boston, *[""#boston"", ""#housing"", ""#boston_housing""])\n        if verbose:\n            print(""\\t\\t- Diabetes Dataset"")\n        diabetes = load_sklearn(load_diabetes, *[""#diabetes""])\n        if verbose:\n            print(""\\t\\t- Breast Cancer Dataset"")\n        breast_cancer = load_sklearn(load_breast_cancer)\n        if verbose:\n            print(""\\t- Digits Dataset"")\n        digits = load_sklearn(load_digits)\n        if verbose:\n            print(""\\t\\t- Iris Dataset"")\n        iris = load_sklearn(load_iris)\n        if verbose:\n            print(""\\t\\t- Wine Dataset"")\n        wine = load_sklearn(load_wine)\n        if verbose:\n            print(""\\t\\t- Linnerud Dataset"")\n        linnerud = load_sklearn(load_linnerud)\n\n        workers = [bob, theo, jason, alice, andy, jon]\n\n        if verbose:\n            print(""\\tDistributing Datasets Amongst Workers..."")\n        distribute_dataset(boston[0], workers)\n        distribute_dataset(boston[1], workers)\n        distribute_dataset(diabetes[0], workers)\n        distribute_dataset(diabetes[1], workers)\n        distribute_dataset(breast_cancer[0], workers)\n        distribute_dataset(breast_cancer[1], workers)\n        distribute_dataset(digits[0], workers)\n        distribute_dataset(digits[1], workers)\n        distribute_dataset(iris[0], workers)\n        distribute_dataset(iris[1], workers)\n        distribute_dataset(wine[0], workers)\n        distribute_dataset(wine[1], workers)\n        distribute_dataset(linnerud[0], workers)\n        distribute_dataset(linnerud[1], workers)\n\n    if verbose:\n        print(""\\tCollecting workers into a VirtualGrid..."")\n    _grid = PrivateGridNetwork(*gbs[""workers""])\n    gbs[""grid""] = _grid\n\n    print(""Done!"")\n\n\ndef make_hook(gbs):\n    return create_sandbox(gbs, False, False)\n'"
syft/version.py,0,"b'__version__ = ""0.2.6""\n'"
test/__init__.py,0,b''
test/conftest.py,0,"b'import builtins\nfrom multiprocessing import Process\nimport sys\nimport time\nimport os\nimport shutil\nimport tempfile\n\nimport pytest\nimport torch\n\nimport syft\nfrom syft import TorchHook\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.workers.websocket_client import WebsocketClientWorker\nfrom syft.workers.websocket_server import WebsocketServerWorker\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\n        ""markers"", ""translation: mark test to run only as part of the translation test suite""\n    )\n\n\ndef pytest_sessionstart(session):\n    session.failed_tests = set()\n\n\ndef pytest_runtest_makereport(item, call):  # pragma: no cover\n    if call.excinfo is not None and item.originalname:\n        item.session.failed_tests.add(item.originalname)\n\n\ndef pytest_runtest_setup(item):  # pragma: no cover\n    if item.originalname in item.session.failed_tests:\n        pytest.skip(f""previous test failed ({item.name})"")\n\n\ndef _start_proc(participant, dataset: str = None, **kwargs):  # pragma: no cover\n    """"""Helper function for spinning up a websocket participant.""""""\n\n    def target():\n        server = participant(**kwargs)\n        if dataset is not None:\n            data, key = dataset\n            server.add_dataset(data, key=key)\n        server.start()\n\n    p = Process(target=target)\n    p.start()\n    return p\n\n\ndef instantiate_websocket_client_worker(max_tries=5, sleep_time=0.1, **kwargs):  # pragma: no cover\n    """"""Helper function to instantiate the websocket client.\n\n    If a connection is refused, we wait a bit (`sleep_time` seconds) and try again.\n    After `max_tries` failed tries, a ConnectionRefusedError is raised.\n    """"""\n    retry_counter = 0\n    connection_open = False\n    while not connection_open:\n        try:\n            remote_proxy = WebsocketClientWorker(**kwargs)\n            connection_open = True\n        except ConnectionRefusedError as e:\n            if retry_counter < max_tries:\n                retry_counter += 1\n                time.sleep(sleep_time)\n            else:\n                raise e\n    return remote_proxy\n\n\n@pytest.fixture()\ndef start_proc():  # pragma: no cover\n    return _start_proc\n\n\n@pytest.fixture()\ndef start_remote_worker():  # pragma: no cover\n    """"""Helper function for starting a websocket worker.""""""\n\n    def _start_remote_worker(\n        id, hook, dataset: str = None, host=""0.0.0.0"", port=8768, max_tries=5, sleep_time=0.01\n    ):\n        kwargs = {""id"": id, ""host"": host, ""port"": port, ""hook"": hook}\n        server = _start_proc(WebsocketServerWorker, dataset=dataset, **kwargs)\n        remote_proxy = instantiate_websocket_client_worker(\n            max_tries=max_tries, sleep_time=sleep_time, **kwargs\n        )\n\n        return server, remote_proxy\n\n    return _start_remote_worker\n\n\n@pytest.fixture()\ndef start_remote_server_worker_only():  # pragma: no cover\n    """"""Helper function for starting a websocket worker.""""""\n\n    def _start_remote_worker(\n        id, hook, dataset: str = None, host=""localhost"", port=8768, max_tries=5, sleep_time=0.01\n    ):\n        kwargs = {""id"": id, ""host"": host, ""port"": port, ""hook"": hook}\n        server = _start_proc(WebsocketServerWorker, dataset=dataset, **kwargs)\n\n        return server\n\n    return _start_remote_worker\n\n\n# This fixture is only used by the notebook tests, which run separately from the\n# test coverage checker in CI and are thus excluded from the coverage checks.\n@pytest.yield_fixture(scope=""function"")\ndef isolated_filesystem():  # pragma: no cover\n    """"""A context manager that creates a temporary folder and changes\n    the current working directory to it for isolated filesystem tests.\n    """"""\n    cwd = os.getcwd()\n    t = tempfile.mkdtemp()\n    shutil.copytree(""examples/tutorials/"", t + ""/examples"")\n    # Path(t + ""/data/"").mkdir(parents=True, exist_ok=True)\n    shutil.copytree(""examples/data/"", t + ""/data/"")\n    os.chdir(t + ""/examples"")\n    try:\n        yield t\n    finally:\n        os.chdir(cwd)\n        shutil.rmtree(t)\n\n\n@pytest.fixture(scope=""session"", autouse=True)\ndef hook():\n    hook = TorchHook(torch)\n    return hook\n\n\n@pytest.fixture(scope=""function"", autouse=True)\ndef workers(hook):\n    # To run a plan locally the local worker can\'t be a client worker,\n    # since it needs to register objects\n    # LaRiffle edit: doing this increases the reference count on pointers and\n    # breaks the auto garbage collection for pointer of pointers, see #2150\n    # hook.local_worker.is_client_worker = False\n\n    # Reset the hook and the local worker\n    syft.local_worker.clear_objects()\n    hook_args.hook_method_args_functions = {}\n    hook_args.hook_method_response_functions = {}\n    hook_args.register_response_functions = {}\n    hook_args.get_tensor_type_functions = {}\n\n    # Define 4 virtual workers\n    alice = syft.VirtualWorker(id=""alice"", hook=hook, is_client_worker=False)\n    bob = syft.VirtualWorker(id=""bob"", hook=hook, is_client_worker=False)\n    charlie = syft.VirtualWorker(id=""charlie"", hook=hook, is_client_worker=False)\n    james = syft.VirtualWorker(id=""james"", hook=hook, is_client_worker=False)\n\n    workers = {\n        ""me"": hook.local_worker,\n        ""alice"": alice,\n        ""bob"": bob,\n        ""charlie"": charlie,\n        ""james"": james,\n    }\n\n    yield workers\n\n    alice.remove_worker_from_local_worker_registry()\n    bob.remove_worker_from_local_worker_registry()\n    charlie.remove_worker_from_local_worker_registry()\n    james.remove_worker_from_local_worker_registry()\n\n\n@pytest.fixture\ndef hide_module():\n    import_orig = builtins.__import__\n    # When we check for imports in dependency_check, we don\'t actually attempt\n    # to import each package, so popping a module from sys.modules and mocking\n    # the import statement is not sufficient to simulate the dependency check\n    # for when the dependency is absent. The way we check for dependencies\n    # (importlib.util.find_spec) uses module Finders in the sys.meta_path when\n    # checking for module specs, so we need to mock the find_spec method of the\n    # Finder that will discover the module we want to hide. That Finder happens\n    # to be in position three of the meta path.\n    find_spec_orig = sys.meta_path[3].find_spec\n\n    def mocked_import(name, globals, locals, fromlist, level):\n        if name in [""tensorflow"", ""tf_encrypted"", ""torch""]:\n            raise ImportError()\n\n        return import_orig(name, globals, locals, fromlist, level)\n\n    def mocked_find_spec(self, fullname, target=None):\n        if self in [""tensorflow"", ""tf_encrypted""]:\n            return None\n        return find_spec_orig(self, fullname, target)\n\n    builtins.__import__ = mocked_import\n    sys.meta_path[3].find_spec = mocked_find_spec\n    yield\n    builtins.__import__ = import_orig\n    sys.meta_path[3].find_spec = find_spec_orig\n'"
test/run_websocket_server.py,0,"b'import subprocess\nimport sys\nfrom pathlib import Path\nimport argparse\n\nif __name__ == ""__main__"":\n\n    # Parse args\n    parser = argparse.ArgumentParser(description=""Run websocket server worker."")\n    parser.add_argument(\n        ""--port"",\n        ""-p"",\n        type=int,\n        help=""port number of the websocket server worker, e.g. --port 8777"",\n    )\n    parser.add_argument(""--host"", type=str, default=""localhost"", help=""host for the connection"")\n    parser.add_argument(\n        ""--id"", type=str, help=""name (id) of the websocket server worker, e.g. --id alice""\n    )\n    parser.add_argument(\n        ""--testing"",\n        action=""store_true"",\n        help=(\n            ""if set, websocket server worker will load the test dataset ""\n            ""instead of the training dataset"",\n        ),\n    )\n    parser.add_argument(\n        ""--verbose"",\n        ""-v"",\n        action=""store_true"",\n        help=""""""if set, websocket server worker will be started in verbose mode"""""",\n    )\n    parser.add_argument(\n        ""--notebook"",\n        type=str,\n        default=""normal"",\n        help=""""""can run websocket server for websockets examples of mnist/mnist-parallel or\n        pen_testing/steal_data_over_sockets. Type \'mnist\' for starting server\n        for websockets-example-MNIST, `mnist-parallel` for websockets-example-MNIST-parallel\n        and \'steal_data\' for pen_tesing stealing data over sockets"""""",\n    )\n    parser.add_argument(""--pytest_testing"", action=""store_true"", help=""""""Used for pytest testing"""""")\n    args = parser.parse_args()\n\n    python = Path(sys.executable).name\n    FILE_PATH = Path(__file__).resolve().parents[1].joinpath(""run_websocket_server.py"")\n    call_alice = [\n        python,\n        FILE_PATH,\n        ""--host"",\n        args.host,\n        ""--port"",\n        str(args.port),\n        ""--id"",\n        args.id,\n        ""--pytest_testing"",\n    ]\n\n    if args.verbose:\n        call_alice.append(""--verbose"")\n\n    if args.testing:\n        call_alice.append(""--testing"")\n\n    subprocess.Popen(call_alice)\n'"
test/test_dependency_check.py,0,"b'import sys\nimport pytest\nfrom syft import dependency_check\n\n\n@pytest.mark.skipif(not dependency_check.tensorflow_available, reason=""tf 2.0+ not installed"")\ndef test_tensorflow_available():  # pragma: no cover\n    sys.modules.pop(""syft"", None)\n    sys.modules.pop(""syft.dependency_check"", None)\n    from syft import dependency_check\n\n    assert dependency_check.tensorflow_available\n\n\n@pytest.mark.skipif(not dependency_check.tfe_available, reason=""tf_encrypted not installed"")\ndef test_tf_encrypted_available():  # pragma: no cover\n    sys.modules.pop(""syft"", None)\n    sys.modules.pop(""syft.dependency_check"", None)\n    from syft import dependency_check\n\n    assert dependency_check.tfe_available\n\n\n@pytest.mark.skipif(not dependency_check.torch_available, reason=""torch not installed"")\ndef test_torch_available():  # pragma: no cover\n    sys.modules.pop(""syft"", None)\n    sys.modules.pop(""syft.dependency_check"", None)\n    from syft import dependency_check\n\n    assert dependency_check.torch_available\n\n\n@pytest.mark.usefixtures(""hide_module"")\ndef test_tensorflow_missing():  # pragma: no cover\n    sys.modules.pop(""syft"", None)\n    sys.modules.pop(""syft.dependency_check"", None)\n    sys.modules.pop(""tensorflow"", None)\n    sys.modules.pop(""tf"", None)\n    import syft.dependency_check\n\n    assert not syft.dependency_check.tensorflow_available\n\n\n@pytest.mark.usefixtures(""hide_module"")\ndef test_tf_encrypted_missing():  # pragma: no cover\n    sys.modules.pop(""syft.dependency_check"", None)\n    sys.modules.pop(""tf_encrypted"", None)\n    sys.modules.pop(""tfe"", None)\n    import syft.dependency_check\n\n    assert not syft.dependency_check.tfe_available\n'"
test/test_exceptions.py,0,"b'import pytest\nimport torch as th\nimport syft as sy\n\n\ndef test_tensors_not_collated_exception(workers):\n    """"""\n    Ensure that the sy.combine_pointers works as expected\n    """"""\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    y = th.tensor([1, 2, 3, 4, 5]).send(alice)\n\n    with pytest.raises(sy.exceptions.TensorsNotCollocatedException):\n        b = x + y\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(alice)\n    y = th.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    with pytest.raises(sy.exceptions.TensorsNotCollocatedException):\n        b = x + y\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(alice)\n    y = th.tensor([1, 2, 3, 4, 5])\n\n    with pytest.raises(sy.exceptions.TensorsNotCollocatedException):\n        b = x + y\n\n    x = th.tensor([1, 2, 3, 4, 5])\n    y = th.tensor([1, 2, 3, 4, 5]).send(alice)\n\n    with pytest.raises(sy.exceptions.TensorsNotCollocatedException):\n        b = x + y\n'"
test/test_grid.py,3,"b'import torch\nimport syft as sy\n\n\ndef test_virtual_grid(workers):\n    """"""This tests our ability to simplify tuple types.\n\n    This test is pretty simple since tuples just serialize to\n    themselves, with a tuple wrapper with the correct ID (1)\n    for tuples so that the detailer knows how to interpret it.""""""\n\n    print(len(workers))\n    print(workers)\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    james = workers[""james""]\n\n    grid = sy.PrivateGridNetwork(*[bob, alice, james])\n\n    x = torch.tensor([1, 2, 3, 4]).tag(""#bob"", ""#male"").send(bob)\n    y = torch.tensor([1, 2, 3, 4]).tag(""#alice"", ""#female"").send(alice)\n    z = torch.tensor([1, 2, 3, 4]).tag(""#james"", ""#male"").send(james)\n\n    results = grid.search()\n    assert len(results) == 3\n\n    assert ""bob"" in results.keys()\n    assert ""alice"" in results.keys()\n    assert ""james"" in results.keys()\n\n    results = grid.search(""#bob"")\n    assert len(results[""bob""]) == 1\n    assert ""alice"" not in results\n    assert ""james"" not in results\n\n    results = grid.search(""#male"")\n    assert len(results[""bob""]) == 1\n    assert ""alice"" not in results\n    assert len(results[""james""]) == 1\n'"
test/test_local_worker.py,0,"b'import torch as th\n\n\ndef test_client_worker_does_not_register_object(hook):\n    me = hook.local_worker\n    me.is_client_worker = True\n    x = th.tensor([1, 2, 3])\n    assert x.id not in me.object_store._objects\n\n\ndef test_object_registration(hook):\n    me = hook.local_worker\n    me.is_client_worker = False\n    x = th.tensor([1, 2, 3])\n    assert x.id in me.object_store._objects\n\n    me.is_client_worker = True\n\n\ndef test_pointer_registration(workers, hook):\n    alice = workers[""alice""]\n    me = hook.local_worker\n    me.is_client_worker = False\n    x_ptr = th.tensor([1, 2, 3]).send(alice)\n    assert x_ptr.id in me.object_store._objects\n\n    me.is_client_worker = True\n\n\ndef test_fix_prec_tensor_registration(hook):\n    me = hook.local_worker\n    me.is_client_worker = False\n    x_sh = th.tensor([1.0, 2, 3]).fix_prec()\n    assert x_sh.id in me.object_store._objects\n\n    me.is_client_worker = True\n\n\ndef test_shared_tensor_registration(workers, hook):\n    alice, bob, charlie = workers[""alice""], workers[""bob""], workers[""charlie""]\n    me = hook.local_worker\n    me.is_client_worker = False\n    x_sh = th.tensor([1.0, 2, 3]).fix_prec().share(alice, bob, crypto_provider=charlie)\n    assert x_sh.id in me.object_store._objects\n\n    me.is_client_worker = True\n\n\ndef test_shared_tensor_registration_pointer(workers, hook):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    me = hook.local_worker\n    me.is_client_worker = False\n    x_ptr = th.tensor([1, 2, 3]).send(alice)\n    x_sh = x_ptr.fix_prec().share(bob, charlie, crypto_provider=james)\n    assert x_sh.id in me.object_store._objects\n\n    me.is_client_worker = True\n\n\ndef test_in_known_workers(hook):\n    # Get local worker\n    local_worker = hook.local_worker\n    id = local_worker.id\n\n    # Get known workers dict\n    known_workers = local_worker._known_workers\n\n    assert id in known_workers and local_worker == known_workers[id]\n'"
test/test_sandbox.py,1,"b'import syft as sy\nimport torch  # noqa: F401\n\n# Import Hook\nfrom syft.frameworks.torch.hook.hook import TorchHook\n\n# Import grids\nfrom syft.grid.private_grid import PrivateGridNetwork\n\n\ndef test_sandbox():\n    sy.create_sandbox(globals(), download_data=False)\n\n    assert alice == alice  # noqa: F821\n    assert isinstance(alice, sy.VirtualWorker)  # noqa: F821\n    assert andy == andy  # noqa: F821\n    assert isinstance(andy, sy.VirtualWorker)  # noqa: F821\n    assert bob == bob  # noqa: F821\n    assert isinstance(bob, sy.VirtualWorker)  # noqa: F821\n    assert jason == jason  # noqa: F821\n    assert isinstance(jason, sy.VirtualWorker)  # noqa: F821\n    assert jon == jon  # noqa: F821\n    assert isinstance(jon, sy.VirtualWorker)  # noqa: F821\n    assert theo == theo  # noqa: F821\n    assert isinstance(theo, sy.VirtualWorker)  # noqa: F821\n\n    assert hook == hook  # noqa: F821\n    assert isinstance(hook, TorchHook)  # noqa: F821\n\n    assert grid == grid  # noqa: F821\n    assert isinstance(grid, PrivateGridNetwork)  # noqa: F821\n\n    assert workers == [bob, theo, jason, alice, andy, jon]  # noqa: F821\n'"
test/test_udacity.py,17,"b'import torch\n\n\ndef test_section_1_differential_privacy():\n    """"""This tests the Udacity course content found at\n    https://github.com/Udacity/private-ai\n    """"""\n\n    # the number of entries in our database\n    num_entries = 5000\n\n    db = torch.rand(num_entries) > 0.5\n\n    db = torch.rand(num_entries) > 0.5\n\n    def get_parallel_db(db, remove_index):\n        return torch.cat((db[0:remove_index], db[remove_index + 1 :]))\n\n    get_parallel_db(db, 52352)\n\n    def get_parallel_dbs(db):\n        parallel_dbs = []\n\n        for i in range(len(db)):\n            pdb = get_parallel_db(db, i)\n            parallel_dbs.append(pdb)\n\n        return parallel_dbs\n\n    pdbs = get_parallel_dbs(db)\n\n    def create_db_and_parallels(num_entries):\n        db = torch.rand(num_entries) > 0.5\n        pdbs = get_parallel_dbs(db)\n\n        return db, pdbs\n\n    db, pdbs = create_db_and_parallels(20)\n\n    db, pdbs = create_db_and_parallels(5000)\n\n    def query(db):\n        return db.sum()\n\n    full_db_result = query(db)\n\n    sensitivity = 0\n    for pdb in pdbs:\n        pdb_result = query(pdb)\n\n        db_distance = torch.abs(pdb_result - full_db_result)\n\n        if db_distance > sensitivity:\n            sensitivity = db_distance\n\n    def sensitivity(query, n_entries=1000):\n\n        db, pdbs = create_db_and_parallels(n_entries)\n\n        full_db_result = query(db)\n\n        max_distance = 0\n        for pdb in pdbs:\n            pdb_result = query(pdb)\n\n            db_distance = torch.abs(pdb_result - full_db_result)\n\n            if db_distance > max_distance:\n                max_distance = db_distance\n\n        return max_distance\n\n    def query(db):\n        return db.float().mean()\n\n    sensitivity(query)\n\n    db, pdbs = create_db_and_parallels(20)\n\n    db\n\n    def query(db, threshold=5):\n        return (db.sum() > threshold).float()\n\n    for i in range(10):\n        sens_f = sensitivity(query, n_entries=10)\n        print(sens_f)\n\n    db, _ = create_db_and_parallels(100)\n\n    pdb = get_parallel_db(db, remove_index=10)\n\n    db[10]\n\n    sum(db)\n\n    # differencing attack using sum query\n\n    sum(db) - sum(pdb)\n\n    # differencing attack using mean query\n\n    (sum(db).float() / len(db)) - (sum(pdb).float() / len(pdb))\n\n    # differencing attack using threshold\n\n    (sum(db).float() > 49).float() - (sum(pdb).float() > 49).float()\n\n    def query(db):\n\n        true_result = torch.mean(db.float())\n\n        first_coin_flip = (torch.rand(len(db)) > 0.5).float()\n        second_coin_flip = (torch.rand(len(db)) > 0.5).float()\n\n        augmented_database = db.float() * first_coin_flip + (1 - first_coin_flip) * second_coin_flip\n\n        db_result = torch.mean(augmented_database.float()) * 2 - 0.5\n\n        return db_result, true_result\n\n    db, pdbs = create_db_and_parallels(10)\n    private_result, true_result = query(db)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(100)\n    private_result, true_result = query(db)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(1000)\n    private_result, true_result = query(db)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(10000)\n    private_result, true_result = query(db)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    def query(db, noise=0.2):\n\n        true_result = torch.mean(db.float())\n\n        first_coin_flip = (torch.rand(len(db)) > noise).float()\n        second_coin_flip = (torch.rand(len(db)) > 0.5).float()\n\n        augmented_database = db.float() * first_coin_flip + (1 - first_coin_flip) * second_coin_flip\n\n        sk_result = augmented_database.float().mean()\n\n        private_result = ((sk_result / noise) - 0.5) * noise / (1 - noise)\n\n        return private_result, true_result\n\n    db, pdbs = create_db_and_parallels(100)\n    private_result, true_result = query(db, noise=0.1)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(100)\n    private_result, true_result = query(db, noise=0.2)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(100)\n    private_result, true_result = query(db, noise=0.4)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(100)\n    private_result, true_result = query(db, noise=0.8)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(10000)\n    private_result, true_result = query(db, noise=0.8)\n    print(""With Noise:"" + str(private_result))\n    print(""Without Noise:"" + str(true_result))\n\n    db, pdbs = create_db_and_parallels(100)\n\n    def query(db):\n        return torch.sum(db.float())\n\n    # def M(db):\n    #     query(db)  # + noise\n    #\n    query(db)\n\n    epsilon = 0.0001\n\n    import numpy as np\n\n    db, pdbs = create_db_and_parallels(100)\n\n    def sum_query(db):\n        return db.sum()\n\n    def laplacian_mechanism(db, query, sensitivity):\n\n        beta = sensitivity / epsilon\n        noise = torch.tensor(np.random.laplace(0, beta, 1))\n\n        return query(db) + noise\n\n    def mean_query(db):\n        return torch.mean(db.float())\n\n    laplacian_mechanism(db, sum_query, 1)\n\n    laplacian_mechanism(db, mean_query, 1 / 100)\n\n    import numpy as np\n\n    num_teachers = 10  # we\'re working with 10 partner hospitals\n    num_examples = 10000  # the size of OUR dataset\n    num_labels = 10  # number of lablels for our classifier\n\n    preds = (\n        (np.random.rand(num_teachers, num_examples) * num_labels).astype(int).transpose(1, 0)\n    )  # fake predictions\n\n    new_labels = []\n    for an_image in preds:\n\n        label_counts = np.bincount(an_image, minlength=num_labels)\n\n        epsilon = 0.1\n        beta = 1 / epsilon\n\n        for i in range(len(label_counts)):\n            label_counts[i] += np.random.laplace(0, beta, 1)\n\n        new_label = np.argmax(label_counts)\n\n        new_labels.append(new_label)\n\n    labels = np.array([9, 9, 3, 6, 9, 9, 9, 9, 8, 2])\n    counts = np.bincount(labels, minlength=10)\n    query_result = np.argmax(counts)\n    query_result\n\n    from syft.frameworks.torch.dp import pate\n\n    num_teachers, num_examples, num_labels = (100, 100, 10)\n    preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int)  # fake preds\n    indices = (np.random.rand(num_examples) * num_labels).astype(int)  # true answers\n\n    preds[:, 0:10] *= 0\n\n    data_dep_eps, data_ind_eps = pate.perform_analysis(\n        teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5\n    )\n\n    assert data_dep_eps < data_ind_eps\n\n    data_dep_eps, data_ind_eps = pate.perform_analysis(\n        teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5\n    )\n    print(""Data Independent Epsilon:"", data_ind_eps)\n    print(""Data Dependent Epsilon:"", data_dep_eps)\n\n    preds[:, 0:50] *= 0\n\n    data_dep_eps, data_ind_eps = pate.perform_analysis(\n        teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5, moments=20\n    )\n    print(""Data Independent Epsilon:"", data_ind_eps)\n    print(""Data Dependent Epsilon:"", data_dep_eps)\n\n\ndef test_section_2_federated_learning(hook):\n    """"""This tests the Udacity course content found at\n    https://github.com/Udacity/private-ai\n    """"""\n\n    import torch as th\n\n    x = th.tensor([1, 2, 3, 4, 5])\n    x\n\n    y = x + x\n\n    print(y)\n\n    import syft as sy\n\n    # commented out because the test needs to use the global one\n    # hook = sy.TorchHook(th)\n\n    th.tensor([1, 2, 3, 4, 5])\n\n    bob = sy.VirtualWorker(hook, id=""bob_udacity"")\n\n    bob._tensors\n\n    x = th.tensor([1, 2, 3, 4, 5])\n\n    x = x.send(bob)\n\n    bob._tensors\n\n    assert len(bob._tensors) == 1\n\n    x.location\n\n    x.id_at_location\n\n    x.id\n\n    x.owner\n\n    hook.local_worker\n\n    x\n\n    x = x.get()\n    x\n\n    bob._tensors\n\n    assert len(bob._tensors) == 0\n\n    alice = sy.VirtualWorker(hook, id=""alice_udacity"")\n\n    x = th.tensor([1, 2, 3, 4, 5])\n\n    x_ptr = x.send(bob, alice)\n\n    x_ptr.get()\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob, alice)\n\n    x.get(sum_results=True)\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    y = th.tensor([1, 1, 1, 1, 1]).send(bob)\n\n    x\n    y\n\n    z = x + y\n\n    z\n\n    z = z.get()\n    z\n\n    z = th.add(x, y)\n    z\n\n    z = z.get()\n    z\n\n    x = th.tensor([1.0, 2, 3, 4, 5], requires_grad=True).send(bob)\n    y = th.tensor([1.0, 1, 1, 1, 1], requires_grad=True).send(bob)\n\n    z = (x + y).sum()\n\n    z.backward()\n\n    x = x.get()\n\n    x\n\n    x.grad\n\n    input = th.tensor([[1.0, 1], [0, 1], [1, 0], [0, 0]], requires_grad=True).send(bob)\n    target = th.tensor([[1.0], [1], [0], [0]], requires_grad=True).send(bob)\n\n    weights = th.tensor([[0.0], [0.0]], requires_grad=True).send(bob)\n\n    for i in range(10):\n        pred = input.mm(weights)\n\n        loss = ((pred - target) ** 2).sum()\n\n        loss.backward()\n\n        weights.data.sub_(weights.grad * 0.1)\n        weights.grad *= 0\n\n        print(loss.get().data)\n\n    bob = bob.clear_objects()\n\n    assert len(bob._objects) == 0\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    assert len(bob._objects) == 1\n\n    del x\n\n    assert len(bob._objects) == 0\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    assert len(bob._objects) == 1\n\n    x = ""asdf""\n\n    assert len(bob._objects) == 0\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    bob._objects\n\n    x = ""asdf""\n\n    bob._objects\n\n    del x\n\n    bob._objects\n\n    bob = bob.clear_objects()\n    bob._objects\n\n    for i in range(1000):\n        x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    assert len(bob._objects) == 1\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    y = th.tensor([1, 1, 1, 1, 1])\n\n    # throws error\n    # z = x + y\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    y = th.tensor([1, 1, 1, 1, 1]).send(alice)\n\n    # throws error\n    # z = x + y\n\n    from torch import nn, optim\n\n    # A Toy Dataset\n    data = th.tensor([[1.0, 1], [0, 1], [1, 0], [0, 0]], requires_grad=True)\n    target = th.tensor([[1.0], [1], [0], [0]], requires_grad=True)\n\n    # A Toy Model\n    model = nn.Linear(2, 1)\n\n    opt = optim.SGD(params=model.parameters(), lr=0.1)\n\n    def train(iterations=20):\n        for iter in range(iterations):\n            opt.zero_grad()\n\n            pred = model(data)\n\n            loss = ((pred - target) ** 2).sum()\n\n            loss.backward()\n\n            opt.step()\n\n            print(loss.data)\n\n    train()\n\n    data_bob = data[0:2].send(bob)\n    target_bob = target[0:2].send(bob)\n\n    data_alice = data[2:4].send(alice)\n    target_alice = target[2:4].send(alice)\n\n    datasets = [(data_bob, target_bob), (data_alice, target_alice)]\n\n    def train(iterations=20):\n\n        model = nn.Linear(2, 1)\n        opt = optim.SGD(params=model.parameters(), lr=0.1)\n\n        for iter in range(iterations):\n\n            for _data, _target in datasets:\n                # send model to the data\n                model = model.send(_data.location)\n\n                # do normal training\n                opt.zero_grad()\n                pred = model(_data)\n                loss = ((pred - _target) ** 2).sum()\n                loss.backward()\n                opt.step()\n\n                # get smarter model back\n                model = model.get()\n\n                print(loss.get())\n\n    train()\n\n    bob.clear_objects()\n    alice.clear_objects()\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    x = x.send(alice)\n\n    bob._objects\n\n    alice._objects\n\n    y = x + x\n\n    y\n\n    bob._objects\n\n    alice._objects\n\n    jon = sy.VirtualWorker(hook, id=""jon"")\n\n    bob.clear_objects()\n    alice.clear_objects()\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob).send(alice)\n\n    bob._objects\n\n    alice._objects\n\n    x = x.get()\n    x\n\n    bob._objects\n\n    alice._objects\n\n    x = x.get()\n    x\n\n    bob._objects\n\n    bob.clear_objects()\n    alice.clear_objects()\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob).send(alice)\n\n    bob._objects\n\n    alice._objects\n\n    del x\n\n    bob._objects\n\n    alice._objects\n\n    bob.clear_objects()\n    alice.clear_objects()\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    bob._objects\n\n    alice._objects\n\n    x.move(alice)\n\n    bob._objects\n\n    alice._objects\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob).send(alice)\n\n    bob._objects\n\n    alice._objects\n\n    x.remote_get()\n\n    bob._objects\n\n    alice._objects\n\n    x.move(bob)\n\n    x\n\n    bob._objects\n\n    alice._objects\n\n\ndef test_section_3_securing_fl(hook):\n    """"""This tests the Udacity course content found at\n    https://github.com/Udacity/private-ai\n    """"""\n\n    import syft as sy\n    import torch as th\n\n    # hook = sy.TorchHook(th)\n    from torch import nn, optim\n\n    # create a couple workers\n\n    bob = sy.VirtualWorker(hook, id=""bob_udacity_3"")\n    alice = sy.VirtualWorker(hook, id=""alice_udacity_3"")\n    secure_worker = sy.VirtualWorker(hook, id=""secure_worker_udacity_3"")\n\n    bob.add_workers([alice, secure_worker])\n    alice.add_workers([bob, secure_worker])\n    secure_worker.add_workers([alice, bob])\n\n    # A Toy Dataset\n    data = th.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]], requires_grad=True)\n    target = th.tensor([[0], [0], [1], [1.0]], requires_grad=True)\n\n    # get pointers to training data on each worker by\n    # sending some training data to bob and alice\n    bobs_data = data[0:2].send(bob)\n    bobs_target = target[0:2].send(bob)\n\n    alices_data = data[2:].send(alice)\n    alices_target = target[2:].send(alice)\n\n    # Iniitalize A Toy Model\n    model = nn.Linear(2, 1)\n\n    bobs_model = model.copy().send(bob)\n    alices_model = model.copy().send(alice)\n\n    bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=0.1)\n    alices_opt = optim.SGD(params=alices_model.parameters(), lr=0.1)\n\n    for i in range(10):\n        # Train Bob\'s Model\n        bobs_opt.zero_grad()\n        bobs_pred = bobs_model(bobs_data)\n        bobs_loss = ((bobs_pred - bobs_target) ** 2).sum()\n        bobs_loss.backward()\n\n        bobs_opt.step()\n        bobs_loss = bobs_loss.get().data\n\n        # Train Alice\'s Model\n        alices_opt.zero_grad()\n        alices_pred = alices_model(alices_data)\n        alices_loss = ((alices_pred - alices_target) ** 2).sum()\n        alices_loss.backward()\n\n        alices_opt.step()\n        alices_loss = alices_loss.get().data\n        alices_loss\n\n    alices_model.move(secure_worker)\n    bobs_model.move(secure_worker)\n\n    with th.no_grad():\n\n        model.weight.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\n        model.bias.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\n\n    iterations = 10\n    worker_iters = 5\n\n    for a_iter in range(iterations):\n\n        bobs_model = model.copy().send(bob)\n        alices_model = model.copy().send(alice)\n\n        bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=0.1)\n        alices_opt = optim.SGD(params=alices_model.parameters(), lr=0.1)\n\n        for wi in range(worker_iters):\n            # Train Bob\'s Model\n            bobs_opt.zero_grad()\n            bobs_pred = bobs_model(bobs_data)\n            bobs_loss = ((bobs_pred - bobs_target) ** 2).sum()\n            bobs_loss.backward()\n\n            bobs_opt.step()\n            bobs_loss = bobs_loss.get().data\n\n            # Train Alice\'s Model\n            alices_opt.zero_grad()\n            alices_pred = alices_model(alices_data)\n            alices_loss = ((alices_pred - alices_target) ** 2).sum()\n            alices_loss.backward()\n\n            alices_opt.step()\n            alices_loss = alices_loss.get().data\n\n        alices_model.move(secure_worker)\n        bobs_model.move(secure_worker)\n\n        with th.no_grad():\n\n            model.weight.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\n            model.bias.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\n\n        print(""Bob:"" + str(bobs_loss) + "" Alice:"" + str(alices_loss))\n\n    preds = model(data)\n    loss = ((preds - target) ** 2).sum()\n\n    print(preds)\n    print(target)\n    print(loss.data)\n\n    x = 5\n\n    bob_x_share = 2\n    alice_x_share = 3\n\n    decrypted_x = bob_x_share + alice_x_share\n    decrypted_x\n\n    bob_x_share = 2 * 2\n    alice_x_share = 3 * 2\n\n    decrypted_x = bob_x_share + alice_x_share\n    decrypted_x\n\n    # encrypted ""5""\n    bob_x_share = 2\n    alice_x_share = 3\n\n    # encrypted ""7""\n    bob_y_share = 5\n    alice_y_share = 2\n\n    # encrypted 5 + 7\n    bob_z_share = bob_x_share + bob_y_share\n    alice_z_share = alice_x_share + alice_y_share\n\n    decrypted_z = bob_z_share + alice_z_share\n    decrypted_z\n\n    x = 5\n\n    Q = 23740629843760239486723\n\n    bob_x_share = 23552870267  # <- a random number\n    alice_x_share = Q - bob_x_share + x\n    alice_x_share\n\n    (bob_x_share + alice_x_share) % Q\n\n    x_share = (2, 5, 7)\n\n    import random\n\n    Q = 23740629843760239486723\n\n    def encrypt(x, n_share=3):\n\n        shares = []\n\n        for i in range(n_share - 1):\n            shares.append(random.randint(0, Q))\n\n        shares.append(Q - (sum(shares) % Q) + x)\n\n        return tuple(shares)\n\n    def decrypt(shares):\n        return sum(shares) % Q\n\n    shares = encrypt(3)\n    shares\n\n    decrypt(shares)\n\n    def add(a, b):\n        c = []\n        for i in range(len(a)):\n            c.append((a[i] + b[i]) % Q)\n        return tuple(c)\n\n    x = encrypt(5)\n    y = encrypt(7)\n    z = add(x, y)\n    decrypt(z)\n\n    BASE = 10\n    PRECISION = 4\n\n    def encode(x):\n        return int((x * (BASE ** PRECISION)) % Q)\n\n    def decode(x):\n        return (x if x <= Q / 2 else x - Q) / BASE ** PRECISION\n\n    encode(3.5)\n\n    decode(35000)\n\n    x = encrypt(encode(5.5))\n    y = encrypt(encode(2.3))\n    z = add(x, y)\n    decode(decrypt(z))\n\n    bob = bob.clear_objects()\n    alice = alice.clear_objects()\n    secure_worker = secure_worker.clear_objects()\n\n    x = th.tensor([1, 2, 3, 4, 5])\n\n    x = x.share(bob, alice, secure_worker)\n\n    bob._objects\n\n    y = x + x\n\n    y\n\n    y.get()\n\n    x = th.tensor([0.1, 0.2, 0.3])\n\n    x = x.fix_prec()\n\n    x.child.child\n\n    y = x + x\n\n    y = y.float_prec()\n    y\n\n    x = th.tensor([0.1, 0.2, 0.3])\n\n    x = x.fix_prec().share(bob, alice, secure_worker)\n\n    y = x + x\n\n    y.get().float_prec()\n'"
docker-images/pysyft-worker/worker-server.py,0,"b'import argparse\nimport torch\nimport syft as sy\nfrom syft import WebsocketServerWorker\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=""Run websocket server worker."")\n    parser.add_argument(\n        ""--port"",\n        ""-p"",\n        type=int,\n        default=8777,\n        help=""port number of the websocket server worker, e.g. --port 8777"",\n    )\n    parser.add_argument(""--host"", type=str, default=""0.0.0.0"", help=""host for the connection"")\n    parser.add_argument(\n        ""--id"", type=str, help=""name (id) of the websocket server worker, e.g. --id alice""\n    )\n    parser.add_argument(\n        ""--verbose"",\n        ""-v"",\n        action=""store_true"",\n        help=""if set, websocket server worker will be started in verbose mode"",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == ""__main__"":\n    hook = sy.TorchHook(torch)\n    args = get_args()\n    kwargs = {\n        ""id"": args.id,\n        ""host"": args.host,\n        ""port"": args.port,\n        ""hook"": hook,\n        ""verbose"": args.verbose,\n    }\n\n    server = WebsocketServerWorker(**kwargs)\n    server.start()\n'"
docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport sphinx_rtd_theme  # noqa: F401\n\n\nsys.path.insert(0, os.path.abspath(""../..""))\n\n# -- Project information -----------------------------------------------------\n\nproject = ""PySyft""\ncopyright = ""2020, Andrew Trask""\nauthor = ""Andrew Trask""\n\n# The full version, including alpha/beta/rc tags\nrelease = ""0.2.3a1""\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [""sphinx.ext.napoleon"", ""autoapi.extension"", ""sphinx_rtd_theme""]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n\ndef setup(app):\n    app.add_stylesheet(""css/theme.css"")\n\n\n# -- Extension configuration -------------------------------------------------\n\n# AutoApi\nautoapi_root = ""api""\nautoapi_type = ""python""\nautoapi_dirs = [""../../syft""]\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = False\n'"
examples/tutorials/__init__.py,0,b''
syft/common/__init__.py,0,b''
syft/common/util.py,6,"b'# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport numpy as np\n\n\ndef chebyshev_series(func, width, terms):\n    r""""""\n    Computes Chebyshev coefficients\n    For n = terms, the ith Chebyshev series coefficient is\n    .. math::\n        c_i = 2/n \\sum_{k=1}^n \\cos(j(2k-1)\\pi / 4n) f(w\\cos((2k-1)\\pi / 4n))\n    Args:\n        func (function): function to be approximated\n        width (int): approximation will support inputs in range [-width, width]\n        terms (int): number of Chebyshev terms used in approximation\n    Returns:\n        Chebyshev coefficients with shape equal to num of terms.\n    """"""\n    n_range = torch.arange(start=0, end=terms).float()\n    x = width * torch.cos((n_range + 0.5) * np.pi / terms)\n    y = func(x)\n    cos_term = torch.cos(torch.ger(n_range, n_range + 0.5) * np.pi / terms)\n    coeffs = (2 / terms) * torch.sum(y * cos_term, axis=1)\n    return coeffs\n\n\ndef chebyshev_polynomials(tensor, terms=32):\n    r""""""\n    Evaluates odd degree Chebyshev polynomials at x\n    Chebyshev Polynomials of the first kind are defined as\n    .. math::\n        P_0(x) = 1, P_1(x) = x, P_n(x) = 2 P_{n - 1}(x) - P_{n-2}(x)\n    Args:\n        tensor (torch.tensor): input at which polynomials are evaluated\n        terms (int): highest degree of Chebyshev polynomials.\n                     Must be even and at least 6.\n    """"""\n    if terms % 2 != 0 or terms < 6:\n        raise ValueError(""Chebyshev terms must be even and >= 6"")\n\n    polynomials = [tensor.clone()]\n    y = 4 * tensor ** 2 - 2\n    z = y - 1\n    polynomials.append(z.mul(tensor))\n\n    for k in range(2, terms // 2):\n        next_polynomial = y * polynomials[k - 1] - polynomials[k - 2]\n        polynomials.append(next_polynomial)\n\n    return torch.stack(polynomials)\n'"
syft/execution/__init__.py,0,b''
syft/execution/action.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\n\nimport syft as sy\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.execution.placeholder_id import PlaceholderId\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft.workers.abstract import AbstractWorker\n\n\nclass Action(ABC, SyftSerializable):\n    """"""Describes the concrete steps workers can take with objects they own\n\n    In Syft, an Action is when one worker wishes to tell another worker to do something with\n    objects contained in the worker.object_store registry (or whatever the official object store is\n    backed with in the case that it\'s been overridden). For example, telling a worker to take two\n    tensors and add them together is an Action. Sending an object from one worker to another is\n    also an Action.""""""\n\n    def __init__(\n        self, name: str, target, args_: tuple, kwargs_: dict, return_ids: tuple, return_value=False\n    ):\n        """"""Initialize an action\n\n        Args:\n            name (String): The name of the method to be invoked (e.g. ""__add__"")\n            target (Tensor): The object to invoke the method on\n            args_ (Tuple): The arguments to the method call\n            kwargs_ (Dictionary): The keyword arguments to the method call\n            return_ids (Tuple): primarily for our async infrastructure (Plan, Protocol, etc.),\n                the id of action results are set by the client. This allows the client to be able to\n                predict where the results will be ahead of time. Importantly, this allows the\n                client to pre-initalize the pointers to the future data, regardless of whether\n                the action has yet executed. It also reduces the size of the response from the\n                action (which is very often empty).\n            return_value (boolean): return the result or not. If true, the result is directly\n                returned, if not, the command sender will create a pointer to the remote result\n                using the return_ids and will need to do .get() later to get the result.\n\n        """"""\n\n        # call the parent constructor - setting the type integer correctly\n        super().__init__()\n\n        self.name = name\n        self.target = target\n        self.args = args_\n        self.kwargs = kwargs_\n        self.return_ids = return_ids\n        self.return_value = return_value\n\n        self._type_check(""name"", str)\n        self._type_check(""args"", tuple)\n        self._type_check(""kwargs"", dict)\n        self._type_check(""return_ids"", tuple)\n\n    def __eq__(self, other):\n        return (\n            self.name == other.name\n            and self.target == other.target\n            and self.args == other.args\n            and self.kwargs == other.kwargs\n            and self.return_ids == other.return_ids\n        )\n\n    def code(self, var_names=None) -> str:\n        """"""Returns pseudo-code representation of computation action""""""\n\n        def stringify(obj):\n            if isinstance(obj, PlaceholderId):\n                id = obj.value\n                if var_names is None:\n                    ret = f""var_{id}""\n                else:\n                    if id in var_names:\n                        ret = var_names[id]\n                    else:\n                        idx = sum(""var_"" in k for k in var_names.values())\n                        name = f""var_{idx}""\n                        var_names[id] = name\n                        ret = name\n            elif isinstance(obj, PlaceHolder):\n                ret = stringify(obj.id)\n            elif isinstance(obj, (tuple, list)):\n                ret = "", "".join(stringify(o) for o in obj)\n            else:\n                ret = str(obj)\n\n            return ret\n\n        out = """"\n        if self.return_ids is not None:\n            out += stringify(self.return_ids) + "" = ""\n        if self.target is not None:\n            out += stringify(self.target) + "".""\n        out += self.name + ""(""\n        out += stringify(self.args)\n        if self.kwargs:\n            if len(self.args) > 0:\n                out += "", ""\n            out += "", "".join(f""{k}={w}"" for k, w in self.kwargs.items())\n        out += "")""\n\n        return out\n\n    def __str__(self) -> str:\n        """"""Returns string representation of this action""""""\n        return f""{type(self).__name__}[{self.code()}]""\n\n    def _type_check(self, field_name, expected_type):\n        actual_value = getattr(self, field_name)\n        assert actual_value is None or isinstance(actual_value, expected_type), (\n            f""{field_name} must be {expected_type.__name__}, but was ""\n            f""{type(actual_value).__name__}: {actual_value}.""\n        )\n\n    # These methods must be implemented by child classes in order to return the correct type\n    # and to be detected by the serdes as serializable. They are therefore marked as abstract\n    # methods even though implementations are provided. Child classes may delegate to these\n    # implementations, but must implement their own conversions to the appropriate classes.\n\n    @staticmethod\n    @abstractmethod\n    def simplify(worker: AbstractWorker, action: ""Action"") -> tuple:\n        """"""\n        This function takes the attributes of a CommunicationAction and saves them in a tuple\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            action (CommunicationAction): a CommunicationAction\n        Returns:\n            tuple: a tuple holding the unique attributes of the CommunicationAction\n        Examples:\n            data = simplify(worker, action)\n        """"""\n        return (\n            sy.serde.msgpack.serde._simplify(worker, action.name),\n            sy.serde.msgpack.serde._simplify(worker, action.target),\n            sy.serde.msgpack.serde._simplify(worker, action.args),\n            sy.serde.msgpack.serde._simplify(worker, action.kwargs),\n            sy.serde.msgpack.serde._simplify(worker, action.return_ids),\n            sy.serde.msgpack.serde._simplify(worker, action.return_value),\n        )\n\n    @staticmethod\n    @abstractmethod\n    def detail(worker: AbstractWorker, action_tuple: tuple) -> ""Action"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into a CommunicationAction. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            communication_tuple (Tuple): the raw information being detailed.\n        Returns:\n            communication (CommunicationAction): a CommunicationAction.\n        Examples:\n            communication = detail(sy.local_worker, communication_tuple)\n        """"""\n        name, target, args_, kwargs_, return_ids, return_value = action_tuple\n\n        return (\n            sy.serde.msgpack.serde._detail(worker, name),\n            sy.serde.msgpack.serde._detail(worker, target),\n            sy.serde.msgpack.serde._detail(worker, args_),\n            sy.serde.msgpack.serde._detail(worker, kwargs_),\n            sy.serde.msgpack.serde._detail(worker, return_ids),\n            sy.serde.msgpack.serde._detail(worker, return_value),\n        )\n\n    @staticmethod\n    @abstractmethod\n    def bufferize(worker: AbstractWorker, action: ""Action"", protobuf_action):\n        """"""\n        This function takes the attributes of a Action and saves them in Protobuf\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            action (Action): an Action\n        Returns:\n            protobuf_obj: a Protobuf message holding the unique attributes of the message\n        Examples:\n            data = bufferize(message)\n        """"""\n        protobuf_action.command = action.name\n\n        protobuf_target = None\n        if isinstance(action.target, sy.generic.pointers.pointer_tensor.PointerTensor):\n            protobuf_target = protobuf_action.target_pointer\n        elif isinstance(action.target, sy.execution.placeholder_id.PlaceholderId):\n            protobuf_target = protobuf_action.target_placeholder_id\n        elif isinstance(action.target, (int, str)):\n            sy.serde.protobuf.proto.set_protobuf_id(protobuf_action.target_id, action.target)\n        elif action.target is not None:\n            protobuf_target = protobuf_action.target_tensor\n\n        if protobuf_target is not None:\n            protobuf_target.CopyFrom(sy.serde.protobuf.serde._bufferize(worker, action.target))\n\n        if action.args:\n            protobuf_action.args.extend(sy.serde.protobuf.serde.bufferize_args(worker, action.args))\n\n        if action.kwargs:\n            for key, value in action.kwargs.items():\n                protobuf_action.kwargs.get_or_create(key).CopyFrom(\n                    sy.serde.protobuf.serde.bufferize_arg(worker, value)\n                )\n\n        if action.return_ids is not None:\n            if not isinstance(action.return_ids, (list, tuple)):\n                return_ids = (action.return_ids,)\n            else:\n                return_ids = action.return_ids\n\n            for return_id in return_ids:\n                if isinstance(return_id, PlaceholderId):\n                    # NOTE to know when we have a PlaceholderId, we store it\n                    # in return_placeholder_ids and not in return_ids\n                    protobuf_action.return_placeholder_ids.append(\n                        sy.serde.protobuf.serde._bufferize(worker, return_id)\n                    )\n                else:\n                    sy.serde.protobuf.proto.set_protobuf_id(\n                        protobuf_action.return_ids.add(), return_id\n                    )\n\n        return protobuf_action\n\n    @staticmethod\n    @abstractmethod\n    def unbufferize(worker: AbstractWorker, protobuf_obj):\n        """"""\n        This function takes the Protobuf version of this message and converts\n        it into an Action. The bufferize() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            protobuf_obj (ActionPB): the Protobuf message\n\n        Returns:\n            obj (tuple): a tuple of the args required to instantiate an Action object\n\n        Examples:\n            message = unbufferize(sy.local_worker, protobuf_msg)\n        """"""\n        command = protobuf_obj.command\n        protobuf_target = protobuf_obj.WhichOneof(""target"")\n        if protobuf_target:\n            target = sy.serde.protobuf.serde._unbufferize(\n                worker, getattr(protobuf_obj, protobuf_obj.WhichOneof(""target""))\n            )\n        else:\n            target = None\n        args_ = sy.serde.protobuf.serde.unbufferize_args(worker, protobuf_obj.args)\n\n        kwargs_ = {}\n        for key in protobuf_obj.kwargs:\n            kwargs_[key] = sy.serde.protobuf.serde.unbufferize_arg(worker, protobuf_obj.kwargs[key])\n\n        return_ids = tuple(\n            sy.serde.protobuf.proto.get_protobuf_id(pb_id) for pb_id in protobuf_obj.return_ids\n        )\n\n        return_placeholder_ids = tuple(\n            (\n                sy.serde.protobuf.serde._unbufferize(worker, placeholder)\n                for placeholder in protobuf_obj.return_placeholder_ids\n            )\n        )\n\n        if return_placeholder_ids:\n            action = (command, target, args_, kwargs_, return_placeholder_ids)\n        else:\n            action = (command, target, args_, kwargs_, return_ids)\n\n        return action\n'"
syft/execution/communication.py,0,"b'from syft.execution.action import Action\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft_proto.execution.v1.communication_action_pb2 import (\n    CommunicationAction as CommunicationActionPB,\n)\n\n\nCOMMUNICATION_METHODS = [\n    ""get"",\n    ""mid_get"",\n    ""move"",\n    ""remote_get"",\n    ""remote_send"",\n    ""send"",\n    ""share"",\n    ""share_"",\n]\n\n\nclass CommunicationAction(Action):\n    """"""Describes communication actions performed on tensors""""""\n\n    def __init__(self, name, target, args_, kwargs_, return_ids, return_value=False):\n        """"""Initialize an action\n\n        Args:\n            name (String): The name of the method to be invoked (e.g. ""send"")\n            target (Tensor): The object to invoke the method on\n            args_ (Tuple): The arguments to the method call\n            kwargs_ (Dictionary): The keyword arguments to the method call\n            return_ids (Tuple): primarily for our async infrastructure (Plan, Protocol, etc.),\n                the id of action results are set by the client. This allows the client to be\n                able to predict where the results will be ahead of time. Importantly, this allows\n                the client to pre-initalize the pointers to the future data, regardless of whether\n                the action has yet executed. It also reduces the size of the response from the\n                action (which is very often empty).\n            return_value (boolean): return the result or not. If true, the result is directly\n                returned, if not, the command sender will create a pointer to the remote result\n                using the return_ids and will need to do .get() later to get the result.\n        """"""\n        if name not in COMMUNICATION_METHODS:\n            raise ValueError(\n                f""Method `{name}` is not supported by CommunicationActions. Consider using ""\n                ""ComputationAction instead.""\n            )\n\n        super().__init__(name, target, args_, kwargs_, return_ids, return_value=return_value)\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, action: ""Action"") -> tuple:\n        """"""\n        This function takes the attributes of a CommunicationAction and saves them in a tuple\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            action (CommunicationAction): a CommunicationAction\n        Returns:\n            tuple: a tuple holding the unique attributes of the CommunicationAction\n        Examples:\n            data = simplify(worker, action)\n        """"""\n        return Action.simplify(worker, action)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, action_tuple: tuple) -> ""Action"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into a CommunicationAction. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            communication_tuple (Tuple): the raw information being detailed.\n        Returns:\n            communication (CommunicationAction): a CommunicationAction.\n        Examples:\n            communication = detail(sy.local_worker, communication_tuple)\n        """"""\n        attrs = Action.detail(worker, action_tuple)\n\n        return CommunicationAction(*attrs)\n\n    @staticmethod\n    def bufferize(\n        worker: AbstractWorker, communication: ""CommunicationAction""\n    ) -> ""CommunicationActionPB"":\n        """"""\n        This function takes the attributes of a CommunicationAction and saves them in Protobuf\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            communication (CommunicationAction): a CommunicationAction\n        Returns:\n            protobuf_obj: a Protobuf message holding the unique attributes of the communication\n        Examples:\n            data = bufferize(sy.local_worker, communication)\n        """"""\n        protobuf_action = CommunicationActionPB()\n\n        return Action.bufferize(worker, communication, protobuf_action)\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_obj: ""CommunicationActionPB""\n    ) -> ""CommunicationAction"":\n        """"""\n        This function takes the Protobuf version of this message and converts\n        it into an Action. The bufferize() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            protobuf_obj (CommunicationActionPB): the Protobuf message\n\n        Returns:\n            obj (CommunicationAction): a CommunicationAction\n\n        Examples:\n            message = unbufferize(sy.local_worker, protobuf_msg)\n        """"""\n        attrs = Action.unbufferize(worker, protobuf_obj)\n\n        return CommunicationAction(*attrs)\n\n    @staticmethod\n    def get_protobuf_schema() -> CommunicationActionPB:\n        return CommunicationActionPB\n'"
syft/execution/computation.py,0,"b'from syft.workers.abstract import AbstractWorker\n\nfrom syft.execution.action import Action\n\nfrom syft_proto.execution.v1.computation_action_pb2 import ComputationAction as ComputationActionPB\n\n\nclass ComputationAction(Action):\n    """"""Describes mathematical operations performed on tensors""""""\n\n    def __init__(self, name, target, args_, kwargs_, return_ids, return_value=False):\n        """"""Initialize an action\n\n        Args:\n            name (String): The name of the method to be invoked (e.g. ""__add__"")\n            target (Tensor): The object to invoke the method on\n            args_ (Tuple): The arguments to the method call\n            kwargs_ (Dictionary): The keyword arguments to the method call\n            return_ids (Tuple): primarily for our async infrastructure (Plan, Protocol, etc.),\n                the id of action results are set by the client. This allows the client to be able\n                to predict where the results will be ahead of time. Importantly, this allows the\n                client to pre-initalize the pointers to the future data, regardless of whether the\n                action has yet executed. It also reduces the size of the response from the action\n                (which is very often empty).\n            return_value (boolean): return the result or not. If true, the result is directly\n                returned, if not, the command sender will create a pointer to the remote result\n                using the return_ids and will need to do .get() later to get the result.\n\n        """"""\n        super().__init__(name, target, args_, kwargs_, return_ids, return_value=return_value)\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, action: ""Action"") -> tuple:\n        """"""\n        This function takes the attributes of a ComputationAction and saves them in a tuple\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            action (ComputationAction): a ComputationAction\n        Returns:\n            tuple: a tuple holding the unique attributes of the ComputationAction\n        Examples:\n            data = simplify(worker, action)\n        """"""\n        return Action.simplify(worker, action)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, action_tuple: tuple) -> ""Action"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into a ComputationAction. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            communication_tuple (Tuple): the raw information being detailed.\n        Returns:\n            communication (ComputationAction): a ComputationAction.\n        Examples:\n            communication = detail(sy.local_worker, communication_tuple)\n        """"""\n        attrs = Action.detail(worker, action_tuple)\n\n        return ComputationAction(*attrs)\n\n    @staticmethod\n    def bufferize(\n        worker: AbstractWorker, communication: ""ComputationAction""\n    ) -> ""ComputationActionPB"":\n        """"""\n        This function takes the attributes of a ComputationAction and saves them in Protobuf\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            communication (ComputationAction): a ComputationAction\n        Returns:\n            protobuf_obj: a Protobuf message holding the unique attributes of the communication\n        Examples:\n            data = bufferize(sy.local_worker, communication)\n        """"""\n        protobuf_action = ComputationActionPB()\n\n        return Action.bufferize(worker, communication, protobuf_action)\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_obj: ""ComputationActionPB""\n    ) -> ""ComputationAction"":\n        """"""\n        This function takes the Protobuf version of this message and converts\n        it into an Action. The bufferize() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            protobuf_obj (ComputationActionPB): the Protobuf message\n\n        Returns:\n            obj (ComputationAction): a ComputationAction\n\n        Examples:\n            message = unbufferize(sy.local_worker, protobuf_msg)\n        """"""\n        attrs = Action.unbufferize(worker, protobuf_obj)\n\n        return ComputationAction(*attrs)\n\n    @staticmethod\n    def get_protobuf_schema() -> ComputationActionPB:\n        return ComputationActionPB\n'"
syft/execution/placeholder.py,0,"b'import syft\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.workers.abstract import AbstractWorker\nfrom syft_proto.execution.v1.placeholder_pb2 import Placeholder as PlaceholderPB\n\n\nclass PlaceHolder(AbstractTensor):\n    def __init__(\n        self,\n        role=None,\n        tracing=False,\n        id=None,\n        tags: set = None,\n        description: str = None,\n        shape=None,\n    ):\n        """"""A PlaceHolder acts as a tensor but does nothing special. It can get\n        ""instantiated"" when a real tensor is appended as a child attribute. It\n        will send forward all the commands it receives to its child tensor.\n\n        When you send a PlaceHolder, you don\'t sent the instantiated tensors.\n\n        Args:\n            id: An optional string or integer id of the PlaceHolder.\n        """"""\n        super().__init__(id=id, tags=tags, description=description)\n\n        if not isinstance(self.id, syft.execution.placeholder_id.PlaceholderId):\n            self.id = syft.execution.placeholder_id.PlaceholderId(self.id)\n\n        self.expected_shape = tuple(shape) if shape is not None else None\n        self.child = None\n        self.role = role\n        self.tracing = tracing\n\n    def get_class_attributes(self):\n        """"""\n        Specify all the attributes need to build a wrapper correctly when returning a response.\n        """"""\n        return {""role"": self.role, ""tracing"": self.tracing}\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """""" Receive an instruction for a function to be applied on a Placeholder,\n        Replace in the args with their child attribute, forward the command\n        instruction to the handle_function_command of the type of the child attributes,\n        get the response and wrap it in a Placeholder.\n        We use this method to perform the tracing.\n\n        Args:\n            command: instruction of a function command: (command name,\n                <no self>, arguments[, kwargs])\n\n        Returns:\n            the response of the function command\n        """"""\n        cmd, _, args, kwargs = command\n\n        # Replace all PlaceHolders with their child attribute\n        new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(cmd, args, kwargs)\n\n        # build the new command\n        new_command = (cmd, None, new_args, new_kwargs)\n\n        # Send it to the appropriate class and get the response\n        response = new_type.handle_func_command(new_command)\n\n        # Find first placeholder in args\n        template_placeholder = None\n        for arg in args:\n            if isinstance(arg, PlaceHolder):\n                template_placeholder = arg\n\n        placeholders = PlaceHolder.convert_to_placeholders(response, template_placeholder)\n\n        if template_placeholder.tracing:\n            template_placeholder.role.register_action(\n                (command, placeholders), syft.execution.computation.ComputationAction\n            )\n\n        return placeholders\n\n    @staticmethod\n    def convert_to_placeholders(response, template_placeholder):\n        """""" Turn back response to PlaceHolders """"""\n        if isinstance(response, (tuple, list)):\n\n            placeholders = tuple(\n                PlaceHolder.create_from(\n                    r, role=template_placeholder.role, tracing=template_placeholder.tracing\n                )\n                for r in response\n            )\n        else:\n            placeholders = PlaceHolder.create_from(\n                response, role=template_placeholder.role, tracing=template_placeholder.tracing\n            )\n\n        return placeholders\n\n    def __getattribute__(self, name):\n        """"""Try to find the attribute in the current object\n        and in case we can not then we forward it to the child\n\n        """"""\n        try:\n            response = object.__getattribute__(self, name)\n        except AttributeError:\n            child = object.__getattribute__(self, ""child"")\n            response = getattr(child, name)\n\n        return response\n\n    def instantiate(self, tensor):\n        """"""\n        Add a tensor as a child attribute. All operations on the placeholder will be also\n        executed on this child tensor.\n\n        We remove Placeholders if is there are any.\n        """"""\n        if isinstance(tensor, PlaceHolder):\n            self.child = tensor.child\n        else:\n            self.child = tensor\n\n        if hasattr(self.child, ""shape""):\n            self.expected_shape = tuple(self.child.shape)\n\n        return self\n\n    def __str__(self) -> str:\n        """"""\n        Compact representation of a Placeholder, including tags and optional child\n        """"""\n        tags = "" "".join(list(self.tags or []))\n\n        out = f""{type(self).__name__ }[Id:{self.id.value}]""\n\n        if hasattr(self, ""child"") and self.child is not None:\n            out += f"">{self.child}""\n\n        return out\n\n    __repr__ = __str__\n\n    def send(self, *args, **kwargs):\n        """"""\n        calls move on child & register_action to role\n        """"""\n        response = self.child.send(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""send"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def move(self, *args, **kwargs):\n        """"""\n        calls move on a pointer tensor & register_action to role\n        """"""\n        response = self.child.move(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""move"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def share(self, *args, **kwargs):\n        """"""\n        Send a command to remote worker to additively share a tensor via pointer tensor\n        """"""\n        response = self.child.share(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""share"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def fix_prec(self, *args, **kwargs):\n        """"""\n        sends command to remote worker to transform a tensor to fix_precision via pointer tensor\n        """"""\n        response = self.child.fix_prec(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""fix_prec"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.computation.ComputationAction\n        )\n        return placeholder\n\n    def mid_get(self, *args, **kwargs):\n        response = self.child.mid_get(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(self.child, self)\n        command = (""mid_get"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def remote_get(self, *args, **kwargs):\n        """"""\n        calls remote_get on child & register_action to role\n        """"""\n        response = self.child.remote_get(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""remote_get"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def remote_send(self, *args, **kwargs):\n        """"""\n        calls remote_send on child & register_action to role\n        """"""\n        response = self.child.remote_send(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""remote_send"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def share_(self, *args, **kwargs):\n        """"""\n        calls share_ on child & register_action to role\n        """"""\n        response = self.child.share_(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""share_"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def get(self, *args, **kwargs):\n        """"""Requests the tensor/chain being pointed to, be serialized and return via child""""""\n        response = self.child.get(*args, **kwargs)\n        placeholder = PlaceHolder.convert_to_placeholders(response, self)\n        command = (""get"", self, args, kwargs)\n        self.role.register_action(\n            (command, placeholder), syft.execution.communication.CommunicationAction\n        )\n        return placeholder\n\n    def copy(self):\n        """"""\n        Copying a placeholder doesn\'t duplicate the child attribute, because all\n        copy operations happen locally where we want to keep reference to the same\n        instantiated object. As the child doesn\'t get sent, this is not an issue.\n        """"""\n        placeholder = PlaceHolder(\n            role=self.role, tracing=self.tracing, tags=self.tags, shape=self.expected_shape\n        )\n        placeholder.child = self.child\n\n        if self.tracing:\n            command = (""copy"", self, (), {}), placeholder\n            self.role.register_action(command, syft.execution.computation.ComputationAction)\n\n        return placeholder\n\n    @staticmethod\n    def create_from(tensor, role=None, tracing=False):\n        """""" Helper method to create a placeholder already\n        instantiated with tensor.\n        """"""\n        return PlaceHolder(role=role, tracing=tracing).instantiate(tensor)\n\n    @staticmethod\n    def insert(tensor, after, role=None, tracing=False):\n        """""" Helper method to add a placeholder in the specific place of tensor chain. """"""\n        current_level = tensor\n        while not isinstance(current_level, after) and current_level is not None:\n            current_level = getattr(current_level, ""child"", None)\n\n        if current_level is None:\n            raise RuntimeError(\n                f""Cannot insert Placeholder, chain does not contain {after.__name__} tensor type.""\n            )\n\n        child = getattr(current_level, ""child"", None)\n        if child is None:\n            raise RuntimeError(\n                f""Cannot insert Placeholder, {after.__name__} does not wrap anything.""\n            )\n\n        placeholder = PlaceHolder.create_from(child, role, tracing)\n        current_level.child = placeholder\n        return placeholder\n\n    @staticmethod\n    def extract(tensor):\n        """""" Helper method to find and return placeholder in the tensor chain. """"""\n        current_level = tensor\n        while not isinstance(current_level, PlaceHolder) and current_level is not None:\n            current_level = getattr(current_level, ""child"", None)\n        return current_level\n\n    @staticmethod\n    def create_placeholders(args_shape):\n        """""" Helper method to create a list of placeholders with shapes\n        in args_shape.\n        """"""\n        # In order to support -1 value in shape to indicate any dimension\n        # we map -1 to 1 for shape dimensions.\n        # TODO: A more complex strategy could be used\n        mapped_shapes = []\n        for shape in args_shape:\n            if list(filter(lambda x: x < -1, shape)):\n                raise ValueError(f""Invalid shape {shape}"")\n            mapped_shapes.append(tuple(map(lambda y: 1 if y == -1 else y, shape)))\n\n        return [\n            syft.framework.hook.create_zeros(shape, requires_grad=False) for shape in mapped_shapes\n        ]\n\n    @staticmethod\n    def instantiate_placeholders(obj, response):\n        """"""\n        Utility function to instantiate recursively an object containing placeholders\n        with a similar object but containing tensors\n        """"""\n        if obj is not None:\n            if isinstance(obj, PlaceHolder):\n                obj.instantiate(response)\n            elif isinstance(obj, (list, tuple)):\n                for ph, rep in zip(obj, response):\n                    PlaceHolder.instantiate_placeholders(ph, rep)\n            else:\n                raise ValueError(\n                    f""Response of type {type(response)} is not supported in ""\n                    ""Placeholder.instantiate.""\n                )\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, placeholder: ""PlaceHolder"") -> tuple:\n        """"""Takes the attributes of a PlaceHolder and saves them in a tuple.\n\n        Args:\n            worker: the worker doing the serialization\n            placeholder: a PlaceHolder.\n\n        Returns:\n            tuple: a tuple holding the unique attributes of the PlaceHolder.\n        """"""\n\n        return (\n            syft.serde.msgpack.serde._simplify(worker, placeholder.id),\n            syft.serde.msgpack.serde._simplify(worker, placeholder.tags),\n            syft.serde.msgpack.serde._simplify(worker, placeholder.description),\n            syft.serde.msgpack.serde._simplify(worker, placeholder.expected_shape),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""PlaceHolder"":\n        """"""\n        This function reconstructs a PlaceHolder given it\'s attributes in form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the PlaceHolder\n        Returns:\n            PlaceHolder: a PlaceHolder\n        """"""\n\n        tensor_id, tags, description, shape = tensor_tuple\n\n        tensor_id = syft.serde.msgpack.serde._detail(worker, tensor_id)\n        tags = syft.serde.msgpack.serde._detail(worker, tags)\n        description = syft.serde.msgpack.serde._detail(worker, description)\n        shape = syft.serde.msgpack.serde._detail(worker, shape)\n\n        return PlaceHolder(id=tensor_id, tags=tags, description=description, shape=shape)\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, placeholder: ""PlaceHolder"") -> PlaceholderPB:\n        """"""Takes the attributes of a PlaceHolder and saves them in a Protobuf message.\n\n        Args:\n            worker: the worker doing the serialization\n            placeholder: a PlaceHolder.\n\n        Returns:\n            PlaceholderPB: a Protobuf message holding the unique attributes of the PlaceHolder.\n        """"""\n\n        protobuf_placeholder = PlaceholderPB()\n        syft.serde.protobuf.proto.set_protobuf_id(protobuf_placeholder.id, placeholder.id.value)\n        protobuf_placeholder.tags.extend(placeholder.tags)\n\n        if placeholder.description:\n            protobuf_placeholder.description = placeholder.description\n\n        if placeholder.expected_shape:\n            protobuf_placeholder.expected_shape.dims.extend(placeholder.expected_shape)\n\n        return protobuf_placeholder\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_placeholder: PlaceholderPB) -> ""PlaceHolder"":\n        """"""\n        This function reconstructs a PlaceHolder given it\'s attributes in form of a\n        Protobuf message.\n        Args:\n            worker: the worker doing the deserialization\n            protobuf_placeholder: a Protobuf message holding the attributes of the PlaceHolder\n        Returns:\n            PlaceHolder: a PlaceHolder\n        """"""\n\n        tensor_id = syft.serde.protobuf.proto.get_protobuf_id(protobuf_placeholder.id)\n        tags = set(protobuf_placeholder.tags)\n\n        description = None\n        if bool(protobuf_placeholder.description):\n            description = protobuf_placeholder.description\n\n        expected_shape = tuple(protobuf_placeholder.expected_shape.dims) or None\n\n        return PlaceHolder(id=tensor_id, tags=tags, description=description, shape=expected_shape)\n\n    @staticmethod\n    def get_protobuf_schema() -> PlaceholderPB:\n        return PlaceholderPB\n\n\n### Register the tensor with hook_args.py ###\nhook_args.default_register_tensor(PlaceHolder)\n'"
syft/execution/placeholder_id.py,0,"b'import syft as sy\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft_proto.execution.v1.placeholder_id_pb2 import PlaceholderId as PlaceholderIdPB\n\n\nclass PlaceholderId(SyftSerializable):\n    """"""\n    PlaceholderIds are used to identify which Placeholder tensors should be used\n    as the inputs and outputs of Actions.\n    """"""\n\n    def __init__(self, value):\n        self.value = value\n\n    def __eq__(self, other):\n        if not isinstance(other, PlaceholderId):\n            return False\n        return self.value == other.value\n\n    def __hash__(self):\n        return hash(self.value)\n\n    @staticmethod\n    def simplify(worker: ""AbstractWorker"", id: ""PlaceholderId"") -> tuple:\n        return (id.value,)\n\n    @staticmethod\n    def detail(worker: ""AbstractWorker"", simplified_id: tuple) -> ""PlaceholderId"":\n        (value,) = simplified_id\n        return PlaceholderId(value)\n\n    @staticmethod\n    def bufferize(worker: ""AbstractWorker"", id: ""PlaceholderId"") -> tuple:\n        protobuf_id = PlaceholderIdPB()\n        sy.serde.protobuf.proto.set_protobuf_id(protobuf_id.id, id.value)\n\n        return protobuf_id\n\n    @staticmethod\n    def unbufferize(worker: ""AbstractWorker"", protobuf_id: tuple) -> ""PlaceholderId"":\n        value = sy.serde.protobuf.proto.get_protobuf_id(protobuf_id.id)\n\n        return PlaceholderId(value)\n\n    @staticmethod\n    def get_protobuf_schema() -> PlaceholderIdPB:\n        return PlaceholderIdPB\n'"
syft/execution/plan.py,4,"b'from typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport copy\nimport inspect\nimport io\nimport torch\nimport warnings\n\nimport syft as sy\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.execution.role import Role\nfrom syft.execution.tracing import FrameworkWrapper\nfrom syft.execution.type_wrapper import NestedTypeWrapper\nfrom syft.execution.translation.abstract import AbstractPlanTranslator\nfrom syft.execution.translation.default import PlanTranslatorDefault\nfrom syft.execution.translation.torchscript import PlanTranslatorTorchscript\nfrom syft.generic.frameworks import framework_packages\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.generic.frameworks.types import FrameworkLayerModule\nfrom syft.generic.abstract.sendable import AbstractSendable\nfrom syft.generic.pointers.pointer_plan import PointerPlan\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.frameworks.torch.tensors.interpreters.autograd import AutogradTensor\n\nfrom syft_proto.execution.v1.plan_pb2 import Plan as PlanPB\n\n\nclass func2plan(object):\n    """"""Decorator which converts a function to a plan.\n\n    Converts a function containing sequential pytorch code into\n    a plan object which can be sent to any arbitrary worker.\n\n    This class should be used only as a decorator.\n    """"""\n\n    def __init__(self, args_shape=None, state=None, trace_autograd=False):\n        self.args_shape = args_shape\n        self.state_tensors = state or ()\n        # include_state is used to distinguish if the initial plan is a function or a class:\n        # if it\'s a function, then the state should be provided in the args, so include_state\n        # will be true. And to know if it was indeed a function, we just need to see if a\n        # ""manual"" state was provided.\n        self.include_state = state is not None\n        self.trace_autograd = trace_autograd\n\n    def __call__(self, plan_function):\n        plan = Plan(\n            name=plan_function.__name__,\n            include_state=self.include_state,\n            forward_func=plan_function,\n            state_tensors=self.state_tensors,\n            id=sy.ID_PROVIDER.pop(),\n            owner=sy.local_worker,\n        )\n\n        # Build the plan automatically\n        if self.args_shape:\n            args_ = PlaceHolder.create_placeholders(self.args_shape)\n            try:\n                plan.build(*args_, trace_autograd=self.trace_autograd)\n            except TypeError as e:\n                raise ValueError(\n                    ""Automatic build using @func2plan failed!\\nCheck that:\\n""\n                    "" - you have provided the correct number of shapes in args_shape\\n""\n                    "" - you have no simple numbers like int or float as args. If you do ""\n                    ""so, please consider using a tensor instead.""\n                )\n        return plan\n\n\nclass Plan(AbstractSendable):\n    """"""\n    A Plan stores a sequence of actions, just like a function.\n\n    A Plan is intended to store a sequence of actions, just like a function,\n    but it allows to send this sequence of actions to remote workers and to keep a\n    reference to it. This way, to compute remotely this sequence of actions on some remote\n    input referenced through pointers, instead of sending multiple messages you need now to send a\n    single message with the references of the plan and the pointers.\n\n    All arguments are optional.\n\n    Args:\n        name: the name of the name\n        state: store the plan tensors like model parameters\n        include_state: if true, implies that the plan is a function, else a class. If true, the\n            state is re-integrated in the args to be accessed within the function\n        is_built: state if the plan has already been built.\n        placeholders: dict of placeholders used in the plan\n        actions: list of commands (called actions)\n        forward_func: the function to be transformed into a plan\n        state_tensors: a tuple of state elements. It can be used to populate a state\n        id: plan id\n        owner: plan owner\n        tags: plan tags\n        description: plan description\n    """"""\n\n    _build_translators = []\n    _wrapped_frameworks = {}\n\n    def __init__(\n        self,\n        name: str = None,\n        include_state: bool = False,\n        is_built: bool = False,\n        forward_func=None,\n        state_tensors=[],\n        role: Role = None,\n        # General kwargs\n        id: Union[str, int] = None,\n        owner: ""sy.workers.BaseWorker"" = None,\n        tags: List[str] = None,\n        input_types: list = None,\n        description: str = None,\n    ):\n        super().__init__(id, owner, tags, description, child=None)\n\n        # Plan instance info\n        self.name = name or self.__class__.__name__\n\n        self.role = role or Role()\n\n        if role is None:\n            for st in state_tensors:\n                self.role.register_state_tensor(st)\n\n        self.include_state = include_state\n        self.is_building = False\n        self.state_attributes = {}\n        self.is_built = is_built\n        self.torchscript = None\n        self.input_types = input_types\n        self.validate_input_types = True\n        self.tracing = False\n\n        # The plan has not been sent so it has no reference to remote locations\n        self.pointers = {}\n\n        if not hasattr(self, ""forward""):\n            self.forward = forward_func or None\n\n        self.__name__ = self.__repr__()  # For PyTorch jit tracing compatibility\n\n        # List of available translations\n        self.translations = []\n\n    @property\n    def state(self):\n        return self.role.state\n\n    @property\n    def actions(self):\n        return self.role.actions\n\n    def parameters(self):\n        """"""\n        This is defined to match the torch api of nn.Module where .parameters()\n\n        Returns:\n            The model tensors / parameters\n        """"""\n        if self.state is not None:\n            return self.state.tensors()\n        else:\n            return []\n\n    def build(self, *args, trace_autograd=False):\n        """"""Builds the plan.\n\n        First, run the function to be converted in a plan in a context which\n        activates the tracing and record the actions in trace.logs\n\n        Second, store the result ids temporarily to helper ordering the output\n        placeholders at return time\n\n        Third, loop through the trace logs and replace the tensors found in the\n        actions logged by PlaceHolders. Record those actions in\n        plan.actions\n\n        Args:\n            args: Input arguments to run the plan\n        """"""\n        # Reset previous build\n        self.role.reset()\n\n        def build_nested_arg(arg, leaf_function):\n            if isinstance(arg, list):\n                return [build_nested_arg(obj, leaf_function) for obj in arg]\n            elif isinstance(arg, tuple):\n                return tuple(build_nested_arg(obj, leaf_function) for obj in arg)\n            elif isinstance(arg, dict):\n                return {k: build_nested_arg(v, leaf_function) for k, v in arg.items()}\n            else:\n                return leaf_function(arg)\n\n        # Enable tracing\n        self.toggle_tracing(True)\n        self.is_building = True\n\n        # Check the types\n        self.input_types = NestedTypeWrapper(args)\n\n        # Run once to build the plan\n        if trace_autograd:\n            # Wrap arguments that require gradients with AutogradTensor,\n            # to be able to trace autograd operations\n            args = build_nested_arg(\n                args,\n                lambda x: AutogradTensor().on(x, wrap=False)\n                if isinstance(x, FrameworkTensor)\n                else PlaceHolder.create_from(x, role=self.role, tracing=True),\n            )\n            # Add Placeholder after AutogradTensor in the chain\n            # so that all operations that happen inside AutogradTensor are recorded by Placeholder\n            args_placeholders = build_nested_arg(\n                args,\n                lambda x: PlaceHolder.insert(x, AutogradTensor, role=self.role, tracing=True)\n                if not isinstance(x, PlaceHolder)\n                else x,\n            )\n        else:\n            # Add Placeholder on top of each arg\n            args = args_placeholders = build_nested_arg(\n                args, lambda x: PlaceHolder.create_from(x, role=self.role, tracing=True),\n            )\n\n        # Add state to args if needed\n        if self.include_state:\n            args += (self.state,)\n\n        # Check the plan arguments to see what framework wrappers we might need to send to the plan\n        framework_kwargs = {}\n\n        forward_args = inspect.getfullargspec(self.forward).args\n        for f_name, wrap_framework_func in Plan._wrapped_frameworks.items():\n            if f_name in forward_args:\n                framework_kwargs[f_name] = wrap_framework_func(self.role)\n\n        results = self.forward(*args, **framework_kwargs)\n\n        # Register inputs in role\n        self.role.register_inputs(args_placeholders)\n\n        # Register outputs in role\n        if isinstance(results, (tuple, list)):\n            results_placeholders = tuple(PlaceHolder.extract(result) for result in results)\n        else:\n            results_placeholders = PlaceHolder.extract(results)\n        self.role.register_outputs(results_placeholders)\n\n        # Disable tracing\n        self.toggle_tracing(False)\n        self.is_building = False\n        self.is_built = True\n\n        # Build registered translations\n        for translator in Plan._build_translators:\n            try:\n                self.add_translation(translator)\n                self.translations.append(translator)\n            except:\n                warnings.warn(f""Failed to translate Plan with {translator.__name__}"")\n\n        return results\n\n    def toggle_tracing(self, value=None):\n        self.tracing = value if value is not None else not self.tracing\n        self.state.tracing = self.tracing\n        for ph in self.role.placeholders.values():\n            ph.tracing = self.tracing\n\n    def copy(self):\n        """"""Creates a copy of a plan.""""""\n        plan_copy = Plan(\n            name=self.name,\n            role=self.role.copy(),\n            include_state=self.include_state,\n            is_built=self.is_built,\n            id=sy.ID_PROVIDER.pop(),\n            owner=self.owner,\n            tags=self.tags,\n            input_types=self.input_types,\n            description=self.description,\n        )\n\n        plan_copy.torchscript = self.torchscript\n\n        return plan_copy\n\n    def __setattr__(self, name, value):\n        """"""Add new tensors or parameter attributes to the state and register them\n        in the owner\'s registry\n        """"""\n        if isinstance(value, torch.jit.ScriptModule):\n            object.__setattr__(self, name, value)\n        elif isinstance(value, FrameworkTensor):\n            self.role.register_state_tensor(value)\n            self.state_attributes[name] = value\n        elif isinstance(value, FrameworkLayerModule):\n            for param in value.parameters():\n                self.role.register_state_tensor(param)\n            self.state_attributes[name] = value\n        else:\n            object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        if name not in self.state_attributes:\n            raise AttributeError(""State attribute not found."")\n\n        value = self.state_attributes[name]\n        if not self.is_building:\n            return value\n\n        if isinstance(value, FrameworkTensor):\n            return self.role.placeholders[value.id]\n        elif isinstance(value, FrameworkLayerModule):\n            # We need to deepcopy here otherwise the real layer is modified when the\n            # Plan is being built\n            copied_layer = copy.deepcopy(value)\n            for copied_param, param in zip(copied_layer.named_parameters(), value.parameters()):\n                (copied_name, _) = copied_param\n                copied_layer._parameters[copied_name] = self.role.placeholders[param.id]\n\n            return copied_layer\n\n    def __call__(self, *args):\n        """"""\n        Calls a plan execution with some arguments.\n\n        When possible, run the original function to improve efficiency. When\n        it\'s not, for example if you fetched the plan from a remote worker,\n        then run it from the tape of actions:\n        - Instantiate input placeholders\n        - for each recorded action, run the action on the placeholders\n          and use the result(s) to instantiate to appropriate placeholder.\n        - Return the instantiation of all the output placeholders.\n        """"""\n        if self.forward is not None:\n            if self.include_state:\n                args = (*args, self.state)\n            return self.forward(*args)\n        else:\n            if self.validate_input_types:\n                self.input_types.input_check(self, args)\n            self.role.instantiate_inputs(args)\n            result = self.role.execute()\n            if len(result) == 1:\n                return result[0]\n            return result\n\n    def run(self, args_: Tuple, result_ids: List[Union[str, int]]):\n        """"""Controls local or remote plan execution.\n        If the plan doesn\'t have the plan built, first build it using the original function.\n\n        Args:\n            args_: Arguments used to run plan.\n            result_ids: List of ids where the results will be stored.\n        """"""\n        # TODO: can we reuse result_ids?\n        return self.__call__(*args_)\n\n    def send(self, *locations: AbstractWorker) -> PointerPlan:\n        """"""Send plan to locations.\n\n        If the plan was not built locally it will raise an exception.\n        If `force` = true plan is going to be sent either way.\n\n        Args:\n            locations: List of workers.\n            force: A boolean indicating if this action should be forced.\n        """"""\n        if not self.is_built:\n            raise RuntimeError(""A plan needs to be built before being sent to a worker."")\n\n        if len(locations) == 1:\n            location = locations[0]\n\n            # Check if plan was already sent at the location\n            if location in self.pointers:\n                return self.pointers[location]\n\n            # Send the Plan\n            pointer = self.owner.send(self, workers=location)\n\n            self.pointers[location] = pointer\n        else:\n            ids_at_location = []\n            for location in locations:\n                if location in self.pointers:\n                    # Use the pointer that was already sent\n                    pointer = self.pointers[location]\n                else:\n                    # Send the Plan\n                    pointer = self.owner.send(self, workers=location)\n\n                    self.pointers[location] = pointer\n\n                ids_at_location.append(pointer.id_at_location)\n\n            pointer = sy.PointerPlan(location=locations, id_at_location=ids_at_location)\n\n        return pointer\n\n    def get_args_shape(self):\n        """"""Returns input tensors shapes""""""\n        if not self.is_built:\n            raise RuntimeError(""A plan needs to be built before input shapes can be known."")\n\n        return [ph.expected_shape for ph in self.role.input_placeholders()]\n\n    def create_dummy_args(self):\n        """"""Returns dummy arguments matching built Plan arguments\' types""""""\n        if not self.is_built:\n            raise RuntimeError(""A plan needs to be built before input shapes can be known."")\n\n        def traverse_nested_types(arg, leaf_function):\n            if isinstance(arg, list):\n                return [traverse_nested_types(obj, leaf_function) for obj in arg]\n            elif isinstance(arg, tuple):\n                return tuple(traverse_nested_types(obj, leaf_function) for obj in arg)\n            elif isinstance(arg, dict):\n                return {k: traverse_nested_types(v, leaf_function) for k, v in arg.items()}\n            else:\n                return leaf_function(arg)\n\n        input_placeholders = (ph for ph in self.role.input_placeholders())\n\n        def create_dummy(input_type, input_placeholder):\n            if issubclass(input_type, FrameworkTensor):\n                return input_type(\n                    PlaceHolder.create_placeholders([input_placeholder.expected_shape])[0]\n                )\n            else:\n                return input_type()\n\n        return traverse_nested_types(\n            self.input_types.nested_input_types,\n            lambda input_type: create_dummy(input_type, input_placeholders.__next__()),\n        )\n\n    @staticmethod\n    def register_build_translator(translator: ""AbstractPlanTranslator""):\n        Plan._build_translators.append(translator)\n\n    @staticmethod\n    def register_framework(f_name, f_package):\n        """"""\n        When we use methods defined in a framework (like: torch.randn) we have a framework\n        wrapper that helps as register and keep track of what methods are called\n        With the below lines, we ""register"" what frameworks we have support to handle\n        Args:\n            f_name (String): framework name (eg. torch, crypten)\n            f_package (imported module): imported library\n        """"""\n\n        def call_wrapped_framework(role):\n            return FrameworkWrapper(f_package, role)\n\n        Plan._wrapped_frameworks[f_name] = call_wrapped_framework\n\n    def add_translation(self, plan_translator: ""AbstractPlanTranslator""):\n        return plan_translator(self).translate()\n\n    def remove_translation(self, plan_translator: ""AbstractPlanTranslator"" = PlanTranslatorDefault):\n        plan_translator(self).remove()\n        return self\n\n    def get_(self):\n        self.state.get_()\n        return self\n\n    get = get_\n\n    def get_pointers(self):\n        return self.pointers\n\n    def fix_precision_(self, *args, **kwargs):\n        self.state.fix_precision_(*args, **kwargs)\n        return self\n\n    fix_precision = fix_prec_ = fix_prec = fix_precision_\n\n    def float_precision_(self):\n        self.state.float_precision_()\n        return self\n\n    float_precision = float_prec_ = float_prec = float_precision_\n\n    def share_(self, *args, **kwargs):\n        self.state.share_(*args, **kwargs)\n        return self\n\n    share = share_\n\n    def create_pointer(\n        self, owner, garbage_collect_data, location=None, id_at_location=None, tags=None, **kwargs\n    ):\n        """"""\n        Create a pointer to the plan\n\n        Args:\n            owner: the owner of the pointer\n            garbage_collect_data: if true, when the pointer is deleted, the remote target\n                        is garbaged collected\n            location: the location of the pointer\n            id_at_location: the remote id at location\n            tags: the tags inherited from the Plan\n\n        Returns:\n            PointerPlan: pointer to the plan\n        """"""\n        return PointerPlan(\n            owner=owner,\n            location=location or self.owner,\n            id_at_location=id_at_location or self.id,\n            garbage_collect_data=garbage_collect_data,\n            tags=tags,\n        )\n\n    def __str__(self):\n        """"""Returns the string representation of Plan.""""""\n        out = ""<""\n        out += str(type(self)).split(""\'"")[1].split(""."")[-1]\n        out += "" "" + str(self.name)\n        out += "" id:"" + str(self.id)\n        out += "" owner:"" + str(self.owner.id)\n\n        if self.tags is not None and len(self.tags):\n            out += "" Tags:""\n            for tag in self.tags:\n                out += "" "" + str(tag)\n\n        if self.is_built:\n            out += "" built""\n\n        out += "">""\n        out += ""\\n""\n        _self = self\n\n        # out += f""def {self.name}(""\n        # out += "", "".join(f""arg_{extract_tag(p)}"" for p in self.find_placeholders(""input""))\n        # out += ""):\\n""\n        # for action in self.actions:\n        #     line = ""    ""\n        #     if action.return_ids is not None:\n        #         if isinstance(action.return_ids, PlaceHolder):\n        #             tag = extract_tag(action.return_ids)\n        #             line += f""_{tag} = ""\n        #         elif isinstance(action.return_ids, tuple):\n        #             line += (\n        #                 "", "".join(\n        #                     f""_{extract_tag(o)}"" if isinstance(o, PlaceHolder) else str(o)\n        #                     for o in action.return_ids\n        #                 )\n        #                 + "" = ""\n        #             )\n        #         else:\n        #             line += str(action.return_ids) + "" = ""\n        #     if action.target is not None:\n        #         line += f""_{extract_tag(self.placeholders[action.target.value])}.""\n        #     line += action.name + ""(""\n        #     line += "", "".join(\n        #         f""_{extract_tag(arg)}"" if isinstance(arg, PlaceHolder) else str(arg)\n        #         for arg in action.args\n        #     )\n        #     if action.kwargs:\n        #         line += "", "" + "", "".join(f""{k}={w}"" for k, w in action.kwargs.items())\n        #     line += "")\\n""\n        #     out += line\n\n        # out += ""    return ""\n        # out += "", "".join(f""_{extract_tag(p)}"" for p in self.find_placeholders(""output""))\n\n        return out\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def replace_non_instanciated_placeholders(plan: ""Plan"") -> ""Plan"":\n        # Replace non-instanciated placeholders from plan.placeholders by instanciated placeholders\n        # from state.state_placeholders\n        # NOTE Maybe state shouldn\'t contain instanciated placeholders but values directly?\n        state_placeholders = {ph.id.value: ph for ph in plan.state.state_placeholders}\n        plan.placeholders = {**plan.placeholders, **state_placeholders}\n\n        return plan\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, plan: ""Plan"") -> tuple:\n        """"""\n        This function takes the attributes of a Plan and saves them in a tuple\n        Args:\n            worker (AbstractWorker): the worker doing the serialization\n            plan (Plan): a Plan object\n        Returns:\n            tuple: a tuple holding the unique attributes of the Plan object\n\n        """"""\n        if not plan.is_built:\n            raise RuntimeError(""A Plan needs to be built before being serialized."")\n\n        return (\n            sy.serde.msgpack.serde._simplify(worker, plan.id),\n            sy.serde.msgpack.serde._simplify(worker, plan.role),\n            sy.serde.msgpack.serde._simplify(worker, plan.include_state),\n            sy.serde.msgpack.serde._simplify(worker, plan.name),\n            sy.serde.msgpack.serde._simplify(worker, plan.tags),\n            sy.serde.msgpack.serde._simplify(worker, plan.description),\n            sy.serde.msgpack.serde._simplify(worker, plan.torchscript),\n            sy.serde.msgpack.serde._simplify(worker, plan.input_types),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, plan_tuple: tuple) -> ""Plan"":\n        """"""This function reconstructs a Plan object given its attributes in the form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            plan_tuple: a tuple holding the attributes of the Plan\n        Returns:\n            plan: a Plan object\n        """"""\n        (id_, role, include_state, name, tags, description, torchscript, input_types,) = plan_tuple\n\n        id_ = sy.serde.msgpack.serde._detail(worker, id_)\n        role = sy.serde.msgpack.serde._detail(worker, role)\n        name = sy.serde.msgpack.serde._detail(worker, name)\n        tags = sy.serde.msgpack.serde._detail(worker, tags)\n        description = sy.serde.msgpack.serde._detail(worker, description)\n        torchscript = sy.serde.msgpack.serde._detail(worker, torchscript)\n        input_types = sy.serde.msgpack.serde._detail(worker, input_types)\n\n        plan = sy.Plan(\n            role=role,\n            include_state=include_state,\n            is_built=True,\n            id=id_,\n            owner=worker,\n            name=name,\n            tags=tags,\n            description=description,\n            input_types=input_types,\n        )\n\n        plan.torchscript = torchscript\n\n        return plan\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, plan: ""Plan"") -> PlanPB:\n        """"""\n        This function takes the attributes of a Plan and saves them in a Protobuf message\n        Args:\n            worker (AbstractWorker): the worker doing the serialization\n            plan (Plan): a Plan object\n        Returns:\n            PlanPB: a Protobuf message holding the unique attributes of the Plan object\n        """"""\n        if not plan.is_built:\n            raise RuntimeError(""A Plan needs to be built before being serialized."")\n\n        protobuf_plan = PlanPB()\n\n        sy.serde.protobuf.proto.set_protobuf_id(protobuf_plan.id, plan.id)\n\n        protobuf_plan.role.CopyFrom(sy.serde.protobuf.serde._bufferize(worker, plan.role))\n\n        protobuf_plan.include_state = plan.include_state\n        protobuf_plan.name = plan.name\n        protobuf_plan.tags.extend(plan.tags)\n\n        if protobuf_plan.description:\n            protobuf_plan.description = plan.description\n\n        if plan.torchscript:\n            protobuf_plan.torchscript = plan.torchscript.save_to_buffer()\n\n        if plan.input_types:\n            input_types = sy.serde.protobuf.serde._bufferize(worker, plan.input_types)\n            protobuf_plan.input_types.CopyFrom(input_types)\n\n        return protobuf_plan\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_plan: PlanPB) -> ""Plan"":\n        """"""This function reconstructs a Plan object given its attributes in the form of a Protobuf message\n        Args:\n            worker: the worker doing the deserialization\n            protobuf_plan: a Protobuf message holding the attributes of the Plan\n        Returns:\n            plan: a Plan object\n        """"""\n        id_ = sy.serde.protobuf.proto.get_protobuf_id(protobuf_plan.id)\n\n        role = sy.serde.protobuf.serde._unbufferize(worker, protobuf_plan.role)\n\n        name = protobuf_plan.name\n        tags = set(protobuf_plan.tags) if protobuf_plan.tags else None\n        description = protobuf_plan.description if protobuf_plan.description else None\n        input_types = sy.serde.protobuf.serde._unbufferize(worker, protobuf_plan.input_types)\n\n        plan = Plan(\n            role=role,\n            include_state=protobuf_plan.include_state,\n            is_built=True,\n            id=id_,\n            owner=worker,\n            name=name,\n            tags=tags,\n            description=description,\n            input_types=input_types,\n        )\n\n        if protobuf_plan.torchscript:\n            torchscript = io.BytesIO(protobuf_plan.torchscript)\n            plan.torchscript = torch.jit.load(torchscript)\n\n        return plan\n\n    @property\n    def code(self) -> str:\n        """"""Returns string representation of Plan actions""""""\n        input_names = {id: f""arg_{i + 1}"" for i, id in enumerate(self.role.input_placeholder_ids)}\n        output_names = {id: f""out_{i + 1}"" for i, id in enumerate(self.role.output_placeholder_ids)}\n        state_names = {\n            ph.id.value: f""state_{i + 1}"" for i, ph in enumerate(self.role.state.state_placeholders)\n        }\n        var_names = {**input_names, **output_names, **state_names}\n\n        out = f""def {self.name}(""\n        out += "", "".join([var_names[id] for id in self.role.input_placeholder_ids])\n        out += ""):\\n""\n        for action in self.role.actions:\n            out += f""    {action.code(var_names)}\\n""\n\n        out += ""    return ""\n        out += "", "".join([var_names[id] for id in self.role.output_placeholder_ids])\n\n        return out\n\n    @staticmethod\n    def get_protobuf_schema() -> PlanPB:\n        return PlanPB\n\n\n# Auto-register Plan build-time translations\nPlan.register_build_translator(PlanTranslatorTorchscript)\n\n# Auto-register Plan build-time frameworks\nfor f_name, f_package in framework_packages.items():\n    Plan.register_framework(f_name, f_package)\n'"
syft/execution/protocol.py,0,"b'from typing import Dict\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport syft as sy\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.execution.role import Role\nfrom syft.execution.role_assignments import RoleAssignments\n\nfrom syft.generic.abstract.sendable import AbstractSendable\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.workers.virtual import VirtualWorker\n\nfrom syft_proto.execution.v1.protocol_pb2 import Protocol as ProtocolPB\n\n\nclass func2protocol(object):\n    """"""Decorator which converts a function to a protocol.\n\n    Converts a function containing sequential pytorch code into\n    a protocol object which can be sent to any arbitrary worker.\n\n    This class should be used only as a decorator.\n    """"""\n\n    def __init__(self, roles: list = [], args_shape: dict = {}, states={}):\n        self.role_names = roles\n        self.args_shape = args_shape\n        self.states = states\n\n    def __call__(self, protocol_function):\n        # create the roles present in decorator\n        roles = {\n            role_id: Role(worker=VirtualWorker(id=role_id, hook=sy.local_worker.hook))\n            for role_id in self.role_names\n        }\n        for role_id, state_tensors in self.states.items():\n            for tensor in state_tensors:\n                roles[role_id].register_state_tensor(tensor)\n\n        protocol = Protocol(\n            name=protocol_function.__name__,\n            forward_func=protocol_function,\n            roles=roles,\n            id=sy.ID_PROVIDER.pop(),\n            owner=sy.local_worker,\n        )\n\n        try:\n            protocol.build()\n        except TypeError as e:\n            raise ValueError(\n                ""Automatic build using @func2protocol failed!\\nCheck that:\\n""\n                "" - you have provided the correct number of shapes in args_shape\\n""\n                "" - you have no simple numbers like int or float as args. If you do ""\n                ""so, please consider using a tensor instead.""\n            )\n\n        return protocol\n\n\nclass Protocol(AbstractSendable):\n    """"""\n    A Protocol stores a sequence of actions, just like a function.\n\n    A Protocol is intended to store a sequence of actions, just like a function,\n    but it allows to send this sequence of actions to remote workers and to keep a\n    reference to it. This way, to compute remotely this sequence of actions on some remote\n    input referenced through pointers, instead of sending multiple messages you need now to send a\n    single message with the references of the protocol and the pointers.\n\n    All arguments are optional.\n\n    Args:\n        name: the name of the name\n        is_built: state if the protocol has already been built.\n        forward_func: the function to be transformed into a protocol\n        id: protocol id\n        owner: protocol owner\n        tags: protocol tags\n        description: protocol description\n    """"""\n\n    def __init__(\n        self,\n        name: str = None,\n        is_built: bool = False,\n        forward_func=None,\n        roles: Dict[str, Role] = {},\n        # General kwargs\n        id: Union[str, int] = None,\n        owner: ""sy.workers.BaseWorker"" = None,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n        super().__init__(id, owner, tags, description, child=None)\n\n        # Protocol instance info\n        self.name = name or self.__class__.__name__\n\n        self.roles = roles\n        self.role_assignments = RoleAssignments(roles.keys())\n\n        self.is_building = False\n        self.is_built = is_built\n        self.torchscript = None\n        self.tracing = False\n\n        if not hasattr(self, ""forward""):\n            self.forward = forward_func or None\n\n        self.__name__ = self.__repr__()  # For PyTorch jit tracing compatibility\n\n    def build(self):\n        """"""Builds the protocol.\n\n        First, run the function to be converted in a protocol in a context which\n        activates the tracing and record the actions in trace.logs\n\n        Second, store the result ids temporarily to helper ordering the output\n        placeholders at return time\n\n        Third, loop through the trace logs and replace the tensors found in the\n        actions logged by PlaceHolders. Record those actions in\n        protocol.actions\n\n        Args:\n            args: Input arguments to run the protocol\n        """"""\n        # Reset previous build\n        for role in self.roles.values():\n            role.reset()\n\n        # Enable tracing\n        self.toggle_tracing(True)\n        self.is_building = True\n\n        results = self.forward(*self.roles.values())\n\n        # Disable tracing\n        self.toggle_tracing(False)\n        self.is_building = False\n\n        if not isinstance(results, (tuple, list)):\n            results = (results,)\n\n        # Register outputs in roles\n        for result in results:\n            if isinstance(result, PlaceHolder):\n                result.role.register_output(result)\n\n        self.is_built = True\n\n        return results\n\n    def toggle_tracing(self, value=None):\n        self.tracing = value if value is not None else not self.tracing\n        # self.state.tracing = self.tracing\n        for role in self.roles.values():\n            role.tracing = value or not self.tracing\n            for ph in role.placeholders.values():\n                ph.tracing = self.tracing\n\n    def copy(self):\n        """"""Creates a copy of a protocol.""""""\n        protocol_copy = Protocol(\n            name=self.name,\n            roles={role_id: role.copy() for role_id, role in self.roles.items()},\n            is_built=self.is_built,\n            id=sy.ID_PROVIDER.pop(),\n            owner=self.owner,\n            tags=self.tags,\n            description=self.description,\n        )\n\n        protocol_copy.torchscript = self.torchscript\n\n        return protocol_copy\n\n    def __call__(self):\n        """"""\n        Run actions on the workers provided for each Role from the Role\'s tape of actions.\n        """"""\n        results_per_role = {}\n        for role_id, role in self.roles.items():\n            results_per_role[role_id] = role.execute()\n\n        return results_per_role\n\n    def run(self, args_: Tuple, result_ids: List[Union[str, int]]):\n        """"""Controls local or remote protocol execution.\n        If the protocol doesn\'t have the protocol built, first build it using the original function.\n\n        Args:\n            args_: Arguments used to run protocol.\n            result_ids: List of ids where the results will be stored.\n        """"""\n        # TODO: can we reuse result_ids?\n        return self.__call__(*args_)\n\n    def assign(self, role_id, worker):\n        """""" Assign a worker to the specified role.\n        """"""\n        self.role_assignments.assign(role_id, worker)\n\n    def assign_roles(self, worker_dict):\n        """""" Assign worker values to correspondent key role.\n        """"""\n        for role_id, worker in worker_dict.items():\n            self.role_assignments.assign(role_id, worker)\n\n    @staticmethod\n    def replace_non_instanciated_placeholders(protocol: ""Protocol"") -> ""Protocol"":\n        # Replace non-instanciated placeholders from protocol.placeholders by\n        # instanciated placeholders from state.state_placeholders\n        # NOTE Maybe state shouldn\'t contain instanciated placeholders but values directly?\n        state_placeholders = {ph.id.value: ph for ph in protocol.state.state_placeholders}\n        protocol.placeholders = {**protocol.placeholders, **state_placeholders}\n\n        return protocol\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, protocol: ""Protocol"") -> tuple:\n        """"""\n        This function takes the attributes of a Protocol and saves them in a tuple\n        Args:\n            worker (AbstractWorker): the worker doing the serialization\n            protocol (Protocol): a Protocol object\n        Returns:\n            tuple: a tuple holding the unique attributes of the Protocol object\n\n        """"""\n        if not protocol.is_built:\n            raise RuntimeError(""A Protocol needs to be built before being serialized."")\n\n        return (\n            sy.serde.msgpack.serde._simplify(worker, protocol.id),\n            sy.serde.msgpack.serde._simplify(worker, protocol.name),\n            sy.serde.msgpack.serde._simplify(worker, protocol.roles),\n            sy.serde.msgpack.serde._simplify(worker, protocol.tags),\n            sy.serde.msgpack.serde._simplify(worker, protocol.description),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, protocol_tuple: tuple) -> ""Protocol"":\n        """"""This function reconstructs a Protocol object given its attributes in the form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            protocol_tuple: a tuple holding the attributes of the Protocol\n        Returns:\n            protocol: a Protocol object\n        """"""\n        (id_, name, roles, tags, description) = protocol_tuple\n\n        id_ = sy.serde.msgpack.serde._detail(worker, id_)\n        name = sy.serde.msgpack.serde._detail(worker, name)\n        roles = sy.serde.msgpack.serde._detail(worker, roles)\n        tags = sy.serde.msgpack.serde._detail(worker, tags)\n        description = sy.serde.msgpack.serde._detail(worker, description)\n\n        return sy.Protocol(\n            id=id_,\n            name=name,\n            owner=worker,\n            roles=roles,\n            is_built=True,\n            tags=tags,\n            description=description,\n        )\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, protocol: ""Protocol"") -> ProtocolPB:\n        """"""\n        This function takes the attributes of a Protocol and saves them in a Protobuf message\n        Args:\n            worker (AbstractWorker): the worker doing the serialization\n            protocol (Protocol): a Protocol object\n        Returns:\n            ProtocolPB: a Protobuf message holding the unique attributes of the Protocol object\n        """"""\n        if not protocol.is_built:\n            raise RuntimeError(""A Protocol needs to be built before being serialized."")\n\n        protobuf_protocol = ProtocolPB()\n\n        sy.serde.protobuf.proto.set_protobuf_id(protobuf_protocol.id, protocol.id)\n        protobuf_protocol.name = protocol.name\n\n        for role_id, role in protocol.roles.items():\n            protobuf_protocol.roles.get_or_create(role_id).CopyFrom(\n                sy.serde.protobuf.serde._bufferize(worker, role)\n            )\n\n        protobuf_protocol.tags.extend(protocol.tags)\n\n        if protocol.description:\n            protobuf_protocol.description = protocol.description\n\n        return protobuf_protocol\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_protocol: ProtocolPB) -> ""Protocol"":\n        """"""This function reconstructs a Protocol object given its attributes in the form\n        of a Protobuf message\n\n        Args:\n            worker: the worker doing the deserialization\n            protobuf_protocol: a Protobuf message holding the attributes of the Protocol\n        Returns:\n            protocol: a Protocol object\n        """"""\n        id_ = sy.serde.protobuf.proto.get_protobuf_id(protobuf_protocol.id)\n        name = protobuf_protocol.name\n\n        roles = {\n            role_id: sy.serde.protobuf.serde._unbufferize(worker, role)\n            for role_id, role in protobuf_protocol.roles.items()\n        }\n\n        tags = set(protobuf_protocol.tags) if protobuf_protocol.tags else None\n        description = protobuf_protocol.description if protobuf_protocol.description else None\n\n        return Protocol(\n            id=id_,\n            name=name,\n            roles=roles,\n            is_built=True,\n            owner=worker,\n            tags=tags,\n            description=description,\n        )\n\n    @staticmethod\n    def get_protobuf_schema() -> ProtocolPB:\n        return ProtocolPB\n'"
syft/execution/role.py,0,"b'from typing import Dict\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\nfrom typing import Callable\n\nfrom syft.generic.frameworks import framework_packages\n\nimport syft as sy\nfrom syft.execution.action import Action\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.execution.placeholder_id import PlaceholderId\nfrom syft.execution.state import State\nfrom syft.execution.tracing import FrameworkWrapper\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft_proto.execution.v1.role_pb2 import Role as RolePB\n\n\nclass Role(SyftSerializable):\n    """"""\n    Roles will mainly be used to build protocols but are still a work in progress.\n    """"""\n\n    def __init__(\n        self,\n        id: Union[str, int] = None,\n        worker: AbstractWorker = None,\n        state: State = None,\n        actions: List[Action] = None,\n        placeholders: Dict[Union[str, int], PlaceHolder] = None,\n        input_placeholder_ids: Tuple[int, str] = None,\n        output_placeholder_ids: Tuple[int, str] = None,\n    ):\n        self.id = id or sy.ID_PROVIDER.pop()\n        self.worker = worker or sy.local_worker\n\n        self.actions = actions or []\n\n        # All placeholders\n        self.placeholders = placeholders or {}\n        # Input placeholders, stored by id\n        self.input_placeholder_ids = input_placeholder_ids or ()\n        # Output placeholders\n        self.output_placeholder_ids = output_placeholder_ids or ()\n\n        self.state = state or State()\n        self.tracing = False\n\n        for name, package in framework_packages.items():\n            tracing_wrapper = FrameworkWrapper(package=package, role=self)\n            setattr(self, name, tracing_wrapper)\n\n    def input_placeholders(self):\n        return [self.placeholders[id_] for id_ in self.input_placeholder_ids]\n\n    def output_placeholders(self):\n        return [self.placeholders[id_] for id_ in self.output_placeholder_ids]\n\n    @staticmethod\n    def nested_object_traversal(obj: any, leaf_function: Callable, leaf_type: type):\n        """"""\n        Class method to iterate through a tree-like structure, where the branching is determined\n        by the elements of list, tuples and dicts, returning the same tree-like structure with a\n        function applied to its leafs.\n\n        Args:\n            obj: The tree-like structure, can be only the root as well.\n            leaf_function: The function to be applied on the leaf nodes of the tree-like structure.\n            leaf_type: On what type on function to apply the function, if the types won\'t match,\n            the leaf is returned, to apply on all leafs pass any.\n\n        Returns:\n            Same structure as the obj argument, but with the function applied to the leaf elements.\n        """"""\n        if isinstance(obj, (list, tuple)):\n            result = [Role.nested_object_traversal(elem, leaf_function, leaf_type) for elem in obj]\n            return type(obj)(result)\n        elif isinstance(obj, dict):\n            return {\n                k: Role.nested_object_traversal(v, leaf_function, leaf_type)\n                for k, v in sorted(obj.items())\n            }\n        elif isinstance(obj, leaf_type):\n            return leaf_function(obj)\n        else:\n            return obj\n\n    def register_input(self, arg_):\n        """""" Takes input argument for this role and generate placeholder.\n        """"""\n        self.input_placeholder_ids += (self._store_placeholders(arg_).value,)\n\n    def register_inputs(self, args_):\n        """""" Takes input arguments for this role and generate placeholders.\n        """"""\n        # TODO Should we be able to rebuild?\n        def traversal_function(obj):\n            if obj.id.value not in self.placeholders:\n                self.placeholders[obj.id.value] = obj\n            self.input_placeholder_ids.append(obj.id.value)\n\n        self.input_placeholder_ids = []\n        Role.nested_object_traversal(args_, traversal_function, PlaceHolder)\n        self.input_placeholder_ids = tuple(self.input_placeholder_ids)\n\n    def register_output(self, result):\n        """""" Takes output tensor for this role and generate placeholder.\n        """"""\n        self.output_placeholder_ids += (self._store_placeholders(result).value,)\n\n    def register_outputs(self, results):\n        """""" Takes output tensors for this role and generate placeholders.\n        """"""\n\n        def traversal_function(obj):\n            if obj.id.value not in self.placeholders:\n                self.placeholders[obj.id.value] = obj\n            self.output_placeholder_ids.append(obj.id.value)\n\n        results = (results,) if not isinstance(results, tuple) else results\n        self.output_placeholder_ids = []\n        Role.nested_object_traversal(results, traversal_function, PlaceHolder)\n        self.output_placeholder_ids = tuple(self.output_placeholder_ids)\n\n    def register_action(self, traced_action, action_type):\n        """""" Build placeholders and store action.\n        """"""\n        command, response = traced_action\n        command_placeholder_ids = self._store_placeholders(command)\n        return_placeholder_ids = None\n\n        if response is not None:\n            return_placeholder_ids = self._store_placeholders(response)\n            if not isinstance(return_placeholder_ids, (list, tuple)):\n                return_placeholder_ids = (return_placeholder_ids,)\n\n        action = action_type(*command_placeholder_ids, return_ids=return_placeholder_ids)\n        self.actions.append(action)\n\n    def register_state_tensor(self, tensor):\n        placeholder = sy.PlaceHolder(id=tensor.id, role=self)\n        placeholder.instantiate(tensor)\n        self.state.state_placeholders.append(placeholder)\n        # TODO isn\'t it weird that state placeholders are both in state and plan?\n        self.placeholders[tensor.id] = placeholder\n\n    def reset(self):\n        """""" Remove the trace actions on this Role to make it possible to build\n        a Plan or a Protocol several times.\n        """"""\n        self.actions = []\n        self.input_placeholder_ids = ()\n        self.output_placeholder_ids = ()\n        # We don\'t want to remove placeholders coming from the state\n        state_ph_ids = [ph.id.value for ph in self.state.state_placeholders]\n        self.placeholders = {\n            ph_id: ph for ph_id, ph in self.placeholders.items() if ph_id in state_ph_ids\n        }\n\n    def execute(self):\n        """""" Make the role execute all its actions.\n        """"""\n        for action in self.actions:\n            self._execute_action(action)\n\n        output_placeholders = tuple(\n            self.placeholders[output_id] for output_id in self.output_placeholder_ids\n        )\n\n        return tuple(p.child for p in output_placeholders)\n\n    def load(self, tensor):\n        """""" Load tensors used in a protocol from worker\'s local store\n        """"""\n        # TODO mock for now, load will use worker\'s store in a future work\n        if self.tracing:\n            return PlaceHolder.create_from(tensor, role=self, tracing=True)\n        else:\n            return tensor\n\n    def load_state(self):\n        """""" Load tensors used in a protocol from worker\'s local store\n        """"""\n        return self.state.read()\n\n    def instantiate_inputs(self, args_):\n        """""" Takes input arguments for this role and generate placeholders.\n        """"""\n\n        def traversal_function(obj):\n            placeholder = input_placeholders.pop(0)\n            placeholder.instantiate(obj)\n\n        input_placeholders = [\n            self.placeholders[input_id] for input_id in self.input_placeholder_ids\n        ]\n\n        Role.nested_object_traversal(args_, traversal_function, FrameworkTensor)\n\n    def _execute_action(self, action):\n        """""" Build placeholders and store action.\n        """"""\n        cmd, _self, args_, kwargs_, return_values = (\n            action.name,\n            action.target,  # target is equivalent to the ""self"" in a method\n            action.args,\n            action.kwargs,\n            action.return_ids,\n        )\n        _self = self._fetch_placeholders_from_ids(_self)\n        args_ = self._fetch_placeholders_from_ids(args_)\n        kwargs_ = self._fetch_placeholders_from_ids(kwargs_)\n        return_values = self._fetch_placeholders_from_ids(return_values)\n\n        # We can only instantiate placeholders, filter them\n        return_placeholders = []\n        Role.nested_object_traversal(\n            return_values, lambda ph: return_placeholders.append(ph), PlaceHolder\n        )\n\n        if _self is None:\n            method = self._fetch_package_method(cmd)\n            response = method(*args_, **kwargs_)\n        else:\n            response = getattr(_self, cmd)(*args_, **kwargs_)\n\n        if not isinstance(response, (tuple, list)):\n            response = (response,)\n\n        PlaceHolder.instantiate_placeholders(return_placeholders, response)\n\n    def _fetch_package_method(self, cmd):\n        cmd_path = cmd.split(""."")\n\n        package_name = cmd_path[0]\n        subpackage_names = cmd_path[1:-1]\n        method_name = cmd_path[-1]\n\n        package = framework_packages[package_name]\n        for subpackage_name in subpackage_names:\n            package = getattr(package, subpackage_name)\n        method = getattr(package, method_name)\n        return method\n\n    def _store_placeholders(self, obj):\n        """"""\n        Replace in an object all FrameworkTensors with Placeholder ids\n        """"""\n\n        def traversal_function(obj):\n            if obj.id.value not in self.placeholders:\n                self.placeholders[obj.id.value] = obj\n            return obj.id\n\n        return Role.nested_object_traversal(obj, traversal_function, PlaceHolder)\n\n    def _fetch_placeholders_from_ids(self, obj):\n        """"""\n        Replace in an object all ids with Placeholders\n        """"""\n        return Role.nested_object_traversal(\n            obj, lambda x: self.placeholders[x.value], PlaceholderId\n        )\n\n    def copy(self):\n        # TODO not the cleanest method ever\n        placeholders = {}\n        old_ids_2_new_ids = {}\n        for ph in self.placeholders.values():\n            copy = ph.copy()\n            old_ids_2_new_ids[ph.id.value] = copy.id.value\n            placeholders[copy.id.value] = copy\n\n        new_input_placeholder_ids = tuple(\n            old_ids_2_new_ids[self.placeholders[input_id].id.value]\n            for input_id in self.input_placeholder_ids\n        )\n        new_output_placeholder_ids = tuple(\n            old_ids_2_new_ids[self.placeholders[output_id].id.value]\n            for output_id in self.output_placeholder_ids\n        )\n\n        state_placeholders = []\n        for ph in self.state.state_placeholders:\n            new_ph = PlaceHolder(id=old_ids_2_new_ids[ph.id.value]).instantiate(ph.child)\n            state_placeholders.append(new_ph)\n\n        state = State(state_placeholders)\n\n        _replace_placeholder_ids = lambda obj: Role.nested_object_traversal(\n            obj, lambda x: PlaceholderId(old_ids_2_new_ids[x.value]), PlaceholderId\n        )\n\n        new_actions = []\n        for action in self.actions:\n            action_type = type(action)\n            target = _replace_placeholder_ids(action.target)\n            args_ = _replace_placeholder_ids(action.args)\n            kwargs_ = _replace_placeholder_ids(action.kwargs)\n            return_ids = _replace_placeholder_ids(action.return_ids)\n            new_actions.append(action_type(action.name, target, args_, kwargs_, return_ids))\n\n        return Role(\n            state=state,\n            actions=new_actions,\n            placeholders=placeholders,\n            input_placeholder_ids=new_input_placeholder_ids,\n            output_placeholder_ids=new_output_placeholder_ids,\n            id=sy.ID_PROVIDER.pop(),\n        )\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, role: ""Role"") -> tuple:\n        """"""\n        This function takes the attributes of a Role and saves them in a tuple\n        Args:\n            worker (AbstractWorker): the worker doing the serialization\n            role (Role): a Role object\n        Returns:\n            tuple: a tuple holding the attributes of the Role object\n        """"""\n        return (\n            sy.serde.msgpack.serde._simplify(worker, role.id),\n            sy.serde.msgpack.serde._simplify(worker, role.actions),\n            sy.serde.msgpack.serde._simplify(worker, role.state),\n            sy.serde.msgpack.serde._simplify(worker, role.placeholders),\n            role.input_placeholder_ids,\n            role.output_placeholder_ids,\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, role_tuple: ""Role"") -> tuple:\n        """"""\n        This function reconstructs a Role object given its attributes in the form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            role_tuple: a tuple holding the attributes of the Role\n        Returns:\n            role: a Role object\n        """"""\n        (\n            id_,\n            actions,\n            state,\n            placeholders,\n            input_placeholder_ids,\n            output_placeholder_ids,\n        ) = role_tuple\n\n        id_ = sy.serde.msgpack.serde._detail(worker, id_)\n        actions = sy.serde.msgpack.serde._detail(worker, actions)\n        state = sy.serde.msgpack.serde._detail(worker, state)\n        placeholders = sy.serde.msgpack.serde._detail(worker, placeholders)\n\n        # TODO should state.state_placeholders be a dict as self.placeholders?\n        # Then, if placeholder not found in self.placeholders, fetch it from\n        # state.state_placeholders. This would prevent us from having the following lines.\n        # Or need to rethink states\n        for ph in state.state_placeholders:\n            placeholders[ph.id.value] = ph\n\n        role = Role(\n            id=id_,\n            actions=actions,\n            input_placeholder_ids=input_placeholder_ids,\n            output_placeholder_ids=output_placeholder_ids,\n        )\n        for ph in placeholders.values():\n            ph.role = role\n        for ph in state.state_placeholders:\n            ph.role = role\n\n        role.placeholders = placeholders\n        role.state = state\n\n        return role\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, role: ""Role"") -> tuple:\n        """"""\n        This function takes the attributes of a Role and saves them in a Protobuf message\n        Args:\n            worker (AbstractWorker): the worker doing the serialization\n            role (Role): a Role object\n        Returns:\n            RolePB: a Protobuf message holding the unique attributes of the Role object\n        """"""\n        protobuf_role = RolePB()\n\n        sy.serde.protobuf.proto.set_protobuf_id(protobuf_role.id, role.id)\n\n        protobuf_actions = [\n            sy.serde.protobuf.serde._bufferize(worker, action) for action in role.actions\n        ]\n        protobuf_role.actions.extend(protobuf_actions)\n\n        protobuf_role.state.CopyFrom(sy.serde.protobuf.serde._bufferize(worker, role.state))\n\n        protobuf_placeholders = [\n            sy.serde.protobuf.serde._bufferize(worker, placeholder)\n            for placeholder in role.placeholders.values()\n        ]\n        protobuf_role.placeholders.extend(protobuf_placeholders)\n\n        for id_ in role.input_placeholder_ids:\n            sy.serde.protobuf.proto.set_protobuf_id(protobuf_role.input_placeholder_ids.add(), id_)\n        for id_ in role.output_placeholder_ids:\n            sy.serde.protobuf.proto.set_protobuf_id(protobuf_role.output_placeholder_ids.add(), id_)\n\n        return protobuf_role\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_role: RolePB) -> tuple:\n        """"""\n        This function reconstructs a Role object given its attributes in the form of a\n        Protobuf message.\n\n        Args:\n            worker: the worker doing the deserialization\n            protobuf_role: a Protobuf message holding the attributes of the Role\n        Returns:\n            role: a Role object\n        """"""\n        id_ = sy.serde.protobuf.proto.get_protobuf_id(protobuf_role.id)\n\n        actions = [\n            sy.serde.protobuf.serde._unbufferize(worker, action) for action in protobuf_role.actions\n        ]\n\n        state = sy.serde.protobuf.serde._unbufferize(worker, protobuf_role.state)\n\n        placeholders = [\n            sy.serde.protobuf.serde._unbufferize(worker, placeholder)\n            for placeholder in protobuf_role.placeholders\n        ]\n        placeholders = {placeholder.id.value: placeholder for placeholder in placeholders}\n        # TODO should state.state_placeholders be a dict as self.placeholders?\n        # Then, if placeholder not found in self.placeholders, fetch it from\n        # state.state_placeholders. This would prevent us from having the following lines.\n        # Or need to rethink states\n        for ph in state.state_placeholders:\n            placeholders[ph.id.value] = ph\n\n        input_placeholder_ids = tuple(\n            sy.serde.protobuf.proto.get_protobuf_id(ph_id)\n            for ph_id in protobuf_role.input_placeholder_ids\n        )\n        output_placeholder_ids = tuple(\n            sy.serde.protobuf.proto.get_protobuf_id(ph_id)\n            for ph_id in protobuf_role.output_placeholder_ids\n        )\n\n        role = Role(\n            id=id_,\n            actions=actions,\n            input_placeholder_ids=input_placeholder_ids,\n            output_placeholder_ids=output_placeholder_ids,\n        )\n        for ph in placeholders.values():\n            ph.role = role\n        for ph in state.state_placeholders:\n            ph.role = role\n\n        role.placeholders = placeholders\n        role.state = state\n\n        return role\n\n    @staticmethod\n    def get_protobuf_schema() -> RolePB:\n        return RolePB\n'"
syft/execution/role_assignments.py,0,"b'import syft as sy\n\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft.workers.abstract import AbstractWorker\n\n\nclass RoleAssignments(SyftSerializable):\n    """""" This object is basically a map from role ids to workers.\n\n    It is used in Protocol execution.\n    The RoleAssignment associated with a Protocol will be sent to each worker\n    having joined the Protocol before an execution so that each party can know\n    which other parties are participating and communicate with them if needed.\n    """"""\n\n    def __init__(self, role_ids: list = None, assignments: dict = None):\n        """"""\n        Args:\n            role_ids: an iterable containing values that indentify the roles of\n                the Protocol to which the RoleAssignments is associated.\n        """"""\n        if assignments is not None:\n            self.assignments = assignments\n        elif role_ids is not None:\n            self.assignments = {role_id: [] for role_id in role_ids}\n        else:\n            raise ValueError(\n                ""You need to provide role_ids or assignments in RoleAssignments\' constructor""\n            )\n\n    def assign(self, role_id, worker):\n        """""" Assign a specific worker to the specified role.\n        """"""\n        if role_id not in self.assignments:\n            raise ValueError(\n                f""role_id {role_id} not present in RoleAssignments ""\n                f""with roles {\', \'.join(self.assignments.keys())}""\n            )\n\n        if isinstance(worker, list):\n            self.assignments[role_id].extend(worker)\n        else:\n            self.assignments[role_id].append(worker)\n\n    def unassign(self, role_id, worker):\n        """""" Unassign a specific worker from the specified role.\n        """"""\n        if role_id not in self.assignments:\n            raise ValueError(\n                f""role_id {role_id} not present in RoleAssignments ""\n                f""with roles {\', \'.join(self.assignments.keys())}""\n            )\n        self.assignments[role_id].remove(worker)\n\n    def reset(self):\n        """""" Remove all the workers from the assignment dict.\n        """"""\n        self.assignments = {role_id: [] for role_id in self.assignments}\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, assignments: ""RoleAssignments"") -> tuple:\n        """""" Simplify a RoleAssignments object.\n        """"""\n        return (sy.serde.msgpack.serde._simplify(worker, assignments.assignments),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, simplified_assignments: tuple) -> ""RoleAssignments"":\n        """""" Detail a simplified RoleAssignments.\n        """"""\n        (assignments,) = simplified_assignments\n        return RoleAssignments(assignments=sy.serde.msgpack.serde._detail(worker, assignments))\n'"
syft/execution/state.py,2,"b'from typing import List\n\nimport torch\n\nimport syft as sy\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft.workers.abstract import AbstractWorker\nfrom syft_proto.execution.v1.state_pb2 import State as StatePB\nfrom syft_proto.execution.v1.state_tensor_pb2 import StateTensor as StateTensorPB\nfrom syft_proto.types.torch.v1.parameter_pb2 import Parameter as ParameterPB\n\n\nclass State(SyftSerializable):\n    """"""The State is a Plan attribute and is used to send tensors along functions.\n\n    It references Plan tensor or parameters attributes using their name, and make\n    sure they are provided to remote workers who are sent the Plan.\n    """"""\n\n    def __init__(self, state_placeholders=None):\n        self.state_placeholders = state_placeholders or []\n        self.tracing = False\n\n    def __str__(self):\n        """"""Returns the string representation of the State.""""""\n        out = ""<""\n        out += ""State:""\n        for state_placeholder in self.state_placeholders:\n            out += f"" {state_placeholder}""\n        out += "">""\n        return out\n\n    def __repr__(self):\n        return self.__str__()\n\n    def tensors(self) -> List:\n        """"""\n        Fetch and return all the state elements.\n        """"""\n        return [placeholder.child for placeholder in self.state_placeholders]\n\n    def copy(self) -> ""State"":\n        return State(self.state_placeholders.copy())\n\n    def read(self):\n        """"""\n        Return state tensors that are from this plan specifically, but not those\n        of plans including in this plan.\n        If run while a plan is building, declare all the state tensors to the plan\n        currently building.\n        """"""\n        if self.tracing:\n            return list(self.state_placeholders)\n        else:\n            return [ph.child for ph in self.state_placeholders]\n\n    @staticmethod\n    def create_grad_if_missing(tensor):\n        if isinstance(tensor, torch.nn.Parameter) and tensor.grad is None:\n            o = tensor.sum()\n            o.backward()\n            if tensor.grad is not None:\n                tensor.grad -= tensor.grad\n\n    def fix_precision_(self, *args, **kwargs):\n        for tensor in self.tensors():\n            self.create_grad_if_missing(tensor)\n            tensor.fix_precision_(*args, **kwargs)\n\n    def float_precision_(self):\n        for tensor in self.tensors():\n            tensor.float_precision_()\n\n    def share_(self, *args, **kwargs):\n        for tensor in self.tensors():\n            self.create_grad_if_missing(tensor)\n            tensor.share_(*args, **kwargs)\n\n    def get_(self):\n        """"""\n        Get functionality that can only be used when getting back state\n        elements converted to additive shared tensors. Other than this,\n        you shouldn\'t need to the get the state separately.\n        """"""\n        # TODO Make it only valid for AST\n        for tensor in self.tensors():\n            tensor.get_()\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, state: ""State"") -> tuple:\n        """"""\n        Simplify the plan\'s state when sending a plan\n        """"""\n        return (\n            sy.serde.msgpack.serde._simplify(worker, state.state_placeholders),\n            sy.serde.msgpack.serde._simplify(worker, state.tensors()),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, state_tuple: tuple) -> ""State"":\n        """"""\n        Reconstruct the plan\'s state from the state elements and supposed\n        ids.\n        """"""\n        state_placeholders, state_elements = state_tuple\n\n        state_placeholders = sy.serde.msgpack.serde._detail(worker, state_placeholders)\n        state_elements = sy.serde.msgpack.serde._detail(worker, state_elements)\n\n        for state_element in state_elements:\n            worker.register_obj(state_element, obj_id=state_element.id)\n\n        for state_placeholder, state_element in zip(state_placeholders, state_elements):\n            state_placeholder.instantiate(state_element)\n\n        state = State(state_placeholders)\n        return state\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, state: ""State"") -> StatePB:\n        """"""\n        Serialize the State to Protobuf message\n        """"""\n        protobuf_state = StatePB()\n\n        protobuf_placeholders = [\n            sy.serde.protobuf.serde._bufferize(worker, placeholder)\n            for placeholder in state.state_placeholders\n        ]\n        protobuf_state.placeholders.extend(protobuf_placeholders)\n\n        state_tensors = []\n        for tensor in state.tensors():\n            protobuf_tensor = sy.serde.protobuf.serde._bufferize(worker, tensor)\n            state_tensor = StateTensorPB()\n            if type(protobuf_tensor) == ParameterPB:\n                state_tensor.torch_param.CopyFrom(\n                    sy.serde.protobuf.serde._bufferize(worker, tensor)\n                )\n            else:\n                state_tensor.torch_tensor.CopyFrom(\n                    sy.serde.protobuf.serde._bufferize(worker, tensor)\n                )\n            state_tensors.append(state_tensor)\n\n        protobuf_state.tensors.extend(state_tensors)\n\n        return protobuf_state\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_state: StatePB) -> ""State"":\n        """"""\n        Reconstruct the plan\'s state from the state elements and supposed\n        ids.\n        """"""\n        state_placeholders = protobuf_state.placeholders\n        state_elements = protobuf_state.tensors\n\n        state_placeholders = [\n            sy.serde.protobuf.serde._unbufferize(worker, placeholder)\n            for placeholder in protobuf_state.placeholders\n        ]\n\n        state_elements = []\n        for protobuf_tensor in protobuf_state.tensors:\n            tensor = getattr(protobuf_tensor, protobuf_tensor.WhichOneof(""tensor""))\n            state_elements.append(sy.serde.protobuf.serde._unbufferize(worker, tensor))\n\n        for state_element in state_elements:\n            worker.register_obj(state_element, obj_id=state_element.id)\n\n        for state_placeholder, state_element in zip(state_placeholders, state_elements):\n            state_placeholder.instantiate(state_element)\n\n        state = State(state_placeholders)\n        return state\n\n    @staticmethod\n    def get_protobuf_schema() -> StatePB:\n        return StatePB\n'"
syft/execution/tracing.py,0,"b'from types import ModuleType\n\nimport syft as sy\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.generic.frameworks.types import FrameworkTensor\n\n\nclass FrameworkWrapper:\n    def __init__(self, package, role):\n        self.package = package\n        self.role = role\n\n    def __getattr__(self, attr_name):\n        package_attr = getattr(self.package, attr_name)\n        # Forward directly the attribute if it\'s not a function\n        if not callable(package_attr):\n            # If it\'s a sub-module, wrap that for tracing too\n            if isinstance(package_attr, ModuleType):\n                return FrameworkWrapper(package_attr, self.role)\n            else:\n                return package_attr\n\n        def trace_wrapper(*args, **kwargs):\n            """"""creates placeholders and registers ComputationAction to role""""""\n            cmd_name = ""."".join((self.package.__name__, attr_name))\n            command = (cmd_name, None, args, kwargs)\n\n            result = package_attr(*args, **kwargs)\n\n            if isinstance(result, PlaceHolder) or (\n                isinstance(result, (list, tuple))\n                and any(isinstance(r, PlaceHolder) for r in result)\n            ):\n                # In this case, the tracing was already done in Placeholder.handle_func_command\n                return result\n\n            if isinstance(result, FrameworkTensor):\n                result = PlaceHolder.create_from(result, role=self.role, tracing=True)\n                self.role.register_action(\n                    (command, result), sy.execution.computation.ComputationAction\n                )\n            elif isinstance(result, (list, tuple)):\n                result = tuple(\n                    PlaceHolder.create_from(r, role=self.role, tracing=True) for r in result\n                )\n                self.role.register_action(\n                    (command, result), sy.execution.computation.ComputationAction\n                )\n            else:\n                self.role.register_action(\n                    (command, None), sy.execution.computation.ComputationAction\n                )\n\n            return result\n\n        return trace_wrapper\n'"
syft/execution/type_wrapper.py,0,"b'from typing import Union\n\nimport syft as sy\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft_proto.execution.v1.type_wrapper_pb2 import NestedTypeWrapper as NestedTypeWrapperPB\nfrom syft_proto.execution.v1.type_wrapper_pb2 import InputTypeDescriptor as InputTypeDescriptorPB\n\n\nclass NestedTypeWrapper(SyftSerializable):\n    """"""\n        Class for input type serialization and type checking for nested structures.\n    """"""\n\n    def __init__(self, nested_type=None):\n        self.nested_input_types = NestedTypeWrapper.enumerate_nested_types(nested_type)\n\n    @staticmethod\n    def get_object_identifiers(obj: any) -> (str, str):\n        """"""\n        Looks for identifiers for different objects, currently, only plans are supported\n        with `name`, other identifiers can be added as well, eg.: `id`.\n\n        Params:\n            ojb: the object that you are typechecking\n\n        Returns:\n            (str, str): a tuple containing the type name and and unique str to identify that object.\n        """"""\n        type_name = type(obj).__name__\n\n        if hasattr(obj, ""name""):\n            object_name = obj.name\n        else:\n            object_name = repr(obj)\n\n        return (type_name, object_name)\n\n    @staticmethod\n    def enumerate_nested_types(input_arg: any) -> Union[list, tuple, dict, type]:\n        """"""\n            Method to enumerate the input of a function/Plan, including nested types.\n\n            Note: supported nested structures: list, tuple, dict with string keys.\n\n            Params:\n                input_arg: *args of a function or Plan.\n\n            Returns:\n                Union[list, tuple, dict, type]: Nested structured with types instead of objects.\n        """"""\n        if input_arg is None:\n            return None\n\n        if isinstance(input_arg, (list, tuple)):\n            result = [NestedTypeWrapper.enumerate_nested_types(elem) for elem in input_arg]\n            return tuple(result) if isinstance(input_arg, tuple) else result\n\n        if isinstance(input_arg, dict):\n            serialized_dict = {\n                k: NestedTypeWrapper.enumerate_nested_types(v) for k, v in input_arg.items()\n            }\n            return serialized_dict\n\n        return type(input_arg)\n\n    @staticmethod\n    def raise_typecheck_err(typechecked_object: any, build: str, call: str, path: str) -> None:\n        """"""\n            Function to raise a type error if two types differ.\n\n            Params:\n                obj_type: the type of the object returned by calling .__name__ on it.\n                obj_name: the name/id of the object.\n                build: the build/reference argument type.\n                call: the called argument type.\n                path: the nested path to reach that obj.\n\n            Returns:\n                 None\n        """"""\n        type_name, obj_name = NestedTypeWrapper.get_object_identifiers(typechecked_object)\n        raise TypeError(\n            f""{type_name} {obj_name} {path} has type {build}, while being built with type {call}."",\n        )\n\n    @staticmethod\n    def raise_missmatch_err(typechecked_object: any, build: int, call: int, path: str) -> None:\n        """"""\n            Function to raise an error if two nested structures differ in length.\n\n            Params:\n                obj_type: the type of the object returned by calling .__name__ on it.\n                obj_name: the name/id of the object.\n                build: the build/reference argument length.\n                call: the called argument length.\n                path: the nested path to reach that obj.\n\n            Returns:\n                 None\n        """"""\n        type_name, obj_name = NestedTypeWrapper.get_object_identifiers(typechecked_object)\n        raise TypeError(\n            f""{type_name} {obj_name} {path} has length {call}, ""\n            f""while being build with length {build}.""\n        )\n\n    @staticmethod\n    def raise_wrong_number_arguments_err(typechecked_object: any, build: int, call: int) -> None:\n        """"""\n            Function to raise an error if the build/reference function has a different number\n            of arguments.\n\n            Params:\n                obj_type: the type of the object returned by calling .__name__ on it.\n                obj_name: the name/id of the object.\n                build: the build/reference input length.\n                call: the called input length.\n\n            Returns:\n                 None\n        """"""\n        type_name, obj_name = NestedTypeWrapper.get_object_identifiers(typechecked_object)\n        raise TypeError(f""{type_name} {obj_name} requires {build} arguments, received {call}."")\n\n    @staticmethod\n    def raise_key_missing_err(typechecked_object: any, key: any, path: str) -> None:\n        """"""\n            Function to raise an error if the build/reference function has a different number\n            of arguments.\n\n            Params:\n                obj_type: the type of the object returned by calling .__name__ on type(obj).\n                obj_name: the name/id of the object.\n                key: the key that is missing from the called dict.\n                path: the nested path to reach that obj.\n\n            Returns:\n                 None\n        """"""\n        type_name, obj_name = NestedTypeWrapper.get_object_identifiers(typechecked_object)\n        raise KeyError(\n            f""{type_name} {obj_name} {path} does not provide the key {key}, ""\n            ""while being build with that key.""\n        )\n\n    def input_check(self, typechecked_object: any, args: list) -> None:\n        """"""\n            Method for input validation by comparing the serialized build input with the\n            current call input, following the following steps:\n                1. Input length validation - checking that build and call inputs match on length.\n                2. Verify the following nested structures: list, tuple, dict recursively. Lengths\n                must match when comparing two nested lists, tuples or dicts. If they differ, an\n                error will be raised.\n                3. If we hit an object for which we don\'t support nesting, we compare types between\n                call input and build input. If they differ, a warning will be raised.\n                4. Dicts on the same nesting level on build and call input must have the same keys.\n                If they differ, an error will be raised.\n\n            Params:\n                obj_type: the type of the object returned by calling .__name__ on type(obj).\n                obj_name: the name/id of the object\n                args: the arguments to be compared with the reference/build one.\n\n            Returns:\n                None\n        """"""\n\n        def check_type_nested_structure(\n            typechecked_object,\n            build_arg_nested_type: Union[list, tuple, dict, type],\n            call_arg_nested_obj: any,\n            path: str,\n        ) -> None:\n            """"""\n                Recursive method to compare the nested input argument and the nested build argument.\n\n                Params:\n                    build_arg_nested_type: Can be either a nested element (list, tuple, dict)\n                                        or a type.\n                    call_arg_nested_obj: Can be either a nested element (list, tuple, dict)\n                                        or an object.\n\n                Returns:\n                    None\n            """"""\n\n            iterable_supported_list = (list, tuple, dict)\n\n            if type(call_arg_nested_obj) not in iterable_supported_list:\n                if not isinstance(call_arg_nested_obj, build_arg_nested_type):\n                    NestedTypeWrapper.raise_typecheck_err(\n                        typechecked_object,\n                        build_arg_nested_type.__name__,\n                        type(call_arg_nested_obj).__name__,\n                        path,\n                    )\n                return\n\n            if type(build_arg_nested_type) != type(call_arg_nested_obj):\n                NestedTypeWrapper.raise_typecheck_err(\n                    typechecked_object,\n                    type(build_arg_nested_type).__name__,\n                    type(call_arg_nested_obj).__name__,\n                    path,\n                )\n                return\n\n            if isinstance(build_arg_nested_type, (list, tuple)):\n                if len(build_arg_nested_type) != len(call_arg_nested_obj):\n                    NestedTypeWrapper.raise_missmatch_err(\n                        typechecked_object,\n                        len(build_arg_nested_type),\n                        len(call_arg_nested_obj),\n                        path,\n                    )\n\n                for idx in range(len(build_arg_nested_type)):\n                    check_type_nested_structure(\n                        typechecked_object,\n                        build_arg_nested_type[idx],\n                        call_arg_nested_obj[idx],\n                        f""element {idx} of "" + path,\n                    )\n\n            if isinstance(build_arg_nested_type, dict):\n                if len(build_arg_nested_type) != len(call_arg_nested_obj):\n                    NestedTypeWrapper.raise_missmatch_err(\n                        typechecked_object,\n                        len(build_arg_nested_type),\n                        len(call_arg_nested_obj),\n                        path,\n                    )\n\n                for key in build_arg_nested_type.keys():\n                    if key in call_arg_nested_obj:\n                        check_type_nested_structure(\n                            typechecked_object,\n                            build_arg_nested_type[key],\n                            call_arg_nested_obj[key],\n                            f""key {key} of "" + path,\n                        )\n                    else:\n                        NestedTypeWrapper.raise_key_missing_err(typechecked_object, key, path)\n\n        if len(args) != len(self.nested_input_types):\n            NestedTypeWrapper.raise_wrong_number_arguments_err(\n                typechecked_object, len(self.nested_input_types), len(args)\n            )\n\n        for idx in range(len(args)):\n            check_type_nested_structure(\n                typechecked_object,\n                self.nested_input_types[idx],\n                args[idx],\n                f""element {idx} of input"",\n            )\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, nested_type_wrapper: ""NestedTypeWrapper"") -> list:\n        return sy.serde.msgpack.serde._simplify(worker, nested_type_wrapper.nested_input_types)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, simplified_nested_type: list) -> ""NestedTypeWrapper"":\n        nested_type_wrapper = sy.serde.msgpack.serde._detail(worker, simplified_nested_type)\n        result = NestedTypeWrapper()\n        result.nested_input_types = nested_type_wrapper\n        return result\n\n    @staticmethod\n    def bufferize(\n        worker: AbstractWorker, nested_type_wrapper: ""NestedTypeWrapper""\n    ) -> NestedTypeWrapperPB:\n        def bufferize_nested_structure(worker: AbstractWorker, obj: any) -> NestedTypeWrapperPB:\n            nested_type_pb = NestedTypeWrapperPB()\n\n            if isinstance(obj, list):\n                container = NestedTypeWrapperPB.TypeContainer()\n                proto_list = NestedTypeWrapperPB.TypeList()\n\n                for elem in obj:\n                    proto_list.nested_types.append(bufferize_nested_structure(worker, elem))\n\n                container.nested_type_list.CopyFrom(proto_list)\n                nested_type_pb.nested_types.CopyFrom(container)\n\n            if isinstance(obj, tuple):\n                container = NestedTypeWrapperPB.TypeContainer()\n                proto_tuple = NestedTypeWrapperPB.TypeTuple()\n\n                for elem in obj:\n                    proto_tuple.nested_types.append(bufferize_nested_structure(worker, elem))\n\n                container.nested_type_tuple.CopyFrom(proto_tuple)\n                nested_type_pb.nested_types.CopyFrom(container)\n\n            if isinstance(obj, dict):\n                container = NestedTypeWrapperPB.TypeContainer()\n                proto_map = NestedTypeWrapperPB.TypeMap()\n\n                for k, v in obj.items():\n                    proto_map.nested_types[k].CopyFrom(bufferize_nested_structure(worker, v))\n\n                container.nested_type_dict.CopyFrom(proto_map)\n                nested_type_pb.nested_types.CopyFrom(container)\n\n            if isinstance(obj, type):\n                container = NestedTypeWrapperPB.TypeContainer()\n                typePB = InputTypeDescriptorPB()\n                module_path = obj.__module__\n                full_path_type = module_path + ""."" + obj.__name__\n                typePB.type_name = full_path_type\n                container.nested_type.CopyFrom(typePB)\n                nested_type_pb.nested_types.CopyFrom(container)\n            return nested_type_pb\n\n        result = bufferize_nested_structure(worker, nested_type_wrapper.nested_input_types)\n        return result\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, message):\n        def unbufferize_nested_structure(worker, message):\n            container = None\n            if message.nested_types.HasField(""nested_type""):\n                return sy.serde.protobuf.serde._unbufferize(\n                    worker, message.nested_types.nested_type\n                )\n\n            if message.nested_types.HasField(""nested_type_list""):\n                container = []\n                for obj in message.nested_types.nested_type_list.nested_types:\n                    container.append(unbufferize_nested_structure(worker, obj))\n\n            if message.nested_types.HasField(""nested_type_tuple""):\n                container = []\n                for obj in message.nested_types.nested_type_tuple.nested_types:\n                    container.append(unbufferize_nested_structure(worker, obj))\n                container = tuple(container)\n\n            if message.nested_types.HasField(""nested_type_dict""):\n                container = {}\n                for k, v in message.nested_types.nested_type_dict.nested_types.items():\n                    container[k] = unbufferize_nested_structure(worker, v)\n\n            return container\n\n        result = unbufferize_nested_structure(worker, message)\n        wrapper = NestedTypeWrapper()\n        wrapper.nested_input_types = result\n        return wrapper\n\n    @staticmethod\n    def get_protobuf_schema():\n        return NestedTypeWrapperPB\n'"
syft/federated/__init__.py,0,b''
syft/federated/fl_client.py,0,"b'from urllib.parse import urlparse\r\nfrom syft.grid.grid_client import GridClient\r\nfrom syft.federated.fl_job import FLJob\r\n\r\n\r\nclass FLClient:\r\n    def __init__(self, url, auth_token, verbose=False):\r\n        self.url = url\r\n        self.auth_token = auth_token\r\n        self.worker_id = None\r\n\r\n        url_fragments = urlparse(url)\r\n        self.grid_client = GridClient(id="""", address=url_fragments.netloc, secure=not verbose,)\r\n\r\n    def new_job(self, model_name, model_version) -> FLJob:\r\n        if self.worker_id is None:\r\n            auth_response = self.grid_client.authenticate(self.auth_token)\r\n            self.worker_id = auth_response[""data""][""worker_id""]\r\n\r\n        job = FLJob(\r\n            fl_client=self,\r\n            grid_client=self.grid_client,\r\n            model_name=model_name,\r\n            model_version=model_version,\r\n        )\r\n        return job\r\n'"
syft/federated/fl_job.py,0,"b'from syft.grid.grid_client import GridClient\r\nfrom syft.grid.grid_client import GridError\r\nfrom syft.execution.state import State\r\nfrom syft.execution.placeholder import PlaceHolder\r\n\r\n\r\nclass EventEmitter:\r\n    def __init__(self):\r\n        self.listeners = {}\r\n        pass\r\n\r\n    def add_listener(self, event_name, fn):\r\n        if event_name not in self.listeners:\r\n            self.listeners[event_name] = []\r\n        self.listeners[event_name].append(fn)\r\n\r\n    def trigger(self, event_name, *args, **kwargs):\r\n        if event_name in self.listeners:\r\n            for fn in self.listeners[event_name]:\r\n                fn(*args, **kwargs)\r\n\r\n\r\nclass FLJob(EventEmitter):\r\n    EVENT_ACCEPTED = ""accepted""\r\n    EVENT_REJECTED = ""rejected""\r\n    EVENT_ERROR = ""error""\r\n\r\n    def __init__(self, fl_client, grid_client: GridClient, model_name: str, model_version: str):\r\n        super().__init__()\r\n        self.fl_client = fl_client\r\n        self.grid_client = grid_client\r\n        self.model_name = model_name\r\n        self.model_version = model_version\r\n\r\n        self.model = None\r\n        self.plans = {}\r\n        self.protocols = {}\r\n        self.cycle_params = {}\r\n        self.client_config = {}\r\n\r\n    def _init_cycle(self, cycle_params: dict):\r\n        self.cycle_params = cycle_params\r\n        self.client_config = cycle_params[""client_config""]\r\n\r\n        worker_id = self.fl_client.worker_id\r\n        request_key = cycle_params[""request_key""]\r\n\r\n        # Load model\r\n        self.model = self.grid_client.get_model(worker_id, request_key, cycle_params[""model_id""])\r\n\r\n        # Load plans\r\n        for plan_name, plan_id in cycle_params[""plans""].items():\r\n            self.plans[plan_name] = self.grid_client.get_plan(\r\n                worker_id, request_key, plan_id, GridClient.PLAN_TYPE_TORCHSCRIPT\r\n            )\r\n\r\n        # Load protocols\r\n        for protocol_name, protocol_id in cycle_params[""protocols""].items():\r\n            self.protocols[protocol_name] = self.grid_client.get_protocol(\r\n                worker_id, request_key, protocol_id\r\n            )\r\n\r\n    def start(self):\r\n        try:\r\n            speed_info = self.grid_client.get_connection_speed(self.fl_client.worker_id)\r\n            cycle_request_response = self.grid_client.cycle_request(\r\n                worker_id=self.fl_client.worker_id,\r\n                model_name=self.model_name,\r\n                model_version=self.model_version,\r\n                speed_info=speed_info,\r\n            )\r\n            cycle_params = cycle_request_response[""data""]\r\n\r\n            if cycle_params[""status""] == GridClient.CYCLE_STATUS_ACCEPTED:\r\n                self._init_cycle(cycle_params)\r\n                self.trigger(self.EVENT_ACCEPTED, self)\r\n            elif cycle_params[""status""] == GridClient.CYCLE_STATUS_REJECTED:\r\n                self.trigger(self.EVENT_REJECTED, self, cycle_params.get(""timeout"", None))\r\n        except GridError as e:\r\n            self.trigger(self.EVENT_ERROR, self, f""Grid communication error: {e.error}"")\r\n\r\n    def report(self, updated_model_params: list):\r\n        # Calc params diff\r\n        orig_params = self.model.tensors()\r\n        diff_params = [orig_params[i] - updated_model_params[i] for i in range(len(orig_params))]\r\n\r\n        # Wrap diff in State\r\n        diff_ph = [PlaceHolder().instantiate(t) for t in diff_params]\r\n        diff = State(state_placeholders=diff_ph)\r\n\r\n        response = self.grid_client.report(\r\n            worker_id=self.fl_client.worker_id,\r\n            request_key=self.cycle_params[""request_key""],\r\n            diff=diff,\r\n        )\r\n        return response\r\n'"
syft/federated/floptimizer.py,0,"b'""""""to maintain a list of optimizer objects,\none for each worker and use them in the appropriate context""""""\nimport copy\n\n\nclass Optims:\n    """"""to create a list of optimizer objects""""""\n\n    def __init__(self, workers, optim):\n        """"""\n        args:\n            workers: list of worker ids\n            optim: class of pytorch optimizer\n        """"""\n        self.optim = optim\n        self.workers = workers\n        self.optimizers = {}\n        for worker in workers:\n            self.optimizers[str(worker)] = copy.copy(self.optim)\n            self.optimizers[str(worker)].load_state_dict((self.optim).state_dict())\n\n    def get_optim(self, worker):\n        """"""returns optimizer for worker\n        args:\n            worker: worker id\n        """"""\n        return self.optimizers[str(worker)]\n\n    def count(self):\n        """"""returns the number of optimizer objects""""""\n        return len(self.workers)\n'"
syft/frameworks/__init__.py,0,"b'import logging\nfrom syft import dependency_check\n\nlogger = logging.getLogger(__name__)\n\n__all__ = []\n\nif dependency_check.tensorflow_available:\n    from syft.frameworks import tensorflow\n\n    __all__.append(tensorflow.__name__)\n\nif dependency_check.tfe_available:\n    from syft.frameworks import keras  # noqa: F401\n\n    __all__.append(""keras"")\n\nif dependency_check.torch_available:\n    from syft.frameworks import torch  # noqa: F401\n\n    __all__.append(""torch"")\n'"
syft/generic/__init__.py,0,b''
syft/generic/id_provider.py,0,"b'import random\nfrom typing import List\nfrom syft import exceptions\n\n\ndef create_random_id():\n    return int(10e10 * random.random())\n\n\nclass IdProvider:\n    """"""Provides Id to all syft objects.\n\n    Generate id and store the list of ids generated\n    Can take a pre set list in input and will complete\n    when it\'s empty.\n\n    An instance of IdProvider is accessible via sy.ID_PROVIDER.\n    """"""\n\n    def __init__(self, given_ids=None):\n        self.given_ids = given_ids if given_ids is not None else []\n        self.generated = set()\n        self.record_ids = False\n        self.recorded_ids = []\n\n    def pop(self, *args) -> int:\n        """"""Provides random ids and store them.\n\n        The syntax .pop() mimics the list syntax for convenience\n        and not the generator syntax.\n\n        Returns:\n            Random Id.\n        """"""\n        if len(self.given_ids):\n            random_id = self.given_ids.pop(-1)\n        else:\n            random_id = create_random_id()\n            while random_id in self.generated:\n                random_id = create_random_id()\n        self.generated.add(random_id)\n        if self.record_ids:\n            self.recorded_ids.append(random_id)\n\n        return random_id\n\n    def set_next_ids(self, given_ids: List, check_ids: bool = True):\n        """"""Sets the next ids returned by the id provider\n\n        Note that the ids are returned in reverse order of the list, as a pop()\n        operation is applied.\n\n        Args:\n            given_ids: List, next ids returned by the id provider\n            check_ids: bool, check whether these ids conflict with already generated ids\n\n        """"""\n        if check_ids:\n            intersect = self.generated.intersection(set(given_ids))\n            if len(intersect) > 0:\n                message = f""Provided IDs {intersect} are contained in already generated IDs""\n                raise exceptions.IdNotUniqueError(message)\n\n        self.given_ids += given_ids\n\n    def start_recording_ids(self):\n        """"""Starts the recording in form of a list of the generated ids.\n        """"""\n        self.record_ids = True\n        self.recorded_ids = []\n\n    def get_recorded_ids(self, continue_recording=False):\n        """"""Returns the generated ids since the last call to start_recording_ids.\n\n        Args:\n            continue_recording: if False, the recording is stopped and the\n                list of recorded ids is reset\n\n        Returns:\n            list of recorded ids\n        """"""\n        ret_val = self.recorded_ids\n        if not continue_recording:\n            self.record_ids = False\n            self.recorded_ids = []\n        return ret_val\n\n    @staticmethod\n    def seed(seed=0):\n        random.seed(seed)\n'"
syft/generic/metrics.py,0,"b'# Some functions to aid monitoring network traffic\n\nimport pyshark\n\n\nclass NetworkMonitor:\n    """"""\n    Provides utility for the monitoring of network traffic, measuring the packets sent through\n    specific filters as passed by user.\n    """"""\n\n    @staticmethod\n    def get_packets(\n        timeout=50,\n        interface=None,\n        bpf_filter=None,\n        display_filter=""tcp.port == 80"",\n        tshark_path=None,\n        output_file=None,\n    ):\n        """"""\n        Returns the captured packets of the transmitted data using Wireshark.\n\n        Args:\n        timeout: An integer. Set for sniffing with tshark. Default to 50 seconds in this setup.\n        interface: A string. Name of the interface to sniff on.\n        bpf_filter: A string. The capture filter in bpf syntax \'tcp port 80\'. Needs to be changed\n                    to match filter for the traffic sent. Not to be confused with the display\n                    filters (e.g. tcp.port == 80). The former are much more limited and is used to\n                    restrict the size of a raw packet capture, whereas the latter is used to hide\n                    some packets from the packet list. More info can be found at\n                    https://wiki.wireshark.org/CaptureFilters.\n        display_filter: A string. Default to \'tcp.port == 80\' (assuming this is the port of the\n                        \'WebsocketClientWorker\'). Please see notes for \'bpf_filter\' for details\n                        regarding differences. More info can be found at\n                        https://wiki.wireshark.org/DisplayFilters.\n        tshark_path: Path to the tshark binary. E.g. \'/usr/local/bin/tshark\'.\n        output_file: A string. Path including the output file name is to saved.\n                     E.g. \'/tmp/mycapture.cap\'\n\n        Returns:\n        catpure: A \'pyshark.capture.live_capture.LiveCapture\' object. Of packets sent\n                 over WebSockets.\n        length: An integer. The number of packets captured at the network interface.\n        """"""\n        capture_output = []\n        if interface is None:\n            raise Exception(""Please provide the interface used."")\n        else:\n            capture = pyshark.LiveCapture(\n                interface=interface,\n                bpf_filter=bpf_filter,\n                tshark_path=tshark_path,\n                output_file=output_file,\n            )\n            capture.sniff(timeout=timeout)\n            length = len(capture)\n            return capture, length\n\n    @staticmethod\n    def read_packet(index=None, capture_input=None):\n        """"""\n        Reads the info of one packet returned by get_packets using pretty_print().\n\n        Args:\n        index: An integer. The index of the packet to be examined.\n\n        Returns:\n        pretty_print: The info of the packet chosen to be read.\n        """"""\n        if index is None:\n            raise Exception(\n                ""Please choose an index within the total number of packets captured by get_packets.""\n            )\n        elif capture_input is None:\n            raise Exception(""Please input the capture_output from get_packets."")\n        elif not isinstance(index, int):\n            raise Exception(""The index passed is not an integer."")\n        else:\n            length = len(capture_input)\n            if index < length:\n                try:\n                    packet = capture_input[index]\n                    return packet.pretty_print()\n                except:\n                    raise Exception(""Something went wrong when retrieving packet data."")\n            else:\n                raise Exception(""The index given is not valid."")\n'"
syft/generic/object_storage.py,0,"b'from collections import defaultdict\nfrom typing import Union\n\nfrom syft.exceptions import ObjectNotFoundError\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.generic.frameworks.types import FrameworkTensorType\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.workers.abstract import AbstractWorker\n\n\nclass ObjectStore:\n    """"""A storage of objects identifiable by their id.\n\n    A wrapper object to a collection of objects where all objects\n    are stored using their IDs as keys.\n    """"""\n\n    def __init__(self, owner: AbstractWorker = None):\n        self.owner = owner\n\n        # This is the collection of objects being stored.\n        self._objects = {}\n        # This is an index to retrieve objects from their tags in an efficient way\n        self._tag_to_object_ids = defaultdict(set)\n\n    @property\n    def _tensors(self):\n        return {id_: obj for id_, obj in self._objects.items() if isinstance(obj, FrameworkTensor)}\n\n    def register_obj(self, obj: object, obj_id: Union[str, int] = None):\n        """"""Registers the specified object with the current worker node.\n\n        Selects an id for the object, assigns a list of owners, and establishes\n        whether it\'s a pointer or not. This method is generally not used by the\n        client and is instead used by internal processes (hooks and workers).\n\n        Args:\n            obj: A torch Tensor or Variable object to be registered.\n            obj_id (int or string): random integer between 0 and 1e10 or\n            string uniquely identifying the object.\n        """"""\n        if obj_id is not None and hasattr(obj, ""id""):\n            obj.id = obj_id\n        self.set_obj(obj)\n\n    def de_register_obj(self, obj: object, _recurse_torch_objs: bool = True):\n        """"""Deregisters the specified object.\n\n        Deregister and remove attributes which are indicative of registration.\n\n        Args:\n            obj: A torch Tensor or Variable object to be deregistered.\n            _recurse_torch_objs: A boolean indicating whether the object is\n                more complex and needs to be explored. Is not supported at the\n                moment.\n        """"""\n        if hasattr(obj, ""id""):\n            self.rm_obj(obj.id)\n        if hasattr(obj, ""_owner""):\n            del obj._owner\n\n    def get_obj(self, obj_id: Union[str, int]) -> object:\n        """"""Returns the object from registry.\n\n        Look up an object from the registry using its ID.\n\n        Args:\n            obj_id: A string or integer id of an object to look up.\n\n        Returns:\n            Object with id equals to `obj_id`.\n        """"""\n\n        try:\n            obj = self._objects[obj_id]\n        except KeyError as e:\n            if obj_id not in self._objects:\n                raise ObjectNotFoundError(obj_id, self)\n            else:\n                raise e\n\n        return obj\n\n    def set_obj(self, obj: Union[FrameworkTensorType, AbstractTensor]) -> None:\n        """"""Adds an object to the registry of objects.\n\n        Args:\n            obj: A torch or syft tensor with an id.\n        """"""\n        obj.owner = self.owner\n        self._objects[obj.id] = obj\n        # Add entry in the tag index\n        if obj.tags:\n            for tag in obj.tags:\n                if tag not in self._tag_to_object_ids:\n                    self._tag_to_object_ids[tag] = {obj.id}\n                else:\n                    self._tag_to_object_ids[tag].add(obj.id)\n\n    def rm_obj(self, obj_id: Union[str, int], force=False):\n        """"""Removes an object.\n\n        Remove the object from the permanent object registry if it exists.\n\n        Args:\n            obj_id: A string or integer representing id of the object to be\n                removed.\n            force: if true, explicitly forces removal of the object modifying the\n                `garbage_collect_data` attribute.\n        """"""\n        if obj_id in self._objects:\n            obj = self._objects[obj_id]\n            # update tag index\n            if obj.tags:\n                for tag in obj.tags:\n                    if tag not in self._tag_to_object_ids:\n                        self._tag_to_object_ids[tag].remove(obj.id)\n\n            if force and hasattr(obj, ""child"") and hasattr(obj.child, ""garbage_collect_data""):\n                obj.child.garbage_collect_data = True\n\n            del self._objects[obj_id]\n\n    def force_rm_obj(self, obj_id: Union[str, int]):\n        self.rm_obj(obj_id, force=True)\n\n    def clear_objects(self):\n        """"""Removes all objects from the object storage.""""""\n        self._objects.clear()\n\n    def current_objects(self):\n        """"""Returns a copy of the objects in the object storage.""""""\n        return self._objects.copy()\n\n    def find_by_id(self, id):\n        """"""Local search by id""""""\n        return self._objects.get(id)\n\n    def find_by_tag(self, tag):\n        """"""Local search by tag\n\n        Args:\n            tag (str): exact tag searched\n\n        Return:\n            A list of results, possibly empty\n        """"""\n        if tag in self._tag_to_object_ids:\n            results = []\n            for obj_id in self._tag_to_object_ids[tag]:\n                obj = self.find_by_id(obj_id)\n                if obj is not None:\n                    results.append(obj)\n            return results\n        return []\n\n    def register_tags(self, obj):\n        # NOTE: this is a fix to correct faulty registration that can sometimes happen\n        if obj.id not in self._objects:\n            self.owner.register_obj(obj)\n\n        for tag in obj.tags:\n            self._tag_to_object_ids[tag].add(obj.id)\n\n    def __len__(self):\n        """"""\n        Return the number of objects in the store\n        """"""\n        return len(self._objects)\n'"
syft/generic/string.py,0,"b'from typing import List\nfrom typing import Tuple\nfrom typing import Union\nimport syft as sy\nfrom syft.generic.pointers.string_pointer import StringPointer\nfrom syft.workers.base import BaseWorker\nfrom syft.generic.abstract.sendable import AbstractSendable\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft_proto.generic.string_pb2 import String as StringPB\n\n\nclass String(AbstractSendable):\n    """"""\n       This is a class that wraps the Python built-in `str` class. In addition to\n    providing access to most of `str`\'s method call API, it allows sending\n    such wrapped string between workers the same way Syft tensors can be\n    moved around among workers.\n    """"""\n\n    # Set of methods from \'str\' to hook/override by String\n    methods_to_hook = {\n        ""__add__"",\n        ""__eq__"",\n        ""__le__"",\n        ""__ge__"",\n        ""__gt__"",\n        ""__lt__"",\n        ""__ne__"",\n        ""__len__"",\n        ""__getitem__"",\n        ""__str__"",\n        ""__repr__"",\n        ""__format__"",\n        ""lower"",\n        ""upper"",\n        ""capitalize"",\n        ""casefold"",\n        ""center"",\n        ""count"",\n        ""encode"",\n        ""endswith"",\n        ""expandtabs"",\n        ""find"",\n        ""format"",\n        ""format_map"",\n        ""index"",\n        ""isalnum"",\n        ""isalpha"",\n        ""isascii"",\n        ""isdecimal"",\n        ""isdigit"",\n        ""isidentifier"",\n        ""islower"",\n        ""isnumeric"",\n        ""isprintable"",\n        ""isspace"",\n        ""istitle"",\n        ""isupper"",\n        ""join"",\n        ""ljust"",\n        ""lstrip"",\n        ""maketrans"",\n        ""partition"",\n        ""replace"",\n        ""rfind"",\n        ""rindex"",\n        ""rjust"",\n        ""rpartition"",\n        ""rsplit"",\n        ""rstrip"",\n        ""split"",\n        ""splitlines"",\n        ""startswith"",\n        ""strip"",\n        ""swapcase"",\n        ""title"",\n        ""translate"",\n        ""zfill"",\n        ""__mod__"",\n    }\n\n    def __init__(\n        self,\n        object: object = None,\n        encoding: str = None,\n        errors: str = None,\n        id: Union[int, str] = None,\n        owner: BaseWorker = None,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n        """"""Initialize a String object.\n\n        Args:\n           object: This could be any object whose string representation,i.e.,\n               the output of its __str__() method is to be wrapped as a\n               String object.\n           encoding: This should be specified if the above `object` argument is\n               a bytes-like object. It specifies the encoding scheme used to create the\n               bytes-like object b\'\'. for example, encoding could be \'utf-8\'.\n               For more details on this argument, please  check the official `str`\n               documentation.\n           errors: This should be specified if the above `object` argument is\n               a bytes-like object. Possible values are \'strict\', \'ignore\' or\n               \'replace\'. For more details on this argument, please\n               check the official `str` documentation.\n           id: An optional string or integer id of the String object\n           owner: An optional BaseWorker object to specify the worker on which\n               the String object is located.\n           tags: an optional set of hashtags corresponding to this object.\n               They are useful when search for this object.\n           description: an optional string describing the purpose of this\n               String object\n\n        """"""\n\n        # get the specified kwargs_ for creating the base \'str\'\n        # class\n\n        self.encoding = encoding\n        self.errors = errors\n\n        # String objects have normally a default owner which is the\n        # local worker. So prevent \'None\' as owner\n        if self.owner is None or owner is not None:\n            self.owner = owner\n\n        str_kwargs = {}\n\n        if object:\n            str_kwargs[""object""] = object\n\n        if encoding:\n            str_kwargs[""encoding""] = encoding\n\n        if errors:\n            str_kwargs[""errors""] = errors\n\n        # Create a str instance as the \'child\' attribute\n        self.child = str(**str_kwargs)\n\n        super(String, self).__init__(\n            id=id, owner=self.owner, tags=tags, description=description, child=self.child\n        )\n\n    def send(self, location: BaseWorker):\n        """"""\n           Sends this String object to the worker specified by \'location\'.\n        and returns a pointer to that string as a StringPointer object.\n\n        Args:\n           location: The BaseWorker object which you want to send this object\n                     to. Note that this is never actually the BaseWorker but instead\n                     a class which inherits the BaseWorker abstraction.\n\n        Returns:\n           A StringPointer objects to self.\n        """"""\n\n        ptr = self.owner.send(self, location)\n\n        return ptr\n\n    def get_class_attributes(self):\n        """"""\n        Returns: minimal necessary keyword arguments to create a\n           String object\n        """"""\n        kwargs_ = {""owner"": self.owner}\n\n        return kwargs_\n\n    def on(self, object: str, wrap=False):\n        """"""Takes and object of type strings and assigns it to\n        self.child\n        """"""\n        self.child = object\n\n        return self\n\n    def __add__(self, other: Union[str, ""String""]):\n        """""" [Important] overriding the `__add__` here is not yet\n        activated. The real hooking happens in\n        syft/generic/frameworks/hook/hook.py.\n        Hooking as implemented here (using @overloaded.method)\n        is to be activated when hook_args.py is adapted\n        to wrapping reponses of `str` types into `String`\n        types. This is not yet supported.\n        """"""\n\n        # The following is necessary in order to adapt the\n        # below `add_string` method to the args hooking logic in\n        # hook_args.py. Please check the doc string of `add_string`\n        # to know more.\n        if isinstance(other, str):\n            other = String(other)\n\n        return self.add_string(other)\n\n    @overloaded.method\n    def add_string(self, _self: ""String"", other: ""String""):\n        """"""This method is created in a way adapted to the logic implemented\n        in hook_args.py. That is, it can be wrapped with the decorator\n        @overloaded.method.\n\n        hook_args.py args hooking logic needs that the data types of\n        argument be unchanged. For instance, \'other\' should always\n        be of a fixed type \'String\' or \'str\' but not alternating\n        between both. This can cause unexpected behaviou due to caching\n        in hook_args.py.\n\n        Args:\n           _self: a String object (as received by the decorator).\n                  It represents the objects on which we called the add method.\n                  It will always be of type `str` inside this method. Since\n                  the decorator methods strips the `str` out of the `String`\n                  object.\n           other: a String object that we wish to concatenate to `_self`.\n                  Same as above, it is a String object as received by the\n                  decorator but here it will always be of type `str`.\n\n        Returns:\n           The concatentenated `str` object between `_self` and `other`.\n           this `str` object will be wrapped by the decorator into a\n           String object\n        """"""\n\n        return _self + other\n\n    @staticmethod\n    def create_pointer(\n        obj,\n        location: BaseWorker = None,\n        id_at_location: (str or int) = None,\n        register: bool = False,\n        owner: BaseWorker = None,\n        ptr_id: (str or int) = None,\n        garbage_collect_data: bool = True,\n    ):\n        """"""\n        Creates a StringPointer object that points to a String object \'obj\'\n        after sending the latter to the worker \'location\'.\n\n        Returns:\n            a StringPointer object\n        """"""\n\n        if id_at_location is None:\n            id_at_location = obj.id\n\n        if owner is None:\n            owner = obj.owner\n\n        string_pointer = StringPointer(\n            location=location,\n            id_at_location=id_at_location,\n            owner=owner,\n            id=ptr_id,\n            garbage_collect_data=garbage_collect_data,\n        )\n\n        return string_pointer\n\n    @staticmethod\n    def simplify(worker: BaseWorker, string: ""String""):\n        """"""\n        Breaks String object into a tuple of simpler objects, its constituting objects that are\n        serializable.\n\n        Args:\n           worker: a BaseWorker object\n           string: the String object to be simplified\n\n         Returns:\n           A tuple of simpler objects that are sufficient to recreate\n           a String object that is a clone of `string`.\n        """"""\n\n        # Encode the string into a bytes object\n        simple_child = sy.serde.msgpack.serde._simplify(worker, string.child)\n        tags = sy.serde.msgpack.serde._simplify(worker, string.tags)\n        description = sy.serde.msgpack.serde._simplify(worker, string.description)\n\n        return (simple_child, string.id, tags, description)\n\n    @staticmethod\n    def detail(worker: BaseWorker, simple_obj: Tuple):\n        """"""\n        Create an object of type String from the reduced representation in `simple_obj`.\n\n\n        Args:\n           worker: BaseWorker\n                   The worker on which the new String object is to be created.\n           simple_obj: tuple\n                       A tuple resulting from the serialized then deserialized returned tuple\n                       from the `simplify` static method above.\n\n        Returns:\n           A String object\n        """"""\n\n        # Get the contents of the tuple represening the simplified object\n        simple_child, id, tags, description = simple_obj\n\n        # It appears that all strings are converted to bytes objects\n        # after deserialization, convert them back to strings\n        tags = sy.serde.msgpack.serde._detail(worker, tags)\n        description = sy.serde.msgpack.serde._detail(worker, description)\n\n        # Rebuild the str child our of the simplified child (the bytes child)\n        child = sy.serde.msgpack.serde._detail(worker, simple_child)\n\n        return String(object=child, id=id, owner=worker, tags=tags, description=description)\n\n    @staticmethod\n    def bufferize(worker, str_object):\n        """"""\n        This method serializes a String into a StringPB.\n\n            Args:\n                str_object (String): input String to be serialized.\n\n            Returns:\n                proto_string (StringPB): serialized String.\n        """"""\n        proto_string = StringPB()\n        proto_string.child = str_object.child\n        for tag in str_object.tags:\n            proto_string.tags.append(tag)\n        if str_object.description:\n            proto_string.description = str_object.description\n\n        return proto_string\n\n    @staticmethod\n    def unbufferize(worker, obj):\n        """"""\n        This method deserializes StringPB into a String.\n\n        Args:\n            obj (StringPB): input serialized StringPB.\n\n        Returns:\n            String: deserialized ScriptFunctionPB.\n        """"""\n        return String(object=obj.child, tags=obj.tags, description=obj.description)\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n        This method returns the protobuf schema used for String.\n\n        Returns:\n           Protobuf schema for String.\n       """"""\n        return StringPB\n\n\n### Register the String object with hook_args.py ###\nhook_args.default_register_tensor(String)\n'"
syft/generic/utils.py,0,"b'class memorize(dict):\n    """"""\n    This is a decorator to cache a function output when the function is\n    deterministic and the input space is small. In such condition, the\n    function will be called many times to perform the same computation\n    so we want this computation to be cached.\n    """"""\n\n    def __init__(self, func):\n        self.func = func\n\n    def __call__(self, *args):\n        return self[args]\n\n    def __missing__(self, key):\n        result = self[key] = self.func(*key)\n        return result\n'"
syft/grid/__init__.py,0,"b'from .network import Network\nimport sys\nimport uuid\n\nDEFAULT_NETWORK_URL = ""ws://ec2-13-59-45-128.us-east-2.compute.amazonaws.com""\n\n_registered_peer = None\n\n\ndef register(**kwargs):\n    """""" Add this process as a new peer registering it in the grid network.\n        Returns:\n            peer: Peer Network instance.\n    """"""\n    global _registered_peer\n\n    if isinstance(_registered_peer, Network):\n        sys.stdout.write(\n            ""\\033[93m"" + ""WARNING"" + ""\\033[0m""\n            "":"" + f"" You are already a registered peer!\\n{_registered_peer}\\n""\n        )\n\n        return _registered_peer\n\n    try:\n        if not kwargs:\n            args = {""max_size"": None, ""timeout"": 444, ""url"": DEFAULT_NETWORK_URL}\n        else:\n            args = kwargs\n\n        peer_id = str(uuid.uuid4())\n        sys.stdout.write(\n            ""Connecting to OpenGrid ("" + ""\\033[94m"" + args[""url""] + ""\\033[0m"" + "") ... ""\n        )\n\n        _registered_peer = Network(peer_id, **args)\n\n        sys.stdout.write(""\\033[92m"" + ""OK"" + ""\\033[0m"" + ""\\n"")\n        sys.stdout.write(""Peer ID: "" + peer_id + ""\\n"")\n\n        sys.stdout.write(\n            ""\\033[93m"" + ""DISCLAIMER"" + ""\\033[0m""\n            "":""\n            + ""\\033[1m""\n            + "" OpenGrid is an experimental feature currently in alpha.""\n            + "" Do not use this to protect real-world data.\\n""\n            + ""\\033[0m""\n        )\n\n        sys.stdout.write(""Where to get help: \\n"")\n        sys.stdout.write(\n            "" - Join our slack  (https://slack.openmined.org) and ask for""\n            + ""help in the #lib_syft channel.\\n""\n        )\n        sys.stdout.write(\n            "" - File a Github Issue: https://github.com/OpenMined/PySyft and add the""\n            + "" string \'#opengrid\' in the issue title.\\n""\n        )\n        sys.stdout.write(\n            "" - Want to join in our development team? Apply here:""\n            + "" https://forms.gle/wcH1vxzvPyDSbSVW6\\n""\n        )\n\n        _registered_peer.start()\n\n        return _registered_peer\n\n    except Exception as e:\n        sys.stdout.write(""\\033[91m"" + ""FAIL"" + ""\\033[0m"" + ""\\n"")\n        sys.stdout.write(""You were not able to register your node.\\n"")\n'"
syft/grid/abstract_grid.py,1,"b'import torch\n\nfrom typing import Union\nfrom typing import List\nfrom typing import Any\nfrom typing import Tuple\nfrom typing import Dict\n\nfrom abc import ABC, abstractmethod\n\nfrom syft.workers.node_client import NodeClient  # noqa: F401\n\n\nclass AbstractGrid(ABC):\n    def __init__(self):\n        self.SMPC_HOST_CHUNK = 4  # (1 host, 2 shares, 1 crypto_provider)\n\n    @abstractmethod\n    def search(self, *query: Union[str]) -> Dict[Any, Any]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def serve_model(\n        self,\n        model,\n        id: str,\n        mpc: bool = False,\n        allow_remote_inference: bool = False,\n        allow_download: bool = False,\n        n_replica: int = 1,\n    ) -> None:\n        raise NotImplementedError\n\n    @abstractmethod\n    def query_model_hosts(\n        self, id: str, mpc: bool = False\n    ) -> Union[""NodeClient"", Tuple[""NodeClient""]]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def run_remote_inference(self, id: str, data: torch.Tensor, mpc: bool = False) -> torch.Tensor:\n        raise NotImplementedError\n\n    def _connect_all_nodes(self, nodes: Tuple[Any], node_type: Any) -> None:\n        """"""Connect all nodes to each other.\n        Args:\n            nodes: A tuple of grid clients.\n        """"""\n        if self._check_node_type(nodes, node_type):  # Avoid connect local workers (Virtual Workers)\n            for i in range(len(nodes)):\n                for j in range(i):\n                    node_i, node_j = nodes[i], nodes[j]\n                    node_i.connect_nodes(node_j)\n                    node_j.connect_nodes(node_i)\n\n    def _check_node_type(self, grid_workers: List[Any], node_type: Any) -> bool:\n        """""" Private method used to verify if workers used by grid network are exactly what we expect.\n\n        Returns:\n            result : Boolean result.\n        """"""\n        return all(isinstance(node, node_type) for node in grid_workers)\n'"
syft/grid/grid_client.py,0,"b'import json\n\nimport binascii\nimport base64\nimport websocket\nimport requests\nfrom timeit import timeit\nimport random\n\nimport syft as sy\nfrom syft.serde import protobuf\n\nfrom syft.execution.state import State\nfrom syft_proto.execution.v1.plan_pb2 import Plan as PlanPB\nfrom syft_proto.execution.v1.state_pb2 import State as StatePB\nfrom syft_proto.execution.v1.protocol_pb2 import Protocol as ProtocolPB\n\nTIMEOUT_INTERVAL = 60\nCHUNK_SIZE = 8192\nSPEED_MULT_FACTOR = 10\nMAX_BUFFER_SIZE = 1048576  # 1 MB\nCHECK_SPEED_EVERY = 10\nMAX_SPEED_TESTS = 50\n\n\nclass GridError(BaseException):\n    def __init__(self, error, status):\n        self.status = status\n        self.error = error\n\n\nclass GridClient:\n    CYCLE_STATUS_ACCEPTED = ""accepted""\n    CYCLE_STATUS_REJECTED = ""rejected""\n    PLAN_TYPE_LIST = ""list""\n    PLAN_TYPE_TORCHSCRIPT = ""torchscript""\n\n    def __init__(self, id: str, address: str, secure: bool = False):\n        self.id = id\n        self.address = address\n        self.secure = secure\n        self.ws = None\n        self.serialize_worker = sy.VirtualWorker(hook=None)\n\n    @property\n    def ws_url(self):\n        return f""wss://{self.address}"" if self.secure else f""ws://{self.address}""\n\n    @property\n    def http_url(self):\n        return f""https://{self.address}"" if self.secure else f""http://{self.address}""\n\n    def connect(self):\n        args_ = {""max_size"": None, ""timeout"": TIMEOUT_INTERVAL, ""url"": self.ws_url}\n\n        self.ws = websocket.create_connection(**args_)\n\n    def _send_msg(self, message: dict) -> dict:\n        """""" Prepare/send a JSON message to a PyGrid server and receive the response.\n\n        Args:\n            message (dict) : message payload.\n        Returns:\n            response (dict) : response payload.\n        """"""\n        if self.ws is None or not self.ws.connected:\n            self.connect()\n\n        self.ws.send(json.dumps(message))\n        json_response = json.loads(self.ws.recv())\n\n        # print(""REQ"", message)\n        # print(""RES"", json_response)\n\n        error = json_response[""data""].get(""error"", None)\n        if error is not None:\n            raise GridError(error, None)\n\n        return json_response\n\n    def _send_http_req(self, method, path: str, params: dict = None, body: bytes = None):\n        if method == ""GET"":\n            res = requests.get(self.http_url + path, params)\n        elif method == ""POST"":\n            res = requests.post(self.http_url + path, params=params, data=body)\n\n        if not res.ok:\n            raise GridError(""HTTP response is not OK"", res.status_code)\n\n        response = res.content\n        return response\n\n    def _yield_chunk_from_request(self, request, chunk_size):\n        for chunk in request.iter_content(chunk_size=chunk_size):\n            yield chunk\n\n    def _read_n_request_chunks(self, chunk_generator, n):\n        for i in range(n):\n            try:\n                next(chunk_generator)\n            except:\n                return False\n        return True\n\n    def _get_ping(self, worker_id, random_id):\n        params = {""is_ping"": 1, ""worker_id"": worker_id, ""random"": random_id}\n        ping = (\n            timeit(\n                lambda: self._send_http_req(""GET"", ""/federated/speed-test"", params),\n                number=MAX_SPEED_TESTS,\n            )\n            * 1000\n        )  # for ms\n        return ping\n\n    def _get_upload_speed(self, worker_id, random_id):\n        buffer_size = CHUNK_SIZE\n        speed_history = []\n\n        for _ in range(MAX_SPEED_TESTS):\n            data_sample = b""x"" * buffer_size\n            params = {""worker_id"": worker_id, ""random"": random_id}\n            body = {""upload_data"": data_sample}\n            time_taken = timeit(\n                lambda: self._send_http_req(""POST"", ""/federated/speed-test"", params, body),\n                number=1,\n            )\n            if time_taken < 0.5:\n                buffer_size = min(\n                    buffer_size * SPEED_MULT_FACTOR, MAX_BUFFER_SIZE * 64\n                )  # 64 MB max file size\n                continue\n            upload_speed = 64 * 1024 / time_taken  # speed in KBps\n            speed_history.append(upload_speed)\n            if len(speed_history) % CHECK_SPEED_EVERY == 0:\n                avg = sum(speed_history) / len(speed_history)\n                deviation = avg - min(speed_history)\n                if (deviation < 20) and (avg > 0):\n                    break\n        if len(speed_history) == 0:\n            return -1\n        else:\n            avg_speed = sum(speed_history) / len(speed_history)\n            return avg_speed\n\n    def _get_download_speed(self, worker_id, random_id):\n        params = {""worker_id"": worker_id, ""random"": random_id}\n        speed_history = []\n        with requests.get(self.http_url + ""/federated/speed-test"", params, stream=True) as r:\n            r.raise_for_status()\n            buffer_size = CHUNK_SIZE\n            chunk_generator = self._yield_chunk_from_request(r, CHUNK_SIZE)\n            for _ in range(MAX_SPEED_TESTS):\n                time_taken = timeit(\n                    lambda: self._read_n_request_chunks(chunk_generator, buffer_size // CHUNK_SIZE),\n                    number=1,\n                )\n                if time_taken < 0.5:\n                    buffer_size = min(buffer_size * SPEED_MULT_FACTOR, MAX_BUFFER_SIZE)\n                    continue\n                new_speed = buffer_size / (time_taken * 1024)\n                speed_history.append(new_speed)\n                if len(speed_history) % CHECK_SPEED_EVERY == 0:\n                    avg = sum(speed_history) / len(speed_history)\n                    deviation = avg - min(speed_history)\n                    if (deviation < 20) and (avg > 0):\n                        break\n\n        if len(speed_history) == 0:\n            return -1\n        else:\n            avg_speed = sum(speed_history) / len(speed_history)\n            return avg_speed\n\n    def _serialize(self, obj):\n        """"""Serializes object to protobuf""""""\n        pb = protobuf.serde._bufferize(self.serialize_worker, obj)\n        return pb.SerializeToString()\n\n    def _serialize_object(self, obj):\n        serialized_object = {}\n        for k, v in obj.items():\n            serialized_object[k] = binascii.hexlify(self._serialize(v)).decode()\n        return serialized_object\n\n    def _unserialize(self, serialized_obj, obj_protobuf_type):\n        pb = obj_protobuf_type()\n        pb.ParseFromString(serialized_obj)\n        serialization_worker = sy.VirtualWorker(hook=None, auto_add=False)\n        return protobuf.serde._unbufferize(serialization_worker, pb)\n\n    def close(self):\n        self.ws.shutdown()\n\n    def host_federated_training(\n        self,\n        model,\n        client_plans,\n        client_protocols,\n        client_config,\n        server_averaging_plan,\n        server_config,\n    ):\n        serialized_model = binascii.hexlify(self._serialize(model)).decode()\n        serialized_plans = self._serialize_object(client_plans)\n        serialized_protocols = self._serialize_object(client_protocols)\n        serialized_avg_plan = binascii.hexlify(self._serialize(server_averaging_plan)).decode()\n\n        # ""federated/host-training"" request body\n        message = {\n            ""type"": ""federated/host-training"",\n            ""data"": {\n                ""model"": serialized_model,\n                ""plans"": serialized_plans,\n                ""protocols"": serialized_protocols,\n                ""averaging_plan"": serialized_avg_plan,\n                ""client_config"": client_config,\n                ""server_config"": server_config,\n            },\n        }\n\n        return self._send_msg(message)\n\n    def authenticate(self, auth_token):\n        message = {\n            ""type"": ""federated/authenticate"",\n            ""data"": {""auth_token"": auth_token},\n        }\n\n        return self._send_msg(message)\n\n    def cycle_request(self, worker_id, model_name, model_version, speed_info):\n        message = {\n            ""type"": ""federated/cycle-request"",\n            ""data"": {\n                ""worker_id"": worker_id,\n                ""model"": model_name,\n                ""version"": model_version,\n                **speed_info,\n            },\n        }\n        return self._send_msg(message)\n\n    def get_model(self, worker_id, request_key, model_id):\n        params = {\n            ""worker_id"": worker_id,\n            ""request_key"": request_key,\n            ""model_id"": model_id,\n        }\n        serialized_model = self._send_http_req(""GET"", ""/federated/get-model"", params)\n        return self._unserialize(serialized_model, StatePB)\n\n    def get_plan(self, worker_id, request_key, plan_id, receive_operations_as):\n        params = {\n            ""worker_id"": worker_id,\n            ""request_key"": request_key,\n            ""plan_id"": plan_id,\n            ""receive_operations_as"": receive_operations_as,\n        }\n        serialized_plan = self._send_http_req(""GET"", ""/federated/get-plan"", params)\n        return self._unserialize(serialized_plan, PlanPB)\n\n    def get_protocol(self, worker_id, request_key, protocol_id):\n        params = {\n            ""worker_id"": worker_id,\n            ""request_key"": request_key,\n            ""plan_id"": protocol_id,\n        }\n        serialized_protocol = self._send_http_req(""GET"", ""/federated/get-protocol"", params)\n        return self._unserialize(serialized_protocol, ProtocolPB)\n\n    def report(self, worker_id: str, request_key: str, diff: State):\n        diff_serialized = self._serialize(diff)\n        diff_base64 = base64.b64encode(diff_serialized).decode(""ascii"")\n        params = {\n            ""type"": ""federated/report"",\n            ""data"": {""worker_id"": worker_id, ""request_key"": request_key, ""diff"": diff_base64},\n        }\n        return self._send_msg(params)\n\n    def get_connection_speed(self, worker_id):\n        random_num = random.getrandbits(128)\n        ping = self._get_ping(worker_id, random_num)\n        upload_speed = self._get_upload_speed(worker_id, random_num)\n        download_speed = self._get_download_speed(worker_id, random_num)\n        return {""ping"": ping, ""download"": download_speed, ""upload"": upload_speed}\n'"
syft/grid/network.py,1,"b'import threading\nimport websocket\nimport json\nfrom syft.codes import NODE_EVENTS, GRID_EVENTS, MSG_FIELD\nfrom syft.frameworks.torch.tensors.interpreters.private import PrivateTensor\nfrom syft.grid.nodes_manager import WebRTCManager\nfrom syft.grid.peer_events import (\n    _monitor,\n    _create_webrtc_scope,\n    _accept_offer,\n    _process_webrtc_answer,\n)\n\nimport syft as sy\nimport time\n\n\nclass Network(threading.Thread):\n    """""" Grid Network class to operate in background processing grid requests\n        and handling multiple peer connections with different nodes.\n    """"""\n\n    # Events called by the grid monitor to health checking and signaling webrtc connections.\n    EVENTS = {\n        NODE_EVENTS.MONITOR: _monitor,\n        NODE_EVENTS.WEBRTC_SCOPE: _create_webrtc_scope,\n        NODE_EVENTS.WEBRTC_OFFER: _accept_offer,\n        NODE_EVENTS.WEBRTC_ANSWER: _process_webrtc_answer,\n    }\n\n    def __init__(self, node_id: str, **kwargs):\n        """""" Create a new thread to send/receive messages from the grid service.\n\n            Args:\n                node_id: ID used to identify this peer.\n        """"""\n        threading.Thread.__init__(self)\n        self._connect(**kwargs)\n        self._worker = self._update_node_infos(node_id)\n        self._worker.models = {}\n        self._connection_handler = WebRTCManager(self._ws, self._worker)\n        self.available = False\n\n    def run(self):\n        """""" Run the thread sending a request to join into the grid network and listening\n        the grid network requests.\n        """"""\n\n        # Join\n        self._join()\n        try:\n            # Listen\n            self._listen()\n        except OSError:  # Avoid IO socket errors\n            pass\n\n    def stop(self):\n        """""" Finish the thread and disconnect with the grid network. """"""\n        self.available = False\n        self._ws.shutdown()\n\n    def _update_node_infos(self, node_id: str):\n        """""" Create a new virtual worker to store/compute datasets owned by this peer.\n\n            Args:\n                node_id: ID used to identify this peer.\n        """"""\n        worker = sy.VirtualWorker(sy.hook, id=node_id)\n        sy.local_worker._known_workers[node_id] = worker\n        sy.local_worker.is_client_worker = False\n        return worker\n\n    def _listen(self):\n        """""" Listen the sockets waiting for grid network health checks and webrtc\n        connection requests.\n        """"""\n        while self.available:\n            message = self._ws.recv()\n            msg = json.loads(message)\n            response = self._handle_messages(msg)\n            if response:\n                self._ws.send(json.dumps(response))\n\n    def _handle_messages(self, message):\n        """""" Route and process the messages received from the websocket connection.\n\n            Args:\n                message : message to be processed.\n        """"""\n        msg_type = message.get(MSG_FIELD.TYPE, None)\n        if msg_type in Network.EVENTS:\n            return Network.EVENTS[msg_type](message, self._connection_handler)\n\n    def _connect(self, **kwargs):\n        """""" Create a websocket connection between this peer and the grid network. """"""\n        self._ws = websocket.create_connection(**kwargs)\n\n    @property\n    def id(self):\n        return self._worker.id\n\n    def connect(self, destination_id: str):\n        """""" Create a webrtc connection between this peer and the destination peer by using the grid network\n            to forward the webrtc connection request protocol.\n\n            Args:\n                destination_id : Id used to identify the peer to be connected.\n        """"""\n\n        # Temporary Notebook async weird constraints\n        # Should be removed after solving #3572\n        if len(self._connection_handler) >= 1:\n            print(\n                ""Due to some jupyter notebook async constraints, we do not recommend handling ""\n                ""multiple connection peers at the same time.""\n            )\n            print(""This issue is in WIP status and may be solved soon."")\n            print(\n                ""You can follow its progress here: https://github.com/OpenMined/PySyft/issues/3572""\n            )\n            return None\n\n        webrtc_request = {MSG_FIELD.TYPE: NODE_EVENTS.WEBRTC_SCOPE, MSG_FIELD.FROM: self.id}\n\n        forward_payload = {\n            MSG_FIELD.TYPE: GRID_EVENTS.FORWARD,\n            MSG_FIELD.DESTINATION: destination_id,\n            MSG_FIELD.CONTENT: webrtc_request,\n        }\n\n        self._ws.send(json.dumps(forward_payload))\n        while not self._connection_handler.get(destination_id):\n            time.sleep(1)\n\n        return self._connection_handler.get(destination_id)\n\n    def disconnect(self, destination_id: str):\n        """""" Disconnect with some peer connected previously.\n\n            Args:\n                destination_id: Id used to identify the peer to be disconnected.\n        """"""\n        _connection = self._connection_handler.get(destination_id)\n        if _connection:\n            _connection.available = False\n\n    def host_dataset(self, dataset):\n        """""" Host dataset using the virtual worker defined previously.\n\n            Args:\n                dataset: Dataset to be hosted.\n        """"""\n        allowed_users = None\n\n        # By default the peer should be allowed to access its own private tensors.\n        if dataset.is_wrapper and type(dataset.child) == PrivateTensor:\n            dataset.child.register_credentials([self._worker.id])\n\n        return dataset.send(self._worker, user=self._worker.id)\n\n    def host_model(self, model):\n        """""" Host model using the virtual worker defined previously. """"""\n        model.nodes.append(self._worker.id)\n        self._worker.models[model.id] = model\n        return model._model\n\n    def _join(self):\n        """""" Send a join requet to register this peer on the grid network. """"""\n        # Join into the network\n        join_payload = {MSG_FIELD.TYPE: GRID_EVENTS.JOIN, MSG_FIELD.NODE_ID: self._worker.id}\n        self._ws.send(json.dumps(join_payload))\n        response = json.loads(self._ws.recv())\n        self.available = True\n        return response\n\n    def __repr__(self):\n        """"""Default String representation""""""\n        repr_str = (\n            f""< Peer ID: {self.id}, ""\n            f""hosted datasets: {list(self._worker.object_store._tag_to_object_ids.keys())}, ""\n            f""hosted_models: {list(self._worker.models.keys())}, ""\n            f""connected_nodes: {list(self._connection_handler.nodes)}""\n        )\n\n    @property\n    def peers(self):\n        """"""\n        Get WebRTCManager object\n        """"""\n        return self._connection_handler\n'"
syft/grid/nodes_manager.py,0,"b'from syft.grid.webrtc_connection import WebRTCConnection\nfrom syft.workers.base import BaseWorker\nimport asyncio\n\n\nclass WebRTCManager(BaseWorker):\n    """""" Class used to manage multiple webrtc peer connections in different threads. """"""\n\n    def __init__(self, grid_descriptor, syft_worker):\n        self._connections = {}\n        self._grid = grid_descriptor\n        self.worker = syft_worker\n\n    @property\n    def nodes(self):\n        """""" Return all the peer nodes connected directly with this peer.""""""\n        return list(self._connections.keys())\n\n    def _send_msg(self, message: bin, location):\n        """""" Forward a local syft request to the proper destination. """"""\n        return asyncio.run(self._connection[location.id].send(message))\n\n    def _recv_msg(self, message):\n        """""" Overwrite BaseWorker\'s abstract method. But it\'s not used. """"""\n        raise NotImplementedError\n\n    def get(self, node_id: str):\n        """""" Return a peer connection reference by its ID. """"""\n        return self._connections.get(node_id, None)\n\n    def process_answer(self, destination: str, content: str):\n        """""" Set the webrtc connection answer message. """"""\n        self._connections[destination].set_msg(content)\n\n    def process_offer(self, destination: str, content: str):\n        """""" Create a thread to process a webrtc offer connection. """"""\n        self._connections[destination] = WebRTCConnection(\n            self._grid, self.worker, destination, self._connections, WebRTCConnection.ANSWER\n        )\n        self._connections[destination].set_msg(content)\n        self._connections[destination].start()\n\n    def start_offer(self, destination: str):\n        """""" Create a new thread to offer a webrtc connection. """"""\n\n        # Temporary Notebook async weird constraints\n        # Should be removed after solving #3572\n        if len(self._connections) >= 1:\n            print(\n                ""Due to some jupyter notebook async constraints, we do not recommend handling ""\n                ""multiple connection peers at the same time.""\n            )\n            print(""This issue is in WIP status and may be solved soon."")\n            print(\n                ""You can follow its progress here: https://github.com/OpenMined/PySyft/issues/3572""\n            )\n            return\n\n        self._connections[destination] = WebRTCConnection(\n            self._grid, self.worker, destination, self._connections, WebRTCConnection.OFFER\n        )\n        self._connections[destination].start()\n\n    def __getitem__(self, key):\n        """"""\n        Args:\n            key: Node ID\n\n        Returns:\n            Return a peer connection reference by its ID.\n        """"""\n\n        return self.get(key)\n\n    def __len__(self):\n        return len(self._connections)\n'"
syft/grid/peer_events.py,0,"b'from syft.codes import MSG_FIELD, GRID_EVENTS\nimport psutil\n\n\ndef _monitor(message: dict, conn_handler):\n    """""" Update peer status sending to the grid network some infos about this peer. """"""\n    response = {MSG_FIELD.TYPE: GRID_EVENTS.MONITOR_ANSWER}\n\n    response[MSG_FIELD.NODES] = conn_handler.nodes\n\n    response[MSG_FIELD.CPU] = psutil.cpu_percent()\n    response[MSG_FIELD.MEM_USAGE] = psutil.virtual_memory().percent\n    models = {model_id: model.json() for model_id, model in conn_handler.worker.models.items()}\n    response[MSG_FIELD.MODELS] = models\n\n    def std_tags(tag):\n        STD_TAGS = [\n            ""#fss_eq_plan_1"",\n            ""#fss_eq_plan_2"",\n            ""#fss_comp_plan_1"",\n            ""#fss_comp_plan_2"",\n            ""#xor_add_1"",\n            ""#xor_add_2"",\n        ]\n        if tag in STD_TAGS:\n            return False\n        return True\n\n    response[MSG_FIELD.DATASETS] = list(\n        filter(std_tags, conn_handler.worker.object_store._tag_to_object_ids.keys())\n    )\n    return response\n\n\ndef _create_webrtc_scope(message: dict, conn_handler):\n    """""" Send a p2p webrtc connection request to be forwarded by the grid network. """"""\n    dest = message[MSG_FIELD.FROM]\n    conn_handler.start_offer(dest)\n\n\ndef _accept_offer(message: dict, conn_handler):\n    """""" Receive a webrtc connection request sended by a peer and forwarded by the grid network. """"""\n    dest = message.get(MSG_FIELD.FROM, None)\n    content = message.get(MSG_FIELD.PAYLOAD, None)\n    conn_handler.process_offer(dest, content)\n\n\ndef _process_webrtc_answer(message: dict, conn_handler):\n    """""" Process the peer answer. """"""\n    dest = message.get(MSG_FIELD.FROM, None)\n    content = message.get(MSG_FIELD.PAYLOAD, None)\n    conn_handler.process_answer(dest, content)\n'"
syft/grid/private_grid.py,5,"b'import random\nimport torch\n\nfrom typing import Any\nfrom typing import Tuple\nfrom typing import Dict\nfrom typing import Union\n\n# Syft imports\n\nimport syft\nfrom syft.grid.abstract_grid import AbstractGrid\nfrom syft.workers.node_client import NodeClient\nfrom syft.execution.plan import Plan\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\n\n\nclass PrivateGridNetwork(AbstractGrid):\n    def __init__(self, *workers):\n        super().__init__()\n        self.workers = list(workers)\n        self._connect_all_nodes(self.workers, NodeClient)\n\n    def search(self, *query) -> Dict[Any, Any]:\n        """""" Searches over a collection of workers, returning pointers to the results\n        grouped by worker.\n\n        Args:\n            query : List of tags used to identify the desired tensor.\n        Returns:\n            results : list of pointers with pointers that matches with tags.\n        """"""\n\n        results = {}\n\n        for worker in self.workers:\n            worker_results = syft.local_worker.request_search(query, location=worker)\n\n            if len(worker_results) > 0:\n                results[worker.id] = worker_results\n\n        return results\n\n    def serve_model(\n        self,\n        model,\n        id: str,\n        mpc: bool = False,\n        allow_remote_inference: bool = False,\n        allow_download: bool = False,\n        n_replica: int = 1,\n    ):\n        """""" Choose some node(s) on grid network to host a unencrypted / encrypted model.\n\n        Args:\n            model: Model to be hosted.\n            id: Model\'s ID.\n            mpc: Boolean flag to host a plain text / encrypted model\n            allow_remote_inference: Allow to run inference remotely.\n            allow_download: Allow to copy the model and run it locally.\n            n_replica: Number of copies distributed through grid network.\n        Raises:\n            RuntimeError: If grid network doesn\'t have enough nodes to replicate the model.\n            NotImplementedError: If workers used by grid network aren\'t grid nodes.\n        """"""\n        # If workers used by grid network aren\'t grid nodes.\n        if not self._check_node_type(self.workers, NodeClient):\n            raise NotImplementedError\n\n        if n_replica > len(self.workers):\n            raise RuntimeError(""Not enough nodes!"")\n        else:\n            nodes = random.sample(self.workers, n_replica)\n\n        for i in range(len(nodes)):\n            if not mpc:\n                # Host plain-text model\n                nodes[i].serve_model(\n                    model,\n                    model_id=id,\n                    allow_download=allow_download,\n                    allow_remote_inference=allow_remote_inference,\n                )\n            else:\n                # Host encrypted model\n                self._host_encrypted_model(model)\n\n    def run_remote_inference(self, id: str, data: torch.Tensor, mpc: bool = False) -> torch.Tensor:\n        """""" Search for a specific model registered on grid network, if found,\n        It will run inference.\n\n        Args:\n            id : Model\'s ID.\n            dataset : Data used to run inference.\n            mpc: Boolean flag to run a plain text / encrypted model\n        Returns:\n            Tensor : Inference\'s result.\n        Raises:\n            NotImplementedError: If workers used by grid network aren\'t grid nodes.\n            RuntimeError: If model id not found.\n        """"""\n        # If workers used by grid network aren\'t grid nodes.\n        if not self._check_node_type(self.workers, NodeClient):\n            raise NotImplementedError\n\n        if not mpc:\n            result = self._run_unencrypted_inference(id, data)\n        else:\n            result = self._run_encrypted_inference(id, data)\n\n        return result\n\n    def query_model_hosts(\n        self, id: str, mpc: bool = False\n    ) -> Union[""NodeClient"", Tuple[""NodeClient""]]:\n        """""" Search for node host from a specific model registered on grid network, if found,\n        It will return the frist host/ set of hosts that contains the desired model.\n\n        Args:\n            id : Model\'s ID.\n            data : Data used to run inference.\n            mpc : Boolean flag to search for a plain text / encrypted model\n        Returns:\n            workers : First worker that contains the desired model.\n        Raises:\n            NotImplementedError: If workers used by grid network aren\'t grid nodes.\n            RuntimeError: If model id not found.\n        """"""\n\n        # If workers used by grid network aren\'t grid nodes.\n        if not self._check_node_type(self.workers, NodeClient):\n            raise NotImplementedError\n\n        # Search for non mpc models.\n        if not mpc:\n            for node in self.workers:\n                if id in node.models:\n                    return node\n        else:\n            # Search for MPC models\n            return self._query_encrypted_model_hosts(id)\n\n    def _host_encrypted_model(self, model, n_shares: int = 4):\n        """""" This method wiil choose some grid nodes at grid network to host an encrypted model.\n\n        Args:\n            model: Model to be hosted.\n            n_shares: number of workers used by MPC protocol.\n        Raise:\n            RuntimeError : If grid network doesn\'t have enough workers\n            to host an encrypted model or if model is not a plan.\n        """"""\n        # Verify if this network have enough workers.\n        if n_shares > len(self.workers):\n            raise RuntimeError(""Not enough nodes!"")\n        elif n_shares < self.SMPC_HOST_CHUNK:\n            raise RuntimeError(""Not enough shares to perform MPC operations!"")\n        else:\n            # Select N workers in your set of workers.\n            nodes = random.sample(self.workers, n_shares)\n\n        # Model needs to be a plan\n        if isinstance(model, Plan):\n            host = nodes[0]  # Host\n            mpc_nodes = nodes[1:-1]  # Shares\n            crypto_provider = nodes[-1]  # Crypto Provider\n\n            # SMPC Share\n            model.fix_precision().share(*mpc_nodes, crypto_provider=crypto_provider)\n\n            # Host model\n            p_model = model.send(host)\n\n            # Save a pointer reference to this model in database.\n            host.serve_model(\n                p_model,\n                model_id=model.id,\n                allow_download=False,\n                allow_remote_inference=False,\n                mpc=True,\n            )\n        # If model isn\'t a plan\n        else:\n            raise RuntimeError(""Model needs to be a plan to be encrypted!"")\n\n    def _query_encrypted_model_hosts(self, id: str) -> Tuple[""NodeClient""]:\n        """""" Search for an encrypted model and return its mpc nodes.\n\n        Args:\n            id: Model\'s ID.\n        Returns:\n            Tuple : Tuple structure containing Host, MPC Nodes and crypto provider.\n        Raises:\n            RuntimeError: If model id not found.\n        """"""\n        host = self.query_model_hosts(id)\n\n        # If it\'s registered on grid nodes.\n        if host:\n            model = host.search(id)[0].get(deregister_ptr=False)\n            mpc_nodes = set()\n            crypto_provider = None\n\n            # Check every state used by this plan\n            for state_id in model.state.state_ids:\n                hook = host.hook\n                obj = hook.local_worker.object_store.get_obj(state_id)\n\n                # Decrease in Tensor Hierarchy.\n                # (we want be a AdditiveSharingTensor to recover workers/crypto_provider addresses)\n                while not isinstance(obj, AdditiveSharingTensor):\n                    obj = obj.child\n\n                # Get a list of mpc nodes.\n                nodes = map(lambda x: hook.local_worker._known_workers.get(x), obj.child.keys())\n\n                mpc_nodes.update(set(nodes))\n\n                if obj.crypto_provider:\n                    crypto_provider = obj.crypto_provider\n\n                return (host, mpc_nodes, crypto_provider)\n        else:\n            raise RuntimeError(""Model ID not found!"")\n\n    def _run_unencrypted_inference(self, id: str, data) -> torch.Tensor:\n        """""" Search for a plain-text model registered on grid network, if found,\n        It will run inference.\n\n        Args:\n            id : Model\'s ID.\n            dataset : Data used to run inference.\n        Returns:\n            Tensor : Inference\'s result.\n        Raises:\n            RuntimeError: If model id not found.\n        """"""\n        node = self.query_model_hosts(id)\n        if node:\n            response = node.run_remote_inference(model_id=id, data=data)\n            return torch.tensor(response)\n        else:\n            raise RuntimeError(""Model not found on Grid Network!"")\n\n    def _run_encrypted_inference(self, id: str, data) -> torch.Tensor:\n        """""" Search for an encrypted model and perform inference.\n\n        Args:\n            model_id: Model\'s ID.\n            data: Dataset to be shared/inferred.\n            copy: Boolean flag to perform encrypted inference without lose plan.\n        Returns:\n            Tensor: Inference\'s result.\n        Raises:\n            RuntimeError: If model id not found.\n        """"""\n        host, mpc_nodes, crypto_provider = self._query_encrypted_model_hosts(id)\n\n        # Share your dataset to same SMPC Workers\n        shared_data = data.fix_precision().share(*mpc_nodes, crypto_provider=crypto_provider)\n\n        # Perform Inference\n        fetched_plan = host.hook.local_worker.fetch_plan(id, host, copy=True)\n\n        return fetched_plan(shared_data).get().float_prec()\n'"
syft/grid/public_grid.py,3,"b'import torch\nimport requests\nimport json\n\nfrom typing import Union\nfrom typing import List\nfrom typing import Any\nfrom typing import Tuple\nfrom typing import Dict\n\n# Syft imports\nfrom syft.grid.authentication.credential import AbstractCredential\nfrom syft.grid.abstract_grid import AbstractGrid\nfrom syft.workers.node_client import NodeClient\nfrom syft.execution.plan import Plan\nfrom syft.codes import GATEWAY_ENDPOINTS\n\n\nclass PublicGridNetwork(AbstractGrid):\n    def __init__(self, hook, gateway_url: str, credential: AbstractCredential = None):\n        super().__init__()\n        self.hook = hook\n        self.gateway_url = gateway_url\n        self.credential = credential\n\n    def search(self, *query: Union[str]) -> Dict[Any, Any]:\n        """""" Search a set of tags across the grid network.\n\n        Args:\n            query : A set of dataset tags.\n        Returns:\n            tensor_results : matrix of tensor pointers.\n        """"""\n        # Ask gateway about desired tags\n        body = json.dumps({""query"": list(query)})\n        match_nodes = self._ask_gateway(requests.post, GATEWAY_ENDPOINTS.SEARCH_TAGS, body)\n\n        # Connect with grid nodes that contains the dataset and get their pointers\n        tensor_results = {}\n        for node_id, node_url in match_nodes:\n            worker = self.__connect_with_node(node_id, node_url)\n            tensor_results[node_id] = worker.search(query)\n        return tensor_results\n\n    def serve_model(\n        self,\n        model,\n        id: Union[str, int],\n        mpc: bool = False,\n        allow_remote_inference: bool = False,\n        allow_download: bool = False,\n        n_replica: int = 1,\n    ) -> None:\n        """""" Choose n (number of replicas defined at gateway) grid nodes registered\n        in the grid network to host a model.\n\n        Args:\n            model : Model to be hosted.\n            id : Model\'s ID.\n            mpc : Boolean flag to serve plan models in an encrypted/unencrypted format.\n            allow_remote_inference : Allow workers to run inference in this model.\n            allow_download : Allow workers to copy the model and run it locally.\n        """"""\n        if not mpc:\n            self._serve_unencrypted_model(model, id, allow_remote_inference, allow_download)\n        else:\n            self._serve_encrypted_model(model)\n\n    def query_model_hosts(\n        self, id: str, mpc: bool = False\n    ) -> Union[""NodeClient"", Tuple[""NodeClient""]]:\n        """""" This method will search for a specific model registered on grid network, if found,\n        It will return all grid nodes that contains the desired model.\n\n        Args:\n            id : Model\'s ID.\n            mpc : Boolean flag to search plan models in an encrypted/unencrypted format.\n        Returns:\n            workers : Worker / list of workers that contains the desired model.\n        Raises:\n            RuntimeError : If grid network doesn\'t have enough workers to host\n            an encrypted model, or if model isn\'t a plan.\n        """"""\n        if not mpc:\n            return self._query_unencrypted_models(id)\n        else:\n            return self._query_encrypted_models(id)\n\n    def run_remote_inference(self, id: str, data: torch.Tensor, mpc: bool = False) -> torch.Tensor:\n        """""" This method will search for a specific model registered on the grid network, if found,\n        It will run inference.\n\n        Args:\n            id : Model\'s ID.\n            dataset : Data used to run inference.\n            mpc : Boolean flag to run encrypted/unencrypted inferences.\n        Returns:\n            Tensor : Inference\'s result.\n        Raises:\n            RuntimeError: If model id not registered on the grid network.\n        """"""\n        if not mpc:\n            return self._run_unencrypted_inference(id, data)\n        else:\n            return self._run_encrypted_inference(id, data)\n\n    def _serve_unencrypted_model(\n        self, model, id, allow_remote_inference: bool, allow_download: bool\n    ) -> None:\n        """""" This method will choose one of grid nodes registered in the grid network\n        to host a plain text model.\n\n        Args:\n            model: Model to be hosted.\n            id: Model\'s ID.\n            allow_remote_inference: Allow workers to run inference in this model.\n            allow_download: Allow workers to copy the model and run it locally.\n        """"""\n        hosts = self._ask_gateway(requests.get, GATEWAY_ENDPOINTS.SELECT_MODEL_HOST)\n\n        for host_id, host_address in hosts:\n            # Host model\n            host_worker = self.__connect_with_node(host_id, host_address)\n            host_worker.serve_model(\n                model,\n                model_id=id,\n                allow_download=allow_download,\n                allow_remote_inference=allow_remote_inference,\n                mpc=False,\n            )\n        host_worker.close()\n\n    def _serve_encrypted_model(self, model) -> None:\n        """""" This method wiil choose some grid nodes at grid network to host an encrypted model.\n\n        Args:\n            model: Model to be hosted.\n        Raises:\n            RuntimeError : If grid network doesn\'t have enough workers to host\n            an encrypted model, or if model isn\'t a plan.\n        """"""\n        # Model needs to be a plan\n        if isinstance(model, Plan):\n            hosts = self._ask_gateway(requests.get, GATEWAY_ENDPOINTS.SELECT_ENCRYPTED_MODEL_HOSTS)\n            if (\n                len(hosts) and len(hosts) % self.SMPC_HOST_CHUNK == 0\n            ):  # Minimum workers chunk to share and host a model (3 to SMPC operations, 1 to host)\n                for i in range(0, len(hosts), self.SMPC_HOST_CHUNK):\n\n                    # Connect with SMPC Workers\n                    smpc_end_interval = i + 2\n                    smpc_workers_info = hosts[i:smpc_end_interval]\n                    smpc_workers = []\n                    for worker in smpc_workers_info:\n                        smpc_workers.append(self.__connect_with_node(*worker))\n\n                    # Connect with crypto provider\n                    crypto_provider = self.__connect_with_node(*hosts[smpc_end_interval])\n\n                    # Connect with host worker\n                    host = self.__connect_with_node(*hosts[smpc_end_interval + 1])\n\n                    # Connect nodes to each other\n                    model_nodes = smpc_workers + [crypto_provider, host]\n                    self._connect_all_nodes(model_nodes, NodeClient)\n\n                    # SMPC Share\n                    model.fix_precision().share(*smpc_workers, crypto_provider=crypto_provider)\n\n                    # Host model\n                    p_model = model.send(host)\n\n                    # Save model pointer\n                    host.serve_model(p_model, model_id=model.id, mpc=True)\n\n                    for node in model_nodes:\n                        node.close()\n            # If host\'s length % SMPC_HOST_CHUNK != 0 or length == 0\n            else:\n                raise RuntimeError(""Not enough workers to host an encrypted model!"")\n        # If model isn\'t a plan\n        else:\n            raise RuntimeError(""Model needs to be a plan to be encrypted!"")\n\n    def _query_unencrypted_models(self, id) -> ""NodeClient"":\n        """""" Search for a specific model registered on grid network, if found,\n        It will return the first node that contains the desired model.\n\n        Args:\n            id : Model\'s ID.\n        Returns:\n            worker : worker that contains the desired model.\n        """"""\n        # Search for a model\n        body = json.dumps({""model_id"": id})\n        match_nodes = self._ask_gateway(requests.post, GATEWAY_ENDPOINTS.SEARCH_MODEL, body)\n\n        for node_id, node_url in match_nodes:\n            # Return the first node that stores the desired model\n            return self.__connect_with_node(node_id, node_url)\n\n    def _query_encrypted_models(self, id) -> List[""NodeClient""]:\n        """""" Search for a specific encrypted model registered on grid network, if found,\n        It will return the first node that hosts the desired model and mpc shares.\n\n        Args:\n            id : Model\'s ID.\n        Returns:\n            workers : List of workers that contains the desired mpc model.\n        """"""\n        # Search for an encrypted model\n        body = json.dumps({""model_id"": id})\n        match_nodes = self._ask_gateway(\n            requests.post, GATEWAY_ENDPOINTS.SEARCH_ENCRYPTED_MODEL, body\n        )\n\n        if len(match_nodes):\n            # Host of encrypted plan\n            node_id = list(match_nodes.keys())[0]  # Get the first one\n            node_address = match_nodes[node_id][""address""]\n\n            # Workers with SMPC parameters tensors\n            worker_infos = match_nodes[node_id][""nodes""][""workers""]\n            crypto_provider = match_nodes[node_id][""nodes""][""crypto_provider""]\n\n            # Connect with host node\n            host_node = self.__connect_with_node(node_id, node_address)\n\n            # Connect with SMPC Workers\n            workers = []\n            for worker_id, worker_address in worker_infos:\n                workers.append(self.__connect_with_node(worker_id, worker_address))\n\n            # Connect with SMPC crypto provider\n            crypto_provider_id = crypto_provider[0]\n            crypto_provider_url = crypto_provider[1]\n\n            crypto_node = self.__connect_with_node(crypto_provider_id, crypto_provider_url)\n\n            # Connect nodes\n            nodes = workers + [host_node, crypto_node]\n            self._connect_all_nodes(tuple(nodes), NodeClient)\n\n            return (host_node, workers, crypto_node)\n        else:\n            raise RuntimeError(""Model not found on Grid Network!"")\n\n    def _run_unencrypted_inference(self, id, data) -> torch.Tensor:\n        """""" Search for an unencrypted model and perform data inference.\n\n        Args:\n            id: Model\'s ID.\n            data: Dataset to be inferred.\n        Returns:\n            Tensor: Inference\'s result.\n        Raises:\n            RuntimeError: If model if not found.\n        """"""\n        worker = self.query_model_hosts(id)\n        if worker:\n            response = worker.run_remote_inference(model_id=id, data=data)\n            worker.close()\n            return torch.tensor(response)\n        else:\n            raise RuntimeError(""Model not found on Grid Network!"")\n\n    def _run_encrypted_inference(self, id, data, copy=True):\n        """""" Search for an encrypted model and perform inference.\n\n        Args:\n            model_id: Model\'s ID.\n            data: Dataset to be shared/inferred.\n            copy: Boolean flag to perform encrypted inference without lose plan.\n        Returns:\n            Tensor: Inference\'s result.\n        Raises:\n            RuntimeError: If model id not found.\n        """"""\n        # Get model\'s host / mpc shares\n        host_node, smpc_workers, crypto_provider = self._query_encrypted_models(id)\n\n        # Share your dataset to same SMPC Workers\n        shared_data = data.fix_precision().share(*smpc_workers, crypto_provider=crypto_provider)\n\n        # Perform Inference\n        fetched_plan = self.hook.local_worker.fetch_plan(id, host_node, copy=copy)\n        return fetched_plan(shared_data).get().float_prec()\n\n    def __connect_with_node(self, node_id, node_url):\n        if node_id not in self.hook.local_worker._known_workers:\n            worker = NodeClient(self.hook, node_url, credential=self.credential)\n        else:\n            # There is already a connection to this node\n            worker = self.hook.local_worker._known_workers[node_id]\n            worker.connect()\n        return worker\n\n    def _ask_gateway(self, request_method, endpoint: str, body: Dict = {}):\n        response = request_method(self.gateway_url + endpoint, data=body)\n        return json.loads(response.content)\n'"
syft/grid/webrtc_connection.py,0,"b'import asyncio\nimport json\n\nimport syft as sy\nfrom syft.codes import MSG_FIELD, GRID_EVENTS, NODE_EVENTS\nfrom aiortc import RTCPeerConnection, RTCSessionDescription\nfrom aiortc.contrib.signaling import (\n    CopyAndPasteSignaling,\n    object_to_string,\n    object_from_string,\n)\nimport threading\nimport queue\nimport time\nfrom syft.workers.base import BaseWorker\nfrom syft.messaging.message import SearchMessage\nfrom syft.exceptions import GetNotPermittedError\n\n\nclass WebRTCConnection(threading.Thread, BaseWorker):\n\n    OFFER = 1\n    ANSWER = 2\n    HOST_REQUEST = b""01""\n    REMOTE_REQUEST = b""02""\n\n    def __init__(self, grid_descriptor, worker, destination, connections, conn_type):\n        """""" Create a new webrtc peer connection.\n\n            Args:\n                grid_descriptor: Grid network\'s websocket descriptor to forward webrtc\n                                connection request.\n                worker: Virtual Worker that represents this peer.\n                destination: Destination Peer ID.\n                connections: Peer connection descriptors.\n                conn_type: Connection responsabilities this peer should provide. (offer, answer)\n        """"""\n        threading.Thread.__init__(self)\n        BaseWorker.__init__(self, hook=sy.hook, id=destination)\n        self._conn_type = conn_type\n        self._origin = worker.id\n        self._worker = worker\n        self._worker.tensor_requests = []\n        self._destination = destination\n        self._grid = grid_descriptor\n        self._msg = """"\n        self._request_pool = queue.Queue()\n        self._response_pool = queue.Queue()\n        self.channel = None\n        self.available = True\n        self.connections = connections\n\n    def _send_msg(self, message: bin, location=None):\n        """""" Add a new syft operation on the request_pool to be processed asynchronously.\n\n            Args:\n                message : Binary Syft message.\n                location : peer location (This parameter should be preserved to keep the\n                BaseWorker compatibility, but we do not use it.)\n\n            Returns:\n                response_message: Binary Syft response message.\n        """"""\n        self._request_pool.put(WebRTCConnection.HOST_REQUEST + message)\n\n        # Wait\n        # PySyft is a sync library and should wait for this response.\n        while self._response_pool.empty():\n            time.sleep(0)\n        return self._response_pool.get()\n\n    def _recv_msg(self, message: bin):\n        """""" Called when someone call syft function locally eg. tensor.send(node)\n\n            PS: This method should be synchronized to keep the compatibility with Syft\n            internal operations.\n            Args:\n                message: Binary Syft message.\n\n            Returns:\n                response_message : Binary syft response message.\n        """"""\n        if self.available:\n            return self._send_msg(message)\n        else:  # PySyft\'s GC delete commands\n            return self._worker._recv_msg(message)\n\n    # Running async all time\n    async def send(self, channel):\n        """""" Async method that will listen peer remote\'s requests and put it into the\n            request_pool queue to be processed.\n\n            Args:\n                channel: Connection channel used by the peers.\n        """"""\n        while self.available:\n            if not self._request_pool.empty():\n                channel.send(self._request_pool.get())\n            await asyncio.sleep(0)\n\n    # Running async all time\n    def process_msg(self, message, channel):\n        """""" Process syft messages forwarding them to the peer virtual worker and put the\n            response into the response_pool queue to be delivered async.\n\n            Args:\n                message: Binary syft message.\n                channel: Connection channel used by the peers.\n        """"""\n        if message[:2] == WebRTCConnection.HOST_REQUEST:\n            try:\n                decoded_response = self._worker._recv_msg(message[2:])\n            except GetNotPermittedError as e:\n                message = sy.serde.deserialize(message[2:], worker=self._worker)\n                self._worker.tensor_requests.append(message)\n                decoded_response = sy.serde.serialize(e)\n\n            channel.send(WebRTCConnection.REMOTE_REQUEST + decoded_response)\n        else:\n            self._response_pool.put(message[2:])\n\n    def search(self, query):\n        """""" Node\'s dataset search method overwrite.\n\n            Args:\n                query: Query used to search by the desired dataset tag.\n            Returns:\n                query_response: Return the peer\'s response.\n        """"""\n        message = SearchMessage(query)\n        serialized_message = sy.serde.serialize(message)\n        response = self._send_msg(serialized_message)\n        return sy.serde.deserialize(response)\n\n    # Main\n    def run(self):\n        """""" Main thread method used to set up the connection and manage all the process.""""""\n        self.signaling = CopyAndPasteSignaling()\n        self.pc = RTCPeerConnection()\n\n        if self._conn_type == WebRTCConnection.OFFER:\n            func = self._set_offer\n        else:\n            func = self._run_answer\n\n        self.loop = asyncio.new_event_loop()\n        try:\n            self.loop.run_until_complete(func(self.pc, self.signaling))\n        except Exception:\n            self.loop.run_until_complete(self.pc.close())\n\n            # Stop loop:\n            self.loop.run_until_complete(self.loop.shutdown_asyncgens())\n            self.loop.close()\n\n    # OFFER\n    async def _set_offer(self, pc, signaling):\n        """""" Private method used to set up an offer to estabilish a new webrtc connection.\n\n            Args:\n                pc: Peer Connection  descriptor\n                signaling: Webrtc signaling instance.\n        """"""\n        await signaling.connect()\n        channel = pc.createDataChannel(""chat"")\n\n        self.channel = channel\n\n        @channel.on(""open"")\n        def on_open():\n            asyncio.ensure_future(self.send(channel))\n\n        @channel.on(""message"")\n        def on_message(message):\n            self.process_msg(message, channel)\n\n        await pc.setLocalDescription(await pc.createOffer())\n        local_description = object_to_string(pc.localDescription)\n\n        response = {\n            MSG_FIELD.TYPE: NODE_EVENTS.WEBRTC_OFFER,\n            MSG_FIELD.PAYLOAD: local_description,\n            MSG_FIELD.FROM: self._origin,\n        }\n\n        forward_payload = {\n            MSG_FIELD.TYPE: GRID_EVENTS.FORWARD,\n            MSG_FIELD.DESTINATION: self._destination,\n            MSG_FIELD.CONTENT: response,\n        }\n\n        self._grid.send(json.dumps(forward_payload))\n        await self.consume_signaling(pc, signaling)\n\n    # ANSWER\n    async def _run_answer(self, pc, signaling):\n        """""" Private method used to set up an answer to estabilish a new webrtc connection.\n\n            Args:\n                pc: Peer connection.\n                signaling: Webrtc signaling instance.\n        """"""\n        await signaling.connect()\n\n        @pc.on(""datachannel"")\n        def on_datachannel(channel):\n            asyncio.ensure_future(self.send(channel))\n\n            self.channel = channel\n\n            @channel.on(""message"")\n            def on_message(message):\n                self.process_msg(message, channel)\n\n        await self.consume_signaling(pc, signaling)\n\n    async def consume_signaling(self, pc, signaling):\n        """""" Consume signaling to go through all the webrtc connection protocol.\n\n            Args:\n                pc: Peer Connection.\n                signaling: Webrtc signaling instance.\n            Exception:\n                ConnectionClosedException: Exception used to finish this connection\n                and close this thread.\n        """"""\n        # Async keep-alive connection thread\n        while self.available:\n            sleep_time = 0\n            if self._msg == """":\n                await asyncio.sleep(sleep_time)\n                continue\n\n            obj = object_from_string(self._msg)\n\n            if isinstance(obj, RTCSessionDescription):\n                await pc.setRemoteDescription(obj)\n                if obj.type == ""offer"":\n                    # send answer\n                    await pc.setLocalDescription(await pc.createAnswer())\n                    local_description = object_to_string(pc.localDescription)\n\n                    response = {\n                        MSG_FIELD.TYPE: NODE_EVENTS.WEBRTC_ANSWER,\n                        MSG_FIELD.FROM: self._origin,\n                        MSG_FIELD.PAYLOAD: local_description,\n                    }\n\n                    forward_payload = {\n                        MSG_FIELD.TYPE: GRID_EVENTS.FORWARD,\n                        MSG_FIELD.DESTINATION: self._destination,\n                        MSG_FIELD.CONTENT: response,\n                    }\n                    self._grid.send(json.dumps(forward_payload))\n                    sleep_time = 10\n            self._msg = """"\n        raise Exception\n\n    def disconnect(self):\n        """""" Disconnect from the peer and finish this thread. """"""\n        self.available = False\n        del self.connections[self._destination]\n\n    def set_msg(self, content: str):\n        self._msg = content\n'"
syft/messaging/__init__.py,0,b''
syft/messaging/message.py,1,"b'""""""\nThis file exists as the Python encoding of all Message types that Syft sends over the network. It is\nan important bottleneck in the system, impacting both security, performance, and cross-platform\ncompatability. As such, message types should strive to not be framework specific (i.e., Torch,\nTensorflow, etc.).\n\nAll Syft message types extend the Message class.\n""""""\n\nfrom abc import ABC\nfrom abc import abstractmethod\n\nimport syft as sy\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.serde.syft_serializable import SyftSerializable\n\nfrom syft.execution.action import Action\nfrom syft.execution.computation import ComputationAction\nfrom syft.execution.communication import CommunicationAction\n\nfrom syft_proto.messaging.v1.message_pb2 import ObjectMessage as ObjectMessagePB\nfrom syft_proto.messaging.v1.message_pb2 import TensorCommandMessage as CommandMessagePB\nfrom syft_proto.messaging.v1.message_pb2 import (\n    ForceObjectDeleteMessage as ForceObjectDeleteMessagePB,\n)\nfrom syft_proto.messaging.v1.message_pb2 import GetShapeMessage as GetShapeMessagePB\nfrom syft_proto.messaging.v1.message_pb2 import IsNoneMessage as IsNoneMessagePB\n\n# TODO: uncomment this when solving the WorkerCommandMessage issue.\n# from syft_proto.messaging.v1.message_pb2 import WorkerCommandMessage as WorkerCommandMessagePB\nfrom syft_proto.messaging.v1.message_pb2 import SearchMessage as SearchMessagePB\nfrom syft_proto.messaging.v1.message_pb2 import ObjectRequestMessage as ObjectRequestMessagePB\nfrom syft_proto.messaging.v1.message_pb2 import PlanCommandMessage as PlanCommandMessagePB\nfrom syft_proto.types.syft.v1.id_pb2 import Id as IdPB\n\n\nclass Message(ABC, SyftSerializable):\n    """"""All syft message types extend this class\n\n    All messages in the pysyft protocol extend this class. This abstraction\n    requires that every message has an integer type, which is important because\n    this integer is what determines how the message is handled when a BaseWorker\n    receives it.\n\n    Additionally, this type supports a default simplifier and detailer, which are\n    important parts of PySyft\'s serialization and deserialization functionality.\n    You can read more abouty detailers and simplifiers in syft/serde/serde.py.\n    """"""\n\n    @abstractmethod\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        pass\n\n    # Intentionally not abstract\n    def __repr__(self):\n        """"""Return a human readable version of this message""""""\n        return self.__str__()\n\n\nclass TensorCommandMessage(Message):\n    """"""All syft actions use this message type\n\n    In Syft, an action is when one worker wishes to tell another worker to do something with\n    objects contained in the worker.object_store registry (or whatever the official object store is\n    backed with in the case that it\'s been overridden). Semantically, one could view all Messages\n    as a kind of action, but when we say action this is what we mean. For example, telling a\n    worker to take two tensors and add them together is an action. However, sending an object\n    from one worker to another is not an action (and would instead use the ObjectMessage type).""""""\n\n    def __init__(self, action: Action):\n        """"""Initialize an action message\n\n        Args:\n            message (Tuple): this is typically the args and kwargs of a method call on the client,\n                but it can be any information necessary to execute the action properly.\n            return_ids (Tuple): primarily for our async infrastructure (Plan, Protocol, etc.),\n                the id of action results are set by the client. This allows the client to be able\n                to predict where the results will be ahead of time. Importantly, this allows the\n                client to pre-initalize the pointers to the future data, regardless of whether\n                the action has yet executed. It also reduces the size of the response from the\n                action (which is very often empty).\n\n        """"""\n\n        self.action = action\n\n    @property\n    def name(self):\n        return self.action.name\n\n    @property\n    def target(self):\n        return self.action.target\n\n    @property\n    def args(self):\n        return self.action.args\n\n    @property\n    def kwargs(self):\n        return self.action.kwargs\n\n    @property\n    def return_ids(self):\n        return self.action.return_ids\n\n    @property\n    def return_value(self):\n        return self.action.return_value\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {self.action})""\n\n    @staticmethod\n    def computation(name, target, args_, kwargs_, return_ids, return_value=False):\n        """""" Helper function to build a TensorCommandMessage containing a ComputationAction\n        directly from the action arguments.\n        """"""\n        action = ComputationAction(name, target, args_, kwargs_, return_ids, return_value)\n        return TensorCommandMessage(action)\n\n    @staticmethod\n    def communication(name, target, args_, kwargs_, return_ids):\n        """""" Helper function to build a TensorCommandMessage containing a CommunicationAction\n        directly from the action arguments.\n        """"""\n        action = CommunicationAction(name, target, args_, kwargs_, return_ids)\n        return TensorCommandMessage(action)\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, ptr: ""TensorCommandMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a TensorCommandMessage and saves them in a tuple\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            ptr (TensorCommandMessage): a Message\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        Examples:\n            data = simplify(ptr)\n        """"""\n        return (sy.serde.msgpack.serde._simplify(worker, ptr.action),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""TensorCommandMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into a TensorCommandMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            ptr (TensorCommandMessage): an TensorCommandMessage.\n        Examples:\n            message = detail(sy.local_worker, msg_tuple)\n        """"""\n        simplified_action = msg_tuple[0]\n\n        detailed_action = sy.serde.msgpack.serde._detail(worker, simplified_action)\n\n        return TensorCommandMessage(detailed_action)\n\n    @staticmethod\n    def bufferize(\n        worker: AbstractWorker, action_message: ""TensorCommandMessage""\n    ) -> ""CommandMessagePB"":\n        """"""\n        This function takes the attributes of a TensorCommandMessage and saves them in Protobuf\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            action_message (TensorCommandMessage): an TensorCommandMessage\n        Returns:\n            protobuf_obj: a Protobuf message holding the unique attributes of the message\n        Examples:\n            data = bufferize(message)\n        """"""\n        protobuf_action_msg = CommandMessagePB()\n\n        protobuf_action = sy.serde.protobuf.serde._bufferize(worker, action_message.action)\n\n        if isinstance(action_message.action, ComputationAction):\n            protobuf_action_msg.computation.CopyFrom(protobuf_action)\n        elif isinstance(action_message.action, CommunicationAction):\n            protobuf_action_msg.communication.CopyFrom(protobuf_action)\n\n        return protobuf_action_msg\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_obj: ""CommandMessagePB""\n    ) -> ""TensorCommandMessage"":\n        """"""\n        This function takes the Protobuf version of this message and converts\n        it into an TensorCommandMessage. The bufferize() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            protobuf_obj (CommandMessagePB): the Protobuf message\n\n        Returns:\n            obj (TensorCommandMessage): an TensorCommandMessage\n\n        Examples:\n            message = unbufferize(sy.local_worker, protobuf_msg)\n        """"""\n        action = getattr(protobuf_obj, protobuf_obj.WhichOneof(""action""))\n        detailed_action = sy.serde.protobuf.serde._unbufferize(worker, action)\n        return TensorCommandMessage(detailed_action)\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for TensorCommandMessage.\n\n            Returns:\n                Protobuf schema for torch.Size.\n        """"""\n        return CommandMessagePB\n\n\nclass ObjectMessage(Message):\n    """"""Send an object to another worker using this message type.\n\n    When a worker has an object in its local object repository (such as a tensor) and it wants\n    to send that object to another worker (and delete its local copy), it uses this message type\n    to do so.\n    """"""\n\n    def __init__(self, object_):\n        """"""Initialize the message.""""""\n\n        self.object = object_\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {self.object})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, msg: ""ObjectMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a Message and saves them in a tuple.\n        The detail() method runs the inverse of this method.\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            msg (Message): a Message\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        Examples:\n            data = simplify(msg)\n        """"""\n        return (sy.serde.msgpack.serde._simplify(worker, msg.object),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""ObjectMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into an ObjectMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            ptr (ObjectMessage): a ObjectMessage.\n        Examples:\n            message = detail(sy.local_worker, msg_tuple)\n        """"""\n        return ObjectMessage(sy.serde.msgpack.serde._detail(worker, msg_tuple[0]))\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, message: ""ObjectMessage"") -> ""ObjectMessagePB"":\n        """"""\n        This function takes the attributes of an Object Message and saves them in a protobuf object\n        Args:\n            message (ObjectMessage): an ObjectMessage\n        Returns:\n            protobuf: a protobuf object holding the unique attributes of the object message\n        Examples:\n            data = bufferize(object_message)\n        """"""\n\n        protobuf_obj_msg = ObjectMessagePB()\n        bufferized_obj = sy.serde.protobuf.serde._bufferize(worker, message.object)\n        protobuf_obj_msg.tensor.CopyFrom(bufferized_obj)\n        return protobuf_obj_msg\n\n    @staticmethod\n    def unbufferize(worker, protobuf_obj):\n        """"""\n            This method deserializes ObjectMessagePB into ObjectMessage.\n\n            Args:\n                protobuf_obj (ObjectMessagePB): input serialized ObjectMessagePB.\n\n            Returns:\n                object_msg (ObjectMessage): deserialized ObjectMessagePB.\n        """"""\n        protobuf_obj = protobuf_obj.tensor\n        object_ = sy.serde.protobuf.serde._unbufferize(worker, protobuf_obj)\n        object_msg = ObjectMessage(object_)\n\n        return object_msg\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for ObjectMessage.\n\n            Returns:\n                Protobuf schema for ObjectMessage.\n        """"""\n        return ObjectMessagePB\n\n\nclass ObjectRequestMessage(Message):\n    """"""Request another worker to send one of its objects\n\n    If ObjectMessage pushes an object to another worker, this Message type pulls an\n    object from another worker. It also assumes that the other worker will delete it\'s\n    local copy of the object after sending it to you.""""""\n\n    # TODO: add more efficient detailer and simplifier custom for this type\n    # https://github.com/OpenMined/PySyft/issues/2512\n\n    def __init__(self, obj_id, user, reason):\n        """"""Initialize the message.""""""\n\n        self.object_id = obj_id\n        self.user = user\n        self.reason = reason\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {self.object_id, self.user, self.reason})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, msg: ""ObjectRequestMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a Message and saves them in a tuple.\n        The detail() method runs the inverse of this method.\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            msg (Message): a Message\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        Examples:\n            data = simplify(msg)\n        """"""\n        return (\n            sy.serde.msgpack.serde._simplify(worker, msg.object_id),\n            sy.serde.msgpack.serde._simplify(worker, msg.user),\n            sy.serde.msgpack.serde._simplify(worker, msg.reason),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""ObjectRequestMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into an ObjectRequestMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            ptr (ObjectRequestMessage): a ObjectRequestMessage.\n        Examples:\n            message = detail(sy.local_worker, msg_tuple)\n        """"""\n        return ObjectRequestMessage(\n            sy.serde.msgpack.serde._detail(worker, msg_tuple[0]),\n            sy.serde.msgpack.serde._detail(worker, msg_tuple[1]),\n            sy.serde.msgpack.serde._detail(worker, msg_tuple[2]),\n        )\n\n    @staticmethod\n    def bufferize(worker, msg):\n        """"""\n            This method serializes a ObjectRequestMessage using ObjectRequestMessagePB.\n\n            Args:\n                msg (ObjectRequestMessage): input ObjectRequestMessage to be serialized.\n\n            Returns:\n                proto_msg (ObjectRequestMessagePB): serialized ObjectRequestMessage.\n        """"""\n        proto_msg = ObjectRequestMessagePB()\n        sy.serde.protobuf.proto.set_protobuf_id(proto_msg.object_id, msg.object_id)\n        proto_msg.reason = msg.reason\n        return proto_msg\n\n    @staticmethod\n    def unbufferize(worker, proto_msg):\n        """"""\n            This method deserializes ObjectRequestMessagePB into ObjectRequestMessage.\n\n            Args:\n                protobuf_msg (ObjectRequestMessagePB): input serialized ObjectRequestMessagePB.\n\n            Returns:\n               ObjectRequestMessage: deserialized ObjectRequestMessagePB.\n        """"""\n        obj_id = sy.serde.protobuf.proto.get_protobuf_id(proto_msg.object_id)\n        # add worker support when it will be available\n        return ObjectRequestMessage(obj_id=obj_id, user=None, reason=proto_msg.reason)\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for ObjectRequestMessage.\n\n            Returns:\n                Protobuf schema for ObjectRequestMessage.\n        """"""\n        return ObjectRequestMessagePB\n\n\nclass IsNoneMessage(Message):\n    """"""Check if a worker does not have an object with a specific id.\n\n    Occasionally we need to verify whether or not a remote worker has a specific\n    object. To do so, we send an IsNoneMessage, which returns True if the object\n    (such as a tensor) does NOT exist.""""""\n\n    # TODO: add more efficient detailer and simplifier custom for this type\n    # https://github.com/OpenMined/PySyft/issues/2512\n\n    def __init__(self, obj_id):\n        """"""Initialize the message.""""""\n\n        self.object_id = obj_id\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {self.object_id})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, msg: ""IsNoneMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a Message and saves them in a tuple.\n        The detail() method runs the inverse of this method.\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            msg (Message): a Message\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        Examples:\n            data = simplify(msg)\n        """"""\n        return (sy.serde.msgpack.serde._simplify(worker, msg.object_id),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""IsNoneMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into an IsNoneMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            ptr (IsNoneMessage): a IsNoneMessage.\n        Examples:\n            message = detail(sy.local_worker, msg_tuple)\n        """"""\n        return IsNoneMessage(sy.serde.msgpack.serde._detail(worker, msg_tuple[0]))\n\n    @staticmethod\n    def bufferize(worker, msg):\n        """"""\n            This method serializes a IsNoneMessage using IsNoneMessagePB.\n\n            Args:\n                msg (IsNoneMessage): input IsNoneMessage to be serialized.\n\n            Returns:\n                protobuf_script (IsNoneMessagePB): serialized IsNoneMessage.\n        """"""\n        proto_msg = IsNoneMessagePB()\n        sy.serde.protobuf.proto.set_protobuf_id(proto_msg.object_id, msg.object_id)\n        return proto_msg\n\n    @staticmethod\n    def unbufferize(worker, proto_msg):\n        """"""\n            This method deserializes IsNoneMessagePB into IsNoneMessage.\n\n            Args:\n                protobuf_msg (IsNoneMessagePB): input serialized IsNoneMessagePB.\n\n            Returns:\n                IsNoneMessage: deserialized IsNoneMessagePB.\n        """"""\n        obj_id = sy.serde.protobuf.proto.get_protobuf_id(proto_msg.object_id)\n        return IsNoneMessage(obj_id=obj_id)\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for IsNoneMessage.\n\n            Returns:\n                Protobuf schema for ObjectRequestMessage.\n        """"""\n        return IsNoneMessagePB\n\n\nclass GetShapeMessage(Message):\n    """"""Get the shape property of a tensor in PyTorch\n\n    We needed to have a special message type for this because .shape had some\n    constraints in the older version of PyTorch.""""""\n\n    # TODO: remove this message type and use ObjectRequestMessage instead.\n    # note that the above to do is likely waiting for custom tensor type support in PyTorch\n    # https://github.com/OpenMined/PySyft/issues/2513\n\n    # TODO: add more efficient detailer and simplifier custom for this type\n    # https://github.com/OpenMined/PySyft/issues/2512\n\n    def __init__(self, tensor_id):\n        """"""Initialize the message.""""""\n\n        self.tensor_id = tensor_id\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {self.tensor_id})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, msg: ""GetShapeMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a Message and saves them in a tuple.\n        The detail() method runs the inverse of this method.\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            msg (Message): a Message\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        Examples:\n            data = simplify(msg)\n        """"""\n        return (sy.serde.msgpack.serde._simplify(worker, msg.tensor_id),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""GetShapeMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into an GetShapeMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            msg (GetShapeMessage): a GetShapeMessage.\n        Examples:\n            message = detail(sy.local_worker, msg_tuple)\n        """"""\n        return GetShapeMessage(sy.serde.msgpack.serde._detail(worker, msg_tuple[0]))\n\n    @staticmethod\n    def bufferize(worker, msg):\n        """"""\n            This method serializes a GetShapeMessage using GetShapeMessagePB.\n\n            Args:\n                msg (GetShapeMessage): input GetShapeMessage to be serialized.\n\n            Returns:\n                proto_msg (GetShapeMessagePB): serialized GetShapeMessage.\n        """"""\n        proto_msg = GetShapeMessagePB()\n        sy.serde.protobuf.proto.set_protobuf_id(proto_msg.object_id, msg.tensor_id)\n        return proto_msg\n\n    @staticmethod\n    def unbufferize(worker, proto_obj):\n        """"""\n            This method deserializes GetShapeMessagePB into GetShapeMessage.\n\n            Args:\n                protobuf_obj (GetShapeMessage): input serialized GetShapeMessagePB.\n\n            Returns:\n                GetShapeMessage: deserialized GetShapeMessagePB.\n        """"""\n        tensor_id = sy.serde.protobuf.proto.get_protobuf_id(proto_obj.object_id)\n        return GetShapeMessage(tensor_id=tensor_id)\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for GetShapeMessage.\n\n            Returns:\n                Protobuf schema for GetShapeMessage.\n        """"""\n        return GetShapeMessagePB\n\n\nclass ForceObjectDeleteMessage(Message):\n    """"""Garbage collect a remote object\n\n    This is the dominant message for garbage collection of remote objects. When\n    a pointer is deleted, this message is triggered by default to tell the object\n    being pointed to to also delete itself.\n    """"""\n\n    # TODO: add more efficient detailer and simplifier custom for this type\n    # https://github.com/OpenMined/PySyft/issues/2512\n\n    def __init__(self, obj_id):\n        """"""Initialize the message.""""""\n\n        self.object_id = obj_id\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {self.object_id})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, msg: ""ForceObjectDeleteMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a Message and saves them in a tuple.\n        The detail() method runs the inverse of this method.\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            msg (Message): a Message\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        Examples:\n            data = simplify(msg)\n        """"""\n        return (sy.serde.msgpack.serde._simplify(worker, msg.object_id),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""ForceObjectDeleteMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into an ForceObjectDeleteMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            msg (ForceObjectDeleteMessage): a ForceObjectDeleteMessage.\n        Examples:\n            message = detail(sy.local_worker, msg_tuple)\n        """"""\n        return ForceObjectDeleteMessage(sy.serde.msgpack.serde._detail(worker, msg_tuple[0]))\n\n    @staticmethod\n    def bufferize(worker, msg):\n        """"""\n            This method serializes a ForceObjectDeleteMessage using ForceObjectDeleteMessagePB.\n\n            Args:\n                msg (ForceObjectDeleteMessage): input ForceObjectDeleteMessage to be serialized.\n\n            Returns:\n                proto_msg (ForceObjectDeleteMessagePB): serialized ForceObjectDeleteMessage.\n        """"""\n        proto_msg = ForceObjectDeleteMessagePB()\n        sy.serde.protobuf.proto.set_protobuf_id(proto_msg.object_id, msg.object_id)\n        return proto_msg\n\n    @staticmethod\n    def unbufferize(worker, proto_msg):\n        """"""\n            This method deserializes ForceObjectDeleteMessagePB into ForceObjectDeleteMessage.\n\n            Args:\n                proto_msg (ForceObjectDeleteMessagePB): input serialized ForceObjectDeleteMessagePB.\n\n            Returns:\n                ForceObjectDeleteMessage: deserialized ForceObjectDeleteMessagePB.\n        """"""\n        obj_id = sy.serde.protobuf.proto.get_protobuf_id(proto_msg.object_id)\n        return ForceObjectDeleteMessage(obj_id=obj_id)\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for ForceObjectDeleteMessage.\n\n            Returns:\n                Protobuf schema for ForceObjectDeleteMessage.\n        """"""\n        return ForceObjectDeleteMessagePB\n\n\nclass SearchMessage(Message):\n    """"""A client queries for a subset of the tensors on a remote worker using this type\n\n    For some workers like SocketWorker we split a worker into a client and a server. For\n    this configuration, a client can request to search for a subset of tensors on the server\n    using this message type (this could also be called a ""QueryMessage"").\n    """"""\n\n    # TODO: add more efficient detailer and simplifier custom for this type\n    # https://github.com/OpenMined/PySyft/issues/2512\n\n    def __init__(self, query):\n        """"""Initialize the message.""""""\n\n        self.query = query\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {self.query})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, msg: ""SearchMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a Message and saves them in a tuple.\n        The detail() method runs the inverse of this method.\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            msg (Message): a Message\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        Examples:\n            data = simplify(msg)\n        """"""\n        return (sy.serde.msgpack.serde._simplify(worker, msg.query),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""SearchMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into an SearchMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            ptr (SearchMessage): a SearchMessage.\n        Examples:\n            message = detail(sy.local_worker, msg_tuple)\n        """"""\n        return SearchMessage(sy.serde.msgpack.serde._detail(worker, msg_tuple[0]))\n\n    @staticmethod\n    def bufferize(worker, msg):\n        """"""\n            This method serializes a SearchMessage using SearchMessagePB.\n\n            Args:\n                msg (SearchMessage): input SearchMessage to be serialized.\n\n            Returns:\n                proto_msg (SearchMessagePB): serialized SearchMessage.\n        """"""\n        proto_msg = SearchMessagePB()\n        for elem in msg.query:\n            id = IdPB()\n            if isinstance(elem, str):\n                id.id_str = elem\n            else:\n                id.id_int = elem\n            proto_msg.query.append(id)\n        return proto_msg\n\n    @staticmethod\n    def unbufferize(worker, proto_obj):\n        """"""\n            This method deserializes SearchMessagePB into SearchMessage.\n\n            Args:\n                proto_msg (SearchMessagePB): input serialized SearchMessagePB.\n\n            Returns:\n                SearchMessage: deserialized SearchMessagePB.\n        """"""\n        query = []\n        for elem in proto_obj.query:\n            query.append(getattr(elem, elem.WhichOneof(""id"")))\n\n        return SearchMessage(query=query)\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for SearchMessage.\n\n            Returns:\n                Protobuf schema for SearchMessage.\n        """"""\n        return SearchMessagePB\n\n\nclass PlanCommandMessage(Message):\n    """"""Message used to execute a command related to plans.""""""\n\n    # TODO: add more efficient detailer and simplifier custom for this type\n    # https://github.com/OpenMined/PySyft/issues/2512\n\n    def __init__(self, command_name: str, args_: tuple):\n        """"""Initialize a PlanCommandMessage.\n\n        Args:\n            command_name (str): name used to identify the command.\n            message (Tuple): this is typically the args and kwargs of a method call on the client,\n                but it can be any information necessary to execute the command properly.\n        """"""\n\n        self.command_name = command_name\n        self.args = args_\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {(self.command_name, self.args)})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, msg: ""PlanCommandMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a PlanCommandMessage and saves them in a tuple\n\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            msg (PlanCommandMessage): a Message\n\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        """"""\n        return (\n            sy.serde.msgpack.serde._simplify(worker, msg.command_name),\n            sy.serde.msgpack.serde._simplify(worker, msg.args),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""PlanCommandMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into a PlanCommandMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            ptr (PlanCommandMessage): a PlanCommandMessage.\n        """"""\n        command_name, args_ = msg_tuple\n        return PlanCommandMessage(\n            sy.serde.msgpack.serde._detail(worker, command_name),\n            sy.serde.msgpack.serde._detail(worker, args_),\n        )\n\n    @staticmethod\n    def bufferize(worker, msg):\n        """"""\n            This method serializes a PlanCommandMessage using PlanCommandMessagePB.\n\n            Args:\n                msg (PlanCommandMessage): input PlanCommandMessage to be serialized.\n\n            Returns:\n                proto_msg (PlanCommandMessagePB): serialized PlanCommandMessage.\n        """"""\n        proto_msg = PlanCommandMessagePB()\n        proto_msg.command_name = msg.command_name\n        for arg in sy.serde.protobuf.serde.bufferize_args(worker, msg.args):\n            proto_msg.args.append(arg)\n        return proto_msg\n\n    @staticmethod\n    def unbufferize(worker, proto_msg):\n        """"""\n            This method deserializes PlanCommandMessagePB into PlanCommandMessage.\n\n            Args:\n                proto_msg (PlanCommandMessagePB): input serialized PlanCommandMessagePB.\n\n            Returns:\n                PlanCommandMessage: deserialized PlanCommandMessagePB.\n        """"""\n        args = sy.serde.protobuf.serde.unbufferize_args(worker, proto_msg.args)\n        return PlanCommandMessage(command_name=proto_msg.command_name, args_=tuple(args))\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for PlanCommandMessage.\n\n            Returns:\n                Protobuf schema for PlanCommandMessage.\n        """"""\n        return PlanCommandMessagePB\n\n\nclass WorkerCommandMessage(Message):\n    """"""Message used to execute a function of the remote worker.""""""\n\n    # TODO: add more efficient detailer and simplifier custom for this type\n    # https://github.com/OpenMined/PySyft/issues/2512\n\n    def __init__(self, command_name: str, message: tuple):\n        """"""Initialize a WorkerCommandMessage.\n\n        Args:\n            command_name (str): name used to identify the command.\n            message (Tuple): this is typically the args and kwargs of a method call on the client,\n                but it can be any information necessary to execute the command properly.\n        """"""\n\n        # call the parent constructor - setting the type integer correctly\n        super().__init__()\n\n        self.command_name = command_name\n        self.message = message\n\n    def __str__(self):\n        """"""Return a human readable version of this message""""""\n        return f""({type(self).__name__} {(self.command_name, self.message)})""\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, ptr: ""WorkerCommandMessage"") -> tuple:\n        """"""\n        This function takes the attributes of a WorkerCommandMessage and saves them in a tuple\n\n        Args:\n            worker (AbstractWorker): a reference to the worker doing the serialization\n            ptr (WorkerCommandMessage): a Message\n\n        Returns:\n            tuple: a tuple holding the unique attributes of the message\n        """"""\n        return (\n            sy.serde.msgpack.serde._simplify(worker, ptr.command_name),\n            sy.serde.msgpack.serde._simplify(worker, ptr.message),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, msg_tuple: tuple) -> ""WorkerCommandMessage"":\n        """"""\n        This function takes the simplified tuple version of this message and converts\n        it into a WorkerCommandMessage. The simplify() method runs the inverse of this method.\n\n        Args:\n            worker (AbstractWorker): a reference to the worker necessary for detailing. Read\n                syft/serde/serde.py for more information on why this is necessary.\n            msg_tuple (Tuple): the raw information being detailed.\n        Returns:\n            ptr (WorkerCommandMessage): a WorkerCommandMessage.\n        """"""\n        command_name, message = msg_tuple\n        return WorkerCommandMessage(\n            sy.serde.msgpack.serde._detail(worker, command_name),\n            sy.serde.msgpack.serde._detail(worker, message),\n        )\n\n    @staticmethod\n    def bufferize(worker, msg):\n        """"""\n            This method serializes a WorkerCommandMessage using WorkerCommandMessagePB.\n\n            Args:\n                msg (WorkerCommandMessage): input WorkerCommandMessage to be serialized.\n\n            Returns:\n                proto_msg (WorkerCommandMessagePB): serialized WorkerCommandMessage.\n        """"""\n        proto_msg = WorkerCommandMessage()\n        proto_msg.command_name = msg.command_name\n        for arg in sy.serde.protobuf.serde.bufferize_args(worker, msg.args):\n            proto_msg.args.append(arg)\n        return proto_msg\n\n    @staticmethod\n    def unbufferize(worker, proto_msg):\n        """"""\n            This method deserializes WorkerCommandMessagePB into WorkerCommandMessage.\n\n            Args:\n                proto_msg (WorkerCommandMessagePB): input serialized WorkerCommandMessagePB.\n\n            Returns:\n                WorkerCommandMessage: deserialized WorkerCommandMessagePB.\n        """"""\n        args = sy.serde.protobuf.serde.unbufferize_args(worker, proto_msg.args)\n        return WorkerCommandMessage(command_name=proto_msg.command_name, args_=tuple(args))\n\n    # TODO: when testing is fixed, uncomment this to enable worker command message support.\n    # @staticmethod\n    # def get_protobuf_schema():\n    #     """"""\n    #         Returns the protobuf schema used for WorkerCommandMessage.\n    #\n    #         Returns:\n    #             Protobuf schema for WorkerCommandMessage.\n    #     """"""\n    #     return WorkerCommandMessagePB\n'"
syft/serde/__init__.py,0,"b'from syft.serde.serde import serialize, deserialize  # noqa: F401\n'"
syft/serde/compression.py,0,"b'""""""\nThis file exists to provide one common place for all compression methods used in\nsimplifying and serializing PySyft objects.\n""""""\nimport zlib\nimport lz4\nfrom lz4 import (  # noqa: F401\n    frame,\n)  # needed as otherwise we will get: module \'lz4\' has no attribute \'frame\'\n\nfrom syft.exceptions import CompressionNotFoundException\n\n# COMPRESSION SCHEME INT CODES\nNO_COMPRESSION = 40\nLZ4 = 41\nZLIB = 42\nscheme_to_bytes = {\n    NO_COMPRESSION: NO_COMPRESSION.to_bytes(1, byteorder=""big""),\n    LZ4: LZ4.to_bytes(1, byteorder=""big""),\n    ZLIB: ZLIB.to_bytes(1, byteorder=""big""),\n}\n\n## SECTION: chosen Compression Algorithm\n\n\ndef _apply_compress_scheme(decompressed_input_bin) -> tuple:\n    """"""\n    Apply the selected compression scheme.\n    By default is used LZ4\n\n    Args:\n        decompressed_input_bin: the binary to be compressed\n    """"""\n    return apply_lz4_compression(decompressed_input_bin)\n\n\ndef apply_zlib_compression(uncompressed_input_bin) -> tuple:\n    """"""\n    Apply zlib compression to the input\n\n    Args:\n        decompressed_input_bin: the binary to be compressed\n\n    Returns:\n        a tuple (compressed_result, ZLIB)\n    """"""\n\n    return zlib.compress(uncompressed_input_bin), ZLIB\n\n\ndef apply_lz4_compression(decompressed_input_bin) -> tuple:\n    """"""\n    Apply LZ4 compression to the input\n\n    Args:\n        decompressed_input_bin: the binary to be compressed\n\n    Returns:\n        a tuple (compressed_result, LZ4)\n    """"""\n    return lz4.frame.compress(decompressed_input_bin), LZ4\n\n\ndef apply_no_compression(decompressed_input_bin) -> tuple:\n    """"""\n    No compression is applied to the input\n\n    Args:\n        decompressed_input_bin: the binary\n\n    Returns:\n        a tuple (the binary, LZ4)\n    """"""\n\n    return decompressed_input_bin, NO_COMPRESSION\n\n\ndef _compress(decompressed_input_bin: bin) -> bin:\n    """"""\n    This function compresses a binary using the function _apply_compress_scheme\n    if the input has been already compressed in some step, it will return it as it is\n\n    Args:\n        decompressed_input_bin (bin): binary to be compressed\n\n    Returns:\n        bin: a compressed binary\n\n    """"""\n    compress_stream, compress_scheme = _apply_compress_scheme(decompressed_input_bin)\n    try:\n        z = scheme_to_bytes[compress_scheme] + compress_stream\n        return z\n    except KeyError:\n        raise CompressionNotFoundException(\n            f""Compression scheme not found for compression code: {str(compress_scheme)}""\n        )\n\n\ndef _decompress(binary: bin) -> bin:\n    """"""\n    This function decompresses a binary using the scheme defined in the first byte of the input\n\n    Args:\n        binary (bin): a compressed binary\n\n    Returns:\n        bin: decompressed binary\n\n    """"""\n\n    # check the 1-byte header to check the compression scheme used\n    compress_scheme = binary[0]\n\n    # remove the 1-byte header from the input stream\n    binary = binary[1:]\n    # 1)  Decompress or return the original stream\n    if compress_scheme == LZ4:\n        return lz4.frame.decompress(binary)\n    elif compress_scheme == ZLIB:\n        return zlib.decompress(binary)\n    elif compress_scheme == NO_COMPRESSION:\n        return binary\n    else:\n        raise CompressionNotFoundException(\n            f""Compression scheme not found for compression code: {str(compress_scheme)}""\n        )\n'"
syft/serde/serde.py,0,"b'""""""\nThis file exists to provide one common place for all serialization to occur\nregardless of framework. By default, we serialize using msgpack and compress\nusing lz4. If different compressions are required, the worker can override\nthe function apply_compress_scheme.\n""""""\n\nfrom typing import Callable\n\n\n## SECTION:  High Level Public Functions (these are the ones you use)\ndef serialize(\n    obj: object,\n    worker=None,\n    simplified: bool = False,\n    force_full_simplification: bool = False,\n    strategy: Callable[[object], bin] = None,\n) -> bin:\n    """"""This method can serialize any object PySyft needs to send or store.\n\n    This is the high level function for serializing any object or collection\n    of objects which PySyft needs to send over the wire. It includes three\n    steps, Simplify, Serialize, and Compress as described inline below.\n\n    Args:\n        obj (object): the object to be serialized\n        simplified (bool): in some cases we want to pass in data which has\n            already been simplified - in which case we must skip double\n            simplification - which would be bad.... so bad... so... so bad\n        force_full_simplification (bool): Some objects are only partially serialized\n            by default. For objects where this is the case, setting this flag to True\n            will force the entire object to be serialized. For example, setting this\n            flag to True will cause a VirtualWorker to be serialized WITH all of its\n            tensors while by default VirtualWorker objects only serialize a small\n            amount of metadata.\n\n    Returns:\n        binary: the serialized form of the object.\n    """"""\n    if strategy is None:\n        from syft.serde.msgpack import serialize\n\n        strategy = serialize\n\n    return strategy(obj, worker, simplified, force_full_simplification)\n\n\ndef deserialize(binary: bin, worker=None, strategy: Callable[[bin], object] = None,) -> object:\n    """""" This method can deserialize any object PySyft needs to send or store.\n\n    This is the high level function for deserializing any object or collection\n    of objects which PySyft has sent over the wire or stored. It includes three\n    steps, Decompress, Deserialize, and Detail as described inline below.\n\n    Args:\n        binary (bin): the serialized object to be deserialized.\n        worker (AbstractWorker): the worker which is acquiring the message content,\n            for example used to specify the owner of a tensor received(not obvious\n            for virtual workers)\n        details (bool): there are some cases where we need to perform the decompression\n            and deserialization part, but we don\'t need to detail all the message.\n            This is the case for Plan workers for instance\n\n    Returns:\n        object: the deserialized form of the binary input.\n    """"""\n\n    if strategy is None:\n        from syft.serde.msgpack import deserialize\n\n        strategy = deserialize\n\n    return strategy(binary, worker)\n'"
syft/serde/syft_serializable.py,1,"b'import inspect\nfrom typing import Callable\n\n\ndef get_from_inheritance_chain(cls: type, condition: Callable) -> set:\n    """"""\n        Generic function that extracts all nodes from the inheritance tree that respects\n        a first order logic condition.\n    """"""\n    original_subclasses = {s for s in cls.__subclasses__() if condition(s)}\n    sub_sets = {\n        s\n        for c in cls.__subclasses__()\n        for s in get_from_inheritance_chain(c, condition)\n        if condition(s)\n    }\n    return original_subclasses.union(sub_sets)\n\n\ndef get_protobuf_wrappers(cls: type) -> set:\n    """"""\n        Function to retrieve all wrappers that implement the protobuf methods from the\n        SyftSerializable class:\n\n        A type that wants to implement to wrap another type (eg. torch.Tensor) for the protobuf\n        interface and to use it with syft-proto has to inherit SyftSerializable (directly or\n        from the parent class) and to implement\n        (cannot inherit from parent class):\n            1. bufferize\n            2. unbufferize\n            3. get_protobuf_schema\n            4. get_original_class\n        If these methods are not implemented, the class won\'t be enrolled in the types that\n        are wrappers can\'t use syft-proto.\n    """"""\n\n    def check_implementation(s):\n        """"""\n            Check if a class has:\n                1. bufferize implemented.\n                2. unbufferize implemented.\n                3. get_protobuf_schema implemented.\n                4. no abstact methods.\n                5. get_original_class method\n            To be sure that it can be used with protobufers.\n        """"""\n        not_abstract = not inspect.isabstract(s)\n        bufferize_implemented = s.bufferize.__qualname__.startswith(s.__name__)\n        unbufferize_implemented = s.unbufferize.__qualname__.startswith(s.__name__)\n        get_protobuf_schema_implemented = s.get_protobuf_schema.__qualname__.startswith(s.__name__)\n        get_original_class = s.get_original_class.__qualname__.startswith(s.__name__)\n        return (\n            not_abstract\n            and bufferize_implemented\n            and unbufferize_implemented\n            and get_protobuf_schema_implemented\n            and get_original_class\n        )\n\n    return get_from_inheritance_chain(cls, check_implementation)\n\n\ndef get_protobuf_classes(cls: type) -> set:\n    """"""\n        Function to retrieve all classes that implement the protobuf methods from the\n        SyftSerializable class:\n\n        A type that wants to implement the protobuf interface and to use it with syft-proto has\n        to inherit SyftSerializable (directly or from the parent class) and to implement\n        (cannot inherit from parent class):\n            1. bufferize\n            2. unbufferize\n            3. get_protobuf_schema\n\n        If these methods are not implemented, the class won\'t be enrolled in the types that can\n        use syft-proto.\n    """"""\n\n    def check_implementation(s):\n        """"""\n            Check if a class has:\n                1. bufferize implemented.\n                2. unbufferize implemented.\n                3. get_protobuf_schema implemented.\n                4. no abstact methods.\n                5. no get_original_class methods\n            To be sure that it can be used with protobufers.\n        """"""\n        not_abstract = not inspect.isabstract(s)\n        bufferize_implemented = s.bufferize.__qualname__.startswith(s.__name__)\n        unbufferize_implemented = s.unbufferize.__qualname__.startswith(s.__name__)\n        get_protobuf_schema_implemented = s.get_protobuf_schema.__qualname__.startswith(s.__name__)\n        get_original_class = not s.get_original_class.__qualname__.startswith(s.__name__)\n        return (\n            not_abstract\n            and bufferize_implemented\n            and unbufferize_implemented\n            and get_protobuf_schema_implemented\n            and get_original_class\n        )\n\n    return get_from_inheritance_chain(cls, check_implementation)\n\n\ndef get_msgpack_subclasses(cls):\n    """"""\n        Function to retrieve all classes that implement the msgpack methods from the\n        SyftSerializable class:\n\n        A type that wants to implement the msgpack interface and to use it in syft has\n        to inherit SyftSerializable (directly or from the parent class) and to implement\n        (cannot inherit from parent class):\n            1. simplify\n            2. detail\n\n        If these methods are not implemented, the class won\'t be enrolled in the types that\n        can use msgpack.\n    """"""\n\n    def check_implementation(s):\n        """"""\n            Check if a class has:\n                1. serialize implemented.\n                2. detail implemented.\n\n            To be sure that it can be used with msgpack.\n        """"""\n        not_abstract = not inspect.isabstract(s)\n        bufferize_implemented = s.simplify.__qualname__.startswith(s.__name__)\n        unbufferize_implemented = s.detail.__qualname__.startswith(s.__name__)\n        return not_abstract and bufferize_implemented and unbufferize_implemented\n\n    original_subclasses = {s for s in cls.__subclasses__() if check_implementation(s)}\n    sub_sets = {\n        s\n        for c in cls.__subclasses__()\n        for s in get_msgpack_subclasses(c)\n        if check_implementation(s) and not inspect.isabstract(s)\n    }\n    return original_subclasses.union(sub_sets)\n\n\nclass SyftSerializable:\n    """"""\n        Interface for the communication protocols in syft.\n\n        syft-proto methods:\n            1. bufferize\n            2. unbufferize\n            3. get_protobuf_schema\n\n        msgpack methods:\n            1. simplify\n            2. detail\n\n        Note: the interface can be inherited from parent class, but each class\n        has to write it\'s own explicit methods, even if they are the ones from the parent class.\n    """"""\n\n    @staticmethod\n    def simplify(worker, obj):\n        """"""\n            Serialization method for msgpack.\n\n            Parameters:\n                worker: the worker on which the serialization is being made.\n                obj: the object to be serialized, an instantiated type of\n                the class that implements SyftSerializable.\n\n            Returns:\n                Serialized object using msgpack.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def detail(worker, obj):\n        """"""\n            Deserialization method for msgpack.\n\n            Parameters:\n                worker: the worker on which the serialization is being made.\n                obj: the object to be deserialized, a serialized object of\n                the class that implements SyftSerializable.\n\n            Returns:\n                Serialized object using msgpack.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def bufferize(worker, obj):\n        """"""\n            Serialization method for protobuf.\n\n            Parameters:\n                worker: the worker on which the bufferize is being made.\n                obj: the object to be bufferized using protobufers, an instantiated type\n                of the class that implements SyftSerializable.\n\n            Returns:\n                Protobuf class for the current type.\n        """"""\n\n        raise NotImplementedError\n\n    @staticmethod\n    def get_msgpack_code():\n        """"""\n            Method that provides a code for msgpack if the type is not present in proto.json.\n\n            The returned object should be similar to:\n            {\n                ""code"": int value,\n                ""forced_code"": int value\n            }\n\n            Both keys are optional, the common and right way would be to add only the ""code"" key.\n\n            Returns:\n                dict: A dict with the ""code"" or ""forced_code"" keys.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def unbufferize(worker, obj):\n        """"""\n            Deserialization method for protobuf.\n\n            Parameters:\n                worker: the worker on which the unbufferize is being made.\n                obj: the object to be unbufferized using protobufers, an instantiated type\n                of the class that implements SyftSerializable.\n\n            Returns:\n                Protobuf class for the current type.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for this type.\n\n            Returns:\n                Protobuf type.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def get_original_class():\n        """"""\n            Returns the original type, only used in wrappers.\n\n            Returns:\n                Wrapped type.\n        """"""\n        return NotImplementedError\n'"
syft/workers/__init__.py,0,b''
syft/workers/abstract.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom syft.serde.syft_serializable import SyftSerializable\n\n\nclass AbstractWorker(ABC, SyftSerializable):\n    @abstractmethod\n    def _send_msg(self, message: bin, location: ""AbstractWorker""):\n        """"""Sends message from one worker to another.\n\n        As AbstractWorker implies, you should never instantiate this class by\n        itself. Instead, you should extend AbstractWorker in a new class which\n        instantiates _send_msg and _recv_msg, each of which should specify the\n        exact way in which two workers communicate with each other. The easiest\n        example to study is VirtualWorker.\n\n        Args:\n            message: A binary message to be sent from one worker\n                to another.\n            location: A AbstractWorker instance that lets you provide the\n                destination to send the message.\n        """"""\n        pass\n\n    @abstractmethod\n    def _recv_msg(self, message: bin):\n        """"""Receives the message.\n\n        As AbstractWorker implies, you should never instantiate this class by\n        itself. Instead, you should extend AbstractWorker in a new class which\n        instantiates _send_msg and _recv_msg, each of which should specify the\n        exact way in which two workers communicate with each other. The easiest\n        example to study is VirtualWorker.\n\n        Args:\n            message: The binary message being received.\n        """"""\n        pass\n'"
syft/workers/base.py,6,"b'from contextlib import contextmanager\n\nimport logging\nfrom typing import List\nfrom typing import Union\nfrom typing import TYPE_CHECKING\n\nimport syft as sy\nfrom syft import codes\nfrom syft.execution.plan import Plan\nfrom syft.frameworks.torch.mpc.primitives import PrimitiveStorage\n\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.frameworks.remote import Remote\nfrom syft.generic.frameworks.types import FrameworkTensorType, framework_packages\nfrom syft.generic.frameworks.types import FrameworkShape\nfrom syft.generic.object_storage import ObjectStore\nfrom syft.generic.pointers.object_pointer import ObjectPointer\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\n\nfrom syft.messaging.message import TensorCommandMessage\nfrom syft.messaging.message import WorkerCommandMessage\nfrom syft.messaging.message import GetShapeMessage\nfrom syft.messaging.message import IsNoneMessage\nfrom syft.messaging.message import Message\nfrom syft.messaging.message import ObjectMessage\nfrom syft.messaging.message import ObjectRequestMessage\nfrom syft.messaging.message import PlanCommandMessage\nfrom syft.messaging.message import SearchMessage\n\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.workers.message_handler import BaseMessageHandler\n\nfrom syft.exceptions import ResponseSignatureError\nfrom syft.exceptions import WorkerNotFoundException\n\n\n# this if statement avoids circular imports between base.py and pointer.py\nif TYPE_CHECKING:\n    from syft.generic.frameworks.hook.hook import FrameworkHook\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseWorker(AbstractWorker):\n    """"""Contains functionality to all workers.\n\n    Other workers will extend this class to inherit all functionality necessary\n    for PySyft\'s protocol. Extensions of this class overrides two key methods\n    _send_msg() and _recv_msg() which are responsible for defining the\n    procedure for sending a binary message to another worker.\n\n    At it\'s core, BaseWorker (and all workers) is a collection of objects owned\n    by a certain machine. Each worker defines how it interacts with objects on\n    other workers as well as how other workers interact with objects owned by\n    itself. Objects are either tensors or of any type supported by the PySyft\n    protocol.\n\n    Args:\n        hook: A reference to the TorchHook object which is used\n            to modify PyTorch with PySyft\'s functionality.\n        id: An optional string or integer unique id of the worker.\n        known_workers: An optional dictionary of all known workers on a\n            network which this worker may need to communicate with in the\n            future. The key of each should be each worker\'s unique ID and\n            the value should be a worker class which extends BaseWorker.\n            Extensions of BaseWorker will include advanced functionality\n            for adding to this dictionary(node discovery). In some cases,\n            one can initialize this with known workers to help bootstrap\n            the network.\n        data: Initialize workers with data on creating worker object\n        is_client_worker: An optional boolean parameter to indicate\n            whether this worker is associated with an end user client. If\n            so, it assumes that the client will maintain control over when\n            variables are instantiated or deleted as opposed to handling\n            tensor/variable/model lifecycle internally. Set to True if this\n            object is not where the objects will be stored, but is instead\n            a pointer to a worker that exists elsewhere.\n        log_msgs: An optional boolean parameter to indicate whether all\n            messages should be saved into a log for later review. This is\n            primarily a development/testing feature.\n        auto_add: Determines whether to automatically add this worker to the\n            list of known workers.\n        message_pending_time (optional): A number of seconds to delay the messages to be sent.\n            The argument may be a floating point number for subsecond\n            precision.\n    """"""\n\n    def __init__(\n        self,\n        hook: ""FrameworkHook"",\n        id: Union[int, str] = 0,\n        data: Union[List, tuple] = None,\n        is_client_worker: bool = False,\n        log_msgs: bool = False,\n        verbose: bool = None,\n        auto_add: bool = True,\n        message_pending_time: Union[int, float] = 0,\n    ):\n        """"""Initializes a BaseWorker.""""""\n        super().__init__()\n        self.hook = hook\n\n        self.object_store = ObjectStore(owner=self)\n        self.message_handlers = []\n        self.message_handlers.append(BaseMessageHandler(self.object_store, self))\n\n        self.id = id\n        self.is_client_worker = is_client_worker\n        self.log_msgs = log_msgs\n        if verbose is None:\n            self.verbose = hook.verbose if hasattr(hook, ""verbose"") else False\n        else:\n            self.verbose = verbose\n\n        if isinstance(hook, sy.TorchHook) and hasattr(hook, ""_syft_workers""):\n            hook._syft_workers.add(self)\n\n        self.auto_add = auto_add\n        self._message_pending_time = message_pending_time\n        self.msg_history = []\n\n        self.load_data(data)\n\n        # Declare workers as appropriate\n        self._known_workers = {}\n        if auto_add:\n            if hook is not None and hook.local_worker is not None:\n                known_workers = self.hook.local_worker._known_workers\n                if self.id in known_workers:\n                    if isinstance(known_workers[self.id], type(self)):\n                        # If a worker with this id already exists and it has the\n                        # same type as the one being created, we copy all the attributes\n                        # of the existing worker to this one.\n                        self.__dict__.update(known_workers[self.id].__dict__)\n                    else:\n                        raise RuntimeError(\n                            ""Worker initialized with the same id and different types.""\n                        )\n                else:\n                    hook.local_worker.add_worker(self)\n                    for worker_id, worker in hook.local_worker._known_workers.items():\n                        if worker_id not in self._known_workers:\n                            self.add_worker(worker)\n                        if self.id not in worker._known_workers:\n                            worker.add_worker(self)\n            else:\n                # Make the local worker aware of itself\n                # self is the to-be-created local worker\n                self.add_worker(self)\n\n        if hook is None:\n            self.framework = None\n        else:\n            # TODO[jvmancuso]: avoid branching here if possible, maybe by changing code in\n            #     execute_tensor_command or command_guard to not expect an attribute named ""torch""\n            #     (#2530)\n            self.framework = hook.framework\n            if hasattr(hook, ""torch""):\n                self.torch = self.framework\n                self.remote = Remote(self, ""torch"")\n            elif hasattr(hook, ""tensorflow""):\n                self.tensorflow = self.framework\n                self.remote = Remote(self, ""tensorflow"")\n\n        # storage object for crypto primitives\n        self.crypto_store = PrimitiveStorage(owner=self)\n\n    def get_obj(self, obj_id: Union[str, int]) -> object:\n        """"""Returns the object from registry.\n\n        Look up an object from the registry using its ID.\n\n        Args:\n            obj_id: A string or integer id of an object to look up.\n        """"""\n        return self.object_store.get_obj(obj_id)\n\n    def register_obj(self, obj):\n        self.object_store.register_obj(self, obj)\n\n    def clear_objects(self, return_self: bool = True):\n        """"""Removes all objects from the object storage.\n\n        Note: the ""return self"" statement is kept for backward compatibility\n        with the Udacity Secure and Private ML course.\n\n        Args:\n            return_self: flag, whether to return self as return value\n\n        Returns:\n            self, if return_self if True, else None\n\n        """"""\n        self.object_store.clear_objects()\n\n        # return based on `return_self` flag is required by Udacity course\n        return self if return_self else None\n\n    @contextmanager\n    def registration_enabled(self):\n        self.is_client_worker = False\n        try:\n            yield self\n        finally:\n            self.is_client_worker = True\n\n    def remove_worker_from_registry(self, worker_id):\n        """"""Removes a worker from the dictionary of known workers.\n        Args:\n            worker_id: id to be removed\n        """"""\n        del self._known_workers[worker_id]\n\n    def remove_worker_from_local_worker_registry(self):\n        """"""Removes itself from the registry of hook.local_worker.\n        """"""\n        self.hook.local_worker.remove_worker_from_registry(worker_id=self.id)\n\n    def load_data(self, data: List[Union[FrameworkTensorType, AbstractTensor]]) -> None:\n        """"""Allows workers to be initialized with data when created\n\n           The method registers the tensor individual tensor objects.\n\n        Args:\n\n            data: A list of tensors\n        """"""\n\n        if data:\n            for tensor in data:\n                self.register_obj(tensor)\n                tensor.owner = self\n\n    def search(self, query: Union[List[Union[str, int]], str, int]) -> List:\n        """"""Search for a match between the query terms and a tensor\'s Id, Tag, or Description.\n\n        Note that the query is an AND query meaning that every item in the list of strings (query*)\n        must be found somewhere on the tensor in order for it to be included in the results.\n\n        Args:\n            query: A list of strings to match against.\n\n        Returns:\n            A list of valid results found.\n\n        TODO Search on description is not supported for the moment\n        """"""\n        if isinstance(query, (str, int)):\n            query = [query]\n        # Empty query returns all the tagged and registered values\n        elif len(query) == 0:\n            result_ids = set()\n            for tag, object_ids in self.object_store._tag_to_object_ids.items():\n                result_ids = result_ids.union(object_ids)\n            return [self.get_obj(result_id) for result_id in result_ids]\n\n        results = None\n        for query_item in query:\n            # Search by id is supported but it\'s not the preferred option\n            # It will return a single element and discard tags if the query\n            # Mixed an id with tags\n            result_by_id = self.object_store.find_by_id(query_item)\n            if result_by_id:\n                results = {result_by_id}\n                break\n\n            # results_by_tag can be the empty list\n            results_by_tag = set(self.object_store.find_by_tag(query_item))\n\n            if results:\n                results = results.intersection(results_by_tag)\n            else:\n                results = results_by_tag\n\n        if results is not None:\n            return list(results)\n        else:\n            return []\n\n    def send_msg(self, message: Message, location: ""BaseWorker"") -> object:\n        """"""Implements the logic to send messages.\n\n        The message is serialized and sent to the specified location. The\n        response from the location (remote worker) is deserialized and\n        returned back.\n\n        Every message uses this method.\n\n        Args:\n            msg_type: A integer representing the message type.\n            message: A Message object\n            location: A BaseWorker instance that lets you provide the\n                destination to send the message.\n\n        Returns:\n            The deserialized form of message from the worker at specified\n            location.\n        """"""\n        if self.verbose:\n            print(f""worker {self} sending {message} to {location}"")\n\n        # Step 1: serialize the message to a binary\n        bin_message = sy.serde.serialize(message, worker=self)\n\n        # Step 2: send the message and wait for a response\n        bin_response = self._send_msg(bin_message, location)\n\n        # Step 3: deserialize the response\n        response = sy.serde.deserialize(bin_response, worker=self)\n\n        return response\n\n    def recv_msg(self, bin_message: bin) -> bin:\n        """"""Implements the logic to receive messages.\n\n        The binary message is deserialized and routed to the appropriate\n        function. And, the response serialized the returned back.\n\n        Every message uses this method.\n\n        Args:\n            bin_message: A binary serialized message.\n\n        Returns:\n            A binary message response.\n        """"""\n        # Step 0: deserialize message\n        msg = sy.serde.deserialize(bin_message, worker=self)\n\n        # Step 1: save message and/or log it out\n        if self.log_msgs:\n            self.msg_history.append(msg)\n\n        if self.verbose:\n            print(\n                f""worker {self} received {type(msg).__name__} {msg.contents}""\n                if hasattr(msg, ""contents"")\n                else f""worker {self} received {type(msg).__name__}""\n            )\n\n        # Step 2: route message to appropriate function\n\n        response = None\n        for handler in self.message_handlers:\n            if handler.supports(msg):\n                response = handler.handle(msg)\n                break\n        # TODO(karlhigley): Raise an exception if no handler is found\n\n        # Step 3: Serialize the message to simple python objects\n        bin_response = sy.serde.serialize(response, worker=self)\n\n        return bin_response\n\n        # SECTION:recv_msg() uses self._message_router to route to these methods\n\n    def send(\n        self,\n        obj: Union[FrameworkTensorType, AbstractTensor],\n        workers: ""BaseWorker"",\n        ptr_id: Union[str, int] = None,\n        garbage_collect_data=None,\n        requires_grad=False,\n        create_pointer=True,\n        **kwargs,\n    ) -> ObjectPointer:\n        """"""Sends tensor to the worker(s).\n\n        Send a syft or torch tensor/object and its child, sub-child, etc (all the\n        syft chain of children) to a worker, or a list of workers, with a given\n        remote storage address.\n\n        Args:\n            obj: A syft/framework tensor/object to send.\n            workers: A BaseWorker object representing the worker(s) that will\n                receive the object.\n            ptr_id: An optional string or integer indicating the remote id of\n                the object on the remote worker(s).\n            garbage_collect_data: argument passed down to create_pointer()\n            requires_grad: Default to False. If true, whenever the remote value of this tensor\n                will have its gradient updated (for example when calling .backward()), a call\n                will be made to set back the local gradient value.\n            create_pointer: if set to False, no pointer to the remote value will be built.\n\n        Example:\n            >>> import torch\n            >>> import syft as sy\n            >>> hook = sy.TorchHook(torch)\n            >>> bob = sy.VirtualWorker(hook)\n            >>> x = torch.Tensor([1, 2, 3, 4])\n            >>> x.send(bob, 1000)\n            Will result in bob having the tensor x with id 1000\n\n        Returns:\n            A PointerTensor object representing the pointer to the remote worker(s).\n        """"""\n\n        if not isinstance(workers, (list, tuple)):\n            workers = [workers]\n\n        assert len(workers) > 0, ""Please provide workers to receive the data""\n\n        if len(workers) == 1:\n            worker = workers[0]\n        else:\n            # If multiple workers are provided , you want to send the same tensor\n            # to all the workers. You\'ll get multiple pointers, or a pointer\n            # with different locations\n            raise NotImplementedError(\n                ""Sending to multiple workers is not \\\n                                        supported at the moment""\n            )\n\n        worker = self.get_worker(worker)\n\n        if requires_grad:\n            obj.origin = self.id\n            obj.id_at_origin = obj.id\n\n        # Send the object\n        self.send_obj(obj, worker)\n\n        if requires_grad:\n            obj.origin = None\n            obj.id_at_origin = None\n\n        # If we don\'t need to create the pointer\n        if not create_pointer:\n            return None\n\n        # Create the pointer if needed\n        if hasattr(obj, ""create_pointer"") and not isinstance(\n            obj, sy.Protocol\n        ):  # TODO: this seems like hack to check a type\n            if ptr_id is None:  # Define a remote id if not specified\n                ptr_id = sy.ID_PROVIDER.pop()\n\n            pointer = type(obj).create_pointer(\n                obj,\n                owner=self,\n                location=worker,\n                id_at_location=obj.id,\n                register=True,\n                ptr_id=ptr_id,\n                garbage_collect_data=garbage_collect_data,\n                **kwargs,\n            )\n        else:\n            pointer = obj\n\n        return pointer\n\n    def send_command(\n        self,\n        recipient: ""BaseWorker"",\n        cmd_name: str,\n        target: PointerTensor = None,\n        args_: tuple = (),\n        kwargs_: dict = {},\n        return_ids: str = None,\n        return_value: bool = False,\n    ) -> Union[List[PointerTensor], PointerTensor]:\n        """"""\n        Sends a command through a message to a recipient worker.\n\n        Args:\n            recipient: A recipient worker.\n            cmd_name: Command number.\n            target: Target pointer Tensor.\n            args_: additional args for command execution.\n            kwargs_: additional kwargs for command execution.\n            return_ids: A list of strings indicating the ids of the\n                tensors that should be returned as response to the command execution.\n\n        Returns:\n            A list of PointerTensors or a single PointerTensor if just one response is expected.\n        """"""\n        if return_ids is None:\n            return_ids = (sy.ID_PROVIDER.pop(),)\n\n        try:\n            message = TensorCommandMessage.computation(\n                cmd_name, target, args_, kwargs_, return_ids, return_value\n            )\n            ret_val = self.send_msg(message, location=recipient)\n        except ResponseSignatureError as e:\n            ret_val = None\n            return_ids = e.ids_generated\n\n        if ret_val is None or type(ret_val) == bytes:\n            responses = []\n            for return_id in return_ids:\n                response = PointerTensor(\n                    location=recipient,\n                    id_at_location=return_id,\n                    owner=self,\n                    id=sy.ID_PROVIDER.pop(),\n                )\n                responses.append(response)\n\n            if len(return_ids) == 1:\n                responses = responses[0]\n        else:\n            responses = ret_val\n        return responses\n\n    def register_obj(self, obj: object, obj_id: Union[str, int] = None):\n        """"""Registers the specified object with the current worker node.\n\n        Selects an id for the object, assigns a list of owners, and establishes\n        whether it\'s a pointer or not. This method is generally not used by the\n        client and is instead used by internal processes (hooks and workers).\n\n        Args:\n            obj: A torch Tensor or Variable object to be registered.\n            obj_id (int or string): random integer between 0 and 1e10 or\n                string uniquely identifying the object.\n        """"""\n        if not self.is_client_worker:\n            self.object_store.register_obj(obj, obj_id=obj_id)\n\n    def de_register_obj(self, obj: object, _recurse_torch_objs: bool = True):\n        """"""\n        De-registers the specified object with the current worker node.\n\n        Args:\n            obj: the object to deregister\n            _recurse_torch_objs: A boolean indicating whether the object is\n                more complex and needs to be explored.\n        """"""\n        if not self.is_client_worker:\n            self.object_store.de_register_obj(obj, _recurse_torch_objs)\n\n    # SECTION: convenience methods for constructing frequently used messages\n\n    def send_obj(self, obj: object, location: ""BaseWorker""):\n        """"""Send a torch object to a worker.\n\n        Args:\n            obj: A torch Tensor or Variable object to be sent.\n            location: A BaseWorker instance indicating the worker which should\n                receive the object.\n        """"""\n        return self.send_msg(ObjectMessage(obj), location)\n\n    def request_obj(\n        self, obj_id: Union[str, int], location: ""BaseWorker"", user=None, reason: str = """"\n    ) -> object:\n        """"""Returns the requested object from specified location.\n\n        Args:\n            obj_id (int or string):  A string or integer id of an object to look up.\n            location (BaseWorker): A BaseWorker instance that lets you provide the lookup\n                location.\n            user (object, optional): user credentials to perform user authentication.\n            reason (string, optional): a description of why the data scientist wants to see it.\n        Returns:\n            A torch Tensor or Variable object.\n        """"""\n        obj = self.send_msg(ObjectRequestMessage(obj_id, user, reason), location)\n        return obj\n\n    # SECTION: Manage the workers network\n\n    def get_worker(\n        self, id_or_worker: Union[str, int, ""BaseWorker""], fail_hard: bool = False\n    ) -> Union[str, int, AbstractWorker]:\n        """"""Returns the worker id or instance.\n\n        Allows for resolution of worker ids to workers to happen automatically\n        while also making the current worker aware of new ones when discovered\n        through other processes.\n\n        If you pass in an ID, it will try to find the worker object reference\n        within self._known_workers. If you instead pass in a reference, it will\n        save that as a known_worker if it does not exist as one.\n\n        This method is useful because often tensors have to store only the ID\n        to a foreign worker which may or may not be known by the worker that is\n        de-serializing it at the time of deserialization.\n\n        Args:\n            id_or_worker: A string or integer id of the object to be returned\n                or the BaseWorker object itself.\n            fail_hard (bool): A boolean parameter indicating whether we want to\n                throw an exception when a worker is not registered at this\n                worker or we just want to log it.\n\n        Returns:\n            A string or integer id of the worker or the BaseWorker instance\n            representing the worker.\n\n        Example:\n            >>> import syft as sy\n            >>> hook = sy.TorchHook(verbose=False)\n            >>> me = hook.local_worker\n            >>> bob = sy.VirtualWorker(id=""bob"",hook=hook, is_client_worker=False)\n            >>> me.add_worker([bob])\n            >>> bob\n            <syft.core.workers.virtual.VirtualWorker id:bob>\n            >>> # we can get the worker using it\'s id (1)\n            >>> me.get_worker(\'bob\')\n            <syft.core.workers.virtual.VirtualWorker id:bob>\n            >>> # or we can get the worker by passing in the worker\n            >>> me.get_worker(bob)\n            <syft.core.workers.virtual.VirtualWorker id:bob>\n        """"""\n        if isinstance(id_or_worker, bytes):\n            id_or_worker = str(id_or_worker, ""utf-8"")\n\n        if isinstance(id_or_worker, str) or isinstance(id_or_worker, int):\n            return self._get_worker_based_on_id(id_or_worker, fail_hard=fail_hard)\n        else:\n            return self._get_worker(id_or_worker)\n\n    def _get_worker(self, worker: AbstractWorker):\n        if worker.id not in self._known_workers:\n            self.add_worker(worker)\n        return worker\n\n    def _get_worker_based_on_id(self, worker_id: Union[str, int], fail_hard: bool = False):\n        # A worker should always know itself\n        if worker_id == self.id:\n            return self\n\n        worker = self._known_workers.get(worker_id, worker_id)\n\n        if worker == worker_id:\n            if fail_hard:\n                raise WorkerNotFoundException\n            logger.warning(""Worker %s couldn\'t recognize worker %s"", self.id, worker_id)\n        return worker\n\n    def add_worker(self, worker: ""BaseWorker""):\n        """"""Adds a single worker.\n\n        Adds a worker to the list of _known_workers internal to the BaseWorker.\n        Endows this class with the ability to communicate with the remote\n        worker  being added, such as sending and receiving objects, commands,\n        or  information about the network.\n\n        Args:\n            worker (:class:`BaseWorker`): A BaseWorker object representing the\n                pointer to a remote worker, which must have a unique id.\n\n        Example:\n            >>> import torch\n            >>> import syft as sy\n            >>> hook = sy.TorchHook(verbose=False)\n            >>> me = hook.local_worker\n            >>> bob = sy.VirtualWorker(id=""bob"",hook=hook, is_client_worker=False)\n            >>> me.add_worker([bob])\n            >>> x = torch.Tensor([1,2,3,4,5])\n            >>> x\n            1\n            2\n            3\n            4\n            5\n            [syft.core.frameworks.torch.tensor.FloatTensor of size 5]\n            >>> x.send(bob)\n            FloatTensor[_PointerTensor - id:9121428371 owner:0 loc:bob\n                        id@loc:47416674672]\n            >>> x.get()\n            1\n            2\n            3\n            4\n            5\n            [syft.core.frameworks.torch.tensor.FloatTensor of size 5]\n        """"""\n        if worker.id in self._known_workers:\n            logger.warning(\n                ""Worker ""\n                + str(worker.id)\n                + "" already exists. Replacing old worker which could cause \\\n                    unexpected behavior""\n            )\n        self._known_workers[worker.id] = worker\n\n        return self\n\n    def add_workers(self, workers: List[""BaseWorker""]):\n        """"""Adds several workers in a single call.\n\n        Args:\n            workers: A list of BaseWorker representing the workers to add.\n        """"""\n        for worker in workers:\n            self.add_worker(worker)\n\n        return self\n\n    def __str__(self):\n        """"""Returns the string representation of BaseWorker.\n\n        A to-string method for all classes that extend BaseWorker.\n\n        Returns:\n            The Type and ID of the worker\n\n        Example:\n            A VirtualWorker instance with id \'bob\' would return a string value of.\n            >>> import syft as sy\n            >>> bob = sy.VirtualWorker(id=""bob"")\n            >>> bob\n            <syft.workers.virtual.VirtualWorker id:bob>\n\n        Note:\n            __repr__ calls this method by default.\n        """"""\n\n        out = ""<""\n        out += str(type(self)).split(""\'"")[1].split(""."")[-1]\n        out += "" id:"" + str(self.id)\n        out += "" #objects:"" + str(len(self.object_store._objects))\n        out += "">""\n        return out\n\n    def __repr__(self):\n        """"""Returns the official string representation of BaseWorker.""""""\n        return self.__str__()\n\n    def __getitem__(self, idx):\n        return self.object_store.get_obj(idx, None)\n\n    def request_is_remote_tensor_none(self, pointer: PointerTensor):\n        """"""\n        Sends a request to the remote worker that holds the target a pointer if\n        the value of the remote tensor is None or not.\n        Note that the pointer must be valid: if there is no target (which is\n        different from having a target equal to None), it will return an error.\n\n        Args:\n            pointer: The pointer on which we can to get information.\n\n        Returns:\n            A boolean stating if the remote value is None.\n        """"""\n        return self.send_msg(IsNoneMessage(pointer.id_at_location), location=pointer.location)\n\n    def request_remote_tensor_shape(self, pointer: PointerTensor) -> FrameworkShape:\n        """"""\n        Sends a request to the remote worker that holds the target a pointer to\n        have its shape.\n\n        Args:\n            pointer: A pointer on which we want to get the shape.\n\n        Returns:\n            A torch.Size object for the shape.\n        """"""\n        shape = self.send_msg(GetShapeMessage(pointer.id_at_location), location=pointer.location)\n        return sy.hook.create_shape(shape)\n\n    def fetch_plan(\n        self, plan_id: Union[str, int], location: ""BaseWorker"", copy: bool = False\n    ) -> ""Plan"":  # noqa: F821\n        """"""Fetchs a copy of a the plan with the given `plan_id` from the worker registry.\n\n        This method is executed for local execution.\n\n        Args:\n            plan_id: A string indicating the plan id.\n\n        Returns:\n            A plan if a plan with the given `plan_id` exists. Returns None otherwise.\n        """"""\n        message = PlanCommandMessage(""fetch_plan"", (plan_id, copy))\n        plan = self.send_msg(message, location=location)\n\n        return plan\n\n    def fetch_protocol(\n        self, protocol_id: Union[str, int], location: ""BaseWorker"", copy: bool = False\n    ) -> ""Plan"":  # noqa: F821\n        """"""Fetch a copy of a the protocol with the given `protocol_id` from the worker registry.\n\n        This method is executed for local execution.\n\n        Args:\n            protocol_id: A string indicating the protocol id.\n\n        Returns:\n            A protocol if a protocol with the given `protocol_id` exists. Returns None otherwise.\n        """"""\n        message = PlanCommandMessage(""fetch_protocol"", (protocol_id, copy))\n        protocol = self.send_msg(message, location=location)\n\n        return protocol\n\n    def request_search(self, query: List[str], location: ""BaseWorker"") -> List:\n        """"""\n        Add a remote worker to perform a search\n        Args:\n            query: the tags or id used in the search\n            location: the remote worker identity\n\n        Returns:\n            A list of pointers to the results\n        """"""\n        results = self.send_msg(SearchMessage(query), location=location)\n        for result in results:\n            self.register_obj(result)\n        return results\n\n    def find_or_request(self, tag, location):\n        """"""\n        Allow efficient retrieval: if the tag is know locally, return the local\n        element. Else, perform a search on location\n        """"""\n        results = self.object_store.find_by_tag(tag)\n        if results:\n            assert all(result.location.id == location.id for result in results)\n            return results\n        else:\n            return self.request_search(tag, location=location)\n\n    def _get_msg(self, index):\n        """"""Returns a decrypted message from msg_history. Mostly useful for testing.\n\n        Args:\n            index: the index of the message you\'d like to receive.\n\n        Returns:\n            A decrypted messaging.Message object.\n\n        """"""\n\n        return self.msg_history[index]\n\n    @property\n    def message_pending_time(self):\n        """"""\n        Returns:\n            The pending time in seconds for messaging between virtual workers.\n        """"""\n        return self._message_pending_time\n\n    @message_pending_time.setter\n    def message_pending_time(self, seconds: Union[int, float]) -> None:\n        """"""Sets the pending time to send messaging between workers.\n\n        Args:\n            seconds: A number of seconds to delay the messages to be sent.\n            The argument may be a floating point number for subsecond\n            precision.\n\n        """"""\n        if self.verbose:\n            print(f""Set message pending time to {seconds} seconds."")\n\n        self._message_pending_time = seconds\n\n    @staticmethod\n    def create_worker_command_message(command_name: str, return_ids=None, *args, **kwargs):\n        """"""helper function creating a worker command message\n\n        Args:\n            command_name: name of the command that shall be called\n            return_ids: optionally set the ids of the return values (for remote objects)\n            *args:  will be passed to the call of command_name\n            **kwargs:  will be passed to the call of command_name\n\n        Returns:\n            cmd_msg: a WorkerCommandMessage\n\n        """"""\n        if return_ids is None:\n            return_ids = []\n        return WorkerCommandMessage(command_name, (args, kwargs, return_ids))\n\n    def feed_crypto_primitive_store(self, types_primitives: dict):\n        self.crypto_store.add_primitives(types_primitives)\n\n    def list_tensors(self):\n        return str(self.object_store._tensors)\n\n    def tensors_count(self):\n        return len(self.object_store._tensors)\n\n    def list_objects(self):\n        return str(self.object_store._objects)\n\n    def objects_count(self):\n        return len(self.object_store._objects)\n\n    def _log_msgs(self, value):\n        self.log_msgs = value\n\n    @property\n    def serializer(self, workers=None) -> codes.TENSOR_SERIALIZATION:\n        """"""\n        Define the serialization strategy to adopt depending on the workers it\'s connected to.\n        This is relevant in particular for Tensors which can be serialized in an efficient way\n        between workers which share the same Deep Learning framework, but must be converted to\n        lists or json-like objects in other cases.\n\n        Args:\n            workers: (Optional) the list of workers involved in the serialization. If not\n                provided, self._known_workers is used.\n\n        Returns:\n            A str code:\n                \'all\': serialization must be compatible with all kinds of workers\n                \'torch\': serialization will only work between workers that support PyTorch\n                (more to come: \'tensorflow\', \'numpy\', etc)\n        """"""\n        if workers is None:\n            workers = [w for w in self._known_workers.values() if isinstance(w, AbstractWorker)]\n\n        if not isinstance(workers, list):\n            workers = [workers]\n\n        workers.append(self)\n\n        frameworks = set()\n        for worker in workers:\n            if worker.framework is not None:\n                framework = worker.framework.__name__\n            else:\n                framework = ""None""\n\n            frameworks.add(framework)\n\n        if len(frameworks) == 1 and frameworks == {""torch""}:\n            return codes.TENSOR_SERIALIZATION.TORCH\n        else:\n            return codes.TENSOR_SERIALIZATION.ALL\n\n    @staticmethod\n    def simplify(_worker: AbstractWorker, worker: AbstractWorker) -> tuple:\n        return (sy.serde.msgpack.serde._simplify(_worker, worker.id),)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, worker_tuple: tuple) -> Union[AbstractWorker, int, str]:\n        """"""\n        This function reconstructs a PlanPointer given it\'s attributes in form of a tuple.\n\n        Args:\n            worker: the worker doing the deserialization\n            plan_pointer_tuple: a tuple holding the attributes of the PlanPointer\n        Returns:\n            A worker id or worker instance.\n        """"""\n        worker_id = sy.serde.msgpack.serde._detail(worker, worker_tuple[0])\n\n        referenced_worker = worker.get_worker(worker_id)\n\n        return referenced_worker\n\n    @staticmethod\n    def force_simplify(_worker: AbstractWorker, worker: AbstractWorker) -> tuple:\n        return (\n            sy.serde.msgpack.serde._simplify(_worker, worker.id),\n            sy.serde.msgpack.serde._simplify(_worker, worker.object_store._objects),\n            worker.auto_add,\n        )\n\n    @staticmethod\n    def force_detail(worker: AbstractWorker, worker_tuple: tuple) -> tuple:\n        worker_id, _objects, auto_add = worker_tuple\n        worker_id = sy.serde.msgpack.serde._detail(worker, worker_id)\n\n        result = sy.VirtualWorker(sy.hook, worker_id, auto_add=auto_add)\n        _objects = sy.serde.msgpack.serde._detail(worker, _objects)\n        result.object_store._objects = _objects\n\n        # make sure they weren\'t accidentally double registered\n        for _, obj in _objects.items():\n            if obj.id in worker.object_store._objects:\n                worker.object_store.rm_obj(obj.id)\n\n        return result\n\n    @classmethod\n    def is_framework_supported(cls, framework: str) -> bool:\n        """"""\n        Returns True if framework is supported, else returns False.\n        :param framework: string\n        :return: True/False\n        """"""\n        return framework.lower() in framework_packages\n'"
syft/workers/message_handler.py,3,"b'from typing import List\nfrom typing import Union\n\nimport syft as sy\nfrom syft import codes\n\nfrom syft.execution.computation import ComputationAction\nfrom syft.execution.communication import CommunicationAction\nfrom syft.generic.abstract.hookable import map_chain_call\nfrom syft.generic.abstract.message_handler import AbstractMessageHandler\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.messaging.message import TensorCommandMessage\nfrom syft.messaging.message import WorkerCommandMessage\nfrom syft.messaging.message import ForceObjectDeleteMessage\nfrom syft.messaging.message import GetShapeMessage\nfrom syft.messaging.message import IsNoneMessage\nfrom syft.messaging.message import ObjectMessage\nfrom syft.messaging.message import ObjectRequestMessage\nfrom syft.messaging.message import PlanCommandMessage\nfrom syft.messaging.message import SearchMessage\n\nfrom syft.exceptions import GetNotPermittedError\nfrom syft.exceptions import ObjectNotFoundError\nfrom syft.exceptions import PlanCommandUnknownError\nfrom syft.exceptions import ResponseSignatureError\n\n\nclass BaseMessageHandler(AbstractMessageHandler):\n    def __init__(self, object_store, worker):\n        super().__init__(object_store)\n        self.worker = worker\n\n        self.plan_routing_table = {\n            codes.PLAN_CMDS.FETCH_PLAN: self._fetch_plan_remote,\n            codes.PLAN_CMDS.FETCH_PROTOCOL: self._fetch_protocol_remote,\n        }\n\n    def init_routing_table(self):\n        return {\n            TensorCommandMessage: self.execute_tensor_command,\n            PlanCommandMessage: self.execute_plan_command,\n            WorkerCommandMessage: self.execute_worker_command,\n            ObjectMessage: self.handle_object_msg,\n            ObjectRequestMessage: self.respond_to_obj_req,\n            ForceObjectDeleteMessage: self.handle_force_delete_object_msg,\n            IsNoneMessage: self.is_object_none,\n            GetShapeMessage: self.handle_get_shape_message,\n            SearchMessage: self.respond_to_search,\n        }\n\n    def execute_tensor_command(self, cmd: TensorCommandMessage) -> PointerTensor:\n        if isinstance(cmd.action, ComputationAction):\n            return self.execute_computation_action(cmd.action)\n        else:\n            return self.execute_communication_action(cmd.action)\n\n    def execute_computation_action(self, action: ComputationAction) -> PointerTensor:\n        """"""\n        Executes commands received from other workers.\n        Args:\n            message: A tuple specifying the command and the args.\n        Returns:\n            The result or None if return_value is False.\n        """"""\n\n        op_name = action.name\n        _self = action.target\n        args_ = action.args\n        kwargs_ = action.kwargs\n        return_ids = action.return_ids\n        return_value = action.return_value\n\n        # Handle methods\n        if _self is not None:\n            if type(_self) == int:\n                _self = self.get_obj(_self)\n                if _self is None:\n                    return\n            elif isinstance(_self, str):\n                if _self == ""self"":\n                    _self = self.worker\n                else:\n                    res: list = self.worker.search(_self)\n                    assert (\n                        len(res) == 1\n                    ), f""Searching for {_self} on {self.worker.id}. /!\\\\ {len(res)} found""\n                    _self = res[0]\n            if sy.framework.is_inplace_method(op_name):\n                # TODO[jvmancuso]: figure out a good way to generalize the\n                # above check (#2530)\n                getattr(_self, op_name)(*args_, **kwargs_)\n                return\n            else:\n                try:\n                    response = getattr(_self, op_name)(*args_, **kwargs_)\n                except TypeError:\n                    # TODO Andrew thinks this is gross, please fix. Instead need to\n                    # properly deserialize strings\n                    new_args = [\n                        arg.decode(""utf-8"") if isinstance(arg, bytes) else arg for arg in args_\n                    ]\n                    response = getattr(_self, op_name)(*new_args, **kwargs_)\n        # Handle functions\n        else:\n            # At this point, the command is ALWAYS a path to a\n            # function (i.e., torch.nn.functional.relu). Thus,\n            # we need to fetch this function and run it.\n\n            sy.framework.command_guard(op_name)\n\n            paths = op_name.split(""."")\n            command = self.worker\n            for path in paths:\n                command = getattr(command, path)\n\n            response = command(*args_, **kwargs_)\n\n        # some functions don\'t return anything (such as .backward())\n        # so we need to check for that here.\n        if response is not None:\n            # Register response and create pointers for tensor elements\n            try:\n                response = hook_args.register_response(\n                    op_name, response, list(return_ids), self.worker\n                )\n                # TODO: Does this mean I can set return_value to False and still\n                # get a response? That seems surprising.\n                if return_value or isinstance(response, (int, float, bool, str)):\n                    return response\n                else:\n                    return None\n            except ResponseSignatureError:\n                return_id_provider = sy.ID_PROVIDER\n                return_id_provider.set_next_ids(return_ids, check_ids=False)\n                return_id_provider.start_recording_ids()\n                response = hook_args.register_response(\n                    op_name, response, return_id_provider, self.worker\n                )\n                new_ids = return_id_provider.get_recorded_ids()\n                raise ResponseSignatureError(new_ids)\n\n    def execute_communication_action(self, action: CommunicationAction) -> PointerTensor:\n        owner = action.target.owner\n        destinations = [self.worker.get_worker(id_) for id_ in action.args]\n        kwargs_ = action.kwargs\n\n        if owner != self.worker:\n            return None\n        else:\n            obj = self.get_obj(action.target.id)\n            response = owner.send(obj, *destinations, **kwargs_)\n            response.garbage_collect_data = False\n            if kwargs_.get(""requires_grad"", False):\n                response = hook_args.register_response(\n                    ""send"", response, [sy.ID_PROVIDER.pop()], self.worker\n                )\n            else:\n                self.object_store.rm_obj(action.target.id)\n            return response\n\n    def handle_object_msg(self, obj_msg: ObjectMessage):\n        # This should be a good seam for separating Workers from ObjectStore (someday),\n        # so that Workers have ObjectStores instead of being ObjectStores. That would open\n        # up the possibility of having a separate ObjectStore for each user, or for each\n        # Plan/Protocol, etc. As Syft moves toward multi-tenancy with Grid and so forth,\n        # that will probably be useful for providing security and permissioning. In that\n        # future, this might look like `self.object_store.set_obj(obj_msg.object)`\n        """"""Receive an object from a another worker\n\n        Args:\n            obj: a Framework Tensor or a subclass of an AbstractTensor with an id\n        """"""\n        obj = obj_msg.object\n\n        self.object_store.set_obj(obj)\n\n        if isinstance(obj, FrameworkTensor):\n            tensor = obj\n            if (\n                tensor.requires_grad\n                and tensor.origin is not None\n                and tensor.id_at_origin is not None\n            ):\n                tensor.register_hook(\n                    tensor.trigger_origin_backward_hook(tensor.origin, tensor.id_at_origin)\n                )\n\n    def respond_to_obj_req(self, msg: ObjectRequestMessage):\n        """"""Returns the deregistered object from registry.\n\n        Args:\n            request_msg (tuple): Tuple containing object id, user credentials and reason.\n        """"""\n        obj_id = msg.object_id\n        user = msg.user\n        reason = msg.reason\n\n        obj = self.get_obj(obj_id)\n\n        permitted = all(map_chain_call(obj, ""allow"", user=user))\n        if not permitted:\n            raise GetNotPermittedError()\n        else:\n            self.object_store.de_register_obj(obj)\n            return obj\n\n    def handle_force_delete_object_msg(self, msg: ForceObjectDeleteMessage):\n        self.object_store.force_rm_obj(msg.object_id)\n\n    def is_object_none(self, msg):\n        obj_id = msg.object_id\n        if obj_id not in self.object_store._objects:\n            # If the object is not present on the worker, raise an error\n            raise ObjectNotFoundError(obj_id, self)\n        obj = self.get_obj(msg.object_id)\n        return obj is None\n\n    def handle_get_shape_message(self, msg: GetShapeMessage) -> List:\n        """"""\n        Returns the shape of a tensor casted into a list, to bypass the serialization of\n        a torch.Size object.\n\n        Args:\n            tensor: A torch.Tensor.\n\n        Returns:\n            A list containing the tensor shape.\n        """"""\n        tensor = self.get_obj(msg.tensor_id)\n        return list(tensor.shape)\n\n    def respond_to_search(self, msg: SearchMessage) -> List[PointerTensor]:\n        """"""\n        When remote worker calling search on this worker, forwarding the call and\n        replace found elements by pointers\n        """"""\n        query = msg.query\n        objects = self.worker.search(query)\n        results = []\n        for obj in objects:\n            # set garbage_collect_data to False because if we\'re searching\n            # for a tensor we don\'t own, then it\'s probably someone else\'s\n            # decision to decide when to delete the tensor.\n            ptr = obj.create_pointer(\n                garbage_collect_data=False, owner=sy.local_worker, tags=obj.tags\n            ).wrap()\n            results.append(ptr)\n\n        return results\n\n    def get_obj(self, obj_id: Union[str, int]) -> object:\n        """"""Returns the object from registry.\n\n        Look up an object from the registry using its ID.\n\n        Args:\n            obj_id: A string or integer id of an object to look up.\n        """"""\n        obj = self.object_store.get_obj(obj_id)\n\n        # An object called with get_obj will be ""with high probability"" serialized\n        # and sent back, so it will be GCed but remote data is any shouldn\'t be\n        # deleted\n        if hasattr(obj, ""child"") and hasattr(obj.child, ""set_garbage_collect_data""):\n            obj.child.set_garbage_collect_data(value=False)\n\n        if hasattr(obj, ""private"") and obj.private:\n            return None\n\n        return obj\n\n    def execute_plan_command(self, msg: PlanCommandMessage):\n        """"""Executes commands related to plans.\n\n        This method is intended to execute all commands related to plans and\n        avoiding having several new message types specific to plans.\n\n        Args:\n            msg: A PlanCommandMessage specifying the command and args.\n        """"""\n        command_name = msg.command_name\n        args_ = msg.args\n\n        try:\n            command = self.plan_routing_table[command_name]\n        except KeyError:\n            raise PlanCommandUnknownError(command_name)\n\n        return command(*args_)\n\n    def _fetch_plan_remote(self, plan_id: Union[str, int], copy: bool) -> ""Plan"":  # noqa: F821\n        """"""Fetches a copy of a the plan with the given `plan_id` from the worker registry.\n\n        This method is executed for remote execution.\n\n        Args:\n            plan_id: A string indicating the plan id.\n\n        Returns:\n            A plan if a plan with the given `plan_id` exists. Returns None otherwise.\n        """"""\n        if plan_id in self.object_store._objects:\n            candidate = self.object_store.get_obj(plan_id)\n            if isinstance(candidate, sy.Plan):\n                if copy:\n                    return candidate.copy()\n                else:\n                    return candidate\n\n        return None\n\n    def _fetch_protocol_remote(\n        self, protocol_id: Union[str, int], copy: bool\n    ) -> ""Protocol"":  # noqa: F821\n        """"""\n        Target function of fetch_protocol, find and return a protocol\n        """"""\n        if protocol_id in self.object_store._objects:\n\n            candidate = self.object_store.get_obj(protocol_id)\n            if isinstance(candidate, sy.Protocol):\n                return candidate\n\n        return None\n\n    def execute_worker_command(self, message: tuple):\n        """"""Executes commands received from other workers.\n\n        Args:\n            message: A tuple specifying the command and the args.\n\n        Returns:\n            A pointer to the result.\n        """"""\n        command_name = message.command_name\n        args_, kwargs_, return_ids = message.message\n\n        response = getattr(self.worker, command_name)(*args_, **kwargs_)\n        #  TODO [midokura-silvia]: send the tensor directly\n        #  TODO this code is currently necessary for the async_fit method in websocket_client.py\n        if isinstance(response, FrameworkTensor):\n            self.object_store.register_obj(obj=response, obj_id=return_ids[0])\n            return None\n        return response\n'"
syft/workers/node_client.py,0,"b'import json\n\nfrom typing import Union\nfrom urllib.parse import urlparse\n\n# Syft imports\nfrom syft.serde import serialize\nfrom syft.version import __version__\nfrom syft.execution.plan import Plan\nfrom syft.codes import REQUEST_MSG, RESPONSE_MSG\nfrom syft.workers.websocket_client import WebsocketClientWorker\nfrom syft.grid.authentication.credential import AbstractCredential\n\n\nclass NodeClient(WebsocketClientWorker):\n    """"""Federated Node Client.""""""\n\n    def __init__(\n        self,\n        hook,\n        address,\n        credential: AbstractCredential = None,\n        id: Union[int, str] = 0,\n        is_client_worker: bool = False,\n        log_msgs: bool = False,\n        verbose: bool = False,\n        encoding: str = ""ISO-8859-1"",\n    ):\n        """"""\n        Args:\n            hook : a normal TorchHook object.\n            address : Address used to connect with remote node.\n            credential : Credential used to perform authentication.\n            id : the unique id of the worker (string or int)\n            is_client_worker : An optional boolean parameter to indicate\n                whether this worker is associated with an end user client. If\n                so, it assumes that the client will maintain control over when\n                variables are instantiated or deleted as opposed to handling\n                tensor/variable/model lifecycle internally. Set to True if this\n                object is not where the objects will be stored, but is instead\n                a pointer to a worker that eists elsewhere.\n                log_msgs : whether or not all messages should be\n                saved locally for later inspection.\n            verbose : a verbose option - will print all messages\n                sent/received to stdout.\n            encoding : Encoding pattern used to send/retrieve models.\n        """"""\n        self.address = address\n        self.encoding = encoding\n        self.credential = credential\n\n        # Parse address string to get scheme, host and port\n        self.secure, self.host, self.port = self._parse_address(address)\n\n        # Initialize WebsocketClientWorker / Federated Client\n        super().__init__(\n            hook,\n            self.host,\n            self.port,\n            self.secure,\n            id,\n            is_client_worker,\n            log_msgs,\n            verbose,\n            None,  # initial data\n        )\n\n        # Update Node reference using node\'s Id given by the remote node\n        self._update_node_reference(self._get_node_infos())\n\n        if self.credential:\n            self._authenticate()\n\n    @property\n    def url(self) -> str:\n        """""" Get Node URL Address.\n\n        Returns:\n            address (str) : Node\'s address.\n        """"""\n        if self.port:\n            return (\n                f""wss://{self.host}:{self.port}"" if self.secure else f""ws://{self.host}:{self.port}""\n            )\n        else:\n            return self.address\n\n    @property\n    def models(self) -> list:\n        """""" Get models stored at remote node.\n\n        Returns:\n            models (List) : List of models stored in this node.\n        """"""\n        message = {REQUEST_MSG.TYPE_FIELD: REQUEST_MSG.LIST_MODELS}\n        response = self._forward_json_to_websocket_server_worker(message)\n        return self._return_bool_result(response, RESPONSE_MSG.MODELS)\n\n    def _authenticate(self):\n        """""" Perform Authentication Process using credentials grid credentials.\n\n        Raises:\n            RuntimeError : If authentication process fail.\n        """"""\n        if not isinstance(self.credential, AbstractCredential):\n            raise RuntimeError(""Your credential needs to be an instance of grid credentials."")\n\n        cred_dict = self.credential.json()\n\n        # Prepare a authentication request to remote grid node\n        cred_dict[REQUEST_MSG.TYPE_FIELD] = REQUEST_MSG.AUTHENTICATE\n        response = self._forward_json_to_websocket_server_worker(cred_dict)\n\n        # If succeeded, update node\'s reference and update client\'s credential.\n        node_id = self._return_bool_result(response, RESPONSE_MSG.NODE_ID)\n\n        if node_id:\n            self._update_node_reference(node_id)\n        else:\n            raise RuntimeError(""Invalid user."")\n\n    def _update_node_reference(self, new_id: str):\n        """""" Update worker references changing node id references at hook structure.\n\n        Args:\n            new_id (str) : New worker ID.\n        """"""\n        del self.hook.local_worker._known_workers[self.id]\n        self.id = new_id\n        self.hook.local_worker._known_workers[new_id] = self\n\n    def _parse_address(self, address: str) -> tuple:\n        """""" Parse Address string to define secure flag and split into host and port.\n\n        Args:\n            address (str) : Adress of remote worker.\n        """"""\n        url = urlparse(address)\n        secure = True if url.scheme == ""wss"" else False\n        return (secure, url.hostname, url.port)\n\n    def _get_node_infos(self) -> str:\n        """""" Get Node ID from remote node worker\n\n        Returns:\n            node_id (str) : node id used by remote worker.\n        """"""\n        message = {REQUEST_MSG.TYPE_FIELD: REQUEST_MSG.GET_ID}\n        response = self._forward_json_to_websocket_server_worker(message)\n        node_version = response.get(RESPONSE_MSG.SYFT_VERSION, None)\n        if node_version != __version__:\n            raise RuntimeError(\n                ""Library version mismatch, The PySyft version of your environment is ""\n                + __version__\n                + "" the Grid Node Syft version is ""\n                + node_version\n            )\n\n        return response.get(RESPONSE_MSG.NODE_ID, None)\n\n    def _forward_json_to_websocket_server_worker(self, message: dict) -> dict:\n        """""" Prepare/send a JSON message to a remote node and receive the response.\n\n        Args:\n            message (dict) : message payload.\n        Returns:\n            node_response (dict) : response payload.\n        """"""\n        self.ws.send(json.dumps(message))\n        return json.loads(self.ws.recv())\n\n    def _forward_to_websocket_server_worker(self, message: bin) -> bin:\n        """""" Send a bin message to a remote node and receive the response.\n\n        Args:\n            message (bytes) : message payload.\n        Returns:\n            node_response (bytes) : response payload.\n        """"""\n        self.ws.send_binary(message)\n        response = self.ws.recv()\n        return response\n\n    def _return_bool_result(self, result, return_key=None):\n        if result.get(RESPONSE_MSG.SUCCESS):\n            return result[return_key] if return_key is not None else True\n        elif result.get(RESPONSE_MSG.ERROR):\n            raise RuntimeError(result[RESPONSE_MSG.ERROR])\n        else:\n            raise RuntimeError(""Something went wrong."")\n\n    def connect_nodes(self, node) -> dict:\n        """""" Connect two remote workers between each other.\n\n        Args:\n            node (WebsocketFederatedClient) : Node that will be connected with this remote worker.\n        Returns:\n            node_response (dict) : node response.\n        """"""\n        message = {\n            REQUEST_MSG.TYPE_FIELD: REQUEST_MSG.CONNECT_NODE,\n            ""address"": node.address,\n            ""id"": node.id,\n        }\n        return self._forward_json_to_websocket_server_worker(message)\n\n    def serve_model(\n        self,\n        model,\n        model_id: str = None,\n        mpc: bool = False,\n        allow_download: bool = False,\n        allow_remote_inference: bool = False,\n    ):\n        """""" Hosts the model and optionally serve it using a Socket / Rest API.\n\n        Args:\n            model : A jit model or Syft Plan.\n            model_id (str): An integer/string representing the model id.\n            If it isn\'t provided and the model is a Plan we use model.id,\n            if the model is a jit model we raise an exception.\n            allow_download (bool) : Allow to copy the model to run it locally.\n            allow_remote_inference (bool) : Allow to run remote inferences.\n        Returns:\n            result (bool) : True if model was served sucessfully.\n        Raises:\n            ValueError: model_id isn\'t provided and model is a jit model.\n            RunTimeError: if there was a problem during model serving.\n        """"""\n\n        # If the model is a Plan we send the model\n        # and host the plan version created after\n        # the send action\n        if isinstance(model, Plan):\n            # We need to use the same id in the model\n            # as in the POST request.\n            pointer_model = model.send(self)\n            res_model = pointer_model\n        else:\n            res_model = model\n\n        serialized_model = serialize(res_model)\n\n        message = {\n            REQUEST_MSG.TYPE_FIELD: REQUEST_MSG.HOST_MODEL,\n            ""encoding"": self.encoding,\n            ""model_id"": model_id,\n            ""allow_download"": str(allow_download),\n            ""mpc"": str(mpc),\n            ""allow_remote_inference"": str(allow_remote_inference),\n            ""model"": serialized_model.decode(self.encoding),\n        }\n        response = self._forward_json_to_websocket_server_worker(message)\n        return self._return_bool_result(response)\n\n    def run_remote_inference(self, model_id, data):\n        """""" Run a dataset inference using a remote model.\n\n        Args:\n            model_id (str) : Model ID.\n            data (Tensor) : dataset to be inferred.\n        Returns:\n            inference (Tensor) : Inference result\n        Raises:\n            RuntimeError : If an unexpected behavior happen.\n        """"""\n        serialized_data = serialize(data).decode(self.encoding)\n        message = {\n            REQUEST_MSG.TYPE_FIELD: REQUEST_MSG.RUN_INFERENCE,\n            ""model_id"": model_id,\n            ""data"": serialized_data,\n            ""encoding"": self.encoding,\n        }\n        response = self._forward_json_to_websocket_server_worker(message)\n        return self._return_bool_result(response, RESPONSE_MSG.INFERENCE_RESULT)\n\n    def delete_model(self, model_id: str) -> bool:\n        """""" Delete a model previously registered.\n\n        Args:\n            model_id (String) : ID of the model that will be deleted.\n        Returns:\n            result (bool) : If succeeded, return True.\n        """"""\n        message = {REQUEST_MSG.TYPE_FIELD: REQUEST_MSG.DELETE_MODEL, ""model_id"": model_id}\n        response = self._forward_json_to_websocket_server_worker(message)\n        return self._return_bool_result(response)\n\n    def __str__(self) -> str:\n        return f""<Federated Worker id:{self.id}>""\n'"
syft/workers/tfe.py,0,"b'""""""To be extended in the near future.""""""\nfrom collections import OrderedDict\nimport logging\nimport os\nimport subprocess\nimport tempfile\n\nimport tf_encrypted as tfe\n\n\nlogger = logging.getLogger(""tf_encrypted"")\n_TMP_DIR = tempfile.gettempdir()\n\n\nclass TFEWorker:\n    # TODO(Morten) this should be turned into a proxy, with existing code\n    # extracted into a new component that\'s launched via a script\n\n    def __init__(self, host=None, auto_managed=True):\n        self.host = host\n        self._server_process = None\n        self._auto_managed = auto_managed\n\n    def start(self, player_name, cluster):\n        """"""\n        Start the worker as a player in the given cluster.\n        Depending on whether the worker was constructed with a host or not\n        this may launch a subprocess running a TensorFlow server.\n        """"""\n\n        if self.host is None:\n            # we\'re running using a tfe.LocalConfig which doesn\'t require us to do anything\n            return\n\n        config_filename = os.path.join(_TMP_DIR, ""tfe.config"")\n        config = cluster.tfe_config\n        config.save(config_filename)\n\n        launch_cmd = ""python -m tf_encrypted.player --config {} {}"".format(\n            config_filename, player_name\n        )\n        if self._auto_managed:\n            self._server_process = subprocess.Popen(launch_cmd.split("" ""))\n        else:\n            logger.info(\n                ""If not done already, please launch the following ""\n                ""command in a terminal on host %s: \'%s\'\\n""\n                ""This can be done automatically in a local subprocess by ""\n                ""setting `auto_managed=True` when instantiating a TFEWorker.\\n"",\n                self.host,\n                launch_cmd,\n            )\n\n    def stop(self):\n        """"""\n        Stop the worker. This will shutdown any TensorFlow server launched\n        in `start()`.\n        """"""\n\n        if self.host is None:\n            # we\'re running using a tfe.LocalConfig which doesn\'t require us to do anything\n            return\n\n        if self._auto_managed:\n            if self._server_process is None:\n                return\n            self._server_process.kill()\n            self._server_process.communicate()\n            self._server_process = None\n        else:\n            logger.info(""Please terminate the process on host \'%s\'."", self.host)\n\n    def connect_to_model(self, input_shape, output_shape, cluster, sess=None):\n        """"""\n        Connect to a TF Encrypted model being served by the given cluster.\n\n        This must be done before querying the model.\n        """"""\n\n        config = cluster.tfe_config\n        tfe.set_config(config)\n\n        prot = tfe.protocol.SecureNN(\n            config.get_player(""server0""), config.get_player(""server1""), config.get_player(""server2"")\n        )\n        tfe.set_protocol(prot)\n\n        self._tf_client = tfe.serving.QueueClient(\n            input_shape=input_shape, output_shape=output_shape\n        )\n\n        if sess is None:\n            sess = tfe.Session(config=config)\n        self._tf_session = sess\n\n    def query_model(self, data):\n        """"""\n        Encrypt data and sent it as input to the model being served.\n\n        This will block until a result is ready, and requires that\n        a connection to the model has already been established via\n        `connect_to_model()`.\n        """"""\n        self.query_model_async(data)\n        return self.query_model_join()\n\n    def query_model_async(self, data):\n        """"""\n        Asynchronous version of `query_model` that will not block until a\n        result is ready. Call `query_model_join` to retrive result.\n\n        This requires that a connection to the model has already been\n        established via `connect_to_model()`.\n        """"""\n        self._tf_client.send_input(self._tf_session, data)\n\n    def query_model_join(self):\n        """"""\n        Retrives the result from calling `query_model_async`, blocking until\n        ready.\n        """"""\n        return self._tf_client.receive_output(self._tf_session)\n\n\nclass TFECluster:\n    """"""\n    A TFECluster represents a group of TFEWorkers that are aware about each\n    other and collectively perform an encrypted computation.\n    """"""\n\n    def __init__(self, *workers):\n        tfe_config, player_to_worker_mapping = self._build_cluster(workers)\n        self.tfe_config = tfe_config\n        self.player_to_worker_mapping = player_to_worker_mapping\n\n    @property\n    def workers(self):\n        return list(self.player_to_worker_mapping.values())\n\n    def start(self):\n        """"""\n        Start all workers in the cluster.\n        """"""\n        # Tell the TFE workers to launch TF servers\n        for player_name, worker in self.player_to_worker_mapping.items():\n            worker.start(player_name, self)\n\n    def stop(self):\n        """"""\n        Stop all workers in the cluster.\n        """"""\n        for worker in self.workers:\n            worker.stop()\n\n    def _build_cluster(self, workers):\n        if len(workers) != 3:\n            raise ValueError(f""Expected three workers but {len(workers)} were given"")\n\n        player_to_worker_mapping = OrderedDict()\n        player_to_worker_mapping[""server0""] = workers[0]\n        player_to_worker_mapping[""server1""] = workers[1]\n        player_to_worker_mapping[""server2""] = workers[2]\n\n        use_local_config = all(worker.host is None for worker in workers)\n        if use_local_config:\n            config = tfe.LocalConfig(\n                player_names=player_to_worker_mapping.keys(), auto_add_unknown_players=False\n            )\n            return config, player_to_worker_mapping\n\n        # use tfe.RemoteConfig\n        hostmap = OrderedDict(\n            [(player_name, worker.host) for player_name, worker in player_to_worker_mapping.items()]\n        )\n        config = tfe.RemoteConfig(hostmap)\n\n        return config, player_to_worker_mapping\n'"
syft/workers/virtual.py,0,"b'from time import sleep\nfrom typing import Union\n\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.workers.base import BaseWorker\n\n\nclass VirtualWorker(BaseWorker):\n    def _send_msg(self, message: bin, location: BaseWorker) -> bin:\n        """"""send message to worker location""""""\n        if self.message_pending_time > 0:\n            if self.verbose:\n                print(f""pending time of {self.message_pending_time} seconds to send message..."")\n            sleep(self.message_pending_time)\n\n        return location._recv_msg(message)\n\n    def _recv_msg(self, message: bin) -> bin:\n        """"""receive message""""""\n        return self.recv_msg(message)\n\n    # For backwards compatibility with Udacity course\n    @property\n    def _objects(self):\n        return self.object_store._objects\n\n    @property\n    def _tensors(self):\n        return self.object_store._tensors\n\n    @staticmethod\n    def simplify(_worker: AbstractWorker, worker: ""VirtualWorker"") -> tuple:\n        return BaseWorker.simplify(_worker, worker)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, worker_tuple: tuple) -> Union[""VirtualWorker"", int, str]:\n        detailed = BaseWorker.detail(worker, worker_tuple)\n\n        if isinstance(detailed, int):\n            result = VirtualWorker(id=detailed, hook=worker.hook)\n        else:\n            result = detailed\n\n        return result\n\n    @staticmethod\n    def force_simplify(_worker: AbstractWorker, worker: AbstractWorker) -> tuple:\n        return BaseWorker.force_simplify(_worker, worker)\n\n    @staticmethod\n    def force_detail(worker: AbstractWorker, worker_tuple: tuple) -> ""VirtualWorker"":\n        return BaseWorker.force_detail(worker, worker_tuple)\n'"
syft/workers/websocket_client.py,1,"b'import binascii\nfrom typing import Union\nfrom typing import List\n\nimport torch\nimport websocket\nimport websockets\nimport logging\nimport ssl\nimport time\nimport asyncio\n\nimport syft as sy\n\nfrom syft.exceptions import ResponseSignatureError\n\nfrom syft.messaging.message import Message\nfrom syft.messaging.message import ObjectRequestMessage\nfrom syft.messaging.message import SearchMessage\nfrom syft.messaging.message import TensorCommandMessage\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.workers.base import BaseWorker\n\nlogger = logging.getLogger(__name__)\n\nTIMEOUT_INTERVAL = 60\n\n\nclass WebsocketClientWorker(BaseWorker):\n    def __init__(\n        self,\n        hook,\n        host: str,\n        port: int,\n        secure: bool = False,\n        id: Union[int, str] = 0,\n        is_client_worker: bool = False,\n        log_msgs: bool = False,\n        verbose: bool = False,\n        data: List[Union[torch.Tensor, AbstractTensor]] = None,\n    ):\n        """"""A client which will forward all messages to a remote worker running a\n        WebsocketServerWorker and receive all responses back from the server.\n        """"""\n\n        self.port = port\n        self.host = host\n\n        super().__init__(\n            hook=hook,\n            id=id,\n            data=data,\n            is_client_worker=is_client_worker,\n            log_msgs=log_msgs,\n            verbose=verbose,\n        )\n\n        # creates the connection with the server which gets held open until the\n        # WebsocketClientWorker is garbage collected.\n        # Secure flag adds a secure layer applying cryptography and authentication\n        self.secure = secure\n        self.ws = None\n        self.connect()\n\n    @property\n    def url(self):\n        return f""wss://{self.host}:{self.port}"" if self.secure else f""ws://{self.host}:{self.port}""\n\n    def connect(self):\n        args_ = {""max_size"": None, ""timeout"": TIMEOUT_INTERVAL, ""url"": self.url}\n\n        if self.secure:\n            args_[""sslopt""] = {""cert_reqs"": ssl.CERT_NONE}\n\n        self.ws = websocket.create_connection(**args_)\n        self._log_msgs_remote(self.log_msgs)\n\n    def close(self):\n        self.ws.shutdown()\n\n    def search(self, query):\n        # Prepare a message requesting the websocket server to search among its objects\n        message = SearchMessage(query)\n        serialized_message = sy.serde.serialize(message)\n        # Send the message and return the deserialized response.\n        response = self._send_msg(serialized_message)\n        return sy.serde.deserialize(response)\n\n    def _send_msg(self, message: bin, location=None) -> bin:\n        return self._recv_msg(message)\n\n    def _forward_to_websocket_server_worker(self, message: bin) -> bin:\n        """"""\n        Note: Is subclassed by the node client when you use the GridNode\n        """"""\n        self.ws.send(str(binascii.hexlify(message)))\n        response = binascii.unhexlify(self.ws.recv()[2:-1])\n        return response\n\n    def _recv_msg(self, message: bin) -> bin:\n        """"""Forwards a message to the WebsocketServerWorker""""""\n        response = self._forward_to_websocket_server_worker(message)\n        if not self.ws.connected:\n            logger.warning(""Websocket connection closed (worker: %s)"", self.id)\n            self.ws.shutdown()\n            time.sleep(0.1)\n            # Avoid timing out on the server-side\n            self.ws = websocket.create_connection(self.url, max_size=None, timeout=TIMEOUT_INTERVAL)\n            logger.warning(""Created new websocket connection"")\n            time.sleep(0.1)\n            response = self._forward_to_websocket_server_worker(message)\n            if not self.ws.connected:\n                raise RuntimeError(\n                    ""Websocket connection closed and creation of new connection failed.""\n                )\n        return response\n\n    def _send_msg_and_deserialize(self, command_name: str, *args, **kwargs):\n        message = self.create_worker_command_message(command_name=command_name, *args, **kwargs)\n\n        # Send the message and return the deserialized response.\n        serialized_message = sy.serde.serialize(message)\n        response = self._send_msg(serialized_message)\n        return sy.serde.deserialize(response)\n\n    def list_tensors_remote(self):\n        return self._send_msg_and_deserialize(""list_tensors"")\n\n    def tensors_count_remote(self):\n        return self._send_msg_and_deserialize(""tensors_count"")\n\n    def list_objects_remote(self):\n        return self._send_msg_and_deserialize(""list_objects"")\n\n    def objects_count_remote(self):\n        return self._send_msg_and_deserialize(""objects_count"")\n\n    def _get_msg_remote(self, index):\n        return self._send_msg_and_deserialize(""_get_msg"", index=index)\n\n    def _log_msgs_remote(self, value=True):\n        return self._send_msg_and_deserialize(""_log_msgs"", value=value)\n\n    def clear_objects_remote(self):\n        return self._send_msg_and_deserialize(""clear_objects"", return_self=False)\n\n    async def async_dispatch(self, workers, commands):\n        results = await asyncio.gather(\n            *[\n                worker.async_send_command(message=command)\n                for worker, command in zip(workers, commands)\n            ]\n        )\n        return results\n\n    async def async_send_msg(self, message: Message) -> object:\n        """"""Asynchronous version of send_msg.""""""\n        if self.verbose:\n            print(""async_send_msg"", message)\n\n        async with websockets.connect(\n            self.url, timeout=TIMEOUT_INTERVAL, max_size=None, ping_timeout=TIMEOUT_INTERVAL\n        ) as websocket:\n            # Step 1: serialize the message to a binary\n            bin_message = sy.serde.serialize(message, worker=self)\n\n            # Step 2: send the message\n            await websocket.send(bin_message)\n\n            # Step 3: wait for a response\n            bin_response = await websocket.recv()\n\n            # Step 4: deserialize the response\n            response = sy.serde.deserialize(bin_response, worker=self)\n\n        return response\n\n    async def async_send_command(\n        self, message: tuple, return_ids: str = None, return_value: bool = False\n    ) -> Union[List[PointerTensor], PointerTensor]:\n        """"""\n        Sends a command through a message to the server part attached to the client\n        Args:\n            message: A tuple representing the message being sent.\n            return_ids: A list of strings indicating the ids of the\n                tensors that should be returned as response to the command execution.\n        Returns:\n            A list of PointerTensors or a single PointerTensor if just one response is expected.\n        Note: this is the async version of send_command, with the major difference that you\n        directly call it on the client worker (so we don\'t have the recipient kw argument)\n        """"""\n\n        if return_ids is None:\n            return_ids = (sy.ID_PROVIDER.pop(),)\n\n        name, target, args_, kwargs_ = message\n\n        # Close the existing websocket connection in order to open a asynchronous connection\n        self.close()\n        try:\n            message = TensorCommandMessage.computation(\n                name, target, args_, kwargs_, return_ids, return_value\n            )\n            ret_val = await self.async_send_msg(message)\n\n        except ResponseSignatureError as e:\n            ret_val = None\n            return_ids = e.ids_generated\n        # Reopen the standard connection\n        self.connect()\n\n        if ret_val is None or type(ret_val) == bytes:\n            responses = []\n            for return_id in return_ids:\n                response = PointerTensor(\n                    location=self,\n                    id_at_location=return_id,\n                    owner=sy.local_worker,\n                    id=sy.ID_PROVIDER.pop(),\n                )\n                responses.append(response)\n\n            if len(return_ids) == 1:\n                responses = responses[0]\n        else:\n            responses = ret_val\n        return responses\n\n    async def async_fit(self, dataset_key: str, device: str = ""cpu"", return_ids: List[int] = None):\n        """"""Asynchronous call to fit function on the remote location.\n\n        Args:\n            dataset_key: Identifier of the dataset which shall be used for the training.\n            return_ids: List of return ids.\n\n        Returns:\n            See return value of the FederatedClient.fit() method.\n        """"""\n        if return_ids is None:\n            return_ids = [sy.ID_PROVIDER.pop()]\n\n        # Close the existing websocket connection in order to open a asynchronous connection\n        # This code is not tested with secure connections (wss protocol).\n        self.close()\n        async with websockets.connect(\n            self.url, timeout=TIMEOUT_INTERVAL, max_size=None, ping_timeout=TIMEOUT_INTERVAL\n        ) as websocket:\n            message = self.create_worker_command_message(\n                command_name=""fit"", return_ids=return_ids, dataset_key=dataset_key, device=device\n            )\n\n            # Send the message and return the deserialized response.\n            serialized_message = sy.serde.serialize(message)\n            await websocket.send(str(binascii.hexlify(serialized_message)))\n            await websocket.recv()  # returned value will be None, so don\'t care\n\n        # Reopen the standard connection\n        self.connect()\n\n        # Send an object request message to retrieve the result tensor of the fit() method\n        msg = ObjectRequestMessage(return_ids[0], None, """")\n        serialized_message = sy.serde.serialize(msg)\n        response = self._send_msg(serialized_message)\n\n        # Return the deserialized response.\n        return sy.serde.deserialize(response)\n\n    def fit(self, dataset_key: str, **kwargs):\n        """"""Call the fit() method on the remote worker (WebsocketServerWorker instance).\n\n        Note: The argument return_ids is provided as kwargs as otherwise there is a miss-match\n        with the signature in VirtualWorker.fit() method. This is important to be able to switch\n        between virtual and websocket workers.\n\n        Args:\n            dataset_key: Identifier of the dataset which shall be used for the training.\n            **kwargs:\n                return_ids: List[str]\n        """"""\n        return_ids = kwargs[""return_ids""] if ""return_ids"" in kwargs else [sy.ID_PROVIDER.pop()]\n\n        self._send_msg_and_deserialize(""fit"", return_ids=return_ids, dataset_key=dataset_key)\n\n        msg = ObjectRequestMessage(return_ids[0], None, """")\n        # Send the message and return the deserialized response.\n        serialized_message = sy.serde.serialize(msg)\n        response = self._send_msg(serialized_message)\n        return sy.serde.deserialize(response)\n\n    def evaluate(\n        self,\n        dataset_key: str,\n        return_histograms: bool = False,\n        nr_bins: int = -1,\n        return_loss=True,\n        return_raw_accuracy: bool = True,\n        device: str = ""cpu"",\n    ):\n        """"""Call the evaluate() method on the remote worker (WebsocketServerWorker instance).\n\n        Args:\n            dataset_key: Identifier of the local dataset that shall be used for training.\n            return_histograms: If True, calculate the histograms of predicted classes.\n            nr_bins: Used together with calculate_histograms. Provide the number of classes/bins.\n            return_loss: If True, loss is calculated additionally.\n            return_raw_accuracy: If True, return nr_correct_predictions and nr_predictions\n            device: ""cuda"" or ""cpu""\n\n        Returns:\n            Dictionary containing depending on the provided flags:\n                * loss: avg loss on data set, None if not calculated.\n                * nr_correct_predictions: number of correct predictions.\n                * nr_predictions: total number of predictions.\n                * histogram_predictions: histogram of predictions.\n                * histogram_target: histogram of target values in the dataset.\n        """"""\n\n        return self._send_msg_and_deserialize(\n            ""evaluate"",\n            dataset_key=dataset_key,\n            return_histograms=return_histograms,\n            nr_bins=nr_bins,\n            return_loss=return_loss,\n            return_raw_accuracy=return_raw_accuracy,\n            device=device,\n        )\n\n    def __str__(self):\n        """"""Returns the string representation of a Websocket worker.\n\n        A to-string method for websocket workers that includes information from the websocket server\n\n        Returns:\n            The Type and ID of the worker\n\n        """"""\n        out = ""<""\n        out += str(type(self)).split(""\'"")[1].split(""."")[-1]\n        out += "" id:"" + str(self.id)\n        out += "" #tensors local:"" + str(len(self.object_store._tensors))\n        out += "" #tensors remote: "" + str(self.tensors_count_remote())\n        out += "">""\n        return out\n'"
syft/workers/websocket_server.py,1,"b'import asyncio\nimport binascii\nimport logging\nimport ssl\nfrom typing import Union\nfrom typing import List\n\nimport tblib.pickling_support\nimport torch\nimport websockets\n\nimport syft as sy\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.workers.virtual import VirtualWorker\n\nfrom syft.exceptions import GetNotPermittedError\nfrom syft.exceptions import ResponseSignatureError\n\ntblib.pickling_support.install()\n\n\nclass WebsocketServerWorker(VirtualWorker):\n    def __init__(\n        self,\n        hook,\n        host: str,\n        port: int,\n        id: Union[int, str] = 0,\n        log_msgs: bool = False,\n        verbose: bool = False,\n        data: List[Union[torch.Tensor, AbstractTensor]] = None,\n        loop=None,\n        cert_path: str = None,\n        key_path: str = None,\n    ):\n        """"""This is a simple extension to normal workers wherein\n        all messages are passed over websockets. Note that because\n        BaseWorker assumes a request/response paradigm, this worker\n        enforces this paradigm by default.\n\n        Args:\n            hook (sy.TorchHook): a normal TorchHook object\n            id (str or id): the unique id of the worker (string or int)\n            log_msgs (bool): whether or not all messages should be\n                saved locally for later inspection.\n            verbose (bool): a verbose option - will print all messages\n                sent/received to stdout\n            host (str): the host on which the server should be run\n            port (int): the port on which the server should be run\n            data (dict): any initial tensors the server should be\n                initialized with (such as datasets)\n            loop: the asyncio event loop if you want to pass one in\n                yourself\n            cert_path: path to used secure certificate, only needed for secure connections\n            key_path: path to secure key, only needed for secure connections\n        """"""\n\n        self.port = port\n        self.host = host\n        self.cert_path = cert_path\n        self.key_path = key_path\n\n        if loop is None:\n            loop = asyncio.new_event_loop()\n\n        # this queue is populated when messages are received\n        # from a client\n        self.broadcast_queue = asyncio.Queue()\n\n        # this is the asyncio event loop\n        self.loop = loop\n\n        # call BaseWorker constructor\n        super().__init__(hook=hook, id=id, data=data, log_msgs=log_msgs, verbose=verbose)\n\n    async def _consumer_handler(self, websocket: websockets.WebSocketCommonProtocol):\n        """"""This handler listens for messages from WebsocketClientWorker\n        objects.\n\n        Args:\n            websocket: the connection object to receive messages from and\n                add them into the queue.\n\n        """"""\n        try:\n            while True:\n                msg = await websocket.recv()\n                await self.broadcast_queue.put(msg)\n        except websockets.exceptions.ConnectionClosed:\n            self._consumer_handler(websocket)\n\n    async def _producer_handler(self, websocket: websockets.WebSocketCommonProtocol):\n        """"""This handler listens to the queue and processes messages as they\n        arrive.\n\n        Args:\n            websocket: the connection object we use to send responses\n                back to the client.\n\n        """"""\n        while True:\n\n            # get a message from the queue\n            message = await self.broadcast_queue.get()\n\n            # convert that string message to the binary it represent\n            message = binascii.unhexlify(message[2:-1])\n\n            # process the message\n            response = self._recv_msg(message)\n\n            # convert the binary to a string representation\n            # (this is needed for the websocket library)\n            response = str(binascii.hexlify(response))\n\n            # send the response\n            await websocket.send(response)\n\n    def _recv_msg(self, message: bin) -> bin:\n        try:\n            return self.recv_msg(message)\n        except (ResponseSignatureError, GetNotPermittedError) as e:\n            return sy.serde.serialize(e)\n\n    async def _handler(self, websocket: websockets.WebSocketCommonProtocol, *unused_args):\n        """"""Setup the consumer and producer response handlers with asyncio.\n\n        Args:\n            websocket: the websocket connection to the client\n\n        """"""\n\n        asyncio.set_event_loop(self.loop)\n        consumer_task = asyncio.ensure_future(self._consumer_handler(websocket))\n        producer_task = asyncio.ensure_future(self._producer_handler(websocket))\n\n        done, pending = await asyncio.wait(\n            [consumer_task, producer_task], return_when=asyncio.FIRST_COMPLETED\n        )\n\n        for task in pending:\n            task.cancel()\n\n    def start(self):\n        """"""Start the server""""""\n        # Secure behavior: adds a secure layer applying cryptography and authentication\n        if not (self.cert_path is None) and not (self.key_path is None):\n            ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n            ssl_context.load_cert_chain(self.cert_path, self.key_path)\n            start_server = websockets.serve(\n                self._handler,\n                self.host,\n                self.port,\n                ssl=ssl_context,\n                max_size=None,\n                ping_timeout=None,\n                close_timeout=None,\n            )\n        else:\n            # Insecure\n            start_server = websockets.serve(\n                self._handler,\n                self.host,\n                self.port,\n                max_size=None,\n                ping_timeout=None,\n                close_timeout=None,\n            )\n\n        asyncio.get_event_loop().run_until_complete(start_server)\n        print(""Serving. Press CTRL-C to stop."")\n        try:\n            asyncio.get_event_loop().run_forever()\n        except KeyboardInterrupt:\n            logging.info(""Websocket server stopped."")\n'"
test/common/test_util.py,6,"b'import torch\nimport itertools\n\nfrom syft.common.util import chebyshev_series, chebyshev_polynomials\n\n\ndef test_chebyshev_polynomials():\n    """"""Tests evaluation of chebyshev polynomials""""""\n    sizes = [(1, 10), (3, 5), (3, 5, 10)]\n    possible_terms = [6, 40]\n    tolerance = 0.05\n\n    for size, terms in itertools.product(sizes, possible_terms):\n        tensor = torch.rand(torch.Size(size)) * 42 - 42\n        result = chebyshev_polynomials(tensor, terms)\n\n        # check number of polynomials\n        assert result.shape[0] == terms // 2\n\n        assert torch.all(result[0] == tensor), ""first term is incorrect""\n\n        second_term = 4 * tensor ** 3 - 3 * tensor\n        diff = (result[1] - second_term).abs()\n        norm_diff = diff.div(result[1].abs() + second_term.abs())\n        assert torch.all(norm_diff <= tolerance), ""second term is incorrect""\n\n\ndef test_chebyshev_series():\n    """"""Checks coefficients returned by chebyshev_series are correct""""""\n    for width, terms in [(6, 10), (6, 20)]:\n        result = chebyshev_series(torch.tanh, width, terms)\n\n        # check shape\n        assert result.shape == torch.Size([terms])\n\n        # check terms\n        assert result[0] < 1e-4\n        assert torch.isclose(result[-1], torch.tensor(3.5e-2), atol=1e-1)\n'"
test/efficiency/__init__.py,0,b''
test/efficiency/assertions.py,0,"b'import time\nfrom functools import wraps\n\n\ndef assert_time(max_time):\n    """"""\n    Decorator used to assert time execution of functions.\n    Args:\n        max_time: int or float. Maximum time in seconds that the decorated\n        function should take to be executed\n    """"""\n\n    def decorate(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            t0 = time.time()\n            func(*args, **kwargs)\n            dt = time.time() - t0\n            assert dt < max_time, f""Test run in {round(dt, 2)} > {round(max_time, 2)} s""\n\n        return wrapper\n\n    return decorate\n'"
test/efficiency/test_activations_time.py,2,"b'import pytest\nimport torch\nfrom test.efficiency.assertions import assert_time\n\n\n@pytest.mark.parametrize(""activation"", [""tanh"", ""sigmoid""])\n@assert_time(max_time=10)\ndef test_activation(activation, hook, workers):\n\n    activation_func = torch.tanh if activation == ""tanh"" else torch.sigmoid\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    crypto_prov = workers[""james""]\n\n    x = torch.randn([10, 10]).fix_precision().share(bob, alice, crypto_provider=crypto_prov)\n    activation_func(x)\n'"
test/efficiency/test_linalg_time.py,3,"b'import torch\nfrom syft.frameworks.torch.linalg import inv_sym\nfrom test.efficiency.assertions import assert_time\n\n\n@assert_time(max_time=40)\ndef test_inv_sym(hook, workers):\n    torch.manual_seed(42)  # Truncation might not always work so we set the random seed\n    N = 100\n    K = 2\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    crypto_prov = workers[""james""]\n\n    x = torch.randn(N, K).fix_precision().share(bob, alice, crypto_provider=crypto_prov)\n    gram = x.t().matmul(x)\n    gram_inv = inv_sym(gram)\n'"
test/execution/test_communication.py,0,"b'import pytest\nfrom syft.execution.communication import CommunicationAction\n\n\ndef test_communication_methods_accepted():\n    c = CommunicationAction(""send"", None, (), {}, ())\n\n    assert c.name == ""send""\n\n\ndef test_computation_methods_rejected():\n    with pytest.raises(ValueError):\n        CommunicationAction(""__add__"", None, (), {}, ())\n'"
test/execution/test_package_wrapper.py,2,"b'import syft as sy\nimport torch\n\n\ndef test_plan_module_tracing():\n    @sy.func2plan(args_shape=[(1,)])\n    def plan_test(x, torch=torch):\n        y = torch.rand([1])\n        return x + y\n\n    plan_test(torch.tensor([3]))\n    assert len(plan_test.role.actions) == 2\n'"
test/execution/test_placeholder.py,2,"b'import syft as sy\nimport torch\nimport pytest\n\nfrom syft.execution.placeholder import PlaceHolder\n\n\ndef test_placeholder_expected_shape():\n    @sy.func2plan(args_shape=[(3, 3), (3, 3)])\n    def test_plan(x, y):\n        return x + y\n\n    for placeholder in test_plan.role.input_placeholders():\n        assert placeholder.expected_shape == (3, 3)\n\n\ndef test_create_from():\n    t = torch.tensor([1, 2, 3])\n    ph = PlaceHolder.create_from(t)\n\n    assert isinstance(ph, PlaceHolder)\n    assert (ph.child == torch.tensor([1, 2, 3])).all()\n\n\ndef test_placeholder_forwarding():\n    class TestClass(object):\n        def child_only(self):\n            return ""Method 1""\n\n        def copy(self):\n            return ""Method 2""  # pragma: no cover\n\n    placeholder = PlaceHolder()\n    placeholder.instantiate(TestClass())\n\n    # Should be forwarded to the child\n    assert placeholder.child_only() == ""Method 1""\n\n    # Should be found in placeholder -- should not be forwarded\n    assert placeholder.copy() != ""Method 2""\n\n    # Not found in placeholder or child\n    with pytest.raises(AttributeError):\n        placeholder.dummy_method()\n'"
test/execution/test_plan.py,18,"b'import pytest\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport syft as sy\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.serde.msgpack import serde\nfrom syft.serde.serde import deserialize\nfrom syft.serde.serde import serialize\n\n\ndef test_plan_built_automatically():\n    @sy.func2plan(args_shape=[(1,)])\n    def plan_abs(data):\n        return data.abs()\n\n    assert isinstance(plan_abs.__str__(), str)\n    assert len(plan_abs.actions) > 0\n    assert plan_abs.is_built\n\n\ndef test_plan_build():\n    @sy.func2plan(args_shape=())\n    def plan_abs(data):\n        return data.abs()\n\n    assert not plan_abs.is_built\n    assert not len(plan_abs.actions)\n\n    plan_abs.build(th.tensor([-1]))\n\n    assert len(plan_abs.actions)\n    assert plan_abs.is_built\n\n\ndef test_tracing_torch():\n    @sy.func2plan()\n    def plan_torch(x, torch=th):\n        a = torch.rand([2])\n        x = torch.mul(a, x)\n        return torch.split(x, 2)\n\n    plan_torch.build(th.tensor([1, 2]))\n    plan_torch.forward = None\n    res = plan_torch(th.tensor([1, 2]))\n\n    assert len(plan_torch.actions) == 3\n    assert len(res) == 2\n\n\ndef test_plan_built_automatically_with_any_dimension():\n    @sy.func2plan(args_shape=[(-1, 1)])\n    def plan_abs(data):\n        return data.abs()\n\n    assert isinstance(plan_abs.__str__(), str)\n    assert len(plan_abs.actions) > 0\n\n\ndef test_raise_exception_for_invalid_shape():\n\n    with pytest.raises(ValueError):\n\n        @sy.func2plan(args_shape=[(1, -20)])\n        def _(data):\n            return data  # pragma: no cover\n\n\ndef test_raise_exception_when_sending_unbuilt_plan(workers):\n    bob = workers[""bob""]\n\n    @sy.func2plan()\n    def plan(data):\n        return data  # pragma: no cover\n\n    with pytest.raises(RuntimeError):\n        plan.send(bob)\n\n\ndef test_plan_execute_locally():\n    @sy.func2plan(args_shape=[(1,)])\n    def plan_abs(data):\n        return data.abs()\n\n    x = th.tensor([-1, 2, 3])\n    x_abs = plan_abs(x)\n    assert (x_abs == th.tensor([1, 2, 3])).all()\n\n\ndef test_plan_execute_locally_ambiguous_output(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def serde_plan(x):\n        x = x + x\n        y = x * 2\n        return x\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n    t = th.tensor([2.3])\n    expected = serde_plan(t)\n    actual = serde_plan_detailed(t)\n    assert actual == expected\n\n\ndef test_plan_execute_locally_ambiguous_input(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,), (1,), (1,)])\n    def serde_plan(x, y, z):\n        a = x + x  # 2\n        b = x + z  # 4\n        c = y + z  # 5\n        return c, b, a  # 5, 4, 2\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n    t1, t2, t3 = th.tensor([1]), th.tensor([2]), th.tensor([3])\n    expected = serde_plan(t1, t2, t3)\n    actual = serde_plan_detailed(t1, t2, t3)\n    assert actual == expected\n\n\ndef test_plan_torch_function_no_args(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def serde_plan(x, torch=th):\n        y = torch.tensor([-1])\n        z = x + y\n        return z\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n\n    t = th.tensor([1.0])\n    expected = serde_plan(t)\n    actual = serde_plan_detailed(t)\n    assert actual == expected == th.tensor([0.0])\n\n    @sy.func2plan(args_shape=[(1,)])\n    def serde_plan(x, torch=th):\n        y = torch.arange(3)\n        z = y + x\n        return z\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n\n    t = th.tensor([1.0])\n    expected = serde_plan(t)\n    actual = serde_plan_detailed(t)\n    assert (actual == expected).all()\n    assert (actual == th.tensor([1, 2, 3])).all()\n\n    @sy.func2plan(args_shape=[(1,)])\n    def serde_plan(x, torch=th):\n        torch.manual_seed(14)\n        y = torch.randint(2, size=(1,), dtype=torch.uint8)\n        y = y + 10\n        return y\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n\n    t = th.tensor([1.0])\n    expected = serde_plan(t)\n    actual = serde_plan_detailed(t)\n    assert actual == expected and actual >= 10\n\n\ndef test_plan_with_comp(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(2,), (2,)])\n    def serde_plan(x, y):\n        z = x > y\n        return z\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n\n    t1 = th.tensor([2.0, 0.0])\n    t2 = th.tensor([1.0, 1.0])\n    expected = serde_plan_detailed(t1, t2)\n    actual = serde_plan_detailed(t1, t2)\n    assert (actual == expected).all()\n\n\ndef test_plan_fixed_len_loop(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def serde_plan(x):\n        for i in range(10):\n            x = x + 1\n        return x\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n\n    t = th.tensor([1.0])\n    expected = serde_plan_detailed(t)\n    actual = serde_plan_detailed(t)\n    assert actual == expected\n\n\ndef test_plan_several_output_action(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(4,)])\n    def serde_plan(x, torch=th):\n        y, z = torch.split(x, 2)\n        return y + z\n\n    serde_plan_simplified = serde._simplify(bob, serde_plan)\n    serde_plan_detailed = serde._detail(bob, serde_plan_simplified)\n\n    t = th.tensor([1, 2, 3, 4])\n    expected = serde_plan_detailed(t)\n    actual = serde_plan_detailed(t)\n    assert (actual == th.tensor([4, 6])).all()\n    assert (actual == expected).all()\n\n\ndef test_plan_method_execute_locally(hook):\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(2, 3)\n            self.fc2 = nn.Linear(3, 2)\n            self.fc3 = nn.Linear(2, 1)\n\n        def forward(self, x, torch=th):\n            x = torch.nn.functional.relu(self.fc1(x))\n            x = self.fc2(x)\n            x = self.fc3(x)\n            return torch.nn.functional.log_softmax(x, dim=0)\n\n    model = Net()\n\n    model.build(th.tensor([1.0, 2]))\n\n    # Call one time\n    assert model(th.tensor([1.0, 2])) == 0\n\n    # Call one more time\n    assert model(th.tensor([1.0, 2.1])) == 0\n\n\ndef test_plan_multiple_send(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def plan_abs(data):\n        return data.abs()\n\n    plan_ptr = plan_abs.send(bob)\n    x_ptr = th.tensor([-1, 7, 3]).send(bob)\n    p = plan_ptr(x_ptr)\n    x_abs = p.get()\n\n    assert (x_abs == th.tensor([1, 7, 3])).all()\n\n    # Test get / send plan\n    plan_ptr = plan_abs.send(alice)\n\n    x_ptr = th.tensor([-1, 2, 3]).send(alice)\n    p = plan_ptr(x_ptr)\n    x_abs = p.get()\n    assert (x_abs == th.tensor([1, 2, 3])).all()\n\n\ndef test_plan_built_on_class(hook):\n    """"""\n    Test class Plans and plan send / get / send\n    """"""\n\n    x11 = th.tensor([-1, 2.0]).tag(""input_data"")\n    x21 = th.tensor([-1, 2.0]).tag(""input_data"")\n\n    device_1 = sy.VirtualWorker(hook, id=""device_1"", data=(x11,))\n    device_2 = sy.VirtualWorker(hook, id=""device_2"", data=(x21,))\n\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(2, 3)\n            self.fc2 = nn.Linear(3, 1)\n\n            self.bias = th.tensor([1000.0])\n\n        def forward(self, x, torch=th):\n            x = torch.nn.functional.relu(self.fc1(x))\n            x = self.fc2(x)\n            return torch.nn.functional.log_softmax(x, dim=0) + self.bias\n\n    net = Net()\n\n    # build\n    net.build(th.tensor([1, 2.0]))\n\n    net_ptr = net.send(device_1)\n    pointer_to_data = hook.local_worker.request_search(""input_data"", location=device_1)[0]\n    pointer_to_result = net_ptr(pointer_to_data)\n\n    result = pointer_to_result.get()\n    assert isinstance(result, th.Tensor)\n    assert result == th.tensor([1000.0])\n\n    net_ptr = net.send(device_2)\n\n    pointer_to_data = hook.local_worker.request_search(""input_data"", location=device_2)[0]\n    pointer_to_result = net_ptr(pointer_to_data)\n\n    result = pointer_to_result.get()\n    assert isinstance(result, th.Tensor)\n    assert result == th.tensor([1000.0])\n\n\ndef test_multiple_workers(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def plan_abs(data):\n        return data.abs()\n\n    plan_ptr = plan_abs.send(bob, alice)\n    x_ptr = th.tensor([-1, 7, 3]).send(bob)\n    p = plan_ptr(x_ptr)\n    x_abs = p.get()\n    assert (x_abs == th.tensor([1, 7, 3])).all()\n\n    x_ptr = th.tensor([-1, 9, 3]).send(alice)\n    p = plan_ptr(x_ptr)\n    x_abs = p.get()\n    assert (x_abs == th.tensor([1, 9, 3])).all()\n\n\ndef test_fetch_plan(hook, workers):\n    alice = workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def plan(data):\n        return data * 3\n\n    plan.send(alice)\n\n    # Fetch plan\n    fetched_plan = plan.owner.fetch_plan(plan.id, alice)\n\n    # Execute it locally\n    x = th.tensor([-1.0, 2, 3])\n    assert (plan(x) == th.tensor([-3.0, 6, 9])).all()\n    assert (fetched_plan(x) == th.tensor([-3.0, 6, 9])).all()\n    assert fetched_plan.forward is None\n    assert fetched_plan.is_built\n\n\n@pytest.mark.parametrize(""is_func2plan"", [True, False])\ndef test_fetch_plan_multiple_times(hook, is_func2plan, workers):\n\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n\n    if is_func2plan:\n\n        @sy.func2plan(args_shape=[(1,)], state=(th.tensor([3.0]),))\n        def plan(data, state):\n            (bias,) = state.read()\n            return data * bias\n\n    else:\n\n        class Net(sy.Plan):\n            def __init__(self):\n                super(Net, self).__init__()\n                self.fc1 = nn.Linear(1, 1)\n\n            def forward(self, x):\n                return self.fc1(x)\n\n        plan = Net()\n        plan.build(th.tensor([1.2]))\n\n    plan_pointer = plan.send(james)\n\n    # Fetch plan\n    fetched_plan = plan_pointer.owner.fetch_plan(plan_pointer.id_at_location, james, copy=True)\n\n    # Execute the fetch plan\n    x = th.tensor([-1.0])\n    result1 = fetched_plan(x)\n\n    # 2. Re-fetch Plan\n    fetched_plan = plan_pointer.owner.fetch_plan(plan_pointer.id_at_location, james, copy=True)\n\n    # Execute the fetch plan\n    x = th.tensor([-1.0])\n    result2 = fetched_plan(x)\n\n    assert th.all(result1 - result2 < 1e-2)\n\n\ndef test_fetch_plan_remote(hook, start_remote_worker):\n\n    server, remote_proxy = start_remote_worker(id=""test_fetch_plan_remote"", hook=hook, port=8803)\n\n    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))\n    def plan_mult_3(data, state):\n        (bias,) = state.read()\n        return data * 3 + bias\n\n    plan_mult_3.send(remote_proxy)\n\n    # Fetch plan\n    fetched_plan = plan_mult_3.owner.fetch_plan(plan_mult_3.id, remote_proxy)\n\n    # Execute it locally\n    x = th.tensor([-1.0, 2, 3])\n    assert (plan_mult_3(x) == th.tensor([-2.0, 7, 10])).all()\n    assert (fetched_plan(x) == th.tensor([-2.0, 7, 10])).all()\n    assert fetched_plan.forward is None\n    assert fetched_plan.is_built\n\n    remote_proxy.close()\n    server.terminate()\n\n\ndef test_plan_serde(hook):\n    @sy.func2plan(args_shape=[(1, 3)])\n    def my_plan(data):\n        x = data * 2\n        y = (x - 2) * 10\n        return x + y\n\n    serialized_plan = serialize(my_plan)\n    deserialized_plan = deserialize(serialized_plan)\n\n    x = th.tensor([-1, 2, 3])\n    assert (deserialized_plan(x) == th.tensor([-42, 24, 46])).all()\n\n\ndef test_execute_plan_remotely(hook, start_remote_worker):\n    """"""Test plan execution remotely.""""""\n\n    @sy.func2plan(args_shape=[(1,)])\n    def my_plan(data):\n        x = data * 2\n        y = (x - 2) * 10\n        return x + y\n\n    x = th.tensor([-1, 2, 3])\n    local_res = my_plan(x)\n\n    server, remote_proxy = start_remote_worker(id=""test_plan_worker"", hook=hook, port=8799)\n\n    plan_ptr = my_plan.send(remote_proxy)\n    x_ptr = x.send(remote_proxy)\n    ptr = plan_ptr(x_ptr)\n    assert isinstance(ptr, FrameworkTensor) and ptr.is_wrapper\n    plan_res = ptr.get()\n\n    assert (plan_res == local_res).all()\n\n    # delete remote object before websocket connection termination\n    del x_ptr\n\n    remote_proxy.close()\n    server.terminate()\n\n\ndef test_execute_plan_module_remotely(hook, start_remote_worker):\n    """"""Test plan execution remotely.""""""\n\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(2, 3)\n            self.fc2 = nn.Linear(3, 2)\n\n            self.bias = th.tensor([1000.0])\n\n        def forward(self, x):\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return F.log_softmax(x, dim=0) + self.bias\n\n    net = Net()\n\n    x = th.tensor([-1, 2.0])\n    local_res = net(x)\n    assert not net.is_built\n\n    net.build(x)\n\n    server, remote_proxy = start_remote_worker(id=""test_plan_worker_2"", port=8799, hook=hook)\n\n    plan_ptr = net.send(remote_proxy)\n    x_ptr = x.send(remote_proxy)\n    ptr = plan_ptr(x_ptr)\n    assert isinstance(ptr, FrameworkTensor) and ptr.is_wrapper\n    remote_res = ptr.get()\n\n    assert (remote_res == local_res).all()\n\n    # delete remote object before websocket connection termination\n    del x_ptr\n\n    remote_proxy.close()\n    server.terminate()\n\n\ndef test_train_plan_locally_and_then_send_it(hook, start_remote_worker):\n    """"""Test training a plan locally and then executing it remotely.""""""\n\n    # Create toy model\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(2, 3)\n            self.fc2 = nn.Linear(3, 2)\n\n        def forward(self, x):\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return F.log_softmax(x, dim=0)\n\n    net = Net()\n\n    # Create toy data\n    x = th.tensor([-1, 2.0])\n    y = th.tensor([1.0])\n\n    # Train Model\n    opt = optim.SGD(params=net.parameters(), lr=0.01)\n    previous_loss = None\n\n    for _ in range(5):\n        # 1) erase previous gradients (if they exist)\n        opt.zero_grad()\n\n        # 2) make a prediction\n        pred = net(x)\n\n        # 3) calculate how much we missed\n        loss = ((pred - y) ** 2).sum()\n\n        # 4) figure out which weights caused us to miss\n        loss.backward()\n\n        # 5) change those weights\n        opt.step()\n\n        if previous_loss is not None:\n            assert loss < previous_loss\n\n        previous_loss = loss\n\n    local_res = net(x)\n    net.build(x)\n\n    server, remote_proxy = start_remote_worker(id=""test_plan_worker_3"", port=8800, hook=hook)\n\n    plan_ptr = net.send(remote_proxy)\n    x_ptr = x.send(remote_proxy)\n    remote_res = plan_ptr(x_ptr).get()\n\n    assert (remote_res == local_res).all()\n\n    # delete remote object before websocket connection termination\n    del x_ptr\n\n    remote_proxy.close()\n    server.terminate()\n\n\ndef test_cached_plan_send(workers):\n    bob = workers[""bob""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def plan_abs(data):\n        return data.abs()\n\n    plan_bob_ptr1 = plan_abs.send(bob)\n    plan_bob_ptr2 = plan_abs.send(bob)\n    pointers = plan_abs.get_pointers()\n\n    assert len(pointers) == 1\n    assert plan_bob_ptr1 is plan_bob_ptr2\n\n\ndef test_cached_multiple_location_plan_send(workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)])\n    def plan_abs(data):\n        return data.abs()\n\n    plan_group_ptr1 = plan_abs.send(bob, alice)\n    plan_group_ptr2 = plan_abs.send(bob, alice)\n\n    pointers = plan_abs.get_pointers()\n\n    assert len(pointers) == 2\n\n\ndef test_plan_input_serialization(hook):\n    @sy.func2plan()\n    def plan_serialized_input_1(x, y, z, t):\n        return x\n\n    plan_serialized_input_1.build(\n        (th.tensor([1.0, -2.0]), th.tensor([1, 2])),\n        {\n            ""k1"": [5, (True, False)],\n            ""k2"": {\n                ""kk1"": [th.tensor([5, 7]), th.tensor([5, 7])],\n                ""kk2"": [True, (th.tensor([9, 10]),)],\n            },\n            ""k3"": th.tensor([8]),\n        },\n        th.tensor([11, 12]),\n        (1, (2, (3, (4, [5, 6])))),\n    )\n\n    reference_serialized_input_1 = (\n        (type(th.tensor([1.0, -2.0])), type(th.tensor([1, 2]))),\n        {\n            ""k1"": [type(5), (type(True), type(False))],\n            ""k2"": {\n                ""kk1"": [type(th.tensor([5, 7])), type(th.tensor([5, 7]))],\n                ""kk2"": [type(True), (type(th.tensor([9, 10])),)],\n            },\n            ""k3"": type(th.tensor([8])),\n        },\n        type(th.tensor([11, 12])),\n        (type(1), (type(2), (type(3), (type(4), [type(5), type(6)])))),\n    )\n\n    serialized_input_1 = plan_serialized_input_1.input_types.nested_input_types\n\n    assert reference_serialized_input_1 == serialized_input_1\n\n\ndef test_plan_input_usage(hook):\n    x11 = th.tensor([-1, 2.0])\n    x12 = th.tensor([1, -2.0])\n\n    device_1 = sy.VirtualWorker(hook, id=""test_dev_1"", data=(x11, x12))\n\n    @sy.func2plan()\n    def plan_test_1(x, y):\n        return x\n\n    @sy.func2plan()\n    def plan_test_2(x, y):\n        return y\n\n    pointer_to_data_1 = x11.send(device_1)\n    pointer_to_data_2 = x12.send(device_1)\n\n    plan_test_1.build(th.tensor([1.0, -2.0]), th.tensor([1, 2]))\n    pointer_plan = plan_test_1.send(device_1)\n    pointer_to_result = pointer_plan(pointer_to_data_1, pointer_to_data_2)\n    result = pointer_to_result.get()\n    assert (result == x11).all()\n\n    pointer_to_data_1 = x11.send(device_1)\n    pointer_to_data_2 = x12.send(device_1)\n\n    plan_test_2.build(th.tensor([1.0, -2.0]), th.tensor([1, 2]))\n    pointer_plan = plan_test_2.send(device_1)\n    pointer_to_result = pointer_plan(pointer_to_data_1, pointer_to_data_2)\n    result = pointer_to_result.get()\n    assert (result == x12).all()\n\n\ndef test_plan_wrong_number_of_parameters(hook):\n    x11 = th.tensor([-1, 2.0])\n    x12 = th.tensor([1, -2.0])\n\n    device_1 = sy.VirtualWorker(hook, id=""test_dev_1"", data=(x11, x12))\n\n    @sy.func2plan()\n    def plan_test(x, y, z, t):\n        return x\n\n    pointer_to_data_1 = x11.send(device_1)\n    pointer_to_data_2 = x12.send(device_1)\n\n    dummy_input_list = [th.tensor([1, -2]), th.tensor([1, 2]), 2, False]\n    input_list = [pointer_to_data_1, pointer_to_data_2, 5, True]\n    corect_number_of_params = 4\n    plan_test.build(*dummy_input_list)\n    pointer_plan = plan_test.send(device_1)\n\n    for i in range(len(input_list) + 1):\n        if i == corect_number_of_params:\n            pointer_plan(*input_list[:i])\n        else:\n            with pytest.raises(TypeError) as e:\n                pointer_plan(*input_list[:i])\n\n            assert f""Plan plan_test requires {len(input_list)} arguments, received {i}."" == str(\n                e.value\n            )\n\n\ndef test_plan_list(hook):\n    x11 = th.tensor([-1, 2.0])\n    x12 = th.tensor([1, -2.0])\n\n    @sy.func2plan()\n    def plan_list(data, x):\n        y = data[0] + data[1]\n        z = data[0] + x\n        return y + z\n\n    device_1 = sy.VirtualWorker(hook, id=""test_plan_list"", data=(x11, x12))\n\n    plan_list.build([th.tensor([1, 2]), th.tensor([2, 3])], th.tensor([0, 0]))\n    pointer_to_plan = plan_list.send(device_1)\n    pointer_to_data_1 = x11.send(device_1)\n    pointer_to_data_2 = x12.send(device_1)\n    result = pointer_to_plan([pointer_to_data_1, pointer_to_data_2], th.tensor([1, 1]))\n    assert (result.get() == th.tensor([0, 3])).all()\n\n\ndef test_plan_tuple(hook):\n    x11 = th.tensor([-1, 2.0])\n    x12 = th.tensor([1, -2.0])\n\n    @sy.func2plan()\n    def plan_tuple(data, x):\n        y = data[0] + data[1]\n        z = data[0] + x\n        return y + z\n\n    device_1 = sy.VirtualWorker(hook, id=""test_plan_tuple"", data=(x11, x12))\n\n    plan_tuple.build((th.tensor([1, 2]), th.tensor([2, 3])), th.tensor([0, 0]))\n    pointer_to_plan = plan_tuple.send(device_1)\n\n    pointer_to_data_1 = x11.send(device_1)\n    pointer_to_data_2 = x12.send(device_1)\n\n    result = pointer_to_plan((pointer_to_data_1, pointer_to_data_2), th.tensor([1, 1]))\n    assert (result.get() == th.tensor([0, 3])).all()\n\n\ndef test_plan_dict(hook):\n    x11 = th.tensor([-1, 2.0])\n    x12 = th.tensor([1, -2.0])\n\n    @sy.func2plan()\n    def plan_dict(data, x):\n        y = data[""input1""] + data[""input2""]\n        z = data[""input1""] + x\n        return y + z\n\n    device_1 = sy.VirtualWorker(hook, id=""test_plan_dict"", data=(x11, x12))\n    plan_dict.build({""input1"": th.tensor([1, 2]), ""input2"": th.tensor([2, 3])}, th.tensor([0, 0]))\n    pointer_to_plan = plan_dict.send(device_1)\n    pointer_to_data_1 = x11.send(device_1)\n    pointer_to_data_2 = x12.send(device_1)\n    result = pointer_to_plan(\n        {""input1"": pointer_to_data_1, ""input2"": pointer_to_data_2}, th.tensor([1, 1])\n    )\n    assert (result.get() == th.tensor([0, 3])).all()\n\n\ndef test_plan_nested_structures(hook):\n    x1 = th.tensor([-1, 2.0])\n    x2 = th.tensor([1, -2.0])\n    x3 = th.tensor([2, -1])\n    x4 = th.tensor([2, 1])\n\n    @sy.func2plan()\n    def plan_nested(dict):\n        """"""\n        dict:\n            ""tensors"":\n                ""test1"": [tensor(-1, 2), tensor(1, -2), [tensor(-1, 2), tensor(2, 1)]]\n                ""test2"": tensor(2, 1)\n            ""tensor_list"": [tensor(-1, 2), tensor(2, -1)]\n        """"""\n        x = dict[""tensors""][""test1""][0]\n        y = dict[""tensors""][""test1""][1]\n        z = dict[""tensors""][""test2""]\n        t = dict[""tensor_list""][0]\n        return x + y + z + t\n\n    dummy_build = {\n        ""tensors"": {\n            ""test1"": [th.tensor([0, 0]), th.tensor([0, 0]), [th.tensor([0, 0]), th.tensor([0, 0])]],\n            ""test2"": th.tensor([0, 0]),\n        },\n        ""tensor_list"": [th.tensor([-1, 2]), th.tensor([1, -2])],\n    }\n\n    device_1 = sy.VirtualWorker(hook, id=""test_nested_structure"", data=(x1, x2, x3, x4))\n    plan_nested.build(dummy_build)\n\n    pointer_to_data_1 = x1.send(device_1)\n    pointer_to_data_2 = x2.send(device_1)\n    pointer_to_data_3 = x3.send(device_1)\n    pointer_to_data_4 = x4.send(device_1)\n\n    call_build = {\n        ""tensors"": {\n            ""test1"": [pointer_to_data_1, pointer_to_data_2, [pointer_to_data_1, pointer_to_data_4]],\n            ""test2"": pointer_to_data_4,\n        },\n        ""tensor_list"": [pointer_to_data_1, pointer_to_data_3],\n    }\n    pointer_to_plan = plan_nested.send(device_1)\n    result = pointer_to_plan(call_build)\n    assert (result.get() == th.tensor([1, 3])).all()\n\n\ndef test_plan_type_error(hook):\n    x1 = th.tensor([-1, 2.0])\n    x2 = th.tensor([1, -2.0])\n\n    @sy.func2plan()\n    def plan_type_err(dic):\n        return dic[""k1""][""kk2""]\n\n    dummy_build = {\n        ""k1"": {\n            ""kk1"": [(th.tensor([0, 0]), 1), [th.tensor([0, 0]), 2.5]],\n            ""kk2"": th.tensor([0, 0]),\n        },\n        ""k2"": [th.tensor([-1, 2]), ""dummy_str""],\n    }\n\n    device_1 = sy.VirtualWorker(hook, id=""test_nested_structure"", data=(x1, x2))\n    plan_type_err.build(dummy_build)\n\n    pointer_to_data_1 = x1.send(device_1)\n    pointer_to_data_2 = x2.send(device_1)\n\n    call_build = {\n        ""k1"": {""kk1"": [(pointer_to_data_1, 1.5), [pointer_to_data_1, True]], ""kk2"": ""warn""},\n        ""k2"": [pointer_to_data_1, pointer_to_data_2],\n    }\n\n    pointer_to_plan = plan_type_err.send(device_1)\n    with pytest.raises(TypeError) as e:\n        _ = pointer_to_plan(call_build)\n\n    assert str(e.value) == (\n        ""Plan plan_type_err element 1 of element 0 of key kk1 of key k1 of element 0 of ""\n        ""input has type int, while being built with type float.""\n    )\n\n\ndef test_plan_missmatch_Err(hook):\n    x1 = th.tensor([-1, 2.0])\n    x2 = th.tensor([1, -2.0])\n\n    @sy.func2plan()\n    def plan_missmatch_err(lst):\n        return lst[0]\n\n    dummy_build = [[th.tensor([0, 0]), th.tensor([0, 0])], [th.tensor([0, 0]), th.tensor([0, 0])]]\n\n    device_1 = sy.VirtualWorker(hook, id=""test_nested_structure"")\n    plan_missmatch_err.build(dummy_build)\n\n    pointer_to_data_1 = x1.send(device_1)\n    pointer_to_data_2 = x2.send(device_1)\n\n    call_build = [\n        [pointer_to_data_1, pointer_to_data_1, pointer_to_data_1],\n        [pointer_to_data_2, pointer_to_data_2, pointer_to_data_2],\n    ]\n\n    pointer_to_plan = plan_missmatch_err.send(device_1)\n    with pytest.raises(TypeError) as e:\n        _ = pointer_to_plan(call_build)\n\n    assert str(e.value) == (\n        ""Plan plan_missmatch_err element 0 of element 0 of input has length 3, while being ""\n        ""build with length 2.""\n    )\n\n\ndef test_wrong_type_err(hook):\n    @sy.func2plan()\n    def plan_wrong_type_err(x):\n        return x\n\n    device_1 = sy.VirtualWorker(hook, id=""test_nested_structure"")\n\n    call_build = [True, 5]\n    dummy_build = ((th.tensor([1, 2, 3]), True),)\n\n    plan_wrong_type_err.build(dummy_build)\n\n    pointer_to_plan = plan_wrong_type_err.send(device_1)\n\n    with pytest.raises(TypeError) as e:\n        pointer_to_plan(call_build)\n\n    assert str(e.value) == (\n        ""Plan plan_wrong_type_err element 0 of input has type tuple, while being ""\n        ""built with type list.""\n    )\n\n\ndef test_wrong_size_dict(hook):\n    @sy.func2plan()\n    def plan_wrong_size_dict(x):\n        return x\n\n    device_1 = sy.VirtualWorker(hook, id=""test_nested_structure"")\n\n    dummy_build = {""k1"": True, ""k2"": False}\n\n    call_build = {""k1"": True, ""k2"": False, ""k3"": 1}\n\n    plan_wrong_size_dict.build(dummy_build)\n\n    pointer_to_plan = plan_wrong_size_dict.send(device_1)\n\n    with pytest.raises(TypeError) as e:\n        pointer_to_plan(call_build)\n\n    assert str(e.value) == (\n        ""Plan plan_wrong_size_dict element 0 of input has length 3, while being ""\n        ""build with length 2.""\n    )\n\n\ndef test_plan_key_error(hook):\n    x1 = th.tensor([-1, 2.0])\n    x2 = th.tensor([1, -2.0])\n\n    @sy.func2plan()\n    def plan_type_warn(dic):\n        return dic[""k1""][""kk2""]\n\n    dummy_build = {\n        ""k1"": {\n            ""kk1"": [(th.tensor([0, 0]), 1), [th.tensor([0, 0]), 2.5]],\n            ""kk2"": th.tensor([0, 0]),\n        },\n        ""k2"": [th.tensor([-1, 2]), ""dummy_str""],\n    }\n\n    device_1 = sy.VirtualWorker(hook, id=""test_nested_structure"", data=(x1, x2))\n    plan_type_warn.build(dummy_build)\n\n    pointer_to_data_1 = x1.send(device_1)\n    pointer_to_data_2 = x2.send(device_1)\n\n    call_build = {\n        ""k1"": {""kk1_wrong"": [(pointer_to_data_1, 1.5), [pointer_to_data_1, True]], ""kk2"": ""warn""},\n        ""k2"": [pointer_to_data_1, pointer_to_data_2],\n    }\n\n    pointer_to_plan = plan_type_warn.send(device_1)\n    with pytest.raises(KeyError) as e:\n        _ = pointer_to_plan(call_build)\n\n    assert str(e.value) == (\n        ""\'Plan plan_type_warn key k1 of element 0 of input does not provide the key kk1, ""\n        ""while being build with that key.\'""\n    )\n\n\ndef test_backward_autograd_can_be_traced(hook, workers):\n    @sy.func2plan(args_shape=[(5, 5)], trace_autograd=True)\n    def autograd_test(X):\n        y = X * 5\n        y = -y.log() / 2\n        y = y.sum()\n        y.backward()\n        return X.grad\n\n    X = th.ones(5, 5, requires_grad=True)\n\n    # Result of torch autograd\n    torch_grads = autograd_test(X)\n\n    # Result of traced backprop\n    autograd_test.forward = None\n    plan_grads = autograd_test(X)\n\n    autograd_str = (\n        ""def autograd_test(arg_1):\\n    var_0 = arg_1.mul(5)\\n    var_1 = var_0.log()\\n    ""\n        ""var_2 = var_1.neg()\\n    var_3 = var_2.div(2)\\n    var_4 = var_3.sum()\\n    ""\n        ""var_5 = var_4.mul(0)\\n    var_6 = var_5.add(1)\\n    var_7 = var_3.mul(0)\\n    ""\n        ""var_8 = var_7.add(1)\\n    var_9 = var_8.mul(var_6)\\n    var_10 = var_9.div(2)\\n    ""\n        ""var_11 = var_10.mul(-1)\\n    var_12 = var_0.__rtruediv__(1)\\n    ""\n        ""var_13 = var_11.mul(var_12)\\n    var_14 = var_13.mul(5)\\n    var_15 = var_13.mul(arg_1)\\n    ""  # noqa:\n        ""out_1 = var_14.copy()\\n    return out_1""\n    )\n    assert autograd_test.code == autograd_str\n    assert torch_grads.eq(plan_grads).all()\n'"
test/execution/test_protocol.py,24,"b'import torch as th\n\nimport syft as sy\nfrom syft.execution.role_assignments import RoleAssignments\nfrom syft.execution.role import Role\n\n\ndef test_func2protocol_creates_roles():\n    @sy.func2protocol(roles=[""alice"", ""bob""])\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        return tensor\n\n    assert protocol.is_built\n    assert len(protocol.roles) == 2\n    assert isinstance(protocol.roles[""alice""], Role)\n    assert isinstance(protocol.roles[""bob""], Role)\n\n\ndef test_framework_methods_traced_by_role():\n    @sy.func2protocol(roles=[""alice"", ""bob""])\n    def protocol(alice, bob):\n        tensor1 = alice.torch.rand([4, 4])\n        tensor2 = bob.torch.rand([4, 4])\n\n        return tensor1, tensor2\n\n    assert protocol.is_built\n\n    for role in protocol.roles.values():\n        assert len(role.actions) == 1\n        assert ""torch.rand"" in (action.name for action in role.actions)\n\n\ndef test_trace_communication_actions_send():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        tensor.send(bob.worker)\n        return tensor\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 2\n    assert ""send"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_get():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker)\n        res = ptr.get()\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 3\n    assert ""get"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_ptr_send():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker)\n        res = ptr.send(alice.worker)\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 3\n    assert ""send"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_move():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker)\n        res = ptr.move(alice.worker)\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 3\n    assert ""move"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_share():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker)\n        ptr = ptr.fix_prec()\n        res = ptr.share(alice.worker, bob.worker)\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 4\n    assert ""share"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_share_():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker)\n        ptr = ptr.fix_prec()\n        res = ptr.share_(alice.worker, bob.worker)\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 4\n    assert ""share_"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_remote_send():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker)\n        res = ptr.remote_send(alice.worker)\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 3\n    assert ""remote_send"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_mid_get():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker)\n        res = ptr.mid_get()\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 3\n    assert ""mid_get"" in (action.name for action in traced_actions)\n\n\ndef test_trace_communication_actions_remote_get():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),)})\n    def protocol(alice, bob):\n        tensor = alice.torch.tensor([1])\n\n        ptr = tensor.send(bob.worker).send(alice.worker)\n        res = ptr.remote_get()\n        return res\n\n    traced_actions = protocol.roles[""alice""].actions\n\n    assert protocol.is_built\n    assert len(traced_actions) == 4\n    assert ""remote_get"" in (action.name for action in traced_actions)\n\n\ndef test_create_roles_from_decorator():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),), ""bob"": ((1,),)})\n    def protocol(alice, bob):\n        tensor1 = alice.torch.tensor([1])\n        tensor2 = bob.torch.tensor([2])\n\n        t1plus = tensor1 + 1\n        t2plus = tensor2 + 1\n\n        return t1plus, t2plus\n\n    assert len(protocol.roles) == 2\n    assert ""alice"" in protocol.roles\n    assert ""bob"" in protocol.roles\n\n\ndef test_multi_role_tracing():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),), ""bob"": ((1,),)})\n    def protocol(alice, bob):\n        tensor1 = alice.torch.tensor([1])\n        tensor2 = bob.torch.tensor([2])\n\n        t1plus = tensor1 + 1\n        t2plus = tensor2 + 1\n\n        return t1plus, t2plus\n\n    protocol.build()\n\n    assert protocol.is_built\n\n    assert len(protocol.roles) == 2\n    assert len(protocol.roles[""alice""].actions) == 2\n    assert len(protocol.roles[""bob""].actions) == 2\n\n\ndef test_multi_role_execution():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,), (1,)), ""bob"": ((1,),)})\n    def protocol(alice, bob):\n        tensor1 = alice.torch.tensor([1])\n        tensor2 = bob.torch.tensor([2])\n        tensor3 = alice.torch.tensor([3])\n\n        res1 = tensor2\n        res2 = tensor1 + tensor3\n        res3 = tensor2 * 3\n\n        return res1, res2, res3\n\n    protocol.build()\n    protocol.forward = None\n\n    dict_res = protocol()\n\n    assert (dict_res[""bob""][0] == th.tensor([2])).all()\n    assert (dict_res[""bob""][1] == th.tensor([6])).all()\n    assert (dict_res[""alice""][0] == th.tensor([4])).all()\n\n\ndef test_stateful_protocol(workers):\n    shapes = {""alice"": ((1,),), ""bob"": ((1,),)}\n    states = {""alice"": (th.tensor([1]), th.tensor([3])), ""bob"": (th.tensor([5]),)}\n\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape=shapes, states=states)\n    def protocol(alice, bob):\n        # fetch tensors from states\n        tensor_a1, tensor_a2 = alice.load_state()\n        (tensor_b1,) = bob.load_state()\n\n        t1plus = tensor_a1 + tensor_a2\n        t2plus = tensor_b1 + 1\n\n        return t1plus, t2plus\n\n    assert all(protocol.roles[""alice""].state.state_placeholders[0].child == th.tensor([1]))\n    assert all(protocol.roles[""alice""].state.state_placeholders[1].child == th.tensor([3]))\n    assert all(protocol.roles[""bob""].state.state_placeholders[0].child == th.tensor([5]))\n\n\ndef test_copy():\n    @sy.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),), ""bob"": ((1,),)})\n    def protocol(alice, bob):\n        tensor1 = alice.torch.tensor([1])\n        tensor2 = bob.torch.tensor([2])\n\n        t1plus = tensor1 + 1\n        t2plus = tensor2 + 1\n\n        return t1plus, t2plus\n\n    protocol.build()\n    copy = protocol.copy()\n\n    assert copy.name == protocol.name\n    assert copy.roles.keys() == protocol.roles.keys()\n    assert [\n        len(copy_role.actions) == len(role.actions)\n        for copy_role, role in zip(copy.roles.values(), protocol.roles.values())\n    ]\n    assert copy.is_built == protocol.is_built\n\n\ndef test_role_assignments(workers):\n    @sy.func2protocol(roles=[""role1"", ""role2""], args_shape={""role1"": ((1,),), ""role2"": ((1,),)})\n    def protocol(role1, role2):\n        tensor1 = role1.torch.tensor([1])\n        tensor2 = role2.torch.tensor([2])\n\n        t1plus = tensor1 + 1\n        t2plus = tensor2 + 1\n\n        return t1plus, t2plus\n\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n\n    protocol.assign(""role1"", alice)\n    protocol.assign(""role2"", bob)\n\n    assert protocol.role_assignments.assignments[""role1""] == [alice]\n    assert protocol.role_assignments.assignments[""role2""] == [bob]\n\n    protocol.role_assignments = RoleAssignments([""role1"", ""role2""])\n    protocol.assign_roles({""role1"": bob, ""role2"": alice})\n\n    assert protocol.role_assignments.assignments[""role1""] == [bob]\n    assert protocol.role_assignments.assignments[""role2""] == [alice]\n'"
test/execution/test_role.py,3,"b'import torch\n\nfrom syft.execution.role import Role\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.execution.computation import ComputationAction\nfrom syft.execution.communication import CommunicationAction\n\n\ndef test_register_computation_action():\n    role = Role()\n    placeholder = PlaceHolder()\n    target = torch.ones([1])\n\n    action = (""__add__"", target, (), {})\n\n    role.register_action((action, placeholder), ComputationAction)\n\n    assert len(role.actions) == 1\n\n    registered = role.actions[0]\n\n    assert isinstance(registered, ComputationAction)\n    assert registered.name == ""__add__""\n    assert registered.target == target\n    assert registered.args == ()\n    assert registered.kwargs == {}\n    assert registered.return_ids == (placeholder.id,)\n\n\ndef test_register_communication_action():\n    role = Role()\n    placeholder = PlaceHolder()\n    target = torch.ones([1])\n\n    action = (""get"", target, (), {})\n\n    role.register_action((action, placeholder), CommunicationAction)\n\n    assert len(role.actions) == 1\n\n    registered = role.actions[0]\n\n    assert isinstance(registered, CommunicationAction)\n    assert registered.name == ""get""\n    assert registered.target == target\n    assert registered.args == ()\n    assert registered.kwargs == {}\n\n    assert registered.return_ids == (placeholder.id,)\n\n\ndef test_reset():\n    role = Role()\n    placeholder = PlaceHolder()\n    target = torch.ones([1])\n\n    action = (""get"", target, (), {})\n\n    role.register_action((action, placeholder), CommunicationAction)\n    role.placeholders = {""ph_id1"": PlaceHolder(), ""ph_id2"": PlaceHolder()}\n    role.input_placeholder_ids = (""input1"", ""input2"")\n    role.output_placeholder_ids = (""output1"",)\n\n    assert len(role.actions) == 1\n    assert len(role.placeholders) == 2\n    assert role.input_placeholder_ids == (""input1"", ""input2"")\n    assert role.output_placeholder_ids == (""output1"",)\n\n    role.reset()\n\n    assert len(role.actions) == 0\n    assert len(role.placeholders) == 0\n    assert role.input_placeholder_ids == ()\n    assert role.output_placeholder_ids == ()\n'"
test/execution/test_role_assignments.py,0,"b'from syft.execution.role_assignments import RoleAssignments\n\n\ndef test_assign(workers):\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n\n    role_assignment = RoleAssignments([""role1"", ""role2""])\n\n    role_assignment.assign(""role1"", alice)\n    role_assignment.assign(""role2"", bob)\n\n    assert role_assignment.assignments[""role1""] == [alice]\n    assert role_assignment.assignments[""role2""] == [bob]\n\n\ndef test_unassign(workers):\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n\n    role_assignment = RoleAssignments([""role1""])\n\n    role_assignment.assign(""role1"", alice)\n    role_assignment.assign(""role1"", bob)\n    role_assignment.unassign(""role1"", alice)\n\n    assert role_assignment.assignments[""role1""] == [bob]\n'"
test/execution/test_state.py,2,"b'import pytest\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport syft as sy\nfrom itertools import starmap\n\n\ndef test_stateful_plan_built_automatically(hook):\n    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))\n    def foo(x, state):\n        (bias,) = state.read()\n        x = x * 2\n        return x + bias\n\n    assert isinstance(foo.__str__(), str)\n    assert len(foo.actions) > 0\n    assert foo.is_built\n\n    t = th.tensor([1.0, 2])\n    x = foo(t)\n\n    assert (x == th.tensor([3.0, 5])).all()\n\n\ndef test_stateful_plan_build(hook):\n    @sy.func2plan(state=(th.tensor([1.0]),))\n    def foo(x, state):\n        (bias,) = state.read()\n        x = x * 2\n        return x + bias\n\n    t = th.tensor([1.0, 2])\n    x = foo(t)\n\n    assert (x == th.tensor([3.0, 5])).all()\n\n\ndef test_add_to_state():\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(2, 3)\n            self.fc2 = th.tensor([1.0])\n\n        def forward(self, x):\n            pass  # pragma: no cover\n\n    model = Net()\n\n    assert len(model.state.state_placeholders) == 3\n\n\ndef test_stateful_plan_method_execute_locally(hook):\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(2, 1)\n            self.bias = th.tensor([1000.0])\n\n        def forward(self, x):\n            x = self.fc1(x)\n            return F.log_softmax(x, dim=0) + self.bias\n\n    model = Net()\n\n    model.build(th.tensor([1.0, 2]))\n\n    # Call one time\n    assert model(th.tensor([1.0, 2])) == th.tensor([1000.0])\n\n    # Call one more time\n    assert model(th.tensor([1.0, 2.1])) == th.tensor([1000.0])\n\n\ndef test_stateful_plan_multiple_send(hook, workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))\n    def plan_abs(x, state):\n        (bias,) = state.read()\n        x = x.abs()\n        return x + bias\n\n    plan_ptr = plan_abs.send(bob)\n    x_ptr = th.tensor([-1.0, 7, 3]).send(bob)\n    p = plan_ptr(x_ptr)\n    res = p.get()\n\n    assert (res == th.tensor([2.0, 8, 4])).all()\n\n    # Test get / send plan\n    plan_ptr = plan_abs.send(alice)\n\n    x_ptr = th.tensor([-1.0, 2, 3]).send(alice)\n    p = plan_ptr(x_ptr)\n    res = p.get()\n    assert (res == th.tensor([2.0, 3, 4])).all()\n\n\ndef test_stateful_plan_multiple_workers(hook, workers):\n    bob, alice = workers[""bob""], workers[""alice""]\n\n    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1]),))\n    def plan_abs(x, state):\n        (bias,) = state.read()\n        x = x.abs()\n        return x + bias\n\n    plan_ptr = plan_abs.send(bob, alice)\n    x_ptr = th.tensor([-1, 7, 3]).send(bob)\n    p = plan_ptr(x_ptr)\n    x_abs = p.get()\n    assert (x_abs == th.tensor([2, 8, 4])).all()\n\n    x_ptr = th.tensor([-1, 9, 3]).send(alice)\n    p = plan_ptr(x_ptr)\n    x_abs = p.get()\n    assert (x_abs == th.tensor([2, 10, 4])).all()\n\n\n@pytest.mark.parametrize(""is_func2plan"", [True, False])\ndef test_fetch_stateful_plan(hook, is_func2plan, workers):\n\n    if is_func2plan:\n\n        @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))\n        def plan(data, state):\n            (bias,) = state.read()\n            return data * bias\n\n    else:\n\n        class Net(sy.Plan):\n            def __init__(self):\n                super(Net, self).__init__()\n                self.fc1 = nn.Linear(1, 1)\n\n            def forward(self, x):\n                return self.fc1(x)\n\n        plan = Net()\n        plan.build(th.tensor([1.2]))\n\n    alice = workers[""alice""]\n    plan_ptr = plan.send(alice)\n\n    # Fetch plan\n    fetched_plan = plan.owner.fetch_plan(plan_ptr.id_at_location, alice)\n\n    # Execute it locally\n    x = th.tensor([-1.26])\n    assert th.all(th.eq(fetched_plan(x), plan(x)))\n    # assert fetched_plan.state.state_placeholders != plan.state.state_placeholders #TODO\n\n    # Make sure fetched_plan is using the actions\n    assert fetched_plan.forward is None\n    assert fetched_plan.is_built\n\n    # Make sure plan is using the blueprint: forward\n    assert plan.forward is not None\n\n\n@pytest.mark.parametrize(""is_func2plan"", [True, False])\ndef test_fetch_stateful_plan_remote(hook, is_func2plan, start_remote_worker):\n\n    server, remote_proxy = start_remote_worker(\n        id=f""test_fetch_stateful_plan_remote_{is_func2plan}"", hook=hook, port=8802\n    )\n\n    if is_func2plan:\n\n        @sy.func2plan(args_shape=[(1,)], state=(th.tensor([3.0]),))\n        def plan(data, state):\n            (bias,) = state.read()\n            return data * bias\n\n    else:\n\n        class Net(sy.Plan):\n            def __init__(self):\n                super(Net, self).__init__()\n                self.fc1 = nn.Linear(1, 1)\n\n            def forward(self, x):\n                return self.fc1(x)\n\n        plan = Net()\n        plan.build(th.tensor([1.2]))\n\n    x = th.tensor([-1.26])\n    expected = plan(x)\n    plan_ptr = plan.send(remote_proxy)\n\n    # Fetch plan\n    fetched_plan = plan.owner.fetch_plan(plan_ptr.id_at_location, remote_proxy)\n\n    # Execute it locally\n    assert th.all(th.eq(fetched_plan(x), expected))\n    # assert fetched_plan.state.state_placeholders != plan.state.state_placeholders #TODO\n\n    # Make sure fetched_plan is using the actions\n    assert fetched_plan.forward is None\n    assert fetched_plan.is_built\n\n    # Make sure plan is using the blueprint: forward\n    assert plan.forward is not None\n\n    remote_proxy.close()\n    server.terminate()\n\n\ndef test_binding_fix_precision_plan(hook):\n    """"""\n    Here we make sure the attributes of a plan are still bound to state\n    elements when calling fix_precision\n    """"""\n\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(1, 1)\n\n        def forward(self, x):\n            return self.fc1(x)\n\n    plan = Net()\n    plan.build(th.tensor([1.2]))\n    original_weight = plan.fc1.weight.clone()\n\n    plan.fix_precision()\n    plan.fc1.weight.float_prec_()\n\n    assert (plan.fc1.weight - original_weight) < 10e-2\n\n\ndef test_binding_encrypted_plan(hook, workers):\n    """"""\n    Here we make sure the attributes of a plan are still bound to state\n    elements when calling fix_prec + share\n    """"""\n\n    alice, bob, charlie = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n    )\n\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(1, 1)\n\n        def forward(self, x):\n            return self.fc1(x)\n\n    plan = Net()\n    plan.build(th.tensor([1.2]))\n    original_weight = plan.fc1.weight.clone()\n\n    plan.fix_precision().share(alice, bob, crypto_provider=charlie)\n    plan.fc1.weight.get_().float_prec_()\n\n    assert (plan.fc1.weight - original_weight) < 10e-2\n\n\n@pytest.mark.parametrize(""is_func2plan"", [True, False])\ndef test_fetch_encrypted_stateful_plan(hook, is_func2plan, workers):\n    # TODO: this test is not working properly with remote workers.\n    # We need to investigate why this might be the case.\n\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n\n    if is_func2plan:\n\n        @sy.func2plan(args_shape=[(1,)], state=(th.tensor([3.0]),))\n        def plan(data, state):\n            (bias,) = state.read()\n            return data * bias\n\n    else:\n\n        class Net(sy.Plan):\n            def __init__(self):\n                super(Net, self).__init__()\n                self.fc1 = nn.Linear(1, 1)\n\n            def forward(self, x):\n                return self.fc1(x)\n\n        plan = Net()\n        plan.build(th.tensor([1.2]))\n\n    x = th.tensor([-1.0])\n    expected = plan(x)\n\n    plan.fix_precision().share(alice, bob, crypto_provider=charlie)\n    ptr_plan = plan.send(james)\n\n    # Fetch plan\n    fetched_plan = plan.owner.fetch_plan(ptr_plan.id_at_location, james)\n\n    # Execute the fetch plan\n    x = th.tensor([-1.0])\n    x_sh = x.fix_precision().share(alice, bob, crypto_provider=charlie)\n    decrypted = fetched_plan(x_sh).get().float_prec()\n\n    # Compare with local plan\n    assert th.all(decrypted - expected.detach() < 1e-2)\n    # assert fetched_plan.state.state_placeholders != plan.state.state_placeholders #TODO\n\n    assert all(\n        starmap(\n            lambda fetched_tensor, tensor: (fetched_tensor == tensor).get(),\n            zip(fetched_plan.state.tensors(), plan.state.tensors()),\n        )\n    )\n\n    # Make sure fetched_plan is using the actions\n    assert fetched_plan.forward is None\n    assert fetched_plan.is_built\n\n    # Make sure plan is using the blueprint: forward\n    assert plan.forward is not None\n'"
test/execution/test_translation.py,2,"b'import torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport syft as sy\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.execution.plan import Plan\nfrom syft.execution.translation.torchscript import PlanTranslatorTorchscript\nfrom syft.serde.serde import deserialize\nfrom syft.serde.serde import serialize\n\n\ndef test_plan_can_be_jit_traced(hook, workers):\n    args_shape = [(1,)]\n\n    @sy.func2plan(args_shape=args_shape, state=(th.tensor([1.0]),))\n    def foo(x, state):\n        (bias,) = state.read()\n        x = x * 2\n        return x + bias\n\n    assert isinstance(foo.__str__(), str)\n    assert len(foo.actions) > 0\n    assert foo.is_built\n\n    t = th.tensor([1.0, 2])\n    x = foo(t)\n\n    assert (x == th.tensor([3.0, 5])).all()\n\n    args = PlaceHolder.create_placeholders(args_shape)\n    torchscript_plan = th.jit.trace(foo, args)\n\n    y = torchscript_plan(t)\n\n    assert (y == th.tensor([3.0, 5])).all()\n\n\ndef test_func_plan_can_be_translated_to_torchscript(hook, workers):\n    # Disable build time auto translation\n    Plan._build_translators = []\n\n    @sy.func2plan(args_shape=[(3, 3)])\n    def plan(x):\n        x = x * 2\n        x = x.abs()\n        return x\n\n    orig_plan = plan.copy()\n\n    inp = th.tensor([1, -1, 2])\n    res1 = plan(inp)\n    plan.add_translation(PlanTranslatorTorchscript)\n    res2 = plan.torchscript(inp)\n    assert (res1 == res2).all()\n\n    # check that translation can be done after serde\n    serde_plan = deserialize(serialize(orig_plan))\n    serde_plan.add_translation(PlanTranslatorTorchscript)\n    res3 = serde_plan.torchscript(inp)\n    assert (res1 == res3).all()\n\n    # check that translation is not lost after serde\n    serde_plan_full = deserialize(serialize(plan))\n    res4 = serde_plan_full.torchscript(inp)\n    assert (res1 == res4).all()\n\n\ndef test_cls_plan_can_be_translated_to_torchscript(hook, workers):\n    # Disable build time auto translation\n    Plan._build_translators = []\n\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(2, 3)\n            self.fc2 = nn.Linear(3, 1)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.fc2(x)\n            return x\n\n    plan = Net()\n    plan.build(th.zeros(10, 2))\n    orig_plan = plan.copy()\n\n    inp = th.randn(10, 2)\n\n    res1 = plan(inp)\n    plan.add_translation(PlanTranslatorTorchscript)\n    res2 = plan.torchscript(inp, plan.parameters())\n    assert (res1 == res2).all()\n\n    # check that translation can be done after serde\n    serde_plan = deserialize(serialize(orig_plan))\n    serde_plan.add_translation(PlanTranslatorTorchscript)\n    res3 = serde_plan.torchscript(inp, serde_plan.parameters())\n    assert (res1 == res3).all()\n\n    # check that translation is not lost after serde\n    serde_plan_full = deserialize(serialize(plan))\n    res4 = serde_plan_full.torchscript(inp, serde_plan_full.parameters())\n    assert (res1 == res4).all()\n\n\ndef test_plan_translation_remove(hook, workers):\n    # Disable build time auto translation\n    Plan._build_translators = []\n\n    @sy.func2plan(args_shape=[(3, 3)])\n    def plan(x):\n        x = x * 2\n        x = x.abs()\n        return x\n\n    plan.add_translation(PlanTranslatorTorchscript)\n\n    full_plan = plan.copy()\n    assert full_plan.torchscript is not None\n\n    assert plan.torchscript is not None\n    assert len(plan.role.actions) > 0\n\n    plan.remove_translation()\n    assert plan.torchscript is not None\n    assert len(plan.role.actions) == 0\n\n    plan.remove_translation(PlanTranslatorTorchscript)\n    assert plan.torchscript is None\n    assert len(plan.role.actions) == 0\n\n    full_plan.remove_translation(PlanTranslatorTorchscript)\n    assert full_plan.torchscript is None\n    assert len(full_plan.role.actions) > 0\n\n\ndef test_plan_translated_on_build(hook, workers):\n    # Enable torchscript translator\n    Plan.register_build_translator(PlanTranslatorTorchscript)\n\n    @sy.func2plan(args_shape=[(3, 3)])\n    def plan(x):\n        x = x * 2\n        x = x.abs()\n        return x\n\n    inp = th.tensor([1, -1, 2])\n    res1 = plan(inp)\n    res2 = plan.torchscript(inp)\n    assert (res1 == res2).all()\n\n\ndef test_backward_autograd_can_be_translated(hook, workers):\n    @sy.func2plan(args_shape=[(5, 5)], trace_autograd=True)\n    def autograd_test(X):\n        y = X * 5\n        y = -y.log() / 2\n        y = y.sum()\n        y.backward()\n        return X.grad\n\n    X = th.ones(5, 5, requires_grad=True)\n\n    # Result of torch autograd\n    torch_grads = autograd_test(X)\n\n    # Translate to torchscript\n    autograd_test.add_translation(PlanTranslatorTorchscript)\n\n    # Result of torchscript\'ed traced backprop\n    ts_plan_grads = autograd_test.torchscript(X)\n\n    # (debug out)\n    print(""Torchscript Plan:\\n"", autograd_test.torchscript.code)\n\n    # Test all results are equal\n    assert torch_grads.eq(ts_plan_grads).all()\n\n\ndef test_fl_mnist_example_training_can_be_translated(hook, workers):\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(784, 392)\n            self.fc2 = nn.Linear(392, 10)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.fc2(x)\n            return x\n\n    model = Net()\n\n    def set_model_params(module, params_list, start_param_idx=0):\n        """""" Set params list into model recursively\n        """"""\n        param_idx = start_param_idx\n\n        for name, param in module._parameters.items():\n            module._parameters[name] = params_list[param_idx]\n            param_idx += 1\n\n        for name, child in module._modules.items():\n            if child is not None:\n                param_idx += set_model_params(child, params_list, param_idx)\n\n        return param_idx\n\n    def softmax_cross_entropy_with_logits(logits, targets, batch_size):\n        """""" Calculates softmax entropy\n            Args:\n                * logits: (NxC) outputs of dense layer\n                * targets: (NxC) one-hot encoded labels\n                * batch_size: value of N, temporarily required because Plan cannot trace .shape\n        """"""\n        # numstable logsoftmax\n        norm_logits = logits - logits.max()\n        log_probs = norm_logits - norm_logits.exp().sum(dim=1, keepdim=True).log()\n        # reduction = mean\n        return -(targets * log_probs).sum() / batch_size\n\n    def naive_sgd(param, **kwargs):\n        return param - kwargs[""lr""] * param.grad\n\n    @sy.func2plan()\n    def train(data, targets, lr, batch_size, model_parameters):\n        # load model params\n        set_model_params(model, model_parameters)\n\n        # forward\n        logits = model(data)\n\n        # loss\n        loss = softmax_cross_entropy_with_logits(logits, targets, batch_size)\n\n        # backward\n        loss.backward()\n\n        # step\n        updated_params = [naive_sgd(param, lr=lr) for param in model_parameters]\n\n        # accuracy\n        pred = th.argmax(logits, dim=1)\n        targets_idx = th.argmax(targets, dim=1)\n        acc = pred.eq(targets_idx).sum().float() / batch_size\n\n        return (loss, acc, *updated_params)\n\n    # Dummy inputs\n    data = th.randn(3, 28 * 28)\n    target = F.one_hot(th.tensor([1, 2, 3]), 10)\n    lr = th.tensor([0.01])\n    batch_size = th.tensor([3.0])\n    model_state = list(model.parameters())\n\n    # Build Plan\n    train.build(data, target, lr, batch_size, model_state, trace_autograd=True)\n\n    # Execute with original forward function (native torch autograd)\n    res_torch = train(data, target, lr, batch_size, model_state)\n\n    # Execute traced operations (traced autograd)\n    train.forward = None\n    res_syft_traced = train(data, target, lr, batch_size, model_state)\n\n    # Translate syft Plan to torchscript and execute it\n    train.add_translation(PlanTranslatorTorchscript)\n    res_torchscript = train.torchscript(data, target, lr, batch_size, model_state)\n\n    # (debug out)\n    print(train.torchscript.code)\n\n    # All variants should be equal\n    for i, out in enumerate(res_torch):\n        assert th.allclose(out, res_syft_traced[i])\n        assert th.allclose(out, res_torchscript[i])\n'"
test/generic/__init__.py,0,b''
test/generic/test_autograd.py,156,"b'import pytest\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport syft\n\nfrom syft.frameworks.torch.tensors.interpreters.autograd import AutogradTensor\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\n\n\ndef test_wrap():\n    """"""\n    Test the .on() wrap functionality for AutogradTensor\n    """"""\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = AutogradTensor().on(x_tensor)\n\n    assert isinstance(x, torch.Tensor)\n    assert isinstance(x.child, AutogradTensor)\n    assert isinstance(x.child.child, torch.Tensor)\n\n\n@pytest.mark.parametrize(""cmd"", [""__add__"", ""__sub__"", ""__mul__"", ""__matmul__""])\n@pytest.mark.parametrize(""backward_one"", [True, False])\ndef test_backward_for_binary_cmd_with_autograd(cmd, backward_one):\n    """"""\n    Test .backward() on local tensors wrapped in an AutogradTensor\n    (It is useless but this is the most basic example)\n    """"""\n    a = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    c = getattr(a, cmd)(b)\n    c_torch = getattr(a_torch, cmd)(b_torch)\n\n    ones = torch.ones(c.shape)\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones if backward_one else None)\n    c_torch.backward(torch.ones(c_torch.shape))\n\n    assert (a.child.grad == a_torch.grad).all()\n    assert (b.child.grad == b_torch.grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""__iadd__"", ""__isub__""])\ndef test_backward_for_inplace_binary_cmd_with_autograd(cmd):\n\n    a = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n    c = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n    c = syft.AutogradTensor().on(c)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n    c_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    r = a * b\n    getattr(r, cmd)(c)\n    r_torch = a_torch * b_torch\n    getattr(r_torch, cmd)(c_torch)\n\n    ones = torch.ones(r.shape)\n    ones = syft.AutogradTensor().on(ones)\n    r.backward(ones)\n    r_torch.backward(torch.ones(r_torch.shape))\n\n    assert (a.child.grad == a_torch.grad).all()\n    assert (b.child.grad == b_torch.grad).all()\n    assert (c.child.grad == c_torch.grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""__add__"", ""__sub__""])\n@pytest.mark.parametrize(\n    ""shapes"",\n    [\n        ((2,), (2,)),\n        ((5, 3, 2), (5, 1, 2)),\n        ((3, 2), (5, 3, 2)),\n        ((3, 1), (5, 3, 2)),\n        ((3, 2), (5, 1, 2)),\n    ],\n)\ndef test_backward_for_binary_cmd_with_inputs_of_different_dim_and_autograd(cmd, shapes):\n    """"""\n    Test .backward() on local tensors wrapped in an AutogradTensor\n    (It is useless but this is the most basic example)\n    """"""\n    a_shape, b_shape = shapes\n    a = torch.ones(a_shape, requires_grad=True)\n    b = torch.ones(b_shape, requires_grad=True)\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n\n    a_torch = torch.ones(a_shape, requires_grad=True)\n    b_torch = torch.ones(b_shape, requires_grad=True)\n\n    c = getattr(a, cmd)(b)\n    c_torch = getattr(a_torch, cmd)(b_torch)\n\n    ones = torch.ones(c.shape)\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones)\n    c_torch.backward(torch.ones(c_torch.shape))\n\n    assert (a.child.grad == a_torch.grad).all()\n    assert (b.child.grad == b_torch.grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""__add__"", ""__sub__"", ""__mul__"", ""__matmul__""])\n@pytest.mark.parametrize(""backward_one"", [True, False])\ndef test_backward_for_remote_binary_cmd_with_autograd(workers, cmd, backward_one):\n    """"""\n    Test .backward() on remote tensors using explicit wrapping\n    with an Autograd Tensor.\n    """"""\n    alice = workers[""alice""]\n\n    a = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True).send(alice)\n    b = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True).send(alice)\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    c = getattr(a, cmd)(b)\n    c_torch = getattr(a_torch, cmd)(b_torch)\n\n    ones = torch.ones(c.shape).send(alice)\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones if backward_one else None)\n    c_torch.backward(torch.ones(c_torch.shape))\n\n    assert (a.grad.get() == a_torch.grad).all()\n    assert (b.grad.get() == b_torch.grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""__iadd__"", ""__isub__""])\ndef test_backward_for_remote_inplace_binary_cmd_with_autograd(workers, cmd):\n    alice = workers[""alice""]\n\n    a = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True).send(alice)\n    b = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True).send(alice)\n    c = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True).send(alice)\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n    c = syft.AutogradTensor().on(c)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n    c_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    r = a * b\n    getattr(r, cmd)(c)\n    r_torch = a_torch * b_torch\n    getattr(r_torch, cmd)(c_torch)\n\n    ones = torch.ones(r.shape).send(alice)\n    ones = syft.AutogradTensor().on(ones)\n    r.backward(ones)\n    r_torch.backward(torch.ones(r_torch.shape))\n\n    assert (a.grad.get() == a_torch.grad).all()\n    assert (b.grad.get() == b_torch.grad).all()\n    assert (c.grad.get() == c_torch.grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""__add__"", ""__sub__"", ""__mul__"", ""__matmul__""])\ndef test_backward_for_remote_binary_cmd_local_autograd(workers, cmd):\n    """"""\n    Test .backward() on remote tensors using implicit wrapping\n    with an Autograd Tensor.\n\n    Distinguish the current use of:\n        a = torch.tensor([[3., 2], [-1, 2]], requires_grad=True)\n        a.send(alice, local_autograd=True)\n\n    instead of the previous:\n        a = torch.tensor([[3., 2], [-1, 2]], requires_grad=True).send(alice)\n        a = syft.AutogradTensor().on(a)\n    """"""\n    alice = workers[""alice""]\n\n    a = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    a = a.send(alice, local_autograd=True)\n    b = b.send(alice, local_autograd=True)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    c = getattr(a, cmd)(b)\n    c_torch = getattr(a_torch, cmd)(b_torch)\n\n    ones = torch.ones(c.shape).send(alice)\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones)\n    c_torch.backward(torch.ones(c_torch.shape))\n\n    assert (a.grad.get() == a_torch.grad).all()\n    assert (b.grad.get() == b_torch.grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""asin"", ""sin"", ""sinh"", ""tanh"", ""sigmoid""])\ndef test_backward_for_remote_unary_cmd_local_autograd(workers, cmd):\n    """"""\n    Test .backward() on unary methods on remote tensors using\n    implicit wrapping\n    """"""\n    alice = workers[""alice""]\n\n    a = torch.tensor([0.3, 0.2, 0], requires_grad=True)\n    a = a.send(alice, local_autograd=True)\n\n    a_torch = torch.tensor([0.3, 0.2, 0], requires_grad=True)\n\n    c = getattr(a, cmd)()\n    c_torch = getattr(a_torch, cmd)()\n\n    ones = torch.ones(c.shape).send(alice)\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones)\n    c_torch.backward(torch.ones_like(c_torch))\n\n    # Have to do .child.grad here because .grad doesn\'t work on Wrappers yet\n    assert (a.grad.get() == a_torch.grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""__add__"", ""__sub__"", ""__mul__"", ""__matmul__""])\n@pytest.mark.parametrize(""backward_one"", [True, False])\ndef test_backward_for_fix_prec_binary_cmd_with_autograd(cmd, backward_one):\n    """"""\n    Test .backward() on Fixed Precision Tensor for a single operation\n    """"""\n    a = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True).fix_prec()\n    b = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True).fix_prec()\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    c = getattr(a, cmd)(b)\n    c_torch = getattr(a_torch, cmd)(b_torch)\n\n    ones = torch.ones(c.shape).fix_prec()\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones if backward_one else None)\n    c_torch.backward(torch.ones(c_torch.shape))\n\n    assert (a.grad.float_prec() == a_torch.grad).all()\n    assert (b.grad.float_prec() == b_torch.grad).all()\n\n\ndef test_backward_for_linear_model_on_fix_prec_params_with_autograd():\n    """"""\n    Test .backward() on Fixed Precision parameters with mixed operations\n    """"""\n    x = torch.tensor([[1.0, 2], [1.0, 2]]).fix_prec()\n    target = torch.tensor([[1.0], [1.0]]).fix_prec()\n    model = nn.Linear(2, 1)\n    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))\n    model.bias = nn.Parameter(torch.tensor([-1.0]))\n    model.fix_precision()\n\n    x = syft.AutogradTensor().on(x)\n    target = syft.AutogradTensor().on(target)\n    model.weight = syft.AutogradTensor().on(model.weight)\n    model.bias = syft.AutogradTensor().on(model.bias)\n\n    output = model(x)\n    loss = ((output - target) ** 2).sum()\n    one = torch.ones(loss.shape).fix_prec()\n    one = syft.AutogradTensor().on(one)\n    loss.backward(one)\n\n    weight_grad = model.weight.grad.float_precision()\n    bias_grad = model.bias.grad.float_precision()\n\n    x = torch.tensor([[1.0, 2], [1.0, 2]])\n    target = torch.tensor([[1.0], [1.0]])\n    model = nn.Linear(2, 1)\n    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))\n    model.bias = nn.Parameter(torch.tensor([-1.0]))\n\n    output = model(x)\n    loss = ((output - target) ** 2).sum()\n\n    one = torch.ones(loss.shape)\n    loss.backward(one)\n    assert (model.weight.grad == weight_grad).all()\n    assert (model.bias.grad == bias_grad).all()\n\n\n@pytest.mark.parametrize(""cmd"", [""__add__"", ""__sub__"", ""__mul__"", ""__matmul__""])\n@pytest.mark.parametrize(""backward_one"", [True, False])\ndef test_backward_for_additive_shared_binary_cmd_with_autograd(workers, cmd, backward_one):\n    """"""\n    Test .backward() on Additive Shared Tensor for a single operation\n    """"""\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n\n    a = (\n        torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n        .fix_prec()\n        .share(alice, bob, crypto_provider=james)\n    )\n    b = (\n        torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n        .fix_prec()\n        .share(alice, bob, crypto_provider=james)\n    )\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n\n    c = getattr(a, cmd)(b)\n    c_torch = getattr(a_torch, cmd)(b_torch)\n\n    ones = torch.ones(c.shape).fix_prec().share(alice, bob, crypto_provider=james)\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones if backward_one else None)\n    c_torch.backward(torch.ones(c_torch.shape))\n\n    assert (a.grad.get().float_prec() == a_torch.grad).all()\n    assert (b.grad.get().float_prec() == b_torch.grad).all()\n\n\n@pytest.mark.parametrize(""backward_one"", [True, False])\ndef test_backward_for_additive_shared_div_with_autograd(workers, backward_one):\n    """"""\n    Test .backward() on Additive Shared Tensor for division with an integer\n    """"""\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n\n    a = (\n        torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n        .fix_prec()\n        .share(alice, bob, crypto_provider=james)\n    )\n    b = 2\n\n    a = syft.AutogradTensor().on(a)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = 2\n\n    c = a / b\n    c_torch = a_torch / b_torch\n\n    ones = torch.ones(c.shape).fix_prec().share(alice, bob, crypto_provider=james)\n    ones = syft.AutogradTensor().on(ones)\n    c.backward(ones if backward_one else None)\n    c_torch.backward(torch.ones(c_torch.shape))\n\n    assert (a.grad.get().float_prec() == a_torch.grad).all()\n\n\ndef test_addmm_backward_for_additive_shared_with_autograd(workers):\n    """"""\n    Test .backward() on Additive Shared Tensor for addmm\n    """"""\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n\n    a = (\n        torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n        .fix_prec()\n        .share(alice, bob, crypto_provider=james)\n    )\n    b = (\n        torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n        .fix_prec()\n        .share(alice, bob, crypto_provider=james)\n    )\n    c = (\n        torch.tensor([[2.0, 2], [0, 1]], requires_grad=True)\n        .fix_prec()\n        .share(alice, bob, crypto_provider=james)\n    )\n\n    a = syft.AutogradTensor().on(a)\n    b = syft.AutogradTensor().on(b)\n    c = syft.AutogradTensor().on(c)\n\n    a_torch = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    b_torch = torch.tensor([[1.0, 2], [3, 2]], requires_grad=True)\n    c_torch = torch.tensor([[2.0, 2], [0, 1]], requires_grad=True)\n\n    r = torch.addmm(c, a, b)\n    r_torch = torch.addmm(c_torch, a_torch, b_torch)\n\n    ones = torch.ones(r.shape).fix_prec().share(alice, bob, crypto_provider=james)\n    ones = syft.AutogradTensor().on(ones)\n    r.backward(ones)\n    r_torch.backward(torch.ones(r_torch.shape))\n\n    assert (a.grad.get().float_prec() == a_torch.grad).all()\n    assert (b.grad.get().float_prec() == b_torch.grad).all()\n    assert (c.grad.get().float_prec() == c_torch.grad).all()\n\n\ndef test_relu_backward_or_additive_shared_with_autograd(workers):\n    """"""\n    Test .backward() on Additive Shared Tensor for F.relu\n    """"""\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n    data = torch.tensor([[-1, -0.1], [1, 0.1]], requires_grad=True)\n    data = data.fix_precision().share(bob, alice, crypto_provider=james, requires_grad=True)\n    loss = F.relu(data)\n    loss.backward()\n    expected = torch.tensor([[0.0, 0], [1, 1]])\n    assert (data.grad.get().float_prec() == expected).all()\n\n\ndef test_backward_for_linear_model_on_additive_shared_with_autograd(workers):\n    """"""\n    Test .backward() on Additive Shared tensors with mixed operations\n    """"""\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n\n    x = torch.tensor([[1.0, 2], [1.0, 2]]).fix_prec().share(bob, alice, crypto_provider=james)\n    target = torch.tensor([[1.0], [1.0]]).fix_prec().share(bob, alice, crypto_provider=james)\n    model = nn.Linear(2, 1)\n    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))\n    model.bias = nn.Parameter(torch.tensor([-1.0]))\n    model.fix_precision().share(bob, alice, crypto_provider=james)\n\n    x = syft.AutogradTensor().on(x)\n    target = syft.AutogradTensor().on(target)\n    model.weight = syft.AutogradTensor().on(model.weight)\n    model.bias = syft.AutogradTensor().on(model.bias)\n\n    output = model(x)\n    loss = ((output - target) ** 2).sum()\n    one = torch.ones(loss.shape).fix_prec().share(bob, alice, crypto_provider=james)\n    one = syft.AutogradTensor().on(one)\n    loss.backward(one)\n\n    weight_grad = model.weight.grad.get().float_precision()\n    bias_grad = model.bias.grad.get().float_precision()\n\n    x = torch.tensor([[1.0, 2], [1.0, 2]])\n    target = torch.tensor([[1.0], [1.0]])\n    model = nn.Linear(2, 1)\n    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))\n    model.bias = nn.Parameter(torch.tensor([-1.0]))\n\n    output = model(x)\n    loss = ((output - target) ** 2).sum()\n\n    one = torch.ones(loss.shape)\n    loss.backward(one)\n    assert (model.weight.grad == weight_grad).all()\n    assert (model.bias.grad == bias_grad).all()\n\n\ndef test_share_with_requires_grad(workers):\n    """"""\n    Test calling fix_precision and share(requires_grad=True) on tensors and model\n    """"""\n    bob, alice, charlie, crypto_provider = (\n        workers[""bob""],\n        workers[""alice""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n\n    t = torch.Tensor([3.0])\n    t = t.fix_precision()\n    t = t.share(alice, bob, crypto_provider=crypto_provider, requires_grad=True)\n\n    assert t.is_wrapper and isinstance(t.child, AutogradTensor)\n\n    t = t.get()\n\n    assert t.is_wrapper and isinstance(t.child, AutogradTensor)\n\n    t = t.float_prec()\n\n    assert t == torch.Tensor([3.0])\n\n\ndef test_remote_share_with_requires_grad(workers):\n    """"""\n    Test calling fix_precision and share(requires_grad=True) on pointers\n    to tensors and model\n    """"""\n    bob, alice, charlie, crypto_provider = (\n        workers[""bob""],\n        workers[""alice""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n\n    t = torch.Tensor([3])\n    t = t.send(charlie)\n    t = t.fix_precision()\n    t = t.share(alice, bob, crypto_provider=crypto_provider, requires_grad=True)\n    t = t.get()\n\n    assert isinstance(t.child, AutogradTensor)\n\n    t = torch.Tensor([3])\n    t = t.fix_precision()\n    t = t.send(charlie)\n    t = t.share(alice, bob, crypto_provider=crypto_provider, requires_grad=True)\n    t = t.get()\n\n    assert isinstance(t.child, AutogradTensor)\n\n    model = nn.Linear(2, 1)\n    model.send(charlie)\n    model.fix_precision()\n    model.share(alice, bob, crypto_provider=crypto_provider, requires_grad=True)\n    model.get()\n\n    assert isinstance(model.weight.child, AutogradTensor)\n\n    # See Issue #2546\n\n    # model = nn.Linear(2, 1)\n    # model.fix_precision()\n    # model.send(charlie)\n    # model.share(alice, bob, crypto_provider=crypto_provider, requires_grad=True)\n    # model.get()\n    #\n    # assert isinstance(model.weight.child, AutogradTensor)\n\n\ndef test_encrypted_training_with_linear_model(workers):\n    """"""\n    Test a minimal example of encrypted training using nn.Linear\n    """"""\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n\n    # A Toy Dataset\n    data = (\n        torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]])\n        .fix_prec()\n        .share(bob, alice, crypto_provider=james)\n    )\n    target = (\n        torch.tensor([[0], [0], [1], [1.0]]).fix_prec().share(bob, alice, crypto_provider=james)\n    )\n\n    # A Toy Model\n    model = nn.Linear(2, 1).fix_precision().share(bob, alice, crypto_provider=james)\n\n    data = syft.AutogradTensor().on(data)\n    target = syft.AutogradTensor().on(target)\n    model.weight = syft.AutogradTensor().on(model.weight)\n    model.bias = syft.AutogradTensor().on(model.bias)\n\n    def train():\n        # Training Logic\n        # Convert the learning rate to fixed precision\n        opt = optim.SGD(params=model.parameters(), lr=0.1).fix_precision()\n\n        for iter in range(10):\n\n            # 1) erase previous gradients (if they exist)\n            opt.zero_grad()\n\n            # 2) make a prediction\n            pred = model(data)\n\n            # 3) calculate how much we missed\n            loss = ((pred - target) ** 2).sum()\n\n            # 4) figure out which weights caused us to miss\n            loss.backward()\n\n            # 5) change those weights\n            opt.step()\n\n        return loss\n\n    loss = train()\n\n    assert loss.child.child.child.virtual_get() < 500\n\n\ndef test_get_float_prec_on_autograd_tensor(workers):\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n\n    x = torch.tensor([0.1, 1.0])\n    x2 = syft.AutogradTensor().on(x.fix_prec())\n    assert (x2.float_precision() == x).all()\n\n    x = torch.tensor([1, 2])\n    x2 = x.share(bob, alice, crypto_provider=james)\n    x2 = syft.AutogradTensor().on(x2)\n    assert (x2.get() == x).all()\n\n    x = torch.tensor([0.1, 1.0])\n    x2 = x.fix_precision()\n    x2 = x2.share(bob, alice, crypto_provider=james, requires_grad=True)\n    assert (x2.get().float_precision() == x).all()\n\n\ndef test_serialize_deserialize_autograd_tensor(workers):\n    # let\'s try to send an autogradTensor to a remote location and get it back\n    alice = workers[""alice""]\n\n    random_tensor = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    assert isinstance(random_tensor, torch.Tensor)\n\n    remote_tensor = random_tensor.send(alice, local_autograd=True)\n    assert isinstance(remote_tensor.child, syft.AutogradTensor)\n\n    local_tensor = remote_tensor.get()\n    assert isinstance(local_tensor, torch.Tensor)\n\n    # check if the tensor sent is equal to the tensor got from the remote version\n    assert torch.all(torch.eq(local_tensor, random_tensor))\n\n\ndef test_types_auto_remote_tensors(workers):\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n\n    random_tensor = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)\n    assert isinstance(random_tensor, torch.Tensor)\n\n    remote_tensor_auto = random_tensor.send(alice, local_autograd=True)\n    assert isinstance(remote_tensor_auto.child, syft.AutogradTensor)\n\n    remote_tensor_remote = remote_tensor_auto.send(bob)\n    assert isinstance(remote_tensor_remote, torch.Tensor)\n\n    assert type(remote_tensor_auto) == type(remote_tensor_remote)\n\n\ndef test_train_remote_autograd_tensor(workers):\n    # Training procedure to train an input model, be it remote or local\n\n    def train(model_input, data_input, target_input, remote=False):\n        opt = optim.SGD(params=model_input.parameters(), lr=0.1)\n        loss_previous = 99999999999  # just a very big number\n        for iter in range(10):\n            # 1) erase previous gradients (if they exist)\n            opt.zero_grad()\n            # 2) make a prediction\n            predictions = model_input(data_input)\n            # 3) calculate how much we missed\n            loss = ((predictions - target_input) ** 2).sum()\n            # check for monotonic decrease of the loss\n\n            if remote:\n                # Remote loss monotonic decrease\n                loss_val_local = loss.copy().get().item()\n                assert loss_val_local < loss_previous\n                loss_previous = loss_val_local\n\n            else:\n                # Local loss monotonic decrease\n                loss_val_local = loss.item()\n                assert loss_val_local < loss_previous\n                loss_previous = loss_val_local\n\n            # 4) Figure out which weights caused us to miss\n            loss.backward()\n            # 5) change those weights\n            opt.step()\n        return (loss, model_input)\n\n    alice = workers[""alice""]\n\n    # Some Toy Data\n    data_local = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]])\n    target_local = torch.tensor([[0], [0], [1], [1.0]])\n\n    # Toy local model\n    model_local = nn.Linear(2, 1)\n\n    # Local training\n    loss_local, model_local_trained = train(model_local, data_local, target_local, remote=False)\n\n    # Remote training, setting autograd for the data and the targets\n    data_remote = data_local.send(alice, local_autograd=True)\n    assert isinstance(data_remote.child, syft.AutogradTensor)\n    assert isinstance(data_remote.child.child, PointerTensor)\n\n    target_remote = target_local.send(alice, local_autograd=True)\n    assert isinstance(target_remote.child, syft.AutogradTensor)\n    assert isinstance(target_remote.child.child, PointerTensor)\n\n    model_remote = model_local.send(alice, local_autograd=True)\n    assert isinstance(model_remote.weight.child, syft.AutogradTensor)\n    assert isinstance(model_remote.weight.child.child, PointerTensor)\n\n    assert type(model_remote) == type(model_local)\n\n    loss_remote, model_remote_trained = train(model_remote, data_remote, target_remote, remote=True)\n\n    # let\'s check if the local version and the remote version have the same weight\n    assert torch.all(\n        torch.eq(\n            model_local_trained.weight.copy().get().data, model_remote.weight.copy().get().data\n        )\n    )\n\n\ndef test_train_without_requires_grad(workers):\n    def train(enc_model, enc_data, enc_target):\n        optimizer = torch.optim.SGD(enc_model.parameters(), lr=0.1).fix_precision()\n\n        for i in range(1):\n            optimizer.zero_grad()\n            enc_pred = enc_model(enc_data).squeeze(1)\n            l = (((enc_pred - enc_target) ** 2)).sum().refresh()\n            l.backward()\n            optimizer.step()\n\n        return enc_model.weight.copy().get().data\n\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n    james = workers[""james""]\n\n    x_1 = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(torch.float)\n    y_1 = torch.tensor([0, 0, 1, 1]).to(torch.float)\n\n    enc_data_1 = x_1.fix_precision().share(alice, bob, crypto_provider=james, requires_grad=True)\n    enc_target_1 = y_1.fix_precision().share(alice, bob, crypto_provider=james, requires_grad=True)\n\n    model_1 = torch.nn.Linear(2, 1)\n    model_2 = torch.nn.Linear(2, 1)\n\n    # Make sure both networks have the same initial values for the parameters\n    for param_model_2, param_model_1 in zip(model_2.parameters(), model_1.parameters()):\n        param_model_2.data = param_model_1.data\n\n    enc_model_1 = model_1.fix_precision().share(\n        alice, bob, crypto_provider=james, requires_grad=True\n    )\n\n    model_weights_1 = train(enc_model_1, enc_data_1, enc_target_1)\n\n    # Prepare for new train\n    bob.clear_objects()\n    alice.clear_objects()\n    james.clear_objects()\n\n    enc_model_2 = model_2.fix_precision().share(\n        alice, bob, crypto_provider=james, requires_grad=True\n    )\n\n    # Without requires_grad\n    x_2 = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(torch.float)\n    y_2 = torch.tensor([0, 0, 1, 1]).to(torch.float)\n\n    enc_data_2 = x_2.fix_precision().share(alice, bob, crypto_provider=james)\n    enc_target_2 = y_2.fix_precision().share(alice, bob, crypto_provider=james)\n\n    model_weights_2 = train(enc_model_2, enc_data_2, enc_target_2)\n\n    # Check the weights for the two models\n    assert torch.all(torch.eq(model_weights_1, model_weights_2))\n\n\ndef test_garbage_collection(workers):\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n    crypto_provider = workers[""james""]\n\n    a = torch.ones(1, 5)\n    b = torch.ones(1, 5)\n    a = a.encrypt(workers=[alice, bob], crypto_provider=crypto_provider, requires_grad=True)\n    b = b.encrypt(workers=[alice, bob], crypto_provider=crypto_provider, requires_grad=True)\n\n    class Classifier(torch.nn.Module):\n        def __init__(self, in_features, out_features):\n            super(Classifier, self).__init__()\n            self.fc = torch.nn.Linear(in_features, out_features)\n\n        def forward(self, x):\n            logits = self.fc(x)\n            return logits\n\n    classifier = Classifier(in_features=5, out_features=5)\n    model = classifier.fix_prec().share(\n        alice, bob, crypto_provider=crypto_provider, requires_grad=True\n    )\n    opt = optim.SGD(params=model.parameters(), lr=0.1).fix_precision()\n    num_objs = 11\n    prev_loss = float(""inf"")\n    for i in range(3):\n        preds = classifier(a)\n        loss = ((b - preds) ** 2).sum()\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        loss = loss.get().float_prec()\n\n        assert len(alice.object_store._objects) == num_objs\n        assert len(bob.object_store._objects) == num_objs\n        assert loss < prev_loss\n\n        prev_loss = loss\n'"
test/generic/test_functions.py,0,"b'import torch as th\nimport syft as sy\n\n\ndef test_combine_pointers(workers):\n    """"""\n    Ensure that the sy.combine_pointers works as expected\n    """"""\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    y = th.tensor([1, 2, 3, 4, 5]).send(alice)\n\n    a = sy.combine_pointers(*[x, y])\n    b = a + a\n\n    c = b.get(sum_results=True)\n    assert (c == th.tensor([4, 8, 12, 16, 20])).all()\n\n    b = a + a\n    c = b.get(sum_results=False)\n    assert len(c) == 2\n    assert (c[0] == th.tensor([2, 4, 6, 8, 10])).all\n'"
test/generic/test_gc.py,11,"b'""""""All the tests relative to garbage collection of all kinds of remote or local tensors""""""\nimport torch\n\nfrom syft.frameworks.torch.tensors.decorators.logging import LoggingTensor\n\n# TESTING POINTERS\n\n\ndef test_explicit_garbage_collect_pointer(workers):\n    """"""Tests whether deleting a PointerTensor garbage collects the remote object too""""""\n    bob = workers[""bob""]\n\n    # create tensor\n    x = torch.Tensor([1, 2])\n\n    # send tensor to bob\n    x_ptr = x.send(bob)\n\n    # ensure bob has tensor\n    assert x.id in bob.object_store._objects\n\n    # delete pointer to tensor, which should\n    # automatically garbage collect the remote\n    # object on Bob\'s machine\n    del x_ptr\n\n    # ensure bob\'s object was garbage collected\n    assert x.id not in bob.object_store._objects\n\n\ndef test_explicit_garbage_collect_double_pointer(workers):\n    """"""Tests whether deleting a pointer to a pointer garbage collects\n    the remote object too""""""\n\n    alice, bob = workers[""alice""], workers[""bob""]\n\n    # create tensor\n    x = torch.Tensor([1, 2])\n\n    # send tensor to bob and then pointer to alice\n    x_ptr = x.send(bob)\n    x_ptr_ptr = x_ptr.send(alice)\n\n    # ensure bob has tensor\n    assert x.id in bob.object_store._objects\n\n    # delete pointer to pointer to tensor, which should automatically\n    # garbage collect the remote object on Bob\'s machine\n    del x_ptr_ptr\n\n    # ensure bob\'s object was garbage collected\n    assert x.id not in bob.object_store._objects\n    # ensure alice\'s object was garbage collected\n    assert x_ptr.id not in workers[""alice""].object_store._objects\n\n    # Chained version\n    x = torch.Tensor([1, 2])\n    x_id = x.id\n\n    # send tensor to bob and then pointer to alice\n    # overwriting variable names at sending in the test, is on purpose,\n    # to be sure nothing weird happens when people do this\n    x = x.send(bob).send(alice)\n\n    # ensure bob has tensor\n    assert x_id in bob.object_store._objects\n    # delete pointer to pointer to tensor\n    del x\n    # ensure bob\'s object was garbage collected\n    assert x_id not in bob.object_store._objects\n\n\ndef test_implicit_garbage_collection_pointer(workers):\n    """"""Tests whether GCing a PointerTensor GCs the remote object too.""""""\n    bob = workers[""bob""]\n\n    # create tensor\n    x = torch.Tensor([1, 2])\n\n    # send tensor to bob\n    x_ptr = x.send(bob)\n\n    # ensure bob has tensor\n    assert x.id in bob.object_store._objects\n\n    # delete pointer to tensor, which should\n    # automatically garbage collect the remote\n    # object on Bob\'s machine\n    x_ptr = ""asdf""\n\n    # ensure bob\'s object was garbage collected\n    assert x.id not in bob.object_store._objects\n\n\ndef test_implicit_garbage_collect_double_pointer(workers):\n    """"""Tests whether GCing a pointer to a pointer garbage collects\n    the remote object too""""""\n\n    alice, bob = workers[""alice""], workers[""bob""]\n\n    # create tensor\n    x = torch.Tensor([1, 2])\n\n    # send tensor to bob and then pointer to alice\n    x_ptr = x.send(bob)\n    x_ptr_ptr = x_ptr.send(alice)\n\n    # ensure bob has tensor\n    assert x.id in bob.object_store._objects\n    # ensure alice has tensor\n    assert x_ptr.id in alice.object_store._objects\n\n    # delete pointer to pointer to tensor, which should automatically\n    # garbage collect the remote object on Bob\'s machine\n    x_ptr_ptr = ""asdf""\n\n    # ensure bob\'s object was garbage collected\n    assert x.id not in bob.object_store._objects\n    # ensure alice\'s object was garbage collected\n    assert x_ptr.id not in alice.object_store._objects\n\n    # Chained version\n    x = torch.Tensor([1, 2])\n    x_id = x.id\n    # send tensor to bob and then pointer to alice\n    # overwriting variable names at sending in the test, is on purpose,\n    # to be sure nothing weird happens when people do this\n    x = x.send(bob).send(alice)\n\n    # ensure bob has tensor\n    assert x_id in bob.object_store._objects\n\n    # delete pointer to pointer to tensor\n    x = ""asdf""\n\n    # ensure bob\'s object was garbage collected\n    assert x_id not in bob.object_store._objects\n\n\n# TESTING IN PLACE METHODS\n\n\ndef test_inplace_method_on_pointer(workers):\n    bob = workers[""bob""]\n\n    tensor = torch.tensor([[1.0, 2], [4.0, 2]])\n    pointer = tensor.send(bob)\n    pointer.add_(pointer)\n    tensor_back = pointer.get()\n    assert (tensor * 2 == tensor_back).all()\n\n\n# TESTING LOGGING TENSORS\n\n\ndef test_explicit_garbage_collect_logging_on_pointer(workers):\n    """"""\n    Tests whether deleting a LoggingTensor on a PointerTensor\n    garbage collects the remote object too\n    """"""\n    bob = workers[""bob""]\n\n    x = torch.Tensor([1, 2])\n    x_id = x.id\n\n    x = x.send(bob)\n    x = LoggingTensor().on(x)\n    assert x_id in bob.object_store._objects\n\n    del x\n\n    assert x_id not in bob.object_store._objects\n\n\ndef test_implicit_garbage_collect_logging_on_pointer(workers):\n    """"""\n    Tests whether GCing a LoggingTensor on a PointerTensor\n    garbage collects the remote object too\n    """"""\n    bob = workers[""bob""]\n\n    x = torch.Tensor([1, 2])\n    x_id = x.id\n\n    x = x.send(bob)\n    x = LoggingTensor().on(x)\n    assert x_id in bob.object_store._objects\n\n    x = ""open-source""\n    assert x_id not in bob.object_store._objects\n\n\ndef test_websocket_garbage_collection(hook, start_remote_worker):\n    server, remote_proxy = start_remote_worker(id=""ws_gc"", hook=hook, port=8555)\n\n    sample_data = torch.tensor([1, 2, 3, 4])\n    sample_ptr = sample_data.send(remote_proxy)\n\n    _ = sample_ptr.get()\n    assert sample_data not in remote_proxy.object_store._objects\n\n    remote_proxy.close()\n    server.terminate()\n'"
test/generic/test_hookable.py,0,"b'from syft.generic.abstract.hookable import map_chain_call\nfrom syft.generic.abstract.hookable import reduce_chain_call\nfrom syft.generic.abstract.hookable import hookable\n\n\ndef test_reduce_chain_call():\n    class Reduceable:\n        def __init__(self, value):\n            self.child = None\n            self.value = value\n\n        def reduceable(self, value):\n            return value + self.value\n\n    c1 = Reduceable(1)\n    c1.child = Reduceable(2)\n    c1.child.child = Reduceable(3)\n\n    return_val = reduce_chain_call(c1, ""reduceable"", 0)\n\n    assert return_val == 6\n\n\ndef test_map_chain_call():\n    class Mappable:\n        def __init__(self, value):\n            self.child = None\n            self.value = value\n\n        def mappable(self):\n            return self.value\n\n    c1 = Mappable(1)\n    c1.child = Mappable(2)\n    c1.child.child = Mappable(3)\n\n    return_val = map_chain_call(c1, ""mappable"")\n\n    assert return_val == [1, 2, 3]\n\n\ndef test_hooks_get_called():\n    class Hookable:\n        def __init__(self):\n            self.child = None\n            self.flags = {}\n\n        def _before_set_flag(self, flag):\n            self.flags[f""before_{flag}""] = True\n\n        @hookable\n        def set_flag(self, flag):\n            self.flags[flag] = True\n\n        def _after_set_flag(self, obj, flag):\n            self.flags[f""after_{flag}""] = True\n\n    h = Hookable()\n\n    h.set_flag(""flag"")\n\n    assert h.flags[""flag""] is True\n    assert h.flags[""before_flag""] is True\n    assert h.flags[""after_flag""] is True\n\n\ndef test_hooks_propagate_return_val():\n    class Alterable:\n        def __init__(self, value, after):\n            self.child = None\n            self.value = value\n            self.after = after\n\n        @hookable\n        def get_value(self):\n            return self.value\n\n        def _after_get_value(self, return_val):\n            # Merge dictionaries\n            return_val = {**return_val, **self.after}\n            return return_val\n\n    a = Alterable({""a"": 1}, {""after_a"": 1})\n    a.child = Alterable({""b"": 2}, {""after_b"": 2})\n    a.child.child = Alterable({""c"": 3}, {""after_c"": 3})\n\n    result = a.get_value()\n\n    assert result == {""a"": 1, ""after_a"": 1, ""after_b"": 2, ""after_c"": 3}\n'"
test/generic/test_id_provider.py,0,"b'import unittest.mock as mock\nimport pytest\n\nfrom syft import exceptions\nfrom syft.generic import id_provider\n\n\ndef test_pop_no_given_ids(hook):\n    provider = id_provider.IdProvider()\n    values = [10, 4, 15, 4, 2, 0]\n\n    orig_func = id_provider.create_random_id\n    mocked_random_numbers = mock.Mock()\n    mocked_random_numbers.side_effect = values\n    id_provider.create_random_id = mocked_random_numbers\n\n    val = provider.pop()\n    assert val == values[0]\n\n    val = provider.pop()\n    assert val == values[1]\n\n    val = provider.pop()\n    assert val == values[2]\n\n    # values[3] is skipped, as value already used.\n\n    val = provider.pop()\n    assert val == values[4]\n\n    val = provider.pop()\n    assert val == values[5]\n\n    id_provider.create_random_id = orig_func\n\n\ndef test_pop_with_given_ids(hook):\n    given_ids = [4, 15, 2]\n    provider = id_provider.IdProvider(given_ids=given_ids.copy())\n    values = [10, 4, 15, 4, 2, 0]\n\n    orig_func = id_provider.create_random_id\n    mocked_random_numbers = mock.Mock()\n    mocked_random_numbers.side_effect = values\n    id_provider.create_random_id = mocked_random_numbers\n\n    val = provider.pop()\n    assert val == given_ids[-1]\n\n    val = provider.pop()\n    assert val == given_ids[-2]\n\n    val = provider.pop()\n    assert val == given_ids[-3]\n\n    val = provider.pop()\n    assert val == values[0]\n\n    # values[1, 2, 3, 4] are skipped, as value already used.\n\n    val = provider.pop()\n    assert val == values[5]\n\n    id_provider.create_random_id = orig_func\n\n\ndef test_given_ids_side_effect(hook):\n    given_ids = [4, 15, 2]\n    provider = id_provider.IdProvider(given_ids=given_ids)\n\n    assert len(given_ids) == 3\n    provider.pop()\n\n    assert len(given_ids) == 2\n\n    provider.pop()\n    assert len(given_ids) == 1\n\n    provider.pop()\n    assert len(given_ids) == 0\n\n\ndef test_set_next_ids(hook):\n    initial_given_ids = [2, 3]\n    provider = id_provider.IdProvider(given_ids=initial_given_ids.copy())\n\n    next_ids = [4, 5]\n    provider.set_next_ids(next_ids.copy())\n\n    val = provider.pop()\n    assert val == next_ids[-1]\n    val = provider.pop()\n    assert val == next_ids[-2]\n\n    val = provider.pop()\n    assert val == initial_given_ids[-1]\n    val = provider.pop()\n    assert val == initial_given_ids[-2]\n\n\ndef test_set_next_ids_with_id_checking(hook):\n    initial_given_ids = [2, 3]\n    provider = id_provider.IdProvider()\n    provider.set_next_ids(initial_given_ids.copy(), check_ids=False)\n\n    # generated the initial 3 ids\n    provider.pop()\n    provider.pop()\n    provider.pop()\n\n    next_ids = [1, 2, 5]\n    with pytest.raises(exceptions.IdNotUniqueError, match=r""\\{2\\}""):\n        provider.set_next_ids(next_ids.copy(), check_ids=True)\n\n    next_ids = [2, 3, 5]\n    with pytest.raises(exceptions.IdNotUniqueError, match=r""\\{2, 3\\}""):\n        provider.set_next_ids(next_ids.copy(), check_ids=True)\n\n\ndef test_start_recording_ids():\n    initial_given_ids = [2, 3]\n    provider = id_provider.IdProvider(given_ids=initial_given_ids.copy())\n    provider.pop()\n    provider.start_recording_ids()\n    provider.pop()\n\n    ids = provider.get_recorded_ids()\n    assert len(ids) == 1\n    assert ids[0] == initial_given_ids[-2]\n\n\ndef test_get_recorded_ids():\n    initial_given_ids = [2, 3, 4]\n    provider = id_provider.IdProvider(given_ids=initial_given_ids.copy())\n    provider.pop()\n    provider.start_recording_ids()\n    provider.pop()\n\n    ids = provider.get_recorded_ids(continue_recording=True)\n    assert len(ids) == 1\n    assert ids[0] == initial_given_ids[-2]\n\n    provider.pop()\n\n    ids = provider.get_recorded_ids()\n    assert len(ids) == 2\n    assert ids[0] == initial_given_ids[-2]\n    assert ids[1] == initial_given_ids[-3]\n'"
test/generic/test_logging.py,15,"b'import pytest\nimport torch\nimport torch.nn.functional as F\n\nfrom syft.frameworks.torch.tensors.decorators.logging import LoggingTensor\n\n\ndef test_wrap():\n    """"""\n    Test the .on() wrap functionality for LoggingTensor\n    """"""\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n\n    assert isinstance(x, torch.Tensor)\n    assert isinstance(x.child, LoggingTensor)\n    assert isinstance(x.child.child, torch.Tensor)\n\n\ndef test_overwritten_method_on_log_chain():\n    """"""\n    Test method call on a chain including a log tensor\n    """"""\n\n    # Build a long chain tensor Wrapper>LoggingTensor>TorchTensor\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n    y = x.add(x)\n\n    assert (y.child.child == x_tensor.add(x_tensor)).all()\n\n    y = x.child.manual_add(x.child)\n\n    assert (y.child == x_tensor.add(x_tensor)).all()\n\n\ndef test_method_on_log_chain():\n    """"""\n    Test method call on a chain including a log tensor\n    """"""\n\n    # Build a long chain tensor Wrapper>LoggingTensor>TorchTensor\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n    y = x.mul(x)\n\n    assert (y.child.child == x_tensor.mul(x_tensor)).all()\n\n\n@pytest.mark.parametrize(""attr"", [""relu"", ""celu"", ""elu""])\ndef test_hook_module_functional_on_log_chain(attr):\n    """"""\n    Test torch function call on a chain including a log tensor\n    """"""\n\n    attr = getattr(F, attr)\n    x = torch.Tensor([1, -1, 3, 4])\n    expected = attr(x)\n\n    x_log = LoggingTensor().on(x)\n    res_log = attr(x_log)\n    res = res_log.child.child\n\n    assert (res == expected).all()\n\n\ndef test_function_on_log_chain():\n    """"""\n    Test torch function call on a chain including a log tensor\n    """"""\n\n    x = LoggingTensor().on(torch.Tensor([1, -1, 3]))\n    y = F.relu(x)\n\n    assert (y.child.child == torch.Tensor([1, 0, 3])).all()\n\n\ndef test_send_get_log_chain(workers):\n    """"""\n    Test sending and getting back a chain including a logtensor\n    """"""\n\n    # Build a long chain tensor Wrapper>LoggingTensor>TorchTensor\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n    x_ptr = x.send(workers[""bob""])\n    x_back = x_ptr.get()\n\n    assert (x_back.child.child == x_tensor).all()\n\n\ndef test_inplace_send_get_log_chain(workers):\n    """"""\n    Test sending and getting back a chain including a logtensor\n    """"""\n\n    # Build a long chain tensor Wrapper>LoggingTensor>TorchTensor\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n    x_ptr = x.send_(workers[""bob""])\n    x_back = x_ptr.get_()\n\n    assert (x_back.child.child == x_tensor).all()\n\n\ndef test_remote_method_on_log_chain(workers):\n    """"""\n    Test remote method call on a chain including a log tensor\n    """"""\n\n    # Build a long chain tensor Wrapper>LoggingTensor>TorchTensor\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n    x_ptr = x.send(workers[""bob""])\n    y_ptr = F.relu(x_ptr)\n    y = y_ptr.get()\n\n    assert (y.child.child == F.relu(x_tensor)).all()\n\n\ndef test_remote_function_on_log_chain(workers):\n    """"""\n    Test remote function call on a chain including a log tensor\n    """"""\n\n    # Build a long chain tensor Wrapper>LoggingTensor>TorchTensor\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n    x_ptr = x.send(workers[""bob""])\n    y_ptr = x_ptr.add(x_ptr)\n    y = y_ptr.get()\n\n    assert (y.child.child == x_tensor.add(x_tensor)).all()\n\n\ndef test_print_log_chain():\n    """"""\n    Test sending and getting back a chain including a logtensor\n    """"""\n\n    # Build a long chain tensor Wrapper>LoggingTensor>TorchTensor\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = LoggingTensor().on(x_tensor)\n\n    assert isinstance(x.__str__(), str)\n    assert isinstance(x.__repr__(), str)\n'"
test/generic/test_object_storage.py,2,"b'import torch\n\nfrom syft.generic import object_storage\n\n\ndef test_clear_objects():\n    """"""\n    Checks the clear_objects method\n    """"""\n    #  obj_storage is a wrapper object to a collection of objects\n    obj_storage = object_storage.ObjectStore()\n\n    x = torch.tensor(1)\n    obj_storage.set_obj(x)\n\n    objs = obj_storage.current_objects()  # Returns a copy of the objects in obj_storage(here:x)\n\n    assert len(objs) == 1\n    assert objs[x.id] == x\n\n    ret_val = obj_storage.clear_objects()  # Completely removes all objects from obj_storage\n\n    objs = obj_storage.current_objects()\n    assert len(objs) == 0\n    assert ret_val is None\n\n\ndef test_set_obj_takes_ownership(workers):\n    me = workers[""me""]\n    bob = workers[""bob""]\n\n    x = torch.tensor(1)\n\n    x.owner = bob\n\n    me.object_store.set_obj(x)\n\n    objs = me.object_store._objects\n\n    assert objs[x.id] == x\n    assert objs[x.id].owner == workers[""me""]\n'"
test/generic/test_private.py,47,"b'import pytest\nimport torch\nimport torch.nn as nn\n\nfrom syft.frameworks.torch.tensors.interpreters.private import PrivateTensor\nfrom syft.exceptions import GetNotPermittedError\nfrom syft.exceptions import SendNotPermittedError\n\n\ndef test_wrap():\n    """"""\n    Test the .on() wrap functionality for LoggingTensor\n    """"""\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = PrivateTensor().on(x_tensor)\n    assert isinstance(x, torch.Tensor)\n    assert isinstance(x.child, PrivateTensor)\n    assert isinstance(x.child.child, torch.Tensor)\n\n\ndef test_native_private_tensor_method():\n    """"""\n    Test native\'s private_tensor method.\n    """"""\n    x_tensor = torch.Tensor([1, 2, 3])\n    private_x = x_tensor.private_tensor(allowed_users=[""testing""])\n    assert isinstance(private_x, torch.Tensor)\n    assert isinstance(private_x.child, PrivateTensor)\n    assert isinstance(private_x.child.child, torch.Tensor)\n\n\ndef test_allow_method():\n    """"""\n    Test native\'s private_tensor method.\n    """"""\n\n    x_tensor = torch.Tensor([1, 2, 3])\n\n    # User credentials mockup\n    class UserAuthMockup(object):\n        def __init__(self, login, password):\n            self.login = login\n            self.password = password\n\n        def __eq__(self, other):\n            if isinstance(other, UserAuthMockup):\n                return self.login == other.login and self.__password == other.password\n\n    allowed_user = UserAuthMockup(""user"", ""password"")\n    second_allowed_user = UserAuthMockup(""second_user"", ""password"")\n    unallowed_user = UserAuthMockup(""example"", ""password"")\n\n    private_x = x_tensor.private_tensor(allowed_users=[allowed_user, second_allowed_user])\n    assert private_x.allow(allowed_user)\n    assert private_x.allow(second_allowed_user)\n    assert not private_x.allow(unallowed_user)\n\n\ndef test_send_method(workers):\n    bob = workers[""bob""]\n    x_tensor = torch.tensor([4, 5, 6, 7, 8])\n\n    private_x = x_tensor.private_tensor(allowed_users=[""User""])\n\n    # Try to call send() without credentials\n    with pytest.raises(SendNotPermittedError):\n        private_x.send(bob)\n\n    # Try to call send() with wrong credentials\n    with pytest.raises(SendNotPermittedError):\n        private_x.send(user=""unallowed_user"")\n\n    # Try to call send() with allowed credentails\n    private_x_pointer = private_x.send(bob, user=""User"")\n\n\ndef test_get_method(workers):\n    bob = workers[""bob""]\n    x_tensor = torch.Tensor([1, 2, 3])\n\n    private_x = x_tensor.private_tensor(allowed_users=[""User""])\n\n    private_x_pointer = private_x.send(bob, user=""User"")\n\n    # Try to call get() without credentials\n    with pytest.raises(GetNotPermittedError):\n        private_x_pointer.get()\n\n    # Try to call get() with wrong credentials\n    with pytest.raises(GetNotPermittedError):\n        private_x_pointer.get(user=""UnregisteredUser"")\n\n    # Try to call get() with allowed credentials\n    result = private_x_pointer.get(user=""User"")\n\n\ndef test_private_tensor_registration(hook):\n    with hook.local_worker.registration_enabled():\n        x = torch.tensor([1.0])\n        private_x = x.private_tensor(allowed_users=[""User""])\n\n        assert hook.local_worker.get_obj(x.id) == x\n\n\ndef test_allowed_to_get():\n    x = torch.tensor([1, 2, 3, 4, 5, 6])\n    assert x.allow(""User"")  # Public tensors always return true.\n\n    private_x = x.private_tensor(allowed_users=[""User""])\n\n    assert private_x.allow(""User"")  # It Returns true to previously registered user.\n    assert not private_x.allow(""AnotherUser"")  # It Returns False to non previously registered user.\n\n\ndef test_add_method():\n    t = torch.tensor([0.1, 0.2, 0.3])\n    x = t.private_tensor(allowed_users=[""User""])\n\n    y = x + x\n\n    # Test if it preserves the wraper stack\n    assert isinstance(x, torch.Tensor)\n    assert isinstance(x.child, PrivateTensor)\n    assert isinstance(x.child.child, torch.Tensor)\n\n    assert x.allow(""User"")  # Test if it preserves the parent user credentials.\n\n\n@pytest.mark.parametrize(""method"", [""t"", ""matmul""])\n@pytest.mark.parametrize(""parameter"", [False, True])\ndef test_methods_for_linear_module(method, parameter):\n    """"""\n    Test all the methods used in the F.linear functions\n    """"""\n    if parameter:\n        tensor = nn.Parameter(torch.tensor([[1.0, 2], [3, 4]]))\n    else:\n        tensor = torch.tensor([[1.0, 2], [3, 4]])\n    fp_tensor = tensor.fix_precision()\n\n    # ADD Private Tensor at wrapper stack\n    private_fp_tensor = fp_tensor.private_tensor(allowed_users=[""User""])  # ADD Private Layer\n\n    if method != ""t"":\n        fp_result = getattr(private_fp_tensor, method)(private_fp_tensor)\n        result = getattr(tensor, method)(tensor)\n    else:\n        fp_result = getattr(private_fp_tensor, method)()\n        result = getattr(tensor, method)()\n\n    assert (result == fp_result.float_precision()).all()\n\n\ndef test_torch_add():\n    x = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n\n    # ADD Private Tensor at wrapper stack\n    x = x.private_tensor(allowed_users=[""User""])\n\n    y = torch.add(x, x)\n\n    assert (y.child.child.child == torch.LongTensor([200, 400, 600])).all()\n\n    y_fp = y.float_prec()\n\n    assert (y_fp == torch.tensor([0.2, 0.4, 0.6])).all()\n\n    # Test if it preserves the parent user credentials.\n    assert y.allow(""User"")\n    assert not y.allow(""NonRegisteredUser"")\n\n    # With negative numbers\n    x = torch.tensor([-0.1, -0.2, 0.3]).fix_prec()\n    y = torch.tensor([0.4, -0.5, -0.6]).fix_prec()\n\n    # ADD Private Tensor at wrapper stack\n    x = x.private_tensor(allowed_users=[""UserCredential""])\n    y = y.private_tensor(allowed_users=[""UserCredential""])\n\n    z = torch.add(x, y)\n    z_fp = z.float_prec()\n\n    assert (z_fp == torch.tensor([0.3, -0.7, -0.3])).all()\n\n    # Test if it preserves the parent user credentials.\n    assert z.allow(""UserCredential"")\n    assert not z.allow(""NonRegisteredUser"")\n\n\ndef test_torch_sub():\n    x = torch.tensor([0.5, 0.8, 1.3]).fix_prec()\n    y = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n\n    # ADD Private Tensor at wrapper stack\n    x = x.private_tensor(allowed_users=[""User""])\n    y = y.private_tensor(allowed_users=[""User""])\n\n    z = torch.sub(x, y)\n\n    # Test if it preserves the parent user credentials.\n    assert z.allow(""User"")\n    assert not z.allow(""NonRegisteredUser"")\n\n    assert (z.child.child.child == torch.LongTensor([400, 600, 1000])).all()\n    z_fp = z.float_prec()\n\n    assert (z_fp == torch.tensor([0.4, 0.6, 1.0])).all()\n\n\ndef test_torch_mul():\n    # mul with non standard fix precision\n    x = torch.tensor([2.113]).fix_prec(precision_fractional=2)\n\n    # ADD Private Tensor at wrapper stack\n    x = x.private_tensor(allowed_users=[""User""])\n\n    y = torch.mul(x, x)\n\n    assert y.child.child.child == torch.LongTensor([445])\n    assert y.child.child.precision_fractional == 2\n\n    # Test if it preserves the parent user credentials.\n    assert y.allow(""User"")\n    assert not y.allow(""NonRegisteredUser"")\n\n    y = y.float_prec()\n\n    assert y == torch.tensor([4.45])\n\n    # Mul with negative numbers\n    x = torch.tensor([2.113]).fix_prec()\n    y = torch.tensor([-0.113]).fix_prec()\n\n    # ADD Private Tensor at wrapper stack\n    x = x.private_tensor(allowed_users=[""User""])\n    y = y.private_tensor(allowed_users=[""User""])\n\n    z = torch.mul(x, y)\n\n    assert z.child.child.precision_fractional == 3\n\n    # Test if it preserves the parent user credentials.\n    assert z.allow(""User"")\n    assert not z.allow(""NonRegisteredUser"")\n\n    z = z.float_prec()\n    assert z == torch.tensor([-0.2380])\n\n    x = torch.tensor([11.0]).fix_prec(field=2 ** 16, precision_fractional=2)\n\n    # ADD Private Tensor at wrapper stack\n    x = x.private_tensor(allowed_users=[""User""])\n\n    y = torch.mul(x, x)\n\n    y = y.float_prec()\n\n    assert y == torch.tensor([121.0])\n\n    # mixing + and *\n    x = torch.tensor([2.113]).fix_prec()\n    y = torch.tensor([-0.113]).fix_prec()\n\n    # ADD Private Tensor at wrapper stack\n    x = x.private_tensor(allowed_users=[""User""])\n    y = y.private_tensor(allowed_users=[""User""])\n\n    z = torch.mul(x, y + y)\n    z = z.float_prec()\n\n    assert z == torch.tensor([-0.4770])\n\n\ndef test_operate_with_integer_constants():\n    x = torch.tensor([1.0])\n    x_fp = x.fix_precision()\n\n    # PrivateTensor at wrapper stack.\n    x_fp = x_fp.private_tensor(allowed_users=[""User""])\n\n    # ADD\n    r_fp = x_fp + 10\n\n    # Test if it preserves the parent user credentials.\n    assert r_fp.allow(""User"")\n    assert not r_fp.allow(""NonRegisteredUser"")\n\n    r = r_fp.float_precision()\n    assert r == x + 10\n\n    # SUB\n    r_fp = x_fp - 7\n\n    # Test if it preserves the parent user credentials.\n    assert r_fp.allow(""User"")\n    assert not r_fp.allow(""NonRegisteredUser"")\n\n    r = r_fp.float_precision()\n    assert r == x - 7\n\n    # MUL\n    r_fp = x_fp * 2\n\n    # Test if it preserves the parent user credentials.\n    assert r_fp.allow(""User"")\n    assert not r_fp.allow(""NonRegisteredUser"")\n\n    assert r_fp.float_precision() == x * 2\n\n    # DIV\n    r_fp = x_fp / 5\n\n    # Test if it preserves the parent user credentials.\n    assert r_fp.allow(""User"")\n    assert not r_fp.allow(""NonRegisteredUser"")\n\n    assert r_fp.float_precision() == x / 5\n'"
test/generic/test_string.py,0,"b'from syft.generic.string import String\nimport syft as sy\n\n\ndef test_string_methods():\n    """"""\n        Tests some of the `String` methods which are hooked from `str`.\n    more tests are to be added\n    """"""\n\n    # Create a string\n    string = String(""Hello PySyft"")\n\n    assert isinstance(string.upper(), String)\n    assert isinstance(string.lower(), String)\n    assert isinstance(string.title(), String)\n\n    assert string == ""Hello PySyft""\n    assert string == String(""Hello PySyft"")\n\n    assert string.upper() == ""HELLO PYSYFT""\n    assert string.upper() == String(""HELLO PYSYFT"")\n\n    assert string.lower() == ""hello pysyft""\n    assert string.lower() == String(""hello pysyft"")\n\n    assert string.title() == ""Hello Pysyft""\n    assert string.title() == String(""Hello Pysyft"")\n    assert string.title() >= String(""Hello Pysyft"")\n    assert string.title() <= String(""Hello Pysyft"")\n\n    assert string.startswith(""Hel"") is True\n    assert string.startswith(String(""Hel"")) is True\n\n    assert string.endswith(""Syft"") is True\n    assert string.endswith(String(""Syft"")) is True\n\n    assert (string > ""Hello PySyfa"") is True\n    assert (string >= ""Hello PySyfa"") is True\n\n    assert (string < ""Hello PySyfz"") is True\n    assert (string <= ""Hello PySyfz"") is True\n\n    assert String("" Hello"").lstrip() == ""Hello""\n    assert String(""Hello "").rstrip() == ""Hello""\n\n    assert String(""Hello"").center(9) == ""  Hello  ""\n    assert String(""Hello"").center(9) == String(""  Hello  "")\n\n    assert String(""Hello"").rjust(10) == ""     Hello""\n    assert String(""Hello"").rjust(10) == String(""     Hello"")\n\n    assert String(""Hello"").ljust(10) == ""Hello     ""\n    assert String(""Hello"").ljust(10) == String(""Hello     "")\n\n    assert string + string == ""Hello PySyftHello PySyft""\n    assert isinstance(string + string, String)\n    assert isinstance(string + "" !"", String)\n\n    assert f""{string} !"" == ""Hello PySyft !""\n\n    assert String(""Hello {}"").format(String(""PySyft"")) == string\n    assert String(""Hello %s"") % ""PySyft"" == string\n\n    assert str(string) == ""Hello PySyft""\n\n    x = String(""Hello PySyft"")\n    bob = sy.VirtualWorker(id=""bob"", hook=sy.hook)\n    out = x.send(bob)\n    assert isinstance(out, sy.generic.pointers.string_pointer.StringPointer)\n'"
test/keras/test_sequential.py,0,"b'import pytest\n\nimport numpy as np\nimport syft as sy\nfrom syft import dependency_check\n\nif dependency_check.tfe_available:  # pragma: no cover\n    import tensorflow as tf\n    import tf_encrypted as tfe\n\n\n@pytest.mark.skipif(not dependency_check.tfe_available, reason=""tf_encrypted not installed"")\ndef test_instantiate_tfe_layer():  # pragma: no cover\n    """"""\n    tests tfe layer by running a constant 4*5 input matrix on a network\n    with one constant tensor of weights, using tf.keras then tfe and comparing results\n    """"""\n\n    from syft.frameworks.keras.model.sequential import _instantiate_tfe_layer\n\n    sy.KerasHook(tf.keras)\n\n    # creates input and weights constant tensors\n    input_shape = [4, 5]\n    input_data = np.ones(input_shape)\n    kernel = np.random.normal(size=[5, 5])\n    initializer = tf.keras.initializers.Constant(kernel)\n\n    # creates a network with 4*5 input shape, 5 output units, and 5*5 weights matrix\n    d_tf = tf.keras.layers.Dense(\n        5, kernel_initializer=initializer, batch_input_shape=input_shape, use_bias=True\n    )\n\n    # runs the model using tf.keras\n    with tf.Session() as sess:\n        x = tf.Variable(input_data, dtype=tf.float32)\n        y = d_tf(x)\n        sess.run(tf.global_variables_initializer())\n        expected = sess.run(y)\n\n    stored_keras_weights = {d_tf.name: d_tf.get_weights()}\n\n    # runs the model using tfe\n    with tf.Graph().as_default():\n        p_x = tfe.define_private_variable(input_data)\n        d_tfe = _instantiate_tfe_layer(d_tf, stored_keras_weights)\n\n        out = d_tfe(p_x)\n\n        with tfe.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            actual = sess.run(out.reveal())\n\n    # compares results and raises error if not equal up to 0.001\n    np.testing.assert_allclose(actual, expected, rtol=0.005, atol=0.001)\n\n\n@pytest.mark.skipif(not dependency_check.tfe_available, reason=""tf_encrypted not installed"")\ndef test_share():  # pragma: no cover\n    """"""tests tfe federated learning functionality by running a constant input on same model\n    using tf.keras\n    then tfe on remote workers and comparing the outputs of both cases\n    """"""\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import Dense\n\n    sy.KerasHook(tf.keras)\n\n    # creates input and weights constant tensors\n    input_shape = (4, 5)\n    kernel = np.random.normal(size=(5, 5))\n    initializer = tf.keras.initializers.Constant(kernel)\n    dummy_input = np.ones(input_shape).astype(np.float32)\n\n    model = Sequential()\n\n    model.add(\n        Dense(5, kernel_initializer=initializer, batch_input_shape=input_shape, use_bias=True)\n    )\n    output_shape = model.output_shape\n    result_public = model.predict(dummy_input)  # runs constant input on the model using tf.keras\n\n    # creats a cluster of tfe workers(remote machines)\n    client = sy.TFEWorker(host=None)\n    alice = sy.TFEWorker(host=None)\n    bob = sy.TFEWorker(host=None)\n    carol = sy.TFEWorker(host=None)\n    cluster = sy.TFECluster(alice, bob, carol)\n\n    cluster.start()\n\n    model.share(cluster)  # sends the model to the workers\n\n    # runs same input on same model on the romte workers and gets back the output\n    with model._tfe_graph.as_default():\n        client.connect_to_model(input_shape, output_shape, cluster, sess=model._tfe_session)\n\n    client.query_model_async(dummy_input)\n\n    model.serve(num_requests=1)\n\n    result_private = client.query_model_join().astype(np.float32)\n    # compares results and raises error if not equal up to 0.01\n    np.testing.assert_allclose(result_private, result_public, atol=0.01)\n\n    model.stop()\n\n    cluster.stop()\n'"
test/message/__init__.py,0,b''
test/message/test_message.py,0,"b'import torch as th\n\nfrom syft.messaging import message\n\n\ndef test_cmd_message(workers):\n\n    bob = workers[""bob""]\n\n    bob.log_msgs = True\n\n    x = th.tensor([1, 2, 3, 4]).send(bob)\n\n    y = x + x  # this is the test\n    assert isinstance(bob._get_msg(-1), message.TensorCommandMessage)\n\n    y = y.get()\n\n    assert (y == th.tensor([2, 4, 6, 8])).all()\n\n    bob.log_msgs = False\n\n\ndef test_obj_message(workers):\n\n    bob = workers[""bob""]\n\n    bob.log_msgs = True\n\n    x = th.tensor([1, 2, 3, 4]).send(bob)  # this is the test\n\n    assert isinstance(bob._get_msg(-1), message.ObjectMessage)\n\n    y = x + x\n\n    y = y.get()\n\n    assert (y == th.tensor([2, 4, 6, 8])).all()\n\n    bob.log_msgs = False\n\n\ndef test_obj_req_message(workers):\n\n    bob = workers[""bob""]\n\n    bob.log_msgs = True\n\n    x = th.tensor([1, 2, 3, 4]).send(bob)\n\n    y = x + x\n\n    y = y.get()  # this is the test\n    assert isinstance(bob._get_msg(-1), message.ObjectRequestMessage)\n\n    assert (y == th.tensor([2, 4, 6, 8])).all()\n\n    bob.log_msgs = False\n\n\ndef test_get_shape_message(workers):\n\n    bob = workers[""bob""]\n\n    bob.log_msgs = True\n\n    x = th.tensor([1, 2, 3, 4]).send(bob)\n\n    y = x + x\n\n    z = y.shape  # this is the test\n    assert isinstance(bob._get_msg(-1), message.GetShapeMessage)\n\n    assert z == th.Size([4])\n\n    bob.log_msgs = False\n\n\ndef test_force_object_delete_message(workers):\n\n    bob = workers[""bob""]\n\n    bob.log_msgs = True\n\n    x = th.tensor([1, 2, 3, 4]).send(bob)\n\n    id_on_worker = x.id_at_location\n\n    assert id_on_worker in bob.object_store._objects\n\n    del x  # this is the test\n    assert isinstance(bob._get_msg(-1), message.ForceObjectDeleteMessage)\n\n    assert id_on_worker not in bob.object_store._objects\n\n    bob.log_msgs = False\n\n\ndef test_is_none_message(workers):\n\n    bob = workers[""bob""]\n\n    bob.log_msgs = True\n\n    x = th.tensor([1, 2, 3, 4]).send(bob)\n\n    y = th.tensor([1]).send(bob)\n    y.child.id_at_location = x.id_at_location\n\n    assert not bob.request_is_remote_tensor_none(x)\n    assert isinstance(bob._get_msg(-1), message.IsNoneMessage)\n    assert not x.child.is_none()\n    assert isinstance(bob._get_msg(-1), message.IsNoneMessage)\n\n    del x\n\n    assert y.child.is_none()\n\n    bob.log_msgs = False\n'"
test/notebooks/test_notebooks.py,1,"b'import glob\nimport os\nimport sys\nimport urllib.request\nfrom pathlib import Path\nfrom zipfile import ZipFile\nimport codecs\n\nimport pytest\nimport nbformat\nimport papermill as pm\n\nimport syft as sy\n\n# lets start by finding all notebooks currently available in examples and subfolders\nall_notebooks = (n for n in glob.glob(""examples/tutorials/**/*.ipynb"", recursive=True))\nbasic_notebooks = (n for n in glob.glob(""examples/tutorials/*.ipynb""))\nadvanced_notebooks = (\n    n for n in glob.glob(""examples/tutorials/advanced/**/*.ipynb"", recursive=True)\n)\ntranslated_notebooks = (\n    n for n in glob.glob(""examples/tutorials/translations/**/*.ipynb"", recursive=True)\n)\n# Exclude all translated basic tutorials that are also\n# excluded in their original version.\nexcluded_translated_notebooks = [\n    Path(nb).name for part in [""10"", ""13b"", ""13c""] for nb in translated_notebooks if part in nb\n]\n\n\n# Include only the translations that have been changed\ngitdiff = Path(""test/notebooks/git-diff.txt"")\nchanged_files = []\nif gitdiff.is_file():\n    changed_files = open(gitdiff, ""r"")\n    changed_files = changed_files.readlines()\n    changed_files = [\n        codecs.decode(file.replace(\'""\', """").replace(""\\n"", """"), ""unicode-escape"")\n        .encode(""latin-1"")\n        .decode()\n        for file in changed_files\n    ]\ntranslated_notebooks_diff = list(set(changed_files) & set(translated_notebooks))\n\n# buggy notebooks with explanation what does not work\nexclusion_list_notebooks = [\n    # Part 10 needs either torch.log2 to be implemented or numpy to be hooked\n    ""Part 10 - Federated Learning with Secure Aggregation.ipynb"",\n    # Part 13b and c need fixing of the tensorflow serving with PySyft\n    ""Part 13b - Secure Classification with Syft Keras and TFE - Secure Model Serving.ipynb"",\n    ""Part 13c - Secure Classification with Syft Keras and TFE - Private Prediction Client.ipynb"",\n    # This notebook is excluded as it needs library code modification which I might add later on\n    ""Build your own tensor type (advanced).ipynb"",\n    ""Federated Recurrent Neural Network.ipynb"",\n    # Outdated websocket client code\n    ""Federated learning with websockets and federated averaging.ipynb"",\n]\n\n# Add excluded translated notebooks to the exclusion list\nexclusion_list_notebooks += excluded_translated_notebooks\n\nexclusion_list_folders = [\n    ""examples/tutorials/websocket"",\n    ""examples/tutorials/advanced/monitor_network_traffic"",\n    ""examples/tutorials/advanced/privacy_attacks"",\n    ""examples/tutorials/advanced/websockets_mnist_parallel"",\n    # To run these notebooks, we need to run grid nodes / grid gateway\n    # previously (they aren\'t  in this repository)\n    ""examples/tutorials/grid"",\n    ""examples/tutorials/grid/federated_learning/spam_prediction"",\n    ""examples/tutorials/grid/federated_learning/mnist"",\n    # This notebook is skipped because it fails in github actions and we\n    # do not know why for the moment\n    ""examples/tutorials/advanced/federated_sms_spam_prediction"",\n]\n\n\nexcluded_notebooks = []\nfor nb in all_notebooks:\n    if Path(nb).name in exclusion_list_notebooks:\n        excluded_notebooks += [nb]\nfor folder in exclusion_list_folders:\n    excluded_notebooks += glob.glob(f""{folder}/**/*.ipynb"", recursive=True)\n\ntested_notebooks = []\n\n\n@pytest.mark.parametrize(""notebook"", sorted(set(basic_notebooks) - set(excluded_notebooks)))\ndef test_notebooks_basic(isolated_filesystem, notebook):\n    """"""Test Notebooks in the tutorial root folder.""""""\n    notebook = notebook.split(""/"")[-1]\n    list_name = Path(""examples/tutorials/"") / notebook\n    tested_notebooks.append(str(list_name))\n    res = pm.execute_notebook(\n        notebook,\n        ""/dev/null"",\n        parameters={""epochs"": 1, ""n_test_batches"": 5, ""n_train_items"": 64, ""n_test_items"": 64},\n        timeout=300,\n    )\n    assert isinstance(res, nbformat.notebooknode.NotebookNode)\n\n\n@pytest.mark.translation\n@pytest.mark.parametrize(\n    ""translated_notebook"", sorted(set(translated_notebooks) - set(excluded_notebooks))\n)\ndef test_notebooks_basic_translations(isolated_filesystem, translated_notebook):  # pragma: no cover\n    """"""Test Notebooks in the tutorial translations folder.""""""\n    notebook = ""/"".join(translated_notebook.split(""/"")[-2:])\n    notebook = f""translations/{notebook}""\n    list_name = Path(f""examples/tutorials/{notebook}"")\n    tested_notebooks.append(str(list_name))\n    res = pm.execute_notebook(\n        notebook,\n        ""/dev/null"",\n        parameters={""epochs"": 1, ""n_test_batches"": 5, ""n_train_items"": 64, ""n_test_items"": 64},\n        timeout=400,\n    )\n    assert isinstance(res, nbformat.notebooknode.NotebookNode)\n\n\n@pytest.mark.translation\n@pytest.mark.parametrize(\n    ""translated_notebook"", sorted(set(translated_notebooks_diff) - set(excluded_notebooks))\n)\ndef test_notebooks_basic_translations_diff(\n    isolated_filesystem, translated_notebook\n):  # pragma: no cover\n    """"""\n    Test Notebooks in the tutorial translations folder if they have been\n    modified in the current pull request. This test should not consider any\n    notebooks locally. It should be used on Github Actions.\n    """"""\n    notebook = ""/"".join(translated_notebook.split(""/"")[-2:])\n    notebook = f""translations/{notebook}""\n    list_name = Path(f""examples/tutorials/{notebook}"")\n    tested_notebooks.append(str(list_name))\n    res = pm.execute_notebook(\n        notebook,\n        ""/dev/null"",\n        parameters={""epochs"": 1, ""n_test_batches"": 5, ""n_train_items"": 64, ""n_test_items"": 64},\n        timeout=300,\n    )\n    assert isinstance(res, nbformat.notebooknode.NotebookNode)\n\n\n@pytest.mark.parametrize(""notebook"", sorted(set(advanced_notebooks) - set(excluded_notebooks)))\ndef test_notebooks_advanced(isolated_filesystem, notebook):\n    notebook = notebook.replace(""examples/tutorials/"", """")\n    list_name = Path(""examples/tutorials/"") / notebook\n    tested_notebooks.append(str(list_name))\n    res = pm.execute_notebook(notebook, ""/dev/null"", parameters={""epochs"": 1}, timeout=300)\n    assert isinstance(res, nbformat.notebooknode.NotebookNode)\n\n\n@pytest.mark.skip\ndef test_fl_sms(isolated_filesystem):  # pragma: no cover\n    sys.path.append(""advanced/federated_sms_spam_prediction/"")\n    import preprocess\n\n    os.chdir(""advanced/federated_sms_spam_prediction/"")\n\n    notebook = ""Federated SMS Spam prediction.ipynb""\n    p_name = Path(""examples/tutorials/advanced/federated_sms_spam_prediction/"")\n    tested_notebooks.append(str(p_name / notebook))\n    Path(""data"").mkdir(parents=True, exist_ok=True)\n    url = ""https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip""\n    urllib.request.urlretrieve(url, ""data.zip"")\n    with ZipFile(""data.zip"", ""r"") as zipObj:\n        # Extract all the contents of the zip file in current directory\n        zipObj.extractall()\n    preprocess.main()\n    res = pm.execute_notebook(notebook, ""/dev/null"", parameters={""epochs"": 1}, timeout=300)\n    assert isinstance(res, nbformat.notebooknode.NotebookNode)\n\n\ndef test_fl_with_websockets_and_averaging(\n    isolated_filesystem, start_remote_server_worker_only, hook\n):\n    os.chdir(""advanced/websockets_mnist/"")\n    notebook = ""Federated learning with websockets and federated averaging.ipynb""\n    p_name = Path(""examples/tutorials/advanced/websockets_mnist/"")\n    tested_notebooks.append(str(p_name / notebook))\n    for n in [""alice"", ""bob"", ""charlie""]:\n        hook.local_worker.remove_worker_from_registry(n)\n    kwargs_list = [\n        {""id"": ""alice"", ""host"": ""localhost"", ""port"": 8777, ""hook"": hook},\n        {""id"": ""bob"", ""host"": ""localhost"", ""port"": 8778, ""hook"": hook},\n        {""id"": ""charlie"", ""host"": ""localhost"", ""port"": 8779, ""hook"": hook},\n    ]\n    processes = [start_remote_server_worker_only(**kwargs) for kwargs in kwargs_list]\n    res = pm.execute_notebook(\n        notebook,\n        ""/dev/null"",\n        parameters={""args"": [""--epochs"", ""1"", ""--test_batch_size"", ""100""], ""abort_after_one"": True},\n        timeout=300,\n    )\n    assert isinstance(res, nbformat.notebooknode.NotebookNode)\n    [server.terminate() for server in processes]\n    for n in [""alice"", ""bob"", ""charlie""]:\n        sy.VirtualWorker(id=n, hook=hook, is_client_worker=False)\n\n\n### These tests must always be last\ndef test_all_notebooks_except_translations():\n    untested_notebooks = (\n        set(all_notebooks)\n        - set(excluded_notebooks)\n        - set(translated_notebooks)\n        - set(tested_notebooks)\n    )\n    assert len(untested_notebooks) == 0, untested_notebooks\n\n\n@pytest.mark.translation\ndef test_all_translation_notebooks():  # pragma: no cover\n    untested_notebooks = set(translated_notebooks) - set(excluded_notebooks) - set(tested_notebooks)\n    assert len(untested_notebooks) == 0, untested_notebooks\n'"
test/scripts/run_websocket_server.py,0,"b'import subprocess\nimport sys\nfrom pathlib import Path\nimport argparse\n\nif __name__ == ""__main__"":\n\n    # Parse args\n    parser = argparse.ArgumentParser(description=""Run websocket server worker."")\n    parser.add_argument(\n        ""--port"",\n        ""-p"",\n        type=int,\n        help=""port number of the websocket server worker, e.g. --port 8777"",\n    )\n    parser.add_argument(""--host"", type=str, default=""localhost"", help=""host for the connection"")\n    parser.add_argument(\n        ""--id"", type=str, help=""name (id) of the websocket server worker, e.g. --id alice""\n    )\n    parser.add_argument(\n        ""--testing"",\n        action=""store_true"",\n        help=(\n            ""if set, websocket server worker will load the ""\n            ""test dataset instead of the training dataset"",\n        ),\n    )\n    parser.add_argument(\n        ""--verbose"",\n        ""-v"",\n        action=""store_true"",\n        help=""""""if set, websocket server worker will be started in verbose mode"""""",\n    )\n    parser.add_argument(\n        ""--notebook"",\n        type=str,\n        default=""normal"",\n        help=""""""can run websocket server for websockets examples of mnist/mnist-parallel or\n    pen_testing/steal_data_over_sockets. Type \'mnist\' for starting server\n    for websockets-example-MNIST, `mnist-parallel` for websockets-example-MNIST-parallel\n    and \'steal_data\' for pen_tesing stealing data over sockets"""""",\n    )\n    parser.add_argument(""--pytest_testing"", action=""store_true"", help=""""""Used for pytest testing"""""")\n    args = parser.parse_args()\n\n    python = Path(sys.executable).name\n    FILE_PATH = Path(__file__).resolve().parents[2].joinpath(""run_websocket_server.py"")\n    call_alice = [\n        python,\n        FILE_PATH,\n        ""--host"",\n        args.host,\n        ""--port"",\n        str(args.port),\n        ""--id"",\n        args.id,\n        ""--pytest_testing"",\n    ]\n\n    if args.verbose:\n        call_alice.append(""--verbose"")\n\n    if args.testing:\n        call_alice.append(""--testing"")\n\n    subprocess.Popen(call_alice)\n'"
test/serde/serde_helpers.py,113,"b'from collections import OrderedDict\nimport numpy\nimport torch\nimport traceback\nimport io\n\nimport syft\nfrom syft.serde import msgpack\nfrom syft.workers.virtual import VirtualWorker\nfrom syft.serde.syft_serializable import SyftSerializable\n\n\nclass SerializableDummyClass(SyftSerializable):\n    def __init__(self, value):\n        self.value = value\n\n    @staticmethod\n    def simplify(worker, obj):\n        return obj.value\n\n    @staticmethod\n    def detail(worker, obj):\n        return SerializableDummyClass(obj)\n\n    @staticmethod\n    def get_msgpack_code():\n        return {""code"": 12345}\n\n\n# Make dict of type codes\nCODE = OrderedDict()\nfor cls, simplifier in msgpack.serde.msgpack_global_state.simplifiers.items():\n    CODE[cls] = simplifier[0]\nFORCED_CODE = OrderedDict()\nfor cls, simplifier in msgpack.serde.msgpack_global_state.forced_full_simplifiers.items():\n    FORCED_CODE[cls] = simplifier[0]\n\n########################################################################\n# Functions that return list of serde samples in the following format:\n# [\n#   {\n#    ""value"": original_value,\n#    ""simplified"": simplified_value,\n#    ""cmp_detailed"": custom_detailed_values_comparison_function, # optional\n#    ""cmp_simplified"": custom_simplified_values_comparison_function, # optional\n#    ""framework"": None or torch, # optional, affects tensor serialization strategy\n#    ""forced"": (bool), # optional, enables forced full simplification\n#   },\n#   ...\n# ]\n########################################################################\n\n########################################################################\n# Native types.\n########################################################################\n\n# None\n\n\ndef make_none(**kwargs):\n    return [{""value"": None}]\n\n\n# Dict.\ndef make_dict(**kwargs):\n    return [\n        {\n            ""value"": {1: ""hello"", 2: ""world""},\n            ""simplified"": (\n                CODE[dict],\n                (\n                    (1, (CODE[str], (b""hello"",))),  # [not simplified tuple]  # key  # value\n                    (2, (CODE[str], (b""world"",))),\n                ),\n            ),\n        },\n        {\n            ""value"": {""hello"": ""world""},\n            ""simplified"": (\n                CODE[dict],\n                (\n                    (  # [not simplified tuple]\n                        (CODE[str], (b""hello"",)),  # key\n                        (CODE[str], (b""world"",)),  # value\n                    ),\n                ),\n            ),\n        },\n        {""value"": {}, ""simplified"": (CODE[dict], ())},\n    ]\n\n\n# List.\ndef make_list(**kwargs):\n    return [\n        {\n            ""value"": [""hello"", ""world""],\n            ""simplified"": (\n                CODE[list],\n                ((CODE[str], (b""hello"",)), (CODE[str], (b""world"",))),  # item\n            ),\n        },\n        {""value"": [""hello""], ""simplified"": (CODE[list], ((CODE[str], (b""hello"",)),))},  # item\n        {""value"": [], ""simplified"": (CODE[list], ())},\n        # Tests that forced full simplify should return just simplified object\n        # if it doesn\'t have full simplifier\n        {\n            ""forced"": True,\n            ""value"": [""hello""],\n            ""simplified"": (CODE[list], ((CODE[str], (b""hello"",)),)),  # item\n        },\n    ]\n\n\n# Tuple.\ndef make_tuple(**kwargs):\n    return [\n        {\n            ""value"": (""hello"", ""world""),\n            ""simplified"": (CODE[tuple], ((CODE[str], (b""hello"",)), (CODE[str], (b""world"",)))),\n        },\n        {""value"": (""hello"",), ""simplified"": (CODE[tuple], ((CODE[str], (b""hello"",)),))},\n        {""value"": (), ""simplified"": (CODE[tuple], ())},\n    ]\n\n\n# Set.\ndef make_set(**kwargs):\n    def compare_simplified(actual, expected):\n        """"""When set is simplified and converted to tuple, elements order in tuple is random\n        We compare tuples as sets because the set order is undefined""""""\n        assert actual[0] == expected[0]\n        assert set(actual[1]) == set(expected[1])\n        return True\n\n    return [\n        {\n            ""value"": {""hello"", ""world""},\n            ""simplified"": (CODE[set], ((CODE[str], (b""world"",)), (CODE[str], (b""hello"",)))),\n            ""cmp_simplified"": compare_simplified,\n        },\n        {""value"": {""hello""}, ""simplified"": (CODE[set], ((CODE[str], (b""hello"",)),))},\n        {""value"": set(), ""simplified"": (CODE[set], ())},\n    ]\n\n\n# Slice.\ndef make_slice(**kwargs):\n    return [\n        {""value"": slice(10, 20, 30), ""simplified"": (CODE[slice], (10, 20, 30))},\n        {""value"": slice(10, 20), ""simplified"": (CODE[slice], (10, 20, None))},\n        {""value"": slice(10), ""simplified"": (CODE[slice], (None, 10, None))},\n    ]\n\n\n# Range.\ndef make_range(**kwargs):\n    return [\n        {""value"": range(1, 3, 4), ""simplified"": (CODE[range], (1, 3, 4))},\n        {""value"": range(1, 3), ""simplified"": (CODE[range], (1, 3, 1))},\n    ]\n\n\n# String.\ndef make_str(**kwargs):\n    return [\n        {""value"": ""a string"", ""simplified"": (CODE[str], (b""a string"",))},\n        {""value"": """", ""simplified"": (CODE[str], (b"""",))},\n    ]\n\n\n# Int.\ndef make_int(**kwargs):\n    return [\n        {""value"": 5, ""simplified"": 5},\n        # Tests that forced full simplify should return just simplified\n        # object if it doesn\'t have full simplifier\n        {""forced"": True, ""value"": 5, ""simplified"": 5},\n    ]\n\n\n# Float.\ndef make_float(**kwargs):\n    return [{""value"": 5.1, ""simplified"": 5.1}]\n\n\n# Ellipsis.\ndef make_ellipsis(**kwargs):\n    return [{""value"": ..., ""simplified"": (CODE[type(Ellipsis)], (b"""",))}]\n\n\n########################################################################\n# Numpy.\n########################################################################\n\n# numpy.ndarray\ndef make_numpy_ndarray(**kwargs):\n    np_array = numpy.random.random((2, 2))\n\n    def compare(detailed, original):\n        """"""Compare numpy arrays""""""\n        assert numpy.array_equal(detailed, original)\n        return True\n\n    return [\n        {\n            ""value"": np_array,\n            ""simplified"": (\n                CODE[type(np_array)],\n                (\n                    np_array.tobytes(),  # (bytes) serialized bin\n                    (CODE[tuple], (2, 2)),  # (tuple) shape\n                    (CODE[str], (b""float64"",)),  # (str) dtype.name\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# numpy.float32, numpy.float64, numpy.int32, numpy.int64\ndef make_numpy_number(dtype, **kwargs):\n    num = numpy.array([2.2], dtype=dtype)[0]\n    return [\n        {\n            ""value"": num,\n            ""simplified"": (\n                CODE[dtype],\n                (\n                    num.tobytes(),  # (bytes)\n                    (CODE[str], (num.dtype.name.encode(""utf-8""),)),  # (str) dtype.name\n                ),\n            ),\n        }\n    ]\n\n\n########################################################################\n# PyTorch.\n########################################################################\n\n# Utility functions.\n\n\ndef compare_modules(detailed, original):\n    """"""Compare ScriptModule instances""""""\n    input = torch.randn(10, 3)\n    # NOTE: after serde TopLevelTracedModule or jit.ScriptFunction become\n    # ScriptModule (that\'s what torch.jit.load returns in detail function)\n    assert isinstance(detailed, torch.jit.ScriptModule)\n    # Code changes after torch.jit.load(): function becomes `forward` method\n    if type(original) != torch.jit.ScriptFunction:\n        assert detailed.code == original.code\n    # model outputs match\n    assert detailed(input).equal(original(input))\n    return True\n\n\ndef save_to_buffer(tensor) -> bin:\n    """"""Serializes a pytorch tensor to binary""""""\n    binary_stream = io.BytesIO()\n    torch.save(tensor, binary_stream)\n    return binary_stream.getvalue()\n\n\n# torch.device\ndef make_torch_device(**kwargs):\n    torch_device = torch.device(""cpu"")\n    return [\n        {\n            ""value"": torch_device,\n            ""simplified"": (CODE[type(torch_device)], ((CODE[str], (b""cpu"",)),)),  # (str) device\n        }\n    ]\n\n\n# torch.dtype\ndef make_torch_dtype(**kwargs):\n    torch_dtype = torch.int32\n    return [\n        {""value"": torch_dtype, ""simplified"": (CODE[type(torch_dtype)], ""int32"")}  # (str) device\n    ]\n\n\n# torch.jit.ScriptModule\ndef make_torch_scriptmodule(**kwargs):\n    class ScriptModule(torch.jit.ScriptModule):\n        def __init__(self):\n            super(ScriptModule, self).__init__()\n\n        @torch.jit.script_method\n        def forward(self, x):  # pragma: no cover\n            return x + 2\n\n    sm = ScriptModule()\n    return [\n        {\n            ""value"": sm,\n            ""simplified"": (\n                CODE[torch.jit.ScriptModule],\n                (sm.save_to_buffer(),),  # (bytes) serialized torchscript\n            ),\n            ""cmp_detailed"": compare_modules,\n        }\n    ]\n\n\n# torch.jit.ScriptFunction\ndef make_torch_scriptfunction(**kwargs):\n    @torch.jit.script\n    def func(x):  # pragma: no cover\n        return x + 2\n\n    return [\n        {\n            ""value"": func,\n            ""simplified"": (\n                CODE[torch.jit.ScriptFunction],\n                (func.save_to_buffer(),),  # (bytes) serialized torchscript\n            ),\n            ""cmp_detailed"": compare_modules,\n        }\n    ]\n\n\n# torch.memory_format\ndef make_torch_memoryformat(**kwargs):\n    memory_format = torch.preserve_format\n\n    return [{""value"": memory_format, ""simplified"": (CODE[torch.memory_format], 3)}]\n\n\n# torch.jit.TopLevelTracedModule\n# NOTE: if the model is created inside the function, it will be serialized differently\n# depending on the context\nclass TopLevelTraceModel(torch.nn.Module):\n    def __init__(self):\n        super(TopLevelTraceModel, self).__init__()\n        self.w1 = torch.nn.Parameter(torch.randn(3, 1), requires_grad=True)\n        self.b1 = torch.nn.Parameter(torch.randn(1), requires_grad=True)\n\n    def forward(self, x):\n        x = x @ self.w1 + self.b1\n        return x\n\n\ndef make_torch_topleveltracedmodule(**kwargs):\n    tm = torch.jit.trace(TopLevelTraceModel(), torch.randn(10, 3))\n\n    return [\n        {\n            ""value"": tm,\n            ""simplified"": (\n                CODE[torch.jit.TopLevelTracedModule],\n                (tm.save_to_buffer(),),  # (bytes) serialized torchscript\n            ),\n            ""cmp_detailed"": compare_modules,\n        }\n    ]\n\n\n# torch.nn.parameter.Parameter\ndef make_torch_parameter(**kwargs):\n    param = torch.nn.Parameter(torch.randn(3, 3), requires_grad=True)\n\n    def compare(detailed, original):\n        assert type(detailed) == torch.nn.Parameter\n        assert detailed.data.equal(original.data)\n        assert detailed.id == original.id\n        assert detailed.requires_grad == original.requires_grad\n        return True\n\n    return [\n        {\n            ""value"": param,\n            ""simplified"": (\n                CODE[torch.nn.Parameter],\n                (\n                    param.id,  # (int) id\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], param.data\n                    ),  # (Tensor) data\n                    param.requires_grad,  # (bool) requires_grad\n                    None,\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# torch.Tensor\ndef make_torch_tensor(**kwargs):\n    tensor = torch.randn(3, 3)\n    tensor.tag(""tag1"")\n    tensor.describe(""desc"")\n\n    def compare(detailed, original):\n        assert type(detailed) == torch.Tensor\n        assert detailed.data.equal(original.data)\n        assert detailed.id == original.id\n        assert detailed.requires_grad == original.requires_grad\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        return True\n\n    return [\n        # Default pytorch tensor serialization strategy\n        {\n            ""value"": tensor,\n            ""simplified"": (\n                CODE[torch.Tensor],\n                (\n                    tensor.id,  # (int) id\n                    save_to_buffer(tensor),  # (bytes) serialized tensor\n                    None,  # (AbstractTensor) chain\n                    None,  # (AbstractTensor) grad_chain\n                    (CODE[set], ((CODE[str], (b""tag1"",)),)),  # (set of str) tags\n                    (CODE[str], (b""desc"",)),  # (str) description\n                    (CODE[str], (b""torch"",)),  # (str) framework\n                    None,  # (int) origin\n                    None,  # (int) id_at_origin\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n        # ""All"" tensor serialization strategy\n        {\n            ""framework"": None,\n            ""value"": tensor,\n            ""simplified"": (\n                CODE[torch.Tensor],\n                (\n                    tensor.id,  # (int) id\n                    (\n                        CODE[tuple],\n                        (  # serialized tensor\n                            (CODE[tuple], (3, 3)),  # tensor.shape\n                            (CODE[str], (b""float32"",)),  # tensor.dtype\n                            (\n                                CODE[list],\n                                tuple(tensor.flatten().tolist()),\n                            ),  # tensor contents as flat list\n                        ),\n                    ),\n                    None,  # (AbstractTensor) chain\n                    None,  # (AbstractTensor) grad_chain\n                    (CODE[set], ((CODE[str], (b""tag1"",)),)),  # (set of str) tags\n                    (CODE[str], (b""desc"",)),  # (str) description\n                    (CODE[str], (b""all"",)),  # (str) framework\n                    None,  # (int) origin\n                    None,  # (int) id_at_origin\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n    ]\n\n\n# torch.Size\ndef make_torch_size(**kwargs):\n    return [\n        {\n            ""value"": torch.randn(3, 3).size(),\n            ""simplified"": (CODE[torch.Size], (3, 3)),  # (int) *shape\n        }\n    ]\n\n\n########################################################################\n# PySyft.\n########################################################################\n\n# Utility functions\n\n\ndef compare_actions(detailed, original):\n    """"""Compare 2 Actions""""""\n    assert len(detailed) == len(original)\n    for original_op, detailed_op in zip(original, detailed):\n        for original_arg, detailed_arg in zip(original_op.args, detailed_op.args):\n            assert original_arg == detailed_arg\n        for original_return, detailed_return in zip(original_op.return_ids, detailed_op.return_ids):\n            assert original_return == detailed_return\n        assert original_op.name == detailed_op.name\n        assert original_op.kwargs == detailed_op.kwargs\n    return True\n\n\ndef compare_placeholders_list(detailed, original):\n    """"""Compare 2 lists of placeholders""""""\n    assert len(detailed) == len(original)\n    for original_ph, detailed_ph in zip(original, detailed):\n        assert detailed_ph.id == original_ph.id\n        assert detailed_ph.tags == original_ph.tags\n        assert detailed_ph.description == original_ph.description\n        assert detailed_ph.expected_shape == original_ph.expected_shape\n    return True\n\n\ndef compare_placeholders_dict(detailed, original):\n    """"""Compare 2 dicts of placeholders""""""\n    assert len(detailed) == len(original)\n    for key, detailed_ph in detailed.items():\n        original_ph = original[key]\n        assert detailed_ph.id == original_ph.id\n        assert detailed_ph.tags == original_ph.tags\n        assert detailed_ph.description == original_ph.description\n        assert detailed_ph.expected_shape == original_ph.expected_shape\n    return True\n\n\ndef compare_roles(detailed, original):\n    """"""Compare 2 Roles""""""\n    assert detailed.id == original.id\n    compare_actions(detailed.actions, original.actions)\n    compare_placeholders_list(detailed.state.state_placeholders, original.state.state_placeholders)\n    compare_placeholders_dict(detailed.placeholders, original.placeholders)\n    assert detailed.input_placeholder_ids == original.input_placeholder_ids\n    assert detailed.output_placeholder_ids == original.output_placeholder_ids\n    return True\n\n\n# AdditiveSharingTensor\ndef make_additivesharingtensor(**kwargs):\n    workers = kwargs[""workers""]\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    tensor = torch.tensor([[3.1, 4.3]]).fix_prec().share(alice, bob, crypto_provider=james)\n    ast = tensor.child.child\n\n    def compare(detailed, original):\n        assert (\n            type(detailed)\n            == syft.frameworks.torch.tensors.interpreters.additive_shared.AdditiveSharingTensor\n        )\n        assert detailed.id == original.id\n        assert detailed.field == original.field\n        assert detailed.child.keys() == original.child.keys()\n        return True\n\n    return [\n        {\n            ""value"": ast,\n            ""simplified"": (\n                CODE[\n                    syft.frameworks.torch.tensors.interpreters.additive_shared.AdditiveSharingTensor\n                ],\n                (\n                    ast.id,  # (int or str) id\n                    (CODE[str], (str(ast.field).encode(""utf-8""),))\n                    if ast.field == 2 ** 64\n                    else ast.field,  # (int or str) field\n                    ast.dtype.encode(""utf-8""),\n                    (CODE[str], (ast.crypto_provider.id.encode(""utf-8""),)),  # (str) worker_id\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], ast.child\n                    ),  # (dict of AbstractTensor) simplified chain\n                    ast.get_garbage_collect_data(),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# FixedPrecisionTensor\ndef make_fixedprecisiontensor(**kwargs):\n    workers = kwargs[""workers""]\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    t = torch.tensor([[3.1, 4.3]])\n    fpt_tensor = t.fix_prec(base=12, precision_fractional=5).share(\n        alice, bob, crypto_provider=james\n    )\n    fpt = fpt_tensor.child\n    fpt.tag(""tag1"")\n    fpt.describe(""desc"")\n    # AdditiveSharingTensor.simplify sets garbage_collect_data=False on child tensors\n    # during simplify\n    # This changes tensors\' internal state in chain and is required to pass the test\n    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], fpt)\n\n    def compare(detailed, original):\n        assert (\n            type(detailed)\n            == syft.frameworks.torch.tensors.interpreters.precision.FixedPrecisionTensor\n        )\n        assert detailed.id == original.id\n        assert detailed.field == original.field\n        assert detailed.base == original.base\n        assert detailed.precision_fractional == original.precision_fractional\n        assert detailed.kappa == original.kappa\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        return True\n\n    return [\n        {\n            ""value"": fpt,\n            ""simplified"": (\n                CODE[syft.frameworks.torch.tensors.interpreters.precision.FixedPrecisionTensor],\n                (\n                    fpt.id,  # (int or str) id\n                    (CODE[str], (str(fpt.field).encode(""utf-8""),))\n                    if fpt.field == 2 ** 64\n                    else fpt.field,  # (int or str) field\n                    fpt.dtype,  # (str) dtype\n                    12,  # (int) base\n                    5,  # (int) precision_fractional\n                    fpt.kappa,  # (int) kappa\n                    (CODE[set], ((CODE[str], (b""tag1"",)),)),  # (set of str) tags\n                    (CODE[str], (b""desc"",)),  # (str) description\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], fpt.child\n                    ),  # (AbstractTensor) chain\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# LoggingTensor\ndef make_loggingtensor(**kwargs):\n    t = torch.randn(3, 3)\n    lt = syft.frameworks.torch.tensors.decorators.logging.LoggingTensor().on(t).child\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.frameworks.torch.tensors.decorators.logging.LoggingTensor\n        assert detailed.id == original.id\n        assert detailed.child.equal(original.child)\n        return True\n\n    return [\n        {\n            ""value"": lt,\n            ""simplified"": (\n                CODE[syft.frameworks.torch.tensors.decorators.logging.LoggingTensor],\n                (\n                    lt.id,  # (int or str) id\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], lt.child\n                    ),  # (AbstractTensor) chain\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.execution.placeholder_id.PlaceholderId\ndef make_placeholder_id(**kwargs):\n    p = syft.execution.placeholder.PlaceHolder()\n    obj_id = p.id\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.placeholder_id.PlaceholderId\n        assert detailed.value == original.value\n        return True\n\n    return [\n        {\n            ""value"": obj_id,\n            ""simplified"": (CODE[syft.execution.placeholder_id.PlaceholderId], (obj_id.value,)),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.generic.pointers.multi_pointer.MultiPointerTensor\ndef make_multipointertensor(**kwargs):\n    workers = kwargs[""workers""]\n    alice, bob = workers[""alice""], workers[""bob""]\n    t = torch.randn(3, 3)\n    mpt = t.send(alice, bob).child\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.generic.pointers.multi_pointer.MultiPointerTensor\n        assert detailed.id == original.id\n        assert detailed.child.keys() == original.child.keys()\n        return True\n\n    return [\n        {\n            ""value"": mpt,\n            ""simplified"": (\n                CODE[syft.generic.pointers.multi_pointer.MultiPointerTensor],\n                (\n                    mpt.id,  # (int or str) id\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], mpt.child),  # (dict)\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.frameworks.torch.fl.dataset\ndef make_basedataset(**kwargs):\n    workers = kwargs[""workers""]\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    dataset = syft.BaseDataset(torch.tensor([1, 2, 3, 4]), torch.tensor([5, 6, 7, 8]))\n    dataset.tag(""#tag1"").describe(""desc"")\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.BaseDataset\n        assert (detailed.data == original.data).all()\n        assert (detailed.targets == original.targets).all()\n        assert detailed.id == original.id\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        return True\n\n    return [\n        {\n            ""value"": dataset,\n            ""simplified"": (\n                CODE[syft.frameworks.torch.fl.dataset.BaseDataset],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], dataset.data),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], dataset.targets),\n                    dataset.id,\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], dataset.tags\n                    ),  # (set of str) tags\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], dataset.description\n                    ),  # (str) description\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], dataset.child),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.generic.pointers.pointer_dataset.PointerDataset\ndef make_pointerdataset(**kwargs):\n    alice, me = kwargs[""workers""][""alice""], kwargs[""workers""][""me""]\n    data = torch.tensor([1, 2, 3, 4])\n    targets = torch.tensor([5, 6, 7, 8])\n    dataset = syft.BaseDataset(data, targets).tag(""#test"")\n    dataset.send(alice)\n    ptr = me.request_search([""#test""], location=alice)[0]\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.generic.pointers.pointer_dataset.PointerDataset\n        assert detailed.id == original.id\n        assert detailed.id_at_location == original.id_at_location\n        assert detailed.location == original.location\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        assert detailed.garbage_collect_data == original.garbage_collect_data\n        return True\n\n    return [\n        {\n            ""value"": ptr,\n            ""simplified"": (\n                CODE[syft.generic.pointers.pointer_dataset.PointerDataset],\n                (\n                    ptr.id,  # (int) id\n                    ptr.id_at_location,  # (int) id_at_location\n                    (CODE[str], (b""alice"",)),  # (str) worker_id\n                    (CODE[set], ((CODE[str], (b""#test"",)),)),  # (set or None) tags\n                    None,  # description\n                    False,  # (bool) garbage_collect_data\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.execution.plan.Plan\ndef make_plan(**kwargs):\n    # Function to plan\n    @syft.func2plan([torch.Size((3,))])\n    def plan(x):\n        x = x + x\n        y = torch.abs(x)\n        return x\n\n    # Model to plan\n    class Net(syft.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = torch.nn.Linear(3, 3)\n            self.fc2 = torch.nn.Linear(3, 2)\n\n        def forward(self, x):\n            x = torch.nn.functional.relu(self.fc1(x))\n            x = self.fc2(x)\n            return torch.nn.functional.log_softmax(x, dim=0)\n\n    with kwargs[""workers""][""serde_worker""].registration_enabled():\n        model_plan = Net()\n        model_plan.build(torch.tensor([1.0, 2.0, 3.0]))\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.plan.Plan\n        assert detailed.id == original.id\n        compare_roles(detailed.role, original.role)\n        assert detailed.include_state == original.include_state\n        assert detailed.is_built == original.is_built\n        assert detailed.name == original.name\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        with kwargs[""workers""][""serde_worker""].registration_enabled():\n            t = torch.tensor([1.1, -2, 3])\n            res1 = detailed(t)\n            res2 = original(t)\n        assert res1.equal(res2)\n        return True\n\n    return [\n        {\n            ""value"": plan,\n            ""simplified"": (\n                CODE[syft.execution.plan.Plan],\n                (\n                    plan.id,  # (int or str) id\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], plan.role),\n                    plan.include_state,\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], plan.name),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], plan.tags),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], plan.description),\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], plan.torchscript\n                    ),  # Torchscript\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], plan.input_types),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n        {\n            ""value"": model_plan,\n            ""simplified"": (\n                CODE[syft.execution.plan.Plan],\n                (\n                    model_plan.id,  # (int or str) id\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], model_plan.role),\n                    model_plan.include_state,\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], model_plan.name),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], model_plan.tags),\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], model_plan.description\n                    ),\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], model_plan.torchscript\n                    ),  # Torchscript\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], model_plan.input_types\n                    ),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n    ]\n\n\n# Role\ndef make_role(**kwargs):\n    @syft.func2plan(args_shape=[(1,)], state=(torch.tensor([1.0]),))\n    def plan_abs(x, state):\n        (bias,) = state.read()\n        x = x.abs()\n        return x + bias\n\n    plan_abs.build(torch.tensor([3.0]))\n    role = plan_abs.role\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.role.Role\n        compare_roles(detailed, original)\n        return True\n\n    return [\n        {\n            ""value"": role,\n            ""simplified"": (\n                CODE[syft.execution.role.Role],\n                (\n                    role.id,\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], role.actions),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], role.state),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], role.placeholders),\n                    role.input_placeholder_ids,\n                    role.output_placeholder_ids,\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\ndef make_type(**kwargs):\n    serialized_type = type(""test"")\n\n    def compare(detailed, original):\n        assert type(detailed) == type(original)\n        assert detailed == original\n        return True\n\n    return [\n        {\n            ""value"": serialized_type,\n            ""simplified"": (msgpack.serde._simplify(syft.hook.local_worker, serialized_type)),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\ndef make_nested_type_wrapper(**kwargs):\n    reference_serialized_input = (\n        (type(torch.tensor([1.0, -2.0])), type(torch.tensor([1, 2]))),\n        {\n            ""k1"": [type(5), (type(True), type(False))],\n            ""k2"": {\n                ""kk1"": [type(torch.tensor([5, 7])), type(torch.tensor([5, 7]))],\n                ""kk2"": [type(True), (type(torch.tensor([9, 10])),)],\n            },\n            ""k3"": type(torch.tensor([8])),\n        },\n        type(torch.tensor([11, 12])),\n        (type(1), (type(2), (type(3), (type(4), [type(5), type(6)])))),\n    )\n\n    wrapper = syft.execution.plan.NestedTypeWrapper()\n    wrapper.nested_input_types = reference_serialized_input\n\n    def compare(detailed, original):\n        assert detailed.nested_input_types == original.nested_input_types\n        return True\n\n    return [\n        {\n            ""value"": wrapper,\n            ""simplified"": syft.serde.msgpack.serde._simplify(syft.hook.local_worker, wrapper),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# State\ndef make_state(**kwargs):\n    me = kwargs[""workers""][""me""]\n\n    t1, t2 = torch.randn(3, 3), torch.randn(3, 3)\n    p1, p2 = syft.PlaceHolder(), syft.PlaceHolder()\n    p1.tag(""state1""), p2.tag(""state2"")\n    p1.instantiate(t1), p2.instantiate(t2)\n    state = syft.execution.state.State(state_placeholders=[p1, p2])\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.state.State\n        compare_placeholders_list(detailed.state_placeholders, original.state_placeholders)\n        for i in range(len(original.tensors())):\n            assert detailed.tensors()[i].equal(original.tensors()[i])\n        return True\n\n    return [\n        {\n            ""value"": state,\n            ""simplified"": (\n                CODE[syft.execution.state.State],\n                (\n                    (\n                        CODE[list],\n                        (  # (list) state_placeholders\n                            msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], p1),\n                            msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], p2),\n                        ),\n                    ),\n                    (\n                        CODE[list],\n                        (  # (list) tensors\n                            msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], t1),\n                            msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], t2),\n                        ),\n                    ),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# Protocol\ndef make_protocol(**kwargs):\n    alice = kwargs[""workers""][""alice""]\n    bob = kwargs[""workers""][""bob""]\n\n    @syft.func2protocol(roles=[""alice"", ""bob""], args_shape={""alice"": ((1,),), ""bob"": ((1,),)})\n    def protocol(alice, bob):\n        tensor1 = alice.load(torch.tensor([1]))\n        tensor2 = bob.load(torch.tensor([1]))\n\n        t1plus = tensor1 + 1\n        t2plus = tensor2 + 1\n\n        return t1plus, t2plus\n\n    protocol.build()\n\n    # plan.owner = worker\n    protocol.tag(""aaa"")\n    protocol.describe(""desc"")\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.protocol.Protocol\n        assert detailed.id == original.id\n        assert detailed.name == original.name\n        assert detailed.roles.keys() == original.roles.keys()\n        for k, v in detailed.roles.items():\n            assert compare_roles(original.roles[k], v)\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        return True\n\n    return [\n        {\n            ""value"": protocol,\n            ""simplified"": (\n                CODE[syft.execution.protocol.Protocol],\n                (\n                    protocol.id,  # (int or str) id\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], protocol.name),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], protocol.roles),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], protocol.tags),\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], protocol.description\n                    ),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# Protocol\ndef make_role_assignments(**kwargs):\n    alice = kwargs[""workers""][""alice""]\n    bob = kwargs[""workers""][""bob""]\n\n    role_assignments = syft.execution.role_assignments.RoleAssignments(\n        {""role1"": alice, ""role2"": bob}\n    )\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.role_assignments.RoleAssignments\n        assert detailed.assignments == original.assignments\n        return True\n\n    return [\n        {\n            ""value"": role_assignments,\n            ""simplified"": (\n                CODE[syft.execution.role_assignments.RoleAssignments],\n                (\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], role_assignments.assignments\n                    ),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.generic.pointers.pointer_tensor.PointerTensor\ndef make_pointertensor(**kwargs):\n    alice = kwargs[""workers""][""alice""]\n    tensor = torch.randn(3, 3)\n    ptr = tensor.send(alice).child\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.generic.pointers.pointer_tensor.PointerTensor\n        assert detailed.id == original.id\n        assert detailed.id_at_location == original.id_at_location\n        assert detailed.location == original.location\n        assert detailed.point_to_attr == original.point_to_attr\n        # Not testing grabage collect data as we are always setting it as False at receiver end\n        # irrespective of its initial value\n        assert detailed.garbage_collect_data == original.garbage_collect_data\n        assert detailed.get().equal(tensor)\n        return True\n\n    return [\n        {\n            ""value"": ptr,\n            ""simplified"": (\n                CODE[syft.generic.pointers.pointer_tensor.PointerTensor],\n                (\n                    ptr.id,  # (int or str) id\n                    ptr.id_at_location,  # (int or str) id_at_location\n                    (CODE[str], (b""alice"",)),  # (str) worker_id\n                    None,  # (str) point_to_attr\n                    (CODE[torch.Size], (3, 3)),  # (torch.Size) _shape\n                    True,  # (bool) garbage_collect_data\n                    ptr.tags,\n                    ptr.description,\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.generic.pointers.pointer_plan.PointerPlan\ndef make_pointerplan(**kwargs):\n    alice, me = kwargs[""workers""][""alice""], kwargs[""workers""][""me""]\n\n    @syft.func2plan([torch.Size((1, 3))])\n    def plan(x):\n        x = x + x\n        x = torch.abs(x)\n        return x\n\n    plan.send(alice)\n    ptr = me.request_search([plan.id], location=alice)[0]\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.generic.pointers.pointer_plan.PointerPlan\n        assert detailed.id == original.id\n        assert detailed.id_at_location == original.id_at_location\n        assert detailed.location == original.location\n        assert detailed.garbage_collect_data == original.garbage_collect_data\n        # execute\n        t = torch.randn(3, 3).send(alice)\n        assert detailed(t).get().equal(original(t).get())\n        return True\n\n    return [\n        {\n            ""value"": ptr,\n            ""simplified"": (\n                CODE[syft.generic.pointers.pointer_plan.PointerPlan],\n                (\n                    ptr.id,  # (int) id\n                    ptr.id_at_location,  # (int) id_at_location\n                    (CODE[str], (b""alice"",)),  # (str) worker_id\n                    (CODE[set], ()),  # (set or None) tags\n                    False,  # (bool) garbage_collect_data\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.generic.pointers.object_wrapper.ObjectWrapper\ndef make_objectwrapper(**kwargs):\n    obj = torch.randn(3, 3)\n    wrapper = syft.generic.pointers.object_wrapper.ObjectWrapper(obj, id=123)\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.generic.pointers.object_wrapper.ObjectWrapper\n        assert detailed.id == original.id\n        # tensors\n        assert detailed.obj.equal(original.obj)\n        return True\n\n    return [\n        {\n            ""value"": wrapper,\n            ""simplified"": (\n                CODE[syft.generic.pointers.object_wrapper.ObjectWrapper],\n                (\n                    123,  # (int) id\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], obj),  # (Any) obj\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.generic.pointers.object_pointer.ObjectPointer\ndef make_objectpointer(**kwargs):\n    alice = kwargs[""workers""][""alice""]\n    obj = torch.randn(3, 3)\n    obj_ptr = obj.send(alice)\n    ptr = syft.generic.pointers.object_pointer.ObjectPointer.create_pointer(obj, alice, obj.id)\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.generic.pointers.object_pointer.ObjectPointer\n        assert detailed.id == original.id\n        assert detailed.id_at_location == original.id_at_location\n        assert detailed.location == original.location\n        assert detailed.point_to_attr == original.point_to_attr\n        assert detailed.garbage_collect_data == original.garbage_collect_data\n        return True\n\n    return [\n        {\n            ""value"": ptr,\n            ""simplified"": (\n                CODE[syft.generic.pointers.object_pointer.ObjectPointer],\n                (\n                    ptr.id,  # (int or str) id\n                    ptr.id_at_location,  # (int or str) id\n                    (CODE[str], (b""alice"",)),  # (str) location.id\n                    None,  # (str) point_to_attr\n                    True,  # (bool) garbage_collect_data\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.generic.string.String\ndef make_string(**kwargs):\n    def compare_simplified(actual, expected):\n        """"""This is a custom comparison function.\n        The reason for using this is that when set is that tags are use. Tags are sets.\n        When sets are simplified and converted to tuple, elements order in tuple is random\n        We compare tuples as sets because the set order is undefined.\n\n        This function is inspired by the one with the same name defined above in `make_set`.\n        """"""\n        assert actual[0] == expected[0]\n        assert actual[1][0] == expected[1][0]\n        assert actual[1][1] == expected[1][1]\n        assert actual[1][2][0] == expected[1][2][0]\n        assert set(actual[1][2][1]) == set(expected[1][2][1])\n        assert actual[1][3] == expected[1][3]\n        return True\n\n    return [\n        {\n            ""value"": syft.generic.string.String(\n                ""Hello World"", id=1234, tags={""tag1"", ""tag2""}, description=""description""\n            ),\n            ""simplified"": (\n                CODE[syft.generic.string.String],\n                (\n                    (CODE[str], (b""Hello World"",)),\n                    1234,\n                    (CODE[set], ((CODE[str], (b""tag1"",)), (CODE[str], (b""tag2"",)))),\n                    (CODE[str], (b""description"",)),\n                ),\n            ),\n            ""cmp_simplified"": compare_simplified,\n        }\n    ]\n\n\n# syft.workers.virtual.VirtualWorker\ndef make_virtual_worker(**kwargs):\n    worker = VirtualWorker(\n        id=f""serde-worker-{cls.__name__}"",\n        hook=kwargs[""workers""][""serde_worker""].hook,\n        auto_add=False,\n    )\n\n    t = torch.rand(3, 3)\n    with worker.registration_enabled():\n        worker.register_obj(t)\n\n    def compare(detailed, original):\n        assert isinstance(detailed, syft.workers.virtual.VirtualWorker)\n        assert detailed.id == original.id\n        return True\n\n    return [\n        {\n            ""value"": worker,\n            ""simplified"": (\n                CODE[syft.workers.virtual.VirtualWorker],\n                ((CODE[str], (b""serde-worker-VirtualWorker"",)),),  # id (str)\n            ),\n            ""cmp_detailed"": compare,\n        },\n        # Forced simplification\n        {\n            ""forced"": True,\n            ""value"": worker,\n            ""simplified"": (\n                FORCED_CODE[syft.workers.virtual.VirtualWorker],\n                (\n                    (CODE[str], (b""serde-worker-VirtualWorker"",)),  # id (str)\n                    msgpack.serde._simplify(\n                        worker, worker.object_store._objects\n                    ),  # (dict) _objects\n                    worker.auto_add,  # (bool) auto_add\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n    ]\n\n\n# syft.frameworks.torch.tensors.interpreters.autograd.AutogradTensor\ndef make_autogradtensor(**kwargs):\n\n    t = torch.tensor([1, 2, 3])\n    agt = (\n        syft.frameworks.torch.tensors.interpreters.autograd.AutogradTensor(\n            owner=kwargs[""workers""][""serde_worker""]\n        )\n        .on(t)\n        .child\n    )\n    agt.tag(""aaa"")\n    agt.describe(""desc"")\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.frameworks.torch.tensors.interpreters.autograd.AutogradTensor\n        assert detailed.owner == original.owner\n        assert detailed.id == original.id\n        assert detailed.child.equal(original.child)\n        assert detailed.requires_grad == original.requires_grad\n        assert detailed.preinitialize_grad == original.preinitialize_grad\n        assert detailed.grad_fn == original.grad_fn\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        return True\n\n    return [\n        {\n            ""value"": agt,\n            ""simplified"": (\n                CODE[syft.frameworks.torch.tensors.interpreters.autograd.AutogradTensor],\n                (\n                    agt.id,  # (int)\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], agt.child\n                    ),  # (AbstractTensor) chain\n                    True,  # (bool) requires_grad\n                    False,  # (bool) preinitialize_grad\n                    None,  # [always None, ignored in constructor] grad_fn\n                    (CODE[set], ((CODE[str], (b""aaa"",)),)),  # (set of str) tags\n                    (CODE[str], (b""desc"",)),  # (str) description\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.frameworks.torch.tensors.interpreters.private.PrivateTensor\ndef make_privatetensor(**kwargs):\n    t = torch.tensor([1, 2, 3])\n    pt = t.private_tensor(allowed_users=[""test""])\n    pt.tag(""tag1"")\n    pt.describe(""private"")\n    pt = pt.child\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.frameworks.torch.tensors.interpreters.private.PrivateTensor\n        assert detailed.id == original.id\n        assert detailed.allowed_users == original.allowed_users\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        assert detailed.child.equal(original.child)\n        return True\n\n    return [\n        {\n            ""value"": pt,\n            ""simplified"": (\n                CODE[syft.frameworks.torch.tensors.interpreters.private.PrivateTensor],\n                (\n                    pt.id,  # (int or str) id\n                    (CODE[tuple], ((CODE[str], (b""test"",)),)),  # (tuple of ?) allowed_users\n                    (CODE[set], ((CODE[str], (b""tag1"",)),)),  # (set of str) tags\n                    (CODE[str], (b""private"",)),  # (str) description\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], t\n                    ),  # (AbstractTensor) chain\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.frameworks.torch.tensors.interpreters.PlaceHolder\ndef make_placeholder(**kwargs):\n    ph = syft.execution.placeholder.PlaceHolder(shape=torch.randn(3, 4).shape)\n    ph.tag(""tag1"")\n    ph.describe(""just a placeholder"")\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.placeholder.PlaceHolder\n        assert detailed.id == original.id\n        assert detailed.tags == original.tags\n        assert detailed.description == original.description\n        assert detailed.expected_shape == original.expected_shape\n        return True\n\n    return [\n        {\n            ""value"": ph,\n            ""simplified"": (\n                CODE[syft.execution.placeholder.PlaceHolder],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], ph.id),\n                    (CODE[set], ((CODE[str], (b""tag1"",)),)),  # (set of str) tags\n                    (CODE[str], (b""just a placeholder"",)),  # (str) description\n                    (CODE[tuple], (3, 4)),  # (tuple of int) expected_shape\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.execution.communication.CommunicationAction\ndef make_communication_action(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    alice = kwargs[""workers""][""alice""]\n    bob.log_msgs = True\n\n    x = torch.tensor([1, 2, 3, 4]).send(bob)\n    x.remote_send(alice)\n    com = bob._get_msg(-1).action\n\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.CommunicationAction\n\n        detailed_msg = (\n            detailed.name,\n            detailed.target,\n            detailed.args,\n            detailed.kwargs,\n            detailed.return_ids,\n            detailed.return_value,\n        )\n        original_msg = (\n            original.name,\n            original.target,\n            original.args,\n            original.kwargs,\n            original.return_ids,\n            original.return_value,\n        )\n\n        for i in range(len(original_msg)):\n            if type(original_msg[i]) != torch.Tensor:\n                assert detailed_msg[i] == original_msg[i], f""{detailed_msg[i]} != {original_msg[i]}""\n            else:\n                assert detailed_msg[i].equal(\n                    original_msg[i]\n                ), f""{detailed_msg[i]} != {original_msg[i]}""\n\n        return True\n\n    return [\n        {\n            ""value"": com,\n            ""simplified"": (\n                CODE[syft.execution.communication.CommunicationAction],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], com.name),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], com.target),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], com.args),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], com.kwargs),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], com.return_ids),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], com.return_value),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.execution.computation.ComputationAction\ndef make_computation_action(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    bob.log_msgs = True\n\n    x = torch.tensor([1, 2, 3, 4]).send(bob)\n    y = x * 2\n    op1 = bob._get_msg(-1).action\n\n    a = torch.tensor([[1, 2], [3, 4]]).send(bob)\n    b = a.sum(1, keepdim=True)\n    op2 = bob._get_msg(-1).action\n\n    c = torch.tensor([[1, 2], [3, 4]]).send(bob)\n    d = c.sum([0, 1], keepdim=True)\n    op3 = bob._get_msg(-1).action\n\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.execution.computation.ComputationAction\n\n        detailed_msg = (\n            detailed.name,\n            detailed.target,\n            detailed.args,\n            detailed.kwargs,\n            detailed.return_ids,\n            detailed.return_value,\n        )\n\n        original_msg = (\n            original.name,\n            original.target,\n            original.args,\n            original.kwargs,\n            original.return_ids,\n            original.return_value,\n        )\n\n        for i in range(len(original_msg)):\n            if type(original_msg[i]) != torch.Tensor:\n                assert detailed_msg[i] == original_msg[i], f""{detailed_msg[i]} != {original_msg[i]}""\n            else:\n                assert detailed_msg[i].equal(\n                    original_msg[i]\n                ), f""{detailed_msg[i]} != {original_msg[i]}""\n\n        return True\n\n    return [\n        {\n            ""value"": op1,\n            ""simplified"": (\n                CODE[syft.execution.computation.ComputationAction],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op1.name),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op1.target),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op1.args),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op1.kwargs),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op1.return_ids),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op1.return_value),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n        {\n            ""value"": op2,\n            ""simplified"": (\n                CODE[syft.execution.computation.ComputationAction],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op2.name),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op2.target),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op2.args),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op2.kwargs),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op2.return_ids),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op2.return_value),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n        {\n            ""value"": op3,\n            ""simplified"": (\n                CODE[syft.execution.computation.ComputationAction],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op3.name),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op3.target),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op3.args),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op3.kwargs),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op3.return_ids),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], op3.return_value),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        },\n    ]\n\n\n# syft.messaging.message.TensorCommandMessage\ndef make_tensor_command_message(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    alice = kwargs[""workers""][""alice""]\n    bob.log_msgs = True\n\n    x = torch.tensor([1, 2, 3, 4]).send(bob)\n    y = x * 2\n    cmd1 = bob._get_msg(-1)\n\n    a = torch.tensor([[1, 2], [3, 4]]).send(bob)\n    b = a.sum(1, keepdim=True)\n    cmd2 = bob._get_msg(-1)\n\n    x = torch.tensor([1, 2, 3, 4]).send(bob)\n    x.remote_send(alice)\n    cmd3 = bob._get_msg(-1)\n\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        detailed_action = detailed.action\n        original_action = original.action\n\n        detailed_action = (\n            detailed.name,\n            detailed.target,\n            detailed.args,\n            detailed.kwargs,\n            detailed.return_ids,\n            detailed.return_value,\n        )\n\n        original_action = (\n            original.name,\n            original.target,\n            original.args,\n            original.kwargs,\n            original.return_ids,\n            original.return_value,\n        )\n\n        for i in range(len(original_action)):\n            if type(original_action[i]) != torch.Tensor:\n                assert detailed_action[i] == original_action[i]\n            else:\n                assert detailed_action[i].equal(original_action[i])\n\n        return True\n\n    return [\n        {\n            ""value"": cmd1,\n            ""simplified"": (\n                CODE[syft.messaging.message.TensorCommandMessage],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], cmd1.action),\n                ),  # (Any) message\n            ),\n            ""cmp_detailed"": compare,\n        },\n        {\n            ""value"": cmd2,\n            ""simplified"": (\n                CODE[syft.messaging.message.TensorCommandMessage],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], cmd2.action),\n                ),  # (Any) message\n            ),\n            ""cmp_detailed"": compare,\n        },\n        {\n            ""value"": cmd3,\n            ""simplified"": (\n                CODE[syft.messaging.message.TensorCommandMessage],\n                (msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], cmd3.action),),\n            ),\n            ""cmp_detailed"": compare,\n        },\n    ]\n\n\n# syft.messaging.message.ObjectMessage\ndef make_objectmessage(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    bob.log_msgs = True\n    x = torch.tensor([1, 2, 3, 4]).send(bob)\n    obj = bob._get_msg(-1)\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.ObjectMessage\n        # torch tensors\n        assert detailed.object.equal(original.object)\n        return True\n\n    return [\n        {\n            ""value"": obj,\n            ""simplified"": (\n                CODE[syft.messaging.message.ObjectMessage],\n                (\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], obj.object\n                    ),  # (Any) simplified object\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# ObjectRequestMessage\ndef make_objectrequestmessage(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    bob.log_msgs = True\n    x = torch.tensor([1, 2, 3, 4]).send(bob)\n    x.get()\n    obj_req = bob._get_msg(-1)\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.ObjectRequestMessage\n        assert detailed.object_id == original.object_id\n        assert detailed.user == original.user\n        assert detailed.reason == original.reason\n        return True\n\n    return [\n        {\n            ""value"": obj_req,\n            ""simplified"": (\n                CODE[syft.messaging.message.ObjectRequestMessage],\n                (\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], obj_req.object_id),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], obj_req.user),\n                    msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], obj_req.reason),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# IsNoneMessage\ndef make_isnonemessage(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    bob.log_msgs = True\n    t = torch.tensor([1, 2, 3, 4])\n    x = t.send(bob)\n    x.child.is_none()\n    nm = bob._get_msg(-1)\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.IsNoneMessage\n        # torch tensors\n        assert detailed.object_id == original.object_id\n        return True\n\n    return [\n        {\n            ""value"": nm,\n            ""simplified"": (\n                CODE[syft.messaging.message.IsNoneMessage],\n                (msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], nm.object_id),),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# GetShapeMessage\ndef make_getshapemessage(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    bob.log_msgs = True\n    t = torch.tensor([1, 2, 3, 4])\n    x = t.send(bob)\n    z = x + x\n    s = z.shape\n    shape_message = bob._get_msg(-1)\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.GetShapeMessage\n        # torch tensor\n        assert detailed.tensor_id == original.tensor_id\n        return True\n\n    return [\n        {\n            ""value"": shape_message,\n            ""simplified"": (\n                CODE[syft.messaging.message.GetShapeMessage],\n                (\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], shape_message.tensor_id\n                    ),  # (Any) simplified tensor\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# ForceObjectDeleteMessage\ndef make_forceobjectdeletemessage(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    bob.log_msgs = True\n    t = torch.tensor([1, 2, 3, 4])\n    id = t.id\n    x = t.send(bob)\n    del x\n    del_message = bob._get_msg(-1)\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.ForceObjectDeleteMessage\n        assert detailed.object_id == original.object_id\n        return True\n\n    return [\n        {\n            ""value"": del_message,\n            ""simplified"": (\n                CODE[syft.messaging.message.ForceObjectDeleteMessage],\n                (id,),  # (int) id\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# SearchMessage\ndef make_searchmessage(**kwargs):\n    search_message = syft.messaging.message.SearchMessage([1, ""test"", 3])\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.SearchMessage\n        assert detailed.query == original.query\n        return True\n\n    return [\n        {\n            ""value"": search_message,\n            ""simplified"": (\n                CODE[syft.messaging.message.SearchMessage],\n                ((CODE[list], (1, (CODE[str], (b""test"",)), 3)),),  # (Any) message\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# PlanCommandMessage\ndef make_plancommandmessage(**kwargs):\n    bob = kwargs[""workers""][""bob""]\n    bob.log_msgs = True\n\n    @syft.func2plan(args_shape=[(1,)])\n    def plan(data):\n        return data * 3\n\n    plan.send(bob)\n    plan.owner.fetch_plan(plan.id, bob)\n    fetch_plan_cmd = bob._get_msg(-1)\n    bob.log_msgs = False\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.PlanCommandMessage\n        assert detailed.command_name == original.command_name\n        assert detailed.args == original.args\n        return True\n\n    return [\n        {\n            ""value"": fetch_plan_cmd,\n            ""simplified"": (\n                CODE[syft.messaging.message.PlanCommandMessage],\n                (\n                    (CODE[str], (b""fetch_plan"",)),  # (str) command\n                    (CODE[tuple], (plan.id, False)),  # (tuple) args\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# WorkerCommandMessage\ndef make_workercommandmessage(**kwargs):\n    server, remote_proxy = kwargs[""start_remote_worker""](\n        id=kwargs[""id""], hook=kwargs[""hook""], port=kwargs[""port""]\n    )\n\n    remote_proxy._log_msgs_remote(value=True)\n    nr_objects = remote_proxy.tensors_count_remote()\n    assert nr_objects == 0\n\n    objects_count_msg = remote_proxy._get_msg_remote(\n        index=-2\n    )  # index -2 as last message is _get_msg message\n\n    remote_proxy.close()\n    server.terminate()\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.messaging.message.WorkerCommandMessage\n        return True\n\n    return [\n        {\n            ""value"": objects_count_msg,\n            ""simplified"": (\n                CODE[syft.messaging.message.WorkerCommandMessage],\n                (\n                    (CODE[str], (b""tensors_count"",)),  # (str) command\n                    (CODE[tuple], ((CODE[tuple], ()), (CODE[dict], ()), (CODE[list], ()))),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.exceptions.GetNotPermittedError\ndef make_getnotpermittederror(**kwargs):\n    try:\n        raise syft.exceptions.GetNotPermittedError()\n    except syft.exceptions.GetNotPermittedError as e:\n        err = e\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.exceptions.GetNotPermittedError\n        assert (\n            traceback.format_tb(detailed.__traceback__)[-1]\n            == traceback.format_tb(original.__traceback__)[-1]\n        )\n        return True\n\n    return [\n        {\n            ""value"": err,\n            ""simplified"": (\n                CODE[syft.exceptions.GetNotPermittedError],\n                (\n                    (CODE[str], (b""GetNotPermittedError"",)),  # (str) __name__\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""],\n                        ""Traceback (most recent call last):\\n""\n                        + """".join(traceback.format_tb(err.__traceback__)),\n                    ),  # (str) traceback\n                    (CODE[dict], ()),  # (dict) attributes\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.exceptions.ResponseSignatureError\ndef make_responsesignatureerror(**kwargs):\n    try:\n        raise syft.exceptions.ResponseSignatureError()\n    except syft.exceptions.ResponseSignatureError as e:\n        err = e\n\n    def compare(detailed, original):\n        assert type(detailed) == syft.exceptions.ResponseSignatureError\n        assert (\n            traceback.format_tb(detailed.__traceback__)[-1]\n            == traceback.format_tb(original.__traceback__)[-1]\n        )\n        assert detailed.get_attributes() == original.get_attributes()\n        return True\n\n    return [\n        {\n            ""value"": err,\n            ""simplified"": (\n                CODE[syft.exceptions.ResponseSignatureError],\n                (\n                    (CODE[str], (b""ResponseSignatureError"",)),  # (str) __name__\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""],\n                        ""Traceback (most recent call last):\\n""\n                        + """".join(traceback.format_tb(err.__traceback__)),\n                    ),  # (str) traceback\n                    msgpack.serde._simplify(\n                        kwargs[""workers""][""serde_worker""], err.get_attributes()\n                    ),  # (dict) attributes\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\n# syft.frameworks.torch.tensors.interpreters.gradients_core.GradFunc\ndef make_gradfn(**kwargs):\n    alice, bob = kwargs[""workers""][""alice""], kwargs[""workers""][""bob""]\n    t = torch.tensor([1, 2, 3])\n\n    x_share = t.share(alice, bob, requires_grad=True)\n    y_share = t.share(alice, bob, requires_grad=True)\n    z_share = x_share + y_share  # AddBackward\n\n    # This is bad. We should find something robust\n    x_share.child.child.set_garbage_collect_data(False)\n    y_share.child.child.set_garbage_collect_data(False)\n\n    grad_fn = z_share.child.grad_fn\n\n    def compare(detailed, original):\n        assert isinstance(\n            detailed, syft.frameworks.torch.tensors.interpreters.gradients_core.GradFunc\n        )\n        assert detailed.__class__.__name__ == original.__class__.__name__\n\n        # This block only works only for syft tensor attributes\n        for detailed_attr, original_attr in zip(detailed._attributes, original._attributes):\n            assert detailed_attr.__class__.__name__ == original_attr.__class__.__name__\n            assert detailed_attr.get().equal(t)\n\n        return True\n\n    return [\n        {\n            ""value"": grad_fn,\n            ""simplified"": (\n                CODE[syft.frameworks.torch.tensors.interpreters.gradients_core.GradFunc],\n                (\n                    CODE[list],\n                    (\n                        (CODE[str], (b""AddBackward"",)),\n                        msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], x_share.child),\n                        msgpack.serde._simplify(kwargs[""workers""][""serde_worker""], y_share.child),\n                    ),\n                ),\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\ndef make_paillier(**kwargs):\n    # TODO: Add proper testing for paillier tensor\n\n    def compare(original, detailed):\n        return True\n\n    tensor = syft.frameworks.torch.tensors.interpreters.paillier.PaillierTensor()\n    simplfied = syft.frameworks.torch.tensors.interpreters.paillier.PaillierTensor.simplify(\n        kwargs[""workers""][""serde_worker""], tensor\n    )\n\n    return [\n        {\n            ""value"": tensor,\n            ""simplified"": (\n                CODE[syft.frameworks.torch.tensors.interpreters.paillier.PaillierTensor],\n                simplfied,\n            ),\n            ""cmp_detailed"": compare,\n        }\n    ]\n\n\ndef make_serializable_dummy_class(**kwargs):\n    def compare(simplified, detailed):\n        assert simplified.value == detailed.value\n        return True\n\n    obj = SerializableDummyClass(""test"")\n    simplified = SerializableDummyClass.simplify(kwargs[""workers""][""serde_worker""], obj)\n\n    return [\n        {\n            ""value"": obj,\n            ""simplified"": (SerializableDummyClass.get_msgpack_code()[""code""], simplified),\n            ""cmp_detailed"": compare,\n        }\n    ]\n'"
test/torch/__init__.py,0,b''
test/torch/test_federated_learning.py,9,"b'""""""All the tests relative to garbage collection of all kinds of remote or local tensors""""""\n\nimport syft as sy\nimport torch\nfrom torch import nn\nfrom torch import optim\n\nhook = sy.TorchHook(torch)\n\n\nclass TestFederatedLearning(object):\n    def setUp(self):\n        hook = sy.TorchHook(torch, verbose=True)\n\n        self.me = hook.local_worker\n        self.me.is_client_worker = True\n\n        instance_id = str(sy.ID_PROVIDER.pop())\n        bob = sy.VirtualWorker(id=f""bob{instance_id}"", hook=hook, is_client_worker=False)\n        alice = sy.VirtualWorker(id=f""alice{instance_id}"", hook=hook, is_client_worker=False)\n        james = sy.VirtualWorker(id=f""james{instance_id}"", hook=hook, is_client_worker=False)\n\n        bob.add_workers([alice, james])\n        alice.add_workers([bob, james])\n        james.add_workers([bob, alice])\n\n        self.hook = hook\n\n        self.bob = bob\n        self.alice = alice\n        self.james = james\n\n        # A Toy Dataset\n        data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]], requires_grad=True)\n        target = torch.tensor([[0], [0], [1], [1.0]], requires_grad=True)\n\n        # get pointers to training data on each worker by\n        # sending some training data to bob and alice\n        data_bob = data[0:2]\n        target_bob = target[0:2]\n\n        data_alice = data[2:]\n        target_alice = target[2:]\n\n        data_bob = data_bob.send(bob)\n        data_alice = data_alice.send(alice)\n        target_bob = target_bob.send(bob)\n        target_alice = target_alice.send(alice)\n\n        # organize pointers into a list\n        self.datasets = [(data_bob, target_bob), (data_alice, target_alice)]\n\n    # POINTERS\n\n    def test_toy_federated_learning(self):\n\n        self.setUp()\n\n        # Initialize A Toy Model\n        model = nn.Linear(2, 1)\n\n        # Training Logic\n        opt = optim.SGD(params=model.parameters(), lr=0.1)\n        for iter in range(20):\n\n            # NEW) iterate through each worker\'s dataset\n            for data, target in self.datasets:\n                # NEW) send model to correct worker\n                model.send(data.location)\n\n                # 1) erase previous gradients (if they exist)\n                opt.zero_grad()\n\n                # 2) make a prediction\n                pred = model(data)\n\n                # 3) calculate how much we missed\n                loss = ((pred - target) ** 2).sum()\n\n                # 4) figure out which weights caused us to miss\n                loss.backward()\n\n                # 5) change those weights\n                opt.step()\n\n                # get model (with gradients)\n                model.get()\n\n                # 6) print our progress\n                print(loss.get())  # NEW) slight edit... need to call .get() on loss\n\n\ndef test_lstm(workers):\n    bob = workers[""bob""]\n    lstm = nn.LSTM(3, 3)\n    inputs = torch.randn(5, 1, 3)\n    hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n    out, hidden = lstm(inputs, hidden)\n    assert out.shape == torch.Size([5, 1, 3])\n    lstm = nn.LSTM(3, 3)\n    lstm.send(bob)\n    inputs = torch.randn(5, 1, 3).send(bob)\n    hidden = (\n        torch.randn(1, 1, 3).send(bob),\n        torch.randn(1, 1, 3).send(bob),\n    )  # clean out hidden state\n    # out, hidden = lstm(inputs, hidden)\n    # This test will pass once the .size() method is implemented for\n    # remote tensors\n    # assert out.shape == torch.Size([5, 1, 3])\n'"
test/torch/test_hook.py,47,"b'""""""Tests relative to verifying the hook process behaves properly.""""""\nimport re\n\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport syft\nfrom syft.exceptions import RemoteObjectFoundError\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\n\n\ndef test___init__(hook):\n    assert torch.torch_hooked\n    assert hook.torch.__version__ == torch.__version__\n\n\ndef test_torch_inplace_method():\n    positives = [""__iadd__"", ""__imul__"", ""__idiv__"", ""share_"", ""get_"", ""encrypt_""]\n    negatives = [\n        ""__add__"",\n        ""__init__"",\n        ""__str__"",\n        ""share"",\n        ""get"",\n        ""encrypt"",\n        ""__foo"",\n        ""_bar"",\n        ""baz__"",\n    ]\n    for pos in positives:\n        assert re.search(syft.framework._inplace_pattern, pos)\n    for neg in negatives:\n        assert not re.search(syft.framework._inplace_pattern, neg)\n\n\ndef test_torch_attributes():\n    with pytest.raises(RuntimeError):\n        syft.framework._command_guard(""false_command"")\n\n    assert syft.framework._is_command_valid_guard(""torch.add"")\n    assert not syft.framework._is_command_valid_guard(""false_command"")\n\n    syft.framework._command_guard(""torch.add"", get_native=False)\n\n\ndef test_worker_registration(hook, workers):\n    boris = syft.VirtualWorker(id=""boris"", hook=hook, is_client_worker=False)\n\n    workers[""me""].add_workers([boris])\n    worker = workers[""me""].get_worker(boris)\n\n    assert boris == worker\n\n\ndef test_pointer_found_exception(workers):\n    ptr_id = syft.ID_PROVIDER.pop()\n    pointer = PointerTensor(id=ptr_id, location=workers[""alice""], owner=workers[""me""])\n\n    try:\n        raise RemoteObjectFoundError(pointer)\n    except RemoteObjectFoundError as err:\n        err_pointer = err.pointer\n        assert isinstance(err_pointer, PointerTensor)\n        assert err_pointer.id == ptr_id\n\n\ndef test_build_get_child_type():\n    from syft.generic.frameworks.hook.hook_args import build_rule\n    from syft.generic.frameworks.hook.hook_args import build_get_tensor_type\n\n    x = torch.Tensor([1, 2, 3])\n    args = (x, [[1, x]])\n    rule = build_rule(args)\n\n    get_child_type_function = build_get_tensor_type(rule)\n    tensor_type = get_child_type_function(args)\n\n    assert tensor_type == torch.Tensor\n\n\n@pytest.mark.parametrize(""attr"", [""abs""])\ndef test_get_pointer_unary_method(attr, workers):\n    x = torch.Tensor([1, 2, 3])\n    native_method = getattr(x, f""native_{attr}"")\n    expected = native_method()\n\n    x_ptr = x.send(workers[""bob""])\n    method = getattr(x_ptr, attr)\n    res_ptr = method()\n    res = res_ptr.get()\n\n    assert (res == expected).all()\n\n\n@pytest.mark.parametrize(""attr"", [""add"", ""mul""])\ndef test_get_pointer_binary_method(attr, workers):\n    x = torch.Tensor([1, 2, 3])\n    native_method = getattr(x, f""native_{attr}"")\n    expected = native_method(x)\n\n    x_ptr = x.send(workers[""bob""])\n    method = getattr(x_ptr, attr)\n    res_ptr = method(x_ptr)\n    res = res_ptr.get()\n\n    assert (res == expected).all()\n\n\n@pytest.mark.parametrize(""attr"", [""abs""])\ndef test_get_pointer_to_pointer_unary_method(attr, workers):\n    x = torch.Tensor([1, 2, 3])\n    native_method = getattr(x, f""native_{attr}"")\n    expected = native_method()\n\n    x_ptr = x.send(workers[""bob""]).send(workers[""alice""])\n    method = getattr(x_ptr, attr)\n    res_ptr = method()\n    res = res_ptr.get().get()\n\n    assert (res == expected).all()\n\n\n@pytest.mark.parametrize(""attr"", [""add"", ""mul""])\ndef test_get_pointer_to_pointer_binary_method(attr, workers):\n    x = torch.Tensor([1, 2, 3])\n    native_method = getattr(x, f""native_{attr}"")\n    expected = native_method(x)\n\n    x_ptr = x.send(workers[""bob""]).send(workers[""alice""])\n    method = getattr(x_ptr, attr)\n    res_ptr = method(x_ptr)\n    res = res_ptr.get().get()\n\n    assert (res == expected).all()\n\n\n@pytest.mark.parametrize(""attr"", [""relu"", ""celu"", ""elu""])\ndef test_hook_module_functional(attr, workers):\n    attr = getattr(F, attr)\n    x = torch.Tensor([1, -1, 3, 4])\n    expected = attr(x)\n\n    x_ptr = x.send(workers[""bob""])\n    res_ptr = attr(x_ptr)\n    res = res_ptr.get()\n\n    assert (res == expected).all()\n\n\n@pytest.mark.parametrize(""attr"", [""relu"", ""celu"", ""elu""])\ndef test_functional_same_in_both_imports(attr):\n    """"""This function tests that the hook modifies the behavior of\n    torch.nn.function regardless of the import namespace\n    """"""\n    fattr = getattr(F, attr)\n    tattr = getattr(torch.nn.functional, attr)\n    x = torch.Tensor([1, -1, 3, 4])\n\n    assert (fattr(x) == tattr(x)).all()\n\n\ndef test_hook_tensor(workers):\n    x = torch.tensor([1.0, -1.0, 3.0, 4.0], requires_grad=True)\n    x.send(workers[""bob""])\n    x = torch.tensor([1.0, -1.0, 3.0, 4.0], requires_grad=True)[0:2]\n    x_ptr = x.send(workers[""bob""])\n    assert hasattr(x_ptr, ""child"")\n\n\ndef test_properties():\n    x = torch.Tensor([1, -1, 3, 4])\n    assert x.is_wrapper is False\n\n\ndef test_signature_cache_change():\n    """"""Tests that calls to the same method using a different\n    signature works correctly. We cache signatures in the\n    hook.build_unwrap_args_from_function dictionary but sometimes they\n    are incorrect if we use the same method with different\n    parameter types. So, we need to test to make sure that\n    this cache missing fails gracefully. This test tests\n    that for the .div(tensor) .div(int) method.""""""\n\n    x = torch.Tensor([1, 2, 3])\n    y = torch.Tensor([1, 2, 3])\n\n    z = x.div(y)\n    z = x.div(2)\n    z = x.div(y)\n\n    assert True\n\n\ndef test_parameter_hooking():\n    """"""Test custom nn.Module and parameter auto listing in m.parameters()""""""\n\n    class MyLayer(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.some_params = torch.nn.Parameter(torch.tensor([5.0]))\n\n    m = MyLayer()\n    out = list(m.parameters())\n\n    assert len(out) == 1\n    assert out[0] == m.some_params\n\n\ndef test_torch_module_hook(workers):\n    """"""Tests sending and getting back torch nn module like nn.Linear""""""\n    model = nn.Linear(2, 1)\n    model_ptr = model.send(workers[""bob""])\n    model_back = model_ptr.get()\n\n    bias = model_back.bias\n    model_back.fix_precision()\n    model_back.float_precision()\n    assert (bias == model_back.bias).all()\n\n\ndef test_functional_hook():\n    x = torch.tensor([[1, 2], [3, 4]])\n    y = torch.einsum(""ij,jk->ik"", x, x)\n    assert (y == torch.tensor([[7, 10], [15, 22]])).all()\n\n\ndef test_hook_args_and_cmd_signature_malleability():\n    """"""Challenge the hook_arg module with methods used with different signatures""""""\n    a = syft.LoggingTensor().on(torch.tensor([1.0, 2]))\n    b = syft.LoggingTensor().on(torch.tensor([1.0, 2]))\n\n    r1 = a + b\n    assert (r1 == syft.LoggingTensor().on(torch.tensor([2.0, 4]))).all()\n\n    r2 = a + 1\n    assert (r2 == syft.LoggingTensor().on(torch.tensor([2.0, 3]))).all()\n\n    r3 = a + b\n    assert (r3 == syft.LoggingTensor().on(torch.tensor([2.0, 4]))).all()\n\n\ndef test_torch_func_signature_without_tensor():\n    """"""The hook on the args of torch commands should work even if the args\n    don\'t contain any tensor""""""\n    x = torch.as_tensor((0.1307,), dtype=torch.float32, device=""cpu"")\n    assert (x == torch.tensor([0.1307])).all()\n\n\ndef test_RNN_grad_set_backpropagation(workers):\n    """"""Perform backpropagation at a remote worker and check if the gradient updates\n    and properly computed within the model""""""\n\n    alice = workers[""alice""]\n\n    class RNN(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(RNN, self).__init__()\n            self.hidden_size = hidden_size\n            self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n            self.i2o = nn.Linear(input_size + hidden_size, output_size)\n            self.softmax = nn.LogSoftmax(dim=1)\n\n        def forward(self, input, hidden):\n            combined = torch.cat((input, hidden), 1)\n            hidden = self.i2h(combined)\n            output = self.i2o(combined)\n            output = self.softmax(output)\n            return output, hidden\n\n        def initHidden(self):\n            return torch.zeros(1, self.hidden_size)\n\n    # let\'s initialize a simple RNN\n    n_hidden = 128\n    n_letters = 57\n    n_categories = 18\n\n    rnn = RNN(n_letters, n_hidden, n_categories)\n\n    # Let\'s send the model to alice, who will be responsible for the tiny computation\n    alice_model = rnn.copy().send(alice)\n\n    # Simple input for the Recurrent Neural Network\n    input_tensor = torch.zeros(size=(1, 57))\n    # Just set a random category for it\n    input_tensor[0][20] = 1\n    alice_input = input_tensor.copy().send(alice)\n\n    label_tensor = torch.randint(low=0, high=(n_categories - 1), size=(1,))\n    alice_label = label_tensor.send(alice)\n\n    hidden_layer = alice_model.initHidden()\n    alice_hidden_layer = hidden_layer.send(alice)\n    # Forward pass into the NN and its hidden layers, notice how it goes sequentially\n    output, alice_hidden_layer = alice_model(alice_input, alice_hidden_layer)\n    criterion = nn.NLLLoss()\n    loss = criterion(output, alice_label)\n    # time to backpropagate...\n    loss.backward()\n\n    # now let\'s get the model and check if its parameters are indeed there\n    model_got = alice_model.get()\n\n    learning_rate = 0.005\n\n    # If the gradients are there, then the backpropagation did indeed complete successfully\n    for param in model_got.parameters():\n        # param.grad.data would raise an exception in case it is none,\n        # so we better check it beforehand\n        assert param.grad.data is not None\n        param.data.add_(-learning_rate, param.grad.data)\n\n\ndef test_local_remote_gradient_clipping(workers):\n    """"""\n    Real test case of gradient clipping for the remote and\n    local parameters of an RNN\n    """"""\n    alice = workers[""alice""]\n\n    class RNN(nn.Module):\n        def __init__(self, input_size, hidden_size, output_size):\n            super(RNN, self).__init__()\n            self.hidden_size = hidden_size\n            self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n            self.i2o = nn.Linear(input_size + hidden_size, output_size)\n            self.softmax = nn.LogSoftmax(dim=1)\n\n        def forward(self, input, hidden):\n            combined = torch.cat((input, hidden), 1)\n            hidden = self.i2h(combined)\n            output = self.i2o(combined)\n            output = self.softmax(output)\n            return output, hidden\n\n        def initHidden(self):\n            return torch.zeros(1, self.hidden_size)\n\n    # let\'s initialize a simple RNN\n    n_hidden = 128\n    n_letters = 57\n    n_categories = 18\n\n    rnn = RNN(n_letters, n_hidden, n_categories)\n\n    # Let\'s send the model to alice, who will be responsible for the tiny computation\n    alice_model = rnn.copy().send(alice)\n\n    # Simple input for the Recurrent Neural Network\n    input_tensor = torch.zeros(size=(1, 57))\n    # Just set a random category for it\n    input_tensor[0][20] = 1\n    alice_input = input_tensor.copy().send(alice)\n\n    label_tensor = torch.randint(low=0, high=(n_categories - 1), size=(1,))\n    alice_label = label_tensor.send(alice)\n\n    hidden_layer = alice_model.initHidden()\n    alice_hidden_layer = hidden_layer.send(alice)\n    # Forward pass into the NN and its hidden layers, notice how it goes sequentially\n    output, alice_hidden_layer = alice_model(alice_input, alice_hidden_layer)\n    criterion = nn.NLLLoss()\n    loss = criterion(output, alice_label)\n    # time to backpropagate...\n    loss.backward()\n\n    # Remote gradient clipping\n    remote_parameters = alice_model.parameters()\n    total_norm_remote = nn.utils.clip_grad_norm_(remote_parameters, 2)\n\n    # Local gradient clipping\n    local_alice_model = alice_model.get()\n    local_parameters = local_alice_model.parameters()\n    total_norm_local = nn.utils.clip_grad_norm_(local_parameters, 2)\n\n    # Is the output of the remote gradient clipping version equal to\n    # the output of the local gradient clipping version?\n    assert torch.isclose(total_norm_remote.get(), total_norm_local, atol=1e-4)\n\n\n# Input: None\n# Output: a local PyTorch tensor with .grad attribute set to [4.5, 4.5, 4.5, 4.5]\n# These operations were taken from this tutorial:\n# https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\ndef produce_tensor_with_grad():\n    x = torch.ones(2, 2, requires_grad=True)\n    a = torch.randn(2, 2)\n    a = (a * 3) / (a - 1)\n    a.requires_grad_(True)\n    b = (a * a).sum()\n    y = x + 2\n    z = y * y * 3\n    out = z.mean()\n    # backward function is used for computing the gradient of the tensor \'x\'\n    out.backward()\n\n    return x\n\n\ndef test_remote_gradient_clipping(workers):\n    """"""\n    Vanishing gradient test over\n    gradients of a remote tensor\n    """"""\n    alice = workers[""alice""]\n    local_tensor = produce_tensor_with_grad()\n    remote_tensor = local_tensor.send(alice)\n    # initially, remote_tensor.grad is [4.5, 4.5, 4.5, 4.5]\n    # Method operates in place\n    nn.utils.clip_grad_norm_(remote_tensor, 2)\n    tensor_comparison_grad = torch.Tensor([[4.5, 4.5], [4.5, 4.5]])\n    tensor_comp_grad_remote = tensor_comparison_grad.send(alice)\n    # Has the gradient decreased w.r.t. the initial value (i.e. was it clipped)?\n    smaller_tensor_check_remote = remote_tensor.grad < tensor_comp_grad_remote\n\n    assert 1 == smaller_tensor_check_remote.all().copy().get().item()\n\n\ndef test_local_gradient_clipping():\n    """"""\n    Vanishing gradient test over\n    gradients of a local tensor\n    """"""\n    local_tensor = produce_tensor_with_grad()\n    # initially, local_tensor.grad is [4.5, 4.5, 4.5, 4.5]\n    # Method operates in place\n    nn.utils.clip_grad_norm_(local_tensor, 2)\n    tensor_comparison_grad = torch.Tensor([[4.5, 4.5], [4.5, 4.5]])\n    # Has the gradient decreased w.r.t. the initial value (i.e. was it clipped)?\n    smaller_tensor_check = local_tensor.grad < tensor_comparison_grad\n\n    assert 1 == smaller_tensor_check.all().item()\n'"
test/workers/__init__.py,0,b''
test/workers/test_base.py,0,"b'import pytest\nimport time\n\nimport syft as sy\nimport torch as th\nfrom unittest import mock\nfrom types import MethodType\n\nfrom syft.workers.websocket_client import WebsocketClientWorker\nfrom syft.workers.websocket_server import WebsocketServerWorker\n\n\ndef test_create_already_existing_worker(hook):\n    # Shares tensor with bob\n    bob = sy.VirtualWorker(hook, ""bob"")\n    x = th.tensor([1, 2, 3]).send(bob)\n\n    # Recreates bob and shares a new tensor\n    bob = sy.VirtualWorker(hook, ""bob"")\n    y = th.tensor([2, 2, 2]).send(bob)\n\n    # Recreates bob and shares a new tensor\n    bob = sy.VirtualWorker(hook, ""bob"")\n    z = th.tensor([2, 2, 10]).send(bob)\n\n    # Both workers should be the same, so the following operation should be valid\n    _ = x + y * z\n\n\ndef test_clear_object_for_worker_created_with_pre_existing_id(hook):\n\n    worker = sy.VirtualWorker(hook, id=""worker"")\n    worker.clear_objects()\n\n    ptr = th.tensor([1, 2, 3]).send(worker)\n\n    assert len(worker._known_workers[worker.id].object_store._objects) == len(\n        worker.object_store._objects\n    )\n    assert len(worker.object_store._objects) == 1\n\n    # create worker with pre-existing id\n    worker = sy.VirtualWorker(hook, id=""worker"")\n    worker.clear_objects()\n\n    assert len(worker._known_workers[worker.id].object_store._objects) == len(\n        worker.object_store._objects\n    )\n    assert len(worker.object_store._objects) == 0\n\n    ptr = th.tensor([1, 2, 3]).send(worker)\n\n    assert len(worker._known_workers[worker.id].object_store._objects) == len(\n        worker.object_store._objects\n    )\n    assert len(worker.object_store._objects) == 1\n\n\ndef test_create_already_existing_worker_with_different_type(hook, start_proc):\n    # Shares tensor with bob\n    bob = sy.VirtualWorker(hook, ""bob"")\n    _ = th.tensor([1, 2, 3]).send(bob)\n\n    kwargs = {""id"": ""fed1"", ""host"": ""localhost"", ""port"": 8765, ""hook"": hook}\n    server = start_proc(WebsocketServerWorker, **kwargs)\n\n    time.sleep(0.1)\n\n    # Recreates bob as a different type of worker\n    kwargs = {""id"": ""bob"", ""host"": ""localhost"", ""port"": 8765, ""hook"": hook}\n    with pytest.raises(RuntimeError):\n        bob = WebsocketClientWorker(**kwargs)\n\n    server.terminate()\n\n\ndef test_execute_worker_function(hook):\n    sy.VirtualWorker.mocked_function = MethodType(\n        mock.Mock(return_value=""bob_mocked_function""), sy.VirtualWorker\n    )\n\n    bob = sy.VirtualWorker(hook, ""bob"")\n    x = th.tensor([1, 2, 3]).send(bob)\n\n    message = bob.create_worker_command_message(\n        command_name=""mocked_function"", command_owner=""self""\n    )\n\n    serialized_message = sy.serde.serialize(message)\n\n    response = bob._recv_msg(serialized_message)\n    response = sy.serde.deserialize(response)\n\n    assert response == ""bob_mocked_function""\n\n    bob.mocked_function.assert_called()\n\n\ndef test_enable_registration_with_ctx(hook):\n    assert hook.local_worker.is_client_worker\n    with hook.local_worker.registration_enabled():\n        hook.local_worker.is_client_worker is False\n    assert hook.local_worker.is_client_worker\n\n\ndef test_send_command_allow_list(hook, workers):\n    bob = workers[""bob""]\n    allow_listed_methods = {\n        ""torch"": {""tensor"": [1, 2, 3], ""rand"": (2, 3), ""randn"": (2, 3), ""zeros"": (2, 3)}\n    }\n\n    for framework, methods in allow_listed_methods.items():\n        attr = getattr(bob.remote, framework)\n\n        for method, inp in methods.items():\n            x = getattr(attr, method)(inp)\n\n            if ""rand"" not in method:\n                assert (x.get() == getattr(th, method)(inp)).all()\n\n\ndef test_send_command_not_allow_listed(hook, workers):\n    bob = workers[""bob""]\n\n    method_not_exist = ""openmind""\n\n    for framework in bob.remote.frameworks:\n        if framework in dir(bob.remote):\n            attr = getattr(bob.remote, framework)\n\n            with pytest.raises(AttributeError):\n                getattr(attr, method_not_exist)\n\n\ndef test_is_framework_supported(hook):\n    worker = sy.VirtualWorker(hook, id=""worker"")\n    assert worker.is_framework_supported(""torch"") is True\n    assert sy.VirtualWorker.is_framework_supported(""torch"") is True\n    assert worker.is_framework_supported(""mock_framework"") is False\n'"
test/workers/test_virtual.py,23,"b'from time import time\nfrom unittest.mock import patch\n\nimport pytest\nimport torch\n\nimport syft as sy\nfrom syft import serde\nfrom syft.generic.pointers.object_wrapper import ObjectWrapper\nfrom syft.messaging.message import ObjectMessage\nfrom syft.messaging.message import ObjectRequestMessage\nfrom syft.workers.virtual import VirtualWorker\n\nfrom syft.exceptions import GetNotPermittedError\nfrom syft.exceptions import ObjectNotFoundError\n\n\ndef test_send_msg():\n    """"""Tests sending a message with a specific ID\n\n    This is a simple test to ensure that the BaseWorker interface\n    can properly send/receive a message containing a tensor.\n    """"""\n\n    # get pointer to local worker\n    me = sy.torch.hook.local_worker\n\n    # pending time to simulate lantency (optional)\n    me.message_pending_time = 0.1\n\n    # create a new worker (to send the object to)\n    worker_id = sy.ID_PROVIDER.pop()\n    bob = VirtualWorker(sy.torch.hook, id=f""bob{worker_id}"")\n\n    # initialize the object and save it\'s id\n    obj = torch.Tensor([100, 100])\n    obj_id = obj.id\n\n    # Send data to bob\n    start_time = time()\n    me.send_msg(ObjectMessage(obj), bob)\n    elapsed_time = time() - start_time\n\n    me.message_pending_time = 0\n\n    # ensure that object is now on bob\'s machine\n    assert obj_id in bob.object_store._objects\n    # ensure that object was sent 0.1 secs later\n    assert elapsed_time > 0.1\n\n\ndef test_send_msg_using_tensor_api():\n    """"""Tests sending a message with a specific ID\n\n    This is a simple test to ensure that the high level tensor .send()\n    method correctly sends a message to another worker.\n    """"""\n\n    # create worker to send object to\n    worker_id = sy.ID_PROVIDER.pop()\n    bob = VirtualWorker(sy.torch.hook, id=f""bob{worker_id}"")\n\n    # create a tensor to send (default on local_worker)\n    obj = torch.Tensor([100, 100])\n\n    # save the object\'s id\n    obj_id = obj.id\n\n    # send the object to Bob (from local_worker)\n    _ = obj.send(bob)\n\n    # ensure tensor made it to Bob\n    assert obj_id in bob.object_store._objects\n\n\ndef test_recv_msg():\n    """"""Tests the recv_msg command with 2 tests\n\n    The first test uses recv_msg to send an object to alice.\n\n    The second test uses recv_msg to request the object\n    previously sent to alice.""""""\n\n    # TEST 1: send tensor to alice\n\n    # create a worker to send data to\n    worker_id = sy.ID_PROVIDER.pop()\n    alice = VirtualWorker(sy.torch.hook, id=f""alice{worker_id}"")\n\n    # create object to send\n    obj = torch.Tensor([100, 100])\n\n    # create/serialize message\n    message = ObjectMessage(obj)\n    bin_msg = serde.serialize(message)\n\n    # have alice receive message\n    alice.recv_msg(bin_msg)\n\n    # ensure that object is now in alice\'s registry\n    assert obj.id in alice.object_store._objects\n\n    # Test 2: get tensor back from alice\n\n    # Create message: Get tensor from alice\n    message = ObjectRequestMessage(obj.id, None, """")\n\n    # serialize message\n    bin_msg = serde.serialize(message)\n\n    # call receive message on alice\n    resp = alice.recv_msg(bin_msg)\n\n    obj_2 = sy.serde.deserialize(resp)\n\n    # assert that response is correct type\n    assert type(resp) == bytes\n\n    # ensure that the object we receive is correct\n    assert obj_2.id == obj.id\n\n\ndef tests_worker_convenience_methods():\n    """"""Tests send and get object methods on BaseWorker\n\n    This test comes in two parts. The first uses the simple\n    BaseWorker.send_obj and BaseWorker.request_obj to send a\n    tensor to Alice and to get the worker back from Alice.\n\n    The second part shows that the same methods work between\n    bob and alice directly.\n    """"""\n\n    me = sy.torch.hook.local_worker\n    worker_id = sy.ID_PROVIDER.pop()\n    bob = VirtualWorker(sy.torch.hook, id=f""bob{worker_id}"")\n    worker_id = sy.ID_PROVIDER.pop()\n    alice = VirtualWorker(sy.torch.hook, id=f""alice{worker_id}"")\n    obj = torch.Tensor([100, 100])\n\n    # Send data to alice\n    me.send_obj(obj, alice)\n\n    # Get data from alice\n    resp_alice = me.request_obj(obj.id, alice)\n\n    assert (resp_alice == obj).all()\n\n    obj2 = torch.Tensor([200, 200])\n\n    # Set data on self\n    bob.object_store.set_obj(obj2)\n\n    # Get data from self\n    resp_bob_self = bob.get_obj(obj2.id)\n\n    assert (resp_bob_self == obj2).all()\n\n    # Get data from bob as alice\n    resp_bob_alice = alice.request_obj(obj2.id, bob)\n\n    assert (resp_bob_alice == obj2).all()\n\n\ndef test_search():\n    worker_id = sy.ID_PROVIDER.pop()\n    bob = VirtualWorker(sy.torch.hook, id=f""bob{worker_id}"")\n\n    x = (\n        torch.tensor([1, 2, 3, 4, 5])\n        .tag(""#fun"", ""#mnist"")\n        .describe(""The images in the MNIST training dataset."")\n        .send(bob)\n    )\n\n    y = (\n        torch.tensor([1, 2, 3, 4, 5])\n        .tag(""#not_fun"", ""#cifar"")\n        .describe(""The images in the MNIST training dataset."")\n        .send(bob)\n    )\n\n    z = (\n        torch.tensor([1, 2, 3, 4, 5])\n        .tag(""#fun"", ""#boston_housing"")\n        .describe(""The images in the MNIST training dataset."")\n        .send(bob)\n    )\n\n    a = (\n        torch.tensor([1, 2, 3, 4, 5])\n        .tag(""#not_fun"", ""#boston_housing"")\n        .describe(""The images in the MNIST training dataset."")\n        .send(bob)\n    )\n\n    assert len(bob.search(""#fun"")) == 2\n    assert len(bob.search(""#mnist"")) == 1\n    assert len(bob.search(""#cifar"")) == 1\n    assert len(bob.search(""#not_fun"")) == 2\n    assert len(bob.search([""#not_fun"", ""#boston_housing""])) == 1\n\n\ndef test_obj_not_found(workers):\n    """"""Test for useful error message when trying to call a method on\n    a tensor which does not exist on a worker anymore.""""""\n\n    bob = workers[""bob""]\n\n    x = torch.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    bob.object_store.clear_objects()\n\n    with pytest.raises(ObjectNotFoundError):\n        y = x + x\n\n\ndef test_get_not_permitted(workers):\n    bob = workers[""bob""]\n    x = torch.tensor([1, 2, 3, 4, 5]).send(bob)\n    with patch.object(torch.Tensor, ""allow"") as mock_allowed_to_get:\n        mock_allowed_to_get.return_value = False\n        with pytest.raises(GetNotPermittedError):\n            x.get()\n        mock_allowed_to_get.assert_called_once()\n\n\ndef test_send_jit_scriptmodule(hook, workers):  # pragma: no cover\n    bob = workers[""bob""]\n\n    @torch.jit.script\n    def foo(x):\n        return x + 2\n\n    foo_wrapper = ObjectWrapper(obj=foo, id=99)\n    foo_ptr = hook.local_worker.send(foo_wrapper, bob)\n\n    res = foo_ptr(torch.tensor(4))\n    assert res == torch.tensor(6)\n\n\ndef test_send_command_allow_list(hook, workers):\n    bob = workers[""bob""]\n    allow_listed_methods = {\n        ""torch"": {""tensor"": [1, 2, 3], ""rand"": (2, 3), ""randn"": (2, 3), ""zeros"": (2, 3)}\n    }\n\n    for framework, methods in allow_listed_methods.items():\n        attr = getattr(bob.remote, framework)\n\n        for method, inp in methods.items():\n            x = getattr(attr, method)(inp)\n\n            if ""rand"" not in method:\n                assert (x.get() == getattr(torch, method)(inp)).all()\n\n\ndef test_send_command_not_allow_listed(hook, workers):\n    bob = workers[""bob""]\n\n    method_not_exist = ""openmind""\n\n    for framework in bob.remote.frameworks:\n        if framework in dir(bob.remote):\n            attr = getattr(bob.remote, framework)\n\n            with pytest.raises(AttributeError):\n                getattr(attr, method_not_exist)\n'"
test/workers/test_websocket_worker.py,16,"b'import time\nfrom OpenSSL import crypto\nimport pytest\nimport torch\n\nfrom syft.workers.websocket_server import WebsocketServerWorker\n\nfrom test.conftest import instantiate_websocket_client_worker\n\n\nPRINT_IN_UNITTESTS = False\n\n\n@pytest.mark.parametrize(""secure"", [True, False])\ndef test_websocket_worker_basic(hook, start_proc, secure, tmpdir):\n    """"""Evaluates that you can do basic tensor operations using\n    WebsocketServerWorker in insecure and secure mode.""""""\n\n    def create_self_signed_cert(cert_path, key_path):\n        # create a key pair\n        k = crypto.PKey()\n        k.generate_key(crypto.TYPE_RSA, 1024)\n\n        # create a self-signed cert\n        cert = crypto.X509()\n        cert.gmtime_adj_notBefore(0)\n        cert.gmtime_adj_notAfter(1000)\n        cert.set_pubkey(k)\n        cert.sign(k, ""sha1"")\n\n        # store keys and cert\n        open(cert_path, ""wb"").write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert))\n        open(key_path, ""wb"").write(crypto.dump_privatekey(crypto.FILETYPE_PEM, k))\n\n    kwargs = {\n        ""id"": ""secure_fed"" if secure else ""not_secure_fed"",\n        ""host"": ""localhost"",\n        ""port"": 8766,\n        ""hook"": hook,\n    }\n\n    if secure:\n        # Create cert and keys\n        cert_path = tmpdir.join(""test.crt"")\n        key_path = tmpdir.join(""test.key"")\n        create_self_signed_cert(cert_path, key_path)\n        kwargs[""cert_path""] = cert_path\n        kwargs[""key_path""] = key_path\n\n    process_remote_worker = start_proc(WebsocketServerWorker, **kwargs)\n\n    time.sleep(0.1)\n    x = torch.ones(5)\n\n    if secure:\n        # unused args\n        del kwargs[""cert_path""]\n        del kwargs[""key_path""]\n\n    kwargs[""secure""] = secure\n    remote_proxy = instantiate_websocket_client_worker(**kwargs)\n\n    x = x.send(remote_proxy)\n    y = x + x\n    y = y.get()\n\n    assert (y == torch.ones(5) * 2).all()\n\n    del x\n\n    remote_proxy.close()\n    time.sleep(0.1)\n    remote_proxy.remove_worker_from_local_worker_registry()\n    process_remote_worker.terminate()\n\n\ndef test_websocket_workers_search(hook, start_remote_worker):\n    """"""Evaluates that a client can search and find tensors that belong\n    to another party""""""\n    # Args for initializing the websocket server and client\n    server, remote_proxy = start_remote_worker(id=""fed2"", hook=hook, port=8767)\n\n    # Sample tensor to store on the server\n    sample_data = torch.tensor([1, 2, 3, 4]).tag(""#sample_data"", ""#another_tag"")\n    _ = sample_data.send(remote_proxy)\n\n    # Search for the tensor located on the server by using its tag\n    results = remote_proxy.search([""#sample_data"", ""#another_tag""])\n\n    assert results\n    assert results[0].owner.id == ""me""\n    assert results[0].location.id == ""fed2""\n\n    # Search multiple times should still work\n    results = remote_proxy.search([""#sample_data"", ""#another_tag""])\n\n    assert results\n    assert results[0].owner.id == ""me""\n    assert results[0].location.id == ""fed2""\n\n    remote_proxy.close()\n    time.sleep(0.1)\n    remote_proxy.remove_worker_from_local_worker_registry()\n    server.terminate()\n\n\ndef test_list_objects_remote(hook, start_remote_worker):\n    server, remote_proxy = start_remote_worker(id=""fed-list-objects"", hook=hook, port=8765)\n    remote_proxy.clear_objects()\n\n    x = torch.tensor([1, 2, 3]).send(remote_proxy)\n\n    res = remote_proxy.list_tensors_remote()\n\n    res_dict = eval(res.replace(""tensor"", ""torch.tensor""))\n    assert len(res_dict) == 1\n\n    y = torch.tensor([4, 5, 6]).send(remote_proxy)\n    res = remote_proxy.list_tensors_remote()\n    res_dict = eval(res.replace(""tensor"", ""torch.tensor""))\n    assert len(res_dict) == 2\n\n    # delete x before terminating the websocket connection\n    del x\n    del y\n    time.sleep(0.1)\n    remote_proxy.close()\n    time.sleep(0.1)\n    remote_proxy.remove_worker_from_local_worker_registry()\n    server.terminate()\n\n\ndef test_objects_count_remote(hook, start_remote_worker):\n    server, remote_proxy = start_remote_worker(id=""fed-count-objects"", hook=hook, port=8764)\n    remote_proxy.clear_objects()\n\n    x = torch.tensor([1, 2, 3]).send(remote_proxy)\n\n    nr_objects = remote_proxy.tensors_count_remote()\n    assert nr_objects == 1\n\n    y = torch.tensor([4, 5, 6]).send(remote_proxy)\n    nr_objects = remote_proxy.tensors_count_remote()\n    assert nr_objects == 2\n\n    x.get()\n    nr_objects = remote_proxy.tensors_count_remote()\n    assert nr_objects == 1\n\n    # delete remote object before terminating the websocket connection\n    del y\n    time.sleep(0.1)\n    remote_proxy.close()\n    time.sleep(0.1)\n    remote_proxy.remove_worker_from_local_worker_registry()\n    server.terminate()\n\n\ndef test_clear_objects_remote(hook, start_remote_worker):\n    server, remote_proxy = start_remote_worker(id=""fed-clear-objects"", hook=hook, port=8769)\n\n    x = torch.tensor([1, 2, 3]).send(remote_proxy, garbage_collect_data=False)\n    y = torch.tensor(4).send(remote_proxy, garbage_collect_data=False)\n\n    nr_objects = remote_proxy.tensors_count_remote()\n    assert nr_objects == 2\n\n    remote_proxy.clear_objects_remote()\n    nr_objects = remote_proxy.objects_count_remote()\n    assert nr_objects == 0\n\n    remote_proxy.close()\n    remote_proxy.remove_worker_from_local_worker_registry()\n    server.terminate()\n\n\ndef test_connect_close(hook, start_remote_worker):\n    server, remote_proxy = start_remote_worker(id=""fed-connect-close"", hook=hook, port=8770)\n\n    x = torch.tensor([1, 2, 3])\n    x_ptr = x.send(remote_proxy)\n\n    assert remote_proxy.tensors_count_remote() == 1\n\n    remote_proxy.close()\n\n    time.sleep(0.1)\n\n    remote_proxy.connect()\n\n    assert remote_proxy.tensors_count_remote() == 1\n\n    x_val = x_ptr.get()\n    assert (x_val == x).all()\n\n    remote_proxy.close()\n    remote_proxy.remove_worker_from_local_worker_registry()\n\n    time.sleep(0.1)\n\n    server.terminate()\n\n\ndef test_websocket_worker_multiple_output_response(hook, start_remote_worker):\n    """"""Evaluates that you can do basic tensor operations using\n    WebsocketServerWorker.""""""\n    server, remote_proxy = start_remote_worker(id=""socket_multiple_output"", hook=hook, port=8771)\n\n    x = torch.tensor([1.0, 3, 2])\n    x = x.send(remote_proxy)\n\n    p1, p2 = torch.sort(x)\n    x1, x2 = p1.get(), p2.get()\n\n    assert (x1 == torch.tensor([1.0, 2, 3])).all()\n    assert (x2 == torch.tensor([0, 2, 1])).all()\n\n    x.get()  # retrieve remote object before closing the websocket connection\n\n    remote_proxy.close()\n    server.terminate()\n\n\ndef test_send_command_allow_list(hook, start_remote_worker):\n    server, remote_proxy = start_remote_worker(\n        id=""worker_call_api_good_methods"", hook=hook, port=8772\n    )\n    allow_listed_methods = {\n        ""torch"": {""tensor"": [1, 2, 3], ""rand"": (2, 3), ""randn"": (2, 3), ""zeros"": (2, 3)}\n    }\n\n    for framework, methods in allow_listed_methods.items():\n        attr = getattr(remote_proxy.remote, framework)\n\n        for method, inp in methods.items():\n            x = getattr(attr, method)(inp)\n\n            if ""rand"" not in method:\n                assert (x.get() == getattr(torch, method)(inp)).all()\n\n    remote_proxy.close()\n    server.terminate()\n\n\ndef test_send_command_not_allow_listed(hook, start_remote_worker):\n    server, remote_proxy = start_remote_worker(\n        id=""worker_call_api_bad_method"", hook=hook, port=8773\n    )\n\n    method_not_exist = ""openmind""\n\n    for framework in remote_proxy.remote.frameworks:\n        if framework in dir(remote_proxy.remote):\n            attr = getattr(remote_proxy.remote, framework)\n            with pytest.raises(AttributeError):\n                getattr(attr, method_not_exist)\n\n    remote_proxy.close()\n    server.terminate()\n'"
test/workers/test_worker.py,1,"b'import pytest\nimport torch\n\nimport syft as sy\nfrom syft.workers.virtual import VirtualWorker\n\nfrom syft.exceptions import WorkerNotFoundException\n\n\ndef test___init__():\n    hook = sy.TorchHook(torch)\n\n    tensor = torch.tensor([1, 2, 3, 4])\n\n    worker_id = sy.ID_PROVIDER.pop()\n    alice_id = f""alice{worker_id}""\n    alice = VirtualWorker(hook, id=alice_id)\n    worker_id = sy.ID_PROVIDER.pop()\n    bob = VirtualWorker(hook, id=f""bob{worker_id}"")\n    worker_id = sy.ID_PROVIDER.pop()\n    charlie = VirtualWorker(hook, id=f""charlie{worker_id}"")\n    worker_id = sy.ID_PROVIDER.pop()\n    dawson = VirtualWorker(hook, id=f""dawson{worker_id}"", data=[tensor])\n\n    # Ensure adding data on signup functionality works as expected\n    assert tensor.owner == dawson\n\n    assert bob.get_worker(alice_id).id == alice.id\n    assert bob.get_worker(alice).id == alice.id\n    assert bob.get_worker(charlie).id == charlie.id\n\n    bob.get_worker(""the_unknown_worker"")\n\n    bob.add_worker(alice)\n\n\ndef test_get_unknown_worker():\n\n    hook = sy.TorchHook(torch)\n\n    bob = VirtualWorker(hook, id=""bob"")\n    charlie = VirtualWorker(hook, id=""charlie"")\n\n    # if an unknown string or id representing a worker is given it fails\n    with pytest.raises(WorkerNotFoundException):\n        bob.get_worker(""the_unknown_worker"", fail_hard=True)\n\n    with pytest.raises(WorkerNotFoundException):\n        bob.get_worker(1, fail_hard=True)\n\n    # if an instance of virtual worker is given it doesn\'t fail\n    assert bob.get_worker(charlie).id == charlie.id\n    assert charlie.id in bob._known_workers\n'"
examples/experimental/Federated Learning Experiment/server.py,0,"b'from flask import Flask\nfrom flask import request\nimport torch as th\nimport syft as sy\n\nsy.create_sandbox(globals())\n\napp = Flask(__name__)\n\n# Iniitalize A Toy Model\nmodel = th.zeros([2, 1])\nptr = None\n\n\n@app.route(""/get_model"", methods=[""GET""])\ndef get_model():\n    global model\n    global ptr\n    ptr = model.create_pointer()\n\n    return sy.serde.serialize(model)\n\n\n@app.route(""/send_data"", methods=[""POST""])\ndef send_data():\n    global ptr\n    ptr = sy.serde.deserialize(request.data)\n\n    return sy.serde.serialize(model)\n'"
examples/tutorials/advanced/__init__.py,0,b''
examples/tutorials/grid/__init__.py,0,b''
examples/tutorials/websocket/__init__.py,0,b''
syft/execution/translation/__init__.py,0,b''
syft/execution/translation/abstract.py,0,"b'from abc import ABC\n\n\nclass AbstractPlanTranslator(ABC):\n    """"""\n    Translator class takes a Plan and makes copy that is translated\n    to different Plan type, e.g. torchscript\n    """"""\n\n    def __init__(self, plan):\n        self.plan = plan\n\n    def translate(self):\n        pass\n\n    def remove(self):\n        pass\n'"
syft/execution/translation/default.py,0,"b'from syft.execution.translation.abstract import AbstractPlanTranslator\n\n\nclass PlanTranslatorDefault(AbstractPlanTranslator):\n    """"""Performs translation from \'list of ops\' Plan into torchscript Plan""""""\n\n    def __init__(self, plan):\n        super().__init__(plan)\n\n    def translate(self):\n        #  do nothing, Plan is built in default ""list of ops"" variant\n        return self.plan\n\n    def remove(self):\n        plan = self.plan\n        plan.role.actions = []\n\n        return plan\n'"
syft/execution/translation/torchscript.py,0,"b'from torch import jit\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.execution.translation.abstract import AbstractPlanTranslator\n\n\nclass PlanTranslatorTorchscript(AbstractPlanTranslator):\n    """"""Performs translation from \'list of ops\' Plan into torchscript Plan""""""\n\n    def __init__(self, plan):\n        super().__init__(plan)\n\n    def translate(self):\n        translation_plan = self.plan.copy()\n        translation_plan.forward = None\n\n        args = translation_plan.create_dummy_args()\n\n        # jit.trace clones input args and can change their type, so we have to skip types check\n        # TODO see if type check can be made less strict,\n        #  e.g. tensor/custom tensor/nn.Parameter could be considered same type\n        translation_plan.validate_input_types = False\n\n        # To avoid storing Plan state tensors in torchscript, they will be sent as parameters\n        # we trace wrapper func, which accepts state parameters as last arg\n        # and sets them into the Plan before executing the Plan\n        def wrap_stateful_plan(*args):\n            role = translation_plan.role\n            state = args[-1]\n            if 0 < len(role.state.state_placeholders) == len(state) and isinstance(\n                state, (list, tuple)\n            ):\n                state_placeholders = tuple(\n                    role.placeholders[ph.id.value] for ph in role.state.state_placeholders\n                )\n                PlaceHolder.instantiate_placeholders(role.state.state_placeholders, state)\n                PlaceHolder.instantiate_placeholders(state_placeholders, state)\n\n            return translation_plan(*args[:-1])\n\n        plan_params = translation_plan.parameters()\n        if len(plan_params) > 0:\n            torchscript_plan = jit.trace(wrap_stateful_plan, (*args, plan_params))\n        else:\n            torchscript_plan = jit.trace(translation_plan, args)\n\n        self.plan.torchscript = torchscript_plan\n        return self.plan\n\n    def remove(self):\n        self.plan.torchscript = None\n\n        return self.plan\n'"
syft/frameworks/keras/__init__.py,0,b'import syft\n\nif syft.dependency_check.tfe_available:\n    from syft.frameworks.keras import layers  # noqa: F401\n    from syft.frameworks.keras import model  # noqa: F401\n    from syft.frameworks.keras.hook import KerasHook\n\n    __all__ = [KerasHook.__name__]\n'
syft/frameworks/keras/hook.py,0,"b'import tf_encrypted as tfe\n\nfrom syft.frameworks.keras.model import serve\nfrom syft.frameworks.keras.model import share\nfrom syft.frameworks.keras.model import stop\nfrom syft.frameworks.keras.layers import add_constructor_registration\nfrom syft.frameworks.keras.layers import filter_layers\n\n\nclass KerasHook:\n    def __init__(self, keras):\n        self.keras = keras\n        if not hasattr(keras, ""_hooked""):\n            self._hook_layers()\n            self._hook_sequential()\n            self.keras._hooked = True\n\n    def _hook_layers(self):\n        for layer_cls in filter_layers(self.keras.layers, tfe.keras.layers):\n            layer_cls = add_constructor_registration(layer_cls)\n\n    def _hook_sequential(self):\n        seq_cls = getattr(self.keras, ""Sequential"")\n        setattr(seq_cls, ""share"", share)\n        setattr(seq_cls, ""serve"", serve)\n        setattr(seq_cls, ""stop"", stop)\n'"
syft/frameworks/tensorflow/__init__.py,0,"b'import syft\n\nif syft.dependency_check.tensorflow_available:\n    from syft_tensorflow.hook import TensorFlowHook\n    from syft_tensorflow.hook import hook_args  # noqa: F401\n    from syft_tensorflow.syft_types import TensorFlowTensor  # noqa: F401\n\n    setattr(syft, ""TensorFlowHook"", TensorFlowHook)\n'"
syft/frameworks/torch/__init__.py,0,b''
syft/frameworks/torch/functions.py,0,"b'from typing import List\n\nfrom syft.generic.pointers.multi_pointer import MultiPointerTensor\nfrom syft.generic.pointers.object_pointer import ObjectPointer\n\n\ndef combine_pointers(*pointers: List[ObjectPointer]) -> MultiPointerTensor:\n    """"""Accepts a list of pointers and returns them as a\n    MultiPointerTensor. See MultiPointerTensor docs for\n    details.\n\n    Arg:\n        *pointers: a list of pointers to tensors (including\n            their wrappers like normal)\n\n    """"""\n\n    return MultiPointerTensor(children=pointers).wrap()\n'"
syft/frameworks/torch/torch_attributes.py,12,"b'import re\nfrom types import ModuleType\n\nfrom syft.frameworks.torch.tensors.interpreters.native import TorchTensor\nfrom syft.generic.frameworks.attributes import FrameworkAttributes\n\n\nclass TorchAttributes(FrameworkAttributes):\n    """"""Adds torch module related custom attributes.\n\n    TorchAttributes is a special class where all custom attributes related\n    to the torch module can be added. Any global parameter, configuration,\n    or reference relating to PyTorch should be stored here instead of\n    attaching it directly to some other part of the global namespace.\n\n    The main reason we need this is because the hooking process occasionally\n    needs to save global objects, notably including what methods to hook and\n    what methods to NOT hook.\n\n    This will hold all necessary attributes PySyft needs.\n\n    Args:\n        torch: A ModuleType indicating the torch module\n        hook: A TorchHook to stash\n    """"""\n\n    # Subclasses must provide the following class attributes\n    ALIAS = ""torch""\n    Tensor = TorchTensor\n\n    def __init__(self, torch: ModuleType, hook: ModuleType) -> None:\n        """"""Initialization of the TorchAttributes class.""""""\n\n        # Stash the hook here for global access elsewhere\n        self.hook = hook\n\n        # SECTION: List all functions in torch module that we want to overload\n\n        # List modules that we will hook\n        self.torch_modules = {\n            ""torch"": torch,\n            ""torch.functional"": torch.functional,\n            ""torch.nn.functional"": torch.nn.functional,\n        }\n\n        # Set of all function names with module as prefix in the modules to hook\n        self._torch_functions = {\n            f""{module_name}.{func_name}""\n            for module_name, torch_module in self.torch_modules.items()\n            for func_name in dir(torch_module)\n        }\n\n        # Add special functions to exclude from the hook **in alphabetical order**\n        # Reasons can be:\n        # - Used for internal process like printing tensors\n        # - Don\'t use tensors so are bound to have local executions\n        # - etc\n        # DON\'T put here:\n        # - functions like native_*\n        # - functions that could use pointers or syft tensors\n        self.exclude = [\n            ""as_tensor"",\n            ""from_numpy"",\n            ""get_default_dtype"",\n            ""get_device"",\n            ""get_file_path"",\n            ""get_num_threads"",\n            ""get_rng_state"",\n            ""has_names"",\n            ""int"",\n            ""int16"",\n            ""int32"",\n            ""int64"",\n            ""int8"",\n            ""is_anomaly_enabled"",\n            ""is_complex"",\n            ""is_distributed"",\n            ""is_floating_point"",\n            ""is_grad_enabled"",\n            ""is_nonzero"",\n            ""is_same_size"",\n            ""is_signed"",\n            ""is_storage"",\n            ""is_tensor"",\n            ""isclose"",\n            ""isfinite"",\n            ""load"",\n            ""save"",\n            ""set_"",\n            ""set_num_threads"",\n            ""short"",\n            ""size"",\n            ""storage"",\n            ""storage_offset"",\n            ""stride"",\n            ""tensor"",\n            ""typename"",\n        ]\n\n        self.worker_methods = [""tensor"", ""rand"", ""zeros"", ""randn"", ""randint""]\n\n        # SECTION: Build the guard, that define which functions or methods can be safely called by\n        # external or local workers\n\n        # Add all tensor types\n        self.guard = {\n            ""FloatTensor"": torch.FloatTensor,\n            ""DoubleTensor"": torch.DoubleTensor,\n            ""HalfTensor"": torch.HalfTensor,\n            ""ByteTensor"": torch.ByteTensor,\n            ""CharTensor"": torch.CharTensor,\n            ""ShortTensor"": torch.ShortTensor,\n            ""IntTensor"": torch.IntTensor,\n            ""LongTensor"": torch.LongTensor,\n            ""Parameter"": torch.nn.Parameter,\n        }\n\n        # Allow the `syft.` prefix to be used\n        keys = list(self.guard.keys())\n        for key in keys:\n            self.guard[f""syft.{key}""] = self.guard[key]\n\n        # Concatenate torch functions\n        self.allowed_commands = self._torch_functions\n\n        # The equivalent concatenation of native torch function names and native torch method names\n        self.native_commands = {\n            command_name: self.get_native_framework_name(command_name)\n            for command_name in self.allowed_commands\n        }\n\n        self.command_guard = self._command_guard\n\n        # Dict {method_name: <is_inplace:bool>\n        self.inplace_methods = {}\n        self._inplace_pattern = re.compile(r""(^__i(?!nit|mport|ter).+_)|^((?!^_+).+[^_])_$"")\n        # Positives:\n        # __iadd__, share_\n\n        # Negatives:\n        # __init__, __import__, __iter__, __foo__, __bar_foo\n\n    def is_inplace_method(self, method_name):\n        """"""Determine if a method is inplace or not.\n\n        Check if the method ends by _ and is not a __xx__, then stash for\n        constant-time lookup.\n\n        Args:\n            method_name: The name for the method.\n        Returns:\n            Boolean denoting if the method is inplace or not.\n        """"""\n        try:\n            return self.inplace_methods[method_name]\n        except KeyError:\n            is_inplace = True if re.search(self._inplace_pattern, method_name) else False\n\n            self.inplace_methods[method_name] = is_inplace\n            return is_inplace\n'"
syft/generic/abstract/__init__.py,0,b''
syft/generic/abstract/hookable.py,0,"b'from functools import wraps\n\n\ndef map_chain_call(obj, method_name, *args, **kwargs):\n    """"""Calls each hook method in sequence and creates a list of the return values""""""\n    results = []\n    current = obj\n    while current is not None:\n        method = getattr(current, method_name, None)\n        if method:\n            results.append(method(*args, **kwargs))\n        current = getattr(current, ""child"", None)\n    return results\n\n\ndef reduce_chain_call(obj, method_name, initial_val, *args, **kwargs):\n    """"""Calls each hook method in sequence, passing return values from one to the next""""""\n    result = initial_val\n    current = obj\n    while current is not None:\n        method = getattr(current, method_name, None)\n        if method:\n            result = method(result, *args, **kwargs)\n        current = getattr(current, ""child"", None)\n    return result\n\n\ndef hookable(hookable_method):\n    """"""Decorator which checks for corresponding hooks and calls them if they exist\n\n    When this decorator is applied to a method, it checks for the existence of `_before_method()`\n    and `_after_method()` hooks, and calls them before/after the correspdoning method if they do.\n\n    This function should be used only as a decorator.\n    """"""\n    method_name = hookable_method.__name__\n\n    @wraps(hookable_method)\n    def hooked_method(self, *args, **kwargs):\n        map_chain_call(self, f""_before_{method_name}"", *args, **kwargs)\n        return_val = hookable_method(self, *args, **kwargs)\n        return_val = reduce_chain_call(self, f""_after_{method_name}"", return_val, *args, **kwargs)\n        return return_val\n\n    return hooked_method\n'"
syft/generic/abstract/message_handler.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\n\nfrom syft.generic.object_storage import ObjectStore\n\n\nclass AbstractMessageHandler(ABC):\n    def __init__(self, object_store: ObjectStore):\n        self.object_store = object_store\n        self.routing_table = self.init_routing_table()\n\n    @abstractmethod\n    def init_routing_table(self):\n        return {}\n\n    def supports(self, msg):\n        return type(msg) in self.routing_table.keys()\n\n    def handle(self, msg):\n        return self.routing_table[type(msg)](msg)\n'"
syft/generic/abstract/object.py,1,"b'from abc import ABC\nimport functools\nfrom typing import Set\n\nimport syft as sy\nfrom syft.generic.frameworks.hook import hook_args\n\n\nclass AbstractObject(ABC):\n    """"""\n    This is a generic object abstraction.\n    """"""\n\n    is_wrapper = False\n\n    def __init__(\n        self,\n        id: int = None,\n        owner: ""sy.workers.AbstractWorker"" = None,\n        tags: Set[str] = None,\n        description: str = None,\n        child=None,\n    ):\n        """"""Initializer for AbstractTensor\n\n        Args:\n            id: An optional string or integer id of the tensor\n            owner: An optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            tags: an optional set of hashtags corresponding to this tensor\n                which this tensor should be searchable for\n            description: an optional string describing the purpose of the\n                tensor\n            child: an optional tensor to put in the .child attribute to build\n                a chain of tensors\n        """"""\n        self.owner = owner or sy.local_worker\n        self.id = id or sy.ID_PROVIDER.pop()\n        self.tags = tags or set()\n        self.description = description\n        self.child = child\n\n    def __str__(self) -> str:\n        if hasattr(self, ""child""):\n            return type(self).__name__ + "">"" + self.child.__str__()\n        else:\n            return type(self).__name__\n\n    def __repr__(self) -> str:\n        if hasattr(self, ""child""):\n            return type(self).__name__ + "">"" + self.child.__repr__()\n        else:\n            return type(self).__name__\n\n    def describe(self, description: str) -> ""AbstractObject"":\n        self.description = description\n        return self\n\n    def tag(self, *tags: str) -> ""AbstractObject"":\n        self.tags = self.tags or set()\n\n        for tag in tags:\n            self.tags.add(tag)\n\n        self.owner.object_store.register_tags(self)\n\n        return self\n\n    def get_class_attributes(self):\n        """"""\n        Return all elements which defines an instance of a certain class.\n        By default there is nothing so we return an empty dict, but for\n        example for fixed precision tensor, the fractional precision is\n        very important.\n        """"""\n        return {}\n\n    @classmethod\n    def on_function_call(cls, *args):\n        """"""\n        Override this to perform a specific action for each call of a torch\n        function with arguments containing syft tensors of the class doing\n        the overloading\n        """"""\n        pass\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """"""\n        Receive an instruction for a function to be applied on a Syft Tensor,\n        Replace in the args_ all the LogTensors with\n        their child attribute, forward the command instruction to the\n        handle_function_command of the type of the child attributes, get the\n        response and replace a Syft Tensor on top of all tensors found in\n        the response.\n\n        Args:\n            command: instruction of a function command: (command name,\n            <no self>, arguments[, kwargs_])\n\n        Returns:\n            the response of the function command\n        """"""\n        cmd, _, args_, kwargs_ = command\n\n        # Check that the function has not been overwritten\n        try:\n            # Try to get recursively the attributes in cmd = ""<attr1>.<attr2>.<attr3>...""\n            cmd = cls.rgetattr(cls, cmd)\n            return cmd(*args_, **kwargs_)\n        except AttributeError:\n            pass\n\n        # Replace all LoggingTensor with their child attribute\n        new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(cmd, args_, kwargs_)\n\n        # build the new command\n        new_command = (cmd, None, new_args, new_kwargs)\n\n        # Do a generic action depending og the call\n        cls.on_function_call(new_command)\n\n        # Send it to the appropriate class and get the response\n        response = new_type.handle_func_command(new_command)\n\n        # Put back LoggingTensor on the tensors found in the response\n        response = hook_args.hook_response(cmd, response, wrap_type=cls)\n\n        return response\n\n    @classmethod\n    def rgetattr(cls, obj, attr, *args):\n        """"""\n        Get an attribute recursively.\n\n        This is a core piece of functionality for the PySyft tensor chain.\n\n        Args:\n            obj: the object holding the attribute\n            attr: nested attribute\n            args: optional arguments to provide\n\n        Returns:\n            the attribute obj.attr\n\n        Example:\n            >>> rgetattr(obj, \'attr1.attr2.attr3\')\n            [Out] obj.attr1.attr2.attr3\n\n        """"""\n\n        def _getattr(obj, attr):\n            return getattr(obj, attr, *args)\n\n        return functools.reduce(_getattr, [obj] + attr.split("".""))\n\n\ndef initialize_object(\n    hook, obj, owner=None, reinitialize=True, id=None, init_args=(), init_kwargs={}\n):\n    """"""Initializes the tensor.\n\n    Args:\n        hook: A reference to TorchHook class.\n        obj: An object to keep track of id, owner and whether it is a native\n            tensor or a wrapper over pytorch.\n        reinitialize: A boolean parameter (default True) to indicate whether\n            to re-execute __init__.\n        owner: The owner of the tensor being initialised, leave it blank\n            to if you have already provided a reference to TorchHook class.\n        id: The id of tensor, a random id will be generated if there is no id\n            specified.\n    """"""\n    obj.is_wrapper = False\n\n    if reinitialize:\n        obj.native___init__(*init_args, **init_kwargs)\n\n    _apply_args(hook, obj, owner, id)\n\n\ndef _apply_args(hook, obj_to_register, owner=None, id=None):\n\n    if owner is None:\n        owner = hook.local_worker\n\n    if id is None:\n        id = sy.ID_PROVIDER.pop()\n\n    obj_to_register.id = id\n    obj_to_register.owner = owner\n'"
syft/generic/abstract/sendable.py,0,"b'from syft.serde.syft_serializable import SyftSerializable\nfrom syft.generic.abstract.object import AbstractObject\n\n\nclass AbstractSendable(AbstractObject, SyftSerializable):\n    """"""\n    This layers functionality for sending objects between workers on top of AbstractObject.\n    """"""\n\n    def send(self, destination):\n        return self.owner.send_obj(self, destination)\n'"
syft/generic/abstract/tensor.py,2,"b'from typing import List\nimport weakref\n\nimport syft as sy\nfrom syft.generic.abstract.object import _apply_args  # noqa: F401\nfrom syft.generic.abstract.sendable import AbstractSendable\nfrom syft.generic.abstract.object import initialize_object\nfrom syft.serde.syft_serializable import SyftSerializable\n\n\nclass AbstractTensor(AbstractSendable, SyftSerializable):\n    def __init__(\n        self,\n        id: int = None,\n        owner: ""sy.workers.AbstractWorker"" = None,\n        tags: List[str] = None,\n        description: str = None,\n        child=None,\n    ):\n        super(AbstractTensor, self).__init__(id, owner, tags, description, child)\n\n    def wrap(self, register=True, type=None, **kwargs):\n        """"""Wraps the class inside an empty object of class `type`.\n\n        Because PyTorch/TF do not (yet) support functionality for creating\n        arbitrary Tensor types (via subclassing torch.Tensor), in order for our\n        new tensor types (such as PointerTensor) to be usable by the rest of\n        PyTorch/TF (such as PyTorch\'s layers and loss functions), we need to\n        wrap all of our new tensor types inside of a native PyTorch type.\n\n        This function adds a .wrap() function to all of our tensor types (by\n        adding it to AbstractTensor), such that (on any custom tensor\n        my_tensor), my_tensor.wrap() will return a tensor that is compatible\n        with the rest of the PyTorch/TensorFlow API.\n\n        Returns:\n            A wrapper tensor of class `type`, or whatever is specified as\n            default by the current syft.framework.Tensor.\n        """"""\n        wrapper = sy.framework.hook.create_wrapper(type, **kwargs)\n        wrapper.child = self\n        wrapper.is_wrapper = True\n        wrapper.child.parent = weakref.ref(wrapper)\n\n        if self.id is None:\n            self.id = sy.ID_PROVIDER.pop()\n\n        if self.owner is not None and register:\n            self.owner.register_obj(wrapper, obj_id=self.id)\n\n        return wrapper\n\n    def on(self, tensor: ""AbstractTensor"", wrap: bool = True) -> ""AbstractTensor"":\n        """"""\n        Add a syft(log) tensor on top of the tensor.\n\n        Args:\n            tensor: the tensor to extend\n            wrap: if true, add the syft tensor between the wrapper\n            and the rest of the chain. If false, just add it at the top\n\n        Returns:\n            a syft/torch tensor\n        """"""\n        if not wrap:\n            self.child = tensor\n            return self\n        else:\n            # if tensor is a wrapper\n            if not hasattr(tensor, ""child""):\n                tensor = tensor.wrap()\n\n            # We usually call .on() on newly created tensor so it\'s not a sacrilege\n            # to rewrite its id\n            self.id = tensor.id\n\n            self.child = tensor.child\n            tensor.child = self\n            return tensor\n\n    def copy(self):\n        return self + 0\n\n    def clone(self):\n        """"""\n        Clone should keep ids unchanged, contrary to copy\n        """"""\n        cloned_tensor = type(self)(**self.get_class_attributes())\n        cloned_tensor.id = self.id\n        cloned_tensor.owner = self.owner\n\n        if hasattr(self, ""child"") and self.child is not None:\n            cloned_tensor.child = self.child.clone()\n\n        return cloned_tensor\n\n    def refresh(self):\n        """"""\n        Forward to Additive Shared Tensor the call to refresh shares\n        """"""\n        if hasattr(self, ""child""):\n            self.child = self.child.refresh()\n            return self\n        else:\n            raise AttributeError(""Refresh should only be called on AdditiveSharedTensors"")\n\n    @property\n    def shape(self):\n        return self.child.shape\n\n    def __len__(self) -> int:\n        """"""Alias .shape[0] with len(), helpful for pointers""""""\n        try:\n            if hasattr(self, ""child"") and not isinstance(self.child, dict):\n                return self.child.shape[0]\n            else:\n                return self.shape[0]\n        except IndexError:\n            return 0\n\n    @property\n    def grad(self):\n        child_grad = self.child.grad\n        if child_grad is None:\n            return None\n        else:\n            return child_grad.wrap()\n\n    def get(self):\n        """"""Just a pass through. This is most commonly used when calling .get() on a\n        Syft tensor which has a child which is a pointer, an additive shared tensor,\n        a multi-pointer, etc.""""""\n        class_attributes = self.get_class_attributes()\n        return type(self)(\n            **class_attributes,\n            owner=self.owner,\n            tags=self.tags,\n            description=self.description,\n            id=self.id,\n        ).on(self.child.get())\n\n    def mid_get(self):\n        """"""This method calls .get() on a child pointer and correctly registers the results""""""\n\n        child_id = self.id\n        tensor = self.get()\n        tensor.id = child_id\n        self.owner.register_obj(tensor)\n\n\ndef initialize_tensor(hook, obj, owner=None, id=None, init_args=(), init_kwargs={}):\n    """"""Initializes the tensor.\n\n    Args:\n        hook: A reference to TorchHook class.\n        cls: An object to keep track of id, owner and whether it is a native\n            tensor or a wrapper over pytorch.\n        is_tensor: A boolean parameter (default False) to indicate whether\n            it is torch tensor or not.\n        owner: The owner of the tensor being initialised, leave it blank\n            to if you have already provided a reference to TorchHook class.\n        id: The id of tensor, a random id will be generated if there is no id\n            specified.\n    """"""\n    initialize_object(\n        hook,\n        obj,\n        owner=owner,\n        reinitialize=False,\n        id=id,\n        init_args=init_args,\n        init_kwargs=init_kwargs,\n    )\n'"
syft/generic/frameworks/__init__.py,0,b'from syft.generic.frameworks.types import framework_packages  # noqa: F401\n'
syft/generic/frameworks/attributes.py,3,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom types import ModuleType\nfrom typing import Union\nfrom typing import Callable\nfrom typing import Any\n\nfrom syft.generic.frameworks.hook.hook import FrameworkHook\n\n\nclass FrameworkAttributes(ABC):\n    @abstractmethod\n    def __init__(self, framework: ModuleType, hook: FrameworkHook):\n        pass\n\n    # Forcing subclasses to define a class-level constant; see\n    # https://stackoverflow.com/a/53417582 for nuance\n    @property\n    @classmethod\n    @abstractmethod\n    def ALIAS(cls):\n        pass\n\n    @property\n    @classmethod\n    @abstractmethod\n    def Tensor(cls):\n        """"""Default Tensor wrapper.""""""\n        pass\n\n    @abstractmethod\n    def is_inplace_method(self, method_name):\n        """"""Determine if a method is inplace or not.\n\n        Framework-dependent, see subclasses for details.\n\n        Args:\n            method_name: The name for the method.\n        Returns:\n            Boolean denoting if the method is inplace or not.\n        """"""\n        pass\n\n    def _command_guard(\n        self, command: str, get_native: bool = False\n    ) -> Union[Callable[..., Any], str]:\n        """"""Check command can be safely used.\n\n        Args:\n            command: A string indicating command name.\n            get_native: A boolean parameter (default False) to indicate whether\n                to return the command name or the native torch function. If\n                False, return command name else return the native torch\n                function.\n\n        Returns:\n            The command name or a native framework function\n        """"""\n        if command not in self.allowed_commands:\n            raise RuntimeError(f\'Command ""{command}"" is not a supported {self.ALIAS} operation.\')\n        if get_native:\n            return self.native_commands[command]\n        return command\n\n    def _is_command_valid_guard(self, command: str) -> bool:\n        """"""Validate the command.\n\n        Indicates whether a command is valid with respect to the framework\n        guard.\n\n        Args:\n            command: A string indicating command to test.\n            framework_domain: A string indicating the framework domain or\n                module in which the command is supposed to be, e.g.\n                dir(torch), dir(torch.Tensor), dir(tensorflow), etc. (roughly)\n\n        Returns:\n            A boolean indicating whether the command is valid.\n        """"""\n        try:\n            self._command_guard(command)\n        except RuntimeError:\n            return False\n        return True\n\n    @classmethod\n    def get_native_framework_name(cls, attr: str) -> str:\n        """"""Return the name of the native command for the given hooked command.\n\n        Args:\n            attr: A string indicating the hooked command name (ex: torch.add)\n\n        Returns:\n            The name of the native command (ex: torch.native_add)\n        """"""\n        parts = attr.split(""."")\n        parts[-1] = ""native_"" + parts[-1]\n        native_func_name = ""."".join(parts)\n        return native_func_name\n'"
syft/generic/frameworks/overload.py,0,"b'from syft.generic.frameworks.hook import hook_args\n\n\nclass Module(object):\n    pass\n\n\nclass Overloaded:\n    def __init__(self):\n        self.method = Overloaded.overload_method\n        self.function = Overloaded.overload_function\n        self.module = Overloaded.overload_module\n\n    @staticmethod\n    def overload_method(attr):\n        """"""\n        hook args and response for methods that hold the @overloaded.method decorator\n        """"""\n\n        def _hook_method_args(self, *args, **kwargs):\n            # Replace all syft tensor with their child attribute\n            new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                attr.__name__, self, args, kwargs\n            )\n\n            # Send it to the appropriate class and get the response\n            response = attr(self, new_self, *new_args, **new_kwargs)\n\n            # Put back SyftTensor on the tensors found in the response\n            response = hook_args.hook_response(\n                attr.__name__, response, wrap_type=type(self), wrap_args=self.get_class_attributes()\n            )\n\n            return response\n\n        return _hook_method_args\n\n    @staticmethod\n    def overload_function(attr):\n        """"""\n        hook args and response for functions that hold the @overloaded.function decorator\n        """"""\n\n        def _hook_function_args(*args, **kwargs):\n\n            # TODO have a better way to infer the type of tensor -> this is implies\n            # that the first argument is a tensor (even if this is the case > 99%)\n            tensor = args[0] if not isinstance(args[0], (tuple, list)) else args[0][0]\n            cls = type(tensor)\n\n            # Replace all syft tensor with their child attribute\n            new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(\n                attr.__name__, args, kwargs\n            )\n\n            # Send it to the appropriate class and get the response\n            response = attr(*new_args, **new_kwargs)\n\n            # Put back SyftTensor on the tensors found in the response\n            response = hook_args.hook_response(\n                attr.__name__, response, wrap_type=cls, wrap_args=tensor.get_class_attributes()\n            )\n\n            return response\n\n        return _hook_function_args\n\n    @staticmethod\n    def overload_module(attr):\n\n        module = Module()\n        attr(module)\n\n        return module\n\n\noverloaded = Overloaded()\n'"
syft/generic/frameworks/remote.py,0,"b'class Remote(object):\n    frameworks = {}\n\n    def __init__(self, worker, framework_name):\n        inst = Remote.frameworks[framework_name](worker)\n        setattr(self, inst.name, inst)\n\n    @staticmethod\n    def register_framework(cls):\n        assert cls.name not in Remote.frameworks\n        Remote.frameworks[cls.name] = cls\n'"
syft/generic/frameworks/types.py,5,"b'from typing import Union\n\nfrom syft import dependency_check\n\nframework_packages = {}\n\nframework_tensors = []\nframework_shapes = []\nframework_layer_modules = []\n\nif dependency_check.tensorflow_available:\n    import tensorflow as tf\n    from tensorflow.python.framework.ops import EagerTensor\n    from tensorflow.python.ops.resource_variable_ops import ResourceVariable\n\n    framework_packages[""tensorflow""] = tf\n\n    framework_tensors.append(EagerTensor)\n    framework_tensors.append(ResourceVariable)\n    framework_shapes.append(tf.TensorShape)\n\n    framework_layer_modules.append(tf.Module)\n\nif dependency_check.torch_available:\n    import torch\n\n    framework_packages[""torch""] = torch\n\n    framework_tensors.append(torch.Tensor)\n    framework_tensors.append(torch.nn.Parameter)\n    framework_shapes.append(torch.Size)\n\n    framework_layer_module = torch.nn.Module\n    framework_layer_module.named_tensors = torch.nn.Module.named_parameters\n    framework_layer_modules.append(framework_layer_module)\n\nframework_tensors = tuple(framework_tensors)\nFrameworkTensorType = Union[framework_tensors]\nFrameworkTensor = framework_tensors\n\nframework_shapes = tuple(framework_shapes)\nFrameworkShapeType = Union[framework_shapes]\nFrameworkShape = framework_shapes\n\nframework_layer_modules = tuple(framework_layer_modules)\nFrameworkLayerModuleType = Union[framework_layer_modules]\nFrameworkLayerModule = framework_layer_modules\n'"
syft/generic/pointers/__init__.py,0,b'from .object_wrapper import ObjectWrapper  # noqa: F401\n'
syft/generic/pointers/callable_pointer.py,0,"b'from typing import List\nfrom typing import Union\nfrom typing import TYPE_CHECKING\n\nimport syft as sy\nfrom syft.generic.pointers.object_pointer import ObjectPointer\n\n# this if statement avoids circular imports\nif TYPE_CHECKING:\n    from syft.workers.base import BaseWorker\n\n\nclass CallablePointer(ObjectPointer):\n    """""" A class of pointers that are callable\n\n    A CallablePointer is an ObjectPointer which implements the __call__ function.\n    This lets you execute a command directly on the object to which it points.\n    """"""\n\n    def __init__(\n        self,\n        location: ""BaseWorker"" = None,\n        id_at_location: Union[str, int] = None,\n        owner: ""BaseWorker"" = None,\n        id: Union[str, int] = None,\n        garbage_collect_data: bool = True,\n        point_to_attr: str = None,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n        """"""\n\n        Args:\n            location: An optional BaseWorker object which points to the worker\n                on which this pointer\'s object can be found.\n            id_at_location: An optional string or integer id of the object\n                being pointed to.\n            owner: An optional BaseWorker object to specify the worker on which\n                the pointer is located. It is also where the pointer is\n                registered if register is set to True. Note that this is\n                different from the location parameter that specifies where the\n                pointer points to.\n            id: An optional string or integer id of the PointerTensor.\n            garbage_collect_data: If true (default), delete the remote object when the\n                pointer is deleted.\n            point_to_attr: string which can tell a pointer to not point directly to\\\n                an object, but to point to an attribute of that object such as .child or\n                .grad. Note the string can be a chain (i.e., .child.child.child or\n                .grad.child.child). Defaults to None, which means don\'t point to any attr,\n                just point to then object corresponding to the id_at_location.\n        """"""\n        super().__init__(\n            location=location,\n            id_at_location=id_at_location,\n            owner=owner,\n            id=id,\n            garbage_collect_data=garbage_collect_data,\n            point_to_attr=point_to_attr,\n            tags=tags,\n            description=description,\n        )\n\n    def __call__(self, *args, **kwargs):\n\n        return_ids = (sy.ID_PROVIDER.pop(),)\n        response = self.owner.send_command(\n            cmd_name=""__call__"",\n            target=self.id_at_location,\n            args_=args,\n            kwargs_=kwargs,\n            recipient=self.location,\n            return_ids=return_ids,\n        )\n        return response\n\n\ndef create_callable_pointer(\n    location: ""BaseWorker"",\n    id: (str or int),\n    id_at_location: (str or int),\n    owner: ""BaseWorker"",\n    tags,\n    description,\n    garbage_collect_data: bool = True,\n    register_pointer: bool = True,\n) -> ObjectPointer:\n    """"""Creates a callable pointer to the object identified by the pair (location, id_at_location).\n\n    Note, that there is no check whether an object with this id exists at the location.\n\n    Args:\n        location:\n        id:\n        id_at_location:\n        owner:\n        tags:\n        description:\n        garbage_collect_data:\n        register_pointer:\n    """"""\n\n    if id is None:\n        id = sy.ID_PROVIDER.pop()\n\n    ptr = CallablePointer(\n        location=location,\n        id_at_location=id_at_location,\n        owner=owner,\n        id=id,\n        garbage_collect_data=garbage_collect_data,\n        tags=tags,\n        description=description,\n    )\n\n    if register_pointer:\n        owner.register_obj(ptr)\n\n    return ptr\n'"
syft/generic/pointers/multi_pointer.py,0,"b'from typing import List\nfrom typing import Union\n\nimport syft as sy\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.generic.frameworks.types import FrameworkShapeType\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.workers.base import BaseWorker\n\n\nclass MultiPointerTensor(AbstractTensor):\n    """"""\n    The MultiPointerTensor gathers together several pointers to different locations which can\n    be operated all at once. The pointers can be referencing the same value or a different one\n    depending on the usage.\n\n    The MultiPointerTensor has the same structure that the AdditiveSharedTensor: its child\n    attribute is a dictionary {worker.id: Pointer}\n\n    MultiPointerTensor can be directly instantiated using x.send(worker1, worker2, etc) where\n    x is a syft or framework tensor. In that case, the value of x will be sent and replicated to\n    each workers.\n    """"""\n\n    def __init__(\n        self,\n        owner: BaseWorker = None,\n        id: Union[str, int] = None,\n        tags: List[str] = None,\n        description: str = None,\n        children: List[AbstractTensor] = [],\n    ):\n        """"""Initializes an Multi Pointer Tensor, whose behaviour is to keep references\n        to several pointers and to distribute computations to all of them in an easy\n        way.\n\n        Args:\n            owner: an optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            id: An optional string or integer id of the MultiPointerTensor.\n            tags: an optional set of hashtags corresponding to this tensor\n                which this tensor should be searchable for\n            description: an optional string describing the purpose of the\n                tensor\n            children: an optional list of children which are PointerTensors\n        """"""\n\n        super().__init__(tags, description)\n\n        self.owner = owner\n        self.id = id\n\n        self.child = {}\n        for c in children:\n            assert c.shape == children[0].shape\n            self.child[c.location.id] = c\n\n    def __str__(self):\n        type_name = type(self).__name__\n        out = f""["" f""{type_name}]""\n        for v in self.child.values():\n            out += ""\\n\\t-> "" + str(v)\n        return out\n\n    @property\n    @overloaded.method\n    def grad(self, self_):\n        results = {}\n        all_none = True\n        for worker, pointer in self_.items():\n            pointer_grad = pointer.grad\n\n            if pointer_grad is None:\n                results[worker] = None\n            elif pointer_grad.is_wrapper:\n                results[worker] = pointer_grad.child\n                all_none = False\n            else:\n                results[worker] = pointer_grad\n                all_none = False\n\n        return results if not all_none else None\n\n    def __eq__(self, other):\n        return self.eq(other)\n\n    def __add__(self, other):\n        """"""\n        Adding a MultiPointer (MPT) and an AdditiveShared Tensor (AST) should return an\n        AdditiveShared Tensor, so if we have this configuration, we permute self and\n        other to use the fact that other.__add__(...) return an object of type other\n\n        Else, we just redirect to .add which works well\n        """"""\n        if isinstance(other, sy.AdditiveSharingTensor):\n            return other.__add__(self)\n        else:\n            return self.add(other)\n\n    def __mul__(self, other):\n        """"""\n        See __add__ for details but, MPT * AST should return AST\n        """"""\n        if isinstance(other, sy.AdditiveSharingTensor):\n            return other.__mul__(self)\n        else:\n            return self.mul(other)\n\n    @property\n    def shape(self) -> FrameworkShapeType:\n        """"""This method returns the shape of the data being pointed to.\n        This shape information SHOULD be cached on self._shape, but\n        occasionally this information may not be present. If this is the\n        case, then it requests the shape information from the remote object\n        directly (which is inefficient and should be avoided).""""""\n\n        return list(self.child.values())[0].shape\n\n    def dim(self) -> int:\n        """"""This method fixes the error that the result of dim was a list of ints\n        stored inside a multipointer tensor""""""\n        return len(self.shape)\n\n    def get(self, sum_results: bool = False) -> FrameworkTensor:\n\n        results = []\n        for v in self.child.values():\n            results.append(v.get())\n\n        if sum_results:\n            return sum(results)\n\n        return results\n\n    def virtual_get(self, sum_results: bool = False):\n        """"""\n        Get the value of the tensor without sending `get` message\n\n        (Only for VirtualWorkers)\n        """"""\n\n        results = []\n        for v in self.child.values():\n            value = v.location.object_store.get_obj(v.id_at_location)\n            results.append(value)\n\n        if sum_results:\n            return sum(results)\n\n        return results\n\n    @staticmethod\n    def dispatch(args_, worker):\n        """"""\n        utility function for handle_func_command which help to select\n        shares (seen as elements of dict) in an argument set. It could\n        perhaps be put elsewhere\n\n        Args:\n            args_: arguments to give to a functions\n            worker: owner of the shares to select\n\n        Return:\n            args_ where the MultiPointerTensor are replaced by\n            the appropriate share\n        """"""\n        return map(lambda x: x[worker] if isinstance(x, dict) else x, args_)\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """"""\n        Receive an instruction for a function to be applied on a Syft Tensor,\n        Replace in the args all the LogTensors with\n        their child attribute, forward the command instruction to the\n        handle_function_command of the type of the child attributes, get the\n        response and replace a Syft Tensor on top of all tensors found in\n        the response.\n\n        Args:\n            command: instruction of a function command: (command name,\n            <no self>, arguments[, kwargs_])\n\n        Returns:\n            the response of the function command\n        """"""\n\n        cmd, _, args_, kwargs_ = command\n\n        tensor = args_[0]\n\n        # Check that the function has not been overwritten\n        try:\n            # Try to get recursively the attributes in cmd = ""<attr1>.<attr2>.<attr3>...""\n            cmd = cls.rgetattr(cls, cmd)\n            return cmd(*args_, **kwargs_)\n        except AttributeError:\n            pass\n\n        # Replace all LoggingTensor with their child attribute\n        new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(cmd, args_, kwargs_)\n\n        results = {}\n        for worker, share in new_args[0].items():\n            new_type = type(share)\n            new_args_worker = tuple(MultiPointerTensor.dispatch(new_args, worker))\n\n            # build the new command\n            new_command = (cmd, None, new_args_worker, new_kwargs)\n\n            # Send it to the appropriate class and get the response\n            results[worker] = new_type.handle_func_command(new_command)\n\n        # Put back MultiPointerTensor on the tensors found in the response\n        response = hook_args.hook_response(\n            cmd, results, wrap_type=cls, wrap_args=tensor.get_class_attributes()\n        )\n\n        return response\n\n    def set_garbage_collect_data(self, value):\n        shares = self.child\n        for _, share in shares.items():\n            share.child.garbage_collect_data = value\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, tensor: ""MultiPointerTensor"") -> tuple:\n        """"""\n        This function takes the attributes of a MultiPointerTensor and saves them in a tuple\n        Args:\n            tensor (MultiPointerTensor): a MultiPointerTensor\n        Returns:\n            tuple: a tuple holding the unique attributes of the additive shared tensor\n        Examples:\n            data = simplify(tensor)\n        """"""\n\n        chain = None\n        if hasattr(tensor, ""child""):\n            chain = sy.serde.msgpack.serde._simplify(worker, tensor.child)\n\n        return (sy.serde.msgpack.serde._simplify(worker, tensor.id), chain)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""MultiPointerTensor"":\n        """"""\n        This function reconstructs a MultiPointerTensor given it\'s attributes in form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the MultiPointerTensor\n        Returns:\n            MultiPointerTensor: a MultiPointerTensor\n        Examples:\n            multi_pointer_tensor = detail(data)\n        """"""\n\n        tensor_id, chain = tensor_tuple\n\n        tensor = sy.MultiPointerTensor(\n            owner=worker, id=sy.serde.msgpack.serde._detail(worker, tensor_id)\n        )\n\n        if chain is not None:\n            chain = sy.serde.msgpack.serde._detail(worker, chain)\n            tensor.child = chain\n\n        return tensor\n\n\n### Register the tensor with hook_args.py ###\nhook_args.default_register_tensor(MultiPointerTensor)\n'"
syft/generic/pointers/object_pointer.py,1,"b'from typing import List\nfrom typing import Union\nfrom typing import TYPE_CHECKING\nimport weakref\n\nimport syft\nfrom syft import exceptions\nfrom syft.generic.frameworks.hook.hook_args import one\nfrom syft.generic.frameworks.hook.hook_args import register_type_rule\nfrom syft.generic.frameworks.hook.hook_args import register_forward_func\nfrom syft.generic.frameworks.hook.hook_args import register_backward_func\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.generic.abstract.sendable import AbstractSendable\nfrom syft.messaging.message import ForceObjectDeleteMessage\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft.exceptions import RemoteObjectFoundError\nfrom syft.serde.syft_serializable import SyftSerializable\n\n# this if statement avoids circular imports between base.py and pointer.py\nif TYPE_CHECKING:\n    from syft.workers.abstract import AbstractWorker\n    from syft.workers.base import BaseWorker\n\n\nclass ObjectPointer(AbstractSendable, SyftSerializable):\n    """"""A pointer to a remote object.\n\n    An ObjectPointer forwards all API calls to the remote. ObjectPointer objects\n    point to objects. They exist to mimic the entire\n    API of an object, but instead of computing a function locally\n    (such as addition, subtraction, etc.) they forward the computation to a\n    remote machine as specified by self.location. Specifically, every\n    ObjectPointer has a object located somewhere that it points to (they should\n    never exist by themselves).\n    The objects being pointed to can be on the same machine or (more commonly)\n    on a different one. Note further that a ObjectPointer does not know the\n    nature how it sends messages to the object it points to (whether over\n    socket, http, or some other protocol) as that functionality is abstracted\n    in the BaseWorker object in self.location.\n    """"""\n\n    def __init__(\n        self,\n        location: ""BaseWorker"" = None,\n        id_at_location: Union[str, int] = None,\n        owner: ""BaseWorker"" = None,\n        id: Union[str, int] = None,\n        garbage_collect_data: bool = True,\n        point_to_attr: str = None,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n        """"""Initializes a ObjectPointer.\n\n        Args:\n            location: An optional BaseWorker object which points to the worker\n                on which this pointer\'s object can be found.\n            id_at_location: An optional string or integer id of the object\n                being pointed to.\n            owner: An optional BaseWorker object to specify the worker on which\n                the pointer is located. It is also where the pointer is\n                registered if register is set to True. Note that this is\n                different from the location parameter that specifies where the\n                pointer points to.\n            id: An optional string or integer id of the ObjectPointer.\n            garbage_collect_data: If true (default), delete the remote object when the\n                pointer is deleted.\n            point_to_attr: string which can tell a pointer to not point directly to\\\n                an object, but to point to an attribute of that object such as .child or\n                .grad. Note the string can be a chain (i.e., .child.child.child or\n                .grad.child.child). Defaults to None, which means don\'t point to any attr,\n                just point to then object corresponding to the id_at_location.\n        """"""\n        super().__init__(id=id, owner=owner, tags=tags, description=description)\n\n        self.location = location\n        self.id_at_location = id_at_location\n        self.garbage_collect_data = garbage_collect_data\n        self.point_to_attr = point_to_attr\n\n    @staticmethod\n    def create_pointer(\n        obj,\n        location: ""AbstractWorker"" = None,\n        id_at_location: (str or int) = None,\n        register: bool = False,\n        owner: ""AbstractWorker"" = None,\n        ptr_id: (str or int) = None,\n        garbage_collect_data=None,\n    ) -> ""ObjectPointer"":\n        """"""Creates a pointer to the ""self"" FrameworkTensor object.\n\n        This method is called on a FrameworkTensor object, returning a pointer\n        to that object. This method is the CORRECT way to create a pointer,\n        and the parameters of this method give all possible attributes that\n        a pointer can be created with.\n\n        Args:\n            location: The AbstractWorker object which points to the worker on which\n                this pointer\'s object can be found. In nearly all cases, this\n                is self.owner and so this attribute can usually be left blank.\n                Very rarely you may know that you are about to move the Tensor\n                to another worker so you can pre-initialize the location\n                attribute of the pointer to some other worker, but this is a\n                rare exception.\n            id_at_location: A string or integer id of the tensor being pointed\n                to. Similar to location, this parameter is almost always\n                self.id and so you can leave this parameter to None. The only\n                exception is if you happen to know that the ID is going to be\n                something different than self.id, but again this is very rare\n                and most of the time, setting this means that you are probably\n                doing something you shouldn\'t.\n            register: A boolean parameter (default False) that determines\n                whether to register the new pointer that gets created. This is\n                set to false by default because most of the time a pointer is\n                initialized in this way so that it can be sent to someone else\n                (i.e., ""Oh you need to point to my tensor? let me create a\n                pointer and send it to you"" ). Thus, when a pointer gets\n                created, we want to skip being registered on the local worker\n                because the pointer is about to be sent elsewhere. However, if\n                you are initializing a pointer you intend to keep, then it is\n                probably a good idea to register it, especially if there is any\n                chance that someone else will initialize a pointer to your\n                pointer.\n            owner: A AbstractWorker parameter to specify the worker on which the\n                pointer is located. It is also where the pointer is registered\n                if register is set to True.\n            ptr_id: A string or integer parameter to specify the id of the pointer\n                in case you wish to set it manually for any special reason.\n                Otherwise, it will be set randomly.\n            garbage_collect_data: If true (default), delete the remote tensor when the\n                pointer is deleted.\n            local_autograd: Use autograd system on the local machine instead of PyTorch\'s\n                autograd on the workers.\n            preinitialize_grad: Initialize gradient for AutogradTensors to a tensor.\n\n        Returns:\n            A FrameworkTensor[ObjectPointer] pointer to self. Note that this\n            object itself will likely be wrapped by a FrameworkTensor wrapper.\n        """"""\n        if owner is None:\n            owner = obj.owner\n\n        if location is None:\n            location = obj.owner.id\n\n        owner = obj.owner.get_worker(owner)\n        location = obj.owner.get_worker(location)\n\n        ptr = ObjectPointer(\n            location=location,\n            id_at_location=id_at_location,\n            owner=owner,\n            id=ptr_id,\n            garbage_collect_data=True if garbage_collect_data is None else garbage_collect_data,\n            tags=obj.tags,\n            description=obj.description,\n        )\n\n        return ptr\n\n    def wrap(self, register=True, type=None, **kwargs):\n        """"""Wraps the class inside framework tensor.\n\n        Because PyTorch/TF do not (yet) support functionality for creating\n        arbitrary Tensor types (via subclassing torch.Tensor), in order for our\n        new tensor types (such as PointerTensor) to be usable by the rest of\n        PyTorch/TF (such as PyTorch\'s layers and loss functions), we need to\n        wrap all of our new tensor types inside of a native PyTorch type.\n\n        This function adds a .wrap() function to all of our tensor types (by\n        adding it to AbstractTensor), such that (on any custom tensor\n        my_tensor), my_tensor.wrap() will return a tensor that is compatible\n        with the rest of the PyTorch/TensorFlow API.\n\n        Returns:\n            A wrapper tensor of class `type`, or whatever is specified as\n            default by the current syft.framework.Tensor.\n        """"""\n        wrapper = syft.framework.hook.create_wrapper(type, **kwargs)\n        wrapper.child = self\n        wrapper.is_wrapper = True\n        wrapper.child.parent = weakref.ref(wrapper)\n\n        if self.id is None:\n            self.id = syft.ID_PROVIDER.pop()\n\n        if self.owner is not None and register:\n            self.owner.register_obj(wrapper, obj_id=self.id)\n\n        return wrapper\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """"""\n        Receive an instruction for a function to be applied on a Pointer,\n        Get the remote location to send the command, send it and get a\n        pointer to the response, return.\n        :param command: instruction of a function command: (command name,\n        None, arguments[, kwargs_])\n        :return: the response of the function command\n        """"""\n        pointer = cls.find_a_pointer(command)\n        # Get info on who needs to send where the command\n        owner = pointer.owner\n        location = pointer.location\n\n        cmd, _, args_, kwargs_ = command\n\n        # Send the command\n        response = owner.send_command(location, cmd_name=cmd, args_=args_, kwargs_=kwargs_)\n\n        return response\n\n    @classmethod\n    def find_a_pointer(cls, command):\n        """"""\n        Find and return the first pointer in the args_ object, using a trick\n        with the raising error RemoteObjectFoundError\n        """"""\n        try:\n            cmd, _, args_, kwargs_ = command\n            _ = hook_args.unwrap_args_from_function(cmd, args_, kwargs_)\n        except exceptions.RemoteObjectFoundError as err:\n            pointer = err.pointer\n            return pointer\n\n    def get(self, user=None, reason: str = """", deregister_ptr: bool = True):\n        """"""Requests the object being pointed to.\n\n        The object to which the pointer points will be requested, serialized and returned.\n\n        Note:\n            This will typically mean that the remote object will be\n            removed/destroyed.\n\n        Args:\n            user (obj, optional) : authenticate/allow user to perform get on remote private objects.\n            reason (str, optional) : a description of why the data scientist wants to see it.\n            deregister_ptr (bool, optional): this determines whether to\n                deregister this pointer from the pointer\'s owner during this\n                method. This defaults to True because the main reason people use\n                this method is to move the tensor from the location to the\n                local one, at which time the pointer has no use.\n\n        Returns:\n            An AbstractObject object which is the tensor (or chain) that this\n            object used to point to on a location.\n\n        TODO: add param get_copy which doesn\'t destroy remote if true.\n        """"""\n\n        if self.point_to_attr is not None:\n\n            raise exceptions.CannotRequestObjectAttribute(\n                ""You called .get() on a pointer to""\n                "" a tensor attribute. This is not yet""\n                "" supported. Call .clone().get() instead.""\n            )\n\n        # if the pointer happens to be pointing to a local object,\n        # just return that object (this is an edge case)\n        if self.location == self.owner:\n            obj = self.owner.get_obj(self.id_at_location)\n            if hasattr(obj, ""child""):\n                obj = obj.child\n        else:\n            # get tensor from location\n            obj = self.owner.request_obj(self.id_at_location, self.location, user, reason)\n\n        # Remove this pointer by default\n        if deregister_ptr:\n            self.owner.de_register_obj(self)\n\n        if self.garbage_collect_data:\n            # data already retrieved, do not collect any more.\n            self.garbage_collect_data = False\n\n        return obj\n\n    def __str__(self):\n        """"""Returns a string version of this pointer.\n\n        This is primarily for end users to quickly see things about the object.\n        This tostring shouldn\'t be used for anything else though as it\'s likely\n        to change. (aka, don\'t try to parse it to extract information. Read the\n        attribute you need directly). Also, don\'t use this to-string as a\n        serialized form of the pointer.\n        """"""\n\n        type_name = type(self).__name__\n        out = (\n            f""[""\n            f""{type_name} | ""\n            f""{str(self.owner.id)}:{self.id}""\n            "" -> ""\n            f""{str(self.location.id)}:{self.id_at_location}""\n            f""]""\n        )\n\n        if self.point_to_attr is not None:\n            out += ""::"" + str(self.point_to_attr).replace(""."", ""::"")\n\n        big_str = False\n\n        if self.tags is not None and len(self.tags):\n            big_str = True\n            out += ""\\n\\tTags: ""\n            for tag in self.tags:\n                out += str(tag) + "" ""\n\n        if big_str and hasattr(self, ""shape""):\n            out += ""\\n\\tShape: "" + str(self.shape)\n\n        if self.description is not None:\n            big_str = True\n            out += ""\\n\\tDescription: "" + str(self.description).split(""\\n"")[0] + ""...""\n\n        return out\n\n    def __repr__(self):\n        """"""Returns the to-string method.\n\n        When called using __repr__, most commonly seen when returned as cells\n        in Jupyter notebooks.\n        """"""\n        return self.__str__()\n\n    def __del__(self):\n        """"""This method garbage collects the object this pointer is pointing to.\n        By default, PySyft assumes that every object only has one pointer to it.\n        Thus, if the pointer gets garbage collected, we want to automatically\n        garbage collect the object being pointed to.\n        """"""\n\n        # if .get() gets called on the pointer before this method is called, then\n        # the remote object has already been removed. This results in an error on\n        # this next line because self no longer has .owner. Thus, we need to check\n        # first here and not try to call self.owner.anything if self doesn\'t have\n        # .owner anymore.\n        if hasattr(self, ""owner"") and self.garbage_collect_data:\n            # attribute pointers are not in charge of GC\n            if self.point_to_attr is None:\n                self.owner.send_msg(ForceObjectDeleteMessage(self.id_at_location), self.location)\n\n    def _create_attr_name_string(self, attr_name):\n        if self.point_to_attr is not None:\n            point_to_attr = f""{self.point_to_attr}.{attr_name}""\n        else:\n            point_to_attr = attr_name\n        return point_to_attr\n\n    def attr(self, attr_name):\n        attr_ptr = syft.ObjectPointer(\n            id=self.id,\n            owner=self.owner,\n            location=self.location,\n            id_at_location=self.id_at_location,\n            point_to_attr=self._create_attr_name_string(attr_name),\n        )  # .wrap()\n        self.__setattr__(attr_name, attr_ptr)\n        return attr_ptr\n\n    def setattr(self, name, value):\n        self.owner.send_command(\n            cmd_name=""__setattr__"",\n            target=self,\n            args_=(name, value),\n            kwargs_={},\n            recipient=self.location,\n        )\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, ptr: ""ObjectPointer"") -> tuple:\n        """"""\n        This function takes the attributes of a ObjectPointer and saves them in a dictionary\n        Args:\n            ptr (ObjectPointer): a ObjectPointer\n        Returns:\n            tuple: a tuple holding the unique attributes of the pointer\n        Examples:\n            data = simplify(ptr)\n        """"""\n\n        return (\n            syft.serde.msgpack.serde._simplify(worker, ptr.id),\n            syft.serde.msgpack.serde._simplify(worker, ptr.id_at_location),\n            syft.serde.msgpack.serde._simplify(worker, ptr.location.id),\n            syft.serde.msgpack.serde._simplify(worker, ptr.point_to_attr),\n            ptr.garbage_collect_data,\n        )\n\n    @staticmethod\n    def detail(worker: ""AbstractWorker"", object_tuple: tuple) -> ""ObjectPointer"":\n        """"""\n        This function reconstructs an ObjectPointer given it\'s attributes in form of a dictionary.\n        We use the spread operator to pass the dict data as arguments\n        to the init method of ObjectPointer\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the ObjectPointer\n        Returns:\n            ObjectPointer: an ObjectPointer\n        Examples:\n            ptr = detail(data)\n        """"""\n        # TODO: fix comment for this and simplifier\n        obj_id, id_at_location, worker_id, point_to_attr, garbage_collect_data = object_tuple\n\n        obj_id = syft.serde.msgpack.serde._detail(worker, obj_id)\n        id_at_location = syft.serde.msgpack.serde._detail(worker, id_at_location)\n        worker_id = syft.serde.msgpack.serde._detail(worker, worker_id)\n        point_to_attr = syft.serde.msgpack.serde._detail(worker, point_to_attr)\n\n        # If the pointer received is pointing at the current worker, we load the tensor instead\n        if worker_id == worker.id:\n            obj = worker.get_obj(id_at_location)\n\n            if point_to_attr is not None and obj is not None:\n\n                point_to_attrs = point_to_attr.split(""."")\n                for attr in point_to_attrs:\n                    if len(attr) > 0:\n                        obj = getattr(obj, attr)\n\n                if obj is not None:\n\n                    if not obj.is_wrapper and not isinstance(obj, FrameworkTensor):\n                        # if the object is a wrapper then it doesn\'t need to be wrapped\n                        # i the object isn\'t a wrapper, BUT it\'s just a plain torch tensor,\n                        # then it doesn\'t need to be wrapped.\n                        # if the object is not a wrapper BUT it\'s also not a framework object,\n                        # then it needs to be wrapped or else it won\'t be able to be used\n                        # by other interfaces\n                        obj = obj.wrap()\n\n            return obj\n        # Else we keep the same Pointer\n        else:\n\n            location = syft.hook.local_worker.get_worker(worker_id)\n\n            ptr = ObjectPointer(\n                location=location,\n                id_at_location=id_at_location,\n                owner=worker,\n                id=obj_id,\n                garbage_collect_data=garbage_collect_data,\n            )\n\n            return ptr\n\n\n### Register the object with hook_args.py ###\nregister_type_rule({ObjectPointer: one})\nregister_forward_func({ObjectPointer: lambda p: (_ for _ in ()).throw(RemoteObjectFoundError(p))})\nregister_backward_func({ObjectPointer: lambda i: i})\n'"
syft/generic/pointers/object_wrapper.py,0,"b'from typing import List\nfrom typing import Union\nfrom typing import TYPE_CHECKING\n\nimport syft as sy\nfrom syft.generic.pointers.callable_pointer import create_callable_pointer\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.serde.syft_serializable import SyftSerializable\n\n# this if statement avoids circular imports between base.py and pointer.py\nif TYPE_CHECKING:\n    from syft.workers.base import BaseWorker\n\n\nclass ObjectWrapper(SyftSerializable):\n    """"""A class that wraps an arbitrary object and provides it with an id, tags, and description\n\n    """"""\n\n    def __init__(self, obj, id: int, tags: List[str] = None, description: str = None):\n        """""" object wrapper initialization\n        Args:\n            obj: An arbitrary object, can also be a function\n            id: id to be associated with the object\n            tags: list of strings of tags of the object\n            description: a description of the object\n        """"""\n        self._obj = obj\n        self.id = id\n        self.tags = tags\n        self.description = description\n\n    def __call__(self, *args, **kwargs):\n        return self._obj(*args, **kwargs)\n\n    def __str__(self):\n        out = ""<""\n        out += str(type(self)).split(""\'"")[1].split(""."")[-1]\n        out += "" id:"" + str(self.id)\n        out += "" obj:"" + str(self._obj)\n        out += "">""\n        return out\n\n    def __repr__(self):\n        return str(self)\n\n    @property\n    def obj(self):\n        return self._obj\n\n    @staticmethod\n    def create_pointer(\n        object,\n        owner: ""BaseWorker"",\n        location: ""BaseWorker"",\n        ptr_id: Union[int, str],\n        id_at_location: Union[int, str] = None,\n        garbage_collect_data=None,\n        **kwargs,\n    ):\n        """""" Creates a callable pointer to the object wrapper instance\n\n        Args:\n            owner: A BaseWorker parameter to specify the worker on which the\n                pointer is located. It is also where the pointer is registered\n                if register is set to True.\n            location: The BaseWorker object which points to the worker on which\n                this pointer\'s object can be found. In nearly all cases, this\n                is self.owner and so this attribute can usually be left blank.\n                Very rarely you may know that you are about to move the Tensor\n                to another worker so you can pre-initialize the location\n                attribute of the pointer to some other worker, but this is a\n                rare exception.\n            ptr_id: A string or integer parameter to specify the id of the pointer.\n            id_at_location: A string or integer id of the object being pointed\n                to. Similar to location, this parameter is almost always\n                self.id and so you can leave this parameter to None. The only\n                exception is if you happen to know that the ID is going to be\n                something different than self.id, but again this is very rare\n                and most of the time, setting this means that you are probably\n                doing something you shouldn\'t.\n            garbage_collect_data: If True, delete the remote object when the\n                pointer is deleted.\n\n        Returns:\n            A pointers.CallablePointer pointer to self.\n        """"""\n        pointer = create_callable_pointer(\n            owner=owner,\n            location=location,\n            id=ptr_id,\n            id_at_location=id_at_location if id_at_location is not None else object.id,\n            tags=object.tags,\n            description=object.description,\n            garbage_collect_data=False if garbage_collect_data is None else garbage_collect_data,\n        )\n        return pointer\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, obj: ""ObjectWrapper"") -> tuple:\n        return (obj.id, sy.serde.msgpack.serde._simplify(worker, obj.obj))\n\n    @staticmethod\n    def detail(worker: AbstractWorker, obj_wrapper_tuple: str) -> ""ObjectWrapper"":\n        obj_wrapper = ObjectWrapper(\n            id=obj_wrapper_tuple[0],\n            obj=sy.serde.msgpack.serde._detail(worker, obj_wrapper_tuple[1]),\n        )\n        return obj_wrapper\n'"
syft/generic/pointers/pointer_dataset.py,0,"b'from typing import List\nfrom typing import Union\n\nimport syft as sy\nfrom syft.generic.pointers.object_pointer import ObjectPointer\nfrom syft.workers.abstract import AbstractWorker\nfrom syft_proto.generic.pointers.v1.pointer_dataset_pb2 import PointerDataset as PointerDatasetPB\n\n\nclass PointerDataset(ObjectPointer):\n    def __init__(\n        self,\n        location: ""AbstractWorker"" = None,\n        id_at_location: Union[str, int] = None,\n        owner: ""AbstractWorker"" = None,\n        garbage_collect_data: bool = True,\n        id: Union[str, int] = None,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n        if owner is None:\n            owner = sy.framework.hook.local_worker\n        self.federated = False  # flag whether it in a federated_dataset object\n        super().__init__(\n            location=location,\n            id_at_location=id_at_location,\n            owner=owner,\n            garbage_collect_data=garbage_collect_data,\n            id=id,\n            tags=tags,\n            description=description,\n        )\n\n    @property\n    def data(self):\n        ptr = self.owner.send_command(\n            cmd_name=""get_data"", target=self.id_at_location, recipient=self.location\n        ).wrap()\n        return ptr\n\n    @property\n    def targets(self):\n        ptr = self.owner.send_command(\n            cmd_name=""get_targets"", target=self.id_at_location, recipient=self.location\n        ).wrap()\n        return ptr\n\n    def wrap(self):\n        return self\n\n    def get(self, user=None, reason: str = """", deregister_ptr: bool = True):\n        if self.federated:\n            raise ValueError(""use .get_dataset(worker) to get this dataset"")\n        dataset = super().get(user, reason, deregister_ptr)\n        return dataset\n\n    def __repr__(self):\n        type_name = type(self).__name__\n        out = (\n            f""[""\n            f""{type_name} | ""\n            f""{str(self.owner.id)}:{self.id}""\n            "" -> ""\n            f""{str(self.location.id)}:{self.id_at_location}""\n            f""]""\n        )\n\n        if self.point_to_attr is not None:\n            out += ""::"" + str(self.point_to_attr).replace(""."", ""::"")\n\n        big_str = False\n\n        if self.tags is not None and len(self.tags):\n            big_str = True\n            out += ""\\n\\tTags: ""\n            for tag in self.tags:\n                out += str(tag) + "" ""\n\n        if big_str and hasattr(self, ""shape""):\n            out += ""\\n\\tShape: "" + str(self.shape)\n\n        if self.description is not None:\n            big_str = True\n            out += ""\\n\\tDescription: "" + str(self.description).split(""\\n"")[0] + ""...""\n\n        return out\n\n    def __len__(self):\n        len = self.owner.send_command(\n            cmd_name=""__len__"", target=self.id_at_location, recipient=self.location\n        )\n        return len\n\n    def __getitem__(self, index):\n        args = [index]\n        data_elem, target_elem = self.owner.send_command(\n            cmd_name=""__getitem__"",\n            target=self.id_at_location,\n            args_=tuple(args),\n            recipient=self.location,\n        )\n        return data_elem.wrap(), target_elem.wrap()\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, ptr: ""PointerDataset"") -> tuple:\n\n        return (\n            sy.serde.msgpack.serde._simplify(worker, ptr.id),\n            sy.serde.msgpack.serde._simplify(worker, ptr.id_at_location),\n            sy.serde.msgpack.serde._simplify(worker, ptr.location.id),\n            sy.serde.msgpack.serde._simplify(worker, ptr.tags),\n            sy.serde.msgpack.serde._simplify(worker, ptr.description),\n            ptr.garbage_collect_data,\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, ptr_tuble: tuple) -> ""PointerDataset"":\n        obj_id, id_at_location, worker_id, tags, description, garbage_collect_data = ptr_tuble\n\n        obj_id = sy.serde.msgpack.serde._detail(worker, obj_id)\n        id_at_location = sy.serde.msgpack.serde._detail(worker, id_at_location)\n        worker_id = sy.serde.msgpack.serde._detail(worker, worker_id)\n        tags = sy.serde.msgpack.serde._detail(worker, tags)\n        description = sy.serde.msgpack.serde._detail(worker, description)\n\n        # If the pointer received is pointing at the current worker, we load the dataset instead\n        if worker_id == worker.id:\n            dataset = worker.get_obj(id_at_location)\n\n            return dataset\n        # Else we keep the same Pointer\n        else:\n            location = sy.hook.local_worker.get_worker(worker_id)\n\n            ptr = PointerDataset(\n                location=location,\n                id_at_location=id_at_location,\n                owner=worker,\n                tags=tags,\n                description=description,\n                garbage_collect_data=garbage_collect_data,\n                id=obj_id,\n            )\n\n            return ptr\n\n    @staticmethod\n    def bufferize(worker, pointer_obj):\n        """"""\n        This method serializes a PointerDataset into a PointerDatasetPB.\n\n        Args:\n            pointer_obj (PointerDataset): input PointerDataset to be serialized.\n\n        Returns:\n            protobuf_script (PointerDatasetPB): serialized PointerDataset.\n        """"""\n        proto_pointer = PointerDatasetPB()\n        sy.serde.protobuf.proto.set_protobuf_id(proto_pointer.object_id, pointer_obj.id)\n        sy.serde.protobuf.proto.set_protobuf_id(proto_pointer.location_id, pointer_obj.location.id)\n        sy.serde.protobuf.proto.set_protobuf_id(\n            proto_pointer.object_id_at_location, pointer_obj.id_at_location\n        )\n        for tag in pointer_obj.tags:\n            proto_pointer.tags.append(tag)\n\n        if pointer_obj.description:\n            proto_pointer.description = pointer_obj.description\n        proto_pointer.garbage_collect_data = pointer_obj.garbage_collect_data\n        return proto_pointer\n\n    @staticmethod\n    def unbufferize(worker, proto_pointer_obj: PointerDatasetPB):\n        """"""\n        This method deserializes PointerDatasetPB into a PointerDataset.\n\n        Args:\n            protobuf_script (PointerDatasetPB): input serialized PointerDatasetPB.\n\n        Returns:\n            loaded_module (PointerDataset): deserialized PointerDatasetPB.\n        """"""\n        obj_id = sy.serde.protobuf.proto.get_protobuf_id(proto_pointer_obj.object_id)\n        id_at_location = sy.serde.protobuf.proto.get_protobuf_id(\n            proto_pointer_obj.object_id_at_location\n        )\n        worker_id = sy.serde.protobuf.proto.get_protobuf_id(proto_pointer_obj.location_id)\n        tags = proto_pointer_obj.tags\n\n        description = proto_pointer_obj.description if proto_pointer_obj.description else None\n\n        garbage_collect_data = proto_pointer_obj.garbage_collect_data\n\n        if worker_id == worker.id:\n            dataset = worker.get_obj(id_at_location)\n            return dataset\n\n        location = sy.hook.local_worker.get_worker(worker_id)\n\n        return PointerDataset(\n            location=location,\n            id_at_location=id_at_location,\n            owner=worker,\n            tags=set(tags),\n            description=description,\n            garbage_collect_data=garbage_collect_data,\n            id=obj_id,\n        )\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n        This method returns the protobuf schema used for PointerDataset.\n\n        Returns:\n           Protobuf schema for PointerDataset.\n       """"""\n        return PointerDatasetPB\n'"
syft/generic/pointers/pointer_plan.py,0,"b'from typing import List\nfrom typing import Union\n\nimport syft as sy\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.pointers.object_pointer import ObjectPointer\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.messaging.message import ForceObjectDeleteMessage\nfrom syft.workers.abstract import AbstractWorker\n\n\nclass PointerPlan(ObjectPointer):\n    """"""\n    The PointerPlan keeps a reference to a remote Plan.\n\n    It allows to:\n    - __call__ an evaluation of the remote plan\n    - get the remote plan\n\n    It\'s a simplification compared to the current hybrid state of\n    Plans which can be seen as pointers, which is ambiguous.\n    """"""\n\n    def __init__(\n        self,\n        location: ""AbstractWorker"" = None,\n        id_at_location: Union[str, int] = None,\n        owner: ""AbstractWorker"" = None,\n        garbage_collect_data: bool = True,\n        id: Union[str, int] = None,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n        if owner is None:\n            owner = sy.framework.hook.local_worker\n\n        self._locations = []\n        self._ids_at_location = []\n\n        super().__init__(\n            location=location,\n            id_at_location=id_at_location,\n            owner=owner,\n            garbage_collect_data=garbage_collect_data,\n            id=id,\n            tags=tags,\n            description=description,\n        )\n\n    # Make PointerPlan compatible with multi pointers\n    @property\n    def location(self):\n        n_locations = len(self._locations)\n        if n_locations != 1:\n            return self._locations\n        else:\n            return self._locations[0]\n\n    @location.setter\n    def location(self, new_location: Union[AbstractWorker, List[AbstractWorker]]):\n        if isinstance(new_location, (list, tuple)):\n            self._locations = new_location\n        else:\n            self._locations = [new_location]\n\n    @property\n    def id_at_location(self):\n        n_ids = len(self._ids_at_location)\n        if n_ids != 1:\n            return self._ids_at_location\n        else:\n            return self._ids_at_location[0]\n\n    @id_at_location.setter\n    def id_at_location(self, new_id_at_location):\n        if isinstance(new_id_at_location, (list, tuple)):\n            self._ids_at_location = new_id_at_location\n        else:\n            self._ids_at_location = [new_id_at_location]\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        Transform the call on the pointer in a request to evaluate the\n        remote plan\n        """"""\n        if len(self._locations) > 1 and isinstance(args[0], sy.MultiPointerTensor):\n            responses = {}\n            for location in self._locations:\n                child_args = [\n                    x.child[location.id] if isinstance(x, sy.MultiPointerTensor) else x\n                    for x in args\n                ]\n                responses[location.id] = self.__call__(*child_args, **kwargs)\n\n            return responses\n\n        if len(self._locations) == 1:\n            location = self.location\n        else:\n            location = args[0].location\n\n        result_ids = [sy.ID_PROVIDER.pop()]\n        response = self.request_run_plan(location, result_ids, *args)\n\n        return response\n\n    def parameters(self) -> List:\n        """"""Return a list of pointers to the plan parameters""""""\n\n        assert (\n            len(self._locations) == 1\n        ), "".parameters() for PointerPlan with > 1 locations is currently not implemented.""\n        # TODO implement this feature using MultiPointerTensor\n\n        location = self._locations[0]\n        id_at_location = self._ids_at_location[0]\n\n        pointers = self.owner.send_command(\n            cmd_name=""parameters"", target=id_at_location, recipient=location\n        )\n\n        for pointer in pointers:\n            pointer.garbage_collect_data = False\n\n        return [pointer.wrap() for pointer in pointers]\n\n    def request_run_plan(\n        self,\n        location: ""sy.workers.BaseWorker"",\n        response_ids: List[Union[str, int]],\n        *args,\n        **kwargs,\n    ) -> object:\n        """"""Requests plan execution.\n\n        Send a request to execute the plan on the remote location.\n\n        Args:\n            location: to which worker the request should be sent\n            response_ids: where the result should be stored\n            args: arguments used as input data for the plan\n            kwargs: named arguments used as input data for the plan\n\n        Returns:\n            Execution response\n        """"""\n        plan_name = f""plan{self.id}""\n\n        args = [args, response_ids]\n\n        if location not in self._locations:\n            raise RuntimeError(\n                f""Requested to run a plan on {location.id} but pointer location(s) is/are"",\n                self._locations,\n            )\n\n        # look for the relevant id in the list of ids\n        id_at_location = None\n        for loc, id_at_loc in zip(self._locations, self._ids_at_location):\n            if loc == location:\n                id_at_location = id_at_loc\n                break\n\n        response = self.owner.send_command(\n            cmd_name=""run"",\n            target=id_at_location,\n            args_=tuple(args),\n            recipient=location,\n            return_ids=tuple(response_ids),\n        )\n        response = hook_args.hook_response(plan_name, response, wrap_type=FrameworkTensor[0])\n        if isinstance(response, (list, tuple)):\n            for r in response:\n                r.garbage_collect_data = False\n        else:\n            response.garbage_collect_data = False\n        return response\n\n    def get(self, deregister_ptr: bool = True):\n        """"""\n        This is an alias to fetch_plan, to behave like a pointer\n        """"""\n        copy = not deregister_ptr\n        plan = self.owner.fetch_plan(self.id_at_location, self.location, copy=copy)\n        return plan\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, ptr: ""PointerPlan"") -> tuple:\n\n        return (\n            sy.serde.msgpack.serde._simplify(worker, ptr.id),\n            sy.serde.msgpack.serde._simplify(worker, ptr.id_at_location),\n            sy.serde.msgpack.serde._simplify(worker, ptr.location.id),\n            sy.serde.msgpack.serde._simplify(worker, ptr.tags),\n            ptr.garbage_collect_data,\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""PointerPlan"":\n        # TODO: fix comment for this and simplifier\n        obj_id, id_at_location, worker_id, tags, garbage_collect_data = tensor_tuple\n\n        obj_id = sy.serde.msgpack.serde._detail(worker, obj_id)\n        id_at_location = sy.serde.msgpack.serde._detail(worker, id_at_location)\n        worker_id = sy.serde.msgpack.serde._detail(worker, worker_id)\n        tags = sy.serde.msgpack.serde._detail(worker, tags)\n\n        # If the pointer received is pointing at the current worker, we load the tensor instead\n        if worker_id == worker.id:\n            plan = worker.get_obj(id_at_location)\n\n            return plan\n        # Else we keep the same Pointer\n        else:\n            location = sy.hook.local_worker.get_worker(worker_id)\n\n            ptr = PointerPlan(\n                location=location,\n                id_at_location=id_at_location,\n                owner=worker,\n                tags=tags,\n                garbage_collect_data=garbage_collect_data,\n                id=obj_id,\n            )\n\n            return ptr\n\n    def wrap(self):\n        return self\n\n    def __str__(self):\n        """"""Returns a string version of this pointer.\n\n        Example:\n            For single pointers:\n            > [PointerPlan | me:33873097403 -> dan:72165846784]\n\n            Or for multi pointers:\n            > [PointerPlan | me:55894304374\n                 -> alice:72165846784\n                 -> bob:72165846784\n            ]\n        """"""\n        type_name = type(self).__name__\n        out = f""["" f""{type_name} | "" f""{str(self.owner.id)}:{self.id}""\n        if len(self._locations) == 1:\n            out += f"" -> {str(self.location.id)}:{self.id_at_location}""\n        else:\n            for location, id_at_location in zip(self.location, self.id_at_location):\n                out += f""\\n\\t -> {str(location.id)}:{id_at_location}""\n            out += ""\\n""\n        out += ""]""\n\n        if self.tags is not None and len(self.tags):\n            out += ""\\n\\tTags: ""\n            for tag in self.tags:\n                out += str(tag) + "" ""\n\n        if self.description is not None:\n            out += ""\\n\\tDescription: "" + str(self.description).split(""\\n"")[0] + ""...""\n\n        return out\n\n    def __del__(self):\n        """"""This method garbage collects the object this pointer is pointing to.\n        By default, PySyft assumes that every object only has one pointer to it.\n        Thus, if the pointer gets garbage collected, we want to automatically\n        garbage collect the object being pointed to.\n        """"""\n        if self.garbage_collect_data:\n            for id_at_location, location in zip(self._ids_at_location, self._locations):\n                self.owner.send_msg(ForceObjectDeleteMessage(id_at_location), location)\n'"
syft/generic/pointers/pointer_tensor.py,0,"b'from typing import List, Union\n\nimport syft\nfrom syft.generic.frameworks.hook.hook_args import one\nfrom syft.generic.frameworks.hook.hook_args import register_type_rule\nfrom syft.generic.frameworks.hook.hook_args import register_forward_func\nfrom syft.generic.frameworks.hook.hook_args import register_backward_func\nfrom syft.generic.frameworks.types import FrameworkShapeType\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.pointers.object_pointer import ObjectPointer\nfrom syft.messaging.message import TensorCommandMessage\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft_proto.generic.pointers.v1.pointer_tensor_pb2 import PointerTensor as PointerTensorPB\n\nfrom syft.exceptions import RemoteObjectFoundError\n\n\nclass PointerTensor(ObjectPointer, AbstractTensor):\n    """"""A pointer to another tensor.\n\n    A PointerTensor forwards all API calls to the remote tensor. PointerTensor\n    objects point to tensors (as their name implies). They exist to mimic the\n    entire API of a normal tensor, but instead of computing a tensor function\n    locally (such as addition, subtraction, etc.) they forward the computation\n    to a remote machine as specified by self.location. Specifically, every\n    PointerTensor has a tensor located somewhere that it points to (they should\n    never exist by themselves). Note that PointerTensor objects can point to\n    both FrameworkTensor objects AND to other PointerTensor objects. Furthermore,\n    the objects being pointed to can be on the same machine or (more commonly)\n    on a different one. Note further that a PointerTensor does not know the\n    nature how it sends messages to the tensor it points to (whether over\n    socket, http, or some other protocol) as that functionality is abstracted\n    in the AbstractWorker object in self.location.\n\n    Example:\n\n     >>> import syft as sy\n     >>> hook = sy.TorchHook()\n     >>> bob = sy.VirtualWorker(id=""bob"")\n     >>> x = sy.Tensor([1,2,3,4,5])\n     >>> y = sy.Tensor([1,1,1,1,1])\n     >>> x_ptr = x.send(bob) # returns a PointerTensor, sends tensor to Bob\n     >>> y_ptr = y.send(bob) # returns a PointerTensor, sends tensor to Bob\n     >>> # executes command on Bob\'s machine\n     >>> z_ptr = x_ptr + y_ptr\n    """"""\n\n    def __init__(\n        self,\n        location: ""AbstractWorker"" = None,\n        id_at_location: Union[str, int] = None,\n        owner: ""AbstractWorker"" = None,\n        id: Union[str, int] = None,\n        garbage_collect_data: bool = True,\n        shape: FrameworkShapeType = None,\n        point_to_attr: str = None,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n        """"""Initializes a PointerTensor.\n\n        Args:\n            location: An optional AbstractWorker object which points to the worker\n                on which this pointer\'s object can be found.\n            id_at_location: An optional string or integer id of the object\n                being pointed to.\n            owner: An optional AbstractWorker object to specify the worker on which\n                the pointer is located. It is also where the pointer is\n                registered if register is set to True. Note that this is\n                different from the location parameter that specifies where the\n                pointer points to.\n            id: An optional string or integer id of the PointerTensor.\n            garbage_collect_data: If true (default), delete the remote object when the\n                pointer is deleted.\n            shape: size of the tensor the pointer points to\n            point_to_attr: string which can tell a pointer to not point directly to\\\n                an object, but to point to an attribute of that object such as .child or\n                .grad. Note the string can be a chain (i.e., .child.child.child or\n                .grad.child.child). Defaults to None, which means don\'t point to any attr,\n                just point to then object corresponding to the id_at_location.\n            tags: an optional set of strings corresponding to this tensor\n                which this tensor should be searchable for.\n            description: an optional string describing the purpose of the tensor.\n        """"""\n\n        super().__init__(\n            location=location,\n            id_at_location=id_at_location,\n            owner=owner,\n            id=id,\n            garbage_collect_data=garbage_collect_data,\n            point_to_attr=point_to_attr,\n            tags=tags,\n            description=description,\n        )\n        self._shape = shape\n\n    def get_shape(self):\n        """"""Request information about the shape to the remote worker""""""\n        return self.owner.request_remote_tensor_shape(self)\n\n    @property\n    def shape(self):\n        """""" This method returns the shape of the data being pointed to.\n        This shape information SHOULD be cached on self._shape, but\n        occasionally this information may not be present. If this is the\n        case, then it requests the shape information from the remote object\n        directly (which is inefficient and should be avoided).\n        """"""\n\n        if self._shape is None:\n            self._shape = self.get_shape()\n\n        return self._shape\n\n    @shape.setter\n    def shape(self, new_shape):\n        self._shape = new_shape\n\n    @property\n    def grad(self):\n        if not hasattr(self, ""_grad""):\n            self._grad = self.attr(""grad"")\n\n        if self._grad.child.is_none():\n            return None\n\n        return self._grad\n\n    @grad.setter\n    def grad(self, new_grad):\n        self._grad = new_grad\n\n    @property\n    def data(self):\n        if not hasattr(self, ""_data""):\n            self._data = self.attr(""data"")\n        return self._data\n\n    @data.setter\n    def data(self, new_data):\n        self._data = new_data\n\n    def is_none(self):\n        try:\n            return self.owner.request_is_remote_tensor_none(self)\n        except:\n            """"""TODO: this might hide useful errors, but we don\'t have good\n            enough remote error handling yet to do anything better.""""""\n            return True\n\n    def clone(self):\n        """"""\n        Clone should keep ids unchanged, contrary to copy.\n        We make the choice that a clone action is local, and can\'t affect\n        the remote tensors, so garbage_collect_data is always False, both\n        for the tensor cloned and the clone.\n        """"""\n        self.garbage_collect_data = False\n        cloned_tensor = type(self)(**self.get_class_attributes())\n        cloned_tensor.id = self.id\n        cloned_tensor.owner = self.owner\n\n        return cloned_tensor\n\n    def get_class_attributes(self):\n        """"""\n        Used for cloning (see AbtractTensor)\n        """"""\n        return {\n            ""location"": self.location,\n            ""id_at_location"": self.id_at_location,\n            ""garbage_collect_data"": self.garbage_collect_data,\n        }\n\n    @staticmethod\n    def create_pointer(\n        tensor,\n        location: Union[AbstractWorker, str] = None,\n        id_at_location: (str or int) = None,\n        owner: Union[AbstractWorker, str] = None,\n        ptr_id: (str or int) = None,\n        garbage_collect_data=None,\n        shape=None,\n    ) -> ""PointerTensor"":\n        """"""Creates a pointer to the ""self"" FrameworkTensor object.\n\n        This method is called on a FrameworkTensor object, returning a pointer\n        to that object. This method is the CORRECT way to create a pointer,\n        and the parameters of this method give all possible attributes that\n        a pointer can be created with.\n\n        Args:\n            location: The AbstractWorker object which points to the worker on which\n                this pointer\'s object can be found. In nearly all cases, this\n                is self.owner and so this attribute can usually be left blank.\n                Very rarely you may know that you are about to move the Tensor\n                to another worker so you can pre-initialize the location\n                attribute of the pointer to some other worker, but this is a\n                rare exception.\n            id_at_location: A string or integer id of the tensor being pointed\n                to. Similar to location, this parameter is almost always\n                self.id and so you can leave this parameter to None. The only\n                exception is if you happen to know that the ID is going to be\n                something different than self.id, but again this is very rare\n                and most of the time, setting this means that you are probably\n                doing something you shouldn\'t.\n            owner: A AbstractWorker parameter to specify the worker on which the\n                pointer is located. It is also where the pointer is registered\n                if register is set to True.\n            ptr_id: A string or integer parameter to specify the id of the pointer\n                in case you wish to set it manually for any special reason.\n                Otherwise, it will be set randomly.\n            garbage_collect_data: If true (default), delete the remote tensor when the\n                pointer is deleted.\n\n        Returns:\n            A FrameworkTensor[PointerTensor] pointer to self. Note that this\n            object itself will likely be wrapped by a FrameworkTensor wrapper.\n        """"""\n        if owner is None:\n            owner = tensor.owner\n\n        if location is None:\n            location = tensor.owner\n\n        owner = tensor.owner.get_worker(owner)\n        location = tensor.owner.get_worker(location)\n\n        # previous_pointer = owner.get_pointer_to(location, id_at_location)\n        previous_pointer = None\n\n        if previous_pointer is None:\n            ptr = PointerTensor(\n                location=location,\n                id_at_location=id_at_location,\n                owner=owner,\n                id=ptr_id,\n                garbage_collect_data=True if garbage_collect_data is None else garbage_collect_data,\n                shape=shape,\n                tags=tensor.tags,\n                description=tensor.description,\n            )\n\n        return ptr\n\n    def move(self, destination: AbstractWorker, requires_grad: bool = False):\n        """"""\n        Will move the remove value from self.location A to destination B\n        Note a A will keep a copy of his value that he sent to B. This follows the\n        .send() paradigm where the local worker keeps a copy of the value he sends.\n        Args:\n            destination: the new location of the remote data\n            requires_grad: see send() for details\n        Returns:\n            A pointer to location\n        """"""\n        # move to local target is equivalent to doing .get()\n        if self.owner.id == destination.id:\n            return self.get()\n\n        if self.location.id == destination.id:\n            return self\n\n        ptr = self.remote_send(destination, requires_grad=requires_grad)\n\n        # We make the pointer point at the remote value. As the id doesn\'t change,\n        # we don\'t update ptr.id_at_location. See issue #3217 about this.\n        # Note that you have now 2 pointers on different locations pointing to the\n        # same tensor.\n        ptr.location = destination\n\n        return ptr\n\n    def remote_send(self, destination: AbstractWorker, requires_grad: bool = False):\n        """""" Request the worker where the tensor being pointed to belongs to send it to destination.\n        For instance, if C holds a pointer, ptr, to a tensor on A and calls ptr.remote_send(B),\n        C will hold a pointer to a pointer on A which points to the tensor on B.\n        Args:\n            destination: where the remote value should be sent\n            requires_grad: if true updating the grad of the remote tensor on destination B will\n                trigger a message to update the gradient of the value on A.\n        """"""\n        kwargs_ = {""inplace"": False, ""requires_grad"": requires_grad}\n        message = TensorCommandMessage.communication(\n            ""remote_send"", self, (destination.id,), kwargs_, (self.id,)\n        )\n        self.owner.send_msg(message=message, location=self.location)\n        return self\n\n    def remote_get(self):\n        self.owner.send_command(cmd_name=""mid_get"", target=self, recipient=self.location)\n        return self\n\n    def get(self, user=None, reason: str = """", deregister_ptr: bool = True):\n        """"""Requests the tensor/chain being pointed to, be serialized and return\n\n        Since PointerTensor objects always point to a remote tensor (or chain\n        of tensors, where a chain is simply a linked-list of tensors linked via\n        their .child attributes), this method will request that the tensor/chain\n        being pointed to be serialized and returned from this function.\n\n        Note:\n            This will typically mean that the remote object will be\n            removed/destroyed. To just bring a copy back to the local worker,\n            call .copy() before calling .get().\n\n\n        Args:\n            user (obj, optional): user credentials to perform authentication process.\n            reason (str, optional): a description of why the data scientist wants to see it.\n            deregister_ptr (bool, optional): this determines whether to\n                deregister this pointer from the pointer\'s owner during this\n                method. This defaults to True because the main reason people use\n                this method is to move the tensor from the remote machine to the\n                local one, at which time the pointer has no use.\n\n        Returns:\n            An AbstractTensor object which is the tensor (or chain) that this\n            object used to point to #on a remote machine.\n        """"""\n        tensor = ObjectPointer.get(self, user=user, reason=reason, deregister_ptr=deregister_ptr)\n\n        # TODO: remove these 3 lines\n        # The fact we have to check this means\n        # something else is probably broken\n        if tensor.is_wrapper:\n            if isinstance(tensor.child, FrameworkTensor):\n                return tensor.child\n        return tensor\n\n    def attr(self, attr_name):\n        attr_ptr = PointerTensor(\n            id=self.id,\n            owner=self.owner,\n            location=self.location,\n            id_at_location=self.id_at_location,\n            point_to_attr=self._create_attr_name_string(attr_name),\n        ).wrap(register=False)\n        self.__setattr__(attr_name, attr_ptr)\n        return attr_ptr\n\n    def dim(self) -> int:\n        return len(self.shape)\n\n    def fix_prec(self, *args, **kwargs):\n        """"""\n        Send a command to remote worker to transform a tensor to fix_precision\n\n        Returns:\n            A pointer to an FixPrecisionTensor\n        """"""\n        response = self.owner.send_command(self.location, ""fix_prec"", self, args, kwargs)\n        return response\n\n    fix_precision = fix_prec\n\n    def float_prec(self, *args, **kwargs):\n        """"""\n        Send a command to remote worker to transform a fix_precision tensor back to float_precision\n\n        Returns:\n            A pointer to a Tensor\n        """"""\n        response = self.owner.send_command(self.location, ""float_prec"", self, args, kwargs)\n        return response\n\n    float_precision = float_prec\n\n    def share(self, *args, **kwargs):\n        """"""\n        Send a command to remote worker to additively share a tensor\n\n        Returns:\n            A pointer to an AdditiveSharingTensor\n        """"""\n        if len(args) < 2:\n            raise RuntimeError(""Error, share must have > 1 arguments all of type syft.workers"")\n\n        response = self.owner.send_command(self.location, ""share"", self, args, kwargs)\n        return response\n\n    def share_(self, *args, **kwargs):\n        """"""\n        Send a command to remote worker to additively share inplace a tensor\n\n        Returns:\n            A pointer to an AdditiveSharingTensor\n        """"""\n        response = self.owner.send_command(self.location, ""share_"", self, args, kwargs)\n        return self\n\n    def set_garbage_collect_data(self, value):\n        self.garbage_collect_data = value\n\n    def item(self) -> None:\n        """"""\n        Raising error with a message to be using .get instead of .item\n        """"""\n        raise RuntimeError(\n            \'Error, Please consider calling "".get"" method instead of "".item"" method, \'\n            ""so you can be safely getting the item you need.""\n        )\n\n    def __eq__(self, other):\n        return self.eq(other)\n\n    def __iter__(self):\n        return (self[idx] for idx in range(self.shape[0]))\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, ptr: ""PointerTensor"") -> tuple:\n        """"""\n        This function takes the attributes of a PointerTensor and saves them in a dictionary\n        Args:\n            worker (AbstractWorker): the worker doing the serialization\n            ptr (PointerTensor): a PointerTensor\n        Returns:\n            tuple: a tuple holding the unique attributes of the pointer\n        Examples:\n            data = simplify(ptr)\n        """"""\n\n        return (\n            syft.serde.msgpack.serde._simplify(worker, ptr.id),\n            syft.serde.msgpack.serde._simplify(worker, ptr.id_at_location),\n            syft.serde.msgpack.serde._simplify(worker, ptr.location.id),\n            syft.serde.msgpack.serde._simplify(worker, ptr.point_to_attr),\n            syft.serde.msgpack.serde._simplify(worker, ptr._shape),\n            ptr.garbage_collect_data,\n            syft.serde.msgpack.serde._simplify(worker, ptr.tags),\n            ptr.description,\n        )\n\n        # a more general but slower/more verbose option\n\n        # data = vars(ptr).copy()\n        # for k, v in data.items():\n        #     if isinstance(v, AbstractWorker):\n        #         data[k] = v.id\n        # return _simplify_dictionary(data)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""PointerTensor"":\n        """"""\n        This function reconstructs a PointerTensor given it\'s attributes in form of a dictionary.\n        We use the spread operator to pass the dict data as arguments\n        to the init method of PointerTensor\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the PointerTensor\n        Returns:\n            PointerTensor: a PointerTensor\n        Examples:\n            ptr = detail(data)\n        """"""\n        # TODO: fix comment for this and simplifier\n        (\n            obj_id,\n            id_at_location,\n            worker_id,\n            point_to_attr,\n            shape,\n            garbage_collect_data,\n            tags,\n            description,\n        ) = tensor_tuple\n\n        obj_id = syft.serde.msgpack.serde._detail(worker, obj_id)\n        id_at_location = syft.serde.msgpack.serde._detail(worker, id_at_location)\n        worker_id = syft.serde.msgpack.serde._detail(worker, worker_id)\n        point_to_attr = syft.serde.msgpack.serde._detail(worker, point_to_attr)\n\n        if shape is not None:\n            shape = syft.hook.create_shape(syft.serde.msgpack.serde._detail(worker, shape))\n\n        # If the pointer received is pointing at the current worker, we load the tensor instead\n        if worker_id == worker.id:\n            tensor = worker.get_obj(id_at_location)\n\n            if point_to_attr is not None and tensor is not None:\n\n                point_to_attrs = point_to_attr.split(""."")\n                for attr in point_to_attrs:\n                    if len(attr) > 0:\n                        tensor = getattr(tensor, attr)\n\n                if tensor is not None:\n\n                    if not tensor.is_wrapper and not isinstance(tensor, FrameworkTensor):\n                        # if the tensor is a wrapper then it doesn\'t need to be wrapped\n                        # if the tensor isn\'t a wrapper, BUT it\'s just a plain torch tensor,\n                        # then it doesn\'t need to be wrapped.\n                        # if the tensor is not a wrapper BUT it\'s also not a torch tensor,\n                        # then it needs to be wrapped or else it won\'t be able to be used\n                        # by other interfaces\n                        tensor = tensor.wrap()\n\n            return tensor\n        # Else we keep the same Pointer\n        else:\n\n            location = syft.hook.local_worker.get_worker(worker_id)\n\n            ptr = PointerTensor(\n                location=location,\n                id_at_location=id_at_location,\n                owner=worker,\n                id=obj_id,\n                shape=shape,\n                garbage_collect_data=garbage_collect_data,\n                tags=tags,\n                description=description,\n            )\n\n            return ptr\n\n        # a more general but slower/more verbose option\n\n        # new_data = {}\n        # for k, v in data.items():\n        #     key = k.decode()\n        #     if type(v) is bytes:\n        #         val_str = v.decode()\n        #         val = syft.local_worker.get_worker(val_str)\n        #     else:\n        #         val = v\n        #     new_data[key] = val\n        # return PointerTensor(**new_data)\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, ptr: ""PointerTensor"") -> PointerTensorPB:\n        protobuf_pointer = PointerTensorPB()\n\n        syft.serde.protobuf.proto.set_protobuf_id(protobuf_pointer.object_id, ptr.id)\n        syft.serde.protobuf.proto.set_protobuf_id(protobuf_pointer.location_id, ptr.location.id)\n        syft.serde.protobuf.proto.set_protobuf_id(\n            protobuf_pointer.object_id_at_location, ptr.id_at_location\n        )\n\n        if ptr.point_to_attr:\n            protobuf_pointer.point_to_attr = ptr.point_to_attr\n        protobuf_pointer.garbage_collect_data = ptr.garbage_collect_data\n        return protobuf_pointer\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_tensor: PointerTensorPB) -> ""PointerTensor"":\n        # Extract the field values\n\n        obj_id = syft.serde.protobuf.proto.get_protobuf_id(protobuf_tensor.object_id)\n        obj_id_at_location = syft.serde.protobuf.proto.get_protobuf_id(\n            protobuf_tensor.object_id_at_location\n        )\n        worker_id = syft.serde.protobuf.proto.get_protobuf_id(protobuf_tensor.location_id)\n        point_to_attr = protobuf_tensor.point_to_attr\n        shape = syft.hook.create_shape(protobuf_tensor.shape.dims)\n        garbage_collect_data = protobuf_tensor.garbage_collect_data\n\n        # If the pointer received is pointing at the current worker, we load the tensor instead\n        if worker_id == worker.id:\n            tensor = worker.get_obj(obj_id_at_location)\n\n            if point_to_attr is not None and tensor is not None:\n\n                point_to_attrs = point_to_attr.split(""."")\n                for attr in point_to_attrs:\n                    if len(attr) > 0:\n                        tensor = getattr(tensor, attr)\n\n                if tensor is not None:\n\n                    if not tensor.is_wrapper and not isinstance(tensor, FrameworkTensor):\n                        # if the tensor is a wrapper then it doesn\'t need to be wrapped\n                        # if the tensor isn\'t a wrapper, BUT it\'s just a plain torch tensor,\n                        # then it doesn\'t need to be wrapped.\n                        # if the tensor is not a wrapper BUT it\'s also not a torch tensor,\n                        # then it needs to be wrapped or else it won\'t be able to be used\n                        # by other interfaces\n                        tensor = tensor.wrap()\n\n            return tensor\n        # Else we keep the same Pointer\n        else:\n            location = syft.hook.local_worker.get_worker(worker_id)\n\n            ptr = PointerTensor(\n                location=location,\n                id_at_location=obj_id_at_location,\n                owner=worker,\n                id=obj_id,\n                shape=shape,\n                garbage_collect_data=garbage_collect_data,\n            )\n\n            return ptr\n\n    @staticmethod\n    def get_protobuf_schema() -> PointerTensorPB:\n        return PointerTensorPB\n\n\n### Register the tensor with hook_args.py ###\nregister_type_rule({PointerTensor: one})\nregister_forward_func({PointerTensor: lambda p: (_ for _ in ()).throw(RemoteObjectFoundError(p))})\nregister_backward_func({PointerTensor: lambda i: i})\n'"
syft/generic/pointers/string_pointer.py,0,"b'from typing import List\nfrom typing import Union\n\nfrom syft.generic.pointers.object_pointer import ObjectPointer\nfrom syft.workers.base import BaseWorker\n\n\nclass StringPointer(ObjectPointer):\n    # , metaclass=PointerClassMaker, pointed_type=String):\n    """"""\n       This class defines a pointer to a \'String\' object that might live\n    on a remote machine. In other words, it holds a pointer to a\n    \'String\' object owned by a possibly different worker (although\n    it can also point to a String owned by the same worker\'.\n\n    All String method are hooked to objects of this class, and calls to\n    such methods are forwarded to the pointed-to String object.\n    """"""\n\n    def __init__(\n        self,\n        location: BaseWorker = None,\n        id_at_location: Union[str, int] = None,\n        owner: BaseWorker = None,\n        id: Union[str, int] = None,\n        garbage_collect_data: bool = True,\n        tags: List[str] = None,\n        description: str = None,\n    ):\n\n        super(StringPointer, self).__init__(\n            location=location,\n            id_at_location=id_at_location,\n            owner=owner,\n            id=id,\n            garbage_collect_data=garbage_collect_data,\n            tags=tags,\n            description=description,\n        )\n'"
syft/grid/authentication/__init__.py,0,b''
syft/grid/authentication/account.py,0,"b'import glob\nimport os.path\nimport json\nfrom syft.grid.authentication.credential import AbstractCredential\n\n\nclass AccountCredential(AbstractCredential):\n    """"""Parse/represent credentials based on username-password structure.\n\n    Expected JSON Format:\n    { ""accounts"": [ {""user"": ""example1"", ""password"": ""pass_example""},\n                      {""user"": ""user2"", ""password"": ""password2""},\n                       ....\n                    ]\n    }\n    """"""\n\n    # Constants used to parse user credential files\n    USERNAME_FIELD = ""username""\n    PASSWORD_FIELD = ""password""\n    CREDENTIAL_FIELD = ""accounts""\n\n    def __init__(self, username, password):\n        """""" Initialize a user authentication object.\n\n        Args:\n            username (str) : Key to identify this object.\n            password (str) : Secret used to validate user.\n        """"""\n        self.username = username\n        self.password = password\n        super().__init__()\n\n    @staticmethod\n    def parse(path: str, file_name: str):\n        """""" Static method used to create new account authentication instances\n        parsing a json file.\n\n        Args:\n            path (str) : Json file path.\n            file_name (str) : File\'s name.\n        Returns:\n            List : List of account objects.\n        """"""\n        user_files = glob.glob(os.path.join(path, file_name))\n        users = {}\n        for f in user_files:\n            with open(f) as json_file:\n                credentials = json.load(json_file)\n                cred_users = credentials[AccountCredential.CREDENTIAL_FIELD]\n                for user in cred_users:\n                    new_user = AccountCredential(\n                        user[AccountCredential.USERNAME_FIELD],\n                        user[AccountCredential.PASSWORD_FIELD],\n                    )\n                    users[new_user.username] = new_user\n        return users\n\n    def json(self):\n        """"""  Convert account instances into a JSON/Dictionary structure. """"""\n        return {\n            AccountCredential.USERNAME_FIELD: self.username,\n            AccountCredential.PASSWORD_FIELD: self.password,\n        }\n\n    def __str__(self):\n        return (\n            ""< AccountCredential - User: ""\n            + self.username\n            + "" Password: ""\n            + (""*"" * len(self.password))\n            + "" >""\n        )\n'"
syft/grid/authentication/credential.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass AbstractCredential(ABC):\n    """""" AbstractCredential is an abstract class that defines generic methods\n    used by all types of authentications defined in this module. """"""\n\n    @abstractmethod\n    def parse(self):\n        """""" Read, parse and load credential files.""""""\n        raise NotImplementedError(""Parse not specified!"")\n\n    @abstractmethod\n    def json(self):\n        """""" Convert credential instances into a JSON structure. """"""\n        raise NotImplementedError(""JSON not specified!"")\n'"
syft/grid/autoscale/gcloud.py,0,"b'""""""To autoscale pygrid workers on Google Cloud Platfrom""""""\nimport json\nimport IPython\nimport terrascript\nimport terrascript.provider\nimport terrascript.resource\nfrom utils.script import terraform_script\nfrom utils.notebook import terraform_notebook\n\n\nclass GoogleCloud:\n    """"""This class defines automates the spinning up of Google Cloud Instances""""""\n\n    def __init__(self, credentials, project_id, region):\n        """"""\n        args:\n            credentials: Path to the credentials json file\n            project_id: project_id of your project in GCP\n            region: region of your GCP project\n        """"""\n        self.credentials = credentials\n        self.project_id = project_id\n        self.region = region\n        self.config = terrascript.Terrascript()\n        self.config += terrascript.provider.google(\n            credentials=self.credentials, project=self.project_id, region=self.region\n        )\n        with open(""main.tf.json"", ""w"") as main_config:\n            json.dump(self.config, main_config, indent=2, sort_keys=False)\n\n        if IPython.get_ipython():\n            terraform_notebook.init()\n        else:\n            terraform_script.init()\n\n    def compute_instance(self, name, machine_type, zone, image_family):\n        """"""\n        args:\n            name: name of the compute instance\n            machine_type: the type of machine\n            zone: zone of your GCP project\n            image_family: image of the OS\n        """"""\n        self.config += terrascript.resource.google_compute_instance(\n            name,\n            name=name,\n            machine_type=machine_type,\n            zone=zone,\n            boot_disk={""initialize_params"": {""image"": image_family}},\n            network_interface={""network"": ""default"", ""access_config"": {}},\n        )\n        with open(""main.tf.json"", ""w"") as main_config:\n            json.dump(self.config, main_config, indent=2, sort_keys=False)\n\n        if IPython.get_ipython():\n            terraform_notebook.apply()\n        else:\n            terraform_script.apply()\n\n    def destroy(self):\n        """"""\n        args:\n        """"""\n        if IPython.get_ipython():\n            terraform_notebook.destroy()\n        else:\n            terraform_script.destroy()\n        del self.credentials\n'"
syft/grid/autoscale/test.py,0,"b'""""""To test the implementation of gcloud.py""""""\nfrom syft.grid.autoscale import gcloud\n\n\nNEW = gcloud.GoogleCloud(\n    credentials=""/usr/terraform.json"", project_id=""project"", region=""us-central1"",\n)\n\nNEW.compute_instance(\n    name=""new-12345"", machine_type=""f1-micro"", zone=""us-central1-a"", image_family=""debian-9"",\n)\n\nNEW.destroy()\n'"
syft/serde/msgpack/__init__.py,0,b'from syft.serde.msgpack import serde  # noqa: F401\nfrom syft.serde.msgpack import native_serde  # noqa: F401\nfrom syft.serde.msgpack import torch_serde  # noqa: F401\nfrom syft.serde.msgpack import proto  # noqa: F401\n\nfrom syft.serde.msgpack.proto import proto_type_info  # noqa: F401\nfrom syft.serde.msgpack.serde import serialize  # noqa: F401\nfrom syft.serde.msgpack.serde import deserialize  # noqa: F401\n'
syft/serde/msgpack/native_serde.py,1,"b'""""""\nThis file exists to provide one common place for all serialisation and simplify_ and _detail\nfor all native python objects.\n""""""\nfrom collections import OrderedDict\nfrom typing import Collection\nfrom typing import Dict\nfrom typing import Union\nfrom typing import Tuple\nimport warnings\nimport pydoc\n\nimport numpy\n\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.serde.msgpack import serde\n\n\n# Simplify/Detail Collections (list, set, tuple, etc.)\n\n\ndef _simplify_collection(\n    worker: AbstractWorker, my_collection: Collection, shallow: bool = False\n) -> Tuple:\n    """"""\n    This function is designed to search a collection for any objects\n    which may need to be simplified (i.e., torch tensors). It iterates\n    through each object in the collection and calls _simplify on it. Finally,\n    it returns the output as the tuple of simplified items of the input collection.\n    This function is used to simplify list, set, and tuple. The reverse function,\n    which undoes the functionality of this function is different for each of these types:\n    _detail_collection_list, _detail_collection_set, _detail_collection_tuple.\n\n    Args:\n        my_collection (Collection): a collection of python objects\n\n    Returns:\n        Tuple: a tuple with simplified objects.\n\n    """"""\n\n    # Don\'t simplify contents\n    if shallow:\n        return tuple(my_collection)\n\n    # Step 0: initialize empty list\n    pieces = []\n\n    # Step 1: serialize each part of the collection\n    for part in my_collection:\n        pieces.append(serde._simplify(worker, part))\n\n    # Step 2: return serialization as tuple of simplified items\n    return tuple(pieces)\n\n\ndef _detail_collection_list(\n    worker: AbstractWorker, my_collection: Tuple, shallow: bool = False\n) -> Collection:\n    """"""\n    This function is designed to operate in the opposite direction of\n    _simplify_collection. It takes a tuple of simple python objects\n    and iterates through it to determine whether objects in the collection\n    need to be converted into more advanced types. In particular, it\n    converts binary objects into torch Tensors where appropriate.\n\n    Args:\n        worker: the worker doing the deserialization\n        my_collection (Tuple): a tuple of simple python objects (including binary).\n\n    Returns:\n        Collection: a collection of the same type as the input where the objects\n            in the collection have been detailed.\n    """"""\n\n    # Don\'t detail contents\n    if shallow:\n        return list(my_collection)\n\n    pieces = []\n\n    # Step 1: deserialize each part of the collection\n    for part in my_collection:\n        detailed = serde._detail(worker, part)\n        pieces.append(detailed)\n\n    return pieces\n\n\ndef _detail_collection_set(\n    worker: AbstractWorker, my_collection: Tuple, shallow: bool = False\n) -> Collection:\n    """"""\n    This function is designed to operate in the opposite direction of\n    _simplify_collection. It takes a tuple of simple python objects\n    and iterates through it to determine whether objects in the collection\n    need to be converted into more advanced types. In particular, it\n    converts binary objects into torch Tensors where appropriate.\n\n    Args:\n        worker: the worker doing the deserialization\n        my_collection (Tuple): a tuple of simple python objects (including binary).\n\n    Returns:\n        Collection: a collection of the same type as the input where the objects\n            in the collection have been detailed.\n    """"""\n\n    # Don\'t detail contents\n    if shallow:\n        return set(my_collection)\n\n    pieces = []\n\n    # Step 1: deserialize each part of the collection\n    for part in my_collection:\n        detailed = serde._detail(worker, part)\n        pieces.append(detailed)\n    return set(pieces)\n\n\ndef _detail_collection_tuple(\n    worker: AbstractWorker, my_tuple: Tuple, shallow: bool = False\n) -> Tuple:\n    """"""\n    This function is designed to operate in the opposite direction of\n    _simplify_collection. It takes a tuple of simple python objects\n    and iterates through it to determine whether objects in the collection\n    need to be converted into more advanced types. In particular, it\n    converts binary objects into torch Tensors where appropriate.\n    This is only applicable to tuples. They need special handling because\n    `msgpack` is encoding a tuple as a list.\n\n    Args:\n        worker: the worker doing the deserialization\n        my_tuple (Tuple): a collection of simple python objects (including binary).\n\n    Returns:\n        tuple: a collection of the same type as the input where the objects\n            in the collection have been detailed.\n    """"""\n\n    # Don\'t detail contents\n    if shallow:\n        return my_tuple\n\n    pieces = []\n\n    # Step 1: deserialize each part of the collection\n    for part in my_tuple:\n        pieces.append(serde._detail(worker, part))\n\n    return tuple(pieces)\n\n\ndef _simplify_dictionary(worker: AbstractWorker, my_dict: Dict, shallow: bool = False) -> Tuple:\n    """"""\n    This function is designed to search a dict for any objects\n    which may need to be simplified (i.e., torch tensors). It iterates\n    through each key, value in the dict and calls _simplify on it. Finally,\n    it returns the output tuple of tuples containing key/value pairs. The\n    reverse function to this function is _detail_dictionary, which undoes\n    the functionality of this function.\n\n    Args:\n        my_dict: A dictionary of python objects.\n\n    Returns:\n        Tuple: Tuple containing tuples of simplified key/value pairs from the\n            input dictionary.\n\n    """"""\n    pieces = []\n    # for dictionaries we want to simplify both the key and the value\n    for key, value in my_dict.items():\n        pieces.append(\n            (serde._simplify(worker, key), serde._simplify(worker, value) if not shallow else value)\n        )\n\n    return tuple(pieces)\n\n\ndef _detail_dictionary(worker: AbstractWorker, my_dict: Tuple, shallow: bool = False) -> Dict:\n    """"""\n    This function is designed to operate in the opposite direction of\n    _simplify_dictionary. It takes a dictionary of simple python objects\n    and iterates through it to determine whether objects in the collection\n    need to be converted into more advanced types. In particular, it\n    converts binary objects into torch Tensors where appropriate.\n\n    Args:\n        worker: the worker doing the deserialization\n        my_dict (Tuple): a simplified dictionary of simple python objects (including binary).\n\n    Returns:\n        Dict: a collection of the same type as the input where the objects\n            in the collection have been detailed.\n    """"""\n    pieces = {}\n    # for dictionaries we want to detail both the key and the value\n    for key, value in my_dict:\n        detailed_key = serde._detail(worker, key)\n\n        if shallow:\n            pieces[detailed_key] = value\n        else:\n            detailed_value = serde._detail(worker, value)\n            pieces[detailed_key] = detailed_value\n\n    return pieces\n\n\n# Simplify/Detail native types\n\n\ndef _simplify_str(worker: AbstractWorker, obj: str) -> tuple:\n    return (obj.encode(""utf-8""),)\n\n\ndef _detail_str(worker: AbstractWorker, str_tuple: tuple) -> str:\n    return str_tuple[0].decode(""utf-8"")\n\n\ndef _simplify_range(worker: AbstractWorker, my_range: range) -> Tuple[int, int, int]:\n    """"""\n    This function extracts the start, stop and step from the range.\n\n    Args:\n        my_range (range): a range object\n\n    Returns:\n        list: a list defining the range parameters [start, stop, step]\n\n    Examples:\n\n        range_parameters = _simplify_range(range(1, 3, 4))\n\n        assert range_parameters == [1, 3, 4]\n\n    """"""\n\n    return (my_range.start, my_range.stop, my_range.step)\n\n\ndef _detail_range(worker: AbstractWorker, my_range_params: Tuple[int, int, int]) -> range:\n    """"""\n    This function extracts the start, stop and step from a tuple.\n\n    Args:\n        worker: the worker doing the deserialization (only here to standardise signature\n            with other _detail functions)\n        my_range_params (tuple): a tuple defining the range parameters [start, stop, step]\n\n    Returns:\n        range: a range object\n\n    Examples:\n        new_range = _detail_range([1, 3, 4])\n\n        assert new_range == range(1, 3, 4)\n\n    """"""\n\n    return range(my_range_params[0], my_range_params[1], my_range_params[2])\n\n\ndef _simplify_ellipsis(worker: AbstractWorker, e: Ellipsis) -> Tuple[bytes]:\n    return (b"""",)\n\n\ndef _detail_ellipsis(worker: AbstractWorker, ellipsis: bytes) -> Ellipsis:\n    return ...\n\n\ndef _simplify_slice(worker: AbstractWorker, my_slice: slice) -> Tuple[int, int, int]:\n    """"""\n    This function creates a list that represents a slice.\n\n    Args:\n        my_slice (slice): a python slice\n\n    Returns:\n        tuple : a list holding the start, stop and step values\n\n    Examples:\n\n        slice_representation = _simplify_slice(slice(1,2,3))\n\n    """"""\n    return (my_slice.start, my_slice.stop, my_slice.step)\n\n\ndef _detail_slice(worker: AbstractWorker, my_slice: Tuple[int, int, int]) -> slice:\n    """"""\n    This function extracts the start, stop and step from a list.\n\n    Args:\n        my_slice (tuple): a list defining the slice parameters [start, stop, step]\n\n    Returns:\n        range: a range object\n\n    Examples:\n        new_range = _detail_range([1, 3, 4])\n\n        assert new_range == range(1, 3, 4)\n\n    """"""\n\n    return slice(my_slice[0], my_slice[1], my_slice[2])\n\n\n#   Numpy array\n\n\ndef _simplify_ndarray(worker: AbstractWorker, my_array: numpy.ndarray) -> Tuple[bin, Tuple, Tuple]:\n    """"""\n    This function gets the byte representation of the array\n        and stores the dtype and shape for reconstruction\n\n    Args:\n        my_array (numpy.ndarray): a numpy array\n\n    Returns:\n        list: a list holding the byte representation, shape and dtype of the array\n\n    Examples:\n\n        arr_representation = _simplify_ndarray(numpy.random.random([1000, 1000])))\n\n    """"""\n    arr_bytes = my_array.tobytes()\n    arr_shape = serde._simplify(worker, my_array.shape)\n    arr_dtype = serde._simplify(worker, my_array.dtype.name)\n\n    return (arr_bytes, arr_shape, arr_dtype)\n\n\ndef _detail_ndarray(\n    worker: AbstractWorker, arr_representation: Tuple[bin, Tuple, str]\n) -> numpy.ndarray:\n    """"""\n    This function reconstruct a numpy array from it\'s byte data, the shape and the dtype\n        by first loading the byte data with the appropiate dtype and then reshaping it into the\n        original shape\n\n    Args:\n        worker: the worker doing the deserialization\n        arr_representation (tuple): a tuple holding the byte representation, shape\n        and dtype of the array\n\n    Returns:\n        numpy.ndarray: a numpy array\n\n    Examples:\n        arr = _detail_ndarray(arr_representation)\n\n    """"""\n    arr_shape = serde._detail(worker, arr_representation[1])\n    arr_dtype = serde._detail(worker, arr_representation[2])\n    res = numpy.frombuffer(arr_representation[0], dtype=arr_dtype).reshape(arr_shape)\n\n    assert type(res) == numpy.ndarray\n\n    return res\n\n\ndef _simplify_type(worker: AbstractWorker, obj_type: type) -> bytes:\n    """"""\n    This function gets an type object and returns its representation as bytes.\n\n    Args:\n        obj_type (s.g builtins.str, builtins.int, torch.tensor): a type\n\n    Returns:\n        str: a string in utf-8 encoding that encodes the path in the module + the actual\n        class that generates the type.\n\n    Examples:\n          str_type_representation = _simplify_type(worker, type(""i\'m a string""))\n    """"""\n    module_path = obj_type.__module__\n    full_path_type = module_path + ""."" + obj_type.__name__\n    return full_path_type.encode(""utf-8"")\n\n\ndef _detail_type(worker: AbstractWorker, type_repr: bytes) -> type:\n    """"""\n    This function gets the byte representation of a type and its path in the module as a string,\n    decodes the string and locates the type in the module, returning the type object.\n\n    Args:\n        type_repr: bytes that encode the path of a type/class in a module\n\n    Returns:\n        type: the type of an object (e.g: builtins.str, builtins.int).\n\n    Warning: if pydoc can\'t locate the type in the current process, might mean that the file\n    layout is different between sender and receiver.\n\n    TODO:\n        As syft-protobuf grows in type support, we should change the type serialization by\n        using those types, enabling cross language typechecking/type validation.\n    """"""\n\n    string_representation = type_repr.decode(""utf-8"")\n    result = pydoc.locate(string_representation)\n    if result is None:\n        warnings.warn(\n            f""{string_representation} can\'t be located in the current process, the layout ""\n            ""of the modules has been changed."",\n            Warning,\n        )\n        return object\n    return result\n\n\ndef _simplify_numpy_number(\n    worker: AbstractWorker, numpy_nb: Union[numpy.int32, numpy.int64, numpy.float32, numpy.float64]\n) -> Tuple[bin, Tuple]:\n    """"""\n    This function gets the byte representation of the numpy number\n        and stores the dtype for reconstruction\n\n    Args:\n        numpy_nb (e.g numpy.float64): a numpy number\n\n    Returns:\n        list: a list holding the byte representation, dtype of the numpy number\n\n    Examples:\n\n        np_representation = _simplify_numpy_number(worker, numpy.float64(2.3)))\n\n    """"""\n    nb_bytes = numpy_nb.tobytes()\n    nb_dtype = serde._simplify(worker, numpy_nb.dtype.name)\n\n    return (nb_bytes, nb_dtype)\n\n\ndef _detail_numpy_number(\n    worker: AbstractWorker, nb_representation: Tuple[bin, Tuple, str]\n) -> Union[numpy.int32, numpy.int64, numpy.float32, numpy.float64]:\n    """"""\n    This function reconstruct a numpy number from it\'s byte data, dtype\n        by first loading the byte data with the appropiate dtype\n\n    Args:\n        worker: the worker doing the deserialization\n        np_representation (tuple): a tuple holding the byte representation\n        and dtype of the numpy number\n\n    Returns:\n        numpy.float or numpy.int: a numpy number\n\n    Examples:\n        nb = _detail_numpy_number(nb_representation)\n\n    """"""\n    nb_dtype = serde._detail(worker, nb_representation[1])\n    nb = numpy.frombuffer(nb_representation[0], dtype=nb_dtype)[0]\n\n    assert type(nb) in [numpy.float32, numpy.float64, numpy.int32, numpy.int64]\n\n    return nb\n\n\n# Maps a type to a tuple containing its simplifier and detailer function\n# IMPORTANT: serialization constants for these objects need to be defined in `proto.json` file\n# in https://github.com/OpenMined/proto\nMAP_NATIVE_SIMPLIFIERS_AND_DETAILERS = OrderedDict(\n    {\n        dict: (_simplify_dictionary, _detail_dictionary),\n        list: (_simplify_collection, _detail_collection_list),\n        range: (_simplify_range, _detail_range),\n        set: (_simplify_collection, _detail_collection_set),\n        slice: (_simplify_slice, _detail_slice),\n        type: (_simplify_type, _detail_type),\n        str: (_simplify_str, _detail_str),\n        tuple: (_simplify_collection, _detail_collection_tuple),\n        type(Ellipsis): (_simplify_ellipsis, _detail_ellipsis),\n        numpy.ndarray: (_simplify_ndarray, _detail_ndarray),\n        numpy.float32: (_simplify_numpy_number, _detail_numpy_number),\n        numpy.float64: (_simplify_numpy_number, _detail_numpy_number),\n        numpy.int32: (_simplify_numpy_number, _detail_numpy_number),\n        numpy.int64: (_simplify_numpy_number, _detail_numpy_number),\n    }\n)\n'"
syft/serde/msgpack/proto.py,0,"b'""""""\nThis file exists to translate python classes to Serde type constants defined in `proto.json` file\nin https://github.com/OpenMined/proto.\nThe reason for this is to have stable constants used in Serde serialization protocol\nand the definition file that can be used not only by PySyft but also in other languages.\n\nhttps://github.com/OpenMined/proto (`pysyft_proto` module) is included as dependency in setup.py\nexposes contents of `proto.json` file in `proto_info` variable.\n\nIMPORTANT: New types added in Serde need to be also defined in `proto.json`.\n""""""\n\nfrom syft_proto import proto_info\nfrom syft.exceptions import InvalidProtocolFileError\nfrom syft.exceptions import UndefinedProtocolTypeError\nfrom syft.exceptions import UndefinedProtocolTypePropertyError\n\nif proto_info is None:\n    raise InvalidProtocolFileError(""Failed to load syft protocol data"")\n\n\nclass TypeInfo:\n    """"""Convenience wrapper for type info defined in `proto_info`.\n    Exposes type constants with error handling.\n    """"""\n\n    def __init__(self, name, obj):\n        """"""Initializes type info for a given class identified by `name` with contents of\n        `proto_info` for this class.\n        """"""\n        self.name = name\n        self.obj = obj\n\n    @property\n    def code(self):\n        """"""Returns `code` property (serialization constant) for class\n        or throws an exception if it\'s not defined in `proto.json`.""""""\n        if ""code"" in self.obj:\n            return self.obj[""code""]\n        else:\n            raise UndefinedProtocolTypePropertyError(f""code is not set for {self.name}"")\n\n    @property\n    def forced_code(self):\n        """"""Returns `forced_code` property (serialization constant) for class\n        or throws an exception if it\'s not defined in `proto.json`.""""""\n        if ""forced_code"" in self.obj:\n            return self.obj[""forced_code""]\n        else:\n            raise UndefinedProtocolTypePropertyError(f""forced_code is not set for {self.name}"")\n\n\ndef fullname(cls):\n    """"""Returns full name of a given *class* (not instance of class).\n    Source:\n    https://stackoverflow.com/questions/2020014/get-fully-qualified-class-name-of-an-object-in-python. # noqa: E501\n    """"""\n    module = cls.__module__\n    if module is None or module == str.__module__:\n        return cls.__name__  # Avoid reporting __builtin__\n    else:\n        return module + ""."" + cls.__name__\n\n\ndef proto_type_info(cls):\n    """"""Returns `TypeInfo` instance for a given *class* identified by `cls` parameter.\n    Throws an exception when such class does not exists in the `proto.json`.\n    """"""\n    type_name = fullname(cls)\n\n    if type_name in proto_info[""TYPES""]:\n        return TypeInfo(name=type_name, obj=proto_info[""TYPES""][type_name])\n    elif cls.get_msgpack_code.__qualname__.startswith(cls.__name__):\n        return TypeInfo(name=type_name, obj=cls.get_msgpack_code())\n    else:\n        raise UndefinedProtocolTypeError(\n            f""{type_name} is not defined in the protocol file and it does not provide a code by""\n            f"" implementing \'get_msgpack_code\'.""\n        )\n'"
syft/serde/msgpack/serde.py,0,"b'""""""\nThis file exists to provide one common place for all msgpack serialization to occur.\nAs msgpack only supports basic types and binary formats every type must be first be\nconverted to one of these types. Thus, we\'ve split our functionality into three steps.\nWhen converting from a PySyft object (or collection of objects) to an object to be\nsent over the wire (a message), those three steps are (in order):\n\n1. Simplify - converts PyTorch objects to simple Python objects (using pickle)\n2. Serialize - converts Python objects to binary\n3. Compress - compresses the binary (Now we\'re ready send!)\n\nInversely, when converting from a message sent over the wire back to a PySyft\nobject, the three steps are (in order):\n\n1. Decompress - converts compressed binary back to decompressed binary\n2. Deserialize - converts from binary to basic python objects\n3. Detail - converts some basic python objects back to PyTorch objects (Tensors)\n\nFurthermore, note that there is different simplification/serialization logic\nfor objects of different types. Thus, instead of using if/else logic, we have\nglobal dictionaries which contain functions and Python types as keys. For\nsimplification logic, this dictionary is called ""simplifiers"". The keys\nare the types and values are the simplification logic. For example,\nsimplifiers[tuple] will return the function which knows how to simplify the\ntuple type. The same is true for all other simplifier/detailer functions.\n\nBy default, the simplification/detail operations expect Torch tensors. If the setup requires other\nserialization process, it can override the functions _serialize_tensor and _deserialize_tensor\n\nBy default, we serialize using msgpack and compress using lz4.\nIf different compressions are required, the worker can override the function apply_compress_scheme\n""""""\nfrom collections import OrderedDict\nimport inspect\nfrom dataclasses import dataclass\n\nimport syft\nimport msgpack as msgpack_lib\nfrom syft import dependency_check\n\nfrom syft.serde import compression\nfrom syft.serde import msgpack\nfrom syft.serde.msgpack.native_serde import MAP_NATIVE_SIMPLIFIERS_AND_DETAILERS\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.workers.virtual import VirtualWorker\n\nfrom syft.exceptions import GetNotPermittedError\nfrom syft.exceptions import ResponseSignatureError\n\nfrom syft.serde.syft_serializable import SyftSerializable, get_msgpack_subclasses\nfrom syft.serde.msgpack.proto import proto_type_info\n\nif dependency_check.torch_available:\n    from syft.serde.msgpack.torch_serde import MAP_TORCH_SIMPLIFIERS_AND_DETAILERS\nelse:\n    MAP_TORCH_SIMPLIFIERS_AND_DETAILERS = {}\n\nif dependency_check.tensorflow_available:\n    from syft_tensorflow.serde import MAP_TF_SIMPLIFIERS_AND_DETAILERS\nelse:\n    MAP_TF_SIMPLIFIERS_AND_DETAILERS = {}\n\n\nclass MetaMsgpackGlobalState(type):\n    @staticmethod\n    def wrapper(wrapped_func):\n        @property\n        def wrapper(self):\n            self = self.update()\n            return wrapped_func.__get__(self, type(self))\n\n        return wrapper\n\n    def __new__(meta, classname, bases, class_dict):\n        for attr_name, attr_body in class_dict.items():\n            if isinstance(attr_body, property):\n                class_dict[attr_name] = MetaMsgpackGlobalState.wrapper(attr_body)\n        return type.__new__(meta, classname, bases, class_dict)\n\n\n@dataclass\nclass MsgpackGlobalState(metaclass=MetaMsgpackGlobalState):\n    """"""\n        Global msgpack state. This should become deprecated soon.\n    """"""\n\n    _OBJ_SIMPLIFIER_AND_DETAILERS = []\n    _MAP_TO_SIMPLFIERS_AND_DETAILERS = OrderedDict()\n    _OBJ_FORCE_FULL_SIMPLIFIER_AND_DETAILERS = []\n    _EXCEPTION_SIMPLIFIER_AND_DETAILERS = []\n    _simplifiers = OrderedDict()\n    _forced_full_simplifiers = OrderedDict()\n    _detailers = OrderedDict()\n    _inherited_simplifiers_found = OrderedDict()\n    _no_simplifiers_found = set()\n    _no_full_simplifiers_found = set()\n\n    stale_state = True\n\n    @property\n    def obj_simplifier_and_detailers(self):\n        return self._OBJ_SIMPLIFIER_AND_DETAILERS\n\n    @property\n    def map_to_simplifiers_and_detailers(self):\n        return self._MAP_TO_SIMPLFIERS_AND_DETAILERS\n\n    @property\n    def obj_force_full_simplifiser_and_detailer(self):\n        return self._OBJ_FORCE_FULL_SIMPLIFIER_AND_DETAILERS\n\n    @property\n    def exception_simplifier_and_detailer(self):\n        return self._EXCEPTION_SIMPLIFIER_AND_DETAILERS\n\n    @property\n    def simplifiers(self):\n        return self._simplifiers\n\n    @property\n    def detailers(self):\n        return self._detailers\n\n    @property\n    def forced_full_simplifiers(self):\n        return self._forced_full_simplifiers\n\n    @property\n    def inherited_simplifiers_found(self):\n        return self._inherited_simplifiers_found\n\n    @property\n    def no_simplifiers_found(self):\n        return self._no_simplifiers_found\n\n    @property\n    def no_full_simplifiers_found(self):\n        return self._no_full_simplifiers_found\n\n    def update(self):\n        if not self.stale_state:\n            return self\n\n        # If an object implements its own simplify and detail functions it should be stored\n        # in this list\n        # NOTE: serialization constants for these objects need to be defined in `proto.json` file\n        # in https://github.com/OpenMined/proto\n\n        self._OBJ_SIMPLIFIER_AND_DETAILERS = list(get_msgpack_subclasses(SyftSerializable))\n\n        # Maps a type to a tuple containing its simplifier and detailer function\n        # NOTE: serialization constants for these objects need to be defined in `proto.json` file\n        # in https://github.com/OpenMined/proto\n\n        self._MAP_TO_SIMPLFIERS_AND_DETAILERS = OrderedDict(\n            list(MAP_NATIVE_SIMPLIFIERS_AND_DETAILERS.items())\n            + list(MAP_TORCH_SIMPLIFIERS_AND_DETAILERS.items())\n            + list(MAP_TF_SIMPLIFIERS_AND_DETAILERS.items())\n        )\n\n        self._OBJ_FORCE_FULL_SIMPLIFIER_AND_DETAILERS = [VirtualWorker]\n        self._EXCEPTION_SIMPLIFIER_AND_DETAILERS = [GetNotPermittedError, ResponseSignatureError]\n\n        self.stale_state = False\n\n        def _add_simplifier_and_detailer(curr_type, simplifier, detailer, forced=False):\n            type_info = proto_type_info(curr_type)\n            if forced:\n                self._forced_full_simplifiers[curr_type] = (type_info.forced_code, simplifier)\n                self._detailers[type_info.forced_code] = detailer\n            else:\n                self._simplifiers[curr_type] = (type_info.code, simplifier)\n                self._detailers[type_info.code] = detailer\n\n        # Register native and torch types\n        for curr_type, (simplifier, detailer) in self._MAP_TO_SIMPLFIERS_AND_DETAILERS.items():\n            _add_simplifier_and_detailer(curr_type, simplifier, detailer)\n\n        # # Register syft objects with custom simplify and detail methods\n        for syft_type in (\n            self._OBJ_SIMPLIFIER_AND_DETAILERS + self._EXCEPTION_SIMPLIFIER_AND_DETAILERS\n        ):\n            simplifier, detailer = syft_type.simplify, syft_type.detail\n            _add_simplifier_and_detailer(syft_type, simplifier, detailer)\n        #\n        # # Register syft objects with custom force_simplify and force_detail methods\n        for syft_type in self._OBJ_FORCE_FULL_SIMPLIFIER_AND_DETAILERS:\n            force_simplifier, force_detailer = syft_type.force_simplify, syft_type.force_detail\n            _add_simplifier_and_detailer(syft_type, force_simplifier, force_detailer, forced=True)\n        return self\n\n\n# cached value\nfield = 2 ** 64\nstr_field = str(2 ** 64)\n\n\n## SECTION: High Level Simplification Router\ndef _force_full_simplify(worker: AbstractWorker, obj: object) -> object:\n    """"""To force a full simplify generally if the usual _simplify is not suitable.\n\n    If we can not full simplify a object we simplify it as usual instead.\n\n    Args:\n        obj: The object.\n\n    Returns:\n        The simplified object.\n    """"""\n    # check to see if there is a full simplifier\n    # for this type. If there is, return the full simplified object.\n    current_type = type(obj)\n    if current_type in msgpack_global_state.forced_full_simplifiers:\n        result = (\n            msgpack_global_state.forced_full_simplifiers[current_type][0],\n            msgpack_global_state.forced_full_simplifiers[current_type][1](worker, obj),\n        )\n        return result\n    # If we already tried to find a full simplifier for this type but failed, we should\n    # simplify it instead.\n    elif current_type in msgpack_global_state.no_full_simplifiers_found:\n        return _simplify(worker, obj)\n    else:\n        # If the object type is not in forced_full_simplifiers,\n        # we check the classes that this object inherits from.\n        # `inspect.getmro` give us all types this object inherits\n        # from, including `type(obj)`. We can skip the type of the\n        # object because we already tried this in the\n        # previous step.\n        classes_inheritance = inspect.getmro(type(obj))[1:]\n\n        for inheritance_type in classes_inheritance:\n            if inheritance_type in msgpack_global_state.forced_full_simplifiers:\n                # Store the inheritance_type in forced_full_simplifiers so next\n                # time we see this type serde will be faster.\n                msgpack_global_state.forced_full_simplifiers[\n                    current_type\n                ] = msgpack_global_state.forced_full_simplifiers[inheritance_type]\n                result = (\n                    msgpack_global_state.forced_full_simplifiers[current_type][0],\n                    msgpack_global_state.forced_full_simplifiers[current_type][1](worker, obj),\n                )\n                return result\n\n        # If there is not a full_simplifier for this\n        # object, then we simplify it.\n        msgpack_global_state.no_full_simplifiers_found.add(current_type)\n        return _simplify(worker, obj)\n\n\n# Store types that are not simplifiable (int, float, None) so we\n# can ignore them during serialization.\n# Store types that use simplifiers from their ancestors so we\n# can look them up quickly during serialization.\n\n\ndef _serialize_msgpack_simple(\n    obj: object,\n    worker: AbstractWorker = None,\n    simplified: bool = False,\n    force_full_simplification: bool = False,\n) -> bin:\n\n    if worker is None:\n        # TODO[jvmancuso]: This might be worth a standalone function.\n        worker = syft.framework.hook.local_worker\n\n    # 1) Simplify\n    # simplify difficult-to-serialize objects. See the _simplify method\n    # for details on how this works. The general purpose is to handle types\n    # which the fast serializer cannot handle\n    if not simplified:\n        if force_full_simplification:\n            simple_objects = _force_full_simplify(worker, obj)\n        else:\n            simple_objects = _simplify(worker, obj)\n    else:\n        simple_objects = obj\n\n    return simple_objects\n\n\ndef _serialize_msgpack_binary(\n    simple_objects: object,\n    worker: AbstractWorker = None,\n    simplified: bool = False,\n    force_full_simplification: bool = False,\n) -> bin:\n    # 2) Serialize\n    # serialize into a binary\n    binary = msgpack_lib.dumps(simple_objects)\n\n    # 3) Compress\n    # compress the binary and return the result\n    # prepend a 1-byte header \'0\' or \'1\' to the output stream\n    # to denote whether output stream is compressed or not\n    # if compressed stream length is greater than input stream\n    # we output the input stream as it is with header set to \'0\'\n    # otherwise we output the compressed stream with header set to \'1\'\n    # even if compressed flag is set to false by the caller we\n    # output the input stream as it is with header set to \'0\'\n    return compression._compress(binary)\n\n\ndef serialize(\n    obj: object,\n    worker: AbstractWorker = None,\n    simplified: bool = False,\n    force_full_simplification: bool = False,\n) -> bin:\n    """"""This method can serialize any object PySyft needs to send or store.\n\n    This is the high level function for serializing any object or collection\n    of objects which PySyft needs to send over the wire. It includes three\n    steps, Simplify, Serialize, and Compress as described inline below.\n\n    Args:\n        obj (object): the object to be serialized\n        simplified (bool): in some cases we want to pass in data which has\n            already been simplified - in which case we must skip double\n            simplification - which would be bad.... so bad... so... so bad\n        force_full_simplification (bool): Some objects are only partially serialized\n            by default. For objects where this is the case, setting this flag to True\n            will force the entire object to be serialized. For example, setting this\n            flag to True will cause a VirtualWorker to be serialized WITH all of its\n            tensors while by default VirtualWorker objects only serialize a small\n            amount of metadata.\n\n    Returns:\n        binary: the serialized form of the object.\n    """"""\n    if worker is None:\n        # TODO[jvmancuso]: This might be worth a standalone function.\n        worker = syft.framework.hook.local_worker\n\n    simple_objects = _serialize_msgpack_simple(obj, worker, simplified, force_full_simplification)\n    return _serialize_msgpack_binary(simple_objects)\n\n\ndef _deserialize_msgpack_binary(binary: bin, worker: AbstractWorker = None) -> object:\n    if worker is None:\n        # TODO[jvmancuso]: This might be worth a standalone function.\n        worker = syft.framework.hook.local_worker\n\n    # 1) Decompress the binary if needed\n    binary = compression._decompress(binary)\n\n    # 2) Deserialize\n    # This function converts the binary into the appropriate python\n    # object (or nested dict/collection of python objects)\n    simple_objects = msgpack_lib.loads(binary, use_list=False)\n\n    # sometimes we want to skip detailing (such as in Plan)\n    return simple_objects\n\n\ndef _deserialize_msgpack_simple(simple_objects: object, worker: AbstractWorker = None) -> object:\n    if worker is None:\n        # TODO[jvmancuso]: This might be worth a standalone function.\n        worker = syft.framework.hook.local_worker\n\n    # 3) Detail\n    # This function converts typed, simple objects into their morefrom typing import Dict\n    # complex (and difficult to serialize) counterparts which the\n    # serialization library wasn\'t natively able to serialize (such\n    # as msgpack\'s inability to serialize torch tensors or ... or\n    # python slice objects\n    return _detail(worker, simple_objects)\n\n\ndef deserialize(binary: bin, worker: AbstractWorker = None) -> object:\n    if worker is None:\n        # TODO[jvmancuso]: This might be worth a standalone function.\n        worker = syft.framework.hook.local_worker\n\n    simple_objects = _deserialize_msgpack_binary(binary, worker)\n    return _deserialize_msgpack_simple(simple_objects, worker)\n\n\ndef _simplify_field(obj):\n    """"""\n    This function converts large numeric values which\n    will cause overflow into str before serialisation\n    """"""\n    current_type = type(obj)\n    if current_type == int and obj >= field:\n        obj = str(obj)\n        current_type = str\n    return current_type, obj\n\n\ndef _simplify(worker: AbstractWorker, obj: object, **kwargs) -> object:\n    """"""\n    This function takes an object as input and returns a simple\n    Python object which is supported by the chosen serialization\n    method (such as JSON or msgpack). The reason we have this function\n    is that some objects are either NOT supported by high level (fast)\n    serializers OR the high level serializers don\'t support the fastest\n    form of serialization. For example, PyTorch tensors have custom pickle\n    functionality thus its better to pre-serialize PyTorch tensors using\n    pickle and then serialize the binary in with the rest of the message\n    being sent.\n\n    Args:\n        obj: An object which may need to be simplified.\n\n    Returns:\n        An simple Python object which msgpack can serialize.\n\n    Raises:\n        ValueError: if `move_this` or `in_front_of_that` are not both single ASCII\n        characters.\n    """"""\n    # Check to see if there is a simplifier\n    # for this type. If there is, return the simplified object.\n\n    current_type, obj = _simplify_field(obj)\n\n    if current_type in msgpack_global_state.simplifiers:\n        result = (\n            msgpack_global_state.simplifiers[current_type][0],\n            msgpack_global_state.simplifiers[current_type][1](worker, obj, **kwargs),\n        )\n        return result\n    elif current_type in msgpack_global_state.inherited_simplifiers_found:\n        result = (\n            msgpack_global_state.inherited_simplifiers_found[current_type][0],\n            msgpack_global_state.inherited_simplifiers_found[current_type][1](\n                worker, obj, **kwargs\n            ),\n        )\n        return result\n\n    # If we already tried to find a simplifier for this type but failed, we should\n    # just return the object as it is.\n    elif current_type in msgpack_global_state.no_simplifiers_found:\n        return obj\n\n    else:\n        # If the object type is not in simplifiers,\n        # we check the classes that this object inherits from.\n        # `inspect.getmro` give us all types this object inherits\n        # from, including `type(obj)`. We can skip the type of the\n        # object because we already tried this in the\n        # previous step.\n        classes_inheritance = inspect.getmro(type(obj))[1:]\n\n        for inheritance_type in classes_inheritance:\n            if inheritance_type in msgpack_global_state.simplifiers:\n                # Store the inheritance_type in simplifiers so next time we see this type\n                # serde will be faster.\n                msgpack_global_state.inherited_simplifiers_found[\n                    current_type\n                ] = msgpack_global_state.simplifiers[inheritance_type]\n                result = (\n                    msgpack_global_state.inherited_simplifiers_found[current_type][0],\n                    msgpack_global_state.inherited_simplifiers_found[current_type][1](\n                        worker, obj, **kwargs\n                    ),\n                )\n                return result\n\n        # if there is not a simplifier for this\n        # object, then it might be one external to\n        # the framework\n        msgpack_global_state.stale_state = True\n        if current_type in msgpack_global_state.simplifiers:\n            result = (\n                msgpack_global_state.simplifiers[current_type][0],\n                msgpack_global_state.simplifiers[current_type][1](worker, obj, **kwargs),\n            )\n            return result\n\n        # if there is not a simplifier for this\n        # object, then the object is already a\n        # simple python object and we can just\n        # return it.\n        msgpack_global_state.no_simplifiers_found.add(current_type)\n        return obj\n\n\ndef _detail_field(typeCode, val):\n    """"""\n    This functions converts the field value 2**64 which was\n    serialised as str to avoid msgpack overflow back to int\n    after deserialisation.\n    """"""\n    if typeCode == msgpack.proto_type_info(str).code and val == str_field:\n        return int(val)\n    else:\n        return val\n\n\ndef _detail(worker: AbstractWorker, obj: object, **kwargs) -> object:\n    """"""Reverses the functionality of _simplify.\n    Where applicable, it converts simple objects into more complex objects such\n    as converting binary objects into torch tensors. Read _simplify for more\n    information on why _simplify and detail are needed.\n\n    Args:\n        worker: the worker which is acquiring the message content, for example\n        used to specify the owner of a tensor received(not obvious for\n        virtual workers).\n        obj: a simple Python object which msgpack deserialized.\n\n    Returns:\n        obj: a more complex Python object which msgpack would have had trouble\n            deserializing directly.\n    """"""\n    if type(obj) in (list, tuple):\n        val = msgpack_global_state.detailers[obj[0]](worker, obj[1], **kwargs)\n        return _detail_field(obj[0], val)\n    else:\n        return obj\n\n\nmsgpack_global_state = MsgpackGlobalState()\n'"
syft/serde/msgpack/torch_serde.py,47,"b'""""""\nThis file exists to provide one common place for all serialisation and simplify_ and _detail\nfor all tensors (Torch and Numpy).\n""""""\nfrom collections import OrderedDict\nimport io\nfrom typing import Tuple\nimport warnings\n\nimport torch\n\nimport syft\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.generic.pointers.multi_pointer import MultiPointerTensor\nfrom syft.generic.abstract.tensor import initialize_tensor\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.serde.msgpack import serde\nfrom syft.codes import TENSOR_SERIALIZATION\n\nfrom syft.serde.torch.serde import TORCH_DTYPE_STR\nfrom syft.serde.torch.serde import TORCH_STR_DTYPE\nfrom syft.serde.torch.serde import TORCH_MFORMAT_ID\nfrom syft.serde.torch.serde import TORCH_ID_MFORMAT\nfrom syft.serde.torch.serde import torch_tensor_serializer\nfrom syft.serde.torch.serde import torch_tensor_deserializer\nfrom syft.serde.torch.serde import numpy_tensor_serializer\nfrom syft.serde.torch.serde import numpy_tensor_deserializer  # noqa: F401\n\n\ndef _serialize_tensor(worker: AbstractWorker, tensor) -> bin:\n    """"""Serialize the tensor using as default Torch serialization strategy\n    This function can be overridden to provide different tensor serialization strategies\n\n    Args\n        (torch.Tensor): an input tensor to be serialized\n\n    Returns\n        A serialized version of the input tensor\n\n    """"""\n    serializers = {\n        TENSOR_SERIALIZATION.TORCH: torch_tensor_serializer,\n        TENSOR_SERIALIZATION.NUMPY: numpy_tensor_serializer,\n        TENSOR_SERIALIZATION.ALL: simplified_tensor_serializer,\n    }\n    if worker.serializer not in serializers:\n        raise NotImplementedError(\n            f""Tensor serialization strategy is not supported: {worker.serializer}""\n        )\n    serializer = serializers[worker.serializer]\n    return serializer(worker, tensor)\n\n\ndef _deserialize_tensor(worker: AbstractWorker, serializer: str, tensor_bin) -> torch.Tensor:\n    """"""Deserialize the input tensor passed as parameter into a Torch tensor.\n    `serializer` parameter selects different deserialization strategies\n\n    Args\n        worker: Worker\n        serializer: Strategy used for tensor deserialization (e.g.: torch, numpy, all)\n        tensor_bin: A simplified representation of a tensor\n\n    Returns\n        a Torch tensor\n    """"""\n    deserializers = {\n        TENSOR_SERIALIZATION.TORCH: torch_tensor_deserializer,\n        TENSOR_SERIALIZATION.NUMPY: numpy_tensor_serializer,\n        TENSOR_SERIALIZATION.ALL: simplified_tensor_deserializer,\n    }\n    if serializer not in deserializers:\n        raise NotImplementedError(\n            f""Cannot deserialize tensor serialized with \'{serializer}\' strategy""\n        )\n    deserializer = deserializers[serializer]\n    return deserializer(worker, tensor_bin)\n\n\ndef simplified_tensor_serializer(worker: AbstractWorker, tensor: torch.Tensor) -> tuple:\n    """"""Strategy to serialize a tensor to native python types.\n    If tensor requires to calculate gradients, it will be detached.\n    """"""\n    if tensor.requires_grad:\n        warnings.warn(\n            ""Torch to native serializer can only be used with tensors that do not require grad. ""\n            ""Detaching tensor to continue""\n        )\n        tensor = tensor.detach()\n\n    tensor_tuple = (tuple(tensor.size()), TORCH_DTYPE_STR[tensor.dtype], tensor.flatten().tolist())\n    return serde._simplify(worker, tensor_tuple)\n\n\ndef simplified_tensor_deserializer(worker: AbstractWorker, tensor_tuple: tuple) -> torch.Tensor:\n    """"""Strategy to deserialize a simplified tensor into a Torch tensor""""""\n\n    size, dtype, data_arr = serde._detail(worker, tensor_tuple)\n    tensor = torch.tensor(data_arr, dtype=TORCH_STR_DTYPE[dtype]).reshape(size)\n    return tensor\n\n\n# Simplify/Detail Torch Tensors\n\n\ndef _simplify_torch_tensor(worker: AbstractWorker, tensor: torch.Tensor) -> bin:\n    """"""\n    This function converts a torch tensor into a serliaized torch tensor\n    using pickle. We choose to use this because PyTorch has a custom and\n    very fast PyTorch pickler.\n\n    Args:\n        tensor (torch.Tensor): an input tensor to be serialized\n\n    Returns:\n        tuple: serialized tuple of torch tensor. The first value is the\n        id of the tensor and the second is the binary for the PyTorch\n        object. The third is the chain of abstractions, and the fourth\n        (optinally) is the chain of graident tensors (nested tuple)\n    """"""\n\n    tensor_bin = _serialize_tensor(worker, tensor)\n\n    # note we need to do this explicitly because torch.save does not\n    # seem to be including .grad by default\n\n    if tensor.grad is not None:\n        if hasattr(tensor, ""child""):\n            if isinstance(tensor.child, (PointerTensor, MultiPointerTensor)):\n                grad_chain = None\n            else:\n                grad_chain = _simplify_torch_tensor(worker, tensor.grad)\n        else:\n            grad_chain = _simplify_torch_tensor(worker, tensor.grad)\n\n    else:\n        grad_chain = None\n\n    chain = None\n\n    # I think the pointer bug is is between here\n\n    if hasattr(tensor, ""child""):\n        chain = serde._simplify(worker, tensor.child)\n\n    # and here... leaving a reerence here so i can find it later\n    # TODO fix pointer bug\n\n    origin = tensor.origin\n    id_at_origin = tensor.id_at_origin\n\n    return (\n        tensor.id,\n        tensor_bin,\n        chain,\n        grad_chain,\n        serde._simplify(worker, tensor.tags),\n        serde._simplify(worker, tensor.description),\n        serde._simplify(worker, worker.serializer),\n        serde._simplify(worker, origin),\n        serde._simplify(worker, id_at_origin),\n    )\n\n\ndef _detail_torch_tensor(worker: AbstractWorker, tensor_tuple: tuple) -> torch.Tensor:\n    """"""\n    This function converts a serialized torch tensor into a torch tensor\n    using pickle.\n\n    Args:\n        tensor_tuple (bin): serialized obj of torch tensor. It\'s a tuple where\n            the first value is the ID, the second vlaue is the binary for the\n            PyTorch object, the third value is the chain of tensor abstractions,\n            and the fourth object is the chain of gradients (.grad.grad, etc.)\n\n    Returns:\n        torch.Tensor: a torch tensor that was serialized\n    """"""\n\n    (\n        tensor_id,\n        tensor_bin,\n        chain,\n        grad_chain,\n        tags,\n        description,\n        serializer,\n        origin,\n        id_at_origin,\n    ) = tensor_tuple\n\n    tensor = _deserialize_tensor(worker, serde._detail(worker, serializer), tensor_bin)\n\n    # note we need to do this explicitly because torch.load does not\n    # include .grad informatino\n    if grad_chain is not None:\n        tensor.grad = _detail_torch_tensor(worker, grad_chain)\n\n    initialize_tensor(\n        hook=syft.torch.hook, obj=tensor, owner=worker, id=tensor_id, init_args=[], init_kwargs={}\n    )\n\n    if chain is not None:\n        chain = serde._detail(worker, chain)\n        tensor.child = chain\n        tensor.is_wrapper = True\n\n    tensor.tags = serde._detail(worker, tags)\n    tensor.description = serde._detail(worker, description)\n    tensor.origin = serde._detail(worker, origin)\n    tensor.id_at_origin = serde._detail(worker, id_at_origin)\n\n    return tensor\n\n\n# Simplify/Detail Parameters\n\n\ndef _simplify_torch_parameter(worker: AbstractWorker, param: torch.nn.Parameter) -> bin:\n    """"""\n    This function converts a torch Parameter into a serialized torch Parameter\n\n    Args:\n        param (torch.nn.Parameter): an input Parameter to be serialized\n\n    Returns:\n        tuple: serialized tuple of torch Parameter. The first value is the\n        id of the Parameter and the second is the binary for the PyTorch\n        tensor data attribute and last is the requires_grad attr.\n    """"""\n\n    tensor = param.data\n    tensor_ser = serde._simplify(worker, tensor)\n\n    grad = param.grad\n\n    if grad is not None and not (hasattr(grad, ""child"") and isinstance(grad.child, PointerTensor)):\n        grad_ser = _simplify_torch_tensor(worker, grad)\n    else:\n        grad_ser = None\n\n    return (param.id, tensor_ser, param.requires_grad, grad_ser)\n\n\ndef _detail_torch_parameter(worker: AbstractWorker, param_tuple: tuple) -> torch.nn.Parameter:\n    """"""\n    This function converts a serialized torch Parameter into a torch Parameter.\n\n    Args:\n        param_tuple (tuple): serialized obj of torch tensor. It\'s a tuple where\n            the first value is the ID and the second value is the binary for the\n            PyTorch data attribute et and third value is the requires_grad attr.\n\n    Returns:\n        torch.Parameter: a torch Parameter that was serialized\n    """"""\n    param_id, tensor_ser, requires_grad, grad_ser = param_tuple\n\n    tensor = serde._detail(worker, tensor_ser)\n\n    if grad_ser is not None:\n        grad = _detail_torch_tensor(worker, grad_ser)\n        grad.garbage_collect_data = False\n    elif hasattr(tensor, ""child"") and isinstance(tensor.child, PointerTensor):\n        grad = tensor.attr(""grad"")\n    else:\n        grad = None\n\n    param = torch.nn.Parameter(tensor, requires_grad)\n    param.id = param_id\n    param.grad = grad\n    param.is_wrapper = isinstance(tensor, AbstractTensor) or tensor.is_wrapper\n\n    # Note: should be\n    #  param.origin = tensor.origin\n    #  param.id_at_origin = tensor.id_at_origin\n    # but the wrapper is lost at serialisation because of the way we hook parameter.data\n    # TODO: fix serialisation of parameters (check in particular .child & .data) See #3214\n    # Below is just a fix:\n    param.origin = tensor.origin if hasattr(tensor, ""origin"") else None\n    param.id_at_origin = tensor.id_at_origin if hasattr(tensor, ""id_at_origin"") else None\n\n    return param\n\n\ndef _simplify_torch_device(worker: AbstractWorker, device: torch.device) -> Tuple:\n    device_type = serde._simplify(worker, device.type)\n    return (device_type,)\n\n\ndef _detail_torch_device(worker: AbstractWorker, device_type: tuple) -> torch.device:\n    return torch.device(type=serde._detail(worker, device_type[0]))\n\n\ndef _simplify_script_module(worker: AbstractWorker, obj: torch.jit.ScriptModule) -> Tuple:\n    """"""Strategy to serialize a script module using Torch.jit""""""\n    return (obj.save_to_buffer(),)\n\n\ndef _detail_script_module(\n    worker: AbstractWorker, script_module_bin: Tuple\n) -> torch.jit.ScriptModule:\n    """"""Strategy to deserialize a binary input using Torch load""""""\n    script_module_stream = io.BytesIO(script_module_bin[0])\n    loaded_module = torch.jit.load(script_module_stream)\n    return loaded_module\n\n\ndef _simplify_torch_size(worker: AbstractWorker, size: torch.Size) -> Tuple[int]:\n    return tuple(size)\n\n\ndef _detail_torch_size(worker: AbstractWorker, size: Tuple[int]) -> torch.Size:\n    return torch.Size(size)\n\n\ndef _simplify_torch_mem_format(worker: AbstractWorker, mem_format: torch.memory_format) -> int:\n    return TORCH_MFORMAT_ID[mem_format]\n\n\ndef _detail_torch_mem_format(worker: AbstractWorker, mem_format: int) -> torch.memory_format:\n    return TORCH_ID_MFORMAT[mem_format]\n\n\ndef _simplify_torch_dtype(worker: AbstractWorker, dtype: torch.dtype) -> Tuple[int]:\n    return TORCH_DTYPE_STR[dtype]\n\n\ndef _detail_torch_dtype(worker: AbstractWorker, dtype: str) -> torch.dtype:\n    if not isinstance(dtype, str):\n        dtype = str(dtype, ""utf-8"")\n    return TORCH_STR_DTYPE[dtype]\n\n\n# Maps a type to a tuple containing its simplifier and detailer function\n# IMPORTANT: serialization constants for these objects need to be defined\n# in `proto.json` file of https://github.com/OpenMined/proto\nMAP_TORCH_SIMPLIFIERS_AND_DETAILERS = OrderedDict(\n    {\n        torch.device: (_simplify_torch_device, _detail_torch_device),\n        torch.jit.ScriptModule: (_simplify_script_module, _detail_script_module),\n        torch.jit.ScriptFunction: (_simplify_script_module, _detail_script_module),\n        torch.jit.TopLevelTracedModule: (_simplify_script_module, _detail_script_module),\n        torch.nn.Parameter: (_simplify_torch_parameter, _detail_torch_parameter),\n        torch.Tensor: (_simplify_torch_tensor, _detail_torch_tensor),\n        torch.Size: (_simplify_torch_size, _detail_torch_size),\n        torch.memory_format: (_simplify_torch_mem_format, _detail_torch_mem_format),\n        torch.dtype: (_simplify_torch_dtype, _detail_torch_dtype),\n    }\n)\n'"
syft/serde/protobuf/__init__.py,0,b'from syft.serde.protobuf import proto  # noqa : F401\nfrom syft.serde.protobuf import serde  # noqa : F401\nfrom syft.serde.protobuf import native_serde  # noqa : F401\nfrom syft.serde.protobuf import torch_serde  # noqa : F401\n\nfrom syft.serde.protobuf.serde import serialize  # noqa : F401\nfrom syft.serde.protobuf.serde import deserialize  # noqa : F401\n'
syft/serde/protobuf/native_serde.py,1,"b'""""""\nThis file exists to provide a common place for all Protobuf\nserialisation for native Python objects. If you\'re adding\nsomething here that isn\'t for `None`, think twice and either\nuse an existing sub-class of Message or add a new one.\n""""""\nimport pydoc\nimport warnings\n\nfrom google.protobuf.empty_pb2 import Empty\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft_proto.execution.v1.type_wrapper_pb2 import InputTypeDescriptor as InputTypeDescriptorPB\n\n\nclass NoneProtoWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes None using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, obj: type(None)) -> Empty:\n        """"""\n        This method converts None into an empty Protobuf message.\n\n        Args:\n            obj (None): makes signature match other bufferize methods\n\n        Returns:\n            protobuf_obj: Empty Protobuf message\n        """"""\n        return Empty()\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, obj: Empty) -> type(None):\n        """"""\n        This method converts an empty Protobuf message back into None.\n\n        Args:\n            obj (Empty): Empty Protobuf message\n\n        Returns:\n            obj: None\n        """"""\n        return None\n\n    @staticmethod\n    def get_protobuf_schema() -> Empty:\n        """"""\n        Method that returns the protobuf schema for the current wrapped type.\n        """"""\n        return Empty\n\n    @staticmethod\n    def get_original_class() -> type(None):\n        """"""\n        Method that returns the type wrapped by the current class.\n        """"""\n        return type(None)\n\n\nclass TypeProtoWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes the type class.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, obj) -> InputTypeDescriptorPB:\n        """"""\n        This method gets the type object and returns the ClassType Protobuf message containing\n        the string with the path of that that and the actual type..\n\n        Args:\n            obj_type (s.g builtins.str, builtins.int, torch.tensor): a type\n\n        Returns:\n            ClassTypePB: the Protobuf message type containg the path where to find the type + type.\n\n        Examples:\n              str_type_representation = _bufferize_type(worker, type(""i\'m a string""))\n        """"""\n\n        proto_type = InputTypeDescriptorPB()\n\n        if isinstance(obj, type):\n            module_path = obj.__module__\n            full_path_type = module_path + ""."" + obj.__name__\n            proto_type.type_name = full_path_type\n\n        return proto_type\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, class_type_msg: InputTypeDescriptorPB):\n        """"""\n        This method receives the ClassType Protobuf message containing the string with the\n        path + type, decodes the string and locates the type in a module, returning the type object.\n\n        Args:\n            class_type_msg: message encoding the type.\n\n        Returns:\n            type: the type of an object (e.g: builtins.str, builtins.int).\n\n        Warning: if pydoc can\'t locate the type in the current process, might mean that the file\n        layout is different between sender and receiver.\n\n        TODO:\n            As syft-protobuf grows in type support, we should change the type serialization\n            by using those types, enabling cross language typechecking/type validation.\n        """"""\n        result = pydoc.locate(class_type_msg.type_name)\n        if result is None:\n            warnings.warn(\n                f""{class_type_msg.type_name} can\'t be located in the current process, ""\n                ""the layout of the modules has been changed."",\n                Warning,\n            )\n            return object\n        return result\n\n    @staticmethod\n    def get_protobuf_schema() -> InputTypeDescriptorPB:\n        """"""\n        This method returns the protobuf schema for the current wrapped type.\n        """"""\n        return InputTypeDescriptorPB\n\n    @staticmethod\n    def get_original_class() -> type:\n        """"""\n        This method returns the type wrapped by the current class.\n        """"""\n        return type\n'"
syft/serde/protobuf/proto.py,0,"b'""""""\nThis file exists to translate python classes to and from Protobuf messages.\nThe reason for this is to have stable serialization protocol that can be used\nnot only by PySyft but also in other languages.\n\nhttps://github.com/OpenMined/syft-proto (`syft_proto` module) is included as\na dependency in setup.py.\n""""""\n\n\ndef set_protobuf_id(field, id):\n    if isinstance(id, str):\n        field.id_str = id\n    else:\n        field.id_int = id\n\n\ndef get_protobuf_id(field):\n    return getattr(field, field.WhichOneof(""id""))\n'"
syft/serde/protobuf/serde.py,0,"b'from collections import OrderedDict\n\nimport inspect\nimport re\nfrom dataclasses import dataclass\n\nimport syft\nfrom syft.messaging.message import ObjectMessage\nfrom syft.messaging.message import TensorCommandMessage\nfrom syft.serde import compression\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft_proto.messaging.v1.message_pb2 import SyftMessage as SyftMessagePB\nfrom syft_proto.types.syft.v1.arg_pb2 import Arg as ArgPB\nfrom syft_proto.types.syft.v1.arg_pb2 import ArgList as ArgListPB\nfrom syft.serde.syft_serializable import (\n    SyftSerializable,\n    get_protobuf_classes,\n    get_protobuf_wrappers,\n)\n\n\nclass MetaProtobufGlobalState(type):\n    """"""\n    Metaclass that wraps all properties in ProtobufGlobalState to be updated\n    when the global state is marked as stale.\n    """"""\n\n    @staticmethod\n    def wrapper(wrapped_func: property) -> property:\n        """"""\n        Method to generate the new property.\n\n        Args:\n            wrapped_func (Property): property of the generated type.\n\n        Returns:\n             Property: new property that is wrapped to get updated when the global state\n             is marked as stale.\n        """"""\n\n        @property\n        def wrapper(self):\n            """"""\n            Generated new property that forces updates if the global state is marked as stale.\n            """"""\n            self = self.update()\n            return wrapped_func.__get__(self, type(self))\n\n        return wrapper\n\n    def __new__(meta, classname, bases, class_dict):\n        """"""\n        Method to generate the new type, wrapping all properties in the given type.\n        """"""\n        for attr_name, attr_body in class_dict.items():\n            if isinstance(attr_body, property):\n                class_dict[attr_name] = MetaProtobufGlobalState.wrapper(attr_body)\n        return type.__new__(meta, classname, bases, class_dict)\n\n\n@dataclass\nclass ProtobufGlobalState(metaclass=MetaProtobufGlobalState):\n    """"""\n        Class to generate a global state of the protobufers in a lazy way. All attributes\n        should be used by their properties, not by their hidden value.\n\n        The global state can be marked as stale by setting stale_state to False, forcing\n        the next usage of the to be updated, enabling dynamic types in serde.\n\n        All types should be enrolled in proto.json in syft-serde (soon to be deprecated,\n        when msgpack is removed).\n\n        Attributes:\n\n            _OBJ_FORCE_FULL_PROTOBUF_TRANSLATORS (list): If a type implements its own\n            force_bufferize and force_unbufferize functions, it should be stored in this list.\n            This will become deprecated soon.\n\n            _bufferizers (OrderedDict): The mapping from a type to its own bufferizer.\n\n            _forced_full_bufferizers (OrderedDict): The mapping from a type to its own forced\n            bufferizer.\n\n            _unbufferizers (OrderedDict): The mapping from a type to its own unbufferizer.\n\n            _no_bufferizers_found (set): In this set we store the primitives that we cannot\n            bufferize anymore.\n\n            _no_full_bufferizers_found (set): In this set we store the primitives that we cannot\n            force bufferize anymore.\n\n            _inherited_bufferizers_found (OrderedDict): In this dict we store the any inherited\n            bufferizer that a type can use. This might become deprecated\n\n            stale_state (Bool): Marks the global state to be stale or not.\n    """"""\n\n    _OBJ_FORCE_FULL_PROTOBUF_TRANSLATORS = []\n    _bufferizers = OrderedDict()\n    _forced_full_bufferizers = OrderedDict()\n    _unbufferizers = OrderedDict()\n    _no_bufferizers_found = set()\n    _no_full_bufferizers_found = set()\n    _inherited_bufferizers_found = OrderedDict()\n\n    stale_state = True\n\n    @property\n    def obj_force_full_protobuf_translators(self):\n        return self._OBJ_FORCE_FULL_PROTOBUF_TRANSLATORS\n\n    @property\n    def forced_full_bufferizers(self):\n        return self._forced_full_bufferizers\n\n    @property\n    def bufferizers(self):\n        return self._bufferizers\n\n    @property\n    def unbufferizers(self):\n        return self._unbufferizers\n\n    @property\n    def no_bufferizers_found(self):\n        return self._no_bufferizers_found\n\n    @property\n    def no_full_bufferizers_found(self):\n        return self._no_full_bufferizers_found\n\n    @property\n    def inherited_bufferizers_found(self):\n        return self._inherited_bufferizers_found\n\n    def update(self):\n        """"""\n            Updates the global state of protobuf.\n        """"""\n        if not self.stale_state:\n            return self\n\n        obj_protobuf_translators = list(get_protobuf_classes(SyftSerializable))\n        obj_protobuf_wrappers = list(get_protobuf_wrappers(SyftSerializable))\n\n        def _add_bufferizer_and_unbufferizer(\n            curr_type, proto_type, bufferizer, unbufferizer, forced=False\n        ):\n\n            if forced:\n                self._forced_full_bufferizers[curr_type] = bufferizer\n                self._unbufferizers[proto_type] = unbufferizer\n            else:\n                self._bufferizers[curr_type] = bufferizer\n                self._unbufferizers[proto_type] = unbufferizer\n\n        for curr_type in obj_protobuf_translators:\n            _add_bufferizer_and_unbufferizer(\n                curr_type,\n                curr_type.get_protobuf_schema(),\n                curr_type.bufferize,\n                curr_type.unbufferize,\n            )\n\n        for curr_type in obj_protobuf_wrappers:\n            _add_bufferizer_and_unbufferizer(\n                curr_type.get_original_class(),\n                curr_type.get_protobuf_schema(),\n                curr_type.bufferize,\n                curr_type.unbufferize,\n            )\n\n        for syft_type in self._OBJ_FORCE_FULL_PROTOBUF_TRANSLATORS:\n            proto_type = syft_type.get_protobuf_schema()\n            force_bufferizer, force_unbufferizer = (\n                syft_type.force_bufferize,\n                syft_type.force_unbufferize,\n            )\n            _add_bufferizer_and_unbufferizer(\n                syft_type, proto_type, force_bufferizer, force_unbufferizer, forced=True\n            )\n\n        self.stale_state = False\n        return self\n\n\ndef _bufferize(worker: AbstractWorker, obj: object, **kwargs) -> object:\n    """"""\n    This function takes an object as input and returns a\n    Protobuf object. The reason we have this function\n    is that some objects are either NOT supported by high level (fast)\n    serializers OR the high level serializers don\'t support the fastest\n    form of serialization. For example, PyTorch tensors have custom pickle\n    functionality thus its better to pre-serialize PyTorch tensors using\n    pickle and then serialize the binary in with the rest of the message\n    being sent.\n\n    Args:\n        obj: An object which needs to be converted to Protobuf.\n\n    Returns:\n        An Protobuf object which Protobuf can serialize.\n    """"""\n\n    # Check to see if there is a bufferizer\n    # for this type. If there is, return the bufferized object.\n\n    current_type = type(obj)\n    if current_type in protobuf_global_state.bufferizers:\n        result = protobuf_global_state.bufferizers[current_type](worker, obj, **kwargs)\n        return result\n    elif current_type in protobuf_global_state.inherited_bufferizers_found:\n        return (\n            protobuf_global_state.inherited_bufferizers_found[current_type][0],\n            protobuf_global_state.inherited_bufferizers_found[current_type][1](\n                worker, obj, **kwargs\n            ),\n        )\n    # If we already tried to find a bufferizer for this type but failed, we should\n    # just return the object as it is.\n    elif current_type in protobuf_global_state.no_bufferizers_found:\n        raise Exception(f""No corresponding Protobuf message found for {current_type}"")\n    else:\n\n        protobuf_global_state.stale_state = True\n        if current_type in protobuf_global_state.bufferizers:\n            result = protobuf_global_state.bufferizers[current_type](worker, obj, **kwargs)\n            return result\n\n        # If the object type is not in bufferizers,\n        # we check the classes that this object inherits from.\n        # `inspect.getmro` give us all types this object inherits\n        # from, including `type(obj)`. We can skip the type of the\n        # object because we already tried this in the\n        # previous step.\n        classes_inheritance = inspect.getmro(type(obj))[1:]\n\n        for inheritance_type in classes_inheritance:\n            if inheritance_type in protobuf_global_state.bufferizers:\n                # Store the inheritance_type in bufferizers so next time we see this type\n                # serde will be faster.\n                protobuf_global_state.inherited_bufferizers_found[\n                    current_type\n                ] = protobuf_global_state.bufferizers[inheritance_type]\n                result = protobuf_global_state.inherited_bufferizers_found[current_type](\n                    worker, obj, **kwargs\n                )\n                return result\n\n        protobuf_global_state.no_bufferizers_found.add(current_type)\n        raise Exception(f""No corresponding Protobuf message found for {current_type}"")\n\n\n## SECTION: High Level Translation Router\ndef _force_full_bufferize(worker: AbstractWorker, obj: object) -> object:\n    """"""To force a full bufferize conversion generally if the usual _bufferize is not suitable.\n\n    If we can not full convert an object we convert it as usual instead.\n\n    Args:\n        obj: The object.\n\n    Returns:\n        The bufferize object.\n    """"""\n    # check to see if there is a full bufferize converter\n    # for this type. If there is, return the full converted object.\n    current_type = type(obj)\n    if current_type in protobuf_global_state.forced_full_bufferizers:\n        result = (\n            protobuf_global_state.forced_full_bufferizers[current_type][0],\n            protobuf_global_state.forced_full_bufferizers[current_type][1](worker, obj),\n        )\n        return result\n    # If we already tried to find a full bufferizer for this type but failed, we should\n    # bufferize it instead.\n    elif current_type in protobuf_global_state.no_full_bufferizers_found:\n        return _bufferize(worker, obj)\n    else:\n        # If the object type is not in forced_full_bufferizers,\n        # we check the classes that this object inherits from.\n        # `inspect.getmro` give us all types this object inherits\n        # from, including `type(obj)`. We can skip the type of the\n        # object because we already tried this in the\n        # previous step.\n        classes_inheritance = inspect.getmro(type(obj))[1:]\n\n        for inheritance_type in classes_inheritance:\n            if inheritance_type in protobuf_global_state.forced_full_bufferizers:\n                # Store the inheritance_type in forced_full_bufferizers so next\n                # time we see this type serde will be faster.\n                protobuf_global_state.forced_full_bufferizers[\n                    current_type\n                ] = protobuf_global_state.forced_full_bufferizers[inheritance_type]\n                result = (\n                    protobuf_global_state.forced_full_bufferizers[current_type][0],\n                    protobuf_global_state.forced_full_bufferizers[current_type][1](worker, obj),\n                )\n                return result\n\n        # If there is not a full_bufferizer for this\n        # object, then we bufferize it.\n        protobuf_global_state.no_full_bufferizers_found.add(current_type)\n        return _bufferize(worker, obj)\n\n\ndef serialize(\n    obj: object,\n    worker: AbstractWorker = None,\n    simplified: bool = False,\n    force_no_compression: bool = False,\n    force_no_serialization: bool = False,\n    force_full_simplification: bool = False,\n) -> bin:\n    """"""This method can serialize any object PySyft needs to send or store.\n\n    This is the high level function for serializing any object or collection\n    of objects which PySyft needs to send over the wire. It includes three\n    steps, Simplify, Serialize, and Compress as described inline below.\n\n    Args:\n        obj (object): the object to be serialized\n        simplified (bool): in some cases we want to pass in data which has\n            already been simplified - in which case we must skip double\n            simplification - which would be bad.... so bad... so... so bad\n        force_no_compression (bool): If true, this will override ANY module\n            settings and not compress the objects being serialized. The primary\n            expected use of this functionality is testing and/or experimentation.\n        force_no_serialization (bool): Primarily a testing tool, this will force\n            this method to return human-readable Python objects which is very useful\n            for testing and debugging (forceably overrides module compression,\n            serialization, and the \'force_no_compression\' override)). In other words,\n            only simplification operations are performed.\n        force_full_simplification (bool): Some objects are only partially serialized\n            by default. For objects where this is the case, setting this flag to True\n            will force the entire object to be serialized. For example, setting this\n            flag to True will cause a VirtualWorker to be serialized WITH all of its\n            tensors while by default VirtualWorker objects only serialize a small\n            amount of metadata.\n\n    Returns:\n        binary: the serialized form of the object.\n    """"""\n\n    if worker is None:\n        # TODO[jvmancuso]: This might be worth a standalone function.\n        worker = syft.framework.hook.local_worker\n\n    if force_no_serialization:\n        # 0) Simplify\n        # bufferize difficult-to-serialize objects. See the _bufferize method\n        # for unbufferizes on how this works. The general purpose is to handle types\n        # which the fast serializer cannot handle\n        simple_objects = obj\n        if not simplified:\n            if force_full_simplification:\n                simple_objects = _force_full_bufferize(worker, obj)\n            else:\n                simple_objects = _bufferize(worker, obj)\n        return simple_objects\n\n    # 1) Convert to Protobuf objects\n    msg_wrapper = SyftMessagePB()\n\n    protobuf_obj = _bufferize(worker, obj)\n\n    obj_type = type(obj)\n    if isinstance(obj_type, None):\n        msg_wrapper.contents_empty_msg.CopyFrom(protobuf_obj)\n    elif obj_type == ObjectMessage:\n        msg_wrapper.contents_object_msg.CopyFrom(protobuf_obj)\n    elif obj_type == TensorCommandMessage:\n        msg_wrapper.contents_action_msg.CopyFrom(protobuf_obj)\n\n    # 2) Serialize\n    # serialize into a binary\n    binary = msg_wrapper.SerializeToString()\n\n    # 3) Compress\n    # optionally compress the binary and return the result\n    # prepend a 1-byte header \'0\' or \'1\' to the output stream\n    # to denote whether output stream is compressed or not\n    # if compressed stream length is greater than input stream\n    # we output the input stream as it is with header set to \'0\'\n    # otherwise we output the compressed stream with header set to \'1\'\n    # even if compressed flag is set to false by the caller we\n    # output the input stream as it is with header set to \'0\'\n    if force_no_compression:\n        return binary\n    else:\n        return compression._compress(binary)\n\n\ndef deserialize(binary: bin, worker: AbstractWorker = None, unbufferizes=True) -> object:\n    """""" This method can deserialize any object PySyft needs to send or store.\n\n    This is the high level function for deserializing any object or collection\n    of objects which PySyft has sent over the wire or stored. It includes three\n    steps, Decompress, Deserialize, and Detail as described inline below.\n\n    Args:\n        binary (bin): the serialized object to be deserialized.\n        worker (AbstractWorker): the worker which is acquiring the message content,\n            for example used to specify the owner of a tensor received(not obvious\n            for virtual workers)\n        unbufferizes (bool): there are some cases where we need to perform the decompression\n            and deserialization part, but we don\'t need to unbufferize all the message.\n            This is the case for Plan workers for instance\n\n    Returns:\n        object: the deserialized form of the binary input.\n    """"""\n\n    if worker is None:\n        # TODO[jvmancuso]: This might be worth a standalone function.\n        worker = syft.framework.hook.local_worker\n\n    # 1) Decompress the binary if needed\n    binary = compression._decompress(binary)\n\n    # 2) Deserialize\n    msg_wrapper = SyftMessagePB()\n    msg_wrapper.ParseFromString(binary)\n\n    # 3) Convert back to a Python object\n    message_type = msg_wrapper.WhichOneof(""contents"")\n    python_obj = _unbufferize(worker, getattr(msg_wrapper, message_type))\n    return python_obj\n\n\ndef _unbufferize(worker: AbstractWorker, obj: object, **kwargs) -> object:\n    """"""Reverses the functionality of _bufferize.\n    Where applicable, it converts simple objects into more complex objects such\n    as converting binary objects into torch tensors. Read _bufferize for more\n    information on why _bufferize and unbufferize are needed.\n\n    Args:\n        worker: the worker which is acquiring the message content, for example\n        used to specify the owner of a tensor received(not obvious for\n        virtual workers).\n        obj: a simple Python object which msgpack deserialized.\n\n    Returns:\n        obj: a more complex Python object which msgpack would have had trouble\n            deserializing directly.\n    """"""\n    current_type = type(obj)\n    if current_type in protobuf_global_state.unbufferizers:\n        return protobuf_global_state.unbufferizers[current_type](worker, obj, **kwargs)\n    else:\n        protobuf_global_state.stale_state = True\n        if current_type in protobuf_global_state.unbufferizers:\n            result = protobuf_global_state.bufferizers[current_type](worker, obj, **kwargs)\n            return result\n\n        raise Exception(f""No unbufferizer found for {current_type}"")\n\n\ndef bufferize_args(worker: AbstractWorker, args_: list) -> list:\n    return [bufferize_arg(worker, arg) for arg in args_]\n\n\ndef bufferize_arg(worker: AbstractWorker, arg: object) -> ArgPB:\n    protobuf_arg = ArgPB()\n\n    if isinstance(arg, list):\n        protobuf_arg_list = ArgListPB()\n        arg_list = [bufferize_arg(worker, i) for i in arg]\n        protobuf_arg_list.args.extend(arg_list)\n        protobuf_arg.arg_list.CopyFrom(protobuf_arg_list)\n\n    else:\n        attr_name = ""arg_"" + _camel2snake(type(arg).__name__)\n\n        try:\n            setattr(protobuf_arg, attr_name, arg)\n        except:\n            getattr(protobuf_arg, attr_name).CopyFrom(_bufferize(worker, arg))\n\n    return protobuf_arg\n\n\ndef unbufferize_args(worker: AbstractWorker, protobuf_args: list) -> list:\n    return tuple((unbufferize_arg(worker, arg) for arg in protobuf_args))\n\n\ndef unbufferize_arg(worker: AbstractWorker, protobuf_arg: ArgPB) -> object:\n    protobuf_field_name = protobuf_arg.WhichOneof(""arg"")\n\n    protobuf_arg_field = getattr(protobuf_arg, protobuf_field_name)\n\n    if protobuf_field_name == ""arg_list"":\n        arg = [unbufferize_arg(worker, i) for i in protobuf_arg_field.args]\n    else:\n        try:\n            arg = _unbufferize(worker, protobuf_arg_field)\n        except:\n            arg = protobuf_arg_field\n\n    return arg\n\n\ndef _camel2snake(string: str):\n    return string[0].lower() + re.sub(r""(?!^)[A-Z]"", lambda x: ""_"" + x.group(0).lower(), string[1:])\n\n\nprotobuf_global_state = ProtobufGlobalState()\n'"
syft/serde/protobuf/torch_serde.py,135,"b'""""""\nThis file exists to provide one common place for all serialisation and bufferize_ and _unbufferize\nfor all tensors (Torch and Numpy).\n""""""\nimport io\n\nimport torch\nimport pydoc\n\nimport syft\nfrom syft.serde.syft_serializable import SyftSerializable\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.generic.abstract.tensor import initialize_tensor\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.codes import TENSOR_SERIALIZATION\n\nfrom syft.serde.protobuf.proto import get_protobuf_id\nfrom syft.serde.protobuf.proto import set_protobuf_id\nfrom syft.serde.torch.serde import TORCH_DTYPE_STR\nfrom syft.serde.torch.serde import TORCH_STR_DTYPE\nfrom syft.serde.torch.serde import torch_tensor_serializer\nfrom syft.serde.torch.serde import torch_tensor_deserializer\nfrom syft.serde.torch.serde import numpy_tensor_serializer\nfrom syft.serde.torch.serde import numpy_tensor_deserializer\n\nfrom syft_proto.types.torch.v1.script_function_pb2 import ScriptFunction as ScriptFunctionPB\nfrom syft_proto.types.torch.v1.device_pb2 import Device as DevicePB\nfrom syft_proto.types.torch.v1.parameter_pb2 import Parameter as ParameterPB\nfrom syft_proto.types.torch.v1.script_module_pb2 import ScriptModule as ScriptModulePB\nfrom syft_proto.types.torch.v1.size_pb2 import Size as SizePB\nfrom syft_proto.types.torch.v1.tensor_data_pb2 import TensorData as TensorDataPB\nfrom syft_proto.types.torch.v1.tensor_pb2 import TorchTensor as TorchTensorPB\nfrom syft_proto.types.torch.v1.traced_module_pb2 import TracedModule as TracedModulePB\nfrom syft_proto.types.torch.v1.memory_format_pb2 import MemoryFormat as MemoryFormatPB\nfrom syft_proto.types.torch.v1.dtype_pb2 import TorchDType as TorchDTypePB\n\nSERIALIZERS_SYFT_TO_PROTOBUF = {\n    TENSOR_SERIALIZATION.TORCH: TorchTensorPB.Serializer.SERIALIZER_TORCH,\n    TENSOR_SERIALIZATION.NUMPY: TorchTensorPB.Serializer.SERIALIZER_NUMPY,\n    TENSOR_SERIALIZATION.ALL: TorchTensorPB.Serializer.SERIALIZER_ALL,\n}\nSERIALIZERS_PROTOBUF_TO_SYFT = {value: key for key, value in SERIALIZERS_SYFT_TO_PROTOBUF.items()}\n\n\ndef _serialize_tensor(worker: AbstractWorker, tensor) -> bin:\n    """"""Serialize the tensor using as default Torch serialization strategy\n    This function can be overridden to provide different tensor serialization strategies\n\n    Args\n        (torch.Tensor): an input tensor to be serialized\n\n    Returns\n        A serialized version of the input tensor\n\n    """"""\n    serializers = {\n        TENSOR_SERIALIZATION.TORCH: torch_tensor_serializer,\n        TENSOR_SERIALIZATION.NUMPY: numpy_tensor_serializer,\n        TENSOR_SERIALIZATION.ALL: protobuf_tensor_serializer,\n    }\n    if worker.serializer not in serializers:\n        raise NotImplementedError(\n            f""Tensor serialization strategy is not supported: {worker.serializer}""\n        )\n    serializer = serializers[worker.serializer]\n    return serializer(worker, tensor)\n\n\ndef _deserialize_tensor(worker: AbstractWorker, serializer: str, tensor_bin) -> torch.Tensor:\n    """"""Deserialize the input tensor passed as parameter into a Torch tensor.\n    `serializer` parameter selects different deserialization strategies\n\n    Args\n        worker: Worker\n        serializer: Strategy used for tensor deserialization (e.g.: torch, numpy, all)\n        tensor_bin: A simplified representation of a tensor\n\n    Returns\n        a Torch tensor\n    """"""\n    deserializers = {\n        TENSOR_SERIALIZATION.TORCH: torch_tensor_deserializer,\n        TENSOR_SERIALIZATION.NUMPY: numpy_tensor_deserializer,\n        TENSOR_SERIALIZATION.ALL: protobuf_tensor_deserializer,\n    }\n    if serializer not in deserializers:\n        raise NotImplementedError(\n            f""Cannot deserialize tensor serialized with \'{serializer}\' strategy""\n        )\n    deserializer = deserializers[serializer]\n    return deserializer(worker, tensor_bin)\n\n\ndef protobuf_tensor_serializer(worker: AbstractWorker, tensor: torch.Tensor) -> TensorDataPB:\n    """"""Strategy to serialize a tensor using Protobuf""""""\n    dtype = TORCH_DTYPE_STR[tensor.dtype]\n\n    protobuf_tensor = TensorDataPB()\n\n    if tensor.is_quantized:\n        protobuf_tensor.is_quantized = True\n        protobuf_tensor.scale = tensor.q_scale()\n        protobuf_tensor.zero_point = tensor.q_zero_point()\n        data = torch.flatten(tensor).int_repr().tolist()\n    else:\n        data = torch.flatten(tensor).tolist()\n\n    protobuf_tensor.dtype = dtype\n    protobuf_tensor.shape.dims.extend(tensor.size())\n    getattr(protobuf_tensor, ""contents_"" + dtype).extend(data)\n\n    return protobuf_tensor\n\n\ndef protobuf_tensor_deserializer(\n    worker: AbstractWorker, protobuf_tensor: TensorDataPB\n) -> torch.Tensor:\n    """"""Strategy to deserialize a binary input using Protobuf""""""\n    size = tuple(protobuf_tensor.shape.dims)\n    data = getattr(protobuf_tensor, ""contents_"" + protobuf_tensor.dtype)\n\n    if protobuf_tensor.is_quantized:\n        # Drop the \'q\' from the beginning of the quantized dtype to get the int type\n        dtype = TORCH_STR_DTYPE[protobuf_tensor.dtype[1:]]\n        int_tensor = torch.tensor(data, dtype=dtype).reshape(size)\n        # Automatically converts int types to quantized types\n        return torch._make_per_tensor_quantized_tensor(\n            int_tensor, protobuf_tensor.scale, protobuf_tensor.zero_point\n        )\n    else:\n        dtype = TORCH_STR_DTYPE[protobuf_tensor.dtype]\n        return torch.tensor(data, dtype=dtype).reshape(size)\n\n\nclass TorchTensorWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.Tensor using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, tensor: torch.Tensor) -> bin:\n        """"""\n        This method converts a Torch tensor into a serialized tensor\n        using Protobuf. Depending on the worker\'s serializer, the tensor\n        contents may be serialized to binary representations using Torch\n        or Numpy, or to a generic inner Protobuf message for cross-platform\n        communication.\n\n        Args:\n            tensor (torch.Tensor): an input tensor to be serialized\n\n        Returns:\n            protobuf_obj: Protobuf version of torch tensor.\n        """"""\n        serialized_tensor = _serialize_tensor(worker, tensor)\n\n        if tensor.grad is not None:\n            if hasattr(tensor, ""child""):\n                if isinstance(tensor.child, PointerTensor):\n                    grad_chain = None\n                else:\n                    grad_chain = TorchTensorWrapper.bufferize(worker, tensor.grad)\n            else:\n                grad_chain = TorchTensorWrapper.bufferize(worker, tensor.grad)\n\n        else:\n            grad_chain = None\n\n        chain = None\n        if hasattr(tensor, ""child""):\n            chain = syft.serde.protobuf.serde._bufferize(worker, tensor.child)\n\n        protobuf_tensor = TorchTensorPB()\n        set_protobuf_id(protobuf_tensor.id, tensor.id)\n\n        protobuf_tensor.serializer = SERIALIZERS_SYFT_TO_PROTOBUF[worker.serializer]\n        if worker.serializer == TENSOR_SERIALIZATION.ALL:\n            protobuf_tensor.contents_data.CopyFrom(serialized_tensor)\n        else:\n            protobuf_tensor.contents_bin = serialized_tensor\n\n        if chain:\n            protobuf_tensor.chain.CopyFrom(chain)\n        if grad_chain:\n            protobuf_tensor.grad_chain.CopyFrom(grad_chain)\n        if tensor.description:\n            protobuf_tensor.description = tensor.description\n\n        protobuf_tensor.tags.extend(tensor.tags)\n\n        return protobuf_tensor\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_tensor: ""TorchTensorPB"") -> torch.Tensor:\n        """"""\n        This method converts a Protobuf torch tensor back into a\n        Torch tensor. The tensor contents can be deserialized from\n        binary representations produced by Torch or Numpy, or from\n        the generic Protobuf message format for cross-platform\n        communication.\n\n        Args:\n            protobuf_tensor (bin): Protobuf message of torch tensor.\n\n        Returns:\n            tensor (torch.Tensor): a torch tensor converted from Protobuf\n        """"""\n        tensor_id = get_protobuf_id(protobuf_tensor.id)\n        tags = protobuf_tensor.tags\n        description = protobuf_tensor.description\n\n        contents_type = protobuf_tensor.WhichOneof(""contents"")\n        serialized_tensor = getattr(protobuf_tensor, contents_type)\n        serializer = SERIALIZERS_PROTOBUF_TO_SYFT[protobuf_tensor.serializer]\n\n        tensor = _deserialize_tensor(worker, (serializer), serialized_tensor)\n\n        # note we need to do this explicitly because torch.load does not\n        # include .grad information\n        if protobuf_tensor.HasField(""grad_chain""):\n            grad_chain = protobuf_tensor.grad_chain\n            tensor.grad = TorchTensorWrapper.unbufferize(worker, grad_chain)\n\n        initialize_tensor(\n            hook=syft.torch.hook,\n            obj=tensor,\n            owner=worker,\n            id=tensor_id,\n            init_args=[],\n            init_kwargs={},\n        )\n\n        if protobuf_tensor.HasField(""chain""):\n            chain = protobuf_tensor.chain\n            chain = TorchTensorWrapper.unbufferize(worker, chain)\n            tensor.child = chain\n            tensor.is_wrapper = True\n\n        tensor.tags = set(tags)\n        tensor.description = description\n\n        return tensor\n\n    @staticmethod\n    def get_original_class() -> type(torch.Tensor):\n        """"""\n        This method returns the wrapped type.\n\n        Returns:\n            torch.Tensor: wrapped type.\n        """"""\n        return torch.Tensor\n\n    @staticmethod\n    def get_protobuf_schema() -> type(TorchTensorPB):\n        """"""\n        This method returns the protobuf schema used for torch.Tensor.\n\n        Returns:\n            protobuf schema for torch.tensor.\n        """"""\n        return TorchTensorPB\n\n\nclass TorchDeviceWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.device using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, device: torch.device) -> DevicePB:\n        """"""\n            This method converts a Torch device into a serialized device\n            using Protobuf.\n\n            Args:\n                device (torch.device): an input device to be serialized\n\n            Returns:\n                protobuf_device (DevicePB): Protobuf version of torch device.\n        """"""\n        protobuf_device = DevicePB()\n        protobuf_device.type = device.type\n        return protobuf_device\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_device: DevicePB) -> torch.device:\n        """"""\n            This method converts a serialized device into a Torch device using the protobuf schema.\n\n            Args:\n                device (DevicePB): serialized input device.\n\n            Returns:\n                torch.device: torch Device.\n        """"""\n        device_type = protobuf_device.type\n        return torch.device(type=device_type)\n\n    @staticmethod\n    def get_original_class() -> type(torch.device):\n        """"""\n            This method returns the wrapped type.\n\n        Returns:\n            type: wrapped type.\n        """"""\n        return torch.device\n\n    @staticmethod\n    def get_protobuf_schema() -> type(DevicePB):\n        """"""\n           Returns the protobuf schema used for torch.device.\n\n           Returns:\n               type: protobuf schema for torch.device.\n        """"""\n        return DevicePB\n\n\nclass ParameterWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.nn.Parameter using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, param: torch.nn.Parameter) -> ParameterPB:\n        """"""\n            This method converts a torch.nn.Parameter into a serialized parameter using ParameterPB.\n\n            Args:\n                param (torch.nn.Parameter): input nn.parameter to be serialized.\n\n            Returns:\n                protobuf_param: serialized torch.nn.Parameter.\n        """"""\n        protobuf_param = ParameterPB()\n        set_protobuf_id(protobuf_param.id, param.id)\n        protobuf_param.tensor.CopyFrom(syft.serde.protobuf.serde._bufferize(worker, param.data))\n        protobuf_param.requires_grad = param.requires_grad\n        if param.grad:\n            protobuf_param.grad.CopyFrom(syft.serde.protobuf.serde._bufferize(worker, param.grad))\n        return protobuf_param\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_param: ParameterPB) -> torch.nn.Parameter:\n        """"""\n            This method converts a ParameterPB into a torch.nn.Parameter.\n\n            Args:\n                protobuf_param (ParameterPB): input ParameterPB to be deserialized.\n\n            Returns:\n                param: (torch.nn.Parameter): deserialized ParameterPB.\n        """"""\n        data = syft.serde.protobuf.serde._unbufferize(worker, protobuf_param.tensor)\n        param = torch.nn.Parameter(data, requires_grad=protobuf_param.requires_grad)\n        param.id = get_protobuf_id(protobuf_param.id)\n        if protobuf_param.HasField(""grad""):\n            param.grad = syft.serde.protobuf.serde._unbufferize(worker, protobuf_param.grad)\n        return param\n\n    @staticmethod\n    def get_original_class() -> type(torch.nn.Parameter):\n        """"""\n        This method returns the wrapped type.\n\n        Returns:\n            Wrapped type.\n        """"""\n        return torch.nn.Parameter\n\n    @staticmethod\n    def get_protobuf_schema() -> type(ParameterPB):\n        """"""\n           This method returns the protobuf schema used for torch.nn.Parameter.\n\n           Returns:\n               Protobuf schema for torch.nn.Parameter.\n        """"""\n        return ParameterPB\n\n\nclass ScriptModuleWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.jit.ScriptModule using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, script_module: torch.jit.ScriptModule) -> ScriptModulePB:\n        """"""\n            This method serializes a torch.jit.ScriptModule using ScriptModulePB.\n\n            Args:\n                script_module (torch.jit.ScriptModule): input jit.ScriptModule to be serialized.\n\n            Returns:\n                protobuf_script (ScriptModulePB): serialized jit.ScriptModule.\n        """"""\n        protobuf_script = ScriptModulePB()\n        protobuf_script.obj = script_module.save_to_buffer()\n        return protobuf_script\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_script: ScriptModulePB\n    ) -> torch.jit.ScriptModule:\n        """"""\n            This method deserializes a serialized script module into a torch.jit.ScriptModule.\n\n            Args:\n                protobuf_script (ScriptModulePB): input ScriptModulePB to be deserialized .\n\n            Returns:\n                loaded_module (torch.jit.ScriptModule): deserialized ScriptModulePB.\n        """"""\n        script_module_stream = io.BytesIO(protobuf_script.obj)\n        loaded_module = torch.jit.load(script_module_stream)\n        return loaded_module\n\n    @staticmethod\n    def get_protobuf_schema() -> type(ScriptModulePB):\n        """"""\n           This methods returns the protobuf schema used for torch.nn.Parameter.\n\n           Returns:\n               Protobuf schema for torch.nn.Parameter.\n        """"""\n        return ScriptModulePB\n\n    @staticmethod\n    def get_original_class() -> type(torch.jit.ScriptModule):\n        """"""\n        This metod returns the wrapped type.\n\n        Returns:\n            Wrapped type.\n        """"""\n        return torch.jit.ScriptModule\n\n\nclass ScriptFunctionWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.jit.ScriptFunction using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(\n        worker: AbstractWorker, script_module: torch.jit.ScriptFunction\n    ) -> ScriptFunctionPB:\n        """"""\n        This method serializes a torch.jit.ScriptFunction into a ScriptFunctionPB.\n\n        Args:\n            script_module (torch.jit.ScriptFunction): input torch.jit.ScriptFunction\n            to be serialized.\n\n        Returns:\n            protobuf_script (ScriptFunctionPB): serialized torch.jit.ScriptFunction.\n        """"""\n        protobuf_script = ScriptFunctionPB()\n        protobuf_script.obj = script_module.save_to_buffer()\n        return protobuf_script\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_script: ScriptFunctionPB\n    ) -> torch.jit.ScriptFunction:\n        """"""\n        This method deserializes ScriptFunctionPB into a torch.jit.ScriptFunction.\n\n        Args:\n            protobuf_script (torch.jit.ScriptFunction): input serialized ScriptFunctionPB.\n\n        Returns:\n            loaded_module (torch.jit.ScriptFunction): deserialized ScriptFunctionPB.\n        """"""\n        script_module_stream = io.BytesIO(protobuf_script.obj)\n        loaded_module = torch.jit.load(script_module_stream)\n        return loaded_module\n\n    @staticmethod\n    def get_original_class() -> type(torch.jit.ScriptFunction):\n        """"""\n        This method returns the wrapped type.\n\n        Returns:\n            Wrapped type.\n        """"""\n        return torch.jit.ScriptFunction\n\n    @staticmethod\n    def get_protobuf_schema() -> type(ScriptFunctionPB):\n        """"""\n        This method returns the protobuf schema used for torch.jit.ScriptFunction.\n\n        Returns:\n           Protobuf schema for torch.jit.ScriptFunction.\n       """"""\n        return ScriptFunctionPB\n\n\nclass TopLevelTracedModuleWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.jit.TopLevelTracedModule using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(\n        worker: AbstractWorker, script_module: torch.jit.TopLevelTracedModule\n    ) -> TracedModulePB:\n        """"""\n            This method serializes a torch.jit.TopLevelTracedModule using TracedModulePB.\n\n            Args:\n                script_module (torch.jit.TopLevelTracedModule): input TopLevelTracedModule\n                to be serialized.\n\n            Returns:\n                protobuf_script (TracedModulePB): serialized TopLevelTracedModule.\n        """"""\n        protobuf_script = ScriptModulePB()\n        protobuf_script.obj = script_module.save_to_buffer()\n        return protobuf_script\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_script: TracedModulePB\n    ) -> torch.jit.TopLevelTracedModule:\n        """"""\n            This method deserializes TracedModulePB into torch.jit.TopLevelTracedModule.\n\n            Args:\n                protobuf_script (TracedModulePB): input serialized TracedModulePB.\n\n            Returns:\n                loaded_module (torch.jit.TopLevelTracedModule): deserialized TracedModulePB.\n        """"""\n        script_module_stream = io.BytesIO(protobuf_script.obj)\n        loaded_module = torch.jit.load(script_module_stream)\n        return loaded_module\n\n    @staticmethod\n    def get_protobuf_schema() -> type(TracedModulePB):\n        """"""\n             This method returns the protobuf schema used for torch.jit.TopLevelTracedModule.\n\n             Returns:\n                Protobuf schema for torch.jit.TopLevelTracedModule.\n        """"""\n        return TracedModulePB\n\n    @staticmethod\n    def get_original_class() -> type(torch.jit.TopLevelTracedModule):\n        """"""\n            This method returns the wrapped type.\n\n            Returns:\n                Wrapped type.\n        """"""\n        return torch.jit.TopLevelTracedModule\n\n\nclass TorchSizeWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.Size using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, size: torch.Size) -> SizePB:\n        """"""\n            This method serializes torch.Size into SizePB.\n\n            Args:\n                size (torch.Size): input torch.Size to be serialized.\n\n            Returns:\n                protobuf_size: serialized torch.Size\n        """"""\n        protobuf_size = SizePB()\n        protobuf_size.dims.extend(size)\n        return protobuf_size\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_size: SizePB) -> torch.Size:\n        """"""\n            This method deserializes SizePB into torch.Size.\n\n            Args:\n                protobuf_size (SizePB): input SizePB to be deserialized.\n\n            Returns:\n                torch.Size: deserialized SizePB\n        """"""\n        return torch.Size(protobuf_size.dims)\n\n    @staticmethod\n    def get_original_class() -> type(torch.Size):\n        """"""\n            This method returns the wrapped type.\n\n            Returns:\n                Wrapped type.\n        """"""\n        return torch.Size\n\n    @staticmethod\n    def get_protobuf_schema() -> type(SizePB):\n        """"""\n            Returns the protobuf schema used for torch.Size.\n\n            Returns:\n                Protobuf schema for torch.Size.\n        """"""\n        return SizePB\n\n\nclass TorchMemFormatWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.memory_format.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, mem_format: torch.memory_format) -> MemoryFormatPB:\n        """"""\n        This method serializes torch.memory_format into MemoryFormatPB.\n\n         Args:\n            size (torch.memory_format): input torch.memory_format to be serialized.\n\n         Returns:\n            message (MemoryFormatPB): serialized torch.memory_format\n        """"""\n        message = MemoryFormatPB()\n        message.memory_format_type = str(mem_format).split(""."")[-1]\n        return message\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_mem_format: MemoryFormatPB\n    ) -> torch.memory_format:\n        """"""\n            This method deserializes MemoryFormatPB into torch.memory_format.\n\n            Args:\n                protobuf_size (MemoryFormatPB): input MemoryFormatPB to be deserialized.\n\n            Returns:\n                torch.memory_format: deserialized MemoryFormatPB\n        """"""\n        return getattr(torch, protobuf_mem_format.memory_format_type)\n\n    @staticmethod\n    def get_original_class() -> type(torch.memory_format):\n        return torch.memory_format\n\n    @staticmethod\n    def get_protobuf_schema() -> type(MemoryFormatPB):\n        """"""\n            Returns the protobuf schema used for torch.memory_format.\n\n            Returns:\n                Protobuf schema for torch.memory_format.\n        """"""\n        return MemoryFormatPB\n\n\nclass TorchDTypeWrapper(SyftSerializable):\n    """"""\n    Wrapper that serializes torch.dtype using protobuffers.\n    """"""\n\n    @staticmethod\n    def bufferize(worker: AbstractWorker, torch_dtype: torch.dtype) -> TorchDTypePB:\n        """"""\n            This method serializes torch.dtype into TorchDTypePB.\n\n            Args:\n                torch_dtype (torch.dtype): input torch.dtype to be serialized.\n\n            Returns:\n                protobuf_size: serialized torch.dtype\n        """"""\n        protobuf_msg = TorchDTypePB()\n        protobuf_msg.torch_type = str(torch_dtype)\n        return protobuf_msg\n\n    @staticmethod\n    def unbufferize(worker: AbstractWorker, protobuf_dtype: TorchDTypePB) -> torch.dtype:\n        """"""\n            This method deserializes TorchDTypePB into torch.dtype.\n\n            Args:\n                protobuf_dtype (TorchDTypePB): input TorchDTypePB to be deserialized.\n\n            Returns:\n                torch.Size: deserialized TorchDTypePB\n        """"""\n        return pydoc.locate(protobuf_dtype.torch_type)\n\n    @staticmethod\n    def get_original_class() -> type(torch.dtype):\n        """"""\n            This method returns the wrapped type.\n\n            Returns:\n                Wrapped type.\n        """"""\n        return torch.dtype\n\n    @staticmethod\n    def get_protobuf_schema() -> type(TorchDTypePB):\n        """"""\n            Returns the protobuf schema used for torch.dtype.\n\n            Returns:\n                Protobuf schema for torch.dtype.\n        """"""\n        return TorchDTypePB\n'"
syft/serde/torch/__init__.py,0,b''
syft/serde/torch/serde.py,24,"b'import io\nimport warnings\n\nimport numpy\nimport torch\n\nfrom syft.workers.abstract import AbstractWorker\n\n# Torch dtypes to string (and back) mappers\nTORCH_DTYPE_STR = {\n    torch.uint8: ""uint8"",\n    torch.int8: ""int8"",\n    torch.int16: ""int16"",\n    torch.int32: ""int32"",\n    torch.int64: ""int64"",\n    torch.float16: ""float16"",\n    torch.float32: ""float32"",\n    torch.float64: ""float64"",\n    torch.complex32: ""complex32"",\n    torch.complex64: ""complex64"",\n    torch.complex128: ""complex128"",\n    torch.bool: ""bool"",\n    torch.qint8: ""qint8"",\n    torch.quint8: ""quint8"",\n    torch.qint32: ""qint32"",\n    torch.bfloat16: ""bfloat16"",\n}\nTORCH_STR_DTYPE = {name: cls for cls, name in TORCH_DTYPE_STR.items()}\n\n\nTORCH_MFORMAT_ID = {torch.channels_last: 1, torch.contiguous_format: 2, torch.preserve_format: 3}\n\nTORCH_ID_MFORMAT = {i: cls for cls, i in TORCH_MFORMAT_ID.items()}\n\n\ndef torch_tensor_serializer(worker: AbstractWorker, tensor) -> bin:\n    """"""Strategy to serialize a tensor using Torch saver""""""\n    binary_stream = io.BytesIO()\n    torch.save(tensor, binary_stream)\n    return binary_stream.getvalue()\n\n\ndef torch_tensor_deserializer(worker: AbstractWorker, tensor_bin) -> torch.Tensor:\n    """"""Strategy to deserialize a binary input using Torch load""""""\n    bin_tensor_stream = io.BytesIO(tensor_bin)\n    return torch.load(bin_tensor_stream)\n\n\ndef numpy_tensor_serializer(worker: AbstractWorker, tensor: torch.Tensor) -> bin:\n    """"""Strategy to serialize a tensor using numpy npy format.\n    If tensor requires to calculate gradients, it will be detached.\n\n    Args\n        (torch.Tensor): an input tensor to be serialized\n\n    Returns\n        A serialized version of the input tensor\n    """"""\n    if tensor.requires_grad:\n        warnings.warn(\n            ""Torch to Numpy serializer can only be used with tensors that do not require grad. ""\n            ""Detaching tensor to continue""\n        )\n        tensor = tensor.detach()\n\n    np_tensor = tensor.numpy()\n    outfile = io.BytesIO()\n    numpy.save(outfile, np_tensor)\n    return outfile.getvalue()\n\n\ndef numpy_tensor_deserializer(tensor_bin) -> torch.Tensor:\n    """"""Strategy to deserialize a binary input in npy format into Torch tensor\n\n    Args\n        tensor_bin: A binary representation of a tensor\n\n    Returns\n        a Torch tensor\n    """"""\n    bin_tensor_stream = io.BytesIO(tensor_bin)\n    return torch.from_numpy(numpy.load(bin_tensor_stream))\n'"
test/generic/pointers/__init__.py,0,b''
test/generic/pointers/test_callable_pointer.py,1,"b'import torch\nfrom syft.generic.pointers import callable_pointer\nfrom syft.generic.pointers.object_wrapper import ObjectWrapper\n\n\ndef test_create_callable_pointer(workers):\n    """"""\n    Asserts that a callable pointer is correctly created.\n    """"""\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n    p = callable_pointer.create_callable_pointer(\n        id=500,\n        id_at_location=2,\n        location=alice,\n        owner=bob,\n        tags=""tags"",\n        description=""description"",\n        register_pointer=True,\n    )\n\n    assert len(alice.object_store._tensors) == 0\n    assert isinstance(bob.object_store.get_obj(500), callable_pointer.CallablePointer)\n\n    p = callable_pointer.create_callable_pointer(\n        id=501,\n        id_at_location=2,\n        location=alice,\n        owner=bob,\n        tags=""tags"",\n        description=""description"",\n        register_pointer=False,\n    )\n\n    assert len(alice.object_store._tensors) == 0\n    assert isinstance(bob.object_store.get_obj(500), callable_pointer.CallablePointer)\n    assert 501 not in bob.object_store._objects\n\n\ndef test_get_obj_callable_pointer(workers):\n    """"""\n    Asserts that correct object values are returned when\n    `callable_pointer` is called.\n    """"""\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n\n    x = torch.tensor(5)\n    x_ptr = x.send(alice)\n\n    obj_ptr = callable_pointer.create_callable_pointer(\n        id=1,\n        id_at_location=x_ptr.id_at_location,\n        location=alice,\n        owner=bob,\n        tags=""tags"",\n        description=""description"",\n        register_pointer=True,\n    )\n\n    assert len(alice.object_store._tensors) == 1\n    assert isinstance(bob.object_store.get_obj(1), callable_pointer.CallablePointer)\n\n    x_get = obj_ptr.get()\n\n    assert len(alice.object_store._tensors) == 0\n    assert len(bob.object_store._tensors) == 0\n    assert 1 not in bob.object_store._objects\n    assert x_get == x\n\n\ndef test_call_callable_pointer(workers):\n    """"""\n    Tests that the correct result after an operation is\n    returned when `callable_pointer` is called.\n    """"""\n\n    def foo(x):\n        """""" Adds 2 to a given input `x`.""""""\n        return x + 2\n\n    alice = workers[""alice""]\n    bob = workers[""bob""]\n\n    id_alice = 100\n    id_bob = 200\n    foo_wrapper = ObjectWrapper(id=id_alice, obj=foo)\n\n    alice.register_obj(foo_wrapper, id_alice)\n\n    foo_ptr = callable_pointer.create_callable_pointer(\n        id=id_bob,\n        id_at_location=id_alice,\n        location=alice,\n        owner=bob,\n        tags=""tags"",\n        description=""description"",\n        register_pointer=True,\n    )\n\n    res = foo_ptr(4)\n\n    assert res == 6\n'"
test/generic/pointers/test_dataset_pointer.py,13,"b'import torch\nfrom syft.generic.pointers.pointer_dataset import PointerDataset\nfrom syft.frameworks.torch.fl.dataset import BaseDataset\n\n\ndef test_create_dataset_pointer(workers):\n    alice, bob, me = workers[""alice""], workers[""bob""], workers[""me""]\n\n    data = torch.tensor([1, 2, 3, 4])\n    target = torch.tensor([5, 6, 7, 8])\n    dataset = BaseDataset(data, target)\n    ptr = dataset.send(alice)\n\n    assert type(ptr) == PointerDataset\n    assert ptr.location == alice\n    assert ptr.owner == me\n\n\ndef test_search_dataset(workers):\n    alice, bob, me = workers[""alice""], workers[""bob""], workers[""me""]\n\n    data = torch.tensor([1, 2, 3, 4])\n    target = torch.tensor([5, 6, 7, 8])\n    dataset = BaseDataset(data, target).tag(""#test"").describe(""test search dataset"")\n    ptr = dataset.send(alice)\n    results = me.request_search([""#test""], location=alice)\n\n    assert results[0].id_at_location == ptr.id_at_location\n\n\ndef test_get_dataset(workers):\n    alice, bob, me = workers[""alice""], workers[""bob""], workers[""me""]\n\n    data = torch.tensor([1, 2, 3, 4])\n    target = torch.tensor([5, 6, 7, 8])\n    dataset = BaseDataset(data, target)\n    ptr = dataset.send(alice)\n    dataset = ptr.get()\n\n    assert torch.equal(dataset.data, data) == 1\n    assert torch.equal(dataset.targets, target) == 1\n    assert dataset.owner == me\n\n\ndef test_get_data_targets(workers):\n    alice, bob, me = workers[""alice""], workers[""bob""], workers[""me""]\n\n    data = torch.tensor([1, 2, 3, 4])\n    target = torch.tensor([5, 6, 7, 8])\n    dataset = BaseDataset(data, target)\n    ptr = dataset.send(alice)\n    remote_targets = ptr.targets.get()\n    remote_data = ptr.data.get()\n\n    assert torch.equal(remote_data, data) == 1\n    assert torch.equal(remote_targets, target) == 1\n'"
test/generic/pointers/test_multi_pointer.py,0,"b'import torch as th\nimport syft as sy\n\nfrom syft.generic.pointers.multi_pointer import MultiPointerTensor\n\n\ndef test_multi_pointers(workers):\n    """"""\n    Ensure that the sy.combine_pointers works as expected\n    """"""\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    a = th.tensor([1, 2, 3, 4, 5]).send(bob, alice)\n\n    b = a + a\n\n    c = b.get(sum_results=True)\n    assert (c == th.tensor([4, 8, 12, 16, 20])).all()\n\n    b = a + a\n    c = b.get(sum_results=False)\n    assert len(c) == 2\n    assert (c[0] == th.tensor([2, 4, 6, 8, 10])).all\n\n    # test default sum pointer state\n    b = a + a\n    c = b.get()\n    assert len(c) == 2\n    assert (c[0] == th.tensor([2, 4, 6, 8, 10])).all\n\n\ndef test_dim(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    a = th.tensor([1, 2, 3, 4, 5]).send(bob, alice)\n\n    assert a.dim() == 1\n\n\ndef test_simplify(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    a = th.tensor([1, 2, 3, 4, 5]).send(bob, alice)\n    ser = sy.serde.serialize(a)\n    detail = sy.serde.deserialize(ser).child\n    assert isinstance(detail, MultiPointerTensor)\n    for key in a.child.child:\n        assert key in detail.child\n'"
test/generic/pointers/test_pointer_plan.py,0,"b'import torch as th\nimport syft as sy\n\nfrom syft.generic.pointers.pointer_plan import PointerPlan\nfrom syft.execution.plan import Plan\n\n\ndef test_create_pointer_to_plan(hook, workers):\n    alice, bob, charlie = workers[""alice""], workers[""bob""], workers[""charlie""]\n\n    hook.local_worker.is_client_worker = False\n\n    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))\n    def plan(x, state):\n        (bias,) = state.read()\n        return x + bias\n\n    plan.send(alice)\n    id_at_location = plan.id\n\n    plan_ptr = PointerPlan(location=alice, id_at_location=id_at_location)\n\n    x = th.tensor([1.0]).send(alice)\n\n    ptr = plan_ptr(x)\n\n    assert (ptr.get() == th.tensor([2.0])).all()\n\n    hook.local_worker.is_client_worker = True\n\n\ndef test_search_plan(hook, workers):\n\n    alice, me = workers[""alice""], workers[""me""]\n    me.is_client_worker = False\n\n    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))\n    def plan(x, state):\n        (bias,) = state.read()\n        return x + bias\n\n    plan.send(alice)\n    id_at_location = plan.id\n\n    plan_ptr = me.request_search([id_at_location], location=alice)[0]\n\n    assert isinstance(plan_ptr, PointerPlan)\n\n    x = th.tensor([1.0]).send(alice)\n    ptr = plan_ptr(x)\n\n    assert (ptr.get() == th.tensor([2.0])).all()\n\n    me.is_client_worker = True\n\n\ndef test_get_plan(workers):\n    alice, me = workers[""alice""], workers[""me""]\n    me.is_client_worker = False\n\n    @sy.func2plan(args_shape=[(1,)], state=(th.tensor([1.0]),))\n    def plan(x, state):\n        (bias,) = state.read()\n        return x + bias\n\n    plan.send(alice)\n    id_at_location = plan.id\n    plan_ptr = me.request_search([id_at_location], location=alice)[0]\n\n    plan = plan_ptr.get()\n\n    assert isinstance(plan, Plan)\n\n    x = th.tensor([1.0])\n    res = plan(x)\n\n    assert (res == th.tensor([2.0])).all()\n\n    me.is_client_worker = True\n\n\ndef test_pointer_plan_parameters(workers):\n    bob, me = workers[""bob""], workers[""me""]\n\n    me.is_client_worker = False\n\n    class Net(sy.Plan):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = th.nn.Linear(2, 1)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            return x\n\n    model = Net()\n    model.build(th.tensor([[0.0, 0.0]]))\n    model = model.send(bob)\n\n    param_ptrs = model.parameters()\n\n    assert len(param_ptrs) == 2\n\n    for param_ptr in param_ptrs:\n        assert param_ptr.is_wrapper\n        assert isinstance(param_ptr.child, sy.PointerTensor)\n\n    me.is_client_worker = True\n'"
test/generic/pointers/test_pointer_tensor.py,57,"b'import torch\nimport torch as th\nimport syft\n\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\nfrom syft.frameworks.torch.tensors.interpreters.precision import FixedPrecisionTensor\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nimport pytest\n\n\ndef test_init(workers):\n    alice, me = workers[""alice""], workers[""me""]\n    pointer = PointerTensor(id=1000, location=alice, owner=me)\n    pointer.__str__()\n\n\ndef test_create_pointer():\n    x = torch.Tensor([1, 2])\n    x.create_pointer()\n\n\ndef test_send_default_garbage_collector_true(workers):\n    """"""\n    Remote tensor should be garbage collected by default on\n    deletion of the Pointer tensor pointing to remote tensor\n    """"""\n    alice = workers[""alice""]\n\n    x = torch.Tensor([-1, 2])\n    x_ptr = x.send(alice)\n    assert x_ptr.child.garbage_collect_data\n\n\ndef test_send_garbage_collect_data_false(workers):\n    """"""\n    Remote tensor should be not garbage collected on\n    deletion of the Pointer tensor pointing to remote tensor\n    """"""\n    alice = workers[""alice""]\n\n    x = torch.Tensor([-1, 2])\n    x_ptr = x.send(alice)\n    x_ptr.garbage_collection = False\n    assert x_ptr.child.garbage_collect_data is False\n\n\ndef test_send_gc_false(workers):\n    """"""\n    Remote tensor should be not garbage collected on\n    deletion of the Pointer tensor pointing to remote tensor\n    """"""\n    alice = workers[""alice""]\n    x = torch.Tensor([-1, 2])\n    x_ptr = x.send(alice)\n    x_ptr.gc = False\n    assert x_ptr.child.garbage_collect_data is False\n    assert x_ptr.gc is False, ""property GC is not in sync""\n    assert x_ptr.garbage_collection is False, ""property garbage_collection is not in sync""\n\n\ndef test_send_gc_true(workers):\n    """"""\n    Remote tensor by default is garbage collected on\n    deletion of Pointer Tensor\n    """"""\n    alice = workers[""alice""]\n\n    x = torch.Tensor([-1, 2])\n    x_ptr = x.send(alice)\n\n    assert x_ptr.gc\n\n\ndef test_send_disable_gc(workers):\n    """"""Pointer tensor should be not garbage collected.""""""\n    alice = workers[""alice""]\n\n    x = torch.Tensor([-1, 2])\n    x_ptr = x.send(alice).disable_gc\n    assert x_ptr.child.garbage_collect_data is False\n    assert x_ptr.gc is False, ""property GC is not in sync""\n    assert x_ptr.garbage_collection is False, ""property garbage_collection is not in sync""\n\n\ndef test_send_get(workers):\n    """"""Test several send get usages""""""\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    # simple send\n    x = torch.Tensor([1, 2])\n    x_ptr = x.send(bob)\n    x_back = x_ptr.get()\n    assert (x == x_back).all()\n\n    # send with variable overwriting\n    x = torch.Tensor([1, 2])\n    x = x.send(bob)\n    x_back = x.get()\n    assert (torch.Tensor([1, 2]) == x_back).all()\n\n    # double send\n    x = torch.Tensor([1, 2])\n    x_ptr = x.send(bob)\n    x_ptr_ptr = x_ptr.send(alice)\n    x_ptr_back = x_ptr_ptr.get()\n    x_back_back = x_ptr_back.get()\n    assert (x == x_back_back).all()\n\n    # double send with variable overwriting\n    x = torch.Tensor([1, 2])\n    x = x.send(bob)\n    x = x.send(alice)\n    x = x.get()\n    x_back = x.get()\n    assert (torch.Tensor([1, 2]) == x_back).all()\n\n    # chained double send\n    x = torch.Tensor([1, 2])\n    x = x.send(bob).send(alice)\n    x_back = x.get().get()\n    assert (torch.Tensor([1, 2]) == x_back).all()\n\n\ndef test_inplace_send_get(workers):\n    bob = workers[""bob""]\n\n    tensor = torch.tensor([1.0, -1.0, 3.0, 4.0])\n    tensor_ptr = tensor.send_(bob)\n\n    assert tensor_ptr.id == tensor.id\n    assert id(tensor_ptr) == id(tensor)\n\n    tensor_back = tensor_ptr.get_()\n\n    assert tensor_back.id == tensor_ptr.id\n    assert tensor_back.id == tensor.id\n    assert id(tensor_back) == id(tensor)\n    assert id(tensor_back) == id(tensor)\n\n    assert (tensor_back == tensor).all()\n\n\ndef test_repeated_send(workers):\n    """"""Tests that repeated calls to .send(bob) works gracefully.\n    Previously garbage collection deleted the remote object\n    when .send() was called twice. This test ensures the fix still\n    works.""""""\n\n    bob = workers[""bob""]\n\n    # create tensor\n    x = torch.Tensor([1, 2])\n\n    # send tensor to bob\n    x_ptr = x.send(bob)\n\n    # send tensor again\n    x_ptr = x.send(bob)\n\n    # ensure bob has tensor\n    assert x.id in bob.object_store._objects\n\n\ndef test_remote_autograd(workers):\n    """"""Tests the ability to backpropagate gradients on a remote\n    worker.""""""\n\n    bob = workers[""bob""]\n\n    # TEST: simple remote grad calculation\n\n    # create a tensor\n    x = torch.tensor([1, 2, 3, 4.0], requires_grad=True)\n\n    # send tensor to bob\n    x = x.send(bob)\n\n    # do some calculation\n    y = (x + x).sum()\n\n    # backpropagate on remote machine\n    y.backward()\n\n    # check that remote gradient is correct\n    x_grad = bob.object_store.get_obj(x.id_at_location).grad\n    x_grad_target = torch.ones(4).float() + 1\n    assert (x_grad == x_grad_target).all()\n\n    # TEST: Ensure remote grad calculation gets properly serded\n\n    # create tensor\n    x = torch.tensor([1, 2, 3, 4.0], requires_grad=True).send(bob)\n\n    # compute function\n    y = x.sum()\n\n    # backpropagate\n    y.backward()\n\n    # get the gradient created from backpropagation manually\n    x_grad = bob.object_store.get_obj(x.id_at_location).grad\n\n    # get the entire x tensor (should bring the grad too)\n    x = x.get()\n\n    # make sure that the grads match\n    assert (x.grad == x_grad).all()\n\n\ndef test_gradient_send_recv(workers):\n    """"""Tests that gradients are properly sent and received along\n    with their tensors.""""""\n\n    bob = workers[""bob""]\n\n    # create a tensor\n    x = torch.tensor([1, 2, 3, 4.0], requires_grad=True)\n\n    # create gradient on tensor\n    x.sum().backward(th.tensor(1.0))\n\n    # save gradient\n    orig_grad = x.grad\n\n    # send and get back\n    t = x.send(bob).get()\n\n    # check that gradient was properly serde\n    assert (t.grad == orig_grad).all()\n\n\ndef test_method_on_attribute(workers):\n\n    bob = workers[""bob""]\n\n    # create remote object with children\n    x = torch.Tensor([1, 2, 3])\n    x = syft.LoggingTensor().on(x).send(bob)\n\n    # call method on data tensor directly\n    x.child.point_to_attr = ""child.child""\n    y = x.add(x)\n    assert isinstance(y.get(), torch.Tensor)\n\n    # call method on loggingtensor directly\n    x.child.point_to_attr = ""child""\n    y = x.add(x)\n    y = y.get()\n    assert isinstance(y.child, syft.LoggingTensor)\n\n    # # call method on zeroth attribute\n    # x.child.point_to_attr = """"\n    # y = x.add(x)\n    # y = y.get()\n    #\n    # assert isinstance(y, torch.Tensor)\n    # assert isinstance(y.child, syft.LoggingTensor)\n    # assert isinstance(y.child.child, torch.Tensor)\n\n    # call .get() on pinter to attribute (should error)\n    x.child.point_to_attr = ""child""\n    try:\n        x.get()\n    except syft.exceptions.CannotRequestObjectAttribute as e:\n        assert True\n\n\ndef test_grad_pointer(workers):\n    """"""Tests the automatic creation of a .grad pointer when\n    calling .send() on a tensor with requires_grad==True""""""\n\n    bob = workers[""bob""]\n\n    x = torch.tensor([1, 2, 3.0], requires_grad=True).send(bob)\n    y = (x + x).sum()\n    y.backward()\n\n    assert (bob.object_store.get_obj(x.id_at_location).grad == torch.tensor([2, 2, 2.0])).all()\n\n\ndef test_move(workers):\n    alice, bob, james, me = workers[""alice""], workers[""bob""], workers[""james""], workers[""me""]\n\n    x = torch.tensor([1, 2, 3, 4, 5]).send(bob)\n\n    assert x.id_at_location in bob.object_store._objects\n    assert x.id_at_location not in alice.object_store._objects\n\n    p = x.move(alice)\n\n    assert x.id_at_location not in bob.object_store._objects\n    assert x.id_at_location in alice.object_store._objects\n\n    x = torch.tensor([1.0, 2, 3, 4, 5], requires_grad=True).send(bob)\n\n    assert x.id_at_location in bob.object_store._objects\n    assert x.id_at_location not in alice.object_store._objects\n\n    p = x.move(alice)\n\n    assert x.id_at_location not in bob.object_store._objects\n    assert x.id_at_location in alice.object_store._objects\n\n    alice.clear_objects()\n    bob.clear_objects()\n    x = torch.tensor([1.0, 2, 3, 4, 5]).send(bob)\n    p = x.move(alice)\n\n    assert len(alice.object_store._tensors) == 1\n\n    # Test .move on remote objects\n\n    james.clear_objects()\n    x = th.tensor([1.0]).send(james)\n    remote_x = james.object_store.get_obj(x.id_at_location)\n    remote_ptr = remote_x.send(bob)\n    assert remote_ptr.id in james.object_store._objects.keys()\n    remote_ptr2 = remote_ptr.move(alice)\n    assert remote_ptr2.id in james.object_store._objects.keys()\n\n    # Test .move back to myself\n\n    alice.clear_objects()\n    bob.clear_objects()\n    t = torch.tensor([1.0, 2, 3, 4, 5])\n    x = t.send(bob)\n    y = x.move(alice)\n    z = y.move(me)\n    assert (z == t).all()\n\n    # Move object to same location\n    alice.clear_objects()\n    t = torch.tensor([1.0, 2, 3, 4, 5]).send(bob)\n    t = t.move(bob)\n    assert torch.all(torch.eq(t.get(), torch.tensor([1.0, 2, 3, 4, 5])))\n\n\ndef test_combine_pointers(workers):\n    """"""\n    Ensure that the sy.combine_pointers works as expected\n    """"""\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    y = th.tensor([1, 2, 3, 4, 5]).send(alice)\n\n    a = x.combine(y)\n    b = a + a\n\n    c = b.get(sum_results=True)\n    assert (c == th.tensor([4, 8, 12, 16, 20])).all()\n\n    b = a + a\n    c = b.get(sum_results=False)\n    assert len(c) == 2\n    assert (c[0] == th.tensor([2, 4, 6, 8, 10])).all\n\n\ndef test_remote_to_cpu_device(workers):\n    """"""Ensure remote .to cpu works""""""\n    device = torch.device(""cpu"")\n    bob = workers[""bob""]\n\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    x.to(device)\n\n\ndef test_get_remote_shape(workers):\n    """"""Test pointer.shape functionality""""""\n    bob = workers[""bob""]\n    # tensor directly sent: shape stored at sending\n    x = th.tensor([1, 2, 3, 4, 5]).send(bob)\n    assert x.shape == torch.Size([5])\n    # result of an operation: need to make a call to the remote worker\n    y = x + x\n    assert y.shape == torch.Size([5])\n\n\ndef test_remote_function_with_multi_ouput(workers):\n    """"""\n    Functions like .split return several tensors, registration and response\n    must be made carefully in this case\n    """"""\n    bob = workers[""bob""]\n\n    tensor = torch.tensor([1, 2, 3, 4.0])\n    ptr = tensor.send(bob)\n    r_ptr = torch.split(ptr, 2)\n    assert (r_ptr[0].get() == torch.tensor([1, 2.0])).all()\n\n    tensor = torch.tensor([1, 2, 3, 4.0])\n    ptr = tensor.send(bob)\n    max_value, argmax_idx = torch.max(ptr, 0)\n\n    assert max_value.get().item() == 4.0\n    assert argmax_idx.get().item() == 3\n\n\ndef test_raising_error_when_item_func_called(workers):\n    pointer = PointerTensor(id=1000, location=workers[""alice""], owner=workers[""me""])\n    with pytest.raises(RuntimeError):\n        pointer.item()\n\n\ndef test_fix_prec_on_pointer_tensor(workers):\n    """"""\n    Ensure .fix_precision() works as expected.\n    Also check that fix_precision() is not inplace.\n    """"""\n    bob = workers[""bob""]\n\n    tensor = torch.tensor([1, 2, 3, 4.0])\n    ptr = tensor.send(bob)\n\n    ptr_fp = ptr.fix_precision()\n\n    remote_tensor = bob.object_store.get_obj(ptr.id_at_location)\n    remote_fp_tensor = bob.object_store.get_obj(ptr_fp.id_at_location)\n\n    # check that fix_precision is not inplace\n    assert (remote_tensor == tensor).all()\n\n    assert isinstance(ptr.child, PointerTensor)\n    assert isinstance(remote_fp_tensor.child, FixedPrecisionTensor)\n\n\ndef test_fix_prec_on_pointer_of_pointer(workers):\n    """"""\n    Ensure .fix_precision() works along a chain of pointers.\n    """"""\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    tensor = torch.tensor([1, 2, 3, 4.0])\n    ptr = tensor.send(bob)\n    ptr = ptr.send(alice)\n\n    ptr = ptr.fix_precision()\n\n    alice_tensor = alice.object_store.get_obj(ptr.id_at_location)\n    remote_tensor = bob.object_store.get_obj(alice_tensor.id_at_location)\n\n    assert isinstance(ptr.child, PointerTensor)\n    assert isinstance(remote_tensor.child, FixedPrecisionTensor)\n\n\ndef test_float_prec_on_pointer_tensor(workers):\n    """"""\n    Ensure .float_precision() works as expected.\n    """"""\n    bob = workers[""bob""]\n\n    tensor = torch.tensor([1, 2, 3, 4.0])\n    ptr = tensor.send(bob)\n    ptr = ptr.fix_precision()\n\n    ptr = ptr.float_precision()\n    remote_tensor = bob.object_store.get_obj(ptr.id_at_location)\n\n    assert isinstance(ptr.child, PointerTensor)\n    assert isinstance(remote_tensor, torch.Tensor)\n\n\ndef test_float_prec_on_pointer_of_pointer(workers):\n    """"""\n    Ensure .float_precision() works along a chain of pointers.\n    """"""\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    tensor = torch.tensor([1, 2, 3, 4.0])\n    ptr = tensor.send(bob)\n    ptr = ptr.send(alice)\n    ptr = ptr.fix_precision()\n\n    ptr = ptr.float_precision()\n\n    alice_tensor = alice.object_store.get_obj(ptr.id_at_location)\n    remote_tensor = bob.object_store.get_obj(alice_tensor.id_at_location)\n\n    assert isinstance(ptr.child, PointerTensor)\n    assert isinstance(remote_tensor, torch.Tensor)\n\n\ndef test_share_get(workers):\n    """"""\n    Ensure .share() works as expected.\n    """"""\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    charlie = workers[""charlie""]\n\n    tensor = torch.tensor([1, 2, 3])\n    ptr = tensor.send(bob)\n\n    ptr = ptr.share(charlie, alice)\n    remote_tensor = bob.object_store.get_obj(ptr.id_at_location)\n\n    assert isinstance(ptr.child, PointerTensor)\n    assert isinstance(remote_tensor.child, AdditiveSharingTensor)\n\n\ndef test_registration_of_action_on_pointer_of_pointer(workers):\n    """"""\n    Ensure actions along a chain of pointers are registered as expected.\n    """"""\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    tensor = torch.tensor([1, 2, 3, 4.0])\n    ptr = tensor.send(bob)\n    ptr = ptr.send(alice)\n    ptr_action = ptr + ptr\n\n    assert len(alice.object_store._tensors) == 2\n    assert len(bob.object_store._tensors) == 2\n\n\ndef test_setting_back_grad_to_origin_after_send(workers):\n    """"""\n    Calling .backward() on a tensor sent using `.send(..., requires_grad=True)`\n    should update the origin tensor gradient\n    """"""\n    me = workers[""me""]\n    alice = workers[""alice""]\n\n    with me.registration_enabled():\n        x = th.tensor([1.0, 2.0, 3, 4, 5], requires_grad=True)\n        y = x + x\n        me.register_obj(y)  # registration on the local worker is sometimes buggy\n\n        y_ptr = y.send(alice, requires_grad=True)\n        z_ptr = y_ptr * 2\n\n        z = z_ptr.sum()\n        z.backward()\n\n        assert (x.grad == th.tensor([4.0, 4.0, 4.0, 4.0, 4.0])).all()\n\n\ndef test_setting_back_grad_to_origin_after_move(workers):\n    """"""\n    Calling .backward() on a tensor moved using `.move(..., requires_grad=True)`\n    should update the origin tensor gradient\n    """"""\n    me = workers[""me""]\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    with me.registration_enabled():\n        x = th.tensor([1.0, 2.0, 3, 4, 5], requires_grad=True)\n        y = x + x\n        me.register_obj(y)  # registration on the local worker is sometimes buggy\n\n        y_ptr = y.send(alice, requires_grad=True)\n        z_ptr = y_ptr * 2\n\n        z_ptr2 = z_ptr.move(bob, requires_grad=True)\n        z = z_ptr2.sum()\n        z.backward()\n\n        assert (x.grad == th.tensor([4.0, 4.0, 4.0, 4.0, 4.0])).all()\n\n\ndef test_iadd(workers):\n    alice = workers[""alice""]\n    a = torch.ones(1, 5)\n    b = torch.ones(1, 5)\n    a_pt = a.send(alice)\n    b_pt = b.send(alice)\n\n    b_pt += a_pt\n\n    assert len(alice.object_store._objects) == 2\n\n\ndef test_inplace_ops_on_remote_long_tensor(workers):\n    alice = workers[""alice""]\n\n    t = torch.LongTensor([2])\n    p = t.send_(alice) * 2\n    p.get_()\n\n    assert p == torch.LongTensor([4])\n\n\ndef test_iterable_pointer(workers):\n    alice = workers[""alice""]\n\n    t = torch.Tensor([[1, 2], [4, 5], [7, 8]])\n\n    p = t.send(alice)\n\n    assert len(alice.object_store) == 1\n    for idx, tensor in enumerate(p):\n        assert len(alice.object_store) == 2\n        assert isinstance(tensor, PointerTensor)\n        assert torch.all(tensor.get() == t[idx])\n\n    assert len(alice.object_store) == 1\n\n    l = []\n    for idx, tensor in enumerate(p):\n        l.append(tensor)\n\n    assert len(alice.object_store) == 4\n\n    del l\n    del tensor\n\n    assert len(alice.object_store) == 1\n    for idx, tensor in enumerate(p[:, 1]):\n\n        # Should be 3 because p[:, 1] will create another tensor on alice side\n        assert len(alice.object_store) == 3\n        assert isinstance(tensor, PointerTensor)\n        assert torch.all(tensor.get() == t[:, 1][idx])\n'"
test/serde/msgpack/test_msgpack_serde.py,34,"b'""""""\nThis file tests the ability for serde.py to convert complex types into\nsimple python types which are serializable by standard serialization tools.\nFor more on how/why this works, see serde.py directly.\n""""""\nimport msgpack as msgpack_lib\nimport numpy\nimport pytest\nimport torch\nfrom torch import Tensor\n\nimport syft\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\nfrom syft.generic.pointers.object_wrapper import ObjectWrapper\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.serde import compression\nfrom syft.serde import msgpack\nfrom syft.serde import serde\nfrom syft.serde.msgpack import native_serde\nfrom syft.serde.msgpack import torch_serde\nfrom syft.workers.virtual import VirtualWorker\nfrom test.serde.serde_helpers import SerializableDummyClass\n\n\ndef test_tuple_simplify(workers):\n    """"""This tests our ability to simplify tuple types.\n\n    This test is pretty simple since tuples just serialize to\n    themselves, with a tuple wrapper with the correct ID (1)\n    for tuples so that the detailer knows how to interpret it.""""""\n\n    me = workers[""me""]\n    input = (""hello"", ""world"")\n    tuple_detail_code = msgpack.proto_type_info(tuple).code\n    str_detail_code = msgpack.proto_type_info(str).code\n    target = (tuple_detail_code, ((str_detail_code, (b""hello"",)), (str_detail_code, (b""world"",))))\n    assert msgpack.serde._simplify(me, input) == target\n\n\ndef test_list_simplify(workers):\n    """"""This tests our ability to simplify list types.\n\n    This test is pretty simple since lists just serialize to\n    themselves, with a tuple wrapper with the correct ID (2)\n    for lists so that the detailer knows how to interpret it.""""""\n\n    me = workers[""me""]\n    input = [""hello"", ""world""]\n    list_detail_code = msgpack.proto_type_info(list).code\n    str_detail_code = msgpack.proto_type_info(str).code\n    target = (list_detail_code, ((str_detail_code, (b""hello"",)), (str_detail_code, (b""world"",))))\n    assert msgpack.serde._simplify(me, input) == target\n\n\ndef test_set_simplify(workers):\n    """"""This tests our ability to simplify set objects.\n\n    This test is pretty simple since sets just serialize to\n    lists, with a tuple wrapper with the correct ID (3)\n    for sets so that the detailer knows how to interpret it.""""""\n\n    me = workers[""me""]\n    input = {""hello"", ""world""}\n    set_detail_code = msgpack.proto_type_info(set).code\n    str_detail_code = msgpack.proto_type_info(str).code\n    target = (set_detail_code, ((str_detail_code, (b""hello"",)), (str_detail_code, (b""world"",))))\n    assert msgpack.serde._simplify(me, input)[0] == target[0]\n    assert set(msgpack.serde._simplify(me, input)[1]) == set(target[1])\n\n\ndef test_float_simplify(workers):\n    """"""This tests our ability to simplify float objects.\n\n    This test is pretty simple since floats just serialize to\n    themselves, with no tuple/id necessary.""""""\n\n    me = workers[""me""]\n    input = 5.6\n    target = 5.6\n    assert msgpack.serde._simplify(me, input) == target\n\n\ndef test_int_simplify(workers):\n    """"""This tests our ability to simplify int objects.\n\n    This test is pretty simple since ints just serialize to\n    themselves, with no tuple/id necessary.""""""\n\n    me = workers[""me""]\n    input = 5\n    target = 5\n    assert msgpack.serde._simplify(me, input) == target\n\n\ndef test_string_simplify(workers):\n    """"""This tests our ability to simplify string objects.\n\n    This test is pretty simple since strings just serialize to\n    themselves, with no tuple/id necessary.""""""\n\n    me = workers[""me""]\n    input = ""hello""\n    target = (msgpack.proto_type_info(str).code, (b""hello"",))\n    assert msgpack.serde._simplify(me, input) == target\n\n\ndef test_dict_simplify(workers):\n    """"""This tests our ability to simplify dict objects.\n\n    This test is pretty simple since dicts just serialize to\n    themselves, with a tuple wrapper with the correct ID\n    for dicts so that the detailer knows how to interpret it.""""""\n\n    me = workers[""me""]\n    input = {""hello"": ""world""}\n    detail_dict_code = msgpack.proto_type_info(dict).code\n    detail_str_code = msgpack.proto_type_info(str).code\n    target = (detail_dict_code, (((detail_str_code, (b""hello"",)), (detail_str_code, (b""world"",))),))\n    assert msgpack.serde._simplify(me, input) == target\n\n\ndef test_range_simplify(workers):\n    """"""This tests our ability to simplify range objects.\n\n    This test is pretty simple since range objs just serialize to\n    themselves, with a tuple wrapper with the correct ID (5)\n    for dicts so that the detailer knows how to interpret it.""""""\n\n    me = workers[""me""]\n    input = range(1, 3, 4)\n    target = (msgpack.proto_type_info(range).code, (1, 3, 4))\n    assert msgpack.serde._simplify(me, input) == target\n\n\ndef test_torch_tensor_simplify(workers):\n    """"""This tests our ability to simplify torch.Tensor objects\n    using ""torch"" serialization strategy.\n\n    At the time of writing, tensors simplify to a tuple where the\n    first value in the tuple is the tensor\'s ID and the second\n    value is a serialized version of the Tensor (serialized\n    by PyTorch\'s torch.save method)\n    """"""\n\n    me = workers[""me""]\n\n    # create a tensor\n    input = Tensor(numpy.random.random((100, 100)))\n\n    # simplify the tnesor\n    output = msgpack.serde._simplify(me, input)\n\n    # make sure outer type is correct\n    assert type(output) == tuple\n\n    # make sure the object type ID is correct\n    # (0 for torch.Tensor)\n    assert (\n        msgpack.serde.msgpack_global_state.detailers[output[0]] == torch_serde._detail_torch_tensor\n    )\n\n    # make sure inner type is correct\n    assert type(output[1]) == tuple\n\n    # make sure ID is correctly encoded\n    assert output[1][0] == input.id\n\n    # make sure tensor data type is correct\n    assert type(output[1][1]) == bytes\n\n\ndef test_torch_tensor_simplify_generic(workers):\n    """"""This tests our ability to simplify torch.Tensor objects\n    using ""all"" serialization strategy\n    """"""\n\n    worker = VirtualWorker(None, id=""non-torch"")\n\n    # create a tensor\n    input = Tensor(numpy.random.random((3, 3, 3)))\n\n    # simplify the tensor\n    output = msgpack.serde._simplify(worker, input)\n\n    # make sure outer type is correct\n    assert type(output) == tuple\n\n    # make sure the object type ID is correct\n    # (0 for torch.Tensor)\n    assert (\n        msgpack.serde.msgpack_global_state.detailers[output[0]] == torch_serde._detail_torch_tensor\n    )\n\n    # make sure inner type is correct\n    assert type(output[1]) == tuple\n\n    # make sure ID is correctly encoded\n    assert output[1][0] == input.id\n\n    # make sure tensor data type is correct\n    assert type(output[1][1]) == tuple\n    assert type(output[1][1][1]) == tuple\n\n    # make sure tensor data matches\n    assert output[1][1][1][0][1] == input.size()\n    assert output[1][1][1][2][1] == tuple(input.flatten().tolist())\n\n\ndef test_torch_tensor_serde_generic(workers):\n    """"""This tests our ability to ser-de torch.Tensor objects\n    using ""all"" serialization strategy\n    """"""\n\n    worker = VirtualWorker(None, id=""non-torch"")\n\n    # create a tensor\n    input = Tensor(numpy.random.random((100, 100)))\n\n    # ser-de the tensor\n    output = msgpack.serde._simplify(worker, input)\n    detailed = msgpack.serde._detail(worker, output)\n\n    # check tensor contents\n    assert input.size() == detailed.size()\n    assert input.dtype == detailed.dtype\n    assert (input == detailed).all()\n\n\ndef test_tensor_gradient_serde():\n    # create a tensor\n    x = torch.tensor([1, 2, 3, 4.0], requires_grad=True)\n\n    # create gradient on tensor\n    x.sum().backward(torch.tensor(1.0))\n\n    # save gradient\n    orig_grad = x.grad\n\n    # serialize\n    blob = syft.serde.serialize(x)\n\n    # deserialize\n    t = syft.serde.deserialize(blob)\n\n    # check that gradient was properly serde\n    assert (t.grad == orig_grad).all()\n\n\ndef test_ndarray_simplify(workers):\n    """"""This tests our ability to simplify numpy.array objects\n\n    At the time of writing, arrays simplify to an object inside\n    of a tuple which specifies the ID for the np.array type (6) so\n    that the detailer knows to turn the simplifed form to a np.array\n    """"""\n\n    me = workers[""me""]\n    input = numpy.random.random((100, 100))\n    output = msgpack.serde._simplify(me, input)\n\n    # make sure simplified type ID is correct\n    assert msgpack.serde.msgpack_global_state.detailers[output[0]] == native_serde._detail_ndarray\n\n    # make sure serialized form is correct\n    assert type(output[1][0]) == bytes\n    assert output[1][1] == msgpack.serde._simplify(me, input.shape)\n    assert output[1][2] == msgpack.serde._simplify(me, input.dtype.name)\n\n\ndef test_numpy_number_simplify(workers):\n    """"""This tests our ability to simplify numpy.float objects\n\n    At the time of writing, numpy number simplify to an object inside\n    of a tuple where the first value is a byte representation of the number\n    and the second value is the dtype\n    """"""\n    me = workers[""me""]\n\n    input = numpy.float32(2.0)\n    output = msgpack.serde._simplify(me, input)\n\n    # make sure simplified type ID is correct\n    assert (\n        msgpack.serde.msgpack_global_state.detailers[output[0]] == native_serde._detail_numpy_number\n    )\n\n    # make sure serialized form is correct\n    assert type(output[1][0]) == bytes\n    assert output[1][1] == msgpack.serde._simplify(me, input.dtype.name)\n\n\ndef test_ellipsis_simplify(workers):\n    """"""Make sure ellipsis simplifies correctly.""""""\n    me = workers[""me""]\n\n    assert (\n        msgpack.serde.msgpack_global_state.detailers[msgpack.serde._simplify(me, Ellipsis)[0]]\n        == native_serde._detail_ellipsis\n    )\n\n    # the simplified ellipsis (empty object)\n    assert msgpack.serde._simplify(me, Ellipsis)[1] == (b"""",)\n\n\ndef test_torch_device_simplify(workers):\n    """"""Test the simplification of torch.device""""""\n\n    me = workers[""me""]\n    device = torch.device(""cpu"")\n\n    assert (\n        msgpack.serde.msgpack_global_state.detailers[msgpack.serde._simplify(me, device)[0]]\n        == torch_serde._detail_torch_device\n    )\n\n    # the simplified torch.device\n    assert msgpack.serde._simplify(me, device)[1][0] == msgpack.serde._simplify(me, ""cpu"")\n\n\ndef test_torch_dtype_simplify(workers):\n    """"""Test the simplification of torch.dtype""""""\n\n    me = workers[""me""]\n    dtype = torch.int32\n\n    assert (\n        msgpack.serde.msgpack_global_state.detailers[msgpack.serde._simplify(me, dtype)[0]]\n        == torch_serde._detail_torch_dtype\n    )\n\n    # the simplified torch.dtype\n    assert msgpack.serde._simplify(me, dtype)[1] == ""int32""\n\n\ndef test_pointer_tensor_simplify(workers):\n    """"""Test the simplification of PointerTensor""""""\n\n    alice, me = workers[""alice""], workers[""me""]\n\n    input_tensor = PointerTensor(id=1000, location=alice, owner=alice)\n\n    output = msgpack.serde._simplify(me, input_tensor)\n\n    assert output[1][0] == input_tensor.id\n    assert output[1][1] == input_tensor.id_at_location\n    assert output[1][2] == msgpack.serde._simplify(me, input_tensor.owner.id)\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_torch_Tensor(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    t = Tensor(numpy.random.random((100, 100)))\n    t_serialized = syft.serde.serialize(t)\n    t_serialized_deserialized = syft.serde.deserialize(t_serialized)\n    assert (t == t_serialized_deserialized).all()\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_tuple(compress):\n    # Test with a simple datatype\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    tuple = (1, 2)\n    tuple_serialized = syft.serde.serialize(tuple)\n    tuple_serialized_deserialized = syft.serde.deserialize(tuple_serialized)\n    assert tuple == tuple_serialized_deserialized\n\n    # Test with a complex data structure\n    tensor_one = Tensor(numpy.random.random((100, 100)))\n    tensor_two = Tensor(numpy.random.random((100, 100)))\n    tuple = (tensor_one, tensor_two)\n    tuple_serialized = syft.serde.serialize(tuple)\n    tuple_serialized_deserialized = syft.serde.deserialize(tuple_serialized)\n    # `assert tuple_serialized_deserialized == tuple` does not work, therefore it\'s split\n    # into 3 assertions\n    assert type(tuple_serialized_deserialized) == type(tuple)\n    assert (tuple_serialized_deserialized[0] == tensor_one).all()\n    assert (tuple_serialized_deserialized[1] == tensor_two).all()\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_bytearray(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    bytearr = bytearray(""This is a teststring"", ""utf-8"")\n    bytearr_serialized = syft.serde.serialize(bytearr)\n    bytearr_serialized_desirialized = syft.serde.deserialize(bytearr_serialized)\n    assert bytearr == bytearr_serialized_desirialized\n\n    bytearr = bytearray(numpy.random.random((100, 100)))\n    bytearr_serialized = syft.serde.serialize(bytearr)\n    bytearr_serialized_desirialized = syft.serde.deserialize(bytearr_serialized)\n    assert bytearr == bytearr_serialized_desirialized\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_ndarray_serde(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n    arr = numpy.random.random((100, 100))\n    arr_serialized = syft.serde.serialize(arr)\n\n    arr_serialized_deserialized = syft.serde.deserialize(arr_serialized)\n\n    assert numpy.array_equal(arr, arr_serialized_deserialized)\n\n\n@pytest.mark.parametrize(\n    ""compress_scheme"", [compression.LZ4, compression.ZLIB, compression.NO_COMPRESSION]\n)\ndef test_compress_decompress(compress_scheme):\n    if compress_scheme == compression.LZ4:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    elif compress_scheme == compression.ZLIB:\n        compression._apply_compress_scheme = compression.apply_zlib_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    original = msgpack_lib.dumps([1, 2, 3])\n    compressed = compression._compress(original)\n    decompressed = compression._decompress(compressed)\n    assert type(compressed) == bytes\n    assert original == decompressed\n\n\n@pytest.mark.parametrize(\n    ""compress_scheme"", [compression.LZ4, compression.ZLIB, compression.NO_COMPRESSION]\n)\ndef test_compressed_serde(compress_scheme):\n    if compress_scheme == compression.LZ4:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    elif compress_scheme == compression.ZLIB:\n        compression._apply_compress_scheme = compression.apply_zlib_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    # using numpy.ones because numpy.random.random is not compressed.\n    arr = numpy.ones((100, 100))\n\n    arr_serialized = syft.serde.serialize(arr)\n\n    arr_serialized_deserialized = syft.serde.deserialize(arr_serialized)\n    assert numpy.array_equal(arr, arr_serialized_deserialized)\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_dict(compress):\n    # Test with integers\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n    _dict = {1: 1, 2: 2, 3: 3}\n    dict_serialized = syft.serde.serialize(_dict)\n    dict_serialized_deserialized = syft.serde.deserialize(dict_serialized)\n    assert _dict == dict_serialized_deserialized\n\n    # Test with strings\n    _dict = {""one"": 1, ""two"": 2, ""three"": 3}\n    dict_serialized = syft.serde.serialize(_dict)\n    dict_serialized_deserialized = syft.serde.deserialize(dict_serialized)\n    assert _dict == dict_serialized_deserialized\n\n    # Test with a complex data structure\n    tensor_one = Tensor(numpy.random.random((100, 100)))\n    tensor_two = Tensor(numpy.random.random((100, 100)))\n    _dict = {0: tensor_one, 1: tensor_two}\n    dict_serialized = syft.serde.serialize(_dict)\n    dict_serialized_deserialized = syft.serde.deserialize(dict_serialized)\n    # `assert dict_serialized_deserialized == _dict` does not work, therefore it\'s split\n    # into 3 assertions\n    assert type(dict_serialized_deserialized) == type(_dict)\n    assert (dict_serialized_deserialized[0] == tensor_one).all()\n    assert (dict_serialized_deserialized[1] == tensor_two).all()\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_range_serde(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    _range = range(1, 2, 3)\n\n    range_serialized = syft.serde.serialize(_range)\n    range_serialized_deserialized = syft.serde.deserialize(range_serialized)\n\n    assert _range == range_serialized_deserialized\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_list(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    # Test with integers\n    _list = [1, 2]\n    list_serialized = syft.serde.serialize(_list)\n    list_serialized_deserialized = syft.serde.deserialize(list_serialized)\n    assert _list == list_serialized_deserialized\n\n    # Test with strings\n    _list = [""hello"", ""world""]\n    list_serialized = syft.serde.serialize(_list)\n    list_serialized_deserialized = syft.serde.deserialize(list_serialized)\n    assert _list == list_serialized_deserialized\n\n    # Test with a complex data structure\n    tensor_one = Tensor(numpy.ones((100, 100)))\n    tensor_two = Tensor(numpy.ones((100, 100)) * 2)\n    _list = (tensor_one, tensor_two)\n\n    list_serialized = syft.serde.serialize(_list)\n    if compress:\n        assert list_serialized[0] == compression.LZ4\n    else:\n        assert list_serialized[0] == compression.NO_COMPRESSION\n\n    list_serialized_deserialized = syft.serde.deserialize(list_serialized)\n    # `assert list_serialized_deserialized == _list` does not work, therefore it\'s split\n    # into 3 assertions\n    assert type(list_serialized_deserialized) == type(_list)\n    assert (list_serialized_deserialized[0] == tensor_one).all()\n    assert (list_serialized_deserialized[1] == tensor_two).all()\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_set(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    # Test with integers\n    _set = {1, 2}\n    set_serialized = syft.serde.serialize(_set)\n\n    set_serialized_deserialized = syft.serde.deserialize(set_serialized)\n    assert _set == set_serialized_deserialized\n\n    # Test with strings\n    _set = {""hello"", ""world""}\n    set_serialized = syft.serde.serialize(_set)\n    set_serialized_deserialized = syft.serde.deserialize(set_serialized)\n    assert _set == set_serialized_deserialized\n\n    # Test with a complex data structure\n    tensor_one = Tensor(numpy.ones((100, 100)))\n    tensor_two = Tensor(numpy.ones((100, 100)) * 2)\n    _set = (tensor_one, tensor_two)\n\n    set_serialized = syft.serde.serialize(_set)\n    if compress:\n        assert set_serialized[0] == compression.LZ4\n    else:\n        assert set_serialized[0] == compression.NO_COMPRESSION\n\n    set_serialized_deserialized = syft.serde.deserialize(set_serialized)\n    # `assert set_serialized_deserialized == _set` does not work, therefore it\'s split\n    # into 3 assertions\n    assert type(set_serialized_deserialized) == type(_set)\n    assert (set_serialized_deserialized[0] == tensor_one).all()\n    assert (set_serialized_deserialized[1] == tensor_two).all()\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_slice(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    s = slice(0, 100, 2)\n    x = numpy.random.rand(100)\n    s_serialized = syft.serde.serialize(s)\n    s_serialized_deserialized = syft.serde.deserialize(s_serialized)\n\n    assert type(s) == type(s_serialized_deserialized)\n    assert (x[s] == x[s_serialized_deserialized]).all()\n\n    s = slice(40, 50)\n    x = numpy.random.rand(100)\n    s_serialized = syft.serde.serialize(s)\n    s_serialized_deserialized = syft.serde.deserialize(s_serialized)\n\n    assert type(s) == type(s_serialized_deserialized)\n    assert (x[s] == x[s_serialized_deserialized]).all()\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_float(compress):\n    if compress:\n        compression._apply_compress_scheme = compression.apply_lz4_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    x = 0.5\n    y = 1.5\n\n    x_serialized = syft.serde.serialize(x)\n    x_serialized_deserialized = syft.serde.deserialize(x_serialized)\n\n    y_serialized = syft.serde.serialize(y)\n    y_serialized_deserialized = syft.serde.deserialize(y_serialized)\n\n    assert x_serialized_deserialized == x\n    assert y_serialized_deserialized == y\n\n\n@pytest.mark.parametrize(\n    ""compress, compress_scheme"",\n    [\n        (True, compression.LZ4),\n        (False, compression.LZ4),\n        (True, compression.ZLIB),\n        (False, compression.ZLIB),\n        (True, compression.NO_COMPRESSION),\n        (False, compression.NO_COMPRESSION),\n    ],\n)\ndef test_hooked_tensor(compress, compress_scheme):\n    if compress:\n        if compress_scheme == compression.LZ4:\n            compression._apply_compress_scheme = compression.apply_lz4_compression\n        elif compress_scheme == compression.ZLIB:\n            compression._apply_compress_scheme = compression.apply_zlib_compression\n        else:\n            compression._apply_compress_scheme = compression.apply_no_compression\n    else:\n        compression._apply_compress_scheme = compression.apply_no_compression\n\n    t = Tensor(numpy.ones((100, 100)))\n    t_serialized = syft.serde.serialize(t)\n    assert (\n        t_serialized[0] == compress_scheme\n        if compress\n        else t_serialized[0] == compression.NO_COMPRESSION\n    )\n    t_serialized_deserialized = syft.serde.deserialize(t_serialized)\n    assert (t == t_serialized_deserialized).all()\n\n\ndef test_pointer_tensor(hook, workers):\n    compression._apply_compress_scheme = compression.apply_no_compression\n    t = PointerTensor(\n        id=1000, location=workers[""alice""], owner=workers[""alice""], id_at_location=12345\n    )\n    t_serialized = syft.serde.serialize(t)\n    t_serialized_deserialized = syft.serde.deserialize(t_serialized)\n    assert t.id == t_serialized_deserialized.id\n    assert t.location.id == t_serialized_deserialized.location.id\n    assert t.id_at_location == t_serialized_deserialized.id_at_location\n\n\n@pytest.mark.parametrize(""id"", [1000, ""1000""])\ndef test_pointer_tensor_detail(id):\n    alice = syft.VirtualWorker(syft.torch.hook, id=id)\n    x = torch.tensor([1, -1, 3, 4])\n    x_ptr = x.send(alice)\n    x_ptr = 2 * x_ptr\n    x_back = x_ptr.get()\n    assert (x_back == 2 * x).all()\n\n\n@pytest.mark.parametrize(\n    ""tensor"",\n    [\n        (torch.tensor(numpy.ones((10, 10)), requires_grad=False)),\n        (torch.tensor([[0.25, 1.5], [0.15, 0.25], [1.25, 0.5]], requires_grad=True)),\n        (torch.randint(low=0, high=10, size=[3, 7], requires_grad=False)),\n    ],\n)\ndef test_numpy_tensor_serde(tensor):\n    compression._apply_compress_scheme = compression.apply_lz4_compression\n\n    serde._serialize_tensor = syft.serde.msgpack.torch_serde.numpy_tensor_serializer\n    serde._deserialize_tensor = syft.serde.msgpack.torch_serde.numpy_tensor_deserializer\n\n    tensor_serialized = syft.serde.serialize(tensor)\n    assert tensor_serialized[0] != compression.NO_COMPRESSION\n    tensor_deserialized = syft.serde.deserialize(tensor_serialized)\n\n    # Back to Pytorch serializer\n    serde._serialize_tensor = syft.serde.msgpack.torch_serde.torch_tensor_serializer\n    serde._deserialize_tensor = syft.serde.msgpack.torch_serde.torch_tensor_deserializer\n\n    assert torch.eq(tensor_deserialized, tensor).all()\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_additive_sharing_tensor_serde(compress, workers):\n    alice, bob, james, me = workers[""alice""], workers[""bob""], workers[""james""], workers[""me""]\n\n    x = torch.tensor([[3.1, 4.3]]).fix_prec().share(alice, bob, crypto_provider=james)\n\n    additive_sharing_tensor = x.child.child\n    data = AdditiveSharingTensor.simplify(me, additive_sharing_tensor)\n    additive_sharing_tensor_reconstructed = AdditiveSharingTensor.detail(me, data)\n\n    assert additive_sharing_tensor_reconstructed.field == additive_sharing_tensor.field\n\n    assert (\n        additive_sharing_tensor_reconstructed.child.keys() == additive_sharing_tensor.child.keys()\n    )\n\n\n@pytest.mark.parametrize(""compress"", [True, False])\ndef test_fixed_precision_tensor_serde(compress, workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    x = (\n        torch.tensor([[3.1, 4.3]])\n        .fix_prec(base=12, precision_fractional=5)\n        .share(alice, bob, crypto_provider=james)\n    )\n\n    serialized_x = syft.serde.serialize(x)\n    deserialized_x = syft.serde.deserialize(serialized_x)\n\n    assert x.id == deserialized_x.child.id\n    assert x.child.field == deserialized_x.child.field\n    assert x.child.kappa == deserialized_x.child.kappa\n    assert x.child.precision_fractional == deserialized_x.child.precision_fractional\n    assert x.child.base == deserialized_x.child.base\n\n\ndef test_serde_object_wrapper_int():\n    obj = 4\n    obj_wrapper = ObjectWrapper(obj, id=100)\n    msg = syft.serde.serialize(obj_wrapper)\n\n    obj_wrapper_received = syft.serde.deserialize(msg)\n\n    assert obj_wrapper.obj == obj_wrapper_received.obj\n    assert obj_wrapper.id == obj_wrapper_received.id\n\n\n@pytest.mark.skipif(\n    torch.__version__ >= ""1.1"",\n    reason=""bug in pytorch version 1.1.0, jit.trace returns raw C function"",\n)\ndef test_serialize_and_deserialize_torch_scriptmodule():  # pragma: no cover\n    @torch.jit.script\n    def foo(x):\n        return x + 2\n\n    bin_message = torch_serde._simplify_script_module(foo)\n    foo_loaded = torch_serde._detail_script_module(None, bin_message)\n\n    assert foo.code == foo_loaded.code\n\n\n@pytest.mark.skipif(\n    torch.__version__ >= ""1.1"",\n    reason=""bug in pytorch version 1.1.0, jit.trace returns raw C function"",\n)\ndef test_torch_jit_script_module_serde():  # pragma: no cover\n    @torch.jit.script\n    def foo(x):\n        return x + 2\n\n    msg = syft.serde.serialize(foo)\n    foo_received = syft.serde.deserialize(msg)\n\n    assert foo.code == foo_received.code\n\n\ndef test_serde_virtual_worker(hook):\n    virtual_worker = syft.VirtualWorker(hook=hook, id=""deserialized_worker1"")\n    # Populate worker\n    tensor1, tensor2 = torch.tensor([1.0, 2.0]), torch.tensor([0.0])\n    ptr1, ptr2 = tensor1.send(virtual_worker), tensor2.send(virtual_worker)\n\n    serialized_worker = syft.serde.serialize(virtual_worker, force_full_simplification=False)\n    deserialized_worker = syft.serde.deserialize(serialized_worker)\n\n    assert virtual_worker.id == deserialized_worker.id\n\n\ndef test_full_serde_virtual_worker(hook):\n    virtual_worker = syft.VirtualWorker(hook=hook, id=""deserialized_worker2"")\n    # Populate worker\n    tensor1, tensor2 = torch.tensor([1.0, 2.0]), torch.tensor([0.0])\n    ptr1, ptr2 = tensor1.send(virtual_worker), tensor2.send(virtual_worker)\n\n    serialized_worker = syft.serde.serialize(virtual_worker, force_full_simplification=True)\n\n    deserialized_worker = syft.serde.deserialize(serialized_worker)\n\n    assert virtual_worker.id == deserialized_worker.id\n    assert virtual_worker.auto_add == deserialized_worker.auto_add\n    assert len(deserialized_worker.object_store._tensors) == 2\n    assert tensor1.id in deserialized_worker.object_store._tensors\n    assert tensor2.id in deserialized_worker.object_store._tensors\n\n\ndef test_serde_object_wrapper_traced_module():\n\n    data = torch.tensor([[-1, 2.0], [0, 1.1], [-1, 2.1], [0, 1.2]])\n\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = torch.nn.Linear(2, 3)\n\n        def forward(self, x):\n            x = torch.nn.functional.relu(self.fc1(x))\n            return x\n\n    obj = torch.jit.trace(Net(), data)\n\n    obj_wrapper = ObjectWrapper(obj, id=200)\n    msg = syft.serde.serialize(obj_wrapper)\n\n    obj_wrapper_received = syft.serde.deserialize(msg)\n\n    pred_before = obj(data)\n\n    pred_after = obj_wrapper_received.obj(data)\n\n    assert (pred_before == pred_after).all()\n    assert obj_wrapper.id == obj_wrapper_received.id\n\n\ndef test_no_simplifier_found(workers):\n    """"""Test that types that can not be simplified are cached.""""""\n    me = workers[""me""]\n    # Clean cache.\n    msgpack.serde._no_simplifiers_found = set()\n    x = bytes(5)\n    assert type(x) not in msgpack.serde.msgpack_global_state._no_simplifiers_found\n    _ = msgpack.serde._simplify(me, x)\n    assert type(x) in msgpack.serde.msgpack_global_state._no_simplifiers_found\n\n\ndef test_external_lib_msgpack():\n    example1 = SerializableDummyClass(""test"")\n    ser1 = syft.serde.serialize(example1)\n    result1 = syft.serde.deserialize(ser1)\n    assert example1.value == result1.value\n\n    saved_attr = SerializableDummyClass.get_msgpack_code\n    delattr(SerializableDummyClass, ""get_msgpack_code"")\n    syft.serde.msgpack.serde.msgpack_global_state.stale_state = True\n\n    with pytest.raises(Exception) as e:\n        example2 = SerializableDummyClass(""test"")\n        _ = syft.serde.serialize(example2)\n\n    assert isinstance(e.value, syft.exceptions.UndefinedProtocolTypeError)\n    setattr(SerializableDummyClass, ""get_msgpack_code"", saved_attr)\n    syft.serde.msgpack.serde.msgpack_global_state.stale_state = True\n'"
test/serde/msgpack/test_msgpack_serde_full.py,17,"b'from collections import OrderedDict\nimport pytest\nimport numpy\nimport torch\nfrom functools import partial\n\nimport syft\nfrom syft.serde import msgpack\nfrom test.serde.serde_helpers import *\n\nsamples = OrderedDict()\n\n# Native\nsamples[float] = make_float\nsamples[int] = make_int\nsamples[dict] = make_dict\nsamples[tuple] = make_tuple\nsamples[list] = make_list\nsamples[set] = make_set\nsamples[slice] = make_slice\nsamples[str] = make_str\nsamples[range] = make_range\nsamples[type(Ellipsis)] = make_ellipsis\nsamples[type] = make_type\n\n# Numpy\nsamples[numpy.float32] = partial(make_numpy_number, numpy.float32)\nsamples[numpy.float64] = partial(make_numpy_number, numpy.float64)\nsamples[numpy.int32] = partial(make_numpy_number, numpy.int32)\nsamples[numpy.int64] = partial(make_numpy_number, numpy.int64)\nsamples[numpy.ndarray] = make_numpy_ndarray\n\n# PyTorch\nsamples[torch.device] = make_torch_device\nsamples[torch.dtype] = make_torch_dtype\nsamples[torch.jit.ScriptModule] = make_torch_scriptmodule\nsamples[torch.jit.ScriptFunction] = make_torch_scriptfunction\nsamples[torch.jit.TopLevelTracedModule] = make_torch_topleveltracedmodule\nsamples[torch.memory_format] = make_torch_memoryformat\nsamples[torch.nn.Parameter] = make_torch_parameter\nsamples[torch.Tensor] = make_torch_tensor\nsamples[torch.Size] = make_torch_size\n\n# PySyft\nsamples[syft.exceptions.GetNotPermittedError] = make_getnotpermittederror\nsamples[syft.exceptions.ResponseSignatureError] = make_responsesignatureerror\n\nsamples[syft.execution.communication.CommunicationAction] = make_communication_action\nsamples[syft.execution.computation.ComputationAction] = make_computation_action\nsamples[syft.execution.placeholder.PlaceHolder] = make_placeholder\nsamples[syft.execution.placeholder_id.PlaceholderId] = make_placeholder_id\nsamples[syft.execution.plan.NestedTypeWrapper] = make_nested_type_wrapper\nsamples[syft.execution.plan.Plan] = make_plan\nsamples[syft.execution.protocol.Protocol] = make_protocol\nsamples[syft.execution.role.Role] = make_role\nsamples[syft.execution.role_assignments.RoleAssignments] = make_role_assignments\nsamples[syft.execution.state.State] = make_state\n\nsamples[syft.frameworks.torch.fl.dataset.BaseDataset] = make_basedataset\nsamples[syft.frameworks.torch.tensors.decorators.logging.LoggingTensor] = make_loggingtensor\nsamples[\n    syft.frameworks.torch.tensors.interpreters.additive_shared.AdditiveSharingTensor\n] = make_additivesharingtensor\nsamples[syft.frameworks.torch.tensors.interpreters.autograd.AutogradTensor] = make_autogradtensor\nsamples[syft.frameworks.torch.tensors.interpreters.gradients_core.GradFunc] = make_gradfn\nsamples[syft.frameworks.torch.tensors.interpreters.paillier.PaillierTensor] = make_paillier\nsamples[\n    syft.frameworks.torch.tensors.interpreters.precision.FixedPrecisionTensor\n] = make_fixedprecisiontensor\nsamples[syft.frameworks.torch.tensors.interpreters.private.PrivateTensor] = make_privatetensor\n\nsamples[syft.generic.pointers.multi_pointer.MultiPointerTensor] = make_multipointertensor\nsamples[syft.generic.pointers.object_pointer.ObjectPointer] = make_objectpointer\nsamples[syft.generic.pointers.object_wrapper.ObjectWrapper] = make_objectwrapper\nsamples[syft.generic.pointers.pointer_tensor.PointerTensor] = make_pointertensor\nsamples[syft.generic.pointers.pointer_plan.PointerPlan] = make_pointerplan\nsamples[syft.generic.pointers.pointer_dataset.PointerDataset] = make_pointerdataset\nsamples[syft.generic.string.String] = make_string\n\nsamples[syft.messaging.message.ForceObjectDeleteMessage] = make_forceobjectdeletemessage\nsamples[syft.messaging.message.GetShapeMessage] = make_getshapemessage\nsamples[syft.messaging.message.IsNoneMessage] = make_isnonemessage\nsamples[syft.messaging.message.ObjectMessage] = make_objectmessage\nsamples[syft.messaging.message.ObjectRequestMessage] = make_objectrequestmessage\nsamples[syft.messaging.message.PlanCommandMessage] = make_plancommandmessage\nsamples[syft.messaging.message.SearchMessage] = make_searchmessage\nsamples[syft.messaging.message.TensorCommandMessage] = make_tensor_command_message\nsamples[syft.messaging.message.WorkerCommandMessage] = make_workercommandmessage\n\nsamples[syft.workers.virtual.VirtualWorker] = make_virtual_worker\n\n# testing\nsamples[SerializableDummyClass] = make_serializable_dummy_class\n\n\ndef test_serde_coverage():\n    """"""Checks all types in serde are tested""""""\n    for cls, _ in msgpack.serde.msgpack_global_state.simplifiers.items():\n        has_sample = cls in samples\n        assert has_sample, f""Serde for {cls} is not tested""\n\n\n@pytest.mark.parametrize(""cls"", samples)\ndef test_serde_roundtrip(cls, workers, hook, start_remote_worker):\n    """"""Checks that values passed through serialization-deserialization stay same""""""\n    serde_worker = syft.VirtualWorker(id=f""serde-worker-{cls.__name__}"", hook=hook, auto_add=False)\n    workers[""serde_worker""] = serde_worker\n    _samples = samples[cls](\n        workers=workers,\n        hook=hook,\n        start_remote_worker=start_remote_worker,\n        port=9000,\n        id=""roundtrip"",\n    )\n    for sample in _samples:\n        _simplify = (\n            msgpack.serde._simplify\n            if not sample.get(""forced"", False)\n            else msgpack.serde._force_full_simplify\n        )\n        serde_worker.framework = sample.get(""framework"", torch)\n        obj = sample.get(""value"")\n        simplified_obj = _simplify(serde_worker, obj)\n        if not isinstance(obj, Exception):\n            detailed_obj = msgpack.serde._detail(serde_worker, simplified_obj)\n        else:\n            try:\n                msgpack.serde._detail(serde_worker, simplified_obj)\n            except Exception as e:\n                detailed_obj = e\n\n        if sample.get(""cmp_detailed"", None):\n            # Custom detailed objects comparison function.\n            assert sample.get(""cmp_detailed"")(detailed_obj, obj) is True\n        else:\n            assert type(detailed_obj) == type(obj)\n            assert detailed_obj == obj\n\n\n@pytest.mark.parametrize(""cls"", samples)\ndef test_serde_simplify(cls, workers, hook, start_remote_worker):\n    """"""Checks that simplified structures match expected""""""\n    serde_worker = syft.VirtualWorker(id=f""serde-worker-{cls.__name__}"", hook=hook, auto_add=False)\n    workers[""serde_worker""] = serde_worker\n    _samples = samples[cls](\n        workers=workers,\n        hook=hook,\n        start_remote_worker=start_remote_worker,\n        port=9001,\n        id=""simplify"",\n    )\n    for sample in _samples:\n        obj, expected_simplified_obj = sample.get(""value""), sample.get(""simplified"")\n        _simplify = (\n            msgpack.serde._simplify\n            if not sample.get(""forced"", False)\n            else msgpack.serde._force_full_simplify\n        )\n        serde_worker.framework = sample.get(""framework"", torch)\n        simplified_obj = _simplify(serde_worker, obj)\n\n        if sample.get(""cmp_simplified"", None):\n            # Custom simplified objects comparison function.\n            assert sample.get(""cmp_simplified"")(simplified_obj, expected_simplified_obj) is True\n        else:\n            assert simplified_obj == expected_simplified_obj\n'"
test/serde/protobuf/test_protobuf_serde.py,7,"b'import pytest\n\nimport torch\n\nimport syft\nfrom syft.serde import protobuf\nfrom syft.serde.torch.serde import TORCH_STR_DTYPE\n\nfrom test.serde.serde_helpers import *\n\n\ndtypes = [\n    ""uint8"",\n    ""int8"",\n    ""int16"",\n    ""int32"",\n    ""int64"",\n    ""float16"",\n    ""float32"",\n    ""float64"",\n    ""bool"",\n    ""bfloat16"",\n]\nquantized_dtypes = [""qint8"", ""quint8"", ""qint32""]\ncomplex_types = []  # not yet implemented in PyTorch\n\n\n@pytest.mark.parametrize(""str_dtype"", dtypes)\ndef test_protobuf_serde_tensor_roundtrip(str_dtype):\n    """"""Checks that tensors passed through serialization-deserialization stay same""""""\n\n    def compare(roundtrip, original):\n        assert type(roundtrip) == torch.Tensor\n        assert roundtrip.dtype == original.dtype\n\n        # PyTorch doesn\'t implement equality checking for bfloat16, so convert to float\n        if original.dtype == torch.bfloat16:\n            roundtrip = roundtrip.float()\n            original = original.float()\n\n        # PyTorch doesn\'t implement equality checking for float16, so use numpy\n        assert numpy.array_equal(roundtrip.data.numpy(), original.data.numpy())\n        return True\n\n    serde_worker = syft.hook.local_worker\n    original_framework = serde_worker.framework\n    serde_worker.framework = None\n\n    tensor = torch.rand([10, 10]) * 16\n    tensor = tensor.to(TORCH_STR_DTYPE[str_dtype])\n\n    protobuf_tensor = protobuf.serde._bufferize(serde_worker, tensor)\n    roundtrip_tensor = protobuf.serde._unbufferize(serde_worker, protobuf_tensor)\n\n    serde_worker.framework = original_framework\n\n    assert compare(roundtrip_tensor, tensor) is True\n\n\n# quantized types can\'t be created by conversion with `tensor.to()`\n@pytest.mark.parametrize(""str_dtype"", quantized_dtypes)\ndef test_protobuf_serde_tensor_roundtrip_quantized(str_dtype):\n    """"""Checks that tensors passed through serialization-deserialization stay same""""""\n\n    def compare(roundtrip, original):\n        assert type(roundtrip) == torch.Tensor\n        assert roundtrip.dtype == original.dtype\n        roundtrip_np = roundtrip.dequantize().numpy()\n        original_np = original.dequantize().numpy()\n        # PyTorch does implement equality checking for float tensors, but\n        # quantized tensors may not be exactly the same after a round trip\n        # plus dequantizing so use numpy close checking with a tolerance\n        assert numpy.allclose(roundtrip_np, original_np, atol=2 / original.q_scale())\n        return True\n\n    serde_worker = syft.hook.local_worker\n    original_framework = serde_worker.framework\n    serde_worker.framework = None\n\n    tensor = torch.rand([10, 10]) * 16\n    tensor = torch.quantize_per_tensor(tensor, 0.1, 10, TORCH_STR_DTYPE[str_dtype])\n\n    protobuf_tensor = protobuf.serde._bufferize(serde_worker, tensor)\n    roundtrip_tensor = protobuf.serde._unbufferize(serde_worker, protobuf_tensor)\n\n    serde_worker.framework = original_framework\n\n    assert compare(roundtrip_tensor, tensor) is True\n'"
test/serde/protobuf/test_protobuf_serde_full.py,12,"b'from collections import OrderedDict\nimport pytest\nimport torch\n\nimport syft\nfrom syft.serde import protobuf\nfrom test.serde.serde_helpers import *\n\n# Dictionary containing test samples functions\nsamples = OrderedDict()\n\n# Native\nsamples[type(None)] = make_none\nsamples[type] = make_type\n\n# PyTorch\nsamples[torch.device] = make_torch_device\nsamples[torch.jit.ScriptModule] = make_torch_scriptmodule\nsamples[torch.jit.ScriptFunction] = make_torch_scriptfunction\nsamples[torch.jit.TopLevelTracedModule] = make_torch_topleveltracedmodule\nsamples[torch.nn.Parameter] = make_torch_parameter\nsamples[torch.Tensor] = make_torch_tensor\nsamples[torch.Size] = make_torch_size\nsamples[torch.memory_format] = make_torch_memoryformat\nsamples[torch.dtype] = make_torch_dtype\n# PySyft\nsamples[\n    syft.frameworks.torch.tensors.interpreters.additive_shared.AdditiveSharingTensor\n] = make_additivesharingtensor\nsamples[syft.frameworks.torch.fl.dataset.BaseDataset] = make_basedataset\n\nsamples[\n    syft.frameworks.torch.tensors.interpreters.precision.FixedPrecisionTensor\n] = make_fixedprecisiontensor\nsamples[syft.execution.placeholder.PlaceHolder] = make_placeholder\nsamples[syft.execution.computation.ComputationAction] = make_computation_action\nsamples[syft.execution.communication.CommunicationAction] = make_communication_action\nsamples[syft.execution.plan.Plan] = make_plan\nsamples[syft.execution.protocol.Protocol] = make_protocol\nsamples[syft.execution.role.Role] = make_role\nsamples[syft.execution.state.State] = make_state\nsamples[syft.execution.placeholder_id.PlaceholderId] = make_placeholder_id\nsamples[syft.execution.plan.NestedTypeWrapper] = make_nested_type_wrapper\nsamples[syft.generic.pointers.pointer_tensor.PointerTensor] = make_pointertensor\nsamples[syft.generic.pointers.pointer_dataset.PointerDataset] = make_pointerdataset\nsamples[syft.generic.string.String] = make_string\n\n# Syft Messages\nsamples[syft.messaging.message.ObjectMessage] = make_objectmessage\nsamples[syft.messaging.message.TensorCommandMessage] = make_tensor_command_message\nsamples[syft.messaging.message.ObjectRequestMessage] = make_objectrequestmessage\nsamples[syft.messaging.message.IsNoneMessage] = make_isnonemessage\nsamples[syft.messaging.message.GetShapeMessage] = make_getshapemessage\nsamples[syft.messaging.message.ForceObjectDeleteMessage] = make_forceobjectdeletemessage\nsamples[syft.messaging.message.SearchMessage] = make_searchmessage\nsamples[syft.messaging.message.PlanCommandMessage] = make_plancommandmessage\n\n# TODO: this should be fixed in a future PR.\n# samples[syft.messaging.message.WorkerCommandMessage] = make_workercommandmessage\n\n\ndef test_serde_coverage():\n    """"""Checks all types in serde are tested""""""\n    for cls, _ in protobuf.serde.protobuf_global_state.bufferizers.items():\n        has_sample = cls in samples\n        assert has_sample, f""Serde for {cls} is not tested""\n\n\n@pytest.mark.parametrize(""cls"", samples)\ndef test_serde_roundtrip_protobuf(cls, workers, hook):\n    """"""Checks that values passed through serialization-deserialization stay same""""""\n    serde_worker = syft.VirtualWorker(id=f""serde-worker-{cls.__name__}"", hook=hook, auto_add=False)\n    original_framework = serde_worker.framework\n    workers[""serde_worker""] = serde_worker\n    _samples = samples[cls](workers=workers)\n    for sample in _samples:\n        _to_protobuf = (\n            protobuf.serde._bufferize\n            if not sample.get(""forced"", False)\n            else protobuf.serde._force_full_bufferize\n        )\n        serde_worker.framework = sample.get(""framework"", torch)\n        obj = sample.get(""value"")\n        protobuf_obj = _to_protobuf(serde_worker, obj)\n        roundtrip_obj = None\n        if not isinstance(obj, Exception):\n            roundtrip_obj = protobuf.serde._unbufferize(serde_worker, protobuf_obj)\n\n        serde_worker.framework = original_framework\n\n        if sample.get(""cmp_detailed"", None):\n            # Custom detailed objects comparison function.\n            assert sample.get(""cmp_detailed"")(roundtrip_obj, obj)\n        else:\n            assert type(roundtrip_obj) == type(obj)\n            assert roundtrip_obj == obj\n'"
test/torch/differential_privacy/__init__.py,0,b''
test/torch/differential_privacy/test_pate.py,3,"b'import numpy as np\n\nimport torch\n\nfrom syft.frameworks.torch.dp import pate\n\nnp.random.seed(0)\n\n\ndef test_base_dataset():\n\n    num_teachers, num_examples, num_labels = (100, 50, 10)\n    preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int)  # fake preds\n\n    indices = (np.random.rand(num_examples) * num_labels).astype(int)  # true answers\n\n    preds[:, 0:10] *= 0\n\n    data_dep_eps, data_ind_eps = pate.perform_analysis(\n        teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5\n    )\n\n    assert data_dep_eps < data_ind_eps\n\n\ndef test_base_dataset_torch():\n\n    num_teachers, num_examples, num_labels = (100, 50, 10)\n    preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int)  # fake preds\n\n    indices = (np.random.rand(num_examples) * num_labels).astype(int)  # true answers\n\n    preds[:, 0:10] *= 0\n\n    data_dep_eps, data_ind_eps = pate.perform_analysis_torch(\n        preds, indices, noise_eps=0.1, delta=1e-5\n    )\n\n    assert data_dep_eps < data_ind_eps\n\n\ndef test_torch_ref_match():\n\n    # Verify if the torch implementation values match the original Numpy implementation.\n\n    num_teachers, num_examples, num_labels = (100, 50, 10)\n    preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int)  # fake preds\n\n    indices = (np.random.rand(num_examples) * num_labels).astype(int)  # true answers\n\n    preds[:, 0:10] *= 0\n\n    data_dep_eps, data_ind_eps = pate.perform_analysis_torch(\n        preds, indices, noise_eps=0.1, delta=1e-5\n    )\n\n    data_dep_eps_ref, data_ind_eps_ref = pate.perform_analysis(\n        preds, indices, noise_eps=0.1, delta=1e-5\n    )\n\n    assert torch.isclose(data_dep_eps, torch.tensor(data_dep_eps_ref, dtype=torch.float32))\n    assert torch.isclose(data_ind_eps, torch.tensor(data_ind_eps_ref, dtype=torch.float32))\n'"
test/torch/federated/__init__.py,0,b''
test/torch/federated/test_dataloader.py,0,"b'import torch as th\nimport syft as sy\nfrom syft.frameworks.torch import fl\n\n\ndef test_federated_dataloader(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    datasets = [\n        fl.BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        fl.BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6])).send(alice),\n    ]\n    fed_dataset = sy.FederatedDataset(datasets)\n\n    fdataloader = sy.FederatedDataLoader(fed_dataset, batch_size=2)\n    counter = 0\n    for batch_idx, (data, target) in enumerate(fdataloader):\n        counter += 1\n\n    assert counter == len(fdataloader), f""{counter} == {len(fdataloader)}""\n\n    fdataloader = sy.FederatedDataLoader(fed_dataset, batch_size=2, drop_last=True)\n    counter = 0\n    for batch_idx, (data, target) in enumerate(fdataloader):\n        counter += 1\n\n    assert counter == len(fdataloader), f""{counter} == {len(fdataloader)}""\n\n\ndef test_federated_dataloader_shuffle(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    datasets = [\n        fl.BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        fl.BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6])).send(alice),\n    ]\n    fed_dataset = sy.FederatedDataset(datasets)\n\n    fdataloader = sy.FederatedDataLoader(fed_dataset, batch_size=2, shuffle=True)\n    for epoch in range(3):\n        counter = 0\n        for batch_idx, (data, target) in enumerate(fdataloader):\n            if counter < 1:  # one batch for bob, two batches for alice (batch_size == 2)\n                assert (\n                    data.location.id == ""bob""\n                ), f""id should be bob, counter = {counter}, epoch = {epoch}""\n            else:\n                assert (\n                    data.location.id == ""alice""\n                ), f""id should be alice, counter = {counter}, epoch = {epoch}""\n            counter += 1\n        assert counter == len(fdataloader), f""{counter} == {len(fdataloader)}""\n\n    num_iterators = 2\n    fdataloader = sy.FederatedDataLoader(\n        fed_dataset, batch_size=2, num_iterators=num_iterators, shuffle=True\n    )\n    assert (\n        fdataloader.num_iterators == num_iterators - 1\n    ), f""{fdataloader.num_iterators} == {num_iterators - 1}""\n\n\ndef test_federated_dataloader_num_iterators(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    james = workers[""james""]\n    datasets = [\n        fl.BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        fl.BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6])).send(alice),\n        fl.BaseDataset(th.tensor([7, 8, 9, 10]), th.tensor([7, 8, 9, 10])).send(james),\n    ]\n\n    fed_dataset = sy.FederatedDataset(datasets)\n    num_iterators = len(datasets)\n    fdataloader = sy.FederatedDataLoader(\n        fed_dataset, batch_size=2, num_iterators=num_iterators, shuffle=True\n    )\n    assert (\n        fdataloader.num_iterators == num_iterators - 1\n    ), f""{fdataloader.num_iterators} == {num_iterators - 1}""\n    counter = 0\n    for batch_idx, batches in enumerate(fdataloader):\n        assert (\n            len(batches.keys()) == num_iterators - 1\n        ), f""len(batches.keys()) == {num_iterators} - 1""\n        if batch_idx < 1:\n            data_bob, target_bob = batches[bob]\n            assert data_bob.location.id == ""bob"", ""id should be bob, batch_idx = {0}"".format(\n                batch_idx\n            )\n        else:  # bob is replaced by james\n            data_james, target_james = batches[james]\n            assert data_james.location.id == ""james"", ""id should be james, batch_idx = {0}"".format(\n                batch_idx\n            )\n        if batch_idx < 2:\n            data_alice, target_alice = batches[alice]\n            assert data_alice.location.id == ""alice"", ""id should be alice, batch_idx = {0}"".format(\n                batch_idx\n            )\n        counter += 1\n    epochs = num_iterators - 1\n    assert counter * (num_iterators - 1) == epochs * len(\n        fdataloader\n    ), "" == epochs * len(fdataloader)""\n\n\ndef test_federated_dataloader_iter_per_worker(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    james = workers[""james""]\n    datasets = [\n        fl.BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        fl.BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6])).send(alice),\n        fl.BaseDataset(th.tensor([7, 8, 9, 10]), th.tensor([7, 8, 9, 10])).send(james),\n    ]\n\n    fed_dataset = sy.FederatedDataset(datasets)\n    fdataloader = sy.FederatedDataLoader(\n        fed_dataset, batch_size=2, iter_per_worker=True, shuffle=True\n    )\n    nr_workers = len(datasets)\n    assert (\n        fdataloader.num_iterators == nr_workers\n    ), ""num_iterators should be equal to number or workers""\n    for batch_idx, batches in enumerate(fdataloader):\n        assert len(batches.keys()) == nr_workers, ""return a batch for each worker""\n\n\ndef test_federated_dataloader_one_worker(workers):\n    bob = workers[""bob""]\n\n    datasets = [fl.BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6])).send(bob)]\n\n    fed_dataset = sy.FederatedDataset(datasets)\n    num_iterators = len(datasets)\n    fdataloader = sy.FederatedDataLoader(fed_dataset, batch_size=2, shuffle=True)\n    assert fdataloader.num_iterators == 1, f""{fdataloader.num_iterators} == {1}""\n'"
test/torch/federated/test_dataset.py,1,"b'import pytest\nimport torch as th\nimport syft as sy\n\nfrom syft.frameworks.torch.fl import BaseDataset\n\n\ndef test_base_dataset(workers):\n\n    bob = workers[""bob""]\n    inputs = th.tensor([1, 2, 3, 4.0])\n    targets = th.tensor([1, 2, 3, 4.0])\n    dataset = BaseDataset(inputs, targets)\n\n    assert len(dataset) == 4\n    assert dataset[2] == (3, 3)\n\n    dataset = dataset.send(bob)\n    assert dataset.data.location.id == ""bob""\n    assert dataset.targets.location.id == ""bob""\n    assert dataset.location.id == ""bob""\n\n\ndef test_base_dataset_transform():\n\n    inputs = th.tensor([1, 2, 3, 4.0])\n    targets = th.tensor([1, 2, 3, 4.0])\n\n    transform_dataset = BaseDataset(inputs, targets)\n\n    def func(x):\n\n        return x * 2\n\n    transform_dataset.transform(func)\n\n    expected_val = th.tensor([2, 4, 6, 8])\n    transformed_val = [val[0].item() for val in transform_dataset]\n\n    assert expected_val.equal(th.tensor(transformed_val).long())\n\n\ndef test_federated_dataset(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    alice_base_dataset = BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6]))\n    datasets = [\n        BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        alice_base_dataset.send(alice),\n    ]\n\n    fed_dataset = sy.FederatedDataset(datasets)\n\n    assert fed_dataset.workers == [""bob"", ""alice""]\n    assert len(fed_dataset) == 6\n\n    alice_remote_data = fed_dataset.get_dataset(""alice"")\n    assert (alice_remote_data.data == alice_base_dataset.data).all()\n    assert alice_remote_data[2] == (5, 5)\n    assert len(alice_remote_data) == 4\n    assert len(fed_dataset) == 2\n\n    assert isinstance(fed_dataset.__str__(), str)\n\n\ndef test_dataset_to_federate(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    dataset = BaseDataset(th.tensor([1.0, 2, 3, 4, 5, 6]), th.tensor([1.0, 2, 3, 4, 5, 6]))\n\n    fed_dataset = dataset.federate((bob, alice))\n\n    assert isinstance(fed_dataset, sy.FederatedDataset)\n\n    assert fed_dataset.workers == [""bob"", ""alice""]\n    assert fed_dataset[""bob""].location.id == ""bob""\n    assert len(fed_dataset) == 6\n\n\ndef test_federated_dataset_search(workers):\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    grid = sy.PrivateGridNetwork(*[bob, alice])\n\n    train_bob = th.Tensor(th.zeros(1000, 100)).tag(""data"").send(bob)\n    target_bob = th.Tensor(th.zeros(1000, 100)).tag(""target"").send(bob)\n\n    train_alice = th.Tensor(th.zeros(1000, 100)).tag(""data"").send(alice)\n    target_alice = th.Tensor(th.zeros(1000, 100)).tag(""target"").send(alice)\n\n    data = grid.search(""data"")\n    target = grid.search(""target"")\n\n    datasets = [\n        BaseDataset(data[""bob""][0], target[""bob""][0]),\n        BaseDataset(data[""alice""][0], target[""alice""][0]),\n    ]\n\n    fed_dataset = sy.FederatedDataset(datasets)\n    train_loader = sy.FederatedDataLoader(fed_dataset, batch_size=4, shuffle=False, drop_last=False)\n\n    counter = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        counter += 1\n\n    assert counter == len(train_loader), f""{counter} == {len(fed_dataset)}""\n\n\ndef test_abstract_dataset():\n    inputs = th.tensor([1, 2, 3, 4.0])\n    targets = th.tensor([1, 2, 3, 4.0])\n    dataset = BaseDataset(inputs, targets, id=1)\n\n    assert dataset.id == 1\n    assert dataset.description is None\n\n\ndef test_get_dataset(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    alice_base_dataset = BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6]))\n    datasets = [\n        BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        alice_base_dataset.send(alice),\n    ]\n    fed_dataset = sy.FederatedDataset(datasets)\n    dataset = fed_dataset.get_dataset(""alice"")\n\n    assert len(fed_dataset) == 2\n    assert len(dataset) == 4\n\n\ndef test_illegal_get(workers):\n    """"""\n    test getting error message when calling .get() on a\n    dataset that\'s a part of fedratedDataset object\n    """"""\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    alice_base_dataset = BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6]))\n    datasets = [\n        BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        alice_base_dataset.send(alice),\n    ]\n    fed_dataset = sy.FederatedDataset(datasets)\n    with pytest.raises(ValueError):\n        fed_dataset[""alice""].get()\n'"
test/torch/federated/test_utils.py,1,"b'import pytest\n\nimport torch as th\nimport syft as sy\n\nfrom syft.frameworks.torch.fl import utils\nfrom syft.frameworks.torch import fl\n\n\ndef test_extract_batches_per_worker(workers):\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    datasets = [\n        fl.BaseDataset(th.tensor([1, 2]), th.tensor([1, 2])).send(bob),\n        fl.BaseDataset(th.tensor([3, 4, 5, 6]), th.tensor([3, 4, 5, 6])).send(alice),\n    ]\n    fed_dataset = sy.FederatedDataset(datasets)\n\n    fdataloader = sy.FederatedDataLoader(fed_dataset, batch_size=2, shuffle=True)\n\n    batches = utils.extract_batches_per_worker(fdataloader)\n\n    assert len(batches.keys()) == len(\n        datasets\n    ), ""each worker should appear as key in the batches dictionary""\n\n\ndef test_add_model():\n    class Net(th.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = th.nn.Linear(2, 2)\n\n    weight1 = th.tensor([1.0, 2.0, 3.0, 4.0])\n    weight2 = th.tensor([11.0, 22.0, 33.0, 44.0])\n\n    bias1 = th.tensor([-1.0, -2.0])\n    bias2 = th.tensor([1.0, 2.0])\n\n    net1 = Net()\n    params1 = net1.named_parameters()\n    dict_params1 = dict(params1)\n    with th.no_grad():\n        dict_params1[""fc1.weight""].set_(weight1)\n        dict_params1[""fc1.bias""].set_(bias1)\n\n    net2 = Net()\n    params2 = net2.named_parameters()\n    dict_params2 = dict(params2)\n    with th.no_grad():\n        dict_params2[""fc1.weight""].set_(weight2)\n        dict_params2[""fc1.bias""].set_(bias2)\n\n    new_model = utils.add_model(net1, net2)\n\n    assert (new_model.fc1.weight.data == (weight1 + weight2)).all()\n    assert (new_model.fc1.bias.data == (bias1 + bias2)).all()\n\n\n@pytest.mark.skipif(not th.cuda.is_available(), reason=""cuda not available"")\ndef test_add_model_cuda():  # pragma: no cover\n    class Net(th.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = th.nn.Linear(2, 2)\n\n    weight1 = th.tensor([1.0, 2.0, 3.0, 4.0]).cuda()\n    weight2 = th.tensor([11.0, 22.0, 33.0, 44.0]).cuda()\n\n    bias1 = th.tensor([-1.0, -2.0]).cuda()\n    bias2 = th.tensor([1.0, 2.0]).cuda()\n\n    net1 = Net().to(th.device(""cuda""))\n    params1 = net1.named_parameters()\n    dict_params1 = dict(params1)\n    with th.no_grad():\n        dict_params1[""fc1.weight""].set_(weight1)\n        dict_params1[""fc1.bias""].set_(bias1)\n\n    net2 = Net().cuda()\n    params2 = net2.named_parameters()\n    dict_params2 = dict(params2)\n    with th.no_grad():\n        dict_params2[""fc1.weight""].set_(weight2)\n        dict_params2[""fc1.bias""].set_(bias2)\n\n    new_model = utils.add_model(net1, net2)\n\n    assert (new_model.fc1.weight.data == (weight1 + weight2)).all()\n    assert (new_model.fc1.bias.data == (bias1 + bias2)).all()\n\n\ndef test_scale_model():\n    class Net(th.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = th.nn.Linear(2, 2)\n\n    weight1 = th.tensor([1.0, 2.0, 3.0, 4.0])\n\n    bias1 = th.tensor([-1.0, -2.0])\n\n    net1 = Net()\n    params1 = net1.named_parameters()\n    dict_params1 = dict(params1)\n    with th.no_grad():\n        dict_params1[""fc1.weight""].set_(weight1)\n        dict_params1[""fc1.bias""].set_(bias1)\n\n    scale = 2.0\n\n    new_model = utils.scale_model(net1, scale)\n\n    assert (new_model.fc1.weight.data == (weight1 * scale)).all()\n    assert (new_model.fc1.bias.data == (bias1 * scale)).all()\n\n\ndef test_accuracy():\n    pred = th.tensor([[0.95, 0.02, 0.03], [0.3, 0.4, 0.3], [0.0, 0.0, 1.0]])\n\n    target = th.tensor([0.0, 1.0, 2.0])\n\n    acc = utils.accuracy(pred, target)\n\n    assert acc == 1.0\n\n    target = th.tensor([2.0, 0.0, 2.0])\n\n    acc = utils.accuracy(pred, target)\n\n    assert acc == 1.0 / 3.0\n\n\ndef test_federated_avg():\n    class Net(th.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = th.nn.Linear(2, 2)\n\n    net1 = Net()\n    net2 = Net()\n    net3 = Net()\n\n    models = {}\n    models[0] = net1\n    models[1] = net2\n    models[2] = net3\n\n    avg_model = utils.federated_avg(models)\n    assert avg_model != net1\n    assert (avg_model.fc1.weight.data != net1.fc1.weight.data).all()\n    assert (avg_model.fc1.bias.data != net1.fc1.bias.data).all()\n'"
test/torch/hook/__init__.py,0,b''
test/torch/hook/test_hook.py,24,"b'import pytest\nimport torch\nimport syft\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""cuda not available"")\ndef test_to():  # pragma: no cover\n    a = torch.Tensor([1.0, 2.0, 3.0])\n    assert a.is_cuda is False\n    a = a.to(torch.device(""cuda""))\n    assert a.is_cuda is True\n    a = a.to(torch.device(""cpu""))\n    assert a.is_cuda is False\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""cuda not available"")\ndef test_cuda():  # pragma: no cover\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = torch.nn.Linear(2, 3)\n\n        def forward(self, x):\n            x = torch.nn.functional.relu(self.fc1(x))\n            return x\n\n    model = Net()\n    assert model.fc1.weight.is_cuda is False\n    model = model.cuda()\n    assert model.fc1.weight.is_cuda is True\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""cuda not available"")\ndef test_data():  # pragma: no cover\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = torch.nn.Linear(2, 3)\n\n        def forward(self, x):\n            x = torch.nn.functional.relu(self.fc1(x))\n            return x\n\n    model = Net()\n    input = torch.tensor([2.0, 4.0])\n    out_cpu = model(input)\n    assert model.fc1.weight.is_cuda is False\n    model = model.cuda()\n    assert model.fc1.weight.is_cuda is True\n    out_cuda = model(input.cuda())\n    assert (out_cpu - out_cuda.cpu() < 1e-3).all()\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""cuda not available"")\ndef test_param_data():  # pragma: no cover\n    param = torch.nn.Parameter(data=torch.Tensor([2.0, 3.0]))\n    data2 = torch.Tensor([4.0, 5.0]).to(""cuda"")\n    param.data = data2\n    assert (param.data == data2).all()\n    assert param.is_cuda\n\n\ndef test_send_frozen():\n    hook = syft.TorchHook(torch)\n    worker = syft.VirtualWorker(hook, id=""worker"")\n\n    d_in, h, d_out = 1000, 100, 10\n\n    model = torch.nn.Sequential(\n        torch.nn.Linear(d_in, h), torch.nn.ReLU(), torch.nn.Linear(h, d_out)\n    )\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    model.send(worker)\n\n\ndef test_send_partially_frozen():\n    hook = syft.TorchHook(torch)\n    worker = syft.VirtualWorker(hook, id=""worker"")\n\n    d_in, h1, h2, d_out = 1000, 1000, 100, 10\n\n    model = torch.nn.Sequential(\n        torch.nn.Linear(d_in, h1),\n        torch.nn.ReLU(),\n        torch.nn.Linear(h1, h2),\n        torch.nn.ReLU(),\n        torch.nn.Linear(h2, d_out),\n    )\n\n    for layer_idx, param in enumerate(model.parameters()):\n        if layer_idx > 2:  # freezing the first two layers\n            pass\n        param.requires_grad = False\n\n    model.send(worker)\n'"
test/torch/hook/test_hook_args.py,16,"b'from syft.generic.frameworks.hook import hook_args\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nimport torch\nimport numpy as np\n\n\ndef test_build_rule_syft_tensors_and_pointers():\n    pointer = PointerTensor(id=1000, location=""location"", owner=""owner"", garbage_collect_data=False)\n    result = hook_args.build_rule(([torch.tensor([1, 2]), pointer], 42))\n    assert result == ([1, 1], 0)\n\n\ndef test_build_rule_numpy():\n    arr = np.array([2.0, 3.0, 4.0])\n    result = hook_args.build_rule([arr, arr + 2, [2, 4, ""string""]])\n    assert result == [1, 1, [0, 0, 0]]\n\n\ndef test_list_as_index(workers):\n    tensor = torch.tensor([10, 20, 30, -2, 3]).send(workers[""bob""])\n    target = torch.tensor([10, 20, 30, 3])\n\n    slice = tensor[[0, 1, 2, 4]].get()\n    slice2 = tensor[2].get()\n\n    assert torch.equal(target, slice)\n    assert torch.equal(torch.tensor(30), slice2)\n\n\ndef test_backward_multiple_use(workers):\n    """"""\n    Test using backward() in different contexts (FL or Encrypted) within\n    the same session.\n    """"""\n    big_hospital, small_hospital, crypto_provider = (\n        workers[""bob""],\n        workers[""alice""],\n        workers[""james""],\n    )\n\n    # A Toy Model\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc = torch.nn.Linear(2, 1)\n\n        def forward(self, x):\n            x = self.fc(x)\n            return x\n\n    def federated():\n        # A Toy Dataset\n        data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]])\n        target = torch.tensor([[0], [0], [1], [1.0]])\n\n        model = Net()\n\n        model_weight = model.fc.weight.copy()\n\n        # Training Logic\n        opt = torch.optim.SGD(params=model.parameters(), lr=0.1)\n\n        data = data.send(big_hospital)\n        target = target.send(big_hospital)\n\n        # NEW) send model to correct worker\n        model.send(data.location)\n\n        # 1) erase previous gradients (if they exist)\n        opt.zero_grad()\n\n        # 2) make a prediction\n        pred = model(data)\n\n        # 3) calculate how much we missed\n        loss = ((pred - target) ** 2).sum()\n\n        # 4) figure out which weights caused us to miss\n        loss.backward()\n\n        # 5) change those weights\n        opt.step()\n\n        assert (model_weight - model.get().fc.weight).sum().abs() > 1.0e-3\n\n    def encrypted():\n        # A Toy Dataset\n        data2 = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]])\n        target2 = torch.tensor([[0], [0], [1], [1.0]])\n\n        model2 = Net()\n\n        model2_weight = model2.fc.weight.copy()\n\n        # We encode everything\n        data2 = data2.fix_precision().share(\n            big_hospital, small_hospital, crypto_provider=crypto_provider, requires_grad=True\n        )\n        target2 = target2.fix_precision().share(\n            big_hospital, small_hospital, crypto_provider=crypto_provider, requires_grad=True\n        )\n        model2 = model2.fix_precision().share(\n            big_hospital, small_hospital, crypto_provider=crypto_provider, requires_grad=True\n        )\n\n        opt2 = torch.optim.SGD(params=model2.parameters(), lr=0.1).fix_precision()\n\n        # 1) erase previous gradients (if they exist)\n        opt2.zero_grad()\n\n        # 2) make a prediction\n        pred2 = model2(data2)\n\n        # 3) calculate how much we missed\n        loss2 = ((pred2 - target2) ** 2).sum()\n\n        # 4) figure out which weights caused us to miss\n        loss2.backward()\n\n        # 5) change those weights\n        opt2.step()\n\n        weight_diff = (model2_weight - model2.fc.weight.get().float_prec()).sum().abs()\n        assert weight_diff > 1.0e-3\n\n    federated()\n    encrypted()\n\n\ndef test_backward_different_signature(workers):\n    bob = workers[""bob""]\n    a = torch.tensor([0.0], requires_grad=True).send(bob)\n    b = torch.tensor([0.0], requires_grad=True).send(bob)\n    a.backward()\n    a.backward(b)\n\n    assert a.get().grad == torch.tensor([1.0])\n'"
test/torch/linalg/__init__.py,0,b''
test/torch/linalg/test_lr.py,20,"b'import pytest\nimport torch\nimport syft as sy\nfrom syft.frameworks.torch.linalg import EncryptedLinearRegression\nfrom syft.frameworks.torch.linalg import DASH\n\n\n@pytest.mark.parametrize(""fit_intercept"", [False, True])\ndef test_crypto_lr(fit_intercept, hook, workers):\n    """"""\n    Test EncryptedLinearRegression, i.e. distributed linear regression with MPC\n    """"""\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    james = workers[""james""]\n    crypto_prov = workers[""james""]\n    hbc_worker = workers[""charlie""]\n\n    ###### Simulate data ######\n\n    K = 2  # number of features\n\n    beta = torch.Tensor([1.0, 2.0]).view(-1, 1)  # ""real"" coefficients\n    intercept = 0.5 if fit_intercept else 0  # ""real"" intercept\n\n    # Alice\'s data\n    torch.manual_seed(0)  # Truncation might not always work so we set the random seed\n    N1 = 10000\n    X_alice = torch.randn(N1, K).send(alice)\n    y_alice = X_alice @ beta.copy().send(alice) + intercept\n\n    # Bob\'s data\n    torch.manual_seed(42)  # Setting another seed to avoid creation of singular matrices\n    N2 = 20000\n    X_bob = torch.randn(N2, K).send(bob)\n    y_bob = X_bob @ beta.copy().send(bob) + intercept\n\n    # James\'s data\n    N3 = 15000\n    X_james = torch.randn(N3, K).send(james)\n    y_james = X_james @ beta.copy().send(james) + intercept\n\n    # Gather pointers into lists\n    X_ptrs = [X_alice, X_bob, X_james]\n    y_ptrs = [y_alice, y_bob, y_james]\n\n    # Perform linear regression\n    crypto_lr = EncryptedLinearRegression(crypto_prov, hbc_worker, fit_intercept=fit_intercept)\n    crypto_lr.fit(X_ptrs, y_ptrs)\n\n    if fit_intercept:\n        assert abs(crypto_lr.intercept.item() - intercept) < 1e-3\n\n    assert ((crypto_lr.coef - beta.squeeze()).abs() < 1e-3).all()\n\n    ###### Test prediction #######\n    # Pointer tensor\n    diff = crypto_lr.predict(X_alice) - y_alice.squeeze()\n    assert (diff.get().abs() < 1e-3).all()\n\n    # Local tensor\n    X_local = X_alice.get()\n    y_local = y_alice.get()\n    diff = crypto_lr.predict(X_local) - y_local.squeeze()\n    assert (diff.abs() < 1e-3).all()\n\n    ##### Test summarize ######\n\n    crypto_lr.summarize()\n\n\ndef test_DASH(hook, workers):\n    """"""\n    Test DASH (Distributed Association Scan Hammer)\n    i.e. distributed linear regression for genetics with SMPC\n    """"""\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    james = workers[""james""]\n    crypto_prov = sy.VirtualWorker(hook, id=""crypto_prov"")\n    hbc_worker = sy.VirtualWorker(hook, id=""hbc_worker"")\n\n    ###### Simulate data ######\n    torch.manual_seed(0)  # Truncation might not always work so we set the random seed\n\n    K = 2  # Number of permanent covariates\n    M = 5  # Number of transient covariates\n\n    # Alice\n    N1 = 100\n    y1 = torch.randn(N1).send(alice)\n    X1 = torch.randn(N1, M).send(alice)\n    C1 = torch.randn(N1, K).send(alice)\n\n    # Bob\n    N2 = 200\n    y2 = torch.randn(N2).send(bob)\n    X2 = torch.randn(N2, M).send(bob)\n    C2 = torch.randn(N2, K).send(bob)\n\n    # James\n    N3 = 150\n    y3 = torch.randn(N3).send(james)\n    X3 = torch.randn(N3, M).send(james)\n    C3 = torch.randn(N3, K).send(james)\n\n    X_ptrs = [X1, X2, X3]\n    C_ptrs = [C1, C2, C3]\n    y_ptrs = [y1, y2, y3]\n\n    ####### Run the model #######\n\n    model = DASH(crypto_prov, hbc_worker)\n    model.fit(X_ptrs, C_ptrs, y_ptrs)\n\n    # Check dimensions are ok\n    assert model.coef.shape == torch.Size([M])\n    assert model.sigma2.shape == torch.Size([M])\n'"
test/torch/linalg/test_operations.py,24,"b'import torch\nfrom syft.frameworks.torch.linalg import inv_sym\nfrom syft.frameworks.torch.linalg import qr\nfrom syft.frameworks.torch.linalg.operations import _norm_mpc\nfrom syft.frameworks.torch.linalg.lr import DASH\n\n\ndef test_inv_sym(hook, workers):\n    """"""\n    Testing inverse of symmetric matrix with MPC\n    """"""\n    torch.manual_seed(42)  # Truncation might not always work so we set the random seed\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    crypto_prov = workers[""james""]\n\n    x = torch.Tensor([[0.4627, 0.8224], [0.8224, 2.4084]])\n\n    x = x.fix_precision(precision_fractional=6).share(bob, alice, crypto_provider=crypto_prov)\n    gram = x.matmul(x.t())\n    gram_inv = inv_sym(gram)\n\n    gram_inv = gram_inv.get().float_precision()\n    gram = gram.get().float_precision()\n\n    diff = (gram_inv - gram.inverse()).abs()\n    assert (diff < 0.0015).all()\n\n\ndef test_norm_mpc(hook, workers):\n    """"""\n    Testing computation of vector norm on an AdditiveSharedTensor\n    """"""\n    torch.manual_seed(42)  # Truncation might not always work so we set the random seed\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    crypto_prov = workers[""james""]\n\n    n = 100\n    t = torch.randn([n])\n    t_sh = t.fix_precision(precision_fractional=6).share(bob, alice, crypto_provider=crypto_prov)\n    norm_sh = _norm_mpc(t_sh, norm_factor=n ** (1 / 2))\n    norm = norm_sh.copy().get().float_precision()\n\n    assert (norm - torch.norm(t)).abs() < 1e-4\n\n\ndef test_qr(hook, workers):\n    """"""\n    Testing QR decomposition with remote matrix\n    """"""\n    torch.manual_seed(42)  # Truncation might not always work so we set the random seed\n\n    bob = workers[""bob""]\n    n_cols = 5\n    n_rows = 10\n    t = torch.randn([n_rows, n_cols])\n    Q, R = qr(t.send(bob), mode=""complete"")\n    Q = Q.get()\n    R = R.get()\n\n    # Check if Q is orthogonal\n    I = Q @ Q.t()\n    assert ((torch.eye(n_rows) - I).abs() < 1e-5).all()\n\n    # Check if R is upper triangular matrix\n    for col in range(n_cols):\n        assert ((R[col + 1 :, col]).abs() < 1e-5).all()\n\n    # Check if QR == t\n    assert ((Q @ R - t).abs() < 1e-5).all()\n\n    # test modes\n    Q, R = qr(t.send(bob), mode=""reduced"")\n    assert Q.shape == (n_rows, n_cols)\n    assert R.shape == (n_cols, n_cols)\n\n    Q, R = qr(t.send(bob), mode=""complete"")\n    assert Q.shape == (n_rows, n_rows)\n    assert R.shape == (n_rows, n_cols)\n\n    R = qr(t.send(bob), mode=""r"")\n    assert R.shape == (n_cols, n_cols)\n\n\ndef test_qr_mpc(hook, workers):\n    """"""\n    Testing QR decomposition with an AdditiveSharedTensor\n    """"""\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    crypto_prov = workers[""james""]\n\n    torch.manual_seed(0)  # Truncation might not always work so we set the random seed\n    n_cols = 3\n    n_rows = 3\n    t = torch.randn([n_rows, n_cols])\n    t_sh = t.fix_precision(precision_fractional=6).share(bob, alice, crypto_provider=crypto_prov)\n\n    Q, R = qr(t_sh, norm_factor=3 ** (1 / 2), mode=""complete"")\n    Q = Q.get().float_precision()\n    R = R.get().float_precision()\n\n    # Check if Q is orthogonal\n    I = Q @ Q.t()\n    assert ((torch.eye(n_rows) - I).abs() < 1e-2).all()\n\n    # Check if R is upper triangular matrix\n    for col in range(n_cols - 1):\n        assert ((R[col + 1 :, col]).abs() < 1e-2).all()\n\n    # Check if QR == t\n    assert ((Q @ R - t).abs() < 1e-2).all()\n\n\ndef test_inv_upper(hook, workers):\n\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n    crypto_prov = workers[""james""]\n\n    torch.manual_seed(0)  # Truncation might not always work so we set the random seed\n    n_cols = 3\n    n_rows = 3\n    R = torch.triu(torch.randn([n_rows, n_cols]))\n    invR = R.inverse()\n\n    R_sh = R.fix_precision(precision_fractional=6).share(bob, alice, crypto_provider=crypto_prov)\n    invR_sh = DASH._inv_upper(R_sh)\n    assert ((invR - invR_sh.get().float_precision()).abs() < 1e-2).all()\n\n\ndef test_remote_random_number_generation(hook, workers):\n    """"""\n    Test random number generation on remote worker machine\n    """"""\n\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super(Model, self).__init__()\n\n        def add_randn_like(self, x):\n            r = torch.randn_like(x)\n            return x + r\n\n        def get_randint(self, n):\n            r = torch.rand(n)\n            return r\n\n        def get_rand(self, n):\n            r = torch.rand(n)\n            return r\n\n        def get_randn(self, n):\n            r = torch.randn(n)\n            return r\n\n        def get_randperm(self, n):\n            r = torch.randperm(n)\n            return r\n\n    alice = workers[""alice""]\n    model = Model().to(""cpu"").send(alice)\n\n    x = torch.tensor([1.0, 2.0, 3.0, 4.0]).send(alice)\n    y = model.add_randn_like(x)\n\n    # Check that returned tensor is same size as original\n    assert len(y.get()) == len(x)\n\n    r = model.get_randint(4)\n    s = model.get_rand(5)\n    t = model.get_randn(6)\n    v = model.get_randperm(7)\n\n    # Check that the returned tensors are the requested size\n    assert len(r) == 4\n    assert len(s) == 5\n    assert len(t) == 6\n    assert len(v) == 7\n'"
test/torch/mpc/__init__.py,0,b''
test/torch/mpc/test_crypto_store.py,0,"b'import pytest\n\nfrom syft.exceptions import EmptyCryptoPrimitiveStoreError\n\n\ndef test_primitives_usage(workers):\n    me, alice, bob, crypto_provider = (\n        workers[""me""],\n        workers[""alice""],\n        workers[""bob""],\n        workers[""james""],\n    )\n    me.crypto_store.provide_primitives([""fss_eq""], [alice, bob], n_instances=6)\n    _ = alice.crypto_store.get_keys(""fss_eq"", 2, remove=False)\n\n    assert len(alice.crypto_store.fss_eq[0]) == 6\n\n    keys = alice.crypto_store.get_keys(""fss_eq"", 4, remove=True)\n\n    assert len(keys[0]) == 4\n    assert len(alice.crypto_store.fss_eq[0]) == 2\n\n    with pytest.raises(EmptyCryptoPrimitiveStoreError):\n        _ = alice.crypto_store.get_keys(""fss_eq"", 4, remove=True)\n'"
test/torch/mpc/test_fss.py,1,"b'import pytest\nimport torch as th\n\nfrom syft.frameworks.torch.mpc.fss import DPF, DIF, n\n\n\n@pytest.mark.parametrize(""op"", [""eq"", ""le""])\ndef test_fss_class(op):\n    class_ = {""eq"": DPF, ""le"": DIF}[op]\n    th_op = {""eq"": th.eq, ""le"": th.le}[op]\n    gather_op = {""eq"": ""__add__"", ""le"": ""__xor__""}[op]\n\n    # single value\n    primitive = class_.keygen(n_values=1)\n    alpha, s_00, s_01, *CW = primitive\n    mask = th.randint(0, 2 ** n, alpha.shape)\n    k0, k1 = [((alpha - mask) % 2 ** n, s_00, *CW), (mask, s_01, *CW)]\n\n    x = th.tensor([0])\n    x_masked = x + k0[0] + k1[0]\n    y0 = class_.eval(0, x_masked, *k0[1:])\n    y1 = class_.eval(1, x_masked, *k1[1:])\n\n    assert (getattr(y0, gather_op)(y1) == th_op(x, 0)).all()\n\n    # 1D tensor\n    primitive = class_.keygen(n_values=3)\n    alpha, s_00, s_01, *CW = primitive\n    mask = th.randint(0, 2 ** n, alpha.shape)\n    k0, k1 = [((alpha - mask) % 2 ** n, s_00, *CW), (mask, s_01, *CW)]\n\n    x = th.tensor([0, 2, -2])\n    x_masked = x + k0[0] + k1[0]\n    y0 = class_.eval(0, x_masked, *k0[1:])\n    y1 = class_.eval(1, x_masked, *k1[1:])\n\n    assert (getattr(y0, gather_op)(y1) == th_op(x, 0)).all()\n\n    # 2D tensor\n    primitive = class_.keygen(n_values=4)\n    alpha, s_00, s_01, *CW = primitive\n    mask = th.randint(0, 2 ** n, alpha.shape)\n    k0, k1 = [((alpha - mask) % 2 ** n, s_00, *CW), (mask, s_01, *CW)]\n\n    x = th.tensor([[0, 2], [-2, 0]])\n    x_masked = x + k0[0].reshape(x.shape) + k1[0].reshape(x.shape)\n    y0 = class_.eval(0, x_masked, *k0[1:])\n    y1 = class_.eval(1, x_masked, *k1[1:])\n\n    assert (getattr(y0, gather_op)(y1) == th_op(x, 0)).all()\n\n    # 3D tensor\n    primitive = class_.keygen(n_values=8)\n    alpha, s_00, s_01, *CW = primitive\n    mask = th.randint(0, 2 ** n, alpha.shape)\n    k0, k1 = [((alpha - mask) % 2 ** n, s_00, *CW), (mask, s_01, *CW)]\n\n    x = th.tensor([[[0, 2], [-2, 0]], [[0, 2], [-2, 0]]])\n    x_masked = x + k0[0].reshape(x.shape) + k1[0].reshape(x.shape)\n    y0 = class_.eval(0, x_masked, *k0[1:])\n    y1 = class_.eval(1, x_masked, *k1[1:])\n\n    assert (getattr(y0, gather_op)(y1) == th_op(x, 0)).all()\n'"
test/torch/mpc/test_multiparty_nn.py,24,"b'import torch\nfrom syft.frameworks.torch.mpc import securenn\n\n\ndef test_select_share(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    alpha_1 = (\n        torch.tensor([[0, 1]]).share(alice, bob, charlie, crypto_provider=james, dtype=""long"").child\n    )\n    ones = (\n        torch.tensor([[1, 1]]).share(alice, bob, charlie, crypto_provider=james, dtype=""long"").child\n    )\n    twos = (\n        torch.tensor([[2, 2]]).share(alice, bob, charlie, crypto_provider=james, dtype=""long"").child\n    )\n    selected = securenn.select_share(alpha_1, ones, twos)\n    assert (selected.get() == torch.tensor([[1, 2]])).all()\n\n\ndef test_private_compare(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    x_bits = (\n        securenn.decompose(torch.tensor([3, 3]), 2 ** 64)\n        .share(alice, bob, charlie, dtype=""custom"", field=67, crypto_provider=james)\n        .child\n    )\n    r = torch.tensor([1, 5]).send(alice, bob, charlie).child\n    b = torch.tensor([0]).send(alice, bob, charlie).child\n    compare = securenn.private_compare(x_bits, r, b, 2 ** 64)\n    assert (compare == torch.tensor([1, 0])).all()\n\n\ndef test_share_convert(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    tensorA = (\n        torch.tensor([10, 20, 30])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    tensorB = securenn.share_convert(tensorA)\n\n    assert (tensorA.get() == tensorB.get()).all()\n\n\ndef test_msb(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    tensorA = (\n        torch.tensor([-10, 0, 10])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""custom"", field=2 ** 60 - 1)\n        .child\n    )\n    assert (securenn.msb(tensorA).get() == torch.tensor([1, 0, 0])).all()\n\n\ndef test_relu_deriv(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    tensorA = (\n        torch.tensor([-10, 0, 10])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    assert (securenn.relu_deriv(tensorA).get() == torch.tensor([0, 1, 1])).all()\n\n\ndef test_relu(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    tensorA = (\n        torch.tensor([-10, 0, 10])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    assert (securenn.relu(tensorA).get() == torch.tensor([0, 0, 10])).all()\n\n\ndef test_division(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    tensorA = (\n        torch.tensor([[0, 10, 10, 20, 20]])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    tensorB = (\n        torch.tensor([[1, 2, 3, 4, 5]])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    division = securenn.division(tensorA, tensorB)\n    assert (division.get() == torch.tensor([[0, 5, 3, 5, 4]])).all()\n\n\ndef test_maxpool(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    tensorA = (\n        torch.tensor([[0, 1, 8, 3, 4]])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    v, i = securenn.maxpool(tensorA)\n    assert (v.get() == torch.tensor([[8]])).all()\n    assert (i.get() == torch.tensor([[2]])).all()\n\n\ndef test_maxpool_deriv(workers):\n    alice, bob, charlie, james = (\n        workers[""alice""],\n        workers[""bob""],\n        workers[""charlie""],\n        workers[""james""],\n    )\n    tensorA = (\n        torch.tensor([[0, 1, 8, 3]])\n        .share(alice, bob, charlie, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    deriv = securenn.maxpool_deriv(tensorA)\n    assert (deriv.get() == torch.tensor([[0, 0, 1, 0]])).all()\n'"
test/torch/mpc/test_securenn.py,71,"b'import pytest\n\nimport torch\n\nfrom syft.frameworks.torch.mpc.securenn import (\n    private_compare,\n    decompose,\n    share_convert,\n    relu_deriv,\n    division,\n    maxpool,\n    maxpool2d,\n    maxpool_deriv,\n)\nfrom syft.generic.pointers.multi_pointer import MultiPointerTensor\n\n\ndef test_xor_implementation(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    r = decompose(torch.LongTensor([3]), 2 ** 64).send(alice, bob).child\n    x_bit_sh = (\n        decompose(torch.LongTensor([23]), 2 ** 64)\n        .share(alice, bob, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    j0 = torch.zeros(x_bit_sh.shape).long().send(bob)\n    j1 = torch.ones(x_bit_sh.shape).long().send(alice)\n    j = MultiPointerTensor(children=[j0.child, j1.child])\n    w = (j * r) + x_bit_sh - (2 * x_bit_sh * r)\n\n    r_real = r.virtual_get()[0]\n    x_real = x_bit_sh.virtual_get()\n    w_real = r_real + x_real - 2 * r_real * x_real\n    assert (w.virtual_get() == w_real).all()\n\n    # For dtype int\n    r = decompose(torch.IntTensor([3]), 2 ** 32).send(alice, bob).child\n    x_bit_sh = (\n        decompose(torch.IntTensor([23]), 2 ** 32)\n        .share(alice, bob, crypto_provider=james, dtype=""int"")\n        .child\n    )\n    assert x_bit_sh.field == 2 ** 32 and x_bit_sh.dtype == ""int""\n    j0 = torch.zeros(x_bit_sh.shape).type(torch.int32).send(bob)\n    j1 = torch.ones(x_bit_sh.shape).type(torch.int32).send(alice)\n    j = MultiPointerTensor(children=[j0.child, j1.child])\n    w = (j * r) + x_bit_sh - (2 * x_bit_sh * r)\n\n    r_real = r.virtual_get()[0]\n    x_real = x_bit_sh.virtual_get()\n    w_real = r_real + x_real - 2 * r_real * x_real\n    assert (w.virtual_get() == w_real).all()\n\n\ndef test_private_compare(workers):\n    """"""\n    Test private compare which returns: \xce\xb2\xe2\x80\xb2 = \xce\xb2 \xe2\x8a\x95 (x > r).\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    L = 2 ** 64\n    x_bit_sh = (\n        decompose(torch.LongTensor([13]), L)\n        .share(alice, bob, crypto_provider=james, field=67, dtype=""custom"")\n        .child\n    )\n    r = torch.LongTensor([12]).send(alice, bob).child\n\n    beta = torch.LongTensor([1]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert not beta_p\n\n    beta = torch.LongTensor([0]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert beta_p\n\n    # Big values\n    x_bit_sh = (\n        decompose(torch.LongTensor([2 ** 60]), L)\n        .share(alice, bob, crypto_provider=james, field=67, dtype=""custom"")\n        .child\n    )\n    r = torch.LongTensor([2 ** 61]).send(alice, bob).child\n\n    beta = torch.LongTensor([1]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert beta_p\n\n    beta = torch.LongTensor([0]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert not beta_p\n\n    # Multidimensional tensors\n    x_bit_sh = (\n        decompose(torch.LongTensor([[13, 44], [1, 28]]), L)\n        .share(alice, bob, crypto_provider=james, field=67, dtype=""custom"")\n        .child\n    )\n    r = torch.LongTensor([[12, 44], [12, 33]]).send(alice, bob).child\n\n    beta = torch.LongTensor([1]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert (beta_p == torch.tensor([[0, 1], [1, 1]])).all()\n\n    beta = torch.LongTensor([0]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert (beta_p == torch.tensor([[1, 0], [0, 0]])).all()\n\n    # Negative values\n    x_val = -105\n    r_val = -52 % 2 ** 63  # The protocol works only for values in Zq\n    x_bit_sh = (\n        decompose(torch.LongTensor([x_val]), L)\n        .share(alice, bob, crypto_provider=james, field=67, dtype=""custom"")\n        .child\n    )\n    r = torch.LongTensor([r_val]).send(alice, bob).child\n\n    beta = torch.LongTensor([1]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert beta_p\n\n    beta = torch.LongTensor([0]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert not beta_p\n\n    # With dtype int\n    L = 2 ** 32\n\n    x_bit_sh = (\n        decompose(torch.IntTensor([13]), L)\n        .share(alice, bob, crypto_provider=james, field=67, dtype=""custom"")\n        .child\n    )\n    r = torch.IntTensor([12]).send(alice, bob).child\n\n    beta = torch.IntTensor([1]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert not beta_p\n\n    beta = torch.IntTensor([0]).send(alice, bob).child\n    beta_p = private_compare(x_bit_sh, r, beta, L)\n    assert beta_p\n\n\ndef test_share_convert(workers):\n    """"""\n    This is a light test as share_convert is not used for the moment\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    L = 2 ** 64\n    a_sh = (\n        torch.LongTensor([-13, 3567, 2 ** 60])\n        .share(alice, bob, crypto_provider=james, field=L)\n        .child\n    )\n\n    res = share_convert(a_sh)\n    assert res.dtype == ""custom""\n    assert res.field == L - 1\n    assert (res.get() == torch.LongTensor([-13, 3567, 2 ** 60])).all()\n\n    # With dtype int\n    L = 2 ** 32\n    a_sh = (\n        torch.IntTensor([13, -3567, 2 ** 30])\n        .share(alice, bob, crypto_provider=james, field=L)\n        .child\n    )\n\n    res = share_convert(a_sh)\n    assert res.dtype == ""custom""\n    assert res.field == L - 1\n    assert (res.get() == torch.IntTensor([13, -3567, 2 ** 30])).all()\n\n\ndef test_relu_deriv(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    x = torch.tensor([10, 0, -3]).share(alice, bob, crypto_provider=james, dtype=""long"").child\n    r = relu_deriv(x)\n\n    assert (r.get() == torch.tensor([1, 1, 0])).all()\n\n    # With dtype int\n    x = torch.tensor([10, 0, -3]).share(alice, bob, crypto_provider=james, dtype=""int"").child\n    r = relu_deriv(x)\n\n    assert (r.get() == torch.tensor([1, 1, 0])).all()\n\n\ndef test_relu(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    x = torch.tensor([1, -3]).share(alice, bob, crypto_provider=james, dtype=""long"")\n    r = x.relu()\n\n    assert (r.get() == torch.tensor([1, 0])).all()\n\n    x = (\n        torch.tensor([1.0, 3.1, -2.1])\n        .fix_prec(dtype=""int"")\n        .share(alice, bob, crypto_provider=james)\n    )\n    r = x.relu()\n\n    assert (r.get().float_prec() == torch.tensor([1, 3.1, 0])).all()\n\n    # With dtype int\n    x = torch.tensor([1, -3]).share(alice, bob, crypto_provider=james, dtype=""int"")\n    r = x.relu()\n\n    assert (r.get() == torch.tensor([1, 0])).all()\n\n\ndef test_division(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    x0 = torch.tensor(10).share(alice, bob, crypto_provider=james, dtype=""long"").child\n    y0 = torch.tensor(2).share(alice, bob, crypto_provider=james, dtype=""long"").child\n    res0 = division(x0, y0, bit_len_max=5)\n\n    x1 = (\n        torch.tensor([[25, 9], [10, 30]])\n        .share(alice, bob, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    y1 = (\n        torch.tensor([[5, 12], [2, 7]]).share(alice, bob, crypto_provider=james, dtype=""long"").child\n    )\n    res1 = division(x1, y1, bit_len_max=5)\n\n    assert res0.get() == torch.tensor(5)\n    assert (res1.get() == torch.tensor([[5, 0], [5, 4]])).all()\n\n    # With dtype int\n    x0 = torch.tensor(10).share(alice, bob, crypto_provider=james, dtype=""int"").child\n    y0 = torch.tensor(2).share(alice, bob, crypto_provider=james, dtype=""int"").child\n    res0 = division(x0, y0, bit_len_max=5)\n\n    x1 = (\n        torch.tensor([[25, 9], [10, 30]])\n        .share(alice, bob, crypto_provider=james, dtype=""int"")\n        .child\n    )\n    y1 = torch.tensor([[5, 12], [2, 7]]).share(alice, bob, crypto_provider=james, dtype=""int"").child\n    res1 = division(x1, y1, bit_len_max=5)\n\n    assert res0.get() == torch.tensor(5)\n    assert (res1.get() == torch.tensor([[5, 0], [5, 4]])).all()\n\n\ndef test_maxpool(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    x = (\n        torch.tensor([[10, 0], [15, 7]])\n        .share(alice, bob, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    max, ind = maxpool(x)\n\n    assert max.get() == torch.tensor(15)\n    assert ind.get() == torch.tensor(2)\n\n    # With dtype int\n    x = torch.tensor([[10, 0], [15, 7]]).share(alice, bob, crypto_provider=james, dtype=""int"").child\n    max, ind = maxpool(x)\n\n    assert max.get() == torch.tensor(15)\n    assert ind.get() == torch.tensor(2)\n\n\ndef test_maxpool_deriv(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    x = (\n        torch.tensor([[10, 0], [15, 7]])\n        .share(alice, bob, crypto_provider=james, dtype=""long"")\n        .child\n    )\n    max_d = maxpool_deriv(x)\n\n    assert (max_d.get() == torch.tensor([[0, 0], [1, 0]])).all()\n\n    # With dtype int\n    x = torch.tensor([[10, 0], [15, 7]]).share(alice, bob, crypto_provider=james, dtype=""int"").child\n    max_d = maxpool_deriv(x)\n\n    assert (max_d.get() == torch.tensor([[0, 0], [1, 0]])).all()\n\n\n@pytest.mark.parametrize(\n    ""kernel_size, stride"", [(1, 1), (2, 1), (3, 1), (1, 2), (2, 2), (3, 2), (3, 3)]\n)\ndef test_maxpool2d(workers, kernel_size, stride):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    def _test_maxpool2d(x):\n        x_sh = x.long().share(alice, bob, crypto_provider=james, dtype=""long"").wrap()\n        y = maxpool2d(x_sh, kernel_size=kernel_size, stride=stride)\n\n        torch_maxpool = torch.nn.MaxPool2d(kernel_size, stride=stride)\n        assert torch.all(torch.eq(y.get(), torch_maxpool(x).long()))\n\n    x1 = torch.Tensor(\n        [\n            [\n                [[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n                [[10.0, 11.0, 12.0], [13.0, 14.0, 15.0], [16.0, 17.0, 18.0]],\n            ]\n        ]\n    )\n\n    _test_maxpool2d(x1)\n\n    x2 = torch.tensor(\n        [\n            [[[10, 9.1, 1, 1], [0.72, -2.5, 1, 1], [0.72, -2.5, 1, 1], [0.72, -2.5, 1, 1]]],\n            [[[15, 0.6, 1, 1], [1, -3, 1, 1], [1, -3, 1, 1], [1, -3, 1, 1]]],\n            [[[1.2, 0.3, 1, 1], [5.5, 6.2, 1, 1], [1, -3, 1, 1], [1, -3, 1, 1]]],\n        ]\n    )\n\n    _test_maxpool2d(x2)\n'"
test/torch/nn/__init__.py,0,b''
test/torch/nn/test_functional.py,37,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef test_torch_nn_functional_linear():\n    tensor = nn.Parameter(torch.tensor([[1.0, 2], [3, 4]]), requires_grad=False).fix_prec()\n    weight = nn.Parameter(torch.tensor([[1.0, 2], [3, 4]]), requires_grad=True).fix_prec()\n\n    result = F.linear(tensor, weight).float_prec()\n\n    expected = torch.tensor([[5.0, 11.0], [11.0, 25.0]])\n\n    assert (result == expected).all()\n\n    tensor = nn.Parameter(torch.tensor([[1.0, -2], [3, 4]]), requires_grad=False).fix_prec()\n    weight = nn.Parameter(torch.tensor([[1.0, 2], [3, 4]]), requires_grad=True).fix_prec()\n\n    result = F.linear(tensor, weight).float_prec()\n\n    expected = torch.tensor([[-3.0, -5], [11.0, 25.0]])\n\n    assert (result == expected).all()\n\n    tensor = nn.Parameter(torch.tensor([[1.0, 2], [3, 4]]), requires_grad=False).fix_prec(\n        precision_fractional=2\n    )\n    weight = nn.Parameter(torch.tensor([[1.0, 2], [3, 4]]), requires_grad=True).fix_prec(\n        precision_fractional=2\n    )\n\n    result = F.linear(tensor, weight).float_prec()\n\n    expected = torch.tensor([[5.0, 11.0], [11.0, 25.0]])\n\n    assert (result == expected).all()\n\n\ndef test_torch_nn_functional_dropout(workers):\n\n    # Test for fixed precision tensor\n    a = torch.rand((20, 20))\n    x = a.fix_prec()\n\n    train_output = F.dropout(x, p=0.5, training=True, inplace=False)\n    assert (train_output.float_prec() == 0).sum() > 0\n\n    # training = False, should return the same input\n    test_output = F.dropout(x, p=0.5, training=False, inplace=False)\n    assert ((test_output == x).float_prec() == 1).all()\n\n    # For AST wrapped under Fixed Precision\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x = a.fix_prec().share(alice, bob, crypto_provider=james)\n\n    train_output = F.dropout(x, p=0.5, training=True, inplace=False)\n    assert (train_output.get().float_prec() == 0).sum() > 0\n\n    # training = False, should return the same input\n    test_output = F.dropout(x, p=0.5, training=False, inplace=False)\n    assert ((test_output == x).get().float_prec() == 1).all()\n\n\ndef test_torch_nn_functional_conv2d(workers):\n    # Test with FixedPrecision tensors\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    im = torch.tensor(\n        [\n            [\n                [[0.5, 1.0, 2.0], [3.5, 4.0, 5.0], [6.0, 7.5, 8.0]],\n                [[10.0, 11.0, 12.0], [13.0, 14.5, 15.0], [16.0, 17.5, 18.0]],\n            ]\n        ]\n    )\n    w = torch.tensor(\n        [\n            [[[0.0, 3.0], [1.5, 1.0]], [[2.0, 2.0], [2.5, 2.0]]],\n            [[[-0.5, -1.0], [-2.0, -1.5]], [[0.0, 0.0], [0.0, 0.5]]],\n        ]\n    )\n    bias = torch.tensor([-1.3, 15.0])\n\n    im_fp = im.fix_prec()\n    w_fp = w.fix_prec()\n    bias_fp = bias.fix_prec()\n\n    res0 = F.conv2d(im_fp, w_fp, bias=bias_fp, stride=1).float_prec()\n    res1 = F.conv2d(\n        im_fp, w_fp[:, 0:1].contiguous(), bias=bias_fp, stride=2, padding=3, dilation=2, groups=2\n    ).float_prec()\n\n    expected0 = torch.conv2d(im, w, bias=bias, stride=1)\n    expected1 = torch.conv2d(\n        im, w[:, 0:1].contiguous(), bias=bias, stride=2, padding=3, dilation=2, groups=2\n    )\n\n    assert (res0 == expected0).all()\n    assert (res1 == expected1).all()\n\n    # Test with AdditiveSharing tensors (Wrapper)>FixedPrecision>AdditiveShared\n    im_shared = im.fix_prec().share(bob, alice, crypto_provider=james)\n    w_shared = w.fix_prec().share(bob, alice, crypto_provider=james)\n    bias_shared = bias.fix_prec().share(bob, alice, crypto_provider=james)\n\n    res0 = F.conv2d(im_shared, w_shared, bias=bias_shared, stride=1).get().float_precision()\n    res1 = (\n        F.conv2d(\n            im_shared,\n            w_shared[:, 0:1].contiguous(),\n            bias=bias_shared,\n            stride=2,\n            padding=3,\n            dilation=2,\n            groups=2,\n        )\n        .get()\n        .float_precision()\n    )\n\n    expected0 = torch.conv2d(im, w, bias=bias, stride=1)\n    expected1 = torch.conv2d(\n        im, w[:, 0:1].contiguous(), bias=bias, stride=2, padding=3, dilation=2, groups=2\n    )\n\n    assert (res0 == expected0).all()\n    assert (res1 == expected1).all()\n\n\ndef test_torch_nn_functional_maxpool(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    enc_tensor = torch.tensor(\n        [[[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]]], dtype=torch.float\n    )\n    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)\n    r_max = F.max_pool2d(enc_tensor, kernel_size=2)\n    r_max = r_max.get().float_prec()\n    exp_max = torch.tensor([[[[6.0, 8.0], [3.0, 4.0]]]])\n    assert (r_max == exp_max).all()\n    # 3d\n    enc_tensor = torch.tensor(\n        [[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]], dtype=torch.float\n    )\n    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)\n    r_max = F.max_pool2d(enc_tensor, kernel_size=2)\n    r_max = r_max.get().float_prec()\n    exp_max = torch.tensor([[[6.0, 8.0], [3.0, 4.0]]])\n    assert (r_max == exp_max).all()\n    # 2d\n    enc_tensor = torch.tensor(\n        [[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]], dtype=torch.float\n    )\n    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)\n    r_max = F.max_pool2d(enc_tensor, kernel_size=2)\n    r_max = r_max.get().float_prec()\n    exp_max = torch.tensor([[6.0, 8.0], [3.0, 4.0]])\n    assert (r_max == exp_max).all()\n\n\ndef test_torch_nn_functional_avgpool(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    enc_tensor = torch.tensor(\n        [[[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]]], dtype=torch.float\n    )\n    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)\n    r_avg = F.avg_pool2d(enc_tensor, kernel_size=2)\n    r_avg = r_avg.get().float_prec()\n    exp_avg = torch.tensor([[[[3.2500, 5.2500], [2.0000, 2.0000]]]])\n    assert (r_avg == exp_avg).all()\n    # 3d\n    enc_tensor = torch.tensor(\n        [[[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]]], dtype=torch.float\n    )\n    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)\n    r_avg = F.avg_pool2d(enc_tensor, kernel_size=2)\n    r_avg = r_avg.get().float_prec()\n    exp_avg = torch.tensor([[[3.2500, 5.2500], [2.0000, 2.0000]]])\n    assert (r_avg == exp_avg).all()\n    # 2d\n    enc_tensor = torch.tensor(\n        [[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]], dtype=torch.float\n    )\n    enc_tensor = enc_tensor.fix_prec().share(bob, alice, crypto_provider=james)\n    r_avg = F.avg_pool2d(enc_tensor, kernel_size=2)\n    r_avg = r_avg.get().float_prec()\n    exp_avg = torch.tensor([[3.2500, 5.2500], [2.0000, 2.0000]])\n    assert (r_avg == exp_avg).all()\n'"
test/torch/nn/test_nn.py,90,"b'import copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport syft.frameworks.torch.nn as syft_nn\n\n\ndef test_nn_linear(workers):\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    t = torch.tensor([[1.0, 2]])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    model = nn.Linear(2, 1)\n    model.weight = nn.Parameter(torch.tensor([[-1.0, 2]]))\n    model.bias = nn.Parameter(torch.tensor([[-1.0]]))\n    model.fix_precision().share(bob, alice, crypto_provider=james)\n\n    y = model(x)\n\n    assert len(alice.object_store._objects) == 4  # x, y, weight, bias\n    assert len(bob.object_store._objects) == 4\n    assert y.get().float_prec() == torch.tensor([[2.0]])\n\n\ndef test_conv2d(workers):\n    """"""\n    Test the nn.Conv2d module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n\n    # Disable mkldnn to avoid rounding errors due to difference in implementation\n    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()\n    torch._C._set_mkldnn_enabled(False)\n\n    # Direct Import from Syft\n    model = syft_nn.Conv2d(1, 2, 3, bias=True)\n    model_1 = nn.Conv2d(1, 2, 3, bias=True)\n    model.weight = model_1.weight.fix_prec()\n    model.bias = model_1.bias.fix_prec()\n    data = torch.rand(10, 1, 28, 28)  # eg. mnist data\n\n    out = model(data.fix_prec()).float_prec()\n    out_1 = model_1(data)\n\n    assert torch.allclose(out, out_1, atol=1e-2)\n\n    # Fixed Precision Tensor\n    model_2 = model_1.copy().fix_prec()\n    out_2 = model_2(data.fix_prec()).float_prec()\n\n    # Note: absolute tolerance can be reduced by increasing precision_fractional of fix_prec()\n    assert torch.allclose(out_1, out_2, atol=1e-2)\n\n    # Additive Shared Tensor\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    shared_data = data.fix_prec().share(bob, alice, crypto_provider=james)\n\n    mode_3 = model_2.share(bob, alice, crypto_provider=james)\n    out_3 = mode_3(shared_data).get().float_prec()\n\n    assert torch.allclose(out_1, out_3, atol=1e-2)\n\n    # Reset mkldnn to the original state\n    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)\n\n\ndef test_pool2d():\n    """"""\n    Test the Pool2d module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n\n    model = nn.Conv2d(\n        in_channels=1,\n        out_channels=32,\n        kernel_size=3,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        padding_mode=""zeros"",\n    )\n\n    pool = syft_nn.AvgPool2d(2)\n    pool_1 = nn.AvgPool2d(2)\n    pool_2 = pool_1.copy().fix_prec()\n\n    data = torch.rand(10, 1, 8, 8)\n\n    model_out = model(data)\n    out = pool(model_out)\n    out_1 = pool_1(model_out)\n    out_2 = pool_2(model_out)\n\n    assert torch.eq(out, out_1).all()\n    assert torch.eq(out_1, out_2).all()\n\n\ndef test_cnn_model(workers):\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n            self.fc2 = nn.Linear(500, 10)\n\n        def forward(self, x):\n            # TODO: uncomment maxpool2d operations\n            # once it is supported with smpc.\n            x = F.relu(self.conv1(x))\n            # x = F.max_pool2d(x, 2, 2)\n            x = F.relu(self.conv2(x))\n            # x = F.max_pool2d(x, 2, 2)\n            x = x.view(-1, 4 * 4 * 50)\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x\n\n    model = Net()\n    sh_model = copy.deepcopy(model).fix_precision().share(alice, bob, crypto_provider=james)\n\n    data = torch.zeros((1, 1, 28, 28))\n    sh_data = torch.zeros((1, 1, 28, 28)).fix_precision().share(alice, bob, crypto_provider=james)\n\n    assert torch.allclose(sh_model(sh_data).get().float_prec(), model(data), atol=1e-2)\n\n\ndef test_RNNCell():\n    """"""\n    Test the RNNCell module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n\n    # Disable mkldnn to avoid rounding errors due to difference in implementation\n    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()\n    torch._C._set_mkldnn_enabled(False)\n\n    batch_size = 5\n    input_size = 10\n    hidden_size = 50\n\n    test_input = torch.rand(batch_size, input_size)\n    test_hidden = torch.rand(batch_size, hidden_size)\n\n    # RNNCell implemented in pysyft\n    rnn_syft = syft_nn.RNNCell(input_size, hidden_size, True, ""tanh"")\n\n    # RNNCell implemented in original pytorch\n    rnn_torch = nn.RNNCell(input_size, hidden_size, True, ""tanh"")\n\n    # Make sure the weights of both RNNCell are identical\n    rnn_syft.fc_xh.weight = rnn_torch.weight_ih\n    rnn_syft.fc_hh.weight = rnn_torch.weight_hh\n    rnn_syft.fc_xh.bias = rnn_torch.bias_ih\n    rnn_syft.fc_hh.bias = rnn_torch.bias_hh\n\n    output_syft = rnn_syft(test_input, test_hidden)\n    output_torch = rnn_torch(test_input, test_hidden)\n\n    assert torch.allclose(output_syft, output_torch, atol=1e-2)\n\n    # Reset mkldnn to the original state\n    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)\n\n\ndef test_GRUCell():\n    """"""\n    Test the GRUCell module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n\n    # Disable mkldnn to avoid rounding errors due to difference in implementation\n    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()\n    torch._C._set_mkldnn_enabled(False)\n\n    batch_size = 5\n    input_size = 10\n    hidden_size = 50\n\n    test_input = torch.rand(batch_size, input_size)\n    test_hidden = torch.rand(batch_size, hidden_size)\n\n    # GRUCell implemented in pysyft\n    rnn_syft = syft_nn.GRUCell(input_size, hidden_size, True)\n\n    # GRUCell implemented in original pytorch\n    rnn_torch = nn.GRUCell(input_size, hidden_size, True)\n\n    # Make sure the weights of both GRUCell are identical\n    rnn_syft.fc_xh.weight = rnn_torch.weight_ih\n    rnn_syft.fc_hh.weight = rnn_torch.weight_hh\n    rnn_syft.fc_xh.bias = rnn_torch.bias_ih\n    rnn_syft.fc_hh.bias = rnn_torch.bias_hh\n\n    output_syft = rnn_syft(test_input, test_hidden)\n    output_torch = rnn_torch(test_input, test_hidden)\n\n    # Reset mkldnn to the original state\n    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)\n\n    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))\n\n\ndef test_LSTMCell():\n    """"""\n    Test the LSTMCell module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n\n    # Disable mkldnn to avoid rounding errors due to difference in implementation\n    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()\n    torch._C._set_mkldnn_enabled(False)\n\n    batch_size = 5\n    input_size = 10\n    hidden_size = 50\n\n    test_input = torch.rand(batch_size, input_size)\n    test_hidden_state = torch.rand(batch_size, hidden_size)\n    test_cell_state = torch.rand(batch_size, hidden_size)\n\n    # LSTMCell implemented in pysyft\n    rnn_syft = syft_nn.LSTMCell(input_size, hidden_size, True)\n\n    # LSTMCell implemented in original pytorch\n    rnn_torch = nn.LSTMCell(input_size, hidden_size, True)\n\n    # Make sure the weights of both LSTMCell are identical\n    rnn_syft.fc_xh.weight = rnn_torch.weight_ih\n    rnn_syft.fc_hh.weight = rnn_torch.weight_hh\n    rnn_syft.fc_xh.bias = rnn_torch.bias_ih\n    rnn_syft.fc_hh.bias = rnn_torch.bias_hh\n\n    hidden_syft, cell_syft = rnn_syft(test_input, (test_hidden_state, test_cell_state))\n    hidden_torch, cell_torch = rnn_torch(test_input, (test_hidden_state, test_cell_state))\n\n    # Reset mkldnn to the original state\n    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)\n\n    # Assert the hidden_state and cell_state of both models are identical separately\n    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))\n    assert torch.all(torch.lt(torch.abs(cell_syft - cell_torch), 1e-6))\n\n\ndef test_RNN():\n    """"""\n    Test the RNN module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n\n    # Disable mkldnn to avoid rounding errors due to difference in implementation\n    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()\n    torch._C._set_mkldnn_enabled(False)\n\n    batch_size = 5\n    input_size = 10\n    hidden_size = 50\n    num_layers = 1\n    seq_len = 8\n\n    test_input = torch.rand(seq_len, batch_size, input_size)\n    test_hidden_state = torch.rand(num_layers, batch_size, hidden_size)\n\n    # RNN implemented in pysyft\n    rnn_syft = syft_nn.RNN(input_size, hidden_size, num_layers)\n\n    # RNN implemented in original pytorch\n    rnn_torch = nn.RNN(input_size, hidden_size, num_layers)\n\n    # Make sure the weights of both RNN are identical\n    rnn_syft.rnn_forward[0].fc_xh.weight = rnn_torch.weight_ih_l0\n    rnn_syft.rnn_forward[0].fc_xh.bias = rnn_torch.bias_ih_l0\n    rnn_syft.rnn_forward[0].fc_hh.weight = rnn_torch.weight_hh_l0\n    rnn_syft.rnn_forward[0].fc_hh.bias = rnn_torch.bias_hh_l0\n\n    output_syft, hidden_syft = rnn_syft(test_input, test_hidden_state)\n    output_torch, hidden_torch = rnn_torch(test_input, test_hidden_state)\n\n    # Reset mkldnn to the original state\n    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)\n\n    # Assert the hidden_state and output of both models are identical separately\n    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))\n    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))\n\n\ndef test_GRU():\n    """"""\n    Test the GRU module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n\n    # Disable mkldnn to avoid rounding errors due to difference in implementation\n    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()\n    torch._C._set_mkldnn_enabled(False)\n\n    batch_size = 5\n    input_size = 10\n    hidden_size = 50\n    num_layers = 1\n    seq_len = 8\n\n    test_input = torch.rand(seq_len, batch_size, input_size)\n    test_hidden_state = torch.rand(num_layers, batch_size, hidden_size)\n\n    # GRU implemented in pysyft\n    rnn_syft = syft_nn.GRU(input_size, hidden_size, num_layers)\n\n    # GRU implemented in original pytorch\n    rnn_torch = nn.GRU(input_size, hidden_size, num_layers)\n\n    # Make sure the weights of both GRU are identical\n    rnn_syft.rnn_forward[0].fc_xh.weight = rnn_torch.weight_ih_l0\n    rnn_syft.rnn_forward[0].fc_xh.bias = rnn_torch.bias_ih_l0\n    rnn_syft.rnn_forward[0].fc_hh.weight = rnn_torch.weight_hh_l0\n    rnn_syft.rnn_forward[0].fc_hh.bias = rnn_torch.bias_hh_l0\n\n    output_syft, hidden_syft = rnn_syft(test_input, test_hidden_state)\n    output_torch, hidden_torch = rnn_torch(test_input, test_hidden_state)\n\n    # Reset mkldnn to the original state\n    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)\n\n    # Assert the hidden_state and output of both models are identical separately\n    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))\n    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))\n\n\ndef test_LSTM():\n    """"""\n    Test the LSTM module to ensure that it produces the exact same\n    output as the primary torch implementation, in the same order.\n    """"""\n\n    # Disable mkldnn to avoid rounding errors due to difference in implementation\n    mkldnn_enabled_init = torch._C._get_mkldnn_enabled()\n    torch._C._set_mkldnn_enabled(False)\n\n    batch_size = 5\n    input_size = 10\n    hidden_size = 50\n    num_layers = 1\n    seq_len = 8\n\n    test_input = torch.rand(seq_len, batch_size, input_size)\n    test_hidden_state = torch.rand(num_layers, batch_size, hidden_size)\n    test_cell_state = torch.rand(num_layers, batch_size, hidden_size)\n\n    # LSTM implemented in pysyft\n    rnn_syft = syft_nn.LSTM(input_size, hidden_size, num_layers)\n\n    # LSTM implemented in original pytorch\n    rnn_torch = nn.LSTM(input_size, hidden_size, num_layers)\n\n    # Make sure the weights of both LSTM are identical\n    rnn_syft.rnn_forward[0].fc_xh.weight = rnn_torch.weight_ih_l0\n    rnn_syft.rnn_forward[0].fc_xh.bias = rnn_torch.bias_ih_l0\n    rnn_syft.rnn_forward[0].fc_hh.weight = rnn_torch.weight_hh_l0\n    rnn_syft.rnn_forward[0].fc_hh.bias = rnn_torch.bias_hh_l0\n\n    output_syft, (hidden_syft, cell_syft) = rnn_syft(\n        test_input, (test_hidden_state, test_cell_state)\n    )\n    output_torch, (hidden_torch, cell_torch) = rnn_torch(\n        test_input, (test_hidden_state, test_cell_state)\n    )\n\n    # Reset mkldnn to the original state\n    torch._C._set_mkldnn_enabled(mkldnn_enabled_init)\n\n    # Assert the hidden_state, cell_state and output of both models are identical separately\n    assert torch.all(torch.lt(torch.abs(output_syft - output_torch), 1e-6))\n    assert torch.all(torch.lt(torch.abs(hidden_syft - hidden_torch), 1e-6))\n    assert torch.all(torch.lt(torch.abs(cell_syft - cell_torch), 1e-6))\n'"
test/torch/tensors/__init__.py,0,b''
test/torch/tensors/test_additive_shared.py,224,"b'import pytest\n\nimport torch\n\nimport syft\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\n\n\ndef test_wrap(workers):\n    """"""\n    Test the .on() wrap functionality for AdditiveSharingTensor\n    """"""\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = AdditiveSharingTensor().on(x_tensor)\n    assert isinstance(x, torch.Tensor)\n    assert isinstance(x.child, AdditiveSharingTensor)\n    assert isinstance(x.child.child, torch.Tensor)\n\n\ndef test___str__(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x_sh = torch.tensor([[3, 4]]).share(alice, bob, crypto_provider=james)\n    assert isinstance(x_sh.__str__(), str)\n\n\ndef test_share_get(workers):\n\n    t = torch.tensor([1, 2, 3])\n    x = t.share(workers[""bob""], workers[""alice""], workers[""james""])\n\n    x = x.get()\n\n    assert (x == t).all()\n\n\ndef test___bool__(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x_sh = torch.tensor([[3, 4]]).share(alice, bob, crypto_provider=james)\n\n    with pytest.raises(ValueError):\n        if x_sh:  # pragma: no cover\n            pass\n\n\ndef test_share_inplace_consistency(workers):\n    """"""Verify that share_ produces the same output then share""""""\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x1 = torch.tensor([-1.0])\n    x1.fix_precision_().share_(alice, bob, crypto_provider=james)\n\n    x2 = torch.tensor([-1.0])\n    x2_sh = x2.fix_precision().share(alice, bob, crypto_provider=james)\n\n    assert (x1 == x2_sh).get().float_prec()\n\n\ndef test_clone(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x = torch.tensor([1.2]).fix_precision().share(alice, bob, crypto_provider=james)\n    original_props = (\n        x.id,\n        x.owner.id,\n        x.child.id,\n        x.child.owner.id,\n        x.child.child.id,\n        x.child.child.owner.id,\n    )\n    xc = x.clone()\n    cloned_props = (\n        xc.id,\n        xc.owner.id,\n        xc.child.id,\n        xc.child.owner.id,\n        xc.child.child.id,\n        xc.child.child.owner.id,\n    )\n    assert original_props == cloned_props\n\n\ndef test_virtual_get(workers):\n    t = torch.tensor([1, 2, 3])\n    x = t.share(workers[""bob""], workers[""alice""], workers[""james""])\n\n    x = x.child.virtual_get()\n\n    assert (x == t).all()\n\n\ndef test_non_client_registration(hook, workers):\n    hook.local_worker.is_client_worker = False\n    bob, alice, james = workers[""bob""], workers[""alice""], workers[""james""]\n    x = torch.tensor([-1.0])\n    x_sh = x.fix_precision().share(alice, bob, crypto_provider=james)\n\n    assert x_sh.id in hook.local_worker.object_store._objects\n    assert (x_sh == hook.local_worker.get_obj(x_sh.id)).get().float_prec()\n    hook.local_worker.is_client_worker = True\n\n\ndef test_autograd_kwarg(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    t = torch.tensor([1, 2, 3])\n    x = t.share(alice, bob, crypto_provider=james, requires_grad=True)\n\n    assert isinstance(x.child, syft.AutogradTensor)\n\n\ndef test_send_get(workers):\n    # For int dtype\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x_sh = torch.tensor([[3, 4]]).fix_prec(dtype=""int"").share(alice, bob, crypto_provider=james)\n\n    alice_t_id = x_sh.child.child.child[""alice""].id_at_location\n    assert alice_t_id in alice.object_store._objects\n\n    ptr_x = x_sh.send(james)\n    ptr_x_id_at_location = ptr_x.id_at_location\n    assert ptr_x_id_at_location in james.object_store._objects\n    assert alice_t_id in alice.object_store._objects\n\n    x_sh_back = ptr_x.get()\n    assert ptr_x_id_at_location not in james.object_store._objects\n    assert alice_t_id in alice.object_store._objects\n\n    x = x_sh_back.get()\n    assert alice_t_id not in alice.object_store._objects\n\n    # For long dtype\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x_sh = torch.tensor([[3, 4]]).fix_prec().share(alice, bob, crypto_provider=james)\n\n    alice_t_id = x_sh.child.child.child[""alice""].id_at_location\n    assert alice_t_id in alice.object_store._objects\n\n    ptr_x = x_sh.send(james)\n    ptr_x_id_at_location = ptr_x.id_at_location\n    assert ptr_x_id_at_location in james.object_store._objects\n    assert alice_t_id in alice.object_store._objects\n\n    x_sh_back = ptr_x.get()\n    assert ptr_x_id_at_location not in james.object_store._objects\n    assert alice_t_id in alice.object_store._objects\n\n    x = x_sh_back.get()\n    assert alice_t_id not in alice.object_store._objects\n\n\ndef test_add(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    # 2 workers\n    t = torch.tensor([1, 2, 3])\n    x = torch.tensor([1, 2, 3]).share(bob, alice)\n\n    y = (x + x).get()\n\n    # 3 workers\n    assert (y == (t + t)).all()\n\n    t = torch.tensor([1, 2, 3])\n    x = torch.tensor([1, 2, 3]).share(bob, alice, james)\n\n    y = (x + x).get()\n\n    # negative numbers\n    assert (y == (t + t)).all()\n\n    t = torch.tensor([1, -2, 3])\n    x = torch.tensor([1, -2, 3]).share(bob, alice, james)\n\n    y = (x + x).get()\n\n    assert (y == (t + t)).all()\n\n    # with fixed precisions\n    t = torch.tensor([1.0, -2, 3])\n    x = torch.tensor([1.0, -2, 3]).fix_prec().share(bob, alice, james)\n\n    y = (x + x).get().float_prec()\n\n    assert (y == (t + t)).all()\n\n    # with FPT>torch.tensor\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    y = t.fix_prec()\n\n    z = (x + y).get().float_prec()\n\n    assert (z == (t + t)).all()\n\n    z = (y + x).get().float_prec()\n\n    assert (z == (t + t)).all()\n\n    # with constant integer\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec().share(alice, bob, crypto_provider=james)\n    c = 4\n\n    z = (x + c).get().float_prec()\n    assert (z == (t + c)).all()\n\n    z = (c + x).get().float_prec()\n    assert (z == (c + t)).all()\n\n    # with constant float\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec().share(alice, bob, crypto_provider=james)\n    c = 4.2\n\n    z = (x + c).get().float_prec()\n    assert ((z - (t + c)) < 10e-3).all()\n\n    z = (c + x).get().float_prec()\n    assert ((z - (c + t)) < 10e-3).all()\n\n    # with dtype int\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec(dtype=""int"").share(alice, bob, crypto_provider=james)\n    y = x + x\n    assert (y.get().float_prec() == torch.tensor([2.0, -4.0, 6.0])).all()\n\n\ndef test_sub(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    # 3 workers\n    t = torch.tensor([1, 2, 3])\n    x = torch.tensor([1, 2, 3]).share(bob, alice, james)\n\n    y = (x - x).get()\n\n    assert (y == (t - t)).all()\n\n    # negative numbers\n    t = torch.tensor([1, -2, 3])\n    x = torch.tensor([1, -2, 3]).share(bob, alice, james)\n\n    y = (x - x).get()\n\n    assert (y == (t - t)).all()\n\n    # with fixed precision\n    t = torch.tensor([1.0, -2, 3])\n    x = torch.tensor([1.0, -2, 3]).fix_prec().share(bob, alice, james)\n\n    y = (x - x).get().float_prec()\n\n    assert (y == (t - t)).all()\n\n    # with FPT>torch.tensor\n    t = torch.tensor([1.0, -2.0, 3.0])\n    u = torch.tensor([4.0, 3.0, 2.0])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    y = u.fix_prec()\n\n    z = (x - y).get().float_prec()\n\n    assert (z == (t - u)).all()\n\n    z = (y - x).get().float_prec()\n\n    assert (z == (u - t)).all()\n\n    # with constant integer\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec().share(alice, bob, crypto_provider=james)\n    c = 4\n\n    z = (x - c).get().float_prec()\n    assert (z == (t - c)).all()\n\n    z = (c - x).get().float_prec()\n    assert (z == (c - t)).all()\n\n    # with constant float\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec().share(alice, bob, crypto_provider=james)\n    c = 4.2\n\n    z = (x - c).get().float_prec()\n    assert ((z - (t - c)) < 10e-3).all()\n\n    z = (c - x).get().float_prec()\n    assert ((z - (c - t)) < 10e-3).all()\n\n    # with dtype int\n    t = torch.tensor([1.0, -2.0, 3.0])\n    u = torch.tensor([4.0, 3.0, 2.0])\n    x = t.fix_prec(dtype=""int"").share(alice, bob, crypto_provider=james)\n    y = u.fix_prec(dtype=""int"").share(alice, bob, crypto_provider=james)\n    z = y - x\n    assert (z.get().float_prec() == torch.tensor([3.0, 5.0, -1.0])).all()\n\n\ndef test_mul(workers):\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n    bob, alice, james, charlie = (\n        workers[""bob""],\n        workers[""alice""],\n        workers[""james""],\n        workers[""charlie""],\n    )\n\n    # 2 workers\n    t = torch.tensor([1, 2, 3, 4])\n    x = t.share(bob, alice, crypto_provider=james)\n    y = x * x\n    assert (y.get() == (t * t)).all()\n\n    # 3 workers\n    t = torch.tensor([1, 2, 3, 4])\n    x = t.share(bob, alice, charlie, crypto_provider=james)\n    y = x * x\n    assert (y.get() == (t * t)).all()\n\n    # with fixed precision\n    x = torch.tensor([1, -2, -3, 4.0]).fix_prec().share(bob, alice, crypto_provider=james)\n    y = torch.tensor([-1, 2, -3, 4.0]).fix_prec().share(bob, alice, crypto_provider=james)\n    y = (x * y).get().float_prec()\n\n    assert (y == torch.tensor([-1, -4, 9, 16.0])).all()\n\n    # with non-default fixed precision\n    t = torch.tensor([1, 2, 3, 4.0])\n    x = t.fix_prec(precision_fractional=2).share(bob, alice, crypto_provider=james)\n    y = (x * x).get().float_prec()\n\n    assert (y == (t * t)).all()\n\n    # with FPT>torch.tensor\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    y = t.fix_prec()\n\n    z = (x * y).get().float_prec()\n\n    assert (z == (t * t)).all()\n\n    # with dtype int\n    x = (\n        torch.tensor([1, -2, -3, 4.0])\n        .fix_prec(dtype=""int"")\n        .share(bob, alice, crypto_provider=james)\n    )\n    y = (\n        torch.tensor([-1, 2, -3, 4.0])\n        .fix_prec(dtype=""int"")\n        .share(bob, alice, crypto_provider=james)\n    )\n    z = x * y\n    assert (z.get().float_prec() == torch.tensor([-1, -4, 9, 16.0])).all()\n\n\ndef test_public_mul(workers):\n    bob, alice, james, charlie = (\n        workers[""bob""],\n        workers[""alice""],\n        workers[""james""],\n        workers[""charlie""],\n    )\n\n    t = torch.tensor([-3.1, 1.0])\n    x = t.fix_prec().share(alice, bob, crypto_provider=james)\n    y = 1\n    z = (x * y).get().float_prec()\n    assert (z == (t * y)).all()\n\n    # 3 workers\n    t = torch.tensor([-3.1, 1.0])\n    x = t.fix_prec().share(alice, bob, charlie, crypto_provider=james)\n    y = 1\n    z = (x * y).get().float_prec()\n    assert (z == (t * y)).all()\n\n    t = torch.tensor([-3.1, 1.0])\n    x = t.fix_prec().share(alice, bob, crypto_provider=james)\n    y = 0\n    z = (x * y).get().float_prec()\n    assert (z == (t * y)).all()\n\n    t_x = torch.tensor([-3.1, 1])\n    t_y = torch.tensor([1.0])\n    x = t_x.fix_prec().share(alice, bob, crypto_provider=james)\n    y = t_y.fix_prec()\n    z = x * y\n    z = z.get().float_prec()\n    assert (z == t_x * t_y).all()\n\n    t_x = torch.tensor([-3.1, 1])\n    t_y = torch.tensor([0.0])\n    x = t_x.fix_prec().share(alice, bob, crypto_provider=james)\n    y = t_y.fix_prec()\n    z = x * y\n    z = z.get().float_prec()\n    assert (z == t_x * t_y).all()\n\n    t_x = torch.tensor([-3.1, 1])\n    t_y = torch.tensor([0.0, 2.1])\n    x = t_x.fix_prec().share(alice, bob, crypto_provider=james)\n    y = t_y.fix_prec()\n    z = x * y\n    z = z.get().float_prec()\n    assert (z == t_x * t_y).all()\n\n    # with dtype int\n    t_x = torch.tensor([-3.1, 1])\n    t_y = torch.tensor([0.0, 2.1])\n    x = t_x.fix_prec(dtype=""int"").share(alice, bob, crypto_provider=james)\n    y = t_y.fix_prec(dtype=""int"")\n    z = x * y\n    z = z.get().float_prec()\n    assert (z == t_x * t_y).all()\n\n\ndef test_div(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    # With scalar\n    t = torch.tensor([[9.0, 12.0], [3.3, 0.0]])\n    x = t.fix_prec(dtype=""long"").share(bob, alice, crypto_provider=james)\n    y = (x / 3).get().float_prec()\n\n    assert (y == torch.tensor([[3.0, 4.0], [1.1, 0.0]])).all()\n\n    # With another encrypted tensor of same shape\n    t1 = torch.tensor([[25, 9], [10, 30]])\n    t2 = torch.tensor([[5, 12], [2, 7]])\n    x1 = t1.fix_prec(dtype=""long"").share(bob, alice, crypto_provider=james)\n    x2 = t2.fix_prec(dtype=""long"").share(bob, alice, crypto_provider=james)\n\n    y = (x1 / x2).get().float_prec()\n    assert (y == torch.tensor([[5.0, 0.75], [5.0, 4.285]])).all()\n\n    # With another encrypted single value\n    t1 = torch.tensor([[25.0, 9], [10, 30]])\n    t2 = torch.tensor([5.0])\n    x1 = t1.fix_prec(dtype=""long"").share(bob, alice, crypto_provider=james)\n    x2 = t2.fix_prec(dtype=""long"").share(bob, alice, crypto_provider=james)\n\n    y = (x1 / x2).get().float_prec()\n    assert (y == torch.tensor([[5.0, 1.8], [2.0, 6.0]])).all()\n\n\ndef test_pow(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    m = torch.tensor([[1, 2], [3, 4.0]])\n    x = m.fix_prec().share(bob, alice, crypto_provider=james)\n    y = (x ** 3).get().float_prec()\n\n    assert (y == (m ** 3)).all()\n\n\ndef test_operate_with_integer_constants(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    x = torch.tensor([2.0])\n    x_sh = x.fix_precision().share(alice, bob, crypto_provider=james)\n\n    r_sh = x_sh + 10\n    assert r_sh.get().float_prec() == x + 10\n\n    r_sh = x_sh - 7\n    assert r_sh.get().float_prec() == x - 7\n\n    r_sh = x_sh * 2\n    assert r_sh.get().float_prec() == x * 2\n\n    r_sh = x_sh / 2\n    assert r_sh.get().float_prec() == x / 2\n\n\ndef test_stack(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    t = torch.tensor([1.3, 2])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    res = torch.stack([x, x]).get().float_prec()\n\n    expected = torch.tensor([[1.3000, 2.0000], [1.3000, 2.0000]])\n\n    assert (res == expected).all()\n\n\ndef test_cat(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    x = t.share(bob, alice, crypto_provider=james)\n\n    res0 = torch.cat([x, x], dim=0).get()\n    res1 = torch.cat([x, x], dim=1).get()\n\n    expected0 = torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6]])\n    expected1 = torch.tensor([[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]])\n\n    assert (res0 == expected0).all()\n    assert (res1 == expected1).all()\n\n    # Test when using more tensors\n    res2 = torch.cat([x, x, x], dim=1).get()\n    expected2 = torch.tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6, 4, 5, 6]])\n\n    assert (res2 == expected2).all()\n\n\ndef test_chunk(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    t = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n    x = t.share(bob, alice, crypto_provider=james)\n\n    res0 = torch.chunk(x, 2, dim=0)\n    res1 = torch.chunk(x, 2, dim=1)\n\n    expected0 = [torch.tensor([[1, 2, 3, 4]]), torch.tensor([[5, 6, 7, 8]])]\n    expected1 = [torch.tensor([[1, 2], [5, 6]]), torch.tensor([[3, 4], [7, 8]])]\n\n    assert all(((res0[i].get() == expected0[i]).all() for i in range(2)))\n    assert all(((res1[i].get() == expected1[i]).all() for i in range(2)))\n\n\ndef test_roll(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n    t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    x = t.share(bob, alice, crypto_provider=james)\n\n    res1 = torch.roll(x, 2)\n    res2 = torch.roll(x, 2, dims=1)\n    res3 = torch.roll(x, (1, 2), dims=(0, 1))\n\n    assert (res1.get() == torch.roll(t, 2)).all()\n    assert (res2.get() == torch.roll(t, 2, dims=1)).all()\n    assert (res3.get() == torch.roll(t, (1, 2), dims=(0, 1))).all()\n\n    # With MultiPointerTensor\n    shifts = torch.tensor(1).send(alice, bob)\n    res = torch.roll(x, shifts)\n\n    shifts1 = torch.tensor(1).send(alice, bob)\n    shifts2 = torch.tensor(2).send(alice, bob)\n    res2 = torch.roll(x, (shifts1, shifts2), dims=(0, 1))\n\n    assert (res.get() == torch.roll(t, 1)).all()\n    assert (res2.get() == torch.roll(t, (1, 2), dims=(0, 1))).all()\n\n\ndef test_matmul(workers):\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    m = torch.tensor([[1, 2], [3, 4.0]])\n    x = m.fix_prec().share(bob, alice, crypto_provider=james)\n    y = (x @ x).get().float_prec()\n\n    assert (y == (m @ m)).all()\n\n    # with FPT>torch.tensor\n    m = torch.tensor([[1, 2], [3, 4.0]])\n    x = m.fix_prec().share(bob, alice, crypto_provider=james)\n    y = m.fix_prec()\n\n    z = (x @ y).get().float_prec()\n\n    assert (z == (m @ m)).all()\n\n    z = (y @ x).get().float_prec()\n\n    assert (z == (m @ m)).all()\n\n\ndef test_mm(workers):\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    t = torch.tensor([[1, 2], [3, 4.0]])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n\n    # Using the method\n    y = (x.mm(x)).get().float_prec()\n    assert (y == (t.mm(t))).all()\n\n    # Using the function\n    y = (torch.mm(x, x)).get().float_prec()\n    assert (y == (torch.mm(t, t))).all()\n\n    # with FPT>torch.tensor\n    t = torch.tensor([[1, 2], [3, 4.0]])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    y = t.fix_prec()\n\n    # Using the method\n    z = (x.mm(y)).get().float_prec()\n    assert (z == (t.mm(t))).all()\n\n    # Using the function\n    z = (torch.mm(x, y)).get().float_prec()\n    assert (z == (torch.mm(t, t))).all()\n\n    # Using the method\n    z = (y.mm(x)).get().float_prec()\n    assert (z == (t.mm(t))).all()\n\n    # Using the function\n    z = (torch.mm(y, x)).get().float_prec()\n    assert (z == (torch.mm(t, t))).all()\n\n\ndef test_fixed_precision_and_sharing(workers):\n\n    bob, alice = (workers[""bob""], workers[""alice""])\n\n    t = torch.tensor([1, 2, 3, 4.0])\n    x = t.fix_prec().share(bob, alice)\n    out = x.get().float_prec()\n\n    assert (out == t).all()\n\n    x = t.fix_prec().share(bob, alice)\n\n    y = x + x\n\n    y = y.get().float_prec()\n    assert (y == (t + t)).all()\n\n\ndef test_fixed_precision_and_sharing_on_pointer(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    t = torch.tensor([1, 2, 3, 4.0])\n    ptr = t.send(james)\n\n    x = ptr.fix_prec().share(bob, alice)\n\n    y = x + x\n\n    y = y.get().get().float_prec()\n    assert (y == (t + t)).all()\n\n\ndef test_pointer_on_fixed_precision_and_sharing(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    t = torch.tensor([1, 2, 3, 4.0])\n\n    x = t.fix_prec().share(bob, alice)\n    x = x.send(james)\n\n    y = x + x\n\n    y = y.get().get().float_prec()\n    assert (y == (t + t)).all()\n\n\ndef test_get_item(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    x = torch.tensor([[3.1, 4.3]]).fix_prec().share(alice, bob, crypto_provider=james)\n    idx = torch.tensor([0]).send(alice, bob)\n\n    # Operate directly AST[MPT]\n    assert x.child.child[:, idx.child].get() == torch.tensor([[3100]])\n\n    # With usual wrappers and FPT\n    x = torch.tensor([[3, 4]]).share(alice, bob, crypto_provider=james)\n    idx = torch.tensor([0]).send(alice, bob)\n    assert x[:, idx].get() == torch.tensor([[3]])\n\n\n@pytest.mark.parametrize(""protocol"", [""snn"", ""fss""])\ndef test_eq(workers, protocol):\n    me, alice, bob, crypto_provider = (\n        workers[""me""],\n        workers[""alice""],\n        workers[""bob""],\n        workers[""james""],\n    )\n\n    if protocol == ""fss"":\n        for worker in workers.values():\n            syft.frameworks.torch.mpc.fss.initialize_crypto_plans(worker)\n        me.crypto_store.provide_primitives([""fss_eq""], [alice, bob], n_instances=6)\n\n    args = (alice, bob)\n    kwargs = {""protocol"": protocol, ""crypto_provider"": crypto_provider}\n\n    x = torch.tensor([3.1]).fix_prec().share(*args, **kwargs)\n    y = torch.tensor([3.1]).fix_prec().share(*args, **kwargs)\n\n    assert (x == y).get().float_prec()\n\n    x = torch.tensor([3.1]).fix_prec().share(*args, **kwargs)\n    y = torch.tensor([2.1]).fix_prec().share(*args, **kwargs)\n\n    assert not (x == y).get().float_prec()\n\n    x = torch.tensor([-3.1]).fix_prec().share(*args, **kwargs)\n    y = torch.tensor([-3.1]).fix_prec().share(*args, **kwargs)\n\n    assert (x == y).get().float_prec()\n\n\n@pytest.mark.parametrize(""protocol"", [""snn"", ""fss""])\ndef test_comp(workers, protocol):\n    me, alice, bob, crypto_provider = (\n        workers[""me""],\n        workers[""alice""],\n        workers[""bob""],\n        workers[""james""],\n    )\n\n    if protocol == ""fss"":\n        for worker in workers.values():\n            syft.frameworks.torch.mpc.fss.initialize_crypto_plans(worker)\n        me.crypto_store.provide_primitives(\n            [""xor_add_couple"", ""fss_eq"", ""fss_comp""], [alice, bob], n_instances=50\n        )\n\n    args = (alice, bob)\n    kwargs = {""protocol"": protocol, ""crypto_provider"": crypto_provider}\n\n    x = torch.tensor([3.1]).fix_prec().share(*args, **kwargs)\n    y = torch.tensor([3.1]).fix_prec().share(*args, **kwargs)\n\n    assert (x >= y).get().float_prec()\n    assert (x <= y).get().float_prec()\n    assert not (x > y).get().float_prec()\n    assert not (x < y).get().float_prec()\n\n    x = torch.tensor([-3.1]).fix_prec().share(*args, **kwargs)\n    y = torch.tensor([-3.1]).fix_prec().share(*args, **kwargs)\n\n    assert (x >= y).get().float_prec()\n    assert (x <= y).get().float_prec()\n    assert not (x > y).get().float_prec()\n    assert not (x < y).get().float_prec()\n\n    x = torch.tensor([3.1]).fix_prec().share(*args, **kwargs)\n    y = torch.tensor([2.1]).fix_prec().share(*args, **kwargs)\n\n    assert (x >= y).get().float_prec()\n    assert not (x <= y).get().float_prec()\n    assert (x > y).get().float_prec()\n    assert not (x < y).get().float_prec()\n\n    t1 = torch.tensor([-2.1, 1.8])\n    t2 = torch.tensor([-3.1, 0.3])\n    x = t1.fix_prec().share(*args, **kwargs)\n    y = t2.fix_prec().share(*args, **kwargs)\n\n    assert ((x >= y).get().float_prec() == (t1 >= t2)).all()\n    assert ((x <= y).get().float_prec() == (t1 <= t2)).all()\n    assert ((x > y).get().float_prec() == (t1 > t2)).all()\n    assert ((x < y).get().float_prec() == (t1 < t2)).all()\n\n    t1 = torch.tensor([[-2.1, 1.8], [-1.1, -0.7]])\n    t2 = torch.tensor([[-3.1, 0.3], [-1.1, 0.3]])\n    x = t1.fix_prec().share(*args, **kwargs)\n    y = t2.fix_prec().share(*args, **kwargs)\n\n    assert ((x >= y).get().float_prec() == (t1 >= t2)).all()\n    assert ((x <= y).get().float_prec() == (t1 <= t2)).all()\n    assert ((x > y).get().float_prec() == (t1 > t2)).all()\n    assert ((x < y).get().float_prec() == (t1 < t2)).all()\n\n\n@pytest.mark.parametrize(""protocol"", [""snn"", ""fss""])\ndef test_max(workers, protocol):\n    me, alice, bob, crypto_provider = (\n        workers[""me""],\n        workers[""alice""],\n        workers[""bob""],\n        workers[""james""],\n    )\n\n    if protocol == ""fss"":\n        for worker in workers.values():\n            syft.frameworks.torch.mpc.fss.initialize_crypto_plans(worker)\n        me.crypto_store.provide_primitives(\n            [""xor_add_couple"", ""fss_eq"", ""fss_comp""], [alice, bob], n_instances=16\n        )\n\n    args = (alice, bob)\n    kwargs = {""protocol"": protocol, ""crypto_provider"": crypto_provider}\n\n    t = torch.tensor([3, 1.0, 2])\n    x = t.fix_prec().share(*args, **kwargs)\n    max_value = x.max().get().float_prec()\n    assert max_value == torch.tensor([3.0])\n\n    t = torch.tensor([3, 4.0])\n    x = t.fix_prec().share(*args, **kwargs)\n    max_value = x.max().get().float_prec()\n    assert max_value == torch.tensor([4.0])\n\n    t = torch.tensor([3, 4.0, 5, 2])\n    x = t.fix_prec().share(*args, **kwargs)\n    max_value = x.max().get().float_prec()\n    assert max_value == torch.tensor([5.0])\n\n\n@pytest.mark.parametrize(""protocol"", [""snn"", ""fss""])\ndef test_argmax(workers, protocol):\n    me, alice, bob, crypto_provider = (\n        workers[""me""],\n        workers[""alice""],\n        workers[""bob""],\n        workers[""james""],\n    )\n\n    if protocol == ""fss"":\n        for worker in workers.values():\n            syft.frameworks.torch.mpc.fss.initialize_crypto_plans(worker)\n        me.crypto_store.provide_primitives(\n            [""xor_add_couple"", ""fss_eq"", ""fss_comp""], [alice, bob], n_instances=32\n        )\n\n    args = (alice, bob)\n    kwargs = {""protocol"": protocol, ""crypto_provider"": crypto_provider}\n\n    t = torch.tensor([3, 1.0, 2])\n    x = t.fix_prec().share(*args, **kwargs)\n    idx = x.argmax().get().float_prec()\n    assert idx == torch.tensor([0.0])\n\n    t = torch.tensor([3, 4.0])\n    x = t.fix_prec().share(*args, **kwargs)\n    idx = x.argmax().get().float_prec()\n    assert idx == torch.tensor([1.0])\n\n    t = torch.tensor([3, 4.0, 5, 2])\n    x = t.fix_prec().share(*args, **kwargs)\n    idx = x.argmax().get().float_prec()\n    assert idx == torch.tensor([2.0])\n\n    # no dim=\n    t = torch.tensor([[1, 2.0, 4], [3, 9.0, 2.0]])\n    x = t.fix_prec().share(*args, **kwargs)\n    ids = x.argmax().get().float_prec()\n    assert ids.long() == torch.argmax(t)\n\n    # dim=1\n    t = torch.tensor([[1, 2.0, 4], [3, 1.0, 2.0]])\n    x = t.fix_prec().share(*args, **kwargs)\n    ids = x.argmax(dim=1).get().float_prec()\n    assert (ids.long() == torch.argmax(t, dim=1)).all()\n\n\ndef test_mod(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    t = torch.tensor([21]).share(bob, alice, crypto_provider=james)\n    assert t.child.mod(8).get() % 8 == torch.tensor([5])\n    assert t.child.mod(-8).get() % -8 == torch.tensor([-3])\n\n    t = torch.tensor([-21]).share(bob, alice, crypto_provider=james)\n    assert t.child.mod(8).get() % 8 == torch.tensor([3])\n    assert t.child.mod(-8).get() % -8 == torch.tensor([-5])\n\n    assert (t.child % 8).get() % 8 == torch.tensor([3])\n\n\ndef test_torch_sum(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    t = torch.tensor([[1, 2, 4], [8, 5, 6]])\n    x = t.share(alice, bob, crypto_provider=james)\n\n    s = torch.sum(x).get()\n    s_dim = torch.sum(x, 0).get()\n    s_dim2 = torch.sum(x, (0, 1)).get()\n    s_keepdim = torch.sum(x, 1, keepdim=True).get()\n\n    assert (s == torch.sum(t)).all()\n    assert (s_dim == torch.sum(t, 0)).all()\n    assert (s_dim2 == torch.sum(t, (0, 1))).all()\n    assert (s_keepdim == torch.sum(t, 1, keepdim=True)).all()\n\n\ndef test_torch_mean(workers):\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n    base = 10\n    prec_frac = 4\n\n    t = torch.tensor([[1.0, 2.5], [8.0, 5.5]])\n    x = t.fix_prec(base=base, precision_fractional=prec_frac).share(\n        alice, bob, crypto_provider=james\n    )\n\n    s = torch.mean(x).get().float_prec()\n    s_dim = torch.mean(x, 0).get().float_prec()\n    s_dim2 = torch.mean(x, (0, 1)).get().float_prec()\n    s_keepdim = torch.mean(x, 1, keepdim=True).get().float_prec()\n\n    assert (s == torch.tensor(4.25)).all()\n    assert (s_dim == torch.tensor([4.5, 4.0])).all()\n    assert (s_dim2 == torch.tensor(4.25)).all()\n    assert (s_keepdim == torch.tensor([[1.75], [6.75]])).all()\n\n\ndef test_torch_dot(workers):\n    torch.manual_seed(121)  # Truncation might not always work so we set the random seed\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]).fix_prec().share(alice, bob, crypto_provider=james)\n    y = torch.tensor([3.0, 3.0, 3.0, 3.0, 3.0]).fix_prec().share(alice, bob, crypto_provider=james)\n\n    assert torch.dot(x, y).get().float_prec() == 45\n\n\ndef test_unbind(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    x = torch.tensor([21, 17]).share(bob, alice, crypto_provider=james).child\n\n    x0, x1 = torch.unbind(x)\n\n    assert x0.get() == torch.tensor(21)\n    assert x1.get() == torch.tensor(17)\n\n\ndef test_handle_func_command(workers):\n    """"""\n    Just to show that handle_func_command works\n    Even if torch.abs should be hooked to return a correct value\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    t = torch.tensor([-21]).share(bob, alice, crypto_provider=james).child\n    _ = torch.abs(t).get()\n\n\ndef test_init_with_no_crypto_provider(workers):\n    alice, bob = workers[""alice""], workers[""bob""]\n\n    x = torch.tensor([21, 17]).share(bob, alice).child\n\n    assert x.crypto_provider.id == syft.hook.local_worker.id\n\n\ndef test_zero_refresh(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    t = torch.tensor([2.2, -1.0])\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n\n    x_sh = x.child.child\n    assert (x_sh.zero().get() == torch.zeros(*t.shape).long()).all()\n\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    x_copy = t.fix_prec().share(bob, alice, crypto_provider=james)\n    x_r = x.refresh()\n\n    assert (x_r.get().float_prec() == x_copy.get().float_prec()).all()\n\n    x = t.fix_prec().share(bob, alice, crypto_provider=james)\n    x_r = x.refresh()\n\n    assert ((x_r / 2).get().float_prec() == t / 2).all()\n\n\ndef test_correct_tag_and_description_after_send(workers):\n    bob, alice, james, me = (workers[""bob""], workers[""alice""], workers[""james""], workers[""me""])\n\n    x = torch.tensor([1, 2, 3]).share(alice, bob, james)\n    x.tags = [""tag_additive_test1"", ""tag_additive_test2""]\n    x.description = ""description_additive_test""\n\n    pointer_x = x.send(alice)\n\n    assert me.request_search(""tag_additive_test1"", location=alice)\n    assert me.request_search(""tag_additive_test2"", location=alice)\n\n\ndef test_dtype(workers):\n    alice, bob, james, me = (workers[""bob""], workers[""alice""], workers[""james""], workers[""me""])\n    # Without fix_prec\n    x = torch.tensor([1, 2, 3]).share(alice, bob, james, dtype=""long"")\n    assert (\n        x.child.dtype == ""long""\n        and x.child.field == 2 ** 64\n        and isinstance(\n            x.child.child[""alice""].location.object_store.get_obj(\n                x.child.child[""alice""].id_at_location\n            ),\n            torch.LongTensor,\n        )\n        and (x.get() == torch.LongTensor([1, 2, 3])).all()\n    )\n\n    x = torch.tensor([4, 5, 6]).share(alice, bob, james, dtype=""int"")\n    assert (\n        x.child.dtype == ""int""\n        and x.child.field == 2 ** 32\n        and isinstance(\n            x.child.child[""alice""].location.object_store.get_obj(\n                x.child.child[""alice""].id_at_location\n            ),\n            torch.IntTensor,\n        )\n        and (x.get() == torch.IntTensor([4, 5, 6])).all()\n    )\n\n    # With dtype custom\n    x = torch.tensor([1, 2, 3]).share(alice, bob, james, dtype=""custom"", field=67)\n    assert (\n        x.child.dtype == ""custom""\n        and x.child.field == 67\n        and isinstance(\n            x.child.child[""alice""].location.object_store.get_obj(\n                x.child.child[""alice""].id_at_location\n            ),\n            torch.IntTensor,\n        )\n        and (x.get() == torch.IntTensor([1, 2, 3])).all()\n    )\n\n    # With fix_prec\n    x = torch.tensor([1.1, 2.2, 3.3]).fix_prec().share(alice, bob, james)\n    assert (\n        x.child.child.dtype == ""long""\n        and x.child.child.field == 2 ** 64\n        and isinstance(\n            x.child.child.child[""alice""].location.object_store.get_obj(\n                x.child.child.child[""alice""].id_at_location\n            ),\n            torch.LongTensor,\n        )\n        and (x.get().float_prec() == torch.tensor([1.1, 2.2, 3.3])).all()\n    )\n\n    x = torch.tensor([4.1, 5.2, 6.3]).fix_prec(dtype=""int"").share(alice, bob, james)\n    assert (\n        x.child.child.dtype == ""int""\n        and x.child.child.field == 2 ** 32\n        and isinstance(\n            x.child.child.child[""alice""].location.object_store.get_obj(\n                x.child.child.child[""alice""].id_at_location\n            ),\n            torch.IntTensor,\n        )\n        and (x.get().float_prec() == torch.tensor([4.1, 5.2, 6.3])).all()\n    )\n\n\ndef test_garbage_collect_reconstruct(workers):\n    bob, alice, james, me = (workers[""bob""], workers[""alice""], workers[""james""], workers[""me""])\n    a = torch.ones(1, 5)\n    a_sh = a.encrypt(workers=[alice, bob], crypto_provider=james)\n    a_recon = a_sh.child.child.reconstruct()\n\n    assert len(alice.object_store._objects) == 2\n    assert len(bob.object_store._objects) == 2\n\n\ndef test_garbage_collect_move(workers):\n    bob, alice, me = (workers[""bob""], workers[""alice""], workers[""me""])\n    a = torch.ones(1, 5).send(alice)\n    b = a.copy().move(bob)\n\n    assert len(alice.object_store._objects) == 1\n    assert len(bob.object_store._objects) == 1\n\n\ndef test_garbage_collect_mul(workers):\n    bob, alice, james, me = (workers[""bob""], workers[""alice""], workers[""james""], workers[""me""])\n    a = torch.ones(1, 5)\n    b = torch.ones(1, 5)\n\n    a = a.encrypt(workers=[alice, bob], crypto_provider=james)\n    b = b.encrypt(workers=[alice, bob], crypto_provider=james)\n\n    for _ in range(3):\n        c = a * b\n\n    assert len(alice.object_store._objects) == 3\n    assert len(bob.object_store._objects) == 3\n'"
test/torch/tensors/test_fv.py,19,"b'import pytest\n\nfrom syft.frameworks.torch.he.fv.util.numth import is_prime\nfrom syft.frameworks.torch.he.fv.util.operations import multiply_many_except\nfrom syft.frameworks.torch.he.fv.modulus import CoeffModulus\nfrom syft.frameworks.torch.he.fv.encryption_params import EncryptionParams\nfrom syft.frameworks.torch.he.fv.modulus import SeqLevelType\nfrom syft.frameworks.torch.he.fv.context import Context\nfrom syft.frameworks.torch.he.fv.util.operations import poly_add_mod\nfrom syft.frameworks.torch.he.fv.util.operations import poly_mul_mod\nfrom syft.frameworks.torch.he.fv.util.operations import poly_negate_mod\nfrom syft.frameworks.torch.he.fv.util.operations import get_significant_count\nfrom syft.frameworks.torch.he.fv.integer_encoder import IntegerEncoder\nfrom syft.frameworks.torch.he.fv.key_generator import KeyGenerator\nfrom syft.frameworks.torch.he.fv.util.base_converter import BaseConvertor\nfrom syft.frameworks.torch.he.fv.util.rns_base import RNSBase\nfrom syft.frameworks.torch.he.fv.util.operations import invert_mod\nfrom syft.frameworks.torch.he.fv.util.operations import xgcd\nfrom syft.frameworks.torch.he.fv.util.operations import reverse_bit\nfrom syft.frameworks.torch.he.fv.encryptor import Encryptor\nfrom syft.frameworks.torch.he.fv.decryptor import Decryptor\n\n\n@pytest.mark.parametrize(\n    ""num, status"",\n    [\n        (0, False),\n        (2, True),\n        (3, True),\n        (4, False),\n        (5, True),\n        (221, False),\n        (65537, True),\n        (65536, False),\n        (59399, True),\n        (72307, True),\n        (36893488147419103, True),\n        (36893488147419107, False),\n        (72307 * 59399, False),\n    ],\n)\ndef test_is_prime(num, status):\n    assert is_prime(num) == status\n\n\n@pytest.mark.parametrize(\n    ""value, result"", [(0, 0), (2, 1), (3, 3), (4, 1), (255, 255), (256, 1), (172, 53)]\n)\ndef test_reverse_bit(value, result):\n    assert reverse_bit(value) == result\n\n\n@pytest.mark.parametrize(\n    ""poly_modulus, plain_modulus, coeff_bit_sizes"",\n    [(128, 2, [30, 40, 50]), (1024, 64, [30, 60, 60]), (64, 64, [30])],\n)\ndef test_EncryptionParams(poly_modulus, plain_modulus, coeff_bit_sizes):\n    params = EncryptionParams(\n        poly_modulus, CoeffModulus().create(poly_modulus, coeff_bit_sizes), plain_modulus\n    )\n    for i in range(len(coeff_bit_sizes)):\n        assert is_prime(params.coeff_modulus[i])\n\n\ndef test_CoeffModulus_create():\n    coeffModulus = CoeffModulus()\n    assert len(coeffModulus.create(2, [])) == 0\n\n    cm = coeffModulus.create(2, [3])\n    assert len(cm) == 1\n    assert cm[0] == 5\n\n    cm = coeffModulus.create(2, [3, 4])\n    assert len(cm) == 2\n    assert cm[0] == 5\n    assert cm[1] == 13\n\n    cm = coeffModulus.create(2, [3, 5, 4, 5])\n    assert len(cm) == 4\n    assert cm[0] == 5\n    assert cm[1] == 17\n    assert cm[2] == 13\n    assert cm[3] == 29\n\n    cm = coeffModulus.create(32, [30, 40, 30, 30, 40])\n    assert len(cm) == 5\n    assert cm[0] % 64 == 1\n    assert cm[1] % 64 == 1\n    assert cm[2] % 64 == 1\n    assert cm[3] % 64 == 1\n    assert cm[4] % 64 == 1\n\n\n@pytest.mark.parametrize(\n    ""poly_modulus, SeqLevelType, result"",\n    [\n        (1024, SeqLevelType.TC128, 1),\n        (1024, SeqLevelType.TC192, 1),\n        (1024, SeqLevelType.TC256, 1),\n        (2048, SeqLevelType.TC128, 1),\n        (2048, SeqLevelType.TC192, 1),\n        (2048, SeqLevelType.TC256, 1),\n        (4096, SeqLevelType.TC128, 3),\n        (4096, SeqLevelType.TC192, 3),\n        (4096, SeqLevelType.TC256, 1),\n        (8192, SeqLevelType.TC128, 5),\n        (8192, SeqLevelType.TC192, 4),\n        (8192, SeqLevelType.TC256, 3),\n        (16384, SeqLevelType.TC128, 9),\n        (16384, SeqLevelType.TC192, 6),\n        (16384, SeqLevelType.TC256, 5),\n        (32768, SeqLevelType.TC128, 16),\n        (32768, SeqLevelType.TC192, 11),\n        (32768, SeqLevelType.TC256, 9),\n    ],\n)\ndef test_CoeffModulus_bfv_default(poly_modulus, SeqLevelType, result):\n    coeffModulus = CoeffModulus()\n    assert len(coeffModulus.bfv_default(poly_modulus, SeqLevelType)) == result\n\n\n@pytest.mark.parametrize(\n    ""op1, op2, mod, result"",\n    [\n        ([0, 0], [0, 0], 3, [0, 0]),\n        ([1, 2, 3, 4], [2, 3, 4, 5], 3, [0, 2, 1, 0]),\n        ([1, 2, 3, 4], [2, 3, 4, 5], 1, [0, 0, 0, 0]),\n        ([1, 2, 3, 4, 5], [1, -4], 3, [1, 2, 0, 2, 1]),\n        ([4, 4], [-4, -4, -4, -4], 4, [0, 0, 0, 0]),\n    ],\n)\ndef test_poly_add_mod(op1, op2, mod, result):\n    assert poly_add_mod(op1, op2, mod) == result\n\n\n@pytest.mark.parametrize(\n    ""op1, op2, mod, result"",\n    [\n        ([1, 1], [2, 1], 5, [1, 3]),\n        ([1, 2, 3, 4], [2, 3, 4, 5], 5, [3, 1, 1]),\n        ([1, 2, 3, 4, 5], [1, -4], 3, [0, 1, 1, 1, 1]),\n        ([4, 4], [-4, -4, -4, -4], 4, [0]),\n    ],\n)\ndef test_poly_mul_mod(op1, op2, mod, result):\n    print(""test poly_mul_mod : "", poly_mul_mod(op1, op2, mod))\n    assert poly_mul_mod(op1, op2, mod) == result\n\n\n@pytest.mark.parametrize(""op1, mod, result"", [([2, 3], 7, [5, 4]), ([0, 0], 7, [0, 0])])\ndef test_poly_negate_mod(op1, mod, result):\n    assert poly_negate_mod(op1, mod) == result\n\n\n@pytest.mark.parametrize(\n    ""ptr, result"",\n    [\n        ([0, 0], 0),\n        ([1, 0], 1),\n        ([2, 0], 1),\n        ([0xFFFFFFFFFFFFFFFF, 0], 1),\n        ([0, 1], 2),\n        ([0xFFFFFFFFFFFFFFFF, 1], 2),\n        ([0xFFFFFFFFFFFFFFFF, 0x8000000000000000], 2),\n        ([0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF], 2),\n    ],\n)\ndef test_get_significant_count(ptr, result):\n    assert result == get_significant_count(ptr)\n\n\n@pytest.mark.parametrize(\n    ""plain_modulus, value"",\n    [\n        (0xFFFFFFFFFFFFFFF, 1),\n        (0xFFFFFFFFFFFFFFF, 2),\n        (0xFFFFFFFFFFFFFFF, -3),\n        (0xFFFFFFFFFFFFFFF, 64),\n        (0xFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF),\n        (0xFFFFFFFFFFFFFFF, 0x80F02),\n        (1024, 64),\n    ],\n)\ndef test_integer_encoder(plain_modulus, value):\n    enc_param = EncryptionParams(\n        16, CoeffModulus().create(plain_modulus, [100, 100, 100]), plain_modulus\n    )\n    ctx = Context(enc_param)\n    encoder = IntegerEncoder(ctx)\n    poly = encoder.encode(value)\n    assert value == encoder.decode(poly)\n\n\n@pytest.mark.parametrize(\n    ""operand, count, exp, result"",\n    [\n        ([0, 0, 0], 2, 0, 0),\n        ([0, 0, 0], 3, 0, 0),\n        ([0, 0, 0], 2, 0, 0),\n        ([2, 3, 5], 2, 0, 3),\n        ([2, 3, 5], 2, 1, 2),\n        ([2, 3, 5], 3, 0, 15),\n        ([2, 3, 5], 3, 1, 10),\n        ([2, 3, 5], 3, 2, 6),\n    ],\n)\ndef test_multiply_many_except(operand, count, exp, result):\n    assert multiply_many_except(operand, count, exp) == result\n\n\n@pytest.mark.parametrize(\n    ""x, y, result"",\n    [\n        (7, 7, [7, 0, 1]),\n        (2, 2, [2, 0, 1]),\n        (1, 1, [1, 0, 1]),\n        (1, 2, [1, 1, 0]),\n        (5, 6, [1, -1, 1]),\n        (13, 19, [1, 3, -2]),\n        (14, 21, [7, -1, 1]),\n        (2, 1, [1, 0, 1]),\n        (6, 5, [1, 1, -1]),\n        (19, 13, [1, -2, 3]),\n        (21, 14, [7, 1, -1]),\n    ],\n)\ndef test_xgcd(x, y, result):\n    assert result == xgcd(x, y)\n\n\n@pytest.mark.parametrize(\n    ""input, modulus, result"", [(1, 2, 1), (3, 2, 1), (0xFFFFFF, 2, 1), (5, 19, 4), (4, 19, 5)]\n)\ndef test_invert_mod(input, modulus, result):\n    assert result == invert_mod(input, modulus)\n\n\n@pytest.mark.parametrize(\n    ""ibase, obase, input, output"",\n    [\n        ([3], [2], [[0, 1, 2]], [[0, 1, 0]]),\n        ([2, 3], [2], [[0, 1, 0], [0, 1, 2]], [[0, 1, 0]]),\n        ([2, 3], [2, 3], [[1, 1, 0], [1, 2, 2]], [[1, 1, 0], [1, 2, 2]]),\n        ([2, 3], [3, 4, 5], [[0, 1, 1], [0, 1, 2]], [[0, 1, 2], [0, 3, 1], [0, 2, 0]]),\n    ],\n)\ndef test_fast_convert_list(ibase, obase, input, output):\n    base_converter = BaseConvertor(RNSBase(ibase), RNSBase(obase))\n    result = base_converter.fast_convert_list(input, 3)\n    for i in range(len(result)):\n        for j in range(len(result[0])):\n            assert result[i][j] == output[i][j]\n\n\n@pytest.mark.parametrize(\n    ""poly_modulus, plain_modulus, coeff_bit_sizes, integer"",\n    [\n        (64, 64, [40], 0x12345678),\n        (4096, 64, [40], 0),\n        (1024, 64, [40], 1),\n        (64, 64, [40], 2),\n        (64, 64, [40], 0x7FFFFFFFFFFFFFFD),\n        (4096, 64, [40], 0x7FFFFFFFFFFFFFFE),\n        (64, 64, [40], 0x7FFFFFFFFFFFFFFF),\n        (4096, 64, [40], 314159265),\n        (128, 128, [40, 40], 0x12345678),\n        (2048, 128, [40, 40], 0),\n        (1024, 128, [40, 40], 1),\n        (2048, 128, [40, 40], 2),\n        (128, 128, [40, 40], 0x7FFFFFFFFFFFFFFD),\n        (128, 128, [40, 40], 0x7FFFFFFFFFFFFFFE),\n        (4096, 128, [40, 40], 0x7FFFFFFFFFFFFFFF),\n        (128, 128, [40, 40], 314159265),\n        (256, 256, [40, 40, 40], 0x12345678),\n        (1024, 256, [40, 40, 40], 0),\n        (256, 256, [40, 40, 40], 1),\n        (256, 256, [40, 40, 40], 2),\n        (4096, 256, [40, 40, 40], 0x7FFFFFFFFFFFFFFD),\n        (1024, 256, [40, 40, 40], 0x7FFFFFFFFFFFFFFE),\n        (256, 256, [40, 40, 40], 0x7FFFFFFFFFFFFFFF),\n        (4096, 256, [40, 40, 40], 314159265),\n    ],\n)\ndef test_fv_encryption_decrption_asymmetric(poly_modulus, plain_modulus, coeff_bit_sizes, integer):\n    ctx = Context(\n        EncryptionParams(\n            poly_modulus, CoeffModulus().create(poly_modulus, coeff_bit_sizes), plain_modulus\n        )\n    )\n    keys = KeyGenerator(ctx).keygen()\n    encoder = IntegerEncoder(ctx)\n    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key\n    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key\n    assert integer == encoder.decode(decryptor.decrypt(encryptor.encrypt(encoder.encode(integer))))\n\n\n@pytest.mark.parametrize(\n    ""poly_modulus, plain_modulus, coeff_bit_sizes, integer"",\n    [\n        (64, 64, [40], 0x12345678),\n        (4096, 64, [40], 0),\n        (1024, 64, [40], 1),\n        (64, 64, [40], 2),\n        (1024, 64, [40], 0x7FFFFFFFFFFFFFFD),\n        (4096, 64, [40], 0x7FFFFFFFFFFFFFFE),\n        (64, 64, [40], 0x7FFFFFFFFFFFFFFF),\n        (4096, 64, [40], 314159265),\n        (1024, 128, [40, 40], 0x12345678),\n        (2048, 128, [40, 40], 0),\n        (1024, 128, [40, 40], 1),\n        (2048, 128, [40, 40], 2),\n        (1024, 128, [40, 40], 0x7FFFFFFFFFFFFFFD),\n        (128, 128, [40, 40], 0x7FFFFFFFFFFFFFFE),\n        (4096, 128, [40, 40], 0x7FFFFFFFFFFFFFFF),\n        (128, 128, [40, 40], 314159265),\n        (4096, 256, [40, 40, 40], 0x12345678),\n        (1024, 256, [40, 40, 40], 0),\n        (256, 256, [40, 40, 40], 1),\n        (256, 256, [40, 40, 40], 2),\n        (4096, 256, [40, 40, 40], 0x7FFFFFFFFFFFFFFD),\n        (1024, 256, [40, 40, 40], 0x7FFFFFFFFFFFFFFE),\n        (64, 256, [40, 40, 40], 0x7FFFFFFFFFFFFFFF),\n        (4096, 256, [40, 40, 40], 314159265),\n    ],\n)\ndef test_fv_encryption_decrption_symmetric(poly_modulus, plain_modulus, coeff_bit_sizes, integer):\n    ctx = Context(\n        EncryptionParams(\n            poly_modulus, CoeffModulus().create(poly_modulus, coeff_bit_sizes), plain_modulus\n        )\n    )\n    keys = KeyGenerator(ctx).keygen()\n    encoder = IntegerEncoder(ctx)\n    encryptor = Encryptor(ctx, keys[0])  # keys[0] = secret_key\n    decryptor = Decryptor(ctx, keys[0])\n    assert integer == encoder.decode(decryptor.decrypt(encryptor.encrypt(encoder.encode(integer))))\n\n\n@pytest.mark.parametrize(\n    ""poly_modulus, plain_modulus, seq_level, integer"",\n    [\n        (1024, 1024, SeqLevelType.TC128, 0x12345678),\n        (4096, 1024, SeqLevelType.TC192, 0),\n        (4096, 1024, SeqLevelType.TC256, 1),\n        (1024, 1024, SeqLevelType.TC128, 2),\n        (1024, 1024, SeqLevelType.TC128, 0x7FFFFFFFFFFFFFFD),\n        (2048, 1024, SeqLevelType.TC192, 0x7FFFFFFFFFFFFFFE),\n        (1024, 1024, SeqLevelType.TC128, 0x7FFFFFFFFFFFFFFF),\n        (1024, 512, SeqLevelType.TC128, 314159265),\n        (2048, 2048, SeqLevelType.TC256, 0x12345678),\n    ],\n)\ndef test_fv_encryption_decrption_standard_seq_level(\n    poly_modulus, plain_modulus, seq_level, integer\n):\n    ctx = Context(\n        EncryptionParams(\n            poly_modulus, CoeffModulus().bfv_default(poly_modulus, seq_level), plain_modulus\n        )\n    )\n    keys = KeyGenerator(ctx).keygen()\n    encoder = IntegerEncoder(ctx)\n    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key\n    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key\n    assert integer == encoder.decode(decryptor.decrypt(encryptor.encrypt(encoder.encode(integer))))\n\n\ndef test_fv_encryption_decrption_without_changing_parameters():\n    ctx = Context(EncryptionParams(1024, CoeffModulus().create(1024, [30, 30]), 1024))\n    keys = KeyGenerator(ctx).keygen()\n    encoder = IntegerEncoder(ctx)\n    encryptor = Encryptor(ctx, keys[1])  # keys[1] = public_key\n    decryptor = Decryptor(ctx, keys[0])  # keys[0] = secret_key\n    values = [0, 1, -1, 100, -100, 1000]\n    for value in values:\n        # Checking simple encryption-decryption with same parameters.\n        assert value == encoder.decode(decryptor.decrypt(encryptor.encrypt(encoder.encode(value))))\n\n        # Checking the decryption of same ciphertext 3 times (checking for ciphertext deepcopy).\n        ct = encryptor.encrypt(encoder.encode(value))\n        for _ in range(3):\n            assert value == encoder.decode(decryptor.decrypt(ct))\n'"
test/torch/tensors/test_native.py,32,"b'import pytest\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.exceptions import InvalidTensorForRemoteGet\nimport syft\n\n\ndef test___str__(workers):\n    bob = workers[""bob""]\n    tensor = torch.Tensor([1, 2, 3, 4])\n    assert isinstance(tensor.__str__(), str)\n\n    tensor_ptr = tensor.send(bob)\n    assert isinstance(tensor_ptr.__str__(), str)\n\n\ndef test___repr__(workers):\n    bob = workers[""bob""]\n\n    tensor = torch.Tensor([1, 2, 3, 4])\n    assert isinstance(tensor.__repr__(), str)\n\n    tensor_ptr = tensor.send(bob)\n    assert isinstance(tensor_ptr.__repr__(), str)\n\n    tensor = torch.Tensor([1, 2, 3, 4]).tag(""#my_tag"").describe(""This is a description"")\n    assert isinstance(tensor.__repr__(), str)\n\n\ndef test_overload_reshape():\n    tensor = torch.Tensor([1, 2, 3, 4])\n    tensor_reshaped = tensor.reshape((2, 2))\n    tensor_matrix = torch.Tensor([[1.0, 2.0], [3.0, 4.0]])\n    assert (tensor_reshaped == tensor_matrix).all()\n\n\ndef test_owner_default(hook):\n    tensor = torch.Tensor([1, 2, 3, 4, 5])\n    assert tensor.owner == hook.local_worker\n\n\ndef test_create_pointer(hook, workers):\n    bob = workers[""bob""]\n\n    tensor = torch.Tensor([1, 2, 3, 4, 5])\n\n    ptr = tensor.create_pointer(\n        location=bob, id_at_location=1, register=False, owner=hook.local_worker, ptr_id=2\n    )\n\n    assert ptr.owner == hook.local_worker\n    assert ptr.location == bob\n    assert ptr.id_at_location == 1\n    assert ptr.id == 2\n\n    ptr2 = tensor.create_pointer(owner=hook.local_worker)\n    assert isinstance(ptr2.__str__(), str)\n    assert isinstance(ptr2.__repr__(), str)\n\n\ndef test_create_pointer_defaults(workers):\n    bob = workers[""bob""]\n\n    tensor = torch.Tensor([1, 2, 3, 4, 5])\n\n    ptr = tensor.create_pointer(location=bob)\n\n    assert ptr.owner == tensor.owner\n    assert ptr.location == bob\n\n\ndef test_get(workers):\n    bob = workers[""bob""]\n\n    tensor = torch.rand(5, 3)\n    pointer = tensor.send(bob)\n\n    assert type(pointer.child) == PointerTensor\n    assert (pointer.get() == tensor).all()\n\n\ndef test_invalid_remote_get(workers):\n    bob = workers[""bob""]\n\n    tensor = torch.rand(5, 3)\n    pointer = tensor.send(bob)\n    with pytest.raises(InvalidTensorForRemoteGet):\n        pointer.remote_get()\n\n\ndef test_remote_get(hook, workers):\n    me = workers[""me""]\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    x = torch.tensor([1, 2, 3, 4, 5])\n    ptr_ptr_x = x.send(bob).send(alice)\n\n    assert ptr_ptr_x.owner == me\n    assert ptr_ptr_x.location == alice\n    assert x.id in bob.object_store._objects\n\n    assert len(bob.object_store._tensors) == 1\n    assert len(alice.object_store._tensors) == 1\n\n    ptr_ptr_x.remote_get()\n\n    assert len(bob.object_store._tensors) == 0\n    assert len(alice.object_store._tensors) == 1\n\n\ndef test_remote_send(hook, workers):\n    me = workers[""me""]\n    bob = workers[""bob""]\n    alice = workers[""alice""]\n\n    x = torch.tensor([1, 2, 3, 4, 5])\n    # Note: behavior has been changed to point to the last pointer\n    ptr_ptr_x = x.send(bob).remote_send(alice)\n\n    assert ptr_ptr_x.owner == me\n    assert ptr_ptr_x.location == bob\n    assert x.id in alice.object_store._objects\n\n\ndef test_copy():\n    tensor = torch.rand(5, 3)\n    copied_tensor = tensor.copy()\n    assert (tensor == copied_tensor).all()\n    assert tensor is not copied_tensor\n\n\ndef test_size():\n    tensor = torch.rand(5, 3)\n    assert tensor.size() == torch.Size([5, 3])\n    assert tensor.size() == tensor.shape\n    assert tensor.size(0) == tensor.shape[0]\n\n\n# Compare local dim with the remote one\ndef test_dim(workers):\n    tensor_local = torch.randn(5, 3)\n    tensor_remote = tensor_local.send(workers[""alice""])\n\n    assert tensor_local.dim() == tensor_remote.dim()\n\n\ndef test_roll(workers):\n    x = torch.tensor([1.0, 2.0, 3, 4, 5])\n    expected = torch.roll(x, -1)\n\n    index = torch.tensor([-1.0])\n    result = torch.roll(x, index)\n\n    assert (result == expected).all()\n\n\ndef test_complex_model(workers):\n    hook = syft.TorchHook(torch)\n    bob = workers[""bob""]\n    tensor_local = torch.rand(4, 1, 32, 32)\n    tensor_remote = tensor_local.send(bob)\n\n    ## Instantiating a model with multiple layer types\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(1, 6, 5)\n            self.conv2 = nn.Conv2d(6, 16, 5)\n            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n            self.bn = nn.BatchNorm1d(120)\n            self.fc2 = nn.Linear(120, 84)\n            self.fc3 = nn.Linear(84, 10)\n\n        def forward(self, x):\n            out = self.conv1(x)\n            out = F.relu(out)\n            out = F.max_pool2d(out, 2)\n            out = F.relu(self.conv2(out))\n            out = F.avg_pool2d(out, 2)\n            out = out.view(out.shape[0], -1)\n            out = F.relu(self.fc1(out))\n            out = self.bn(out)\n            out = F.relu(self.fc2(out))\n            out = self.fc3(out)\n            return out\n\n    model_net = Net()\n    model_net.send(bob)\n\n    ## Forward on the remote model\n    pred = model_net(tensor_remote)\n\n    assert pred.is_wrapper\n    assert isinstance(pred.child, syft.PointerTensor)\n\n    model_net.get()\n\n    for p in model_net.parameters():\n        assert isinstance(p, torch.nn.Parameter)\n        assert not hasattr(p, ""child"")\n\n\ndef test_encrypt_decrypt(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    x = torch.randint(10, (1, 5), dtype=torch.float32)\n    x_encrypted = x.encrypt(workers=[bob, alice], crypto_provider=james, base=10)\n    x_decrypted = x_encrypted.decrypt()\n    assert torch.all(torch.eq(x_decrypted, x))\n\n    x = torch.randint(10, (1, 5), dtype=torch.float32)\n    x_encrypted = x.encrypt(workers=[bob, alice], crypto_provider=james)\n    x_decrypted = x_encrypted.decrypt()\n    assert torch.all(torch.eq(x_decrypted, x))\n\n    x = torch.randint(10, (1, 5), dtype=torch.float32)\n    public, private = syft.frameworks.torch.he.paillier.keygen()\n    x_encrypted = x.encrypt(protocol=""paillier"", public_key=public)\n    x_decrypted = x_encrypted.decrypt(protocol=""paillier"", private_key=private)\n    assert torch.all(torch.eq(x_decrypted, x))\n\n\ndef test_get_response():\n    test_func = lambda x: x\n    t = torch.tensor(73)\n    # a non overloaded function\n    setattr(torch, ""_test_func"", test_func)\n    result = torch.Tensor._get_response(""torch._test_func"", t, {})\n    delattr(torch, ""_test_func"")\n    assert t == result\n'"
test/torch/tensors/test_numpy.py,0,"b'import torch as th\nimport numpy as np\nimport syft as sy\n\n\ndef test_numpy_add():\n    """"""\n    Test basic NumpyTensor addition\n    """"""\n\n    x = sy.NumpyTensor(numpy_tensor=[[1, 2, 3, 4]])\n    y = x + x\n    assert (y.child.child == np.array([2, 4, 6, 8])).all()\n\n\ndef test_numpy_subtract():\n    """"""\n    Test basic NumpyTensor subtraction\n    """"""\n\n    x = sy.NumpyTensor(numpy_tensor=np.array([[1, 2, 3, 4]]))\n    y = x - x\n    assert (y.child.child == np.array([0, 0, 0, 0])).all()\n\n\ndef test_numpy_multiply():\n    """"""\n    Test basic NumpyTensor multiplication\n    """"""\n\n    x = sy.NumpyTensor(numpy_tensor=np.array([[1, 2, 3, 4]]))\n    y = x * x\n    assert (y.child.child == np.array([1, 4, 9, 16])).all()\n\n\ndef test_numpy_divide():\n    """"""\n    Test basic NumpyTensor division\n    """"""\n\n    x = sy.NumpyTensor(numpy_tensor=np.array([[1, 2, 3, 4]]))\n    y = x / x\n    assert (y.child.child == np.array([1, 1, 1, 1])).all()\n\n\ndef test_numpy_dot():\n    """"""\n    Test basic NumpyTensor dot product\n    """"""\n    x = sy.NumpyTensor(numpy_tensor=np.array([[1, 2, 3, 4]]))\n    y = x.dot(x.transpose())\n    assert (y.child.child == np.array([[30]])).all()\n\n\ndef test_numpy_mm():\n    """"""\n    Test basic NumpyTensor matrix multiply\n    """"""\n    x = sy.NumpyTensor(numpy_tensor=np.array([[1, 2, 3, 4]]))\n    y = x.mm(x.transpose())\n    assert (y.child.child == np.array([[30]])).all()\n\n\ndef test_numpy_mm2():\n    """"""\n    Test @ based NumpyTensor matrix multiply\n    """"""\n    x = sy.NumpyTensor(numpy_tensor=np.array([[1, 2, 3, 4]]))\n    y = x @ (x.transpose())\n    assert (y.child.child == np.array([[30]])).all()\n\n\ndef test_numpy_transpose():\n    """"""\n    Test basic NumpyTensor transpose\n    """"""\n    x = sy.NumpyTensor(numpy_tensor=np.array([[1, 2, 3, 4]]))\n    y = x.transpose(0, 1)\n    assert (y.child.child == np.array([[1], [2], [3], [4]])).all()\n\n\ndef test_numpy_casting():\n    """"""\n    This tests the ability to cast a data tensor to a tensor chain\n    with an underlying Numpy representation.\n    """"""\n\n    out = th.tensor([1, 2, 23, 4]).numpy_tensor()\n    assert (out.child.child == np.array([1, 2, 23, 4])).all()\n'"
test/torch/tensors/test_paillier.py,18,"b'import torch\nimport syft as sy\n\n\ndef test_encrypt_and_decrypt():\n    """"""\n    Test the basic paillier encrypt/decrypt functionality\n    """"""\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y = x.decrypt(protocol=""paillier"", private_key=pri)\n\n    assert (x_tensor == y).all()\n\n\ndef test_encrypted_encrypted_add():\n    """"""\n    Test addition of two encrypted values\n    """"""\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y = (x + x).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor + x_tensor) == y).all()\n\n\ndef test_encrypted_decrypted_add():\n    """"""\n    Test addition of an encryptd and decrypted value\n    """"""\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y = (x + x_tensor).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor + x_tensor) == y).all()\n\n\ndef test_decrypted_encrypted_add():\n    """"""\n    Test the addition of a decrypted and encrypted value\n    """"""\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y = (x_tensor + x).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor + x_tensor) == y).all()\n\n\ndef test_encrypted_encrypted_sub():\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y_tensor = torch.Tensor([2, 2, 2])\n    y = y_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    z = (x - y).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor - y_tensor) == z).all()\n\n\ndef test_encrypted_decrypted_sub():\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y_tensor = torch.Tensor([2, 2, 2])\n    y = y_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    z = (x - y_tensor).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor - y_tensor) == z).all()\n\n\ndef test_decrypted_encrypted_sub():\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y_tensor = torch.Tensor([2, 2, 2])\n    y = y_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    z = (x_tensor - y).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor - y_tensor) == z).all()\n\n\ndef test_encrypted_decrypted_mul():\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y_tensor = torch.Tensor([2, 2, 2])\n    y = y_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    z = (x * y_tensor).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor * y_tensor) == z).all()\n\n\ndef test_decrypted_encrypted_mul():\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y_tensor = torch.Tensor([2, 2, 2])\n    y = y_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    z = (x_tensor * y).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor * y_tensor) == z).all()\n\n\ndef test_encrypted_decrypted_matmul():\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.tensor([1, 2, 3])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y_tensor = torch.tensor([2, 2, 2])\n    y = y_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    z = (x.mm(y_tensor)).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert (z == 12).all()\n\n\ndef test_decrypted_encrypted_matmul():\n\n    pub, pri = sy.keygen()\n\n    x_tensor = torch.Tensor([[1, 2, 3]])\n    x = x_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    y_tensor = torch.Tensor([[2], [2], [2]])\n    y = y_tensor.encrypt(protocol=""paillier"", public_key=pub)\n\n    z = (x_tensor.mm(y)).decrypt(protocol=""paillier"", private_key=pri)\n\n    assert ((x_tensor.mm(y_tensor)) == z).all()\n'"
test/torch/tensors/test_parameter.py,10,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\n\n\ndef test_param_on_pointer(workers):\n    tensor = torch.tensor([1.0, -1.0, 3.0, 4.0])\n    ptr = tensor.send(workers[""bob""])\n    param = Parameter(ptr)\n    local_param = param.get()\n\n    assert (local_param.data == tensor).all()\n\n\ndef test_param_send_get(workers):\n    tensor = torch.tensor([1.0, -1.0, 3.0, 4.0])\n    param = Parameter(data=tensor.clone())\n    param_ptr = param.send(workers[""bob""])\n    param_back = param_ptr.get()\n\n    assert (param_back.data == tensor).all()\n\n\ndef test_param_inplace_send_get(workers):\n    tensor = torch.tensor([1.0, -1.0, 3.0, 4.0])\n    param = Parameter(data=tensor.clone())\n    param_ptr = param.send_(workers[""bob""])\n\n    assert param_ptr.id == param.id\n    assert id(param_ptr) == id(param)\n\n    param_back = param_ptr.get_()\n\n    assert param_back.id == param_ptr.id\n    assert param_back.id == param.id\n    assert id(param_back) == id(param_ptr)\n    assert id(param_back) == id(param)\n\n    assert (param_back.data == tensor).all()\n\n\ndef test_param_double_send_get(workers):\n    tensor = torch.tensor([[1.0, 1]])\n    param = Parameter(tensor)\n\n    param = param.send(workers[""bob""]).send(workers[""alice""])\n    param = param.get().get()\n\n    assert (param.data == tensor).all()\n\n\ndef test_param_remote_binary_method(workers):\n    tensor = torch.tensor([1.0, -1.0, 3.0, 4.0])\n    param = Parameter(data=tensor.clone())\n    param_ptr = param.send(workers[""bob""])\n    param_double_ptr = param_ptr + param_ptr\n    param_double_back = param_double_ptr.get()\n    double_tensor = tensor + tensor\n\n    assert (param_double_back.data == double_tensor).all()\n\n\ndef test_local_param_in_nn_module_linear():\n    model = nn.Linear(2, 1)\n    tensor = torch.tensor([1.0, -1.0])\n    res = model(tensor)\n\n\ndef test_remote_param_in_nn_module_linear(workers):\n    model = nn.Linear(2, 1, bias=False)\n    tensor = torch.tensor([1.0, -1.0])\n    model_ptr = model.send(workers[""bob""])\n    tensor_ptr = tensor.send(workers[""bob""])\n    res_ptr = model_ptr(tensor_ptr)\n    res = res_ptr.get()\n\n    model = nn.Linear(2, 1)\n    tensor = torch.tensor([1.0, -1.0])\n    model_ptr = model.send(workers[""bob""])\n    tensor_ptr = tensor.send(workers[""bob""])\n    res_ptr = model_ptr(tensor_ptr)\n    res = res_ptr.get()\n'"
test/torch/tensors/test_polynomial.py,28,"b'# import torch\n# import numpy as np\n\n# from syft.frameworks.torch.tensors.interpreters.polynomial import PolynomialTensor\n\n\n# def test_sigmoid():\n#\n#     poly_tensor = PolynomialTensor()\n#\n#     # x = torch.tensor(np.linspace(-3, 3, 10), dtype=torch.double)\n#\n#     expected = torch.tensor(\n#         [0.0337, 0.0886, 0.1759, 0.2921, 0.4283, 0.5717, 0.7079, 0.8241, 0.9114, 0.9663]\n#     )\n#\n#\n#     # allclose function to compare the expected values and approximations with fixed precision\n#     # result = poly_tensor.get_val(""sigmoid"", x)\n#\n#     # assert torch.abs(expected - expected).max()[0] < 1e-03\n#     _ = (expected - expected).max()\n#     print(_)\n#     assert True\n\n#\n# def test_exp():\n#\n#     poly_tensor = PolynomialTensor()\n#\n#     x = torch.tensor(np.linspace(-3, 3, 10), dtype=torch.double)\n#\n#     expected = torch.tensor(\n#         [0.0498, 0.0970, 0.1889, 0.3679, 0.7165, 1.1176, 2.9503, 5.1088, 10.2501, 20.2955],\n#         dtype=torch.double,\n#     )\n#\n#     # allclose function to compare the expected values and approximations with fixed precision\n#     result = poly_tensor.get_val(""exp"", x)\n#     assert torch.allclose(expected, result, atol=1e-03)\n#\n#\n# def test_tanh():\n#\n#     # Test if tanh approximation works as expected\n#\n#     poly_tensor = PolynomialTensor()\n#\n#     x = torch.tensor(np.linspace(-3, 3, 10), dtype=torch.double)\n#\n#     expected = torch.tensor(\n#         [-0.9937, -0.9811, -0.9329, -0.7596, -0.3239, 0.3239, 0.7596, 0.9329, 0.9811, 0.9937],\n#         dtype=torch.double,\n#     )\n#\n#     result = poly_tensor.get_val(""tanh"", x)\n#\n#     # allclose function to compare the expected values and approximations with fixed precision\n#     assert torch.allclose(expected, result, atol=1e-03)\n#\n#\n# def test_interpolate():\n#\n#     # Test if interpolation function works as expected by verifying an approximation of\n#     # exponential function\n#\n#     expected = torch.tensor([1.2220, 2.9582, 7.1763, 20.3064, 54.4606], dtype=torch.double)\n#\n#     poly_tensor = PolynomialTensor()\n#\n#     f1 = poly_tensor.interpolate((lambda x: np.exp(x)), np.linspace(0, 10, 50))\n#\n#     assert torch.allclose(expected, torch.tensor(f1(torch.tensor([0, 1, 2, 3, 4]))), 1e-04)\n#\n#\n# def test_custom_function():\n#     poly_tensor = PolynomialTensor()\n#     poly_tensor.add_function(\n#         ""Custom"", 10, [[0, 10, 100, 10, poly_tensor.fit_function]], lambda x: x + 2\n#     )\n#\n#     assert round(poly_tensor.get_val(""Custom"", 3)) == 5\n#     assert round(poly_tensor.get_val(""Custom"", 6)) == 8\n#\n#\n# def test_random_function():\n#\n#     x = torch.tensor(np.linspace(-3, 3, 10), dtype=torch.double)\n#     poly_tensor = PolynomialTensor(function=lambda x: x * 2)\n#     scaled_result = poly_tensor.get_val(""exp"", x.clone())\n#\n#     poly_tensor = PolynomialTensor()\n#     original_result = poly_tensor.get_val(""exp"", x)\n#\n#     assert torch.all(torch.eq(scaled_result, torch.mul(original_result, 2)))\n#\n#\n# def test_log_function():\n#\n#     # Test if log approximation works as expected\n#\n#     poly_tensor = PolynomialTensor()\n#\n#     x = torch.tensor(np.linspace(1, 10, 10), dtype=torch.double)\n#\n#     expected = torch.tensor(\n#         [\n#             3.7160e-04,\n#             6.9319e-01,\n#             1.0986e00,\n#             1.3863e00,\n#             1.6096e00,\n#             1.7932e00,\n#             1.9526e00,\n#             2.1056e00,\n#             2.2835e00,\n#             2.5537e00,\n#         ],\n#         dtype=torch.double,\n#     )\n#\n#     result = poly_tensor.get_val(""log"", x.clone())\n#\n#     # allclose function to compare the expected values and approximations with fixed precision\n#     assert torch.allclose(expected, result, atol=1e-03)\n#\n#\n# def test_exp_taylor():\n#\n#     expected = torch.tensor(\n#         [-0.1076, 0.0664, 0.1852, 0.3677, 0.7165, 1.3956, 2.7180, 5.2867, 10.2325, 19.5933],\n#         dtype=torch.double,\n#     )\n#     poly_tensor = PolynomialTensor()\n#     x = torch.tensor(np.linspace(-3, 3, 10), dtype=torch.double)\n#     result = poly_tensor.exp(x)\n#\n#     # allclose function to compare the expected values and approximations with fixed precision\n#     assert torch.allclose(expected, result, atol=1e-03)\n#\n#\n# def test_sigmoid_taylor():\n#\n#     expected = torch.tensor(\n#         [0.1000, 0.1706, 0.2473, 0.3392, 0.4447, 0.5553, 0.6608, 0.7527, 0.8294, 0.9000],\n#         dtype=torch.double,\n#     )\n#     poly_tensor = PolynomialTensor()\n#     x = torch.tensor(np.linspace(-2, 2, 10), dtype=torch.double)\n#     result = poly_tensor.sigmoid(x)\n#\n#     # allclose function to compare the expected values and approximations with fixed precision\n#     assert torch.allclose(expected, result, atol=1e-03)\n'"
test/torch/tensors/test_precision.py,157,"b'import pytest\nimport torch\nimport torch.nn as nn\n\nfrom syft.frameworks.torch.tensors.interpreters.precision import FixedPrecisionTensor\n\n\ndef test_wrap(workers):\n    """"""\n    Test the .on() wrap functionality for LoggingTensor\n    """"""\n\n    x_tensor = torch.Tensor([1, 2, 3])\n    x = FixedPrecisionTensor().on(x_tensor)\n    assert isinstance(x, torch.Tensor)\n    assert isinstance(x.child, FixedPrecisionTensor)\n    assert isinstance(x.child.child, torch.Tensor)\n\n\n@pytest.mark.parametrize(""parameter"", [False, True])\ndef test_encode_decode(workers, parameter):\n    x = torch.tensor([0.1, 0.2, 0.3])\n    if parameter:\n        x = nn.Parameter(x)\n    x = x.fix_prec()\n    assert (x.child.child == torch.LongTensor([100, 200, 300])).all()\n    x = x.float_prec()\n\n    assert (x == torch.tensor([0.1, 0.2, 0.3])).all()\n\n\ndef test_fix_prec_registration(hook):\n    with hook.local_worker.registration_enabled():\n        x = torch.tensor([1.0])\n        x_fpt = x.fix_precision()\n\n        assert hook.local_worker.get_obj(x.id) == x\n\n\ndef test_inplace_encode_decode(workers):\n\n    x = torch.tensor([0.1, 0.2, 0.3])\n    x.fix_prec_()\n    assert (x.child.child == torch.LongTensor([100, 200, 300])).all()\n    x.float_prec_()\n\n    assert (x == torch.tensor([0.1, 0.2, 0.3])).all()\n\n\ndef test_fix_prec_inplace_registration(hook):\n\n    with hook.local_worker.registration_enabled():\n        x = torch.tensor([1.0])\n        x.fix_precision_()\n        assert hook.local_worker.get_obj(x.id) == torch.tensor([1.0]).fix_precision()\n\n\n@pytest.mark.parametrize(""method"", [""t"", ""matmul""])\n@pytest.mark.parametrize(""parameter"", [False, True])\ndef test_methods_for_linear_module(method, parameter):\n    """"""\n    Test all the methods used in the F.linear functions\n    """"""\n    if parameter:\n        tensor = nn.Parameter(torch.tensor([[1.0, 2], [3, 4]]))\n    else:\n        tensor = torch.tensor([[1.0, 2], [3, 4]])\n    fp_tensor = tensor.fix_precision()\n    if method != ""t"":\n        fp_result = getattr(fp_tensor, method)(fp_tensor)\n        result = getattr(tensor, method)(tensor)\n    else:\n        fp_result = getattr(fp_tensor, method)()\n        result = getattr(tensor, method)()\n\n    assert (result == fp_result.float_precision()).all()\n\n\ndef test_torch_add(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    # Method syntax\n    x = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n\n    y = x + x\n\n    assert (y.child.child == torch.LongTensor([200, 400, 600])).all()\n    y = y.float_prec()\n\n    assert (y == torch.tensor([0.2, 0.4, 0.6])).all()\n\n    # Function syntax\n    x = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n\n    y = torch.add(x, x)\n\n    assert (y.child.child == torch.LongTensor([200, 400, 600])).all()\n    y = y.float_prec()\n\n    assert (y == torch.tensor([0.2, 0.4, 0.6])).all()\n\n    # With negative numbers\n    x = torch.tensor([-0.1, -0.2, 0.3]).fix_prec()\n    y = torch.tensor([0.4, -0.5, -0.6]).fix_prec()\n\n    z = torch.add(x, y).float_prec()\n\n    assert (z == torch.tensor([0.3, -0.7, -0.3])).all()\n\n    # with AdditiveSharingTensor\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec()\n    y = t.fix_prec().share(bob, alice, crypto_provider=james)\n\n    z = torch.add(x, y).get().float_prec()\n    assert (z == torch.add(t, t)).all()\n\n    z = torch.add(y, x).get().float_prec()\n    assert (z == torch.add(t, t)).all()\n\n    # with constant integer\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec()\n    c = 4\n\n    z = (x + c).float_prec()\n    assert (z == (t + c)).all()\n\n    z = (c + x).float_prec()\n    assert (z == (c + t)).all()\n\n    # with constant float\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec()\n    c = 4.2\n\n    z = (x + c).float_prec()\n    assert ((z - (t + c)) < 10e-3).all()\n\n    z = (c + x).float_prec()\n    assert ((z - (c + t)) < 10e-3).all()\n\n    # with dtype int\n    x = torch.tensor([1.0, 2.0, 3.0]).fix_prec(dtype=""int"")\n    y = torch.tensor([0.1, 0.2, 0.3]).fix_prec(dtype=""int"")\n\n    z = x + y\n    assert (z.float_prec() == torch.tensor([1.1, 2.2, 3.3])).all()\n\n\ndef test_torch_add_():\n    x = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n\n    y = x.add_(x)\n\n    assert (y.child.child == torch.LongTensor([200, 400, 600])).all()\n    y = y.float_prec()\n\n    assert (y == torch.tensor([0.2, 0.4, 0.6])).all()\n\n    x = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n    lr = torch.tensor(0.5).fix_prec()\n\n    y = x.add_(lr, x)\n\n    assert (y.child.child == torch.LongTensor([150, 300, 450])).all()\n    y = y.float_prec()\n\n    assert (y == torch.tensor([0.15, 0.3, 0.45])).all()\n\n\ndef test_torch_sub(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    x = torch.tensor([0.5, 0.8, 1.3]).fix_prec()\n    y = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n\n    z = torch.sub(x, y)\n\n    assert (z.child.child == torch.LongTensor([400, 600, 1000])).all()\n    z = z.float_prec()\n\n    assert (z == torch.tensor([0.4, 0.6, 1.0])).all()\n\n    # with AdditiveSharingTensor\n    tx = torch.tensor([1.0, -2.0, 3.0])\n    ty = torch.tensor([0.1, 0.2, 0.3])\n    x = tx.fix_prec()\n    y = ty.fix_prec().share(bob, alice, crypto_provider=james)\n\n    z1 = torch.sub(y, x).get().float_prec()\n    z2 = torch.sub(x, y).get().float_prec()\n\n    assert (z1 == torch.sub(ty, tx)).all()\n    assert (z2 == torch.sub(tx, ty)).all()\n\n    # with constant integer\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec()\n    c = 4\n\n    z = (x - c).float_prec()\n    assert (z == (t - c)).all()\n\n    z = (c - x).float_prec()\n    assert (z == (c - t)).all()\n\n    # with constant float\n    t = torch.tensor([1.0, -2.0, 3.0])\n    x = t.fix_prec()\n    c = 4.2\n\n    z = (x - c).float_prec()\n    assert ((z - (t - c)) < 10e-3).all()\n\n    z = (c - x).float_prec()\n    assert ((z - (c - t)) < 10e-3).all()\n\n    # with dtype int\n    x = torch.tensor([1.0, 2.0, 3.0]).fix_prec(dtype=""int"")\n    y = torch.tensor([0.1, 0.2, 0.3]).fix_prec(dtype=""int"")\n\n    z = x - y\n    assert (z.float_prec() == torch.tensor([0.9, 1.8, 2.7])).all()\n\n\ndef test_torch_sub_():\n    x = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n\n    y = x.sub_(x)\n\n    assert (y.child.child == torch.LongTensor([0, 0, 0])).all()\n    y = y.float_prec()\n\n    assert (y == torch.tensor([0, 0, 0.0])).all()\n\n    x = torch.tensor([0.1, 0.2, 0.3]).fix_prec()\n    lr = torch.tensor(0.5).fix_prec()\n\n    y = x.sub_(lr, x)\n\n    assert (y.child.child == torch.LongTensor([50, 100, 150])).all()\n    y = y.float_prec()\n\n    assert (y == torch.tensor([0.05, 0.1, 0.15])).all()\n\n\ndef test_torch_mul(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    # mul with non standard fix precision\n    x = torch.tensor([2.113]).fix_prec(precision_fractional=2)\n\n    y = torch.mul(x, x)\n\n    assert y.child.child == torch.LongTensor([445])\n    assert y.child.precision_fractional == 2\n\n    y = y.float_prec()\n\n    assert y == torch.tensor([4.45])\n\n    # Mul with negative numbers\n    x = torch.tensor([2.113]).fix_prec()\n    y = torch.tensor([-0.113]).fix_prec()\n\n    z = torch.mul(x, y)\n\n    assert z.child.precision_fractional == 3\n\n    z = z.float_prec()\n    assert z == torch.tensor([-0.2380])\n\n    x = torch.tensor([11.0]).fix_prec(field=2 ** 16, precision_fractional=2)\n    y = torch.mul(x, x).float_prec()\n\n    assert y == torch.tensor([121.0])\n\n    # mixing + and *\n    x = torch.tensor([2.113]).fix_prec()\n    y = torch.tensor([-0.113]).fix_prec()\n    z = torch.mul(x, y + y)\n\n    assert z.child.precision_fractional == 3\n\n    z = z.float_prec()\n\n    assert z == torch.tensor([-0.4770])\n\n    # with AST\n    t = torch.tensor([1.0, -2.0, 3.0])\n    u = torch.tensor([1.0, -2.0, -3.0])\n    x = t.fix_prec()\n    y = u.fix_prec().share(bob, alice, crypto_provider=james)\n\n    z = torch.mul(x, y).get().float_prec()\n\n    assert (z == torch.mul(t, u)).all()\n\n    # with dtype int\n    x = torch.tensor([1.0, 2.0, 3.0]).fix_prec(dtype=""int"")\n    y = torch.tensor([0.1, 0.2, 0.3]).fix_prec(dtype=""int"")\n\n    z = x * y\n    assert (z.float_prec() == torch.tensor([0.1, 0.4, 0.9])).all()\n\n\ndef test_torch_div(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    # With scalar\n    x = torch.tensor([[9.0, 25.42], [3.3, 0.0]]).fix_prec()\n    y = torch.tensor([[3.0, 6.2], [3.3, 4.7]]).fix_prec()\n\n    z = torch.div(x, y).float_prec()\n\n    assert (z == torch.tensor([[3.0, 4.1], [1.0, 0.0]])).all()\n\n    # With negative numbers\n    x = torch.tensor([[-9.0, 25.42], [-3.3, 0.0]]).fix_prec()\n    y = torch.tensor([[3.0, -6.2], [-3.3, 4.7]]).fix_prec()\n\n    z = torch.div(x, y).float_prec()\n\n    assert (z == torch.tensor([[-3.0, -4.1], [1.0, 0.0]])).all()\n\n    # AST divided by FPT\n    x = torch.tensor([[9.0, 25.42], [3.3, 0.0]]).fix_prec().share(bob, alice, crypto_provider=james)\n    y = torch.tensor([[3.0, 6.2], [3.3, 4.7]]).fix_prec()\n\n    z = torch.div(x, y).get().float_prec()\n\n    assert (z == torch.tensor([[3.0, 4.1], [1.0, 0.0]])).all()\n\n    # With dtype int\n    x = torch.tensor([[-9.0, 25.42], [-3.3, 0.0]]).fix_prec(dtype=""int"")\n    y = torch.tensor([[3.0, -6.2], [-3.3, 4.7]]).fix_prec(dtype=""int"")\n\n    z = torch.div(x, y)\n    assert (z.float_prec() == torch.tensor([[-3.0, -4.1], [1.0, 0.0]])).all()\n\n\ndef test_inplace_operations():\n    a = torch.tensor([5.0, 6.0]).fix_prec()\n    b = torch.tensor([2.0]).fix_prec()\n\n    a /= b\n    assert (a.float_prec() == torch.tensor([2.5, 3.0])).all()\n\n    a *= b\n    assert (a.float_prec() == torch.tensor([5.0, 6.0])).all()\n\n    a += b\n    assert (a.float_prec() == torch.tensor([7.0, 8.0])).all()\n\n    a -= b\n    assert (a.float_prec() == torch.tensor([5.0, 6.0])).all()\n\n\ndef test_torch_pow():\n\n    m = torch.tensor([[1, 2], [3, 4.0]])\n    x = m.fix_prec()\n    y = (x ** 3).float_prec()\n\n    assert (y == (m ** 3)).all()\n\n\ndef test_torch_matmul(workers):\n    bob, alice, james = (workers[""bob""], workers[""alice""], workers[""james""])\n\n    m = torch.tensor([[1, 2], [3, 4.0]])\n    x = m.fix_prec()\n    y = torch.matmul(x, x).float_prec()\n\n    assert (y == torch.matmul(m, m)).all()\n\n    # with AST\n    m = torch.tensor([[1, 2], [3, 4.0]])\n    x = m.fix_prec()\n    y = m.fix_prec().share(bob, alice, crypto_provider=james)\n\n    z = (x @ y).get().float_prec()\n\n    assert (z == torch.matmul(m, m)).all()\n\n\ndef test_torch_addmm():\n    weight = nn.Parameter(torch.tensor([[1.0, 2], [4.0, 2]])).fix_precision()\n    inputs = nn.Parameter(torch.tensor([[1.0, 2]])).fix_precision()\n    bias = nn.Parameter(torch.tensor([1.0, 2])).fix_precision()\n\n    fp_result = torch.addmm(bias, inputs, weight)\n\n    assert (fp_result.float_precision() == torch.tensor([[10.0, 8.0]])).all()\n\n\ndef test_torch_dot(workers):\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]).fix_prec()\n    y = torch.tensor([3.0, 3.0, 3.0, 3.0, 3.0]).fix_prec()\n\n    assert torch.dot(x, y).float_prec() == 45\n\n\ndef test_torch_inverse_approx(workers):\n    """"""\n    Test the approximate inverse with different tolerance depending on\n    the precision_fractional considered\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    fix_prec_tolerance = {3: 1 / 100, 4: 1 / 100, 5: 1 / 100}\n\n    for prec_frac, tolerance in fix_prec_tolerance.items():\n        for t in [\n            torch.tensor([[0.4, -0.1], [-0.4, 2.0]]),\n            torch.tensor([[1, -0.6], [0.4, 4.0]]),\n            torch.tensor([[1, 0.2], [0.4, 4.0]]),\n        ]:\n            t_sh = t.fix_precision(precision_fractional=prec_frac).share(\n                alice, bob, crypto_provider=james\n            )\n            r_sh = t_sh.inverse()\n            r = r_sh.get().float_prec()\n            t = t.inverse()\n            diff = (r - t).abs().max()\n            norm = (r + t).abs().max() / 2\n\n            assert (diff / (tolerance * norm)) < 1\n\n\n@pytest.mark.parametrize(""prec_frac, tolerance"", [(3, 20 / 100), (4, 5 / 100), (5, 4 / 100)])\ndef test_torch_exp_approx(prec_frac, tolerance, workers):\n    """"""\n    Test the approximate exponential with different tolerance depending on\n    the precision_fractional considered\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    cumsum = torch.zeros(5)\n    for i in range(10):\n        t = torch.tensor([0.0, 1, 2, 3, 4])\n        t_sh = t.fix_precision(precision_fractional=prec_frac).share(\n            alice, bob, crypto_provider=james\n        )\n        r_sh = t_sh.exp()\n        r = r_sh.get().float_prec()\n        t = t.exp()\n        diff = (r - t).abs()\n        norm = (r + t) / 2\n        cumsum += diff / (tolerance * norm)\n\n    cumsum /= 10\n    assert (cumsum < 1).all()\n\n\n@pytest.mark.parametrize(\n    ""method, prec_frac, tolerance"",\n    [\n        (""chebyshev"", 3, 6 / 100),\n        (""chebyshev"", 4, 1 / 1000),\n        (""exp"", 3, 6.5 / 100),\n        (""exp"", 4, 1 / 100),\n        (""maclaurin"", 3, 7 / 100),\n        (""maclaurin"", 4, 15 / 100),\n    ],\n)\ndef test_torch_sigmoid_approx(method, prec_frac, tolerance, workers):\n    """"""\n    Test the approximate sigmoid with different tolerance depending on\n    the precision_fractional considered\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    t = torch.tensor(range(-10, 10)) * 0.5\n    t_sh = t.fix_precision(precision_fractional=prec_frac).share(alice, bob, crypto_provider=james)\n    r_sh = t_sh.sigmoid(method=method)\n    r = r_sh.get().float_prec()\n    t = t.sigmoid()\n    diff = (r - t).abs().max()\n    norm = (r + t).abs().max() / 2\n\n    assert (diff / (tolerance * norm)) < 1\n\n\n@pytest.mark.parametrize(\n    ""method, prec_frac, tolerance"",\n    [\n        (""chebyshev"", 3, 3 / 100),\n        (""chebyshev"", 4, 2 / 100),\n        (""sigmoid"", 3, 10 / 100),\n        (""sigmoid"", 4, 5 / 100),\n    ],\n)\ndef test_torch_tanh_approx(method, prec_frac, tolerance, workers):\n    """"""\n    Test the approximate tanh with different tolerance depending on\n    the precision_fractional considered\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    t = torch.tensor(range(-10, 10)) * 0.5\n    t_sh = t.fix_precision(precision_fractional=prec_frac).share(alice, bob, crypto_provider=james)\n    r_sh = t_sh.tanh(method)\n    r = r_sh.get().float_prec()\n    t = t.tanh()\n    diff = (r - t).abs().max()\n    norm = (r + t).abs().max() / 2\n\n    assert (diff / (tolerance * norm)) < 1\n\n\n@pytest.mark.parametrize(""prec_frac, tolerance"", [(3, 100 / 100), (4, 3 / 100)])\ndef test_torch_log_approx(prec_frac, tolerance, workers):\n    """"""\n    Test the approximate logarithm with different tolerance depending on\n    the precision_fractional considered\n    """"""\n    alice, bob, james = workers[""alice""], workers[""bob""], workers[""james""]\n\n    cumsum = torch.zeros(9)\n    for i in range(10):\n        t = torch.tensor([0.1, 0.5, 2, 5, 10, 20, 50, 100, 250])\n        t_sh = t.fix_precision(precision_fractional=prec_frac).share(\n            alice, bob, crypto_provider=james\n        )\n        r_sh = t_sh.log()\n        r = r_sh.get().float_prec()\n        t = t.log()\n        diff = (r - t).abs()\n        norm = (r + t) / 2\n        cumsum += diff / (tolerance * norm)\n\n    cumsum /= 10\n    assert (cumsum.abs() < 1).all()\n\n\ndef test_operate_with_integer_constants():\n    x = torch.tensor([1.0])\n    x_fp = x.fix_precision()\n\n    r_fp = x_fp + 10\n    r = r_fp.float_precision()\n    assert r == x + 10\n\n    r_fp = x_fp - 7\n    r = r_fp.float_precision()\n    assert r == x - 7\n\n    r_fp = x_fp * 2\n    assert r_fp.float_precision() == x * 2\n\n    r_fp = x_fp / 5\n    assert r_fp.float_precision() == x / 5\n\n\ndef test_fixed_precision_and_sharing(workers):\n\n    bob, alice = (workers[""bob""], workers[""alice""])\n\n    x = torch.tensor([1, 2, 3, 4.0]).fix_prec().share(bob, alice)\n    out = x.get().float_prec()\n\n    assert (out == torch.tensor([1, 2, 3, 4.0])).all()\n\n    x = torch.tensor([1, 2, 3, 4.0]).fix_prec().share(bob, alice)\n\n    y = x + x\n\n    y = y.get().float_prec()\n    assert (y == torch.tensor([2, 4, 6, 8.0])).all()\n\n\ndef test_get_preserves_attributes(workers):\n    bob, alice = (workers[""bob""], workers[""alice""])\n\n    x = torch.tensor([1, 2, 3, 4.0]).fix_prec(precision_fractional=1).share(bob, alice)\n    out = x.get().float_prec()\n\n    assert (out == torch.tensor([1, 2, 3, 4.0])).all()\n\n\ndef test_comp():\n    x = torch.tensor([3.1]).fix_prec()\n    y = torch.tensor([3.1]).fix_prec()\n\n    assert (x >= y).float_prec()\n    assert (x <= y).float_prec()\n    assert not (x > y).float_prec()\n    assert not (x < y).float_prec()\n\n    x = torch.tensor([3.1]).fix_prec()\n    y = torch.tensor([2.1]).fix_prec()\n\n    assert (x >= y).float_prec()\n    assert not (x <= y).float_prec()\n    assert (x > y).float_prec()\n    assert not (x < y).float_prec()\n\n    x = torch.tensor([2.1]).fix_prec()\n    y = torch.tensor([3.1]).fix_prec()\n\n    assert not (x >= y).float_prec()\n    assert (x <= y).float_prec()\n    assert not (x > y).float_prec()\n    assert (x < y).float_prec()\n\n    # with dtype int\n    x = torch.tensor([2.1]).fix_prec(dtype=""int"")\n    y = torch.tensor([3.1]).fix_prec(dtype=""int"")\n\n    assert not (x >= y).float_prec()\n    assert (x <= y).float_prec()\n    assert not (x > y).float_prec()\n    assert (x < y).float_prec()\n\n\ndef test_dtype():\n    x = torch.tensor([3.1]).fix_prec()\n    assert (\n        x.child.dtype == ""long""\n        and x.child.field == 2 ** 64\n        and isinstance(x.child.child, torch.LongTensor)\n    )\n\n    x = torch.tensor([2.1]).fix_prec(dtype=""int"")\n    assert (\n        x.child.dtype == ""int""\n        and x.child.field == 2 ** 32\n        and isinstance(x.child.child, torch.IntTensor)\n    )\n\n    x = torch.tensor([2.1]).fix_prec(dtype=None, field=2 ** 16)\n    assert (\n        x.child.dtype == ""int""\n        and x.child.field == 2 ** 32\n        and isinstance(x.child.child, torch.IntTensor)\n    )\n\n    x = torch.tensor([3.1]).fix_prec(dtype=None, field=2 ** 62)\n    assert (\n        x.child.dtype == ""long""\n        and x.child.field == 2 ** 64\n        and isinstance(x.child.child, torch.LongTensor)\n    )\n'"
test/torch/tensors/test_tensor.py,2,"b'import torch\nimport syft\n\n\ndef test_init():\n    hook = syft.TorchHook(torch, verbose=True)\n    tensor_extension = torch.Tensor()\n    assert tensor_extension.id is not None\n    assert tensor_extension.owner is not None\n\n\ndef test_decrypt_mpc(workers):\n    alice = workers.get(""alice"")\n    bob = workers.get(""bob"")\n    cp = workers.get(""charlie"")\n    t = torch.tensor(73)\n    # without grad\n    t_encrypted = t.encrypt(protocol=""mpc"", workers=[alice, bob], crypto_provider=cp)\n    assert t_encrypted.decrypt() == t\n    # with grad\n    t_encrypted = t.encrypt(\n        protocol=""mpc"", workers=[alice, bob], crypto_provider=cp, requires_grad=True\n    )\n    assert t_encrypted.decrypt() == t\n'"
examples/tutorials/advanced/federated_sms_spam_prediction/__init__.py,0,b''
examples/tutorials/advanced/federated_sms_spam_prediction/handcrafted_GRU.py,1,"b'import numpy as np\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass GRUCell(nn.Module):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(GRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n\n        # reset gate\n        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)\n        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)\n\n        # update gate\n        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)\n        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)\n\n        # new gate\n        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)\n        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        std = 1.0 / np.sqrt(self.hidden_size)\n        for w in self.parameters():\n            w.data.uniform_(-std, std)\n\n    def forward(self, x, h):\n\n        x = x.view(-1, x.shape[1])\n\n        i_r = self.fc_ir(x)\n        h_r = self.fc_hr(h)\n        i_z = self.fc_iz(x)\n        h_z = self.fc_hz(h)\n        i_n = self.fc_in(x)\n        h_n = self.fc_hn(h)\n\n        resetgate = F.sigmoid(i_r + h_r)\n        inputgate = F.sigmoid(i_z + h_z)\n        newgate = F.tanh(i_n + (resetgate * h_n))\n\n        hy = newgate + inputgate * (h - newgate)\n\n        return hy\n\n\nclass GRU(nn.Module):\n    def __init__(\n        self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, bias=True, dropout=0.2\n    ):\n        super(GRU, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_size = output_size\n\n        # Dropout layer\n        self.dropout = nn.Dropout(p=dropout)\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # GRU Cell\n        self.gru_cell = GRUCell(embedding_dim, hidden_dim)\n        # Fully-connected layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n        # Sigmoid layer\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, h):\n\n        batch_size = x.shape[0]\n\n        # Deal with cases were the current batch_size is different from general batch_size\n        # It occurrs at the end of iteration with the Dataloaders\n        if h.shape[0] != batch_size:\n            h = h[:batch_size, :].contiguous()\n\n        # Apply embedding\n        x = self.embedding(x)\n\n        # GRU cells\n        for t in range(x.shape[1]):\n            h = self.gru_cell(x[:, t, :], h)\n\n        # Output corresponds to the last hidden state\n        out = h.contiguous().view(-1, self.hidden_dim)\n\n        # Dropout and fully-connected layers\n        out = self.dropout(out)\n        sig_out = self.sigmoid(self.fc(out))\n\n        return sig_out, h\n'"
examples/tutorials/advanced/federated_sms_spam_prediction/preprocess.py,0,"b'import numpy as np\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords  # noqa: F401\n\nSTOPWORDS = {}  # {stopwords.words(\'english\')}\n\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r""[^a-z\\s]"", """", text)\n    text = "" "".join([word for word in text.split() if word not in STOPWORDS])\n    return text\n\n\ndef tokenize(text, word_to_idx):\n    tokens = []\n    for word in text.split():\n        tokens.append(word_to_idx[word])\n    return tokens\n\n\ndef pad_and_truncate(messages, max_length=30):\n    features = np.zeros((len(messages), max_length), dtype=int)\n    for i, sms in enumerate(messages):\n        if len(sms):\n            features[i, -len(sms) :] = sms[:max_length]\n    return features\n\n\ndef main():\n    data = pd.read_csv(""SMSSpamCollection"", sep=""\\t"", header=None, names=[""label"", ""sms""])\n    data.sms = data.sms.apply(clean_text)\n    words = set(("" "".join(data.sms)).split())\n    word_to_idx = {word: i for i, word in enumerate(words, 1)}\n    tokens = data.sms.apply(lambda x: tokenize(x, word_to_idx))\n    inputs = pad_and_truncate(tokens)\n\n    labels = np.array((data.label == ""spam"").astype(int))\n\n    np.save(""./data/labels.npy"", labels)\n    np.save(""./data/inputs.npy"", inputs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/tutorials/advanced/monitor_network_traffic/__init__.py,0,b''
examples/tutorials/advanced/split_neural_network/__init__.py,0,b''
examples/tutorials/advanced/websockets_mnist/__init__.py,0,b''
examples/tutorials/advanced/websockets_mnist/run_websocket_client.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as f\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport logging\nimport argparse\nimport sys\n\nimport syft as sy\nfrom syft.workers.websocket_client import WebsocketClientWorker\nfrom syft.workers.virtual import VirtualWorker\nfrom syft.frameworks.torch.fl import utils\n\nlogger = logging.getLogger(__name__)\n\nLOG_INTERVAL = 25\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = f.relu(self.conv1(x))\n        x = f.max_pool2d(x, 2, 2)\n        x = f.relu(self.conv2(x))\n        x = f.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = f.relu(self.fc1(x))\n        x = self.fc2(x)\n        return f.log_softmax(x, dim=1)\n\n\ndef train_on_batches(worker, batches, model_in, device, lr):\n    """"""Train the model on the worker on the provided batches\n\n    Args:\n        worker(syft.workers.BaseWorker): worker on which the\n        training will be executed\n        batches: batches of data of this worker\n        model_in: machine learning model, training will be done on a copy\n        device (torch.device): where to run the training\n        lr: learning rate of the training steps\n\n    Returns:\n        model, loss: obtained model and loss after training\n\n    """"""\n    model = model_in.copy()\n    optimizer = optim.SGD(model.parameters(), lr=lr)  # TODO momentum is not supported at the moment\n\n    model.train()\n    model.send(worker)\n    loss_local = False\n\n    for batch_idx, (data, target) in enumerate(batches):\n        loss_local = False\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = f.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % LOG_INTERVAL == 0:\n            loss = loss.get()  # <-- NEW: get the loss back\n            loss_local = True\n            logger.debug(\n                ""Train Worker {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}"".format(\n                    worker.id,\n                    batch_idx,\n                    len(batches),\n                    100.0 * batch_idx / len(batches),\n                    loss.item(),\n                )\n            )\n\n    if not loss_local:\n        loss = loss.get()  # <-- NEW: get the loss back\n    model.get()  # <-- NEW: get the model back\n    return model, loss\n\n\ndef get_next_batches(fdataloader: sy.FederatedDataLoader, nr_batches: int):\n    """"""retrieve next nr_batches of the federated data loader and group\n    the batches by worker\n\n    Args:\n        fdataloader (sy.FederatedDataLoader): federated data loader\n        over which the function will iterate\n        nr_batches (int): number of batches (per worker) to retrieve\n\n    Returns:\n        Dict[syft.workers.BaseWorker, List[batches]]\n\n    """"""\n    batches = {}\n    for worker_id in fdataloader.workers:\n        worker = fdataloader.federated_dataset.datasets[worker_id].location\n        batches[worker] = []\n    try:\n        for i in range(nr_batches):\n            next_batches = next(fdataloader)\n            for worker in next_batches:\n                batches[worker].append(next_batches[worker])\n    except StopIteration:\n        pass\n    return batches\n\n\ndef train(\n    model, device, federated_train_loader, lr, federate_after_n_batches, abort_after_one=False\n):\n    model.train()\n\n    nr_batches = federate_after_n_batches\n\n    models = {}\n    loss_values = {}\n\n    iter(federated_train_loader)  # initialize iterators\n    batches = get_next_batches(federated_train_loader, nr_batches)\n    counter = 0\n\n    while True:\n        logger.debug(f""Starting training round, batches [{counter}, {counter + nr_batches}]"")\n        data_for_all_workers = True\n        for worker in batches:\n            curr_batches = batches[worker]\n            if curr_batches:\n                models[worker], loss_values[worker] = train_on_batches(\n                    worker, curr_batches, model, device, lr\n                )\n            else:\n                data_for_all_workers = False\n        counter += nr_batches\n        if not data_for_all_workers:\n            logger.debug(""At least one worker ran out of data, stopping."")\n            break\n\n        model = utils.federated_avg(models)\n        batches = get_next_batches(federated_train_loader, nr_batches)\n        if abort_after_one:\n            break\n    return model\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += f.nll_loss(output, target, reduction=""sum"").item()  # sum up batch loss\n            pred = output.argmax(1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    logger.debug(""\\n"")\n    accuracy = 100.0 * correct / len(test_loader.dataset)\n    logger.info(\n        ""Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n"".format(\n            test_loss, correct, len(test_loader.dataset), accuracy\n        )\n    )\n\n\ndef define_and_get_arguments(args=sys.argv[1:]):\n    parser = argparse.ArgumentParser(\n        description=""Run federated learning using websocket client workers.""\n    )\n    parser.add_argument(""--batch_size"", type=int, default=64, help=""batch size of the training"")\n    parser.add_argument(\n        ""--test_batch_size"", type=int, default=1000, help=""batch size used for the test data""\n    )\n    parser.add_argument(""--epochs"", type=int, default=2, help=""number of epochs to train"")\n    parser.add_argument(\n        ""--federate_after_n_batches"",\n        type=int,\n        default=50,\n        help=""number of training steps performed on each remote worker "" ""before averaging"",\n    )\n    parser.add_argument(""--lr"", type=float, default=0.01, help=""learning rate"")\n    parser.add_argument(""--cuda"", action=""store_true"", help=""use cuda"")\n    parser.add_argument(""--seed"", type=int, default=1, help=""seed used for randomization"")\n    parser.add_argument(""--save_model"", action=""store_true"", help=""if set, model will be saved"")\n    parser.add_argument(\n        ""--verbose"",\n        ""-v"",\n        action=""store_true"",\n        help=""if set, websocket client workers will "" ""be started in verbose mode"",\n    )\n    parser.add_argument(\n        ""--use_virtual"", action=""store_true"", help=""if set, virtual workers will be used""\n    )\n\n    args = parser.parse_args(args=args)\n    return args\n\n\ndef main():\n    args = define_and_get_arguments()\n\n    hook = sy.TorchHook(torch)\n\n    if args.use_virtual:\n        alice = VirtualWorker(id=""alice"", hook=hook, verbose=args.verbose)\n        bob = VirtualWorker(id=""bob"", hook=hook, verbose=args.verbose)\n        charlie = VirtualWorker(id=""charlie"", hook=hook, verbose=args.verbose)\n    else:\n        kwargs_websocket = {""host"": ""localhost"", ""hook"": hook, ""verbose"": args.verbose}\n        alice = WebsocketClientWorker(id=""alice"", port=8777, **kwargs_websocket)\n        bob = WebsocketClientWorker(id=""bob"", port=8778, **kwargs_websocket)\n        charlie = WebsocketClientWorker(id=""charlie"", port=8779, **kwargs_websocket)\n\n    workers = [alice, bob, charlie]\n\n    use_cuda = args.cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    kwargs = {""num_workers"": 1, ""pin_memory"": True} if use_cuda else {}\n\n    federated_train_loader = sy.FederatedDataLoader(\n        datasets.MNIST(\n            ""../data"",\n            train=True,\n            download=True,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n            ),\n        ).federate(tuple(workers)),\n        batch_size=args.batch_size,\n        shuffle=True,\n        iter_per_worker=True,\n        **kwargs,\n    )\n\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            ""../data"",\n            train=False,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n            ),\n        ),\n        batch_size=args.test_batch_size,\n        shuffle=True,\n        **kwargs,\n    )\n\n    model = Net().to(device)\n\n    for epoch in range(1, args.epochs + 1):\n        logger.info(""Starting epoch %s/%s"", epoch, args.epochs)\n        model = train(model, device, federated_train_loader, args.lr, args.federate_after_n_batches)\n        test(model, device, test_loader)\n\n    if args.save_model:\n        torch.save(model.state_dict(), ""mnist_cnn.pt"")\n\n\nif __name__ == ""__main__"":\n    FORMAT = ""%(asctime)s %(levelname)s %(filename)s(l:%(lineno)d) - %(message)s""\n    LOG_LEVEL = logging.DEBUG\n    logging.basicConfig(format=FORMAT, level=LOG_LEVEL)\n\n    websockets_logger = logging.getLogger(""websockets"")\n    websockets_logger.setLevel(logging.DEBUG)\n    websockets_logger.addHandler(logging.StreamHandler())\n\n    main()\n'"
examples/tutorials/advanced/websockets_mnist/start_websocket_servers.py,0,"b'import subprocess\nimport sys\nfrom pathlib import Path\n\npython = Path(sys.executable).name\n\nFILE_PATH = Path(__file__).resolve().parents[4].joinpath(""run_websocket_server.py"")\n\ncall_alice = [python, FILE_PATH, ""--port"", ""8777"", ""--id"", ""alice""]\n\ncall_bob = [python, FILE_PATH, ""--port"", ""8778"", ""--id"", ""bob""]\n\ncall_charlie = [python, FILE_PATH, ""--port"", ""8779"", ""--id"", ""charlie""]\n\n\nprint(""Starting server for Alice"")\nsubprocess.Popen(call_alice)\n\nprint(""Starting server for Bob"")\nsubprocess.Popen(call_bob)\n\nprint(""Starting server for Charlie"")\nsubprocess.Popen(call_charlie)\n'"
examples/tutorials/grid/federated_learning/__init__.py,0,b''
examples/tutorials/websocket/deploy_workers/__init__.py,0,b''
examples/tutorials/websocket/pen_testing/__init__.py,0,b''
syft/frameworks/keras/layers/__init__.py,0,"b'from syft.frameworks.keras.layers.constructor import add_constructor_registration\nfrom syft.frameworks.keras.layers.constructor import filter_layers\n\n__all__ = [""add_constructor_registration"", ""filter_layers""]\n'"
syft/frameworks/keras/layers/constructor.py,0,"b'import functools\nimport inspect\nimport re\n\nimport tf_encrypted as tfe\n\n\ndef add_constructor_registration(layer_cls):\n    """"""\n    This method rewires the layer\'s constructor to record arguments passed to it.\n    """"""\n    layer_cls._native_keras_constructor = layer_cls.__init__\n    sig = inspect.signature(layer_cls.__init__)\n\n    @functools.wraps(layer_cls.__init__)\n    def syft_keras_constructor(self, *args, **kwargs):\n        self._constructor_parameters_store = sig.bind(self, *args, **kwargs)\n        self._native_keras_constructor(*args, **kwargs)\n\n    setattr(layer_cls, ""__init__"", syft_keras_constructor)\n\n\ndef filter_layers(layers_module, tfe_layers_module):\n    """"""\n    Returns all layer types in module.\n    """"""\n    # We recognize Layer classes based on their compliance with PEP8.\n    pattern = re.compile(""[A-Z_][a-zA-Z0-9]+$"")\n    for attr_name in dir(layers_module):\n        match_result = pattern.match(attr_name)\n        if match_result is None:\n            continue\n        else:\n            layer_type = match_result.group(0)\n            if hasattr(tfe.keras.layers, layer_type):\n                yield getattr(layers_module, layer_type)\n'"
syft/frameworks/keras/model/__init__.py,0,"b'from syft.frameworks.keras.model.sequential import serve\nfrom syft.frameworks.keras.model.sequential import share\nfrom syft.frameworks.keras.model.sequential import stop\n\n__all__ = [""serve"", ""share"", ""stop""]\n'"
syft/frameworks/keras/model/sequential.py,0,"b'import inspect\n\nimport tensorflow as tf\nimport tf_encrypted as tfe\n\n# When instantiating tfe layers, exclude not supported arguments in TFE.\n_args_not_supported_by_tfe = [\n    ""activity_regularizer"",\n    ""kernel_regularizer"",\n    ""bias_regularizer"",\n    ""kernel_constraint"",\n    ""bias_constraint"",\n    ""dilation_rate"",\n]\n\n\ndef share(model, cluster, target_graph=None):\n    """"""\n    Secret share the model between `workers`.\n\n    This is done by rebuilding the model as a TF Encrypted model inside\n    `target_graph` and pushing this graph to TFEWorkers running the cluster.\n\n    Note that this keeps a TensorFlow session alive that must later be\n    shutdown using `model.stop()`.\n    """"""\n\n    # Store Keras weights before loading them in the TFE layers.\n    # TODO(Morten) we could optimize runtime by running a single model.get_weights instead\n    stored_keras_weights = {\n        keras_layer.name: keras_layer.get_weights() for keras_layer in model.layers\n    }\n\n    # Handle input combinations to configure TFE\n    _configure_tfe(cluster)\n\n    if target_graph is None:\n        # By default we create a new graph for the shared model\n        target_graph = tf.Graph()\n\n    model._tfe_graph = target_graph\n    with model._tfe_graph.as_default():\n        tfe_model, batch_input_shape = _rebuild_tfe_model(model, stored_keras_weights)\n\n        # Set up a new tfe.serving.QueueServer for the shared TFE model\n        q_input_shape = batch_input_shape\n        q_output_shape = model.output_shape\n\n        model._queue_server = tfe.serving.QueueServer(\n            input_shape=q_input_shape, output_shape=q_output_shape, computation_fn=tfe_model\n        )\n\n        initializer = tf.global_variables_initializer()\n\n    # Push and initialize shared model on servers\n    model._tfe_session = tfe.Session(graph=model._tfe_graph)\n    model._tfe_session.run(initializer)\n\n\ndef serve(model, num_requests=5):\n    """"""\n    Serve the specified number of predictions using the shared model,\n    blocking until completed.\n    """"""\n\n    global request_ix\n    request_ix = 1\n\n    def step_fn():\n        global request_ix\n        print(f""Served encrypted prediction {request_ix} to client."")\n        request_ix += 1\n\n    model._queue_server.run(model._tfe_session, num_steps=num_requests, step_fn=step_fn)\n\n\ndef stop(model):\n    """"""\n    Shutdown the TensorFlow session that was used to serve the shared model.\n    """"""\n\n    if model._tfe_session is not None:\n        sess = model._tfe_session\n        model._tfe_session = None\n        sess.close()\n        del sess\n\n\ndef _configure_tfe(cluster):\n\n    if not cluster or len(cluster.workers) != 3:\n        raise RuntimeError(""TF Encrypted expects three parties for its sharing protocols."")\n\n    config = cluster.tfe_config\n    tfe.set_config(config)\n\n    prot = tfe.protocol.SecureNN(\n        config.get_player(""server0""), config.get_player(""server1""), config.get_player(""server2"")\n    )\n    tfe.set_protocol(prot)\n    tfe.clear_initializers()\n\n\ndef _rebuild_tfe_model(keras_model, stored_keras_weights):\n    """"""\n    Rebuild the plaintext Keras model as a TF Encrypted Keras model\n    from the plaintext weights in `stored_keras_weights` using the\n    current TensorFlow graph, and the current TF Encrypted protocol\n    and configuration.\n    """"""\n\n    tfe_model = tfe.keras.Sequential()\n\n    for keras_layer in keras_model.layers:\n        tfe_layer = _instantiate_tfe_layer(keras_layer, stored_keras_weights)\n        tfe_model.add(tfe_layer)\n\n        if hasattr(tfe_layer, ""_batch_input_shape""):\n            batch_input_shape = tfe_layer._batch_input_shape\n\n    return tfe_model, batch_input_shape\n\n\ndef _instantiate_tfe_layer(keras_layer, stored_keras_weights):\n\n    # Get original layer\'s constructor parameters\n    # This method is added by the KerasHook object in syft.keras\n    constructor_params = keras_layer._constructor_parameters_store\n    constructor_params.apply_defaults()\n    _trim_params(constructor_params.arguments, _args_not_supported_by_tfe + [""self""])\n\n    # Identify tf.keras layer type, and grab the corresponding tfe.keras layer\n    keras_layer_type = _get_layer_type(keras_layer)\n    try:\n        tfe_layer_cls = getattr(tfe.keras.layers, keras_layer_type)\n    except AttributeError:\n        # TODO: rethink how we warn the user about this, maybe codegen a list of\n        #       supported layers in a doc somewhere\n        raise RuntimeError(f""TF Encrypted does not yet support the {keras_layer_type} layer."")\n\n    # Extract argument list expected by layer __init__\n    # TODO[jason]: find a better way\n    tfe_params = list(inspect.signature(tfe_layer_cls.__init__).parameters.keys())\n\n    # Remove arguments currently not supported by TFE layers\n    _trim_params(tfe_params, _args_not_supported_by_tfe + [""self"", ""kwargs""])\n\n    # Load weights from Keras layer into TFE layer\n    # TODO[jason]: generalize to n weights -- will require special handling for\n    #              optional weights like bias (or batchnorm trainables)\n    if ""kernel_initializer"" in constructor_params.arguments:\n        kernel_weights = stored_keras_weights[keras_layer.name][0]\n        k_initializer = tf.keras.initializers.Constant(kernel_weights)\n        constructor_params.arguments[""kernel_initializer""] = k_initializer\n\n    if ""use_bias"" in constructor_params.arguments:\n        if constructor_params.arguments[""use_bias""]:\n            bias_weights = stored_keras_weights[keras_layer.name][1]\n            b_initializer = tf.keras.initializers.Constant(bias_weights)\n            constructor_params.arguments[""bias_initializer""] = b_initializer\n\n    unpacked_kwargs = constructor_params.arguments.pop(""kwargs"")\n    tfe_kwargs = {**constructor_params.arguments, **unpacked_kwargs}\n\n    return tfe_layer_cls(**tfe_kwargs)\n\n\ndef _get_layer_type(keras_layer_cls):\n    return keras_layer_cls.__class__.__name__\n\n\ndef _trim_params(params, filter_list):\n    for arg_name in filter_list:\n        try:\n            del params[arg_name]\n        except TypeError:\n            # params is a list of strings\n            if arg_name in params:\n                params.remove(arg_name)\n        except KeyError:\n            continue\n'"
syft/frameworks/torch/dp/__init__.py,0,b'from . import pate  # noqa: F401\n'
syft/frameworks/torch/dp/pate.py,21,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# ==============================================================================\n# Modifications copyright (C) 2020 OpenMined\n#\n# Added type hints to functions\n# Added moment values to print statements when calculating sensitivity\n# ==============================================================================\n\n\n""""""\nThis script computes bounds on the privacy cost of training the\nstudent model from noisy aggregation of labels predicted by teachers.\nIt should be used only after training the student (and therefore the\nteachers as well). We however include the label files required to\nreproduce key results from our paper (https://arxiv.org/abs/1610.05755):\nthe epsilon bounds for MNIST and SVHN students.\n""""""\nimport math\nfrom typing import List, Tuple, Union\n\nimport numpy as np\nimport torch\n\n\ndef compute_q_noisy_max(counts: Union[np.ndarray, List[float]], noise_eps: float) -> float:\n    """"""\n    Returns ~ Pr[outcome != winner].\n\n    Args:\n        counts: a list of scores\n        noise_eps: privacy parameter for noisy_max\n    Returns:\n        q: the probability that outcome is different from true winner.\n    """"""\n    # For noisy max, we only get an upper bound.\n    # Pr[ j beats i*] \\leq (2+gap(j,i*))/ 4 exp(gap(j,i*)\n    # proof at http://mathoverflow.net/questions/66763/\n    # tight-bounds-on-probability-of-sum-of-laplace-random-variables\n\n    winner = np.argmax(counts)\n    counts_normalized = noise_eps * (counts - counts[winner])\n\n    counts_rest = np.array([counts_normalized[i] for i in range(len(counts)) if i != winner])\n    q = 0.0\n\n    for c in counts_rest:\n        gap = -c\n        q += (gap + 2.0) / (4.0 * math.exp(gap))\n\n    return min(q, 1.0 - (1.0 / len(counts)))\n\n\ndef compute_q_noisy_max_approx(counts: List[float], noise_eps: float) -> float:\n    """"""\n    Returns ~ Pr[outcome != winner].\n\n    Args:\n        counts: a list of scores\n        noise_eps: privacy parameter for noisy_max\n    Returns:\n        q: the probability that outcome is different from true winner.\n    """"""\n    # For noisy max, we only get an upper bound.\n    # Pr[ j beats i*] \\leq (2+gap(j,i*))/ 4 exp(gap(j,i*)\n    # proof at http://mathoverflow.net/questions/66763/\n    # tight-bounds-on-probability-of-sum-of-laplace-random-variables\n    # This code uses an approximation that is faster and easier\n    # to get local sensitivity bound on.\n\n    winner = np.argmax(counts)\n    counts_normalized = noise_eps * (counts - counts[winner])\n    counts_rest = np.array([counts_normalized[i] for i in range(len(counts)) if i != winner])\n    gap = -max(counts_rest)\n    q = (len(counts) - 1) * (gap + 2.0) / (4.0 * math.exp(gap))\n    return min(q, 1.0 - (1.0 / len(counts)))\n\n\ndef logmgf_exact(q: float, priv_eps: float, l: int) -> float:\n    """"""\n    Computes the logmgf value given q and privacy eps.\n\n    The bound used is the min of three terms. The first term is from\n    https://arxiv.org/pdf/1605.02065.pdf.\n    The second term is based on the fact that when event has probability (1-q) for\n    q close to zero, q can only change by exp(eps), which corresponds to a\n    much smaller multiplicative change in (1-q)\n    The third term comes directly from the privacy guarantee.\n\n    Args:\n        q: pr of non-optimal outcome\n        priv_eps: eps parameter for DP\n        l: moment to compute.\n    Returns:\n        Upper bound on logmgf\n    """"""\n    if q < 0.5:\n        t_one = (1 - q) * math.pow((1 - q) / (1 - math.exp(priv_eps) * q), l)\n        t_two = q * math.exp(priv_eps * l)\n        t = t_one + t_two\n        try:\n            log_t = math.log(t)\n        except ValueError:\n            print(""Got ValueError in math.log for values :"" + str((q, priv_eps, l, t)))\n            log_t = priv_eps * l\n    else:\n        log_t = priv_eps * l\n\n    return min(0.5 * priv_eps * priv_eps * l * (l + 1), log_t, priv_eps * l)\n\n\ndef logmgf_from_counts(counts: Union[np.ndarray, List[float]], noise_eps: float, l: int) -> float:\n    """"""\n    ReportNoisyMax mechanism with noise_eps with 2*noise_eps-DP\n    in our setting where one count can go up by one and another\n    can go down by 1.\n\n    Args:\n        counts: an array of scores\n        noise_eps: noise epsilon used\n        l: moment to compute\n    Returns:\n        q: Upper bound on logmgf\n    """"""\n    q = compute_q_noisy_max(counts, noise_eps)\n    return logmgf_exact(q, 2.0 * noise_eps, l)\n\n\ndef sens_at_k(counts: np.ndarray, noise_eps: float, l: int, k: float) -> float:\n    """"""\n    Return sensitivity at distance k.\n\n    Args:\n        counts: an array of scores\n        noise_eps: noise parameter used\n        l: moment whose sensitivity is being computed\n        k: distance\n    Returns:\n        sensitivity: at distance k\n    """"""\n    counts_sorted = sorted(counts, reverse=True)\n\n    if 0.5 * noise_eps * l > 1:\n        print(f""l of {l} too large to compute sensitivity with noise epsilon {noise_eps}"")\n        return 0\n\n    # Now we can assume that at k, gap remains positive\n    # or we have reached the point where logmgf_exact is\n    # determined by the first term and ind of q.\n    if counts[0] < counts[1] + k:\n        return 0\n\n    counts_sorted[0] -= k\n    counts_sorted[1] += k\n    val = logmgf_from_counts(counts_sorted, noise_eps, l)\n\n    counts_sorted[0] -= 1\n    counts_sorted[1] += 1\n    val_changed = logmgf_from_counts(counts_sorted, noise_eps, l)\n\n    return val_changed - val\n\n\ndef smoothed_sens(counts: np.ndarray, noise_eps: float, l: int, beta: float) -> float:\n    """"""\n    Compute beta-smooth sensitivity.\n\n    Args:\n        counts: array of scores\n        noise_eps: noise parameter\n        l: moment of interest\n        beta: smoothness parameter\n    Returns:\n        smooth_sensitivity: a beta smooth upper bound\n    """"""\n    k = 0\n    smoothed_sensitivity = sens_at_k(counts, noise_eps, l, k)\n\n    while k < max(counts):\n        k += 1\n        sensitivity_at_k = sens_at_k(counts, noise_eps, l, k)\n        smoothed_sensitivity = max(smoothed_sensitivity, math.exp(-beta * k) * sensitivity_at_k)\n\n        if sensitivity_at_k == 0.0:\n            break\n\n    return smoothed_sensitivity\n\n\ndef perform_analysis(\n    teacher_preds: np.ndarray,\n    indices: np.ndarray,\n    noise_eps: float,\n    delta: float = 1e-5,\n    moments: int = 8,\n    beta: float = 0.09,\n) -> Tuple[float, float]:\n    """"""\n    Performs PATE analysis on predictions from teachers and combined predictions for student.\n\n    Args:\n        teacher_preds: a numpy array of dim (num_teachers x num_examples). Each value corresponds\n            to the index of the label which a teacher gave for a specific example\n        indices: a numpy array of dim (num_examples) of aggregated examples which were aggregated\n            using the noisy max mechanism.\n        noise_eps: the epsilon level used to create the indices\n        delta: the desired level of delta\n        moments: the number of moments to track (see the paper)\n        beta: a smoothing parameter (see the paper)\n    Returns:\n        tuple: first value is the data dependent epsilon, then the data independent epsilon\n    """"""\n    num_teachers, num_examples = teacher_preds.shape\n    _num_examples = indices.shape[0]\n    labels = set(teacher_preds.flatten())\n    num_labels = len(labels)\n\n    assert num_examples == _num_examples\n\n    counts_mat = np.zeros((num_examples, num_labels))\n\n    for i in range(num_examples):\n        for j in range(num_teachers):\n            counts_mat[i, int(teacher_preds[j, i])] += 1\n\n    l_list = 1.0 + np.array(range(moments))\n\n    total_log_mgf_nm = np.array([0.0 for _ in l_list])\n    total_ss_nm = np.array([0.0 for _ in l_list])\n\n    for i in indices:\n        total_log_mgf_nm += np.array(\n            [logmgf_from_counts(counts_mat[i], noise_eps, l) for l in l_list]\n        )\n\n        total_ss_nm += np.array([smoothed_sens(counts_mat[i], noise_eps, l, beta) for l in l_list])\n\n    # We want delta = exp(alpha - eps l).\n    # Solving gives eps = (alpha - ln (delta))/l\n\n    eps_list_nm = (total_log_mgf_nm - math.log(delta)) / l_list\n\n    # If beta < eps / 2 ln (1/delta), then adding noise Lap(1) * 2 SS/eps\n    # is eps,delta DP\n    # Also if beta < eps / 2(gamma +1), then adding noise 2(gamma+1) SS eta / eps\n    # where eta has density proportional to 1 / (1+|z|^gamma) is eps-DP\n    # Both from Corolloary 2.4 in\n    # http://www.cse.psu.edu/~ads22/pubs/NRS07/NRS07-full-draft-v1.pdf\n    # Print the first one\'s scale\n\n    ss_eps = 2.0 * beta * math.log(1 / delta)\n    ss_scale = 2.0 / ss_eps\n\n    if min(eps_list_nm) == eps_list_nm[-1]:\n        print(\n            ""Warning: May not have used enough values of l. Increase \'moments\' variable and ""\n            ""run again.""\n        )\n\n    # Data independent bound, as mechanism is\n    # 2*noise_eps DP.\n    data_ind_log_mgf = np.array([0.0 for _ in l_list])\n    data_ind_log_mgf += num_examples * np.array(\n        [logmgf_exact(1.0, 2.0 * noise_eps, l) for l in l_list]\n    )\n\n    data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list\n\n    return min(eps_list_nm), min(data_ind_eps_list)\n\n\ndef tensors_to_literals(tensor_list: List[torch.Tensor]) -> List[Union[float, int]]:\n    """"""\n    Converts list of torch tensors to list of integers/floats. Fix for not having the functionality\n    which converts list of tensors to tensors\n\n    Args:\n        tensor_list: List of torch tensors\n    Returns:\n        literal_list: List of floats/integers\n    """"""\n    literal_list = []\n\n    for tensor in tensor_list:\n        literal_list.append(tensor.item())\n\n    return literal_list\n\n\ndef logmgf_exact_torch(q: float, priv_eps: float, l: int) -> float:\n    """"""\n    Computes the logmgf value given q and privacy eps.\n\n    The bound used is the min of three terms. The first term is from\n    https://arxiv.org/pdf/1605.02065.pdf.\n    The second term is based on the fact that when event has probability (1-q) for\n    q close to zero, q can only change by exp(eps), which corresponds to a\n    much smaller multiplicative change in (1-q)\n    The third term comes directly from the privacy guarantee.\n\n    Args:\n        q: pr of non-optimal outcome\n        priv_eps: eps parameter for DP\n        l: moment to compute.\n    Returns:\n        Upper bound on logmgf\n    """"""\n    if q < 0.5:\n        t_one = (1 - q) * math.pow((1 - q) / (1 - math.exp(priv_eps) * q), l)\n        t_two = q * math.exp(priv_eps * l)\n        t = t_one + t_two\n\n        try:\n            log_t = math.log(t)\n        except ValueError:\n            print(""Got ValueError in math.log for values :"" + str((q, priv_eps, l, t)))\n            log_t = priv_eps * l\n    else:\n        log_t = priv_eps * l\n\n    return min(0.5 * priv_eps * priv_eps * l * (l + 1), log_t, priv_eps * l)\n\n\ndef compute_q_noisy_max_torch(\n    counts: Union[List[torch.Tensor], torch.Tensor], noise_eps: float\n) -> float:\n    """"""\n    Returns ~ Pr[outcome != winner].\n\n    Args:\n        counts: a list of scores\n        noise_eps: privacy parameter for noisy_max\n    Returns:\n        q: the probability that outcome is different from true winner.\n    """"""\n    if type(counts) != torch.tensor:\n        counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)\n\n    _, winner = counts.max(0)\n    counts_normalized = noise_eps * (counts.clone().detach().type(torch.float) - counts[winner])\n\n    counts_normalized = tensors_to_literals(counts_normalized)\n    counts_rest = torch.tensor(\n        [counts_normalized[i] for i in range(len(counts)) if i != winner], dtype=torch.float\n    )\n    q = 0.0\n\n    index = 0\n    for c in counts_rest:\n        gap = -c\n        q += (gap + 2.0) / (4.0 * math.exp(gap))\n\n        index += 1\n\n    return min(q, 1.0 - (1.0 / len(counts)))\n\n\ndef logmgf_from_counts_torch(\n    counts: Union[List[torch.Tensor], torch.Tensor], noise_eps: float, l: int\n) -> float:\n    """"""\n    ReportNoisyMax mechanism with noise_eps with 2*noise_eps-DP\n    in our setting where one count can go up by one and another\n    can go down by 1.\n\n    Args:\n        counts: a list of scores\n        noise_eps: noise parameter used\n        l: moment whose sensitivity is being computed\n    Returns:\n        q: the probability that outcome is different from true winner\n    """"""\n    q = compute_q_noisy_max_torch(counts, noise_eps)\n\n    return logmgf_exact_torch(q, 2.0 * noise_eps, l)\n\n\ndef sens_at_k_torch(counts: torch.Tensor, noise_eps: float, l: int, k: int) -> float:\n    """"""\n    Return sensitivity at distane k.\n\n    Args:\n        counts: tensor of scores\n        noise_eps: noise parameter used\n        l: moment whose sensitivity is being computed\n        k: distance\n    Returns:\n        sensitivity: at distance k\n    """"""\n\n    counts_sorted = sorted(counts, reverse=True)\n\n    if 0.5 * noise_eps * l > 1:\n        print(f""l of {l} is too large to compute sensitivity with noise epsilon {noise_eps}"")\n        return 0\n\n    if counts[0] < counts[1] + k:\n        return 0\n\n    counts_sorted[0] -= k\n    counts_sorted[1] += k\n    val = logmgf_from_counts_torch(counts_sorted, noise_eps, l)\n\n    counts_sorted[0] -= 1\n    counts_sorted[1] += 1\n    val_changed = logmgf_from_counts_torch(counts_sorted, noise_eps, l)\n\n    return val_changed - val\n\n\ndef smooth_sens_torch(counts: torch.Tensor, noise_eps: float, l: int, beta: float) -> float:\n    """"""Compute beta-smooth sensitivity.\n\n    Args:\n        counts: tensor of scores\n        noise_eps: noise parameter\n        l: moment of interest\n        beta: smoothness parameter\n    Returns:\n        smooth_sensitivity: a beta smooth upper bound\n    """"""\n    k = 0\n    smoothed_sensitivity = sens_at_k_torch(counts, noise_eps, l, k)\n\n    while k < max(counts):\n        k += 1\n        sensitivity_at_k = sens_at_k_torch(counts, noise_eps, l, k)\n        smoothed_sensitivity = max(smoothed_sensitivity, math.exp(-beta * k) * sensitivity_at_k)\n\n        if sensitivity_at_k == 0.0:\n            break\n\n    return smoothed_sensitivity\n\n\ndef perform_analysis_torch(\n    preds: torch.Tensor,\n    indices: torch.Tensor,\n    noise_eps: float = 0.1,\n    delta: float = 1e-5,\n    moments: int = 8,\n    beta: float = 0.09,\n) -> Tuple[float, float]:\n    """"""\n    Performs PATE analysis on predictions from teachers and combined predictions for student.\n\n    Args:\n        preds: a torch tensor of dim (num_teachers x num_examples). Each value corresponds to the\n            index of the label which a teacher gave for a specific example\n        indices: a torch tensor of dim (num_examples) of aggregated examples which were aggregated\n            using the noisy max mechanism.\n        noise_eps: the epsilon level used to create the indices\n        delta: the desired level of delta\n        moments: the number of moments to track (see the paper)\n        beta: a smoothing parameter (see the paper)\n    Returns:\n        tuple: first value is the data dependent epsilon, then the data independent epsilon\n    """"""\n    num_teachers, num_examples = preds.shape\n    _num_examples = indices.shape[0]\n\n    # Check that preds is shape (teachers x examples)\n    assert num_examples == _num_examples\n\n    labels = list(preds.flatten())\n    labels = {tensor.item() for tensor in labels}\n    num_labels = len(labels)\n\n    counts_mat = torch.zeros(num_examples, num_labels, dtype=torch.float32)\n\n    # Count number of teacher predictions of each label for each example\n    for i in range(num_examples):\n        for j in range(num_teachers):\n            counts_mat[i, int(preds[j, i])] += 1\n\n    l_list = 1 + torch.tensor(range(moments), dtype=torch.float)\n\n    total_log_mgf_nm = torch.tensor([0.0 for _ in l_list], dtype=torch.float)\n    total_ss_nm = torch.tensor([0.0 for _ in l_list], dtype=torch.float)\n\n    for i in indices:\n        total_log_mgf_nm += torch.tensor(\n            [logmgf_from_counts_torch(counts_mat[i].clone(), noise_eps, l) for l in l_list]\n        )\n\n        total_ss_nm += torch.tensor(\n            [smooth_sens_torch(counts_mat[i].clone(), noise_eps, l, beta) for l in l_list],\n            dtype=torch.float,\n        )\n\n    eps_list_nm = (total_log_mgf_nm - math.log(delta)) / l_list\n    ss_eps = 2.0 * beta * math.log(1 / delta)\n    ss_scale = 2.0 / ss_eps\n\n    if min(eps_list_nm) == eps_list_nm[-1]:\n        print(\n            ""Warning: May not have used enough values of l. Increase \'moments\' variable ""\n            ""and run again.""\n        )\n\n    # Computer epsilon when not taking teacher quorum into account\n    data_ind_log_mgf = torch.tensor([0.0 for _ in l_list])\n    data_ind_log_mgf += num_examples * torch.tensor(\n        tensors_to_literals([logmgf_exact_torch(1.0, 2.0 * noise_eps, l) for l in l_list])\n    )\n\n    data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list\n\n    return min(eps_list_nm), min(data_ind_eps_list)\n'"
syft/frameworks/torch/fl/__init__.py,0,b'from .dataset import BaseDataset  # noqa: F401\nfrom .dataset import FederatedDataset  # noqa: F401\nfrom .dataloader import FederatedDataLoader  # noqa: F401\n'
syft/frameworks/torch/fl/dataloader.py,16,"b'import torch\nfrom torch.utils.data import SequentialSampler, RandomSampler, BatchSampler\nfrom torch._six import string_classes, int_classes, container_abcs\n\nimport logging\nimport math\n\nnumpy_type_map = {\n    ""float64"": torch.DoubleTensor,\n    ""float32"": torch.FloatTensor,\n    ""float16"": torch.HalfTensor,\n    ""int64"": torch.LongTensor,\n    ""int32"": torch.IntTensor,\n    ""int16"": torch.ShortTensor,\n    ""int8"": torch.CharTensor,\n    ""uint8"": torch.ByteTensor,\n}\n\n\ndef default_collate(batch):\n    """"""Puts each data field into a tensor with outer dimension batch size""""""\n\n    error_msg = ""batch must contain tensors, numbers, dicts or lists; found {}""\n    elem_type = type(batch[0])\n    if isinstance(batch[0], torch.Tensor):\n        return torch.stack(batch, 0)\n    elif (\n        elem_type.__module__ == ""numpy""\n        and elem_type.__name__ != ""str_""\n        and elem_type.__name__ != ""string_""\n    ):  # pragma: no cover\n        elem = batch[0]\n        if elem_type.__name__ == ""ndarray"":\n            return torch.stack([torch.from_numpy(b) for b in batch], 0)\n        if elem.shape == ():  # scalars\n            py_type = float if elem.dtype.name.startswith(""float"") else int\n            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n    elif isinstance(batch[0], int_classes):  # pragma: no cover\n        return torch.LongTensor(batch)\n    elif isinstance(batch[0], float):  # pragma: no cover\n        return torch.DoubleTensor(batch)\n    elif isinstance(batch[0], string_classes):  # pragma: no cover\n        return batch\n    elif isinstance(batch[0], container_abcs.Mapping):  # pragma: no cover\n        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], container_abcs.Sequence):  # pragma: no cover\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n\n    raise TypeError((error_msg.format(type(batch[0]))))\n\n\nclass _DataLoaderIter(object):\n    """"""Iterates once over the DataLoader\'s dataset, as specified by the samplers""""""\n\n    def __init__(self, loader, worker_idx):\n        self.loader = loader\n        self.federated_dataset = loader.federated_dataset\n\n        # Assign the first worker to invoke\n        self.worker_idx = worker_idx\n        # List workers in a dict\n        self.workers = {idx: worker for idx, worker in enumerate(loader.workers)}\n\n        # The function used to stack all samples together\n        self.collate_fn = loader.collate_fn\n\n        # Create a sample iterator for each worker\n        self.sample_iter = {\n            worker: iter(batch_sampler) for worker, batch_sampler in loader.batch_samplers.items()\n        }\n\n    def __len__(self):\n        return len(self.federated_dataset)\n\n    def _get_batch(self):\n        # If all workers have been used, end the iterator\n        if len(self.workers) == 0:\n            self.stop()\n\n        worker = self.workers[self.worker_idx]\n\n        try:\n            indices = next(self.sample_iter[worker])\n            batch = self.collate_fn([self.federated_dataset[worker][i] for i in indices])\n            return batch\n        # All the data for this worker has been used\n        except StopIteration:\n            # Forget this worker\n            del self.workers[self.worker_idx]\n            # Find another worker which is not busy\n            worker_busy_ids = [it.worker_idx for it in self.loader.iterators]\n            for idx in self.workers.keys():\n                if idx not in worker_busy_ids:\n                    self.worker_idx = idx\n                    return self._get_batch()\n\n            # If nothing is found, stop the iterator\n            self.stop()\n\n    def __next__(self):\n        batch = self._get_batch()\n        return batch\n\n    def __iter__(self):\n        return self\n\n    def stop(self):\n        self.worker_idx = -1\n        raise StopIteration\n\n\nclass _DataLoaderOneWorkerIter(object):\n    """"""Iterates once over the worker\'s dataset, as specified by its sampler""""""\n\n    def __init__(self, loader, worker_idx):\n        self.loader = loader\n        self.federated_dataset = loader.federated_dataset\n\n        # Assign the worker to invoke\n        self.worker = loader.workers[worker_idx]\n\n        # The function used to stack all samples together\n        self.collate_fn = loader.collate_fn\n\n        # Create a sample iterator for each worker\n        self.sample_iter = iter(loader.batch_samplers[self.worker])\n\n    def _get_batch(self):\n        # If all workers have been used, end the iterator\n        if not self.worker:\n            self.stop()\n\n        try:\n            indices = next(self.sample_iter)\n            batch = self.collate_fn([self.federated_dataset[self.worker][i] for i in indices])\n            return batch\n        # All the data for this worker has been used\n        except StopIteration:\n            # If nothing is found, stop the iterator\n            self.stop()\n\n    # TODO: implement a length function. It should return the number of elements\n    #       of the federated dataset that are located at this worker\n    # def __len__(self):\n    #    return len(self.federated_dataset)\n\n    def __next__(self):\n        return self._get_batch()\n\n    def __iter__(self):\n        return self\n\n    def stop(self):\n        self.worker = None\n        raise StopIteration\n\n\nclass FederatedDataLoader(object):\n    """"""\n    Data loader. Combines a dataset and a sampler, and provides\n    single or several iterators over the dataset.\n\n    Arguments:\n        federated_dataset (FederatedDataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: ``1``).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: ``False``).\n        collate_fn (callable, optional): merges a list of samples to form a mini-batch.\n        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n            if the dataset size is not divisible by the batch size. If ``False`` and\n            the size of dataset is not divisible by the batch size, then the last batch\n            will be smaller. (default: ``False``)\n        num_iterators (int): number of workers from which to retrieve data in parallel.\n            num_iterators <= len(federated_dataset.workers) - 1\n            the effect is to retrieve num_iterators epochs of data but at each step data\n            from num_iterators distinct workers is returned.\n        iter_per_worker (bool): if set to true, __next__() will return a dictionary\n            containing one batch per worker\n    """"""\n\n    __initialized = False\n\n    def __init__(\n        self,\n        federated_dataset,\n        batch_size=8,\n        shuffle=False,\n        num_iterators=1,\n        drop_last=False,\n        collate_fn=default_collate,\n        iter_per_worker=False,\n        **kwargs,\n    ):\n        if len(kwargs) > 0:\n            options = "", "".join([f""{k}: {v}"" for k, v in kwargs.items()])\n            logging.warning(f""The following options are not supported: {options}"")\n\n        try:\n            self.workers = federated_dataset.workers\n        except AttributeError:\n            raise Exception(\n                ""Your dataset is not a FederatedDataset, please use ""\n                ""torch.utils.data.DataLoader instead.""\n            )\n\n        self.federated_dataset = federated_dataset\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        self.collate_fn = collate_fn\n        self.iter_class = _DataLoaderOneWorkerIter if iter_per_worker else _DataLoaderIter\n\n        # Build a batch sampler per worker\n        self.batch_samplers = {}\n        for worker in self.workers:\n            data_range = range(len(federated_dataset[worker]))\n            if shuffle:\n                sampler = RandomSampler(data_range)\n            else:\n                sampler = SequentialSampler(data_range)\n            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n            self.batch_samplers[worker] = batch_sampler\n\n        if iter_per_worker:\n            self.num_iterators = len(self.workers)\n        else:\n            # You can\'t have more iterators than n - 1 workers, because you always\n            # need a worker idle in the worker switch process made by iterators\n            if len(self.workers) == 1:\n                self.num_iterators = 1\n            else:\n                self.num_iterators = min(num_iterators, len(self.workers) - 1)\n\n    def __iter__(self):\n        self.iterators = []\n        for idx in range(self.num_iterators):\n            self.iterators.append(self.iter_class(self, worker_idx=idx))\n        return self\n\n    def __next__(self):\n        if self.num_iterators > 1:\n            batches = {}\n            for iterator in self.iterators:\n                data, target = next(iterator)\n                batches[data.location] = (data, target)\n            return batches\n        else:\n            iterator = self.iterators[0]\n            data, target = next(iterator)\n            return data, target\n\n    def __len__(self):\n        length = len(self.federated_dataset) / self.batch_size\n        if self.drop_last:\n            return int(length)\n        else:\n            return math.ceil(length)\n'"
syft/frameworks/torch/fl/dataset.py,6,"b'import math\nimport logging\nfrom syft.generic.abstract.sendable import AbstractSendable\nfrom syft.workers.base import BaseWorker\nfrom syft.generic.pointers.pointer_dataset import PointerDataset\nfrom syft_proto.frameworks.torch.fl.v1.dataset_pb2 import BaseDataset as BaseDatasetPB\n\nimport torch\nfrom torch.utils.data import Dataset\nimport syft\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseDataset(AbstractSendable):\n    """"""\n    This is a base class to be used for manipulating a dataset. This is composed\n    of a .data attribute for inputs and a .targets one for labels. It is to\n    be used like the MNIST Dataset object, and is useful to avoid handling\n    the two inputs and label tensors separately.\n\n    Args:\n\n        data[list,torch tensors]: the data points\n        targets: Corresponding labels of the data points\n        transform: Function to transform the datapoints\n\n    """"""\n\n    def __init__(self, data, targets, transform=None, owner=None, **kwargs):\n        if owner is None:\n            owner = syft.framework.hook.local_worker\n        super().__init__(owner=owner, **kwargs)\n        self.data = data\n        self.targets = targets\n        self.transform_ = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n\n            index[integer]: index of item to get\n\n        Returns:\n\n            data: Data points corresponding to the given index\n            targets: Targets correspoding to given datapoint\n        """"""\n        data_elem = self.data[index]\n        if self.transform_ is not None:\n            # TODO: avoid passing through numpy domain\n            data_elem = torch.tensor(self.transform_(data_elem.numpy()))\n\n        return data_elem, self.targets[index]\n\n    def transform(self, transform):\n        """""" Allows a transform to be applied on given dataset.\n\n        Args:\n            transform: The transform to be applied on the data\n        """"""\n\n        # Transforms cannot be applied to Pointer, Fixed Precision or Float Precision tensors.\n        if type(self.data) == torch.Tensor:\n\n            self.data = transform(self.data)\n\n        else:\n\n            raise TypeError(""Transforms can be applied only on torch tensors"")\n\n    def send(self, location: BaseWorker):\n        ptr = self.owner.send(self, workers=location)\n        return ptr\n\n    def get(self):\n        """"""\n        Gets the data back from respective workers.\n        """"""\n\n        self.data.get_()\n        self.targets.get_()\n        return self\n\n    def get_data(self):\n        return self.data\n\n    def get_targets(self):\n        return self.targets\n\n    def fix_prec(self, *args, **kwargs):\n        """"""\n        Converts data of BaseDataset into fixed precision\n        """"""\n        self.data.fix_prec_(*args, **kwargs)\n        self.targets.fix_prec_(*args, **kwargs)\n        return self\n\n    fix_precision = fix_prec\n\n    def float_prec(self, *args, **kwargs):\n        """"""\n        Converts data of BaseDataset into float precision\n        """"""\n        self.data.float_prec_(*args, **kwargs)\n        self.targets.float_prec_(*args, **kwargs)\n        return self\n\n    float_precision = float_prec\n\n    def share(self, *args, **kwargs):\n        """"""\n        Share the data with the respective workers\n        """"""\n        self.data.share_(*args, **kwargs)\n        self.targets.share_(*args, **kwargs)\n        return self\n\n    def create_pointer(\n        self, owner, garbage_collect_data, location=None, id_at_location=None, **kwargs\n    ):\n        """"""creats a pointer to the self dataset""""""\n        if owner is None:\n            owner = self.owner\n\n        if location is None:\n            location = self.owner\n\n        owner = self.owner.get_worker(owner)\n        location = self.owner.get_worker(location)\n\n        return PointerDataset(\n            owner=owner,\n            location=location,\n            id_at_location=id_at_location or self.id,\n            garbage_collect_data=garbage_collect_data,\n            tags=self.tags,\n            description=self.description,\n        )\n\n    def __repr__(self):\n\n        fmt_str = ""BaseDataset\\n""\n        fmt_str += f""\\tData: {self.data}\\n""\n        fmt_str += f""\\ttargets: {self.targets}""\n\n        if self.tags is not None and len(self.tags):\n            fmt_str += ""\\n\\tTags: ""\n            for tag in self.tags:\n                fmt_str += str(tag) + "" ""\n\n        if self.description is not None:\n            fmt_str += ""\\n\\tDescription: "" + str(self.description).split(""\\n"")[0] + ""...""\n\n        return fmt_str\n\n    @property\n    def location(self):\n        """"""\n        Get location of the data\n        """"""\n        return self.data.location\n\n    @staticmethod\n    def simplify(worker, dataset: ""BaseDataset"") -> tuple:\n        chain = None\n        if hasattr(dataset, ""child""):\n            chain = syft.serde.msgpack.serde._simplify(worker, dataset.child)\n        return (\n            syft.serde.msgpack.serde._simplify(worker, dataset.data),\n            syft.serde.msgpack.serde._simplify(worker, dataset.targets),\n            dataset.id,\n            syft.serde.msgpack.serde._simplify(worker, dataset.tags),\n            syft.serde.msgpack.serde._simplify(worker, dataset.description),\n            chain,\n        )\n\n    @staticmethod\n    def detail(worker, dataset_tuple: tuple) -> ""BaseDataset"":\n        data, targets, id, tags, description, chain = dataset_tuple\n        dataset = BaseDataset(\n            syft.serde.msgpack.serde._detail(worker, data),\n            syft.serde.msgpack.serde._detail(worker, targets),\n            owner=worker,\n            id=id,\n            tags=syft.serde.msgpack.serde._detail(worker, tags),\n            description=syft.serde.msgpack.serde._detail(worker, description),\n        )\n        if chain is not None:\n            chain = syft.serde.msgpack.serde._detail(worker, chain)\n            dataset.child = chain\n        return dataset\n\n    @staticmethod\n    def bufferize(worker, dataset):\n        """"""\n        This method serializes a BaseDataset into a BaseDatasetPB.\n\n        Args:\n            dataset (BaseDataset): input BaseDataset to be serialized.\n\n        Returns:\n            proto_dataset (BaseDatasetPB): serialized BaseDataset.\n        """"""\n        proto_dataset = BaseDatasetPB()\n        proto_dataset.data.CopyFrom(syft.serde.protobuf.serde._bufferize(worker, dataset.data))\n        proto_dataset.targets.CopyFrom(\n            syft.serde.protobuf.serde._bufferize(worker, dataset.targets)\n        )\n        syft.serde.protobuf.proto.set_protobuf_id(proto_dataset.id, dataset.id)\n        for tag in dataset.tags:\n            proto_dataset.tags.append(tag)\n\n        if dataset.child:\n            proto_dataset.child.CopyFrom(dataset.child)\n\n        proto_dataset.description = dataset.description\n        return proto_dataset\n\n    @staticmethod\n    def unbufferize(worker, proto_dataset):\n        """"""\n        This method deserializes BaseDatasetPB into a BaseDataset.\n\n        Args:\n            proto_dataset (BaseDatasetPB): input serialized BaseDatasetPB.\n\n        Returns:\n             BaseDataset: deserialized BaseDatasetPB.\n        """"""\n        data = syft.serde.protobuf.serde._unbufferize(worker, proto_dataset.data)\n        targets = syft.serde.protobuf.serde._unbufferize(worker, proto_dataset.targets)\n        dataset_id = syft.serde.protobuf.proto.get_protobuf_id(proto_dataset.id)\n        child = None\n        if proto_dataset.HasField(""child""):\n            child = syft.serde.protobuf.serde._unbufferize(worker, proto_dataset.child)\n        return BaseDataset(\n            data=data,\n            targets=targets,\n            id=dataset_id,\n            tags=set(proto_dataset.tags),\n            description=proto_dataset.description,\n            child=child,\n        )\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n        This method returns the protobuf schema used for BaseDataset.\n\n        Returns:\n           Protobuf schema for BaseDataset.\n       """"""\n        return BaseDatasetPB\n\n\ndef dataset_federate(dataset, workers):\n    """"""\n    Add a method to easily transform a torch.Dataset or a sy.BaseDataset\n    into a sy.FederatedDataset. The dataset given is split in len(workers)\n    part and sent to each workers\n    """"""\n    logger.info(f""Scanning and sending data to {\', \'.join([w.id for w in workers])}..."")\n\n    # take ceil to have exactly len(workers) sets after splitting\n    data_size = math.ceil(len(dataset) / len(workers))\n\n    datasets = []\n    data_loader = torch.utils.data.DataLoader(dataset, batch_size=data_size)\n    for dataset_idx, (data, targets) in enumerate(data_loader):\n        worker = workers[dataset_idx % len(workers)]\n        logger.debug(""Sending data to worker %s"", worker.id)\n        data = data.send(worker)\n        targets = targets.send(worker)\n        datasets.append(BaseDataset(data, targets))  # .send(worker)\n\n    logger.debug(""Done!"")\n    return FederatedDataset(datasets)\n\n\nDataset.federate = dataset_federate\nBaseDataset.federate = dataset_federate\n\n\nclass FederatedDataset:\n    def __init__(self, datasets):\n        """"""This class takes a list of datasets, each of which is supposed\n        to be already sent to a remote worker (they have a location), and\n        acts like a dictionary based on the worker ids.\n        It serves like an input for the FederatedDataLoader.\n        Args:\n            datasets (list): list of remote Datasets\n        """"""\n        self.datasets = {}\n        for dataset in datasets:\n            worker_id = dataset.data.location.id\n            self.datasets[worker_id] = dataset\n            dataset.federated = True\n\n        # Check that data and targets for a worker are consistent\n        """"""for worker_id in self.workers:\n            dataset = self.datasets[worker_id]\n            assert (\n                dataset.data.shape == dataset.targets.shape\n            ), ""On each worker, the input and target must have the same number of rows."""""" """"\n\n    @property\n    def workers(self):\n        """"""\n        Returns: list of workers\n        """"""\n\n        return list(self.datasets.keys())\n\n    def get_dataset(self, worker):\n        self[worker].federated = False\n        dataset = self[worker].get()\n        del self.datasets[worker]\n        return dataset\n\n    def __getitem__(self, worker_id):\n        """"""\n        Args:\n            worker_id[str,int]: ID of respective worker\n\n        Returns:\n            Get Datasets from the respective worker\n        """"""\n\n        return self.datasets[worker_id]\n\n    def __len__(self):\n\n        return sum(len(dataset) for dataset in self.datasets.values())\n\n    def __repr__(self):\n\n        fmt_str = ""FederatedDataset\\n""\n        fmt_str += f""    Distributed accross: {\', \'.join(str(x) for x in self.workers)}\\n""\n        fmt_str += f""    Number of datapoints: {self.__len__()}\\n""\n        return fmt_str\n'"
syft/frameworks/torch/fl/utils.py,17,"b'import syft as sy\nimport torch\nfrom typing import Dict\nfrom typing import Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef extract_batches_per_worker(federated_train_loader: sy.FederatedDataLoader):\n    """"""Extracts the batches from the federated_train_loader and stores them\n       in a dictionary (keys = data.location).\n\n    Args:\n        federated_train_loader: the connection object we use to send responses.\n                back to the client.\n    """"""\n    logging_interval = 100\n    batches = {}\n    for worker_id in federated_train_loader.workers:\n        worker = federated_train_loader.federated_dataset.datasets[worker_id].location\n        batches[worker] = []\n\n    for batch_idx, (data, target) in enumerate(federated_train_loader):\n        if batch_idx % logging_interval == 0:\n            logger.debug(""Extracted %s batches from federated_train_loader"", batch_idx)\n        batches[data.location].append((data, target))\n\n    return batches\n\n\ndef add_model(dst_model, src_model):\n    """"""Add the parameters of two models.\n\n    Args:\n        dst_model (torch.nn.Module): the model to which the src_model will be added.\n        src_model (torch.nn.Module): the model to be added to dst_model.\n    Returns:\n        torch.nn.Module: the resulting model of the addition.\n\n    """"""\n\n    params1 = src_model.named_parameters()\n    params2 = dst_model.named_parameters()\n    dict_params2 = dict(params2)\n    with torch.no_grad():\n        for name1, param1 in params1:\n            if name1 in dict_params2:\n                dict_params2[name1].set_(param1.data + dict_params2[name1].data)\n    return dst_model\n\n\ndef scale_model(model, scale):\n    """"""Scale the parameters of a model.\n\n    Args:\n        model (torch.nn.Module): the models whose parameters will be scaled.\n        scale (float): the scaling factor.\n    Returns:\n        torch.nn.Module: the module with scaled parameters.\n\n    """"""\n    params = model.named_parameters()\n    dict_params = dict(params)\n    with torch.no_grad():\n        for name, param in dict_params.items():\n            dict_params[name].set_(dict_params[name].data * scale)\n    return model\n\n\ndef federated_avg(models: Dict[Any, torch.nn.Module]) -> torch.nn.Module:\n    """"""Calculate the federated average of a dictionary containing models.\n       The models are extracted from the dictionary\n       via the models.values() command.\n\n    Args:\n        models (Dict[Any, torch.nn.Module]): a dictionary of models\n        for which the federated average is calculated.\n\n    Returns:\n        torch.nn.Module: the module with averaged parameters.\n    """"""\n    nr_models = len(models)\n    model_list = list(models.values())\n    model = type(model_list[0])()\n\n    for i in range(nr_models):\n        model = add_model(model, model_list[i])\n    model = scale_model(model, 1.0 / nr_models)\n    return model\n\n\ndef accuracy(pred_softmax, target):\n    """"""Calculate the accuray of a given prediction.\n\n    This functions assumes pred_softmax to be converted into the final prediction by\n    taking the argmax.\n\n    Args:\n        pred_softmax: array type(float), providing nr_classes values per element in target.\n        target: array type(int), correct classes, taking values in range [0, nr_classes).\n\n    Returns:\n        accuracy: float, fraction of correct predictions.\n\n    """"""\n    nr_elems = len(target)\n    pred = pred_softmax.argmax(dim=1)\n    return (pred.float() == target.view(pred.shape).float()).sum().numpy() / float(nr_elems)\n\n\ndef create_gaussian_mixture_toy_data(nr_samples: int):  # pragma: no cover\n    """""" Create a simple toy data for binary classification\n\n    The data is drawn from two normal distributions\n    target = 1: mu = 2, sigma = 1\n    target = 0: mu = 0, sigma = 1\n    The dataset is balanced with an equal number of positive and negative samples\n\n    Args:\n        nr_samples: number of samples to generate\n\n    Returns:\n        data, targets\n\n\n    """"""\n    sample_dim = 2\n    one_half = int(nr_samples / 2)\n    x1 = torch.randn(one_half, sample_dim, requires_grad=True) - 5\n    x2 = torch.randn(one_half, sample_dim, requires_grad=True) + 5\n    x = torch.cat([x1, x2], dim=0)\n    y1 = torch.zeros(one_half, requires_grad=False).long()\n    y2 = torch.ones(one_half, requires_grad=False).long()\n    y = torch.cat([y1, y2], dim=0)\n    return x, y\n\n\ndef iris_data_partial():\n    """"""\n\n    Returns: 30 samples from the iris data set: https://archive.ics.uci.edu/ml/datasets/iris\n\n    """"""\n    data = [\n        [5.1, 3.5, 1.4, 0.2],\n        [4.9, 3.0, 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5.0, 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5.0, 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n    ]\n\n    targets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n    data += [\n        [7.0, 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4.0, 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1.0],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n    ]\n\n    targets += [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n    data += [\n        [6.3, 3.3, 6.0, 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3.0, 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3.0, 5.8, 2.2],\n        [7.6, 3.0, 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n    ]\n\n    targets += [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n\n    return torch.tensor(data), torch.tensor(targets)\n'"
syft/frameworks/torch/he/__init__.py,0,b''
syft/frameworks/torch/he/paillier.py,0,b'from phe.paillier import generate_paillier_keypair\n\nkeygen = generate_paillier_keypair\n'
syft/frameworks/torch/hook/__init__.py,0,b''
syft/frameworks/torch/hook/hook.py,95,"b'import copy\nfrom functools import wraps\nimport logging\nfrom math import inf\nimport torch\nimport weakref\n\nimport syft\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.hook.hook import FrameworkHook\nfrom syft.generic.frameworks.remote import Remote\nfrom syft.frameworks.torch.tensors.interpreters.autograd import AutogradTensor\nfrom syft.frameworks.torch.tensors.interpreters.native import TorchTensor\nfrom syft.frameworks.torch.tensors.interpreters.hook import HookedTensor\nfrom syft.frameworks.torch.tensors.interpreters.paillier import PaillierTensor\nfrom syft.frameworks.torch.tensors.decorators.logging import LoggingTensor\nfrom syft.frameworks.torch.tensors.interpreters.precision import FixedPrecisionTensor\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\nfrom syft.frameworks.torch.tensors.interpreters.private import PrivateTensor\nfrom syft.execution.placeholder import PlaceHolder\nfrom syft.frameworks.torch.torch_attributes import TorchAttributes\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.generic.abstract.tensor import _apply_args\nfrom syft.workers.base import BaseWorker\nfrom syft.workers.virtual import VirtualWorker\nfrom syft.execution.plan import Plan\n\n\nclass TorchHook(FrameworkHook):\n    """"""A Hook which Overrides Methods on PyTorch Tensors.\n\n    The purpose of this class is to:\n        * extend torch methods to allow for the moving of tensors from one\n        worker to another.\n        * override torch methods to execute commands on one worker that are\n        called on tensors controlled by the local worker.\n\n    This class is typically the first thing you will initialize when using\n    PySyft with PyTorch because it is responsible for augmenting PyTorch with\n    PySyft\'s added functionality (such as remote execution).\n\n    Args:\n        local_worker: An optional BaseWorker instance that lets you provide a\n            local worker as a parameter which TorchHook will assume to be the\n            worker owned by the local machine. If you leave it empty,\n            TorchClient will automatically initialize a\n            :class:`.workers.VirtualWorker` under the assumption you\'re looking\n            to do local experimentation or development.\n        is_client: An optional boolean parameter (default True), indicating\n            whether TorchHook is being initialized as an end-user client.This\n            can impact whether or not variables are deleted when they fall out\n            of scope. If you set this incorrectly on a end user client, Tensors\n            and Variables will never be deleted. If you set this incorrectly on\n            a remote machine (not a client), tensors will not get saved. It\'s\n            really only important if you\'re not initializing the local worker\n            yourself.\n        verbose: An optional boolean parameter (default True) to indicate\n            whether or not to print the operations as they occur.\n        queue_size: An integer optional parameter (default 0) to specify the\n            max length of the list that stores the messages to be sent.\n\n    Example:\n        >>> import torch as th\n        >>> import syft as sy\n        >>> hook = sy.TorchHook(th)\n        Hooking into Torch...\n        Overloading Complete.\n        # constructing a normal torch tensor in pysyft\n        >>> x = th.Tensor([-2,-1,0,1,2,3])\n        >>> x\n        -2\n        -1\n        0\n        1\n        2\n        3\n        [syft.core.frameworks.torch.tensor.FloatTensor of size 6]\n    """"""\n\n    def __init__(\n        self,\n        torch,\n        local_worker: BaseWorker = None,\n        is_client: bool = True,\n        verbose: bool = False,\n        seed=None,\n    ):\n        """"""\n        Initializes the hook.\n\n        Initialize the hook and define all the attributes pertaining to the\n        torch hook in a special TorchAttibute class, that will be added in the\n        syft.torch attributes. Hence, this parameters are now conveyed by the\n        syft module.\n        """"""\n        # Save the provided torch module as an attribute of the hook\n        self.torch = torch\n        self.framework = self.torch\n        if seed is not None:\n            syft.ID_PROVIDER.seed(seed)\n        self.verbose = verbose\n\n        # Save the local worker as an attribute\n        self.local_worker = local_worker\n\n        if hasattr(torch, ""torch_hooked""):\n            logging.warning(""Torch was already hooked... skipping hooking process"")\n            self.local_worker = syft.local_worker\n            return\n        else:\n            torch.torch_hooked = True\n\n        # Add all the torch attributes in the syft.torch attr\n        syft.torch = TorchAttributes(torch, self)\n        syft.framework = syft.torch\n\n        # Hook some torch methods such that tensors could be created directy at workers\n        self._hook_worker_methods()\n\n        if self.local_worker is None:\n            # Every TorchHook instance should have a local worker which is\n            # responsible for interfacing with other workers. The worker\n            # interface is what allows the Torch specific code in TorchHook to\n            # be agnostic to the means by which workers communicate (such as\n            # peer-to-peer, sockets, through local ports, or all within the\n            # same process)\n            self.local_worker = VirtualWorker(\n                hook=self, is_client_worker=is_client, id=""me"", verbose=verbose\n            )\n        else:\n            self.local_worker.hook = self\n\n        self._syft_workers = {self.local_worker}\n\n        self.to_auto_overload = {}\n\n        self.args_hook_for_overloaded_attr = {}\n\n        self._hook_native_tensor(torch.Tensor, TorchTensor)\n\n        # Add all hooked tensor methods to pointer but change behaviour to have the cmd sent\n        self._hook_pointer_tensor_methods(self.torch.Tensor)\n\n        # Add all hooked tensor methods to AdditiveSharingTensor tensor but change behaviour\n        # to all shares (when it makes sense, otherwise the method is overwritten in the\n        # AdditiveSharingTensor class)\n        self._hook_additive_shared_tensor_methods()\n\n        # Add all hooked tensor methods to multi_pointer to change behavior to have the cmd\n        # sent to all child pointers.\n        self._hook_multi_pointer_tensor_methods(self.torch.Tensor)\n\n        # Add all hooked tensor methods to Logging tensor but change behaviour to just forward\n        # the cmd to the next child (behaviour can be changed in the SyftTensor class file)\n        self._hook_syft_tensor_methods(LoggingTensor)\n\n        # Add all hooked tensor methods to Paillier tensor but change behaviour to just forward\n        # the cmd to the next child (behaviour can be changed in the SyftTensor class file)\n        self._hook_syft_tensor_methods(PaillierTensor)\n\n        # Add all hooked tensor methods to FixedPrecisionTensor tensor but change behaviour\n        # to just forward the cmd to the next child (behaviour can be changed in the\n        # SyftTensor class file)\n        self._hook_syft_tensor_methods(FixedPrecisionTensor)\n\n        # Add all hooked tensor methods to AutogradTensor tensor but change behaviour\n        # to just forward the cmd to the next child (behaviour can be changed in the\n        # SyftTensor class file)\n        self._hook_syft_tensor_methods(AutogradTensor)\n\n        # Add all hooked tensor methods to PrivateTensor tensor but change behaviour\n        # to just forward the cmd to the next child (behaviour can be changed in the\n        # SyftTensor class file)\n        self._hook_private_tensor_methods(PrivateTensor)\n\n        # Add all hooked tensor methods to PlaceHolder tensor but change behaviour\n        # to just forward the cmd to the next child (behaviour can be changed in the\n        # SyftTensor class file)\n        self._hook_syft_placeholder_methods(self.torch.Tensor, PlaceHolder)\n\n        # Add all hooked tensor methods to AdditiveSharingTensor tensor but change behaviour\n        # to just forward the cmd to the next child (behaviour can be changed in the\n        # SyftTensor class file)\n        self._hook_syft_tensor_methods(AdditiveSharingTensor)\n\n        # Add all hooked tensor methods to NumpyTensor tensor\n        self._hook_syft_tensor_methods(HookedTensor)\n\n        # Add all built-in \'str\' methods to String\n        self._hook_string_methods(owner=self.local_worker)\n\n        # Add all string methods to StringPointer\n        # This method call should strictly come after the\n        # call to self._hook_string_methods()\n        self._hook_string_pointer_methods()\n\n        # Hook the tensor constructor function\n        self._hook_tensor()\n\n        # Hook the Parameter methods to store tensor chains in parameters\n        self._hook_parameters()\n\n        # Hook torch functions from modules like torch.add OR\n        # torch.nn.functional (containing relu, etc.)\n        self._hook_torch_module()\n\n        # Hook torch.nn (containing Linear and Convolution layers)\n        self._hook_module()\n\n        # Hook torch.optim (containing optim.SGD, Adam, etc)\n        self._hook_optim()\n\n        # Add the local_worker to syft so that it can be found if the hook is\n        # called several times\n        syft.local_worker = self.local_worker\n        syft.hook = self\n\n    def create_shape(cls, shape_dims):\n        return torch.Size(shape_dims)\n\n    def create_wrapper(cls, wrapper_type):\n        # Note this overrides FrameworkHook.create_wrapper, so it must conform to\n        # that classmethod\'s signature\n        if wrapper_type is None or wrapper_type == torch.Tensor:\n            return torch.Tensor()\n        elif isinstance(wrapper_type, torch.dtype):\n            return torch.tensor([], dtype=wrapper_type)\n        else:\n            raise ValueError(\n                ""Wrapper type should be None, torch.Tensor, or a torch.dtype like torch.long""\n            )\n\n    def create_zeros(cls, *shape, dtype=None, **kwargs):\n        return torch.zeros(*shape, dtype=dtype, **kwargs)\n\n    def _hook_native_tensor(self, tensor_type: type, syft_type: type):\n        """"""Adds PySyft Tensor Functionality to the given native tensor type.\n\n        Overloads the given native Torch tensor to add PySyft Tensor\n        Functionality. Overloading involves modifying the tensor type with\n        PySyft\'s added functionality. You may read about what kind of\n        modifications are made in the methods that this method calls.\n\n        Args:\n            tensor_type: The type of tensor being hooked (in this refactor\n                this is only ever torch.Tensor, but in previous versions of\n                PySyft this iterated over all tensor types.\n            syft_type: The abstract type whose methods should all be added to\n                the tensor_type class. In practice this is always TorchTensor.\n                Read more about it there.\n        """"""\n\n        # Overload Torch tensor properties with Syft properties\n        self._hook_properties(tensor_type)\n\n        # Returns a list of methods to be overloaded, stored in the dict to_auto_overload\n        # with tensor_type as a key\n        self.to_auto_overload[tensor_type] = self._which_methods_should_we_auto_overload(\n            tensor_type\n        )\n\n        # [We don\'t rename native methods as torch tensors are not hooked] Rename native functions\n        # #self._rename_native_functions(tensor_type)\n\n        # Overload auto overloaded with Torch methods\n        self._transfer_methods_to_native_tensor(tensor_type, syft_type)\n\n        self._hook_native_methods(tensor_type)\n\n    def __hook_properties(self, tensor_type):\n        super()._hook_properties(tensor_type)\n        tensor_type.native_shape = tensor_type.shape\n\n    def _hook_syft_tensor_methods(self, syft_type: type):\n        tensor_type = self.torch.Tensor\n        super()._hook_syft_tensor_methods(tensor_type, syft_type)\n\n    def _hook_private_tensor_methods(self, syft_type: type):\n        tensor_type = self.torch.Tensor\n        super()._hook_private_tensor_methods(tensor_type, syft_type)\n\n    def _hook_worker_methods(self):\n        class Torch(object):\n            name = ""torch""\n\n            def __init__(self, worker, *args, **kwargs):\n                self.worker = weakref.ref(worker)\n\n        Remote.register_framework(Torch)\n\n        for attr in syft.torch.worker_methods:\n            new_method = self._get_hooked_base_worker_method(attr)\n            setattr(Torch, attr, new_method)\n\n    def _get_hooked_base_worker_method(hook_self, attr):\n        @wraps(attr)\n        def overloaded_attr(self_torch, *args, **kwargs):\n            ptr = hook_self.local_worker.send_command(\n                recipient=self_torch.worker(),\n                cmd_name=f""{\'torch\'}.{attr}"",\n                args_=args,\n                kwargs_=kwargs,\n            )\n\n            return ptr.wrap()\n\n        return overloaded_attr\n\n    def _hook_additive_shared_tensor_methods(self):\n        """"""\n        Add hooked version of all methods of the torch Tensor to the\n        Additive Shared tensor: instead of performing the native tensor\n        method, it will be forwarded to each share when it is relevant\n        """"""\n\n        tensor_type = self.torch.Tensor\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[tensor_type]:\n            if attr not in dir(AdditiveSharingTensor):\n                new_method = self._get_hooked_additive_shared_method(attr)\n                setattr(AdditiveSharingTensor, attr, new_method)\n\n    def _hook_parameters(self):\n        """"""\n        This method overrides the torch Parameter class such that\n        it works correctly with our overridden tensor types. The\n        native torch Parameter class kept deleting all of our\n        attributes on our custom tensors, so we wrote our own.\n        """"""\n\n        # Hook __new__ to handle when non-pure torch tensors are given as data attribute\n\n        def hooked__new__(cls, data=None, requires_grad=True):\n            if data is None:\n                data = torch.Tensor()\n\n            # If data is not a pure torch tensor you need to store the chain in a\n            # specific place otherwise it will get deleted\n            if not isinstance(data, torch.Tensor) or hasattr(data, ""child""):\n                p = torch.Tensor._make_subclass(cls, torch.Tensor(), requires_grad)\n                if isinstance(data, torch.Tensor):  # so it\'s a wrapper: remove it\n                    p.child = data.child\n                else:\n                    p.child = data\n            else:\n                p = torch.Tensor._make_subclass(cls, data, requires_grad)\n\n            return p\n\n        torch.nn.Parameter.__new__ = hooked__new__\n\n        # Hook __repr__ to handle chain repr when needed\n\n        torch.nn.Parameter.native_param___repr__ = torch.nn.Parameter.__repr__\n\n        def hooked__repr__(self):\n            if hasattr(self, ""child""):\n                return ""Parameter containing:\\n"" + self.child.__repr__()\n            else:\n                return self.native_param___repr__()\n\n        # torch.nn.Parameter.__repr__ = hooked__repr__\n\n        def get_data(self):\n            if hasattr(self, ""child""):\n                to_return = self.child.attr(""data"")\n            else:\n                to_return = self.native_data\n\n                # good to ensure that the ID stays consistent\n                # not 100% this is required but it\'s at least\n                # good practice\n                try:\n                    to_return.id = self.data_id\n                except AttributeError:\n                    self.data_id = to_return.id\n\n            return to_return\n\n        def set_data(self, new_data):\n            # If data is not a pure torch tensor you need to store the chain in a\n            # specific place otherwise it will get deleted\n            if not isinstance(new_data, torch.Tensor) or hasattr(new_data, ""child""):\n                self.child = new_data  # .wrap()\n            else:\n                if hasattr(self, ""child""):\n                    del self.child\n\n                with torch.no_grad():\n                    self.native_data = new_data\n            return self\n\n        torch.nn.Parameter.data = property(fget=get_data, fset=set_data)\n\n        # Hook .grad to handle chain assignment when needed\n\n        torch.nn.Parameter.native_param_grad = torch.nn.Parameter.grad\n\n        @property\n        def grad(self):\n\n            if hasattr(self, ""child""):\n                to_return = self.child.attr(""grad"")\n                if to_return is not None and isinstance(to_return.child, PointerTensor):\n                    if to_return.child.is_none():\n                        to_return = None\n\n            else:\n                to_return = self.native_param_grad\n\n                # good to ensure that the ID stays consistent\n                # not 100% this is required but it\'s at least\n                # good practice\n                try:\n                    to_return.id = self.grad_id\n                except AttributeError:\n                    if to_return is not None and hasattr(to_return, ""id""):\n                        self.grad_id = to_return.id\n\n            return to_return\n\n        @grad.setter\n        def grad(self, new_grad):\n\n            # If grad is not a pure torch tensor you need to store the chain in a\n            # specific place otherwise it will get deleted\n            if new_grad is not None and (\n                not isinstance(new_grad, torch.Tensor) or hasattr(new_grad, ""child"")\n            ):\n                self.child.grad = new_grad  # .wrap()\n            else:\n                if self.native_param_grad is not None:\n                    with torch.no_grad():\n                        self.native_param_grad = new_grad\n                elif new_grad is not None:\n                    self.native_param_grad = new_grad\n            return self\n\n        torch.nn.Parameter.grad = grad\n\n    def _hook_torch_module(self):\n        """"""Overloads functions in the main torch modules.\n        The way this is accomplished is by first moving all existing module\n        functions in the torch module to native_<function_name_here>.\n\n        Example:\n            the real :func:`torch.cat` will become :func:`torch.native_cat`\n            and :func:`torch.cat` will have our hooking code.\n        """"""\n        torch_modules = syft.torch.torch_modules\n\n        for module_name, torch_module in torch_modules.items():\n            for func in dir(torch_module):\n\n                # Some functions we want to ignore (not override). Such functions have been hard\n                # coded into the torch_attribute exclude (see TorchAttribute class)\n                if func in syft.torch.exclude:\n                    continue\n\n                # ignore dunder functions\n                if ""__"" in func:\n                    continue\n\n                # ignore capitalized func values which are Classes not functions\n                if func[0].isupper():\n                    continue\n\n                # ignore hidden functins\n                if func[0] == ""_"":\n                    continue\n\n                # If we haven\'t already overloaded this function\n                if ""native_"" in func or f""native_{func}"" in dir(torch_module):\n                    continue\n\n                self._perform_function_overloading(module_name, torch_module, func)\n\n    def _get_hooked_additive_shared_method(hook_self, attr):\n        """"""\n        Hook a method to send it multiple remote workers\n\n        Args:\n            attr (str): the method to hook\n        Return:\n            the hooked method\n        """"""\n\n        def dispatch(args_, k):\n            return map(lambda x: x[k] if isinstance(x, dict) else x, args_)\n\n        @wraps(attr)\n        def overloaded_attr(self, *args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n\n            # Replace all syft tensor with their child attribute\n            new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                attr, self, args, kwargs\n            )\n\n            results = {}\n            for k, v in new_self.items():\n                results[k] = v.__getattribute__(attr)(*dispatch(new_args, k), **new_kwargs)\n\n            # Put back AdditiveSharingTensor on the tensors found in the response\n            response = hook_args.hook_response(\n                attr,\n                results,\n                wrap_type=AdditiveSharingTensor,\n                wrap_args=self.get_class_attributes(),\n            )\n\n            return response\n\n        return overloaded_attr\n\n    def _hook_tensor(hook_self):\n        """"""Hooks the function torch.tensor()\n        We need to do this seperately from hooking the class because internally\n        torch does not pick up the change to add the args\n        Args:\n            hook_self: the hook itself\n        """"""\n\n        if ""native_tensor"" not in dir(hook_self.torch):\n            hook_self.torch.native_tensor = hook_self.torch.tensor\n\n        def new_tensor(*args, owner=None, id=None, register=True, **kwargs):\n            current_tensor = hook_self.torch.native_tensor(*args, **kwargs)\n            _apply_args(hook_self, current_tensor, owner, id)\n            if register:\n                current_tensor.owner.register_obj(current_tensor)\n\n            return current_tensor\n\n        hook_self.torch.tensor = new_tensor\n\n    @classmethod\n    def _transfer_methods_to_native_tensor(cls, tensor_type: type, syft_type: type):\n        """"""Adds methods from the TorchTensor class to the native torch tensor.\n\n        The class TorchTensor is a proxy to avoid extending directly the torch\n        tensor class.\n\n        Args:\n            tensor_type: The tensor type to which we are adding methods\n                from TorchTensor class.\n        """"""\n        exclude = [\n            ""__class__"",\n            ""__delattr__"",\n            ""__dir__"",\n            ""__doc__"",\n            ""__dict__"",\n            ""__format__"",\n            ""__getattribute__"",\n            ""__hash__"",\n            ""__init__"",\n            ""__init_subclass__"",\n            ""__weakref__"",\n            ""__module__"",\n            ""__ne__"",\n            ""__new__"",\n            ""__reduce__"",\n            ""__reduce_ex__"",\n            ""__setattr__"",\n            ""__sizeof__"",\n            ""__subclasshook__"",\n            ""_get_type"",\n            # FIXME it now overwritten in native.py to use torch.eq, because\n            # of pb between == & __eq__ See #2030\n            # ""__eq__"",\n            ""__gt__"",\n            ""__ge__"",\n            ""__lt__"",\n            ""__le__"",\n        ]\n        cls._transfer_methods_to_framework_class(tensor_type, syft_type, exclude)\n\n    def _hook_module(self):\n        """"""Overloading torch.nn.Module with PySyft functionality, the primary module\n        responsible for core ML functionality such as Neural network layers and\n        loss functions.\n\n        It is important to note that all the operations are actually in-place.\n        """"""\n        self.element_iter_dict = {}\n\n        def register_element_iterator(name, func):\n            """"""register an internal element buffer iterator\n            """"""\n            if name in self.element_iter_dict.keys():\n                return\n            self.element_iter_dict[name] = func\n\n        def tensor_iterator(nn_self):\n            """"""adding relavant iterators for the tensor elements""""""\n            iterators = [\n                ""parameters"",\n                ""buffers"",\n            ]  # all the element iterators from nn module should be listed here,\n            return [getattr(nn_self, iter) for iter in iterators]\n\n        def module_is_missing_grad(model):\n            """"""Checks if all the parameters in the model have been assigned a gradient""""""\n            for p in model.parameters():\n                if p.grad is None:\n                    return True\n            return False\n\n        def create_grad_objects(model):\n            """"""Assigns gradient to model parameters if not assigned""""""\n            for p in model.parameters():\n                if p.requires_grad:  # check if the object requires a grad object\n                    o = p.sum()\n                    o.backward()\n                    if p.grad is not None:\n                        p.grad -= p.grad\n\n        def module_send_(nn_self, *dest, force_send=False, **kwargs):\n            """"""Overloads torch.nn instances so that they could be sent to other workers""""""\n\n            if module_is_missing_grad(nn_self):\n                create_grad_objects(nn_self)\n\n            for element_iter in tensor_iterator(nn_self):\n                for p in element_iter():\n                    p.send_(*dest, **kwargs)\n\n            if isinstance(nn_self.forward, Plan):\n                nn_self.forward.send(*dest, force=force_send)\n\n            return nn_self\n\n        self.torch.nn.Module.send = module_send_\n        self.torch.nn.Module.send_ = module_send_\n\n        def module_move_(nn_self, destination):\n\n            params = list(nn_self.parameters())\n            for p in params:\n                p.move_(destination)\n\n        self.torch.nn.Module.move = module_move_\n\n        # def module_end_get_(nn_self):\n        #     """"""Overloads send to remote for torch.nn.Module.""""""\n        #     if module_is_missing_grad(nn_self):\n        #         create_grad_objects(nn_self)\n        #\n        #     for p in nn_self.parameters():\n        #         p.end_get()\n        #\n        #     return nn_self\n        #\n        # self.torch.nn.Module.end_get = module_end_get_\n        #\n        # def module_move_(nn_self, dest):\n        #     return nn_self.send(dest).end_get()\n        #\n        # self.torch.nn.Module.move = module_move_\n\n        def module_get_(nn_self):\n            """"""\n            overloads torch.nn instances with get method so that parameters\n            could be sent back to owner\n            """"""\n            for element_iter in tensor_iterator(nn_self):\n                for p in element_iter():\n                    p.get_()\n\n            if isinstance(nn_self.forward, Plan):\n                nn_self.forward.get()\n\n            return nn_self\n\n        self.torch.nn.Module.get_ = module_get_\n        self.torch.nn.Module.get = module_get_\n\n        def module_share_(nn_self, *args, **kwargs):\n            """"""Overloads fix_precision for torch.nn.Module.""""""\n            if module_is_missing_grad(nn_self):\n                create_grad_objects(nn_self)\n\n            for element_iter in tensor_iterator(nn_self):\n                for p in element_iter():\n                    p.share_(*args, **kwargs)\n\n            return nn_self\n\n        self.torch.nn.Module.share_ = module_share_\n        self.torch.nn.Module.share = module_share_\n\n        def module_fix_precision_(nn_self, *args, **kwargs):\n            """"""Overloads fix_precision for torch.nn.Module.""""""\n            if module_is_missing_grad(nn_self):\n                create_grad_objects(nn_self)\n\n            for element_iter in tensor_iterator(nn_self):\n                for p in element_iter():\n                    p.fix_precision_(*args, **kwargs)\n\n            return nn_self\n\n        self.torch.nn.Module.fix_precision_ = module_fix_precision_\n        self.torch.nn.Module.fix_precision = module_fix_precision_\n        self.torch.nn.Module.fix_prec = module_fix_precision_\n\n        def module_float_precision_(nn_self):\n            """"""Overloads float_precision for torch.nn.Module, convert fix_precision\n            parameters to normal float parameters""""""\n            # TODO: add .data and .grad to syft tensors\n            # if module_is_missing_grad(nn_self):\n            #    create_grad_objects(nn_self)\n\n            for element_iter in tensor_iterator(nn_self):\n                for p in element_iter():\n                    p.float_precision_()\n\n            return nn_self\n\n        self.torch.nn.Module.float_precision_ = module_float_precision_\n        self.torch.nn.Module.float_precision = module_float_precision_\n        self.torch.nn.Module.float_prec = module_float_precision_\n\n        def module_copy(nn_self):\n            """"""Returns a copy of a torch.nn.Module""""""\n            return copy.deepcopy(nn_self)\n\n        self.torch.nn.Module.copy = module_copy\n\n        @property\n        def owner(nn_self):\n            for p in nn_self.parameters():\n                return p.owner\n\n        self.torch.nn.Module.owner = owner\n\n        @property\n        def location(nn_self):\n            try:\n                for p in nn_self.parameters():\n                    return p.location\n            except AttributeError:\n                raise AttributeError(\n                    ""Module has no attribute location, did you already send it to some location?""\n                )\n\n        self.torch.nn.Module.location = location\n\n        # Make sure PySyft uses the PyTorch version\n        self.torch.nn.modules.rnn._rnn_impls[""LSTM""] = self.torch.lstm\n\n        # Add support for GRUs\n        self.torch.nn.modules.rnn._rnn_impls[""GRU""] = self.torch.gru\n\n        # Override _VF.LSTM_Cell and _VF.GRU_Cell with torch.LSTM_Cell and torch.GRU_Cell\n        # With the pytorch-based version\n        self.torch.nn.modules.rnn._VF = self.torch\n\n    def _hook_optim(self):\n        """"""Overloading torch.optim.Optimizer with PySyft functionality. Optimizer\n        hyper-parameters should indeed be converted to fixed precision to interact\n        with fixed precision or additive shared tensors.\n\n        It is important to note that all the operations are actually in-place.\n        """"""\n\n        def optim_fix_precision_(optim_self, *args, **kwargs):\n            """"""Overloads fix_precision for torch.optim.Optimizer""""""\n\n            for param_group in optim_self.param_groups:\n                for key, param in param_group.items():\n                    if isinstance(param, (float, int, bool)) and param != 0 and key != ""params"":\n                        param_group[key] = torch.tensor(param).fix_precision(*args, **kwargs).child\n\n            return optim_self\n\n        self.torch.optim.Optimizer.fix_precision = optim_fix_precision_\n\n        def optim_float_precision_(optim_self):\n            """"""Overloads float_precision for torch.optim.Optimizer, convert fix_precision\n            hyper-parameters to normal float values""""""\n\n            for param_group in optim_self.param_groups:\n                for key, param in param_group.items():\n                    if isinstance(param, FixedPrecisionTensor) and key != ""params"":\n                        param_group[key] = param.float_precision().item()\n\n            return optim_self\n\n        self.torch.optim.Optimizer.float_precision = optim_float_precision_\n\n        # Modification of torch/nn/utils/clip_grad.py. The plain PyTorch method was not compatible\n        # with PySyft remote tensors, so this method adds support for gradient clipping of remote\n        # tensors, and keeps functionalities from PyTorch to clip local PyTorch tensors.\n        def clip_grad_norm_remote_(parameters, max_norm, norm_type=2):\n            """"""Clips gradient norm of an iterable of parameters stored over a remote model\n\n            The norm is computed over all gradients together, as if they were\n            concatenated into a single vector. Gradients are modified in-place.\n\n            Arguments:\n                - parameters (Iterable[Tensor] or Tensor): an iterable of PySyft remote\n                Tensors or PyTorch tensor will have gradients normalized or a single\n                PySyfy / PyTorch tensor.\n                - max_norm (float or int): max norm of the gradients\n                - worker: The worker where the parameters are hosted and where the gradient clipping\n                will be performed\n                - norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\n                    infinity norm.\n\n            Returns:\n                Total norm of the parameters (viewed as a single vector).\n            """"""\n\n            def param_is_pointer_tensor(param):\n                """"""\n                A list of parameters is remote if all params contained in the list are\n                remote (i.e., the child of each param is a pointer tensor).\n                This method checks if a single param is indeed remote, so whether\n                the child of a parameter is a pointer tensor\n                """"""\n                return hasattr(param, ""child"") and isinstance(param.child, PointerTensor)\n\n            if isinstance(parameters, torch.Tensor):\n                parameters = [parameters]\n\n            parameters = list(filter(lambda p: p.grad is not None, parameters))\n            max_norm = float(max_norm)\n            norm_type = float(norm_type)\n            if norm_type == inf:\n                total_norm = max(p.grad.data.abs().max() for p in parameters)\n            else:\n                # all parameters are remote\n                if all(param_is_pointer_tensor(param) for param in parameters):\n                    total_norm = torch.zeros(1)\n                    # Let\'s send the total norm over to the remote where the remote tensor is\n                    total_norm = total_norm.send(parameters[0].location)\n                else:\n                    total_norm = 0\n                for p in parameters:\n                    param_norm = p.grad.data.norm(norm_type)\n                    total_norm += param_norm ** norm_type\n\n                total_norm = total_norm ** (1.0 / norm_type)\n            clip_coef = max_norm / (total_norm + 1e-6)\n            if clip_coef < 1:\n                for p in parameters:\n                    p.grad.data.mul_(clip_coef)\n            return total_norm\n\n        self.torch.nn.utils.clip_grad_norm_ = clip_grad_norm_remote_\n\n    def set_verbose(self, flag):\n        for workers in self._syft_workers:\n            workers.verbose = flag\n'"
syft/frameworks/torch/hook/hook_args.py,15,"b'import torch\n\nfrom syft.frameworks.torch.tensors.interpreters.native import TorchTensor\nfrom syft.generic.frameworks.hook.hook_args import (\n    register_ambiguous_method,\n    register_ambiguous_function,\n    register_backward_func,\n    register_forward_func,\n    register_type_rule,\n    one,\n)\n\nfrom syft.exceptions import PureFrameworkTensorFoundError\n\ntype_rule = {\n    torch.Tensor: one,\n    torch.nn.Parameter: one,\n}\n\nforward_func = {\n    torch.Tensor: lambda i: i.child\n    if hasattr(i, ""child"")\n    else (_ for _ in ()).throw(PureFrameworkTensorFoundError),\n    torch.nn.Parameter: lambda i: i.child\n    if hasattr(i, ""child"")\n    else (_ for _ in ()).throw(PureFrameworkTensorFoundError),\n}\n\nbackward_func = {\n    TorchTensor: lambda i, **kwargs: i.wrap(**kwargs),\n    torch.Tensor: lambda i, **kwargs: i.wrap(**kwargs),\n    torch.nn.Parameter: lambda i, **kwargs: torch.nn.Parameter(data=i),\n}\n\n# Methods or functions whose signature changes a lot and that we don\'t want to ""cache"", because\n# they have an arbitrary number of tensors in args which can trigger unexpected behaviour\nambiguous_methods = {\n    ""__getitem__"",\n    ""__setitem__"",\n    ""_getitem_public"",\n    ""add_"",\n    ""backward"",\n    ""cat"",\n    ""chunk"",\n    ""new"",\n    ""permute"",\n    ""reshape"",\n    ""split"",\n    ""stack"",\n    ""sub_"",\n    ""view"",\n}\n\nambiguous_functions = {\n    ""torch.unbind"",\n    ""unbind"",\n    ""torch.stack"",\n    ""stack"",\n    ""torch.cat"",\n    ""cat"",\n    ""torch.mean"",\n    ""torch.sum"",\n    ""torch.chunk"",\n    ""chunk"",\n    ""torch.functional.split"",\n    ""torch.split"",\n    ""split"",\n    ""backward"",\n}\n\nregister_ambiguous_method(*ambiguous_methods)\nregister_ambiguous_function(*ambiguous_functions)\nregister_type_rule(type_rule)\nregister_forward_func(forward_func)\nregister_backward_func(backward_func)\n'"
syft/frameworks/torch/linalg/__init__.py,4,b'from syft.frameworks.torch.linalg.operations import inv_sym  # noqa: F401\nfrom syft.frameworks.torch.linalg.operations import qr  # noqa: F401\nfrom syft.frameworks.torch.linalg.lr import EncryptedLinearRegression  # noqa: F401\nfrom syft.frameworks.torch.linalg.lr import DASH  # noqa: F401\n'
syft/frameworks/torch/linalg/lr.py,18,"b'import random\nfrom typing import List\n\nimport torch\n\nfrom syft.workers.base import BaseWorker\nfrom syft.frameworks.torch.linalg.operations import inv_sym\nfrom syft.frameworks.torch.linalg.operations import qr\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\n\nfrom syft.exceptions import DependencyError\n\ntry:\n    from scipy.stats import t\nexcept ImportError:\n    raise DependencyError(""scipy"", ""scipy"")\n\n\nclass EncryptedLinearRegression:\n    """"""\n    Multi-Party Linear Regressor based on Jonathan Bloom\'s algorithm.\n    It performs linear regression using Secure Multi-Party Computation.\n    While the training is performed in SMPC, the final regression coefficients\n    are public at the end and predictions are made in clear on local or pointer\n    Tensors.\n\n    Reference: Section 2 of https://arxiv.org/abs/1901.09531\n\n    Args:\n        crypto_provider: a BaseWorker providing crypto elements for\n            AdditiveSharingTensors (used for SMPC) such as Beaver triples\n        hbc_worker: The ""Honest but Curious"" BaseWorker. SMPC operations in PySyft\n            use SecureNN protocols, which are based on 3-party computations. In order to\n            apply it for more than 3 parties, we need a ""Honest but Curious"" worker.\n            To perform the Encrypted Linear Regression, the algorithm chooses randomly one of the\n            workers in the pool and secret share all tensors with the chosen worker,\n            the crypto provider and the ""Honest but Curious"" worker. Its main role\n            is to avoid collusion between two workers in the pool if the algorithm\n            secred shared the tensors with two randomly chosen workers and the\n            crypto provider. The ""Honest but Curious"" worker is essentially a\n            legitimate participant in a communication protocol who will not deviate\n            from the defined protocol but will attempt to learn all possible\n            information from legitimately received messages.\n        precision_fractional: precision chosen for FixedPrecisionTensors\n        fit_intercept:  whether to calculate the intercept for this model. If set\n            to False, no intercept will be used in calculations (e.g. data is\n            expected to be already centered)\n\n    Attributes:\n        coef: torch.Tensor of shape (n_features, ). Estimated coefficients for\n            the linear regression problem.\n        intercept: torch.Tensor of shape (1, ) if fit_intercept is set to True,\n            None otherwise. Estimated intercept for the linear regression.\n        pvalue_coef: numpy.array of shape (n_features, ). Two-sided p-value for a\n            hypothesis test whose null hypothesis is that the each coeff is zero.\n        pvalue_intercept: numpy.array of shape (1, ) if fit_intercept is set to True,\n            None otherwise. Two-sided p-value for a hypothesis test whose null\n            hypothesis is that the intercept is zero.\n\n    """"""\n\n    def __init__(\n        self,\n        crypto_provider: BaseWorker,\n        hbc_worker: BaseWorker,\n        precision_fractional: int = 6,\n        fit_intercept: bool = True,\n    ):\n\n        self.crypto_provider = crypto_provider\n        self.hbc_worker = hbc_worker\n        self.precision_fractional = precision_fractional\n        self.fit_intercept = fit_intercept\n\n    def fit(self, X_ptrs: List[torch.Tensor], y_ptrs: List[torch.Tensor]):\n        """"""\n        Fits the linear model using Secured Multi-Party Linear Regression.\n        The final results (i.e. coefficients and p-values) will be public.\n        """"""\n\n        # Checking if the pointers are as expected\n        self._check_ptrs(X_ptrs, y_ptrs)\n\n        # Check if each y is a 2-dim or 1-dim tensor, unsqueeze it if it\'s 1-dim\n        for i, y in enumerate(y_ptrs):\n            if len(y.shape) < 2:\n                y_ptrs[i] = y.unsqueeze(1)\n\n        if self.fit_intercept:\n            X_ptrs = self._add_intercept(X_ptrs)\n            self._dgf -= 1\n\n        self.workers = self._get_workers(X_ptrs)\n\n        # Computing aggregated pairwise dot products remotelly\n        XX_ptrs, Xy_ptrs, yy_ptrs = self._remote_dot_products(X_ptrs, y_ptrs)\n\n        # Secred share tensors between hbc_worker, crypto_provider and a random worker\n        # and compute aggregates. It corresponds to the Combine stage of Bloom\'s algorithm\n        idx = random.randint(0, len(self.workers) - 1)\n        XX_shared = sum(self._share_ptrs(XX_ptrs, idx))\n        Xy_shared = sum(self._share_ptrs(Xy_ptrs, idx))\n        yy_shared = sum(self._share_ptrs(yy_ptrs, idx))\n\n        ##################### Compute inverse of Gram Matrix ###########################\n        # We need to normalize it by dividing the Gram matrix by the total sample size\n        # because matrix inversion in MPC is not precise for large numbers, which is the\n        # case of the Gram matrix when total_size is large. We only resize back the\n        # values we are interested in (i.e. the coefficients and std errors) locally at\n        # the end in order to make sure the subsequent computations are still precise\n\n        XX_shared = XX_shared / self.total_size\n        XX_inv_shared = inv_sym(XX_shared)\n\n        # Compute shared coefficients\n        coef_shared = XX_inv_shared @ Xy_shared\n\n        sigma2_shared = yy_shared * self.total_size - coef_shared.t() @ XX_shared @ coef_shared\n        sigma2_shared = sigma2_shared / self._dgf\n\n        var_diag_shared = torch.diag(XX_inv_shared) * sigma2_shared\n\n        # Store results locally and resize by dividing by total_size\n        self.coef = coef_shared.get().float_precision() / self.total_size\n        self.coef = self.coef.squeeze()\n        self.se_coef = torch.sqrt(var_diag_shared.get().float_precision()) / self.total_size\n        self.se_coef = self.se_coef.squeeze()\n\n        self.sigma = torch.sqrt(sigma2_shared.get().float_precision().squeeze() / self.total_size)\n\n        if self.fit_intercept:\n            self.intercept = self.coef[0]\n            self.coef = self.coef[1:]\n            self.se_intercept = self.se_coef[0]\n            self.se_coef = self.se_coef[1:]\n        else:\n            self.intercept = None\n            self.se_intercept = None\n\n        self._compute_pvalues()\n\n        return self\n\n    def predict(self, X: torch.Tensor):\n        """"""\n        Performs predicion of linear model on X, which can be a local torch.Tensor\n        or a wrapped PointerTensor. The result will be either a local torch.Tensor\n        or a wrapped PointerTensor, depending on the nature of X.\n        """"""\n        coef = self.coef.copy()\n        intercept = self.intercept.copy() if self.fit_intercept else None\n\n        # Send coef and intercept to remote worker if X is a pointer\n        if X.has_child() and isinstance(X.child, PointerTensor):\n            coef = coef.send(X.child.location)\n            if self.fit_intercept:\n                intercept = intercept.send(X.child.location)\n\n        y = X @ coef.unsqueeze(1)\n        if self.fit_intercept:\n            y += intercept\n        return y.squeeze()\n\n    def summarize(self):\n        """"""\n        Prints a summary of the coefficients and its statistics.\n        This method should be called only after training of the model.\n        """"""\n        print(""="" * 52)\n        print("" "" * 11 + ""SMPC Linear Regression Results"")\n        print(""="" * 52)\n        print("" "" * 17 + ""value"" + "" "" * 9 + ""stderr"" + "" "" * 8 + ""p-value"")\n        print(""-"" * 52)\n        for i, cf in enumerate(self.coef):\n            print(\n                ""coef"" + f""{i + 1:<3d}"",\n                f""{cf:>14.4f}"",\n                f""{self.se_coef[i]:>14.4f}"",\n                f""{self.pvalue_coef[i]:>14.4f}"",\n            )\n        if self.fit_intercept:\n            print(\n                ""intercept"",\n                f""{self.intercept:>12.4f}"",\n                f""{self.se_intercept:>14.4f}"",\n                f""{self.pvalue_intercept:>14.4f}"",\n            )\n        print(""-"" * 52)\n\n    def _check_ptrs(self, X_ptrs, y_ptrs):\n        """"""\n        Method that check if the lists of pointers corresponding to the explanatory and\n        explained variables have their elements as expected.\n        It also computes parallelly some Regressor\'s attributes such as number of features and\n        total sample size.\n        """"""\n        # Set number of features\n        self.n_features = X_ptrs[0].shape[1]\n\n        x_size, y_size = 0, 0\n        for x, y in zip(X_ptrs, y_ptrs):\n            # Check wrapper\n            if not (x.has_child() and y.has_child()):\n                raise TypeError(\n                    ""Some tensors are not wrapped, please provide a wrapped Pointer Tensor""\n                )\n\n            # Check if x and y are pointers\n            if not (isinstance(x.child, PointerTensor) and isinstance(y.child, PointerTensor)):\n                raise TypeError(\n                    ""Some tensors are not pointers, please provided a wrapped Pointer Tensor""\n                )\n\n            # Check if both are in the same worker\n            if not x.child.location == y.child.location:\n                raise RuntimeError(""Some pairs (X, y) are not located in the same worker"")\n\n            # Check if they have the same size\n            x_size += x.shape[0]\n            y_size += y.shape[0]\n            if x_size != y_size:\n                raise ValueError(""Some pairs (X, y) do not have the same number of samples"")\n\n            # Check if all tensors have the same number of features\n            if x.shape[1] != self.n_features:\n                raise ValueError(""Tensors do not have the same number of features"")\n\n        # Set total size\n        self.total_size = x_size\n\n        # Set degrees of freedom\n        self._dgf = self.total_size - self.n_features\n\n    @staticmethod\n    def _add_intercept(X_ptrs):\n        """"""\n        Adds a column-vector of 1\'s at the beginning of the tensors X_ptrs\n        """"""\n        X_ptrs_new = []\n        for i, x in enumerate(X_ptrs):\n            ones = torch.ones_like(x[:, :1])\n            x = torch.cat((ones, x), 1)\n            X_ptrs_new.append(x)\n        return X_ptrs_new\n\n    @staticmethod\n    def _get_workers(ptrs):\n        """"""\n        Method that returns the pool of workers in a tuple\n        """"""\n        workers = set()\n        for ptr in ptrs:\n            workers.add(ptr.child.location)\n        return tuple(workers)\n\n    @staticmethod\n    def _remote_dot_products(X_ptrs, y_ptrs):\n        """"""\n        This method computes the aggregated dot-products remotely. It corresponds\n        to the Compression stage (or Compression within) of Bloom\'s algorithm\n        """"""\n        XX_ptrs = []\n        Xy_ptrs = []\n        yy_ptrs = []\n        for x, y in zip(X_ptrs, y_ptrs):\n            XX_ptrs.append(x.t() @ x)\n            Xy_ptrs.append(x.t() @ y)\n            yy_ptrs.append(y.t() @ y)\n\n        return XX_ptrs, Xy_ptrs, yy_ptrs\n\n    def _share_ptrs(self, ptrs, worker_idx):\n        """"""\n        Method that secret share a list of remote tensors between a worker of\n        the pool and the \'honest but curious\' worker, using a crypto_provider worker\n        """"""\n        shared_tensors = []\n        for ptr in ptrs:\n            fpt_tensor = ptr.fix_precision(precision_fractional=self.precision_fractional)\n            shared_tensor = fpt_tensor.share(\n                self.workers[worker_idx], self.hbc_worker, crypto_provider=self.crypto_provider\n            ).get()\n            shared_tensors.append(shared_tensor)\n        return shared_tensors\n\n    def _compute_pvalues(self):\n        """"""\n        Compute p-values of coefficients (and intercept if fit_intercept==True)\n        """"""\n        tstat_coef = self.coef / self.se_coef\n        self.pvalue_coef = 2 * t.cdf(-abs(tstat_coef), self._dgf)\n\n        if self.fit_intercept:\n            tstat_intercept = self.intercept / self.se_intercept\n            self.pvalue_intercept = 2 * t.cdf(-abs(tstat_intercept), self._dgf)\n        else:\n            self.pvalue_intercept = None\n\n\nclass DASH:\n    """"""\n    Distributed Association Scan Hammer (DASH) algorithm based on Jonathan Bloom\'s algorithm.\n    It uses Secured Multi-Party Computation at combine phase.\n    While the training is performed in SMPC, the final regression coefficients\n    are public at the end.\n\n    Reference: Section 2 of https://arxiv.org/abs/1901.09531\n\n    Args:\n        crypto_provider: a BaseWorker providing crypto elements for ASTs such as\n            Beaver triples\n        hbc_worker: The ""Honest but Curious"" BaseWorker. SMPC operations in PySyft\n            use SecureNN protocols, which are based on 3-party computations. In order to\n            apply it for more than 3 parties, we need a ""Honest but Curious"" worker.\n            To perform the DASH algorithm, we choose randomly one of the workers\n            in the pool and secret share all tensors with the chosen worker,the crypto\n            provider and the ""Honest but Curious"" worker. Its main role is to avoid\n            collusion between two workers in the pool if the algorithm secred shared\n            the tensors with two randomly chosen workers and the crypto provider.\n            The ""Honest but Curious"" worker is essentially a legitimate participant\n            in a communication protocol who will not deviate from the defined protocol\n            but will attempt to learn all possible information from legitimately\n            received messages.\n        precision_fractional: precision chosen for FixedPrecisionTensors\n\n    Attributes:\n        coef: torch.Tensor of shape (n_features, ). Estimated coefficients for\n            DASH algorithm.\n        pvalue: numpy.array of shape (n_features, ). Two-sided p-value for a\n            hypothesis test whose null hypothesis is that the each coeff is zero.\n\n    """"""\n\n    def __init__(\n        self, crypto_provider: BaseWorker, hbc_worker: BaseWorker, precision_fractional: int = 6\n    ):\n\n        self.crypto_provider = crypto_provider\n        self.hbc_worker = hbc_worker\n        self.precision_fractional = precision_fractional\n\n    def fit(\n        self, X_ptrs: List[torch.Tensor], C_ptrs: List[torch.Tensor], y_ptrs: List[torch.Tensor]\n    ):\n\n        # Checking if the pointers are as expected\n        self._check_ptrs(X_ptrs, C_ptrs, y_ptrs)\n\n        # Check if each y is a 2-dim or 1-dim tensor, unsqueeze it if it\'s 1-dim\n        for i, y in enumerate(y_ptrs):\n            if len(y.shape) < 2:\n                y_ptrs[i] = y.unsqueeze(1)\n\n        self.workers = self._get_workers(X_ptrs)\n\n        # Computing aggregated pairwise dot products remotely\n        XX_ptrs, Xy_ptrs, yy_ptrs, CX_ptrs, Cy_ptrs = self._remote_dot_products(\n            X_ptrs, C_ptrs, y_ptrs\n        )\n\n        # Compute remote QR decompositions\n        R_ptrs = self._remote_qr(C_ptrs)\n\n        # Secred share tensors between hbc_worker, crypto_provider and a random worker\n        # and compute aggregates. It corresponds to the Combine stage of DASH\'s algorithm\n        idx = random.randint(0, len(self.workers) - 1)\n        XX_shared = sum(self._share_ptrs(XX_ptrs, idx))\n        Xy_shared = sum(self._share_ptrs(Xy_ptrs, idx))\n        yy_shared = sum(self._share_ptrs(yy_ptrs, idx))\n        CX_shared = sum(self._share_ptrs(CX_ptrs, idx))\n        Cy_shared = sum(self._share_ptrs(Cy_ptrs, idx))\n        R_cat_shared = torch.cat(self._share_ptrs(R_ptrs, idx), dim=0)\n\n        # QR decomposition of R_cat_shared\n        _, R_shared = qr(R_cat_shared, norm_factor=self.total_size ** (1 / 2))\n\n        # Compute inverse of upper matrix\n        R_shared_inv = self._inv_upper(R_shared)\n\n        Qy = R_shared_inv.t() @ Cy_shared\n        QX = R_shared_inv.t() @ CX_shared\n\n        denominator = XX_shared - (QX ** 2).sum(dim=0)\n        # Need the line below to perform inverse of a number in MPC\n        inv_denominator = ((0 * denominator + 1) / denominator).squeeze()\n\n        coef_shared = (Xy_shared - QX.t() @ Qy).squeeze() * inv_denominator\n\n        sigma2_shared = (\n            (yy_shared - Qy.t() @ Qy).squeeze() * inv_denominator - coef_shared ** 2\n        ) / self._dgf\n\n        self.coef = coef_shared.get().float_precision()\n        self.sigma2 = sigma2_shared.get().float_precision()\n        self.se = self.sigma2 ** (1 / 2)\n\n        self._compute_pvalues()\n\n    def get_coeff(self):\n        return self.coef\n\n    def get_standard_errors(self):\n        return self.se\n\n    def get_p_values(self):\n        return self.pvalue\n\n    def _check_ptrs(self, X_ptrs, C_ptrs, y_ptrs):\n        """"""\n        Method that check if the lists of pointers corresponding to the response vector,\n        transient covariate vectors and independent permanent covariate vectors have\n        their elements as expected. It also computes parallelly some Regressor\'s\n        attributes such as degrees of freedom and total sample size.\n        """"""\n        # Set number of features\n        self.n_features = X_ptrs[0].shape[1]\n        # Set number of permanent covariate features\n        self.n_permanent = C_ptrs[0].shape[1]\n\n        x_size, c_size, y_size = 0, 0, 0\n        for x, c, y in zip(X_ptrs, C_ptrs, y_ptrs):\n            # Check wrappers and if x, c and y are pointers\n            if not all(\n                map(lambda t: t.has_child() and isinstance(t.child, PointerTensor), (x, c, y))\n            ):\n                raise TypeError(\n                    ""Some tensors are not pointers or are not wrapped, please provided a wrapped ""\n                    ""Pointer Tensor""\n                )\n\n            # Check if both are in the same worker\n            if not (x.child.location == c.child.location and x.child.location == y.child.location):\n                raise RuntimeError(""Some tuples (X, C, y) are not located in the same worker"")\n\n            # Check if they have the same size\n            x_size += x.shape[0]\n            c_size += c.shape[0]\n            y_size += y.shape[0]\n            if x_size != c_size or x_size != y_size:\n                raise ValueError(""Some tuples (X, C, y) do not have the same number of samples"")\n\n            # Check if all tensors have the same number of features\n            if x.shape[1] != self.n_features:\n                raise ValueError(\n                    ""Transient covariate vectors do not have the same number of features""\n                )\n\n            if c.shape[1] != self.n_permanent:\n                raise ValueError(\n                    ""Permanent covariate vectors do not have the same number of features""\n                )\n\n        # Set total size\n        self.total_size = x_size\n\n        # Set degrees of freedom\n        self._dgf = self.total_size - self.n_permanent - 1\n\n    @staticmethod\n    def _get_workers(ptrs):\n        """"""\n        Method that returns the pool of workers in a tuple\n        """"""\n        workers = set()\n        for ptr in ptrs:\n            workers.add(ptr.child.location)\n        return tuple(workers)\n\n    @staticmethod\n    def _remote_dot_products(X_ptrs, C_ptrs, y_ptrs):\n        """"""\n        This method computes the aggregated dot-products remotely. It corresponds\n        to the Compression stage (or Compression within) of DASH algorithm\n        """"""\n        XX_ptrs = []\n        Xy_ptrs = []\n        yy_ptrs = []\n        CX_ptrs = []\n        Cy_ptrs = []\n        for x, c, y in zip(X_ptrs, C_ptrs, y_ptrs):\n            XX_ptrs.append((x.t() @ x).sum(dim=0))\n            Xy_ptrs.append(x.t() @ y)\n            yy_ptrs.append(y.t() @ y)\n            CX_ptrs.append(c.t() @ x)\n            Cy_ptrs.append(c.t() @ y)\n\n        return XX_ptrs, Xy_ptrs, yy_ptrs, CX_ptrs, Cy_ptrs\n\n    @staticmethod\n    def _remote_qr(C_ptrs):\n        """"""\n        Performs the QR decompositions of permanent covariate matrices remotely.\n        It returns a list with the upper right matrices located in each worker\n        """"""\n        R_ptrs = []\n        for c in C_ptrs:\n            _, r = qr(c)\n            R_ptrs.append(r)\n        return R_ptrs\n\n    @staticmethod\n    def _inv_upper(R):\n        """"""\n        Performs the inversion of a right upper matrix (2-dim tensor) in MPC by\n        solving the linear equation R * R_inv = I with backward substitution.\n        """"""\n        R_inv = torch.zeros_like(R)\n        N = R.shape[0]\n\n        # Identity Matrix\n        I = torch.zeros_like(R)\n        for i in range(N):\n            I[i, i] += 1\n\n        for i in range(N - 1, -1, -1):\n            if i == N - 1:\n                R_inv[i, :] = I[i, :] / R[i, i]\n            else:\n                R_inv[i, :] = I[i, :] - (R[i : i + 1, (i + 1) : N].t() * R_inv[(i + 1) : N, :]).sum(\n                    dim=0\n                )\n                R_inv[i, :] = R_inv[i, :] / R[i, i]\n\n        return R_inv\n\n    def _share_ptrs(self, ptrs, worker_idx):\n        """"""\n        Method that secret share a list of remote tensors between a worker of\n        the pool and the \'honest but curious\' worker, using a crypto_provider worker\n        """"""\n        shared_tensors = []\n        for ptr in ptrs:\n            fpt_tensor = ptr.fix_precision(precision_fractional=self.precision_fractional)\n            shared_tensor = fpt_tensor.share(\n                self.workers[worker_idx], self.hbc_worker, crypto_provider=self.crypto_provider\n            ).get()\n            shared_tensors.append(shared_tensor)\n        return shared_tensors\n\n    def _compute_pvalues(self):\n        """"""\n        Compute p-values of coefficients\n        """"""\n        tstat = self.coef / self.se\n        self.pvalue = 2 * t.cdf(-abs(tstat), self._dgf)\n'"
syft/frameworks/torch/linalg/operations.py,16,"b'import torch\n\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\n\n\ndef inv_sym(t):\n    """"""\n    This function performs the inversion of a symmetric matrix (2-dim tensor) in MPC.\n    It uses LDLt decomposition, which is better than Cholensky decomposition in our case\n    since it doesn\'t use square root.\n    Algorithm reference: https://arxiv.org/abs/1111.4144 - Section IV\n\n    Args:\n        t: symmetric 2-dim tensor\n\n    Returns:\n        t_inv: inverse of t as 2-dim tensor\n\n    """"""\n\n    n = t.shape[0]\n    l, d, inv_d = _ldl(t)\n    l_t = l.t()\n    t_inv = torch.zeros_like(t)\n    for j in range(n - 1, -1, -1):\n        for i in range(j, -1, -1):\n            if i == j:\n                t_inv[i, j] = inv_d[i] - (l_t[i, i + 1 : n] * t_inv[i + 1 : n, j]).sum()\n            else:\n                t_inv[j, i] = t_inv[i, j] = -(l_t[i, i + 1 : n] * t_inv[i + 1 : n, j]).sum()\n\n    return t_inv\n\n\ndef _ldl(t):\n    """"""\n    This function performs the LDLt decomposition of a symmetric matrix (2-dim tensor)\n\n    Args:\n        t: symmetric 2-dim tensor\n\n    Returns:\n        l: lower triangular matrix as a 2-dim tensor with same type as t\n        d: 1-dim tensor which represents the diagonal in the LDLt decomposition\n        inv_d: 1-dim tensor which represents the inverse of the diagonal d. It is usefull\n               when computing inverse of a symmetric matrix, by caching it we avoid repeated\n               computations with division, which is very slow in MPC\n    """"""\n    n = t.shape[0]\n    l = torch.zeros_like(t)\n    d = torch.diag(l).copy()\n    inv_d = d.copy()\n\n    for i in range(n):\n        d[i] = t[i, i] - (l[i, :i] ** 2 * d[:i]).sum()\n        inv_d[i] = (0 * d[i] + 1) / d[i]  # Needed to compute inv of a number in MPC\n        for j in range(i, n):\n            # The diagonal of L in LDLt decomposition is 1\n            if j == i:\n                l[j, i] += 1\n            else:\n                l[j, i] = (t[j, i] - (l[j, :i] * l[i, :i] * d[:i]).sum()) * inv_d[i]\n\n    return l, d, inv_d\n\n\ndef qr(t, mode=""reduced"", norm_factor=None):  # noqa: C901\n    """"""\n    This function performs the QR decomposition of a matrix (2-dim tensor). The\n    decomposition is performed using Householder Reflection.\n\n    Args:\n        t: 2-dim tensor, shape(M, N). It should be whether a local tensor, a\n            pointer to a remote tensor or an AdditiveSharedTensor\n\n        mode: {\'reduced\', \'complete\', \'r\'}. If K = min(M, N), then\n            - \'reduced\' : returns q, r with dimensions (M, K), (K, N) (default)\n            - \'complete\' : returns q, r with dimensions (M, M), (M, N)\n            - \'r\' : returns r only with dimensions (K, N)\n\n        norm_factor: float. The normalization factor used to avoid overflow when\n            performing QR decomposition on an AdditiveSharedTensor. For example in\n            the case of the DASH algorithm, this norm_factor should be of the\n            order of the square root of number of entries in the original matrix\n            used to perform the compression phase assuming the entries are standardized.\n\n    Returns:\n        q: orthogonal matrix as a 2-dim tensor with same type as t\n        r: lower triangular matrix as a 2-dim tensor with same type as t\n    """"""\n\n    # Check if t is a PointerTensor or an AST\n    if t.has_child() and isinstance(t.child, PointerTensor):\n        t_type = ""pointer""\n        location = t.child.location\n    elif t.has_child() and isinstance(t.child.child, AdditiveSharingTensor):\n        t_type = ""ast""\n        if norm_factor is None:\n            raise ValueError(\n                ""You are trying to perform QR decompostion on an "",\n                ""AdditiveSharingTensor, please provide a value for the "",\n                ""norm_factor argument."",\n            )\n        workers = t.child.child.locations\n        crypto_prov = t.child.child.crypto_provider\n        prec_frac = t.child.precision_fractional\n    elif isinstance(t, torch.Tensor):\n        t_type = ""local""\n    else:\n        raise TypeError(\n            ""The provided matrix should be a local torch.Tensor, a PointerTensor, "",\n            ""or an AdditiveSharedTensor"",\n        )\n\n    # Check if t is 2-dim\n    assert len(t.shape) == 2\n\n    if mode not in [""reduced"", ""complete"", ""r""]:\n        raise ValueError(\n            ""mode should have one of the values in the list:"" + str([""reduced"", ""complete"", ""r""])\n        )\n\n    ######## QR decomposition via Householder Reflection #########\n    n_rows, n_cols = t.shape\n\n    # Initiate R matrix from t\n    R = t.copy()\n\n    # Initiate identity matrix with size (n_rows, n_rows)\n    I = torch.diag(torch.Tensor([1.0] * n_rows))\n\n    # Send it to remote worker if t is pointer, secret share it if it\'s an AST\n    if t_type == ""pointer"":\n        I = I.send(location)\n    if t_type == ""ast"":\n        I = I.fix_prec(precision_fractional=prec_frac).share(*workers, crypto_provider=crypto_prov)\n\n    if not mode == ""r"":\n        # Initiate Q_transpose\n        Q_t = I.copy()\n\n    # Iteration via Household Reflection\n    for i in range(min(n_rows, n_cols)):\n\n        # Identity for this iteration, it has size (n_cols-i, n_cols-i)\n        I_i = I[i:, i:]\n\n        # Init 1st vector of the canonical base in the same worker as t\n        e = torch.zeros_like(t)[i:, 0].view(-1, 1)\n        e[0, 0] += 1\n\n        # Current vector in R to perform reflection\n        x = R[i:, i].view(-1, 1)\n\n        # Compute norm in MPC if it\'s an AST\n        x_norm = _norm_mpc(x, norm_factor) if t_type == ""ast"" else torch.sqrt(x.t() @ x).squeeze()\n\n        # Compute Householder transform\n        numerator = x @ x.t() - x_norm * (e @ x.t() + x @ e.t()) + (x.t() @ x) * (e @ e.t())\n        denominator = x.t() @ x - x_norm * x[0, 0]\n        # Need the line below to perform inverse of a number in MPC\n        inv_denominator = (0 * denominator + 1) / denominator\n        H = I_i - numerator * inv_denominator\n\n        # If it is not the 1st iteration\n        # expand matrix H with Identity at diagonal and zero elsewhere\n        if i > 0:\n            down_zeros = torch.zeros([n_rows - i, i])\n            up_zeros = torch.zeros([i, n_rows - i])\n\n            # Send them to remote worker if t is pointer, secret share it if its an AST\n            if t_type == ""pointer"":\n                down_zeros = down_zeros.send(location)\n                up_zeros = up_zeros.send(location)\n            if t_type == ""ast"":\n                down_zeros = down_zeros.fix_prec(precision_fractional=prec_frac).share(\n                    *workers, crypto_provider=crypto_prov\n                )\n                up_zeros = up_zeros.fix_prec(precision_fractional=prec_frac).share(\n                    *workers, crypto_provider=crypto_prov\n                )\n            left_cat = torch.cat((I[:i, :i], down_zeros), dim=0)\n            right_cat = torch.cat((up_zeros, H), dim=0)\n            H = torch.cat((left_cat, right_cat), dim=1)\n\n        # Update R\n        R = H @ R\n        if not mode == ""r"":\n            # Update Q_transpose\n            Q_t = H @ Q_t\n\n    if mode == ""reduced"":\n        R = R[:n_cols, :]\n        Q_t = Q_t[:n_cols, :]\n\n    if mode == ""r"":\n        R = R[:n_cols, :]\n        return R\n\n    return Q_t.t(), R\n\n\ndef _norm_mpc(t, norm_factor):\n    """"""\n    Computation of a norm of a vector in MPC. The vector should be an AdditiveSharedTensor.\n\n    It performs the norm calculation by masking the tensor with a multiplication\n    by a big random number drawn from a uniform distribution, and computing the\n    square root of the squared norm of the masked tensor, which is computed\n    beforehand with a dot product in MPC.\n\n    In order to maintain stability and avoid overflow, this functions uses a\n    norm_factor that scales down the tensor for MPC computations and rescale it at the end.\n    For example in the case of the DASH algorithm, this norm_factor should be of\n    the order of the square root of number of entries in the original matrix\n    used to perform the compression phase assuming the entries are standardized.\n\n    Args:\n        t: 1-dim AdditiveSharedTensor, representing a vector.\n        norm_factor: float. The normalization factor used to avoid overflow\n\n    Returns:\n        the norm of the vector as an AdditiveSharedTensor\n\n    """"""\n\n    workers = t.child.child.locations\n    crypto_prov = t.child.child.crypto_provider\n    prec_frac = t.child.precision_fractional\n    field = t.child.child.field\n    norm_factor = int(norm_factor)\n    t_normalized = t / norm_factor\n    Q = int(field ** (1 / 2) / 10 ** (prec_frac / 2))\n\n    norm_sq = (t_normalized ** 2).sum().squeeze()\n\n    # Random big number\n    r = (\n        torch.LongTensor([0])\n        .fix_precision(precision_fractional=prec_frac)\n        .share(*workers, crypto_provider=crypto_prov)\n        .random_(0, Q)\n    )\n\n    # Compute masked norm\n    masked_norm_sq = r ** 2 * norm_sq\n\n    # Get compute square root\n    masked_norm_sq = masked_norm_sq.send(crypto_prov).remote_get().float_precision()\n    masked_norm = torch.sqrt(masked_norm_sq)\n\n    # Secret share and compute unmasked norm in MPC\n    masked_norm = (\n        masked_norm.fix_precision(precision_fractional=prec_frac)\n        .share(*workers, crypto_provider=crypto_prov)\n        .get()\n    )\n    norm = masked_norm / r * norm_factor\n\n    return norm.squeeze()\n'"
syft/frameworks/torch/mpc/__init__.py,0,"b'protocol_store = {}\n\n\ndef crypto_protocol(protocol_name):\n    """"""\n    Decorator to define a specific operation behaviour depending on the crypto\n    protocol used\n\n    Args:\n        protocol_name: the name of the protocol. Currently supported:\n            - snn: SecureNN\n            - fss: Function Secret Sharing\n\n    Example in a tensor file:\n        ```\n        @crypto_protocol(""snn"")\n        def foo(...):\n            # SNN specific code\n\n        @crypto_protocol(""fss"")\n        def foo(...):\n            # FSS specific code\n        ```\n\n        See additive_sharing.py for more usage\n    """"""\n\n    def decorator(f):\n        name = f.__qualname__\n        protocol_store[(name, protocol_name)] = f\n\n        def method(self, *args, **kwargs):\n            f = protocol_store[(name, self.protocol)]\n            return f(self, *args, **kwargs)\n\n        return method\n\n    return decorator\n'"
syft/frameworks/torch/mpc/beaver.py,5,"b'from typing import Callable\nimport torch\n\nfrom syft.workers.abstract import AbstractWorker\n\n\ndef request_triple(\n    crypto_provider: AbstractWorker,\n    cmd: Callable,\n    field: int,\n    dtype: str,\n    a_size: tuple,\n    b_size: tuple,\n    locations: list,\n):\n    """"""Generates a multiplication triple and sends it to all locations.\n\n    Args:\n        crypto_provider: worker you would like to request the triple from\n        cmd: An equation in einsum notation.\n        field: An integer representing the field size.\n        dtype: represents the dtype of shares\n        a_size: A tuple which is the size that a should be or\n                a torch.Size instance\n        b_size: A tuple which is the size that b should be or\n                a torch.Size intance\n        locations: A list of workers where the triple should be shared between.\n\n    Returns:\n        A triple of AdditiveSharedTensors such that c_shared = cmd(a_shared, b_shared).\n    """"""\n    a = crypto_provider.remote.torch.randint(-(field // 2), (field - 1) // 2, a_size)\n    b = crypto_provider.remote.torch.randint(-(field // 2), (field - 1) // 2, b_size)\n    c = cmd(a, b)\n\n    res = torch.cat((a.view(-1), b.view(-1), c.view(-1)))\n\n    shares = (\n        res.share(*locations, field=field, dtype=dtype, crypto_provider=crypto_provider).get().child\n    )\n    a_shared = shares[: a.numel()].reshape(a_size)\n    b_shared = shares[a.numel() : -c.numel()].reshape(b_size)\n    c_shared = shares[-c.numel() :].reshape(c.shape)\n\n    return a_shared, b_shared, c_shared\n'"
syft/frameworks/torch/mpc/fss.py,0,"b'""""""\nThis is an implementation of Function Secret Sharing\n\nUseful papers are:\n- Function Secret Sharing- Improvements and Extensions, Boyle 2017\n  Link: https://eprint.iacr.org/2018/707.pdf\n- Secure Computation with Preprocessing via Function Secret Sharing, Boyle 2019\n  Link: https://eprint.iacr.org/2019/1095\n\nNote that the protocols are quite different in aspect from those papers\n""""""\nimport hashlib\n\nimport torch as th\nimport syft as sy\n\n\n\xce\xbb = 110  # 6  # 110 or 63  # security parameter\nn = 32  # 8  # 32  # bit precision\ndtype = th.int32\n\nno_wrap = {""no_wrap"": True}\n\n\ndef initialize_crypto_plans(worker):\n    """"""\n    This is called manually for the moment, to build the plan used to perform\n    Function Secret Sharing on a specific worker.\n    """"""\n    eq_plan_1 = sy.Plan(\n        forward_func=lambda x, y: mask_builder(x, y, ""eq""),\n        owner=worker,\n        tags=[""#fss_eq_plan_1""],\n        is_built=True,\n    )\n    worker.register_obj(eq_plan_1)\n    eq_plan_2 = sy.Plan(\n        forward_func=eq_eval_plan, owner=worker, tags=[""#fss_eq_plan_2""], is_built=True\n    )\n    worker.register_obj(eq_plan_2)\n\n    comp_plan_1 = sy.Plan(\n        forward_func=lambda x, y: mask_builder(x, y, ""comp""),\n        owner=worker,\n        tags=[""#fss_comp_plan_1""],\n        is_built=True,\n    )\n    worker.register_obj(comp_plan_1)\n    comp_plan_2 = sy.Plan(\n        forward_func=comp_eval_plan, owner=worker, tags=[""#fss_comp_plan_2""], is_built=True\n    )\n    worker.register_obj(comp_plan_2)\n\n    xor_add_plan = sy.Plan(\n        forward_func=xor_add_convert_1, owner=worker, tags=[""#xor_add_1""], is_built=True\n    )\n    worker.register_obj(xor_add_plan)\n    xor_add_plan = sy.Plan(\n        forward_func=xor_add_convert_2, owner=worker, tags=[""#xor_add_2""], is_built=True\n    )\n    worker.register_obj(xor_add_plan)\n\n\ndef request_run_plan(worker, plan_tag, location, return_value, args=(), kwargs={}):\n    response_ids = (sy.ID_PROVIDER.pop(),)\n    args = (args, response_ids)\n\n    response = worker.send_command(\n        cmd_name=""run"",\n        target=plan_tag,\n        recipient=location,\n        return_ids=response_ids,\n        return_value=return_value,\n        kwargs_=kwargs,\n        args_=args,\n    )\n    return response\n\n\ndef fss_op(x1, x2, type_op=""eq""):\n    """"""\n    Define the workflow for a binary operation using Function Secret Sharing\n\n    Currently supported operand are = & <=, respectively corresponding to\n    type_op = \'eq\' and \'comp\'\n\n    Args:\n        x1: first AST\n        x2: second AST\n        type_op: type of operation to perform, should be \'eq\' or \'comp\'\n\n    Returns:\n        shares of the comparison\n    """"""\n\n    me = sy.local_worker\n    locations = x1.locations\n\n    shares = []\n    for location in locations:\n        args = (x1.child[location.id], x2.child[location.id])\n        share = request_run_plan(\n            me, f""#fss_{type_op}_plan_1"", location, return_value=True, args=args\n        )\n        shares.append(share)\n\n    mask_value = sum(shares) % 2 ** n\n\n    shares = []\n    for i, location in enumerate(locations):\n        args = (th.IntTensor([i]), mask_value)\n        share = request_run_plan(\n            me, f""#fss_{type_op}_plan_2"", location, return_value=False, args=args\n        )\n        shares.append(share)\n\n    if type_op == ""comp"":\n        prev_shares = shares\n        shares = []\n        for prev_share, location in zip(prev_shares, locations):\n            share = request_run_plan(\n                me, ""#xor_add_1"", location, return_value=True, args=(prev_share,)\n            )\n            shares.append(share)\n\n        masked_value = shares[0] ^ shares[1]  # TODO case >2 workers ?\n\n        shares = {}\n        for i, prev_share, location in zip(range(len(locations)), prev_shares, locations):\n            share = request_run_plan(\n                me,\n                ""#xor_add_2"",\n                location,\n                return_value=False,\n                args=(th.IntTensor([i]), masked_value),\n            )\n            shares[location.id] = share\n    else:\n        shares = {loc.id: share for loc, share in zip(locations, shares)}\n\n    response = sy.AdditiveSharingTensor(shares, **x1.get_class_attributes())\n    return response\n\n\n# share level\ndef mask_builder(x1, x2, type_op):\n    x = x1 - x2\n    # Keep the primitive in store as we use it after\n    alpha, s_0, *CW = x1.owner.crypto_store.get_keys(\n        f""fss_{type_op}"", n_instances=x1.numel(), remove=False\n    )\n    return x + alpha.reshape(x.shape)\n\n\n# share level\ndef eq_eval_plan(b, x_masked):\n    alpha, s_0, *CW = x_masked.owner.crypto_store.get_keys(\n        type_op=""fss_eq"", n_instances=x_masked.numel(), remove=True\n    )\n    result_share = DPF.eval(b, x_masked, s_0, *CW)\n    return result_share\n\n\n# share level\ndef comp_eval_plan(b, x_masked):\n    alpha, s_0, *CW = x_masked.owner.crypto_store.get_keys(\n        type_op=""fss_comp"", n_instances=x_masked.numel(), remove=True\n    )\n    result_share = DIF.eval(b, x_masked, s_0, *CW)\n    return result_share\n\n\ndef xor_add_convert_1(x):\n    xor_share, add_share = x.owner.crypto_store.get_keys(\n        type_op=""xor_add_couple"", n_instances=x.numel(), remove=False\n    )\n    return x ^ xor_share.reshape(x.shape)\n\n\ndef xor_add_convert_2(b, x):\n    xor_share, add_share = x.owner.crypto_store.get_keys(\n        type_op=""xor_add_couple"", n_instances=x.numel(), remove=True\n    )\n    return add_share.reshape(x.shape) * (1 - 2 * x) + x * b\n\n\ndef eq(x1, x2):\n    return fss_op(x1, x2, ""eq"")\n\n\ndef le(x1, x2):\n    return fss_op(x1, x2, ""comp"")\n\n\nclass DPF:\n    """"""Distributed Point Function - used for equality""""""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def keygen(n_values=1):\n        beta = th.tensor([1], dtype=dtype)\n        alpha = th.randint(0, 2 ** n, (n_values,))\n\n        \xce\xb1 = bit_decomposition(alpha)\n        s, t, CW = (\n            Array(n + 1, 2, \xce\xbb, n_values),\n            Array(n + 1, 2, n_values),\n            Array(n, 2 * (\xce\xbb + 1), n_values),\n        )\n        s[0] = randbit(size=(2, \xce\xbb, n_values))\n        t[0] = th.tensor([[0, 1]] * n_values, dtype=th.uint8).t()\n        for i in range(0, n):\n            g0 = G(s[i, 0])\n            g1 = G(s[i, 1])\n            # Re-use useless randomness\n            sL_0, _, sR_0, _ = split(g0, [\xce\xbb, 1, \xce\xbb, 1])\n            sL_1, _, sR_1, _ = split(g1, [\xce\xbb, 1, \xce\xbb, 1])\n            s_rand = (sL_0 ^ sL_1) * \xce\xb1[i] + (sR_0 ^ sR_1) * (1 - \xce\xb1[i])\n\n            cw_i = TruthTableDPF(s_rand, \xce\xb1[i])\n            CW[i] = cw_i ^ g0 ^ g1\n\n            for b in (0, 1):\n                \xcf\x84 = [g0, g1][b] ^ (t[i, b] * CW[i])\n                \xcf\x84 = \xcf\x84.reshape(2, \xce\xbb + 1, n_values)\n                # filtered_\xcf\x84 = \xcf\x84[\xf0\x9d\x9b\xbc[i]] OLD\n                \xce\xb1_i = \xce\xb1[i].unsqueeze(0).expand(\xce\xbb + 1, n_values).unsqueeze(0).long()\n                filtered_\xcf\x84 = th.gather(\xcf\x84, 0, \xce\xb1_i).squeeze(0)\n                s[i + 1, b], t[i + 1, b] = split(filtered_\xcf\x84, [\xce\xbb, 1])\n\n        CW_n = (-1) ** t[n, 1].to(dtype) * (beta - Convert(s[n, 0]) + Convert(s[n, 1]))\n\n        return (alpha,) + s[0].unbind() + (CW, CW_n)\n\n    @staticmethod\n    def eval(b, x, *k_b):\n        original_shape = x.shape\n        x = x.reshape(-1)\n        n_values = x.shape[0]\n        x = bit_decomposition(x)\n        s, t = Array(n + 1, \xce\xbb, n_values), Array(n + 1, 1, n_values)\n        s[0] = k_b[0]\n        # here k[1:] is (CW, CW_n)\n        CW = k_b[1].unbind() + (k_b[2],)\n        t[0] = b\n        for i in range(0, n):\n            \xcf\x84 = G(s[i]) ^ (t[i] * CW[i])\n            \xcf\x84 = \xcf\x84.reshape(2, \xce\xbb + 1, n_values)\n            x_i = x[i].unsqueeze(0).expand(\xce\xbb + 1, n_values).unsqueeze(0).long()\n            filtered_\xcf\x84 = th.gather(\xcf\x84, 0, x_i).squeeze(0)\n            s[i + 1], t[i + 1] = split(filtered_\xcf\x84, [\xce\xbb, 1])\n        flat_result = (-1) ** b * (Convert(s[n]) + t[n].squeeze() * CW[n])\n        return flat_result.reshape(original_shape)\n\n\nclass DIF:\n    """"""Distributed Interval Function - used for comparison <=""""""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def keygen(n_values=1):\n        alpha = th.randint(0, 2 ** n, (n_values,))\n        \xce\xb1 = bit_decomposition(alpha)\n        s, t, CW = (\n            Array(n + 1, 2, \xce\xbb, n_values),\n            Array(n + 1, 2, n_values),\n            Array(n, 2 + 2 * (\xce\xbb + 1), n_values),\n        )\n        s[0] = randbit(size=(2, \xce\xbb, n_values))\n        t[0] = th.tensor([[0, 1]] * n_values, dtype=th.uint8).t()\n        for i in range(0, n):\n            h0 = H(s[i, 0])\n            h1 = H(s[i, 1])\n            # Re-use useless randomness\n            _, _, sL_0, _, sR_0, _ = split(h0, [1, 1, \xce\xbb, 1, \xce\xbb, 1])\n            _, _, sL_1, _, sR_1, _ = split(h1, [1, 1, \xce\xbb, 1, \xce\xbb, 1])\n            s_rand = (sL_0 ^ sL_1) * \xce\xb1[i] + (sR_0 ^ sR_1) * (1 - \xce\xb1[i])\n            cw_i = TruthTableDIF(s_rand, \xce\xb1[i])\n            CW[i] = cw_i ^ h0 ^ h1\n\n            for b in (0, 1):\n                \xcf\x84 = [h0, h1][b] ^ (t[i, b] * CW[i])\n                \xcf\x84 = \xcf\x84.reshape(2, \xce\xbb + 2, n_values)\n                # filtered_\xcf\x84 = \xcf\x84[\xf0\x9d\x9b\xbc[i]] OLD\n                \xce\xb1_i = \xce\xb1[i].unsqueeze(0).expand(\xce\xbb + 2, n_values).unsqueeze(0).long()\n                filtered_\xcf\x84 = th.gather(\xcf\x84, 0, \xce\xb1_i).squeeze(0)\n                \xcf\x83_leaf, s[i + 1, b], t[i + 1, b] = split(filtered_\xcf\x84, [1, \xce\xbb, 1])\n\n        return (alpha,) + s[0].unbind() + (CW,)\n\n    @staticmethod\n    def eval(b, x, *k_b):\n        original_shape = x.shape\n        x = x.reshape(-1)\n        n_values = x.shape[0]\n        x = bit_decomposition(x)\n        FnOutput = Array(n + 1, n_values)\n        s, t = Array(n + 1, \xce\xbb, n_values), Array(n + 1, 1, n_values)\n        s[0] = k_b[0]\n        CW = k_b[1].unbind()\n        t[0] = b\n        for i in range(0, n):\n            \xcf\x84 = H(s[i]) ^ (t[i] * CW[i])\n            \xcf\x84 = \xcf\x84.reshape(2, \xce\xbb + 2, n_values)\n            x_i = x[i].unsqueeze(0).expand(\xce\xbb + 2, n_values).unsqueeze(0).long()\n            filtered_\xcf\x84 = th.gather(\xcf\x84, 0, x_i).squeeze(0)\n            \xcf\x83_leaf, s[i + 1], t[i + 1] = split(filtered_\xcf\x84, [1, \xce\xbb, 1])\n            FnOutput[i] = \xcf\x83_leaf\n\n        # Last tour, the other \xcf\x83 is also a leaf:\n        FnOutput[n] = t[n]\n        flat_result = FnOutput.sum(axis=0) % 2\n        return flat_result.reshape(original_shape)\n\n\n# PRG\ndef G(seed):\n    assert seed.shape[0] == \xce\xbb\n    seed_t = seed.t().tolist()\n    gen_list = []\n    for seed_bit in seed_t:\n        enc_str = str(seed_bit).encode()\n        h = hashlib.sha3_256(enc_str)\n        r = h.digest()\n        binary_str = bin(int.from_bytes(r, byteorder=""big""))[2 : 2 + (2 * (\xce\xbb + 1))]\n        gen_list.append(list(map(int, binary_str)))\n\n    return th.tensor(gen_list, dtype=th.uint8).t()\n\n\ndef H(seed):\n    assert seed.shape[0] == \xce\xbb\n    seed_t = seed.t().tolist()\n    gen_list = []\n    for seed_bit in seed_t:\n        enc_str = str(seed_bit).encode()\n        h = hashlib.sha3_256(enc_str)\n        r = h.digest()\n        binary_str = bin(int.from_bytes(r, byteorder=""big""))[2 : 2 + 2 + (2 * (\xce\xbb + 1))]\n        gen_list.append(list(map(int, binary_str)))\n\n    return th.tensor(gen_list, dtype=th.uint8).t()\n\n\ndef Convert(bits):\n    bit_pow_lambda = th.flip(2 ** th.arange(\xce\xbb), (0,)).unsqueeze(-1).to(th.long)\n    return (bits.to(th.long) * bit_pow_lambda).sum(dim=0).to(dtype)\n\n\ndef Array(*shape):\n    return th.empty(shape, dtype=th.uint8)\n\n\nbit_pow_n = th.flip(2 ** th.arange(n), (0,))\n\n\ndef bit_decomposition(x):\n    x = x.unsqueeze(-1)\n    z = bit_pow_n & x\n    z = z.t()\n    return (z > 0).to(th.uint8)\n\n\ndef randbit(size):\n    return th.randint(2, size=size)\n\n\ndef concat(*args, **kwargs):\n    return th.cat(args, **kwargs)\n\n\ndef split(x, idx):\n    return th.split(x, idx)\n\n\ndef TruthTableDPF(s, \xce\xb1_i):\n    one = th.ones((1, s.shape[1])).to(th.uint8)\n    s_one = concat(s, one)\n    Table = th.zeros((2, \xce\xbb + 1, len(\xce\xb1_i)), dtype=th.uint8)\n    for j, el in enumerate(\xce\xb1_i):\n        Table[el.item(), :, j] = s_one[:, j]\n    return Table.reshape(-1, Table.shape[2])\n\n\ndef TruthTableDIF(s, \xce\xb1_i):\n    leafTable = th.zeros((2, 1, len(\xce\xb1_i)), dtype=th.uint8)\n    # TODO optimize: just put alpha on first line\n    leaf_value = \xce\xb1_i\n    for j, el in enumerate(\xce\xb1_i):\n        leafTable[(1 - el).item(), 0, j] = leaf_value[j]\n\n    one = th.ones((1, s.shape[1])).to(th.uint8)\n    s_one = concat(s, one)\n    nextTable = th.zeros((2, \xce\xbb + 1, len(\xce\xb1_i)), dtype=th.uint8)\n    for j, el in enumerate(\xce\xb1_i):\n        nextTable[el.item(), :, j] = s_one[:, j]\n\n    Table = concat(leafTable, nextTable, axis=1)\n    Table = Table.reshape(-1, Table.shape[2])\n    return Table\n'"
syft/frameworks/torch/mpc/primitives.py,3,"b'from collections import defaultdict\nfrom typing import List, Union\n\nimport torch as th\nimport syft as sy\nfrom syft.exceptions import EmptyCryptoPrimitiveStoreError\nfrom syft.workers.abstract import AbstractWorker\n\n\nclass PrimitiveStorage:\n    """"""\n    Used by normal workers to store crypto primitives\n    Used by crypto providers to build crypto primitives\n    """"""\n\n    def __init__(self, owner):\n        """"""\n        Their are below different kinds of primitives available.\n        Each primitive stack is a fixed length list corresponding to all the\n        components for the primitive. For example, the beaver triple primitive would\n        have 3 components. Each component is a high dimensional tensor whose\n        last dimension is the same and corresponds to the number of instances available\n        for this primitive. That\'s why get_keys uses a quite complicated dimension\n        selector. This structure helps generating efficiently primitives using\n        tensorized key generation algorithms.\n        """"""\n        self.fss_eq: list = []\n        self.fss_comp: list = []\n        self.beaver: list = []\n        self.xor_add_couple: list = []  # couple of the same value shared via ^ or + op\n\n        self._owner: AbstractWorker = owner\n        self._builders: dict = {\n            ""fss_eq"": self.build_fss_keys(type_op=""eq""),\n            ""fss_comp"": self.build_fss_keys(type_op=""comp""),\n            ""beaver"": self.build_triples,\n            ""xor_add_couple"": self.build_xor_add_couple,\n        }\n\n    def get_keys(self, type_op, n_instances=1, remove=True):\n        """"""\n        Return FSS keys primitives\n\n        Args:\n            type_op: fss_eq, fss_comp, or xor_add_couple\n            n_instances: how many primitives to retrieve. Comparison is pointwise so this is\n                convenient: for any matrice of size nxm I can unstack n*m elements for the\n                comparison\n            remove: if true, pop out the primitive. If false, only read it. Read mode is\n                needed because we\'re working on virtual workers and they need to gather\n                a some point and then re-access the keys.\n        """"""\n        primitive_stack = getattr(self, type_op)\n\n        available_instances = len(primitive_stack[0]) if len(primitive_stack) > 0 else -1\n        if available_instances >= n_instances:\n            keys = []\n            # We iterate on the different elements that constitute a given primitive, for\n            # example of the beaver triples, you would have 3 elements.\n            for i, prim in enumerate(primitive_stack):\n                # We\'re selecting on the last dimension of the tensor because it\'s simpler for\n                # generating those primitives in crypto protocols\n                # th.narrow(dim, index_start, length)\n                keys.append(th.narrow(prim, -1, 0, n_instances))\n                if remove:\n                    length = prim.shape[-1] - n_instances\n                    primitive_stack[i] = th.narrow(prim, -1, n_instances, length)\n\n            return keys\n        else:\n            raise EmptyCryptoPrimitiveStoreError(self, type_op, available_instances, n_instances)\n\n    def provide_primitives(\n        self,\n        crypto_types: Union[str, List[str]],\n        workers: List[AbstractWorker],\n        n_instances: int = 10,\n        **kwargs,\n    ):\n        """""" Build n_instances of crypto primitives of the different crypto_types given and\n        send them to some workers.\n\n        Args:\n            crypto_types: type of primitive (fss_eq, etc)\n            workers: recipients for those primitive\n            n_instances: how many of them are needed\n            **kwargs: any parameters needs for the primitive builder\n        """"""\n        if isinstance(crypto_types, str):\n            crypto_types = [crypto_types]\n\n        worker_types_primitives = defaultdict(dict)\n        for crypto_type in crypto_types:\n            builder = self._builders[crypto_type]\n\n            primitives = builder(n_party=len(workers), n_instances=n_instances, **kwargs)\n\n            for worker_primitives, worker in zip(primitives, workers):\n                worker_types_primitives[worker][crypto_type] = worker_primitives\n\n        for i, worker in enumerate(workers):\n            worker_message = self._owner.create_worker_command_message(\n                ""feed_crypto_primitive_store"", None, worker_types_primitives[worker]\n            )\n            self._owner.send_msg(worker_message, worker)\n\n    def add_primitives(self, types_primitives: dict):\n        """"""\n        Include primitives in the store\n\n        Args:\n            types_primitives: dict {crypto_type: str: primitives: list}\n        """"""\n        for crypto_type, primitives in types_primitives.items():\n            assert hasattr(self, crypto_type), f""Unknown crypto primitives {crypto_type}""\n\n            current_primitives = getattr(self, crypto_type)\n            if len(current_primitives) == 0:\n                setattr(self, crypto_type, list(primitives))\n            else:\n                for i, primitive in enumerate(primitives):\n                    if len(current_primitives[i]) == 0:\n                        current_primitives[i] = primitive\n                    else:\n                        current_primitives[i] = th.cat(\n                            (current_primitives[i], primitive), dim=len(primitive.shape) - 1\n                        )\n\n    def build_fss_keys(self, type_op):\n        """"""\n        The builder to generate functional keys for Function Secret Sharing (FSS)\n        """"""\n        if type_op == ""eq"":\n            fss_class = sy.frameworks.torch.mpc.fss.DPF\n        elif type_op == ""comp"":\n            fss_class = sy.frameworks.torch.mpc.fss.DIF\n        else:\n            raise ValueError(f""type_op {type_op} not valid"")\n\n        n = sy.frameworks.torch.mpc.fss.n\n\n        def build_separate_fss_keys(n_party, n_instances=100):\n            assert (\n                n_party == 2\n            ), f""The FSS protocol only works for 2 workers, {n_party} were provided.""\n            alpha, s_00, s_01, *CW = fss_class.keygen(n_values=n_instances)\n            # simulate sharing TODO clean this\n            mask = th.randint(0, 2 ** n, alpha.shape)\n            return [((alpha - mask) % 2 ** n, s_00, *CW), (mask, s_01, *CW)]\n\n        return build_separate_fss_keys\n\n    @staticmethod\n    def build_xor_add_couple(n_party, n_instances=100):\n        assert (\n            n_party == 2\n        ), f""build_xor_add_couple is only implemented for 2 workers, {n_party} were provided.""\n        r = th.randint(2, size=(n_instances,))\n        mask1 = th.randint(2, size=(n_instances,))\n        mask2 = th.randint(2, size=(n_instances,))\n\n        return [(r ^ mask1, r - mask2), (mask1, mask2)]\n\n    def build_triples(self, **kwargs):\n        raise NotImplementedError\n'"
syft/frameworks/torch/mpc/securenn.py,30,"b'""""""\nThis is an implementation of the SecureNN paper\nhttps://eprint.iacr.org/2018/442.pdf\n\nExtended to a general case with N parties\n\nNote that there is a difference here in that our shares can be\nnegative numbers while they are always positive in the paper\n""""""\nimport math\nimport torch\nimport syft as sy\nfrom syft.generic.utils import memorize\n\n# p is introduced in the SecureNN paper https://eprint.iacr.org/2018/442.pdf\n# it is a small field for efficient additive sharing\np = 67\nno_wrap = {""no_wrap"": True}\n\n\n# Cached values\n@memorize\ndef Q_BITS(field):\n    return 64 if field > 2 ** 32 else 32\n\n\n@memorize\ndef get_torch_dtype(field: int):\n    return torch.int64 if field > 2 ** 32 else torch.int32\n\n\n@memorize\ndef get_n_bits(field: int):\n    return round(math.log(field, 2))\n\n\n@memorize\ndef get_max_val_field(field: int):\n    return (field - 1) // 2\n\n\n@memorize\ndef get_min_val_field(field: int):\n    return -(field // 2)\n\n\n@memorize\ndef get_r_mask(field: int):\n    return (field // 2) - 1\n\n\n@memorize\ndef get_dtype(field: int):\n    return ""long"" if field > 2 ** 32 else ""int""\n\n\ndef decompose(tensor, field):\n    """"""decompose a tensor into its binary representation.""""""\n    torch_dtype = get_torch_dtype(field)\n    n_bits = get_n_bits(field)\n    powers = torch.arange(n_bits, dtype=torch_dtype)\n    if hasattr(tensor, ""child"") and isinstance(tensor.child, dict):\n        powers = powers.send(*list(tensor.child.keys()), **no_wrap)\n    for _ in range(len(tensor.shape)):\n        powers = powers.unsqueeze(0)\n    tensor = tensor.unsqueeze(-1)\n    moduli = 2 ** powers\n    tensor = torch.fmod((tensor / moduli.type_as(tensor)), 2)\n    return tensor\n\n\ndef flip(x, dim, dtype):\n    """"""\n    Reverse the order of the elements in a tensor\n    """"""\n    assert (\n        x.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n    indices = torch.arange(x.shape[dim] - 1, -1, -1).type(dtype)\n\n    if hasattr(x, ""child"") and isinstance(x.child, dict):\n        indices = indices.send(*list(x.child.keys()), **no_wrap)\n\n    return x.index_select(dim, indices)\n\n\ndef _random_common_bit(*workers):\n    """"""\n    Return a bit chosen by a worker and sent to all workers,\n    in the form of a MultiPointerTensor\n    """"""\n    pointer = torch.tensor([1]).send(workers[0], **no_wrap).random_(2)\n    pointers = [pointer]\n    for worker in workers[1:]:\n        pointers.append(pointer.copy().move(worker))\n    bit = sy.MultiPointerTensor(children=pointers)\n\n    return bit\n\n\ndef _random_common_value(max_value, *workers):\n    """"""\n    Return n in [0, max_value-1] chosen by a worker and sent to all workers,\n    in the form of a MultiPointerTensor\n    """"""\n    torch_dtype = get_torch_dtype(max_value)\n    pointer = (\n        torch.tensor([1], dtype=torch_dtype)\n        .send(workers[0], **no_wrap)\n        .random_(1, get_max_val_field(max_value))\n    )\n    pointers = [pointer]\n    for worker in workers[1:]:\n        pointers.append(pointer.copy().move(worker))\n    common_value = sy.MultiPointerTensor(children=pointers)\n\n    return common_value\n\n\ndef _shares_of_zero(size, field, dtype, crypto_provider, *workers):\n    """"""\n    Return shares of zeros generated by a worker and sent to all workers,\n    in the form of a MultiPointerTensor\n    """"""\n    torch_dtype = get_torch_dtype(field)\n    u = (\n        torch.zeros(size, dtype=torch_dtype)\n        .send(workers[0])\n        .share(*workers, field=field, dtype=dtype, crypto_provider=crypto_provider, **no_wrap)\n        .get()\n        .child\n    )\n\n    return u\n\n\ndef select_share(alpha_sh, x_sh, y_sh):\n    """""" Performs select share protocol\n    If the bit alpha_sh is 0, x_sh is returned\n    If the bit alpha_sh is 1, y_sh is returned\n\n    Args:\n        x_sh (AdditiveSharingTensor): the first share to select\n        y_sh (AdditiveSharingTensor): the second share to select\n        alpha_sh (AdditiveSharingTensor): the bit to choose between x_sh and y_sh\n\n    Return:\n        z_sh = (1 - alpha_sh) * x_sh + alpha_sh * y_sh\n    """"""\n    assert (\n        alpha_sh.dtype == x_sh.dtype == y_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n\n    workers = alpha_sh.locations\n    crypto_provider = alpha_sh.crypto_provider\n    L = alpha_sh.field\n    dtype = get_dtype(L)\n    u_sh = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n\n    # 1)\n    w_sh = y_sh - x_sh\n\n    # 2)\n    c_sh = alpha_sh * w_sh\n\n    # 3)\n    z_sh = x_sh + c_sh + u_sh\n\n    return z_sh\n\n\ndef private_compare(x_bit_sh, r, beta, L):\n    """"""\n    Perform privately x > r\n\n    args:\n        x (AdditiveSharedTensor): the private tensor\n        r (MultiPointerTensor): the threshold commonly held by the workers\n        beta (MultiPointerTensor): a boolean commonly held by the workers to\n            hide the result of computation for the crypto provider\n        L(int): field size for r\n\n    return:\n        \xce\xb2\xe2\x80\xb2 = \xce\xb2 \xe2\x8a\x95 (x > r).\n    """"""\n    assert isinstance(x_bit_sh, sy.AdditiveSharingTensor)\n    assert isinstance(r, sy.MultiPointerTensor)\n    assert isinstance(beta, sy.MultiPointerTensor)\n    # Would it be safer to have a different r/beta for each value in the tensor?\n\n    workers = x_bit_sh.locations\n    crypto_provider = x_bit_sh.crypto_provider\n    p = x_bit_sh.field\n\n    # the commented out numbers below correspond to the\n    # line numbers in Algorithm 3 of the SecureNN paper\n    # https://eprint.iacr.org/2018/442.pdf\n\n    # Common randomess\n    s = torch.randint(1, p, x_bit_sh.shape).send(*workers, **no_wrap)\n    u = torch.randint(1, p, x_bit_sh.shape).send(*workers, **no_wrap)\n    perm = torch.randperm(x_bit_sh.shape[-1]).send(*workers, **no_wrap)\n\n    j = sy.MultiPointerTensor(\n        children=[torch.tensor([int(i == 0)]).send(w, **no_wrap) for i, w in enumerate(workers)]\n    )\n\n    # 1)\n    t = r + 1\n    t_bit = decompose(t, L)\n    r_bit = decompose(r, L)\n\n    # if beta == 0\n    # 5)\n    w = x_bit_sh + (j * r_bit) - (2 * r_bit * x_bit_sh)\n    # 6)\n    wc = w.flip(-1).cumsum(-1).flip(-1) - w\n    c_beta0 = -x_bit_sh + (j * r_bit) + j + wc\n\n    # elif beta == 1 AND r != 2^l- 1\n    # 8)\n    w = x_bit_sh + (j * t_bit) - (2 * t_bit * x_bit_sh)\n    # 9)\n    wc = w.flip(-1).cumsum(-1).flip(-1) - w\n    c_beta1 = x_bit_sh + (-j * t_bit) + j + wc\n\n    # else\n    # 11)\n    c_igt1 = (1 - j) * (u + 1) - (j * u)\n    c_ie1 = (1 - 2 * j) * u\n\n    l1_mask = torch.zeros(x_bit_sh.shape).long()\n    l1_mask[..., 0] = 1\n    l1_mask = l1_mask.send(*workers, **no_wrap)\n    # c_else = if i == 1 c_ie1 else c_igt1\n    c_else = (l1_mask * c_ie1) + ((1 - l1_mask) * c_igt1)\n\n    # Mask for the case r == 2^l \xe2\x88\x921\n    r_mask = (r == get_r_mask(L)).long()\n    r_mask = r_mask.unsqueeze(-1)\n\n    # Mask combination to execute the if / else statements of 4), 7), 10)\n    c = (1 - beta) * c_beta0 + (beta * (1 - r_mask)) * c_beta1 + (beta * r_mask) * c_else\n\n    # 14)\n    # Hide c values\n    mask = s * c\n\n    # Permute the mask\n    # I have to create idx because Ellipsis are still not supported\n    # (I would like to do permuted_mask = mask[..., perm])\n    idx = [slice(None)] * (len(x_bit_sh.shape) - 1) + [perm]\n    permuted_mask = mask[idx]\n    # Send it to another worker\n    # We do this because we can\'t allow the local worker to get and see permuted_mask\n    # because otherwise it can inverse the permutation and remove s to get c.\n    # So opening the permuted_mask should be made by a worker which doesn\'t have access\n    # to the randomness\n    remote_mask = permuted_mask.wrap().send(crypto_provider, **no_wrap)\n\n    # 15)\n    d_ptr = remote_mask.remote_get()\n    beta_prime = (d_ptr == 0).sum(-1)\n\n    # Get result back\n    res = beta_prime.get()\n    return res\n\n\ndef msb(a_sh):\n    """"""\n    Compute the most significant bit in a_sh, this is an implementation of the\n    SecureNN paper https://eprint.iacr.org/2018/442.pdf\n\n    Args:\n        a_sh (AdditiveSharingTensor): the tensor of study\n    Return:\n        the most significant bit\n    """"""\n\n    workers = a_sh.locations\n    crypto_provider = a_sh.crypto_provider\n    L = a_sh.field + 1  # field of a is L - 1\n    dtype = get_dtype(L)\n    input_shape = a_sh.shape\n    a_sh = a_sh.view(-1)\n\n    # the commented out numbers below correspond to the\n    # line numbers in Table 5 of the SecureNN paper\n    # https://eprint.iacr.org/2018/442.pdf\n\n    # Common Randomness\n    beta = _random_common_bit(*workers)\n    u = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n\n    # 1)\n    x = torch.tensor(a_sh.shape).random_(get_max_val_field(L - 1))\n    x_bit = decompose(x, L)\n    x_sh = x.share(\n        *workers, field=L - 1, dtype=""custom"", crypto_provider=crypto_provider, **no_wrap\n    )\n    x_bit_0 = x_bit[..., 0]\n    x_bit_sh_0 = x_bit_0.share(*workers, field=L, crypto_provider=crypto_provider, **no_wrap)\n    x_bit_sh = x_bit.share(\n        *workers, field=p, dtype=""custom"", crypto_provider=crypto_provider, **no_wrap\n    )\n\n    # 2)\n    y_sh = a_sh * 2\n    r_sh = y_sh + x_sh\n\n    # 3)\n    r = r_sh.reconstruct()  # convert an additive sharing in multi pointer Tensor\n    r_0 = decompose(r, L)[..., 0]\n\n    # 4)\n    beta_prime = private_compare(x_bit_sh, r, beta, L)\n\n    # 5)\n    beta_prime_sh = beta_prime.share(\n        *workers, field=L, dtype=dtype, crypto_provider=crypto_provider, **no_wrap\n    )\n\n    # 7)\n    j = sy.MultiPointerTensor(\n        children=[torch.tensor([int(i == 0)]).send(w, **no_wrap) for i, w in enumerate(workers)]\n    )\n    gamma = beta_prime_sh + (j * beta) - (2 * beta * beta_prime_sh)\n\n    # 8)\n    delta = x_bit_sh_0 + (j * r_0) - (2 * r_0 * x_bit_sh_0)\n\n    # 9)\n    theta = gamma * delta\n\n    # 10)\n    a = gamma + delta - (theta * 2) + u\n\n    if len(input_shape):\n        return a.view(*list(input_shape))\n    else:\n        return a\n\n\n# def wrap(a_sh, L):\n#     """"""\n#     Calculates how many times the shares of a will wrap around L when added together\n\n#     Args:\n#         a_sh (AdditiveSharingTensor): the tensor of study. It\'s value should be public\n#         L (int): The value at which to wrap\n#     Return:\n#         The tensor in which each entry is how many times does that entry of a wrap around L\n#     """"""\n#     # The code could be simpler, but then there are some overflow errors\n#     workers = a_sh.locations\n#     total = torch.zeros(a_sh.shape).send(workers[0]).child\n#     remainders = torch.zeros(a_sh.shape).send(workers[0]).child\n#     for i, worker in enumerate(workers):\n#         cur = a_sh.child[worker.id].copy()\n#         if i != 0:\n#             cur = cur.move(workers[0])\n#         total += cur // L\n#         remainders += cur % L\n#         total += remainders // L\n#         remainders = remainders % L\n#     return total.long()\n\n\ndef share_convert(a_sh):\n    """"""\n    Convert shares of a in field L to shares of a in field L - 1\n\n    Args:\n        a_sh (AdditiveSharingTensor): the additive sharing tensor who owns\n            the shares in field L to convert\n\n    Return:\n        An additive sharing tensor with shares in field L-1\n    """"""\n    assert isinstance(a_sh, sy.AdditiveSharingTensor)\n    assert (\n        a_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n\n    workers = a_sh.locations\n    crypto_provider = a_sh.crypto_provider\n    L = a_sh.field\n    # torch_dtype = get_torch_dtype(L)\n    # dtype = get_dtype(L)\n    # Common randomness\n    # eta_pp = _random_common_bit(*workers)\n    # r = _random_common_value(L, *workers)\n\n    # Share remotely r\n    # r_sh = (\n    #    (r * 1)\n    #    .child[workers[0].id]\n    #    .share(*workers, field=L, dtype=dtype, crypto_provider=crypto_provider)\n    #    .get()\n    #    .child\n    # )\n    # r_shares = r_sh.child\n\n    # WORKS WITH N PARTIES, NEEDS BUGFIXING IN WRAP\n    # alpha0 = wrap(r_sh, L)\n    # alphas = [alpha0.copy().move(w) for w in workers[1:]]\n    # alpha = sy.MultiPointerTensor(children=[alpha0, *alphas])\n\n    # WORKS WITH 2 PARTIES\n    # alpha0 = (\n    #    (\n    #        (r_shares[workers[0].id] + r_shares[workers[1].id].copy().move(workers[0]))\n    #        > get_max_val_field(L)\n    #    )\n    # ).type(torch_dtype)\n    # alpha1 = alpha0.copy().move(workers[1])\n    # alpha = sy.MultiPointerTensor(children=[alpha0, alpha1])\n\n    u_sh = _shares_of_zero(1, L - 1, ""custom"", crypto_provider, *workers)\n\n    # 2)\n    # a_tilde_sh = a_sh + r_sh\n    # a_shares = a_sh.child\n    # beta0 = (\n    #    ((a_shares[workers[0].id] + r_shares[workers[0].id]) > get_max_val_field(L))\n    #    + ((a_shares[workers[0].id] + r_shares[workers[0].id]) < get_min_val_field(L))\n    # ).type(torch_dtype)\n    # beta1 = (\n    #    ((a_shares[workers[1].id] + r_shares[workers[1].id]) > get_max_val_field(L))\n    #    + ((a_shares[workers[1].id] + r_shares[workers[1].id]) < get_min_val_field(L))\n    # ).type(torch_dtype)\n\n    # beta = sy.MultiPointerTensor(children=[beta0, beta1])\n\n    # 4)\n    # a_tilde_shares = a_tilde_sh.child\n    # delta = a_tilde_shares[workers[0].id].copy().get()\n    #         + a_tilde_shares[workers[1].id].copy().get()\n    # Check for both positive and negative overflows\n    # delta = ((delta > get_max_val_field(L)) + (delta < get_min_val_field(L))).type(torch_dtype)\n    # x = a_tilde_sh.get()\n\n    # 5)\n    # x_bit = decompose(x, L)\n    # x_bit_sh = x_bit.share(\n    #    *workers, field=p, dtype=""custom"", crypto_provider=crypto_provider, **no_wrap\n    # )\n    # delta_sh = delta.share(\n    #    *workers, field=L - 1, dtype=""custom"", crypto_provider=crypto_provider, **no_wrap\n    # )\n\n    # 6)\n    # eta_p = private_compare(x_bit_sh, r - 1, eta_pp, L)\n    # 7)\n    # eta_p_sh = eta_p.share(\n    #    *workers, field=L - 1, dtype=""custom"", crypto_provider=crypto_provider, **no_wrap\n    # )\n\n    # 9)\n    # j = sy.MultiPointerTensor(\n    #    children=[\n    #        torch.tensor([int(i != 0)]).send(w, **no_wrap)\n    #        for i, w in enumerate(workers)\n    #    ]\n    # )\n    # eta_sh = eta_p_sh + (1 - j) * eta_pp - 2 * eta_pp * eta_p_sh\n    # 10)\n    # theta_sh = beta - (1 - j) * (alpha + 1) + delta_sh + eta_sh\n    # 11)\n    # NOTE:\n    # It seems simple operation with shares in L-1 field is enough to conver a_sh from L to L-1\n    # Conversion of shares is handled internally in AST ops for custom dtype\n    y_sh = u_sh + a_sh\n    return y_sh\n\n\ndef relu_deriv(a_sh):\n    """"""\n    Compute the derivative of Relu\n\n    Args:\n        a_sh (AdditiveSharingTensor): the private tensor on which the op applies\n\n    Returns:\n        0 if Dec(a_sh) < 0\n        1 if Dec(a_sh) > 0\n        encrypted in an AdditiveSharingTensor\n    """"""\n    assert (\n        a_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n\n    workers = a_sh.locations\n    crypto_provider = a_sh.crypto_provider\n    L = a_sh.field\n    dtype = get_dtype(L)\n    # Common randomness\n    u = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n\n    # 1)\n    y_sh = a_sh * 2\n\n    # 2) Not applicable with algebraic shares\n    y_sh = share_convert(y_sh)\n\n    # 3)\n    alpha_sh = msb(y_sh)\n\n    # 4)\n    j = sy.MultiPointerTensor(\n        children=[torch.tensor([int(i == 0)]).send(w, **no_wrap) for i, w in enumerate(workers)]\n    )\n    gamma_sh = j - alpha_sh + u\n    return gamma_sh\n\n\ndef relu(a_sh):\n    """"""\n    Compute Relu\n\n    Args:\n        a_sh (AdditiveSharingTensor): the private tensor on which the op applies\n\n    Returns:\n        Dec(a_sh) > 0\n        encrypted in an AdditiveSharingTensor\n    """"""\n    assert (\n        a_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n\n    workers = a_sh.locations\n    crypto_provider = a_sh.crypto_provider\n    L = a_sh.field\n    dtype = get_dtype(L)\n    # Common Randomness\n    u = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n\n    return a_sh * relu_deriv(a_sh) + u\n\n\n# In division, bit_len_max is set to Q_BITS // 2 to avoid overflow problems (multiplying by\n# 2**64 would almost always lead to overflow).\ndef division(x_sh, y_sh, bit_len_max=None):\n    """""" Performs division of encrypted numbers\n\n    Args:\n        x_sh, y_sh (AdditiveSharingTensor): the private tensors on which the op applies\n\n    Returns:\n        element-wise integer division of x_sh by y_sh\n    """"""\n    assert (\n        x_sh.dtype == y_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n    workers = x_sh.locations\n    crypto_provider = x_sh.crypto_provider\n    L = x_sh.field\n    dtype = get_dtype(L)\n    if bit_len_max is None:\n        bit_len_max = Q_BITS(L) // 2\n\n    x_shape = x_sh.shape\n    y_shape = y_sh.shape\n    assert x_shape == y_shape or list(y_shape) == [1]\n\n    x_sh = x_sh.view(-1)\n    y_sh = y_sh.view(-1)\n\n    # Common Randomness\n    w_sh = _shares_of_zero(bit_len_max, L, dtype, crypto_provider, *workers)\n    s_sh = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n    u_sh = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n\n    ks = []\n    for i in range(bit_len_max - 1, -1, -1):\n        # 3)\n        z_sh = x_sh - u_sh - 2 ** i * y_sh + w_sh[i]\n\n        # 4)\n        beta_sh = relu_deriv(z_sh)\n\n        # 5)\n        v_sh = beta_sh * (2 ** i * y_sh)\n\n        # 6)\n        k_sh = beta_sh * 2 ** i\n        ks.append(k_sh)\n\n        # 7)\n        u_sh = u_sh + v_sh\n\n    # 9)\n    q = sum(ks) + s_sh\n\n    if len(x_shape):\n        return q.view(*x_shape)\n    else:\n        return q\n\n\ndef maxpool(x_sh):\n    """""" Compute MaxPool: returns fresh shares of the max value in the input tensor\n    and the index of this value in the flattened tensor\n\n    Args:\n        x_sh (AdditiveSharingTensor): the private tensor on which the op applies\n\n    Returns:\n        maximum value as an AdditiveSharingTensor\n        index of this value in the flattened tensor as an AdditiveSharingTensor\n    """"""\n    assert (\n        x_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n\n    if x_sh.is_wrapper:\n        x_sh = x_sh.child\n    workers = x_sh.locations\n    crypto_provider = x_sh.crypto_provider\n    L = x_sh.field\n    dtype = get_dtype(L)\n\n    x_sh = x_sh.contiguous().view(-1)\n\n    # Common Randomness\n    u_sh = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n    v_sh = _shares_of_zero(1, L, dtype, crypto_provider, *workers)\n\n    # 1)\n    max_sh = x_sh[0]\n    ind_sh = torch.tensor([0]).share(\n        *workers, field=L, dtype=dtype, crypto_provider=crypto_provider, **no_wrap\n    )  # I did not manage to create an AST with 0 and 0 as shares\n\n    for i in range(1, len(x_sh)):\n        # 3)\n        w_sh = x_sh[i] - max_sh\n\n        # 4)\n        beta_sh = relu_deriv(w_sh)\n\n        # 5)\n        max_sh = select_share(beta_sh, max_sh, x_sh[i])\n\n        # 6)\n        k = torch.tensor([i]).share(\n            *workers, field=L, dtype=dtype, crypto_provider=crypto_provider, **no_wrap\n        )  # I did not manage to create an AST with 0 and i as shares\n\n        # 7)\n        ind_sh = select_share(beta_sh, ind_sh, k)\n\n    return max_sh + u_sh, ind_sh + v_sh\n\n\ndef maxpool_deriv(x_sh):\n    """""" Compute derivative of MaxPool\n\n    Args:\n        x_sh (AdditiveSharingTensor): the private tensor on which the op applies\n\n    Returns:\n        an AdditiveSharingTensor of the same shape as x_sh full of zeros except for\n        a 1 at the position of the max value\n    """"""\n    assert (\n        x_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n\n    workers = x_sh.locations\n    crypto_provider = x_sh.crypto_provider\n    L = x_sh.field\n    dtype = get_dtype(L)\n    torch_dtype = get_torch_dtype(L)\n\n    n1, n2 = x_sh.shape\n    n = n1 * n2\n    assert L % n == 0\n    x_sh = x_sh.view(-1)\n\n    # Common Randomness\n    U_sh = _shares_of_zero(n, L, dtype, crypto_provider, *workers)\n\n    r = _random_common_value(L, *workers)\n\n    # 1)\n    _, ind_max_sh = maxpool(x_sh)\n\n    # 2)\n    j = sy.MultiPointerTensor(\n        children=[torch.tensor([int(i == 0)]).send(w, **no_wrap) for i, w in enumerate(workers)]\n    )\n    k_sh = ind_max_sh + j * r\n\n    # 3)\n    t = k_sh.get()\n    k = t % n\n    E_k = torch.zeros(n, dtype=torch_dtype)\n    E_k[k] = 1\n    E_sh = E_k.share(*workers, field=L, dtype=dtype, **no_wrap)\n\n    # 4)\n    g = r % n\n    D_sh = torch.roll(E_sh, -g)\n\n    maxpool_d_sh = D_sh + U_sh\n    return maxpool_d_sh.view(n1, n2)\n\n\ndef maxpool2d(a_sh, kernel_size: int = 1, stride: int = 1, padding: int = 0):\n    """"""Applies a 2D max pooling over an input signal composed of several input planes.\n    This interface is similar to torch.nn.MaxPool2D.\n    Args:\n        kernel_size: the size of the window to take a max over\n        stride: the stride of the window\n        padding: implicit zero padding to be added on both sides\n    """"""\n    assert (\n        a_sh.dtype != ""custom""\n    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""\n    assert len(a_sh.shape) == 4\n\n    # Change to tuple if not one\n    kernel = torch.nn.modules.utils._pair(kernel_size)\n    stride = torch.nn.modules.utils._pair(stride)\n    padding = torch.nn.modules.utils._pair(padding)\n\n    # TODO: support dilation.\n    dilation = torch.nn.modules.utils._pair(1)\n\n    # Extract a few useful values\n    batch_size, nb_channels, nb_rows_in, nb_cols_in = a_sh.shape\n\n    # Calculate output shapes\n    nb_rows_out = int(\n        (nb_rows_in + 2 * padding[0] - dilation[0] * (kernel[0] - 1) - 1) / stride[0] + 1\n    )\n    nb_cols_out = int(\n        (nb_cols_in + 2 * padding[1] - dilation[1] * (kernel[1] - 1) - 1) / stride[1] + 1\n    )\n\n    # Apply padding to the input\n    if padding != (0, 0):\n        a_sh = torch.nn.functional.pad(\n            a_sh, (padding[1], padding[1], padding[0], padding[0]), ""constant""\n        )\n        # Update shape after padding\n        nb_rows_in += 2 * padding[0]\n        nb_cols_in += 2 * padding[1]\n\n    res = []\n    # TODO: make this operation more efficient in order to be used with cnn modules.\n    for batch in range(batch_size):\n        for channel in range(nb_channels):\n            for r_in in range(0, nb_rows_in - (kernel[0] - 1), stride[0]):\n                for c_in in range(0, nb_cols_in - (kernel[1] - 1), stride[1]):\n                    m, _ = maxpool(\n                        a_sh[\n                            batch, channel, r_in : r_in + kernel[0], c_in : c_in + kernel[1],\n                        ].child\n                    )\n                    res.append(m.wrap())\n\n    res = torch.stack(res).reshape(batch_size, nb_channels, nb_rows_out, nb_cols_out)\n    return res\n'"
syft/frameworks/torch/mpc/spdz.py,3,"b'from typing import Callable\n\nimport torch\n\nimport syft as sy\nfrom syft.frameworks.torch.mpc.beaver import request_triple\nfrom syft.workers.abstract import AbstractWorker\n\nno_wrap = {""no_wrap"": True}\n\n\ndef spdz_mul(cmd: Callable, x_sh, y_sh, crypto_provider: AbstractWorker, field: int, dtype: str):\n    """"""Abstractly multiplies two tensors (mul or matmul)\n\n    Args:\n        cmd: a callable of the equation to be computed (mul or matmul)\n        x_sh (AdditiveSharingTensor): the left part of the operation\n        y_sh (AdditiveSharingTensor): the right part of the operation\n        crypto_provider (AbstractWorker): an AbstractWorker which is used to generate triples\n        field (int): an integer denoting the size of the field\n        dtype (str): denotes the dtype of shares\n\n    Return:\n        an AdditiveSharingTensor\n    """"""\n    assert isinstance(x_sh, sy.AdditiveSharingTensor)\n    assert isinstance(y_sh, sy.AdditiveSharingTensor)\n\n    locations = x_sh.locations\n    torch_dtype = x_sh.torch_dtype\n\n    # Get triples\n    a, b, a_mul_b = request_triple(\n        crypto_provider, cmd, field, dtype, x_sh.shape, y_sh.shape, locations\n    )\n\n    delta = x_sh - a\n    epsilon = y_sh - b\n    # Reconstruct and send to all workers\n    delta = delta.reconstruct()\n    epsilon = epsilon.reconstruct()\n\n    delta_epsilon = cmd(delta, epsilon)\n\n    # Trick to keep only one child in the MultiPointerTensor (like in SNN)\n    j1 = torch.ones(delta_epsilon.shape).type(torch_dtype).send(locations[0], **no_wrap)\n    j0 = torch.zeros(delta_epsilon.shape).type(torch_dtype).send(*locations[1:], **no_wrap)\n    if len(locations) == 2:\n        j = sy.MultiPointerTensor(children=[j1, j0])\n    else:\n        j = sy.MultiPointerTensor(children=[j1] + list(j0.child.values()))\n\n    delta_b = cmd(delta, b)\n    a_epsilon = cmd(a, epsilon)\n    res = delta_epsilon * j + delta_b + a_epsilon + a_mul_b\n    res = res.type(torch_dtype)\n    return res\n'"
syft/frameworks/torch/nn/__init__.py,15,"b'from syft.frameworks.torch.nn.conv import Conv2d\nfrom syft.frameworks.torch.nn.functional import conv2d\nfrom syft.frameworks.torch.nn.functional import maxpool2d\nfrom syft.frameworks.torch.nn.functional import avgpool2d\nfrom syft.frameworks.torch.nn.functional import dropout\nfrom syft.frameworks.torch.nn.functional import linear\nfrom syft.frameworks.torch.nn.pool import AvgPool2d\nfrom syft.frameworks.torch.nn.rnn import GRU\nfrom syft.frameworks.torch.nn.rnn import GRUCell\nfrom syft.frameworks.torch.nn.rnn import LSTM\nfrom syft.frameworks.torch.nn.rnn import LSTMCell\nfrom syft.frameworks.torch.nn.rnn import RNN\nfrom syft.frameworks.torch.nn.rnn import RNNBase\nfrom syft.frameworks.torch.nn.rnn import RNNCell\nfrom syft.frameworks.torch.nn.rnn import RNNCellBase\nfrom syft.generic.frameworks.overload import overloaded\n\n\n@overloaded.module\ndef nn(module):\n    """"""\n    The syntax is the same, so @overloaded.module handles recursion\n    Note that we don\'t need to add the @staticmethod decorator\n    """"""\n\n    @overloaded.module\n    def functional(module):\n        module.conv2d = conv2d\n        module.dropout = dropout\n        module.linear = linear\n        module.max_pool2d = maxpool2d\n        module.avg_pool2d = avgpool2d\n\n    module.functional = functional\n\n    module.Conv2d = Conv2d\n    module.AvgPool2d = AvgPool2d\n    module.GRU = GRU\n    module.GRUCell = GRUCell\n    module.LSTM = LSTM\n    module.LSTMCell = LSTMCell\n    module.RNN = RNN\n    module.RNNBase = RNNBase\n    module.RNNCell = RNNCell\n    module.RNNCellBase = RNNCellBase\n'"
syft/frameworks/torch/nn/conv.py,8,"b'import torch as th\nimport torch.nn as nn\n\nfrom syft.frameworks.torch.nn.functional import conv2d\n\n\nclass Conv2d(nn.Module):\n    """"""\n    This class tries to be an exact python port of the torch.nn.Conv2d\n    module. Because PySyft cannot hook into layers which are implemented in C++,\n    our special functionalities (such as encrypted computation) do not work with\n    torch.nn.Conv2d and so we must have python ports available for all layer types\n    which we seek to use.\n\n    Note: This module is tested to ensure that it outputs the exact output\n    values that the main module outputs in the same order that the main module does.\n\n    This module has not yet been tested with GPUs but should work out of the box.\n    """"""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=False,\n        padding_mode=""zeros"",\n    ):\n        """"""For information on the constructor arguments, please see PyTorch\'s\n        documentation in torch.nn.Conv2d""""""\n\n        super().__init__()\n\n        # temp_init to get weights and bias\n        temp_init = th.nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n            padding_mode=padding_mode,\n        )\n\n        self.weight = th.Tensor(temp_init.weight).fix_prec()\n        if bias:\n            self.bias = th.Tensor(temp_init.bias).fix_prec()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n\n        # These are modified and converted to tuples\n        self.stride = temp_init.stride\n        self.padding = temp_init.padding\n        self.dilation = temp_init.dilation\n\n        self.groups = groups\n        self.padding_mode = padding_mode\n\n    def forward(self, input):\n\n        assert input.shape[1] == self.in_channels\n\n        return conv2d(\n            input, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups\n        )\n\n\n# IMPLEMENTED BY @IAMTRASK IN https://github.com/OpenMined/PySyft/pull/2896\n\n# class Conv2d(Module):\n#     """"""\n#     This class is the beginning of an exact python port of the torch.nn.Conv2d\n#     module. Because PySyft cannot hook into layers which are implemented in C++,\n#     our special functionalities (such as encrypted computation) do not work with\n#     torch.nn.Conv2d and so we must have python ports available for all layer types\n#     which we seek to use.\n#\n#     Note that this module has been tested to ensure that it outputs the exact output\n#     values that the main module outputs in the same order that the main module does.\n#\n#     However, there is often some rounding error of unknown origin, usually less than\n#     1e-6 in magnitude.\n#\n#     This module has not yet been tested with GPUs but should work out of the box.\n#     """"""\n#\n#     def __init__(\n#         self,\n#         in_channels,\n#         out_channels,\n#         kernel_size,\n#         stride=1,\n#         padding=0,\n#         dilation=1,\n#         groups=1,\n#         bias=False,\n#         padding_mode=""zeros"",\n#     ):\n#         """"""For information on the constructor arguments, please see PyTorch\'s\n#         documentation in torch.nn.Conv2d""""""\n#\n#         super().__init__()\n#\n#         # because my particular experiment does not demand full functionality of\n#         # a convolutional layer, I will only implement the basic functionality.\n#         # These assertions are the required settings.\n#\n#         assert in_channels == 1\n#         assert stride == 1\n#         assert padding == 0\n#         assert dilation == 1\n#         assert groups == 1\n#         assert padding_mode == ""zeros""\n#\n#         self.in_channels = in_channels\n#         self.out_channels = out_channels\n#         self.kernel_size = kernel_size\n#         self.stride = stride\n#         self.padding = padding\n#         self.dilation = dilation\n#         self.groups = groups\n#         self.has_bias = bias\n#         self.padding_mode = padding_mode\n#\n#         temp_init = th.nn.Conv2d(\n#             in_channels=self.in_channels,\n#             out_channels=self.out_channels,\n#             kernel_size=self.kernel_size,\n#             stride=self.stride,\n#             padding=self.padding,\n#             dilation=self.dilation,\n#             groups=self.groups,\n#             bias=self.has_bias,\n#             padding_mode=self.padding_mode,\n#         )\n#\n#         self.weight = temp_init.weight\n#         self.bias = temp_init.bias\n#\n#     def forward(self, data):\n#\n#         batch_size, _, rows, cols = data.shape\n#\n#         flattened_model = self.weight.reshape(self.out_channels, -1)\n#         flattened_data = th.nn.functional.unfold(data, kernel_size=self.kernel_size)\n#\n#         # Loop over batch as direct multiplication results in rounding errors\n#         kernel_results = list()\n#         for n in range(0, batch_size):\n#             kernel_results.append(flattened_model @ flattened_data[n])\n#\n#         pred = th.stack(kernel_results, axis=0).view(\n#             batch_size, self.out_channels,\n#             rows - self.kernel_size + 1, cols - self.kernel_size + 1\n#         )\n#\n#         if self.has_bias:\n#             pred = pred + self.bias.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand(\n#                 batch_size,\n#                 self.out_channels,\n#                 rows - self.kernel_size + 1,\n#                 cols - self.kernel_size + 1,\n#             )\n#\n#         return pred\n'"
syft/frameworks/torch/nn/functional.py,18,"b'import torch\n\n\ndef linear(*args):\n    """"""\n    Un-hook the function to have its detailed behaviour\n    """"""\n    return torch.nn.functional.native_linear(*args)\n\n\ndef dropout(input, p=0.5, training=True, inplace=False):\n    """"""\n    Args:\n        p: probability of an element to be zeroed. Default: 0.5\n        training: If training, cause dropout layers are not used during evaluation of model\n        inplace: If set to True, will do this operation in-place. Default: False\n    """"""\n\n    if training:\n        binomial = torch.distributions.binomial.Binomial(probs=1 - p)\n\n        # we must convert the normal tensor to fixed precision before multiplication\n        # Note that: Weights of a model are alwasy Float values\n        # Hence input will always be of type FixedPrecisionTensor > ...\n        noise = (binomial.sample(input.shape).type(torch.FloatTensor) * (1.0 / (1.0 - p))).fix_prec(\n            **input.get_class_attributes(), no_wrap=True\n        )\n\n        if inplace:\n            input = input * noise\n            return input\n\n        return input * noise\n\n    return input\n\n\ndef conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):  # noqa: C901\n    """"""\n    Overloads torch.nn.functional.conv2d to be able to use MPC on convolutional networks.\n    The idea is to build new tensors from input and weight to compute a\n    matrix multiplication equivalent to the convolution.\n    Args:\n        input: input image\n        weight: convolution kernels\n        bias: optional additive bias\n        stride: stride of the convolution kernels\n        padding:  implicit paddings on both sides of the input.\n        dilation: spacing between kernel elements\n        groups: split input into groups, in_channels should be divisible by the number of groups\n    Returns:\n        the result of the convolution (FixedPrecision Tensor)\n    """"""\n\n    assert len(input.shape) == 4\n    assert len(weight.shape) == 4\n\n    # Change to tuple if not one\n    stride = torch.nn.modules.utils._pair(stride)\n    padding = torch.nn.modules.utils._pair(padding)\n    dilation = torch.nn.modules.utils._pair(dilation)\n\n    # Extract a few useful values\n    batch_size, nb_channels_in, nb_rows_in, nb_cols_in = input.shape\n    nb_channels_out, nb_channels_kernel, nb_rows_kernel, nb_cols_kernel = weight.shape\n\n    if bias is not None:\n        assert len(bias) == nb_channels_out\n\n    # Check if inputs are coherent\n    assert nb_channels_in == nb_channels_kernel * groups\n    assert nb_channels_in % groups == 0\n    assert nb_channels_out % groups == 0\n\n    # Compute output shape\n    nb_rows_out = int(\n        ((nb_rows_in + 2 * padding[0] - dilation[0] * (nb_rows_kernel - 1) - 1) / stride[0]) + 1\n    )\n    nb_cols_out = int(\n        ((nb_cols_in + 2 * padding[1] - dilation[1] * (nb_cols_kernel - 1) - 1) / stride[1]) + 1\n    )\n\n    # Apply padding to the input\n    if padding != (0, 0):\n        padding_mode = ""constant""\n        input = torch.nn.functional.pad(\n            input, (padding[1], padding[1], padding[0], padding[0]), padding_mode\n        )\n        # Update shape after padding\n        nb_rows_in += 2 * padding[0]\n        nb_cols_in += 2 * padding[1]\n\n    # We want to get relative positions of values in the input tensor that are used\n    # by one filter convolution.\n    # It basically is the position of the values used for the top left convolution.\n    pattern_ind = []\n    for ch in range(nb_channels_in):\n        for r in range(nb_rows_kernel):\n            for c in range(nb_cols_kernel):\n                pixel = r * nb_cols_in * dilation[0] + c * dilation[1]\n                pattern_ind.append(pixel + ch * nb_rows_in * nb_cols_in)\n\n    # The image tensor is reshaped for the matrix multiplication:\n    # on each row of the new tensor will be the input values used for each filter convolution\n    # We will get a matrix [[in values to compute out value 0],\n    #                       [in values to compute out value 1],\n    #                       ...\n    #                       [in values to compute out value nb_rows_out*nb_cols_out]]\n    im_flat = input.view(batch_size, -1)\n    im_reshaped = []\n    for cur_row_out in range(nb_rows_out):\n        for cur_col_out in range(nb_cols_out):\n            # For each new output value, we just need to shift the receptive field\n            offset = cur_row_out * stride[0] * nb_cols_in + cur_col_out * stride[1]\n            tmp = [ind + offset for ind in pattern_ind]\n            im_reshaped.append(im_flat[:, tmp])\n    im_reshaped = torch.stack(im_reshaped).permute(1, 0, 2)\n\n    # The convolution kernels are also reshaped for the matrix multiplication\n    # We will get a matrix [[weights for out channel 0],\n    #                       [weights for out channel 1],\n    #                       ...\n    #                       [weights for out channel nb_channels_out]].TRANSPOSE()\n    weight_reshaped = weight.view(nb_channels_out // groups, -1).t()\n\n    # Now that everything is set up, we can compute the result\n    if groups > 1:\n        res = []\n        chunks_im = torch.chunk(im_reshaped, groups, dim=2)\n        chunks_weights = torch.chunk(weight_reshaped, groups, dim=0)\n        for g in range(groups):\n            tmp = chunks_im[g].matmul(chunks_weights[g])\n            res.append(tmp)\n        res = torch.cat(res, dim=2)\n    else:\n        res = im_reshaped.matmul(weight_reshaped)\n\n    # Add a bias if needed\n    if bias is not None:\n        if bias.is_wrapper and res.is_wrapper:\n            res += bias\n        elif bias.is_wrapper:\n            res += bias.child\n        else:\n            res += bias\n\n    # ... And reshape it back to an image\n    res = (\n        res.permute(0, 2, 1)\n        .view(batch_size, nb_channels_out, nb_rows_out, nb_cols_out)\n        .contiguous()\n    )\n    return res\n\n\ndef _pool(tensor, kernel_size: int = 2, stride: int = 2, mode=""max""):\n    output_shape = (\n        (tensor.shape[0] - kernel_size) // stride + 1,\n        (tensor.shape[1] - kernel_size) // stride + 1,\n    )\n    kernel_size = (kernel_size, kernel_size)\n    b = torch.ones(tensor.shape)  # when torch.Tensor.stride() is supported: replace with A.stride()\n    a_strides = b.stride()\n    a_w = torch.as_strided(\n        tensor,\n        size=output_shape + kernel_size,\n        stride=(stride * a_strides[0], stride * a_strides[1]) + a_strides,\n    )\n    a_w = a_w.reshape(-1, *kernel_size)\n    result = []\n    if mode == ""max"":\n        for channel in range(a_w.shape[0]):\n            result.append(a_w[channel].max())\n    elif mode == ""mean"":\n        for channel in range(a_w.shape[0]):\n            result.append(torch.mean(a_w[channel]))\n    else:\n        raise ValueError(""unknown pooling mode"")\n\n    result = torch.stack(result).reshape(output_shape)\n    return result\n\n\ndef pool2d(tensor, kernel_size: int = 2, stride: int = 2, mode=""max""):\n    assert len(tensor.shape) < 5\n    if len(tensor.shape) == 2:\n        return _pool(tensor, kernel_size, stride, mode)\n    if len(tensor.shape) == 3:\n        return torch.squeeze(pool2d(torch.unsqueeze(tensor, dim=0), kernel_size, stride, mode))\n    batches = tensor.shape[0]\n    channels = tensor.shape[1]\n    out_shape = (\n        batches,\n        channels,\n        (tensor.shape[2] - kernel_size) // stride + 1,\n        (tensor.shape[3] - kernel_size) // stride + 1,\n    )\n    result = []\n    for batch in range(batches):\n        for channel in range(channels):\n            result.append(_pool(tensor[batch][channel], kernel_size, stride, mode))\n    result = torch.stack(result).reshape(out_shape)\n    return result\n\n\ndef maxpool2d(tensor, kernel_size: int = 2, stride: int = 2):\n    return pool2d(tensor, kernel_size, stride)\n\n\ndef avgpool2d(tensor, kernel_size: int = 2, stride: int = 2):\n    return pool2d(tensor, kernel_size, stride, mode=""mean"")\n'"
syft/frameworks/torch/nn/pool.py,5,"b'import torch as th\nfrom torch.nn import Module\n\n\nclass AvgPool2d(Module):\n    """"""\n    This class is the beginning of an exact python port of the torch.nn.AvgPool2d\n    module. Because PySyft cannot hook into layers which are implemented in C++,\n    our special functionalities (such as encrypted computation) do not work with\n    torch.nn.AvgPool2d and so we must have python ports available for all layer types\n    which we seek to use.\n\n    Note that this module has been tested to ensure that it outputs the exact output\n    values that the main module outputs in the same order that the main module does.\n\n    However, there is often some rounding error of unknown origin, usually less than\n    1e-6 in magnitude.\n\n    This module has not yet been tested with GPUs but should work out of the box.\n    """"""\n\n    def __init__(\n        self,\n        kernel_size,\n        stride=None,\n        padding=0,\n        ceil_mode=False,\n        count_include_pad=True,\n        divisor_override=None,\n    ):\n        """"""For information on the constructor arguments, please see PyTorch\'s\n        documentation in torch.nn.AvgPool2d""""""\n\n        super().__init__()\n\n        # I have not implemented all functionality from torch.nn.AvgPool2d\n        # These assertions are the required settings.\n\n        assert padding == 0\n        assert ceil_mode is False\n        assert count_include_pad is True\n        assert divisor_override is None\n\n        if stride is None:\n            stride = kernel_size\n\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.ceil_mode = ceil_mode\n        self.count_include_pad = count_include_pad\n        self.divisor_override = divisor_override\n\n        self._one_over_kernel_size = 1 / (self.kernel_size * self.kernel_size)\n\n    def forward(self, data):\n\n        batch_size, out_channels, rows, cols = data.shape\n\n        kernel_results = []\n\n        for i in range(0, rows - self.kernel_size + 1, self.stride):\n            for j in range(0, cols - self.kernel_size + 1, self.stride):\n                kernel_out = (\n                    data[:, :, i : i + self.kernel_size, j : j + self.kernel_size].sum((2, 3))\n                    * self._one_over_kernel_size\n                )\n                kernel_results.append(kernel_out.unsqueeze(2))\n\n        pred = th.cat(kernel_results, axis=2).view(\n            batch_size, out_channels, int(rows / self.stride), int(cols / self.stride)\n        )\n\n        return pred\n'"
syft/frameworks/torch/nn/rnn.py,29,"b'import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import init\n\n\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\nfrom syft.frameworks.torch.tensors.interpreters import precision\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\n\n\nclass RNNCellBase(nn.Module):\n    """"""\n    Cell to be used as base for all RNN cells, including GRU and LSTM\n    This class overrides the torch.nn.RNNCellBase\n    Only Linear and Dropout layers are used to be able to use MPC\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias, num_chunks, nonlinearity=None):\n        super(RNNCellBase, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.num_chunks = num_chunks\n        self.nonlinearity = nonlinearity\n        self.fc_xh = nn.Linear(input_size, self.num_chunks * hidden_size, bias=bias)\n        self.fc_hh = nn.Linear(hidden_size, self.num_chunks * hidden_size, bias=bias)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""\n        This method initializes or reset all the parameters of the cell.\n        The paramaters are initiated following a uniform distribution.\n        """"""\n        std = 1.0 / np.sqrt(self.hidden_size)\n        for w in self.parameters():\n            init.uniform_(w, -std, std)\n\n    def init_hidden(self, input):\n        """"""\n        This method initializes a hidden state when no hidden state is provided\n        in the forward method. It creates a hidden state with zero values.\n        """"""\n        h = torch.zeros(input.shape[0], self.hidden_size, dtype=input.dtype, device=input.device)\n        if input.has_child() and isinstance(input.child, PointerTensor):\n            h = h.send(input.child.location)\n        if input.has_child() and isinstance(input.child, precision.FixedPrecisionTensor):\n            h = h.fix_precision()\n            child = input.child\n            if isinstance(child.child, AdditiveSharingTensor):\n                crypto_provider = child.child.crypto_provider\n                owners = child.child.locations\n                h = h.share(*owners, crypto_provider=crypto_provider)\n        return h\n\n\nclass RNNCell(RNNCellBase):\n    """"""\n    Python implementation of RNNCell with tanh or relu non-linearity for MPC\n    This class overrides the torch.nn.RNNCell\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=""tanh""):\n        super(RNNCell, self).__init__(input_size, hidden_size, bias, num_chunks=1)\n\n        if nonlinearity == ""tanh"":\n            self.nonlinearity = torch.tanh\n        elif nonlinearity == ""relu"":\n            self.nonlinearity = torch.relu\n        else:\n            raise ValueError(f""Unknown nonlinearity: {nonlinearity}"")\n\n    def forward(self, x, h=None):\n\n        if h is None:\n            h = self.init_hidden(x)\n        h_ = self.nonlinearity(self.fc_xh(x) + self.fc_hh(h))\n\n        return h_\n\n\nclass GRUCell(RNNCellBase):\n    """"""\n    Python implementation of GRUCell for MPC\n    This class overrides the torch.nn.GRUCell\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=None):\n        super(GRUCell, self).__init__(input_size, hidden_size, bias, num_chunks=3)\n\n    def forward(self, x, h=None):\n\n        if h is None:\n            h = self.init_hidden(x)\n\n        gate_x = self.fc_xh(x)\n        gate_h = self.fc_hh(h)\n        x_r, x_z, x_n = gate_x.chunk(self.num_chunks, 1)\n        h_r, h_z, h_n = gate_h.chunk(self.num_chunks, 1)\n\n        resetgate = torch.sigmoid(x_r + h_r)\n        updategate = torch.sigmoid(x_z + h_z)\n        newgate = torch.tanh(x_n + (resetgate * h_n))\n\n        h_ = newgate + updategate * (h - newgate)\n\n        return h_\n\n\nclass LSTMCell(RNNCellBase):\n    """"""\n    Python implementation of LSTMCell for MPC\n    This class overrides the torch.nn.LSTMCell\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=None):\n        super(LSTMCell, self).__init__(input_size, hidden_size, bias, num_chunks=4)\n\n    def reset_parameters(self):\n        super(LSTMCell, self).reset_parameters()\n\n        # Bias of forget gate should be initialize with 1 or 2\n        # Ref: http://proceedings.mlr.press/v37/jozefowicz15.pdf\n        incr_bias = 1.0 / self.hidden_size\n        init.constant_(self.fc_xh.bias[self.hidden_size : 2 * self.hidden_size], incr_bias)\n        init.constant_(self.fc_hh.bias[self.hidden_size : 2 * self.hidden_size], incr_bias)\n\n    def forward(self, x, hc=None):\n\n        if hc is None:\n            hc = (self.init_hidden(x), self.init_hidden(x))\n        h, c = hc\n\n        gate_x = self.fc_xh(x)\n        gate_h = self.fc_hh(h)\n\n        x_i, x_f, x_c, x_o = gate_x.chunk(self.num_chunks, 1)\n        h_i, h_f, h_c, h_o = gate_h.chunk(self.num_chunks, 1)\n\n        inputgate = torch.sigmoid(x_i + h_i)\n        forgetgate = torch.sigmoid(x_f + h_f)\n        cellgate = torch.tanh(x_c + h_c)\n        outputgate = torch.sigmoid(x_o + h_o)\n\n        c_ = torch.mul(forgetgate, c) + torch.mul(inputgate, cellgate)\n\n        h_ = torch.mul(outputgate, torch.tanh(c_))\n\n        return h_, c_\n\n\nclass RNNBase(nn.Module):\n    """"""\n    Module to be used as base for all RNN modules, including GRU and LSTM\n    This class overrides the torch.nn.RNNBase\n    Only Linear and Dropout layers are used to be able to use MPC\n    """"""\n\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        num_layers,\n        bias,\n        batch_first,\n        dropout,\n        bidirectional,\n        base_cell,\n        nonlinearity=None,\n    ):\n        super(RNNBase, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.dropout = float(dropout)\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n        self.is_lstm = base_cell is LSTMCell\n        self.nonlinearity = nonlinearity\n\n        # Dropout layers\n        # TODO: implement a nn.Dropout class for PySyft\n        # Link to issue: https://github.com/OpenMined/PySyft/issues/2500\n\n        # Build RNN forward layers\n        sizes = [input_size, *(hidden_size for _ in range(self.num_layers - 1))]\n        self.rnn_forward = nn.ModuleList(\n            (base_cell(sz, hidden_size, bias, nonlinearity) for sz in sizes)\n        )\n\n        # Build RNN backward layers, if needed\n        if self.bidirectional:\n            self.rnn_backward = nn.ModuleList(\n                (base_cell(sz, hidden_size, bias, nonlinearity) for sz in sizes)\n            )\n\n    def forward(self, x, hc=None):  # noqa: C901\n        # If batch_first == True, swap batch with seq_len\n        # At the end of the procedure we swap it back to the original structure\n        if self.batch_first:\n            x = x.transpose(0, 1)\n\n        # If hc is not None, hc is either a Tensor (RNNCell or GRUCell hidden state),\n        #   or a 2-tuple of Tensors (LSTMCell hidden and cell states).\n        # For convenience, we make hc always listy so that:\n        #   hc[0] is the hidden state\n        #   hc[1] if it exists, is the cell state\n        # At the end of the procedure, we swap it back to the original structure\n        if hc is None:\n            # Initialize hc\n            hc = [self._init_hidden(x) for _ in range(2 if self.is_lstm else 1)]\n        else:\n            # Standardize hc per comment above\n            if not self.is_lstm:\n                hc = [hc]\n\n            # As we did to x above, we swap back at the end of the procedure\n            if self.batch_first:\n                hc = [item.transpose(0, 1) for item in hc]\n\n        batch_size = x.shape[1]\n        seq_len = x.shape[0]\n\n        # If bidirectional==True, split states in two, one for each direction\n        if self.bidirectional:\n            hc = [\n                item.contiguous().view(self.num_layers, 2, batch_size, self.hidden_size)\n                for item in hc\n            ]\n            hc_fwd = [item[:, 0, :, :] for item in hc]\n            hc_back = [item[:, 1, :, :] for item in hc]\n        else:\n            hc_fwd = hc\n\n        # Run through rnn in the forward direction\n        output = x.new(seq_len, batch_size, self.hidden_size).zero_()\n        for t in range(seq_len):\n            hc_fwd = self._apply_time_step(x, hc_fwd, t)\n            output[t, :, :] = hc_fwd[0][-1, :, :]\n\n        # Run through rnn in the backward direction if bidirectional==True\n        if self.bidirectional:\n            output_back = x.new(seq_len, batch_size, self.hidden_size).zero_()\n            for t in range(seq_len - 1, -1, -1):\n                hc_back = self._apply_time_step(x, hc_back, t, reverse_direction=True)\n                output_back[t, :, :] = hc_back[0][-1, :, :]\n\n            # Concatenate both directions\n            output = torch.cat((output, output_back), dim=-1)\n            hidden = [\n                torch.cat((hid_item, back_item), dim=0)\n                for hid_item, back_item in zip(hc_fwd, hc_back)\n            ]\n        else:\n            hidden = hc_fwd\n\n        # If batch_first == True, swap axis back to get original structure\n        if self.batch_first:\n            output = output.transpose(0, 1)\n            hidden = [item.transpose(0, 1) for item in hidden]\n\n        # Reshape hidden to the original shape of hc\n        hidden = tuple(hidden) if self.is_lstm else hidden[0]\n\n        return output, hidden\n\n    def _init_hidden(self, input):\n        """"""\n        This method initializes a hidden state when no hidden state is provided\n        in the forward method. It creates a hidden state with zero values for each\n        layer of the network.\n        """"""\n        h = torch.zeros(\n            self.num_layers * self.num_directions,\n            input.shape[1],\n            self.hidden_size,\n            dtype=input.dtype,\n            device=input.device,\n        )\n        if input.has_child() and isinstance(input.child, PointerTensor):\n            h = h.send(input.child.location)\n        if input.has_child() and isinstance(input.child, precision.FixedPrecisionTensor):\n            h = h.fix_precision()\n            child = input.child\n            if isinstance(child.child, AdditiveSharingTensor):\n                crypto_provider = child.child.crypto_provider\n                owners = child.child.locations\n                h = h.share(*owners, crypto_provider=crypto_provider)\n        return h\n\n    def _apply_time_step(self, x, hc, t, reverse_direction=False):\n        """"""\n        Apply RNN layers at time t, given input and previous hidden states\n        """"""\n        rnn_layers = self.rnn_backward if reverse_direction else self.rnn_forward\n\n        hc = torch.stack([*hc])\n        hc_next = torch.zeros_like(hc)\n\n        for layer in range(self.num_layers):\n            inp = x[t, :, :] if layer == 0 else hc_next[0][layer - 1, :, :].clone()\n\n            if self.is_lstm:\n                hc_next[:, layer, :, :] = torch.stack(rnn_layers[layer](inp, hc[:, layer, :, :]))\n            else:\n                hc_next[0][layer, :, :] = rnn_layers[layer](inp, hc[0][layer, :, :])\n\n        return hc_next\n\n\nclass RNN(RNNBase):\n    """"""\n    Python implementation of RNN for MPC\n    This class overrides the torch.nn.RNN\n    """"""\n\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        num_layers=1,\n        nonlinearity=""tanh"",\n        bias=True,\n        batch_first=False,\n        dropout=0,\n        bidirectional=False,\n    ):\n\n        super(RNN, self).__init__(\n            input_size,\n            hidden_size,\n            num_layers,\n            bias,\n            batch_first,\n            dropout,\n            bidirectional,\n            RNNCell,\n            nonlinearity,\n        )\n\n\nclass GRU(RNNBase):\n    """"""\n    Python implementation of GRU for MPC\n    This class overrides the torch.nn.GRU\n    """"""\n\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        num_layers=1,\n        bias=True,\n        batch_first=False,\n        dropout=0,\n        bidirectional=False,\n    ):\n\n        super(GRU, self).__init__(\n            input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, GRUCell\n        )\n\n\nclass LSTM(RNNBase):\n    """"""\n    Python implementation of LSTM for MPC\n    This class overrides the torch.nn.LSTM\n    """"""\n\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        num_layers=1,\n        bias=True,\n        batch_first=False,\n        dropout=0,\n        bidirectional=False,\n    ):\n\n        super(LSTM, self).__init__(\n            input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, LSTMCell\n        )\n'"
syft/frameworks/torch/tensors/__init__.py,0,b''
syft/generic/frameworks/hook/__init__.py,0,b''
syft/generic/frameworks/hook/hook.py,1,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom functools import wraps\nimport types\nfrom typing import List\n\nimport syft\nfrom syft.generic.frameworks.hook import hook_args\n\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.workers.base import BaseWorker\n\nfrom syft.exceptions import route_method_exception\n\n\nfrom syft.generic.frameworks.hook.pointers import PointerHook\nfrom syft.generic.frameworks.hook.string import StringHook\nfrom syft.generic.frameworks.hook.tensors import TensorHook\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\n\n\nclass FrameworkHook(TensorHook, PointerHook, StringHook, ABC):\n    """"""Composite hook for ALL THE FRAMEWORK THINGS that must be overloaded and/or modified""""""\n\n    @abstractmethod\n    def __init__(self, framework_module, local_worker: BaseWorker = None, is_client: bool = True):\n        pass\n\n    boolean_comparators = [""__gt__"", ""__ge__"", ""__lt__"", ""__le__""]\n\n    ### Public API: framework-specific factory methods ###\n    @classmethod\n    @abstractmethod\n    def create_shape(cls, shape_dims):\n        """"""Factory method for creating a generic FrameworkShape.""""""\n        pass\n\n    @classmethod\n    @abstractmethod\n    def create_zeros(cls, shape, dtype, **kwargs):\n        """"""Factory method for creating a generic zero FrameworkTensor.""""""\n        pass\n\n    @classmethod\n    def create_wrapper(cls, wrapper_type, *args, **kwargs):\n        """"""Factory method for creating a generic wrapper of type wrapper_type.""""""\n        if wrapper_type is None:\n            wrapper_type = syft.framework.Tensor\n\n        return wrapper_type(*args, **kwargs)\n\n    @classmethod\n    def _transfer_methods_to_framework_class(\n        hook_cls, framework_cls: type, from_cls: type, exclude: List[str]\n    ):\n        """"""Adds methods from the from_cls class to the framework_cls class.\n\n        The class from_cls is a proxy class useful to avoid extending\n        the native framework class directly.\n\n        Args:\n            framework_cls: The class to which we are adding methods, e.g.\n                torch.Tensor or tf.Variable.\n            from_cls: The class from which we are adding methods, e.g.\n                TorchTensor, or TensorFlowVariable.\n            exclude: A list of method names to exclude from the hooking process.\n        """"""\n        # For all methods defined in syft_type which are not internal methods\n        # (like __class__, etc)\n        for attr in dir(from_cls):\n            if attr not in exclude:\n                if hasattr(framework_cls, attr):\n                    setattr(framework_cls, f""native_{attr}"", getattr(framework_cls, attr))\n                # Add to the native tensor this method\n                setattr(framework_cls, attr, getattr(from_cls, attr))\n\n    @classmethod\n    def _perform_function_overloading(cls, parent_module_name, parent_module, func_name):\n\n        # Where the overloading happens\n        # 1. Get native function\n        native_func = getattr(parent_module, func_name)\n        # 2. Check it is a proper function\n        if type(native_func) in [types.FunctionType, types.BuiltinFunctionType]:\n            # 3. Build the hooked function\n            new_func = cls._get_hooked_func(parent_module_name, func_name, native_func)\n            # 4. Move the native function\n            setattr(parent_module, f""native_{func_name}"", native_func)\n            # 5. Put instead the hooked one\n            setattr(parent_module, func_name, new_func)\n\n    @classmethod\n    def _get_hooked_syft_method(cls, attr):\n        """"""\n        Hook a method in order to replace all args/kwargs syft/torch tensors with\n        their child attribute, forward this method with the new args and new self,\n        get response and ""rebuild"" the syft tensor wrapper upon all tensors found\n\n        Args:\n            attr (str): the method to hook\n        Return:\n            the hooked method\n        """"""\n\n        @wraps(attr)\n        def overloaded_syft_method(self, *args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n            # Replace all syft tensor with their child attribute\n            new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                attr, self, args, kwargs\n            )\n\n            # Send it to the appropriate class and get the response\n            response = getattr(new_self, attr)(*new_args, **new_kwargs)\n\n            # Put back SyftTensor on the tensors found in the response\n            response = hook_args.hook_response(\n                attr, response, wrap_type=type(self), wrap_args=self.get_class_attributes()\n            )\n\n            return response\n\n        return overloaded_syft_method\n\n    @classmethod\n    def _get_hooked_method(cls, tensor_type, method_name):\n        """"""\n        Hook a method in order to replace all args/kwargs syft/torch tensors with\n        their child attribute if they exist\n        If so, forward this method with the new args and new self, get response\n        and ""rebuild"" the torch tensor wrapper upon all tensors found\n        If not, just execute the native torch methodn\n\n        Args:\n            attr (str): the method to hook\n        Return:\n            the hooked method\n        """"""\n\n        @wraps(getattr(tensor_type, method_name))\n        def overloaded_native_method(self, *args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n\n            if not hasattr(self, ""child""):  # means that it\'s not a wrapper\n\n                # if self is a natural tensor but the first argument isn\'t,\n                # wrap self with the appropriate type and re-run\n                if len(args) > 0 and hasattr(args[0], ""child""):\n\n                    # if we allow this for PointerTensors it opens the potential\n                    # that we could accidentally serialize and send a tensor in the\n                    # arguments\n                    if not isinstance(args[0].child, PointerTensor):\n                        self = type(args[0].child)().on(self, wrap=True)\n                        args = [args[0]]\n                        return overloaded_native_method(self, *args, **kwargs)\n\n                method = getattr(self, f""native_{method_name}"")\n                # Run the native function with the new args\n\n                try:\n                    response = method(*args, **kwargs)\n\n                except BaseException as e:\n                    # we can make some errors more descriptive with this method\n                    raise route_method_exception(e, self, args, kwargs)\n\n            else:  # means that there is a wrapper to remove\n\n                try:\n                    # Replace all torch tensor with their child attribute\n                    new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                        method_name, self, args, kwargs\n                    )\n\n                except BaseException as e:  # if there\'s a type mismatch, try to fix it!\n\n                    try:\n                        # if the first argument has no child (meaning it\'s probably raw data),\n                        # try wrapping it with the type of self. We have to except PointerTensor\n                        # because otherwise it can lead to inadvertently sending data to another\n                        # machine\n                        if not hasattr(args[0], ""child"") and not isinstance(\n                            self.child, PointerTensor\n                        ):\n                            # TODO: add check to make sure this isn\'t getting around\n                            # a security class\n\n                            _args = []\n                            _args.append(type(self)().on(args[0], wrap=False))\n                            for a in args[1:]:\n                                _args.append(a)\n\n                            args = _args\n\n                        # Replace all torch tensor with their child attribute\n                        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                            method_name, self, args, kwargs\n                        )\n                    except BaseException as e:\n                        # we can make some errors more descriptive with this method\n                        raise route_method_exception(e, self, args, kwargs)\n\n                # Send the new command to the appropriate class and get the response\n                method = getattr(new_self, method_name)\n                response = method(*new_args, **new_kwargs)\n\n                # For inplace methods, just directly return self\n                if syft.framework.is_inplace_method(method_name):\n                    return self\n\n                # Put back the wrappers where needed\n                response = hook_args.hook_response(\n                    method_name,\n                    response,\n                    wrap_type=type(self),\n                    new_self=self,\n                    wrap_args=self.get_class_attributes(),\n                )\n\n            return response\n\n        return overloaded_native_method\n\n    @classmethod\n    def _get_hooked_private_method(cls, method_name):\n        """"""\n        Hook a method in order to replace all args/kwargs syft/torch tensors with\n        their child attribute if they exist\n        If so, forward this method with the new args and new self, get response\n        and ""rebuild"" the torch tensor wrapper upon all tensors found\n        If not, just execute the native torch methodn\n\n        Args:\n            attr (str): the method to hook\n        Return:\n            the hooked method\n        """"""\n\n        @wraps(method_name)\n        def overloaded_native_method(self, *args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n            if not hasattr(self, ""child""):  # means that it\'s not a wrapper\n                method = getattr(self, f""native_{method_name}"")\n                # Run the native function with the new args\n\n                try:\n                    response = method(*args, **kwargs)\n                except BaseException as e:\n                    # we can make some errors more descriptive with this method\n                    raise route_method_exception(e, self, args, kwargs)\n\n            else:  # means that there is a wrapper to remove\n                try:\n                    # Replace all torch tensor with their child attribute\n                    new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                        method_name, self, args, kwargs\n                    )\n                except BaseException as e:\n                    # we can make some errors more descriptive with this method\n                    raise route_method_exception(e, self, args, kwargs)\n\n                # Send the new command to the appropriate class and get the response\n                method = getattr(new_self, method_name)\n                response = method(*new_args, **new_kwargs)\n\n                response.parents = (self.id, new_self.id)\n\n                # For inplace methods, just directly return self\n                if syft.framework.is_inplace_method(method_name):\n                    return self\n\n                # Put back the wrappers where needed\n                response = hook_args.hook_response(\n                    method_name,\n                    response,\n                    wrap_type=type(self),\n                    new_self=self,\n                    wrap_args=self.get_class_attributes(),\n                )\n                if args:\n                    response.parents = (self, args[0])\n                else:\n                    response.parents = self\n                response.command = method_name\n            return response\n\n        return overloaded_native_method\n\n    @classmethod\n    def _get_hooked_func(cls, public_module_name, func_api_name, func):\n        """"""\n        Hook a function in order to inspect its args and search for pointer\n        or other syft tensors.\n        - Calls to this function with normal tensors or numbers / string trigger\n          usual behaviour\n        - Calls with pointers send the command to the location of the pointer(s)\n        - Calls with syft tensor will in the future trigger specific behaviour\n\n        Args:\n            public_module_name (str): the name of the public module you are\n                hooking this function on (ie the same name that the user would import).\n            attr (str): the method to hook\n        Return:\n            the hooked method\n        """"""\n\n        cmd_name = f""{public_module_name}.{func_api_name}""\n\n        @wraps(func)\n        def overloaded_func(*args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n\n            try:\n                tensor_type = (\n                    type(args[0]) if not isinstance(args[0], (tuple, list)) else type(args[0][0])\n                )\n            except IndexError:\n                tensor_type = syft.framework.Tensor\n\n            command = (cmd_name, None, args, kwargs)\n\n            try:\n                handle_func_command = tensor_type.handle_func_command\n            except AttributeError:\n                handle_func_command = syft.framework.Tensor.handle_func_command\n\n            response = handle_func_command(command)\n\n            return response\n\n        return overloaded_func\n'"
syft/generic/frameworks/hook/hook_args.py,1,"b'from typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport numpy as np\n\nfrom syft.generic.frameworks.types import FrameworkTensorType\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft import exceptions\n\n\nhook_method_args_functions = {}\nhook_method_response_functions = {}\nget_tensor_type_functions = {}\n\nbase_types = {int, float, str, bool, bytes, bytearray, complex}\n\none = lambda _args: 1\nget_child = lambda i: i.child\n\n### Hook Args Registries ###\n# If you have a type that will be fed into hooked functions, you must add it to\n# these registeries using the public functions in the Registration Logic\n# section below.\n# WARNING: Do not attempt to manipulate them by hand. These registries should\n#    not be used outside of this module. Use the helper functions instead.\n\n# dict to specify the action depending of the type found\ntype_rule = {\n    list: lambda _args: [build_rule(a) for a in _args],\n    tuple: lambda _args: tuple(build_rule(a) for a in _args),\n    dict: one,  # FIXME This is for additiveShareTensor.child, it can be confusing and AST.child\n    np.ndarray: one,\n    # should perhaps be of type ShareDict extending dict or something like this\n}\n\n# Dict to return the proper lambda function for the right framework or syft tensor type\nforward_func = {""my_syft_tensor_type"": get_child}\n\n# Dict to return the proper lambda function for the right framework or syft tensor type\nbackward_func = {\n    ""my_syft_tensor_type"": lambda i, **kwargs: ""my_syft_tensor_type(**kwargs).on(i, wrap=False)""\n}\n\n# Methods or functions whose signature changes a lot and that we don\'t want to ""cache"", because\n# they have an arbitrary number of tensors in args which can trigger unexpected behaviour\nambiguous_methods = set()\nambiguous_functions = {""run""}\n\n\n### Registration logic ###\ndef register_type_rule(new_type_rules: Dict):\n    global type_rule\n    type_rule = {**type_rule, **new_type_rules}\n\n\ndef register_forward_func(new_forward_rules: Dict):\n    global forward_func\n    forward_func = {**forward_func, **new_forward_rules}\n\n\ndef register_backward_func(new_backward_rules: Dict):\n    global backward_func\n    backward_func = {**backward_func, **new_backward_rules}\n\n\ndef register_ambiguous_method(*method):\n    global ambiguous_methods\n    ambiguous_methods.update(set(method))\n\n\ndef register_ambiguous_function(*function):\n    global ambiguous_functions\n    ambiguous_functions.update(set(function))\n\n\ndef default_backward_func(tensorcls):\n    return lambda i, **kwargs: tensorcls(**kwargs).on(i, wrap=False)\n\n\ndef default_register_tensor(*tensorcls):\n    register_type_rule({t: one for t in tensorcls})\n    register_forward_func({t: get_child for t in tensorcls})\n    register_backward_func({t: default_backward_func(t) for t in tensorcls})\n\n\n### Main hook args implementation ###\n\n\ndef unwrap_args_from_method(attr, method_self, args_, kwargs_):\n    """"""Method arguments are sometimes simple types (such as strings or ints) but sometimes\n    they are custom Syft tensors such as wrappers (i.e. FrameworkTensor), LoggingTensor\n    or some other tensor type. Complex types (which have a .child attribute) need to\n    have arguments converted from the arg to arg.child so that the types match as the\n    method is being called down the chain. To make this efficient, we cache which args\n    need to be replaced with their children in a dictionary called\n    hook_method_args_functions. However, sometimes a method (an attr) has multiple\n    different argument signatures, such that sometimes arguments have .child objects\n    and other times they don\'t (such as x.div(), which can accept either a tensor or a\n    float as an argument). This invalidates the cache, so we need to have a try/except\n    which refreshes the cache if the signature triggers an error.\n\n    Args:\n        attr (str): the name of the method being called\n        method_self: the tensor on which the method is being called\n        args_ (list): the arguments being passed to the method\n        kwargs_ (dict): the keyword arguments being passed to the function\n            (these are not hooked ie replace with their .child attr)\n    """"""\n    # Specify an id to distinguish methods from different classes\n    # As they won\'t be used with the same arg types\n    attr_id = type(method_self).__name__ + ""."" + attr\n    try:\n        assert attr not in ambiguous_methods\n\n        # Load the utility function to transform the args\n        hook_args = hook_method_args_functions[attr_id]\n        # Try running it\n        new_self, new_args = hook_args((method_self, args_))\n\n    except (IndexError, KeyError, AssertionError):  # Update the function in case of an error\n        args_hook_function, _ = build_unwrap_args_from_function((method_self, args_))\n        # Store this utility function in the registry\n        hook_method_args_functions[attr_id] = args_hook_function\n        # Run it\n        new_self, new_args = args_hook_function((method_self, args_))\n\n    return new_self, new_args, kwargs_\n\n\ndef unwrap_args_from_function(attr, args_, kwargs_, return_args_type=False):\n    """"""See unwrap_args_from_method for details\n\n    Args:\n        attr (str): the name of the function being called\n        args_ (list): the arguments being passed to the function\n        kwargs_ (dict): the keyword arguments being passed to the function\n            (these are not hooked ie replace with their .child attr)\n        return_args_type (bool): return the type of the tensors in the\n        original arguments\n\n    Returns:\n        - the arguments where all tensors are replaced with their child\n        - the type of this new child\n        (- the type of the tensors in the arguments)\n    """"""\n    try:\n        assert attr not in ambiguous_functions\n        # Load the utility function to transform the args\n        # TODO rename registry or use another one than for methods\n        hook_args = hook_method_args_functions[attr]\n        get_tensor_type_function = get_tensor_type_functions[attr]\n\n        # Try running it\n        new_args = hook_args(args_)\n\n    except (IndexError, KeyError, AssertionError):  # Update the function in case of an error\n        args_hook_function, get_tensor_type_function = build_unwrap_args_from_function(\n            args_, return_tuple=True\n        )\n        # Store the utility functions in registries\n        hook_method_args_functions[attr] = args_hook_function\n        get_tensor_type_functions[attr] = get_tensor_type_function\n        # Run it\n        new_args = args_hook_function(args_)\n\n    new_type = get_tensor_type_function(new_args)\n    if return_args_type:\n        args_type = get_tensor_type_function(args_)\n        return new_args, kwargs_, new_type, args_type\n    else:\n        return new_args, kwargs_, new_type\n\n\ndef build_unwrap_args_from_function(args_, return_tuple=False):\n    """"""\n    Build the function f that hook the arguments:\n    f(args_) = new_args\n    """"""\n    # Inspect the call to find tensor arguments and return a rule whose\n    # structure is the same as the args_ object, with 1 where there was\n    # (framework or syft) tensors and 0 when not (ex: number, str, ...)\n    rule = build_rule(args_)\n    # Build a function with this rule to efficiently replace syft tensors\n    # (but not pointer) with their child in the args_ objects\n    args_hook_function = build_unwrap_args_with_rules(args_, rule, return_tuple)\n    # Build a function with this rule to efficiently the child type of the\n    # tensor found in the args_\n    get_tensor_type_function = build_get_tensor_type(rule)\n    return args_hook_function, get_tensor_type_function\n\n\ndef hook_response(attr, response, wrap_type, wrap_args={}, new_self=None):\n    """"""\n    When executing a command, arguments are inspected and all tensors are replaced\n    with their child attribute until a pointer or a framework tensor is found (for\n    example an argument could be a framework wrapper with a child being a LoggingTensor, with\n    a child being a framework tensor). When the result of the command is calculated,\n    we need to rebuild this chain in the reverse order (in our example put back\n    a LoggingTensor on top of the result and then a framework wrapper).\n    To make this efficient, we cache which elements of the response (which can be more\n    complicated with nested tuples for example) need to be wrapped in a dictionary called\n    hook_method_response_functions. However, sometimes a method (an attr) has multiple\n    different response signatures. This invalidates the cache, so we need to have a\n    try/except which refreshes the cache if the signature triggers an error.\n\n    Args:\n        attr (str): the name of the method being called\n        response (list or dict): the arguments being passed to the tensor\n        wrap_type (type): the type of wrapper we\'d like to have\n        wrap_args (dict): options to give to the wrapper (for example the\n        precision for the precision tensor)\n        new_self: used for the can just below of inplace ops\n    """"""\n\n    # inline methods should just return new_self\n    if ""__i"" == attr[0:3] and attr != ""__iter__"":\n        return new_self\n\n    # TODO: Why do we need to cast it in a tuple? this is a (small) time waste\n    response_is_tuple = isinstance(response, tuple)\n\n    # Add an artificial tuple\n    if not response_is_tuple:\n        response = (response, 1)\n\n    hash_wrap_args = hash(frozenset(wrap_args.items()))\n    attr_id = f""{attr}@{wrap_type.__name__}.{response_is_tuple}.{hash_wrap_args}""\n\n    try:\n        assert attr not in ambiguous_functions\n\n        # Load the utility function to transform the args\n        response_hook_function = hook_method_response_functions[attr_id]\n        # Try running it\n        new_response = response_hook_function(response)\n\n    except (IndexError, KeyError, AssertionError):  # Update the function in case of an error\n        response_hook_function = build_wrap_response_from_function(response, wrap_type, wrap_args)\n        # Store this utility function in the registry\n        hook_method_response_functions[attr_id] = response_hook_function\n        # Run it\n        new_response = response_hook_function(response)\n\n    # Remove the artificial tuple\n    if not response_is_tuple:\n        new_response, _ = new_response\n\n    return new_response\n\n\ndef build_wrap_response_from_function(response, wrap_type, wrap_args):\n    """"""\n    Build the function that hook the response.\n\n    Example:\n        p is of type Pointer\n        f is the hook_response_function\n        then f(p) = (Wrapper)>Pointer\n    """"""\n    # Inspect the call to find tensor arguments and return a rule whose\n    # structure is the same as the response object, with 1 where there was\n    # (framework or syft) tensors and 0 when not (ex: number, str, ...)\n    rule = build_rule(response)\n    # Build a function with this rule to efficiently replace syft tensors\n    # (but not pointer) with their child in the args objects\n    response_hook_function = build_wrap_response_with_rules(response, rule, wrap_type, wrap_args)\n    return response_hook_function\n\n\ndef build_rule(args_):\n    """"""\n    Inspect the args object to find framework or syft tensor arguments and\n    return a rule whose structure is the same as the args object,\n    with 1 where there was (framework or syft) tensors and 0 when\n    not (ex: number, str, ...)\n\n    Example:\n        in: ([tensor(1, 2), Pointer@bob], 42)\n        out: ([1, 1], 0)\n    """"""\n\n    type_args = type(args_)\n    # for list, tuple but also tensors and syft tensors\n    if type_args in type_rule:\n        return type_rule[type_args](args_)\n    # for int, float, str, etc\n    elif type_args in base_types:\n        return 0\n    else:\n        # New kind of return with pytorch 1.1\n        if ""torch.return_types"" in str(type_args):\n            return type_rule[tuple](args_)\n        # Still remain ellipsis, slices, etc.\n        return 0\n\n\ndef build_unwrap_args_with_rules(args_, rules, return_tuple=False, return_list=False):\n    """"""\n    Build a function given some rules to efficiently replace in the args object\n    syft tensors with their child (but not pointer as they don\'t have .child),\n    and do nothing for other type of object including framework tensors, str,\n    numbers, bool, etc.\n    Pointers trigger an error which can be caught to get the location for\n    forwarding the call.\n\n    Args:\n        args_ (tuple): the arguments given to the function / method\n        rules (tuple): the same structure but with boolean, true when there is\n            a tensor\n        return_tuple (bool): force to return a tuple even with a single element\n        return_list (bool): force to return a list instead of a tuple\n\n    Return:\n        a function that replace syft arg in args_ with arg.child\n    """"""\n\n    # get the transformation lambda for each args\n    lambdas = [\n        typed_identity(a)  # return the same obj with an identity fct with a type check if needed\n        if not r  # if the rule is a number == 0.\n        else build_unwrap_args_with_rules(a, r, True, True)\n        if isinstance(r, list)\n        else build_unwrap_args_with_rules(\n            a, r, True\n        )  # If not, call recursively build_unwrap_args_with_rules\n        if isinstance(r, tuple)\n        # Last if not, rule is probably == 1 so use type to return the right transformation.\n        else lambda i: forward_func[type(i)](i)\n        for a, r in zip(args_, rules)  # And do this for all the args / rules provided\n    ]\n\n    # Instead of iterating which is slow, we use trick to efficiently\n    # apply each lambda to each arg\n    folds = {\n        0: zero_fold,\n        1: one_fold(return_tuple),\n        2: two_fold,\n        3: three_fold,\n        4: four_fold,\n        5: five_fold,\n        6: six_fold,\n        7: seven_fold,\n        8: eight_fold,\n    }\n    try:\n        f = folds[len(lambdas)]\n    except KeyError:\n        f = many_fold\n\n    if return_list:\n        return lambda x: list(f(lambdas, x))\n\n    return lambda x: f(lambdas, x)\n\n\ndef build_get_tensor_type(rules, layer=None):\n    """"""\n    Build a function which uses some rules to find efficiently the first tensor in\n    the args objects and return the type of its child.\n\n    Args:\n        rules (tuple): a skeleton object with the same structure as args but each tensor\n            is replaced with a 1 and other types (int, str) with a 0\n        layer (list or None): keep track of the path of inspection: each element in the list\n            stand for one layer of deepness into the object, and its value for the index\n            in the current layer. See example for details\n\n    Returns:\n        a function returning a type\n\n    Example:\n        *Understanding the layer parameter*\n        obj = (a, [b, (c, d)], e)\n        the layer position is for:\n        a: [0]\n        b: [1, 0]\n        c: [1, 1, 0]\n        d: [1, 1, 1]\n        e: [2]\n\n        *Global behaviour example*\n        rules = (0, [1, (0, 0), 0)\n        - First recursion level\n          0 found -> do nothing\n          list found -> recursive call with layer = [1]\n        - Second recursion level\n          1 found -> update layer to [1, 0]\n                     build the function x: type(x[1][0])\n                     break\n        - Back to first recursion level\n          save the function returned in the lambdas list\n          0 found -> do nothing\n          exit loop\n          return the first (and here unique) function\n\n\n    """"""\n    # We keep note of the first layer or recursion level to return at the end\n    # only one function and instantiate the layer list the first time\n    first_layer = layer is None\n\n    if first_layer:\n        layer = []\n\n    # Iteration through the rules object\n    lambdas = []\n    for i, r in enumerate(rules):\n        if r == 1:  # if a tensor is found\n            layer.append(i)\n            lambdas.append(\n                # the layer object is given to build a getter to reach the\n                # tensor position and then the type() is called on the obj found\n                lambda a: type(get_element_at[len(layer)](*layer)(a))\n            )\n            # we only need one to get the type of all tensors as they should be the same\n            break\n        if isinstance(r, (list, tuple)):  # we iterate recursively if necessary\n            layer.append(i)\n            lambdas += build_get_tensor_type(r, layer)\n\n    if first_layer:\n        try:\n            return lambdas[0]\n        except IndexError:\n            # Some functions don\'t have tensors in their signature so rules is only made of 0s,\n            # Hence lambdas is empty. Raising PureFrameworkTensorFoundError triggers an execution of\n            # the un-hooked (so native) function which is perfect in that case.\n            raise exceptions.PureFrameworkTensorFoundError\n    else:\n        return lambdas\n\n\n# Function helpers to convert [a, b, c, ...] -> obj[a][b][c][...]\ndef one_layer(idx1):\n    return lambda l: l[idx1]\n\n\ndef two_layers(idx1, idx2):\n    return lambda l: one_layer(idx2)(l[idx1])\n\n\ndef three_layers(idx1, *ids):\n    return lambda l: two_layers(*ids)(l[idx1])\n\n\ndef four_layers(idx1, *ids):\n    return lambda l: three_layers(*ids)(l[idx1])\n\n\nget_element_at = {1: one_layer, 2: two_layers, 3: three_layers, 4: four_layers}\n\n\ndef build_wrap_response_with_rules(\n    response, rules, wrap_type, wrap_args, return_tuple=False, return_list=False\n):\n    """"""\n    Build a function given some rules to efficiently replace in the response object\n    syft or framework tensors with a wrapper, and do nothing for other types of object\n    including , str, numbers, bool, etc.\n\n    Args:\n        response: a response used to build the hook function\n        rules: the same structure objects but with boolean, at true when is replaces\n            a tensor\n        return_tuple: force to return a tuple even with a single element\n        return_list: force to return a list instead of a tuple\n\n    Response:\n        a function to ""wrap"" the response\n    """"""\n\n    # get the transformation lambda for each args\n    lambdas = [\n        (lambda i: i)  # return the same object\n        if not r  # if the rule is a number == 0.\n        else build_wrap_response_with_rules(a, r, wrap_type, wrap_args, True, True)\n        if isinstance(r, list)\n        else build_wrap_response_with_rules(\n            a, r, wrap_type, wrap_args, True\n        )  # If not, call recursively build_wrap_response_with_rules\n        if isinstance(r, tuple)\n        # Last if not, rule is probably == 1 so use type to return the right transformation.\n        else lambda i: backward_func[wrap_type](i, **wrap_args)\n        for a, r in zip(response, rules)  # And do this for all the responses / rules provided\n    ]\n\n    # Instead of iterating which is slow, we use trick to efficiently\n    # apply each lambda to each arg\n    folds = {\n        0: zero_fold,\n        1: one_fold(return_tuple),\n        2: two_fold,\n        3: three_fold,\n        4: four_fold,\n        5: five_fold,\n        6: six_fold,\n        7: seven_fold,\n        8: eight_fold,\n    }\n    try:\n        f = folds[len(lambdas)]\n    except KeyError:\n        f = many_fold\n\n    if return_list:\n        return lambda x: list(f(lambdas, x))\n\n    return lambda x: f(lambdas, x)\n\n\ndef zero_fold(*a, **k):\n    return ()\n\n\ndef one_fold(return_tuple, **kwargs):\n    def _one_fold(lambdas, args_, **kwargs):\n        return lambdas[0](args_[0], **kwargs)\n\n    def tuple_one_fold(lambdas, args_):\n        return (lambdas[0](args_[0], **kwargs),)\n\n    return {False: _one_fold, True: tuple_one_fold}[return_tuple]\n\n\ndef two_fold(lambdas, args_, **kwargs):\n    return lambdas[0](args_[0], **kwargs), lambdas[1](args_[1], **kwargs)\n\n\ndef three_fold(lambdas, args_, **kwargs):\n    return (\n        lambdas[0](args_[0], **kwargs),\n        lambdas[1](args_[1], **kwargs),\n        lambdas[2](args_[2], **kwargs),\n    )\n\n\ndef four_fold(lambdas, args_, **kwargs):\n    return (\n        lambdas[0](args_[0], **kwargs),\n        lambdas[1](args_[1], **kwargs),\n        lambdas[2](args_[2], **kwargs),\n        lambdas[3](args_[3], **kwargs),\n    )\n\n\ndef five_fold(lambdas, args_, **kwargs):\n    return (\n        lambdas[0](args_[0], **kwargs),\n        lambdas[1](args_[1], **kwargs),\n        lambdas[2](args_[2], **kwargs),\n        lambdas[3](args_[3], **kwargs),\n        lambdas[4](args_[4], **kwargs),\n    )\n\n\ndef six_fold(lambdas, args_, **kwargs):\n    return (\n        lambdas[0](args_[0], **kwargs),\n        lambdas[1](args_[1], **kwargs),\n        lambdas[2](args_[2], **kwargs),\n        lambdas[3](args_[3], **kwargs),\n        lambdas[4](args_[4], **kwargs),\n        lambdas[5](args_[5], **kwargs),\n    )\n\n\ndef seven_fold(lambdas, args_, **kwargs):\n    return (\n        lambdas[0](args_[0], **kwargs),\n        lambdas[1](args_[1], **kwargs),\n        lambdas[2](args_[2], **kwargs),\n        lambdas[3](args_[3], **kwargs),\n        lambdas[4](args_[4], **kwargs),\n        lambdas[5](args_[5], **kwargs),\n        lambdas[6](args_[6], **kwargs),\n    )\n\n\ndef eight_fold(lambdas, args_, **kwargs):\n    return (\n        lambdas[0](args_[0], **kwargs),\n        lambdas[1](args_[1], **kwargs),\n        lambdas[2](args_[2], **kwargs),\n        lambdas[3](args_[3], **kwargs),\n        lambdas[4](args_[4], **kwargs),\n        lambdas[5](args_[5], **kwargs),\n        lambdas[6](args_[6], **kwargs),\n        lambdas[7](args_[7], **kwargs),\n    )\n\n\ndef many_fold(lambdas, args_, **kwargs):\n    return tuple(lambdas[i](args_[i], **kwargs) for i in range(len(lambdas)))\n\n\n# Add the possibility to make a type check in the identity function applied\n# On some arg which could be None are of another type.\n# Could add more checks but not sure it is needed so far.\n\n\ndef typed_identity(a):\n    """"""\n    We need to add typed identity for arguments which can be either number\n    or tensors. If the argument changes from an int to a tensor, the\n    assertion error triggered by typed_identity will be caught and a\n    new signature will be computed for the command.\n    """"""\n    if a is None:\n\n        def none_identity(i):\n            assert i is None\n            return i\n\n        return none_identity\n\n    elif type(a) in (int, float, bool):\n\n        def number_identity(i):\n            assert isinstance(i, type(a))\n            return i\n\n        return number_identity\n\n    else:\n        return lambda i: i\n\n\n# -- Fast way to register responses and transform tensors in pointers\n\nregister_response_functions = {}\n\n\ndef register_response(\n    attr: str, response: object, response_ids: object, owner: AbstractWorker\n) -> object:\n    """"""\n    When a remote worker execute a command sent by someone else, the response is\n    inspected: all tensors are stored by this worker and a Pointer tensor is\n    made for each of them.\n\n    To make this efficient, we cache which elements of the response (which can be more\n    complicated with nested tuples for example) in the dict register_response_functions\n\n    However, sometimes a function  (an attr) has multiple different response signatures.\n    This invalidates the cache, so we need to have a try/except which refreshes the\n    cache if the signature triggers an error.\n\n    Args:\n        attr (str): the name of the function being called\n        response (object): the response of this function\n        owner (BaseWorker): the worker which registers the tensors\n    """"""\n\n    # TODO: Why do we need to cast it in a tuple? this is a (small) time waste\n    response_is_tuple = isinstance(response, tuple)\n\n    # Add an artificial tuple\n    if not response_is_tuple:\n        response = (response, 1)\n\n    attr_id = f""{attr}""\n\n    try:\n        assert attr not in ambiguous_functions\n        assert attr not in ambiguous_methods\n\n        # Load the utility function to register the response and transform tensors with pointers\n        register_response_function = register_response_functions[attr_id]\n        # Try running it\n        new_response = register_response_function(response, response_ids=response_ids, owner=owner)\n\n    except (IndexError, KeyError, AssertionError):  # Update the function in cas of an error\n        register_response_function = build_register_response_function(response)\n        # Store this utility function in the registry\n        register_response_functions[attr_id] = register_response_function\n        # Run it\n        new_response = register_response_function(response, response_ids=response_ids, owner=owner)\n\n    # Remove the artificial tuple\n    if not response_is_tuple:\n        new_response, _ = new_response\n\n    return new_response\n\n\ndef build_register_response_function(response: object) -> Callable:\n    """"""\n    Build the function that registers the response and replaces tensors with pointers.\n\n    Example:\n        (1, tensor([1, 2]) is the response\n        f is the register_response_function\n        then f(p) = (1, (Wrapper)>Pointer)\n    """"""\n    # Inspect the call to find tensor arguments and return a rule whose\n    # structure is the same as the response object, with 1 where there was\n    # (framework or syft) tensors and 0 when not (ex: number, str, ...)\n    rule = build_rule(response)\n    # Build a function with this rule to efficiently replace syft tensors\n    # (but not pointer) with their child in the args_ objects\n    response_hook_function = build_register_response(response, rule)\n    return response_hook_function\n\n\ndef register_tensor(tensor: FrameworkTensorType, owner: AbstractWorker, response_ids: List = []):\n    """"""\n    Registers a tensor.\n\n    Args:\n        tensor: A tensor.\n        owner: The owner that makes the registration.\n        response_ids: List of ids where the tensor should be stored\n            and each id is pop out when needed.\n    """"""\n    # This method often leads to re-registration of tensors\n    # hence creating two copies of the same info. The older tensor\n    # is left hanging and is never deleted. De-Registering the original\n    # tensor (if-exists) before registration addresses this problem.\n    owner.de_register_obj(tensor)  # Doesn\'t raise Exceptions if absent on owner\n    tensor.owner = owner\n    try:\n        tensor.id = response_ids.pop(-1)\n    except IndexError:\n        raise exceptions.ResponseSignatureError\n\n    owner.register_obj(tensor)\n\n    return tensor\n\n\ndef build_register_response(response: object, rules: Tuple, return_tuple: bool = False) -> Callable:\n    """"""\n    Build a function given some rules to efficiently replace in the response object\n    framework tensors with a pointer after they are registered, and do nothing for other\n    types of object including , str, numbers, bool, etc.\n\n    Args:\n        response: the response\n        rules: the rule specifying where the tensors are\n        return_tuple: force to return a tuple even with a single element\n    Returns:\n        The function to apply on generic responses\n    """"""\n\n    # get the transformation lambda for each args_\n    lambdas = [\n        (lambda i, **kwargs: i)  # return the same object\n        if not r  # or not hasattr(a, ""owner"")  # if the rule is a number == 0.\n        else build_register_response(\n            a, r, True\n        )  # If not, call recursively build_wrap_response_with_rules\n        if isinstance(r, (list, tuple))  # if the rule is a list or tuple.\n        # Last if not, rule is probably == 1 so use type to return the right transformation.\n        else lambda i, **kwargs: register_tensor(i, **kwargs)\n        for a, r in zip(response, rules)  # And do this for all the responses / rules provided\n    ]\n\n    # Instead of iterating which is slow, we use trick to efficiently\n    # apply each lambda to each arg\n    folds = {\n        0: zero_fold,\n        1: one_fold(return_tuple),\n        2: two_fold,\n        3: three_fold,\n        4: four_fold,\n        5: five_fold,\n        6: six_fold,\n        7: seven_fold,\n        8: eight_fold,\n    }\n    try:\n        f = folds[len(lambdas)]\n    except KeyError:\n        f = many_fold\n\n    return lambda x, **kwargs: f(lambdas, x, **kwargs)\n'"
syft/generic/frameworks/hook/pointers.py,0,"b'from abc import ABC\nfrom functools import wraps\n\nimport syft\n\nfrom syft.exceptions import TensorsNotCollocatedException\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.pointers.multi_pointer import MultiPointerTensor\nfrom syft.generic.pointers.object_pointer import ObjectPointer\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\n\n\nclass PointerHook(ABC):\n    """"""Hook for ALL THE POINTER THINGS that must be overloaded and/or modified""""""\n\n    def _hook_pointer_tensor_methods(self, tensor_type):\n        """"""\n        Add hooked version of all methods of the tensor_type to the\n        Pointer tensor: instead of performing the native tensor\n        method, it will be sent remotely to the location the pointer\n        is pointing at.\n        """"""\n\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[tensor_type]:\n            if attr not in dir(PointerTensor) or attr in self.boolean_comparators:\n                new_method = self._get_hooked_pointer_method(attr)\n                setattr(PointerTensor, attr, new_method)\n\n    def _hook_object_pointer_methods(self, framework_cls):\n        """"""\n        Add hooked version of all methods of the framework_cls to the\n        ObjectPointer: instead of performing the native object\n        method, it will be sent remotely to the location the pointer\n        is pointing at.\n        """"""\n\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[framework_cls]:\n            new_method = self._get_hooked_pointer_method(attr)\n            setattr(ObjectPointer, attr, new_method)\n\n    def _hook_multi_pointer_tensor_methods(self, tensor_type):\n        """"""\n        Add hooked version of all methods of the torch Tensor to the\n        Multi Pointer tensor: instead of performing the native tensor\n        method, it will be sent remotely for each pointer to the\n        location it is pointing at.\n        """"""\n\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[tensor_type]:\n            if attr not in dir(MultiPointerTensor):\n                new_method = self._get_hooked_multi_pointer_method(attr)\n                setattr(MultiPointerTensor, attr, new_method)\n\n    @classmethod\n    def _get_hooked_pointer_method(cls, attr):\n        """"""\n        Hook a method to send it to remote worker\n\n        Args:\n            attr (str): the method to hook\n        Return:\n            the hooked method\n        """"""\n\n        @wraps(attr)\n        def overloaded_pointer_method(self, *args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n            pointer = self\n            # Get info on who needs to send where the command\n            owner = pointer.owner\n            location = pointer.location\n\n            if len(args) > 0:\n                if isinstance(args[0], ObjectPointer):\n                    if args[0].location.id != location.id:\n                        raise TensorsNotCollocatedException(pointer, args[0], attr)\n\n            # Send the command\n            response = owner.send_command(location, attr, self, args, kwargs)\n\n            # For inplace methods, just directly return self\n            if syft.framework.is_inplace_method(attr):\n                return self\n\n            return response\n\n        return overloaded_pointer_method\n\n    @classmethod\n    def _get_hooked_multi_pointer_method(cls, attr):\n        """"""\n        Hook a method to send it multiple remote workers\n\n        Args:\n            attr (str): the method to hook\n        Return:\n            the hooked method\n        """"""\n\n        def dispatch(args_, k):\n            return map(lambda x: x[k] if isinstance(x, dict) else x, args_)\n\n        @wraps(attr)\n        def overloaded_attr(self, *args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n\n            # Replace all syft tensor with their child attribute\n            new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                attr, self, args, kwargs\n            )\n\n            results = {}\n            for k, v in new_self.items():\n                results[k] = v.__getattribute__(attr)(*dispatch(new_args, k), **new_kwargs)\n\n            # Put back MultiPointerTensor on the tensors found in the response\n            response = hook_args.hook_response(\n                attr, results, wrap_type=MultiPointerTensor, wrap_args=self.get_class_attributes()\n            )\n\n            return response\n\n        return overloaded_attr\n'"
syft/generic/frameworks/hook/string.py,0,"b'from abc import ABC\nfrom functools import wraps\nfrom typing import Tuple\n\nfrom syft.generic.pointers.string_pointer import StringPointer\nfrom syft.generic.string import String\n\n\nclass StringHook(ABC):\n    """"""Hook for ALL THE STRING THINGS that must be overloaded and/or modified""""""\n\n    def _hook_string_methods(self, owner):\n\n        # Set the default owner\n        setattr(String, ""owner"", owner)\n\n        for attr in dir(str):\n\n            if attr in String.methods_to_hook:\n\n                # Create the hooked method\n                new_method = self._get_hooked_string_method(attr)\n\n                # Add the hooked method\n                setattr(String, attr, new_method)\n\n    def _hook_string_pointer_methods(self):\n\n        for attr in dir(String):\n\n            if attr in String.methods_to_hook:\n\n                # Create the hooked method\n                new_method = self._get_hooked_string_pointer_method(attr)\n\n                # Add the hooked method\n                setattr(StringPointer, attr, new_method)\n\n    @classmethod\n    def _string_input_args_adaptor(cls, args_: Tuple[object]):\n        """"""\n           This method is used when hooking String methods.\n\n           Some \'String\' methods which are overriden from \'str\'\n           such as the magic \'__add__\' method\n           expects an object of type \'str\' as its first\n           argument. However, since the \'__add__\' method\n           here is hooked to a String type, it will receive\n           arguments of type \'String\' not \'str\' in some cases.\n           This won\'t worker for the underlying hooked method\n           \'__add__\' of the \'str\' type.\n           That is why the \'String\' argument to \'__add__\' should\n           be peeled down to \'str\'\n\n           Args:\n               args_: A tuple or positional arguments of the method\n                     being hooked to the String class.\n\n           Return:\n               A list of adapted positional arguments.\n\n        """"""\n\n        new_args = []\n\n        for arg in args_:\n\n            # If \'arg\' is an object of type String\n            # replace it by and \'str\' object\n            if isinstance(arg, String):\n                new_args.append(arg.child)\n            else:\n                new_args.append(arg)\n\n        return new_args\n\n    @classmethod\n    def _wrap_str_return_value(cls, _self, attr: str, value: object):\n\n        # The outputs of the following attributed won\'t\n        # be wrapped\n        ignored_attr = {""__str__"", ""__repr__"", ""__format__""}\n\n        if isinstance(value, str) and attr not in ignored_attr:\n\n            return String(object=value, owner=_self.owner)\n\n        return value\n\n    @classmethod\n    def _get_hooked_string_method(cls, attr):\n        """"""\n           Hook a `str` method to a corresponding method  of\n          `String` with the same name.\n\n           Args:\n               attr (str): the method to hook\n           Return:\n               the hooked method\n\n        """"""\n\n        @wraps(attr)\n        def overloaded_attr(_self, *args, **kwargs):\n\n            args = cls._string_input_args_adaptor(args)\n\n            # Call the method of the core builtin type\n            native_response = getattr(_self.child, attr)(*args, **kwargs)\n\n            # Some return types should be wrapped using the String\n            # class. For instance, if \'foo\' is an object of type\n            # \'String\' which wraps \'str\'. calling foo.upper()\n            # should also be of type \'String\' not \'str\'.\n            # However, the return value of foo.__str__ should\n            # be of type \'str\'.\n            response = cls._wrap_str_return_value(_self, attr, native_response)\n\n            return response\n\n        return overloaded_attr\n\n    @classmethod\n    def _get_hooked_string_pointer_method(cls, attr):\n        """"""\n           Hook a `String` method to a corresponding method  of\n          `StringPointer` with the same name.\n\n           Args:\n               attr (str): the method to hook\n           Return:\n               the hooked method\n\n        """"""\n\n        @wraps(attr)\n        def overloaded_attr(_self, *args, **kwargs):\n            """"""\n            Operate the hooking\n            """"""\n\n            owner = _self.owner\n            location = _self.location\n            # id_at_location = self.id_at_location\n\n            # Create a \'command\' variable  that is understood by\n            # the send_command() method of a worker.\n            # command = (attr, id_at_location, args, kwargs)\n\n            # send the command\n            response = owner.send_command(location, attr, _self, args, kwargs)\n\n            return response\n\n        return overloaded_attr\n'"
syft/generic/frameworks/hook/tensors.py,1,"b'from abc import ABC\nfrom abc import abstractmethod\n\nimport inspect\nimport re\nimport types\n\nimport syft\n\n\nclass TensorHook(ABC):\n    """"""Hook for ALL THE TENSOR THINGS that must be overloaded and/or modified""""""\n\n    @abstractmethod\n    def _hook_native_tensor(self, tensor_type: type, syft_type: type):\n        """"""Add PySyft-specific tensor functionality to the given tensor type.\n\n        See framework-specific implementations for more details.\n        """"""\n        # _hook_native_tensor is framework-specific, but it calls the methods\n        # defined below!\n        pass\n\n    def _hook_native_methods(self, tensor_type: type):\n        """"""\n        Add hooked version of all methods of to_auto_overload[tensor_type]\n        to the tensor_type; instead of performing the native tensor\n        method, the hooked version will be called\n\n        Args:\n            tensor_type: the tensor_type which holds the methods\n        """"""\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[tensor_type]:\n            # if we haven\'t already overloaded this function\n            if f""native_{attr}"" not in dir(tensor_type):\n                native_method = getattr(tensor_type, attr)\n                setattr(tensor_type, f""native_{attr}"", native_method)\n                new_method = self._get_hooked_method(tensor_type, attr)\n                setattr(tensor_type, attr, new_method)\n\n    def _hook_properties(hook_self, tensor_type: type):\n        """"""Overloads tensor_type properties.\n\n        If you\'re not sure how properties work, read:\n        https://www.programiz.com/python-programming/property\n        Args:\n            tensor_type: The tensor class which is having properties\n                added to it.\n        """"""\n\n        @property\n        def location(self):\n            if hasattr(self, ""child""):\n                return self.child.location\n            else:\n                return None\n\n        tensor_type.location = location\n\n        @property\n        def id_at_location(self):\n            return self.child.id_at_location\n\n        tensor_type.id_at_location = id_at_location\n\n        @property\n        def id(self):\n            if not hasattr(self, ""_syft_id""):\n                self._syft_id = syft.ID_PROVIDER.pop()\n            return self._syft_id\n\n        @id.setter\n        def id(self, new_syft_id):\n            self._syft_id = new_syft_id\n            return self\n\n        tensor_type.id = id\n\n        @property\n        def owner(self):\n            if not hasattr(self, ""_owner""):\n                self._owner = hook_self.local_worker\n            return self._owner\n\n        @owner.setter\n        def owner(self, new_owner):\n            self._owner = new_owner\n            return self\n\n        tensor_type.owner = owner\n\n        @property\n        def is_wrapper(self):\n            if not hasattr(self, ""_is_wrapper""):\n                self._is_wrapper = False\n            return self._is_wrapper\n\n        @is_wrapper.setter\n        def is_wrapper(self, it_is_a_wrapper):\n            self._is_wrapper = it_is_a_wrapper\n            return self\n\n        tensor_type.is_wrapper = is_wrapper\n\n        def dim(self):\n            return len(self.shape)\n\n        tensor_type.dim = dim\n\n    def _which_methods_should_we_auto_overload(self, tensor_type: type):\n        """"""Creates a list of Torch methods to auto overload.\n\n        By default, it looks for the intersection between the methods of\n        tensor_type and torch_type minus those in the exception list\n        (syft.torch.exclude).\n\n        Args:\n            tensor_type: Iterate through the properties of this tensor type.\n            syft_type: Iterate through all attributes in this type.\n\n        Returns:\n            A list of methods to be overloaded.\n        """"""\n\n        to_overload = self.boolean_comparators.copy()\n\n        native_pattern = re.compile(""native*"")\n\n        for attr in dir(tensor_type):\n\n            # Conditions for not overloading the method\n            # TODO[jvmancuso] separate func exclusion from method exclusion\n            if attr in syft.framework.exclude:\n                continue\n            if not hasattr(tensor_type, attr):\n                continue\n\n            lit = getattr(tensor_type, attr)\n            is_base = attr in dir(object)\n            is_desc = inspect.ismethoddescriptor(lit)\n            is_func = isinstance(lit, types.FunctionType)\n            is_overloaded = native_pattern.match(attr) is not None\n\n            if (is_desc or is_func) and not is_base and not is_overloaded:\n                to_overload.append(attr)\n\n        return set(to_overload)\n\n    def _hook_syft_tensor_methods(self, tensor_type: type, syft_type: type):\n        """"""\n        Add hooked version of all methods of to_auto_overload[tensor_type]\n        to the syft_type, so that they act like regular tensors in\n        terms of functionality, but instead of performing the native tensor\n        method, it will be forwarded to each share when it is relevant\n\n        Args:\n            tensor_type: The tensor type to which we are adding methods.\n            syft_type: the syft_type which holds the methods\n        """"""\n\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[tensor_type]:\n            if attr not in dir(syft_type):\n                new_method = self._get_hooked_syft_method(attr)\n                setattr(syft_type, attr, new_method)\n\n    def _hook_syft_placeholder_methods(self, tensor_type: type, syft_type: type):\n        """"""\n        Slight variant of _hook_syft_tensor_methods, which adds the boolean\n        comparators to the hooking\n        """"""\n\n        def create_tracing_method(base_method, name):\n            def tracing_method(self, *args, **kwargs):\n                response = base_method(self, *args, **kwargs)\n                command = (name, self, args, kwargs), response\n                if self.tracing:\n                    self.role.register_action(command, syft.execution.computation.ComputationAction)\n                return response\n\n            return tracing_method\n\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[tensor_type]:\n            if attr not in dir(syft_type) or attr in self.boolean_comparators:\n                new_method = create_tracing_method(self._get_hooked_syft_method(attr), attr)\n                setattr(syft_type, attr, new_method)\n\n    def _hook_private_tensor_methods(self, tensor_type: type, syft_type: type):\n        """"""\n        Add hooked version of all methods of the tensor_type to the\n        Private Tensor: It\'ll add references to its parents and save\n        command/actions history.\n        """"""\n        # Use a pre-defined list to select the methods to overload\n        for attr in self.to_auto_overload[tensor_type]:\n            if attr not in dir(syft_type):\n                new_method = self._get_hooked_private_method(attr)\n                setattr(syft_type, attr, new_method)\n'"
examples/tutorials/grid/federated_learning/mnist/__init__.py,0,b''
examples/tutorials/grid/federated_learning/spam_prediction/__init__.py,0,b''
examples/tutorials/grid/federated_learning/spam_prediction/handcrafted_GRU.py,1,"b'import numpy as np\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass GRUCell(nn.Module):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(GRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n\n        # reset gate\n        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)\n        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)\n\n        # update gate\n        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)\n        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)\n\n        # new gate\n        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)\n        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        std = 1.0 / np.sqrt(self.hidden_size)\n        for w in self.parameters():\n            w.data.uniform_(-std, std)\n\n    def forward(self, x, h):\n\n        x = x.view(-1, x.shape[1])\n\n        i_r = self.fc_ir(x)\n        h_r = self.fc_hr(h)\n        i_z = self.fc_iz(x)\n        h_z = self.fc_hz(h)\n        i_n = self.fc_in(x)\n        h_n = self.fc_hn(h)\n\n        resetgate = F.sigmoid(i_r + h_r)\n        inputgate = F.sigmoid(i_z + h_z)\n        newgate = F.tanh(i_n + (resetgate * h_n))\n\n        hy = newgate + inputgate * (h - newgate)\n\n        return hy\n\n\nclass GRU(nn.Module):\n    def __init__(\n        self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, bias=True, dropout=0.2\n    ):\n        super(GRU, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.output_size = output_size\n\n        # Dropout layer\n        self.dropout = nn.Dropout(p=dropout)\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # GRU Cell\n        self.gru_cell = GRUCell(embedding_dim, hidden_dim)\n        # Fully-connected layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n        # Sigmoid layer\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, h):\n\n        batch_size = x.shape[0]\n\n        # Deal with cases were the current batch_size is different from general batch_size\n        # It occurrs at the end of iteration with the Dataloaders\n        if h.shape[0] != batch_size:\n            h = h[:batch_size, :].contiguous()\n\n        # Apply embedding\n        x = self.embedding(x)\n\n        # GRU cells\n        for t in range(x.shape[1]):\n            h = self.gru_cell(x[:, t, :], h)\n\n        # Output corresponds to the last hidden state\n        out = h.contiguous().view(-1, self.hidden_dim)\n\n        # Dropout and fully-connected layers\n        out = self.dropout(out)\n        sig_out = self.sigmoid(self.fc(out))\n\n        return sig_out, h\n'"
examples/tutorials/grid/federated_learning/spam_prediction/preprocess.py,0,"b'import numpy as np\nimport pandas as pd\nimport re\n\nfrom nltk.corpus import stopwords  # noqa: F401\n\nSTOPWORDS = {}  # {stopwords.words(\'english\')}\n\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r""[^a-z\\s]"", """", text)\n    text = "" "".join([word for word in text.split() if word not in STOPWORDS])\n    return text\n\n\ndef tokenize(text, word_to_idx):\n    tokens = []\n    for word in text.split():\n        tokens.append(word_to_idx[word])\n    return tokens\n\n\ndef pad_and_truncate(messages, max_length=30):\n    features = np.zeros((len(messages), max_length), dtype=int)\n    for i, sms in enumerate(messages):\n        if len(sms):\n            features[i, -len(sms) :] = sms[:max_length]\n    return features\n\n\ndef preprocess_spam():\n    data = pd.read_csv(""./data/SMSSpamCollection"", sep=""\\t"", header=None, names=[""label"", ""sms""])\n    data.sms = data.sms.apply(clean_text)\n    words = set(("" "".join(data.sms)).split())\n    word_to_idx = {word: i for i, word in enumerate(words, 1)}\n    tokens = data.sms.apply(lambda x: tokenize(x, word_to_idx))\n    inputs = pad_and_truncate(tokens)\n\n    labels = np.array((data.label == ""spam"").astype(int))\n\n    np.save(""./data/labels.npy"", labels)\n    np.save(""./data/inputs.npy"", inputs)\n'"
examples/tutorials/websocket/pen_testing/steal_data_over_sockets/__init__.py,0,b''
examples/tutorials/websocket/pen_testing/steal_data_over_sockets/start_websocket_servers.py,0,"b'import subprocess\nimport sys\nfrom pathlib import Path\n\npython = Path(sys.executable).name\n\nFILE_PATH = Path(__file__).resolve().parents[5].joinpath(""run_websocket_server.py"")\n\ncall_alice = [python, FILE_PATH, ""--port"", ""8777"", ""--id"", ""alice"", ""--notebook"", ""steal_data""]\n\ncall_bob = [python, FILE_PATH, ""--port"", ""8778"", ""--id"", ""bob"", ""--notebook"", ""steal_data""]\n\ncall_charlie = [python, FILE_PATH, ""--port"", ""8779"", ""--id"", ""charlie"", ""--notebook"", ""steal_data""]\n\n\nprint(""Starting server for Alice"")\nsubprocess.Popen(call_alice)\n\nprint(""Starting server for Bob"")\nsubprocess.Popen(call_bob)\n\nprint(""Starting server for Charlie"")\nsubprocess.Popen(call_charlie)\n'"
syft/frameworks/torch/he/fv/ciphertext.py,0,"b'class CipherText:\n    """"""A wrapper class for representing ciphertext.\n\n    Typical format of ciphertext data would be [c0, c1, c2...] where ci represents\n    list of polynomials.\n\n    Attributes:\n        data: A 3-dim list representing ciphertext values.\n    """"""\n\n    def __init__(self, data):\n        self.data = data\n'"
syft/frameworks/torch/he/fv/context.py,1,"b'from syft.frameworks.torch.he.fv.util.rns_tool import RNSTool\n\n\nclass Context:\n    """"""A class used as for holding and easily supplying of all the general\n    parameters required throughout the implementation.\n\n    Attributes:\n        param: An EncryptionParams object.\n        coeff_div_plain_modulus: A list of float values equal to (q[i]/t),\n            In research papers denoted by delta.\n        rns_tool: A RNSTool class instance.\n    """"""\n\n    def __init__(self, encryption_param):\n\n        self.param = encryption_param\n\n        self.coeff_div_plain_modulus = [\n            x / encryption_param.plain_modulus for x in encryption_param.coeff_modulus\n        ]\n\n        self.rns_tool = RNSTool(encryption_param)\n'"
syft/frameworks/torch/he/fv/decryptor.py,4,"b'import copy\nfrom numpy.polynomial import polynomial as poly\n\n\nfrom syft.frameworks.torch.he.fv.plaintext import PlainText\nfrom syft.frameworks.torch.he.fv.util.operations import get_significant_count\nfrom syft.frameworks.torch.he.fv.util.operations import poly_add_mod\nfrom syft.frameworks.torch.he.fv.util.operations import poly_mul_mod\n\n\nclass Decryptor:\n    """"""Decrypts Ciphertext objects into Plaintext objects.\n\n    Args:\n        context (Context): Context for extracting encryption parameters.\n        secret_key: A secret key from same pair of keys(secretkey or publickey) used in encryptor.\n    """"""\n\n    def __init__(self, context, secret_key):\n        self._context = context\n        self._coeff_modulus = context.param.coeff_modulus\n        self._coeff_count = context.param.poly_modulus\n        self._secret_key = secret_key.data\n\n    def decrypt(self, encrypted):\n        """"""Decrypts the encrypted ciphertext objects.\n\n        Args:\n            encrypted: A ciphertext object which has to be decrypted.\n\n        Returns:\n            A PlainText object containing the decrypted result.\n        """"""\n\n        # Calculate [c0 + c1 * sk + c2 * sk^2 ...]_q\n        temp_product_modq = self._mul_ct_sk(copy.deepcopy(encrypted.data))\n\n        # Divide scaling variant using BEHZ FullRNS techniques\n        result = self._context.rns_tool.decrypt_scale_and_round(temp_product_modq)\n\n        # removing leading zeroes in plaintext representation.\n        plain_coeff_count = get_significant_count(result)\n        return PlainText(result[:plain_coeff_count])\n\n    def _mul_ct_sk(self, encrypted):\n        """"""Calculate [c0 + c1 * sk + c2 * sk^2 ...]_q\n\n        where [c0, c1, ...] represents ciphertext element and sk^n represents\n        secret key raised to the power n.\n\n        Args:\n            encrypted: A ciphertext object of encrypted data.\n\n        Returns:\n            A 2-dim list containing result of [c0 + c1 * sk + c2 * sk^2 ...]_q.\n        """"""\n        phase = encrypted[0]\n\n        secret_key_array = self._get_sufficient_sk_power(len(encrypted))\n\n        for j in range(1, len(encrypted)):\n            for i in range(len(self._coeff_modulus)):\n                phase[i] = poly_add_mod(\n                    poly_mul_mod(\n                        encrypted[j][i], secret_key_array[j - 1][i], self._coeff_modulus[i]\n                    ),\n                    phase[i],\n                    self._coeff_modulus[i],\n                )\n\n        return phase\n\n    def _get_sufficient_sk_power(self, max_power):\n        """"""Generate an list of secret key polynomial raised to 1...max_power.\n\n        Args:\n            max_power: heighest power up to which we want to raise secretkey.\n\n        Returns:\n            A 2-dim list having secretkey powers.\n        """"""\n        sk_power = [[] for _ in range(max_power)]\n\n        sk_power[0] = self._secret_key\n\n        for i in range(2, max_power + 1):\n            for j in range(len(self._coeff_modulus)):\n                sk_power[i - 1].append(poly.polypow(self._secret_key[j], i).astype(int).tolist())\n        return sk_power\n'"
syft/frameworks/torch/he/fv/encryption_params.py,0,"b'class EncryptionParams:\n    """"""A class for holding all the encryption parameters at one place for easy accessing\n    by any component of scheme.\n\n    Attribute:\n        poly_modulus: The polynomial modulus directly affects the number of coefficients in\n            plaintext polynomials, the size of ciphertext elements, the computational\n            performance of the scheme (bigger is worse), and the security level (bigger\n            is better). In general the degree of the polynomial modulus should be\n            a power of 2 (e.g.  1024, 2048, 4096, 8192, 16384, or 32768).\n\n        coeff_modulus: The coefficient modulus consists of a list of distinct prime numbers,\n            and is represented as a list. The coefficient modulus directly affects the size\n            of ciphertext elements, the amount of computation that the scheme can perform\n            (bigger is better), and the security level (bigger is worse).\n\n        plain_modulus: The plaintext modulus determines the largest coefficient that plaintext\n            polynomials can represent. It also affects the amount of computation that the scheme\n            can perform (bigger is worse).\n    """"""\n\n    def __init__(self, poly_modulus, coeff_modulus, plain_modulus):\n\n        if poly_modulus >= 2:\n            if (poly_modulus & (poly_modulus - 1) == 0) and poly_modulus != 0:\n                self.poly_modulus = poly_modulus\n            else:\n                raise ValueError(""poly_modulus must be a power of two 2"")\n        else:\n            raise ValueError(""poly_modulus must be at least 2"")\n\n        self.coeff_modulus = coeff_modulus\n\n        self.plain_modulus = plain_modulus\n'"
syft/frameworks/torch/he/fv/encryptor.py,5,"b'from syft.frameworks.torch.he.fv.secret_key import SecretKey\nfrom syft.frameworks.torch.he.fv.public_key import PublicKey\nfrom syft.frameworks.torch.he.fv.util.rlwe import encrypt_symmetric\nfrom syft.frameworks.torch.he.fv.util.rlwe import encrypt_asymmetric\nfrom syft.frameworks.torch.he.fv.util.operations import multiply_add_plain_with_delta\n\n\nclass Encryptor:\n    """"""Encrypts Plaintext objects into Ciphertext objects. Constructing an Encryptor\n    requires a Context with valid encryption parameters, the public key or the secret\n    key. If an Encrytor is given a secret key, it supports symmetric-key encryption.\n    If an Encryptor is given a public key, it supports asymmetric-key encryption.\n\n    Args:\n        context (Context): Context for extracting encryption parameters.\n        key: A public key or secret key that we want to use for encryption.\n    """"""\n\n    def __init__(self, context, key):\n        self._context = context\n        self._key = key\n\n    def encrypt(self, message):\n        """"""Encrypts an Plaintext data using the FV HE Scheme.\n\n        Args:\n            message (Plaintext): An plaintext which has to be encrypted.\n\n        Retruns:\n            A Ciphertext object containing the encrypted result.\n\n        Raises:\n            ValueError: Key provided for encryption is not a valid key object.\n        """"""\n\n        if isinstance(self._key, PublicKey):\n            return self._encrypt(message, True)\n\n        elif isinstance(self._key, SecretKey):\n            return self._encrypt(message, False)\n\n        else:\n            raise ValueError(""key for encryption is not valid"")\n\n    def _encrypt(self, message, is_asymmetric):\n        """"""Encrypts the message according to the key provided.\n\n        Args:\n            message (Plaintext): An Plaintext object which has to be encrypted.\n            is_asymmetric (bool): Based on the key provided for encryption select\n                the mode of encryption.\n\n        Returns:\n            A Ciphertext object contating the encrypted result.\n        """"""\n\n        result = None\n        if is_asymmetric:\n            result = encrypt_asymmetric(\n                self._context, self._key.data\n            )  # Public key used for encryption\n\n        else:\n            result = encrypt_symmetric(\n                self._context, self._key.data\n            )  # Secret key used for encryption\n\n        return multiply_add_plain_with_delta(result, message, self._context)\n'"
syft/frameworks/torch/he/fv/integer_encoder.py,2,"b'from syft.frameworks.torch.he.fv.plaintext import PlainText\nfrom syft.frameworks.torch.he.fv.util.operations import get_significant_count\n\n\nclass IntegerEncoder:\n    """"""Encodes integers into plaintext polynomials that Encryptor class can encrypt.\n\n    An instance of the IntegerEncoder class converts an integer into a plaintext polynomial\n    by placing its binary digits as the coefficients of the polynomial. Decoding the integer\n    amounts to evaluating the plaintext polynomial at x=2.\n\n    Negative integers are represented by using -1 instead of 1 in the binary representation,\n    and the negative coefficients are stored in the plaintext polynomials as unsigned integers\n    that represent them modulo the plaintext modulus. Thus, for example, a coefficient of -1\n    would be stored as a polynomial coefficient plain_modulus-1.\n\n    Args:\n        context (Context): Context for extracting encryption parameters.\n    """"""\n\n    def __init__(self, context):\n        self.plain_modulus = context.param.plain_modulus\n\n        if self.plain_modulus <= 1:\n            raise ValueError(""plain_modulus must be at least 2"")\n\n        if self.plain_modulus == 2:\n            # In this case we don\'t allow any negative numbers\n            self.coeff_neg_threshold = 2\n        else:\n            # Normal negative threshold case\n            self.coeff_neg_threshold = (self.plain_modulus + 1) >> 1\n\n        self.neg_one = self.plain_modulus - 1\n\n    def encode(self, value):\n        """"""Encodes a signed integer into a plaintext polynomial.\n        Args:\n            value: The signed integer to be encode.\n\n        Returns:\n            A PlainText object containing the integer value.\n        """"""\n\n        coeff_index = 0\n        if value < 0:\n            # negative value.\n            value = -1 * value\n            encode_coeff_count = value.bit_length()\n            plaintext = [0] * encode_coeff_count\n            while value != 0:\n                if (value & 1) != 0:\n                    plaintext[coeff_index] = self.neg_one\n                value >>= 1\n                coeff_index += 1\n        else:\n            # positive value.\n            encode_coeff_count = value.bit_length()\n            plaintext = [0] * encode_coeff_count\n            while value != 0:\n                if (value & 1) != 0:\n                    plaintext[coeff_index] = 1\n                value >>= 1\n                coeff_index += 1\n\n        return PlainText(plaintext)\n\n    def decode(self, plain):\n        """"""Decodes a plaintext polynomial and returns the integer.\n\n        Mathematically this amounts to evaluating the input polynomial at x=2.\n\n        Args:\n            plain: The plaintext to be decoded.\n\n        Returns:\n            An integer value.\n        """"""\n\n        result = 0\n        bit_index = get_significant_count(plain.data)\n        while bit_index > 0:\n            bit_index -= 1\n            coeff = plain.data[bit_index]\n\n            # Left shift result.\n            next_result = result << 1\n            if (next_result < 0) != (result < 0):\n                # Check for overflow.\n                raise OverflowError(""output out of range"")\n\n            if coeff >= self.plain_modulus:\n                # Coefficient is bigger than plaintext modulus\n                raise ValueError(""plain does not represent a valid plaintext polynomial"")\n\n            coeff_is_negative = coeff >= self.coeff_neg_threshold\n            pos_value = coeff\n\n            if coeff_is_negative:\n                pos_value = self.plain_modulus - pos_value\n\n            coeff_value = pos_value\n            if coeff_is_negative:\n                coeff_value = -coeff_value\n\n            next_result_was_negative = next_result < 0\n            next_result += coeff_value\n            next_result_is_negative = next_result < 0\n            if (\n                next_result_was_negative == coeff_is_negative\n                and next_result_was_negative != next_result_is_negative\n            ):\n                # Accumulation and coefficient had same signs, but accumulator changed signs\n                # after addition, so must be overflow.\n                raise OverflowError(""output out of range"")\n            result = next_result\n        return result\n'"
syft/frameworks/torch/he/fv/key_generator.py,5,"b'from syft.frameworks.torch.he.fv.context import Context\nfrom syft.frameworks.torch.he.fv.util.rlwe import sample_poly_ternary\nfrom syft.frameworks.torch.he.fv.util.rlwe import encrypt_symmetric\nfrom syft.frameworks.torch.he.fv.secret_key import SecretKey\nfrom syft.frameworks.torch.he.fv.public_key import PublicKey\n\n\nclass KeyGenerator:\n    """"""It is used for generating matching secret key and public key.\n    Constructing a KeyGenerator requires only a Context class instance with valid\n    encryption parameters.\n\n    Args:\n           context (Context): Context for extracting encryption parameters.\n    """"""\n\n    def __init__(self, context):\n        if not isinstance(context, Context):\n            raise ValueError(""invalid context"")\n\n        self._public_key = None\n        self._secret_key = None\n        self._context = context\n\n    def keygen(self):\n        """"""Generate the secret key and public key.\n\n        Returns:\n            A list of (size = 2) containing secret_key and public_key in respectively.\n        """"""\n        self._generate_sk()\n        self._generate_pk()\n        return [self._secret_key, self._public_key]\n\n    def _generate_sk(self, is_initialized=False):\n        param = self._context.param\n\n        if not is_initialized:\n            self._secret_key = SecretKey(sample_poly_ternary(param))\n\n    def _generate_pk(self):\n        if self._secret_key is None:\n            raise RuntimeError(""cannot generate public key for unspecified secret key"")\n\n        public_key = encrypt_symmetric(self._context, self._secret_key.data)\n        self._public_key = PublicKey(public_key.data)\n'"
syft/frameworks/torch/he/fv/modulus.py,4,"b'from collections import defaultdict\nfrom enum import Enum\n\nfrom syft.frameworks.torch.he.fv.util.numth import get_primes\nfrom syft.frameworks.torch.he.fv.util.global_variable import DEFAULT_C0EFF_MODULUS_128\nfrom syft.frameworks.torch.he.fv.util.global_variable import DEFAULT_C0EFF_MODULUS_192\nfrom syft.frameworks.torch.he.fv.util.global_variable import DEFAULT_C0EFF_MODULUS_256\n\n\nclass SeqLevelType(Enum):\n    """"""Represents standard security level according to the HomomorphicEncryption.org\n    security standard. Can be used to select the suggested secure coefficient modulus.\n    """"""\n\n    TC128 = 128\n    TC192 = 192\n    TC256 = 256\n\n\nclass CoeffModulus:\n    def bfv_default(self, poly_modulus_degree, seq_level=SeqLevelType.TC128):\n        """"""Provide secure coefficient modulus for BFV scheme that guarantees\n        a given security level when used with a given polynomial modulus,\n        according to the HomomorphicEncryption.org security standard.\n\n        Args:\n            poly_modulus_degree: The value of the polynomial modulus.\n            seq_level: (optional) The desired standard security level.\n\n        Returns:\n            A list of coefficient modulus.\n\n        Raises;\n            ValueError: if coefficient modulus are not available for provided\n                polynomial modulus.\n\n        """"""\n\n        if seq_level == SeqLevelType.TC128:\n            return DEFAULT_C0EFF_MODULUS_128[poly_modulus_degree]\n\n        if seq_level == SeqLevelType.TC192:\n            return DEFAULT_C0EFF_MODULUS_192[poly_modulus_degree]\n\n        if seq_level == SeqLevelType.TC256:\n            return DEFAULT_C0EFF_MODULUS_256[poly_modulus_degree]\n\n        raise ValueError(f""{seq_level} is not a valid standard security level"")\n\n    def create(self, poly_modulus_degree, bit_sizes):\n        """"""Generate coefficient modulus suitable for use with the specified\n        polynomial modulus. The return value will be a list consisting of\n        distinct prime numbers of bit-lengths as given in the bit_sizes parameter.\n\n        Args:\n            poly_modulus_degree: The value of the polynomial modulus.\n            bit_sizes: (list) The bit-lengths of coefficient modulus value to be generated.\n\n        Returns:\n            A list of coefficient modulus.\n        """"""\n\n        count_table = defaultdict(lambda: 0)\n        prime_table = defaultdict(lambda: 0)\n\n        for size in bit_sizes:\n            count_table[size] += 1\n\n        for table_elt in count_table:\n            prime_table[table_elt] = get_primes(\n                poly_modulus_degree, table_elt, count_table[table_elt]\n            )\n\n        result = []\n        for size in bit_sizes:\n            result.append(prime_table[size][-1])\n            prime_table[size].pop()\n\n        return result\n'"
syft/frameworks/torch/he/fv/plaintext.py,0,"b'class PlainText:\n    """"""A wrapper class for representing plaintext.\n\n    Typical format of plaintext data would be [x0, x1, x2...] where xi represents\n    coefficients of the polynomial.\n\n    Attributes:\n        data: A 1-dim list representing plaintext coefficient values.\n    """"""\n\n    def __init__(self, data):\n        self.data = data\n'"
syft/frameworks/torch/he/fv/public_key.py,0,"b'class PublicKey:\n    """"""A wrapper class for representing public key.\n\n    Typical format of public key data would be [c0, c1, ...] where ci represents\n    a list of polynomials.\n\n    Attributes:\n        data: A 3-dim list representing public key values.\n    """"""\n\n    def __init__(self, data):\n        self.data = data\n'"
syft/frameworks/torch/he/fv/secret_key.py,0,"b'class SecretKey:\n    """"""A wrapper class for representing secret key.\n\n    Typical format of secret key data would be [p1. p2, p3...] where pi represents\n    polynomials for each coefficient modulus.\n\n    Elements of each polynomails is taken from {-1, 0, 1} represented in their respective\n    modulus.\n\n    Attributes:\n        data: A 2-dim list representing secret key values.\n    """"""\n\n    def __init__(self, data):\n        self.data = data\n'"
syft/frameworks/torch/tensors/decorators/__init__.py,0,b''
syft/frameworks/torch/tensors/decorators/logging.py,6,"b'import syft as sy\n\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.generic.frameworks.hook.hook_args import (\n    get_child,\n    register_backward_func,\n    register_forward_func,\n    register_type_rule,\n    one,\n)\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.generic.abstract.tensor import AbstractTensor\n\n\nclass LoggingTensor(AbstractTensor):\n    def __init__(self, owner=None, id=None, tags=None, description=None):\n        """"""Initializes a LoggingTensor, whose behaviour is to log all actions\n        applied on it.\n\n        Args:\n            owner: An optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            id: An optional string or integer id of the LoggingTensor.\n        """"""\n        super().__init__(id=id, owner=owner, tags=tags, description=description)\n\n    # Method overloading\n\n    @overloaded.method\n    def add(self, _self, *args, **kwargs):\n        """"""\n        Here is an example of how to use the @overloaded.method decorator. To see\n        what this decorator do, just look at the next method manual_add: it does\n        exactly the same but without the decorator.\n\n        Note the subtlety between self and _self: you should use _self and NOT self.\n        """"""\n        print(""Log method add"")\n        response = getattr(_self, ""add"")(*args, **kwargs)\n\n        return response\n\n    def manual_add(self, *args, **kwargs):\n        """"""\n        Here is the version of the add method without the decorator: as you can see\n        it is much more complicated. However you might need sometimes to specify\n        some particular behaviour: so here what to start from :)\n        """"""\n        # Replace all syft tensor with their child attribute\n        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n            ""add"", self, args, kwargs\n        )\n\n        print(""Log method manual_add"")\n        # Send it to the appropriate class and get the response\n        response = getattr(new_self, ""add"")(*new_args, **new_kwargs)\n\n        # Put back SyftTensor on the tensors found in the response\n        response = hook_args.hook_response(""add"", response, wrap_type=type(self))\n        return response\n\n    # Module & Function overloading\n\n    # We overload two torch functions:\n    # - torch.add\n    # - torch.nn.functional.relu\n\n    @staticmethod\n    @overloaded.module\n    def torch(module):\n        """"""\n        We use the @overloaded.module to specify we\'re writing here\n        a function which should overload the function with the same\n        name in the <torch> module\n        :param module: object which stores the overloading functions\n\n        Note that we used the @staticmethod decorator as we\'re in a\n        class\n        """"""\n\n        def add(x, y):\n            """"""\n            You can write the function to overload in the most natural\n            way, so this will be called whenever you call torch.add on\n            Logging Tensors, and the x and y you get are also Logging\n            Tensors, so compared to the @overloaded.method, you see\n            that the @overloaded.module does not hook the arguments.\n            """"""\n            print(""Log function torch.add"")\n            return x + y\n\n        # Just register it using the module variable\n        module.add = add\n\n        @overloaded.function\n        def mul(x, y):\n            """"""\n            You can also add the @overloaded.function decorator to also\n            hook arguments, ie all the LoggingTensor are replaced with\n            their child attribute\n            """"""\n            print(""Log function torch.mul"")\n            return x * y\n\n        # Just register it using the module variable\n        module.mul = mul\n\n        # You can also overload functions in submodules!\n        @overloaded.module\n        def nn(module):\n            """"""\n            The syntax is the same, so @overloaded.module handles recursion\n            Note that we don\'t need to add the @staticmethod decorator\n            """"""\n\n            @overloaded.module\n            def functional(module):\n                def relu(x):\n                    print(""Log function torch.nn.functional.relu"")\n                    return x * (x.child > 0)\n\n                module.relu = relu\n\n            module.functional = functional\n\n        # Modules should be registered just like functions\n        module.nn = nn\n\n    @classmethod\n    def on_function_call(cls, command):\n        """"""\n        Override this to perform a specific action for each call of a torch\n        function with arguments containing syft tensors of the class doing\n        the overloading\n        """"""\n        cmd, _, args_, kwargs_ = command\n        print(""Default log"", cmd)\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, tensor: ""LoggingTensor"") -> tuple:\n        """"""\n        This function takes the attributes of a LogTensor and saves them in a tuple\n        Args:\n            tensor (LoggingTensor): a LogTensor\n        Returns:\n            tuple: a tuple holding the unique attributes of the log tensor\n        Examples:\n            data = _simplify(tensor)\n        """"""\n\n        chain = None\n        if hasattr(tensor, ""child""):\n            chain = sy.serde.msgpack.serde._simplify(worker, tensor.child)\n        return (sy.serde.msgpack.serde._simplify(worker, tensor.id), chain)\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""LoggingTensor"":\n        """"""\n        This function reconstructs a LogTensor given it\'s attributes in form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the LogTensor\n        Returns:\n            LoggingTensor: a LogTensor\n        Examples:\n            logtensor = detail(data)\n        """"""\n        obj_id, chain = tensor_tuple\n\n        tensor = LoggingTensor(owner=worker, id=sy.serde.msgpack.serde._detail(worker, obj_id))\n\n        if chain is not None:\n            chain = sy.serde.msgpack.serde._detail(worker, chain)\n            tensor.child = chain\n\n        return tensor\n\n\nregister_type_rule({LoggingTensor: one})\nregister_forward_func({LoggingTensor: get_child})\nregister_backward_func({LoggingTensor: lambda i, **kwargs: LoggingTensor().on(i, wrap=False)})\n'"
syft/frameworks/torch/tensors/interpreters/__init__.py,0,b''
syft/frameworks/torch/tensors/interpreters/additive_shared.py,36,"b'import math\nimport torch\nimport warnings\n\nimport syft as sy\nfrom syft.frameworks.torch.mpc import crypto_protocol\nfrom syft.frameworks.torch.mpc import spdz\nfrom syft.frameworks.torch.mpc import securenn\nfrom syft.frameworks.torch.mpc import fss\nfrom syft.generic.utils import memorize\n\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft_proto.frameworks.torch.tensors.interpreters.v1.additive_shared_pb2 import (\n    AdditiveSharingTensor as AdditiveSharingTensorPB,\n)\n\nno_wrap = {""no_wrap"": True}\n\n\nclass AdditiveSharingTensor(AbstractTensor):\n    def __init__(\n        self,\n        shares: dict = None,\n        owner=None,\n        id=None,\n        field=None,\n        protocol=""snn"",\n        dtype=None,\n        crypto_provider=None,\n        tags=None,\n        description=None,\n    ):\n        """"""Initializes an Additive Sharing Tensor, whose behaviour is to split a\n        single tensor into shares, distribute the shares amongst several machines,\n        and then manage how those shares are used to compute various arithmetic\n        functions.\n\n        Args:\n\n            shares: Optional dictionary with the shares already split\n            owner: An optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            id: An optional string or integer id of the AdditiveSharingTensor.\n            field: size of the arithmetic field in which the shares live\n            dtype: dtype of the field in which shares live\n            crypto_provider: an optional BaseWorker providing crypto elements\n                such as Beaver triples\n            tags: an optional set of hashtags corresponding to this tensor\n                which this tensor should be searchable for\n            description: an optional string describing the purpose of the\n                tensor\n        """"""\n        super().__init__(id=id, owner=owner, tags=tags, description=description)\n\n        self.child = shares\n        self.dtype = dtype\n        if dtype == ""custom"":\n            if field is None:\n                raise ValueError(""Field cannot be None for custom dtype"")\n            self.field = field\n            self.torch_dtype = torch.int32 if field <= 2 ** 32 else torch.int64\n        elif dtype == ""long"" or dtype == ""int64"":\n            self.field = 2 ** 64\n            self.torch_dtype = torch.int64\n            self.dtype = ""long""\n        elif dtype == ""int"" or dtype == ""int32"":\n            self.field = 2 ** 32\n            self.torch_dtype = torch.int32\n            self.dtype = ""int""\n\n        else:\n            if dtype is not None:\n                raise ValueError(""Invalid dtype value: "" + dtype)\n            warnings.warn(""Use dtype instead of field"")\n            # Since n mod 0 is not defined\n            if isinstance(field, int) and field > 0:\n                if field <= 2 ** 32:\n                    self.dtype = ""int""\n                    self.field = 2 ** 32\n                    self.torch_dtype = torch.int32\n                else:\n                    self.dtype = ""long""\n                    self.field = 2 ** 64\n                    self.torch_dtype = torch.int64\n            else:\n                warnings.warn(""Default args selected"")\n                # Default args\n                self.dtype = ""long""\n                self.field = 2 ** 64\n                self.torch_dtype = torch.int64\n\n        if shares is not None:\n            self.child = {}\n            for location, share in shares.items():\n                if isinstance(share, sy.PointerTensor):\n                    self.child[location] = share\n                elif share.is_wrapper and isinstance(share.child, sy.PointerTensor):\n                    self.child[location] = share.child\n                else:\n                    raise ValueError(\n                        ""Shares should be a dict of Pointers, optionally wrapped, ""\n                        f""but got:\\n{shares}""\n                    )\n        else:\n            self.child = None\n\n        self.n_bits = self.calculateBits(self.field)\n        # assert 2 ** self.n_bits == self.field\n\n        # min value for shares in field\n        self._min_value = None\n        # max value for shares in field\n        self._max_value = None\n\n        self.crypto_provider = (\n            crypto_provider if crypto_provider is not None else sy.hook.local_worker\n        )\n\n        self.protocol = protocol\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        type_name = type(self).__name__\n        out = f""["" f""{type_name}]""\n        if self.child is not None:\n            for v in self.child.values():\n                out += ""\\n\\t-> "" + str(v)\n        if self.crypto_provider is not None:\n            out += f""\\n\\t*crypto provider: {self.crypto_provider.id}*""\n        return out\n\n    def __bool__(self):\n        """"""Prevent evaluation of encrypted tensor""""""\n        raise ValueError(\n            ""Additive shared tensors can\'t be converted boolean values. ""\n            ""You should decrypt it first.""\n        )\n\n    @property\n    def locations(self):\n        """"""Provide a locations attribute""""""\n        return [s.location for s in self.child.values()]\n\n    @property\n    def shape(self):\n        """"""\n        Return the shape which is the shape of any of the shares\n        """"""\n        for share in self.child.values():\n            return share.shape\n\n    @property\n    def min_value(self):\n        if self._min_value is None:\n            self._min_value = -(self.field // 2)\n        return self._min_value\n\n    @property\n    def max_value(self):\n        if self._max_value is None:\n            self._max_value = (self.field - 1) // 2\n        return self._max_value\n\n    def dim(self):\n        for share in self.child.values():\n            return len(share.shape)\n\n    def clone(self):\n        """"""\n        Clone should keep ids unchanged, contrary to copy\n        """"""\n        cloned_tensor = type(self)(**self.get_class_attributes())\n        cloned_tensor.id = self.id\n        cloned_tensor.owner = self.owner\n\n        cloned_tensor.child = {location: share.clone() for location, share in self.child.items()}\n\n        return cloned_tensor\n\n    def get_class_attributes(self):\n        """"""\n        Specify all the attributes need to build a wrapper correctly when returning a response,\n        for example precision_fractional is important when wrapping the result of a method\n        on a self which is a fixed precision tensor with a non default precision_fractional.\n        """"""\n        return {\n            ""crypto_provider"": self.crypto_provider,\n            ""dtype"": self.dtype,\n            ""field"": self.field,\n            ""protocol"": self.protocol,\n        }\n\n    @property\n    def grad(self):\n        """"""\n        Gradient makes no sense for Additive Shared Tensor, so we make it clear\n        that if someone query .grad on a Additive Shared Tensor it doesn\'t error\n        but returns grad and can\'t be set\n        """"""\n        return None\n\n    def backward(self, *args, **kwargs):\n        """"""Calling backward on Additive Shared Tensor doesn\'t make sense, but sometimes a call\n        can be propagated downward the chain to an AST (for example in create_grad_objects), so\n        we just ignore the call.""""""\n        pass\n\n    @staticmethod\n    @memorize\n    def calculateBits(field: int):\n        return round(math.log(field, 2))\n\n    def modulo(self, x):\n        if self.dtype == ""custom"":\n            mask_pos = x > self.max_value\n            mask_neg = x < self.min_value\n            if mask_pos.any():\n                mask_pos = mask_pos.long()\n                return self.modulo(x - (mask_pos * self.field))\n            elif mask_neg.any():\n                mask_neg = mask_neg.long()\n                return self.modulo(x + (mask_neg * self.field))\n            else:\n                return x.type(self.torch_dtype)\n        else:\n            return x\n\n    def get(self):\n        """"""Fetches all shares and returns the plaintext tensor they represent""""""\n\n        shares = []\n\n        for share in self.child.values():\n            if isinstance(share, sy.PointerTensor):\n                shares.append(share.get())\n            else:\n                shares.append(share)\n                self.owner.de_register_obj(share)\n\n        # For dtype values long and int modulo is automatically handled by native torch tensors\n        result = self.modulo(sum(shares))\n        return result\n\n    def virtual_get(self):\n        """"""Get the value of the tensor without calling get\n        - Useful for debugging, only for VirtualWorkers\n        """"""\n\n        shares = []\n\n        for v in self.child.values():\n            share = v.location.object_store.get_obj(v.id_at_location)\n            shares.append(share)\n\n        result = self.modulo(sum(shares))\n        return result\n\n    def init_shares(self, *owners):\n        """"""Initializes shares and distributes them amongst their respective owners\n\n        Args:\n            *owners the list of shareholders. Can be of any length.\n\n        """"""\n        shares = self.generate_shares(\n            self.child, n_workers=len(owners), random_type=self.torch_dtype\n        )\n\n        shares_dict = {}\n        for share, owner in zip(shares, owners):\n            share_ptr = share.send(owner, **no_wrap)\n            shares_dict[share_ptr.location.id] = share_ptr\n\n        self.child = shares_dict\n        return self\n\n    def generate_shares(self, secret, n_workers, random_type):\n        """"""The cryptographic method for generating shares given a secret tensor.\n\n        Args:\n            secret: the tensor to be shared.\n            n_workers: the number of shares to generate for each value\n                (i.e., the number of tensors to return)\n            random_type: the torch type shares should be encoded in (use the smallest possible)\n                given the choice of mod""\n        """"""\n        random_type = torch.LongTensor if random_type == torch.int64 else torch.IntTensor\n        if not isinstance(secret, random_type):\n            secret = secret.type(random_type)\n\n        random_shares = [random_type(secret.shape) for _ in range(n_workers - 1)]\n\n        for share in random_shares:\n            share.random_(self.min_value, self.max_value)\n        shares = []\n        for i in range(n_workers):\n            if i == 0:\n                share = random_shares[i]\n            elif i < n_workers - 1:\n                share = random_shares[i] - random_shares[i - 1]\n            else:\n                share = secret - random_shares[i - 1]\n            shares.append(self.modulo(share))\n        return shares\n\n    def reconstruct(self):\n        """"""\n        Reconstruct the shares of the AdditiveSharingTensor remotely without\n        its owner being able to see any sensitive value\n\n        Returns:\n            A MultiPointerTensor where all workers hold the reconstructed value\n        """"""\n        workers = self.locations\n\n        ptr_to_sh = self.copy().wrap().send(workers[0], **no_wrap)\n        pointer = ptr_to_sh.remote_get()\n\n        pointers = [pointer]\n        for worker in workers[1:]:\n            pointers.append(pointer.copy().move(worker))\n\n        return sy.MultiPointerTensor(children=pointers)\n\n    def zero(self, shape=None):\n        """"""\n        Build an additive shared tensor of value zero with the same\n        properties as self\n        """"""\n\n        if shape is None or len(shape) == 0:\n            shape = self.shape if self.shape else [1]\n        zero = torch.zeros(*shape, dtype=self.torch_dtype).share(\n            *self.locations,\n            field=self.field,\n            dtype=self.dtype,\n            crypto_provider=self.crypto_provider,\n            **no_wrap,\n        )\n        return zero\n\n    def refresh(self):\n        """"""\n        Refresh shares by adding shares of zero\n        """"""\n        zero = self.zero()\n        r = self + zero\n        return r\n\n    @overloaded.overload_method\n    def _getitem_multipointer(self, self_shares, indices_shares):\n        """"""\n        Support x[i] where x is an AdditiveSharingTensor and i a MultiPointerTensor\n\n        Args:\n            self_shares (dict): the dict of shares of x\n            indices_shares (dict): the dict of shares of i\n\n        Returns:\n            an AdditiveSharingTensor\n        """"""\n        selected_shares = {}\n        for worker, share in self_shares.items():\n            indices = []\n            for index in indices_shares:\n                if isinstance(index, slice):\n                    indices.append(index)\n                elif isinstance(index, dict):\n                    indices.append(index[worker])\n                else:\n                    raise NotImplementedError(""Index type"", type(indices), ""not supported"")\n            selected_share = share[tuple(indices)]\n            selected_shares[worker] = selected_share\n\n        return selected_shares\n\n    @overloaded.overload_method\n    def _getitem_public(self, self_shares, indices):\n        """"""\n        Support x[i] where x is an AdditiveSharingTensor and i a MultiPointerTensor\n\n        Args:\n            self_shares (dict): the dict of shares of x\n            indices_shares (tuples of ints): integers indices\n\n        Returns:\n            an AdditiveSharingTensor\n\n        """"""\n        selected_shares = {}\n        for worker, share in self_shares.items():\n            selected_shares[worker] = share[indices]\n\n        return selected_shares\n\n    def __getitem__(self, indices):\n        if not isinstance(indices, (tuple, list)):\n            indices = (indices,)\n        tensor_type = type(indices[-1])\n\n        if tensor_type == sy.MultiPointerTensor:\n            return self._getitem_multipointer(indices)\n        else:\n            return self._getitem_public(indices)\n\n    ## SECTION SPDZ\n\n    @overloaded.method\n    def add(self, shares: dict, other):\n        """"""Adds operand to the self AST instance.\n\n        Args:\n            shares: a dictionary <location_id -> PointerTensor) of shares corresponding to\n                self. Equivalent to calling self.child.\n            other: the operand being added to self, can be:\n                - a dictionary <location_id -> PointerTensor) of shares\n                - a torch tensor\n                - a constant\n        """"""\n        if isinstance(other, int):\n            other = torch.tensor([other], dtype=self.torch_dtype)\n\n        if isinstance(other, (torch.LongTensor, torch.IntTensor)):\n            # if someone passes a torch tensor, we share it and keep the dict\n            other = other.share(\n                *self.child.keys(),\n                field=self.field,\n                dtype=self.dtype,\n                crypto_provider=self.crypto_provider,\n                **no_wrap,\n            ).child\n        elif not isinstance(other, dict):\n            # if someone passes in a constant, we cast it to a tensor, share it and keep the dict\n            other = (\n                torch.tensor([other], dtype=self.torch_dtype)\n                .share(\n                    *self.child.keys(),\n                    field=self.field,\n                    dtype=self.dtype,\n                    crypto_provider=self.crypto_provider,\n                    **no_wrap,\n                )\n                .child\n            )\n\n        assert len(shares) == len(other)\n\n        # matches each share which needs to be added according\n        # to the location of the share\n        new_shares = {}\n        for k, v in shares.items():\n            new_shares[k] = self.modulo(other[k] + v)\n        return new_shares\n\n    __add__ = add\n    __radd__ = add\n\n    @overloaded.method\n    def sub(self, shares: dict, other):\n        """"""Subtracts an operand from the self AST instance.\n\n        Args:\n            shares: a dictionary <location_id -> PointerTensor) of shares corresponding to\n                self. Equivalent to calling self.child.\n            other: the operand being subtracted from self, can be:\n                - a dictionary <location_id -> PointerTensor) of shares\n                - a torch tensor\n                - a constant\n        """"""\n\n        if isinstance(other, int):\n            other = torch.tensor([other], dtype=self.torch_dtype)\n\n        if isinstance(other, (torch.LongTensor, torch.IntTensor)):\n            # if someone passes a torch tensor, we share it and keep the dict\n            other = other.share(\n                *self.child.keys(),\n                field=self.field,\n                dtype=self.dtype,\n                crypto_provider=self.crypto_provider,\n                **no_wrap,\n            ).child\n        elif not isinstance(other, dict):\n            # if someone passes in a constant, we cast it to a tensor, share it and keep the dict\n            other = (\n                torch.tensor([other], dtype=self.torch_dtype)\n                .share(\n                    *self.child.keys(),\n                    field=self.field,\n                    dtype=self.dtype,\n                    crypto_provider=self.crypto_provider,\n                    **no_wrap,\n                )\n                .child\n            )\n\n        assert len(shares) == len(other)\n\n        # matches each share which needs to be added according\n        # to the location of the share\n        new_shares = {}\n        for k, v in shares.items():\n            new_shares[k] = self.modulo(v - other[k])\n\n        return new_shares\n\n    __sub__ = sub\n\n    def __rsub__(self, other):\n        return (self - other) * -1\n\n    def _private_mul(self, other, equation: str):\n        """"""Abstractly Multiplies two tensors\n\n        Args:\n            self: an AdditiveSharingTensor\n            other: another AdditiveSharingTensor\n            equation: a string representation of the equation to be computed in einstein\n                summation form\n        """"""\n        # check to see that operation is either mul or matmul\n        assert equation == ""mul"" or equation == ""matmul""\n        cmd = getattr(torch, equation)\n\n        assert isinstance(other, AdditiveSharingTensor)\n\n        assert len(self.child) == len(other.child)\n\n        if self.crypto_provider is None:\n            raise AttributeError(""For multiplication a crypto_provider must be passed."")\n\n        shares = spdz.spdz_mul(cmd, self, other, self.crypto_provider, self.field, self.dtype)\n\n        return shares\n\n    @overloaded.method\n    def _public_mul(self, shares, other, equation):\n        """"""Multiplies an AdditiveSharingTensor with a non-private value\n        (int, torch tensor, MultiPointerTensor, etc.)\n\n        When other is a constant equal to zero, the shares vanish so we need to add fresh\n        shares of zero.\n\n        Args:\n            shares (dict): a dictionary <location_id -> PointerTensor) of shares corresponding to\n                self. Equivalent to calling self.child.\n            other (dict of int): operand being multiplied with self, can be:\n                - a dictionary <location_id -> PointerTensor) of shares\n                - a torch tensor (Int or Long)\n                - or an integer\n            equation: a string representation of the equation to be computed in einstein\n                summation form\n        """"""\n        assert equation == ""mul"" or equation == ""matmul""\n        cmd = getattr(torch, equation)\n        if isinstance(other, dict):\n            return {\n                worker: (self.modulo(cmd(share, other[worker]))) for worker, share in shares.items()\n            }\n        else:\n            other_is_zero = False\n            if isinstance(other, (torch.LongTensor, torch.IntTensor)):\n                if (other == 0).any():\n                    other_is_zero = True\n            elif other == 0:\n                other_is_zero = True\n\n            if other_is_zero:\n                res = {}\n                first_it = True\n\n                for worker, share in shares.items():\n                    cmd_res = cmd(share, other)\n                    if first_it:\n                        first_it = False\n                        zero_shares = self.zero(cmd_res.shape).child\n                    res[worker] = self.modulo(cmd(share, other) + zero_shares[worker])\n                return res\n            else:\n                return {\n                    worker: (self.modulo(cmd(share, other))) for worker, share in shares.items()\n                }\n\n    def mul(self, other):\n        """"""Multiplies two tensors together\n\n        Args:\n            self (AdditiveSharingTensor): an AdditiveSharingTensor\n            other: another AdditiveSharingTensor, or a MultiPointerTensor, or an integer\n        """"""\n        if not isinstance(other, sy.AdditiveSharingTensor):\n            return self._public_mul(other, ""mul"")\n\n        return self._private_mul(other, ""mul"")\n\n    def __mul__(self, other, **kwargs):\n        return self.mul(other, **kwargs)\n\n    def __imul__(self, other):\n        self = self.mul(other)\n        return self\n\n    def square(self):\n        return self.mul(self)\n\n    def pow(self, power):\n        """"""\n        Compute integer power of a number by recursion using mul\n\n        This uses the following trick:\n         - Divide power by 2 and multiply base to itself (if the power is even)\n         - Decrement power by 1 to make it even and then follow the first step\n        """"""\n        base = self\n\n        result = 1\n        while power > 0:\n            # If power is odd\n            if power % 2 == 1:\n                result = result * base\n\n            # Divide the power by 2\n            power = power // 2\n            # Multiply base to itself\n            base = base * base\n\n        return result\n\n    __pow__ = pow\n\n    def matmul(self, other):\n        """"""Multiplies two tensors matrices together\n\n        Args:\n            self: an AdditiveSharingTensor\n            other: another AdditiveSharingTensor or a MultiPointerTensor\n        """"""\n        # If the multiplication can be public\n        if not isinstance(other, sy.AdditiveSharingTensor):\n            return self._public_mul(other, ""matmul"")\n\n        return self._private_mul(other, ""matmul"")\n\n    def mm(self, *args, **kwargs):\n        """"""Multiplies two tensors matrices together\n        """"""\n        return self.matmul(*args, **kwargs)\n\n    def __matmul__(self, *args, **kwargs):\n        """"""Multiplies two tensors matrices together\n        """"""\n        return self.matmul(*args, **kwargs)\n\n    def __itruediv__(self, *args, **kwargs):\n\n        result = self.__truediv__(*args, **kwargs)\n        self.child = result.child\n\n    def _private_div(self, divisor):\n        return securenn.division(self, divisor)\n\n    @overloaded.method\n    def _public_div(self, shares: dict, divisor):\n        # TODO: how to correctly handle division in Zq?\n        divided_shares = {}\n        for i_worker, (location, pointer) in enumerate(shares.items()):\n            # Still no solution to perform a real division on a additive shared tensor\n            # without a heavy crypto protocol.\n            # For now, the solution works in most cases when the tensor is shared between 2 workers\n            divided_shares[location] = pointer / divisor\n\n        return divided_shares\n\n    def div(self, divisor):\n        if isinstance(divisor, AdditiveSharingTensor):\n            return self._private_div(divisor)\n        else:\n            return self._public_div(divisor)\n\n    __truediv__ = div\n\n    @overloaded.method\n    def mod(self, shares: dict, modulus: int):\n        assert isinstance(modulus, int)\n\n        moded_shares = {}\n        for location, pointer in shares.items():\n            moded_shares[location] = pointer % modulus\n\n        return moded_shares\n\n    def __mod__(self, *args, **kwargs):\n        return self.mod(*args, **kwargs)\n\n    @overloaded.method\n    def chunk(self, shares, *args, **kwargs):\n        """"""\n        This method overrides the torch.Tensor.chunk() method of Pytorch\n        """"""\n        results = None\n\n        for worker, share in shares.items():\n            share_results = share.chunk(*args, **kwargs)\n            if isinstance(share_results, (tuple, list)):\n                if results is None:\n                    results = [{worker: share_result} for share_result in share_results]\n                else:\n                    for result, share_result in zip(results, share_results):\n                        result[worker] = share_result\n            else:\n                if results is None:\n                    results = {}\n                results[worker] = share_results\n\n        return results\n\n    @staticmethod\n    @overloaded.module\n    def torch(module):\n        def add(self, other):\n            """"""Overload add(x, y) to redirect to add(y)""""""\n            return self.add(other)\n\n        module.add = add\n\n        def mul(self, other):\n            """"""Overload torch.mul(x, y) to redirect to x.mul(y)""""""\n            return self.mul(other)\n\n        module.mul = mul\n\n        def matmul(self, other):\n            """"""Overload torch.matmul(x, y) to redirect to x.matmul(y)""""""\n            return self.matmul(other)\n\n        module.matmul = matmul\n\n        def sum(self, *args, **kwargs):\n            """"""Overload torch.sum(x) to redirect to x.sum()""""""\n            return self.sum(*args, **kwargs)\n\n        module.sum = sum\n\n        def dot(self, other):\n            """"""Overload torch.dot(x, y)""""""\n            return self.mul(other).sum()\n\n        module.dot = dot\n\n        def mean(self, *args, **kwargs):\n            """"""Overload torch.mean(x)""""""\n            # We cannot directly use mean on Long tensors\n            # so we do it by hand with a sum and a division\n            sum = self.sum(*args, **kwargs)\n\n            # We need to know how many input values are used for each\n            # output value to divide\n            dims_to_reduce = args[0] if args else range(self.dim())\n            if isinstance(dims_to_reduce, int):\n                dims_to_reduce = (dims_to_reduce,)\n\n            div = 1\n            for i, s in enumerate(self.shape):\n                if i in dims_to_reduce:\n                    div *= s\n\n            return sum // div\n\n        module.mean = mean\n\n        @overloaded.function\n        def unbind(tensor_shares, **kwargs):\n            results = None\n\n            for worker, share in tensor_shares.items():\n                share_results = torch.unbind(share, **kwargs)\n                if results is None:\n                    results = [{worker: share_result} for share_result in share_results]\n                else:\n                    for result, share_result in zip(results, share_results):\n                        result[worker] = share_result\n\n            return results\n\n        module.unbind = unbind\n\n        def share_combine(tensors_shares):\n            results = {}\n            workers = tensors_shares[0].keys()\n\n            for worker in workers:\n                tensors_share = []\n                for tensor_shares in tensors_shares:\n                    tensor_share = tensor_shares[worker]\n                    tensors_share.append(tensor_share)\n                results[worker] = tensors_share\n\n            return results\n\n        @overloaded.function\n        def stack(tensors_shares, **kwargs):\n            return {\n                worker: torch.stack(share, **kwargs)\n                for (worker, share) in share_combine(tensors_shares).items()\n            }\n\n        module.stack = stack\n\n        @overloaded.function\n        def cat(tensors_shares, **kwargs):\n            return {\n                worker: torch.cat(share, **kwargs)\n                for (worker, share) in share_combine(tensors_shares).items()\n            }\n\n        module.cat = cat\n\n        def chunk(tensor, *args, **kwargs):\n            return tensor.chunk(*args, **kwargs)\n\n        module.chunk = chunk\n\n        @overloaded.function\n        def roll(tensor_shares, shifts, **kwargs):\n            """""" Return a tensor where values are cyclically shifted compared to the original one.\n            For instance, torch.roll([1, 2, 3], 1) returns torch.tensor([3, 1, 2]).\n            In **kwargs should be dims, an argument to tell along which dimension the tensor should\n            be rolled. If dims is None, the tensor is flattened, rolled, and restored to its\n            original shape. shifts and dims can be tuples of same length to perform several\n            rolls along different dimensions.\n            """"""\n            results = {}\n            for worker, share in tensor_shares.items():\n                if isinstance(shifts, dict):\n                    results[worker] = torch.roll(share, shifts[worker], **kwargs)\n                elif isinstance(shifts, tuple) and isinstance(shifts[0], dict):\n                    worker_shifts = [s[worker] for s in shifts]\n                    results[worker] = torch.roll(share, worker_shifts, **kwargs)\n                else:\n                    results[worker] = torch.roll(share, shifts, **kwargs)\n\n            return results\n\n        module.roll = roll\n\n        def max(tensor, **kwargs):\n            return tensor.max(**kwargs)\n\n        module.max = max\n\n        def argmax(tensor, **kwargs):\n            return tensor.argmax(**kwargs)\n\n        module.argmax = argmax\n\n        @overloaded.module\n        def functional(module):\n            @overloaded.function\n            def split(tensor_shares, *args, **kwargs):\n                results = None\n\n                for worker, share in tensor_shares.items():\n                    share_results = torch.split(share, *args, **kwargs)\n                    if results is None:\n                        results = [{worker: share_result} for share_result in share_results]\n                    else:\n                        for result, share_result in zip(results, share_results):\n                            result[worker] = share_result\n\n                return results\n\n            module.split = split\n\n        module.functional = functional\n\n        @overloaded.module\n        def nn(module):\n            @overloaded.module\n            def functional(module):\n                def relu(tensor_shares, inplace=False):\n                    return tensor_shares.relu()\n\n                module.relu = relu\n\n                @overloaded.function\n                def pad(input_shares, pad, mode=""constant"", value=0):\n                    padded_shares = {}\n                    for location, shares in input_shares.items():\n                        padded_shares[location] = torch.nn.functional.pad(shares, pad, mode, value)\n\n                    return padded_shares\n\n                module.pad = pad\n\n            module.functional = functional\n\n        module.nn = nn\n\n    ## SECTION SNN\n\n    def relu(self, inplace=False):\n        return securenn.relu(self)\n\n    def positive(self):\n        # self >= 0\n        return securenn.relu_deriv(self)\n\n    def gt(self, other):\n        r = self - other - 1\n        return r.positive()\n\n    @crypto_protocol(""snn"")\n    def __gt__(self, other):\n        return self.gt(other)\n\n    @crypto_protocol(""fss"")\n    def __gt__(self, other):\n        return (other + 1) <= self\n\n    def ge(self, other):\n        return (self - other).positive()\n\n    @crypto_protocol(""snn"")\n    def __ge__(self, other):\n        return self.ge(other)\n\n    @crypto_protocol(""fss"")\n    def __ge__(self, other):\n        return other <= self\n\n    def lt(self, other):\n        return (other - self - 1).positive()\n\n    @crypto_protocol(""snn"")\n    def __lt__(self, other):\n        return self.lt(other)\n\n    @crypto_protocol(""fss"")\n    def __lt__(self, other):\n        return (self + 1) <= other\n\n    def le(self, other):\n        return (other - self).positive()\n\n    @crypto_protocol(""snn"")\n    def __le__(self, other):\n        return self.le(other)\n\n    @crypto_protocol(""fss"")\n    def __le__(self, other):\n        return fss.le(self, other)\n\n    @crypto_protocol(""snn"")\n    def eq(self, other):\n        diff = self - other\n        diff2 = diff * diff\n        negdiff2 = diff2 * -1\n        return negdiff2.positive()\n\n    @crypto_protocol(""fss"")\n    def eq(self, other):\n        return fss.eq(self, other)\n\n    def __eq__(self, other):\n        return self.eq(other)\n\n    def max(self, dim=None, return_idx=False):\n        """"""\n        Return the maximum value of an additive shared tensor\n\n        Args:\n            dim (None or int): if not None, the dimension on which\n                the comparison should be done\n            return_idx (bool): Return the index of the maximum value\n                Note that if dim is specified then the index is returned\n                anyway to match the Pytorch syntax.\n\n        return:\n            the maximum value (possibly across an axis)\n            and optionally the index of the maximum value (possibly across an axis)\n        """"""\n        values = self\n        n_dim = self.dim()\n\n        # Make checks and transformation\n        assert dim is None or (0 <= dim < n_dim), f""Dim overflow  0 <= {dim} < {n_dim}""\n        # FIXME make it cleaner and robust for more options\n        if n_dim == 2:\n            if dim is None:\n                values = values.view(-1)\n            elif dim == 1:\n                values = values.t()\n        assert n_dim <= 2, ""Max on tensor with len(shape) > 2 is not supported.""\n\n        # Init max vals and idx to the first element\n        max_value = values[0]\n        max_index = torch.tensor([0]).share(\n            *self.locations,\n            field=self.field,\n            dtype=self.dtype,\n            crypto_provider=self.crypto_provider,\n            **no_wrap,\n        )\n\n        for i in range(1, len(values)):\n            a = values[i]\n            beta = a >= max_value\n            max_index = max_index + beta * (i - max_index)\n            max_value = max_value + beta * (a - max_value)\n\n        if dim is None and return_idx is False:\n            return max_value\n        else:\n            return max_value, max_index * 1000\n\n    def argmax(self, dim=None):\n\n        max_value, max_index = self.max(dim=dim, return_idx=True)\n\n        return max_index\n\n    ## STANDARD\n\n    @staticmethod\n    def select_worker(args_, worker):\n        """"""\n        utility function for handle_func_command which help to select\n        shares (seen as elements of dict) in an argument set. It could\n        perhaps be put elsewhere\n\n        Args:\n            args_: arguments to give to a functions\n            worker: owner of the shares to select\n\n        Return:\n            args_ where the AdditiveSharedTensors are replaced by\n            the appropriate share\n        """"""\n        return map(lambda x: x[worker] if isinstance(x, dict) else x, args_)\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """"""\n        Receive an instruction for a function to be applied on a Syft Tensor,\n        Replace in the args all the LogTensors with\n        their child attribute, forward the command instruction to the\n        handle_function_command of the type of the child attributes, get the\n        response and replace a Syft Tensor on top of all tensors found in\n        the response.\n\n        Args:\n            command: instruction of a function command: (command name,\n            <no self>, arguments[, kwargs_])\n\n        Returns:\n            the response of the function command\n        """"""\n        cmd, _, args_, kwargs_ = command\n\n        # Check that the function has not been overwritten\n        try:\n            # Try to get recursively the attributes in cmd = ""<attr1>.<attr2>.<attr3>...""\n            cmd = cls.rgetattr(cls, cmd)\n        except AttributeError:\n            pass\n        if not isinstance(cmd, str):\n            return cmd(*args_, **kwargs_)\n\n        tensor = args_[0] if not isinstance(args_[0], (tuple, list)) else args_[0][0]\n\n        # Replace all SyftTensors with their child attribute\n        new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(cmd, args_, kwargs_)\n\n        results = {}\n        for worker, share in new_args[0].items():\n            new_type = type(share)\n            new_args_worker = tuple(AdditiveSharingTensor.select_worker(new_args, worker))\n\n            # build the new command\n            new_command = (cmd, None, new_args_worker, new_kwargs)\n\n            # Send it to the appropriate class and get the response\n            results[worker] = new_type.handle_func_command(new_command)\n\n        # Put back AdditiveSharingTensor on the tensors found in the response\n        response = hook_args.hook_response(\n            cmd, results, wrap_type=cls, wrap_args=tensor.get_class_attributes()\n        )\n\n        return response\n\n    def set_garbage_collect_data(self, value):\n        shares = self.child\n        for _, share in shares.items():\n            share.garbage_collect_data = value\n\n    def get_garbage_collect_data(self):\n        garbage_collect_data_dict = {}\n        shares = self.child\n\n        for worker, share in shares.items():\n            garbage_collect_data_dict[worker] = share.garbage_collect_data\n\n        return garbage_collect_data_dict\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, tensor: ""AdditiveSharingTensor"") -> tuple:\n        """"""\n        This function takes the attributes of a AdditiveSharingTensor and saves them in a tuple\n        Args:\n            tensor (AdditiveSharingTensor): a AdditiveSharingTensor\n        Returns:\n            tuple: a tuple holding the unique attributes of the additive shared tensor\n        Examples:\n            data = simplify(tensor)\n        """"""\n\n        chain = None\n        if hasattr(tensor, ""child""):\n            chain = sy.serde.msgpack.serde._simplify(worker, tensor.child)\n\n        # Don\'t delete the remote values of the shares at simplification\n        garbage_collect = tensor.get_garbage_collect_data()\n        tensor.set_garbage_collect_data(False)\n\n        return (\n            sy.serde.msgpack.serde._simplify(worker, tensor.id),\n            sy.serde.msgpack.serde._simplify(worker, tensor.field),\n            tensor.dtype.encode(""utf-8""),\n            sy.serde.msgpack.serde._simplify(worker, tensor.crypto_provider.id),\n            chain,\n            garbage_collect,\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""AdditiveSharingTensor"":\n        """"""\n            This function reconstructs a AdditiveSharingTensor given it\'s attributes in\n        form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the AdditiveSharingTensor\n        Returns:\n            AdditiveSharingTensor: a AdditiveSharingTensor\n        Examples:\n            shared_tensor = detail(data)\n        """"""\n        tensor_id, field, dtype, crypto_provider, chain, garbage_collect = tensor_tuple\n\n        crypto_provider = sy.serde.msgpack.serde._detail(worker, crypto_provider)\n\n        tensor = AdditiveSharingTensor(\n            owner=worker,\n            id=sy.serde.msgpack.serde._detail(worker, tensor_id),\n            field=sy.serde.msgpack.serde._detail(worker, field),\n            dtype=dtype.decode(""utf-8""),\n            crypto_provider=worker.get_worker(crypto_provider),\n        )\n\n        if chain is not None:\n            chain = sy.serde.msgpack.serde._detail(worker, chain)\n            tensor.child = chain\n\n        tensor.set_garbage_collect_data(garbage_collect)\n\n        return tensor\n\n    @staticmethod\n    def bufferize(\n        worker: AbstractWorker, tensor: ""AdditiveSharingTensor""\n    ) -> ""AdditiveSharingTensorPB"":\n        """"""\n            This function takes the attributes of a AdditiveSharingTensor and saves them in a\n        protobuf object\n        Args:\n            tensor (AdditiveSharingTensor): a AdditiveSharingTensor\n        Returns:\n            protobuf: a protobuf object holding the unique attributes of the additive shared tensor\n        Examples:\n            data = protobuf(tensor)\n        """"""\n        protobuf_tensor = AdditiveSharingTensorPB()\n\n        if hasattr(tensor, ""child""):\n            for key, value in tensor.child.items():\n                sy.serde.protobuf.proto.set_protobuf_id(protobuf_tensor.location_ids.add(), key)\n                protobuf_share = sy.serde.protobuf.serde._bufferize(worker, value)\n                protobuf_tensor.shares.append(protobuf_share)\n\n        # Don\'t delete the remote values of the shares at simplification\n        tensor.set_garbage_collect_data(False)\n\n        sy.serde.protobuf.proto.set_protobuf_id(protobuf_tensor.id, tensor.id)\n        sy.serde.protobuf.proto.set_protobuf_id(\n            protobuf_tensor.crypto_provider_id, tensor.crypto_provider.id\n        )\n\n        if tensor.field >= 2 ** 64:\n            protobuf_tensor.field_str = str(tensor.field)\n        else:\n            protobuf_tensor.field_int = tensor.field\n        protobuf_tensor.dtype = tensor.dtype\n\n        return protobuf_tensor\n\n    @staticmethod\n    def unbufferize(\n        worker: AbstractWorker, protobuf_tensor: ""AdditiveSharingTensorPB""\n    ) -> ""AdditiveSharingTensor"":\n        """"""\n            This function reconstructs a AdditiveSharingTensor given its\' attributes in form of a\n            protobuf object.\n        Args:\n            worker: the worker doing the deserialization\n            protobuf_tensor: a protobuf object holding the attributes of the AdditiveSharingTensor\n        Returns:\n            AdditiveSharingTensor: a AdditiveSharingTensor\n        Examples:\n            shared_tensor = unprotobuf(data)\n        """"""\n\n        tensor_id = sy.serde.protobuf.proto.get_protobuf_id(protobuf_tensor.id)\n        crypto_provider_id = sy.serde.protobuf.proto.get_protobuf_id(\n            protobuf_tensor.crypto_provider_id\n        )\n        field = int(getattr(protobuf_tensor, protobuf_tensor.WhichOneof(""field_size"")))\n        dtype = protobuf_tensor.dtype\n\n        tensor = AdditiveSharingTensor(\n            owner=worker,\n            id=tensor_id,\n            field=field,\n            dtype=dtype,\n            crypto_provider=worker.get_worker(crypto_provider_id),\n        )\n\n        if protobuf_tensor.location_ids is not None:\n            chain = {}\n            for pb_location_id, share in zip(protobuf_tensor.location_ids, protobuf_tensor.shares):\n                location_id = sy.serde.protobuf.proto.get_protobuf_id(pb_location_id)\n                chain[location_id] = sy.serde.protobuf.serde._unbufferize(worker, share)\n            tensor.child = chain\n\n        return tensor\n\n    @staticmethod\n    def get_protobuf_schema() -> AdditiveSharingTensorPB:\n        return AdditiveSharingTensorPB\n\n\n### Register the tensor with hook_args.py ###\nhook_args.default_register_tensor(AdditiveSharingTensor)\n'"
syft/frameworks/torch/tensors/interpreters/autograd.py,3,"b'import torch\n\nimport syft\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.generic.frameworks.hook.hook_args import (\n    get_child,\n    register_backward_func,\n    register_forward_func,\n    register_type_rule,\n    one,\n)\nfrom syft.workers.abstract import AbstractWorker\nfrom . import gradients\n\n\ndef backwards_grad(grad_fn, in_grad=None):\n    if grad_fn is None:\n        raise ValueError(\n            ""The gradient for one of the command you used was not found. Check gradients.py ""\n            ""to see if it\'s missing.""\n        )\n    back_grad = grad_fn(in_grad)\n    for next_grad_fn, next_grad in zip(grad_fn.next_functions, back_grad):\n        backwards_grad(next_grad_fn, next_grad)\n\n\nclass AutogradTensor(AbstractTensor):\n    """""" A tensor that tracks operations to build a dynamic graph and backprops\n    through the graph to calculate gradients.\n    """"""\n\n    def __init__(\n        self, data=None, requires_grad=True, owner=None, id=None, preinitialize_grad=False, **kwargs\n    ):\n        super().__init__(\n            id=id, owner=owner, tags=kwargs.get(""tags""), description=kwargs.get(""description"")\n        )\n\n        self.child = data\n        self.requires_grad = requires_grad\n        self.preinitialize_grad = preinitialize_grad\n\n        if preinitialize_grad:\n            self.grad = data * 0\n        else:\n            self.grad = None\n\n        self.grad_fn = kwargs.get(""grad_fn"")\n\n    def backward(self, grad=None):\n        if grad is None:\n            # Build a torch tensor of ones with the same shape\n            # And chain structure than self\n            grad = self * 0 + 1\n        backwards_grad(self.grad_fn, grad)\n\n    @property\n    def data(self):\n        # TODO why is that? Normally .data is detached from autograd\n        return self\n\n    @data.setter\n    def data(self, new_data):\n        self.child = new_data.child\n        return self\n\n    @property\n    def grad(self):\n        return self._grad\n\n    @grad.setter\n    def grad(self, value):\n        self._grad = value\n\n    def attr(self, attr_name):\n        if attr_name == ""grad"":\n            return self.grad\n\n        attr_val = self.child.attr(attr_name)\n        return attr_val\n\n    def __add__(self, other):\n        if isinstance(self, AutogradTensor) and not isinstance(other, AutogradTensor):\n            other = AutogradTensor(requires_grad=False).on(other, wrap=False)\n        return self.add(other)\n\n    def __iadd__(self, other):\n        result = self.add(other)\n        self.child = result.child\n        self.grad_fn = result.grad_fn\n\n    def __sub__(self, other):\n        if isinstance(self, AutogradTensor) and not isinstance(other, AutogradTensor):\n            other = AutogradTensor(requires_grad=False).on(other, wrap=False)\n        return self.sub(other)\n\n    def __isub__(self, other):\n        result = self.sub(other)\n        self.child = result.child\n        self.grad_fn = result.grad_fn\n\n    def __mul__(self, other):\n        if isinstance(self, AutogradTensor) and not isinstance(other, AutogradTensor):\n            other = AutogradTensor(requires_grad=False).on(other, wrap=False)\n        return self.mul(other)\n\n    def __neg__(self):\n        return self.neg()\n\n    def __matmul__(self, other):\n        if isinstance(self, AutogradTensor) and not isinstance(other, AutogradTensor):\n            other = AutogradTensor(requires_grad=False).on(other, wrap=False)\n\n        return self.matmul(other)\n\n    def __pow__(self, power, **kwargs):\n        return self.pow(power, **kwargs)\n\n    def __truediv__(self, other):\n        return self.div(other)\n\n    @overloaded.method\n    def __gt__(self, _self, other):\n        return _self.__gt__(other)\n\n    @overloaded.method\n    def __ge__(self, _self, other):\n        return _self.__ge__(other)\n\n    @overloaded.method\n    def __lt__(self, _self, other):\n        return _self.__lt__(other)\n\n    @overloaded.method\n    def __le__(self, _self, other):\n        return _self.__le__(other)\n\n    @overloaded.method\n    def eq(self, _self, other):\n        return _self.eq(other)\n\n    @overloaded.method\n    def relu(self, self_):\n        return self_.relu()\n\n    def __getattribute__(self, name):\n        # Automatically attaching gradient functions if they are defined in the\n        # gradients module.\n        grad_fn = getattr(gradients, name.capitalize() + ""Backward"", None)\n\n        # print(f""getattribute {name}"")\n        if grad_fn is not None:\n\n            def method_with_grad(*args, **kwargs):\n                new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                    name, self, args, kwargs\n                )\n\n                result = getattr(new_self, name)(*new_args, **new_kwargs)\n\n                # Put back SyftTensor on the tensors found in the response\n                result = hook_args.hook_response(name, result, wrap_type=type(self))\n                result.grad_fn = grad_fn(self, *args, **kwargs)\n\n                return result\n\n            return method_with_grad\n        else:\n            return object.__getattribute__(self, name)\n\n    @staticmethod\n    @overloaded.module\n    def torch(module):\n        def add(self, other):\n            return self.add(other)\n\n        module.add = add\n\n        def sub(self, other):\n            return self.sub(other)\n\n        module.sub = sub\n\n        def mul(self, other):\n            return self.mul(other)\n\n        module.mul = mul\n\n        def neg(self):\n            return self.neg()\n\n        module.neg = neg\n\n        def log(self):\n            """"""Overriding torch\'s log method.\n            """"""\n            return self.log()\n\n        module.log = log\n\n        def exp(self):\n            """"""Overriding torch\'s exp function.\n            """"""\n            return self.exp()\n\n        module.exp = exp\n\n        def sum(self, **kwargs):\n            """"""Overriding torch\'s sum function.\n            """"""\n            return self.sum(**kwargs)\n\n        module.sum = sum\n\n        def mean(self, **kwargs):\n            return self.mean(**kwargs)\n\n        module.mean = mean\n\n        def matmul(self, other):\n            return self.matmul(other)\n\n        module.matmul = matmul\n\n        def div(self, other):\n            return self.div(other)\n\n        module.div = div\n\n        def addmm(bias, input_tensor, weight):\n            if not isinstance(input_tensor, AutogradTensor):\n                input_tensor = AutogradTensor(requires_grad=False).on(input_tensor, wrap=False)\n\n            matmul = input_tensor.matmul(weight)\n            result = bias.add(matmul)\n            return result\n\n        module.addmm = addmm\n\n        @overloaded.module\n        def nn(module):\n            """"""\n            The syntax is the same, so @overloaded.module handles recursion\n            Note that we don\'t need to add the @staticmethod decorator\n            """"""\n\n            @overloaded.module\n            def functional(module):\n                def linear(*args):\n                    """"""\n                    Un-hook the function to have its detailed behaviour\n                    """"""\n                    return torch.nn.functional.native_linear(*args)\n\n                module.linear = linear\n\n                def relu(tensor):\n                    return tensor.relu()\n\n                module.relu = relu\n\n            module.functional = functional\n\n        # Modules should be registered just like functions\n        module.nn = nn\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """"""\n        Receive an instruction for a function to be applied on a AutogradTensor,\n        Perform some specific action (like logging) which depends of the\n        instruction content, replace in the args all the LogTensors with\n        their child attribute, forward the command instruction to the\n        handle_function_command of the type of the child attributes, get the\n        response and replace a AutogradTensor on top of all tensors found in\n        the response.\n        :param command: instruction of a function command: (command name,\n        <no self>, arguments[, kwargs_])\n        :return: the response of the function command\n        """"""\n\n        cmd, _, args_, kwargs_ = command\n\n        # Check that the function has not been overwritten\n        try:\n            # Try to get recursively the attributes in cmd = ""<attr1>.<attr2>.<attr3>...""\n            cmd = cls.rgetattr(cls, cmd)\n            return cmd(*args_, **kwargs_)\n        except AttributeError:\n            pass\n\n        # Replace all AutogradTensor with their child attribute\n        new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(cmd, args_, kwargs_)\n\n        # build the new command\n        new_command = (cmd, None, new_args, new_kwargs)\n\n        # Send it to the appropriate class and get the response\n        response = new_type.handle_func_command(new_command)\n\n        # Put back AutogradTensor on the tensors found in the response\n        response = hook_args.hook_response(cmd, response, wrap_type=cls)\n\n        return response\n\n    def get(self):\n        """"""Just a pass through. This is most commonly used when calling .get() on a\n        AutogradTensor which has also been shared.""""""\n        tensor = self.child.get()\n\n        if isinstance(tensor, torch.Tensor):\n            # Remove the autograd node if a simple tensor is received\n            if not tensor.is_wrapper:\n                return tensor\n            # If it\'s a wrapper, then insert the autograd under the wrapper\n            else:\n                self.child = tensor.child\n                tensor.child = self\n                return tensor\n\n        self.child = tensor\n        return self\n\n    def float_precision(self):\n        """"""Just a pass through. This is most commonly used when calling .float_precision() on a\n        AutogradTensor which has also been shared.""""""\n        self.child = self.child.float_precision()\n        # Remove the autograd node if a simple tensor is received\n        if isinstance(self.child, torch.Tensor) and not self.child.is_wrapper:\n            return self.child\n        return self\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, tensor: ""AutogradTensor"") -> tuple:\n        """"""Takes the attributes of an AutogradTensor and saves them in a tuple.\n            Or simply said, it serializes an AutogradTensor\n        Args:\n            tensor: an AutogradTensor.\n\n        Returns:\n            tuple: a tuple holding the unique attributes of the AutogradTensor.\n        """"""\n        chain = (\n            syft.serde.msgpack.serde._simplify(worker, tensor.child)\n            if hasattr(tensor, ""child"")\n            else None\n        )\n\n        return (\n            syft.serde.msgpack.serde._simplify(worker, tensor.id),\n            chain,\n            tensor.requires_grad,\n            tensor.preinitialize_grad,\n            syft.serde.msgpack.serde._simplify(worker, tensor.grad_fn),\n            # tensor.local_autograd,\n            syft.serde.msgpack.serde._simplify(worker, tensor.tags),\n            syft.serde.msgpack.serde._simplify(worker, tensor.description),\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""AutogradTensor"":\n        """"""\n            This function reconstructs (deserializes) an AutogradTensor given its\n        attributes in form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the AutogradTensor\n        Returns:\n            AutogradTensor: an AutogradTensor\n        Examples:\n            shared_tensor = detail(data)\n        """"""\n        (\n            tensor_id,\n            chain,\n            requires_grad,\n            preinitialize_grad,\n            grad_fn,\n            tags,\n            description,\n        ) = tensor_tuple\n\n        if chain is not None:\n            chain = syft.serde.msgpack.serde._detail(worker, chain)\n\n        tensor = AutogradTensor(\n            owner=worker,\n            id=syft.serde.msgpack.serde._detail(worker, tensor_id),\n            requires_grad=requires_grad,  # ADDED!\n            preinitialize_grad=preinitialize_grad,\n            # local_autograd=local_autograd,\n            grad_fn=syft.serde.msgpack.serde._detail(worker, grad_fn),\n            data=chain,  # pass the de-serialized data\n            tags=syft.serde.msgpack.serde._detail(worker, tags),\n            description=syft.serde.msgpack.serde._detail(worker, description),\n        )\n\n        return tensor\n\n\nregister_type_rule({AutogradTensor: one})\nregister_forward_func({AutogradTensor: get_child})\nregister_backward_func(\n    {AutogradTensor: lambda i, **kwargs: AutogradTensor(data=i).on(i, wrap=False)}\n)\n'"
syft/frameworks/torch/tensors/interpreters/build_gradients.py,0,"b'# This script reads in derivative definitions from derivatives.yaml and writes appropriately\n# constructed GradFunc classes to gradients.py. It could probably be optimized.\n\nimport re\nimport yaml\n\ntab = "" "" * 4\n\n\ndef split_signature(signature):\n    return re.match(r""^(\\S+)\\((.+)\\)"", signature).groups()\n\n\n# There is probably a better way to do this\n# Need to handle keyword arguments as well\ndef construct_grad_fn_class(grad_def):\n    lines = []\n\n    name, arguments = split_signature(grad_def[""name""])\n\n    # This won\'t work right if keyword arguments are present. I should refactor\n    # this with it\'s own function. TODO\n    input_args = arguments.split("", "")\n    signature = arguments.replace(""self"", ""self_"")\n\n    lines.append(f""class {name.capitalize()}Backward(GradFunc):"")\n    lines.append(tab + f""def __init__(self, {signature}):"")\n    lines.append(2 * tab + f""super().__init__(self, {signature})"")\n    for arg in signature.split("", ""):\n        lines.append(2 * tab + f""self.{arg} = {arg}"")\n    lines.append("""")\n\n    lines.append(tab + ""def gradient(self, grad):"")\n    for arg in input_args:\n        formula = grad_def[arg]\n        for in_arg in input_args:\n            formula = formula.replace(in_arg, f""self.{in_arg}"")\n        formula = formula.replace("".self"", "".self_"").replace(""result"", ""self.result"")\n        lines.append(2 * tab + f""grad_{arg.replace(\'self\', \'self_\')} = {formula}"")\n    grad_signature = "", "".join([f""grad_{arg}"".replace(""self"", ""self_"") for arg in input_args])\n    lines.append(2 * tab + f""return ({grad_signature},)"")\n\n    return ""\\n"".join(lines) + ""\\n""\n\n\nif __name__ == ""__main__"":\n    with open(""syft/frameworks/torch/tensors/interpreters/derivatives.yaml"", ""r"") as f:\n        derivatives = yaml.safe_load(f.read())\n\n    with open(""syft/frameworks/torch/tensors/interpreters/gradients.py"", ""w"") as f:\n        grad_fn_map = {}\n        f.write(""# This file is generated from build_gradients.py\\n\\n"")\n        f.write(""from . gradients_core import *\\n\\n"")\n        for definition in derivatives:\n            f.write(construct_grad_fn_class(definition))\n            f.write(""\\n"")\n'"
syft/frameworks/torch/tensors/interpreters/gradients.py,0,"b'from .gradients_core import GradFunc\nfrom .gradients_core import apply_dim_transformations\n\n\nclass AddBackward(GradFunc):\n    def __init__(self, self_, other):\n        super().__init__(self, self_, other)\n        self.self_ = self_\n        self.other = other\n\n    def gradient(self, grad):\n        grad_self = grad.copy()\n        grad_other = grad.copy() if isinstance(self.self_, type(self.other)) else None\n\n        if self.self_.shape != self.other.shape:\n            grad_self, grad_other = apply_dim_transformations(\n                grad_self, grad_other, self.self_.shape, self.other.shape\n            )\n\n        return (grad_self, grad_other)\n\n\nclass SubBackward(GradFunc):\n    def __init__(self, self_, other):\n        super().__init__(self, self_, other)\n        self.self_ = self_\n        self.other = other\n\n    def gradient(self, grad):\n        grad_self = grad.copy()\n        grad_other = grad * -1 if isinstance(self.self_, type(self.other)) else None\n\n        if self.self_.shape != self.other.shape:\n            grad_self, grad_other = apply_dim_transformations(\n                grad_self, grad_other, self.self_.shape, self.other.shape\n            )\n        return (grad_self, grad_other)\n\n\nclass SumBackward(GradFunc):\n    """"""Tensor Sum backward gradient class\n    """"""\n\n    def __init__(self, self_, **kwargs):\n        super().__init__(self, self_)\n        self.self_ = self_\n        self.kwargs = kwargs\n\n    def gradient(self, grad):\n        self_grad = self.self_ * 0 + 1\n        return (self_grad * grad,)\n\n\nclass AsinBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * (-self.self_ * self.self_ + 1).rsqrt()\n        return (grad_self_,)\n\n\nclass LogBackward(GradFunc):\n    """"""Log backward gradient class\n    """"""\n\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * (1 / self.self_)\n        return (grad_self_,)\n\n\nclass ExpBackward(GradFunc):\n    """"""Exp backward gradient class\n    """"""\n\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * self.self_.exp()\n        return (grad_self_,)\n\n\nclass MulBackward(GradFunc):\n    def __init__(self, self_, other):\n        super().__init__(self, self_, other)\n        self.self_ = self_\n        self.other = other\n\n    def gradient(self, grad):\n        grad_self_ = grad * self.other\n        grad_other = grad * self.self_ if isinstance(self.self_, type(self.other)) else None\n        return (grad_self_, grad_other)\n\n\nclass NegBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * -1\n        return (grad_self_,)\n\n\nclass DivBackward(GradFunc):\n    def __init__(self, self_, other):\n        super().__init__(self, self_, other)\n        self.self_ = self_\n        self.other = other\n\n    def gradient(self, grad):\n        # assert isinstance(self.other, int)\n        grad_self_ = grad / self.other\n        return (grad_self_,)\n\n\nclass PowBackward(GradFunc):\n    def __init__(self, self_, power):\n        super().__init__(self, self_, power)\n        self.self_ = self_\n        self.power = power\n\n    def gradient(self, grad):\n        power = self.power\n        return (power * self.self_ ** (power - 1) * grad,)\n\n\nclass MatmulBackward(GradFunc):\n    def __init__(self, self_, other):\n        super().__init__(self, self_, other)\n        self.self_ = self_\n        self.other = other\n\n    def gradient(self, grad):\n        grad_self_ = grad @ self.other.t()\n        grad_other = self.self_.t() @ grad if isinstance(self.self_, type(self.other)) else None\n        return (grad_self_, grad_other)\n\n\nclass TBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        return (grad.t(),)\n\n\nclass SigmoidBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * self.self_.sigmoid() * (1 - self.self_.sigmoid())\n        return (grad_self_,)\n\n\nclass SinBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * self.self_.cos()\n        return (grad_self_,)\n\n\nclass SinhBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * self.self_.cosh()\n        return (grad_self_,)\n\n\n# class SqrtBackward(GradFunc):\n#     def __init__(self, self_):\n#         super().__init__(self, self_)\n#         self.self_ = self_\n#\n#     def gradient(self, grad):\n#         TODO: Broken as of Garbage Collection for `AutoGradTensor` (#3387)\n#         grad_self_ = grad / (2 * self.result)\n#         return (grad_self_,)\n\n\nclass TanhBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        grad_self_ = grad * (1 - self.self_.tanh() ** 2)\n        return (grad_self_,)\n\n\nclass ReluBackward(GradFunc):\n    def __init__(self, self_):\n        super().__init__(self, self_)\n        self.self_ = self_\n\n    def gradient(self, grad):\n        zero = self.self_ * 0\n        gt_zero = self.self_ > zero\n        return (gt_zero * grad,)\n'"
syft/frameworks/torch/tensors/interpreters/gradients_core.py,1,"b'# Module for implementing gradients used in the autograd system\n\nimport syft\nfrom syft.workers.abstract import AbstractWorker\nfrom syft.serde.syft_serializable import SyftSerializable\n\nfrom . import gradients\n\n__all__ = [""GradFunc"", ""apply_dim_transformations""]\n\n\ndef forward_grad(tensor):\n    ## tensor here should be an AutogradTensor or a Tensor where we can set .grad\n\n    try:\n        grad_fn = tensor.grad_fn\n    except AttributeError:\n        return None\n\n    # If a tensor doesn\'t have a grad_fn already attached to it, that means\n    # it\'s a leaf of the graph and we want to accumulate the gradient\n    if grad_fn is None and tensor.requires_grad:\n        return Accumulate(tensor)\n    else:\n        return grad_fn\n\n\nclass GradFunc(SyftSerializable):\n    def __init__(self, *args):\n        # This part builds our graph. It takes grad functions (if they exist)\n        # from the input arguments and builds a tuple pointing to them. This way\n        # we can use .next_functions to traverse through the entire graph.\n        # _attributes is a private list that records all the attributes (required for\n        # serialization/deserialization)\n\n        self.next_functions = tuple(\n            filter(lambda x: x is not None, (forward_grad(arg) for arg in args))\n        )\n        # self.result = None TODO: Broken as of Garbage Collection for `AutoGradTensor` (#3387)\n        self._attributes = []\n\n    def gradient(self, grad):\n        raise NotImplementedError\n\n    def __call__(self, grad):\n        return self.gradient(grad)\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n    def __setattr__(self, name, value):\n        self.__dict__[name] = value\n\n        # add attributes of grad function to _attributes list\n        # essential for serialization and deserialization\n        if name not in {""next_functions"", ""result"", ""_attributes""}:\n            self._attributes.append(value)\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, grad_fn) -> tuple:\n        """""" Takes the attributes of a grad_fn object and saves them in a tuple\n            Every gradient function class that extends `GradFunc` uses this function to\n            simplify the attributes.\n\n        Args:\n            grad_fn: gradient function object (AddBackward, SubBackward, etc.)\n\n        Returns:\n            grad_fn_attrs: a tuple containing all the simplified attributes\n            of gradient function\n        """"""\n\n        # Add class name of the grad_fn object at the begining of attributes list.\n        # Essential while deserializing the object\n        cls = grad_fn.__class__.__name__\n        grad_fn_attrs = [cls] + grad_fn._attributes\n\n        # Simplify the attributes list\n        grad_fn_attrs = syft.serde.msgpack.serde._simplify(worker, grad_fn_attrs)\n\n        return grad_fn_attrs\n\n    @staticmethod\n    def detail(worker: AbstractWorker, gradfn_tuple):\n        """""" This function reconstructs (deserializes) the gradient function object,\n         given its attributes in the form of a tuple\n\n         Args:\n            gradfn_tuple: a tuple containing all the simplified attributes of a\n            grad function (along with the class name at begining)\n\n        Returns:\n            A correct gradient function object\n\n        """"""\n        # Detail and extract the class name and attributes from the tuple\n        grad_fn_attrs = []\n        cls, *grad_fn_attrs = syft.serde.msgpack.serde._detail(worker, gradfn_tuple)\n\n        if cls == ""GradFunc"":\n            cls = GradFunc\n        else:\n            cls = getattr(gradients, cls)\n\n        return cls(*grad_fn_attrs)\n\n\n# Accumulate gradients at graph leafs\nclass Accumulate:\n    def __init__(self, tensor):\n        # Note that tensor here should be an AutogradTensor so we can update\n        # .grad appropriately\n        self.next_functions = []\n        self.tensor = tensor\n\n    def __call__(self, grad):\n        if self.tensor.grad is not None:\n            self.tensor.grad.add_(grad.child)\n        else:\n            self.tensor.grad = grad.child.copy()\n        return ()\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\ndef get_mismatch_dims(\n    x_shape, y_shape, x_squash_dims, x_squash_keep_dims, y_squash_keep_dims, current_dim=None\n):\n    """"""\n    Given two tensors shape, build three lists referencing specific dimension indices where\n    the dimension for the two tensors is not the same:\n\n    Note that we assume len(x_shape) <= len(y_shape) and inverse x and y if necessary.\n\n    1. If one tensor has a longer shape (so lives in higher dimension)(so it is y because\n    len(x_shape) <= len(y_shape)), then all the extra dimensions indices end up in x_squash_dims.\n    Indeed, x will be automatically expanded in all the extra dimensions at forward operation time\n    to match the tensor with the highest dimension. Example: tensor([1]) + tensor([[2], [3]]) is\n    rewritten by torch as tensor([[1], [1]]) + tensor([[2], [3]]). So we need to remove all these\n    dimensions of expansion to get back the gradient of x.\n\n    2. If x or y has a 1 in some dimension and doesn\'t match the other tensor\n    (ex: torch.Size([3, 7, 5]) and torch.Size([3, 1, 5])), then when an operation is called the\n    tensor is expanded on this dimension, so we need to register this information to do the same\n    squashing as before but we keep the dimension this time. This apply to both tensors and hence\n    we use the lists x_squash_keep_dims and y_squash_keep_dims\n\n    3. current_dim is used to keep count of the dimension, as we iterate on the dimension starting\n    from the highest (the most right one) down to zero. Generally, remember that the shape tuple\n    should match when align on the right, unless there is a 1 instead of the other value: shapes\n    (5,4,3,2) and (4,1,2) are compatible, (5,4,3,2) and (5,4,2,2) are not.\n    """"""\n    # Initialize the dimension if needed\n    if current_dim is None:\n        current_dim = max(len(x_shape), len(y_shape)) - 1\n        # Inverse x and y if needed to satisfy len(x_shape) <= len(y_shape)\n        if len(x_shape) > len(y_shape):\n            return get_mismatch_dims(\n                y_shape, x_shape, x_squash_dims, x_squash_keep_dims, y_squash_keep_dims, current_dim\n            )\n    if len(y_shape) == 0:  # implies also len(x_shape) == 0\n        return\n    elif len(x_shape) == 0:  # elif implies now len(y_shape) > 0 (See case 1.)\n        x_squash_dims.append(current_dim)\n        get_mismatch_dims(\n            x_shape,\n            y_shape[:-1],\n            x_squash_dims,\n            x_squash_keep_dims,\n            y_squash_keep_dims,\n            current_dim - 1,\n        )\n    else:  # implies len(x_shape) > 0 and len(y_shape) > 0 (See case 2.)\n        if x_shape[-1] != y_shape[-1]:\n            if x_shape[-1] == 1:\n                x_squash_keep_dims.append(current_dim)\n            elif y_shape[-1] == 1:\n                y_squash_keep_dims.append(current_dim)\n        get_mismatch_dims(\n            x_shape[:-1],\n            y_shape[:-1],\n            x_squash_dims,\n            x_squash_keep_dims,\n            y_squash_keep_dims,\n            current_dim - 1,\n        )\n\n\ndef apply_dim_transformations(grad_self, grad_other, self_shape, other_shape):\n    """"""\n    Given computed gradients and initial shapes, reshape the gradients to match the\n    initial shapes by reverse engineering the expansion operations made by PyTorch\n    when operating two tensors with different shapes.\n\n    Args:\n        grad_self: computed gradient for self\n        grad_other: computed gradient for other\n        self_shape: initial shape for self\n        other_shape: initial shape for other\n\n    Returns:\n        grad_self, grad_other with the proper shape\n    """"""\n    short_squash_dims = []\n    short_squash_keep_dims = []\n    long_squash_keep_dims = []\n    get_mismatch_dims(\n        self_shape, other_shape, short_squash_dims, short_squash_keep_dims, long_squash_keep_dims\n    )\n\n    # Flip self and other if needed\n    if len(self_shape) <= len(other_shape):\n        short_grad, long_grad = grad_self.child, grad_other.child\n    else:\n        short_grad, long_grad = grad_other.child, grad_self.child\n\n    # Reduce dimensions by summations\n    if short_squash_keep_dims:\n        short_grad = short_grad.sum(dim=short_squash_keep_dims, keepdim=True)\n    if short_squash_dims:\n        short_grad = short_grad.sum(dim=short_squash_dims)\n    if long_squash_keep_dims:\n        long_grad = long_grad.sum(dim=long_squash_keep_dims, keepdim=True)\n\n    # Reverse the flip\n    if len(self_shape) <= len(other_shape):\n        grad_self.child, grad_other.child = short_grad, long_grad\n    else:\n        grad_self.child, grad_other.child = long_grad, short_grad\n\n    return grad_self, grad_other\n'"
syft/frameworks/torch/tensors/interpreters/hook.py,0,"b'from syft.generic.frameworks.hook import hook_args\nfrom syft.generic.abstract.tensor import AbstractTensor\n\n\nclass HookedTensor(AbstractTensor):\n    """"""HookedTensor is an abstraction which should not be used directly on its own. Its purpose\n    is only to allow other tensors to extend it so that they automatically have all of the Torch\n    method hooked without having to add it to the hook.py file.\n    """"""\n\n    def __init__(self, owner=None, id=None, tags=None, description=None, verbose=False):\n        """"""Initializes a HookedTensor.\n\n        Args:\n            owner (BaseWorker): An optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            id (str or int): An optional string or integer id of the LargePrecisionTensor.\n            tags (list): list of tags for searching.\n            description (str): a description of this tensor.\n        """"""\n        super().__init__(id=id, owner=owner, tags=tags, description=description)\n        self.verbose = verbose\n\n\n### Register the tensor with hook_args.py ###\nhook_args.default_register_tensor(HookedTensor)\n'"
syft/frameworks/torch/tensors/interpreters/native.py,21,"b'from typing import Union, List\nimport weakref\nimport warnings\n\nimport torch\n\nimport syft\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.frameworks.torch.tensors.interpreters.paillier import PaillierTensor\nfrom syft.messaging.message import TensorCommandMessage\nfrom syft.generic.frameworks.types import FrameworkTensor\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.abstract.hookable import hookable\nfrom syft.generic.pointers.pointer_tensor import PointerTensor\nfrom syft.generic.utils import memorize\nfrom syft.workers.base import BaseWorker\n\nfrom syft.exceptions import PureFrameworkTensorFoundError\nfrom syft.exceptions import InvalidTensorForRemoteGet\n\n\ndef _get_maximum_precision():\n    """"""This function returns the maximum value allowed for precision fractions before the\n    chain decides to use LPT.\n\n    This function can be overridden if the setup requires the use of LargePrecisionTensor\n    from a smaller precision.\n\n    The default value is the size of torch.long\n\n    Returns:\n        The maximum value for precision allowed in this setup\n    """"""\n    return default_pytorch_maximum_precision()\n\n\ndef default_pytorch_maximum_precision():\n    """"""Dealing with integers > 2**63-1 is not fun with precision tensors.\n    """"""\n    return 63\n\n\nclass TorchTensor(AbstractTensor):\n    """"""Add methods to this tensor to have them added to every torch.Tensor object.\n\n    This tensor is simply a more convenient way to add custom functions to\n    all Torch tensor types. When you add a function to this tensor, it will\n    be added to EVERY native torch tensor type (i.e. torch.Torch) automatically\n    by the TorchHook (which is in frameworks/torch/hook.py).\n\n    Note: all methods from AbstractTensor will also be included because this\n    tensor extends AbstractTensor. So, if you\'re looking for a method on\n    the native torch tensor API but it\'s not listed here, you might try\n    checking AbstractTensor.\n    """"""\n\n    origin = None\n    id_at_origin = None\n\n    def has_child(self):\n        return hasattr(self, ""child"")\n\n    def trigger_origin_backward_hook(self, origin: str, id_at_origin: int):\n        """"""\n        This hook is triggered when a tensor which was received from a sender has\n        a gradient update. It will send back to this sender and his original tensor\n        this gradient value to be set remotely. Also, because this is triggered during\n        backward(), the backward command is also forwarded back.\n\n        Args:\n            origin (str): id of the worker where this tensor comes from\n            id_at_origin (int): what was its original id\n        """"""\n\n        def trigger_origin_backward(grad):\n            """"""\n            The function setting back the gradient and calling backward\n\n            Args:\n                grad: the gradient tensor being set\n            """"""\n\n            location = self.owner.get_worker(origin)\n\n            # set gradient at the origin\n            message = TensorCommandMessage.computation(""set_grad"", id_at_origin, (grad,), {}, None)\n            self.owner.send_msg(message=message, location=location)\n\n            # call backward()\n            message = TensorCommandMessage.computation(""backward"", id_at_origin, (grad,), {}, None)\n            self.owner.send_msg(message=message, location=location)\n\n        return trigger_origin_backward\n\n    def set_grad(self, grad):\n        self.grad = grad\n\n    @property\n    def tags(self):\n        if self.has_child():\n            return self.child.tags\n        else:\n            if not hasattr(self, ""_tags""):\n                self._tags = None\n            return self._tags\n\n    @tags.setter\n    def tags(self, new_tags):\n        if self.has_child():\n            if new_tags is not None:\n                self.child.tags = set(new_tags)\n            else:\n                self.child.tags = set()\n        else:\n            self._tags = new_tags\n\n    @property\n    def description(self):\n        if self.has_child():\n            return self.child.description\n        else:\n            if not hasattr(self, ""_description""):\n                self._description = None\n            return self._description\n\n    @description.setter\n    def description(self, new_desc):\n        if self.has_child():\n            self.child.description = new_desc\n        else:\n            self._description = new_desc\n\n    @property\n    def shape(self):\n        if self.is_wrapper:\n            return self.child.shape\n        else:\n            return self.native_shape\n\n    @property\n    def data(self):\n        if self.is_wrapper:\n            return self.child.data\n        else:\n            return self.native_data\n\n    @property\n    def grad(self):\n        if self.is_wrapper:\n            child_grad = self.child.grad\n            if child_grad is None:\n                return None\n            else:\n                if child_grad.is_wrapper:\n                    return child_grad\n                else:\n                    return child_grad.wrap()\n        else:\n            to_return = self.native_grad\n\n            # good to ensure that the ID stays consistent\n            # not 100% this is required but it\'s at least\n            # good practice\n            try:\n                to_return.id = self.grad_id\n            except AttributeError:\n                if to_return is not None and hasattr(to_return, ""id""):\n                    self.grad_id = to_return.id\n\n            return to_return\n\n    @grad.setter\n    def grad(self, new_grad):\n\n        # If grad is not a pure torch tensor you need to store the chain in a\n        # specific place otherwise it will get deleted\n        if new_grad is not None and (\n            not isinstance(new_grad, torch.Tensor) or hasattr(new_grad, ""child"")\n        ):\n            self.child.grad = new_grad  # .wrap()\n        else:\n            if hasattr(self, ""native_grad""):\n                with torch.no_grad():\n                    self.native_grad = new_grad\n            elif new_grad is not None:\n                self.native_grad = new_grad\n        return self\n\n    def __str__(self) -> str:\n        if self.has_child():\n            if self.is_wrapper:\n                return ""(Wrapper)>"" + self.child.__str__()\n            else:\n                return type(self).__name__ + "">"" + self.child.__str__()\n        else:\n            return self.native___str__()\n\n    def __repr__(self) -> str:\n        if self.has_child():\n            if self.is_wrapper:\n                return ""(Wrapper)>"" + self.child.__str__()\n            else:\n                return type(self).__name__ + "">"" + self.child.__repr__()\n        else:\n            out = self.native___repr__()\n\n            big_repr = False\n\n            if self.tags is not None and len(self.tags):\n                big_repr = True\n                out += ""\\n\\tTags: ""\n                for tag in self.tags:\n                    out += str(tag) + "" ""\n\n            if self.description is not None:\n                big_repr = True\n                out += ""\\n\\tDescription: "" + str(self.description).split(""\\n"")[0] + ""...""\n\n            if big_repr:\n                out += ""\\n\\tShape: "" + str(self.shape)\n\n            return out\n\n    def __eq__(self, other):\n        return self.eq(other)\n\n    @property\n    def id(self):\n        if self.is_wrapper:\n            return self.child.id\n        else:\n            try:\n                return self._id\n            except AttributeError:\n                self._id = syft.ID_PROVIDER.pop()\n                return self._id\n\n    @property\n    def gc(self):\n        return self.garbage_collection\n\n    @gc.setter\n    def gc(self, flag):\n        self.garbage_collection = flag\n\n    @property\n    def disable_gc(self):\n        self.child.garbage_collect_data = False\n        self.garbage_collection = False\n        return self\n\n    @property\n    def garbage_collection(self):\n        if not self.has_child():\n            if hasattr(self, ""ptr"") and self.ptr is not None:\n                self.child = self.ptr\n                self.child.garbage_collect_data = True\n        return self.child.garbage_collect_data\n\n    @garbage_collection.setter\n    def garbage_collection(self, flag):\n        if not self.has_child():\n            if hasattr(self, ""ptr"") and self.ptr is not None:\n                self.child = self.ptr\n        self.child.garbage_collect_data = flag\n\n    @id.setter\n    def id(self, new_id):\n        if self.is_wrapper:\n            self.child.id = new_id\n        else:\n            self._id = new_id\n\n    def get_class_attributes(self):\n        """"""\n        Return class attributes for torch tensors\n        """"""\n        return {""type"": self.dtype}\n\n    def _is_parameter(self):\n        """"""\n        Utility method to test if the tensor is in fact a Parameter\n        """"""\n        return isinstance(self, torch.nn.Parameter)\n\n    # Fix handle_command_function to correct this. #2637\n    @staticmethod\n    @overloaded.module\n    def torch(module):\n        def roll(tensor, shifts, **kwargs):\n            int_shifts = int(shifts.item())\n            return torch.native_roll(tensor, int_shifts, **kwargs)\n\n        module.roll = roll\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """"""\n        Operates as a router for functions. A function call always starts\n        by being handled here and 3 scenarii must be considered:\n\n        Real Torch tensor:\n            The arguments of the function are real tensors so we should\n            run the native torch command\n\n        Torch wrapper:\n            The arguments are just wrappers at the top of a chain\n            (ex: wrapper>LoggingTensor>Torch tensor), so just forward\n            the instruction to the next layer type in the chain (in\n            the example above to LoggingTensor.handle_func_command),\n            get the response and replace a wrapper on top of all tensors\n            found in the response.\n\n        Syft Tensor:\n            The arguments are syft tensors of same type: this can happen\n            if at any node of the chain where some function is forwarded,\n            the handle_func_command modify the function and make a new\n            call but keeps the arguments ""un-wrapped"". Making a new call\n            means that by default the command is treated here in the\n            global router.\n\n        :param command: instruction of a function command: (command name,\n        <no self>, arguments[, kwargs_])\n        :return: the response of the function command\n        """"""\n        cmd, _, args_, kwargs_ = command\n\n        try:  # will work if tensors are wrappers\n\n            # Replace all torch tensor with their child attribute\n            # Note that we return also args_type which helps handling case 3 in the docstring\n            new_args, new_kwargs, new_type, args_type = hook_args.unwrap_args_from_function(\n                cmd, args_, kwargs_, return_args_type=True\n            )\n            # This handles case 3: it redirects the command to the appropriate class depending\n            # of the syft type of the arguments and returns\n            if args_type not in FrameworkTensor:\n                return args_type.handle_func_command(command)\n            # build the new command\n            new_command = (cmd, None, new_args, new_kwargs)\n            # Send it to the appropriate class and get the response\n            try:\n                response = new_type.handle_func_command(new_command)\n            except RuntimeError:\n                # Change the library path to avoid errors on layers like AvgPooling\n                list_new_command = list(new_command)\n                list_new_command[0] = cls._fix_torch_library(new_command[0])\n                new_command = tuple(list_new_command)\n                response = new_type.handle_func_command(new_command)\n\n            # Put back the wrappers where needed\n            response = hook_args.hook_response(cmd, response, wrap_type=args_type)\n        except PureFrameworkTensorFoundError:  # means that it\'s not a wrapper but a pure tensor\n\n            # Check that the function has not been overwritten\n            try:\n                # Try to get recursively the attributes in cmd = ""<attr1>.<attr2>.<attr3>...""\n                command = cls.rgetattr(cls, cmd)\n                return command(*args_, **kwargs_)\n            except AttributeError:\n                pass\n\n            # Run the native function with the new args\n            # Note the the cmd should already be checked upon reception by the worker\n            # in the execute_command function\n            try:\n                response = cls._get_response(cmd, args_, kwargs_)\n            except AttributeError:\n                # Change the library path to avoid errors on layers like AvgPooling\n                cmd = cls._fix_torch_library(cmd)\n                response = cls._get_response(cmd, args_, kwargs_)\n\n        return response\n\n    @staticmethod\n    @memorize\n    def _get_method(cmd):\n        module = syft.local_worker.hook\n        segments = cmd.split(""."")\n        submodules = segments[:-1]\n        command = segments[-1]\n\n        for sm in submodules:\n            module = getattr(module, sm)\n\n        try:\n            command_method = getattr(module, f""native_{command}"")\n        except AttributeError:  # the function isn\'t overloaded\n            command_method = getattr(module, command)\n\n        return command_method\n\n    @staticmethod\n    def _get_response(cmd, args_, kwargs_):\n        """"""\n        Return the evaluation of the cmd string parameter\n        """"""\n        command_method = TorchTensor._get_method(cmd)\n\n        if isinstance(args_, tuple):\n            response = command_method(*args_, **kwargs_)\n        else:\n            response = command_method(args_, **kwargs_)\n\n        return response\n\n    def _fix_torch_library(cmd):\n        """"""\n        Change the cmd string parameter to use nn.functional path to avoid erros.\n        """"""\n        if ""_C._nn"" in cmd:\n            cmd = cmd.replace(""_C._nn"", ""nn.functional"")\n        return cmd\n\n    @hookable\n    def send(\n        self,\n        *location,\n        inplace: bool = False,\n        user: object = None,\n        local_autograd: bool = False,\n        requires_grad: bool = False,\n        preinitialize_grad: bool = False,\n        no_wrap: bool = False,\n        garbage_collect_data: bool = True,\n    ):\n        """"""Gets the pointer to a new remote object.\n\n        One of the most commonly used methods in PySyft, this method serializes the object upon\n        which it is called (self), sends the object to a remote worker, creates a pointer to\n        that worker, and then returns that pointer from this function.\n\n        Args:\n            location: The BaseWorker object which you want to send this object to. Note that\n                this is never actually the BaseWorker but instead a class which instantiates the\n                BaseWorker abstraction.\n            inplace: if true, return the same object instance, else a new wrapper\n            user (object,optional): User credentials to be verified.\n            local_autograd: Use autograd system on the local machine instead of PyTorch\'s\n                autograd on the workers.\n            requires_grad: Default to False. If true, whenever the remote value of this tensor\n                will have its gradient updated (for example when calling .backward()), a call\n                will be made to set back the local gradient value.\n            preinitialize_grad: Initialize gradient for AutogradTensors to a tensor\n            no_wrap: If True, wrap() is called on the created pointer\n            garbage_collect_data: argument passed down to create_pointer()\n\n        Returns:\n            A torch.Tensor[PointerTensor] pointer to self. Note that this\n            object will likely be wrapped by a torch.Tensor wrapper.\n\n        Raises:\n                SendNotPermittedError: Raised if send is not permitted on this tensor.\n        """"""\n\n        # If you send a pointer p1, you want the pointer to pointer p2 to control\n        # the garbage collection and not the remaining old p1 (here self). Because if\n        # p2 is not GCed, GCing p1 shouldn\'t delete the remote tensor, but if you\n        # want to do so, as p2 is not GCed, you can still do `del p2`.\n        # This allows to chain multiple .send().send() calls.\n\n        if len(location) == 1:\n\n            location = location[0]\n\n            if hasattr(self, ""child"") and isinstance(self.child, PointerTensor):\n                self.child.garbage_collect_data = False\n                if self._is_parameter():\n                    self.data.child.garbage_collect_data = False\n\n            ptr = self.owner.send(\n                self,\n                location,\n                local_autograd=local_autograd,\n                requires_grad=requires_grad,\n                preinitialize_grad=preinitialize_grad,\n                garbage_collect_data=garbage_collect_data,\n            )\n\n            ptr.description = self.description\n            ptr.tags = self.tags\n\n            # The last pointer should control remote GC, not the previous self.ptr\n            if hasattr(self, ""ptr"") and self.ptr is not None:\n                ptr_ = self.ptr()\n                if ptr_ is not None:\n                    ptr_.garbage_collect_data = False\n\n            # we need to cache this weak reference to the pointer so that\n            # if this method gets called multiple times we can simply re-use\n            # the same pointer which was previously created\n            self.ptr = weakref.ref(ptr)\n\n            if self._is_parameter():\n                if inplace:\n                    self.is_wrapper = True\n                    with torch.no_grad():\n                        self.set_()\n                    self.data = ptr\n                    output = self\n                else:\n                    if no_wrap:\n                        raise ValueError(""Parameters can\'t accept no_wrap=True"")\n                    wrapper = torch.Tensor()\n                    param_wrapper = torch.nn.Parameter(wrapper)\n                    param_wrapper.is_wrapper = True\n                    with torch.no_grad():\n                        param_wrapper.set_()\n                    param_wrapper.data = ptr\n                    output = param_wrapper\n            else:\n                if inplace:\n                    self.is_wrapper = True\n                    self.set_()\n                    self.child = ptr\n                    return self\n                else:\n                    output = ptr if no_wrap else ptr.wrap()\n\n            if self.requires_grad:\n                # This is for AutogradTensor to work on MultiPointerTensors\n                # With pre-initialized gradients, this should get it from AutogradTensor.grad\n                if preinitialize_grad:\n                    grad = output.child.grad\n                else:\n                    grad = output.attr(""grad"")\n\n                output.grad = grad\n\n                # Because of the way PyTorch works, .grad is prone to\n                # create entirely new Python objects for the tensor, which\n                # inadvertently deletes our custom attributes (like .child)\n                # But, if we keep a backup reference around, PyTorch seems\n                # to re-use it, which means .grad keeps the attributes we\n                # want it to keep. #HackAlert\n                output.backup_grad = grad\n\n            if local_autograd:\n                output = syft.AutogradTensor(data=output, preinitialize_grad=preinitialize_grad).on(\n                    output\n                )\n\n        else:\n\n            children = []\n            for loc in location:\n                children.append(self.clone().send(loc, no_wrap=True))\n\n            output = syft.MultiPointerTensor(children=children)\n\n            if not no_wrap:\n                output = output.wrap()\n\n        return output\n\n    def send_(self, *location, **kwargs):\n        """"""\n        Calls send() with inplace option, but only with a single location\n        :param location: workers locations\n        :return:\n        """"""\n        if len(location) > 1:\n            raise NotImplementedError(""Inplace send to several workers is currently not supported."")\n\n        return self.send(*location, inplace=True, **kwargs)\n\n    def create_pointer(\n        self,\n        location: BaseWorker = None,\n        id_at_location: (str or int) = None,\n        owner: BaseWorker = None,\n        ptr_id: (str or int) = None,\n        garbage_collect_data: bool = True,\n        shape=None,\n        **kwargs,\n    ) -> PointerTensor:\n        """"""Creates a pointer to the ""self"" torch.Tensor object.\n\n        Returns:\n            A PointerTensor pointer to self. Note that this\n            object will likely be wrapped by a torch.Tensor wrapper.\n        """"""\n        if id_at_location is None:\n            id_at_location = self.id\n\n        if ptr_id is None:\n            if location is not None and location.id != self.owner.id:\n                ptr_id = self.id\n            else:\n                ptr_id = syft.ID_PROVIDER.pop()\n\n        if shape is None:\n            shape = self.shape\n\n        ptr = syft.PointerTensor.create_pointer(\n            self, location, id_at_location, owner, ptr_id, garbage_collect_data, shape\n        )\n\n        return ptr\n\n    def mid_get(self):\n        """"""This method calls .get() on a child pointer and correctly registers the results""""""\n        if not hasattr(self, ""child""):\n            raise InvalidTensorForRemoteGet(self)\n\n        self.child.mid_get()\n\n    def remote_get(self):\n        """"""Assuming .child is a PointerTensor, this method calls .get() on the tensor\n        that the .child is pointing to (which should also be a PointerTensor)\n\n        TODO: make this kind of message forwarding generic?\n        """"""\n        if not hasattr(self, ""child""):\n            raise InvalidTensorForRemoteGet(self)\n\n        self.child.remote_get()\n\n        return self\n\n    def get(self, *args, inplace: bool = False, user=None, reason: str = """", **kwargs):\n        """"""Requests the tensor/chain being pointed to, be serialized and return\n        Args:\n            args: args to forward to worker\n            inplace: if true, return the same object instance, else a new wrapper\n            kwargs: kwargs to forward to worker\n        Raises:\n            GetNotPermittedError: Raised if get is not permitted on this tensor\n        """"""\n\n        # If it is a local tensor/chain, we don\'t need to verify permissions\n        if not isinstance(self.child, syft.PointerTensor):\n            tensor = self.child.get(*args, **kwargs)\n        else:  # Remote tensor/chain\n            tensor = self.child.get(*args, user=user, reason=reason, **kwargs)\n\n        # Clean the wrapper\n        delattr(self, ""child"")\n\n        # Parameters use .data instead of children\n        # so we need to have special support to make sure\n        # that Parmeters operate inline (because they\'re\n        # typically being managed inside of a model/optimizer\n        # so not using the same wrapper can cause the model/\n        # optimizer to lose track of where the actual weights\n        # are.\n        if isinstance(self, torch.nn.Parameter):\n            self.is_wrapper = tensor.data.is_wrapper\n            if inplace:\n                self.data = tensor.data\n                self.grad = tensor.grad\n                return self\n            else:\n                return tensor\n\n        if inplace:\n            self.set_(tensor)\n            if hasattr(tensor, ""child""):\n                self.child = tensor.child\n            else:\n                self.is_wrapper = False\n            return self\n        else:\n            return tensor\n\n    def get_(self, *args, **kwargs):\n        """"""\n        Calls get() with inplace option set to True\n        """"""\n        return self.get(*args, inplace=True, **kwargs)\n\n    def allow(self, user=None) -> bool:\n        """""" This function returns will return True if it isn\'t a PrivateTensor, otherwise it will\n        return the result of PrivateTensor\'s allow method.\n\n            Args:\n                user (object,optional): User credentials to be verified.\n\n            Returns:\n                boolean: If it is a public tensor/ allowed user, returns true, otherwise it returns\n                false.\n        """"""\n        # If it is a wrapper\n        if self.is_wrapper:\n            current_tensor = self.child\n\n            # Verify permissions for each element on the tensor chain.\n            while hasattr(current_tensor, ""child""):\n\n                # If it has a list of allowed users, verify permissions,\n                # otherwise (public tensors) go to the next.\n                if hasattr(current_tensor, ""allowed_users""):\n                    allow = current_tensor.allow(user)\n                    if not allow:\n                        return False\n\n                # Go to next element on the tensor chain\n                current_tensor = current_tensor.child\n        return True\n\n    def move(self, location: BaseWorker, requires_grad: bool = False):\n        """"""\n        Move acts on a pointer to A to move the remote value to B (=location).\n\n        Note a A will keep a copy of his value that he sent to B. This follows the\n        .send() paradigm where the local worker keeps a copy of the value he sends.\n\n        Args:\n            location: the worker where the remote value should be moved\n            requires_grad: see send() for details\n\n        Returns:\n            A pointer to the worker location\n        """"""\n        new_ptr = self.child.move(location, requires_grad)\n        # We get the owner from self.child because the owner of a wrapper is\n        # not reliable and sometimes end up being the syft.local_worker\n        self.child.owner.register_obj(self)\n        if isinstance(new_ptr, PointerTensor):\n            return new_ptr.wrap()\n        else:\n            return new_ptr\n\n    def move_(self, location: BaseWorker, requires_grad: bool = False):\n        """"""\n        Inplace version of move\n        """"""\n        new_ptr = self.move(location, requires_grad)\n        self.child = new_ptr\n        return self\n\n    def remote_send(self, location):\n        return self.child.remote_send(location).wrap()\n\n    def attr(self, attr_name):\n        """"""""""""\n\n        if self.is_wrapper:\n            attr_val = self.child.attr(attr_name)\n\n            if attr_name == ""grad"":\n                self.grad = attr_val\n        else:\n            attr_val = getattr(self, attr_name)\n\n        return attr_val\n\n    def clone(self, *args, **kwargs):\n        """"""\n        Clone should keep ids unchanged, contrary to copy\n        """"""\n        cloned_tensor = self.native_clone(*args, **kwargs)\n        cloned_tensor.id = self.id\n        cloned_tensor.owner = self.owner\n        cloned_tensor.is_wrapper = self.is_wrapper\n\n        if self.has_child():\n            cloned_tensor.child = self.child.clone(*args, **kwargs)\n\n        return cloned_tensor\n\n    def float_prec(self):\n        if isinstance(self.child, PointerTensor):\n            self.child = self.child.float_precision()\n            return self\n\n        return self.child.float_precision()\n\n    float_precision = float_prec\n\n    def float_prec_(self):\n        tensor = self.float_prec()\n        if hasattr(tensor, ""child""):\n            self.child = tensor.child\n        elif self._is_parameter():\n            self.is_wrapper = False\n            self.data = tensor\n            self.data.is_wrapper = False\n        else:\n            del self.child\n            self.set_(tensor)\n            self.is_wrapper = False\n        return self\n\n    float_precision_ = float_prec_\n\n    def private_tensor(self, *args, allowed_users: List[str], no_wrap: bool = False, **kwargs):\n        """"""\n        Convert a tensor or syft tensor to private tensor\n\n        Args:\n            *args (tuple): args to transmit to the private tensor.\n            allowed_users (list): List of allowed users.\n            no_wrap (bool): if True, we don\'t add a wrapper on top of the private tensor\n            **kwargs (dict): kwargs to transmit to the private tensor\n        """"""\n\n        if not kwargs.get(""owner""):\n            kwargs[""owner""] = self.owner\n\n        if self.is_wrapper:\n            self.child = (\n                syft.PrivateTensor(tags=self.tags, *args, **kwargs)\n                .on(self.child, wrap=False)\n                .register_credentials(tuple(allowed_users))\n            )\n            if no_wrap:\n                return self.child\n            else:\n                return self\n\n        private_tensor = (\n            syft.PrivateTensor(tags=self.tags, *args, **kwargs)\n            .on(self, wrap=False)\n            .register_credentials(tuple(allowed_users))\n        )\n        if not no_wrap:\n            private_tensor = private_tensor.wrap()\n\n        return private_tensor\n\n    def fix_prec(self, *args, no_wrap: bool = False, **kwargs):\n        """"""\n        Convert a tensor or syft tensor to fixed precision\n\n        Args:\n            *args (tuple): args to transmit to the fixed precision tensor\n            no_wrap (bool): if True, we don\'t add a wrapper on top of the fixed precision tensor\n            **kwargs (dict): kwargs to transmit to the fixed precision tensor\n        """"""\n\n        if not kwargs.get(""owner""):\n            kwargs[""owner""] = self.owner\n\n        if self.is_wrapper:\n            child = self.child.fix_prec(*args, **kwargs)\n            if no_wrap:\n                return child\n            else:\n                return child.wrap()\n\n        base = kwargs.get(""base"", 10)\n        prec_fractional = kwargs.get(""precision_fractional"", 3)\n\n        max_precision = _get_maximum_precision()\n        fpt_tensor = syft.FixedPrecisionTensor(*args, **kwargs).on(self, wrap=False).fix_precision()\n\n        if not no_wrap:\n            fpt_tensor = fpt_tensor.wrap()\n\n        return fpt_tensor\n\n    fix_precision = fix_prec\n\n    def fix_prec_(self, *args, **kwargs):\n        """"""\n        Performs an inplace transformation to fixed precision and change self to\n        be a wrapper\n\n        Args:\n            *args: args to transmit to fix_prec\n            **kwargs: kwargs to transmit to fix_prec\n\n        Returns:\n            self seen as a wrapper\n        """"""\n        # We specify id to make sure the inplace op doesn\'t change the tensor id\n        self.child = self.fix_prec(*args, no_wrap=True, id=self.id, **kwargs)\n        self.is_wrapper = True\n        return self\n\n    fix_precision_ = fix_prec_\n\n    def share(\n        self,\n        *owners: List[BaseWorker],\n        protocol: str = ""snn"",\n        field: Union[int, None] = None,\n        dtype: Union[str, None] = None,\n        crypto_provider: Union[BaseWorker, None] = None,\n        requires_grad: bool = False,\n        no_wrap: bool = False,\n    ):\n        """"""This is a pass through method which calls .share on the child.\n\n        Args:\n            owners (list): A list of BaseWorker objects determining who to send shares to.\n            protocol (str): the crypto protocol used to perform the computations (\'snn\' or \'fss\')\n            field (int or None): The arithmetic field where live the shares.\n            dtype (str or None): The dtype of shares\n            crypto_provider (BaseWorker or None): The worker providing the crypto primitives.\n            requires_grad (bool): Should we add AutogradTensor to allow gradient computation,\n                default is False.\n        """"""\n        if self.has_child():\n            chain = self.child\n\n            kwargs_ = (\n                {""requires_grad"": requires_grad} if isinstance(chain, syft.PointerTensor) else {}\n            )\n            shared_tensor = chain.share(\n                *owners,\n                protocol=protocol,\n                field=field,\n                dtype=dtype,\n                crypto_provider=crypto_provider,\n                **kwargs_,\n            )\n        else:\n            if self.type() == ""torch.FloatTensor"":\n                raise TypeError(""FloatTensor cannot be additively shared, Use fix_precision."")\n\n            shared_tensor = (\n                syft.AdditiveSharingTensor(\n                    protocol=protocol,\n                    field=field,\n                    dtype=dtype,\n                    crypto_provider=crypto_provider,\n                    owner=self.owner,\n                )\n                .on(self.copy(), wrap=False)\n                .init_shares(*owners)\n            )\n\n        if requires_grad and not isinstance(shared_tensor, syft.PointerTensor):\n            shared_tensor = syft.AutogradTensor().on(shared_tensor, wrap=False)\n\n        if not no_wrap:\n            shared_tensor = shared_tensor.wrap()\n\n        return shared_tensor\n\n    def share_(self, *args, **kwargs):\n        """"""\n        Allows to call .share() as an inplace operation\n        """"""\n        if self.has_child():\n            requires_grad = kwargs.get(""requires_grad"", False)\n            # Reset the requires_grad kwargs if the call is local\n            if not isinstance(self.child, syft.PointerTensor):\n                kwargs[""requires_grad""] = False\n\n            shared_tensor = self.child.share_(*args, **kwargs)\n\n            if requires_grad and not isinstance(shared_tensor, syft.PointerTensor):\n                shared_tensor = syft.AutogradTensor().on(shared_tensor, wrap=False)\n\n            self.child = shared_tensor\n            return self\n        else:\n            return self.share(*args, **kwargs)  # TODO change to inplace\n\n    def combine(self, *pointers):\n        """"""This method will combine the child pointer with another list of pointers\n\n        Args:\n            *pointers a list of pointers to be combined into a MultiPointerTensor\n\n        """"""\n\n        assert isinstance(self.child, PointerTensor)\n\n        ps = list(pointers)\n        ps.append(self)\n\n        return syft.combine_pointers(*ps)\n\n    def torch_type(self):\n\n        if isinstance(self, torch.Tensor) and not self.is_wrapper:\n            return self.type()\n        else:\n            return self.child.torch_type()\n\n    def encrypt(self, protocol=""mpc"", **kwargs):\n        """"""\n        This method will encrypt each value in the tensor using Multi Party\n        Computation (default) or Paillier Homomorphic Encryption\n\n        Args:\n            protocol (str): Currently supports \'mpc\' for Multi Party\n                Computation and \'paillier\' for Paillier Homomorphic Encryption\n            **kwargs:\n                With Respect to MPC accepts:\n                    workers (list): Parties involved in the sharing of the Tensor\n                    crypto_provider (syft.VirtualWorker): Worker responsible for the\n                        generation of the random numbers for encryption\n                    requires_grad (bool): If true, whenever the remote value of this tensor\n                        will have its gradient updated (for example when calling .backward()),\n                        a call will be made to set back the local gradient value.\n                    no_wrap (bool): If True, wrap() is called on the created pointer\n                    Keyword Args: To be parsed as kwargs for the .fix_prec() method\n\n                With Respect to Paillier accepts:\n                    public_key (phe.paillier.PaillierPublicKey): Can be obtained using\n                        ```public_key, private_key = sy.frameworks.torch.he.paillier.keygen()```\n        Returns:\n            An encrypted version of the Tensor following the protocol specified\n\n        Raises:\n            NotImplementedError: If protocols other than the ones mentioned above are queried\n\n        """"""\n        if protocol.lower() == ""mpc"":\n            workers = kwargs.pop(""workers"")\n            crypto_provider = kwargs.pop(""crypto_provider"")\n            requires_grad = kwargs.pop(""requires_grad"", False)\n            no_wrap = kwargs.pop(""no_wrap"", False)\n            kwargs_fix_prec = kwargs  # Rest of kwargs for fix_prec method\n\n            x_shared = self.fix_prec(**kwargs_fix_prec).share(\n                *workers,\n                crypto_provider=crypto_provider,\n                requires_grad=requires_grad,\n                no_wrap=no_wrap,\n            )\n            return x_shared\n\n        elif protocol.lower() == ""paillier"":\n            public_key = kwargs.get(""public_key"")\n\n            x = self.copy()\n            x_encrypted = PaillierTensor().on(x)  # Instantiate the class\n            x_encrypted.child.encrypt_(public_key)  # Perform Homomorphic Encryption\n\n            return x_encrypted\n\n        else:\n            raise NotImplementedError(\n                ""Currently the .encrypt() method only supports Paillier Homomorphic ""\n                ""Encryption and Secure Multi-Party Computation""\n            )\n\n    def decrypt(self, **kwargs):\n        """"""\n        This method will decrypt each value in the tensor using Multi Party\n        Computation (default) or Paillier Homomorphic Encryption\n\n        Args:\n            **kwargs:\n                With Respect to MPC accepts:\n                    None\n\n                With Respect to Paillier accepts:\n                    private_key (phe.paillier.PaillierPrivateKey): Can be obtained using\n                        ```public_key, private_key = sy.frameworks.torch.he.paillier.keygen()```\n        Returns:\n            An decrypted version of the Tensor following the protocol guessed from its type\n\n        Raises:\n            NotImplementedError: If protocols other than the ones mentioned above are queried\n\n        """"""\n\n        protocol = kwargs.get(""protocol"", None)\n        if protocol:\n            warnings.warn(""protocol should no longer be used in decrypt"")\n\n        if isinstance(self.child, (syft.FixedPrecisionTensor, syft.AutogradTensor)):\n            x_encrypted = self.copy()\n            x_decrypted = x_encrypted.get().float_prec()\n            return x_decrypted\n\n        elif isinstance(self.child, PaillierTensor):\n            # self.copy() not required as PaillierTensor\'s decrypt method is not inplace\n            private_key = kwargs.get(""private_key"")\n            return self.child.decrypt(private_key)\n\n        else:\n            raise NotImplementedError(\n                ""Currently the .decrypt() method only supports Paillier Homomorphic ""\n                ""Encryption and Secure Multi-Party Computation""\n            )\n\n    def numpy_tensor(self):\n        """"""This method will cast the current tensor to one with numpy as the underlying\n        representation. The tensor chain will be Wrapper > NumpyTensor > np.ndarray""""""\n\n        if not self.is_wrapper:\n            return syft.NumpyTensor(self.numpy())\n        else:\n            raise Exception(\n                ""Can only cast a data tensor to NumpyTensor. You called this "",\n                ""on a wrapper. Add NumpyTensor to the chain by hand if you want ""\n                ""this functionality."",\n            )\n'"
syft/frameworks/torch/tensors/interpreters/numpy.py,1,"b'import numpy as np\n\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.frameworks.torch.tensors.interpreters.hook import HookedTensor\n\n\nclass NumpyTensor(HookedTensor):\n    """"""NumpyTensor is a tensor which seeks to wrap the Numpy API with the PyTorch tensor API.\n    This is useful because Numpy can offer a wide range of existing functionality ranging from\n    large precision, custom scalar types, and polynomial arithmetic.\n    """"""\n\n    def __init__(\n        self, numpy_tensor=None, owner=None, id=None, tags=None, description=None, verbose=False\n    ):\n        """"""Initializes a NumpyTensor.\n\n        Args:\n            numpy_tensor (np.array): The numpy array which this tensor should wrap.\n            owner (BaseWorker): An optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            id (str or int): An optional string or integer id of the LargePrecisionTensor.\n            tags (list): list of tags for searching.\n            description (str): a description of this tensor.\n        """"""\n        super().__init__(id=id, owner=owner, tags=tags, description=description)\n        self.verbose = verbose\n\n        if isinstance(numpy_tensor, list):\n            numpy_tensor = np.array(numpy_tensor)\n\n        self.child = numpy_tensor\n\n    @overloaded.method\n    def mm(self, _self, other):\n        return _self.dot(other)\n\n    @overloaded.method\n    def transpose(self, _self, *dims):\n        # TODO: the semantics of the .transpose() dimensions are a bit different\n        # for Numpy than they are for PyTorch. Fix this.\n        # Related: https://github.com/pytorch/pytorch/issues/7609\n        return _self.transpose(*reversed(dims))\n\n\ndef create_numpy_tensor(numpy_tensor):\n    return NumpyTensor(numpy_tensor).wrap()\n\n\n### Register the tensor with hook_args.py ###\nhook_args.default_register_tensor(NumpyTensor)\n'"
syft/frameworks/torch/tensors/interpreters/paillier.py,9,"b'import syft as sy\nimport numpy as np\nimport torch as th\n\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.hook.hook_args import (\n    get_child,\n    register_backward_func,\n    register_forward_func,\n    register_type_rule,\n    one,\n)\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.workers.abstract import AbstractWorker\n\n\nclass PaillierTensor(AbstractTensor):\n    def __init__(self, owner=None, id=None, tags=None, description=None):\n        """"""Initializes a PaillierTensor, whose behaviour is to log all operations\n        applied on it.\n\n        Args:\n            owner: An optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            id: An optional string or integer id of the PaillierTensor.\n        """"""\n        super().__init__(id=id, owner=owner, tags=tags, description=description)\n\n    def encrypt(self, public_key):\n        """"""This method will encrypt each value in the tensor using Paillier\n        homomorphic encryption.\n\n        Args:\n            *public_key a public key created using\n                syft.frameworks.torch.he.paillier.keygen()\n        """"""\n\n        output = PaillierTensor()\n        output.child = self.child\n        output.encrypt_(public_key)\n        return output\n\n    def encrypt_(self, public_key):\n        """"""This method will encrypt each value in the tensor using Paillier\n        homomorphic encryption.\n\n        Args:\n            *public_key a public key created using\n                syft.frameworks.torch.he.paillier.keygen()\n        """"""\n\n        inputs = self.child.flatten().tolist()\n        new_child = sy.pool().map(public_key.encrypt, inputs)\n\n        data = np.array(new_child).reshape(self.child.shape)\n        self.child = data\n        self.pubkey = public_key\n\n    def decrypt(self, private_key):\n        """"""This method will decrypt each value in the tensor, returning a normal\n        torch tensor.\n\n        =Args:\n            *private_key a private key created using\n                syft.frameworks.torch.he.paillier.keygen()\n        """"""\n\n        if not isinstance(self.child, np.ndarray):\n            return th.tensor(private_key.decrypt(self.child))\n\n        inputs = self.child.flatten().tolist()\n\n        new_child = sy.pool().map(private_key.decrypt, inputs)\n\n        return th.tensor(new_child).view(*self.child.shape)\n\n    def __add__(self, *args, **kwargs):\n        """"""\n        Here is the version of the add method without the decorator: as you can see\n        it is much more complicated. However you misght need sometimes to specify\n        some particular behaviour: so here what to start from :)\n        """"""\n\n        if isinstance(args[0], th.Tensor):\n            data = self.child + args[0].numpy()\n            obj = PaillierTensor()\n            obj.child = data\n            return obj\n\n        if isinstance(self.child, th.Tensor):\n            self.child = self.child.numpy()\n\n        # Replace all syft tensor with their child attribute\n        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n            ""__add__"", self, args, kwargs\n        )\n\n        # Send it to the appropriates class and get the response\n        response = getattr(new_self, ""__add__"")(*new_args, **new_kwargs)\n\n        # Put back SyftTensor on the tensors found in the response\n        response = hook_args.hook_response(""__add__"", response, wrap_type=type(self))\n        return response\n\n    def __sub__(self, *args, **kwargs):\n        """"""\n        Here is the version of the add method without the decorator: as you can see\n        it is much more complicated. However you misght need sometimes to specify\n        some particular behaviour: so here what to start from :)\n        """"""\n\n        if isinstance(args[0], th.Tensor):\n            data = self.child - args[0].numpy()\n            obj = PaillierTensor()\n            obj.child = data\n            return obj\n\n        if isinstance(self.child, th.Tensor):\n            self.child = self.child.numpy()\n\n        # Replace all syft tensor with their child attribute\n        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n            ""__sub__"", self, args, kwargs\n        )\n\n        # Send it to the appropriate class and get the response\n        response = getattr(new_self, ""__sub__"")(*new_args, **new_kwargs)\n\n        # Put back SyftTensor on the tensors found in the response\n        response = hook_args.hook_response(""__sub__"", response, wrap_type=type(self))\n        return response\n\n    def __mul__(self, *args, **kwargs):\n        """"""\n        Here is the version of the add method without the decorator: as you can see\n        it is much more complicated. However you misght need sometimes to specify\n        some particular behaviour: so here what to start from :)\n        """"""\n\n        if isinstance(args[0], th.Tensor):\n            data = self.child * args[0].numpy()\n            obj = PaillierTensor()\n            obj.child = data\n            return obj\n\n        if isinstance(self.child, th.Tensor):\n            self.child = self.child.numpy()\n\n        # Replace all syft tensor with their child attribute\n        new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n            ""__mul__"", self, args, kwargs\n        )\n\n        # Send it to the appropriate class and get the response\n        response = getattr(new_self, ""__mul__"")(*new_args, **new_kwargs)\n\n        # Put back SyftTensor on the tensors found in the response\n        response = hook_args.hook_response(""__mul__"", response, wrap_type=type(self))\n        return response\n\n    def mm(self, *args, **kwargs):\n        """"""\n        Here is matrix multiplication between an encrypted and unencrypted tensor. Note that\n        we cannot matrix multiply two encrypted tensors because Paillier does not support\n        the multiplication of two encrypted values.\n        """"""\n        out = PaillierTensor()\n\n        # if self is not encrypted and args[0] is encrypted\n        if isinstance(self.child, th.Tensor):\n            out.child = self.child.numpy().dot(args[0].child)\n\n        # if self is encrypted and args[0] is not encrypted\n        else:\n            out.child = self.child.dot(args[0])\n\n        return out\n\n    # Method overloading\n    @overloaded.method\n    def add(self, _self, *args, **kwargs):\n        """"""\n        Here is an example of how to use the @overloaded.method decorator. To see\n        what this decorator do, just look at the next method manual_add: it does\n        exactly the same but without the decorator.\n\n        Note the subtlety between self and _self: you should use _self and NOT self.\n        """"""\n\n        return self + args[0]\n\n    # Method overloading\n    @overloaded.method\n    def sub(self, _self, *args, **kwargs):\n        """"""\n        Here is an example of how to use the @overloaded.method decorator. To see\n        what this decorator do, just look at the next method manual_add: it does\n        exactly the same but without the decorator.\n\n        Note the subtlety between self and _self: you should use _self and NOT self.\n        """"""\n\n        return self - args[0]\n\n    # Method overloading\n    @overloaded.method\n    def mul(self, _self, *args, **kwargs):\n        """"""\n        Here is an example of how to use the @overloaded.method decorator. To see\n        what this decorator do, just look at the next method manual_add: it does\n        exactly the same but without the decorator.\n\n        Note the subtlety between self and _self: you should use _self and NOT self.\n        """"""\n\n        return self * args[0]\n\n    # Module & Function overloading\n\n    # We overload two torch functions:\n    # - torch.add\n    # - torch.nn.functional.relu\n\n    @staticmethod\n    @overloaded.module\n    def torch(module):\n        """"""\n        We use the @overloaded.module to specify we\'re writing here\n        a function which should overload the function with the same\n        name in the <torch> module\n        :param module: object which stores the overloading functions\n\n        Note that we used the @staticmethod decorator as we\'re in a\n        class\n        """"""\n\n        def add(x, y):\n            """"""\n            You can write the function to overload in the most natural\n            way, so this will be called whenever you call torch.add on\n            Logging Tensors, and the x and y you get are also Logging\n            Tensors, so compared to the @overloaded.method, you see\n            that the @overloaded.module does not hook the arguments.\n            """"""\n            print(""Log function torch.add"")\n            return x + y\n\n        # Just register it using the module variable\n        module.add = add\n\n        def mul(x, y):\n            """"""\n            You can write the function to overload in the most natural\n            way, so this will be called whenever you call torch.add on\n            Logging Tensors, and the x and y you get are also Logging\n            Tensors, so compared to the @overloaded.method, you see\n            that the @overloaded.module does not hook the arguments.\n            """"""\n            print(""Log function torch.mul"")\n            return x * y\n\n        # Just register it using the module variable\n        module.mul = mul\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, tensor: ""PaillierTensor"") -> tuple:\n        """"""\n        This function takes the attributes of a LogTensor and saves them in a tuple\n        Args:\n            tensor (PaillierTensor): a LogTensor\n        Returns:\n            tuple: a tuple holding the unique attributes of the log tensor\n        Examples:\n            data = _simplify(tensor)\n        """"""\n\n        chain = None\n        if hasattr(tensor, ""child""):\n            chain = sy.serde.msgpack.serde._simplify(worker, tensor.child)\n        return tensor.id, chain\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""PaillierTensor"":\n        """"""\n        This function reconstructs a LogTensor given it\'s attributes in form of a tuple.\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the LogTensor\n        Returns:\n            PaillierTensor: a LogTensor\n        Examples:\n            logtensor = detail(data)\n        """"""\n        obj_id, chain = tensor_tuple\n\n        tensor = PaillierTensor(owner=worker, id=obj_id)\n\n        if chain is not None:\n            chain = sy.serde.msgpack.serde._detail(worker, chain)\n            tensor.child = chain\n\n        return tensor\n\n\nregister_type_rule({PaillierTensor: one})\nregister_forward_func({PaillierTensor: get_child})\nregister_backward_func({PaillierTensor: lambda i, **kwargs: PaillierTensor().on(i, wrap=False)})\n'"
syft/frameworks/torch/tensors/interpreters/polynomial.py,3,"b'# from syft.generic.abstract.tensor import AbstractTensor\n# import torch\n# import numpy as np\n# from typing import Callable, List, Union\n#\n#\n# class PolynomialTensor(AbstractTensor):\n#     """"""\n#     Tensor type to provide non-linear function approximations\n#\n#     MPC and Homomorphic Encryption are capable of performing some addition and logical operations.\n#     Non-linear functions could be approximated as a series of approximated functions of basic\n#     arithmetic operations using function approximations such as interpolation/Taylor series.\n#\n#     The polynomial tensor provides flexibility to consider every non-linear function as piecewise\n#     linear function and fit over different intervals.\n#     """"""\n#\n#     def __init__(self, function=lambda x: x, precision=10):\n#         """"""\n#         Args:\n#             function[callable,Optional]: Function to applied to function approximation\n#                 coefficients.\n#                 Used to encrypt coefficients.\n#             precision[integer]: Precision of approximated values\n#         """"""\n#\n#         self.function = function\n#         self.precision = precision\n#\n#         # Stores parameters of function approximations such as precision, degree, piecewise\n#         # functions and base function\n#         self.function_attr = {}\n#\n#         # Stores fitted function\n#         self.func_approx = {}\n#\n#         self.default_functions()\n#\n#     def default_functions(self):\n#         """"""Initializes default function approximations exp, log, sigmoid and tanh""""""\n#\n#         self.add_function(\n#             ""exp"",\n#             10,\n#             [[0, 10, 100, 10, self.fit_function], [-10, 0, 100, 10, self.fit_function]],\n#             lambda x: np.exp(x),\n#         )\n#         self.add_function(""log"", 10, [[1, 10, 100, 10, self.fit_function]], lambda x: np.log(x))\n#         self.add_function(\n#             ""sigmoid"", 10, [[-10, 10, 100, 10, self.fit_function]],\n#             (lambda x: 1 / (1 + np.exp(-x)))\n#         )\n#         self.add_function(\n#             ""tanh"",\n#             10,\n#             [[0, 10, 1000, 10, self.fit_function], [-10, 0, 1000, 10, self.fit_function]],\n#             lambda x: np.tanh(x),\n#         )\n#\n#     def add_function(self, name, degree, piecewise, function):\n#         """"""Add function to function_attr dictionary.\n#\n#         Args:\n#             name[str]: Name of function\n#             degree[int]: Degree of function\n#             piecewise[List]: List of piecewise functions in format [min_val of fit,max_val\n#                   of fit,step of fit,function to fit values]\n#             function[callable]: Base function\n#         """"""\n#\n#         self.function_attr[name + ""_degree""] = degree\n#         self.function_attr[name + ""_piecewise""] = piecewise\n#         self.function_attr[name + ""_function""] = function\n#\n#         self.func_approx[name] = self.piecewise_linear_fit(\n#             name, self.function_attr[name + ""_piecewise""]\n#         )\n#\n#     def get_val(self, name, x):\n#         """"""Get value of given function approximation\n#\n#         Args:\n#             name[str]: Name of function\n#             value[torch tensor,float,integer]: Value to be approximated\n#\n#         Returns:\n#             Approximated value using given function approximation\n#         """"""\n#\n#         value = x\n#\n#         if type(x) == torch.Tensor:\n#\n#             return value.apply_(lambda k: self.piecewise_linear_eval(self.func_approx[name], k))\n#\n#         return self.piecewise_linear_eval(self.func_approx[name], x)\n#\n#     def interpolate(\n#         self, function: Callable, interval: List[Union[int, float]], degree: int = 10\n#     ) -> np.poly1d:\n#\n#         """"""Returns a interpolated version of given function using Numpy\'s polyfit method\n#\n#         Args:\n#             function (a lambda function): Base function to be approximated\n#             interval (list of floats/integers): Interval of values to be approximated\n#             degree (Integer): Degree of polynomial approximation\n#             precision (Integer): Precision of coefficients\n#\n#         Returns:\n#             f_interpolated (Numpy poly1d): Approximated Function\n#         """"""\n#\n#         # function we wish to approximate\n#         f_real = function\n#         # interval over which we wish to optimize\n#         f_interval = interval\n#\n#         # interpolate polynomial of given max degree\n#         degree = 10\n#         coefs = np.polyfit(f_interval, f_real(f_interval), degree)\n#\n#         # reduce precision of interpolated coefficients\n#         precision = self.precision\n#         coefs = [int(x * 10 ** precision) / 10 ** precision for x in coefs]\n#\n#         # approximation function\n#         f_interpolated = np.poly1d(coefs)\n#\n#         return f_interpolated\n#\n#     def apply_coefs(self, polyinstance, function):\n#         """"""Apply a given function over Numpy interpolation instances.This function could be used\n#         to encrypt coefficients of function approximations approximated using interpolation\n#\n#         Args:\n#             polyinstance (Numpy poly1d): Interpolation instance\n#             function (Callable): Function to be applied\n#         """"""\n#\n#         val = torch.from_numpy(polyinstance.coef)\n#         return function(val)\n#\n#     def piecewise_linear_fit(self, name, array):\n#         """"""Fit a piecewise linear function. This can be used to approximate a non-linear function\n#         as separate linear functions valid for separate ranges.\n#         For instance function approximations are more accurate for exponential when separate\n#         instances of interpolation are fit between -10 to 0 and 0 to 10.\n#\n#         Args:\n#             array[2D List]: Each instance of list must take four values [min_val, steps, max_val,\n#                 function approximation method]\n#\n#         Returns:\n#             array[2D List]: Each instance of list with four\n#                       values [min_val,max_val,Approximated function]\n#         """"""\n#\n#         arguments = []\n#\n#         for element in array:\n#\n#             min_val = element[0]\n#             max_val = element[1]\n#             steps = element[2]\n#             degree = element[3]\n#             function = element[4]\n#             arguments.append(\n#                 [\n#                     min_val,\n#                     max_val,\n#                     function(name, min_val=min_val, max_val=max_val, steps=steps, degree=degree),\n#                 ]\n#             )\n#\n#         return arguments\n#\n#     def piecewise_linear_eval(self, data, x):\n#         """"""Get approximated value for a given function. This takes only scalar value.\n#         If you have a Numpy array or torch tensor consider passing it using a lambda\n#         or torch.apply_ method.\n#\n#         Args:\n#             data[2D List]: Instance of piecewise linear fit taking values [min_val, max_val,\n#                 function approximation method]\n#             x[Float or Integer]: Value to be approximated\n#         """"""\n#\n#         for element in data:\n#\n#             min_val = element[0]\n#             max_val = element[1]\n#             function = element[2]\n#\n#             if min_val <= x <= max_val:\n#\n#                 return function(x)\n#\n#     def fit_function(self, name, min_val=0, max_val=10, steps=100, degree=10) -> np.poly1d:\n#         """"""Interpolated approximation of given function\n#\n#         Args:\n#             name: Name of function as defined in self.setting\n#             min_val: Minimum range of interpolation fit\n#             max_val: Maximum range of interpolation fit\n#             steps:   Steps of interpolation fit\n#             degree: Degree of interpolation fit\n#             function: The function used to encrypt function approximation coefficients\n#\n#         Returns:\n#             f_interpolated (Numpy Poly1d): Approximated function\n#         """"""\n#\n#         fitted_function = self.interpolate(\n#             self.function_attr[name + ""_"" + ""function""],\n#             np.linspace(min_val, max_val, steps),\n#             degree=degree,\n#         )\n#\n#         fitted_function = self.apply_coefs(fitted_function, self.function)\n#\n#         return np.poly1d(fitted_function)\n#\n#     def sigmoid(self, x):\n#         """"""Method provides Sigmoid function approximation interms of Taylor Series\n#\n#         Args:\n#             x: Torch tensor\n#\n#         Returns:\n#             approximation of the sigmoid function as a torch tensor\n#         """"""\n#\n#         return (\n#             (self.function(1 / 2))\n#             + ((x) * self.function(1 / 4))\n#             - ((x ** 3) * self.function(1 / 48))\n#             + ((x ** 5) * self.function((1 / 480)))\n#         )\n#\n#     def exp(self, x):\n#         """"""\n#         Method provides exponential function approximation interms of Taylor Series\n#\n#         Args:\n#             x: Torch tensor\n#\n#         Returns:\n#             approximation of the sigmoid function as a torch tensor\n#         """"""\n#\n#         return (\n#             self.function(1)\n#             + self.function(x)\n#             + (x ** 2) * (self.function(1 / 2))\n#             + (x ** 3) * (self.function(1 / 6))\n#             + (x ** 4) * (self.function(1 / (24)))\n#             + (x ** 5) * (self.function(1 / (120)))\n#             + (x ** 6) * (self.function(1 / (840)))\n#             + (x ** 7) * (self.function(1 / (6720)))\n#         )\n'"
syft/frameworks/torch/tensors/interpreters/precision.py,35,"b'import torch\nimport warnings\n\nimport syft\nfrom syft.frameworks.torch.nn import nn\nfrom syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.generic.pointers.multi_pointer import MultiPointerTensor\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.workers.abstract import AbstractWorker\n\nfrom syft_proto.frameworks.torch.tensors.interpreters.v1.precision_pb2 import (\n    FixedPrecisionTensor as FixedPrecisionTensorPB,\n)\n\n\nclass FixedPrecisionTensor(AbstractTensor):\n    def __init__(\n        self,\n        owner=None,\n        id=None,\n        field: int = None,\n        dtype: str = ""long"",\n        base: int = 10,\n        precision_fractional: int = 3,\n        kappa: int = 1,\n        tags: set = None,\n        description: str = None,\n    ):\n        """"""Initializes a Fixed Precision tensor, which encodes all decimal point\n        values using an underlying integer value.\n\n        The FixedPrecision enables to manipulate floats over an interface which\n        supports only integers, Such as _SPDZTensor.\n\n        This is done by specifying a precision p and given a float x,\n        multiply it with 10**p before rounding to an integer (hence you keep\n        p decimals)\n\n        Args:\n            owner: An optional BaseWorker object to specify the worker on which\n                the tensor is located.\n            id: An optional string or integer id of the FixedPrecisionTensor.\n        """"""\n        super().__init__(id=id, owner=owner, tags=tags, description=description)\n\n        self.base = base\n        self.precision_fractional = precision_fractional\n        self.kappa = kappa\n        self.dtype = dtype\n        if dtype == ""long"":\n            self.field = 2 ** 64\n            self.torch_dtype = torch.int64\n        elif dtype == ""int"":\n            self.field = 2 ** 32\n            self.torch_dtype = torch.int32\n        else:\n            # Since n mod 0 is not defined\n            warnings.warn(""Prefer to use dtype instead of field"")\n            if isinstance(field, int) and field > 0:\n                if field <= 2 ** 32:\n                    self.dtype = ""int""\n                    self.field = 2 ** 32\n                    self.torch_dtype = torch.int32\n                else:\n                    self.dtype = ""long""\n                    self.field = 2 ** 64\n                    self.torch_dtype = torch.int64\n            else:\n                # Invalid args dtype and field\n                raise ValueError(\n                    ""Unsupported arg value for dtype. Use dtype=\'long\' or dtype=\'int\'.""\n                )\n\n    def get_class_attributes(self):\n        """"""\n        Specify all the attributes need to build a wrapper correctly when returning a response,\n        for example precision_fractional is important when wrapping the result of a method\n        on a self which is a fixed precision tensor with a non default precision_fractional.\n        """"""\n        return {\n            ""field"": self.field,\n            ""base"": self.base,\n            ""precision_fractional"": self.precision_fractional,\n            ""kappa"": self.kappa,\n            ""dtype"": self.dtype,\n        }\n\n    @property\n    def data(self):\n        return self\n\n    @data.setter\n    def data(self, new_data):\n        self.child = new_data.child\n        return self\n\n    @property\n    def grad(self):\n        """"""\n        Gradient makes no sense for Fixed Precision Tensor, so we make it clear\n        that if someone query .grad on a Fixed Precision Tensor it doesn\'t error\n        but returns grad and can\'t be set\n        """"""\n        return None\n\n    def backward(self, *args, **kwargs):\n        """"""Calling backward on Precision Tensor doesn\'t make sense, but sometimes a call\n        can be propagated downward the chain to an Precision Tensor (for example in\n        create_grad_objects), so we just ignore the call.""""""\n        pass\n\n    def attr(self, attr_name):\n        return self.__getattribute__(attr_name)\n\n    def fix_precision(self, check_range=True):\n        """"""This method encodes the .child object using fixed precision""""""\n\n        rational = self.child\n        upscaled = (rational * self.base ** self.precision_fractional).long()\n        if check_range:\n            assert (\n                upscaled.abs() < (self.field / 2)\n            ).all(), (\n                f""{rational} cannot be correctly embedded: choose bigger field or a lower precision""\n            )\n\n        field_element = upscaled\n        field_element.owner = rational.owner\n        self.child = field_element.type(self.torch_dtype)\n        return self\n\n    def float_precision(self):\n        """"""this method returns a new tensor which has the same values as this\n        one, encoded with floating point precision""""""\n        value = self.child.type(self.torch_dtype)\n        gate = value.native_lt(0).type(self.torch_dtype)\n\n        neg_nums = value * gate\n        pos_nums = value * (1 - gate)\n        result = (neg_nums + pos_nums).float() / (self.base ** self.precision_fractional)\n\n        return result\n\n    def truncate(self, precision_fractional, check_sign=True):\n        truncation = self.base ** precision_fractional\n\n        # We need to make sure that values are truncated ""towards 0""\n        # i.e. for a field of 100, 70 (equivalent to -30), should be truncated\n        # at 97 (equivalent to -3), not 7\n        if isinstance(self.child, AdditiveSharingTensor) or not check_sign:  # Handle FPT>(wrap)>AST\n            self.child = self.child / truncation\n            return self\n        else:\n            gate = self.child.native_lt(0).type(self.torch_dtype)\n            neg_nums = self.child / truncation\n            pos_nums = self.child / truncation\n            self.child = neg_nums * gate + pos_nums * (1 - gate)\n            return self\n\n    @overloaded.method\n    def add(self, _self, other):\n        """"""Add two fixed precision tensors together.\n        """"""\n        if isinstance(other, (int, float)):\n            scaled_int = int(other * self.base ** self.precision_fractional)\n            return getattr(_self, ""add"")(scaled_int)\n\n        if isinstance(_self, AdditiveSharingTensor) and isinstance(other, torch.Tensor):\n            # If we try to add a FPT>(wrap)>AST and a FPT>torch.tensor,\n            # we want to perform AST + torch.tensor\n            other = other.wrap()\n        elif isinstance(other, AdditiveSharingTensor) and isinstance(_self, torch.Tensor):\n            # If we try to add a FPT>torch.tensor and a FPT>(wrap)>AST,\n            # we swap operators so that we do the same operation as above\n            _self, other = other, _self.wrap()\n\n        response = getattr(_self, ""add"")(other)\n\n        return response\n\n    __add__ = add\n    __radd__ = add\n\n    def add_(self, value_or_tensor, tensor=None):\n        if tensor is None:\n            result = self.add(value_or_tensor)\n        else:\n            result = self.add(value_or_tensor * tensor)\n\n        self.child = result.child\n        return self\n\n    def __iadd__(self, other):\n        """"""Add two fixed precision tensors together.\n        """"""\n        self.child = self.add(other).child\n\n        return self\n\n    @overloaded.method\n    def sub(self, _self, other):\n        """"""Subtracts a fixed precision tensor from another one.\n        """"""\n        if isinstance(other, (int, float)):\n            scaled_int = int(other * self.base ** self.precision_fractional)\n            return getattr(_self, ""sub"")(scaled_int)\n\n        if isinstance(_self, AdditiveSharingTensor) and isinstance(other, torch.Tensor):\n            # If we try to subtract a FPT>(wrap)>AST and a FPT>torch.tensor,\n            # we want to perform AST - torch.tensor\n            other = other.wrap()\n        elif isinstance(other, AdditiveSharingTensor) and isinstance(_self, torch.Tensor):\n            # If we try to subtract a FPT>torch.tensor and a FPT>(wrap)>AST,\n            # we swap operators so that we do the same operation as above\n            _self, other = -other, -_self.wrap()\n\n        response = getattr(_self, ""sub"")(other)\n\n        return response\n\n    __sub__ = sub\n\n    def __rsub__(self, other):\n        return (self - other) * -1\n\n    def sub_(self, value_or_tensor, tensor=None):\n        if tensor is None:\n            result = self.sub(value_or_tensor)\n        else:\n            result = self.sub(value_or_tensor * tensor)\n\n        self.child = result.child\n        return self\n\n    def __isub__(self, other):\n        self.child = self.sub(other).child\n\n        return self\n\n    @overloaded.method\n    def t(self, _self, *args, **kwargs):\n        """"""Transpose a tensor. Hooked is handled by the decorator""""""\n        response = getattr(_self, ""t"")(*args, **kwargs)\n\n        return response\n\n    def mul_and_div(self, other, cmd):\n        """"""\n        Hook manually mul and div to add the truncation/rescaling part\n        which is inherent to these operations in the fixed precision setting\n        """"""\n        changed_sign = False\n        if isinstance(other, FixedPrecisionTensor):\n            assert (\n                self.precision_fractional == other.precision_fractional\n            ), ""In mul and div, all args should have the same precision_fractional""\n            assert self.base == other.base, ""In mul and div, all args should have the same base""\n\n        if isinstance(other, (int, torch.Tensor, AdditiveSharingTensor)):\n            new_self = self.child\n            new_other = other\n        elif isinstance(other, float):\n            raise NotImplementedError(\n                ""Can\'t multiply or divide a FixedPrecisionTensor with a float value""\n            )\n\n        elif isinstance(self.child, (AdditiveSharingTensor, MultiPointerTensor)) and isinstance(\n            other.child, torch.Tensor\n        ):\n            # If operands are FPT>AST and FPT>torch.tensor,\n            # we want to perform the operation on AST and torch.tensor\n            if cmd == ""mul"":\n                new_self = self.child\n            elif cmd == ""div"":\n                new_self = self.child * self.base ** self.precision_fractional\n            new_other = other\n\n        elif isinstance(other.child, (AdditiveSharingTensor, MultiPointerTensor)) and isinstance(\n            self.child, torch.Tensor\n        ):\n            # If operands are FPT>torch.tensor and FPT>AST,\n            # we swap operators so that we do the same operation as above\n            if cmd == ""mul"":\n                new_self = other.child\n                new_other = self\n            elif cmd == ""div"":\n                # TODO how to divide by AST?\n                raise NotImplementedError(\n                    ""Division of a FixedPrecisionTensor by an AdditiveSharingTensor not implemented""\n                )\n\n        elif (\n            cmd == ""mul""\n            and isinstance(self.child, (AdditiveSharingTensor, MultiPointerTensor))\n            and isinstance(other.child, (AdditiveSharingTensor, MultiPointerTensor))\n        ):\n            # If we try to multiply a FPT>torch.tensor with a FPT>AST,\n            # we swap operators so that we do the same operation as above\n            new_self, new_other, _ = hook_args.unwrap_args_from_method(""mul"", self, other, None)\n\n        else:\n            # Replace all syft tensor with their child attribute\n            new_self, new_other, _ = hook_args.unwrap_args_from_method(cmd, self, other, None)\n\n            # To avoid problems with negative numbers\n            # we take absolute value of the operands\n            # The problems could be 1) bad truncation for multiplication\n            # 2) overflow when scaling self in division\n\n            # sgn_self is 1 when new_self is positive else it\'s 0\n            # The comparison is different if new_self is a torch tensor or an AST\n            sgn_self = (new_self > 0).type(self.torch_dtype)\n            pos_self = new_self * sgn_self\n            neg_self = new_self * (sgn_self - 1)\n            new_self = neg_self + pos_self\n\n            # sgn_other is 1 when new_other is positive else it\'s 0\n            # The comparison is different if new_other is a torch tensor or an AST\n            sgn_other = (new_other > 0).type(self.torch_dtype)\n            pos_other = new_other * sgn_other\n            neg_other = new_other * (sgn_other - 1)\n            new_other = neg_other + pos_other\n\n            # If both have the same sign, sgn is 1 else it\'s 0\n            # To be able to write sgn = 1 - (sgn_self - sgn_other) ** 2,\n            # we would need to overload the __add__ for operators int and AST.\n            sgn = -((sgn_self - sgn_other) ** 2) + 1\n            changed_sign = True\n\n            if cmd == ""div"":\n                new_self *= self.base ** self.precision_fractional\n        # Send it to the appropriate class and get the response\n        response = getattr(new_self, cmd)(new_other)\n        # Put back SyftTensor on the tensors found in the response\n        response = hook_args.hook_response(\n            cmd, response, wrap_type=type(self), wrap_args=self.get_class_attributes()\n        )\n        if not isinstance(other, (int, torch.Tensor, AdditiveSharingTensor)):\n            if cmd == ""mul"":\n                # If operation is mul, we need to truncate\n                response = response.truncate(self.precision_fractional, check_sign=False)\n\n            if changed_sign:\n                # Give back its sign to response\n                pos_res = response * sgn\n                neg_res = response * (sgn - 1)\n                response = neg_res + pos_res\n\n        return response\n\n    def mul(self, other):\n        return self.mul_and_div(other, ""mul"")\n\n    __mul__ = mul\n\n    def __imul__(self, other):\n        self.child = self.mul_and_div(other, ""mul"").child\n        return self\n\n    mul_ = __imul__\n\n    def div(self, other):\n        return self.mul_and_div(other, ""div"")\n\n    __truediv__ = div\n\n    def __itruediv__(self, other):\n        self.child = self.mul_and_div(other, ""div"").child\n        return self\n\n    def pow(self, power):\n        """"""\n        Compute integer power of a number by recursion using mul\n\n        This uses the following trick:\n         - Divide power by 2 and multiply base to itself (if the power is even)\n         - Decrement power by 1 to make it even and then follow the first step\n\n        Args:\n            power (int): the exponent supposed to be an integer > 0\n        """"""\n        base = self\n\n        result = None\n        while power > 0:\n            # If power is odd\n            if power % 2 == 1:\n                result = result * base if result is not None else base\n\n            # Divide the power by 2\n            power = power // 2\n            # Multiply base to itself\n            base = base * base\n\n        return result\n\n    __pow__ = pow\n\n    def matmul(self, *args, **kwargs):\n        """"""\n        Hook manually matmul to add the truncation part which is inherent to multiplication\n        in the fixed precision setting\n        """"""\n\n        other = args[0]\n\n        if isinstance(other, FixedPrecisionTensor):\n            assert (\n                self.precision_fractional == other.precision_fractional\n            ), ""In matmul, all args should have the same precision_fractional""\n\n        if isinstance(self.child, AdditiveSharingTensor) and isinstance(other.child, torch.Tensor):\n            # If we try to matmul a FPT>AST with a FPT>torch.tensor,\n            # we want to perform AST @ torch.tensor\n            new_self = self.child\n            new_args = (other,)\n            new_kwargs = kwargs\n\n        elif isinstance(other.child, AdditiveSharingTensor) and isinstance(\n            self.child, torch.Tensor\n        ):\n            # If we try to matmul a FPT>torch.tensor with a FPT>AST,\n            # we swap operators so that we do the same operation as above\n            new_self = other.child\n            new_args = (self,)\n            new_kwargs = kwargs\n        else:\n            # Replace all syft tensor with their child attribute\n            new_self, new_args, new_kwargs = hook_args.unwrap_args_from_method(\n                ""matmul"", self, args, kwargs\n            )\n\n        # Send it to the appropriate class and get the response\n        response = getattr(new_self, ""matmul"")(*new_args, **new_kwargs)\n\n        # Put back SyftTensor on the tensors found in the response\n        response = hook_args.hook_response(\n            ""matmul"", response, wrap_type=type(self), wrap_args=self.get_class_attributes()\n        )\n\n        response = response.truncate(other.precision_fractional)\n\n        return response\n\n    __matmul__ = matmul\n    mm = matmul\n\n    def reciprocal(self):\n        ones = self * 0 + 1\n        return ones / self\n\n    # Approximations:\n    def inverse(self, iterations=8):\n        """"""\n        Computes an approximation of the matrix inversion using Newton-Schulz\n        iterations\n        """"""\n        # TODO: should we add non-approximate version if self.child is a pure tensor?\n\n        assert len(self.shape) >= 2, ""Can\'t compute inverse on non-matrix""\n        assert self.shape[-1] == self.shape[-2], ""Must be batches of square matrices""\n\n        inverse = (0.1 * torch.eye(self.shape[-1])).fix_prec(**self.get_class_attributes()).child\n\n        for _ in range(iterations):\n            inverse = 2 * inverse - inverse @ self @ inverse\n\n        return inverse\n\n    def exp(self, iterations=8):\n        r""""""\n        Approximates the exponential function using a limit approximation:\n        exp(x) = \\lim_{n -> infty} (1 + x / n) ^ n\n\n        Here we compute exp by choosing n = 2 ** d for some large d equal to\n        iterations. We then compute (1 + x / n) once and square `d` times.\n\n        Args:\n            iterations (int): number of iterations for limit approximation\n\n        Ref: https://github.com/LaRiffle/approximate-models\n        """"""\n        return (1 + self / 2 ** iterations) ** (2 ** iterations)\n\n    def sign(self):\n        return (self > 0) + (self < 0) * (-1)\n\n    @staticmethod\n    def _sigmoid_exp(tensor):\n        """"""\n        Implementation taken from FacebookResearch - CrypTen project\n\n        Compute the sigmoid using the exp approximation\n        sigmoid(x) = 1 / (1 + exp(-x))\n\n        For stability:\n            sigmoid(x) = (sigmoid(|x|) - 0.5) * sign(x) + 0.5\n\n        Ref: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/#numerically_stable_sigmoid_function # noqa: E501\n\n        Args:\n            tensor (tensor): values where sigmoid should be approximated\n        """"""\n\n        sign = tensor.sign()\n\n        # Make sure the elements are all positive\n        x = tensor * sign\n        ones = tensor * 0 + 1\n        half = ones.div(2)\n        result = (ones + (-ones * x).exp()).reciprocal()\n        return (result - half) * sign + half\n\n    @staticmethod\n    def _sigmoid_maclaurin(tensor):\n        """"""\n        Approximates the sigmoid function using Maclaurin, with polynomial\n        interpolation of degree 5 over [-8,8]\n        NOTE: This method is faster but not as precise as ""exp""\n        Ref: https://mortendahl.github.io/2017/04/17/private-deep-learning-with-mpc/#approximating-sigmoid # noqa: E501\n\n        Args:\n            tensor (tensor): values where sigmoid should be approximated\n        """"""\n\n        weights = (\n            torch.tensor([0.5, 1.91204779e-01, -4.58667307e-03, 4.20690803e-05])\n            .fix_precision(**tensor.get_class_attributes())\n            .child\n        )\n        degrees = [0, 1, 3, 5]\n\n        # initiate with term of degree 0 to avoid errors with tensor ** 0\n        one = tensor * 0 + 1\n        result = one * weights[0]\n        for i, d in enumerate(degrees[1:]):\n            result += (tensor ** d) * weights[i + 1]\n\n        return result\n\n    @staticmethod\n    def _sigmoid_chebyshev(tensor, maxval: int = 6, terms: int = 32):\n        """"""\n        Implementation taken from FacebookResearch - CrypTen project\n        Computes the sigmoid function as\n                 sigmoid(x) = (tanh(x /2) + 1) / 2\n\n        Tanh is approximated using chebyshev polynomials\n        Args:\n             maxval (int): interval width used for tanh chebyshev polynomials\n             terms (int): highest degree of Chebyshev polynomials for tanh.\n                          Must be even and at least 6.\n        """"""\n        tanh_approx = tensor._tanh_chebyshev(tensor.div(2), maxval, terms)\n\n        return tanh_approx.div(2) + 0.5\n\n    def sigmoid(tensor, method=""exp""):\n        """"""\n        Approximates the sigmoid function using a given method\n\n        Args:\n            tensor: the fixed precision tensor\n            method (str): (default = ""chebyshev"")\n                Possible values: ""exp"", ""maclaurin"", ""chebyshev""\n        """"""\n\n        sigmoid_f = getattr(tensor, f""_sigmoid_{method}"")\n\n        return sigmoid_f(tensor)\n\n    def log(self, iterations=2, exp_iterations=8):\n        """"""Approximates the natural logarithm using 8th order modified Householder iterations.\n        Recall that Householder method is an algorithm to solve a non linear equation f(x) = 0.\n        Here  f: x -> 1 - C * exp(-x)  with C = self\n\n        Iterations are computed by:\n            y_0 = some constant\n            h = 1 - self * exp(-y_n)\n            y_{n+1} = y_n - h * (1 + h / 2 + h^2 / 3 + h^3 / 6 + h^4 / 5 + h^5 / 7)\n\n        Args:\n            iterations (int): number of iterations for 6th order modified\n                Householder approximation.\n            exp_iterations (int): number of iterations for limit approximation of exp\n\n        Ref: https://github.com/LaRiffle/approximate-models\n        """"""\n\n        y = self / 31 + 1.59 - 20 * (-2 * self - 1.4).exp(iterations=exp_iterations)\n\n        # 6th order Householder iterations\n        for i in range(iterations):\n            h = [1 - self * (-y).refresh().exp(iterations=exp_iterations)]\n            for i in range(1, 5):\n                h.append(h[-1] * h[0])\n\n            y -= h[0] * (1 + h[0] / 2 + h[1] / 3 + h[2] / 4 + h[3] / 5 + h[4] / 6)\n\n        return y\n\n    @staticmethod\n    def _tanh_chebyshev(tensor, maxval: int = 6, terms: int = 32):\n        r""""""\n        Implementation taken from FacebookResearch - CrypTen project\n        Computes tanh via Chebyshev approximation with truncation.\n          tanh(x) = \\sum_{j=1}^terms c_{2j - 1} P_{2j - 1} (x / maxval)\n          where c_i is the ith Chebyshev series coefficient and P_i is ith polynomial.\n        The approximation is truncated to +/-1 outside [-maxval, maxval].\n\n        Args:\n            tensor (tensor): values where the tanh needs to be approximated\n            maxval (int): interval width used for computing chebyshev polynomials\n            terms (int): highest degree of Chebyshev polynomials.\n                         Must be even and at least 6.\n\n        More details can be found in the paper:\n           Guo, Chuan and Hannun, Awni and Knott, Brian and van der Maaten,\n           Laurens and Tygert, Mark and Zhu, Ruiyu,\n           ""Secure multiparty computations in floating-point arithmetic"", Jan-2020\n           Link: http://tygert.com/realcrypt.pdf\n\n        """"""\n\n        coeffs = syft.common.util.chebyshev_series(torch.tanh, maxval, terms)[1::2]\n        coeffs = coeffs.fix_precision(**tensor.get_class_attributes())\n        coeffs = coeffs.unsqueeze(1)\n\n        value = torch.tensor(maxval).fix_precision(**tensor.get_class_attributes())\n        tanh_polys = syft.common.util.chebyshev_polynomials(tensor.div(value.child), terms)\n        tanh_polys_flipped = tanh_polys.unsqueeze(dim=-1).transpose(0, -1).squeeze(dim=0)\n\n        out = tanh_polys_flipped.matmul(coeffs.child)\n\n        # truncate outside [-maxval, maxval]\n        gate_up = tensor > value\n        gate_down = -tensor > value\n        res = gate_up - gate_down\n        out = out.squeeze(1) * (1 - gate_up - gate_down)\n        out = res + out\n\n        return out\n\n    @staticmethod\n    def _tanh_sigmoid(tensor):\n        """"""\n        Compute the tanh using the sigmoid approximation\n\n        Args:\n            tensor (tensor): values where tanh should be approximated\n        """"""\n\n        return 2 * torch.sigmoid(2 * tensor) - 1\n\n    def tanh(tensor, method=""chebyshev""):\n        tanh_f = getattr(tensor, f""_tanh_{method}"")\n\n        return tanh_f(tensor)\n\n    # Binary ops\n    @overloaded.method\n    def __gt__(self, _self, other):\n        result = _self.__gt__(other)\n        return result.type(self.torch_dtype) * self.base ** self.precision_fractional\n\n    @overloaded.method\n    def __ge__(self, _self, other):\n        result = _self.__ge__(other)\n        return result.type(self.torch_dtype) * self.base ** self.precision_fractional\n\n    @overloaded.method\n    def __lt__(self, _self, other):\n        result = _self.__lt__(other)\n        return result.type(self.torch_dtype) * self.base ** self.precision_fractional\n\n    @overloaded.method\n    def __le__(self, _self, other):\n        result = _self.__le__(other)\n        return result.type(self.torch_dtype) * self.base ** self.precision_fractional\n\n    @overloaded.method\n    def eq(self, _self, other):\n        result = _self.eq(other)\n        return result.type(self.torch_dtype) * self.base ** self.precision_fractional\n\n    __eq__ = eq\n\n    @staticmethod\n    @overloaded.module\n    def torch(module):\n        def add(self, other):\n            return self.__add__(other)\n\n        module.add = add\n\n        def sub(self, other):\n            return self.__sub__(other)\n\n        module.sub = sub\n\n        def mul(self, other):\n            return self.__mul__(other)\n\n        module.mul = mul\n\n        def div(self, other):\n            return self.__truediv__(other)\n\n        module.div = div\n\n        def matmul(self, other):\n            return self.matmul(other)\n\n        module.matmul = matmul\n        module.mm = matmul\n\n        def addmm(bias, input_tensor, weight):\n            matmul = input_tensor.matmul(weight)\n            result = bias.add(matmul)\n            return result\n\n        module.addmm = addmm\n\n        def inverse(self):\n            return self.inverse()\n\n        module.inverse = inverse\n\n        def exp(tensor):\n            return tensor.exp()\n\n        module.exp = exp\n\n        def sigmoid(tensor):\n            return tensor.sigmoid()\n\n        module.sigmoid = sigmoid\n\n        def log(tensor):\n            return tensor.log()\n\n        module.log = log\n\n        def tanh(tensor):\n            return tensor.tanh()\n\n        module.tanh = tanh\n\n        def dot(self, other):\n            return self.__mul__(other).sum()\n\n        module.dot = dot\n\n        # You can also overload functions in submodules!\n        # Modules should be registered just like functions\n        module.nn = nn  # Handles all the overloading properly\n\n    @classmethod\n    def handle_func_command(cls, command):\n        """"""\n        Receive an instruction for a function to be applied on a FixedPrecision Tensor,\n        Perform some specific action (like logging) which depends of the\n        instruction content, replace in the args all the FPTensors with\n        their child attribute, forward the command instruction to the\n        handle_function_command of the type of the child attributes, get the\n        response and replace a FixedPrecision on top of all tensors found in\n        the response.\n        :param command: instruction of a function command: (command name,\n        <no self>, arguments[, kwargs_])\n        :return: the response of the function command\n        """"""\n        cmd, _, args_, kwargs_ = command\n\n        tensor = args_[0] if not isinstance(args_[0], (tuple, list)) else args_[0][0]\n\n        # Check that the function has not been overwritten\n        try:\n            # Try to get recursively the attributes in cmd = ""<attr1>.<attr2>.<attr3>...""\n            cmd = cls.rgetattr(cls, cmd)\n            return cmd(*args_, **kwargs_)\n        except AttributeError:\n            pass\n\n        # Replace all FixedPrecisionTensor with their child attribute\n        new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(cmd, args_, kwargs_)\n\n        # build the new command\n        new_command = (cmd, None, new_args, new_kwargs)\n\n        # Send it to the appropriate class and get the response\n        response = new_type.handle_func_command(new_command)\n\n        # Put back FixedPrecisionTensor on the tensors found in the response\n        response = hook_args.hook_response(\n            cmd, response, wrap_type=cls, wrap_args=tensor.get_class_attributes()\n        )\n\n        return response\n\n    def share(self, *owners, protocol=None, field=None, dtype=None, crypto_provider=None):\n        """"""\n        Forward the .share() command to the child tensor, and reconstruct a new\n        FixedPrecisionTensor since the command is not inplace and should return\n        a new chain\n\n        Args:\n            *owners: the owners of the shares of the resulting AdditiveSharingTensor\n            protocol: the crypto protocol used to perform the computations (\'snn\' or \'fss\')\n            field: the field size in which the share values live\n            dtype: the dtype in which the share values live\n            crypto_provider: the worker used to provide the crypto primitives used\n                to perform some computations on AdditiveSharingTensors\n\n        Returns:\n            A FixedPrecisionTensor whose child has been shared\n        """"""\n        if dtype is None:\n            dtype = self.dtype\n        else:\n            assert (\n                dtype == self.dtype\n            ), ""When sharing a FixedPrecisionTensor, the dtype of the resulting AdditiveSharingTensor \\\n                must be the same as the one of the original tensor""\n\n        tensor = FixedPrecisionTensor(owner=self.owner, **self.get_class_attributes())\n\n        tensor.child = self.child.share(\n            *owners, protocol=protocol, dtype=dtype, crypto_provider=crypto_provider, no_wrap=True\n        )\n        return tensor\n\n    def share_(self, *args, **kwargs):\n        """"""\n        Performs an inplace call to share. The FixedPrecisionTensor returned is therefore the same,\n        contrary to the classic share version\n        """"""\n        self.child = self.child.share_(*args, no_wrap=True, **kwargs)\n        return self\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, tensor: ""FixedPrecisionTensor"") -> tuple:\n        """"""Takes the attributes of a FixedPrecisionTensor and saves them in a tuple.\n\n        Args:\n            worker: the worker doing the serialization\n            tensor: a FixedPrecisionTensor.\n\n        Returns:\n            tuple: a tuple holding the unique attributes of the fixed precision tensor.\n        """"""\n        chain = None\n        if hasattr(tensor, ""child""):\n            chain = syft.serde.msgpack.serde._simplify(worker, tensor.child)\n\n        return (\n            syft.serde.msgpack.serde._simplify(worker, tensor.id),\n            syft.serde.msgpack.serde._simplify(worker, tensor.field),\n            tensor.dtype,\n            tensor.base,\n            tensor.precision_fractional,\n            tensor.kappa,\n            syft.serde.msgpack.serde._simplify(worker, tensor.tags),\n            syft.serde.msgpack.serde._simplify(worker, tensor.description),\n            chain,\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""FixedPrecisionTensor"":\n        """"""This function reconstructs a FixedPrecisionTensor given it\'s attributes in form\n        of a tuple.\n\n        Args:\n            worker: the worker doing the deserialization\n            tensor_tuple: a tuple holding the attributes of the FixedPrecisionTensor\n        Returns:\n            FixedPrecisionTensor: a FixedPrecisionTensor\n        Examples:\n            shared_tensor = detail(data)\n        """"""\n\n        (\n            tensor_id,\n            field,\n            dtype,\n            base,\n            precision_fractional,\n            kappa,\n            tags,\n            description,\n            chain,\n        ) = tensor_tuple\n\n        tensor = FixedPrecisionTensor(\n            owner=worker,\n            id=syft.serde.msgpack.serde._detail(worker, tensor_id),\n            field=syft.serde.msgpack.serde._detail(worker, field),\n            dtype=dtype,\n            base=base,\n            precision_fractional=precision_fractional,\n            kappa=kappa,\n            tags=syft.serde.msgpack.serde._detail(worker, tags),\n            description=syft.serde.msgpack.serde._detail(worker, description),\n        )\n\n        if chain is not None:\n            chain = syft.serde.msgpack.serde._detail(worker, chain)\n            tensor.child = chain\n\n        return tensor\n\n    @staticmethod\n    def bufferize(worker, prec_tensor):\n        """"""\n         This method serializes FixedPrecisionTensor into FixedPrecisionTensorPB.\n\n          Args:\n             prec_tensor (FixedPrecisionTensor): input FixedPrecisionTensor to be serialized.\n\n          Returns:\n             proto_prec_tensor (FixedPrecisionTensorPB): serialized FixedPrecisionTensor\n         """"""\n        proto_prec_tensor = FixedPrecisionTensorPB()\n        syft.serde.protobuf.proto.set_protobuf_id(proto_prec_tensor.id, prec_tensor.id)\n        proto_prec_tensor.field = str(prec_tensor.field)\n        proto_prec_tensor.dtype = prec_tensor.dtype\n        proto_prec_tensor.base = prec_tensor.base\n        proto_prec_tensor.kappa = prec_tensor.kappa\n        proto_prec_tensor.precision_fractional = prec_tensor.precision_fractional\n        for tag in prec_tensor.tags:\n            proto_prec_tensor.tags.append(tag)\n        proto_prec_tensor.description = prec_tensor.description\n        if hasattr(prec_tensor, ""child""):\n            proto_prec_tensor.child.CopyFrom(\n                syft.serde.protobuf.serde._bufferize(worker, prec_tensor.child)\n            )\n\n        return proto_prec_tensor\n\n    @staticmethod\n    def unbufferize(worker, proto_prec_tensor):\n        """"""\n            This method deserializes FixedPrecisionTensorPB into FixedPrecisionTensor.\n\n            Args:\n                proto_prec_tensor (FixedPrecisionTensorPB): input FixedPrecisionTensor to be\n                deserialized.\n\n            Returns:\n                tensor (FixedPrecisionTensor): deserialized FixedPrecisionTensorPB\n        """"""\n        proto_id = syft.serde.protobuf.proto.get_protobuf_id(proto_prec_tensor.id)\n\n        child = None\n        if proto_prec_tensor.HasField(""child""):\n            child = syft.serde.protobuf.serde._unbufferize(worker, proto_prec_tensor.child)\n\n        tensor = FixedPrecisionTensor(\n            owner=worker,\n            id=proto_id,\n            field=proto_prec_tensor.field,\n            dtype=proto_prec_tensor.dtype,\n            base=proto_prec_tensor.base,\n            precision_fractional=proto_prec_tensor.precision_fractional,\n            kappa=proto_prec_tensor.kappa,\n            tags=set(proto_prec_tensor.tags),\n            description=proto_prec_tensor.description,\n        )\n\n        tensor.child = child\n        return tensor\n\n    @staticmethod\n    def get_protobuf_schema():\n        """"""\n            Returns the protobuf schema used for FixedPrecisionTensor.\n\n            Returns:\n                Protobuf schema for FixedPrecisionTensor.\n        """"""\n        return FixedPrecisionTensorPB\n\n\n### Register the tensor with hook_args.py ###\nhook_args.default_register_tensor(FixedPrecisionTensor)\n'"
syft/frameworks/torch/tensors/interpreters/private.py,1,"b'import torch\nimport syft\n\nfrom typing import List, Tuple\n\nfrom syft.exceptions import SendNotPermittedError\nfrom syft.generic.abstract.tensor import AbstractTensor\nfrom syft.generic.frameworks.hook import hook_args\nfrom syft.generic.frameworks.overload import overloaded\nfrom syft.workers.abstract import AbstractWorker\n\n\nclass PrivateTensor(AbstractTensor):\n    def __init__(\n        self,\n        owner=None,\n        id=None,\n        tags: set = None,\n        description: str = None,\n        allowed_users: Tuple[str] = (),\n        parents: Tuple[str] = (),\n        command: str = None,\n    ):\n        """""" Initialize a Private tensor, which manages permissions restricting get operations.\n\n        Args:\n            owner (BaseWorker, optional): A BaseWorker object to specify the worker on which\n            the tensor is located.\n            id (string or int, optional): An optional string or integer id of the PrivateTensor.\n            tags (set, optional): A set of tags to label this tensor.\n            description (string, optional): A brief description about this tensor.\n            allowed_users (Tuple, optional): User credentials.\n            parents (tuple, optional): If it was generated by other tensors, it\'ll be\n                                    referenced here.\n            command (string, optional): If it was generated by some operation, it\'ll be\n                                    registered here.\n        """"""\n        super().__init__(tags=tags, description=description)\n        self.owner = owner\n        self.id = id if id else syft.ID_PROVIDER.pop()\n        self.child = None\n        self.allowed_users = allowed_users\n        self.parents = parents\n        self.command = command\n\n    def get_class_attributes(self):\n        """""" Specify all the attributes need to build a wrapper correctly when returning\n        a response.\n        """"""\n        return {""allowed_users"": self.allowed_users}\n\n    def allow(self, user) -> bool:\n        """""" Overwrite native\'s allowed to verify if a specific user is allowed to get this tensor.\n\n        Args:\n            user (object): user to be verified.\n\n        Returns:\n            bool : A boolean value (True if the user is allowed and false if it isn\'t).\n        """"""\n        return user in self.allowed_users\n\n    def _before_send(self, *location, user: object = None, **kwargs):\n        if not self.allow(user):\n            raise SendNotPermittedError()\n\n    def register_credentials(self, users: List[str]) -> ""PrivateTensor"":\n        """""" Register a new user credential(s) into the list of allowed users to get this tensor.\n\n            Args:\n                users (list): Credential(s) to be registered.\n        """"""\n        if not hasattr(self, ""allowed_users""):\n            self.allowed_users = ()\n\n        self.allowed_users = self.allowed_users + tuple(users)\n\n        return self\n\n    def float_precision(self):\n        """""" Forward float_precision method to next child on tensor stack. """"""\n        return self.child.float_precision()\n\n    @staticmethod\n    @overloaded.module\n    def torch(module):\n        def add(self, other):\n            return self.__add__(other)\n\n        module.add = add\n\n        def sub(self, other):\n            return self.__sub__(other)\n\n        module.sub = sub\n\n        def mul(self, other):\n            return self.__mul__(other)\n\n        module.mul = mul\n\n        def div(self, other):\n            return self.__truediv__(other)\n\n        module.div = div\n\n        def matmul(self, other):\n            return self.matmul(other)\n\n        module.matmul = matmul\n        module.mm = matmul\n\n        def addmm(bias, input_tensor, weight):\n            matmul = input_tensor.matmul(weight)\n            result = bias.add(matmul)\n            return result\n\n        module.addmm = addmm\n\n        def dot(self, other):\n            return self.__mul__(other).sum()\n\n        module.dot = dot\n\n        # You can also overload functions in submodules!\n        @overloaded.module\n        def nn(module):\n            """"""\n            The syntax is the same, so @overloaded.module handles recursion\n            Note that we don\'t need to add the @staticmethod decorator\n            """"""\n\n            @overloaded.module\n            def functional(module):\n                def linear(*args):\n                    """"""\n                    Un-hook the function to have its detailed behaviour\n                    """"""\n                    return torch.nn.functional.native_linear(*args)\n\n                module.linear = linear\n\n            module.functional = functional\n\n        # Modules should be registered just like functions\n        module.nn = nn\n\n    @staticmethod\n    def simplify(worker: AbstractWorker, tensor: ""PrivateTensor"") -> tuple:\n        """"""Takes the attributes of a PrivateTensor and saves them in a tuple.\n\n        Args:\n            tensor (PrivateTensor): a PrivateTensor.\n\n        Returns:\n            tuple: a tuple holding the unique attributes of the fixed private tensor.\n        """"""\n\n        chain = None\n        if hasattr(tensor, ""child""):\n            chain = syft.serde.msgpack.serde._simplify(worker, tensor.child)\n\n        return (\n            syft.serde.msgpack.serde._simplify(worker, tensor.id),\n            syft.serde.msgpack.serde._simplify(worker, tensor.allowed_users),\n            syft.serde.msgpack.serde._simplify(worker, tensor.tags),\n            syft.serde.msgpack.serde._simplify(worker, tensor.description),\n            chain,\n        )\n\n    @staticmethod\n    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""PrivateTensor"":\n        """"""\n        This function reconstructs a PrivateTensor given it\'s attributes in form of a tuple.\n        Args:\n            worker (AbstractWorker): the worker doing the deserialization\n            tensor_tuple (tuple): a tuple holding the attributes of the PrivateTensor\n        Returns:\n            PrivateTensor: a PrivateTensor\n        Examples:\n            shared_tensor = detail(data)\n        """"""\n\n        tensor_id, allowed_users, tags, description, chain = tensor_tuple\n\n        tensor = PrivateTensor(\n            owner=worker,\n            id=syft.serde.msgpack.serde._detail(worker, tensor_id),\n            tags=syft.serde.msgpack.serde._detail(worker, tags),\n            description=syft.serde.msgpack.serde._detail(worker, description),\n            allowed_users=syft.serde.msgpack.serde._detail(worker, allowed_users),\n        )\n\n        if chain is not None:\n            chain = syft.serde.msgpack.serde._detail(worker, chain)\n            tensor.child = chain\n\n        return tensor\n\n\n### Register the tensor with hook_args.py ###\nhook_args.default_register_tensor(PrivateTensor)\n'"
syft/grid/autoscale/utils/notebook/terraform_notebook.py,0,"b'""""""Helper methods to call terraform commands""""""\nimport sys\nimport threading\nimport subprocess\n\n\ndef init():\n    """"""\n    args:\n    """"""\n    proc = subprocess.Popen(\n        ""/bin/sh"", stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.STDOUT,\n    )\n\n    def outloop():\n        running = True\n        while running:\n            line = proc.stdout.readline().decode(sys.stdout.encoding)\n            print(line, end="""")\n            running = ""\\n"" in line\n        print(""Exited"")\n\n    threading.Thread(target=outloop).start()\n\n    commands = [b""terraform init\\n"", b""exit\\n""]\n    i = 0\n    while proc.poll() is None and i < len(commands):\n        inp = commands[i]\n        if inp == ""INPUT"":\n            inp = bytearray(input("""") + ""\\n"", sys.stdin.encoding)  # nosec\n        if proc.poll() is None:\n            proc.stdin.write(inp)\n            proc.stdin.flush()\n        i += 1\n\n\ndef apply():\n    """"""\n    args:\n    """"""\n    proc = subprocess.Popen(\n        ""/bin/sh"", stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.STDOUT,\n    )\n\n    def outloop():\n        running = True\n        while running:\n            line = proc.stdout.readline().decode(sys.stdout.encoding)\n            print(line, end="""")\n            running = ""\\n"" in line\n        print(""Exited"")\n\n    threading.Thread(target=outloop).start()\n\n    commands = [b""terraform apply\\n"", ""INPUT"", b""exit\\n""]\n    i = 0\n    while proc.poll() is None and i < len(commands):\n        inp = commands[i]\n        if inp == ""INPUT"":\n            inp = bytearray(input("""") + ""\\n"", sys.stdin.encoding)  # nosec\n        if proc.poll() is None:\n            proc.stdin.write(inp)\n            proc.stdin.flush()\n        i += 1\n\n\ndef destroy():\n    """"""\n    args:\n    """"""\n    proc = subprocess.Popen(\n        ""/bin/sh"", stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.STDOUT,\n    )\n\n    def outloop():\n        running = True\n        while running:\n            line = proc.stdout.readline().decode(sys.stdout.encoding)\n            print(line, end="""")\n            running = ""\\n"" in line\n        print(""Exited"")\n\n    threading.Thread(target=outloop).start()\n\n    commands = [b""terraform destroy\\n"", ""INPUT"", b""exit\\n""]\n    i = 0\n    while proc.poll() is None and i < len(commands):\n        inp = commands[i]\n        if inp == ""INPUT"":\n            inp = bytearray(input("""") + ""\\n"", sys.stdin.encoding)  # nosec\n        if proc.poll() is None:\n            proc.stdin.write(inp)\n            proc.stdin.flush()\n        i += 1\n'"
syft/grid/autoscale/utils/script/terraform_script.py,0,"b'""""""Helper methods to call terraform commands""""""\nimport subprocess\n\n\ndef init():\n    """"""\n    args:\n    """"""\n    subprocess.call(""terraform init"", shell=True)\n\n\ndef apply():\n    """"""\n    args:\n    """"""\n    subprocess.call(""terraform apply"", shell=True)\n\n\ndef destroy():\n    """"""\n    args:\n    """"""\n    subprocess.call(""terraform destroy"", shell=True)\n'"
syft/frameworks/torch/he/fv/util/base_converter.py,1,"b'from syft.frameworks.torch.he.fv.util.operations import multiply_mod\n\n\nclass BaseConvertor:\n    """"""BaseConvertor is used for converting plain/base of integer\n    from one set of bases to another set of bases.\n\n    Args:\n        ibase: A list of integer denoting the input base.\n        obase: A list of integer denoting the base of the output required.\n    """"""\n\n    def __init__(self, ibase, obase):\n        self._ibase = ibase\n        self._obase = obase\n\n        # base_change_matrix is helpful for fast conversion as many pre-computation\n        # are already done here only once.\n        self._base_change_matrix = [[]] * self._obase.size\n        for i in range(self._obase.size):\n            self._base_change_matrix[i] = [0] * self._ibase.size\n\n            for j in range(self._ibase.size):\n                self._base_change_matrix[i][j] = (\n                    self._ibase.punctured_prod_list[j] % self._obase.base[i]\n                )\n\n    def fast_convert_list(self, input, count):\n        """"""Converts the plain/base of input list from input base to output base\n        declared at the time of initialization of BaseConvertor class.\n\n        Args:\n            input: A list of integers needed to be converted from input base to output base.\n            count: An integer denoting the coefficient count of output base.\n\n        Returns:\n            A list of integers converted from input base plain to output base plain.\n        """"""\n\n        output = [0] * self._obase.size\n        for i in range(self._obase.size):\n            output[i] = [0] * count\n\n        temp = [0] * self._ibase.size\n        for i in range(self._ibase.size):\n            temp[i] = [0] * count\n\n        for i in range(self._ibase.size):\n            inv_punctured_prod_mod_ibase = self._ibase.inv_punctured_prod_mod_base_list[i]\n            ibase = self._ibase.base[i]\n\n            for k in range(count):\n                temp[i][k] = multiply_mod(input[i][k], inv_punctured_prod_mod_ibase, ibase)\n\n        for j in range(self._obase.size):\n            obase = self._obase.base[j]\n\n            for k in range(count):\n                dot_product = 0\n\n                for tt in range(self._ibase.size):\n                    dot_product += multiply_mod(temp[tt][k], self._base_change_matrix[j][tt], obase)\n                output[j][k] = dot_product % obase\n\n        return output\n'"
syft/frameworks/torch/he/fv/util/global_variable.py,0,"b'""""""Attributes:\n    DEFAULT_C0EFF_MODULUS_128: A dictionary that maps degrees of the polynomial\n    modulus to list of Modulus elements so that when used with the default value\n    for the standard deviation of the noise distribution (noise_standard_deviation),\n    the security level is at least 128 bits according to http://HomomorphicEncryption.org.\n    This makes it easy for non-expert users to select secure parameters.\n\n    DEFAULT_C0EFF_MODULUS_192: A dictionary that maps degrees of the polynomial\n    modulus to list of Modulus elements so that when used with the default value\n    for the standard deviation of the noise distribution (noise_standard_deviation),\n    the security level is at least 192 bits according to http://HomomorphicEncryption.org.\n    This makes it easy for non-expert users to select secure parameters.\n\n    DEFAULT_C0EFF_MODULUS_256: A dictionary that maps degrees of the polynomial\n    modulus to list of Modulus elements so that when used with the default value\n    for the standard deviation of the noise distribution (noise_standard_deviation),\n    the security level is at least 256 bits according to http://HomomorphicEncryption.org.\n    This makes it easy for non-expert users to select secure parameters.\n""""""\nDEFAULT_C0EFF_MODULUS_128 = {\n    # Polynomial modulus: 1x^1024 + 1\n    # Modulus count: 1\n    # Total bit count: 27\n    1024: [0x7E00001],\n    # Polynomial modulus: 1x^2048 + 1\n    # Modulus count: 1\n    # Total bit count: 54\n    2048: [0x3FFFFFFF000001],\n    # Polynomial modulus: 1x^4096 + 1\n    # Modulus count: 3\n    # Total bit count: 109 = 2 * 36 + 37\n    4096: [0xFFFFEE001, 0xFFFFC4001, 0x1FFFFE0001],\n    # Polynomial modulus: 1x^8192 + 1\n    # Modulus count: 5\n    # Total bit count: 218 = 2 * 43 + 3 * 44\n    8192: [0x7FFFFFD8001, 0x7FFFFFC8001, 0xFFFFFFFC001, 0xFFFFFF6C001, 0xFFFFFEBC001],\n    # Polynomial modulus: 1x^16384 + 1\n    # Modulus count: 9\n    # Total bit count: 438 = 3 * 48 + 6 * 49\n    16384: [\n        0xFFFFFFFD8001,\n        0xFFFFFFFA0001,\n        0xFFFFFFF00001,\n        0x1FFFFFFF68001,\n        0x1FFFFFFF50001,\n        0x1FFFFFFEE8001,\n        0x1FFFFFFEA0001,\n        0x1FFFFFFE88001,\n        0x1FFFFFFE48001,\n    ],\n    # Polynomial modulus: 1x^32768 + 1\n    # Modulus count: 16\n    # Total bit count: 881 = 15 * 55 + 56\n    32768: [\n        0x7FFFFFFFE90001,\n        0x7FFFFFFFBF0001,\n        0x7FFFFFFFBD0001,\n        0x7FFFFFFFBA0001,\n        0x7FFFFFFFAA0001,\n        0x7FFFFFFFA50001,\n        0x7FFFFFFF9F0001,\n        0x7FFFFFFF7E0001,\n        0x7FFFFFFF770001,\n        0x7FFFFFFF380001,\n        0x7FFFFFFF330001,\n        0x7FFFFFFF2D0001,\n        0x7FFFFFFF170001,\n        0x7FFFFFFF150001,\n        0x7FFFFFFEF00001,\n        0xFFFFFFFFF70001,\n    ],\n}\n\nDEFAULT_C0EFF_MODULUS_192 = {\n    # Polynomial modulus: 1x^1024 + 1\n    # Modulus count: 1\n    # Total bit count: 19\n    1024: [0x7F001],\n    # Polynomial modulus: 1x^2048 + 1\n    # Modulus count: 1\n    # Total bit count: 37\n    2048: [0x1FFFFC0001],\n    # Polynomial modulus: 1x^4096 + 1\n    # Modulus count: 3\n    # Total bit count: 75 = 3 * 25\n    4096: [0x1FFC001, 0x1FCE001, 0x1FC0001],\n    # Polynomial modulus: 1x^8192 + 1\n    # Modulus count: 4\n    # Total bit count: 152 = 4 * 38\n    8192: [0x3FFFFAC001, 0x3FFFF54001, 0x3FFFF48001, 0x3FFFF28001],\n    # Polynomial modulus: 1x^16384 + 1\n    # Modulus count: 6\n    # Total bit count: 300 = 6 * 50\n    16384: [\n        0x3FFFFFFDF0001,\n        0x3FFFFFFD48001,\n        0x3FFFFFFD20001,\n        0x3FFFFFFD18001,\n        0x3FFFFFFCD0001,\n        0x3FFFFFFC70001,\n    ],\n    # Polynomial modulus: 1x^32768 + 1\n    # Modulus count: 11\n    # Total bit count: 600 = 5 * 54 + 6 * 55\n    32768: [\n        0x3FFFFFFFD60001,\n        0x3FFFFFFFCA0001,\n        0x3FFFFFFF6D0001,\n        0x3FFFFFFF5D0001,\n        0x3FFFFFFF550001,\n        0x7FFFFFFFE90001,\n        0x7FFFFFFFBF0001,\n        0x7FFFFFFFBD0001,\n        0x7FFFFFFFBA0001,\n        0x7FFFFFFFAA0001,\n        0x7FFFFFFFA50001,\n    ],\n}\n\nDEFAULT_C0EFF_MODULUS_256 = {\n    # Polynomial modulus: 1x^1024 + 1\n    # Modulus count: 1\n    # Total bit count: 14\n    1024: [0x3001],\n    # Polynomial modulus: 1x^2048 + 1\n    # Modulus count: 1\n    # Total bit count: 29\n    2048: [0x1FFC0001],\n    # Polynomial modulus: 1x^4096 + 1\n    # Modulus count: 1\n    # Total bit count: 58\n    4096: [0x3FFFFFFFF040001],\n    # Polynomial modulus: 1x^8192 + 1\n    # Modulus count: 3\n    # Total bit count: 118 = 2 * 39 + 40\n    8192: [0x7FFFFEC001, 0x7FFFFB0001, 0xFFFFFDC001],\n    # Polynomial modulus: 1x^16384 + 1\n    # Modulus count: 5\n    # Total bit count: 237 = 3 * 47 + 2 * 48\n    16384: [0x7FFFFFFC8001, 0x7FFFFFF00001, 0x7FFFFFE70001, 0xFFFFFFFD8001, 0xFFFFFFFA0001],\n    # Polynomial modulus: 1x^32768 + 1\n    # Modulus count: 9\n    # Total bit count: 476 = 52 + 8 * 53\n    32768: [\n        0xFFFFFFFF00001,\n        0x1FFFFFFFE30001,\n        0x1FFFFFFFD80001,\n        0x1FFFFFFFD10001,\n        0x1FFFFFFFC50001,\n        0x1FFFFFFFBF0001,\n        0x1FFFFFFFB90001,\n        0x1FFFFFFFB60001,\n        0x1FFFFFFFA50001,\n    ],\n}\n\ngamma = 0x1FFFFFFFFFC80001\n\nNOISE_STANDARD_DEVIATION = 3.20\nNOISE_DISTRIBUTION_WIDTH_MULTIPLIER = 6\nNOISE_MAX_DEVIATION = NOISE_DISTRIBUTION_WIDTH_MULTIPLIER * NOISE_STANDARD_DEVIATION\n'"
syft/frameworks/torch/he/fv/util/numth.py,2,"b'import random\nfrom syft.frameworks.torch.he.fv.util.operations import exponentiate_mod\nfrom syft.frameworks.torch.he.fv.util.operations import multiply_mod\n\n\ndef is_prime(value, num_rounds=40):\n    """"""Check for the integer if it probably prime.\n    Not intrested in strictly checking for prime.\n\n    Returns:\n        True if it is sufficiently prime no else False.\n    """"""\n    # First check the simplest cases.\n    if value < 2:\n        return False\n    if value == 2:\n        return True\n    if 0 == value % 2:\n        return False\n    if 3 == value:\n        return True\n    if 0 == value % 3:\n        return False\n    if 5 == value:\n        return True\n    if 0 == value % 5:\n        return False\n    if 7 == value:\n        return True\n    if 0 == value % 7:\n        return False\n    if 11 == value:\n        return True\n    if 0 == value % 11:\n        return False\n    if 13 == value:\n        return True\n    if 0 == value % 13:\n        return False\n\n    # Second, Miller-Rabin test.\n    # Find r and odd d that satisfy value = 2^r * d + 1.\n    d = value - 1\n    r = 0\n    while 0 == d & 1:\n        d >>= 1\n        r += 1\n\n    if r == 0:\n        return False\n\n    # 1) Pick a = 2, check a^(value - 1).\n    # 2) Pick a randomly from [3, value - 1], check a^(value - 1).\n    # 3) Repeat 2) for another num_rounds - 1 times.\n    for i in range(num_rounds):\n        a = random.randint(3, value - 1) if i != 0 else 2\n        x = exponentiate_mod(a, d, value)\n\n        if x == 1 or x == value - 1:\n            continue\n        count = 0\n\n        while True:\n            x = multiply_mod(x, x, value)\n            count += 1\n            if not (x != value - 1 and count < r - 1):\n                break\n\n        if x != value - 1:\n            return False\n    return True\n\n\ndef get_primes(size, bit_size, count):\n    """"""Generate a list of probably prime numbers with at least a constant\n    factor difference between the numbers.\n\n    Args:\n        size: polynomail modulus value.\n        bit_size: bit lengths of the prime no\'s to be generated.\n        count: No\'s of prime numbers required.\n\n    Returns:\n        A list of probably primes numbers.\n\n    Raises:\n        ValueError: if `count <= 0` or `size <= 0`.\n    """"""\n    if count <= 0:\n        raise ValueError(f""{count} must be positive value."")\n    if size <= 0:\n        raise ValueError(f""{size} must be positive value."")\n\n    result = []\n    factor = 2 * size\n\n    # Start with 2^bit_size - 2 * size + 1\n    value = 1 << bit_size\n    value = value - factor + 1\n\n    lower_bound = 1 << (bit_size - 1)\n    while count > 0 and value > lower_bound:\n        if is_prime(value):\n            result.append(value)\n            count -= 1\n        value -= factor\n    if count > 0:\n        raise RuntimeError(""failed to find enough qualifying primes"")\n\n    return result\n'"
syft/frameworks/torch/he/fv/util/operations.py,1,"b'import numpy as np\nfrom numpy.polynomial import polynomial as poly\n\nfrom syft.frameworks.torch.he.fv.ciphertext import CipherText\n\n\ndef multiply_mod(operand1, operand2, modulus):\n    return (operand1 * operand2) % modulus\n\n\ndef negate_mod(operand, modulus):\n    """"""returns (-1 * operand) % modulus""""""\n    if modulus == 0:\n        raise ValueError(""Modulus cannot be 0"")\n    if operand >= modulus:\n        raise OverflowError(""operand cannot be greater than modulus"")\n    non_zero = operand != 0\n    return (modulus - operand) & (-int(non_zero))\n\n\ndef exponentiate_mod(operand, exponent, modulus):\n    if exponent == 0:\n        return 1\n\n    if exponent == 1:\n        return operand\n\n    # Perform binary exponentiation.\n    power = operand\n    product = 0\n    intermediate = 1\n\n    # Initially: power = operand and intermediate = 1, product is irrelevant.\n    while True:\n        if exponent & 1:\n            product = multiply_mod(power, intermediate, modulus)\n            product, intermediate = intermediate, product\n\n        exponent >>= 1\n\n        if exponent == 0:\n            break\n\n        product = multiply_mod(power, power, modulus)\n        product, power = power, product\n\n    return intermediate\n\n\ndef invert_mod(value, modulus):\n    """"""calculate inverse modulus for given value and modulus""""""\n    gcd_tuple = xgcd(value, modulus)\n\n    if gcd_tuple[1] < 0:\n        return gcd_tuple[1] + modulus\n    else:\n        return gcd_tuple[1]\n\n\ndef poly_add_mod(op1, op2, modulus):\n    """"""return addition of two polynomials with all coefficients of\n    polynomial %q(coefficient modulus)""""""\n    return np.mod(np.polyadd(op1, op2), modulus).tolist()\n\n\ndef poly_mul_mod(op1, op2, modulus):\n    """"""return multiplication of two polynomials with all coefficients of\n    polynomial %q(coefficient modulus) and result polynomial % t(polynomial modulus)""""""\n\n    # For non same size polynomails we have to shift the polynomials because numpy consider right\n    # side as lower order of polynomial and we consider right side as heigher order.\n    if len(op1) != len(op2):\n        if len(op1) > len(op2):\n            op2 = op2 + [0] * (len(op1) - len(op2))\n        else:\n            op1 = op1 + [0] * (len(op2) - len(op1))\n\n    poly_mod = np.array([1] + [0] * (len(op1) - 1) + [1])\n    result = (\n        poly.polydiv(\n            poly.polymul(np.array(op1, dtype=""object""), np.array(op2, dtype=""object"")) % modulus,\n            poly_mod,\n        )[1]\n        % modulus\n    ).tolist()\n    return [round(x) for x in result]\n\n\ndef poly_negate_mod(op, modulus):\n    """"""returns negative of polynomial i.e (-1 * op)""""""\n    coeff_count = len(op)\n\n    result = [0] * coeff_count\n    for i in range(coeff_count):\n        if modulus == 0:\n            raise ValueError(""Modulus cannot be 0"")\n        if op[i] >= modulus:\n            raise OverflowError(""operand cannot be greater than modulus"")\n        non_zero = op[i] != 0\n        result[i] = (modulus - op[i]) & (-int(non_zero))\n    return result\n\n\ndef get_significant_count(values):\n    """"""removes leading zero\'s from the list.""""""\n    count = len(values)\n    i = count - 1\n    while count and not values[i]:\n        i -= 1\n        count -= 1\n    return count\n\n\ndef reverse_bit(value):\n    """"""calculate the value of the reverse binary representation of the given integer.""""""\n    result = 0\n    while value:\n        result = (result << 1) + (value & 1)\n        value >>= 1\n    return result\n\n\ndef multiply_many_except(operands, count, expt):\n    result = 1\n    for i in range(count):\n        if i != expt:\n            result *= operands[i]\n    return result\n\n\ndef xgcd(x, y):\n    """""" Extended GCD\n\n    Args:\n        x (integer)\n        y (integer)\n\n    Returns:\n        (gcd, x, y) where gcd is the greatest common divisor of a and b.\n            The numbers x, y are such that gcd = ax + by.\n    """"""\n    prev_a = 1\n    a = 0\n    prev_b = 0\n    b = 1\n\n    while y != 0:\n        q = x // y\n        temp = x % y\n        x = y\n        y = temp\n\n        temp = a\n        a = prev_a - q * a\n        prev_a = temp\n\n        temp = b\n        b = prev_b - q * b\n        prev_b = temp\n    return [x, prev_a, prev_b]\n\n\ndef multiply_add_plain_with_delta(phase, message, context):\n    """"""Add message (PlainText) into phase.\n\n    Args:\n        phase: phase is pre-computed carrier polynomial where we can add message data.\n        message (Plaintext): A plaintext representation of integer data to be encrypted.\n        context (Context): Context for extracting encryption parameters.\n\n    Returns:\n        A Ciphertext object with the encrypted result of encryption process.\n    """"""\n    coeff_modulus = context.param.coeff_modulus\n    message = message.data\n    plain_coeff_count = len(message)\n    delta = context.coeff_div_plain_modulus\n    phase0, phase1 = phase.data  # here phase = pk * u * e\n\n    # Coefficients of plain m multiplied by coeff_modulus q, divided by plain_modulus t,\n    # and rounded to the nearest integer (rounded up in case of a tie). Equivalent to\n    for i in range(plain_coeff_count):\n        for j in range(len(coeff_modulus)):\n            temp = round(delta[j] * message[i]) % coeff_modulus[j]\n            phase0[j][i] = (phase0[j][i] + temp) % coeff_modulus[j]\n\n    return CipherText([phase0, phase1])  # phase0 = pk0 * u * e + delta * m\n'"
syft/frameworks/torch/he/fv/util/rlwe.py,6,"b'import torch as th\nfrom secrets import SystemRandom\nfrom secrets import randbits\nfrom torch.distributions import Normal\n\nfrom syft.frameworks.torch.he.fv.ciphertext import CipherText\nfrom syft.frameworks.torch.he.fv.util.operations import poly_add_mod\nfrom syft.frameworks.torch.he.fv.util.operations import poly_mul_mod\nfrom syft.frameworks.torch.he.fv.util.operations import poly_negate_mod\nfrom syft.frameworks.torch.he.fv.util.global_variable import NOISE_STANDARD_DEVIATION\n\n\ndef sample_poly_ternary(parms):\n    """"""Generate a ternary polynomial uniformally with elements [-1, 0, 1]\n    where -1 is represented as (modulus - 1) because -1 % modulus == modulus - 1.\n\n    Used for generating secret key using coeff_modulus(list of prime nos) which\n    represents as \'q\' in the research paper.\n\n    Args:\n       parms (EncryptionParam): Encryption parameters.\n\n    Returns:\n        A 2-dim list having integer from [-1, 0, 1].\n    """"""\n    coeff_modulus = parms.coeff_modulus\n    coeff_count = parms.poly_modulus\n    coeff_mod_size = len(coeff_modulus)\n\n    result = [0] * coeff_mod_size\n    for i in range(coeff_mod_size):\n        result[i] = [0] * coeff_count\n\n    for i in range(coeff_count):\n        rand_index = SystemRandom().choice([-1, 0, 1])\n        if rand_index == 1:\n            for j in range(coeff_mod_size):\n                result[j][i] = 1\n        elif rand_index == -1:\n            for j in range(coeff_mod_size):\n                result[j][i] = coeff_modulus[j] - 1\n        else:\n            for j in range(coeff_mod_size):\n                result[j][i] = 0\n    return result\n\n\ndef sample_poly_normal(param):\n    """"""Generate a polynomial from normal distribution where negative values are\n    represented as (modulus - value) a positive value.\n\n    Args:\n        parms (EncryptionParam): Encryption parameters.\n\n    Returns:\n        A 2-dim list having integer from normal distributions.\n    """"""\n    coeff_modulus = param.coeff_modulus\n    coeff_mod_size = len(coeff_modulus)\n    coeff_count = param.poly_modulus\n\n    result = [0] * coeff_mod_size\n    for i in range(coeff_mod_size):\n        result[i] = [0] * coeff_count\n\n    for i in range(coeff_count):\n        noise = Normal(th.tensor([0.0]), th.tensor(NOISE_STANDARD_DEVIATION))\n        noise = int(noise.sample().item())\n        if noise > 0:\n            for j in range(coeff_mod_size):\n                result[j][i] = noise\n        elif noise < 0:\n            noise = -noise\n            for j in range(coeff_mod_size):\n                result[j][i] = coeff_modulus[j] - noise\n        else:\n            for j in range(coeff_mod_size):\n                result[j][i] = 0\n    return result\n\n\ndef sample_poly_uniform(param):\n    """"""Generate a polynomial from uniform distribution.\n\n    Args:\n        parms (EncryptionParam): Encryption parameters.\n    Returns:\n        A 2-dim list having integer from uniform distributions.\n    """"""\n    coeff_modulus = param.coeff_modulus\n    coeff_mod_size = len(coeff_modulus)\n    coeff_count = param.poly_modulus\n\n    max_random = 0x7FFFFFFFFFFFFFFF\n    result = [0] * coeff_mod_size\n    for i in range(coeff_mod_size):\n        result[i] = [0] * coeff_count\n\n    for j in range(coeff_mod_size):\n        modulus = coeff_modulus[j]\n        max_multiple = max_random - (max_random % modulus) - 1\n        for i in range(coeff_count):\n            # This ensures uniform distribution.\n            while True:\n                rand = randbits(32) << 31 | randbits(32) >> 1\n                if rand < max_multiple:\n                    break\n            result[j][i] = rand % modulus\n    return result\n\n\ndef encrypt_asymmetric(context, public_key):\n    """"""Create encryption of zero values with a public key which can be used in\n    subsequent processes to add a message into it.\n\n    Args:\n        context (Context): A valid context required for extracting the encryption\n            parameters.\n        public_key (PublicKey): A public key generated with same encryption parameters.\n\n    Returns:\n        A ciphertext object containing encryption of zeroes by asymmetric encryption procedure.\n    """"""\n    param = context.param\n    coeff_modulus = param.coeff_modulus\n    coeff_mod_size = len(coeff_modulus)\n    encrypted_size = len(public_key)\n\n    # Generate u <-- R_3\n    u = sample_poly_ternary(param)\n\n    c_0 = [0] * coeff_mod_size\n\n    c_1 = [0] * coeff_mod_size\n    result = [c_0, c_1]\n\n    # c[i] = u * public_key[i]\n    # Generate e_j <-- chi\n    # c[i] = public_key[i] * u + e[i]\n    for j in range(encrypted_size):\n        e = sample_poly_normal(param)\n        for i in range(coeff_mod_size):\n            result[j][i] = poly_add_mod(\n                poly_mul_mod(public_key[j][i], u[i], coeff_modulus[i]), e[i], coeff_modulus[i]\n            )\n    return CipherText(result)\n\n\ndef encrypt_symmetric(context, secret_key):\n    """"""Create encryption of zero values with a secret key which can be used in subsequent\n    processes to add a message into it.\n\n    Args:\n        context (Context): A valid context required for extracting the encryption parameters.\n        secret_key (SecretKey): A secret key generated with same encryption parameters.\n\n    Returns:\n        A ciphertext object containing encryption of zeroes by symmetric encryption procedure.\n    """"""\n    coeff_modulus = context.param.coeff_modulus\n    coeff_mod_size = len(coeff_modulus)\n\n    # Sample uniformly at random\n    c1 = sample_poly_uniform(context.param)\n\n    # Sample e <-- chi\n    e = sample_poly_normal(context.param)\n\n    # calculate -(a*s + e) (mod q) and store in c0\n\n    c0 = [0] * coeff_mod_size\n\n    for i in range(coeff_mod_size):\n        c0[i] = poly_negate_mod(\n            poly_add_mod(\n                poly_mul_mod(c1[i], secret_key[i], coeff_modulus[i]), e[i], coeff_modulus[i]\n            ),\n            coeff_modulus[i],\n        )\n\n    return CipherText([c0, c1])\n'"
syft/frameworks/torch/he/fv/util/rns_base.py,2,"b'from math import gcd\n\nfrom syft.frameworks.torch.he.fv.util.operations import multiply_many_except\nfrom syft.frameworks.torch.he.fv.util.operations import invert_mod\n\n\nclass RNSBase:\n    """"""A model class for creating basic blocks required in RNSTools class with\n    pre-computed attributes.\n\n    Args:\n        base: A list of Base values.\n\n    Attributes:\n        size: The number of base values given.\n        base: A list of Base values.\n        base_prod: An integer denoting the product of all base values.\n\n        punctured_prod_list: A list of products of all base values except\n        the base value at that index.\n\n        inv_punctured_prod_mod_base_list: A list of values equal to modulus\n        inverse of punctured_prod_list values.\n    """"""\n\n    def __init__(self, base):\n        self.size = len(base)\n\n        for i in range(self.size):\n            if base[i] == 0:\n                raise ValueError(""rns_base is invalid"")\n\n            # The base must be coprime\n            for j in base[:i]:\n                if gcd(base[i], j) != 1:\n                    raise ValueError(""rns_base is invalid"")\n\n        self.base = base\n        self.base_prod = None\n        self.punctured_prod_list = [0] * self.size\n        self.inv_punctured_prod_mod_base_list = [0] * self.size\n\n        if self.size > 1:\n            # Compute punctured product\n            for i in range(self.size):\n                self.punctured_prod_list[i] = multiply_many_except(self.base, self.size, i)\n\n            # Compute the full product\n            self.base_prod = self.punctured_prod_list[0] * self.base[0]\n\n            # Compute inverses of punctured products mod primes\n            for i in range(self.size):\n                self.inv_punctured_prod_mod_base_list[i] = (\n                    self.punctured_prod_list[i] % self.base[i]\n                )\n                self.inv_punctured_prod_mod_base_list[i] = invert_mod(\n                    self.inv_punctured_prod_mod_base_list[i], self.base[i]\n                )\n\n        else:\n            self.base_prod = self.base[0]\n            self.punctured_prod_list[0] = 1\n            self.inv_punctured_prod_mod_base_list[0] = 1\n'"
syft/frameworks/torch/he/fv/util/rns_tool.py,6,"b'from syft.frameworks.torch.he.fv.util.global_variable import gamma\nfrom syft.frameworks.torch.he.fv.util.operations import negate_mod\nfrom syft.frameworks.torch.he.fv.util.operations import invert_mod\nfrom syft.frameworks.torch.he.fv.util.operations import multiply_mod\nfrom syft.frameworks.torch.he.fv.util.base_converter import BaseConvertor\nfrom syft.frameworks.torch.he.fv.util.rns_base import RNSBase\n\n\nclass RNSTool:\n    """"""A class performing major operations required in the process of decryption\n    in RNS variant of FV HE Scheme.\n\n    After the multiplication of secret key with the ciphertext as [ct0 + ct1 * sk + ct2 * sk^2...]\n    we apply the decrypt_scale_and_round method of this class to get the plaintext object.\n\n    Args:\n        encryption_param (EncryptionParams): For extracting encryption parameters.\n    """"""\n\n    def __init__(self, encryption_param):\n        n = encryption_param.poly_modulus\n        q = encryption_param.coeff_modulus\n        t = encryption_param.plain_modulus\n\n        self._coeff_count = n\n        self.base_q = RNSBase(q)\n        self.base_q_size = len(q)\n        self._t = t\n        self._base_t_gamma = RNSBase([t, gamma])\n        self._base_t_gamma_size = 2\n        self.prod_t_gamma_mod_q = [(t * gamma) % q for q in self.base_q.base]\n        self._inv_gamma_mod_t = invert_mod(gamma, self._t)\n\n        # Compute -prod(q)^(-1) mod {t, gamma}\n        self.neg_inv_q_mod_t_gamma = [0] * self._base_t_gamma_size\n        for i in range(self._base_t_gamma_size):\n            self.neg_inv_q_mod_t_gamma[i] = self.base_q.base_prod % self._base_t_gamma.base[i]\n            self.neg_inv_q_mod_t_gamma[i] = invert_mod(\n                self.neg_inv_q_mod_t_gamma[i], self._base_t_gamma.base[i]\n            )\n            self.neg_inv_q_mod_t_gamma[i] = negate_mod(\n                self.neg_inv_q_mod_t_gamma[i], self._base_t_gamma.base[i]\n            )\n\n    def decrypt_scale_and_round(self, input):\n        """"""Perform the remaining procedure of decryptions process after getting the result of\n        [c0 + c1 * sk + c2 * sk^2 ...]_q.\n\n        Args:\n            input: Result of [c0 + c1 * sk + c2 * sk^2 ...]_q.\n\n        Returns:\n            A 1-dim list representing plaintext polynomial of the decrypted result.\n        """"""\n        result = [0] * self._coeff_count\n\n        # Computing |gamma * t|_qi * ct(s)\n        temp = [0] * self.base_q_size\n        for i in range(self.base_q_size):\n            temp[i] = [0] * self._coeff_count\n\n        for j in range(self.base_q_size):\n            for i in range(self._coeff_count):\n                temp[j][i] = multiply_mod(\n                    input[j][i], self.prod_t_gamma_mod_q[j], self.base_q.base[j]\n                )\n\n        # Base conversion: convert from q to {t, gamma}\n        base_q_to_t_gamma_conv = BaseConvertor(self.base_q, self._base_t_gamma)\n        temp_t_gamma = base_q_to_t_gamma_conv.fast_convert_list(temp, self._coeff_count)\n\n        # Multiply by -prod(q)^(-1) mod {t, gamma}\n        for j in range(self._base_t_gamma_size):\n            for i in range(self._coeff_count):\n                temp_t_gamma[j][i] = multiply_mod(\n                    temp_t_gamma[j][i], self.neg_inv_q_mod_t_gamma[j], self._base_t_gamma.base[j]\n                )\n\n        # Need to correct values in temp_t_gamma (gamma component only) which are larger\n        # than floor(gamma/2)\n        gamma_div_2 = gamma >> 1\n\n        # Now compute the subtraction to remove error and perform final multiplication by gamma\n        # inverse mod t\n        for i in range(self._coeff_count):\n            # Need correction because of centered mod\n            if temp_t_gamma[1][i] > gamma_div_2:\n\n                # Compute -(gamma - a) instead of (a - gamma)\n                result[i] = (temp_t_gamma[0][i] + (gamma - temp_t_gamma[1][i]) % self._t) % self._t\n            else:\n                # No correction needed\n                result[i] = (temp_t_gamma[0][i] - temp_t_gamma[1][i]) % self._t\n\n            # If this coefficient was non-zero, multiply by t^(-1)\n            if 0 != result[i]:\n\n                # Perform final multiplication by gamma inverse mod t\n                result[i] = multiply_mod(result[i], self._inv_gamma_mod_t, self._t)\n\n        return result\n'"
