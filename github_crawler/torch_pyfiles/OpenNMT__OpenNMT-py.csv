file_path,api_count,code
preprocess.py,0,"b'#!/usr/bin/env python\nfrom onmt.bin.preprocess import main\n\n\nif __name__ == ""__main__"":\n    main()\n'"
server.py,0,"b'#!/usr/bin/env python\nfrom onmt.bin.server import main\n\n\nif __name__ == ""__main__"":\n    main()\n'"
setup.py,0,"b'#!/usr/bin/env python\nfrom setuptools import setup, find_packages\nfrom os import path\n\nthis_directory = path.abspath(path.dirname(__file__))\nwith open(path.join(this_directory, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'OpenNMT-py\',\n    description=\'A python implementation of OpenNMT\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    version=\'1.1.1\',\n    packages=find_packages(),\n    project_urls={\n        ""Documentation"": ""http://opennmt.net/OpenNMT-py/"",\n        ""Forum"": ""http://forum.opennmt.net/"",\n        ""Gitter"": ""https://gitter.im/OpenNMT/OpenNMT-py"",\n        ""Source"": ""https://github.com/OpenNMT/OpenNMT-py/""\n    },\n    install_requires=[\n        ""six"",\n        ""tqdm"",\n        ""torch>=1.4.0"",\n        ""torchtext==0.4.0"",\n        ""future"",\n        ""configargparse"",\n        ""tensorboard>=1.14"",\n        ""flask"",\n        ""waitress"",\n        ""pyonmttok==1.*;platform_system==\'Linux\'"",\n        ""pyyaml"",\n    ],\n    entry_points={\n        ""console_scripts"": [\n            ""onmt_server=onmt.bin.server:main"",\n            ""onmt_train=onmt.bin.train:main"",\n            ""onmt_translate=onmt.bin.translate:main"",\n            ""onmt_preprocess=onmt.bin.preprocess:main"",\n            ""onmt_release_model=onmt.bin.release_model:main"",\n            ""onmt_average_models=onmt.bin.average_models:main""\n        ],\n    }\n)\n'"
train.py,0,"b'#!/usr/bin/env python\nfrom onmt.bin.train import main\n\n\nif __name__ == ""__main__"":\n    main()\n'"
translate.py,0,"b'#!/usr/bin/env python\nfrom onmt.bin.translate import main\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onmt/__init__.py,0,"b'"""""" Main entry point of the ONMT library """"""\nfrom __future__ import division, print_function\n\nimport onmt.inputters\nimport onmt.encoders\nimport onmt.decoders\nimport onmt.models\nimport onmt.utils\nimport onmt.modules\nfrom onmt.trainer import Trainer\nimport sys\nimport onmt.utils.optimizers\nonmt.utils.optimizers.Optim = onmt.utils.optimizers.Optimizer\nsys.modules[""onmt.Optim""] = onmt.utils.optimizers\n\n# For Flake\n__all__ = [onmt.inputters, onmt.encoders, onmt.decoders, onmt.models,\n           onmt.utils, onmt.modules, ""Trainer""]\n\n__version__ = ""1.1.1""\n'"
onmt/model_builder.py,7,"b'""""""\nThis file is for models creation, which consults options\nand creates each encoder and decoder accordingly.\n""""""\nimport re\nimport torch\nimport torch.nn as nn\nfrom torch.nn.init import xavier_uniform_\n\nimport onmt.inputters as inputters\nimport onmt.modules\nfrom onmt.encoders import str2enc\n\nfrom onmt.decoders import str2dec\n\nfrom onmt.modules import Embeddings, VecEmbedding, CopyGenerator\nfrom onmt.modules.util_class import Cast\nfrom onmt.utils.misc import use_gpu\nfrom onmt.utils.logging import logger\nfrom onmt.utils.parse import ArgumentParser\n\n\ndef build_embeddings(opt, text_field, for_encoder=True):\n    """"""\n    Args:\n        opt: the option in current environment.\n        text_field(TextMultiField): word and feats field.\n        for_encoder(bool): build Embeddings for encoder or decoder?\n    """"""\n    emb_dim = opt.src_word_vec_size if for_encoder else opt.tgt_word_vec_size\n\n    if opt.model_type == ""vec"" and for_encoder:\n        return VecEmbedding(\n            opt.feat_vec_size,\n            emb_dim,\n            position_encoding=opt.position_encoding,\n            dropout=(opt.dropout[0] if type(opt.dropout) is list\n                     else opt.dropout),\n        )\n\n    pad_indices = [f.vocab.stoi[f.pad_token] for _, f in text_field]\n    word_padding_idx, feat_pad_indices = pad_indices[0], pad_indices[1:]\n\n    num_embs = [len(f.vocab) for _, f in text_field]\n    num_word_embeddings, num_feat_embeddings = num_embs[0], num_embs[1:]\n\n    fix_word_vecs = opt.fix_word_vecs_enc if for_encoder \\\n        else opt.fix_word_vecs_dec\n\n    emb = Embeddings(\n        word_vec_size=emb_dim,\n        position_encoding=opt.position_encoding,\n        feat_merge=opt.feat_merge,\n        feat_vec_exponent=opt.feat_vec_exponent,\n        feat_vec_size=opt.feat_vec_size,\n        dropout=opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n        word_padding_idx=word_padding_idx,\n        feat_padding_idx=feat_pad_indices,\n        word_vocab_size=num_word_embeddings,\n        feat_vocab_sizes=num_feat_embeddings,\n        sparse=opt.optim == ""sparseadam"",\n        fix_word_vecs=fix_word_vecs\n    )\n    return emb\n\n\ndef build_encoder(opt, embeddings):\n    """"""\n    Various encoder dispatcher function.\n    Args:\n        opt: the option in current environment.\n        embeddings (Embeddings): vocab embeddings for this encoder.\n    """"""\n    enc_type = opt.encoder_type if opt.model_type == ""text"" \\\n        or opt.model_type == ""vec"" else opt.model_type\n    return str2enc[enc_type].from_opt(opt, embeddings)\n\n\ndef build_decoder(opt, embeddings):\n    """"""\n    Various decoder dispatcher function.\n    Args:\n        opt: the option in current environment.\n        embeddings (Embeddings): vocab embeddings for this decoder.\n    """"""\n    dec_type = ""ifrnn"" if opt.decoder_type == ""rnn"" and opt.input_feed \\\n               else opt.decoder_type\n    return str2dec[dec_type].from_opt(opt, embeddings)\n\n\ndef load_test_model(opt, model_path=None):\n    if model_path is None:\n        model_path = opt.models[0]\n    checkpoint = torch.load(model_path,\n                            map_location=lambda storage, loc: storage)\n\n    model_opt = ArgumentParser.ckpt_model_opts(checkpoint[\'opt\'])\n    ArgumentParser.update_model_opts(model_opt)\n    ArgumentParser.validate_model_opts(model_opt)\n    vocab = checkpoint[\'vocab\']\n    if inputters.old_style_vocab(vocab):\n        fields = inputters.load_old_vocab(\n            vocab, opt.data_type, dynamic_dict=model_opt.copy_attn\n        )\n    else:\n        fields = vocab\n\n    model = build_base_model(model_opt, fields, use_gpu(opt), checkpoint,\n                             opt.gpu)\n    if opt.fp32:\n        model.float()\n    model.eval()\n    model.generator.eval()\n    return fields, model, model_opt\n\n\ndef build_base_model(model_opt, fields, gpu, checkpoint=None, gpu_id=None):\n    """"""Build a model from opts.\n\n    Args:\n        model_opt: the option loaded from checkpoint. It\'s important that\n            the opts have been updated and validated. See\n            :class:`onmt.utils.parse.ArgumentParser`.\n        fields (dict[str, torchtext.data.Field]):\n            `Field` objects for the model.\n        gpu (bool): whether to use gpu.\n        checkpoint: the model gnerated by train phase, or a resumed snapshot\n                    model from a stopped training.\n        gpu_id (int or NoneType): Which GPU to use.\n\n    Returns:\n        the NMTModel.\n    """"""\n\n    # for back compat when attention_dropout was not defined\n    try:\n        model_opt.attention_dropout\n    except AttributeError:\n        model_opt.attention_dropout = model_opt.dropout\n\n    # Build embeddings.\n    if model_opt.model_type == ""text"" or model_opt.model_type == ""vec"":\n        src_field = fields[""src""]\n        src_emb = build_embeddings(model_opt, src_field)\n    else:\n        src_emb = None\n\n    # Build encoder.\n    encoder = build_encoder(model_opt, src_emb)\n\n    # Build decoder.\n    tgt_field = fields[""tgt""]\n    tgt_emb = build_embeddings(model_opt, tgt_field, for_encoder=False)\n\n    # Share the embedding matrix - preprocess with share_vocab required.\n    if model_opt.share_embeddings:\n        # src/tgt vocab should be the same if `-share_vocab` is specified.\n        assert src_field.base_field.vocab == tgt_field.base_field.vocab, \\\n            ""preprocess with -share_vocab if you use share_embeddings""\n\n        tgt_emb.word_lut.weight = src_emb.word_lut.weight\n\n    decoder = build_decoder(model_opt, tgt_emb)\n\n    # Build NMTModel(= encoder + decoder).\n    if gpu and gpu_id is not None:\n        device = torch.device(""cuda"", gpu_id)\n    elif gpu and not gpu_id:\n        device = torch.device(""cuda"")\n    elif not gpu:\n        device = torch.device(""cpu"")\n    model = onmt.models.NMTModel(encoder, decoder)\n\n    # Build Generator.\n    if not model_opt.copy_attn:\n        if model_opt.generator_function == ""sparsemax"":\n            gen_func = onmt.modules.sparse_activations.LogSparsemax(dim=-1)\n        else:\n            gen_func = nn.LogSoftmax(dim=-1)\n        generator = nn.Sequential(\n            nn.Linear(model_opt.dec_rnn_size,\n                      len(fields[""tgt""].base_field.vocab)),\n            Cast(torch.float32),\n            gen_func\n        )\n        if model_opt.share_decoder_embeddings:\n            generator[0].weight = decoder.embeddings.word_lut.weight\n    else:\n        tgt_base_field = fields[""tgt""].base_field\n        vocab_size = len(tgt_base_field.vocab)\n        pad_idx = tgt_base_field.vocab.stoi[tgt_base_field.pad_token]\n        generator = CopyGenerator(model_opt.dec_rnn_size, vocab_size, pad_idx)\n        if model_opt.share_decoder_embeddings:\n            generator.linear.weight = decoder.embeddings.word_lut.weight\n\n    # Load the model states from checkpoint or initialize them.\n    if checkpoint is not None:\n        # This preserves backward-compat for models using customed layernorm\n        def fix_key(s):\n            s = re.sub(r\'(.*)\\.layer_norm((_\\d+)?)\\.b_2\',\n                       r\'\\1.layer_norm\\2.bias\', s)\n            s = re.sub(r\'(.*)\\.layer_norm((_\\d+)?)\\.a_2\',\n                       r\'\\1.layer_norm\\2.weight\', s)\n            return s\n\n        checkpoint[\'model\'] = {fix_key(k): v\n                               for k, v in checkpoint[\'model\'].items()}\n        # end of patch for backward compatibility\n\n        model.load_state_dict(checkpoint[\'model\'], strict=False)\n        generator.load_state_dict(checkpoint[\'generator\'], strict=False)\n    else:\n        if model_opt.param_init != 0.0:\n            for p in model.parameters():\n                p.data.uniform_(-model_opt.param_init, model_opt.param_init)\n            for p in generator.parameters():\n                p.data.uniform_(-model_opt.param_init, model_opt.param_init)\n        if model_opt.param_init_glorot:\n            for p in model.parameters():\n                if p.dim() > 1:\n                    xavier_uniform_(p)\n            for p in generator.parameters():\n                if p.dim() > 1:\n                    xavier_uniform_(p)\n\n        if hasattr(model.encoder, \'embeddings\'):\n            model.encoder.embeddings.load_pretrained_vectors(\n                model_opt.pre_word_vecs_enc)\n        if hasattr(model.decoder, \'embeddings\'):\n            model.decoder.embeddings.load_pretrained_vectors(\n                model_opt.pre_word_vecs_dec)\n\n    model.generator = generator\n    model.to(device)\n    if model_opt.model_dtype == \'fp16\' and model_opt.optim == \'fusedadam\':\n        model.half()\n    return model\n\n\ndef build_model(model_opt, opt, fields, checkpoint):\n    logger.info(\'Building model...\')\n    model = build_base_model(model_opt, fields, use_gpu(opt), checkpoint)\n    logger.info(model)\n    return model\n'"
onmt/opts.py,2,"b'"""""" Implementation of all available options """"""\nfrom __future__ import print_function\n\nimport configargparse\nimport onmt\n\nfrom onmt.models.sru import CheckSRU\n\n\ndef config_opts(parser):\n    parser.add(\'-config\', \'--config\', required=False,\n               is_config_file_arg=True, help=\'config file path\')\n    parser.add(\'-save_config\', \'--save_config\', required=False,\n               is_write_out_config_file_arg=True,\n               help=\'config file save path\')\n\n\ndef model_opts(parser):\n    """"""\n    These options are passed to the construction of the model.\n    Be careful with these as they will be used during translation.\n    """"""\n\n    # Embedding Options\n    group = parser.add_argument_group(\'Model-Embeddings\')\n    group.add(\'--src_word_vec_size\', \'-src_word_vec_size\',\n              type=int, default=500,\n              help=\'Word embedding size for src.\')\n    group.add(\'--tgt_word_vec_size\', \'-tgt_word_vec_size\',\n              type=int, default=500,\n              help=\'Word embedding size for tgt.\')\n    group.add(\'--word_vec_size\', \'-word_vec_size\', type=int, default=-1,\n              help=\'Word embedding size for src and tgt.\')\n\n    group.add(\'--share_decoder_embeddings\', \'-share_decoder_embeddings\',\n              action=\'store_true\',\n              help=""Use a shared weight matrix for the input and ""\n                   ""output word  embeddings in the decoder."")\n    group.add(\'--share_embeddings\', \'-share_embeddings\', action=\'store_true\',\n              help=""Share the word embeddings between encoder ""\n                   ""and decoder. Need to use shared dictionary for this ""\n                   ""option."")\n    group.add(\'--position_encoding\', \'-position_encoding\', action=\'store_true\',\n              help=""Use a sin to mark relative words positions. ""\n                   ""Necessary for non-RNN style models."")\n\n    group = parser.add_argument_group(\'Model-Embedding Features\')\n    group.add(\'--feat_merge\', \'-feat_merge\', type=str, default=\'concat\',\n              choices=[\'concat\', \'sum\', \'mlp\'],\n              help=""Merge action for incorporating features embeddings. ""\n                   ""Options [concat|sum|mlp]."")\n    group.add(\'--feat_vec_size\', \'-feat_vec_size\', type=int, default=-1,\n              help=""If specified, feature embedding sizes ""\n                   ""will be set to this. Otherwise, feat_vec_exponent ""\n                   ""will be used."")\n    group.add(\'--feat_vec_exponent\', \'-feat_vec_exponent\',\n              type=float, default=0.7,\n              help=""If -feat_merge_size is not set, feature ""\n                   ""embedding sizes will be set to N^feat_vec_exponent ""\n                   ""where N is the number of values the feature takes."")\n\n    # Encoder-Decoder Options\n    group = parser.add_argument_group(\'Model- Encoder-Decoder\')\n    group.add(\'--model_type\', \'-model_type\', default=\'text\',\n              choices=[\'text\', \'img\', \'audio\', \'vec\'],\n              help=""Type of source model to use. Allows ""\n                   ""the system to incorporate non-text inputs. ""\n                   ""Options are [text|img|audio|vec]."")\n    group.add(\'--model_dtype\', \'-model_dtype\', default=\'fp32\',\n              choices=[\'fp32\', \'fp16\'],\n              help=\'Data type of the model.\')\n\n    group.add(\'--encoder_type\', \'-encoder_type\', type=str, default=\'rnn\',\n              choices=[\'rnn\', \'brnn\', \'ggnn\', \'mean\', \'transformer\', \'cnn\'],\n              help=""Type of encoder layer to use. Non-RNN layers ""\n                   ""are experimental. Options are ""\n                   ""[rnn|brnn|ggnn|mean|transformer|cnn]."")\n    group.add(\'--decoder_type\', \'-decoder_type\', type=str, default=\'rnn\',\n              choices=[\'rnn\', \'transformer\', \'cnn\'],\n              help=""Type of decoder layer to use. Non-RNN layers ""\n                   ""are experimental. Options are ""\n                   ""[rnn|transformer|cnn]."")\n\n    group.add(\'--layers\', \'-layers\', type=int, default=-1,\n              help=\'Number of layers in enc/dec.\')\n    group.add(\'--enc_layers\', \'-enc_layers\', type=int, default=2,\n              help=\'Number of layers in the encoder\')\n    group.add(\'--dec_layers\', \'-dec_layers\', type=int, default=2,\n              help=\'Number of layers in the decoder\')\n    group.add(\'--rnn_size\', \'-rnn_size\', type=int, default=-1,\n              help=""Size of rnn hidden states. Overwrites ""\n                   ""enc_rnn_size and dec_rnn_size"")\n    group.add(\'--enc_rnn_size\', \'-enc_rnn_size\', type=int, default=500,\n              help=""Size of encoder rnn hidden states. ""\n                   ""Must be equal to dec_rnn_size except for ""\n                   ""speech-to-text."")\n    group.add(\'--dec_rnn_size\', \'-dec_rnn_size\', type=int, default=500,\n              help=""Size of decoder rnn hidden states. ""\n                   ""Must be equal to enc_rnn_size except for ""\n                   ""speech-to-text."")\n    group.add(\'--audio_enc_pooling\', \'-audio_enc_pooling\',\n              type=str, default=\'1\',\n              help=""The amount of pooling of audio encoder, ""\n                   ""either the same amount of pooling across all layers ""\n                   ""indicated by a single number, or different amounts of ""\n                   ""pooling per layer separated by comma."")\n    group.add(\'--cnn_kernel_width\', \'-cnn_kernel_width\', type=int, default=3,\n              help=""Size of windows in the cnn, the kernel_size is ""\n                   ""(cnn_kernel_width, 1) in conv layer"")\n\n    group.add(\'--input_feed\', \'-input_feed\', type=int, default=1,\n              help=""Feed the context vector at each time step as ""\n                   ""additional input (via concatenation with the word ""\n                   ""embeddings) to the decoder."")\n    group.add(\'--bridge\', \'-bridge\', action=""store_true"",\n              help=""Have an additional layer between the last encoder ""\n                   ""state and the first decoder state"")\n    group.add(\'--rnn_type\', \'-rnn_type\', type=str, default=\'LSTM\',\n              choices=[\'LSTM\', \'GRU\', \'SRU\'],\n              action=CheckSRU,\n              help=""The gate type to use in the RNNs"")\n    # group.add(\'--residual\', \'-residual\',   action=""store_true"",\n    #                     help=""Add residual connections between RNN layers."")\n\n    group.add(\'--brnn\', \'-brnn\', action=DeprecateAction,\n              help=""Deprecated, use `encoder_type`."")\n\n    group.add(\'--context_gate\', \'-context_gate\', type=str, default=None,\n              choices=[\'source\', \'target\', \'both\'],\n              help=""Type of context gate to use. ""\n                   ""Do not select for no context gate."")\n\n    # The following options (bridge_extra_node to src_vocab) are used\n    # for training with --encoder_type ggnn (Gated Graph Neural Network).\n    group.add(\'--bridge_extra_node\', \'-bridge_extra_node\',\n              type=bool, default=True,\n              help=\'Graph encoder bridges only extra node to decoder as input\')\n    group.add(\'--bidir_edges\', \'-bidir_edges\', type=bool, default=True,\n              help=\'Graph encoder autogenerates bidirectional edges\')\n    group.add(\'--state_dim\', \'-state_dim\', type=int, default=512,\n              help=\'Number of state dimensions in the graph encoder\')\n    group.add(\'--n_edge_types\', \'-n_edge_types\', type=int, default=2,\n              help=\'Number of edge types in the graph encoder\')\n    group.add(\'--n_node\', \'-n_node\', type=int, default=2,\n              help=\'Number of nodes in the graph encoder\')\n    group.add(\'--n_steps\', \'-n_steps\', type=int, default=2,\n              help=\'Number of steps to advance graph encoder\')\n    # The ggnn uses src_vocab during training because the graph is built\n    # using edge information which requires parsing the input sequence.\n    group.add(\'--src_vocab\', \'-src_vocab\', default="""",\n              help=""Path to an existing source vocabulary. Format: ""\n                   ""one word per line."")\n\n    # Attention options\n    group = parser.add_argument_group(\'Model- Attention\')\n    group.add(\'--global_attention\', \'-global_attention\',\n              type=str, default=\'general\',\n              choices=[\'dot\', \'general\', \'mlp\', \'none\'],\n              help=""The attention type to use: ""\n                   ""dotprod or general (Luong) or MLP (Bahdanau)"")\n    group.add(\'--global_attention_function\', \'-global_attention_function\',\n              type=str, default=""softmax"", choices=[""softmax"", ""sparsemax""])\n    group.add(\'--self_attn_type\', \'-self_attn_type\',\n              type=str, default=""scaled-dot"",\n              help=\'Self attention type in Transformer decoder \'\n                   \'layer -- currently ""scaled-dot"" or ""average"" \')\n    group.add(\'--max_relative_positions\', \'-max_relative_positions\',\n              type=int, default=0,\n              help=""Maximum distance between inputs in relative ""\n                   ""positions representations. ""\n                   ""For more detailed information, see: ""\n                   ""https://arxiv.org/pdf/1803.02155.pdf"")\n    group.add(\'--heads\', \'-heads\', type=int, default=8,\n              help=\'Number of heads for transformer self-attention\')\n    group.add(\'--transformer_ff\', \'-transformer_ff\', type=int, default=2048,\n              help=\'Size of hidden transformer feed-forward\')\n    group.add(\'--aan_useffn\', \'-aan_useffn\', action=""store_true"",\n              help=\'Turn on the FFN layer in the AAN decoder\')\n\n    # Alignement options\n    group = parser.add_argument_group(\'Model - Alignement\')\n    group.add(\'--lambda_align\', \'-lambda_align\', type=float, default=0.0,\n              help=""Lambda value for alignement loss of Garg et al (2019)""\n                   ""For more detailed information, see: ""\n                   ""https://arxiv.org/abs/1909.02074"")\n    group.add(\'--alignment_layer\', \'-alignment_layer\', type=int, default=-3,\n              help=\'Layer number which has to be supervised.\')\n    group.add(\'--alignment_heads\', \'-alignment_heads\', type=int, default=0,\n              help=\'N. of cross attention heads per layer to supervised with\')\n    group.add(\'--full_context_alignment\', \'-full_context_alignment\',\n              action=""store_true"",\n              help=\'Whether alignment is conditioned on full target context.\')\n\n    # Generator and loss options.\n    group = parser.add_argument_group(\'Generator\')\n    group.add(\'--copy_attn\', \'-copy_attn\', action=""store_true"",\n              help=\'Train copy attention layer.\')\n    group.add(\'--copy_attn_type\', \'-copy_attn_type\',\n              type=str, default=None,\n              choices=[\'dot\', \'general\', \'mlp\', \'none\'],\n              help=""The copy attention type to use. Leave as None to use ""\n                   ""the same as -global_attention."")\n    group.add(\'--generator_function\', \'-generator_function\', default=""softmax"",\n              choices=[""softmax"", ""sparsemax""],\n              help=""Which function to use for generating ""\n                   ""probabilities over the target vocabulary (choices: ""\n                   ""softmax, sparsemax)"")\n    group.add(\'--copy_attn_force\', \'-copy_attn_force\', action=""store_true"",\n              help=\'When available, train to copy.\')\n    group.add(\'--reuse_copy_attn\', \'-reuse_copy_attn\', action=""store_true"",\n              help=""Reuse standard attention for copy"")\n    group.add(\'--copy_loss_by_seqlength\', \'-copy_loss_by_seqlength\',\n              action=""store_true"",\n              help=""Divide copy loss by length of sequence"")\n    group.add(\'--coverage_attn\', \'-coverage_attn\', action=""store_true"",\n              help=\'Train a coverage attention layer.\')\n    group.add(\'--lambda_coverage\', \'-lambda_coverage\', type=float, default=0.0,\n              help=\'Lambda value for coverage loss of See et al (2017)\')\n    group.add(\'--loss_scale\', \'-loss_scale\', type=float, default=0,\n              help=""For FP16 training, the static loss scale to use. If not ""\n                   ""set, the loss scale is dynamically computed."")\n    group.add(\'--apex_opt_level\', \'-apex_opt_level\', type=str, default=""O1"",\n              choices=[""O0"", ""O1"", ""O2"", ""O3""],\n              help=""For FP16 training, the opt_level to use.""\n                   ""See https://nvidia.github.io/apex/amp.html#opt-levels."")\n\n\ndef preprocess_opts(parser):\n    """""" Pre-procesing options """"""\n    # Data options\n    group = parser.add_argument_group(\'Data\')\n    group.add(\'--data_type\', \'-data_type\', default=""text"",\n              help=""Type of the source input. ""\n                   ""Options are [text|img|audio|vec]."")\n\n    group.add(\'--train_src\', \'-train_src\', required=True, nargs=\'+\',\n              help=""Path(s) to the training source data"")\n    group.add(\'--train_tgt\', \'-train_tgt\', required=True, nargs=\'+\',\n              help=""Path(s) to the training target data"")\n    group.add(\'--train_align\', \'-train_align\', nargs=\'+\', default=[None],\n              help=""Path(s) to the training src-tgt alignment"")\n    group.add(\'--train_ids\', \'-train_ids\', nargs=\'+\', default=[None],\n              help=""ids to name training shards, used for corpus weighting"")\n\n    group.add(\'--valid_src\', \'-valid_src\',\n              help=""Path to the validation source data"")\n    group.add(\'--valid_tgt\', \'-valid_tgt\',\n              help=""Path to the validation target data"")\n    group.add(\'--valid_align\', \'-valid_align\', default=None,\n              help=""Path(s) to the validation src-tgt alignment"")\n\n    group.add(\'--src_dir\', \'-src_dir\', default="""",\n              help=""Source directory for image or audio files."")\n\n    group.add(\'--save_data\', \'-save_data\', required=True,\n              help=""Output file for the prepared data"")\n\n    group.add(\'--max_shard_size\', \'-max_shard_size\', type=int, default=0,\n              help=""""""Deprecated use shard_size instead"""""")\n\n    group.add(\'--shard_size\', \'-shard_size\', type=int, default=1000000,\n              help=""Divide src_corpus and tgt_corpus into ""\n                   ""smaller multiple src_copus and tgt corpus files, then ""\n                   ""build shards, each shard will have ""\n                   ""opt.shard_size samples except last shard. ""\n                   ""shard_size=0 means no segmentation ""\n                   ""shard_size>0 means segment dataset into multiple shards, ""\n                   ""each shard has shard_size samples"")\n\n    group.add(\'--num_threads\', \'-num_threads\', type=int, default=1,\n              help=""Number of shards to build in parallel."")\n\n    group.add(\'--overwrite\', \'-overwrite\', action=""store_true"",\n              help=""Overwrite existing shards if any."")\n\n    # Dictionary options, for text corpus\n\n    group = parser.add_argument_group(\'Vocab\')\n    # if you want to pass an existing vocab.pt file, pass it to\n    # -src_vocab alone as it already contains tgt vocab.\n    group.add(\'--src_vocab\', \'-src_vocab\', default="""",\n              help=""Path to an existing source vocabulary. Format: ""\n                   ""one word per line."")\n    group.add(\'--tgt_vocab\', \'-tgt_vocab\', default="""",\n              help=""Path to an existing target vocabulary. Format: ""\n                   ""one word per line."")\n    group.add(\'--features_vocabs_prefix\', \'-features_vocabs_prefix\',\n              type=str, default=\'\',\n              help=""Path prefix to existing features vocabularies"")\n    group.add(\'--src_vocab_size\', \'-src_vocab_size\', type=int, default=50000,\n              help=""Size of the source vocabulary"")\n    group.add(\'--tgt_vocab_size\', \'-tgt_vocab_size\', type=int, default=50000,\n              help=""Size of the target vocabulary"")\n    group.add(\'--vocab_size_multiple\', \'-vocab_size_multiple\',\n              type=int, default=1,\n              help=""Make the vocabulary size a multiple of this value"")\n\n    group.add(\'--src_words_min_frequency\',\n              \'-src_words_min_frequency\', type=int, default=0)\n    group.add(\'--tgt_words_min_frequency\',\n              \'-tgt_words_min_frequency\', type=int, default=0)\n\n    group.add(\'--dynamic_dict\', \'-dynamic_dict\', action=\'store_true\',\n              help=""Create dynamic dictionaries"")\n    group.add(\'--share_vocab\', \'-share_vocab\', action=\'store_true\',\n              help=""Share source and target vocabulary"")\n\n    # Truncation options, for text corpus\n    group = parser.add_argument_group(\'Pruning\')\n    group.add(\'--src_seq_length\', \'-src_seq_length\', type=int, default=50,\n              help=""Maximum source sequence length"")\n    group.add(\'--src_seq_length_trunc\', \'-src_seq_length_trunc\',\n              type=int, default=None,\n              help=""Truncate source sequence length."")\n    group.add(\'--tgt_seq_length\', \'-tgt_seq_length\', type=int, default=50,\n              help=""Maximum target sequence length to keep."")\n    group.add(\'--tgt_seq_length_trunc\', \'-tgt_seq_length_trunc\',\n              type=int, default=None,\n              help=""Truncate target sequence length."")\n    group.add(\'--lower\', \'-lower\', action=\'store_true\', help=\'lowercase data\')\n    group.add(\'--filter_valid\', \'-filter_valid\', action=\'store_true\',\n              help=\'Filter validation data by src and/or tgt length\')\n\n    # Data processing options\n    group = parser.add_argument_group(\'Random\')\n    group.add(\'--shuffle\', \'-shuffle\', type=int, default=0,\n              help=""Shuffle data"")\n    group.add(\'--seed\', \'-seed\', type=int, default=3435,\n              help=""Random seed"")\n\n    group = parser.add_argument_group(\'Logging\')\n    group.add(\'--report_every\', \'-report_every\', type=int, default=100000,\n              help=""Report status every this many sentences"")\n    group.add(\'--log_file\', \'-log_file\', type=str, default="""",\n              help=""Output logs to a file under this path."")\n    group.add(\'--log_file_level\', \'-log_file_level\', type=str,\n              action=StoreLoggingLevelAction,\n              choices=StoreLoggingLevelAction.CHOICES,\n              default=""0"")\n\n    # Options most relevant to speech\n    group = parser.add_argument_group(\'Speech\')\n    group.add(\'--sample_rate\', \'-sample_rate\', type=int, default=16000,\n              help=""Sample rate."")\n    group.add(\'--window_size\', \'-window_size\', type=float, default=.02,\n              help=""Window size for spectrogram in seconds."")\n    group.add(\'--window_stride\', \'-window_stride\', type=float, default=.01,\n              help=""Window stride for spectrogram in seconds."")\n    group.add(\'--window\', \'-window\', default=\'hamming\',\n              help=""Window type for spectrogram generation."")\n\n    # Option most relevant to image input\n    group.add(\'--image_channel_size\', \'-image_channel_size\',\n              type=int, default=3,\n              choices=[3, 1],\n              help=""Using grayscale image can training ""\n                   ""model faster and smaller"")\n\n    # Options for experimental source noising (BART style)\n    group = parser.add_argument_group(\'Noise\')\n    group.add(\'--subword_prefix\', \'-subword_prefix\',\n              type=str, default=""\xe2\x96\x81"",\n              help=""subword prefix to build wordstart mask"")\n    group.add(\'--subword_prefix_is_joiner\', \'-subword_prefix_is_joiner\',\n              action=\'store_true\',\n              help=""mask will need to be inverted if prefix is joiner"")\n\n\ndef train_opts(parser):\n    """""" Training and saving options """"""\n\n    group = parser.add_argument_group(\'General\')\n    group.add(\'--data\', \'-data\', required=True,\n              help=\'Path prefix to the "".train.pt"" and \'\n                   \'"".valid.pt"" file path from preprocess.py\')\n\n    group.add(\'--data_ids\', \'-data_ids\', nargs=\'+\', default=[None],\n              help=""In case there are several corpora."")\n    group.add(\'--data_weights\', \'-data_weights\', type=int, nargs=\'+\',\n              default=[1], help=""""""Weights of different corpora,\n              should follow the same order as in -data_ids."""""")\n    group.add(\'--data_to_noise\', \'-data_to_noise\', nargs=\'+\', default=[],\n              help=""IDs of datasets on which to apply noise."")\n\n    group.add(\'--save_model\', \'-save_model\', default=\'model\',\n              help=""Model filename (the model will be saved as ""\n                   ""<save_model>_N.pt where N is the number ""\n                   ""of steps"")\n\n    group.add(\'--save_checkpoint_steps\', \'-save_checkpoint_steps\',\n              type=int, default=5000,\n              help=""""""Save a checkpoint every X steps"""""")\n    group.add(\'--keep_checkpoint\', \'-keep_checkpoint\', type=int, default=-1,\n              help=""Keep X checkpoints (negative: keep all)"")\n\n    # GPU\n    group.add(\'--gpuid\', \'-gpuid\', default=[], nargs=\'*\', type=int,\n              help=""Deprecated see world_size and gpu_ranks."")\n    group.add(\'--gpu_ranks\', \'-gpu_ranks\', default=[], nargs=\'*\', type=int,\n              help=""list of ranks of each process."")\n    group.add(\'--world_size\', \'-world_size\', default=1, type=int,\n              help=""total number of distributed processes."")\n    group.add(\'--gpu_backend\', \'-gpu_backend\',\n              default=""nccl"", type=str,\n              help=""Type of torch distributed backend"")\n    group.add(\'--gpu_verbose_level\', \'-gpu_verbose_level\', default=0, type=int,\n              help=""Gives more info on each process per GPU."")\n    group.add(\'--master_ip\', \'-master_ip\', default=""localhost"", type=str,\n              help=""IP of master for torch.distributed training."")\n    group.add(\'--master_port\', \'-master_port\', default=10000, type=int,\n              help=""Port of master for torch.distributed training."")\n    group.add(\'--queue_size\', \'-queue_size\', default=40, type=int,\n              help=""Size of queue for each process in producer/consumer"")\n\n    group.add(\'--seed\', \'-seed\', type=int, default=-1,\n              help=""Random seed used for the experiments ""\n                   ""reproducibility."")\n\n    # Init options\n    group = parser.add_argument_group(\'Initialization\')\n    group.add(\'--param_init\', \'-param_init\', type=float, default=0.1,\n              help=""Parameters are initialized over uniform distribution ""\n                   ""with support (-param_init, param_init). ""\n                   ""Use 0 to not use initialization"")\n    group.add(\'--param_init_glorot\', \'-param_init_glorot\', action=\'store_true\',\n              help=""Init parameters with xavier_uniform. ""\n                   ""Required for transformer."")\n\n    group.add(\'--train_from\', \'-train_from\', default=\'\', type=str,\n              help=""If training from a checkpoint then this is the ""\n                   ""path to the pretrained model\'s state_dict."")\n    group.add(\'--reset_optim\', \'-reset_optim\', default=\'none\',\n              choices=[\'none\', \'all\', \'states\', \'keep_states\'],\n              help=""Optimization resetter when train_from."")\n\n    # Pretrained word vectors\n    group.add(\'--pre_word_vecs_enc\', \'-pre_word_vecs_enc\',\n              help=""If a valid path is specified, then this will load ""\n                   ""pretrained word embeddings on the encoder side. ""\n                   ""See README for specific formatting instructions."")\n    group.add(\'--pre_word_vecs_dec\', \'-pre_word_vecs_dec\',\n              help=""If a valid path is specified, then this will load ""\n                   ""pretrained word embeddings on the decoder side. ""\n                   ""See README for specific formatting instructions."")\n    # Fixed word vectors\n    group.add(\'--fix_word_vecs_enc\', \'-fix_word_vecs_enc\',\n              action=\'store_true\',\n              help=""Fix word embeddings on the encoder side."")\n    group.add(\'--fix_word_vecs_dec\', \'-fix_word_vecs_dec\',\n              action=\'store_true\',\n              help=""Fix word embeddings on the decoder side."")\n\n    # Optimization options\n    group = parser.add_argument_group(\'Optimization- Type\')\n    group.add(\'--batch_size\', \'-batch_size\', type=int, default=64,\n              help=\'Maximum batch size for training\')\n    group.add(\'--batch_size_multiple\', \'-batch_size_multiple\',\n              type=int, default=None,\n              help=\'Batch size multiple for token batches.\')\n    group.add(\'--batch_type\', \'-batch_type\', default=\'sents\',\n              choices=[""sents"", ""tokens""],\n              help=""Batch grouping for batch_size. Standard ""\n                   ""is sents. Tokens will do dynamic batching"")\n    group.add(\'--pool_factor\', \'-pool_factor\', type=int, default=8192,\n              help=""""""Factor used in data loading and batch creations.\n              It will load the equivalent of `pool_factor` batches,\n              sort them by the according `sort_key` to produce\n              homogeneous batches and reduce padding, and yield\n              the produced batches in a shuffled way.\n              Inspired by torchtext\'s pool mechanism."""""")\n    group.add(\'--normalization\', \'-normalization\', default=\'sents\',\n              choices=[""sents"", ""tokens""],\n              help=\'Normalization method of the gradient.\')\n    group.add(\'--accum_count\', \'-accum_count\', type=int, nargs=\'+\',\n              default=[1],\n              help=""Accumulate gradient this many times. ""\n                   ""Approximately equivalent to updating ""\n                   ""batch_size * accum_count batches at once. ""\n                   ""Recommended for Transformer."")\n    group.add(\'--accum_steps\', \'-accum_steps\', type=int, nargs=\'+\',\n              default=[0], help=""Steps at which accum_count values change"")\n    group.add(\'--valid_steps\', \'-valid_steps\', type=int, default=10000,\n              help=\'Perfom validation every X steps\')\n    group.add(\'--valid_batch_size\', \'-valid_batch_size\', type=int, default=32,\n              help=\'Maximum batch size for validation\')\n    group.add(\'--max_generator_batches\', \'-max_generator_batches\',\n              type=int, default=32,\n              help=""Maximum batches of words in a sequence to run ""\n                   ""the generator on in parallel. Higher is faster, but ""\n                   ""uses more memory. Set to 0 to disable."")\n    group.add(\'--train_steps\', \'-train_steps\', type=int, default=100000,\n              help=\'Number of training steps\')\n    group.add(\'--single_pass\', \'-single_pass\', action=\'store_true\',\n              help=""Make a single pass over the training dataset."")\n    group.add(\'--epochs\', \'-epochs\', type=int, default=0,\n              help=\'Deprecated epochs see train_steps\')\n    group.add(\'--early_stopping\', \'-early_stopping\', type=int, default=0,\n              help=\'Number of validation steps without improving.\')\n    group.add(\'--early_stopping_criteria\', \'-early_stopping_criteria\',\n              nargs=""*"", default=None,\n              help=\'Criteria to use for early stopping.\')\n    group.add(\'--optim\', \'-optim\', default=\'sgd\',\n              choices=[\'sgd\', \'adagrad\', \'adadelta\', \'adam\',\n                       \'sparseadam\', \'adafactor\', \'fusedadam\'],\n              help=""Optimization method."")\n    group.add(\'--adagrad_accumulator_init\', \'-adagrad_accumulator_init\',\n              type=float, default=0,\n              help=""Initializes the accumulator values in adagrad. ""\n                   ""Mirrors the initial_accumulator_value option ""\n                   ""in the tensorflow adagrad (use 0.1 for their default)."")\n    group.add(\'--max_grad_norm\', \'-max_grad_norm\', type=float, default=5,\n              help=""If the norm of the gradient vector exceeds this, ""\n                   ""renormalize it to have the norm equal to ""\n                   ""max_grad_norm"")\n    group.add(\'--dropout\', \'-dropout\', type=float, default=[0.3], nargs=\'+\',\n              help=""Dropout probability; applied in LSTM stacks."")\n    group.add(\'--attention_dropout\', \'-attention_dropout\', type=float,\n              default=[0.1], nargs=\'+\',\n              help=""Attention Dropout probability."")\n    group.add(\'--dropout_steps\', \'-dropout_steps\', type=int, nargs=\'+\',\n              default=[0], help=""Steps at which dropout changes."")\n    group.add(\'--truncated_decoder\', \'-truncated_decoder\', type=int, default=0,\n              help=""""""Truncated bptt."""""")\n    group.add(\'--adam_beta1\', \'-adam_beta1\', type=float, default=0.9,\n              help=""The beta1 parameter used by Adam. ""\n                   ""Almost without exception a value of 0.9 is used in ""\n                   ""the literature, seemingly giving good results, ""\n                   ""so we would discourage changing this value from ""\n                   ""the default without due consideration."")\n    group.add(\'--adam_beta2\', \'-adam_beta2\', type=float, default=0.999,\n              help=\'The beta2 parameter used by Adam. \'\n                   \'Typically a value of 0.999 is recommended, as this is \'\n                   \'the value suggested by the original paper describing \'\n                   \'Adam, and is also the value adopted in other frameworks \'\n                   \'such as Tensorflow and Keras, i.e. see: \'\n                   \'https://www.tensorflow.org/api_docs/python/tf/train/Adam\'\n                   \'Optimizer or https://keras.io/optimizers/ . \'\n                   \'Whereas recently the paper ""Attention is All You Need"" \'\n                   \'suggested a value of 0.98 for beta2, this parameter may \'\n                   \'not work well for normal models / default \'\n                   \'baselines.\')\n    group.add(\'--label_smoothing\', \'-label_smoothing\', type=float, default=0.0,\n              help=""Label smoothing value epsilon. ""\n                   ""Probabilities of all non-true labels ""\n                   ""will be smoothed by epsilon / (vocab_size - 1). ""\n                   ""Set to zero to turn off label smoothing. ""\n                   ""For more detailed information, see: ""\n                   ""https://arxiv.org/abs/1512.00567"")\n    group.add(\'--average_decay\', \'-average_decay\', type=float, default=0,\n              help=""Moving average decay. ""\n                   ""Set to other than 0 (e.g. 1e-4) to activate. ""\n                   ""Similar to Marian NMT implementation: ""\n                   ""http://www.aclweb.org/anthology/P18-4020 ""\n                   ""For more detail on Exponential Moving Average: ""\n                   ""https://en.wikipedia.org/wiki/Moving_average"")\n    group.add(\'--average_every\', \'-average_every\', type=int, default=1,\n              help=""Step for moving average. ""\n                   ""Default is every update, ""\n                   ""if -average_decay is set."")\n    group.add(""--src_noise"", ""-src_noise"", type=str, nargs=\'+\',\n              default=[],\n              choices=onmt.modules.source_noise.MultiNoise.NOISES.keys())\n    group.add(""--src_noise_prob"", ""-src_noise_prob"", type=float, nargs=\'+\',\n              default=[],\n              help=""Probabilities of src_noise functions"")\n\n    # learning rate\n    group = parser.add_argument_group(\'Optimization- Rate\')\n    group.add(\'--learning_rate\', \'-learning_rate\', type=float, default=1.0,\n              help=""Starting learning rate. ""\n                   ""Recommended settings: sgd = 1, adagrad = 0.1, ""\n                   ""adadelta = 1, adam = 0.001"")\n    group.add(\'--learning_rate_decay\', \'-learning_rate_decay\',\n              type=float, default=0.5,\n              help=""If update_learning_rate, decay learning rate by ""\n                   ""this much if steps have gone past ""\n                   ""start_decay_steps"")\n    group.add(\'--start_decay_steps\', \'-start_decay_steps\',\n              type=int, default=50000,\n              help=""Start decaying every decay_steps after ""\n                   ""start_decay_steps"")\n    group.add(\'--decay_steps\', \'-decay_steps\', type=int, default=10000,\n              help=""Decay every decay_steps"")\n\n    group.add(\'--decay_method\', \'-decay_method\', type=str, default=""none"",\n              choices=[\'noam\', \'noamwd\', \'rsqrt\', \'none\'],\n              help=""Use a custom decay rate."")\n    group.add(\'--warmup_steps\', \'-warmup_steps\', type=int, default=4000,\n              help=""Number of warmup steps for custom decay."")\n\n    group = parser.add_argument_group(\'Logging\')\n    group.add(\'--report_every\', \'-report_every\', type=int, default=50,\n              help=""Print stats at this interval."")\n    group.add(\'--log_file\', \'-log_file\', type=str, default="""",\n              help=""Output logs to a file under this path."")\n    group.add(\'--log_file_level\', \'-log_file_level\', type=str,\n              action=StoreLoggingLevelAction,\n              choices=StoreLoggingLevelAction.CHOICES,\n              default=""0"")\n    group.add(\'--exp_host\', \'-exp_host\', type=str, default="""",\n              help=""Send logs to this crayon server."")\n    group.add(\'--exp\', \'-exp\', type=str, default="""",\n              help=""Name of the experiment for logging."")\n    # Use Tensorboard for visualization during training\n    group.add(\'--tensorboard\', \'-tensorboard\', action=""store_true"",\n              help=""Use tensorboard for visualization during training. ""\n                   ""Must have the library tensorboard >= 1.14."")\n    group.add(""--tensorboard_log_dir"", ""-tensorboard_log_dir"",\n              type=str, default=""runs/onmt"",\n              help=""Log directory for Tensorboard. ""\n                   ""This is also the name of the run."")\n\n    group = parser.add_argument_group(\'Speech\')\n    # Options most relevant to speech\n    group.add(\'--sample_rate\', \'-sample_rate\', type=int, default=16000,\n              help=""Sample rate."")\n    group.add(\'--window_size\', \'-window_size\', type=float, default=.02,\n              help=""Window size for spectrogram in seconds."")\n\n    # Option most relevant to image input\n    group.add(\'--image_channel_size\', \'-image_channel_size\',\n              type=int, default=3, choices=[3, 1],\n              help=""Using grayscale image can training ""\n                   ""model faster and smaller"")\n\n\ndef translate_opts(parser):\n    """""" Translation / inference options """"""\n    group = parser.add_argument_group(\'Model\')\n    group.add(\'--model\', \'-model\', dest=\'models\', metavar=\'MODEL\',\n              nargs=\'+\', type=str, default=[], required=True,\n              help=""Path to model .pt file(s). ""\n                   ""Multiple models can be specified, ""\n                   ""for ensemble decoding."")\n    group.add(\'--fp32\', \'-fp32\', action=\'store_true\',\n              help=""Force the model to be in FP32 ""\n                   ""because FP16 is very slow on GTX1080(ti)."")\n    group.add(\'--avg_raw_probs\', \'-avg_raw_probs\', action=\'store_true\',\n              help=""If this is set, during ensembling scores from ""\n                   ""different models will be combined by averaging their ""\n                   ""raw probabilities and then taking the log. Otherwise, ""\n                   ""the log probabilities will be averaged directly. ""\n                   ""Necessary for models whose output layers can assign ""\n                   ""zero probability."")\n\n    group = parser.add_argument_group(\'Data\')\n    group.add(\'--data_type\', \'-data_type\', default=""text"",\n              help=""Type of the source input. Options: [text|img]."")\n\n    group.add(\'--src\', \'-src\', required=True,\n              help=""Source sequence to decode (one line per ""\n                   ""sequence)"")\n    group.add(\'--src_dir\', \'-src_dir\', default="""",\n              help=\'Source directory for image or audio files\')\n    group.add(\'--tgt\', \'-tgt\',\n              help=\'True target sequence (optional)\')\n    group.add(\'--tgt_prefix\', \'-tgt_prefix\', action=\'store_true\',\n              help=\'Generate predictions using provided `-tgt` as prefix.\')\n    group.add(\'--shard_size\', \'-shard_size\', type=int, default=10000,\n              help=""Divide src and tgt (if applicable) into ""\n                   ""smaller multiple src and tgt files, then ""\n                   ""build shards, each shard will have ""\n                   ""opt.shard_size samples except last shard. ""\n                   ""shard_size=0 means no segmentation ""\n                   ""shard_size>0 means segment dataset into multiple shards, ""\n                   ""each shard has shard_size samples"")\n    group.add(\'--output\', \'-output\', default=\'pred.txt\',\n              help=""Path to output the predictions (each line will ""\n                   ""be the decoded sequence"")\n    group.add(\'--report_align\', \'-report_align\', action=\'store_true\',\n              help=""Report alignment for each translation."")\n    group.add(\'--report_time\', \'-report_time\', action=\'store_true\',\n              help=""Report some translation time metrics"")\n\n    # Options most relevant to summarization.\n    group.add(\'--dynamic_dict\', \'-dynamic_dict\', action=\'store_true\',\n              help=""Create dynamic dictionaries"")\n    group.add(\'--share_vocab\', \'-share_vocab\', action=\'store_true\',\n              help=""Share source and target vocabulary"")\n\n    group = parser.add_argument_group(\'Random Sampling\')\n    group.add(\'--random_sampling_topk\', \'-random_sampling_topk\',\n              default=1, type=int,\n              help=""Set this to -1 to do random sampling from full ""\n                   ""distribution. Set this to value k>1 to do random ""\n                   ""sampling restricted to the k most likely next tokens. ""\n                   ""Set this to 1 to use argmax or for doing beam ""\n                   ""search."")\n    group.add(\'--random_sampling_temp\', \'-random_sampling_temp\',\n              default=1., type=float,\n              help=""If doing random sampling, divide the logits by ""\n                   ""this before computing softmax during decoding."")\n    group.add(\'--seed\', \'-seed\', type=int, default=829,\n              help=""Random seed"")\n\n    group = parser.add_argument_group(\'Beam\')\n    group.add(\'--beam_size\', \'-beam_size\', type=int, default=5,\n              help=\'Beam size\')\n    group.add(\'--min_length\', \'-min_length\', type=int, default=0,\n              help=\'Minimum prediction length\')\n    group.add(\'--max_length\', \'-max_length\', type=int, default=100,\n              help=\'Maximum prediction length.\')\n    group.add(\'--max_sent_length\', \'-max_sent_length\', action=DeprecateAction,\n              help=""Deprecated, use `-max_length` instead"")\n\n    # Alpha and Beta values for Google Length + Coverage penalty\n    # Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7\n    group.add(\'--stepwise_penalty\', \'-stepwise_penalty\', action=\'store_true\',\n              help=""Apply penalty at every decoding step. ""\n                   ""Helpful for summary penalty."")\n    group.add(\'--length_penalty\', \'-length_penalty\', default=\'none\',\n              choices=[\'none\', \'wu\', \'avg\'],\n              help=""Length Penalty to use."")\n    group.add(\'--ratio\', \'-ratio\', type=float, default=-0.,\n              help=""Ratio based beam stop condition"")\n    group.add(\'--coverage_penalty\', \'-coverage_penalty\', default=\'none\',\n              choices=[\'none\', \'wu\', \'summary\'],\n              help=""Coverage Penalty to use."")\n    group.add(\'--alpha\', \'-alpha\', type=float, default=0.,\n              help=""Google NMT length penalty parameter ""\n                   ""(higher = longer generation)"")\n    group.add(\'--beta\', \'-beta\', type=float, default=-0.,\n              help=""Coverage penalty parameter"")\n    group.add(\'--block_ngram_repeat\', \'-block_ngram_repeat\',\n              type=int, default=0,\n              help=\'Block repetition of ngrams during decoding.\')\n    group.add(\'--ignore_when_blocking\', \'-ignore_when_blocking\',\n              nargs=\'+\', type=str, default=[],\n              help=""Ignore these strings when blocking repeats. ""\n                   ""You want to block sentence delimiters."")\n    group.add(\'--replace_unk\', \'-replace_unk\', action=""store_true"",\n              help=""Replace the generated UNK tokens with the ""\n                   ""source token that had highest attention weight. If ""\n                   ""phrase_table is provided, it will look up the ""\n                   ""identified source token and give the corresponding ""\n                   ""target token. If it is not provided (or the identified ""\n                   ""source token does not exist in the table), then it ""\n                   ""will copy the source token."")\n    group.add(\'--phrase_table\', \'-phrase_table\', type=str, default="""",\n              help=""If phrase_table is provided (with replace_unk), it will ""\n                   ""look up the identified source token and give the ""\n                   ""corresponding target token. If it is not provided ""\n                   ""(or the identified source token does not exist in ""\n                   ""the table), then it will copy the source token."")\n    group = parser.add_argument_group(\'Logging\')\n    group.add(\'--verbose\', \'-verbose\', action=""store_true"",\n              help=\'Print scores and predictions for each sentence\')\n    group.add(\'--log_file\', \'-log_file\', type=str, default="""",\n              help=""Output logs to a file under this path."")\n    group.add(\'--log_file_level\', \'-log_file_level\', type=str,\n              action=StoreLoggingLevelAction,\n              choices=StoreLoggingLevelAction.CHOICES,\n              default=""0"")\n    group.add(\'--attn_debug\', \'-attn_debug\', action=""store_true"",\n              help=\'Print best attn for each word\')\n    group.add(\'--align_debug\', \'-align_debug\', action=""store_true"",\n              help=\'Print best align for each word\')\n    group.add(\'--dump_beam\', \'-dump_beam\', type=str, default="""",\n              help=\'File to dump beam information to.\')\n    group.add(\'--n_best\', \'-n_best\', type=int, default=1,\n              help=""If verbose is set, will output the n_best ""\n                   ""decoded sentences"")\n\n    group = parser.add_argument_group(\'Efficiency\')\n    group.add(\'--batch_size\', \'-batch_size\', type=int, default=30,\n              help=\'Batch size\')\n    group.add(\'--batch_type\', \'-batch_type\', default=\'sents\',\n              choices=[""sents"", ""tokens""],\n              help=""Batch grouping for batch_size. Standard ""\n                   ""is sents. Tokens will do dynamic batching"")\n    group.add(\'--gpu\', \'-gpu\', type=int, default=-1,\n              help=""Device to run on"")\n\n    # Options most relevant to speech.\n    group = parser.add_argument_group(\'Speech\')\n    group.add(\'--sample_rate\', \'-sample_rate\', type=int, default=16000,\n              help=""Sample rate."")\n    group.add(\'--window_size\', \'-window_size\', type=float, default=.02,\n              help=\'Window size for spectrogram in seconds\')\n    group.add(\'--window_stride\', \'-window_stride\', type=float, default=.01,\n              help=\'Window stride for spectrogram in seconds\')\n    group.add(\'--window\', \'-window\', default=\'hamming\',\n              help=\'Window type for spectrogram generation\')\n\n    # Option most relevant to image input\n    group.add(\'--image_channel_size\', \'-image_channel_size\',\n              type=int, default=3, choices=[3, 1],\n              help=""Using grayscale image can training ""\n                   ""model faster and smaller"")\n\n\n# Copyright 2016 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\nclass StoreLoggingLevelAction(configargparse.Action):\n    """""" Convert string to logging level """"""\n    import logging\n    LEVELS = {\n        ""CRITICAL"": logging.CRITICAL,\n        ""ERROR"": logging.ERROR,\n        ""WARNING"": logging.WARNING,\n        ""INFO"": logging.INFO,\n        ""DEBUG"": logging.DEBUG,\n        ""NOTSET"": logging.NOTSET\n    }\n\n    CHOICES = list(LEVELS.keys()) + [str(_) for _ in LEVELS.values()]\n\n    def __init__(self, option_strings, dest, help=None, **kwargs):\n        super(StoreLoggingLevelAction, self).__init__(\n            option_strings, dest, help=help, **kwargs)\n\n    def __call__(self, parser, namespace, value, option_string=None):\n        # Get the key \'value\' in the dict, or just use \'value\'\n        level = StoreLoggingLevelAction.LEVELS.get(value, value)\n        setattr(namespace, self.dest, level)\n\n\nclass DeprecateAction(configargparse.Action):\n    """""" Deprecate action """"""\n\n    def __init__(self, option_strings, dest, help=None, **kwargs):\n        super(DeprecateAction, self).__init__(option_strings, dest, nargs=0,\n                                              help=help, **kwargs)\n\n    def __call__(self, parser, namespace, values, flag_name):\n        help = self.help if self.help is not None else """"\n        msg = ""Flag \'%s\' is deprecated. %s"" % (flag_name, help)\n        raise configargparse.ArgumentTypeError(msg)\n'"
onmt/train_single.py,3,"b'#!/usr/bin/env python\n""""""Training on a single process.""""""\nimport os\n\nimport torch\n\nfrom onmt.inputters.inputter import build_dataset_iter, patch_fields, \\\n    load_old_vocab, old_style_vocab, build_dataset_iter_multiple\nfrom onmt.model_builder import build_model\nfrom onmt.utils.optimizers import Optimizer\nfrom onmt.utils.misc import set_random_seed\nfrom onmt.trainer import build_trainer\nfrom onmt.models import build_model_saver\nfrom onmt.utils.logging import init_logger, logger\nfrom onmt.utils.parse import ArgumentParser\n\n\ndef _check_save_model_path(opt):\n    save_model_path = os.path.abspath(opt.save_model)\n    model_dirname = os.path.dirname(save_model_path)\n    if not os.path.exists(model_dirname):\n        os.makedirs(model_dirname)\n\n\ndef _tally_parameters(model):\n    enc = 0\n    dec = 0\n    for name, param in model.named_parameters():\n        if \'encoder\' in name:\n            enc += param.nelement()\n        else:\n            dec += param.nelement()\n    return enc + dec, enc, dec\n\n\ndef configure_process(opt, device_id):\n    if device_id >= 0:\n        torch.cuda.set_device(device_id)\n    set_random_seed(opt.seed, device_id >= 0)\n\n\ndef main(opt, device_id, batch_queue=None, semaphore=None):\n    # NOTE: It\'s important that ``opt`` has been validated and updated\n    # at this point.\n    configure_process(opt, device_id)\n    init_logger(opt.log_file)\n    assert len(opt.accum_count) == len(opt.accum_steps), \\\n        \'Number of accum_count values must match number of accum_steps\'\n    # Load checkpoint if we resume from a previous training.\n    if opt.train_from:\n        logger.info(\'Loading checkpoint from %s\' % opt.train_from)\n        checkpoint = torch.load(opt.train_from,\n                                map_location=lambda storage, loc: storage)\n        model_opt = ArgumentParser.ckpt_model_opts(checkpoint[""opt""])\n        ArgumentParser.update_model_opts(model_opt)\n        ArgumentParser.validate_model_opts(model_opt)\n        logger.info(\'Loading vocab from checkpoint at %s.\' % opt.train_from)\n        vocab = checkpoint[\'vocab\']\n    else:\n        checkpoint = None\n        model_opt = opt\n        vocab = torch.load(opt.data + \'.vocab.pt\')\n\n    # check for code where vocab is saved instead of fields\n    # (in the future this will be done in a smarter way)\n    if old_style_vocab(vocab):\n        fields = load_old_vocab(\n            vocab, opt.model_type, dynamic_dict=opt.copy_attn)\n    else:\n        fields = vocab\n\n    # patch for fields that may be missing in old data/model\n    patch_fields(opt, fields)\n\n    # Report src and tgt vocab sizes, including for features\n    for side in [\'src\', \'tgt\']:\n        f = fields[side]\n        try:\n            f_iter = iter(f)\n        except TypeError:\n            f_iter = [(side, f)]\n        for sn, sf in f_iter:\n            if sf.use_vocab:\n                logger.info(\' * %s vocab size = %d\' % (sn, len(sf.vocab)))\n\n    # Build model.\n    model = build_model(model_opt, opt, fields, checkpoint)\n    n_params, enc, dec = _tally_parameters(model)\n    logger.info(\'encoder: %d\' % enc)\n    logger.info(\'decoder: %d\' % dec)\n    logger.info(\'* number of parameters: %d\' % n_params)\n    _check_save_model_path(opt)\n\n    # Build optimizer.\n    optim = Optimizer.from_opt(model, opt, checkpoint=checkpoint)\n\n    # Build model saver\n    model_saver = build_model_saver(model_opt, opt, model, fields, optim)\n\n    trainer = build_trainer(\n        opt, device_id, model, fields, optim, model_saver=model_saver)\n\n    if batch_queue is None:\n        if len(opt.data_ids) > 1:\n            train_shards = []\n            for train_id in opt.data_ids:\n                shard_base = ""train_"" + train_id\n                train_shards.append(shard_base)\n            train_iter = build_dataset_iter_multiple(train_shards, fields, opt)\n        else:\n            if opt.data_ids[0] is not None:\n                shard_base = ""train_"" + opt.data_ids[0]\n            else:\n                shard_base = ""train""\n            train_iter = build_dataset_iter(shard_base, fields, opt)\n\n    else:\n        assert semaphore is not None, \\\n            ""Using batch_queue requires semaphore as well""\n\n        def _train_iter():\n            while True:\n                batch = batch_queue.get()\n                semaphore.release()\n                yield batch\n\n        train_iter = _train_iter()\n\n    valid_iter = build_dataset_iter(\n        ""valid"", fields, opt, is_train=False)\n\n    if len(opt.gpu_ranks):\n        logger.info(\'Starting training on GPU: %s\' % opt.gpu_ranks)\n    else:\n        logger.info(\'Starting training on CPU, could be very slow\')\n    train_steps = opt.train_steps\n    if opt.single_pass and train_steps > 0:\n        logger.warning(""Option single_pass is enabled, ignoring train_steps."")\n        train_steps = 0\n\n    trainer.train(\n        train_iter,\n        train_steps,\n        save_checkpoint_steps=opt.save_checkpoint_steps,\n        valid_iter=valid_iter,\n        valid_steps=opt.valid_steps)\n\n    if trainer.report_manager.tensorboard_writer is not None:\n        trainer.report_manager.tensorboard_writer.close()\n'"
onmt/trainer.py,1,"b'""""""\n    This is the loadable seq2seq trainer library that is\n    in charge of training details, loss compute, and statistics.\n    See train.py for a use case of this library.\n\n    Note: To make this a general library, we implement *only*\n          mechanism things here(i.e. what to do), and leave the strategy\n          things to users(i.e. how to do it). Also see train.py(one of the\n          users of this library) for the strategy things we do.\n""""""\n\nimport torch\nimport traceback\n\nimport onmt.utils\nfrom onmt.utils.logging import logger\n\n\ndef build_trainer(opt, device_id, model, fields, optim, model_saver=None):\n    """"""\n    Simplify `Trainer` creation based on user `opt`s*\n\n    Args:\n        opt (:obj:`Namespace`): user options (usually from argument parsing)\n        model (:obj:`onmt.models.NMTModel`): the model to train\n        fields (dict): dict of fields\n        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n        data_type (str): string describing the type of data\n            e.g. ""text"", ""img"", ""audio""\n        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n            used to save the model\n    """"""\n\n    tgt_field = dict(fields)[""tgt""].base_field\n    train_loss = onmt.utils.loss.build_loss_compute(model, tgt_field, opt)\n    valid_loss = onmt.utils.loss.build_loss_compute(\n        model, tgt_field, opt, train=False)\n\n    trunc_size = opt.truncated_decoder  # Badly named...\n    shard_size = opt.max_generator_batches if opt.model_dtype == \'fp32\' else 0\n    norm_method = opt.normalization\n    accum_count = opt.accum_count\n    accum_steps = opt.accum_steps\n    n_gpu = opt.world_size\n    average_decay = opt.average_decay\n    average_every = opt.average_every\n    dropout = opt.dropout\n    dropout_steps = opt.dropout_steps\n    if device_id >= 0:\n        gpu_rank = opt.gpu_ranks[device_id]\n    else:\n        gpu_rank = 0\n        n_gpu = 0\n    gpu_verbose_level = opt.gpu_verbose_level\n\n    earlystopper = onmt.utils.EarlyStopping(\n        opt.early_stopping, scorers=onmt.utils.scorers_from_opts(opt)) \\\n        if opt.early_stopping > 0 else None\n\n    source_noise = None\n    if len(opt.src_noise) > 0:\n        src_field = dict(fields)[""src""].base_field\n        corpus_id_field = dict(fields).get(""corpus_id"", None)\n        if corpus_id_field is not None:\n            ids_to_noise = corpus_id_field.numericalize(opt.data_to_noise)\n        else:\n            ids_to_noise = None\n        source_noise = onmt.modules.source_noise.MultiNoise(\n            opt.src_noise,\n            opt.src_noise_prob,\n            ids_to_noise=ids_to_noise,\n            pad_idx=src_field.pad_token,\n            end_of_sentence_mask=src_field.end_of_sentence_mask,\n            word_start_mask=src_field.word_start_mask,\n            device_id=device_id\n        )\n\n    report_manager = onmt.utils.build_report_manager(opt, gpu_rank)\n    trainer = onmt.Trainer(model, train_loss, valid_loss, optim, trunc_size,\n                           shard_size, norm_method,\n                           accum_count, accum_steps,\n                           n_gpu, gpu_rank,\n                           gpu_verbose_level, report_manager,\n                           with_align=True if opt.lambda_align > 0 else False,\n                           model_saver=model_saver if gpu_rank == 0 else None,\n                           average_decay=average_decay,\n                           average_every=average_every,\n                           model_dtype=opt.model_dtype,\n                           earlystopper=earlystopper,\n                           dropout=dropout,\n                           dropout_steps=dropout_steps,\n                           source_noise=source_noise)\n    return trainer\n\n\nclass Trainer(object):\n    """"""\n    Class that controls the training process.\n\n    Args:\n            model(:py:class:`onmt.models.model.NMTModel`): translation model\n                to train\n            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n               training loss computation\n            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n               training loss computation\n            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n               the optimizer responsible for update\n            trunc_size(int): length of truncated back propagation through time\n            shard_size(int): compute loss in shards of this size for efficiency\n            data_type(string): type of the source input: [text|img|audio]\n            norm_method(string): normalization methods: [sents|tokens]\n            accum_count(list): accumulate gradients this many times.\n            accum_steps(list): steps for accum gradients changes.\n            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n                the object that creates reports, or None\n            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n                used to save a checkpoint.\n                Thus nothing will be saved if this parameter is None\n    """"""\n\n    def __init__(self, model, train_loss, valid_loss, optim,\n                 trunc_size=0, shard_size=32,\n                 norm_method=""sents"", accum_count=[1],\n                 accum_steps=[0],\n                 n_gpu=1, gpu_rank=1, gpu_verbose_level=0,\n                 report_manager=None, with_align=False, model_saver=None,\n                 average_decay=0, average_every=1, model_dtype=\'fp32\',\n                 earlystopper=None, dropout=[0.3], dropout_steps=[0],\n                 source_noise=None):\n        # Basic attributes.\n        self.model = model\n        self.train_loss = train_loss\n        self.valid_loss = valid_loss\n        self.optim = optim\n        self.trunc_size = trunc_size\n        self.shard_size = shard_size\n        self.norm_method = norm_method\n        self.accum_count_l = accum_count\n        self.accum_count = accum_count[0]\n        self.accum_steps = accum_steps\n        self.n_gpu = n_gpu\n        self.gpu_rank = gpu_rank\n        self.gpu_verbose_level = gpu_verbose_level\n        self.report_manager = report_manager\n        self.with_align = with_align\n        self.model_saver = model_saver\n        self.average_decay = average_decay\n        self.moving_average = None\n        self.average_every = average_every\n        self.model_dtype = model_dtype\n        self.earlystopper = earlystopper\n        self.dropout = dropout\n        self.dropout_steps = dropout_steps\n        self.source_noise = source_noise\n\n        for i in range(len(self.accum_count_l)):\n            assert self.accum_count_l[i] > 0\n            if self.accum_count_l[i] > 1:\n                assert self.trunc_size == 0, \\\n                    """"""To enable accumulated gradients,\n                       you must disable target sequence truncating.""""""\n\n        # Set model in training mode.\n        self.model.train()\n\n    def _accum_count(self, step):\n        for i in range(len(self.accum_steps)):\n            if step > self.accum_steps[i]:\n                _accum = self.accum_count_l[i]\n        return _accum\n\n    def _maybe_update_dropout(self, step):\n        for i in range(len(self.dropout_steps)):\n            if step > 1 and step == self.dropout_steps[i] + 1:\n                self.model.update_dropout(self.dropout[i])\n                logger.info(""Updated dropout to %f from step %d""\n                            % (self.dropout[i], step))\n\n    def _accum_batches(self, iterator):\n        batches = []\n        normalization = 0\n        self.accum_count = self._accum_count(self.optim.training_step)\n        for batch in iterator:\n            batches.append(batch)\n            if self.norm_method == ""tokens"":\n                num_tokens = batch.tgt[1:, :, 0].ne(\n                    self.train_loss.padding_idx).sum()\n                normalization += num_tokens.item()\n            else:\n                normalization += batch.batch_size\n            if len(batches) == self.accum_count:\n                yield batches, normalization\n                self.accum_count = self._accum_count(self.optim.training_step)\n                batches = []\n                normalization = 0\n        if batches:\n            yield batches, normalization\n\n    def _update_average(self, step):\n        if self.moving_average is None:\n            copy_params = [params.detach().float()\n                           for params in self.model.parameters()]\n            self.moving_average = copy_params\n        else:\n            average_decay = max(self.average_decay,\n                                1 - (step + 1)/(step + 10))\n            for (i, avg), cpt in zip(enumerate(self.moving_average),\n                                     self.model.parameters()):\n                self.moving_average[i] = \\\n                    (1 - average_decay) * avg + \\\n                    cpt.detach().float() * average_decay\n\n    def train(self,\n              train_iter,\n              train_steps,\n              save_checkpoint_steps=5000,\n              valid_iter=None,\n              valid_steps=10000):\n        """"""\n        The main training loop by iterating over `train_iter` and possibly\n        running validation on `valid_iter`.\n\n        Args:\n            train_iter: A generator that returns the next training batch.\n            train_steps: Run training for this many iterations.\n            save_checkpoint_steps: Save a checkpoint every this many\n              iterations.\n            valid_iter: A generator that returns the next validation batch.\n            valid_steps: Run evaluation every this many iterations.\n\n        Returns:\n            The gathered statistics.\n        """"""\n        if valid_iter is None:\n            logger.info(\'Start training loop without validation...\')\n        else:\n            logger.info(\'Start training loop and validate every %d steps...\',\n                        valid_steps)\n\n        total_stats = onmt.utils.Statistics()\n        report_stats = onmt.utils.Statistics()\n        self._start_report_manager(start_time=total_stats.start_time)\n\n        for i, (batches, normalization) in enumerate(\n                self._accum_batches(train_iter)):\n            step = self.optim.training_step\n            # UPDATE DROPOUT\n            self._maybe_update_dropout(step)\n\n            if self.gpu_verbose_level > 1:\n                logger.info(""GpuRank %d: index: %d"", self.gpu_rank, i)\n            if self.gpu_verbose_level > 0:\n                logger.info(""GpuRank %d: reduce_counter: %d \\\n                            n_minibatch %d""\n                            % (self.gpu_rank, i + 1, len(batches)))\n\n            if self.n_gpu > 1:\n                normalization = sum(onmt.utils.distributed\n                                    .all_gather_list\n                                    (normalization))\n\n            self._gradient_accumulation(\n                batches, normalization, total_stats,\n                report_stats)\n\n            if self.average_decay > 0 and i % self.average_every == 0:\n                self._update_average(step)\n\n            report_stats = self._maybe_report_training(\n                step, train_steps,\n                self.optim.learning_rate(),\n                report_stats)\n\n            if valid_iter is not None and step % valid_steps == 0:\n                if self.gpu_verbose_level > 0:\n                    logger.info(\'GpuRank %d: validate step %d\'\n                                % (self.gpu_rank, step))\n                valid_stats = self.validate(\n                    valid_iter, moving_average=self.moving_average)\n                if self.gpu_verbose_level > 0:\n                    logger.info(\'GpuRank %d: gather valid stat \\\n                                step %d\' % (self.gpu_rank, step))\n                valid_stats = self._maybe_gather_stats(valid_stats)\n                if self.gpu_verbose_level > 0:\n                    logger.info(\'GpuRank %d: report stat step %d\'\n                                % (self.gpu_rank, step))\n                self._report_step(self.optim.learning_rate(),\n                                  step, valid_stats=valid_stats)\n                # Run patience mechanism\n                if self.earlystopper is not None:\n                    self.earlystopper(valid_stats, step)\n                    # If the patience has reached the limit, stop training\n                    if self.earlystopper.has_stopped():\n                        break\n\n            if (self.model_saver is not None\n                and (save_checkpoint_steps != 0\n                     and step % save_checkpoint_steps == 0)):\n                self.model_saver.save(step, moving_average=self.moving_average)\n\n            if train_steps > 0 and step >= train_steps:\n                break\n\n        if self.model_saver is not None:\n            self.model_saver.save(step, moving_average=self.moving_average)\n        return total_stats\n\n    def validate(self, valid_iter, moving_average=None):\n        """""" Validate model.\n            valid_iter: validate data iterator\n        Returns:\n            :obj:`nmt.Statistics`: validation loss statistics\n        """"""\n        valid_model = self.model\n        if moving_average:\n            # swap model params w/ moving average\n            # (and keep the original parameters)\n            model_params_data = []\n            for avg, param in zip(self.moving_average,\n                                  valid_model.parameters()):\n                model_params_data.append(param.data)\n                param.data = avg.data.half() if self.optim._fp16 == ""legacy"" \\\n                    else avg.data\n\n        # Set model in validating mode.\n        valid_model.eval()\n\n        with torch.no_grad():\n            stats = onmt.utils.Statistics()\n\n            for batch in valid_iter:\n                src, src_lengths = batch.src if isinstance(batch.src, tuple) \\\n                                   else (batch.src, None)\n                tgt = batch.tgt\n\n                # F-prop through the model.\n                outputs, attns = valid_model(src, tgt, src_lengths,\n                                             with_align=self.with_align)\n\n                # Compute loss.\n                _, batch_stats = self.valid_loss(batch, outputs, attns)\n\n                # Update statistics.\n                stats.update(batch_stats)\n        if moving_average:\n            for param_data, param in zip(model_params_data,\n                                         self.model.parameters()):\n                param.data = param_data\n\n        # Set model back to training mode.\n        valid_model.train()\n\n        return stats\n\n    def _gradient_accumulation(self, true_batches, normalization, total_stats,\n                               report_stats):\n        if self.accum_count > 1:\n            self.optim.zero_grad()\n\n        for k, batch in enumerate(true_batches):\n            target_size = batch.tgt.size(0)\n            # Truncated BPTT: reminder not compatible with accum > 1\n            if self.trunc_size:\n                trunc_size = self.trunc_size\n            else:\n                trunc_size = target_size\n\n            batch = self.maybe_noise_source(batch)\n\n            src, src_lengths = batch.src if isinstance(batch.src, tuple) \\\n                else (batch.src, None)\n            if src_lengths is not None:\n                report_stats.n_src_words += src_lengths.sum().item()\n\n            tgt_outer = batch.tgt\n\n            bptt = False\n            for j in range(0, target_size-1, trunc_size):\n                # 1. Create truncated target.\n                tgt = tgt_outer[j: j + trunc_size]\n\n                # 2. F-prop all but generator.\n                if self.accum_count == 1:\n                    self.optim.zero_grad()\n\n                outputs, attns = self.model(src, tgt, src_lengths, bptt=bptt,\n                                            with_align=self.with_align)\n                bptt = True\n\n                # 3. Compute loss.\n                try:\n                    loss, batch_stats = self.train_loss(\n                        batch,\n                        outputs,\n                        attns,\n                        normalization=normalization,\n                        shard_size=self.shard_size,\n                        trunc_start=j,\n                        trunc_size=trunc_size)\n\n                    if loss is not None:\n                        self.optim.backward(loss)\n\n                    total_stats.update(batch_stats)\n                    report_stats.update(batch_stats)\n\n                except Exception:\n                    traceback.print_exc()\n                    logger.info(""At step %d, we removed a batch - accum %d"",\n                                self.optim.training_step, k)\n\n                # 4. Update the parameters and statistics.\n                if self.accum_count == 1:\n                    # Multi GPU gradient gather\n                    if self.n_gpu > 1:\n                        grads = [p.grad.data for p in self.model.parameters()\n                                 if p.requires_grad\n                                 and p.grad is not None]\n                        onmt.utils.distributed.all_reduce_and_rescale_tensors(\n                            grads, float(1))\n                    self.optim.step()\n\n                # If truncated, don\'t backprop fully.\n                # TO CHECK\n                # if dec_state is not None:\n                #    dec_state.detach()\n                if self.model.decoder.state is not None:\n                    self.model.decoder.detach_state()\n\n        # in case of multi step gradient accumulation,\n        # update only after accum batches\n        if self.accum_count > 1:\n            if self.n_gpu > 1:\n                grads = [p.grad.data for p in self.model.parameters()\n                         if p.requires_grad\n                         and p.grad is not None]\n                onmt.utils.distributed.all_reduce_and_rescale_tensors(\n                    grads, float(1))\n            self.optim.step()\n\n    def _start_report_manager(self, start_time=None):\n        """"""\n        Simple function to start report manager (if any)\n        """"""\n        if self.report_manager is not None:\n            if start_time is None:\n                self.report_manager.start()\n            else:\n                self.report_manager.start_time = start_time\n\n    def _maybe_gather_stats(self, stat):\n        """"""\n        Gather statistics in multi-processes cases\n\n        Args:\n            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n                or None (it returns None in this case)\n\n        Returns:\n            stat: the updated (or unchanged) stat object\n        """"""\n        if stat is not None and self.n_gpu > 1:\n            return onmt.utils.Statistics.all_gather_stats(stat)\n        return stat\n\n    def _maybe_report_training(self, step, num_steps, learning_rate,\n                               report_stats):\n        """"""\n        Simple function to report training stats (if report_manager is set)\n        see `onmt.utils.ReportManagerBase.report_training` for doc\n        """"""\n        if self.report_manager is not None:\n            return self.report_manager.report_training(\n                step, num_steps, learning_rate, report_stats,\n                multigpu=self.n_gpu > 1)\n\n    def _report_step(self, learning_rate, step, train_stats=None,\n                     valid_stats=None):\n        """"""\n        Simple function to report stats (if report_manager is set)\n        see `onmt.utils.ReportManagerBase.report_step` for doc\n        """"""\n        if self.report_manager is not None:\n            return self.report_manager.report_step(\n                learning_rate, step, train_stats=train_stats,\n                valid_stats=valid_stats)\n\n    def maybe_noise_source(self, batch):\n        if self.source_noise is not None:\n            return self.source_noise(batch)\n        return batch\n'"
tools/apply_bpe.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n# flake8: noqa\n\n""""""Use operations learned with learn_bpe.py to encode a new text.\nThe text will not be smaller, but use only a fixed vocabulary, with rare words\nencoded as variable-length sequences of subword units.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2015). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n# This file is retrieved from https://github.com/rsennrich/subword-nmt\n\nfrom __future__ import unicode_literals, division\n\nimport sys\nimport codecs\nimport io\nimport argparse\nimport json\nimport re\nfrom collections import defaultdict\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n\nclass BPE(object):\n\n    def __init__(self, codes, separator=\'@@\', vocab=None, glossaries=None):\n\n        # check version information\n        firstline = codes.readline()\n        if firstline.startswith(\'#version:\'):\n            self.version = tuple([int(x) for x in re.sub(\n                r\'(\\.0+)*$\', \'\', firstline.split()[-1]).split(""."")])\n        else:\n            self.version = (0, 1)\n            codes.seek(0)\n\n        self.bpe_codes = [tuple(item.split()) for item in codes]\n\n        # some hacking to deal with duplicates (only consider first instance)\n        self.bpe_codes = dict(\n            [(code, i) for (i, code) in reversed(list(enumerate(self.bpe_codes)))])\n\n        self.bpe_codes_reverse = dict(\n            [(pair[0] + pair[1], pair) for pair, i in self.bpe_codes.items()])\n\n        self.separator = separator\n\n        self.vocab = vocab\n\n        self.glossaries = glossaries if glossaries else []\n\n        self.cache = {}\n\n    def segment(self, sentence):\n        """"""segment single sentence (whitespace-tokenized string) with BPE encoding""""""\n        output = []\n        for word in sentence.split():\n            new_word = [out for segment in self._isolate_glossaries(word)\n                        for out in encode(segment,\n                                          self.bpe_codes,\n                                          self.bpe_codes_reverse,\n                                          self.vocab,\n                                          self.separator,\n                                          self.version,\n                                          self.cache,\n                                          self.glossaries)]\n\n            for item in new_word[:-1]:\n                output.append(item + self.separator)\n            output.append(new_word[-1])\n\n        return \' \'.join(output)\n\n    def _isolate_glossaries(self, word):\n        word_segments = [word]\n        for gloss in self.glossaries:\n            word_segments = [out_segments for segment in word_segments\n                             for out_segments in isolate_glossary(segment, gloss)]\n        return word_segments\n\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input file (default: standard input)."")\n    parser.add_argument(\n        \'--codes\', \'-c\', type=argparse.FileType(\'r\'), metavar=\'PATH\',\n        required=True,\n        help=""File with BPE codes (created by learn_bpe.py)."")\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file (default: standard output)"")\n    parser.add_argument(\n        \'--separator\', \'-s\', type=str, default=\'@@\', metavar=\'STR\',\n        help=""Separator between non-final subword units (default: \'%(default)s\'))"")\n    parser.add_argument(\n        \'--vocabulary\', type=argparse.FileType(\'r\'), default=None,\n        metavar=""PATH"",\n        help=""Vocabulary file (built with get_vocab.py). If provided, this script reverts any merge operations that produce an OOV."")\n    parser.add_argument(\n        \'--vocabulary-threshold\', type=int, default=None,\n        metavar=""INT"",\n        help=""Vocabulary threshold. If vocabulary is provided, any word with frequency < threshold will be treated as OOV"")\n    parser.add_argument(\n        \'--glossaries\', type=str, nargs=\'+\', default=None,\n        metavar=""STR"",\n        help=""Glossaries. The strings provided in glossaries will not be affected"" +\n             ""by the BPE (i.e. they will neither be broken into subwords, nor concatenated with other subwords"")\n\n    return parser\n\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries=None):\n    """"""Encode word based on list of BPE merge operations, which are applied consecutively\n    """"""\n\n    if orig in cache:\n        return cache[orig]\n\n    if orig in glossaries:\n        cache[orig] = (orig,)\n        return (orig,)\n\n    if version == (0, 1):\n        word = tuple(orig) + (\'</w>\',)\n    elif version == (0, 2):  # more consistent handling of word-final segments\n        word = tuple(orig[:-1]) + (orig[-1] + \'</w>\',)\n    else:\n        raise NotImplementedError\n\n    pairs = get_pairs(word)\n\n    if not pairs:\n        return orig\n\n    while True:\n        bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float(\'inf\')))\n        if bigram not in bpe_codes:\n            break\n        first, second = bigram\n        new_word = []\n        i = 0\n        while i < len(word):\n            try:\n                j = word.index(first, i)\n                new_word.extend(word[i:j])\n                i = j\n            except:\n                new_word.extend(word[i:])\n                break\n\n            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                new_word.append(first + second)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_word = tuple(new_word)\n        word = new_word\n        if len(word) == 1:\n            break\n        else:\n            pairs = get_pairs(word)\n\n    # don\'t print end-of-word symbols\n    if word[-1] == \'</w>\':\n        word = word[:-1]\n    elif word[-1].endswith(\'</w>\'):\n        word = word[:-1] + (word[-1].replace(\'</w>\', \'\'),)\n\n    if vocab:\n        word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n\n    cache[orig] = word\n    return word\n\n\ndef recursive_split(segment, bpe_codes, vocab, separator, final=False):\n    """"""Recursively split segment into smaller units (by reversing BPE merges)\n    until all units are either in-vocabulary, or cannot be split futher.""""""\n\n    try:\n        if final:\n            left, right = bpe_codes[segment + \'</w>\']\n            right = right[:-4]\n        else:\n            left, right = bpe_codes[segment]\n    except:\n        #sys.stderr.write(\'cannot split {0} further.\\n\'.format(segment))\n        yield segment\n        return\n\n    if left + separator in vocab:\n        yield left\n    else:\n        for item in recursive_split(left, bpe_codes, vocab, separator, False):\n            yield item\n\n    if (final and right in vocab) or (not final and right + separator in vocab):\n        yield right\n    else:\n        for item in recursive_split(right, bpe_codes, vocab, separator, final):\n            yield item\n\n\ndef check_vocab_and_split(orig, bpe_codes, vocab, separator):\n    """"""Check for each segment in word if it is in-vocabulary,\n    and segment OOV segments into smaller units by reversing the BPE merge operations""""""\n\n    out = []\n\n    for segment in orig[:-1]:\n        if segment + separator in vocab:\n            out.append(segment)\n        else:\n            #sys.stderr.write(\'OOV: {0}\\n\'.format(segment))\n            for item in recursive_split(segment, bpe_codes, vocab, separator, False):\n                out.append(item)\n\n    segment = orig[-1]\n    if segment in vocab:\n        out.append(segment)\n    else:\n        #sys.stderr.write(\'OOV: {0}\\n\'.format(segment))\n        for item in recursive_split(segment, bpe_codes, vocab, separator, True):\n            out.append(item)\n\n    return out\n\n\ndef read_vocabulary(vocab_file, threshold):\n    """"""read vocabulary file produced by get_vocab.py, and filter according to frequency threshold.\n    """"""\n\n    vocabulary = set()\n\n    for line in vocab_file:\n        word, freq = line.split()\n        freq = int(freq)\n        if threshold == None or freq >= threshold:\n            vocabulary.add(word)\n\n    return vocabulary\n\n\ndef isolate_glossary(word, glossary):\n    """"""\n    Isolate a glossary present inside a word.\n\n    Returns a list of subwords. In which all \'glossary\' glossaries are isolated \n\n    For example, if \'USA\' is the glossary and \'1934USABUSA\' the word, the return value is:\n        [\'1934\', \'USA\', \'B\', \'USA\']\n    """"""\n    if word == glossary or glossary not in word:\n        return [word]\n    else:\n        splits = word.split(glossary)\n        segments = [segment.strip() for split in splits[:-1]\n                    for segment in [split, glossary] if segment != \'\']\n        return segments + [splits[-1].strip()] if splits[-1] != \'\' else segments\n\n\nif __name__ == \'__main__\':\n\n    # python 2/3 compatibility\n    if sys.version_info < (3, 0):\n        sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n        sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n        sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n    else:\n        sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding=\'utf-8\')\n        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding=\'utf-8\')\n        sys.stdout = io.TextIOWrapper(\n            sys.stdout.buffer, encoding=\'utf-8\', write_through=True, line_buffering=True)\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    # read/write files as UTF-8\n    args.codes = codecs.open(args.codes.name, encoding=\'utf-8\')\n    if args.input.name != \'<stdin>\':\n        args.input = codecs.open(args.input.name, encoding=\'utf-8\')\n    if args.output.name != \'<stdout>\':\n        args.output = codecs.open(args.output.name, \'w\', encoding=\'utf-8\')\n    if args.vocabulary:\n        args.vocabulary = codecs.open(args.vocabulary.name, encoding=\'utf-8\')\n\n    if args.vocabulary:\n        vocabulary = read_vocabulary(\n            args.vocabulary, args.vocabulary_threshold)\n    else:\n        vocabulary = None\n\n    bpe = BPE(args.codes, args.separator, vocabulary, args.glossaries)\n\n    for line in args.input:\n        args.output.write(bpe.segment(line).strip())\n        args.output.write(\'\\n\')\n'"
tools/average_models.py,0,"b'#!/usr/bin/env python\nfrom onmt.bin.average_models import main\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/create_vocabulary.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport argparse\nimport sys\nimport os\n\n\ndef read_files_batch(file_list):\n    """"""Reads the provided files in batches""""""\n    batch = []  # Keep batch for each file\n    fd_list = []  # File descriptor list\n\n    exit = False  # Flag used for quitting the program in case of error\n    try:\n        for filename in file_list:\n            fd_list.append(open(filename))\n\n        for lines in zip(*fd_list):\n            for i, line in enumerate(lines):\n                line = line.rstrip(""\\n"").split("" "")\n                batch.append(line)\n\n            yield batch\n            batch = []  # Reset batch\n\n    except IOError:\n        print(""Error reading file "" + filename + ""."")\n        exit = True  # Flag to exit the program\n\n    finally:\n        for fd in fd_list:\n            fd.close()\n\n        if exit:  # An error occurred, end execution\n            sys.exit(-1)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-file_type\', default=\'text\',\n                        choices=[\'text\', \'field\'], required=True,\n                        help=""""""Options for vocabulary creation.\n                               The default is \'text\' where the user passes\n                               a corpus or a list of corpora files for which\n                               they want to create a vocabulary from.\n                               If choosing the option \'field\', we assume\n                               the file passed is a torch file created during\n                               the preprocessing stage of an already\n                               preprocessed corpus. The vocabulary file created\n                               will just be the vocabulary inside the field\n                               corresponding to the argument \'side\'."""""")\n    parser.add_argument(""-file"", type=str, nargs=""+"", required=True)\n    parser.add_argument(""-out_file"", type=str, required=True)\n    parser.add_argument(""-side"", choices=[\'src\', \'tgt\'], help=""""""Specifies\n                               \'src\' or \'tgt\' side for \'field\' file_type."""""")\n\n    opt = parser.parse_args()\n\n    vocabulary = {}\n    if opt.file_type == \'text\':\n        print(""Reading input file..."")\n        for batch in read_files_batch(opt.file):\n            for sentence in batch:\n                for w in sentence:\n                    if w in vocabulary:\n                        vocabulary[w] += 1\n                    else:\n                        vocabulary[w] = 1\n\n        print(""Writing vocabulary file..."")\n        with open(opt.out_file, ""w"") as f:\n            for w, count in sorted(vocabulary.items(), key=lambda x: x[1],\n                                   reverse=True):\n                f.write(""{0}\\n"".format(w))\n    else:\n        if opt.side not in [\'src\', \'tgt\']:\n            raise ValueError(""If using -file_type=\'field\', specifies ""\n                             ""\'src\' or \'tgt\' argument for -side."")\n        import torch\n        try:\n            from onmt.inputters.inputter import _old_style_vocab\n        except ImportError:\n            sys.path.insert(1, os.path.join(sys.path[0], \'..\'))\n            from onmt.inputters.inputter import _old_style_vocab\n\n        print(""Reading input file..."")\n        if not len(opt.file) == 1:\n            raise ValueError(""If using -file_type=\'field\', only pass one ""\n                             ""argument for -file."")\n        vocabs = torch.load(opt.file[0])\n        voc = dict(vocabs)[opt.side]\n        if _old_style_vocab(voc):\n            word_list = voc.itos\n        else:\n            try:\n                word_list = voc[0][1].base_field.vocab.itos\n            except AttributeError:\n                word_list = voc[0][1].vocab.itos\n\n        print(""Writing vocabulary file..."")\n        with open(opt.out_file, ""wb"") as f:\n            for w in word_list:\n                f.write(u""{0}\\n"".format(w).encode(""utf-8""))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/embeddings_to_torch.py,7,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import division\nimport six\nimport argparse\nimport torch\nfrom onmt.utils.logging import init_logger, logger\nfrom onmt.inputters.inputter import _old_style_vocab\n\n\ndef get_vocabs(dict_path):\n    fields = torch.load(dict_path)\n\n    vocs = []\n    for side in [\'src\', \'tgt\']:\n        if _old_style_vocab(fields):\n            vocab = next((v for n, v in fields if n == side), None)\n        else:\n            try:\n                vocab = fields[side].base_field.vocab\n            except AttributeError:\n                vocab = fields[side].vocab\n        vocs.append(vocab)\n    enc_vocab, dec_vocab = vocs\n\n    logger.info(""From: %s"" % dict_path)\n    logger.info(""\\t* source vocab: %d words"" % len(enc_vocab))\n    logger.info(""\\t* target vocab: %d words"" % len(dec_vocab))\n\n    return enc_vocab, dec_vocab\n\n\ndef read_embeddings(file_enc, skip_lines=0, filter_set=None):\n    embs = dict()\n    total_vectors_in_file = 0\n    with open(file_enc, \'rb\') as f:\n        for i, line in enumerate(f):\n            if i < skip_lines:\n                continue\n            if not line:\n                break\n            if len(line) == 0:\n                # is this reachable?\n                continue\n\n            l_split = line.decode(\'utf8\').strip().split(\' \')\n            if len(l_split) == 2:\n                continue\n            total_vectors_in_file += 1\n            if filter_set is not None and l_split[0] not in filter_set:\n                continue\n            embs[l_split[0]] = [float(em) for em in l_split[1:]]\n    return embs, total_vectors_in_file\n\n\ndef convert_to_torch_tensor(word_to_float_list_dict, vocab):\n    dim = len(six.next(six.itervalues(word_to_float_list_dict)))\n    tensor = torch.zeros((len(vocab), dim))\n    for word, values in word_to_float_list_dict.items():\n        tensor[vocab.stoi[word]] = torch.Tensor(values)\n    return tensor\n\n\ndef calc_vocab_load_stats(vocab, loaded_embed_dict):\n    matching_count = len(\n        set(vocab.stoi.keys()) & set(loaded_embed_dict.keys()))\n    missing_count = len(vocab) - matching_count\n    percent_matching = matching_count / len(vocab) * 100\n    return matching_count, missing_count, percent_matching\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'embeddings_to_torch.py\')\n    parser.add_argument(\'-emb_file_both\', required=False,\n                        help=""loads Embeddings for both source and target ""\n                             ""from this file."")\n    parser.add_argument(\'-emb_file_enc\', required=False,\n                        help=""source Embeddings from this file"")\n    parser.add_argument(\'-emb_file_dec\', required=False,\n                        help=""target Embeddings from this file"")\n    parser.add_argument(\'-output_file\', required=True,\n                        help=""Output file for the prepared data"")\n    parser.add_argument(\'-dict_file\', required=True,\n                        help=""Dictionary file"")\n    parser.add_argument(\'-verbose\', action=""store_true"", default=False)\n    parser.add_argument(\'-skip_lines\', type=int, default=0,\n                        help=""Skip first lines of the embedding file"")\n    parser.add_argument(\'-type\', choices=[""GloVe"", ""word2vec""],\n                        default=""GloVe"")\n    opt = parser.parse_args()\n\n    enc_vocab, dec_vocab = get_vocabs(opt.dict_file)\n\n    # Read in embeddings\n    skip_lines = 1 if opt.type == ""word2vec"" else opt.skip_lines\n    if opt.emb_file_both is not None:\n        if opt.emb_file_enc is not None:\n            raise ValueError(""If --emb_file_both is passed in, you should not""\n                             ""set --emb_file_enc."")\n        if opt.emb_file_dec is not None:\n            raise ValueError(""If --emb_file_both is passed in, you should not""\n                             ""set --emb_file_dec."")\n        set_of_src_and_tgt_vocab = \\\n            set(enc_vocab.stoi.keys()) | set(dec_vocab.stoi.keys())\n        logger.info(""Reading encoder and decoder embeddings from {}"".format(\n            opt.emb_file_both))\n        src_vectors, total_vec_count = \\\n            read_embeddings(opt.emb_file_both, skip_lines,\n                            set_of_src_and_tgt_vocab)\n        tgt_vectors = src_vectors\n        logger.info(""\\tFound {} total vectors in file"".format(total_vec_count))\n    else:\n        if opt.emb_file_enc is None:\n            raise ValueError(""If --emb_file_enc not provided. Please specify ""\n                             ""the file with encoder embeddings, or pass in ""\n                             ""--emb_file_both"")\n        if opt.emb_file_dec is None:\n            raise ValueError(""If --emb_file_dec not provided. Please specify ""\n                             ""the file with encoder embeddings, or pass in ""\n                             ""--emb_file_both"")\n        logger.info(""Reading encoder embeddings from {}"".format(\n            opt.emb_file_enc))\n        src_vectors, total_vec_count = read_embeddings(\n            opt.emb_file_enc, skip_lines,\n            filter_set=enc_vocab.stoi\n        )\n        logger.info(""\\tFound {} total vectors in file."".format(\n            total_vec_count))\n        logger.info(""Reading decoder embeddings from {}"".format(\n            opt.emb_file_dec))\n        tgt_vectors, total_vec_count = read_embeddings(\n            opt.emb_file_dec, skip_lines,\n            filter_set=dec_vocab.stoi\n        )\n        logger.info(""\\tFound {} total vectors in file"".format(total_vec_count))\n    logger.info(""After filtering to vectors in vocab:"")\n    logger.info(""\\t* enc: %d match, %d missing, (%.2f%%)""\n                % calc_vocab_load_stats(enc_vocab, src_vectors))\n    logger.info(""\\t* dec: %d match, %d missing, (%.2f%%)""\n                % calc_vocab_load_stats(dec_vocab, tgt_vectors))\n\n    # Write to file\n    enc_output_file = opt.output_file + "".enc.pt""\n    dec_output_file = opt.output_file + "".dec.pt""\n    logger.info(""\\nSaving embedding as:\\n\\t* enc: %s\\n\\t* dec: %s""\n                % (enc_output_file, dec_output_file))\n    torch.save(\n        convert_to_torch_tensor(src_vectors, enc_vocab),\n        enc_output_file\n    )\n    torch.save(\n        convert_to_torch_tensor(tgt_vectors, dec_vocab),\n        dec_output_file\n    )\n    logger.info(""\\nDone."")\n\n\nif __name__ == ""__main__"":\n    init_logger(\'embeddings_to_torch.log\')\n    main()\n'"
tools/extract_embeddings.py,2,"b'import argparse\n\nimport torch\n\nimport onmt\nimport onmt.model_builder\nimport onmt.inputters as inputters\nimport onmt.opts\n\nfrom onmt.utils.misc import use_gpu\nfrom onmt.utils.logging import init_logger, logger\n\nparser = argparse.ArgumentParser(description=\'translate.py\')\n\nparser.add_argument(\'-model\', required=True,\n                    help=\'Path to model .pt file\')\nparser.add_argument(\'-output_dir\', default=\'.\',\n                    help=""""""Path to output the embeddings"""""")\nparser.add_argument(\'-gpu\', type=int, default=-1,\n                    help=""Device to run on"")\n\n\ndef write_embeddings(filename, dict, embeddings):\n    with open(filename, \'wb\') as file:\n        for i in range(min(len(embeddings), len(dict.itos))):\n            str = dict.itos[i].encode(""utf-8"")\n            for j in range(len(embeddings[0])):\n                str = str + ("" %5f"" % (embeddings[i][j])).encode(""utf-8"")\n            file.write(str + b""\\n"")\n\n\ndef main():\n    dummy_parser = argparse.ArgumentParser(description=\'train.py\')\n    onmt.opts.model_opts(dummy_parser)\n    dummy_opt = dummy_parser.parse_known_args([])[0]\n    opt = parser.parse_args()\n    opt.cuda = opt.gpu > -1\n    if opt.cuda:\n        torch.cuda.set_device(opt.gpu)\n\n    # Add in default model arguments, possibly added since training.\n    checkpoint = torch.load(opt.model,\n                            map_location=lambda storage, loc: storage)\n    model_opt = checkpoint[\'opt\']\n\n    vocab = checkpoint[\'vocab\']\n    if inputters.old_style_vocab(vocab):\n        fields = onmt.inputters.load_old_vocab(vocab)\n    else:\n        fields = vocab\n    src_dict = fields[\'src\'].base_field.vocab  # assumes src is text\n    tgt_dict = fields[\'tgt\'].base_field.vocab\n\n    model_opt = checkpoint[\'opt\']\n    for arg in dummy_opt.__dict__:\n        if arg not in model_opt:\n            model_opt.__dict__[arg] = dummy_opt.__dict__[arg]\n\n    model = onmt.model_builder.build_base_model(\n        model_opt, fields, use_gpu(opt), checkpoint)\n    encoder = model.encoder\n    decoder = model.decoder\n\n    encoder_embeddings = encoder.embeddings.word_lut.weight.data.tolist()\n    decoder_embeddings = decoder.embeddings.word_lut.weight.data.tolist()\n\n    logger.info(""Writing source embeddings"")\n    write_embeddings(opt.output_dir + ""/src_embeddings.txt"", src_dict,\n                     encoder_embeddings)\n\n    logger.info(""Writing target embeddings"")\n    write_embeddings(opt.output_dir + ""/tgt_embeddings.txt"", tgt_dict,\n                     decoder_embeddings)\n\n    logger.info(\'... done.\')\n    logger.info(\'Converting model...\')\n\n\nif __name__ == ""__main__"":\n    init_logger(\'extract_embeddings.log\')\n    main()\n'"
tools/learn_bpe.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n# flake8: noqa\n\n""""""Use byte pair encoding (BPE) to learn a variable-length encoding of the vocabulary in a text.\nUnlike the original BPE, it does not compress the plain text, but can be used to reduce the vocabulary\nof a text to a configurable number of symbols, with only a small increase in the number of tokens.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n# This file is retrieved from https://github.com/rsennrich/subword-nmt\n\nfrom __future__ import unicode_literals\n\nimport sys\nimport codecs\nimport re\nimport copy\nimport argparse\nfrom collections import defaultdict, Counter\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input text (default: standard input)."")\n\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file for BPE codes (default: standard output)"")\n    parser.add_argument(\n        \'--symbols\', \'-s\', type=int, default=10000,\n        help=""Create this many new symbols (each representing a character n-gram) (default: %(default)s))"")\n    parser.add_argument(\n        \'--min-frequency\', type=int, default=2, metavar=\'FREQ\',\n        help=\'Stop if no symbol pair has frequency >= FREQ (default: %(default)s))\')\n    parser.add_argument(\'--dict-input\', action=""store_true"",\n                        help=""If set, input file is interpreted as a dictionary where each line contains a word-count pair"")\n    parser.add_argument(\n        \'--verbose\', \'-v\', action=""store_true"",\n        help=""verbose mode."")\n\n    return parser\n\n\ndef get_vocabulary(fobj, is_dict=False):\n    """"""Read text and return dictionary that encodes vocabulary\n    """"""\n    vocab = Counter()\n    for line in fobj:\n        if is_dict:\n            word, count = line.strip().split()\n            vocab[word] = int(count)\n        else:\n            for word in line.split():\n                vocab[word] += 1\n    return vocab\n\n\ndef update_pair_statistics(pair, changed, stats, indices):\n    """"""Minimally update the indices and frequency of symbol pairs\n\n    if we merge a pair of symbols, only pairs that overlap with occurrences\n    of this pair are affected, and need to be updated.\n    """"""\n    stats[pair] = 0\n    indices[pair] = defaultdict(int)\n    first, second = pair\n    new_pair = first + second\n    for j, word, old_word, freq in changed:\n\n        # find all instances of pair, and update frequency/indices around it\n        i = 0\n        while True:\n            # find first symbol\n            try:\n                i = old_word.index(first, i)\n            except ValueError:\n                break\n            # if first symbol is followed by second symbol, we\'ve found an occurrence of pair (old_word[i:i+2])\n            if i < len(old_word) - 1 and old_word[i + 1] == second:\n                # assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""\n                if i:\n                    prev = old_word[i - 1:i + 1]\n                    stats[prev] -= freq\n                    indices[prev][j] -= 1\n                if i < len(old_word) - 2:\n                    # assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".\n                    # however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block\n                    if old_word[i + 2] != first or i >= len(old_word) - 3 or old_word[i + 3] != second:\n                        nex = old_word[i + 1:i + 3]\n                        stats[nex] -= freq\n                        indices[nex][j] -= 1\n                i += 2\n            else:\n                i += 1\n\n        i = 0\n        while True:\n            try:\n                # find new pair\n                i = word.index(new_pair, i)\n            except ValueError:\n                break\n            # assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""\n            if i:\n                prev = word[i - 1:i + 1]\n                stats[prev] += freq\n                indices[prev][j] += 1\n            # assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""\n            # however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block\n            if i < len(word) - 1 and word[i + 1] != new_pair:\n                nex = word[i:i + 2]\n                stats[nex] += freq\n                indices[nex][j] += 1\n            i += 1\n\n\ndef get_pair_statistics(vocab):\n    """"""Count frequency of all symbol pairs, and create index""""""\n\n    # data structure of pair frequencies\n    stats = defaultdict(int)\n\n    # index from pairs to words\n    indices = defaultdict(lambda: defaultdict(int))\n\n    for i, (word, freq) in enumerate(vocab):\n        prev_char = word[0]\n        for char in word[1:]:\n            stats[prev_char, char] += freq\n            indices[prev_char, char][i] += 1\n            prev_char = char\n\n    return stats, indices\n\n\ndef replace_pair(pair, vocab, indices):\n    """"""Replace all occurrences of a symbol pair (\'A\', \'B\') with a new symbol \'AB\'""""""\n    first, second = pair\n    pair_str = \'\'.join(pair)\n    pair_str = pair_str.replace(\'\\\\\', \'\\\\\\\\\')\n    changes = []\n    pattern = re.compile(\n        r\'(?<!\\S)\' + re.escape(first + \' \' + second) + r\'(?!\\S)\')\n    if sys.version_info < (3, 0):\n        iterator = indices[pair].iteritems()\n    else:\n        iterator = indices[pair].items()\n    for j, freq in iterator:\n        if freq < 1:\n            continue\n        word, freq = vocab[j]\n        new_word = \' \'.join(word)\n        new_word = pattern.sub(pair_str, new_word)\n        new_word = tuple(new_word.split())\n\n        vocab[j] = (new_word, freq)\n        changes.append((j, new_word, word, freq))\n\n    return changes\n\n\ndef prune_stats(stats, big_stats, threshold):\n    """"""Prune statistics dict for efficiency of max()\n\n    The frequency of a symbol pair never increases, so pruning is generally safe\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\n    big_stats keeps full statistics for when we need to access pruned items\n    """"""\n    for item, freq in list(stats.items()):\n        if freq < threshold:\n            del stats[item]\n            if freq < 0:\n                big_stats[item] += freq\n            else:\n                big_stats[item] = freq\n\n\ndef main(infile, outfile, num_symbols, min_frequency=2, verbose=False, is_dict=False):\n    """"""Learn num_symbols BPE operations from vocabulary, and write to outfile.\n    """"""\n\n    # version 0.2 changes the handling of the end-of-word token (\'</w>\');\n    # version numbering allows bckward compatibility\n    outfile.write(\'#version: 0.2\\n\')\n\n    vocab = get_vocabulary(infile, is_dict)\n    vocab = dict([(tuple(x[:-1]) + (x[-1] + \'</w>\',), y)\n                  for (x, y) in vocab.items()])\n    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n\n    stats, indices = get_pair_statistics(sorted_vocab)\n    big_stats = copy.deepcopy(stats)\n    # threshold is inspired by Zipfian assumption, but should only affect speed\n    threshold = max(stats.values()) / 10\n    for i in range(num_symbols):\n        if stats:\n            most_frequent = max(stats, key=lambda x: (stats[x], x))\n\n        # we probably missed the best pair because of pruning; go back to full statistics\n        if not stats or (i and stats[most_frequent] < threshold):\n            prune_stats(stats, big_stats, threshold)\n            stats = copy.deepcopy(big_stats)\n            most_frequent = max(stats, key=lambda x: (stats[x], x))\n            # threshold is inspired by Zipfian assumption, but should only affect speed\n            threshold = stats[most_frequent] * i / (i + 10000.0)\n            prune_stats(stats, big_stats, threshold)\n\n        if stats[most_frequent] < min_frequency:\n            sys.stderr.write(\n                \'no pair has frequency >= {0}. Stopping\\n\'.format(min_frequency))\n            break\n\n        if verbose:\n            sys.stderr.write(\'pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n\'.format(\n                i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n        outfile.write(\'{0} {1}\\n\'.format(*most_frequent))\n        changes = replace_pair(most_frequent, sorted_vocab, indices)\n        update_pair_statistics(most_frequent, changes, stats, indices)\n        stats[most_frequent] = 0\n        if not i % 100:\n            prune_stats(stats, big_stats, threshold)\n\n\nif __name__ == \'__main__\':\n\n    # python 2/3 compatibility\n    if sys.version_info < (3, 0):\n        sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n        sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n        sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n    else:\n        sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr.buffer)\n        sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout.buffer)\n        sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin.buffer)\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    # read/write files as UTF-8\n    if args.input.name != \'<stdin>\':\n        args.input = codecs.open(args.input.name, encoding=\'utf-8\')\n    if args.output.name != \'<stdout>\':\n        args.output = codecs.open(args.output.name, \'w\', encoding=\'utf-8\')\n\n    main(args.input, args.output, args.symbols,\n         args.min_frequency, args.verbose, is_dict=args.dict_input)\n'"
tools/release_model.py,0,"b'#!/usr/bin/env python\nfrom onmt.bin.release_model import main\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/test_rouge.py,0,"b'# -*- encoding: utf-8 -*-\nimport argparse\nimport os\nimport time\nimport pyrouge\nimport shutil\nimport sys\nimport codecs\n\nfrom onmt.utils.logging import init_logger, logger\n\n\ndef test_rouge(cand, ref):\n    """"""Calculate ROUGE scores of sequences passed as an iterator\n       e.g. a list of str, an open file, StringIO or even sys.stdin\n    """"""\n    current_time = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime())\n    tmp_dir = "".rouge-tmp-{}"".format(current_time)\n    try:\n        if not os.path.isdir(tmp_dir):\n            os.mkdir(tmp_dir)\n            os.mkdir(tmp_dir + ""/candidate"")\n            os.mkdir(tmp_dir + ""/reference"")\n        candidates = [line.strip() for line in cand]\n        references = [line.strip() for line in ref]\n        assert len(candidates) == len(references)\n        cnt = len(candidates)\n        for i in range(cnt):\n            if len(references[i]) < 1:\n                continue\n            with open(tmp_dir + ""/candidate/cand.{}.txt"".format(i), ""w"",\n                      encoding=""utf-8"") as f:\n                f.write(candidates[i])\n            with open(tmp_dir + ""/reference/ref.{}.txt"".format(i), ""w"",\n                      encoding=""utf-8"") as f:\n                f.write(references[i])\n        r = pyrouge.Rouge155()\n        r.model_dir = tmp_dir + ""/reference/""\n        r.system_dir = tmp_dir + ""/candidate/""\n        r.model_filename_pattern = \'ref.#ID#.txt\'\n        r.system_filename_pattern = r\'cand.(\\d+).txt\'\n        rouge_results = r.convert_and_evaluate()\n        results_dict = r.output_to_dict(rouge_results)\n        return results_dict\n    finally:\n        pass\n        if os.path.isdir(tmp_dir):\n            shutil.rmtree(tmp_dir)\n\n\ndef rouge_results_to_str(results_dict):\n    return "">> ROUGE(1/2/3/L/SU4): {:.2f}/{:.2f}/{:.2f}/{:.2f}/{:.2f}"".format(\n        results_dict[""rouge_1_f_score""] * 100,\n        results_dict[""rouge_2_f_score""] * 100,\n        results_dict[""rouge_3_f_score""] * 100,\n        results_dict[""rouge_l_f_score""] * 100,\n        results_dict[""rouge_su*_f_score""] * 100)\n\n\nif __name__ == ""__main__"":\n    init_logger(\'test_rouge.log\')\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', type=str, default=""candidate.txt"",\n                        help=\'candidate file\')\n    parser.add_argument(\'-r\', type=str, default=""reference.txt"",\n                        help=\'reference file\')\n    args = parser.parse_args()\n    if args.c.upper() == ""STDIN"":\n        candidates = sys.stdin\n    else:\n        candidates = codecs.open(args.c, encoding=""utf-8"")\n    references = codecs.open(args.r, encoding=""utf-8"")\n\n    results_dict = test_rouge(candidates, references)\n    logger.info(rouge_results_to_str(results_dict))\n'"
tools/vid_feature_extractor.py,10,"b'import argparse\nimport os\n\nimport tqdm\nfrom multiprocessing import Manager\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nimport pretrainedmodels\nfrom pretrainedmodels.utils import TransformImage\n\n\nQ_FIN = ""finished""  # end-of-queue flag\n\n\ndef read_to_imgs(file):\n    """"""Yield images and their frame number from a video file.""""""\n    vidcap = cv2.VideoCapture(file)\n    success, image = vidcap.read()\n    idx = 0\n    while success:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        yield image, idx\n        idx += 1\n        success, image = vidcap.read()\n\n\ndef vid_len(path):\n    """"""Return the length of a video.""""""\n    return int(cv2.VideoCapture(path).get(cv2.CAP_PROP_FRAME_COUNT))\n\n\nclass VidDset(object):\n    """"""For each video, yield its frames.""""""\n    def __init__(self, model, root_dir, filenames):\n        self.root_dir = root_dir\n        self.filenames = filenames\n        self.paths = [os.path.join(self.root_dir, f) for f in self.filenames]\n        self.xform = TransformImage(model)\n\n        self.current = 0\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, i):\n        path = self.paths[i]\n        return ((path, idx, self.xform(Image.fromarray(img)))\n                for img, idx in read_to_imgs(path))\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current >= len(self):\n            raise StopIteration\n        else:\n            self.current += 1\n            return self[self.current - 1]\n\n    def __next__(self):\n        return self.next()\n\n\ndef collate_tensor(batch):\n    batch[-1] = torch.stack(batch[-1], 0)\n\n\ndef batch(dset, batch_size):\n    """"""Collate frames into batches of equal length.""""""\n    batch = [[], [], []]\n    batch_ct = 0\n    for seq in dset:\n        for path, idx, img in seq:\n            if batch_ct == batch_size:\n                collate_tensor(batch)\n                yield batch\n                batch = [[], [], []]\n                batch_ct = 0\n            batch[0].append(path)\n            batch[1].append(idx)\n            batch[2].append(img)\n            batch_ct += 1\n    if batch_ct != 0:\n        collate_tensor(batch)\n        yield batch\n\n\nclass FeatureExtractor(nn.Module):\n    """"""Extract feature vectors from a batch of frames.""""""\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        self.model = pretrainedmodels.resnet152()\n        self.FEAT_SIZE = 2048\n\n    def forward(self, x):\n        return self.model.avgpool(\n            self.model.features(x)).view(-1, 1, self.FEAT_SIZE)\n\n\nclass Reconstructor(object):\n    """"""Turn batches of feature vectors into sequences for each video.\n    Assumes data is ordered (use one reconstructor per process).\n    :func:`push()` batches in. When finished, :func:`flush()`\n    the last sequence.\n    """"""\n\n    def __init__(self, out_path, finished_queue):\n        self.out_path = out_path\n        self.feats = None\n        self.finished_queue = finished_queue\n\n    def save(self, path, feats):\n        np.save(path, feats.numpy())\n\n    @staticmethod\n    def name_(path, out_path):\n        vid_path = path\n        vid_fname = os.path.basename(vid_path)\n        vid_id = os.path.splitext(vid_fname)[0]\n\n        save_fname = vid_id + "".npy""\n        save_path = os.path.join(out_path, save_fname)\n        return save_path, vid_id\n\n    def name(self, path):\n        return self.name_(path, self.out_path)\n\n    def push(self, paths, idxs, feats):\n        start = 0\n        for i, idx in enumerate(idxs):\n            if idx == 0:\n                if self.feats is None and i == 0:\n                    # degenerate case\n                    continue\n                these_finished_seq_feats = feats[start:i]\n                if self.feats is not None:\n                    all_last_seq_feats = torch.cat(\n                        [self.feats, these_finished_seq_feats], 0)\n                else:\n                    all_last_seq_feats = these_finished_seq_feats\n                if i - 1 < 0:\n                    name = self.path\n                else:\n                    name = paths[i-1]\n                save_path, vid_id = self.name(name)\n                self.save(save_path, all_last_seq_feats)\n                n_feats = all_last_seq_feats.shape[0]\n                self.finished_queue.put((vid_id, n_feats))\n                self.feats = None\n                start = i\n        # cache the features\n        if self.feats is None:\n            self.feats = feats[start:]\n        else:\n            self.feats = torch.cat([self.feats, feats[start:]], 0)\n        self.path = paths[-1]\n\n    def flush(self):\n        if self.feats is not None:  # shouldn\'t be\n            save_path, vid_id = self.name(self.path)\n            self.save(save_path, self.feats)\n            self.finished_queue.put((vid_id, self.feats.shape[0]))\n\n\ndef finished_watcher(finished_queue, world_size, root_dir, files):\n    """"""Keep a progress bar of frames finished.""""""\n    n_frames = sum(vid_len(os.path.join(root_dir, f)) for f in files)\n    n_finished_frames = 0\n    with tqdm.tqdm(total=n_frames, unit=""Fr"") as pbar:\n        n_proc_finished = 0\n        while True:\n            item = finished_queue.get()\n            if item == Q_FIN:\n                n_proc_finished += 1\n                if n_proc_finished == world_size:\n                    return\n            else:\n                vid_id, n_these_frames = item\n                n_finished_frames += n_these_frames\n                pbar.set_postfix(vid=vid_id)\n                pbar.update(n_these_frames)\n\n\ndef run(device_id, world_size, root_dir, batch_size_per_device,\n        feats_queue, files):\n    """"""Process a disjoint subset of the videos on each device.""""""\n    if world_size > 1:\n        these_files = [f for i, f in enumerate(files)\n                       if i % world_size == device_id]\n    else:\n        these_files = files\n\n    fe = FeatureExtractor()\n    dset = VidDset(fe.model, root_dir, these_files)\n    dev = torch.device(""cuda"", device_id) \\\n        if device_id >= 0 else torch.device(""cpu"")\n    fe.to(dev)\n    fe = fe.eval()\n    with torch.no_grad():\n        for samp in batch(dset, batch_size_per_device):\n            paths, idxs, images = samp\n            images = images.to(dev)\n            feats = fe(images)\n            if torch.is_tensor(feats):\n                feats = feats.to(""cpu"")\n            else:\n                feats = [f.to(""cpu"") for f in feats]\n            feats_queue.put((paths, idxs, feats))\n    feats_queue.put(Q_FIN)\n    return\n\n\ndef saver(out_path, feats_queue, finished_queue):\n    rc = Reconstructor(out_path, finished_queue)\n    while True:\n        item = feats_queue.get()\n        if item == Q_FIN:\n            rc.flush()\n            finished_queue.put(Q_FIN)\n            return\n        else:\n            paths, idxs, feats = item\n            rc.push(paths, idxs, feats)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--root_dir"", type=str, required=True,\n                        help=""Directory of videos."")\n    parser.add_argument(""--out_dir"", type=str, required=True,\n                        help=""Directory for output features."")\n    parser.add_argument(""--world_size"", type=int, default=1,\n                        help=""Number of devices to run on."")\n    parser.add_argument(""--batch_size_per_device"", type=int, default=512)\n    opt = parser.parse_args()\n\n    batch_size_per_device = opt.batch_size_per_device\n    root_dir = opt.root_dir\n    out_path = opt.out_dir\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n\n    # mp queues don\'t work well between procs unless they\'re from a manager\n    manager = Manager()\n    finished_queue = manager.Queue()\n\n    world_size = opt.world_size if torch.cuda.is_available() else -1\n\n    mp = torch.multiprocessing.get_context(""spawn"")\n    procs = []\n\n    print(""Starting processing. Progress bar startup can take some time, but ""\n          ""processing will start in the meantime."")\n\n    files = list(sorted(list(os.listdir(root_dir))))\n    files = [f for f in files\n             if os.path.basename(Reconstructor.name_(f, out_path)[0])\n             not in os.listdir(out_path)]\n\n    procs.append(mp.Process(\n        target=finished_watcher,\n        args=(finished_queue, world_size, root_dir, files),\n        daemon=False\n    ))\n    procs[0].start()\n\n    if world_size >= 1:\n        feat_queues = [manager.Queue(2) for _ in range(world_size)]\n        for feats_queue, device_id in zip(feat_queues, range(world_size)):\n            # each device has its own saver so that reconstructing is easier\n            procs.append(mp.Process(\n                target=run,\n                args=(device_id, world_size, root_dir,\n                      batch_size_per_device, feats_queue, files),\n                daemon=True))\n            procs[-1].start()\n            procs.append(mp.Process(\n                target=saver,\n                args=(out_path, feats_queue, finished_queue),\n                daemon=True))\n            procs[-1].start()\n    else:\n        feats_queue = manager.Queue()\n        procs.append(mp.Process(\n            target=run,\n            args=(-1, 1, root_dir,\n                  batch_size_per_device, feats_queue, files),\n            daemon=True))\n        procs[-1].start()\n        procs.append(mp.Process(\n            target=saver,\n            args=(out_path, feats_queue, finished_queue),\n            daemon=True))\n        procs[-1].start()\n\n    for p in procs:\n        p.join()\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# OpenNMT-py documentation build configuration file, created by\n# sphinx-quickstart on Sun Dec 17 12:07:14 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nfrom recommonmark.parser import CommonMarkParser\nimport sphinx_rtd_theme\nfrom recommonmark.transform import AutoStructify\n\nsource_parsers = {\n    \'.md\': CommonMarkParser,\n}\n\nsource_suffix = [\'.rst\', \'.md\']\nextensions = [\'sphinx.ext.autodoc\',\n              \'sphinx.ext.mathjax\',\n              \'sphinx.ext.viewcode\',\n              \'sphinx.ext.coverage\',\n              \'sphinx.ext.githubpages\',\n              \'sphinx.ext.napoleon\',\n              \'sphinxcontrib.mermaid\',\n              \'sphinxcontrib.bibtex\',\n              \'sphinxarg.ext\',\n              \'sphinx_markdown_tables\']\n\n# Show base classes\nautodoc_default_options = {\n    \'show-inheritance\': True\n}\n\n# Use ""variables"" section for Attributes instead of weird block things\n# mimicking the function style.\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'.templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'OpenNMT-py\'\ncopyright = \'2017, srush\'\nauthor = \'srush\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n\n\n# html_theme = \'sphinx_materialdesign_theme\'\n# html_theme_path = [sphinx_materialdesign_theme.get_path()]\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_context = {\n    \'css_files\': [\n        \'_static/theme_overrides.css\',  # override wide tables in RTD theme\n        ],\n     }\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'OpenNMT-pydoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'OpenNMT-py.tex\', \'OpenNMT-py Documentation\',\n     \'srush\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'opennmt-py\', \'OpenNMT-py Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'OpenNMT-py\', \'OpenNMT-py Documentation\',\n     author, \'OpenNMT-py\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\ngithub_doc_root = \'https://github.com/opennmt/opennmt-py/tree/master/doc/\'\n\n\ndef setup(app):\n    print(""hello"")\n    app.add_config_value(\'recommonmark_config\', {\n        \'enable_eval_rst\': True\n    }, True)\n    app.add_transform(AutoStructify)\n'"
onmt/bin/__init__.py,0,b''
onmt/bin/average_models.py,2,"b'#!/usr/bin/env python\nimport argparse\nimport torch\n\n\ndef average_models(model_files, fp32=False):\n    vocab = None\n    opt = None\n    avg_model = None\n    avg_generator = None\n\n    for i, model_file in enumerate(model_files):\n        m = torch.load(model_file, map_location=\'cpu\')\n        model_weights = m[\'model\']\n        generator_weights = m[\'generator\']\n\n        if fp32:\n            for k, v in model_weights.items():\n                model_weights[k] = v.float()\n            for k, v in generator_weights.items():\n                generator_weights[k] = v.float()\n\n        if i == 0:\n            vocab, opt = m[\'vocab\'], m[\'opt\']\n            avg_model = model_weights\n            avg_generator = generator_weights\n        else:\n            for (k, v) in avg_model.items():\n                avg_model[k].mul_(i).add_(model_weights[k]).div_(i + 1)\n\n            for (k, v) in avg_generator.items():\n                avg_generator[k].mul_(i).add_(generator_weights[k]).div_(i + 1)\n\n    final = {""vocab"": vocab, ""opt"": opt, ""optim"": None,\n             ""generator"": avg_generator, ""model"": avg_model}\n    return final\n\n\ndef main():\n    parser = argparse.ArgumentParser(description="""")\n    parser.add_argument(""-models"", ""-m"", nargs=""+"", required=True,\n                        help=""List of models"")\n    parser.add_argument(""-output"", ""-o"", required=True,\n                        help=""Output file"")\n    parser.add_argument(""-fp32"", ""-f"", action=""store_true"",\n                        help=""Cast params to float32"")\n    opt = parser.parse_args()\n\n    final = average_models(opt.models, opt.fp32)\n    torch.save(final, opt.output)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onmt/bin/preprocess.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\n    Pre-process Data / features files and build vocabulary\n""""""\nimport codecs\nimport glob\nimport gc\nimport torch\nfrom collections import Counter, defaultdict\n\nfrom onmt.utils.logging import init_logger, logger\nfrom onmt.utils.misc import split_corpus\nimport onmt.inputters as inputters\nimport onmt.opts as opts\nfrom onmt.utils.parse import ArgumentParser\nfrom onmt.inputters.inputter import _build_fields_vocab,\\\n                                    _load_vocab, \\\n                                    old_style_vocab, \\\n                                    load_old_vocab\n\nfrom functools import partial\nfrom multiprocessing import Pool\n\n\ndef check_existing_pt_files(opt, corpus_type, ids, existing_fields):\n    """""" Check if there are existing .pt files to avoid overwriting them """"""\n    existing_shards = []\n    for maybe_id in ids:\n        if maybe_id:\n            shard_base = corpus_type + ""_"" + maybe_id\n        else:\n            shard_base = corpus_type\n        pattern = opt.save_data + \'.{}.*.pt\'.format(shard_base)\n        if glob.glob(pattern):\n            if opt.overwrite:\n                maybe_overwrite = (""will be overwritten because ""\n                                   ""`-overwrite` option is set."")\n            else:\n                maybe_overwrite = (""won\'t be overwritten, pass the ""\n                                   ""`-overwrite` option if you want to."")\n            logger.warning(""Shards for corpus {} already exist, {}""\n                           .format(shard_base, maybe_overwrite))\n            existing_shards += [maybe_id]\n    return existing_shards\n\n\ndef process_one_shard(corpus_params, params):\n    corpus_type, fields, src_reader, tgt_reader, align_reader, opt,\\\n         existing_fields, src_vocab, tgt_vocab = corpus_params\n    i, (src_shard, tgt_shard, align_shard, maybe_id, filter_pred) = params\n    # create one counter per shard\n    sub_sub_counter = defaultdict(Counter)\n    assert len(src_shard) == len(tgt_shard)\n    logger.info(""Building shard %d."" % i)\n\n    src_data = {""reader"": src_reader, ""data"": src_shard, ""dir"": opt.src_dir}\n    tgt_data = {""reader"": tgt_reader, ""data"": tgt_shard, ""dir"": None}\n    align_data = {""reader"": align_reader, ""data"": align_shard, ""dir"": None}\n    _readers, _data, _dir = inputters.Dataset.config(\n        [(\'src\', src_data), (\'tgt\', tgt_data), (\'align\', align_data)])\n\n    dataset = inputters.Dataset(\n        fields, readers=_readers, data=_data, dirs=_dir,\n        sort_key=inputters.str2sortkey[opt.data_type],\n        filter_pred=filter_pred,\n        corpus_id=maybe_id\n    )\n    if corpus_type == ""train"" and existing_fields is None:\n        for ex in dataset.examples:\n            sub_sub_counter[\'corpus_id\'].update(\n                [""train"" if maybe_id is None else maybe_id])\n            for name, field in fields.items():\n                if ((opt.data_type == ""audio"") and (name == ""src"")):\n                    continue\n                try:\n                    f_iter = iter(field)\n                except TypeError:\n                    f_iter = [(name, field)]\n                    all_data = [getattr(ex, name, None)]\n                else:\n                    all_data = getattr(ex, name)\n                for (sub_n, sub_f), fd in zip(\n                        f_iter, all_data):\n                    has_vocab = (sub_n == \'src\' and\n                                 src_vocab is not None) or \\\n                                (sub_n == \'tgt\' and\n                                 tgt_vocab is not None)\n                    if (hasattr(sub_f, \'sequential\')\n                            and sub_f.sequential and not has_vocab):\n                        val = fd\n                        sub_sub_counter[sub_n].update(val)\n    if maybe_id:\n        shard_base = corpus_type + ""_"" + maybe_id\n    else:\n        shard_base = corpus_type\n    data_path = ""{:s}.{:s}.{:d}.pt"".\\\n        format(opt.save_data, shard_base, i)\n\n    logger.info("" * saving %sth %s data shard to %s.""\n                % (i, shard_base, data_path))\n\n    dataset.save(data_path)\n\n    del dataset.examples\n    gc.collect()\n    del dataset\n    gc.collect()\n\n    return sub_sub_counter\n\n\ndef maybe_load_vocab(corpus_type, counters, opt):\n    src_vocab = None\n    tgt_vocab = None\n    existing_fields = None\n    if corpus_type == ""train"":\n        if opt.src_vocab != """":\n            try:\n                logger.info(""Using existing vocabulary..."")\n                existing_fields = torch.load(opt.src_vocab)\n            except torch.serialization.pickle.UnpicklingError:\n                logger.info(""Building vocab from text file..."")\n                src_vocab, src_vocab_size = _load_vocab(\n                    opt.src_vocab, ""src"", counters,\n                    opt.src_words_min_frequency)\n        if opt.tgt_vocab != """":\n            tgt_vocab, tgt_vocab_size = _load_vocab(\n                opt.tgt_vocab, ""tgt"", counters,\n                opt.tgt_words_min_frequency)\n    return src_vocab, tgt_vocab, existing_fields\n\n\ndef build_save_dataset(corpus_type, fields, src_reader, tgt_reader,\n                       align_reader, opt):\n    assert corpus_type in [\'train\', \'valid\']\n\n    if corpus_type == \'train\':\n        counters = defaultdict(Counter)\n        srcs = opt.train_src\n        tgts = opt.train_tgt\n        ids = opt.train_ids\n        aligns = opt.train_align\n    elif corpus_type == \'valid\':\n        counters = None\n        srcs = [opt.valid_src]\n        tgts = [opt.valid_tgt]\n        ids = [None]\n        aligns = [opt.valid_align]\n\n    src_vocab, tgt_vocab, existing_fields = maybe_load_vocab(\n        corpus_type, counters, opt)\n\n    existing_shards = check_existing_pt_files(\n        opt, corpus_type, ids, existing_fields)\n\n    # every corpus has shards, no new one\n    if existing_shards == ids and not opt.overwrite:\n        return\n\n    def shard_iterator(srcs, tgts, ids, aligns, existing_shards,\n                       existing_fields, corpus_type, opt):\n        """"""\n        Builds a single iterator yielding every shard of every corpus.\n        """"""\n        for src, tgt, maybe_id, maybe_align in zip(srcs, tgts, ids, aligns):\n            if maybe_id in existing_shards:\n                if opt.overwrite:\n                    logger.warning(""Overwrite shards for corpus {}""\n                                   .format(maybe_id))\n                else:\n                    if corpus_type == ""train"":\n                        assert existing_fields is not None,\\\n                            (""A \'vocab.pt\' file should be passed to ""\n                             ""`-src_vocab` when adding a corpus to ""\n                             ""a set of already existing shards."")\n                    logger.warning(""Ignore corpus {} because ""\n                                   ""shards already exist""\n                                   .format(maybe_id))\n                    continue\n            if ((corpus_type == ""train"" or opt.filter_valid)\n                    and tgt is not None):\n                filter_pred = partial(\n                    inputters.filter_example,\n                    use_src_len=opt.data_type == ""text"",\n                    max_src_len=opt.src_seq_length,\n                    max_tgt_len=opt.tgt_seq_length)\n            else:\n                filter_pred = None\n            src_shards = split_corpus(src, opt.shard_size)\n            tgt_shards = split_corpus(tgt, opt.shard_size)\n            align_shards = split_corpus(maybe_align, opt.shard_size)\n            for i, (ss, ts, a_s) in enumerate(\n                    zip(src_shards, tgt_shards, align_shards)):\n                yield (i, (ss, ts, a_s, maybe_id, filter_pred))\n\n    shard_iter = shard_iterator(srcs, tgts, ids, aligns, existing_shards,\n                                existing_fields, corpus_type, opt)\n\n    with Pool(opt.num_threads) as p:\n        dataset_params = (corpus_type, fields, src_reader, tgt_reader,\n                          align_reader, opt, existing_fields,\n                          src_vocab, tgt_vocab)\n        func = partial(process_one_shard, dataset_params)\n        for sub_counter in p.imap(func, shard_iter):\n            if sub_counter is not None:\n                for key, value in sub_counter.items():\n                    counters[key].update(value)\n\n    if corpus_type == ""train"":\n        vocab_path = opt.save_data + \'.vocab.pt\'\n        new_fields = _build_fields_vocab(\n            fields, counters, opt.data_type,\n            opt.share_vocab, opt.vocab_size_multiple,\n            opt.src_vocab_size, opt.src_words_min_frequency,\n            opt.tgt_vocab_size, opt.tgt_words_min_frequency,\n            subword_prefix=opt.subword_prefix,\n            subword_prefix_is_joiner=opt.subword_prefix_is_joiner)\n        if existing_fields is None:\n            fields = new_fields\n        else:\n            fields = existing_fields\n\n        if old_style_vocab(fields):\n            fields = load_old_vocab(\n                fields, opt.data_type, dynamic_dict=opt.dynamic_dict)\n\n        # patch corpus_id\n        if fields.get(""corpus_id"", False):\n            fields[""corpus_id""].vocab = new_fields[""corpus_id""].vocab_cls(\n                counters[""corpus_id""])\n\n        torch.save(fields, vocab_path)\n\n\ndef build_save_vocab(train_dataset, fields, opt):\n    fields = inputters.build_vocab(\n        train_dataset, fields, opt.data_type, opt.share_vocab,\n        opt.src_vocab, opt.src_vocab_size, opt.src_words_min_frequency,\n        opt.tgt_vocab, opt.tgt_vocab_size, opt.tgt_words_min_frequency,\n        vocab_size_multiple=opt.vocab_size_multiple\n    )\n    vocab_path = opt.save_data + \'.vocab.pt\'\n    torch.save(fields, vocab_path)\n\n\ndef count_features(path):\n    """"""\n    path: location of a corpus file with whitespace-delimited tokens and\n                    \xef\xbf\xa8-delimited features within the token\n    returns: the number of features in the dataset\n    """"""\n    with codecs.open(path, ""r"", ""utf-8"") as f:\n        first_tok = f.readline().split(None, 1)[0]\n        return len(first_tok.split(u""\xef\xbf\xa8"")) - 1\n\n\ndef preprocess(opt):\n    ArgumentParser.validate_preprocess_args(opt)\n    torch.manual_seed(opt.seed)\n\n    init_logger(opt.log_file)\n\n    logger.info(""Extracting features..."")\n\n    src_nfeats = 0\n    tgt_nfeats = 0\n    src_nfeats = count_features(opt.train_src[0]) if opt.data_type == \'text\' \\\n        else 0\n    tgt_nfeats = count_features(opt.train_tgt[0])  # tgt always text so far\n    if len(opt.train_src) > 1 and opt.data_type == \'text\':\n        for src, tgt in zip(opt.train_src[1:], opt.train_tgt[1:]):\n            assert src_nfeats == count_features(src),\\\n                ""%s seems to mismatch features of ""\\\n                ""the other source datasets"" % src\n            assert tgt_nfeats == count_features(tgt),\\\n                ""%s seems to mismatch features of ""\\\n                ""the other target datasets"" % tgt\n    logger.info("" * number of source features: %d."" % src_nfeats)\n    logger.info("" * number of target features: %d."" % tgt_nfeats)\n\n    logger.info(""Building `Fields` object..."")\n    fields = inputters.get_fields(\n        opt.data_type,\n        src_nfeats,\n        tgt_nfeats,\n        dynamic_dict=opt.dynamic_dict,\n        with_align=opt.train_align[0] is not None,\n        src_truncate=opt.src_seq_length_trunc,\n        tgt_truncate=opt.tgt_seq_length_trunc)\n\n    src_reader = inputters.str2reader[opt.data_type].from_opt(opt)\n    tgt_reader = inputters.str2reader[""text""].from_opt(opt)\n    align_reader = inputters.str2reader[""text""].from_opt(opt)\n\n    logger.info(""Building & saving training data..."")\n    build_save_dataset(\n        \'train\', fields, src_reader, tgt_reader, align_reader, opt)\n\n    if opt.valid_src and opt.valid_tgt:\n        logger.info(""Building & saving validation data..."")\n        build_save_dataset(\n            \'valid\', fields, src_reader, tgt_reader, align_reader, opt)\n\n\ndef _get_parser():\n    parser = ArgumentParser(description=\'preprocess.py\')\n\n    opts.config_opts(parser)\n    opts.preprocess_opts(parser)\n    return parser\n\n\ndef main():\n    parser = _get_parser()\n\n    opt = parser.parse_args()\n    preprocess(opt)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onmt/bin/release_model.py,2,"b'#!/usr/bin/env python\nimport argparse\nimport torch\n\n\ndef get_ctranslate2_model_spec(opt):\n    """"""Creates a CTranslate2 model specification from the model options.""""""\n    with_relative_position = getattr(opt, ""max_relative_positions"", 0) > 0\n    is_ct2_compatible = (\n        opt.encoder_type == ""transformer""\n        and opt.decoder_type == ""transformer""\n        and getattr(opt, ""self_attn_type"", ""scaled-dot"") == ""scaled-dot""\n        and ((opt.position_encoding and not with_relative_position)\n             or (with_relative_position and not opt.position_encoding)))\n    if not is_ct2_compatible:\n        return None\n    import ctranslate2\n    num_heads = getattr(opt, ""heads"", 8)\n    return ctranslate2.specs.TransformerSpec(\n        (opt.enc_layers, opt.dec_layers),\n        num_heads,\n        with_relative_position=with_relative_position)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Release an OpenNMT-py model for inference"")\n    parser.add_argument(""--model"", ""-m"",\n                        help=""The model path"", required=True)\n    parser.add_argument(""--output"", ""-o"",\n                        help=""The output path"", required=True)\n    parser.add_argument(""--format"",\n                        choices=[""pytorch"", ""ctranslate2""],\n                        default=""pytorch"",\n                        help=""The format of the released model"")\n    parser.add_argument(""--quantization"", ""-q"",\n                        choices=[""int8"", ""int16""],\n                        default=None,\n                        help=""Quantization type for CT2 model."")\n    opt = parser.parse_args()\n\n    model = torch.load(opt.model)\n    if opt.format == ""pytorch"":\n        model[""optim""] = None\n        torch.save(model, opt.output)\n    elif opt.format == ""ctranslate2"":\n        model_spec = get_ctranslate2_model_spec(model[""opt""])\n        if model_spec is None:\n            raise ValueError(""This model is not supported by CTranslate2. Go ""\n                             ""to https://github.com/OpenNMT/CTranslate2 for ""\n                             ""more information on supported models."")\n        import ctranslate2\n        converter = ctranslate2.converters.OpenNMTPyConverter(opt.model)\n        converter.convert(opt.output, model_spec, force=True,\n                          quantization=opt.quantization)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onmt/bin/server.py,0,"b'#!/usr/bin/env python\nimport configargparse\n\nfrom flask import Flask, jsonify, request\nfrom waitress import serve\nfrom onmt.translate import TranslationServer, ServerModelError\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\nSTATUS_OK = ""ok""\nSTATUS_ERROR = ""error""\n\n\ndef start(config_file,\n          url_root=""./translator"",\n          host=""0.0.0.0"",\n          port=5000,\n          debug=False):\n    def prefix_route(route_function, prefix=\'\', mask=\'{0}{1}\'):\n        def newroute(route, *args, **kwargs):\n            return route_function(mask.format(prefix, route), *args, **kwargs)\n        return newroute\n\n    if debug:\n        logger = logging.getLogger(""main"")\n        log_format = logging.Formatter(\n            ""[%(asctime)s %(levelname)s] %(message)s"")\n        file_handler = RotatingFileHandler(\n            ""debug_requests.log"",\n            maxBytes=1000000, backupCount=10)\n        file_handler.setFormatter(log_format)\n        logger.addHandler(file_handler)\n\n    app = Flask(__name__)\n    app.route = prefix_route(app.route, url_root)\n    translation_server = TranslationServer()\n    translation_server.start(config_file)\n\n    @app.route(\'/models\', methods=[\'GET\'])\n    def get_models():\n        out = translation_server.list_models()\n        return jsonify(out)\n\n    @app.route(\'/health\', methods=[\'GET\'])\n    def health():\n        out = {}\n        out[\'status\'] = STATUS_OK\n        return jsonify(out)\n\n    @app.route(\'/clone_model/<int:model_id>\', methods=[\'POST\'])\n    def clone_model(model_id):\n        out = {}\n        data = request.get_json(force=True)\n        timeout = -1\n        if \'timeout\' in data:\n            timeout = data[\'timeout\']\n            del data[\'timeout\']\n\n        opt = data.get(\'opt\', None)\n        try:\n            model_id, load_time = translation_server.clone_model(\n                model_id, opt, timeout)\n        except ServerModelError as e:\n            out[\'status\'] = STATUS_ERROR\n            out[\'error\'] = str(e)\n        else:\n            out[\'status\'] = STATUS_OK\n            out[\'model_id\'] = model_id\n            out[\'load_time\'] = load_time\n\n        return jsonify(out)\n\n    @app.route(\'/unload_model/<int:model_id>\', methods=[\'GET\'])\n    def unload_model(model_id):\n        out = {""model_id"": model_id}\n\n        try:\n            translation_server.unload_model(model_id)\n            out[\'status\'] = STATUS_OK\n        except Exception as e:\n            out[\'status\'] = STATUS_ERROR\n            out[\'error\'] = str(e)\n\n        return jsonify(out)\n\n    @app.route(\'/translate\', methods=[\'POST\'])\n    def translate():\n        inputs = request.get_json(force=True)\n        if debug:\n            logger.info(inputs)\n        out = {}\n        try:\n            trans, scores, n_best, _, aligns = translation_server.run(inputs)\n            assert len(trans) == len(inputs) * n_best\n            assert len(scores) == len(inputs) * n_best\n            assert len(aligns) == len(inputs) * n_best\n\n            out = [[] for _ in range(n_best)]\n            for i in range(len(trans)):\n                response = {""src"": inputs[i // n_best][\'src\'], ""tgt"": trans[i],\n                            ""n_best"": n_best, ""pred_score"": scores[i]}\n                if len(aligns[i]) > 0 and aligns[i][0] is not None:\n                    response[""align""] = aligns[i]\n                out[i % n_best].append(response)\n        except ServerModelError as e:\n            model_id = inputs[0].get(""id"")\n            if debug:\n                logger.warning(""Unload model #{} ""\n                               ""because of an error"".format(model_id))\n            translation_server.models[model_id].unload()\n            out[\'error\'] = str(e)\n            out[\'status\'] = STATUS_ERROR\n        if debug:\n            logger.info(out)\n        return jsonify(out)\n\n    @app.route(\'/to_cpu/<int:model_id>\', methods=[\'GET\'])\n    def to_cpu(model_id):\n        out = {\'model_id\': model_id}\n        translation_server.models[model_id].to_cpu()\n\n        out[\'status\'] = STATUS_OK\n        return jsonify(out)\n\n    @app.route(\'/to_gpu/<int:model_id>\', methods=[\'GET\'])\n    def to_gpu(model_id):\n        out = {\'model_id\': model_id}\n        translation_server.models[model_id].to_gpu()\n\n        out[\'status\'] = STATUS_OK\n        return jsonify(out)\n\n    serve(app, host=host, port=port)\n\n\ndef _get_parser():\n    parser = configargparse.ArgumentParser(\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        description=""OpenNMT-py REST Server"")\n    parser.add_argument(""--ip"", type=str, default=""0.0.0.0"")\n    parser.add_argument(""--port"", type=int, default=""5000"")\n    parser.add_argument(""--url_root"", type=str, default=""/translator"")\n    parser.add_argument(""--debug"", ""-d"", action=""store_true"")\n    parser.add_argument(""--config"", ""-c"", type=str,\n                        default=""./available_models/conf.json"")\n    return parser\n\n\ndef main():\n    parser = _get_parser()\n    args = parser.parse_args()\n    start(args.config, url_root=args.url_root, host=args.ip, port=args.port,\n          debug=args.debug)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onmt/bin/train.py,11,"b'#!/usr/bin/env python\n""""""Train models.""""""\nimport os\nimport signal\nimport torch\n\nimport onmt.opts as opts\nimport onmt.utils.distributed\n\nfrom onmt.utils.misc import set_random_seed\nfrom onmt.utils.logging import init_logger, logger\nfrom onmt.train_single import main as single_main\nfrom onmt.utils.parse import ArgumentParser\nfrom onmt.inputters.inputter import build_dataset_iter, patch_fields, \\\n    load_old_vocab, old_style_vocab, build_dataset_iter_multiple\n\nfrom itertools import cycle\n\n\ndef train(opt):\n    ArgumentParser.validate_train_opts(opt)\n    ArgumentParser.update_model_opts(opt)\n    ArgumentParser.validate_model_opts(opt)\n\n    set_random_seed(opt.seed, False)\n\n    # Load checkpoint if we resume from a previous training.\n    if opt.train_from:\n        logger.info(\'Loading checkpoint from %s\' % opt.train_from)\n        checkpoint = torch.load(opt.train_from,\n                                map_location=lambda storage, loc: storage)\n        logger.info(\'Loading vocab from checkpoint at %s.\' % opt.train_from)\n        vocab = checkpoint[\'vocab\']\n    else:\n        vocab = torch.load(opt.data + \'.vocab.pt\')\n\n    # check for code where vocab is saved instead of fields\n    # (in the future this will be done in a smarter way)\n    if old_style_vocab(vocab):\n        fields = load_old_vocab(\n            vocab, opt.model_type, dynamic_dict=opt.copy_attn)\n    else:\n        fields = vocab\n\n    # patch for fields that may be missing in old data/model\n    patch_fields(opt, fields)\n\n    if len(opt.data_ids) > 1:\n        train_shards = []\n        for train_id in opt.data_ids:\n            shard_base = ""train_"" + train_id\n            train_shards.append(shard_base)\n        train_iter = build_dataset_iter_multiple(train_shards, fields, opt)\n    else:\n        if opt.data_ids[0] is not None:\n            shard_base = ""train_"" + opt.data_ids[0]\n        else:\n            shard_base = ""train""\n        train_iter = build_dataset_iter(shard_base, fields, opt)\n\n    nb_gpu = len(opt.gpu_ranks)\n\n    if opt.world_size > 1:\n        queues = []\n        mp = torch.multiprocessing.get_context(\'spawn\')\n        semaphore = mp.Semaphore(opt.world_size * opt.queue_size)\n        # Create a thread to listen for errors in the child processes.\n        error_queue = mp.SimpleQueue()\n        error_handler = ErrorHandler(error_queue)\n        # Train with multiprocessing.\n        procs = []\n        for device_id in range(nb_gpu):\n            q = mp.Queue(opt.queue_size)\n            queues += [q]\n            procs.append(mp.Process(target=run, args=(\n                opt, device_id, error_queue, q, semaphore), daemon=True))\n            procs[device_id].start()\n            logger.info("" Starting process pid: %d  "" % procs[device_id].pid)\n            error_handler.add_child(procs[device_id].pid)\n        producer = mp.Process(target=batch_producer,\n                              args=(train_iter, queues, semaphore, opt,),\n                              daemon=True)\n        producer.start()\n        error_handler.add_child(producer.pid)\n\n        for p in procs:\n            p.join()\n        producer.terminate()\n\n    elif nb_gpu == 1:  # case 1 GPU only\n        single_main(opt, 0)\n    else:   # case only CPU\n        single_main(opt, -1)\n\n\ndef batch_producer(generator_to_serve, queues, semaphore, opt):\n    init_logger(opt.log_file)\n    set_random_seed(opt.seed, False)\n    # generator_to_serve = iter(generator_to_serve)\n\n    def pred(x):\n        """"""\n        Filters batches that belong only\n        to gpu_ranks of current node\n        """"""\n        for rank in opt.gpu_ranks:\n            if x[0] % opt.world_size == rank:\n                return True\n\n    generator_to_serve = filter(\n        pred, enumerate(generator_to_serve))\n\n    def next_batch(device_id):\n        new_batch = next(generator_to_serve)\n        semaphore.acquire()\n        return new_batch[1]\n\n    b = next_batch(0)\n\n    for device_id, q in cycle(enumerate(queues)):\n        b.dataset = None\n        if isinstance(b.src, tuple):\n            b.src = tuple([_.to(torch.device(device_id))\n                           for _ in b.src])\n        else:\n            b.src = b.src.to(torch.device(device_id))\n        b.tgt = b.tgt.to(torch.device(device_id))\n        b.indices = b.indices.to(torch.device(device_id))\n        b.alignment = b.alignment.to(torch.device(device_id)) \\\n            if hasattr(b, \'alignment\') else None\n        b.src_map = b.src_map.to(torch.device(device_id)) \\\n            if hasattr(b, \'src_map\') else None\n        b.align = b.align.to(torch.device(device_id)) \\\n            if hasattr(b, \'align\') else None\n        b.corpus_id = b.corpus_id.to(torch.device(device_id)) \\\n            if hasattr(b, \'corpus_id\') else None\n\n        # hack to dodge unpicklable `dict_keys`\n        b.fields = list(b.fields)\n        q.put(b)\n        b = next_batch(device_id)\n\n\ndef run(opt, device_id, error_queue, batch_queue, semaphore):\n    """""" run process """"""\n    try:\n        gpu_rank = onmt.utils.distributed.multi_init(opt, device_id)\n        if gpu_rank != opt.gpu_ranks[device_id]:\n            raise AssertionError(""An error occurred in \\\n                  Distributed initialization"")\n        single_main(opt, device_id, batch_queue, semaphore)\n    except KeyboardInterrupt:\n        pass  # killed by parent, do nothing\n    except Exception:\n        # propagate exception to parent process, keeping original traceback\n        import traceback\n        error_queue.put((opt.gpu_ranks[device_id], traceback.format_exc()))\n\n\nclass ErrorHandler(object):\n    """"""A class that listens for exceptions in children processes and propagates\n    the tracebacks to the parent process.""""""\n\n    def __init__(self, error_queue):\n        """""" init error handler """"""\n        import signal\n        import threading\n        self.error_queue = error_queue\n        self.children_pids = []\n        self.error_thread = threading.Thread(\n            target=self.error_listener, daemon=True)\n        self.error_thread.start()\n        signal.signal(signal.SIGUSR1, self.signal_handler)\n\n    def add_child(self, pid):\n        """""" error handler """"""\n        self.children_pids.append(pid)\n\n    def error_listener(self):\n        """""" error listener """"""\n        (rank, original_trace) = self.error_queue.get()\n        self.error_queue.put((rank, original_trace))\n        os.kill(os.getpid(), signal.SIGUSR1)\n\n    def signal_handler(self, signalnum, stackframe):\n        """""" signal handler """"""\n        for pid in self.children_pids:\n            os.kill(pid, signal.SIGINT)  # kill children processes\n        (rank, original_trace) = self.error_queue.get()\n        msg = """"""\\n\\n-- Tracebacks above this line can probably\n                 be ignored --\\n\\n""""""\n        msg += original_trace\n        raise Exception(msg)\n\n\ndef _get_parser():\n    parser = ArgumentParser(description=\'train.py\')\n\n    opts.config_opts(parser)\n    opts.model_opts(parser)\n    opts.train_opts(parser)\n    return parser\n\n\ndef main():\n    parser = _get_parser()\n\n    opt = parser.parse_args()\n    train(opt)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onmt/bin/translate.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import unicode_literals\n\nfrom onmt.utils.logging import init_logger\nfrom onmt.utils.misc import split_corpus\nfrom onmt.translate.translator import build_translator\n\nimport onmt.opts as opts\nfrom onmt.utils.parse import ArgumentParser\n\n\ndef translate(opt):\n    ArgumentParser.validate_translate_opts(opt)\n    logger = init_logger(opt.log_file)\n\n    translator = build_translator(opt, logger=logger, report_score=True)\n    src_shards = split_corpus(opt.src, opt.shard_size)\n    tgt_shards = split_corpus(opt.tgt, opt.shard_size)\n    shard_pairs = zip(src_shards, tgt_shards)\n\n    for i, (src_shard, tgt_shard) in enumerate(shard_pairs):\n        logger.info(""Translating shard %d."" % i)\n        translator.translate(\n            src=src_shard,\n            tgt=tgt_shard,\n            src_dir=opt.src_dir,\n            batch_size=opt.batch_size,\n            batch_type=opt.batch_type,\n            attn_debug=opt.attn_debug,\n            align_debug=opt.align_debug\n            )\n\n\ndef _get_parser():\n    parser = ArgumentParser(description=\'translate.py\')\n\n    opts.config_opts(parser)\n    opts.translate_opts(parser)\n    return parser\n\n\ndef main():\n    parser = _get_parser()\n\n    opt = parser.parse_args()\n    translate(opt)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onmt/decoders/__init__.py,0,"b'""""""Module defining decoders.""""""\nfrom onmt.decoders.decoder import DecoderBase, InputFeedRNNDecoder, \\\n    StdRNNDecoder\nfrom onmt.decoders.transformer import TransformerDecoder\nfrom onmt.decoders.cnn_decoder import CNNDecoder\n\n\nstr2dec = {""rnn"": StdRNNDecoder, ""ifrnn"": InputFeedRNNDecoder,\n           ""cnn"": CNNDecoder, ""transformer"": TransformerDecoder}\n\n__all__ = [""DecoderBase"", ""TransformerDecoder"", ""StdRNNDecoder"", ""CNNDecoder"",\n           ""InputFeedRNNDecoder"", ""str2dec""]\n'"
onmt/decoders/cnn_decoder.py,5,"b'""""""Implementation of the CNN Decoder part of\n""Convolutional Sequence to Sequence Learning""\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom onmt.modules import ConvMultiStepAttention, GlobalAttention\nfrom onmt.utils.cnn_factory import shape_transform, GatedConv\nfrom onmt.decoders.decoder import DecoderBase\n\nSCALE_WEIGHT = 0.5 ** 0.5\n\n\nclass CNNDecoder(DecoderBase):\n    """"""Decoder based on ""Convolutional Sequence to Sequence Learning""\n    :cite:`DBLP:journals/corr/GehringAGYD17`.\n\n    Consists of residual convolutional layers, with ConvMultiStepAttention.\n    """"""\n\n    def __init__(self, num_layers, hidden_size, attn_type,\n                 copy_attn, cnn_kernel_width, dropout, embeddings,\n                 copy_attn_type):\n        super(CNNDecoder, self).__init__()\n\n        self.cnn_kernel_width = cnn_kernel_width\n        self.embeddings = embeddings\n\n        # Decoder State\n        self.state = {}\n\n        input_size = self.embeddings.embedding_size\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.conv_layers = nn.ModuleList(\n            [GatedConv(hidden_size, cnn_kernel_width, dropout, True)\n             for i in range(num_layers)]\n        )\n        self.attn_layers = nn.ModuleList(\n            [ConvMultiStepAttention(hidden_size) for i in range(num_layers)]\n        )\n\n        # CNNDecoder has its own attention mechanism.\n        # Set up a separate copy attention layer if needed.\n        assert not copy_attn, ""Copy mechanism not yet tested in conv2conv""\n        if copy_attn:\n            self.copy_attn = GlobalAttention(\n                hidden_size, attn_type=copy_attn_type)\n        else:\n            self.copy_attn = None\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.dec_layers,\n            opt.dec_rnn_size,\n            opt.global_attention,\n            opt.copy_attn,\n            opt.cnn_kernel_width,\n            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n            embeddings,\n            opt.copy_attn_type)\n\n    def init_state(self, _, memory_bank, enc_hidden):\n        """"""Init decoder state.""""""\n        self.state[""src""] = (memory_bank + enc_hidden) * SCALE_WEIGHT\n        self.state[""previous_input""] = None\n\n    def map_state(self, fn):\n        self.state[""src""] = fn(self.state[""src""], 1)\n        if self.state[""previous_input""] is not None:\n            self.state[""previous_input""] = fn(self.state[""previous_input""], 1)\n\n    def detach_state(self):\n        self.state[""previous_input""] = self.state[""previous_input""].detach()\n\n    def forward(self, tgt, memory_bank, step=None, **kwargs):\n        """""" See :obj:`onmt.modules.RNNDecoderBase.forward()`""""""\n\n        if self.state[""previous_input""] is not None:\n            tgt = torch.cat([self.state[""previous_input""], tgt], 0)\n\n        dec_outs = []\n        attns = {""std"": []}\n        if self.copy_attn is not None:\n            attns[""copy""] = []\n\n        emb = self.embeddings(tgt)\n        assert emb.dim() == 3  # len x batch x embedding_dim\n\n        tgt_emb = emb.transpose(0, 1).contiguous()\n        # The output of CNNEncoder.\n        src_memory_bank_t = memory_bank.transpose(0, 1).contiguous()\n        # The combination of output of CNNEncoder and source embeddings.\n        src_memory_bank_c = self.state[""src""].transpose(0, 1).contiguous()\n\n        emb_reshape = tgt_emb.contiguous().view(\n            tgt_emb.size(0) * tgt_emb.size(1), -1)\n        linear_out = self.linear(emb_reshape)\n        x = linear_out.view(tgt_emb.size(0), tgt_emb.size(1), -1)\n        x = shape_transform(x)\n\n        pad = torch.zeros(x.size(0), x.size(1), self.cnn_kernel_width - 1, 1)\n\n        pad = pad.type_as(x)\n        base_target_emb = x\n\n        for conv, attention in zip(self.conv_layers, self.attn_layers):\n            new_target_input = torch.cat([pad, x], 2)\n            out = conv(new_target_input)\n            c, attn = attention(base_target_emb, out,\n                                src_memory_bank_t, src_memory_bank_c)\n            x = (x + (c + out) * SCALE_WEIGHT) * SCALE_WEIGHT\n        output = x.squeeze(3).transpose(1, 2)\n\n        # Process the result and update the attentions.\n        dec_outs = output.transpose(0, 1).contiguous()\n        if self.state[""previous_input""] is not None:\n            dec_outs = dec_outs[self.state[""previous_input""].size(0):]\n            attn = attn[:, self.state[""previous_input""].size(0):].squeeze()\n            attn = torch.stack([attn])\n        attns[""std""] = attn\n        if self.copy_attn is not None:\n            attns[""copy""] = attn\n\n        # Update the state.\n        self.state[""previous_input""] = tgt\n        # TODO change the way attns is returned dict => list or tuple (onnx)\n        return dec_outs, attns\n\n    def update_dropout(self, dropout):\n        for layer in self.conv_layers:\n            layer.dropout.p = dropout\n'"
onmt/decoders/decoder.py,6,"b'import torch\nimport torch.nn as nn\n\nfrom onmt.models.stacked_rnn import StackedLSTM, StackedGRU\nfrom onmt.modules import context_gate_factory, GlobalAttention\nfrom onmt.utils.rnn_factory import rnn_factory\n\nfrom onmt.utils.misc import aeq\n\n\nclass DecoderBase(nn.Module):\n    """"""Abstract class for decoders.\n\n    Args:\n        attentional (bool): The decoder returns non-empty attention.\n    """"""\n\n    def __init__(self, attentional=True):\n        super(DecoderBase, self).__init__()\n        self.attentional = attentional\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.\n\n        Subclasses should override this method.\n        """"""\n\n        raise NotImplementedError\n\n\nclass RNNDecoderBase(DecoderBase):\n    """"""Base recurrent attention-based decoder class.\n\n    Specifies the interface used by different decoder types\n    and required by :class:`~onmt.models.NMTModel`.\n\n\n    .. mermaid::\n\n       graph BT\n          A[Input]\n          subgraph RNN\n             C[Pos 1]\n             D[Pos 2]\n             E[Pos N]\n          end\n          G[Decoder State]\n          H[Decoder State]\n          I[Outputs]\n          F[memory_bank]\n          A--emb-->C\n          A--emb-->D\n          A--emb-->E\n          H-->C\n          C-- attn --- F\n          D-- attn --- F\n          E-- attn --- F\n          C-->I\n          D-->I\n          E-->I\n          E-->G\n          F---I\n\n    Args:\n       rnn_type (str):\n          style of recurrent unit to use, one of [RNN, LSTM, GRU, SRU]\n       bidirectional_encoder (bool) : use with a bidirectional encoder\n       num_layers (int) : number of stacked layers\n       hidden_size (int) : hidden size of each layer\n       attn_type (str) : see :class:`~onmt.modules.GlobalAttention`\n       attn_func (str) : see :class:`~onmt.modules.GlobalAttention`\n       coverage_attn (str): see :class:`~onmt.modules.GlobalAttention`\n       context_gate (str): see :class:`~onmt.modules.ContextGate`\n       copy_attn (bool): setup a separate copy attention mechanism\n       dropout (float) : dropout value for :class:`torch.nn.Dropout`\n       embeddings (onmt.modules.Embeddings): embedding module to use\n       reuse_copy_attn (bool): reuse the attention for copying\n       copy_attn_type (str): The copy attention style. See\n        :class:`~onmt.modules.GlobalAttention`.\n    """"""\n\n    def __init__(self, rnn_type, bidirectional_encoder, num_layers,\n                 hidden_size, attn_type=""general"", attn_func=""softmax"",\n                 coverage_attn=False, context_gate=None,\n                 copy_attn=False, dropout=0.0, embeddings=None,\n                 reuse_copy_attn=False, copy_attn_type=""general""):\n        super(RNNDecoderBase, self).__init__(\n            attentional=attn_type != ""none"" and attn_type is not None)\n\n        self.bidirectional_encoder = bidirectional_encoder\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.embeddings = embeddings\n        self.dropout = nn.Dropout(dropout)\n\n        # Decoder state\n        self.state = {}\n\n        # Build the RNN.\n        self.rnn = self._build_rnn(rnn_type,\n                                   input_size=self._input_size,\n                                   hidden_size=hidden_size,\n                                   num_layers=num_layers,\n                                   dropout=dropout)\n\n        # Set up the context gate.\n        self.context_gate = None\n        if context_gate is not None:\n            self.context_gate = context_gate_factory(\n                context_gate, self._input_size,\n                hidden_size, hidden_size, hidden_size\n            )\n\n        # Set up the standard attention.\n        self._coverage = coverage_attn\n        if not self.attentional:\n            if self._coverage:\n                raise ValueError(""Cannot use coverage term with no attention."")\n            self.attn = None\n        else:\n            self.attn = GlobalAttention(\n                hidden_size, coverage=coverage_attn,\n                attn_type=attn_type, attn_func=attn_func\n            )\n\n        if copy_attn and not reuse_copy_attn:\n            if copy_attn_type == ""none"" or copy_attn_type is None:\n                raise ValueError(\n                    ""Cannot use copy_attn with copy_attn_type none"")\n            self.copy_attn = GlobalAttention(\n                hidden_size, attn_type=copy_attn_type, attn_func=attn_func\n            )\n        else:\n            self.copy_attn = None\n\n        self._reuse_copy_attn = reuse_copy_attn and copy_attn\n        if self._reuse_copy_attn and not self.attentional:\n            raise ValueError(""Cannot reuse copy attention with no attention."")\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.rnn_type,\n            opt.brnn,\n            opt.dec_layers,\n            opt.dec_rnn_size,\n            opt.global_attention,\n            opt.global_attention_function,\n            opt.coverage_attn,\n            opt.context_gate,\n            opt.copy_attn,\n            opt.dropout[0] if type(opt.dropout) is list\n            else opt.dropout,\n            embeddings,\n            opt.reuse_copy_attn,\n            opt.copy_attn_type)\n\n    def init_state(self, src, memory_bank, encoder_final):\n        """"""Initialize decoder state with last state of the encoder.""""""\n        def _fix_enc_hidden(hidden):\n            # The encoder hidden is  (layers*directions) x batch x dim.\n            # We need to convert it to layers x batch x (directions*dim).\n            if self.bidirectional_encoder:\n                hidden = torch.cat([hidden[0:hidden.size(0):2],\n                                    hidden[1:hidden.size(0):2]], 2)\n            return hidden\n\n        if isinstance(encoder_final, tuple):  # LSTM\n            self.state[""hidden""] = tuple(_fix_enc_hidden(enc_hid)\n                                         for enc_hid in encoder_final)\n        else:  # GRU\n            self.state[""hidden""] = (_fix_enc_hidden(encoder_final), )\n\n        # Init the input feed.\n        batch_size = self.state[""hidden""][0].size(1)\n        h_size = (batch_size, self.hidden_size)\n        self.state[""input_feed""] = \\\n            self.state[""hidden""][0].data.new(*h_size).zero_().unsqueeze(0)\n        self.state[""coverage""] = None\n\n    def map_state(self, fn):\n        self.state[""hidden""] = tuple(fn(h, 1) for h in self.state[""hidden""])\n        self.state[""input_feed""] = fn(self.state[""input_feed""], 1)\n        if self._coverage and self.state[""coverage""] is not None:\n            self.state[""coverage""] = fn(self.state[""coverage""], 1)\n\n    def detach_state(self):\n        self.state[""hidden""] = tuple(h.detach() for h in self.state[""hidden""])\n        self.state[""input_feed""] = self.state[""input_feed""].detach()\n\n    def forward(self, tgt, memory_bank, memory_lengths=None, step=None,\n                **kwargs):\n        """"""\n        Args:\n            tgt (LongTensor): sequences of padded tokens\n                 ``(tgt_len, batch, nfeats)``.\n            memory_bank (FloatTensor): vectors from the encoder\n                 ``(src_len, batch, hidden)``.\n            memory_lengths (LongTensor): the padded source lengths\n                ``(batch,)``.\n\n        Returns:\n            (FloatTensor, dict[str, FloatTensor]):\n\n            * dec_outs: output from the decoder (after attn)\n              ``(tgt_len, batch, hidden)``.\n            * attns: distribution over src at each tgt\n              ``(tgt_len, batch, src_len)``.\n        """"""\n\n        dec_state, dec_outs, attns = self._run_forward_pass(\n            tgt, memory_bank, memory_lengths=memory_lengths)\n\n        # Update the state with the result.\n        if not isinstance(dec_state, tuple):\n            dec_state = (dec_state,)\n        self.state[""hidden""] = dec_state\n        self.state[""input_feed""] = dec_outs[-1].unsqueeze(0)\n        self.state[""coverage""] = None\n        if ""coverage"" in attns:\n            self.state[""coverage""] = attns[""coverage""][-1].unsqueeze(0)\n\n        # Concatenates sequence of tensors along a new dimension.\n        # NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list\n        #       (in particular in case of SRU) it was not raising error in 0.3\n        #       since stack(Variable) was allowed.\n        #       In 0.4, SRU returns a tensor that shouldn\'t be stacke\n        if type(dec_outs) == list:\n            dec_outs = torch.stack(dec_outs)\n\n            for k in attns:\n                if type(attns[k]) == list:\n                    attns[k] = torch.stack(attns[k])\n        return dec_outs, attns\n\n    def update_dropout(self, dropout):\n        self.dropout.p = dropout\n        self.embeddings.update_dropout(dropout)\n\n\nclass StdRNNDecoder(RNNDecoderBase):\n    """"""Standard fully batched RNN decoder with attention.\n\n    Faster implementation, uses CuDNN for implementation.\n    See :class:`~onmt.decoders.decoder.RNNDecoderBase` for options.\n\n\n    Based around the approach from\n    ""Neural Machine Translation By Jointly Learning To Align and Translate""\n    :cite:`Bahdanau2015`\n\n\n    Implemented without input_feeding and currently with no `coverage_attn`\n    or `copy_attn` support.\n    """"""\n\n    def _run_forward_pass(self, tgt, memory_bank, memory_lengths=None):\n        """"""\n        Private helper for running the specific RNN forward pass.\n        Must be overriden by all subclasses.\n\n        Args:\n            tgt (LongTensor): a sequence of input tokens tensors\n                ``(len, batch, nfeats)``.\n            memory_bank (FloatTensor): output(tensor sequence) from the\n                encoder RNN of size ``(src_len, batch, hidden_size)``.\n            memory_lengths (LongTensor): the source memory_bank lengths.\n\n        Returns:\n            (Tensor, List[FloatTensor], Dict[str, List[FloatTensor]):\n\n            * dec_state: final hidden state from the decoder.\n            * dec_outs: an array of output of every time\n              step from the decoder.\n            * attns: a dictionary of different\n              type of attention Tensor array of every time\n              step from the decoder.\n        """"""\n\n        assert self.copy_attn is None  # TODO, no support yet.\n        assert not self._coverage  # TODO, no support yet.\n\n        attns = {}\n        emb = self.embeddings(tgt)\n\n        if isinstance(self.rnn, nn.GRU):\n            rnn_output, dec_state = self.rnn(emb, self.state[""hidden""][0])\n        else:\n            rnn_output, dec_state = self.rnn(emb, self.state[""hidden""])\n\n        # Check\n        tgt_len, tgt_batch, _ = tgt.size()\n        output_len, output_batch, _ = rnn_output.size()\n        aeq(tgt_len, output_len)\n        aeq(tgt_batch, output_batch)\n\n        # Calculate the attention.\n        if not self.attentional:\n            dec_outs = rnn_output\n        else:\n            dec_outs, p_attn = self.attn(\n                rnn_output.transpose(0, 1).contiguous(),\n                memory_bank.transpose(0, 1),\n                memory_lengths=memory_lengths\n            )\n            attns[""std""] = p_attn\n\n        # Calculate the context gate.\n        if self.context_gate is not None:\n            dec_outs = self.context_gate(\n                emb.view(-1, emb.size(2)),\n                rnn_output.view(-1, rnn_output.size(2)),\n                dec_outs.view(-1, dec_outs.size(2))\n            )\n            dec_outs = dec_outs.view(tgt_len, tgt_batch, self.hidden_size)\n\n        dec_outs = self.dropout(dec_outs)\n        return dec_state, dec_outs, attns\n\n    def _build_rnn(self, rnn_type, **kwargs):\n        rnn, _ = rnn_factory(rnn_type, **kwargs)\n        return rnn\n\n    @property\n    def _input_size(self):\n        return self.embeddings.embedding_size\n\n\nclass InputFeedRNNDecoder(RNNDecoderBase):\n    """"""Input feeding based decoder.\n\n    See :class:`~onmt.decoders.decoder.RNNDecoderBase` for options.\n\n    Based around the input feeding approach from\n    ""Effective Approaches to Attention-based Neural Machine Translation""\n    :cite:`Luong2015`\n\n\n    .. mermaid::\n\n       graph BT\n          A[Input n-1]\n          AB[Input n]\n          subgraph RNN\n            E[Pos n-1]\n            F[Pos n]\n            E --> F\n          end\n          G[Encoder]\n          H[memory_bank n-1]\n          A --> E\n          AB --> F\n          E --> H\n          G --> H\n    """"""\n\n    def _run_forward_pass(self, tgt, memory_bank, memory_lengths=None):\n        """"""\n        See StdRNNDecoder._run_forward_pass() for description\n        of arguments and return values.\n        """"""\n        # Additional args check.\n        input_feed = self.state[""input_feed""].squeeze(0)\n        input_feed_batch, _ = input_feed.size()\n        _, tgt_batch, _ = tgt.size()\n        aeq(tgt_batch, input_feed_batch)\n        # END Additional args check.\n\n        dec_outs = []\n        attns = {}\n        if self.attn is not None:\n            attns[""std""] = []\n        if self.copy_attn is not None or self._reuse_copy_attn:\n            attns[""copy""] = []\n        if self._coverage:\n            attns[""coverage""] = []\n\n        emb = self.embeddings(tgt)\n        assert emb.dim() == 3  # len x batch x embedding_dim\n\n        dec_state = self.state[""hidden""]\n        coverage = self.state[""coverage""].squeeze(0) \\\n            if self.state[""coverage""] is not None else None\n\n        # Input feed concatenates hidden state with\n        # input at every time step.\n        for emb_t in emb.split(1):\n            decoder_input = torch.cat([emb_t.squeeze(0), input_feed], 1)\n            rnn_output, dec_state = self.rnn(decoder_input, dec_state)\n            if self.attentional:\n                decoder_output, p_attn = self.attn(\n                    rnn_output,\n                    memory_bank.transpose(0, 1),\n                    memory_lengths=memory_lengths)\n                attns[""std""].append(p_attn)\n            else:\n                decoder_output = rnn_output\n            if self.context_gate is not None:\n                # TODO: context gate should be employed\n                # instead of second RNN transform.\n                decoder_output = self.context_gate(\n                    decoder_input, rnn_output, decoder_output\n                )\n            decoder_output = self.dropout(decoder_output)\n            input_feed = decoder_output\n\n            dec_outs += [decoder_output]\n\n            # Update the coverage attention.\n            if self._coverage:\n                coverage = p_attn if coverage is None else p_attn + coverage\n                attns[""coverage""] += [coverage]\n\n            if self.copy_attn is not None:\n                _, copy_attn = self.copy_attn(\n                    decoder_output, memory_bank.transpose(0, 1))\n                attns[""copy""] += [copy_attn]\n            elif self._reuse_copy_attn:\n                attns[""copy""] = attns[""std""]\n\n        return dec_state, dec_outs, attns\n\n    def _build_rnn(self, rnn_type, input_size,\n                   hidden_size, num_layers, dropout):\n        assert rnn_type != ""SRU"", ""SRU doesn\'t support input feed! "" \\\n            ""Please set -input_feed 0!""\n        stacked_cell = StackedLSTM if rnn_type == ""LSTM"" else StackedGRU\n        return stacked_cell(num_layers, input_size, hidden_size, dropout)\n\n    @property\n    def _input_size(self):\n        """"""Using input feed by concatenating input with attention vectors.""""""\n        return self.embeddings.embedding_size + self.hidden_size\n\n    def update_dropout(self, dropout):\n        self.dropout.p = dropout\n        self.rnn.dropout.p = dropout\n        self.embeddings.update_dropout(dropout)\n'"
onmt/decoders/ensemble.py,4,"b'""""""Ensemble decoding.\n\nDecodes using multiple models simultaneously,\ncombining their prediction distributions by averaging.\nAll models in the ensemble must share a target vocabulary.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom onmt.encoders.encoder import EncoderBase\nfrom onmt.decoders.decoder import DecoderBase\nfrom onmt.models import NMTModel\nimport onmt.model_builder\n\n\nclass EnsembleDecoderOutput(object):\n    """"""Wrapper around multiple decoder final hidden states.""""""\n    def __init__(self, model_dec_outs):\n        self.model_dec_outs = tuple(model_dec_outs)\n\n    def squeeze(self, dim=None):\n        """"""Delegate squeeze to avoid modifying\n        :func:`onmt.translate.translator.Translator.translate_batch()`\n        """"""\n        return EnsembleDecoderOutput([\n            x.squeeze(dim) for x in self.model_dec_outs])\n\n    def __getitem__(self, index):\n        return self.model_dec_outs[index]\n\n\nclass EnsembleEncoder(EncoderBase):\n    """"""Dummy Encoder that delegates to individual real Encoders.""""""\n    def __init__(self, model_encoders):\n        super(EnsembleEncoder, self).__init__()\n        self.model_encoders = nn.ModuleList(model_encoders)\n\n    def forward(self, src, lengths=None):\n        enc_hidden, memory_bank, _ = zip(*[\n            model_encoder(src, lengths)\n            for model_encoder in self.model_encoders])\n        return enc_hidden, memory_bank, lengths\n\n\nclass EnsembleDecoder(DecoderBase):\n    """"""Dummy Decoder that delegates to individual real Decoders.""""""\n    def __init__(self, model_decoders):\n        model_decoders = nn.ModuleList(model_decoders)\n        attentional = any([dec.attentional for dec in model_decoders])\n        super(EnsembleDecoder, self).__init__(attentional)\n        self.model_decoders = model_decoders\n\n    def forward(self, tgt, memory_bank, memory_lengths=None, step=None,\n                **kwargs):\n        """"""See :func:`onmt.decoders.decoder.DecoderBase.forward()`.""""""\n        # Memory_lengths is a single tensor shared between all models.\n        # This assumption will not hold if Translator is modified\n        # to calculate memory_lengths as something other than the length\n        # of the input.\n        dec_outs, attns = zip(*[\n            model_decoder(\n                tgt, memory_bank[i],\n                memory_lengths=memory_lengths, step=step, **kwargs)\n            for i, model_decoder in enumerate(self.model_decoders)])\n        mean_attns = self.combine_attns(attns)\n        return EnsembleDecoderOutput(dec_outs), mean_attns\n\n    def combine_attns(self, attns):\n        result = {}\n        for key in attns[0].keys():\n            result[key] = torch.stack(\n                [attn[key] for attn in attns if attn[key] is not None]).mean(0)\n        return result\n\n    def init_state(self, src, memory_bank, enc_hidden):\n        """""" See :obj:`RNNDecoderBase.init_state()` """"""\n        for i, model_decoder in enumerate(self.model_decoders):\n            model_decoder.init_state(src, memory_bank[i], enc_hidden[i])\n\n    def map_state(self, fn):\n        for model_decoder in self.model_decoders:\n            model_decoder.map_state(fn)\n\n\nclass EnsembleGenerator(nn.Module):\n    """"""\n    Dummy Generator that delegates to individual real Generators,\n    and then averages the resulting target distributions.\n    """"""\n    def __init__(self, model_generators, raw_probs=False):\n        super(EnsembleGenerator, self).__init__()\n        self.model_generators = nn.ModuleList(model_generators)\n        self._raw_probs = raw_probs\n\n    def forward(self, hidden, attn=None, src_map=None):\n        """"""\n        Compute a distribution over the target dictionary\n        by averaging distributions from models in the ensemble.\n        All models in the ensemble must share a target vocabulary.\n        """"""\n        distributions = torch.stack(\n                [mg(h) if attn is None else mg(h, attn, src_map)\n                 for h, mg in zip(hidden, self.model_generators)]\n            )\n        if self._raw_probs:\n            return torch.log(torch.exp(distributions).mean(0))\n        else:\n            return distributions.mean(0)\n\n\nclass EnsembleModel(NMTModel):\n    """"""Dummy NMTModel wrapping individual real NMTModels.""""""\n    def __init__(self, models, raw_probs=False):\n        encoder = EnsembleEncoder(model.encoder for model in models)\n        decoder = EnsembleDecoder(model.decoder for model in models)\n        super(EnsembleModel, self).__init__(encoder, decoder)\n        self.generator = EnsembleGenerator(\n            [model.generator for model in models], raw_probs)\n        self.models = nn.ModuleList(models)\n\n\ndef load_test_model(opt):\n    """"""Read in multiple models for ensemble.""""""\n    shared_fields = None\n    shared_model_opt = None\n    models = []\n    for model_path in opt.models:\n        fields, model, model_opt = \\\n            onmt.model_builder.load_test_model(opt, model_path=model_path)\n        if shared_fields is None:\n            shared_fields = fields\n        else:\n            for key, field in fields.items():\n                try:\n                    f_iter = iter(field)\n                except TypeError:\n                    f_iter = [(key, field)]\n                for sn, sf in f_iter:\n                    if sf is not None and \'vocab\' in sf.__dict__:\n                        sh_field = shared_fields[key]\n                        try:\n                            sh_f_iter = iter(sh_field)\n                        except TypeError:\n                            sh_f_iter = [(key, sh_field)]\n                        sh_f_dict = dict(sh_f_iter)\n                        assert sf.vocab.stoi == sh_f_dict[sn].vocab.stoi, \\\n                            ""Ensemble models must use the same "" \\\n                            ""preprocessed data""\n        models.append(model)\n        if shared_model_opt is None:\n            shared_model_opt = model_opt\n    ensemble_model = EnsembleModel(models, opt.avg_raw_probs)\n    return shared_fields, ensemble_model, shared_model_opt\n'"
onmt/decoders/transformer.py,6,"b'""""""\nImplementation of ""Attention is All You Need""\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom onmt.decoders.decoder import DecoderBase\nfrom onmt.modules import MultiHeadedAttention, AverageAttention\nfrom onmt.modules.position_ffn import PositionwiseFeedForward\nfrom onmt.utils.misc import sequence_mask\n\n\nclass TransformerDecoderLayer(nn.Module):\n    """"""Transformer Decoder layer block in Pre-Norm style.\n    Pre-Norm style is an improvement w.r.t. Original paper\'s Post-Norm style,\n    providing better converge speed and performance. This is also the actual\n    implementation in tensor2tensor and also avalable in fairseq.\n    See https://tunz.kr/post/4 and :cite:`DeeperTransformer`.\n\n    .. mermaid::\n\n        graph LR\n        %% ""*SubLayer"" can be self-attn, src-attn or feed forward block\n            A(input) --> B[Norm]\n            B --> C[""*SubLayer""]\n            C --> D[Drop]\n            D --> E((+))\n            A --> E\n            E --> F(out)\n\n\n    Args:\n        d_model (int): the dimension of keys/values/queries in\n            :class:`MultiHeadedAttention`, also the input size of\n            the first-layer of the :class:`PositionwiseFeedForward`.\n        heads (int): the number of heads for MultiHeadedAttention.\n        d_ff (int): the second-layer of the :class:`PositionwiseFeedForward`.\n        dropout (float): dropout in residual, self-attn(dot) and feed-forward\n        attention_dropout (float): dropout in context_attn (and self-attn(avg))\n        self_attn_type (string): type of self-attention scaled-dot, average\n        max_relative_positions (int):\n            Max distance between inputs in relative positions representations\n        aan_useffn (bool): Turn on the FFN layer in the AAN decoder\n        full_context_alignment (bool):\n            whether enable an extra full context decoder forward for alignment\n        alignment_heads (int):\n            N. of cross attention heads to use for alignment guiding\n    """"""\n\n    def __init__(self, d_model, heads, d_ff, dropout, attention_dropout,\n                 self_attn_type=""scaled-dot"", max_relative_positions=0,\n                 aan_useffn=False, full_context_alignment=False,\n                 alignment_heads=0):\n        super(TransformerDecoderLayer, self).__init__()\n\n        if self_attn_type == ""scaled-dot"":\n            self.self_attn = MultiHeadedAttention(\n                heads, d_model, dropout=attention_dropout,\n                max_relative_positions=max_relative_positions)\n        elif self_attn_type == ""average"":\n            self.self_attn = AverageAttention(d_model,\n                                              dropout=attention_dropout,\n                                              aan_useffn=aan_useffn)\n\n        self.context_attn = MultiHeadedAttention(\n            heads, d_model, dropout=attention_dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.layer_norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n        self.layer_norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n        self.drop = nn.Dropout(dropout)\n        self.full_context_alignment = full_context_alignment\n        self.alignment_heads = alignment_heads\n\n    def forward(self, *args, **kwargs):\n        """""" Extend `_forward` for (possibly) multiple decoder pass:\n        Always a default (future masked) decoder forward pass,\n        Possibly a second future aware decoder pass for joint learn\n        full context alignement, :cite:`garg2019jointly`.\n\n        Args:\n            * All arguments of _forward.\n            with_align (bool): whether return alignment attention.\n\n        Returns:\n            (FloatTensor, FloatTensor, FloatTensor or None):\n\n            * output ``(batch_size, T, model_dim)``\n            * top_attn ``(batch_size, T, src_len)``\n            * attn_align ``(batch_size, T, src_len)`` or None\n        """"""\n        with_align = kwargs.pop(\'with_align\', False)\n        output, attns = self._forward(*args, **kwargs)\n        top_attn = attns[:, 0, :, :].contiguous()\n        attn_align = None\n        if with_align:\n            if self.full_context_alignment:\n                # return _, (B, Q_len, K_len)\n                _, attns = self._forward(*args, **kwargs, future=True)\n\n            if self.alignment_heads > 0:\n                attns = attns[:, :self.alignment_heads, :, :].contiguous()\n            # layer average attention across heads, get ``(B, Q, K)``\n            # Case 1: no full_context, no align heads -> layer avg baseline\n            # Case 2: no full_context, 1 align heads -> guided align\n            # Case 3: full_context, 1 align heads -> full cte guided align\n            attn_align = attns.mean(dim=1)\n        return output, top_attn, attn_align\n\n    def _forward(self, inputs, memory_bank, src_pad_mask, tgt_pad_mask,\n                 layer_cache=None, step=None, future=False):\n        """""" A naive forward pass for transformer decoder.\n\n        # T: could be 1 in the case of stepwise decoding or tgt_len\n\n        Args:\n            inputs (FloatTensor): ``(batch_size, T, model_dim)``\n            memory_bank (FloatTensor): ``(batch_size, src_len, model_dim)``\n            src_pad_mask (bool): ``(batch_size, 1, src_len)``\n            tgt_pad_mask (bool): ``(batch_size, 1, T)``\n            layer_cache (dict or None): cached layer info when stepwise decode\n            step (int or None): stepwise decoding counter\n            future (bool): If set True, do not apply future_mask.\n\n        Returns:\n            (FloatTensor, FloatTensor):\n\n            * output ``(batch_size, T, model_dim)``\n            * attns ``(batch_size, head, T, src_len)``\n\n        """"""\n        dec_mask = None\n\n        if step is None:\n            tgt_len = tgt_pad_mask.size(-1)\n            if not future:  # apply future_mask, result mask in (B, T, T)\n                future_mask = torch.ones(\n                    [tgt_len, tgt_len],\n                    device=tgt_pad_mask.device,\n                    dtype=torch.uint8)\n                future_mask = future_mask.triu_(1).view(1, tgt_len, tgt_len)\n                # BoolTensor was introduced in pytorch 1.2\n                try:\n                    future_mask = future_mask.bool()\n                except AttributeError:\n                    pass\n                dec_mask = torch.gt(tgt_pad_mask + future_mask, 0)\n            else:  # only mask padding, result mask in (B, 1, T)\n                dec_mask = tgt_pad_mask\n\n        input_norm = self.layer_norm_1(inputs)\n\n        if isinstance(self.self_attn, MultiHeadedAttention):\n            query, _ = self.self_attn(input_norm, input_norm, input_norm,\n                                      mask=dec_mask,\n                                      layer_cache=layer_cache,\n                                      attn_type=""self"")\n        elif isinstance(self.self_attn, AverageAttention):\n            query, _ = self.self_attn(input_norm, mask=dec_mask,\n                                      layer_cache=layer_cache, step=step)\n\n        query = self.drop(query) + inputs\n\n        query_norm = self.layer_norm_2(query)\n        mid, attns = self.context_attn(memory_bank, memory_bank, query_norm,\n                                       mask=src_pad_mask,\n                                       layer_cache=layer_cache,\n                                       attn_type=""context"")\n        output = self.feed_forward(self.drop(mid) + query)\n\n        return output, attns\n\n    def update_dropout(self, dropout, attention_dropout):\n        self.self_attn.update_dropout(attention_dropout)\n        self.context_attn.update_dropout(attention_dropout)\n        self.feed_forward.update_dropout(dropout)\n        self.drop.p = dropout\n\n\nclass TransformerDecoder(DecoderBase):\n    """"""The Transformer decoder from ""Attention is All You Need"".\n    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`\n\n    .. mermaid::\n\n       graph BT\n          A[input]\n          B[multi-head self-attn]\n          BB[multi-head src-attn]\n          C[feed forward]\n          O[output]\n          A --> B\n          B --> BB\n          BB --> C\n          C --> O\n\n\n    Args:\n        num_layers (int): number of decoder layers.\n        d_model (int): size of the model\n        heads (int): number of heads\n        d_ff (int): size of the inner FF layer\n        copy_attn (bool): if using a separate copy attention\n        self_attn_type (str): type of self-attention scaled-dot, average\n        dropout (float): dropout in residual, self-attn(dot) and feed-forward\n        attention_dropout (float): dropout in context_attn (and self-attn(avg))\n        embeddings (onmt.modules.Embeddings):\n            embeddings to use, should have positional encodings\n        max_relative_positions (int):\n            Max distance between inputs in relative positions representations\n        aan_useffn (bool): Turn on the FFN layer in the AAN decoder\n        full_context_alignment (bool):\n            whether enable an extra full context decoder forward for alignment\n        alignment_layer (int): N\xc2\xb0 Layer to supervise with for alignment guiding\n        alignment_heads (int):\n            N. of cross attention heads to use for alignment guiding\n    """"""\n\n    def __init__(self, num_layers, d_model, heads, d_ff,\n                 copy_attn, self_attn_type, dropout, attention_dropout,\n                 embeddings, max_relative_positions, aan_useffn,\n                 full_context_alignment, alignment_layer,\n                 alignment_heads):\n        super(TransformerDecoder, self).__init__()\n\n        self.embeddings = embeddings\n\n        # Decoder State\n        self.state = {}\n\n        self.transformer_layers = nn.ModuleList(\n            [TransformerDecoderLayer(d_model, heads, d_ff, dropout,\n             attention_dropout, self_attn_type=self_attn_type,\n             max_relative_positions=max_relative_positions,\n             aan_useffn=aan_useffn,\n             full_context_alignment=full_context_alignment,\n             alignment_heads=alignment_heads)\n             for i in range(num_layers)])\n\n        # previously, there was a GlobalAttention module here for copy\n        # attention. But it was never actually used -- the ""copy"" attention\n        # just reuses the context attention.\n        self._copy = copy_attn\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n        self.alignment_layer = alignment_layer\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.dec_layers,\n            opt.dec_rnn_size,\n            opt.heads,\n            opt.transformer_ff,\n            opt.copy_attn,\n            opt.self_attn_type,\n            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n            opt.attention_dropout[0] if type(opt.attention_dropout)\n            is list else opt.dropout,\n            embeddings,\n            opt.max_relative_positions,\n            opt.aan_useffn,\n            opt.full_context_alignment,\n            opt.alignment_layer,\n            alignment_heads=opt.alignment_heads)\n\n    def init_state(self, src, memory_bank, enc_hidden):\n        """"""Initialize decoder state.""""""\n        self.state[""src""] = src\n        self.state[""cache""] = None\n\n    def map_state(self, fn):\n        def _recursive_map(struct, batch_dim=0):\n            for k, v in struct.items():\n                if v is not None:\n                    if isinstance(v, dict):\n                        _recursive_map(v)\n                    else:\n                        struct[k] = fn(v, batch_dim)\n\n        self.state[""src""] = fn(self.state[""src""], 1)\n        if self.state[""cache""] is not None:\n            _recursive_map(self.state[""cache""])\n\n    def detach_state(self):\n        self.state[""src""] = self.state[""src""].detach()\n\n    def forward(self, tgt, memory_bank, step=None, **kwargs):\n        """"""Decode, possibly stepwise.""""""\n        if step == 0:\n            self._init_cache(memory_bank)\n\n        tgt_words = tgt[:, :, 0].transpose(0, 1)\n\n        emb = self.embeddings(tgt, step=step)\n        assert emb.dim() == 3  # len x batch x embedding_dim\n\n        output = emb.transpose(0, 1).contiguous()\n        src_memory_bank = memory_bank.transpose(0, 1).contiguous()\n\n        pad_idx = self.embeddings.word_padding_idx\n        src_lens = kwargs[""memory_lengths""]\n        src_max_len = self.state[""src""].shape[0]\n        src_pad_mask = ~sequence_mask(src_lens, src_max_len).unsqueeze(1)\n        tgt_pad_mask = tgt_words.data.eq(pad_idx).unsqueeze(1)  # [B, 1, T_tgt]\n\n        with_align = kwargs.pop(\'with_align\', False)\n        attn_aligns = []\n\n        for i, layer in enumerate(self.transformer_layers):\n            layer_cache = self.state[""cache""][""layer_{}"".format(i)] \\\n                if step is not None else None\n            output, attn, attn_align = layer(\n                output,\n                src_memory_bank,\n                src_pad_mask,\n                tgt_pad_mask,\n                layer_cache=layer_cache,\n                step=step,\n                with_align=with_align)\n            if attn_align is not None:\n                attn_aligns.append(attn_align)\n\n        output = self.layer_norm(output)\n        dec_outs = output.transpose(0, 1).contiguous()\n        attn = attn.transpose(0, 1).contiguous()\n\n        attns = {""std"": attn}\n        if self._copy:\n            attns[""copy""] = attn\n        if with_align:\n            attns[""align""] = attn_aligns[self.alignment_layer]  # `(B, Q, K)`\n            # attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg\n\n        # TODO change the way attns is returned dict => list or tuple (onnx)\n        return dec_outs, attns\n\n    def _init_cache(self, memory_bank):\n        self.state[""cache""] = {}\n        batch_size = memory_bank.size(1)\n        depth = memory_bank.size(-1)\n\n        for i, layer in enumerate(self.transformer_layers):\n            layer_cache = {""memory_keys"": None, ""memory_values"": None}\n            if isinstance(layer.self_attn, AverageAttention):\n                layer_cache[""prev_g""] = torch.zeros((batch_size, 1, depth),\n                                                    device=memory_bank.device)\n            else:\n                layer_cache[""self_keys""] = None\n                layer_cache[""self_values""] = None\n            self.state[""cache""][""layer_{}"".format(i)] = layer_cache\n\n    def update_dropout(self, dropout, attention_dropout):\n        self.embeddings.update_dropout(dropout)\n        for layer in self.transformer_layers:\n            layer.update_dropout(dropout, attention_dropout)\n'"
onmt/encoders/__init__.py,0,"b'""""""Module defining encoders.""""""\nfrom onmt.encoders.encoder import EncoderBase\nfrom onmt.encoders.transformer import TransformerEncoder\nfrom onmt.encoders.ggnn_encoder import GGNNEncoder\nfrom onmt.encoders.rnn_encoder import RNNEncoder\nfrom onmt.encoders.cnn_encoder import CNNEncoder\nfrom onmt.encoders.mean_encoder import MeanEncoder\nfrom onmt.encoders.audio_encoder import AudioEncoder\nfrom onmt.encoders.image_encoder import ImageEncoder\n\n\nstr2enc = {""ggnn"": GGNNEncoder, ""rnn"": RNNEncoder, ""brnn"": RNNEncoder,\n           ""cnn"": CNNEncoder, ""transformer"": TransformerEncoder,\n           ""img"": ImageEncoder, ""audio"": AudioEncoder, ""mean"": MeanEncoder}\n\n__all__ = [""EncoderBase"", ""TransformerEncoder"", ""RNNEncoder"", ""CNNEncoder"",\n           ""MeanEncoder"", ""str2enc""]\n'"
onmt/encoders/audio_encoder.py,3,"b'""""""Audio encoder""""""\nimport math\n\nimport torch.nn as nn\n\nfrom torch.nn.utils.rnn import pack_padded_sequence as pack\nfrom torch.nn.utils.rnn import pad_packed_sequence as unpack\n\nfrom onmt.utils.rnn_factory import rnn_factory\nfrom onmt.encoders.encoder import EncoderBase\n\n\nclass AudioEncoder(EncoderBase):\n    """"""A simple encoder CNN -> RNN for audio input.\n\n    Args:\n        rnn_type (str): Type of RNN (e.g. GRU, LSTM, etc).\n        enc_layers (int): Number of encoder layers.\n        dec_layers (int): Number of decoder layers.\n        brnn (bool): Bidirectional encoder.\n        enc_rnn_size (int): Size of hidden states of the rnn.\n        dec_rnn_size (int): Size of the decoder hidden states.\n        enc_pooling (str): A comma separated list either of length 1\n            or of length ``enc_layers`` specifying the pooling amount.\n        dropout (float): dropout probablity.\n        sample_rate (float): input spec\n        window_size (int): input spec\n    """"""\n\n    def __init__(self, rnn_type, enc_layers, dec_layers, brnn,\n                 enc_rnn_size, dec_rnn_size, enc_pooling, dropout,\n                 sample_rate, window_size):\n        super(AudioEncoder, self).__init__()\n        self.enc_layers = enc_layers\n        self.rnn_type = rnn_type\n        self.dec_layers = dec_layers\n        num_directions = 2 if brnn else 1\n        self.num_directions = num_directions\n        assert enc_rnn_size % num_directions == 0\n        enc_rnn_size_real = enc_rnn_size // num_directions\n        assert dec_rnn_size % num_directions == 0\n        self.dec_rnn_size = dec_rnn_size\n        dec_rnn_size_real = dec_rnn_size // num_directions\n        self.dec_rnn_size_real = dec_rnn_size_real\n        self.dec_rnn_size = dec_rnn_size\n        input_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n        enc_pooling = enc_pooling.split(\',\')\n        assert len(enc_pooling) == enc_layers or len(enc_pooling) == 1\n        if len(enc_pooling) == 1:\n            enc_pooling = enc_pooling * enc_layers\n        enc_pooling = [int(p) for p in enc_pooling]\n        self.enc_pooling = enc_pooling\n\n        if type(dropout) is not list:\n            dropout = [dropout]\n        if max(dropout) > 0:\n            self.dropout = nn.Dropout(dropout[0])\n        else:\n            self.dropout = None\n        self.W = nn.Linear(enc_rnn_size, dec_rnn_size, bias=False)\n        self.batchnorm_0 = nn.BatchNorm1d(enc_rnn_size, affine=True)\n        self.rnn_0, self.no_pack_padded_seq = \\\n            rnn_factory(rnn_type,\n                        input_size=input_size,\n                        hidden_size=enc_rnn_size_real,\n                        num_layers=1,\n                        dropout=dropout[0],\n                        bidirectional=brnn)\n        self.pool_0 = nn.MaxPool1d(enc_pooling[0])\n        for l in range(enc_layers - 1):\n            batchnorm = nn.BatchNorm1d(enc_rnn_size, affine=True)\n            rnn, _ = \\\n                rnn_factory(rnn_type,\n                            input_size=enc_rnn_size,\n                            hidden_size=enc_rnn_size_real,\n                            num_layers=1,\n                            dropout=dropout[0],\n                            bidirectional=brnn)\n            setattr(self, \'rnn_%d\' % (l + 1), rnn)\n            setattr(self, \'pool_%d\' % (l + 1),\n                    nn.MaxPool1d(enc_pooling[l + 1]))\n            setattr(self, \'batchnorm_%d\' % (l + 1), batchnorm)\n\n    @classmethod\n    def from_opt(cls, opt, embeddings=None):\n        """"""Alternate constructor.""""""\n        if embeddings is not None:\n            raise ValueError(""Cannot use embeddings with AudioEncoder."")\n        return cls(\n            opt.rnn_type,\n            opt.enc_layers,\n            opt.dec_layers,\n            opt.brnn,\n            opt.enc_rnn_size,\n            opt.dec_rnn_size,\n            opt.audio_enc_pooling,\n            opt.dropout,\n            opt.sample_rate,\n            opt.window_size)\n\n    def forward(self, src, lengths=None):\n        """"""See :func:`onmt.encoders.encoder.EncoderBase.forward()`""""""\n        batch_size, _, nfft, t = src.size()\n        src = src.transpose(0, 1).transpose(0, 3).contiguous() \\\n                 .view(t, batch_size, nfft)\n        orig_lengths = lengths\n        lengths = lengths.view(-1).tolist()\n\n        for l in range(self.enc_layers):\n            rnn = getattr(self, \'rnn_%d\' % l)\n            pool = getattr(self, \'pool_%d\' % l)\n            batchnorm = getattr(self, \'batchnorm_%d\' % l)\n            stride = self.enc_pooling[l]\n            packed_emb = pack(src, lengths)\n            memory_bank, tmp = rnn(packed_emb)\n            memory_bank = unpack(memory_bank)[0]\n            t, _, _ = memory_bank.size()\n            memory_bank = memory_bank.transpose(0, 2)\n            memory_bank = pool(memory_bank)\n            lengths = [int(math.floor((length - stride) / stride + 1))\n                       for length in lengths]\n            memory_bank = memory_bank.transpose(0, 2)\n            src = memory_bank\n            t, _, num_feat = src.size()\n            src = batchnorm(src.contiguous().view(-1, num_feat))\n            src = src.view(t, -1, num_feat)\n            if self.dropout and l + 1 != self.enc_layers:\n                src = self.dropout(src)\n\n        memory_bank = memory_bank.contiguous().view(-1, memory_bank.size(2))\n        memory_bank = self.W(memory_bank).view(-1, batch_size,\n                                               self.dec_rnn_size)\n\n        state = memory_bank.new_full((self.dec_layers * self.num_directions,\n                                      batch_size, self.dec_rnn_size_real), 0)\n        if self.rnn_type == \'LSTM\':\n            # The encoder hidden is  (layers*directions) x batch x dim.\n            encoder_final = (state, state)\n        else:\n            encoder_final = state\n        return encoder_final, memory_bank, orig_lengths.new_tensor(lengths)\n\n    def update_dropout(self, dropout):\n        self.dropout.p = dropout\n        for i in range(self.enc_layers - 1):\n            getattr(self, \'rnn_%d\' % i).dropout = dropout\n'"
onmt/encoders/cnn_encoder.py,1,"b'""""""\nImplementation of ""Convolutional Sequence to Sequence Learning""\n""""""\nimport torch.nn as nn\n\nfrom onmt.encoders.encoder import EncoderBase\nfrom onmt.utils.cnn_factory import shape_transform, StackedCNN\n\nSCALE_WEIGHT = 0.5 ** 0.5\n\n\nclass CNNEncoder(EncoderBase):\n    """"""Encoder based on ""Convolutional Sequence to Sequence Learning""\n    :cite:`DBLP:journals/corr/GehringAGYD17`.\n    """"""\n\n    def __init__(self, num_layers, hidden_size,\n                 cnn_kernel_width, dropout, embeddings):\n        super(CNNEncoder, self).__init__()\n\n        self.embeddings = embeddings\n        input_size = embeddings.embedding_size\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.cnn = StackedCNN(num_layers, hidden_size,\n                              cnn_kernel_width, dropout)\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.enc_layers,\n            opt.enc_rnn_size,\n            opt.cnn_kernel_width,\n            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n            embeddings)\n\n    def forward(self, input, lengths=None, hidden=None):\n        """"""See :class:`onmt.modules.EncoderBase.forward()`""""""\n        self._check_args(input, lengths, hidden)\n\n        emb = self.embeddings(input)\n        # s_len, batch, emb_dim = emb.size()\n\n        emb = emb.transpose(0, 1).contiguous()\n        emb_reshape = emb.view(emb.size(0) * emb.size(1), -1)\n        emb_remap = self.linear(emb_reshape)\n        emb_remap = emb_remap.view(emb.size(0), emb.size(1), -1)\n        emb_remap = shape_transform(emb_remap)\n        out = self.cnn(emb_remap)\n\n        return emb_remap.squeeze(3).transpose(0, 1).contiguous(), \\\n            out.squeeze(3).transpose(0, 1).contiguous(), lengths\n\n    def update_dropout(self, dropout):\n        self.cnn.dropout.p = dropout\n'"
onmt/encoders/encoder.py,1,"b'""""""Base class for encoders and generic multi encoders.""""""\n\nimport torch.nn as nn\n\nfrom onmt.utils.misc import aeq\n\n\nclass EncoderBase(nn.Module):\n    """"""\n    Base encoder class. Specifies the interface used by different encoder types\n    and required by :class:`onmt.Models.NMTModel`.\n\n    .. mermaid::\n\n       graph BT\n          A[Input]\n          subgraph RNN\n            C[Pos 1]\n            D[Pos 2]\n            E[Pos N]\n          end\n          F[Memory_Bank]\n          G[Final]\n          A-->C\n          A-->D\n          A-->E\n          C-->F\n          D-->F\n          E-->F\n          E-->G\n    """"""\n\n    @classmethod\n    def from_opt(cls, opt, embeddings=None):\n        raise NotImplementedError\n\n    def _check_args(self, src, lengths=None, hidden=None):\n        n_batch = src.size(1)\n        if lengths is not None:\n            n_batch_, = lengths.size()\n            aeq(n_batch, n_batch_)\n\n    def forward(self, src, lengths=None):\n        """"""\n        Args:\n            src (LongTensor):\n               padded sequences of sparse indices ``(src_len, batch, nfeat)``\n            lengths (LongTensor): length of each sequence ``(batch,)``\n\n\n        Returns:\n            (FloatTensor, FloatTensor):\n\n            * final encoder state, used to initialize decoder\n            * memory bank for attention, ``(src_len, batch, hidden)``\n        """"""\n\n        raise NotImplementedError\n'"
onmt/encoders/ggnn_encoder.py,17,"b'""""""Define GGNN-based encoders.""""""\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom onmt.encoders.encoder import EncoderBase\n\n\nclass GGNNAttrProxy(object):\n    """"""\n    Translates index lookups into attribute lookups.\n    To implement some trick which able to use list of nn.Module in a nn.Module\n    see https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219/2\n    """"""\n    def __init__(self, module, prefix):\n        self.module = module\n        self.prefix = prefix\n\n    def __getitem__(self, i):\n        return getattr(self.module, self.prefix + str(i))\n\n\nclass GGNNPropogator(nn.Module):\n    """"""\n    Gated Propogator for GGNN\n    Using LSTM gating mechanism\n    """"""\n    def __init__(self, state_dim, n_node, n_edge_types):\n        super(GGNNPropogator, self).__init__()\n\n        self.n_node = n_node\n        self.n_edge_types = n_edge_types\n\n        self.reset_gate = nn.Sequential(\n            nn.Linear(state_dim*3, state_dim),\n            nn.Sigmoid()\n        )\n        self.update_gate = nn.Sequential(\n            nn.Linear(state_dim*3, state_dim),\n            nn.Sigmoid()\n        )\n        self.tansform = nn.Sequential(\n            nn.Linear(state_dim*3, state_dim),\n            nn.LeakyReLU()\n        )\n\n    def forward(self, state_in, state_out, state_cur, edges, nodes):\n        edges_in = edges[:, :, :nodes*self.n_edge_types]\n        edges_out = edges[:, :, nodes*self.n_edge_types:]\n\n        a_in = torch.bmm(edges_in, state_in)\n        a_out = torch.bmm(edges_out, state_out)\n        a = torch.cat((a_in, a_out, state_cur), 2)\n\n        r = self.reset_gate(a)\n        z = self.update_gate(a)\n        joined_input = torch.cat((a_in, a_out, r * state_cur), 2)\n        h_hat = self.tansform(joined_input)\n\n        output = (1 - z) * state_cur + z * h_hat\n\n        return output\n\n\nclass GGNNEncoder(EncoderBase):\n    """""" A gated graph neural network configured as an encoder.\n       Based on github.com/JamesChuanggg/ggnn.pytorch.git,\n       which is based on the paper ""Gated Graph Sequence Neural Networks""\n       by Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel.\n\n    Args:\n       rnn_type (str):\n          style of recurrent unit to use, one of [LSTM]\n       state_dim (int) : Number of state dimensions in nodes\n       n_edge_types (int) : Number of edge types\n       bidir_edges (bool): True if reverse edges should be autocreated\n       n_node (int) : Max nodes in graph\n       bridge_extra_node (bool): True indicates only 1st extra node\n          (after token listing) should be used for decoder init.\n       n_steps (int): Steps to advance graph encoder for stabilization\n       src_vocab (int): Path to source vocabulary\n    """"""\n\n    def __init__(self, rnn_type, state_dim, bidir_edges,\n                 n_edge_types, n_node, bridge_extra_node, n_steps, src_vocab):\n        super(GGNNEncoder, self).__init__()\n\n        self.state_dim = state_dim\n        self.n_edge_types = n_edge_types\n        self.n_node = n_node\n        self.n_steps = n_steps\n        self.bidir_edges = bidir_edges\n        self.bridge_extra_node = bridge_extra_node\n\n        for i in range(self.n_edge_types):\n            # incoming and outgoing edge embedding\n            in_fc = nn.Linear(self.state_dim, self.state_dim)\n            out_fc = nn.Linear(self.state_dim, self.state_dim)\n            self.add_module(""in_{}"".format(i), in_fc)\n            self.add_module(""out_{}"".format(i), out_fc)\n\n        self.in_fcs = GGNNAttrProxy(self, ""in_"")\n        self.out_fcs = GGNNAttrProxy(self, ""out_"")\n\n        # Find vocab data for tree builting\n        f = open(src_vocab, ""r"")\n        idx = 0\n        self.COMMA = -1\n        self.DELIMITER = -1\n        self.idx2num = []\n        for ln in f:\n            ln = ln.strip(\'\\n\')\n            if ln == "","":\n                self.COMMA = idx\n            if ln == ""<EOT>"":\n                self.DELIMITER = idx\n            if ln.isdigit():\n                self.idx2num.append(int(ln))\n            else:\n                self.idx2num.append(-1)\n            idx += 1\n\n        # Propogation Model\n        self.propogator = GGNNPropogator(self.state_dim, self.n_node,\n                                         self.n_edge_types)\n\n        self._initialization()\n\n        # Initialize the bridge layer\n        self._initialize_bridge(rnn_type, self.state_dim, 1)\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.rnn_type,\n            opt.state_dim,\n            opt.bidir_edges,\n            opt.n_edge_types,\n            opt.n_node,\n            opt.bridge_extra_node,\n            opt.n_steps,\n            opt.src_vocab)\n\n    def _initialization(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                m.weight.data.normal_(0.0, 0.02)\n                m.bias.data.fill_(0)\n\n    def forward(self, src, lengths=None):\n        """"""See :func:`EncoderBase.forward()`""""""\n        self._check_args(src, lengths)\n        nodes = self.n_node\n        batch_size = src.size()[1]\n        first_extra = np.zeros(batch_size, dtype=np.int32)\n        prop_state = np.zeros((batch_size, nodes, self.state_dim),\n                              dtype=np.int32)\n        edges = np.zeros((batch_size, nodes, nodes*self.n_edge_types*2),\n                         dtype=np.int32)\n        npsrc = src[:, :, 0].cpu().data.numpy().astype(np.int32)\n\n        # Initialize graph using formatted input sequence\n        for i in range(batch_size):\n            tokens_done = False\n            # Number of flagged nodes defines node count for this sample\n            # (Nodes can have no flags on them, but must be in \'flags\' list).\n            flags = 0\n            flags_done = False\n            edge = 0\n            source_node = -1\n            for j in range(len(npsrc)):\n                token = npsrc[j][i]\n                if not tokens_done:\n                    if token == self.DELIMITER:\n                        tokens_done = True\n                        first_extra[i] = j\n                    else:\n                        prop_state[i][j][token] = 1\n                elif token == self.DELIMITER:\n                    flags += 1\n                    flags_done = True\n                    assert flags <= nodes\n                elif not flags_done:\n                    # The total number of integers in the vocab should allow\n                    # for all features and edges to be defined.\n                    if token == self.COMMA:\n                        flags = 0\n                    else:\n                        num = self.idx2num[token]\n                        if num >= 0:\n                            prop_state[i][flags][num+self.DELIMITER] = 1\n                        flags += 1\n                elif token == self.COMMA:\n                    edge += 1\n                    assert source_node == -1, \'Error in graph edge input\'\n                    assert (edge <= 2*self.n_edge_types and\n                            (not self.bidir_edges or edge < self.n_edge_types))\n                else:\n                    num = self.idx2num[token]\n                    if source_node < 0:\n                        source_node = num\n                    else:\n                        edges[i][source_node][num+nodes*edge] = 1\n                        if self.bidir_edges:\n                            edges[i][num][nodes*(edge+self.n_edge_types)\n                                          + source_node] = 1\n                        source_node = -1\n\n        if torch.cuda.is_available():\n            prop_state = torch.from_numpy(prop_state).float().to(""cuda:0"")\n            edges = torch.from_numpy(edges).float().to(""cuda:0"")\n        else:\n            prop_state = torch.from_numpy(prop_state).float()\n            edges = torch.from_numpy(edges).float()\n\n        for i_step in range(self.n_steps):\n            in_states = []\n            out_states = []\n            for i in range(self.n_edge_types):\n                in_states.append(self.in_fcs[i](prop_state))\n                out_states.append(self.out_fcs[i](prop_state))\n            in_states = torch.stack(in_states).transpose(0, 1).contiguous()\n            in_states = in_states.view(-1, nodes*self.n_edge_types,\n                                       self.state_dim)\n            out_states = torch.stack(out_states).transpose(0, 1).contiguous()\n            out_states = out_states.view(-1, nodes*self.n_edge_types,\n                                         self.state_dim)\n\n            prop_state = self.propogator(in_states, out_states, prop_state,\n                                         edges, nodes)\n\n        prop_state = prop_state.transpose(0, 1)\n        if self.bridge_extra_node:\n            # Use first extra node as only source for decoder init\n            join_state = prop_state[first_extra, torch.arange(batch_size)]\n        else:\n            # Average all nodes to get bridge input\n            join_state = prop_state.mean(0)\n        join_state = torch.stack((join_state, join_state,\n                                  join_state, join_state))\n        join_state = (join_state, join_state)\n\n        encoder_final = self._bridge(join_state)\n\n        return encoder_final, prop_state, lengths\n\n    def _initialize_bridge(self, rnn_type,\n                           hidden_size,\n                           num_layers):\n\n        # LSTM has hidden and cell state, other only one\n        number_of_states = 2 if rnn_type == ""LSTM"" else 1\n        # Total number of states\n        self.total_hidden_dim = hidden_size * num_layers\n\n        # Build a linear layer for each\n        self.bridge = nn.ModuleList([nn.Linear(self.total_hidden_dim,\n                                               self.total_hidden_dim,\n                                               bias=True)\n                                     for _ in range(number_of_states)])\n\n    def _bridge(self, hidden):\n        """"""Forward hidden state through bridge.""""""\n        def bottle_hidden(linear, states):\n            """"""\n            Transform from 3D to 2D, apply linear and return initial size\n            """"""\n            size = states.size()\n            result = linear(states.view(-1, self.total_hidden_dim))\n            return F.leaky_relu(result).view(size)\n\n        if isinstance(hidden, tuple):  # LSTM\n            outs = tuple([bottle_hidden(layer, hidden[ix])\n                          for ix, layer in enumerate(self.bridge)])\n        else:\n            outs = bottle_hidden(self.bridge[0], hidden)\n        return outs\n'"
onmt/encoders/image_encoder.py,5,"b'""""""Image Encoder.""""""\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nfrom onmt.encoders.encoder import EncoderBase\n\n\nclass ImageEncoder(EncoderBase):\n    """"""A simple encoder CNN -> RNN for image src.\n\n    Args:\n        num_layers (int): number of encoder layers.\n        bidirectional (bool): bidirectional encoder.\n        rnn_size (int): size of hidden states of the rnn.\n        dropout (float): dropout probablity.\n    """"""\n\n    def __init__(self, num_layers, bidirectional, rnn_size, dropout,\n                 image_chanel_size=3):\n        super(ImageEncoder, self).__init__()\n        self.num_layers = num_layers\n        self.num_directions = 2 if bidirectional else 1\n        self.hidden_size = rnn_size\n\n        self.layer1 = nn.Conv2d(image_chanel_size, 64, kernel_size=(3, 3),\n                                padding=(1, 1), stride=(1, 1))\n        self.layer2 = nn.Conv2d(64, 128, kernel_size=(3, 3),\n                                padding=(1, 1), stride=(1, 1))\n        self.layer3 = nn.Conv2d(128, 256, kernel_size=(3, 3),\n                                padding=(1, 1), stride=(1, 1))\n        self.layer4 = nn.Conv2d(256, 256, kernel_size=(3, 3),\n                                padding=(1, 1), stride=(1, 1))\n        self.layer5 = nn.Conv2d(256, 512, kernel_size=(3, 3),\n                                padding=(1, 1), stride=(1, 1))\n        self.layer6 = nn.Conv2d(512, 512, kernel_size=(3, 3),\n                                padding=(1, 1), stride=(1, 1))\n\n        self.batch_norm1 = nn.BatchNorm2d(256)\n        self.batch_norm2 = nn.BatchNorm2d(512)\n        self.batch_norm3 = nn.BatchNorm2d(512)\n\n        src_size = 512\n        dropout = dropout[0] if type(dropout) is list else dropout\n        self.rnn = nn.LSTM(src_size, int(rnn_size / self.num_directions),\n                           num_layers=num_layers,\n                           dropout=dropout,\n                           bidirectional=bidirectional)\n        self.pos_lut = nn.Embedding(1000, src_size)\n\n    @classmethod\n    def from_opt(cls, opt, embeddings=None):\n        """"""Alternate constructor.""""""\n        if embeddings is not None:\n            raise ValueError(""Cannot use embeddings with ImageEncoder."")\n        # why is the model_opt.__dict__ check necessary?\n        if ""image_channel_size"" not in opt.__dict__:\n            image_channel_size = 3\n        else:\n            image_channel_size = opt.image_channel_size\n        return cls(\n            opt.enc_layers,\n            opt.brnn,\n            opt.enc_rnn_size,\n            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n            image_channel_size\n        )\n\n    def load_pretrained_vectors(self, opt):\n        """"""Pass in needed options only when modify function definition.""""""\n        pass\n\n    def forward(self, src, lengths=None):\n        """"""See :func:`onmt.encoders.encoder.EncoderBase.forward()`""""""\n\n        batch_size = src.size(0)\n        # (batch_size, 64, imgH, imgW)\n        # layer 1\n        src = F.relu(self.layer1(src[:, :, :, :] - 0.5), True)\n\n        # (batch_size, 64, imgH/2, imgW/2)\n        src = F.max_pool2d(src, kernel_size=(2, 2), stride=(2, 2))\n\n        # (batch_size, 128, imgH/2, imgW/2)\n        # layer 2\n        src = F.relu(self.layer2(src), True)\n\n        # (batch_size, 128, imgH/2/2, imgW/2/2)\n        src = F.max_pool2d(src, kernel_size=(2, 2), stride=(2, 2))\n\n        #  (batch_size, 256, imgH/2/2, imgW/2/2)\n        # layer 3\n        # batch norm 1\n        src = F.relu(self.batch_norm1(self.layer3(src)), True)\n\n        # (batch_size, 256, imgH/2/2, imgW/2/2)\n        # layer4\n        src = F.relu(self.layer4(src), True)\n\n        # (batch_size, 256, imgH/2/2/2, imgW/2/2)\n        src = F.max_pool2d(src, kernel_size=(1, 2), stride=(1, 2))\n\n        # (batch_size, 512, imgH/2/2/2, imgW/2/2)\n        # layer 5\n        # batch norm 2\n        src = F.relu(self.batch_norm2(self.layer5(src)), True)\n\n        # (batch_size, 512, imgH/2/2/2, imgW/2/2/2)\n        src = F.max_pool2d(src, kernel_size=(2, 1), stride=(2, 1))\n\n        # (batch_size, 512, imgH/2/2/2, imgW/2/2/2)\n        src = F.relu(self.batch_norm3(self.layer6(src)), True)\n\n        # # (batch_size, 512, H, W)\n        all_outputs = []\n        for row in range(src.size(2)):\n            inp = src[:, :, row, :].transpose(0, 2) \\\n                .transpose(1, 2)\n            row_vec = torch.Tensor(batch_size).type_as(inp.data) \\\n                .long().fill_(row)\n            pos_emb = self.pos_lut(row_vec)\n            with_pos = torch.cat(\n                (pos_emb.view(1, pos_emb.size(0), pos_emb.size(1)), inp), 0)\n            outputs, hidden_t = self.rnn(with_pos)\n            all_outputs.append(outputs)\n        out = torch.cat(all_outputs, 0)\n\n        return hidden_t, out, lengths\n\n    def update_dropout(self, dropout):\n        self.rnn.dropout = dropout\n'"
onmt/encoders/mean_encoder.py,1,"b'""""""Define a minimal encoder.""""""\nfrom onmt.encoders.encoder import EncoderBase\nfrom onmt.utils.misc import sequence_mask\nimport torch\n\n\nclass MeanEncoder(EncoderBase):\n    """"""A trivial non-recurrent encoder. Simply applies mean pooling.\n\n    Args:\n       num_layers (int): number of replicated layers\n       embeddings (onmt.modules.Embeddings): embedding module to use\n    """"""\n\n    def __init__(self, num_layers, embeddings):\n        super(MeanEncoder, self).__init__()\n        self.num_layers = num_layers\n        self.embeddings = embeddings\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.enc_layers,\n            embeddings)\n\n    def forward(self, src, lengths=None):\n        """"""See :func:`EncoderBase.forward()`""""""\n        self._check_args(src, lengths)\n\n        emb = self.embeddings(src)\n        _, batch, emb_dim = emb.size()\n\n        if lengths is not None:\n            # we avoid padding while mean pooling\n            mask = sequence_mask(lengths).float()\n            mask = mask / lengths.unsqueeze(1).float()\n            mean = torch.bmm(mask.unsqueeze(1), emb.transpose(0, 1)).squeeze(1)\n        else:\n            mean = emb.mean(0)\n\n        mean = mean.expand(self.num_layers, batch, emb_dim)\n        memory_bank = emb\n        encoder_final = (mean, mean)\n        return encoder_final, memory_bank, lengths\n'"
onmt/encoders/rnn_encoder.py,5,"b'""""""Define RNN-based encoders.""""""\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.utils.rnn import pack_padded_sequence as pack\nfrom torch.nn.utils.rnn import pad_packed_sequence as unpack\n\nfrom onmt.encoders.encoder import EncoderBase\nfrom onmt.utils.rnn_factory import rnn_factory\n\n\nclass RNNEncoder(EncoderBase):\n    """""" A generic recurrent neural network encoder.\n\n    Args:\n       rnn_type (str):\n          style of recurrent unit to use, one of [RNN, LSTM, GRU, SRU]\n       bidirectional (bool) : use a bidirectional RNN\n       num_layers (int) : number of stacked layers\n       hidden_size (int) : hidden size of each layer\n       dropout (float) : dropout value for :class:`torch.nn.Dropout`\n       embeddings (onmt.modules.Embeddings): embedding module to use\n    """"""\n\n    def __init__(self, rnn_type, bidirectional, num_layers,\n                 hidden_size, dropout=0.0, embeddings=None,\n                 use_bridge=False):\n        super(RNNEncoder, self).__init__()\n        assert embeddings is not None\n\n        num_directions = 2 if bidirectional else 1\n        assert hidden_size % num_directions == 0\n        hidden_size = hidden_size // num_directions\n        self.embeddings = embeddings\n\n        self.rnn, self.no_pack_padded_seq = \\\n            rnn_factory(rnn_type,\n                        input_size=embeddings.embedding_size,\n                        hidden_size=hidden_size,\n                        num_layers=num_layers,\n                        dropout=dropout,\n                        bidirectional=bidirectional)\n\n        # Initialize the bridge layer\n        self.use_bridge = use_bridge\n        if self.use_bridge:\n            self._initialize_bridge(rnn_type,\n                                    hidden_size,\n                                    num_layers)\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.rnn_type,\n            opt.brnn,\n            opt.enc_layers,\n            opt.enc_rnn_size,\n            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n            embeddings,\n            opt.bridge)\n\n    def forward(self, src, lengths=None):\n        """"""See :func:`EncoderBase.forward()`""""""\n        self._check_args(src, lengths)\n\n        emb = self.embeddings(src)\n        # s_len, batch, emb_dim = emb.size()\n\n        packed_emb = emb\n        if lengths is not None and not self.no_pack_padded_seq:\n            # Lengths data is wrapped inside a Tensor.\n            lengths_list = lengths.view(-1).tolist()\n            packed_emb = pack(emb, lengths_list)\n\n        memory_bank, encoder_final = self.rnn(packed_emb)\n\n        if lengths is not None and not self.no_pack_padded_seq:\n            memory_bank = unpack(memory_bank)[0]\n\n        if self.use_bridge:\n            encoder_final = self._bridge(encoder_final)\n        return encoder_final, memory_bank, lengths\n\n    def _initialize_bridge(self, rnn_type,\n                           hidden_size,\n                           num_layers):\n\n        # LSTM has hidden and cell state, other only one\n        number_of_states = 2 if rnn_type == ""LSTM"" else 1\n        # Total number of states\n        self.total_hidden_dim = hidden_size * num_layers\n\n        # Build a linear layer for each\n        self.bridge = nn.ModuleList([nn.Linear(self.total_hidden_dim,\n                                               self.total_hidden_dim,\n                                               bias=True)\n                                     for _ in range(number_of_states)])\n\n    def _bridge(self, hidden):\n        """"""Forward hidden state through bridge.""""""\n        def bottle_hidden(linear, states):\n            """"""\n            Transform from 3D to 2D, apply linear and return initial size\n            """"""\n            size = states.size()\n            result = linear(states.view(-1, self.total_hidden_dim))\n            return F.relu(result).view(size)\n\n        if isinstance(hidden, tuple):  # LSTM\n            outs = tuple([bottle_hidden(layer, hidden[ix])\n                          for ix, layer in enumerate(self.bridge)])\n        else:\n            outs = bottle_hidden(self.bridge[0], hidden)\n        return outs\n\n    def update_dropout(self, dropout):\n        self.rnn.dropout = dropout\n'"
onmt/encoders/transformer.py,2,"b'""""""\nImplementation of ""Attention is All You Need""\n""""""\n\nimport torch.nn as nn\n\nfrom onmt.encoders.encoder import EncoderBase\nfrom onmt.modules import MultiHeadedAttention\nfrom onmt.modules.position_ffn import PositionwiseFeedForward\nfrom onmt.utils.misc import sequence_mask\n\n\nclass TransformerEncoderLayer(nn.Module):\n    """"""\n    A single layer of the transformer encoder.\n\n    Args:\n        d_model (int): the dimension of keys/values/queries in\n                   MultiHeadedAttention, also the input size of\n                   the first-layer of the PositionwiseFeedForward.\n        heads (int): the number of head for MultiHeadedAttention.\n        d_ff (int): the second-layer of the PositionwiseFeedForward.\n        dropout (float): dropout probability(0-1.0).\n    """"""\n\n    def __init__(self, d_model, heads, d_ff, dropout, attention_dropout,\n                 max_relative_positions=0):\n        super(TransformerEncoderLayer, self).__init__()\n\n        self.self_attn = MultiHeadedAttention(\n            heads, d_model, dropout=attention_dropout,\n            max_relative_positions=max_relative_positions)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, inputs, mask):\n        """"""\n        Args:\n            inputs (FloatTensor): ``(batch_size, src_len, model_dim)``\n            mask (LongTensor): ``(batch_size, 1, src_len)``\n\n        Returns:\n            (FloatTensor):\n\n            * outputs ``(batch_size, src_len, model_dim)``\n        """"""\n        input_norm = self.layer_norm(inputs)\n        context, _ = self.self_attn(input_norm, input_norm, input_norm,\n                                    mask=mask, attn_type=""self"")\n        out = self.dropout(context) + inputs\n        return self.feed_forward(out)\n\n    def update_dropout(self, dropout, attention_dropout):\n        self.self_attn.update_dropout(attention_dropout)\n        self.feed_forward.update_dropout(dropout)\n        self.dropout.p = dropout\n\n\nclass TransformerEncoder(EncoderBase):\n    """"""The Transformer encoder from ""Attention is All You Need""\n    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`\n\n    .. mermaid::\n\n       graph BT\n          A[input]\n          B[multi-head self-attn]\n          C[feed forward]\n          O[output]\n          A --> B\n          B --> C\n          C --> O\n\n    Args:\n        num_layers (int): number of encoder layers\n        d_model (int): size of the model\n        heads (int): number of heads\n        d_ff (int): size of the inner FF layer\n        dropout (float): dropout parameters\n        embeddings (onmt.modules.Embeddings):\n          embeddings to use, should have positional encodings\n\n    Returns:\n        (torch.FloatTensor, torch.FloatTensor):\n\n        * embeddings ``(src_len, batch_size, model_dim)``\n        * memory_bank ``(src_len, batch_size, model_dim)``\n    """"""\n\n    def __init__(self, num_layers, d_model, heads, d_ff, dropout,\n                 attention_dropout, embeddings, max_relative_positions):\n        super(TransformerEncoder, self).__init__()\n\n        self.embeddings = embeddings\n        self.transformer = nn.ModuleList(\n            [TransformerEncoderLayer(\n                d_model, heads, d_ff, dropout, attention_dropout,\n                max_relative_positions=max_relative_positions)\n             for i in range(num_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    @classmethod\n    def from_opt(cls, opt, embeddings):\n        """"""Alternate constructor.""""""\n        return cls(\n            opt.enc_layers,\n            opt.enc_rnn_size,\n            opt.heads,\n            opt.transformer_ff,\n            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n            opt.attention_dropout[0] if type(opt.attention_dropout)\n            is list else opt.attention_dropout,\n            embeddings,\n            opt.max_relative_positions)\n\n    def forward(self, src, lengths=None):\n        """"""See :func:`EncoderBase.forward()`""""""\n        self._check_args(src, lengths)\n\n        emb = self.embeddings(src)\n\n        out = emb.transpose(0, 1).contiguous()\n        mask = ~sequence_mask(lengths).unsqueeze(1)\n        # Run the forward pass of every layer of the tranformer.\n        for layer in self.transformer:\n            out = layer(out, mask)\n        out = self.layer_norm(out)\n\n        return emb, out.transpose(0, 1).contiguous(), lengths\n\n    def update_dropout(self, dropout, attention_dropout):\n        self.embeddings.update_dropout(dropout)\n        for layer in self.transformer:\n            layer.update_dropout(dropout, attention_dropout)\n'"
onmt/inputters/__init__.py,0,"b'""""""Module defining inputters.\n\nInputters implement the logic of transforming raw data to vectorized inputs,\ne.g., from a line of text to a sequence of embeddings.\n""""""\nfrom onmt.inputters.inputter import \\\n    load_old_vocab, get_fields, OrderedIterator, \\\n    build_vocab, old_style_vocab, filter_example\nfrom onmt.inputters.dataset_base import Dataset\nfrom onmt.inputters.text_dataset import text_sort_key, TextDataReader\nfrom onmt.inputters.image_dataset import img_sort_key, ImageDataReader\nfrom onmt.inputters.audio_dataset import audio_sort_key, AudioDataReader\nfrom onmt.inputters.vec_dataset import vec_sort_key, VecDataReader\nfrom onmt.inputters.datareader_base import DataReaderBase\n\nstr2reader = {\n    ""text"": TextDataReader, ""img"": ImageDataReader, ""audio"": AudioDataReader,\n    ""vec"": VecDataReader}\nstr2sortkey = {\n    \'text\': text_sort_key, \'img\': img_sort_key, \'audio\': audio_sort_key,\n    \'vec\': vec_sort_key}\n\n\n__all__ = [\'Dataset\', \'load_old_vocab\', \'get_fields\', \'DataReaderBase\',\n           \'filter_example\', \'old_style_vocab\',\n           \'build_vocab\', \'OrderedIterator\',\n           \'text_sort_key\', \'img_sort_key\', \'audio_sort_key\', \'vec_sort_key\',\n           \'TextDataReader\', \'ImageDataReader\', \'AudioDataReader\',\n           \'VecDataReader\']\n'"
onmt/inputters/audio_dataset.py,8,"b'# -*- coding: utf-8 -*-\nimport os\nfrom tqdm import tqdm\n\nimport torch\nfrom torchtext.data import Field\n\nfrom onmt.inputters.datareader_base import DataReaderBase\n\n# imports of datatype-specific dependencies\ntry:\n    import torchaudio\n    import librosa\n    import numpy as np\nexcept ImportError:\n    torchaudio, librosa, np = None, None, None\n\n\nclass AudioDataReader(DataReaderBase):\n    """"""Read audio data from disk.\n\n    Args:\n        sample_rate (int): sample_rate.\n        window_size (float) : window size for spectrogram in seconds.\n        window_stride (float): window stride for spectrogram in seconds.\n        window (str): window type for spectrogram generation. See\n            :func:`librosa.stft()` ``window`` for more details.\n        normalize_audio (bool): subtract spectrogram by mean and divide\n            by std or not.\n        truncate (int or NoneType): maximum audio length\n            (0 or None for unlimited).\n\n    Raises:\n        onmt.inputters.datareader_base.MissingDependencyException: If\n            importing any of ``torchaudio``, ``librosa``, or ``numpy`` fail.\n    """"""\n\n    def __init__(self, sample_rate=0, window_size=0, window_stride=0,\n                 window=None, normalize_audio=True, truncate=None):\n        self._check_deps()\n        self.sample_rate = sample_rate\n        self.window_size = window_size\n        self.window_stride = window_stride\n        self.window = window\n        self.normalize_audio = normalize_audio\n        self.truncate = truncate\n\n    @classmethod\n    def from_opt(cls, opt):\n        return cls(sample_rate=opt.sample_rate, window_size=opt.window_size,\n                   window_stride=opt.window_stride, window=opt.window)\n\n    @classmethod\n    def _check_deps(cls):\n        if any([torchaudio is None, librosa is None, np is None]):\n            cls._raise_missing_dep(\n                ""torchaudio"", ""librosa"", ""numpy"")\n\n    def extract_features(self, audio_path):\n        # torchaudio loading options recently changed. It\'s probably\n        # straightforward to rewrite the audio handling to make use of\n        # up-to-date torchaudio, but in the meantime there is a legacy\n        # method which uses the old defaults\n        sound, sample_rate_ = torchaudio.legacy.load(audio_path)\n        if self.truncate and self.truncate > 0:\n            if sound.size(0) > self.truncate:\n                sound = sound[:self.truncate]\n\n        assert sample_rate_ == self.sample_rate, \\\n            \'Sample rate of %s != -sample_rate (%d vs %d)\' \\\n            % (audio_path, sample_rate_, self.sample_rate)\n\n        sound = sound.numpy()\n        if len(sound.shape) > 1:\n            if sound.shape[1] == 1:\n                sound = sound.squeeze()\n            else:\n                sound = sound.mean(axis=1)  # average multiple channels\n\n        n_fft = int(self.sample_rate * self.window_size)\n        win_length = n_fft\n        hop_length = int(self.sample_rate * self.window_stride)\n        # STFT\n        d = librosa.stft(sound, n_fft=n_fft, hop_length=hop_length,\n                         win_length=win_length, window=self.window)\n        spect, _ = librosa.magphase(d)\n        spect = np.log1p(spect)\n        spect = torch.FloatTensor(spect)\n        if self.normalize_audio:\n            mean = spect.mean()\n            std = spect.std()\n            spect.add_(-mean)\n            spect.div_(std)\n        return spect\n\n    def read(self, data, side, src_dir=None):\n        """"""Read data into dicts.\n\n        Args:\n            data (str or Iterable[str]): Sequence of audio paths or\n                path to file containing audio paths.\n                In either case, the filenames may be relative to ``src_dir``\n                (default behavior) or absolute.\n            side (str): Prefix used in return dict. Usually\n                ``""src""`` or ``""tgt""``.\n            src_dir (str): Location of source audio files. See ``data``.\n\n        Yields:\n            A dictionary containing audio data for each line.\n        """"""\n\n        assert src_dir is not None and os.path.exists(src_dir),\\\n            ""src_dir must be a valid directory if data_type is audio""\n\n        if isinstance(data, str):\n            data = DataReaderBase._read_file(data)\n\n        for i, line in enumerate(tqdm(data)):\n            line = line.decode(""utf-8"").strip()\n            audio_path = os.path.join(src_dir, line)\n            if not os.path.exists(audio_path):\n                audio_path = line\n\n            assert os.path.exists(audio_path), \\\n                \'audio path %s not found\' % line\n\n            spect = self.extract_features(audio_path)\n            yield {side: spect, side + \'_path\': line, \'indices\': i}\n\n\ndef audio_sort_key(ex):\n    """"""Sort using duration time of the sound spectrogram.""""""\n    return ex.src.size(1)\n\n\nclass AudioSeqField(Field):\n    """"""Defines an audio datatype and instructions for converting to Tensor.\n\n    See :class:`Fields` for attribute descriptions.\n    """"""\n\n    def __init__(self, preprocessing=None, postprocessing=None,\n                 include_lengths=False, batch_first=False, pad_index=0,\n                 is_target=False):\n        super(AudioSeqField, self).__init__(\n            sequential=True, use_vocab=False, init_token=None,\n            eos_token=None, fix_length=False, dtype=torch.float,\n            preprocessing=preprocessing, postprocessing=postprocessing,\n            lower=False, tokenize=None, include_lengths=include_lengths,\n            batch_first=batch_first, pad_token=pad_index, unk_token=None,\n            pad_first=False, truncate_first=False, stop_words=None,\n            is_target=is_target\n        )\n\n    def pad(self, minibatch):\n        """"""Pad a batch of examples to the length of the longest example.\n\n        Args:\n            minibatch (List[torch.FloatTensor]): A list of audio data,\n                each having shape 1 x n_feats x len where len is variable.\n\n        Returns:\n            torch.FloatTensor or Tuple[torch.FloatTensor, List[int]]: The\n                padded tensor of shape ``(batch_size, 1, n_feats, max_len)``.\n                and a list of the lengths if `self.include_lengths` is `True`\n                else just returns the padded tensor.\n        """"""\n\n        assert not self.pad_first and not self.truncate_first \\\n            and not self.fix_length and self.sequential\n        minibatch = list(minibatch)\n        lengths = [x.size(1) for x in minibatch]\n        max_len = max(lengths)\n        nfft = minibatch[0].size(0)\n        sounds = torch.full((len(minibatch), 1, nfft, max_len), self.pad_token)\n        for i, (spect, len_) in enumerate(zip(minibatch, lengths)):\n            sounds[i, :, :, 0:len_] = spect\n        if self.include_lengths:\n            return (sounds, lengths)\n        return sounds\n\n    def numericalize(self, arr, device=None):\n        """"""Turn a batch of examples that use this field into a Variable.\n\n        If the field has ``include_lengths=True``, a tensor of lengths will be\n        included in the return value.\n\n        Args:\n            arr (torch.FloatTensor or Tuple(torch.FloatTensor, List[int])):\n                List of tokenized and padded examples, or tuple of List of\n                tokenized and padded examples and List of lengths of each\n                example if self.include_lengths is True. Examples have shape\n                ``(batch_size, 1, n_feats, max_len)`` if `self.batch_first`\n                else ``(max_len, batch_size, 1, n_feats)``.\n            device (str or torch.device): See `Field.numericalize`.\n        """"""\n\n        assert self.use_vocab is False\n        if self.include_lengths and not isinstance(arr, tuple):\n            raise ValueError(""Field has include_lengths set to True, but ""\n                             ""input data is not a tuple of ""\n                             ""(data batch, batch lengths)."")\n        if isinstance(arr, tuple):\n            arr, lengths = arr\n            lengths = torch.tensor(lengths, dtype=torch.int, device=device)\n\n        if self.postprocessing is not None:\n            arr = self.postprocessing(arr, None)\n\n        if self.sequential and not self.batch_first:\n            arr = arr.permute(3, 0, 1, 2)\n        if self.sequential:\n            arr = arr.contiguous()\n        arr = arr.to(device)\n        if self.include_lengths:\n            return arr, lengths\n        return arr\n\n\ndef audio_fields(**kwargs):\n    audio = AudioSeqField(pad_index=0, batch_first=True, include_lengths=True)\n    return audio\n'"
onmt/inputters/datareader_base.py,0,"b'# coding: utf-8\n\n\n# several data readers need optional dependencies. There\'s no\n# appropriate builtin exception\nclass MissingDependencyException(Exception):\n    pass\n\n\nclass DataReaderBase(object):\n    """"""Read data from file system and yield as dicts.\n\n    Raises:\n        onmt.inputters.datareader_base.MissingDependencyException: A number\n            of DataReaders need specific additional packages.\n            If any are missing, this will be raised.\n    """"""\n\n    @classmethod\n    def from_opt(cls, opt):\n        """"""Alternative constructor.\n\n        Args:\n            opt (argparse.Namespace): The parsed arguments.\n        """"""\n\n        return cls()\n\n    @classmethod\n    def _read_file(cls, path):\n        """"""Line-by-line read a file as bytes.""""""\n        with open(path, ""rb"") as f:\n            for line in f:\n                yield line\n\n    @staticmethod\n    def _raise_missing_dep(*missing_deps):\n        """"""Raise missing dep exception with standard error message.""""""\n        raise MissingDependencyException(\n            ""Could not create reader. Be sure to install ""\n            ""the following dependencies: "" + "", "".join(missing_deps))\n\n    def read(self, data, side, src_dir):\n        """"""Read data from file system and yield as dicts.""""""\n        raise NotImplementedError()\n'"
onmt/inputters/dataset_base.py,3,"b'# coding: utf-8\n\nfrom itertools import chain, starmap\nfrom collections import Counter\n\nimport torch\nfrom torchtext.data import Dataset as TorchtextDataset\nfrom torchtext.data import Example\nfrom torchtext.vocab import Vocab\n\n\ndef _join_dicts(*args):\n    """"""\n    Args:\n        dictionaries with disjoint keys.\n\n    Returns:\n        a single dictionary that has the union of these keys.\n    """"""\n\n    return dict(chain(*[d.items() for d in args]))\n\n\ndef _dynamic_dict(example, src_field, tgt_field):\n    """"""Create copy-vocab and numericalize with it.\n\n    In-place adds ``""src_map""`` to ``example``. That is the copy-vocab\n    numericalization of the tokenized ``example[""src""]``. If ``example``\n    has a ``""tgt""`` key, adds ``""alignment""`` to example. That is the\n    copy-vocab numericalization of the tokenized ``example[""tgt""]``. The\n    alignment has an initial and final UNK token to match the BOS and EOS\n    tokens.\n\n    Args:\n        example (dict): An example dictionary with a ``""src""`` key and\n            maybe a ``""tgt""`` key. (This argument changes in place!)\n        src_field (torchtext.data.Field): Field object.\n        tgt_field (torchtext.data.Field): Field object.\n\n    Returns:\n        torchtext.data.Vocab and ``example``, changed as described.\n    """"""\n\n    src = src_field.tokenize(example[""src""])\n    # make a small vocab containing just the tokens in the source sequence\n    unk = src_field.unk_token\n    pad = src_field.pad_token\n    src_ex_vocab = Vocab(Counter(src), specials=[unk, pad])\n    unk_idx = src_ex_vocab.stoi[unk]\n    # Map source tokens to indices in the dynamic dict.\n    src_map = torch.LongTensor([src_ex_vocab.stoi[w] for w in src])\n    example[""src_map""] = src_map\n    example[""src_ex_vocab""] = src_ex_vocab\n\n    if ""tgt"" in example:\n        tgt = tgt_field.tokenize(example[""tgt""])\n        mask = torch.LongTensor(\n            [unk_idx] + [src_ex_vocab.stoi[w] for w in tgt] + [unk_idx])\n        example[""alignment""] = mask\n    return src_ex_vocab, example\n\n\nclass Dataset(TorchtextDataset):\n    """"""Contain data and process it.\n\n    A dataset is an object that accepts sequences of raw data (sentence pairs\n    in the case of machine translation) and fields which describe how this\n    raw data should be processed to produce tensors. When a dataset is\n    instantiated, it applies the fields\' preprocessing pipeline (but not\n    the bit that numericalizes it or turns it into batch tensors) to the raw\n    data, producing a list of :class:`torchtext.data.Example` objects.\n    torchtext\'s iterators then know how to use these examples to make batches.\n\n    Args:\n        fields (dict[str, Field]): a dict with the structure\n            returned by :func:`onmt.inputters.get_fields()`. Usually\n            that means the dataset side, ``""src""`` or ``""tgt""``. Keys match\n            the keys of items yielded by the ``readers``, while values\n            are lists of (name, Field) pairs. An attribute with this\n            name will be created for each :class:`torchtext.data.Example`\n            object and its value will be the result of applying the Field\n            to the data that matches the key. The advantage of having\n            sequences of fields for each piece of raw input is that it allows\n            the dataset to store multiple ""views"" of each input, which allows\n            for easy implementation of token-level features, mixed word-\n            and character-level models, and so on. (See also\n            :class:`onmt.inputters.TextMultiField`.)\n        readers (Iterable[onmt.inputters.DataReaderBase]): Reader objects\n            for disk-to-dict. The yielded dicts are then processed\n            according to ``fields``.\n        data (Iterable[Tuple[str, Any]]): (name, ``data_arg``) pairs\n            where ``data_arg`` is passed to the ``read()`` method of the\n            reader in ``readers`` at that position. (See the reader object for\n            details on the ``Any`` type.)\n        dirs (Iterable[str or NoneType]): A list of directories where\n            data is contained. See the reader object for more details.\n        sort_key (Callable[[torchtext.data.Example], Any]): A function\n            for determining the value on which data is sorted (i.e. length).\n        filter_pred (Callable[[torchtext.data.Example], bool]): A function\n            that accepts Example objects and returns a boolean value\n            indicating whether to include that example in the dataset.\n\n    Attributes:\n        src_vocabs (List[torchtext.data.Vocab]): Used with dynamic dict/copy\n            attention. There is a very short vocab for each src example.\n            It contains just the source words, e.g. so that the generator can\n            predict to copy them.\n    """"""\n\n    def __init__(self, fields, readers, data, dirs, sort_key,\n                 filter_pred=None, corpus_id=None):\n        self.sort_key = sort_key\n        can_copy = \'src_map\' in fields and \'alignment\' in fields\n\n        read_iters = [r.read(dat[1], dat[0], dir_) for r, dat, dir_\n                      in zip(readers, data, dirs)]\n\n        # self.src_vocabs is used in collapse_copy_scores and Translator.py\n        self.src_vocabs = []\n        examples = []\n        for ex_dict in starmap(_join_dicts, zip(*read_iters)):\n            if corpus_id is not None:\n                ex_dict[""corpus_id""] = corpus_id\n            else:\n                ex_dict[""corpus_id""] = ""train""\n            if can_copy:\n                src_field = fields[\'src\']\n                tgt_field = fields[\'tgt\']\n                # this assumes src_field and tgt_field are both text\n                src_ex_vocab, ex_dict = _dynamic_dict(\n                    ex_dict, src_field.base_field, tgt_field.base_field)\n                self.src_vocabs.append(src_ex_vocab)\n            ex_fields = {k: [(k, v)] for k, v in fields.items() if\n                         k in ex_dict}\n            ex = Example.fromdict(ex_dict, ex_fields)\n            examples.append(ex)\n\n        # fields needs to have only keys that examples have as attrs\n        fields = []\n        for _, nf_list in ex_fields.items():\n            assert len(nf_list) == 1\n            fields.append(nf_list[0])\n\n        super(Dataset, self).__init__(examples, fields, filter_pred)\n\n    def __getattr__(self, attr):\n        # avoid infinite recursion when fields isn\'t defined\n        if \'fields\' not in vars(self):\n            raise AttributeError\n        if attr in self.fields:\n            return (getattr(x, attr) for x in self.examples)\n        else:\n            raise AttributeError\n\n    def save(self, path, remove_fields=True):\n        if remove_fields:\n            self.fields = []\n        torch.save(self, path)\n\n    @staticmethod\n    def config(fields):\n        readers, data, dirs = [], [], []\n        for name, field in fields:\n            if field[""data""] is not None:\n                readers.append(field[""reader""])\n                data.append((name, field[""data""]))\n                dirs.append(field[""dir""])\n        return readers, data, dirs\n'"
onmt/inputters/image_dataset.py,2,"b'# -*- coding: utf-8 -*-\n\nimport os\n\nimport torch\nfrom torchtext.data import Field\n\nfrom onmt.inputters.datareader_base import DataReaderBase\n\n# domain specific dependencies\ntry:\n    from PIL import Image\n    from torchvision import transforms\n    import cv2\nexcept ImportError:\n    Image, transforms, cv2 = None, None, None\n\n\nclass ImageDataReader(DataReaderBase):\n    """"""Read image data from disk.\n\n    Args:\n        truncate (tuple[int] or NoneType): maximum img size. Use\n            ``(0,0)`` or ``None`` for unlimited.\n        channel_size (int): Number of channels per image.\n\n    Raises:\n        onmt.inputters.datareader_base.MissingDependencyException: If\n            importing any of ``PIL``, ``torchvision``, or ``cv2`` fail.\n    """"""\n\n    def __init__(self, truncate=None, channel_size=3):\n        self._check_deps()\n        self.truncate = truncate\n        self.channel_size = channel_size\n\n    @classmethod\n    def from_opt(cls, opt):\n        return cls(channel_size=opt.image_channel_size)\n\n    @classmethod\n    def _check_deps(cls):\n        if any([Image is None, transforms is None, cv2 is None]):\n            cls._raise_missing_dep(\n                ""PIL"", ""torchvision"", ""cv2"")\n\n    def read(self, images, side, img_dir=None):\n        """"""Read data into dicts.\n\n        Args:\n            images (str or Iterable[str]): Sequence of image paths or\n                path to file containing audio paths.\n                In either case, the filenames may be relative to ``src_dir``\n                (default behavior) or absolute.\n            side (str): Prefix used in return dict. Usually\n                ``""src""`` or ``""tgt""``.\n            img_dir (str): Location of source image files. See ``images``.\n\n        Yields:\n            a dictionary containing image data, path and index for each line.\n        """"""\n        if isinstance(images, str):\n            images = DataReaderBase._read_file(images)\n\n        for i, filename in enumerate(images):\n            filename = filename.decode(""utf-8"").strip()\n            img_path = os.path.join(img_dir, filename)\n            if not os.path.exists(img_path):\n                img_path = filename\n\n            assert os.path.exists(img_path), \\\n                \'img path %s not found\' % filename\n\n            if self.channel_size == 1:\n                img = transforms.ToTensor()(\n                    Image.fromarray(cv2.imread(img_path, 0)))\n            else:\n                img = transforms.ToTensor()(Image.open(img_path))\n            if self.truncate and self.truncate != (0, 0):\n                if not (img.size(1) <= self.truncate[0]\n                        and img.size(2) <= self.truncate[1]):\n                    continue\n            yield {side: img, side + \'_path\': filename, \'indices\': i}\n\n\ndef img_sort_key(ex):\n    """"""Sort using the size of the image: (width, height).""""""\n    return ex.src.size(2), ex.src.size(1)\n\n\ndef batch_img(data, vocab):\n    """"""Pad and batch a sequence of images.""""""\n    c = data[0].size(0)\n    h = max([t.size(1) for t in data])\n    w = max([t.size(2) for t in data])\n    imgs = torch.zeros(len(data), c, h, w).fill_(1)\n    for i, img in enumerate(data):\n        imgs[i, :, 0:img.size(1), 0:img.size(2)] = img\n    return imgs\n\n\ndef image_fields(**kwargs):\n    img = Field(\n        use_vocab=False, dtype=torch.float,\n        postprocessing=batch_img, sequential=False)\n    return img\n'"
onmt/inputters/inputter.py,14,"b'# -*- coding: utf-8 -*-\nimport glob\nimport os\nimport codecs\nimport math\n\nfrom collections import Counter, defaultdict\nfrom itertools import chain, cycle\n\nimport torch\nimport torchtext.data\nfrom torchtext.data import Field, RawField, LabelField\nfrom torchtext.vocab import Vocab\nfrom torchtext.data.utils import RandomShuffler\n\nfrom onmt.inputters.text_dataset import text_fields, TextMultiField\nfrom onmt.inputters.image_dataset import image_fields\nfrom onmt.inputters.audio_dataset import audio_fields\nfrom onmt.inputters.vec_dataset import vec_fields\nfrom onmt.utils.logging import logger\n# backwards compatibility\nfrom onmt.inputters.text_dataset import _feature_tokenize  # noqa: F401\nfrom onmt.inputters.image_dataset import (  # noqa: F401\n    batch_img as make_img)\n\nimport gc\n\n\n# monkey-patch to make torchtext Vocab\'s pickleable\ndef _getstate(self):\n    return dict(self.__dict__, stoi=dict(self.stoi))\n\n\ndef _setstate(self, state):\n    self.__dict__.update(state)\n    self.stoi = defaultdict(lambda: 0, self.stoi)\n\n\nVocab.__getstate__ = _getstate\nVocab.__setstate__ = _setstate\n\n\ndef make_src(data, vocab):\n    src_size = max([t.size(0) for t in data])\n    src_vocab_size = max([t.max() for t in data]) + 1\n    alignment = torch.zeros(src_size, len(data), src_vocab_size)\n    for i, sent in enumerate(data):\n        for j, t in enumerate(sent):\n            alignment[j, i, t] = 1\n    return alignment\n\n\ndef make_tgt(data, vocab):\n    tgt_size = max([t.size(0) for t in data])\n    alignment = torch.zeros(tgt_size, len(data)).long()\n    for i, sent in enumerate(data):\n        alignment[:sent.size(0), i] = sent\n    return alignment\n\n\nclass AlignField(LabelField):\n    """"""\n    Parse [\'<src>-<tgt>\', ...] into [\'<src>\',\'<tgt>\', ...]\n    """"""\n\n    def __init__(self, **kwargs):\n        kwargs[\'use_vocab\'] = False\n        kwargs[\'preprocessing\'] = parse_align_idx\n        super(AlignField, self).__init__(**kwargs)\n\n    def process(self, batch, device=None):\n        """""" Turn a batch of align-idx to a sparse align idx Tensor""""""\n        sparse_idx = []\n        for i, example in enumerate(batch):\n            for src, tgt in example:\n                # +1 for tgt side to keep coherent after ""bos"" padding,\n                # register [\'N\xc2\xb0_in_batch\', \'tgt_id+1\', \'src_id\']\n                sparse_idx.append([i, tgt + 1, src])\n\n        align_idx = torch.tensor(sparse_idx, dtype=self.dtype, device=device)\n\n        return align_idx\n\n\ndef parse_align_idx(align_pharaoh):\n    """"""\n    Parse Pharaoh alignment into [[<src>, <tgt>], ...]\n    """"""\n    align_list = align_pharaoh.strip().split(\' \')\n    flatten_align_idx = []\n    for align in align_list:\n        try:\n            src_idx, tgt_idx = align.split(\'-\')\n        except ValueError:\n            logger.warning(""{} in `{}`"".format(align, align_pharaoh))\n            logger.warning(""Bad alignement line exists. Please check file!"")\n            raise\n        flatten_align_idx.append([int(src_idx), int(tgt_idx)])\n    return flatten_align_idx\n\n\ndef get_fields(\n    src_data_type,\n    n_src_feats,\n    n_tgt_feats,\n    pad=\'<blank>\',\n    bos=\'<s>\',\n    eos=\'</s>\',\n    dynamic_dict=False,\n    with_align=False,\n    src_truncate=None,\n    tgt_truncate=None\n):\n    """"""\n    Args:\n        src_data_type: type of the source input. Options are [text|img|audio].\n        n_src_feats (int): the number of source features (not counting tokens)\n            to create a :class:`torchtext.data.Field` for. (If\n            ``src_data_type==""text""``, these fields are stored together\n            as a ``TextMultiField``).\n        n_tgt_feats (int): See above.\n        pad (str): Special pad symbol. Used on src and tgt side.\n        bos (str): Special beginning of sequence symbol. Only relevant\n            for tgt.\n        eos (str): Special end of sequence symbol. Only relevant\n            for tgt.\n        dynamic_dict (bool): Whether or not to include source map and\n            alignment fields.\n        with_align (bool): Whether or not to include word align.\n        src_truncate: Cut off src sequences beyond this (passed to\n            ``src_data_type``\'s data reader - see there for more details).\n        tgt_truncate: Cut off tgt sequences beyond this (passed to\n            :class:`TextDataReader` - see there for more details).\n\n    Returns:\n        A dict mapping names to fields. These names need to match\n        the dataset example attributes.\n    """"""\n\n    assert src_data_type in [\'text\', \'img\', \'audio\', \'vec\'], \\\n        ""Data type not implemented""\n    assert not dynamic_dict or src_data_type == \'text\', \\\n        \'it is not possible to use dynamic_dict with non-text input\'\n    fields = {}\n\n    fields_getters = {""text"": text_fields,\n                      ""img"": image_fields,\n                      ""audio"": audio_fields,\n                      ""vec"": vec_fields}\n\n    src_field_kwargs = {""n_feats"": n_src_feats,\n                        ""include_lengths"": True,\n                        ""pad"": pad, ""bos"": None, ""eos"": None,\n                        ""truncate"": src_truncate,\n                        ""base_name"": ""src""}\n    fields[""src""] = fields_getters[src_data_type](**src_field_kwargs)\n\n    tgt_field_kwargs = {""n_feats"": n_tgt_feats,\n                        ""include_lengths"": False,\n                        ""pad"": pad, ""bos"": bos, ""eos"": eos,\n                        ""truncate"": tgt_truncate,\n                        ""base_name"": ""tgt""}\n    fields[""tgt""] = fields_getters[""text""](**tgt_field_kwargs)\n\n    indices = Field(use_vocab=False, dtype=torch.long, sequential=False)\n    fields[""indices""] = indices\n\n    corpus_ids = Field(use_vocab=True, sequential=False)\n    fields[""corpus_id""] = corpus_ids\n\n    if dynamic_dict:\n        src_map = Field(\n            use_vocab=False, dtype=torch.float,\n            postprocessing=make_src, sequential=False)\n        fields[""src_map""] = src_map\n\n        src_ex_vocab = RawField()\n        fields[""src_ex_vocab""] = src_ex_vocab\n\n        align = Field(\n            use_vocab=False, dtype=torch.long,\n            postprocessing=make_tgt, sequential=False)\n        fields[""alignment""] = align\n\n    if with_align:\n        word_align = AlignField()\n        fields[""align""] = word_align\n\n    return fields\n\n\ndef patch_fields(opt, fields):\n    dvocab = torch.load(opt.data + \'.vocab.pt\')\n    maybe_cid_field = dvocab.get(\'corpus_id\', None)\n    if maybe_cid_field is not None:\n        fields.update({\'corpus_id\': maybe_cid_field})\n\n\ndef load_old_vocab(vocab, data_type=""text"", dynamic_dict=False):\n    """"""Update a legacy vocab/field format.\n\n    Args:\n        vocab: a list of (field name, torchtext.vocab.Vocab) pairs. This is the\n            format formerly saved in *.vocab.pt files. Or, text data\n            not using a :class:`TextMultiField`.\n        data_type (str): text, img, or audio\n        dynamic_dict (bool): Used for copy attention.\n\n    Returns:\n        a dictionary whose keys are the field names and whose values Fields.\n    """"""\n\n    if _old_style_vocab(vocab):\n        # List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]\n        # -> dict[str, Field]\n        vocab = dict(vocab)\n        n_src_features = sum(\'src_feat_\' in k for k in vocab)\n        n_tgt_features = sum(\'tgt_feat_\' in k for k in vocab)\n        fields = get_fields(\n            data_type, n_src_features, n_tgt_features,\n            dynamic_dict=dynamic_dict)\n        for n, f in fields.items():\n            try:\n                f_iter = iter(f)\n            except TypeError:\n                f_iter = [(n, f)]\n            for sub_n, sub_f in f_iter:\n                if sub_n in vocab:\n                    sub_f.vocab = vocab[sub_n]\n        return fields\n\n    if _old_style_field_list(vocab):  # upgrade to multifield\n        # Dict[str, List[Tuple[str, Field]]]\n        # doesn\'t change structure - don\'t return early.\n        fields = vocab\n        for base_name, vals in fields.items():\n            if ((base_name == \'src\' and data_type == \'text\') or\n                    base_name == \'tgt\'):\n                assert not isinstance(vals[0][1], TextMultiField)\n                fields[base_name] = [(base_name, TextMultiField(\n                    vals[0][0], vals[0][1], vals[1:]))]\n\n    if _old_style_nesting(vocab):\n        # Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]\n        # -> dict[str, Field]\n        fields = dict(list(chain.from_iterable(vocab.values())))\n\n    return fields\n\n\ndef _old_style_vocab(vocab):\n    """"""Detect old-style vocabs (``List[Tuple[str, torchtext.data.Vocab]]``).\n\n    Args:\n        vocab: some object loaded from a *.vocab.pt file\n\n    Returns:\n        Whether ``vocab`` is a list of pairs where the second object\n        is a :class:`torchtext.vocab.Vocab` object.\n\n    This exists because previously only the vocab objects from the fields\n    were saved directly, not the fields themselves, and the fields needed to\n    be reconstructed at training and translation time.\n    """"""\n\n    return isinstance(vocab, list) and \\\n        any(isinstance(v[1], Vocab) for v in vocab)\n\n\ndef _old_style_nesting(vocab):\n    """"""Detect old-style nesting (``dict[str, List[Tuple[str, Field]]]``).""""""\n    return isinstance(vocab, dict) and \\\n        any(isinstance(v, list) for v in vocab.values())\n\n\ndef _old_style_field_list(vocab):\n    """"""Detect old-style text fields.\n\n    Not old style vocab, old nesting, and text-type fields not using\n    ``TextMultiField``.\n\n    Args:\n        vocab: some object loaded from a *.vocab.pt file\n\n    Returns:\n        Whether ``vocab`` is not an :func:`_old_style_vocab` and not\n        a :class:`TextMultiField` (using an old-style text representation).\n    """"""\n\n    # if tgt isn\'t using TextMultiField, then no text field is.\n    return (not _old_style_vocab(vocab)) and _old_style_nesting(vocab) and \\\n        (not isinstance(vocab[\'tgt\'][0][1], TextMultiField))\n\n\ndef old_style_vocab(vocab):\n    """"""The vocab/fields need updated.""""""\n    return _old_style_vocab(vocab) or _old_style_field_list(vocab) or \\\n        _old_style_nesting(vocab)\n\n\ndef filter_example(ex, use_src_len=True, use_tgt_len=True,\n                   min_src_len=1, max_src_len=float(\'inf\'),\n                   min_tgt_len=1, max_tgt_len=float(\'inf\')):\n    """"""Return whether an example is an acceptable length.\n\n    If used with a dataset as ``filter_pred``, use :func:`partial()`\n    for all keyword arguments.\n\n    Args:\n        ex (torchtext.data.Example): An object with a ``src`` and ``tgt``\n            property.\n        use_src_len (bool): Filter based on the length of ``ex.src``.\n        use_tgt_len (bool): Similar to above.\n        min_src_len (int): A non-negative minimally acceptable length\n            (examples of exactly this length will be included).\n        min_tgt_len (int): Similar to above.\n        max_src_len (int or float): A non-negative (possibly infinite)\n            maximally acceptable length (examples of exactly this length\n            will be included).\n        max_tgt_len (int or float): Similar to above.\n    """"""\n\n    src_len = len(ex.src[0])\n    tgt_len = len(ex.tgt[0])\n    return (not use_src_len or min_src_len <= src_len <= max_src_len) and \\\n        (not use_tgt_len or min_tgt_len <= tgt_len <= max_tgt_len)\n\n\ndef _pad_vocab_to_multiple(vocab, multiple):\n    vocab_size = len(vocab)\n    if vocab_size % multiple == 0:\n        return\n    target_size = int(math.ceil(vocab_size / multiple)) * multiple\n    padding_tokens = [\n        ""averyunlikelytoken%d"" % i for i in range(target_size - vocab_size)]\n    vocab.extend(Vocab(Counter(), specials=padding_tokens))\n    return vocab\n\n\ndef _build_field_vocab(field, counter, size_multiple=1, **kwargs):\n    # this is basically copy-pasted from torchtext.\n    all_specials = [\n        field.unk_token, field.pad_token, field.init_token, field.eos_token\n    ]\n    specials = [tok for tok in all_specials if tok is not None]\n    field.vocab = field.vocab_cls(counter, specials=specials, **kwargs)\n    if size_multiple > 1:\n        _pad_vocab_to_multiple(field.vocab, size_multiple)\n\n\ndef _load_vocab(vocab_path, name, counters, min_freq):\n    # counters changes in place\n    vocab = _read_vocab_file(vocab_path, name)\n    vocab_size = len(vocab)\n    logger.info(\'Loaded %s vocab has %d tokens.\' % (name, vocab_size))\n    for i, token in enumerate(vocab):\n        # keep the order of tokens specified in the vocab file by\n        # adding them to the counter with decreasing counting values\n        counters[name][token] = vocab_size - i + min_freq\n    return vocab, vocab_size\n\n\ndef _build_fv_from_multifield(multifield, counters, build_fv_args,\n                              size_multiple=1):\n    for name, field in multifield:\n        _build_field_vocab(\n            field,\n            counters[name],\n            size_multiple=size_multiple,\n            **build_fv_args[name])\n        logger.info("" * %s vocab size: %d."" % (name, len(field.vocab)))\n\n\ndef _build_fields_vocab(fields, counters, data_type, share_vocab,\n                        vocab_size_multiple,\n                        src_vocab_size, src_words_min_frequency,\n                        tgt_vocab_size, tgt_words_min_frequency,\n                        subword_prefix=""\xe2\x96\x81"",\n                        subword_prefix_is_joiner=False):\n    build_fv_args = defaultdict(dict)\n    build_fv_args[""src""] = dict(\n        max_size=src_vocab_size, min_freq=src_words_min_frequency)\n    build_fv_args[""tgt""] = dict(\n        max_size=tgt_vocab_size, min_freq=tgt_words_min_frequency)\n    tgt_multifield = fields[""tgt""]\n    _build_fv_from_multifield(\n        tgt_multifield,\n        counters,\n        build_fv_args,\n        size_multiple=vocab_size_multiple if not share_vocab else 1)\n\n    if fields.get(""corpus_id"", False):\n        fields[""corpus_id""].vocab = fields[""corpus_id""].vocab_cls(\n            counters[""corpus_id""])\n\n    if data_type == \'text\':\n        src_multifield = fields[""src""]\n        _build_fv_from_multifield(\n            src_multifield,\n            counters,\n            build_fv_args,\n            size_multiple=vocab_size_multiple if not share_vocab else 1)\n\n        if share_vocab:\n            # `tgt_vocab_size` is ignored when sharing vocabularies\n            logger.info("" * merging src and tgt vocab..."")\n            src_field = src_multifield.base_field\n            tgt_field = tgt_multifield.base_field\n            _merge_field_vocabs(\n                src_field, tgt_field, vocab_size=src_vocab_size,\n                min_freq=src_words_min_frequency,\n                vocab_size_multiple=vocab_size_multiple)\n            logger.info("" * merged vocab size: %d."" % len(src_field.vocab))\n\n        build_noise_field(\n            src_multifield.base_field,\n            subword_prefix=subword_prefix,\n            is_joiner=subword_prefix_is_joiner)\n    return fields\n\n\ndef build_noise_field(src_field, subword=True,\n                      subword_prefix=""\xe2\x96\x81"", is_joiner=False,\n                      sentence_breaks=[""."", ""?"", ""!""]):\n    """"""In place add noise related fields i.e.:\n         - word_start\n         - end_of_sentence\n    """"""\n    if subword:\n        def is_word_start(x): return (x.startswith(subword_prefix) ^ is_joiner)\n        sentence_breaks = [subword_prefix + t for t in sentence_breaks]\n    else:\n        def is_word_start(x): return True\n\n    vocab_size = len(src_field.vocab)\n    word_start_mask = torch.zeros([vocab_size]).bool()\n    end_of_sentence_mask = torch.zeros([vocab_size]).bool()\n    for i, t in enumerate(src_field.vocab.itos):\n        if is_word_start(t):\n            word_start_mask[i] = True\n        if t in sentence_breaks:\n            end_of_sentence_mask[i] = True\n    src_field.word_start_mask = word_start_mask\n    src_field.end_of_sentence_mask = end_of_sentence_mask\n\n\ndef build_vocab(train_dataset_files, fields, data_type, share_vocab,\n                src_vocab_path, src_vocab_size, src_words_min_frequency,\n                tgt_vocab_path, tgt_vocab_size, tgt_words_min_frequency,\n                vocab_size_multiple=1):\n    """"""Build the fields for all data sides.\n\n    Args:\n        train_dataset_files: a list of train dataset pt file.\n        fields (dict[str, Field]): fields to build vocab for.\n        data_type (str): A supported data type string.\n        share_vocab (bool): share source and target vocabulary?\n        src_vocab_path (str): Path to src vocabulary file.\n        src_vocab_size (int): size of the source vocabulary.\n        src_words_min_frequency (int): the minimum frequency needed to\n            include a source word in the vocabulary.\n        tgt_vocab_path (str): Path to tgt vocabulary file.\n        tgt_vocab_size (int): size of the target vocabulary.\n        tgt_words_min_frequency (int): the minimum frequency needed to\n            include a target word in the vocabulary.\n        vocab_size_multiple (int): ensure that the vocabulary size is a\n            multiple of this value.\n\n    Returns:\n        Dict of Fields\n    """"""\n\n    counters = defaultdict(Counter)\n\n    if src_vocab_path:\n        try:\n            logger.info(""Using existing vocabulary..."")\n            vocab = torch.load(src_vocab_path)\n            # return vocab to dump with standard name\n            return vocab\n        except torch.serialization.pickle.UnpicklingError:\n            logger.info(""Building vocab from text file..."")\n            # empty train_dataset_files so that vocab is only loaded from\n            # given paths in src_vocab_path, tgt_vocab_path\n            train_dataset_files = []\n\n    # Load vocabulary\n    if src_vocab_path:\n        src_vocab, src_vocab_size = _load_vocab(\n            src_vocab_path, ""src"", counters,\n            src_words_min_frequency)\n    else:\n        src_vocab = None\n\n    if tgt_vocab_path:\n        tgt_vocab, tgt_vocab_size = _load_vocab(\n            tgt_vocab_path, ""tgt"", counters,\n            tgt_words_min_frequency)\n    else:\n        tgt_vocab = None\n\n    for i, path in enumerate(train_dataset_files):\n        dataset = torch.load(path)\n        logger.info("" * reloading %s."" % path)\n        for ex in dataset.examples:\n            for name, field in fields.items():\n                try:\n                    f_iter = iter(field)\n                except TypeError:\n                    f_iter = [(name, field)]\n                    all_data = [getattr(ex, name, None)]\n                else:\n                    all_data = getattr(ex, name)\n                for (sub_n, sub_f), fd in zip(\n                        f_iter, all_data):\n                    has_vocab = (sub_n == \'src\' and src_vocab) or \\\n                                (sub_n == \'tgt\' and tgt_vocab)\n                    if sub_f.sequential and not has_vocab:\n                        val = fd\n                        counters[sub_n].update(val)\n\n        # Drop the none-using from memory but keep the last\n        if i < len(train_dataset_files) - 1:\n            dataset.examples = None\n            gc.collect()\n            del dataset.examples\n            gc.collect()\n            del dataset\n            gc.collect()\n\n    fields = _build_fields_vocab(\n        fields, counters, data_type,\n        share_vocab, vocab_size_multiple,\n        src_vocab_size, src_words_min_frequency,\n        tgt_vocab_size, tgt_words_min_frequency)\n\n    return fields  # is the return necessary?\n\n\ndef _merge_field_vocabs(src_field, tgt_field, vocab_size, min_freq,\n                        vocab_size_multiple):\n    # in the long run, shouldn\'t it be possible to do this by calling\n    # build_vocab with both the src and tgt data?\n    specials = [tgt_field.unk_token, tgt_field.pad_token,\n                tgt_field.init_token, tgt_field.eos_token]\n    merged = sum(\n        [src_field.vocab.freqs, tgt_field.vocab.freqs], Counter()\n    )\n    merged_vocab = Vocab(\n        merged, specials=specials,\n        max_size=vocab_size, min_freq=min_freq\n    )\n    if vocab_size_multiple > 1:\n        _pad_vocab_to_multiple(merged_vocab, vocab_size_multiple)\n    src_field.vocab = merged_vocab\n    tgt_field.vocab = merged_vocab\n    assert len(src_field.vocab) == len(tgt_field.vocab)\n\n\ndef _read_vocab_file(vocab_path, tag):\n    """"""Loads a vocabulary from the given path.\n\n    Args:\n        vocab_path (str): Path to utf-8 text file containing vocabulary.\n            Each token should be on a line by itself. Tokens must not\n            contain whitespace (else only before the whitespace\n            is considered).\n        tag (str): Used for logging which vocab is being read.\n    """"""\n\n    logger.info(""Loading {} vocabulary from {}"".format(tag, vocab_path))\n\n    if not os.path.exists(vocab_path):\n        raise RuntimeError(\n            ""{} vocabulary not found at {}"".format(tag, vocab_path))\n    else:\n        with codecs.open(vocab_path, \'r\', \'utf-8\') as f:\n            return [line.strip().split()[0] for line in f if line.strip()]\n\n\ndef batch_iter(data, batch_size, batch_size_fn=None, batch_size_multiple=1):\n    """"""Yield elements from data in chunks of batch_size, where each chunk size\n    is a multiple of batch_size_multiple.\n\n    This is an extended version of torchtext.data.batch.\n    """"""\n    if batch_size_fn is None:\n        def batch_size_fn(new, count, sofar):\n            return count\n    minibatch, size_so_far = [], 0\n    for ex in data:\n        minibatch.append(ex)\n        size_so_far = batch_size_fn(ex, len(minibatch), size_so_far)\n        if size_so_far >= batch_size:\n            overflowed = 0\n            if size_so_far > batch_size:\n                overflowed += 1\n            if batch_size_multiple > 1:\n                overflowed += (\n                    (len(minibatch) - overflowed) % batch_size_multiple)\n            if overflowed == 0:\n                yield minibatch\n                minibatch, size_so_far = [], 0\n            else:\n                if overflowed == len(minibatch):\n                    logger.warning(\n                        ""An example was ignored, more tokens""\n                        "" than allowed by tokens batch_size"")\n                else:\n                    yield minibatch[:-overflowed]\n                    minibatch = minibatch[-overflowed:]\n                    size_so_far = 0\n                    for i, ex in enumerate(minibatch):\n                        size_so_far = batch_size_fn(ex, i + 1, size_so_far)\n    if minibatch:\n        yield minibatch\n\n\ndef _pool(data, batch_size, batch_size_fn, batch_size_multiple,\n          sort_key, random_shuffler, pool_factor):\n    for p in torchtext.data.batch(\n            data, batch_size * pool_factor,\n            batch_size_fn=batch_size_fn):\n        p_batch = list(batch_iter(\n            sorted(p, key=sort_key),\n            batch_size,\n            batch_size_fn=batch_size_fn,\n            batch_size_multiple=batch_size_multiple))\n        for b in random_shuffler(p_batch):\n            yield b\n\n\nclass OrderedIterator(torchtext.data.Iterator):\n\n    def __init__(self,\n                 dataset,\n                 batch_size,\n                 pool_factor=1,\n                 batch_size_multiple=1,\n                 yield_raw_example=False,\n                 **kwargs):\n        super(OrderedIterator, self).__init__(dataset, batch_size, **kwargs)\n        self.batch_size_multiple = batch_size_multiple\n        self.yield_raw_example = yield_raw_example\n        self.dataset = dataset\n        self.pool_factor = pool_factor\n\n    def create_batches(self):\n        if self.train:\n            if self.yield_raw_example:\n                self.batches = batch_iter(\n                    self.data(),\n                    1,\n                    batch_size_fn=None,\n                    batch_size_multiple=1)\n            else:\n                self.batches = _pool(\n                    self.data(),\n                    self.batch_size,\n                    self.batch_size_fn,\n                    self.batch_size_multiple,\n                    self.sort_key,\n                    self.random_shuffler,\n                    self.pool_factor)\n        else:\n            self.batches = []\n            for b in batch_iter(\n                    self.data(),\n                    self.batch_size,\n                    batch_size_fn=self.batch_size_fn,\n                    batch_size_multiple=self.batch_size_multiple):\n                self.batches.append(sorted(b, key=self.sort_key))\n\n    def __iter__(self):\n        """"""\n        Extended version of the definition in torchtext.data.Iterator.\n        Added yield_raw_example behaviour to yield a torchtext.data.Example\n        instead of a torchtext.data.Batch object.\n        """"""\n        while True:\n            self.init_epoch()\n            for idx, minibatch in enumerate(self.batches):\n                # fast-forward if loaded from state\n                if self._iterations_this_epoch > idx:\n                    continue\n                self.iterations += 1\n                self._iterations_this_epoch += 1\n                if self.sort_within_batch:\n                    # NOTE: `rnn.pack_padded_sequence` requires that a\n                    # minibatch be sorted by decreasing order, which\n                    #  requires reversing relative to typical sort keys\n                    if self.sort:\n                        minibatch.reverse()\n                    else:\n                        minibatch.sort(key=self.sort_key, reverse=True)\n                if self.yield_raw_example:\n                    yield minibatch[0]\n                else:\n                    yield torchtext.data.Batch(\n                        minibatch,\n                        self.dataset,\n                        self.device)\n            if not self.repeat:\n                return\n\n\nclass MultipleDatasetIterator(object):\n    """"""\n    This takes a list of iterable objects (DatasetLazyIter) and their\n    respective weights, and yields a batch in the wanted proportions.\n    """"""\n    def __init__(self,\n                 train_shards,\n                 fields,\n                 device,\n                 opt):\n        self.index = -1\n        self.iterables = []\n        self.weights = []\n        for shard, weight in zip(train_shards, opt.data_weights):\n            if weight > 0:\n                self.iterables.append(\n                    build_dataset_iter(shard, fields, opt, multi=True))\n                self.weights.append(weight)\n        self.init_iterators = True\n        # self.weights = opt.data_weights\n        self.batch_size = opt.batch_size\n        self.batch_size_fn = max_tok_len \\\n            if opt.batch_type == ""tokens"" else None\n        if opt.batch_size_multiple is not None:\n            self.batch_size_multiple = opt.batch_size_multiple\n        else:\n            self.batch_size_multiple = 8 if opt.model_dtype == ""fp16"" else 1\n        self.device = device\n        # Temporarily load one shard to retrieve sort_key for data_type\n        temp_dataset = torch.load(self.iterables[0]._paths[0])\n        self.sort_key = temp_dataset.sort_key\n        self.random_shuffler = RandomShuffler()\n        self.pool_factor = opt.pool_factor\n        del temp_dataset\n\n    def _iter_datasets(self):\n        if self.init_iterators:\n            self.iterators = [iter(iterable) for iterable in self.iterables]\n            self.init_iterators = False\n        for weight in self.weights:\n            self.index = (self.index + 1) % len(self.iterators)\n            for i in range(weight):\n                yield self.iterators[self.index]\n\n    def _iter_examples(self):\n        for iterator in cycle(self._iter_datasets()):\n            yield next(iterator)\n\n    def __iter__(self):\n        while True:\n            for minibatch in _pool(\n                    self._iter_examples(),\n                    self.batch_size,\n                    self.batch_size_fn,\n                    self.batch_size_multiple,\n                    self.sort_key,\n                    self.random_shuffler,\n                    self.pool_factor):\n                minibatch = sorted(minibatch, key=self.sort_key, reverse=True)\n                yield torchtext.data.Batch(minibatch,\n                                           self.iterables[0].dataset,\n                                           self.device)\n\n\nclass DatasetLazyIter(object):\n    """"""Yield data from sharded dataset files.\n\n    Args:\n        dataset_paths: a list containing the locations of dataset files.\n        fields (dict[str, Field]): fields dict for the\n            datasets.\n        batch_size (int): batch size.\n        batch_size_fn: custom batch process function.\n        device: See :class:`OrderedIterator` ``device``.\n        is_train (bool): train or valid?\n    """"""\n\n    def __init__(self, dataset_paths, fields, batch_size, batch_size_fn,\n                 batch_size_multiple, device, is_train, pool_factor,\n                 repeat=True, num_batches_multiple=1, yield_raw_example=False):\n        self._paths = dataset_paths\n        self.fields = fields\n        self.batch_size = batch_size\n        self.batch_size_fn = batch_size_fn\n        self.batch_size_multiple = batch_size_multiple\n        self.device = device\n        self.is_train = is_train\n        self.repeat = repeat\n        self.num_batches_multiple = num_batches_multiple\n        self.yield_raw_example = yield_raw_example\n        self.pool_factor = pool_factor\n\n    def _iter_dataset(self, path):\n        logger.info(\'Loading dataset from %s\' % path)\n        cur_dataset = torch.load(path)\n        logger.info(\'number of examples: %d\' % len(cur_dataset))\n        cur_dataset.fields = self.fields\n        cur_iter = OrderedIterator(\n            dataset=cur_dataset,\n            batch_size=self.batch_size,\n            pool_factor=self.pool_factor,\n            batch_size_multiple=self.batch_size_multiple,\n            batch_size_fn=self.batch_size_fn,\n            device=self.device,\n            train=self.is_train,\n            sort=False,\n            sort_within_batch=True,\n            repeat=False,\n            yield_raw_example=self.yield_raw_example\n        )\n        for batch in cur_iter:\n            self.dataset = cur_iter.dataset\n            yield batch\n\n        # NOTE: This is causing some issues for consumer/producer,\n        # as we may still have some of those examples in some queue\n        # cur_dataset.examples = None\n        # gc.collect()\n        # del cur_dataset\n        # gc.collect()\n\n    def __iter__(self):\n        num_batches = 0\n        paths = self._paths\n        if self.is_train and self.repeat:\n            # Cycle through the shards indefinitely.\n            paths = cycle(paths)\n        for path in paths:\n            for batch in self._iter_dataset(path):\n                yield batch\n                num_batches += 1\n        if self.is_train and not self.repeat and \\\n           num_batches % self.num_batches_multiple != 0:\n            # When the dataset is not repeated, we might need to ensure that\n            # the number of returned batches is the multiple of a given value.\n            # This is important for multi GPU training to ensure that all\n            # workers have the same number of batches to process.\n            for path in paths:\n                for batch in self._iter_dataset(path):\n                    yield batch\n                    num_batches += 1\n                    if num_batches % self.num_batches_multiple == 0:\n                        return\n\n\ndef max_tok_len(new, count, sofar):\n    """"""\n    In token batching scheme, the number of sequences is limited\n    such that the total number of src/tgt tokens (including padding)\n    in a batch <= batch_size\n    """"""\n    # Maintains the longest src and tgt length in the current batch\n    global max_src_in_batch, max_tgt_in_batch  # this is a hack\n    # Reset current longest length at a new batch (count=1)\n    if count == 1:\n        max_src_in_batch = 0\n        max_tgt_in_batch = 0\n    # Src: [<bos> w1 ... wN <eos>]\n    max_src_in_batch = max(max_src_in_batch, len(new.src[0]) + 2)\n    # Tgt: [w1 ... wM <eos>]\n    max_tgt_in_batch = max(max_tgt_in_batch, len(new.tgt[0]) + 1)\n    src_elements = count * max_src_in_batch\n    tgt_elements = count * max_tgt_in_batch\n    return max(src_elements, tgt_elements)\n\n\ndef build_dataset_iter(corpus_type, fields, opt, is_train=True, multi=False):\n    """"""\n    This returns user-defined train/validate data iterator for the trainer\n    to iterate over. We implement simple ordered iterator strategy here,\n    but more sophisticated strategy like curriculum learning is ok too.\n    """"""\n    dataset_glob = opt.data + \'.\' + corpus_type + \'.[0-9]*.pt\'\n    dataset_paths = list(sorted(\n        glob.glob(dataset_glob),\n        key=lambda p: int(p.split(""."")[-2])))\n\n    if not dataset_paths:\n        if is_train:\n            raise ValueError(\'Training data %s not found\' % dataset_glob)\n        else:\n            return None\n    if multi:\n        batch_size = 1\n        batch_fn = None\n        batch_size_multiple = 1\n    else:\n        batch_size = opt.batch_size if is_train else opt.valid_batch_size\n        batch_fn = max_tok_len \\\n            if is_train and opt.batch_type == ""tokens"" else None\n        batch_size_multiple = 8 if opt.model_dtype == ""fp16"" else 1\n\n    device = ""cuda"" if opt.gpu_ranks else ""cpu""\n\n    return DatasetLazyIter(\n        dataset_paths,\n        fields,\n        batch_size,\n        batch_fn,\n        batch_size_multiple,\n        device,\n        is_train,\n        opt.pool_factor,\n        repeat=not opt.single_pass,\n        num_batches_multiple=max(opt.accum_count) * opt.world_size,\n        yield_raw_example=multi)\n\n\ndef build_dataset_iter_multiple(train_shards, fields, opt):\n    return MultipleDatasetIterator(\n        train_shards, fields, ""cuda"" if opt.gpu_ranks else ""cpu"", opt)\n'"
onmt/inputters/text_dataset.py,3,"b'# -*- coding: utf-8 -*-\nfrom functools import partial\n\nimport six\nimport torch\nfrom torchtext.data import Field, RawField\n\nfrom onmt.inputters.datareader_base import DataReaderBase\n\n\nclass TextDataReader(DataReaderBase):\n    def read(self, sequences, side, _dir=None):\n        """"""Read text data from disk.\n\n        Args:\n            sequences (str or Iterable[str]):\n                path to text file or iterable of the actual text data.\n            side (str): Prefix used in return dict. Usually\n                ``""src""`` or ``""tgt""``.\n            _dir (NoneType): Leave as ``None``. This parameter exists to\n                conform with the :func:`DataReaderBase.read()` signature.\n\n        Yields:\n            dictionaries whose keys are the names of fields and whose\n            values are more or less the result of tokenizing with those\n            fields.\n        """"""\n        assert _dir is None or _dir == """", \\\n            ""Cannot use _dir with TextDataReader.""\n        if isinstance(sequences, str):\n            sequences = DataReaderBase._read_file(sequences)\n        for i, seq in enumerate(sequences):\n            if isinstance(seq, six.binary_type):\n                seq = seq.decode(""utf-8"")\n            yield {side: seq, ""indices"": i}\n\n\ndef text_sort_key(ex):\n    """"""Sort using the number of tokens in the sequence.""""""\n    if hasattr(ex, ""tgt""):\n        return len(ex.src[0]), len(ex.tgt[0])\n    return len(ex.src[0])\n\n\n# mix this with partial\ndef _feature_tokenize(\n        string, layer=0, tok_delim=None, feat_delim=None, truncate=None):\n    """"""Split apart word features (like POS/NER tags) from the tokens.\n\n    Args:\n        string (str): A string with ``tok_delim`` joining tokens and\n            features joined by ``feat_delim``. For example,\n            ``""hello|NOUN|\'\' Earth|NOUN|PLANET""``.\n        layer (int): Which feature to extract. (Not used if there are no\n            features, indicated by ``feat_delim is None``). In the\n            example above, layer 2 is ``\'\' PLANET``.\n        truncate (int or NoneType): Restrict sequences to this length of\n            tokens.\n\n    Returns:\n        List[str] of tokens.\n    """"""\n\n    tokens = string.split(tok_delim)\n    if truncate is not None:\n        tokens = tokens[:truncate]\n    if feat_delim is not None:\n        tokens = [t.split(feat_delim)[layer] for t in tokens]\n    return tokens\n\n\nclass TextMultiField(RawField):\n    """"""Container for subfields.\n\n    Text data might use POS/NER/etc labels in addition to tokens.\n    This class associates the ""base"" :class:`Field` with any subfields.\n    It also handles padding the data and stacking it.\n\n    Args:\n        base_name (str): Name for the base field.\n        base_field (Field): The token field.\n        feats_fields (Iterable[Tuple[str, Field]]): A list of name-field\n            pairs.\n\n    Attributes:\n        fields (Iterable[Tuple[str, Field]]): A list of name-field pairs.\n            The order is defined as the base field first, then\n            ``feats_fields`` in alphabetical order.\n    """"""\n\n    def __init__(self, base_name, base_field, feats_fields):\n        super(TextMultiField, self).__init__()\n        self.fields = [(base_name, base_field)]\n        for name, ff in sorted(feats_fields, key=lambda kv: kv[0]):\n            self.fields.append((name, ff))\n\n    @property\n    def base_field(self):\n        return self.fields[0][1]\n\n    def process(self, batch, device=None):\n        """"""Convert outputs of preprocess into Tensors.\n\n        Args:\n            batch (List[List[List[str]]]): A list of length batch size.\n                Each element is a list of the preprocess results for each\n                field (which are lists of str ""words"" or feature tags.\n            device (torch.device or str): The device on which the tensor(s)\n                are built.\n\n        Returns:\n            torch.LongTensor or Tuple[LongTensor, LongTensor]:\n                A tensor of shape ``(seq_len, batch_size, len(self.fields))``\n                where the field features are ordered like ``self.fields``.\n                If the base field returns lengths, these are also returned\n                and have shape ``(batch_size,)``.\n        """"""\n\n        # batch (list(list(list))): batch_size x len(self.fields) x seq_len\n        batch_by_feat = list(zip(*batch))\n        base_data = self.base_field.process(batch_by_feat[0], device=device)\n        if self.base_field.include_lengths:\n            # lengths: batch_size\n            base_data, lengths = base_data\n\n        feats = [ff.process(batch_by_feat[i], device=device)\n                 for i, (_, ff) in enumerate(self.fields[1:], 1)]\n        levels = [base_data] + feats\n        # data: seq_len x batch_size x len(self.fields)\n        data = torch.stack(levels, 2)\n        if self.base_field.include_lengths:\n            return data, lengths\n        else:\n            return data\n\n    def preprocess(self, x):\n        """"""Preprocess data.\n\n        Args:\n            x (str): A sentence string (words joined by whitespace).\n\n        Returns:\n            List[List[str]]: A list of length ``len(self.fields)`` containing\n                lists of tokens/feature tags for the sentence. The output\n                is ordered like ``self.fields``.\n        """"""\n\n        return [f.preprocess(x) for _, f in self.fields]\n\n    def __getitem__(self, item):\n        return self.fields[item]\n\n\ndef text_fields(**kwargs):\n    """"""Create text fields.\n\n    Args:\n        base_name (str): Name associated with the field.\n        n_feats (int): Number of word level feats (not counting the tokens)\n        include_lengths (bool): Optionally return the sequence lengths.\n        pad (str, optional): Defaults to ``""<blank>""``.\n        bos (str or NoneType, optional): Defaults to ``""<s>""``.\n        eos (str or NoneType, optional): Defaults to ``""</s>""``.\n        truncate (bool or NoneType, optional): Defaults to ``None``.\n\n    Returns:\n        TextMultiField\n    """"""\n\n    n_feats = kwargs[""n_feats""]\n    include_lengths = kwargs[""include_lengths""]\n    base_name = kwargs[""base_name""]\n    pad = kwargs.get(""pad"", ""<blank>"")\n    bos = kwargs.get(""bos"", ""<s>"")\n    eos = kwargs.get(""eos"", ""</s>"")\n    truncate = kwargs.get(""truncate"", None)\n    fields_ = []\n    feat_delim = u""\xef\xbf\xa8"" if n_feats > 0 else None\n    for i in range(n_feats + 1):\n        name = base_name + ""_feat_"" + str(i - 1) if i > 0 else base_name\n        tokenize = partial(\n            _feature_tokenize,\n            layer=i,\n            truncate=truncate,\n            feat_delim=feat_delim)\n        use_len = i == 0 and include_lengths\n        feat = Field(\n            init_token=bos, eos_token=eos,\n            pad_token=pad, tokenize=tokenize,\n            include_lengths=use_len)\n        fields_.append((name, feat))\n    assert fields_[0][0] == base_name  # sanity check\n    field = TextMultiField(fields_[0][0], fields_[0][1], fields_[1:])\n    return field\n'"
onmt/inputters/vec_dataset.py,8,"b'import os\n\nimport torch\nfrom torchtext.data import Field\n\nfrom onmt.inputters.datareader_base import DataReaderBase\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n\n\nclass VecDataReader(DataReaderBase):\n    """"""Read feature vector data from disk.\n    Raises:\n        onmt.inputters.datareader_base.MissingDependencyException: If\n            importing ``np`` fails.\n    """"""\n\n    def __init__(self):\n        self._check_deps()\n\n    @classmethod\n    def _check_deps(cls):\n        if np is None:\n            cls._raise_missing_dep(""np"")\n\n    def read(self, vecs, side, vec_dir=None):\n        """"""Read data into dicts.\n        Args:\n            vecs (str or Iterable[str]): Sequence of feature vector paths or\n                path to file containing feature vector paths.\n                In either case, the filenames may be relative to ``vec_dir``\n                (default behavior) or absolute.\n            side (str): Prefix used in return dict. Usually\n                ``""src""`` or ``""tgt""``.\n            vec_dir (str): Location of source vectors. See ``vecs``.\n        Yields:\n            A dictionary containing feature vector data.\n        """"""\n\n        if isinstance(vecs, str):\n            vecs = DataReaderBase._read_file(vecs)\n\n        for i, filename in enumerate(vecs):\n            filename = filename.decode(""utf-8"").strip()\n            vec_path = os.path.join(vec_dir, filename)\n            if not os.path.exists(vec_path):\n                vec_path = filename\n\n            assert os.path.exists(vec_path), \\\n                \'vec path %s not found\' % filename\n\n            vec = np.load(vec_path)\n            yield {side: torch.from_numpy(vec),\n                   side + ""_path"": filename, ""indices"": i}\n\n\ndef vec_sort_key(ex):\n    """"""Sort using the length of the vector sequence.""""""\n    return ex.src.shape[0]\n\n\nclass VecSeqField(Field):\n    """"""Defines an vector datatype and instructions for converting to Tensor.\n    See :class:`Fields` for attribute descriptions.\n    """"""\n\n    def __init__(self, preprocessing=None, postprocessing=None,\n                 include_lengths=False, batch_first=False, pad_index=0,\n                 is_target=False):\n        super(VecSeqField, self).__init__(\n            sequential=True, use_vocab=False, init_token=None,\n            eos_token=None, fix_length=False, dtype=torch.float,\n            preprocessing=preprocessing, postprocessing=postprocessing,\n            lower=False, tokenize=None, include_lengths=include_lengths,\n            batch_first=batch_first, pad_token=pad_index, unk_token=None,\n            pad_first=False, truncate_first=False, stop_words=None,\n            is_target=is_target\n        )\n\n    def pad(self, minibatch):\n        """"""Pad a batch of examples to the length of the longest example.\n        Args:\n            minibatch (List[torch.FloatTensor]): A list of audio data,\n                each having shape ``(len, n_feats, feat_dim)``\n                where len is variable.\n        Returns:\n            torch.FloatTensor or Tuple[torch.FloatTensor, List[int]]: The\n                padded tensor of shape\n                ``(batch_size, max_len, n_feats, feat_dim)``.\n                and a list of the lengths if `self.include_lengths` is `True`\n                else just returns the padded tensor.\n        """"""\n\n        assert not self.pad_first and not self.truncate_first \\\n            and not self.fix_length and self.sequential\n        minibatch = list(minibatch)\n        lengths = [x.size(0) for x in minibatch]\n        max_len = max(lengths)\n        nfeats = minibatch[0].size(1)\n        feat_dim = minibatch[0].size(2)\n        feats = torch.full((len(minibatch), max_len, nfeats, feat_dim),\n                           self.pad_token)\n        for i, (feat, len_) in enumerate(zip(minibatch, lengths)):\n            feats[i, 0:len_, :, :] = feat\n        if self.include_lengths:\n            return (feats, lengths)\n        return feats\n\n    def numericalize(self, arr, device=None):\n        """"""Turn a batch of examples that use this field into a Variable.\n        If the field has ``include_lengths=True``, a tensor of lengths will be\n        included in the return value.\n        Args:\n            arr (torch.FloatTensor or Tuple(torch.FloatTensor, List[int])):\n                List of tokenized and padded examples, or tuple of List of\n                tokenized and padded examples and List of lengths of each\n                example if self.include_lengths is True.\n            device (str or torch.device): See `Field.numericalize`.\n        """"""\n\n        assert self.use_vocab is False\n        if self.include_lengths and not isinstance(arr, tuple):\n            raise ValueError(""Field has include_lengths set to True, but ""\n                             ""input data is not a tuple of ""\n                             ""(data batch, batch lengths)."")\n        if isinstance(arr, tuple):\n            arr, lengths = arr\n            lengths = torch.tensor(lengths, dtype=torch.int, device=device)\n        arr = arr.to(device)\n\n        if self.postprocessing is not None:\n            arr = self.postprocessing(arr, None)\n\n        if self.sequential and not self.batch_first:\n            arr = arr.permute(1, 0, 2, 3)\n        if self.sequential:\n            arr = arr.contiguous()\n\n        if self.include_lengths:\n            return arr, lengths\n        return arr\n\n\ndef vec_fields(**kwargs):\n    vec = VecSeqField(pad_index=0, include_lengths=True)\n    return vec\n'"
onmt/models/__init__.py,0,"b'""""""Module defining models.""""""\nfrom onmt.models.model_saver import build_model_saver, ModelSaver\nfrom onmt.models.model import NMTModel\n\n__all__ = [""build_model_saver"", ""ModelSaver"", ""NMTModel""]\n'"
onmt/models/model.py,1,"b'"""""" Onmt NMT Model base class definition """"""\nimport torch.nn as nn\n\n\nclass NMTModel(nn.Module):\n    """"""\n    Core trainable object in OpenNMT. Implements a trainable interface\n    for a simple, generic encoder + decoder model.\n\n    Args:\n      encoder (onmt.encoders.EncoderBase): an encoder object\n      decoder (onmt.decoders.DecoderBase): a decoder object\n    """"""\n\n    def __init__(self, encoder, decoder):\n        super(NMTModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, tgt, lengths, bptt=False, with_align=False):\n        """"""Forward propagate a `src` and `tgt` pair for training.\n        Possible initialized with a beginning decoder state.\n\n        Args:\n            src (Tensor): A source sequence passed to encoder.\n                typically for inputs this will be a padded `LongTensor`\n                of size ``(len, batch, features)``. However, may be an\n                image or other generic input depending on encoder.\n            tgt (LongTensor): A target sequence passed to decoder.\n                Size ``(tgt_len, batch, features)``.\n            lengths(LongTensor): The src lengths, pre-padding ``(batch,)``.\n            bptt (Boolean): A flag indicating if truncated bptt is set.\n                If reset then init_state\n            with_align (Boolean): A flag indicating whether output alignment,\n                Only valid for transformer decoder.\n\n        Returns:\n            (FloatTensor, dict[str, FloatTensor]):\n\n            * decoder output ``(tgt_len, batch, hidden)``\n            * dictionary attention dists of ``(tgt_len, batch, src_len)``\n        """"""\n        dec_in = tgt[:-1]  # exclude last target from inputs\n\n        enc_state, memory_bank, lengths = self.encoder(src, lengths)\n\n        if bptt is False:\n            self.decoder.init_state(src, memory_bank, enc_state)\n        dec_out, attns = self.decoder(dec_in, memory_bank,\n                                      memory_lengths=lengths,\n                                      with_align=with_align)\n        return dec_out, attns\n\n    def update_dropout(self, dropout):\n        self.encoder.update_dropout(dropout)\n        self.decoder.update_dropout(dropout)\n'"
onmt/models/model_saver.py,1,"b'import os\nimport torch\n\nfrom collections import deque\nfrom onmt.utils.logging import logger\n\nfrom copy import deepcopy\n\n\ndef build_model_saver(model_opt, opt, model, fields, optim):\n    model_saver = ModelSaver(opt.save_model,\n                             model,\n                             model_opt,\n                             fields,\n                             optim,\n                             opt.keep_checkpoint)\n    return model_saver\n\n\nclass ModelSaverBase(object):\n    """"""Base class for model saving operations\n\n    Inherited classes must implement private methods:\n    * `_save`\n    * `_rm_checkpoint\n    """"""\n\n    def __init__(self, base_path, model, model_opt, fields, optim,\n                 keep_checkpoint=-1):\n        self.base_path = base_path\n        self.model = model\n        self.model_opt = model_opt\n        self.fields = fields\n        self.optim = optim\n        self.last_saved_step = None\n        self.keep_checkpoint = keep_checkpoint\n        if keep_checkpoint > 0:\n            self.checkpoint_queue = deque([], maxlen=keep_checkpoint)\n\n    def save(self, step, moving_average=None):\n        """"""Main entry point for model saver\n\n        It wraps the `_save` method with checks and apply `keep_checkpoint`\n        related logic\n        """"""\n\n        if self.keep_checkpoint == 0 or step == self.last_saved_step:\n            return\n\n        save_model = self.model\n        if moving_average:\n            model_params_data = []\n            for avg, param in zip(moving_average, save_model.parameters()):\n                model_params_data.append(param.data)\n                param.data = avg.data\n\n        chkpt, chkpt_name = self._save(step, save_model)\n        self.last_saved_step = step\n\n        if moving_average:\n            for param_data, param in zip(model_params_data,\n                                         save_model.parameters()):\n                param.data = param_data\n\n        if self.keep_checkpoint > 0:\n            if len(self.checkpoint_queue) == self.checkpoint_queue.maxlen:\n                todel = self.checkpoint_queue.popleft()\n                self._rm_checkpoint(todel)\n            self.checkpoint_queue.append(chkpt_name)\n\n    def _save(self, step):\n        """"""Save a resumable checkpoint.\n\n        Args:\n            step (int): step number\n\n        Returns:\n            (object, str):\n\n            * checkpoint: the saved object\n            * checkpoint_name: name (or path) of the saved checkpoint\n        """"""\n\n        raise NotImplementedError()\n\n    def _rm_checkpoint(self, name):\n        """"""Remove a checkpoint\n\n        Args:\n            name(str): name that indentifies the checkpoint\n                (it may be a filepath)\n        """"""\n\n        raise NotImplementedError()\n\n\nclass ModelSaver(ModelSaverBase):\n    """"""Simple model saver to filesystem""""""\n\n    def _save(self, step, model):\n        model_state_dict = model.state_dict()\n        model_state_dict = {k: v for k, v in model_state_dict.items()\n                            if \'generator\' not in k}\n        generator_state_dict = model.generator.state_dict()\n\n        # NOTE: We need to trim the vocab to remove any unk tokens that\n        # were not originally here.\n\n        vocab = deepcopy(self.fields)\n        for side in [""src"", ""tgt""]:\n            keys_to_pop = []\n            if hasattr(vocab[side], ""fields""):\n                unk_token = vocab[side].fields[0][1].vocab.itos[0]\n                for key, value in vocab[side].fields[0][1].vocab.stoi.items():\n                    if value == 0 and key != unk_token:\n                        keys_to_pop.append(key)\n                for key in keys_to_pop:\n                    vocab[side].fields[0][1].vocab.stoi.pop(key, None)\n\n        checkpoint = {\n            \'model\': model_state_dict,\n            \'generator\': generator_state_dict,\n            \'vocab\': vocab,\n            \'opt\': self.model_opt,\n            \'optim\': self.optim.state_dict(),\n        }\n\n        logger.info(""Saving checkpoint %s_step_%d.pt"" % (self.base_path, step))\n        checkpoint_path = \'%s_step_%d.pt\' % (self.base_path, step)\n        torch.save(checkpoint, checkpoint_path)\n        return checkpoint, checkpoint_path\n\n    def _rm_checkpoint(self, name):\n        if os.path.exists(name):\n            os.remove(name)\n'"
onmt/models/sru.py,12,"b'"""""" SRU Implementation """"""\n# flake8: noqa\n\nimport subprocess\nimport platform\nimport os\nimport re\nimport configargparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom collections import namedtuple\n\n\n# For command-line option parsing\nclass CheckSRU(configargparse.Action):\n    def __init__(self, option_strings, dest, **kwargs):\n        super(CheckSRU, self).__init__(option_strings, dest, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if values == \'SRU\':\n            check_sru_requirement(abort=True)\n        # Check pass, set the args.\n        setattr(namespace, self.dest, values)\n\n\n# This SRU version implements its own cuda-level optimization,\n# so it requires that:\n# 1. `cupy` and `pynvrtc` python package installed.\n# 2. pytorch is built with cuda support.\n# 3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.\ndef check_sru_requirement(abort=False):\n    """"""\n    Return True if check pass; if check fails and abort is True,\n    raise an Exception, othereise return False.\n    """"""\n\n    # Check 1.\n    try:\n        if platform.system() == \'Windows\':\n            subprocess.check_output(\'pip freeze | findstr cupy\', shell=True)\n            subprocess.check_output(\'pip freeze | findstr pynvrtc\',\n                                    shell=True)\n        else:  # Unix-like systems\n            subprocess.check_output(\'pip freeze | grep -w cupy\', shell=True)\n            subprocess.check_output(\'pip freeze | grep -w pynvrtc\',\n                                    shell=True)\n    except subprocess.CalledProcessError:\n        if not abort:\n            return False\n        raise AssertionError(""Using SRU requires \'cupy\' and \'pynvrtc\' ""\n                             ""python packages installed."")\n\n    # Check 2.\n    if torch.cuda.is_available() is False:\n        if not abort:\n            return False\n        raise AssertionError(""Using SRU requires pytorch built with cuda."")\n\n    # Check 3.\n    pattern = re.compile("".*cuda/lib.*"")\n    ld_path = os.getenv(\'LD_LIBRARY_PATH\', """")\n    if re.match(pattern, ld_path) is None:\n        if not abort:\n            return False\n        raise AssertionError(""Using SRU requires setting cuda lib path, e.g. ""\n                             ""export LD_LIBRARY_PATH=/usr/local/cuda/lib64."")\n\n    return True\n\n\nSRU_CODE = """"""\nextern ""C"" {\n    __forceinline__ __device__ float sigmoidf(float x)\n    {\n        return 1.f / (1.f + expf(-x));\n    }\n    __forceinline__ __device__ float reluf(float x)\n    {\n        return (x > 0.f) ? x : 0.f;\n    }\n    __global__ void sru_fwd(const float * __restrict__ u,\n                            const float * __restrict__ x,\n                            const float * __restrict__ bias,\n                            const float * __restrict__ init,\n                            const float * __restrict__ mask_h,\n                            const int len, const int batch,\n                            const int d, const int k,\n                            float * __restrict__ h,\n                            float * __restrict__ c,\n                            const int activation_type)\n    {\n        assert ((k == 3) || (x == NULL));\n        int ncols = batch*d;\n        int col = blockIdx.x * blockDim.x + threadIdx.x;\n        if (col >= ncols) return;\n        int ncols_u = ncols*k;\n        int ncols_x = (k == 3) ? ncols : ncols_u;\n        const float bias1 = *(bias + (col%d));\n        const float bias2 = *(bias + (col%d) + d);\n        const float mask = (mask_h == NULL) ? 1.0 : (*(mask_h + col));\n        float cur = *(init + col);\n        const float *up = u + (col*k);\n        const float *xp = (k == 3) ? (x + col) : (up + 3);\n        float *cp = c + col;\n        float *hp = h + col;\n        for (int row = 0; row < len; ++row)\n        {\n            float g1 = sigmoidf((*(up+1))+bias1);\n            float g2 = sigmoidf((*(up+2))+bias2);\n            cur = (cur-(*up))*g1 + (*up);\n            *cp = cur;\n            float val = (activation_type == 1) ? tanh(cur) : (\n                (activation_type == 2) ? reluf(cur) : cur\n            );\n            *hp = (val*mask-(*xp))*g2 + (*xp);\n            up += ncols_u;\n            xp += ncols_x;\n            cp += ncols;\n            hp += ncols;\n        }\n    }\n    __global__ void sru_bwd(const float * __restrict__ u,\n                            const float * __restrict__ x,\n                            const float * __restrict__ bias,\n                            const float * __restrict__ init,\n                            const float * __restrict__ mask_h,\n                            const float * __restrict__ c,\n                            const float * __restrict__ grad_h,\n                            const float * __restrict__ grad_last,\n                            const int len,\n                            const int batch, const int d, const int k,\n                            float * __restrict__ grad_u,\n                            float * __restrict__ grad_x,\n                            float * __restrict__ grad_bias,\n                            float * __restrict__ grad_init,\n                            int activation_type)\n    {\n        assert((k == 3) || (x == NULL));\n        assert((k == 3) || (grad_x == NULL));\n        int ncols = batch*d;\n        int col = blockIdx.x * blockDim.x + threadIdx.x;\n        if (col >= ncols) return;\n        int ncols_u = ncols*k;\n        int ncols_x = (k == 3) ? ncols : ncols_u;\n        const float bias1 = *(bias + (col%d));\n        const float bias2 = *(bias + (col%d) + d);\n        const float mask = (mask_h == NULL) ? 1.0 : (*(mask_h + col));\n        float gbias1 = 0;\n        float gbias2 = 0;\n        float cur = *(grad_last + col);\n        const float *up = u + (col*k) + (len-1)*ncols_u;\n        const float *xp = (k == 3) ? (x + col + (len-1)*ncols) : (up + 3);\n        const float *cp = c + col + (len-1)*ncols;\n        const float *ghp = grad_h + col + (len-1)*ncols;\n        float *gup = grad_u + (col*k) + (len-1)*ncols_u;\n        float *gxp = (k == 3) ? (grad_x + col + (len-1)*ncols) : (gup + 3);\n        for (int row = len-1; row >= 0; --row)\n        {\n            const float g1 = sigmoidf((*(up+1))+bias1);\n            const float g2 = sigmoidf((*(up+2))+bias2);\n            const float c_val = (activation_type == 1) ? tanh(*cp) : (\n                (activation_type == 2) ? reluf(*cp) : (*cp)\n            );\n            const float x_val = *xp;\n            const float u_val = *up;\n            const float prev_c_val = (row>0) ? (*(cp-ncols)) : (*(init+col));\n            const float gh_val = *ghp;\n            // h = c*g2 + x*(1-g2) = (c-x)*g2 + x\n            // c = c\'*g1 + g0*(1-g1) = (c\'-g0)*g1 + g0\n            // grad wrt x\n            *gxp = gh_val*(1-g2);\n            // grad wrt g2, u2 and bias2\n            float gg2 = gh_val*(c_val*mask-x_val)*(g2*(1-g2));\n            *(gup+2) = gg2;\n            gbias2 += gg2;\n            // grad wrt c\n            const float tmp = (activation_type == 1) ? (g2*(1-c_val*c_val)) : (\n                ((activation_type == 0) || (c_val > 0)) ? g2 : 0.f\n            );\n            const float gc = gh_val*mask*tmp + cur;\n            // grad wrt u0\n            *gup = gc*(1-g1);\n            // grad wrt g1, u1, and bias1\n            float gg1 = gc*(prev_c_val-u_val)*(g1*(1-g1));\n            *(gup+1) = gg1;\n            gbias1 += gg1;\n            // grad wrt c\'\n            cur = gc*g1;\n            up -= ncols_u;\n            xp -= ncols_x;\n            cp -= ncols;\n            gup -= ncols_u;\n            gxp -= ncols_x;\n            ghp -= ncols;\n        }\n        *(grad_bias + col) = gbias1;\n        *(grad_bias + col + ncols) = gbias2;\n        *(grad_init +col) = cur;\n    }\n    __global__ void sru_bi_fwd(const float * __restrict__ u,\n                               const float * __restrict__ x,\n                               const float * __restrict__ bias,\n                               const float * __restrict__ init,\n                               const float * __restrict__ mask_h,\n                               const int len, const int batch,\n                               const int d, const int k,\n                               float * __restrict__ h,\n                               float * __restrict__ c,\n                               const int activation_type)\n    {\n        assert ((k == 3) || (x == NULL));\n        assert ((k == 3) || (k == 4));\n        int ncols = batch*d*2;\n        int col = blockIdx.x * blockDim.x + threadIdx.x;\n        if (col >= ncols) return;\n        int ncols_u = ncols*k;\n        int ncols_x = (k == 3) ? ncols : ncols_u;\n        const float mask = (mask_h == NULL) ? 1.0 : (*(mask_h + col));\n        float cur = *(init + col);\n        const int d2 = d*2;\n        const bool flip = (col%d2) >= d;\n        const float bias1 = *(bias + (col%d2));\n        const float bias2 = *(bias + (col%d2) + d2);\n        const float *up = u + (col*k);\n        const float *xp = (k == 3) ? (x + col) : (up + 3);\n        float *cp = c + col;\n        float *hp = h + col;\n        if (flip) {\n            up += (len-1)*ncols_u;\n            xp += (len-1)*ncols_x;\n            cp += (len-1)*ncols;\n            hp += (len-1)*ncols;\n        }\n        int ncols_u_ = flip ? -ncols_u : ncols_u;\n        int ncols_x_ = flip ? -ncols_x : ncols_x;\n        int ncols_ = flip ? -ncols : ncols;\n        for (int cnt = 0; cnt < len; ++cnt)\n        {\n            float g1 = sigmoidf((*(up+1))+bias1);\n            float g2 = sigmoidf((*(up+2))+bias2);\n            cur = (cur-(*up))*g1 + (*up);\n            *cp = cur;\n            float val = (activation_type == 1) ? tanh(cur) : (\n                (activation_type == 2) ? reluf(cur) : cur\n            );\n            *hp = (val*mask-(*xp))*g2 + (*xp);\n            up += ncols_u_;\n            xp += ncols_x_;\n            cp += ncols_;\n            hp += ncols_;\n        }\n    }\n    __global__ void sru_bi_bwd(const float * __restrict__ u,\n                               const float * __restrict__ x,\n                               const float * __restrict__ bias,\n                               const float * __restrict__ init,\n                               const float * __restrict__ mask_h,\n                               const float * __restrict__ c,\n                               const float * __restrict__ grad_h,\n                               const float * __restrict__ grad_last,\n                               const int len, const int batch,\n                               const int d, const int k,\n                               float * __restrict__ grad_u,\n                               float * __restrict__ grad_x,\n                               float * __restrict__ grad_bias,\n                               float * __restrict__ grad_init,\n                               int activation_type)\n    {\n        assert((k == 3) || (x == NULL));\n        assert((k == 3) || (grad_x == NULL));\n        assert((k == 3) || (k == 4));\n        int ncols = batch*d*2;\n        int col = blockIdx.x * blockDim.x + threadIdx.x;\n        if (col >= ncols) return;\n        int ncols_u = ncols*k;\n        int ncols_x = (k == 3) ? ncols : ncols_u;\n        const float mask = (mask_h == NULL) ? 1.0 : (*(mask_h + col));\n        float gbias1 = 0;\n        float gbias2 = 0;\n        float cur = *(grad_last + col);\n        const int d2 = d*2;\n        const bool flip = ((col%d2) >= d);\n        const float bias1 = *(bias + (col%d2));\n        const float bias2 = *(bias + (col%d2) + d2);\n        const float *up = u + (col*k);\n        const float *xp = (k == 3) ? (x + col) : (up + 3);\n        const float *cp = c + col;\n        const float *ghp = grad_h + col;\n        float *gup = grad_u + (col*k);\n        float *gxp = (k == 3) ? (grad_x + col) : (gup + 3);\n        if (!flip) {\n            up += (len-1)*ncols_u;\n            xp += (len-1)*ncols_x;\n            cp += (len-1)*ncols;\n            ghp += (len-1)*ncols;\n            gup += (len-1)*ncols_u;\n            gxp += (len-1)*ncols_x;\n        }\n        int ncols_u_ = flip ? -ncols_u : ncols_u;\n        int ncols_x_ = flip ? -ncols_x : ncols_x;\n        int ncols_ = flip ? -ncols : ncols;\n        for (int cnt = 0; cnt < len; ++cnt)\n        {\n            const float g1 = sigmoidf((*(up+1))+bias1);\n            const float g2 = sigmoidf((*(up+2))+bias2);\n            const float c_val = (activation_type == 1) ? tanh(*cp) : (\n                (activation_type == 2) ? reluf(*cp) : (*cp)\n            );\n            const float x_val = *xp;\n            const float u_val = *up;\n            const float prev_c_val = (cnt<len-1)?(*(cp-ncols_)):(*(init+col));\n            const float gh_val = *ghp;\n            // h = c*g2 + x*(1-g2) = (c-x)*g2 + x\n            // c = c\'*g1 + g0*(1-g1) = (c\'-g0)*g1 + g0\n            // grad wrt x\n            *gxp = gh_val*(1-g2);\n            // grad wrt g2, u2 and bias2\n            float gg2 = gh_val*(c_val*mask-x_val)*(g2*(1-g2));\n            *(gup+2) = gg2;\n            gbias2 += gg2;\n            // grad wrt c\n            const float tmp = (activation_type == 1) ? (g2*(1-c_val*c_val)) : (\n                ((activation_type == 0) || (c_val > 0)) ? g2 : 0.f\n            );\n            const float gc = gh_val*mask*tmp + cur;\n            // grad wrt u0\n            *gup = gc*(1-g1);\n            // grad wrt g1, u1, and bias1\n            float gg1 = gc*(prev_c_val-u_val)*(g1*(1-g1));\n            *(gup+1) = gg1;\n            gbias1 += gg1;\n            // grad wrt c\'\n            cur = gc*g1;\n            up -= ncols_u_;\n            xp -= ncols_x_;\n            cp -= ncols_;\n            gup -= ncols_u_;\n            gxp -= ncols_x_;\n            ghp -= ncols_;\n        }\n        *(grad_bias + col) = gbias1;\n        *(grad_bias + col + ncols) = gbias2;\n        *(grad_init +col) = cur;\n    }\n}\n""""""\nSRU_FWD_FUNC, SRU_BWD_FUNC = None, None\nSRU_BiFWD_FUNC, SRU_BiBWD_FUNC = None, None\nSRU_STREAM = None\n\n\ndef load_sru_mod():\n    global SRU_FWD_FUNC, SRU_BWD_FUNC, SRU_BiFWD_FUNC, SRU_BiBWD_FUNC\n    global SRU_STREAM\n    if check_sru_requirement():\n        from cupy.cuda import function\n        from pynvrtc.compiler import Program\n\n        # This sets up device to use.\n        device = torch.device(""cuda"")\n        tmp_ = torch.rand(1, 1).to(device)\n\n        sru_prog = Program(SRU_CODE.encode(\'utf-8\'),\n                           \'sru_prog.cu\'.encode(\'utf-8\'))\n        sru_ptx = sru_prog.compile()\n        sru_mod = function.Module()\n        sru_mod.load(bytes(sru_ptx.encode()))\n\n        SRU_FWD_FUNC = sru_mod.get_function(\'sru_fwd\')\n        SRU_BWD_FUNC = sru_mod.get_function(\'sru_bwd\')\n        SRU_BiFWD_FUNC = sru_mod.get_function(\'sru_bi_fwd\')\n        SRU_BiBWD_FUNC = sru_mod.get_function(\'sru_bi_bwd\')\n\n        stream = namedtuple(\'Stream\', [\'ptr\'])\n        SRU_STREAM = stream(ptr=torch.cuda.current_stream().cuda_stream)\n\n\nclass SRU_Compute(Function):\n\n    def __init__(self, activation_type, d_out, bidirectional=False):\n        SRU_Compute.maybe_load_sru_mod()\n        super(SRU_Compute, self).__init__()\n        self.activation_type = activation_type\n        self.d_out = d_out\n        self.bidirectional = bidirectional\n\n    @staticmethod\n    def maybe_load_sru_mod():\n        global SRU_FWD_FUNC\n\n        if SRU_FWD_FUNC is None:\n            load_sru_mod()\n\n    def forward(self, u, x, bias, init=None, mask_h=None):\n        bidir = 2 if self.bidirectional else 1\n        length = x.size(0) if x.dim() == 3 else 1\n        batch = x.size(-2)\n        d = self.d_out\n        k = u.size(-1) // d\n        k_ = k // 2 if self.bidirectional else k\n        ncols = batch * d * bidir\n        thread_per_block = min(512, ncols)\n        num_block = (ncols - 1) // thread_per_block + 1\n\n        init_ = x.new(ncols).zero_() if init is None else init\n        size = (length, batch, d * bidir) if x.dim() == 3 else (batch, d * bidir)\n        c = x.new(*size)\n        h = x.new(*size)\n\n        FUNC = SRU_FWD_FUNC if not self.bidirectional else SRU_BiFWD_FUNC\n        FUNC(args=[\n            u.contiguous().data_ptr(),\n            x.contiguous().data_ptr() if k_ == 3 else 0,\n            bias.data_ptr(),\n            init_.contiguous().data_ptr(),\n            mask_h.data_ptr() if mask_h is not None else 0,\n            length,\n            batch,\n            d,\n            k_,\n            h.data_ptr(),\n            c.data_ptr(),\n            self.activation_type],\n            block=(thread_per_block, 1, 1), grid=(num_block, 1, 1),\n            stream=SRU_STREAM\n        )\n\n        self.save_for_backward(u, x, bias, init, mask_h)\n        self.intermediate = c\n        if x.dim() == 2:\n            last_hidden = c\n        elif self.bidirectional:\n            # -> directions x batch x dim\n            last_hidden = torch.stack((c[-1, :, :d], c[0, :, d:]))\n        else:\n            last_hidden = c[-1]\n        return h, last_hidden\n\n    def backward(self, grad_h, grad_last):\n        if self.bidirectional:\n            grad_last = torch.cat((grad_last[0], grad_last[1]), 1)\n        bidir = 2 if self.bidirectional else 1\n        u, x, bias, init, mask_h = self.saved_tensors\n        c = self.intermediate\n        length = x.size(0) if x.dim() == 3 else 1\n        batch = x.size(-2)\n        d = self.d_out\n        k = u.size(-1) // d\n        k_ = k // 2 if self.bidirectional else k\n        ncols = batch * d * bidir\n        thread_per_block = min(512, ncols)\n        num_block = (ncols - 1) // thread_per_block + 1\n\n        init_ = x.new(ncols).zero_() if init is None else init\n        grad_u = u.new(*u.size())\n        grad_bias = x.new(2, batch, d * bidir)\n        grad_init = x.new(batch, d * bidir)\n\n        # For DEBUG\n        # size = (length, batch, x.size(-1)) \\\n        #         if x.dim() == 3 else (batch, x.size(-1))\n        # grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()\n\n        # Normal use\n        grad_x = x.new(*x.size()) if k_ == 3 else None\n\n        FUNC = SRU_BWD_FUNC if not self.bidirectional else SRU_BiBWD_FUNC\n        FUNC(args=[\n            u.contiguous().data_ptr(),\n            x.contiguous().data_ptr() if k_ == 3 else 0,\n            bias.data_ptr(),\n            init_.contiguous().data_ptr(),\n            mask_h.data_ptr() if mask_h is not None else 0,\n            c.data_ptr(),\n            grad_h.contiguous().data_ptr(),\n            grad_last.contiguous().data_ptr(),\n            length,\n            batch,\n            d,\n            k_,\n            grad_u.data_ptr(),\n            grad_x.data_ptr() if k_ == 3 else 0,\n            grad_bias.data_ptr(),\n            grad_init.data_ptr(),\n            self.activation_type],\n            block=(thread_per_block, 1, 1), grid=(num_block, 1, 1),\n            stream=SRU_STREAM\n        )\n        return grad_u, grad_x, grad_bias.sum(1).view(-1), grad_init, None\n\n\nclass SRUCell(nn.Module):\n    def __init__(self, n_in, n_out, dropout=0, rnn_dropout=0,\n                 bidirectional=False, use_tanh=1, use_relu=0):\n        super(SRUCell, self).__init__()\n        self.n_in = n_in\n        self.n_out = n_out\n        self.rnn_dropout = rnn_dropout\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.activation_type = 2 if use_relu else (1 if use_tanh else 0)\n\n        out_size = n_out * 2 if bidirectional else n_out\n        k = 4 if n_in != out_size else 3\n        self.size_per_dir = n_out * k\n        self.weight = nn.Parameter(torch.Tensor(\n            n_in,\n            self.size_per_dir * 2 if bidirectional else self.size_per_dir\n        ))\n        self.bias = nn.Parameter(torch.Tensor(\n            n_out * 4 if bidirectional else n_out * 2\n        ))\n        self.init_weight()\n\n    def init_weight(self):\n        val_range = (3.0 / self.n_in)**0.5\n        self.weight.data.uniform_(-val_range, val_range)\n        self.bias.data.zero_()\n\n    def set_bias(self, bias_val=0):\n        n_out = self.n_out\n        if self.bidirectional:\n            self.bias.data[n_out * 2:].zero_().add_(bias_val)\n        else:\n            self.bias.data[n_out:].zero_().add_(bias_val)\n\n    def forward(self, input, c0=None):\n        assert input.dim() == 2 or input.dim() == 3\n        n_in, n_out = self.n_in, self.n_out\n        batch = input.size(-2)\n        if c0 is None:\n            c0 = input.data.new(\n                batch, n_out if not self.bidirectional else n_out * 2\n            ).zero_()\n\n        if self.training and (self.rnn_dropout > 0):\n            mask = self.get_dropout_mask_((batch, n_in), self.rnn_dropout)\n            x = input * mask.expand_as(input)\n        else:\n            x = input\n\n        x_2d = x if x.dim() == 2 else x.contiguous().view(-1, n_in)\n        u = x_2d.mm(self.weight)\n\n        if self.training and (self.dropout > 0):\n            bidir = 2 if self.bidirectional else 1\n            mask_h = self.get_dropout_mask_(\n                (batch, n_out * bidir), self.dropout)\n            h, c = SRU_Compute(self.activation_type, n_out,\n                               self.bidirectional)(\n                                   u, input, self.bias, c0, mask_h\n            )\n        else:\n            h, c = SRU_Compute(self.activation_type, n_out,\n                               self.bidirectional)(\n                                   u, input, self.bias, c0\n            )\n\n        return h, c\n\n    def get_dropout_mask_(self, size, p):\n        w = self.weight.data\n        return w.new(*size).bernoulli_(1 - p).div_(1 - p)\n\n\nclass SRU(nn.Module):\n    """"""\n    Implementation of ""Training RNNs as Fast as CNNs""\n    :cite:`DBLP:journals/corr/abs-1709-02755`\n\n    TODO: turn to pytorch\'s implementation when it is available.\n\n    This implementation is adpoted from the author of the paper:\n    https://github.com/taolei87/sru/blob/master/cuda_functional.py.\n\n    Args:\n      input_size (int): input to model\n      hidden_size (int): hidden dimension\n      num_layers (int): number of layers\n      dropout (float): dropout to use (stacked)\n      rnn_dropout (float): dropout to use (recurrent)\n      bidirectional (bool): bidirectional\n      use_tanh (bool): activation\n      use_relu (bool): activation\n    """"""\n\n    def __init__(self, input_size, hidden_size,\n                 num_layers=2, dropout=0, rnn_dropout=0,\n                 bidirectional=False, use_tanh=1, use_relu=0):\n        # An entry check here, will catch on train side and translate side\n        # if requirements are not satisfied.\n        check_sru_requirement(abort=True)\n        super(SRU, self).__init__()\n        self.n_in = input_size\n        self.n_out = hidden_size\n        self.depth = num_layers\n        self.dropout = dropout\n        self.rnn_dropout = rnn_dropout\n        self.rnn_lst = nn.ModuleList()\n        self.bidirectional = bidirectional\n        self.out_size = hidden_size * 2 if bidirectional else hidden_size\n\n        for i in range(num_layers):\n            sru_cell = SRUCell(\n                n_in=self.n_in if i == 0 else self.out_size,\n                n_out=self.n_out,\n                dropout=dropout if i + 1 != num_layers else 0,\n                rnn_dropout=rnn_dropout,\n                bidirectional=bidirectional,\n                use_tanh=use_tanh,\n                use_relu=use_relu,\n            )\n            self.rnn_lst.append(sru_cell)\n\n    def set_bias(self, bias_val=0):\n        for l in self.rnn_lst:\n            l.set_bias(bias_val)\n\n    def forward(self, input, c0=None, return_hidden=True):\n        assert input.dim() == 3  # (len, batch, n_in)\n        dir_ = 2 if self.bidirectional else 1\n        if c0 is None:\n            zeros = input.data.new(\n                input.size(1), self.n_out * dir_\n            ).zero_()\n            c0 = [zeros for i in range(self.depth)]\n        else:\n            if isinstance(c0, tuple):\n                # RNNDecoderState wraps hidden as a tuple.\n                c0 = c0[0]\n            assert c0.dim() == 3    # (depth, batch, dir_*n_out)\n            c0 = [h.squeeze(0) for h in c0.chunk(self.depth, 0)]\n\n        prevx = input\n        lstc = []\n        for i, rnn in enumerate(self.rnn_lst):\n            h, c = rnn(prevx, c0[i])\n            prevx = h\n            lstc.append(c)\n\n        if self.bidirectional:\n            # fh -> (layers*directions) x batch x dim\n            fh = torch.cat(lstc)\n        else:\n            fh = torch.stack(lstc)\n\n        if return_hidden:\n            return prevx, fh\n        else:\n            return prevx\n'"
onmt/models/stacked_rnn.py,4,"b'"""""" Implementation of ONMT RNN for Input Feeding Decoding """"""\nimport torch\nimport torch.nn as nn\n\n\nclass StackedLSTM(nn.Module):\n    """"""\n    Our own implementation of stacked LSTM.\n    Needed for the decoder, because we do input feeding.\n    """"""\n\n    def __init__(self, num_layers, input_size, rnn_size, dropout):\n        super(StackedLSTM, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n\n        for _ in range(num_layers):\n            self.layers.append(nn.LSTMCell(input_size, rnn_size))\n            input_size = rnn_size\n\n    def forward(self, input_feed, hidden):\n        h_0, c_0 = hidden\n        h_1, c_1 = [], []\n        for i, layer in enumerate(self.layers):\n            h_1_i, c_1_i = layer(input_feed, (h_0[i], c_0[i]))\n            input_feed = h_1_i\n            if i + 1 != self.num_layers:\n                input_feed = self.dropout(input_feed)\n            h_1 += [h_1_i]\n            c_1 += [c_1_i]\n\n        h_1 = torch.stack(h_1)\n        c_1 = torch.stack(c_1)\n\n        return input_feed, (h_1, c_1)\n\n\nclass StackedGRU(nn.Module):\n    """"""\n    Our own implementation of stacked GRU.\n    Needed for the decoder, because we do input feeding.\n    """"""\n\n    def __init__(self, num_layers, input_size, rnn_size, dropout):\n        super(StackedGRU, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n\n        for _ in range(num_layers):\n            self.layers.append(nn.GRUCell(input_size, rnn_size))\n            input_size = rnn_size\n\n    def forward(self, input_feed, hidden):\n        h_1 = []\n        for i, layer in enumerate(self.layers):\n            h_1_i = layer(input_feed, hidden[0][i])\n            input_feed = h_1_i\n            if i + 1 != self.num_layers:\n                input_feed = self.dropout(input_feed)\n            h_1 += [h_1_i]\n\n        h_1 = torch.stack(h_1)\n        return input_feed, (h_1,)\n'"
onmt/modules/__init__.py,0,"b'""""""  Attention and normalization modules  """"""\nfrom onmt.modules.util_class import Elementwise\nfrom onmt.modules.gate import context_gate_factory, ContextGate\nfrom onmt.modules.global_attention import GlobalAttention\nfrom onmt.modules.conv_multi_step_attention import ConvMultiStepAttention\nfrom onmt.modules.copy_generator import CopyGenerator, CopyGeneratorLoss, \\\n    CopyGeneratorLossCompute\nfrom onmt.modules.multi_headed_attn import MultiHeadedAttention\nfrom onmt.modules.embeddings import Embeddings, PositionalEncoding, \\\n    VecEmbedding\nfrom onmt.modules.weight_norm import WeightNormConv2d\nfrom onmt.modules.average_attn import AverageAttention\n\nimport onmt.modules.source_noise # noqa\n\n__all__ = [""Elementwise"", ""context_gate_factory"", ""ContextGate"",\n           ""GlobalAttention"", ""ConvMultiStepAttention"", ""CopyGenerator"",\n           ""CopyGeneratorLoss"", ""CopyGeneratorLossCompute"",\n           ""MultiHeadedAttention"", ""Embeddings"", ""PositionalEncoding"",\n           ""WeightNormConv2d"", ""AverageAttention"", ""VecEmbedding""]\n'"
onmt/modules/average_attn.py,10,"b'# -*- coding: utf-8 -*-\n""""""Average Attention module.""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom onmt.modules.position_ffn import PositionwiseFeedForward\n\n\nclass AverageAttention(nn.Module):\n    """"""\n    Average Attention module from\n    ""Accelerating Neural Transformer via an Average Attention Network""\n    :cite:`DBLP:journals/corr/abs-1805-00631`.\n\n    Args:\n       model_dim (int): the dimension of keys/values/queries,\n           must be divisible by head_count\n       dropout (float): dropout parameter\n    """"""\n\n    def __init__(self, model_dim, dropout=0.1, aan_useffn=False):\n        self.model_dim = model_dim\n        self.aan_useffn = aan_useffn\n        super(AverageAttention, self).__init__()\n        if aan_useffn:\n            self.average_layer = PositionwiseFeedForward(model_dim, model_dim,\n                                                         dropout)\n        self.gating_layer = nn.Linear(model_dim * 2, model_dim * 2)\n\n    def cumulative_average_mask(self, batch_size, inputs_len, device):\n        """"""\n        Builds the mask to compute the cumulative average as described in\n        :cite:`DBLP:journals/corr/abs-1805-00631` -- Figure 3\n\n        Args:\n            batch_size (int): batch size\n            inputs_len (int): length of the inputs\n\n        Returns:\n            (FloatTensor):\n\n            * A Tensor of shape ``(batch_size, input_len, input_len)``\n        """"""\n\n        triangle = torch.tril(torch.ones(inputs_len, inputs_len,\n                              dtype=torch.float, device=device))\n        weights = torch.ones(1, inputs_len, dtype=torch.float, device=device) \\\n            / torch.arange(1, inputs_len + 1, dtype=torch.float, device=device)\n        mask = triangle * weights.transpose(0, 1)\n\n        return mask.unsqueeze(0).expand(batch_size, inputs_len, inputs_len)\n\n    def cumulative_average(self, inputs, mask_or_step,\n                           layer_cache=None, step=None):\n        """"""\n        Computes the cumulative average as described in\n        :cite:`DBLP:journals/corr/abs-1805-00631` -- Equations (1) (5) (6)\n\n        Args:\n            inputs (FloatTensor): sequence to average\n                ``(batch_size, input_len, dimension)``\n            mask_or_step: if cache is set, this is assumed\n                to be the current step of the\n                dynamic decoding. Otherwise, it is the mask matrix\n                used to compute the cumulative average.\n            layer_cache: a dictionary containing the cumulative average\n                of the previous step.\n\n        Returns:\n            a tensor of the same shape and type as ``inputs``.\n        """"""\n\n        if layer_cache is not None:\n            step = mask_or_step\n            average_attention = (inputs + step *\n                                 layer_cache[""prev_g""]) / (step + 1)\n            layer_cache[""prev_g""] = average_attention\n            return average_attention\n        else:\n            mask = mask_or_step\n            return torch.matmul(mask.to(inputs.dtype), inputs)\n\n    def forward(self, inputs, mask=None, layer_cache=None, step=None):\n        """"""\n        Args:\n            inputs (FloatTensor): ``(batch_size, input_len, model_dim)``\n\n        Returns:\n            (FloatTensor, FloatTensor):\n\n            * gating_outputs ``(batch_size, input_len, model_dim)``\n            * average_outputs average attention\n                ``(batch_size, input_len, model_dim)``\n        """"""\n\n        batch_size = inputs.size(0)\n        inputs_len = inputs.size(1)\n        average_outputs = self.cumulative_average(\n          inputs, self.cumulative_average_mask(batch_size,\n                                               inputs_len, inputs.device)\n          if layer_cache is None else step, layer_cache=layer_cache)\n        if self.aan_useffn:\n            average_outputs = self.average_layer(average_outputs)\n        gating_outputs = self.gating_layer(torch.cat((inputs,\n                                                      average_outputs), -1))\n        input_gate, forget_gate = torch.chunk(gating_outputs, 2, dim=2)\n        gating_outputs = torch.sigmoid(input_gate) * inputs + \\\n            torch.sigmoid(forget_gate) * average_outputs\n\n        return gating_outputs, average_outputs\n'"
onmt/modules/conv_multi_step_attention.py,11,"b'"""""" Multi Step Attention for CNN """"""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom onmt.utils.misc import aeq\n\n\nSCALE_WEIGHT = 0.5 ** 0.5\n\n\ndef seq_linear(linear, x):\n    """""" linear transform for 3-d tensor """"""\n    batch, hidden_size, length, _ = x.size()\n    h = linear(torch.transpose(x, 1, 2).contiguous().view(\n        batch * length, hidden_size))\n    return torch.transpose(h.view(batch, length, hidden_size, 1), 1, 2)\n\n\nclass ConvMultiStepAttention(nn.Module):\n    """"""\n    Conv attention takes a key matrix, a value matrix and a query vector.\n    Attention weight is calculated by key matrix with the query vector\n    and sum on the value matrix. And the same operation is applied\n    in each decode conv layer.\n    """"""\n\n    def __init__(self, input_size):\n        super(ConvMultiStepAttention, self).__init__()\n        self.linear_in = nn.Linear(input_size, input_size)\n        self.mask = None\n\n    def apply_mask(self, mask):\n        """""" Apply mask """"""\n        self.mask = mask\n\n    def forward(self, base_target_emb, input_from_dec, encoder_out_top,\n                encoder_out_combine):\n        """"""\n        Args:\n            base_target_emb: target emb tensor\n            input_from_dec: output of decode conv\n            encoder_out_top: the key matrix for calculation of attetion weight,\n                which is the top output of encode conv\n            encoder_out_combine:\n                the value matrix for the attention-weighted sum,\n                which is the combination of base emb and top output of encode\n        """"""\n\n        # checks\n        # batch, channel, height, width = base_target_emb.size()\n        batch, _, height, _ = base_target_emb.size()\n        # batch_, channel_, height_, width_ = input_from_dec.size()\n        batch_, _, height_, _ = input_from_dec.size()\n        aeq(batch, batch_)\n        aeq(height, height_)\n\n        # enc_batch, enc_channel, enc_height = encoder_out_top.size()\n        enc_batch, _, enc_height = encoder_out_top.size()\n        # enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()\n        enc_batch_, _, enc_height_ = encoder_out_combine.size()\n\n        aeq(enc_batch, enc_batch_)\n        aeq(enc_height, enc_height_)\n\n        preatt = seq_linear(self.linear_in, input_from_dec)\n        target = (base_target_emb + preatt) * SCALE_WEIGHT\n        target = torch.squeeze(target, 3)\n        target = torch.transpose(target, 1, 2)\n        pre_attn = torch.bmm(target, encoder_out_top)\n\n        if self.mask is not None:\n            pre_attn.data.masked_fill_(self.mask, -float(\'inf\'))\n\n        attn = F.softmax(pre_attn, dim=2)\n\n        context_output = torch.bmm(\n            attn, torch.transpose(encoder_out_combine, 1, 2))\n        context_output = torch.transpose(\n            torch.unsqueeze(context_output, 3), 1, 2)\n        return context_output, attn\n'"
onmt/modules/copy_generator.py,11,"b'import torch\nimport torch.nn as nn\n\nfrom onmt.utils.misc import aeq\nfrom onmt.utils.loss import NMTLossCompute\n\n\ndef collapse_copy_scores(scores, batch, tgt_vocab, src_vocabs=None,\n                         batch_dim=1, batch_offset=None):\n    """"""\n    Given scores from an expanded dictionary\n    corresponeding to a batch, sums together copies,\n    with a dictionary word when it is ambiguous.\n    """"""\n    offset = len(tgt_vocab)\n    for b in range(scores.size(batch_dim)):\n        blank = []\n        fill = []\n\n        if src_vocabs is None:\n            src_vocab = batch.src_ex_vocab[b]\n        else:\n            batch_id = batch_offset[b] if batch_offset is not None else b\n            index = batch.indices.data[batch_id]\n            src_vocab = src_vocabs[index]\n\n        for i in range(1, len(src_vocab)):\n            sw = src_vocab.itos[i]\n            ti = tgt_vocab.stoi[sw]\n            if ti != 0:\n                blank.append(offset + i)\n                fill.append(ti)\n        if blank:\n            blank = torch.Tensor(blank).type_as(batch.indices.data)\n            fill = torch.Tensor(fill).type_as(batch.indices.data)\n            score = scores[:, b] if batch_dim == 1 else scores[b]\n            score.index_add_(1, fill, score.index_select(1, blank))\n            score.index_fill_(1, blank, 1e-10)\n    return scores\n\n\nclass CopyGenerator(nn.Module):\n    """"""An implementation of pointer-generator networks\n    :cite:`DBLP:journals/corr/SeeLM17`.\n\n    These networks consider copying words\n    directly from the source sequence.\n\n    The copy generator is an extended version of the standard\n    generator that computes three values.\n\n    * :math:`p_{softmax}` the standard softmax over `tgt_dict`\n    * :math:`p(z)` the probability of copying a word from\n      the source\n    * :math:`p_{copy}` the probility of copying a particular word.\n      taken from the attention distribution directly.\n\n    The model returns a distribution over the extend dictionary,\n    computed as\n\n    :math:`p(w) = p(z=1)  p_{copy}(w)  +  p(z=0)  p_{softmax}(w)`\n\n\n    .. mermaid::\n\n       graph BT\n          A[input]\n          S[src_map]\n          B[softmax]\n          BB[switch]\n          C[attn]\n          D[copy]\n          O[output]\n          A --> B\n          A --> BB\n          S --> D\n          C --> D\n          D --> O\n          B --> O\n          BB --> O\n\n\n    Args:\n       input_size (int): size of input representation\n       output_size (int): size of output vocabulary\n       pad_idx (int)\n    """"""\n\n    def __init__(self, input_size, output_size, pad_idx):\n        super(CopyGenerator, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n        self.linear_copy = nn.Linear(input_size, 1)\n        self.pad_idx = pad_idx\n\n    def forward(self, hidden, attn, src_map):\n        """"""\n        Compute a distribution over the target dictionary\n        extended by the dynamic dictionary implied by copying\n        source words.\n\n        Args:\n           hidden (FloatTensor): hidden outputs ``(batch x tlen, input_size)``\n           attn (FloatTensor): attn for each ``(batch x tlen, input_size)``\n           src_map (FloatTensor):\n               A sparse indicator matrix mapping each source word to\n               its index in the ""extended"" vocab containing.\n               ``(src_len, batch, extra_words)``\n        """"""\n\n        # CHECKS\n        batch_by_tlen, _ = hidden.size()\n        batch_by_tlen_, slen = attn.size()\n        slen_, batch, cvocab = src_map.size()\n        aeq(batch_by_tlen, batch_by_tlen_)\n        aeq(slen, slen_)\n\n        # Original probabilities.\n        logits = self.linear(hidden)\n        logits[:, self.pad_idx] = -float(\'inf\')\n        prob = torch.softmax(logits, 1)\n\n        # Probability of copying p(z=1) batch.\n        p_copy = torch.sigmoid(self.linear_copy(hidden))\n        # Probability of not copying: p_{word}(w) * (1 - p(z))\n        out_prob = torch.mul(prob, 1 - p_copy)\n        mul_attn = torch.mul(attn, p_copy)\n        copy_prob = torch.bmm(\n            mul_attn.view(-1, batch, slen).transpose(0, 1),\n            src_map.transpose(0, 1)\n        ).transpose(0, 1)\n        copy_prob = copy_prob.contiguous().view(-1, cvocab)\n        return torch.cat([out_prob, copy_prob], 1)\n\n\nclass CopyGeneratorLoss(nn.Module):\n    """"""Copy generator criterion.""""""\n    def __init__(self, vocab_size, force_copy, unk_index=0,\n                 ignore_index=-100, eps=1e-20):\n        super(CopyGeneratorLoss, self).__init__()\n        self.force_copy = force_copy\n        self.eps = eps\n        self.vocab_size = vocab_size\n        self.ignore_index = ignore_index\n        self.unk_index = unk_index\n\n    def forward(self, scores, align, target):\n        """"""\n        Args:\n            scores (FloatTensor): ``(batch_size*tgt_len)`` x dynamic vocab size\n                whose sum along dim 1 is less than or equal to 1, i.e. cols\n                softmaxed.\n            align (LongTensor): ``(batch_size x tgt_len)``\n            target (LongTensor): ``(batch_size x tgt_len)``\n        """"""\n        # probabilities assigned by the model to the gold targets\n        vocab_probs = scores.gather(1, target.unsqueeze(1)).squeeze(1)\n\n        # probability of tokens copied from source\n        copy_ix = align.unsqueeze(1) + self.vocab_size\n        copy_tok_probs = scores.gather(1, copy_ix).squeeze(1)\n        # Set scores for unk to 0 and add eps\n        copy_tok_probs[align == self.unk_index] = 0\n        copy_tok_probs += self.eps  # to avoid -inf logs\n\n        # find the indices in which you do not use the copy mechanism\n        non_copy = align == self.unk_index\n        if not self.force_copy:\n            non_copy = non_copy | (target != self.unk_index)\n\n        probs = torch.where(\n            non_copy, copy_tok_probs + vocab_probs, copy_tok_probs\n        )\n\n        loss = -probs.log()  # just NLLLoss; can the module be incorporated?\n        # Drop padding.\n        loss[target == self.ignore_index] = 0\n        return loss\n\n\nclass CopyGeneratorLossCompute(NMTLossCompute):\n    """"""Copy Generator Loss Computation.""""""\n    def __init__(self, criterion, generator, tgt_vocab, normalize_by_length,\n                 lambda_coverage=0.0):\n        super(CopyGeneratorLossCompute, self).__init__(\n            criterion, generator, lambda_coverage=lambda_coverage)\n        self.tgt_vocab = tgt_vocab\n        self.normalize_by_length = normalize_by_length\n\n    def _make_shard_state(self, batch, output, range_, attns):\n        """"""See base class for args description.""""""\n        if getattr(batch, ""alignment"", None) is None:\n            raise AssertionError(""using -copy_attn you need to pass in ""\n                                 ""-dynamic_dict during preprocess stage."")\n\n        shard_state = super(CopyGeneratorLossCompute, self)._make_shard_state(\n            batch, output, range_, attns)\n\n        shard_state.update({\n            ""copy_attn"": attns.get(""copy""),\n            ""align"": batch.alignment[range_[0] + 1: range_[1]]\n        })\n        return shard_state\n\n    def _compute_loss(self, batch, output, target, copy_attn, align,\n                      std_attn=None, coverage_attn=None):\n        """"""Compute the loss.\n\n        The args must match :func:`self._make_shard_state()`.\n\n        Args:\n            batch: the current batch.\n            output: the predict output from the model.\n            target: the validate target to compare output with.\n            copy_attn: the copy attention value.\n            align: the align info.\n        """"""\n        target = target.view(-1)\n        align = align.view(-1)\n        scores = self.generator(\n            self._bottle(output), self._bottle(copy_attn), batch.src_map\n        )\n        loss = self.criterion(scores, align, target)\n\n        if self.lambda_coverage != 0.0:\n            coverage_loss = self._compute_coverage_loss(std_attn,\n                                                        coverage_attn)\n            loss += coverage_loss\n\n        # this block does not depend on the loss value computed above\n        # and is used only for stats\n        scores_data = collapse_copy_scores(\n            self._unbottle(scores.clone(), batch.batch_size),\n            batch, self.tgt_vocab, None)\n        scores_data = self._bottle(scores_data)\n\n        # this block does not depend on the loss value computed above\n        # and is used only for stats\n        # Correct target copy token instead of <unk>\n        # tgt[i] = align[i] + len(tgt_vocab)\n        # for i such that tgt[i] == 0 and align[i] != 0\n        target_data = target.clone()\n        unk = self.criterion.unk_index\n        correct_mask = (target_data == unk) & (align != unk)\n        offset_align = align[correct_mask] + len(self.tgt_vocab)\n        target_data[correct_mask] += offset_align\n\n        # Compute sum of perplexities for stats\n        stats = self._stats(loss.sum().clone(), scores_data, target_data)\n\n        # this part looks like it belongs in CopyGeneratorLoss\n        if self.normalize_by_length:\n            # Compute Loss as NLL divided by seq length\n            tgt_lens = batch.tgt[:, :, 0].ne(self.padding_idx).sum(0).float()\n            # Compute Total Loss per sequence in batch\n            loss = loss.view(-1, batch.batch_size).sum(0)\n            # Divide by length of each sequence and sum\n            loss = torch.div(loss, tgt_lens).sum()\n        else:\n            loss = loss.sum()\n\n        return loss, stats\n'"
onmt/modules/embeddings.py,7,"b'"""""" Embeddings module """"""\nimport math\nimport warnings\n\nimport torch\nimport torch.nn as nn\n\nfrom onmt.modules.util_class import Elementwise\n\n\nclass PositionalEncoding(nn.Module):\n    """"""Sinusoidal positional encoding for non-recurrent neural networks.\n\n    Implementation based on ""Attention Is All You Need""\n    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`\n\n    Args:\n       dropout (float): dropout parameter\n       dim (int): embedding size\n    """"""\n\n    def __init__(self, dropout, dim, max_len=5000):\n        if dim % 2 != 0:\n            raise ValueError(""Cannot use sin/cos positional encoding with ""\n                             ""odd dim (got dim={:d})"".format(dim))\n        pe = torch.zeros(max_len, dim)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n                             -(math.log(10000.0) / dim)))\n        pe[:, 0::2] = torch.sin(position.float() * div_term)\n        pe[:, 1::2] = torch.cos(position.float() * div_term)\n        pe = pe.unsqueeze(1)\n        super(PositionalEncoding, self).__init__()\n        self.register_buffer(\'pe\', pe)\n        self.dropout = nn.Dropout(p=dropout)\n        self.dim = dim\n\n    def forward(self, emb, step=None):\n        """"""Embed inputs.\n\n        Args:\n            emb (FloatTensor): Sequence of word vectors\n                ``(seq_len, batch_size, self.dim)``\n            step (int or NoneType): If stepwise (``seq_len = 1``), use\n                the encoding for this position.\n        """"""\n\n        emb = emb * math.sqrt(self.dim)\n        if step is None:\n            emb = emb + self.pe[:emb.size(0)]\n        else:\n            emb = emb + self.pe[step]\n        emb = self.dropout(emb)\n        return emb\n\n\nclass VecEmbedding(nn.Module):\n    def __init__(self, vec_size,\n                 emb_dim,\n                 position_encoding=False,\n                 dropout=0):\n        super(VecEmbedding, self).__init__()\n        self.embedding_size = emb_dim\n        self.proj = nn.Linear(vec_size, emb_dim, bias=False)\n        self.word_padding_idx = 0  # vector seqs are zero-padded\n        self.position_encoding = position_encoding\n\n        if self.position_encoding:\n            self.pe = PositionalEncoding(dropout, self.embedding_size)\n\n    def forward(self, x, step=None):\n        """"""\n        Args:\n            x (FloatTensor): input, ``(len, batch, 1, vec_feats)``.\n\n        Returns:\n            FloatTensor: embedded vecs ``(len, batch, embedding_size)``.\n        """"""\n        x = self.proj(x).squeeze(2)\n        if self.position_encoding:\n            x = self.pe(x, step=step)\n\n        return x\n\n    def load_pretrained_vectors(self, file):\n        assert not file\n\n\nclass Embeddings(nn.Module):\n    """"""Words embeddings for encoder/decoder.\n\n    Additionally includes ability to add sparse input features\n    based on ""Linguistic Input Features Improve Neural Machine Translation""\n    :cite:`sennrich2016linguistic`.\n\n\n    .. mermaid::\n\n       graph LR\n          A[Input]\n          C[Feature 1 Lookup]\n          A-->B[Word Lookup]\n          A-->C\n          A-->D[Feature N Lookup]\n          B-->E[MLP/Concat]\n          C-->E\n          D-->E\n          E-->F[Output]\n\n    Args:\n        word_vec_size (int): size of the dictionary of embeddings.\n        word_padding_idx (int): padding index for words in the embeddings.\n        feat_padding_idx (List[int]): padding index for a list of features\n                                   in the embeddings.\n        word_vocab_size (int): size of dictionary of embeddings for words.\n        feat_vocab_sizes (List[int], optional): list of size of dictionary\n            of embeddings for each feature.\n        position_encoding (bool): see :class:`~onmt.modules.PositionalEncoding`\n        feat_merge (string): merge action for the features embeddings:\n            concat, sum or mlp.\n        feat_vec_exponent (float): when using `-feat_merge concat`, feature\n            embedding size is N^feat_dim_exponent, where N is the\n            number of values the feature takes.\n        feat_vec_size (int): embedding dimension for features when using\n            `-feat_merge mlp`\n        dropout (float): dropout probability.\n    """"""\n\n    def __init__(self, word_vec_size,\n                 word_vocab_size,\n                 word_padding_idx,\n                 position_encoding=False,\n                 feat_merge=""concat"",\n                 feat_vec_exponent=0.7,\n                 feat_vec_size=-1,\n                 feat_padding_idx=[],\n                 feat_vocab_sizes=[],\n                 dropout=0,\n                 sparse=False,\n                 fix_word_vecs=False):\n        self._validate_args(feat_merge, feat_vocab_sizes, feat_vec_exponent,\n                            feat_vec_size, feat_padding_idx)\n\n        if feat_padding_idx is None:\n            feat_padding_idx = []\n        self.word_padding_idx = word_padding_idx\n\n        self.word_vec_size = word_vec_size\n\n        # Dimensions and padding for constructing the word embedding matrix\n        vocab_sizes = [word_vocab_size]\n        emb_dims = [word_vec_size]\n        pad_indices = [word_padding_idx]\n\n        # Dimensions and padding for feature embedding matrices\n        # (these have no effect if feat_vocab_sizes is empty)\n        if feat_merge == \'sum\':\n            feat_dims = [word_vec_size] * len(feat_vocab_sizes)\n        elif feat_vec_size > 0:\n            feat_dims = [feat_vec_size] * len(feat_vocab_sizes)\n        else:\n            feat_dims = [int(vocab ** feat_vec_exponent)\n                         for vocab in feat_vocab_sizes]\n        vocab_sizes.extend(feat_vocab_sizes)\n        emb_dims.extend(feat_dims)\n        pad_indices.extend(feat_padding_idx)\n\n        # The embedding matrix look-up tables. The first look-up table\n        # is for words. Subsequent ones are for features, if any exist.\n        emb_params = zip(vocab_sizes, emb_dims, pad_indices)\n        embeddings = [nn.Embedding(vocab, dim, padding_idx=pad, sparse=sparse)\n                      for vocab, dim, pad in emb_params]\n        emb_luts = Elementwise(feat_merge, embeddings)\n\n        # The final output size of word + feature vectors. This can vary\n        # from the word vector size if and only if features are defined.\n        # This is the attribute you should access if you need to know\n        # how big your embeddings are going to be.\n        self.embedding_size = (sum(emb_dims) if feat_merge == \'concat\'\n                               else word_vec_size)\n\n        # The sequence of operations that converts the input sequence\n        # into a sequence of embeddings. At minimum this consists of\n        # looking up the embeddings for each word and feature in the\n        # input. Model parameters may require the sequence to contain\n        # additional operations as well.\n        super(Embeddings, self).__init__()\n        self.make_embedding = nn.Sequential()\n        self.make_embedding.add_module(\'emb_luts\', emb_luts)\n\n        if feat_merge == \'mlp\' and len(feat_vocab_sizes) > 0:\n            in_dim = sum(emb_dims)\n            mlp = nn.Sequential(nn.Linear(in_dim, word_vec_size), nn.ReLU())\n            self.make_embedding.add_module(\'mlp\', mlp)\n\n        self.position_encoding = position_encoding\n\n        if self.position_encoding:\n            pe = PositionalEncoding(dropout, self.embedding_size)\n            self.make_embedding.add_module(\'pe\', pe)\n\n        if fix_word_vecs:\n            self.word_lut.weight.requires_grad = False\n\n    def _validate_args(self, feat_merge, feat_vocab_sizes, feat_vec_exponent,\n                       feat_vec_size, feat_padding_idx):\n        if feat_merge == ""sum"":\n            # features must use word_vec_size\n            if feat_vec_exponent != 0.7:\n                warnings.warn(""Merging with sum, but got non-default ""\n                              ""feat_vec_exponent. It will be unused."")\n            if feat_vec_size != -1:\n                warnings.warn(""Merging with sum, but got non-default ""\n                              ""feat_vec_size. It will be unused."")\n        elif feat_vec_size > 0:\n            # features will use feat_vec_size\n            if feat_vec_exponent != -1:\n                warnings.warn(""Not merging with sum and positive ""\n                              ""feat_vec_size, but got non-default ""\n                              ""feat_vec_exponent. It will be unused."")\n        else:\n            if feat_vec_exponent <= 0:\n                raise ValueError(""Using feat_vec_exponent to determine ""\n                                 ""feature vec size, but got feat_vec_exponent ""\n                                 ""less than or equal to 0."")\n        n_feats = len(feat_vocab_sizes)\n        if n_feats != len(feat_padding_idx):\n            raise ValueError(""Got unequal number of feat_vocab_sizes and ""\n                             ""feat_padding_idx ({:d} != {:d})"".format(\n                                n_feats, len(feat_padding_idx)))\n\n    @property\n    def word_lut(self):\n        """"""Word look-up table.""""""\n        return self.make_embedding[0][0]\n\n    @property\n    def emb_luts(self):\n        """"""Embedding look-up table.""""""\n        return self.make_embedding[0]\n\n    def load_pretrained_vectors(self, emb_file):\n        """"""Load in pretrained embeddings.\n\n        Args:\n          emb_file (str) : path to torch serialized embeddings\n        """"""\n\n        if emb_file:\n            pretrained = torch.load(emb_file)\n            pretrained_vec_size = pretrained.size(1)\n            if self.word_vec_size > pretrained_vec_size:\n                self.word_lut.weight.data[:, :pretrained_vec_size] = pretrained\n            elif self.word_vec_size < pretrained_vec_size:\n                self.word_lut.weight.data \\\n                    .copy_(pretrained[:, :self.word_vec_size])\n            else:\n                self.word_lut.weight.data.copy_(pretrained)\n\n    def forward(self, source, step=None):\n        """"""Computes the embeddings for words and features.\n\n        Args:\n            source (LongTensor): index tensor ``(len, batch, nfeat)``\n\n        Returns:\n            FloatTensor: Word embeddings ``(len, batch, embedding_size)``\n        """"""\n\n        if self.position_encoding:\n            for i, module in enumerate(self.make_embedding._modules.values()):\n                if i == len(self.make_embedding._modules.values()) - 1:\n                    source = module(source, step=step)\n                else:\n                    source = module(source)\n        else:\n            source = self.make_embedding(source)\n\n        return source\n\n    def update_dropout(self, dropout):\n        if self.position_encoding:\n            self._modules[\'make_embedding\'][1].dropout.p = dropout\n'"
onmt/modules/gate.py,3,"b'"""""" ContextGate module """"""\nimport torch\nimport torch.nn as nn\n\n\ndef context_gate_factory(gate_type, embeddings_size, decoder_size,\n                         attention_size, output_size):\n    """"""Returns the correct ContextGate class""""""\n\n    gate_types = {\'source\': SourceContextGate,\n                  \'target\': TargetContextGate,\n                  \'both\': BothContextGate}\n\n    assert gate_type in gate_types, ""Not valid ContextGate type: {0}"".format(\n        gate_type)\n    return gate_types[gate_type](embeddings_size, decoder_size, attention_size,\n                                 output_size)\n\n\nclass ContextGate(nn.Module):\n    """"""\n    Context gate is a decoder module that takes as input the previous word\n    embedding, the current decoder state and the attention state, and\n    produces a gate.\n    The gate can be used to select the input from the target side context\n    (decoder state), from the source context (attention state) or both.\n    """"""\n\n    def __init__(self, embeddings_size, decoder_size,\n                 attention_size, output_size):\n        super(ContextGate, self).__init__()\n        input_size = embeddings_size + decoder_size + attention_size\n        self.gate = nn.Linear(input_size, output_size, bias=True)\n        self.sig = nn.Sigmoid()\n        self.source_proj = nn.Linear(attention_size, output_size)\n        self.target_proj = nn.Linear(embeddings_size + decoder_size,\n                                     output_size)\n\n    def forward(self, prev_emb, dec_state, attn_state):\n        input_tensor = torch.cat((prev_emb, dec_state, attn_state), dim=1)\n        z = self.sig(self.gate(input_tensor))\n        proj_source = self.source_proj(attn_state)\n        proj_target = self.target_proj(\n            torch.cat((prev_emb, dec_state), dim=1))\n        return z, proj_source, proj_target\n\n\nclass SourceContextGate(nn.Module):\n    """"""Apply the context gate only to the source context""""""\n\n    def __init__(self, embeddings_size, decoder_size,\n                 attention_size, output_size):\n        super(SourceContextGate, self).__init__()\n        self.context_gate = ContextGate(embeddings_size, decoder_size,\n                                        attention_size, output_size)\n        self.tanh = nn.Tanh()\n\n    def forward(self, prev_emb, dec_state, attn_state):\n        z, source, target = self.context_gate(\n            prev_emb, dec_state, attn_state)\n        return self.tanh(target + z * source)\n\n\nclass TargetContextGate(nn.Module):\n    """"""Apply the context gate only to the target context""""""\n\n    def __init__(self, embeddings_size, decoder_size,\n                 attention_size, output_size):\n        super(TargetContextGate, self).__init__()\n        self.context_gate = ContextGate(embeddings_size, decoder_size,\n                                        attention_size, output_size)\n        self.tanh = nn.Tanh()\n\n    def forward(self, prev_emb, dec_state, attn_state):\n        z, source, target = self.context_gate(prev_emb, dec_state, attn_state)\n        return self.tanh(z * target + source)\n\n\nclass BothContextGate(nn.Module):\n    """"""Apply the context gate to both contexts""""""\n\n    def __init__(self, embeddings_size, decoder_size,\n                 attention_size, output_size):\n        super(BothContextGate, self).__init__()\n        self.context_gate = ContextGate(embeddings_size, decoder_size,\n                                        attention_size, output_size)\n        self.tanh = nn.Tanh()\n\n    def forward(self, prev_emb, dec_state, attn_state):\n        z, source, target = self.context_gate(prev_emb, dec_state, attn_state)\n        return self.tanh((1. - z) * target + z * source)\n'"
onmt/modules/global_attention.py,8,"b'""""""Global attention modules (Luong / Bahdanau)""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom onmt.modules.sparse_activations import sparsemax\nfrom onmt.utils.misc import aeq, sequence_mask\n\n# This class is mainly used by decoder.py for RNNs but also\n# by the CNN / transformer decoder when copy attention is used\n# CNN has its own attention mechanism ConvMultiStepAttention\n# Transformer has its own MultiHeadedAttention\n\n\nclass GlobalAttention(nn.Module):\n    r""""""\n    Global attention takes a matrix and a query vector. It\n    then computes a parameterized convex combination of the matrix\n    based on the input query.\n\n    Constructs a unit mapping a query `q` of size `dim`\n    and a source matrix `H` of size `n x dim`, to an output\n    of size `dim`.\n\n\n    .. mermaid::\n\n       graph BT\n          A[Query]\n          subgraph RNN\n            C[H 1]\n            D[H 2]\n            E[H N]\n          end\n          F[Attn]\n          G[Output]\n          A --> F\n          C --> F\n          D --> F\n          E --> F\n          C -.-> G\n          D -.-> G\n          E -.-> G\n          F --> G\n\n    All models compute the output as\n    :math:`c = \\sum_{j=1}^{\\text{SeqLength}} a_j H_j` where\n    :math:`a_j` is the softmax of a score function.\n    Then then apply a projection layer to [q, c].\n\n    However they\n    differ on how they compute the attention score.\n\n    * Luong Attention (dot, general):\n       * dot: :math:`\\text{score}(H_j,q) = H_j^T q`\n       * general: :math:`\\text{score}(H_j, q) = H_j^T W_a q`\n\n\n    * Bahdanau Attention (mlp):\n       * :math:`\\text{score}(H_j, q) = v_a^T \\text{tanh}(W_a q + U_a h_j)`\n\n\n    Args:\n       dim (int): dimensionality of query and key\n       coverage (bool): use coverage term\n       attn_type (str): type of attention to use, options [dot,general,mlp]\n       attn_func (str): attention function to use, options [softmax,sparsemax]\n\n    """"""\n\n    def __init__(self, dim, coverage=False, attn_type=""dot"",\n                 attn_func=""softmax""):\n        super(GlobalAttention, self).__init__()\n\n        self.dim = dim\n        assert attn_type in [""dot"", ""general"", ""mlp""], (\n            ""Please select a valid attention type (got {:s})."".format(\n                attn_type))\n        self.attn_type = attn_type\n        assert attn_func in [""softmax"", ""sparsemax""], (\n            ""Please select a valid attention function."")\n        self.attn_func = attn_func\n\n        if self.attn_type == ""general"":\n            self.linear_in = nn.Linear(dim, dim, bias=False)\n        elif self.attn_type == ""mlp"":\n            self.linear_context = nn.Linear(dim, dim, bias=False)\n            self.linear_query = nn.Linear(dim, dim, bias=True)\n            self.v = nn.Linear(dim, 1, bias=False)\n        # mlp wants it with bias\n        out_bias = self.attn_type == ""mlp""\n        self.linear_out = nn.Linear(dim * 2, dim, bias=out_bias)\n\n        if coverage:\n            self.linear_cover = nn.Linear(1, dim, bias=False)\n\n    def score(self, h_t, h_s):\n        """"""\n        Args:\n          h_t (FloatTensor): sequence of queries ``(batch, tgt_len, dim)``\n          h_s (FloatTensor): sequence of sources ``(batch, src_len, dim``\n\n        Returns:\n          FloatTensor: raw attention scores (unnormalized) for each src index\n            ``(batch, tgt_len, src_len)``\n        """"""\n\n        # Check input sizes\n        src_batch, src_len, src_dim = h_s.size()\n        tgt_batch, tgt_len, tgt_dim = h_t.size()\n        aeq(src_batch, tgt_batch)\n        aeq(src_dim, tgt_dim)\n        aeq(self.dim, src_dim)\n\n        if self.attn_type in [""general"", ""dot""]:\n            if self.attn_type == ""general"":\n                h_t_ = h_t.view(tgt_batch * tgt_len, tgt_dim)\n                h_t_ = self.linear_in(h_t_)\n                h_t = h_t_.view(tgt_batch, tgt_len, tgt_dim)\n            h_s_ = h_s.transpose(1, 2)\n            # (batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)\n            return torch.bmm(h_t, h_s_)\n        else:\n            dim = self.dim\n            wq = self.linear_query(h_t.view(-1, dim))\n            wq = wq.view(tgt_batch, tgt_len, 1, dim)\n            wq = wq.expand(tgt_batch, tgt_len, src_len, dim)\n\n            uh = self.linear_context(h_s.contiguous().view(-1, dim))\n            uh = uh.view(src_batch, 1, src_len, dim)\n            uh = uh.expand(src_batch, tgt_len, src_len, dim)\n\n            # (batch, t_len, s_len, d)\n            wquh = torch.tanh(wq + uh)\n\n            return self.v(wquh.view(-1, dim)).view(tgt_batch, tgt_len, src_len)\n\n    def forward(self, source, memory_bank, memory_lengths=None, coverage=None):\n        """"""\n\n        Args:\n          source (FloatTensor): query vectors ``(batch, tgt_len, dim)``\n          memory_bank (FloatTensor): source vectors ``(batch, src_len, dim)``\n          memory_lengths (LongTensor): the source context lengths ``(batch,)``\n          coverage (FloatTensor): None (not supported yet)\n\n        Returns:\n          (FloatTensor, FloatTensor):\n\n          * Computed vector ``(tgt_len, batch, dim)``\n          * Attention distribtutions for each query\n            ``(tgt_len, batch, src_len)``\n        """"""\n\n        # one step input\n        if source.dim() == 2:\n            one_step = True\n            source = source.unsqueeze(1)\n        else:\n            one_step = False\n\n        batch, source_l, dim = memory_bank.size()\n        batch_, target_l, dim_ = source.size()\n        aeq(batch, batch_)\n        aeq(dim, dim_)\n        aeq(self.dim, dim)\n        if coverage is not None:\n            batch_, source_l_ = coverage.size()\n            aeq(batch, batch_)\n            aeq(source_l, source_l_)\n\n        if coverage is not None:\n            cover = coverage.view(-1).unsqueeze(1)\n            memory_bank += self.linear_cover(cover).view_as(memory_bank)\n            memory_bank = torch.tanh(memory_bank)\n\n        # compute attention scores, as in Luong et al.\n        align = self.score(source, memory_bank)\n\n        if memory_lengths is not None:\n            mask = sequence_mask(memory_lengths, max_len=align.size(-1))\n            mask = mask.unsqueeze(1)  # Make it broadcastable.\n            align.masked_fill_(~mask, -float(\'inf\'))\n\n        # Softmax or sparsemax to normalize attention weights\n        if self.attn_func == ""softmax"":\n            align_vectors = F.softmax(align.view(batch*target_l, source_l), -1)\n        else:\n            align_vectors = sparsemax(align.view(batch*target_l, source_l), -1)\n        align_vectors = align_vectors.view(batch, target_l, source_l)\n\n        # each context vector c_t is the weighted average\n        # over all the source hidden states\n        c = torch.bmm(align_vectors, memory_bank)\n\n        # concatenate\n        concat_c = torch.cat([c, source], 2).view(batch*target_l, dim*2)\n        attn_h = self.linear_out(concat_c).view(batch, target_l, dim)\n        if self.attn_type in [""general"", ""dot""]:\n            attn_h = torch.tanh(attn_h)\n\n        if one_step:\n            attn_h = attn_h.squeeze(1)\n            align_vectors = align_vectors.squeeze(1)\n\n            # Check output sizes\n            batch_, dim_ = attn_h.size()\n            aeq(batch, batch_)\n            aeq(dim, dim_)\n            batch_, source_l_ = align_vectors.size()\n            aeq(batch, batch_)\n            aeq(source_l, source_l_)\n\n        else:\n            attn_h = attn_h.transpose(0, 1).contiguous()\n            align_vectors = align_vectors.transpose(0, 1).contiguous()\n            # Check output sizes\n            target_l_, batch_, dim_ = attn_h.size()\n            aeq(target_l, target_l_)\n            aeq(batch, batch_)\n            aeq(dim, dim_)\n            target_l_, batch_, source_l_ = align_vectors.size()\n            aeq(target_l, target_l_)\n            aeq(batch, batch_)\n            aeq(source_l, source_l_)\n\n        return attn_h, align_vectors\n'"
onmt/modules/multi_headed_attn.py,5,"b'"""""" Multi-Head Attention module """"""\nimport math\nimport torch\nimport torch.nn as nn\n\nfrom onmt.utils.misc import generate_relative_positions_matrix,\\\n                            relative_matmul\n# from onmt.utils.misc import aeq\n\n\nclass MultiHeadedAttention(nn.Module):\n    """"""Multi-Head Attention module from ""Attention is All You Need""\n    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n\n    Similar to standard `dot` attention but uses\n    multiple attention distributions simulataneously\n    to select relevant items.\n\n    .. mermaid::\n\n       graph BT\n          A[key]\n          B[value]\n          C[query]\n          O[output]\n          subgraph Attn\n            D[Attn 1]\n            E[Attn 2]\n            F[Attn N]\n          end\n          A --> D\n          C --> D\n          A --> E\n          C --> E\n          A --> F\n          C --> F\n          D --> O\n          E --> O\n          F --> O\n          B --> O\n\n    Also includes several additional tricks.\n\n    Args:\n       head_count (int): number of parallel heads\n       model_dim (int): the dimension of keys/values/queries,\n           must be divisible by head_count\n       dropout (float): dropout parameter\n    """"""\n\n    def __init__(self, head_count, model_dim, dropout=0.1,\n                 max_relative_positions=0):\n        assert model_dim % head_count == 0\n        self.dim_per_head = model_dim // head_count\n        self.model_dim = model_dim\n\n        super(MultiHeadedAttention, self).__init__()\n        self.head_count = head_count\n\n        self.linear_keys = nn.Linear(model_dim,\n                                     head_count * self.dim_per_head)\n        self.linear_values = nn.Linear(model_dim,\n                                       head_count * self.dim_per_head)\n        self.linear_query = nn.Linear(model_dim,\n                                      head_count * self.dim_per_head)\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(dropout)\n        self.final_linear = nn.Linear(model_dim, model_dim)\n\n        self.max_relative_positions = max_relative_positions\n\n        if max_relative_positions > 0:\n            vocab_size = max_relative_positions * 2 + 1\n            self.relative_positions_embeddings = nn.Embedding(\n                vocab_size, self.dim_per_head)\n\n    def forward(self, key, value, query, mask=None,\n                layer_cache=None, attn_type=None):\n        """"""\n        Compute the context vector and the attention vectors.\n\n        Args:\n           key (FloatTensor): set of `key_len`\n               key vectors ``(batch, key_len, dim)``\n           value (FloatTensor): set of `key_len`\n               value vectors ``(batch, key_len, dim)``\n           query (FloatTensor): set of `query_len`\n               query vectors  ``(batch, query_len, dim)``\n           mask: binary mask 1/0 indicating which keys have\n               zero / non-zero attention ``(batch, query_len, key_len)``\n        Returns:\n           (FloatTensor, FloatTensor):\n\n           * output context vectors ``(batch, query_len, dim)``\n           * Attention vector in heads ``(batch, head, query_len, key_len)``.\n        """"""\n\n        # CHECKS\n        # batch, k_len, d = key.size()\n        # batch_, k_len_, d_ = value.size()\n        # aeq(batch, batch_)\n        # aeq(k_len, k_len_)\n        # aeq(d, d_)\n        # batch_, q_len, d_ = query.size()\n        # aeq(batch, batch_)\n        # aeq(d, d_)\n        # aeq(self.model_dim % 8, 0)\n        # if mask is not None:\n        #    batch_, q_len_, k_len_ = mask.size()\n        #    aeq(batch_, batch)\n        #    aeq(k_len_, k_len)\n        #    aeq(q_len_ == q_len)\n        # END CHECKS\n\n        batch_size = key.size(0)\n        dim_per_head = self.dim_per_head\n        head_count = self.head_count\n        key_len = key.size(1)\n        query_len = query.size(1)\n\n        def shape(x):\n            """"""Projection.""""""\n            return x.view(batch_size, -1, head_count, dim_per_head) \\\n                .transpose(1, 2)\n\n        def unshape(x):\n            """"""Compute context.""""""\n            return x.transpose(1, 2).contiguous() \\\n                    .view(batch_size, -1, head_count * dim_per_head)\n\n        # 1) Project key, value, and query.\n        if layer_cache is not None:\n            if attn_type == ""self"":\n                query, key, value = self.linear_query(query),\\\n                                    self.linear_keys(query),\\\n                                    self.linear_values(query)\n                key = shape(key)\n                value = shape(value)\n                if layer_cache[""self_keys""] is not None:\n                    key = torch.cat(\n                        (layer_cache[""self_keys""], key),\n                        dim=2)\n                if layer_cache[""self_values""] is not None:\n                    value = torch.cat(\n                        (layer_cache[""self_values""], value),\n                        dim=2)\n                layer_cache[""self_keys""] = key\n                layer_cache[""self_values""] = value\n            elif attn_type == ""context"":\n                query = self.linear_query(query)\n                if layer_cache[""memory_keys""] is None:\n                    key, value = self.linear_keys(key),\\\n                                 self.linear_values(value)\n                    key = shape(key)\n                    value = shape(value)\n                else:\n                    key, value = layer_cache[""memory_keys""],\\\n                               layer_cache[""memory_values""]\n                layer_cache[""memory_keys""] = key\n                layer_cache[""memory_values""] = value\n        else:\n            key = self.linear_keys(key)\n            value = self.linear_values(value)\n            query = self.linear_query(query)\n            key = shape(key)\n            value = shape(value)\n\n        if self.max_relative_positions > 0 and attn_type == ""self"":\n            key_len = key.size(2)\n            # 1 or key_len x key_len\n            relative_positions_matrix = generate_relative_positions_matrix(\n                key_len, self.max_relative_positions,\n                cache=True if layer_cache is not None else False)\n            #  1 or key_len x key_len x dim_per_head\n            relations_keys = self.relative_positions_embeddings(\n                relative_positions_matrix.to(key.device))\n            #  1 or key_len x key_len x dim_per_head\n            relations_values = self.relative_positions_embeddings(\n                relative_positions_matrix.to(key.device))\n\n        query = shape(query)\n\n        key_len = key.size(2)\n        query_len = query.size(2)\n\n        # 2) Calculate and scale scores.\n        query = query / math.sqrt(dim_per_head)\n        # batch x num_heads x query_len x key_len\n        query_key = torch.matmul(query, key.transpose(2, 3))\n\n        if self.max_relative_positions > 0 and attn_type == ""self"":\n            scores = query_key + relative_matmul(query, relations_keys, True)\n        else:\n            scores = query_key\n        scores = scores.float()\n\n        if mask is not None:\n            mask = mask.unsqueeze(1)  # [B, 1, 1, T_values]\n            scores = scores.masked_fill(mask, -1e18)\n\n        # 3) Apply attention dropout and compute context vectors.\n        attn = self.softmax(scores).to(query.dtype)\n        drop_attn = self.dropout(attn)\n\n        context_original = torch.matmul(drop_attn, value)\n\n        if self.max_relative_positions > 0 and attn_type == ""self"":\n            context = unshape(context_original\n                              + relative_matmul(drop_attn,\n                                                relations_values,\n                                                False))\n        else:\n            context = unshape(context_original)\n\n        output = self.final_linear(context)\n        # CHECK\n        # batch_, q_len_, d_ = output.size()\n        # aeq(q_len, q_len_)\n        # aeq(batch, batch_)\n        # aeq(d, d_)\n\n        # Return multi-head attn\n        attns = attn \\\n            .view(batch_size, head_count,\n                  query_len, key_len)\n\n        return output, attns\n\n    def update_dropout(self, dropout):\n        self.dropout.p = dropout\n'"
onmt/modules/position_ffn.py,1,"b'""""""Position feed-forward network from ""Attention is All You Need"".""""""\n\nimport torch.nn as nn\n\n\nclass PositionwiseFeedForward(nn.Module):\n    """""" A two-layer Feed-Forward-Network with residual layer norm.\n\n    Args:\n        d_model (int): the size of input for the first-layer of the FFN.\n        d_ff (int): the hidden layer size of the second-layer\n            of the FNN.\n        dropout (float): dropout probability in :math:`[0, 1)`.\n    """"""\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n        self.dropout_2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        """"""Layer definition.\n\n        Args:\n            x: ``(batch_size, input_len, model_dim)``\n\n        Returns:\n            (FloatTensor): Output ``(batch_size, input_len, model_dim)``.\n        """"""\n\n        inter = self.dropout_1(self.relu(self.w_1(self.layer_norm(x))))\n        output = self.dropout_2(self.w_2(inter))\n        return output + x\n\n    def update_dropout(self, dropout):\n        self.dropout_1.p = dropout\n        self.dropout_2.p = dropout\n'"
onmt/modules/source_noise.py,26,"b'import math\nimport torch\n\n\ndef aeq(ref, *args):\n    for i, e in enumerate(args):\n        assert ref == e, ""%s != %s (element %d)"" % (str(ref), str(e), i)\n\n\nclass NoiseBase(object):\n    def __init__(self, prob, pad_idx=1, device_id=""cpu"",\n                 ids_to_noise=[], **kwargs):\n        self.prob = prob\n        self.pad_idx = 1\n        self.skip_first = 1\n        self.device_id = device_id\n        self.ids_to_noise = set([t.item() for t in ids_to_noise])\n\n    def __call__(self, batch):\n        return self.noise_batch(batch)\n\n    def to_device(self, t):\n        return t.to(torch.device(self.device_id))\n\n    def noise_batch(self, batch):\n        source, lengths = batch.src if isinstance(batch.src, tuple) \\\n            else (batch.src, [None] * batch.src.size(1))\n        # noise_skip = batch.noise_skip\n        # aeq(len(batch.noise_skip) == source.size(1))\n\n        # source is [src_len x bs x feats]\n        skipped = source[:self.skip_first, :, :]\n        source = source[self.skip_first:]\n        for i in range(source.size(1)):\n            if hasattr(batch, \'corpus_id\'):\n                corpus_id = batch.corpus_id[i]\n                if corpus_id.item() not in self.ids_to_noise:\n                    continue\n            tokens = source[:, i, 0]\n            mask = tokens.ne(self.pad_idx)\n\n            masked_tokens = tokens[mask]\n            noisy_tokens, length = self.noise_source(\n                masked_tokens, length=lengths[i])\n\n            lengths[i] = length\n\n            # source might increase length so we need to resize the whole\n            # tensor\n            delta = length - (source.size(0) - self.skip_first)\n            if delta > 0:\n                pad = torch.ones([delta],\n                                 device=source.device,\n                                 dtype=source.dtype)\n                pad *= self.pad_idx\n                pad = pad.unsqueeze(1).expand(-1, 15).unsqueeze(2)\n\n                source = torch.cat([source, source])\n            source[:noisy_tokens.size(0), i, 0] = noisy_tokens\n\n        source = torch.cat([skipped, source])\n\n        # remove useless pad\n        max_len = lengths.max()\n        source = source[:max_len, :, :]\n\n        batch.src = source, lengths\n        return batch\n\n    def noise_source(self, source, **kwargs):\n        raise NotImplementedError()\n\n\nclass MaskNoise(NoiseBase):\n    def noise_batch(self, batch):\n        raise ValueError(""MaskNoise has not been updated to tensor noise"")\n    # def s(self, tokens):\n    #     prob = self.prob\n    #     r = torch.rand([len(tokens)])\n    #     mask = False\n    #     masked = []\n    #     for i, tok in enumerate(tokens):\n    #         if tok.startswith(subword_prefix):\n    #             if r[i].item() <= prob:\n    #                 masked.append(mask_tok)\n    #                 mask = True\n    #             else:\n    #                 masked.append(tok)\n    #                 mask = False\n    #         else:\n    #             if mask:\n    #                 pass\n    #             else:\n    #                 masked.append(tok)\n    #     return masked\n\n\nclass SenShufflingNoise(NoiseBase):\n    def __init__(self, *args, end_of_sentence_mask=None, **kwargs):\n        super(SenShufflingNoise, self).__init__(*args, **kwargs)\n        assert end_of_sentence_mask is not None\n        self.end_of_sentence_mask = self.to_device(end_of_sentence_mask)\n\n    def is_end_of_sentence(self, source):\n        return self.end_of_sentence_mask.gather(0, source)\n\n    def noise_source(self, source, length=None, **kwargs):\n        # aeq(source.size(0), length)\n        full_stops = self.is_end_of_sentence(source)\n        # Pretend it ends with a full stop so last span is a sentence\n        full_stops[-1] = 1\n\n        # Tokens that are full stops, where the previous token is not\n        sentence_ends = (full_stops[1:] * ~full_stops[:-1]).nonzero() + 2\n        result = source.clone()\n\n        num_sentences = sentence_ends.size(0)\n        num_to_permute = math.ceil((num_sentences * 2 * self.prob) / 2.0)\n        substitutions = torch.randperm(num_sentences)[:num_to_permute]\n        ordering = torch.arange(0, num_sentences)\n        ordering[substitutions] = substitutions[torch.randperm(num_to_permute)]\n\n        index = 0\n        for i in ordering:\n            sentence = source[(sentence_ends[i - 1] if i >\n                               0 else 1):sentence_ends[i]]\n            result[index:index + sentence.size(0)] = sentence\n            index += sentence.size(0)\n        # aeq(source.size(0), length)\n        return result, length\n\n\nclass InfillingNoise(NoiseBase):\n    def __init__(self, *args, infilling_poisson_lambda=3.0,\n                 word_start_mask=None, **kwargs):\n        super(InfillingNoise, self).__init__(*args, **kwargs)\n        self.poisson_lambda = infilling_poisson_lambda\n        self.mask_span_distribution = self._make_poisson(self.poisson_lambda)\n        self.mask_idx = 0\n        assert word_start_mask is not None\n        self.word_start_mask = self.to_device(word_start_mask)\n\n        # -1: keep everything (i.e. 1 mask per token)\n        #  0: replace everything (i.e. no mask)\n        #  1: 1 mask per span\n        self.replace_length = 1\n\n    def _make_poisson(self, poisson_lambda):\n        # fairseq/data/denoising_dataset.py\n        _lambda = poisson_lambda\n\n        lambda_to_the_k = 1\n        e_to_the_minus_lambda = math.exp(-_lambda)\n        k_factorial = 1\n        ps = []\n        for k in range(0, 128):\n            ps.append(e_to_the_minus_lambda * lambda_to_the_k / k_factorial)\n            lambda_to_the_k *= _lambda\n            k_factorial *= (k + 1)\n            if ps[-1] < 0.0000001:\n                break\n        ps = torch.tensor(ps, device=torch.device(self.device_id))\n        return torch.distributions.Categorical(ps)\n\n    def is_word_start(self, source):\n        # print(""src size: "", source.size())\n        # print(""ws size: "", self.word_start_mask.size())\n        # print(""max: "", source.max())\n        # assert source.max() < self.word_start_mask.size(0)\n        # assert source.min() >= 0\n        return self.word_start_mask.gather(0, source)\n\n    def noise_source(self, source, **kwargs):\n\n        is_word_start = self.is_word_start(source)\n        # assert source.size() == is_word_start.size()\n        # aeq(source.eq(self.pad_idx).long().sum(), 0)\n\n        # we manually add this hypothesis since it\'s required for the rest\n        # of the function and kindof make sense\n        is_word_start[-1] = 0\n\n        p = self.prob\n        num_to_mask = (is_word_start.float().sum() * p).ceil().long()\n        num_inserts = 0\n        if num_to_mask == 0:\n            return source\n\n        if self.mask_span_distribution is not None:\n            lengths = self.mask_span_distribution.sample(\n                sample_shape=(num_to_mask,))\n\n            # Make sure we have enough to mask\n            cum_length = torch.cumsum(lengths, 0)\n            while cum_length[-1] < num_to_mask:\n                lengths = torch.cat([\n                    lengths,\n                    self.mask_span_distribution.sample(\n                        sample_shape=(num_to_mask,))\n                ], dim=0)\n                cum_length = torch.cumsum(lengths, 0)\n\n            # Trim to masking budget\n            i = 0\n            while cum_length[i] < num_to_mask:\n                i += 1\n            lengths[i] = num_to_mask - (0 if i == 0 else cum_length[i - 1])\n            num_to_mask = i + 1\n            lengths = lengths[:num_to_mask]\n\n            # Handle 0-length mask (inserts) separately\n            lengths = lengths[lengths > 0]\n            num_inserts = num_to_mask - lengths.size(0)\n            num_to_mask -= num_inserts\n            if num_to_mask == 0:\n                return self.add_insertion_noise(\n                    source, num_inserts / source.size(0))\n            # assert (lengths > 0).all()\n        else:\n            raise ValueError(""Not supposed to be there"")\n            lengths = torch.ones((num_to_mask,), device=source.device).long()\n        # assert is_word_start[-1] == 0\n        word_starts = is_word_start.nonzero()\n        indices = word_starts[torch.randperm(word_starts.size(0))[\n            :num_to_mask]].squeeze(1)\n\n        source_length = source.size(0)\n        # TODO why?\n        # assert source_length - 1 not in indices\n        to_keep = torch.ones(\n            source_length,\n            dtype=torch.bool,\n            device=source.device)\n\n        is_word_start = is_word_start.long()\n        # acts as a long length, so spans don\'t go over the end of doc\n        is_word_start[-1] = 10e5\n        if self.replace_length == 0:\n            to_keep[indices] = 0\n        else:\n            # keep index, but replace it with [MASK]\n            source[indices] = self.mask_idx\n            # random ratio disabled\n            # source[indices[mask_random]] = torch.randint(\n            #     1, len(self.vocab), size=(mask_random.sum(),))\n\n        # if self.mask_span_distribution is not None:\n        # assert len(lengths.size()) == 1\n        # assert lengths.size() == indices.size()\n        lengths -= 1\n        while indices.size(0) > 0:\n            # assert lengths.size() == indices.size()\n            lengths -= is_word_start[indices + 1].long()\n            uncompleted = lengths >= 0\n            indices = indices[uncompleted] + 1\n\n            # mask_random = mask_random[uncompleted]\n            lengths = lengths[uncompleted]\n            if self.replace_length != -1:\n                # delete token\n                to_keep[indices] = 0\n            else:\n                # keep index, but replace it with [MASK]\n                source[indices] = self.mask_idx\n                # random ratio disabled\n                # source[indices[mask_random]] = torch.randint(\n                #     1, len(self.vocab), size=(mask_random.sum(),))\n        # else:\n        #     # A bit faster when all lengths are 1\n        #     while indices.size(0) > 0:\n        #         uncompleted = is_word_start[indices + 1] == 0\n        #         indices = indices[uncompleted] + 1\n        #         mask_random = mask_random[uncompleted]\n        #         if self.replace_length != -1:\n        #             # delete token\n        #             to_keep[indices] = 0\n        #         else:\n        #             # keep index, but replace it with [MASK]\n        #             source[indices] = self.mask_idx\n        #             source[indices[mask_random]] = torch.randint(\n        #                 1, len(self.vocab), size=(mask_random.sum(),))\n\n        #         assert source_length - 1 not in indices\n\n        source = source[to_keep]\n\n        if num_inserts > 0:\n            source = self.add_insertion_noise(\n                source, num_inserts / source.size(0))\n\n        # aeq(source.eq(self.pad_idx).long().sum(), 0)\n        final_length = source.size(0)\n        return source, final_length\n\n    def add_insertion_noise(self, tokens, p):\n        if p == 0.0:\n            return tokens\n\n        num_tokens = tokens.size(0)\n        n = int(math.ceil(num_tokens * p))\n\n        noise_indices = torch.randperm(num_tokens + n - 2)[:n] + 1\n        noise_mask = torch.zeros(\n            size=(\n                num_tokens + n,\n            ),\n            dtype=torch.bool,\n            device=tokens.device)\n        noise_mask[noise_indices] = 1\n        result = torch.ones([n + len(tokens)],\n                            dtype=torch.long,\n                            device=tokens.device) * -1\n\n        # random ratio disabled\n        # num_random = int(math.ceil(n * self.random_ratio))\n        result[noise_indices] = self.mask_idx\n        # result[noise_indices[:num_random]] = torch.randint(\n        #    low=1, high=len(self.vocab), size=(num_random,))\n\n        result[~noise_mask] = tokens\n\n        # assert (result >= 0).all()\n        return result\n\n\nclass MultiNoise(NoiseBase):\n    NOISES = {\n        ""sen_shuffling"": SenShufflingNoise,\n        ""infilling"": InfillingNoise,\n        ""mask"": MaskNoise\n    }\n\n    def __init__(self, noises=[], probs=[], **kwargs):\n        assert len(noises) == len(probs)\n        super(MultiNoise, self).__init__(probs, **kwargs)\n\n        self.noises = []\n        for i, n in enumerate(noises):\n            cls = MultiNoise.NOISES.get(n)\n            if n is None:\n                raise ValueError(""Unknown noise function \'%s\'"" % n)\n            else:\n                noise = cls(probs[i], **kwargs)\n                self.noises.append(noise)\n\n    def noise_source(self, source, length=None, **kwargs):\n        for noise in self.noises:\n            source, length = noise.noise_source(\n                source, length=length, **kwargs)\n        return source, length\n'"
onmt/modules/sparse_activations.py,7,"b'""""""\nAn implementation of sparsemax (Martins & Astudillo, 2016). See\n:cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n\nBy Ben Peters and Vlad Niculae\n""""""\n\nimport torch\nfrom torch.autograd import Function\nimport torch.nn as nn\n\n\ndef _make_ix_like(input, dim=0):\n    d = input.size(dim)\n    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n    view = [1] * input.dim()\n    view[0] = -1\n    return rho.view(view).transpose(0, dim)\n\n\ndef _threshold_and_support(input, dim=0):\n    """"""Sparsemax building block: compute the threshold\n\n    Args:\n        input: any dimension\n        dim: dimension along which to apply the sparsemax\n\n    Returns:\n        the threshold value\n    """"""\n\n    input_srt, _ = torch.sort(input, descending=True, dim=dim)\n    input_cumsum = input_srt.cumsum(dim) - 1\n    rhos = _make_ix_like(input, dim)\n    support = rhos * input_srt > input_cumsum\n\n    support_size = support.sum(dim=dim).unsqueeze(dim)\n    tau = input_cumsum.gather(dim, support_size - 1)\n    tau /= support_size.to(input.dtype)\n    return tau, support_size\n\n\nclass SparsemaxFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, dim=0):\n        """"""sparsemax: normalizing sparse transform (a la softmax)\n\n        Parameters:\n            input (Tensor): any shape\n            dim: dimension along which to apply sparsemax\n\n        Returns:\n            output (Tensor): same shape as input\n        """"""\n        ctx.dim = dim\n        max_val, _ = input.max(dim=dim, keepdim=True)\n        input -= max_val  # same numerical stability trick as for softmax\n        tau, supp_size = _threshold_and_support(input, dim=dim)\n        output = torch.clamp(input - tau, min=0)\n        ctx.save_for_backward(supp_size, output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output = ctx.saved_tensors\n        dim = ctx.dim\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n\n        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n        v_hat = v_hat.unsqueeze(dim)\n        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n        return grad_input, None\n\n\nsparsemax = SparsemaxFunction.apply\n\n\nclass Sparsemax(nn.Module):\n\n    def __init__(self, dim=0):\n        self.dim = dim\n        super(Sparsemax, self).__init__()\n\n    def forward(self, input):\n        return sparsemax(input, self.dim)\n\n\nclass LogSparsemax(nn.Module):\n\n    def __init__(self, dim=0):\n        self.dim = dim\n        super(LogSparsemax, self).__init__()\n\n    def forward(self, input):\n        return torch.log(sparsemax(input, self.dim))\n'"
onmt/modules/sparse_losses.py,7,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom onmt.modules.sparse_activations import _threshold_and_support\nfrom onmt.utils.misc import aeq\n\n\nclass SparsemaxLossFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, target):\n        """"""\n        input (FloatTensor): ``(n, num_classes)``.\n        target (LongTensor): ``(n,)``, the indices of the target classes\n        """"""\n        input_batch, classes = input.size()\n        target_batch = target.size(0)\n        aeq(input_batch, target_batch)\n\n        z_k = input.gather(1, target.unsqueeze(1)).squeeze()\n        tau_z, support_size = _threshold_and_support(input, dim=1)\n        support = input > tau_z\n        x = torch.where(\n            support, input**2 - tau_z**2,\n            torch.tensor(0.0, device=input.device)\n        ).sum(dim=1)\n        ctx.save_for_backward(input, target, tau_z)\n        # clamping necessary because of numerical errors: loss should be lower\n        # bounded by zero, but negative values near zero are possible without\n        # the clamp\n        return torch.clamp(x / 2 - z_k + 0.5, min=0.0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, target, tau_z = ctx.saved_tensors\n        sparsemax_out = torch.clamp(input - tau_z, min=0)\n        delta = torch.zeros_like(sparsemax_out)\n        delta.scatter_(1, target.unsqueeze(1), 1)\n        return sparsemax_out - delta, None\n\n\nsparsemax_loss = SparsemaxLossFunction.apply\n\n\nclass SparsemaxLoss(nn.Module):\n    """"""\n    An implementation of sparsemax loss, first proposed in\n    :cite:`DBLP:journals/corr/MartinsA16`. If using\n    a sparse output layer, it is not possible to use negative log likelihood\n    because the loss is infinite in the case the target is assigned zero\n    probability. Inputs to SparsemaxLoss are arbitrary dense real-valued\n    vectors (like in nn.CrossEntropyLoss), not probability vectors (like in\n    nn.NLLLoss).\n    """"""\n\n    def __init__(self, weight=None, ignore_index=-100,\n                 reduction=\'elementwise_mean\'):\n        assert reduction in [\'elementwise_mean\', \'sum\', \'none\']\n        self.reduction = reduction\n        self.weight = weight\n        self.ignore_index = ignore_index\n        super(SparsemaxLoss, self).__init__()\n\n    def forward(self, input, target):\n        loss = sparsemax_loss(input, target)\n        if self.ignore_index >= 0:\n            ignored_positions = target == self.ignore_index\n            size = float((target.size(0) - ignored_positions.sum()).item())\n            loss.masked_fill_(ignored_positions, 0.0)\n        else:\n            size = float(target.size(0))\n        if self.reduction == \'sum\':\n            loss = loss.sum()\n        elif self.reduction == \'elementwise_mean\':\n            loss = loss.sum() / size\n        return loss\n'"
onmt/modules/structured_attention.py,5,"b'import torch.nn as nn\nimport torch\nimport torch.cuda\n\n\nclass MatrixTree(nn.Module):\n    """"""Implementation of the matrix-tree theorem for computing marginals\n    of non-projective dependency parsing. This attention layer is used\n    in the paper ""Learning Structured Text Representations""\n    :cite:`DBLP:journals/corr/LiuL17d`.\n    """"""\n\n    def __init__(self, eps=1e-5):\n        self.eps = eps\n        super(MatrixTree, self).__init__()\n\n    def forward(self, input):\n        laplacian = input.exp() + self.eps\n        output = input.clone()\n        for b in range(input.size(0)):\n            lap = laplacian[b].masked_fill(\n                torch.eye(input.size(1), device=input.device).ne(0), 0)\n            lap = -lap + torch.diag(lap.sum(0))\n            # store roots on diagonal\n            lap[0] = input[b].diag().exp()\n            inv_laplacian = lap.inverse()\n\n            factor = inv_laplacian.diag().unsqueeze(1)\\\n                                         .expand_as(input[b]).transpose(0, 1)\n            term1 = input[b].exp().mul(factor).clone()\n            term2 = input[b].exp().mul(inv_laplacian.transpose(0, 1)).clone()\n            term1[:, 0] = 0\n            term2[0] = 0\n            output[b] = term1 - term2\n            roots_output = input[b].diag().exp().mul(\n                inv_laplacian.transpose(0, 1)[0])\n            output[b] = output[b] + torch.diag(roots_output)\n        return output\n'"
onmt/modules/util_class.py,2,"b'"""""" Misc classes """"""\nimport torch\nimport torch.nn as nn\n\n\n# At the moment this class is only used by embeddings.Embeddings look-up tables\nclass Elementwise(nn.ModuleList):\n    """"""\n    A simple network container.\n    Parameters are a list of modules.\n    Inputs are a 3d Tensor whose last dimension is the same length\n    as the list.\n    Outputs are the result of applying modules to inputs elementwise.\n    An optional merge parameter allows the outputs to be reduced to a\n    single Tensor.\n    """"""\n\n    def __init__(self, merge=None, *args):\n        assert merge in [None, \'first\', \'concat\', \'sum\', \'mlp\']\n        self.merge = merge\n        super(Elementwise, self).__init__(*args)\n\n    def forward(self, inputs):\n        inputs_ = [feat.squeeze(2) for feat in inputs.split(1, dim=2)]\n        assert len(self) == len(inputs_)\n        outputs = [f(x) for f, x in zip(self, inputs_)]\n        if self.merge == \'first\':\n            return outputs[0]\n        elif self.merge == \'concat\' or self.merge == \'mlp\':\n            return torch.cat(outputs, 2)\n        elif self.merge == \'sum\':\n            return sum(outputs)\n        else:\n            return outputs\n\n\nclass Cast(nn.Module):\n    """"""\n    Basic layer that casts its input to a specific data type. The same tensor\n    is returned if the data type is already correct.\n    """"""\n\n    def __init__(self, dtype):\n        super(Cast, self).__init__()\n        self._dtype = dtype\n\n    def forward(self, x):\n        return x.to(self._dtype)\n'"
onmt/modules/weight_norm.py,24,"b'""""""  Weights normalization modules  """"""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\n\ndef get_var_maybe_avg(namespace, var_name, training, polyak_decay):\n    """""" utility for retrieving polyak averaged params\n        Update average\n    """"""\n    v = getattr(namespace, var_name)\n    v_avg = getattr(namespace, var_name + \'_avg\')\n    v_avg -= (1 - polyak_decay) * (v_avg - v.data)\n\n    if training:\n        return v\n    else:\n        return v_avg\n\n\ndef get_vars_maybe_avg(namespace, var_names, training, polyak_decay):\n    """""" utility for retrieving polyak averaged params """"""\n    vars = []\n    for vn in var_names:\n        vars.append(get_var_maybe_avg(\n            namespace, vn, training, polyak_decay))\n    return vars\n\n\nclass WeightNormLinear(nn.Linear):\n    """"""\n    Implementation of ""Weight Normalization: A Simple Reparameterization\n    to Accelerate Training of Deep Neural Networks""\n    :cite:`DBLP:journals/corr/SalimansK16`\n\n    As a reparameterization method, weight normalization is same\n    as BatchNormalization, but it doesn\'t depend on minibatch.\n\n    NOTE: This is used nowhere in the code at this stage\n          Vincent Nguyen 05/18/2018\n    """"""\n\n    def __init__(self, in_features, out_features,\n                 init_scale=1., polyak_decay=0.9995):\n        super(WeightNormLinear, self).__init__(\n            in_features, out_features, bias=True)\n\n        self.V = self.weight\n        self.g = Parameter(torch.Tensor(out_features))\n        self.b = self.bias\n\n        self.register_buffer(\n            \'V_avg\', torch.zeros(out_features, in_features))\n        self.register_buffer(\'g_avg\', torch.zeros(out_features))\n        self.register_buffer(\'b_avg\', torch.zeros(out_features))\n\n        self.init_scale = init_scale\n        self.polyak_decay = polyak_decay\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        return\n\n    def forward(self, x, init=False):\n        if init is True:\n            # out_features * in_features\n            self.V.data.copy_(torch.randn(self.V.data.size()).type_as(\n                self.V.data) * 0.05)\n            # norm is out_features * 1\n            v_norm = self.V.data / \\\n                self.V.data.norm(2, 1).expand_as(self.V.data)\n            # batch_size * out_features\n            x_init = F.linear(x, v_norm).data\n            # out_features\n            m_init, v_init = x_init.mean(0).squeeze(\n                0), x_init.var(0).squeeze(0)\n            # out_features\n            scale_init = self.init_scale / \\\n                torch.sqrt(v_init + 1e-10)\n            self.g.data.copy_(scale_init)\n            self.b.data.copy_(-m_init * scale_init)\n            x_init = scale_init.view(1, -1).expand_as(x_init) \\\n                * (x_init - m_init.view(1, -1).expand_as(x_init))\n            self.V_avg.copy_(self.V.data)\n            self.g_avg.copy_(self.g.data)\n            self.b_avg.copy_(self.b.data)\n            return x_init\n        else:\n            v, g, b = get_vars_maybe_avg(self, [\'V\', \'g\', \'b\'],\n                                         self.training,\n                                         polyak_decay=self.polyak_decay)\n            # batch_size * out_features\n            x = F.linear(x, v)\n            scalar = g / torch.norm(v, 2, 1).squeeze(1)\n            x = scalar.view(1, -1).expand_as(x) * x + \\\n                b.view(1, -1).expand_as(x)\n            return x\n\n\nclass WeightNormConv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, init_scale=1.,\n                 polyak_decay=0.9995):\n        super(WeightNormConv2d, self).__init__(in_channels, out_channels,\n                                               kernel_size, stride, padding,\n                                               dilation, groups)\n\n        self.V = self.weight\n        self.g = Parameter(torch.Tensor(out_channels))\n        self.b = self.bias\n\n        self.register_buffer(\'V_avg\', torch.zeros(self.V.size()))\n        self.register_buffer(\'g_avg\', torch.zeros(out_channels))\n        self.register_buffer(\'b_avg\', torch.zeros(out_channels))\n\n        self.init_scale = init_scale\n        self.polyak_decay = polyak_decay\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        return\n\n    def forward(self, x, init=False):\n        if init is True:\n            # out_channels, in_channels // groups, * kernel_size\n            self.V.data.copy_(torch.randn(self.V.data.size()\n                                          ).type_as(self.V.data) * 0.05)\n            v_norm = self.V.data / self.V.data.view(self.out_channels, -1)\\\n                .norm(2, 1).view(self.out_channels, *(\n                    [1] * (len(self.kernel_size) + 1))).expand_as(self.V.data)\n            x_init = F.conv2d(x, v_norm, None, self.stride,\n                              self.padding, self.dilation, self.groups).data\n            t_x_init = x_init.transpose(0, 1).contiguous().view(\n                self.out_channels, -1)\n            m_init, v_init = t_x_init.mean(1).squeeze(\n                1), t_x_init.var(1).squeeze(1)\n            # out_features\n            scale_init = self.init_scale / \\\n                torch.sqrt(v_init + 1e-10)\n            self.g.data.copy_(scale_init)\n            self.b.data.copy_(-m_init * scale_init)\n            scale_init_shape = scale_init.view(\n                1, self.out_channels, *([1] * (len(x_init.size()) - 2)))\n            m_init_shape = m_init.view(\n                1, self.out_channels, *([1] * (len(x_init.size()) - 2)))\n            x_init = scale_init_shape.expand_as(\n                x_init) * (x_init - m_init_shape.expand_as(x_init))\n            self.V_avg.copy_(self.V.data)\n            self.g_avg.copy_(self.g.data)\n            self.b_avg.copy_(self.b.data)\n            return x_init\n        else:\n            v, g, b = get_vars_maybe_avg(\n                self, [\'V\', \'g\', \'b\'], self.training,\n                polyak_decay=self.polyak_decay)\n\n            scalar = torch.norm(v.view(self.out_channels, -1), 2, 1)\n            if len(scalar.size()) == 2:\n                scalar = g / scalar.squeeze(1)\n            else:\n                scalar = g / scalar\n\n            w = scalar.view(self.out_channels, *\n                            ([1] * (len(v.size()) - 1))).expand_as(v) * v\n\n            x = F.conv2d(x, w, b, self.stride,\n                         self.padding, self.dilation, self.groups)\n            return x\n\n# This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)\n\n\nclass WeightNormConvTranspose2d(nn.ConvTranspose2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, output_padding=0, groups=1, init_scale=1.,\n                 polyak_decay=0.9995):\n        super(WeightNormConvTranspose2d, self).__init__(\n            in_channels, out_channels,\n            kernel_size, stride,\n            padding, output_padding,\n            groups)\n        # in_channels, out_channels, *kernel_size\n        self.V = self.weight\n        self.g = Parameter(torch.Tensor(out_channels))\n        self.b = self.bias\n\n        self.register_buffer(\'V_avg\', torch.zeros(self.V.size()))\n        self.register_buffer(\'g_avg\', torch.zeros(out_channels))\n        self.register_buffer(\'b_avg\', torch.zeros(out_channels))\n\n        self.init_scale = init_scale\n        self.polyak_decay = polyak_decay\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        return\n\n    def forward(self, x, init=False):\n        if init is True:\n            # in_channels, out_channels, *kernel_size\n            self.V.data.copy_(torch.randn(self.V.data.size()).type_as(\n                self.V.data) * 0.05)\n            v_norm = self.V.data / self.V.data.transpose(0, 1).contiguous() \\\n                .view(self.out_channels, -1).norm(2, 1).view(\n                    self.in_channels, self.out_channels,\n                    *([1] * len(self.kernel_size))).expand_as(self.V.data)\n            x_init = F.conv_transpose2d(\n                x, v_norm, None, self.stride,\n                self.padding, self.output_padding, self.groups).data\n            # self.out_channels, 1\n            t_x_init = x_init.tranpose(0, 1).contiguous().view(\n                self.out_channels, -1)\n            # out_features\n            m_init, v_init = t_x_init.mean(1).squeeze(\n                1), t_x_init.var(1).squeeze(1)\n            # out_features\n            scale_init = self.init_scale / \\\n                torch.sqrt(v_init + 1e-10)\n            self.g.data.copy_(scale_init)\n            self.b.data.copy_(-m_init * scale_init)\n            scale_init_shape = scale_init.view(\n                1, self.out_channels, *([1] * (len(x_init.size()) - 2)))\n            m_init_shape = m_init.view(\n                1, self.out_channels, *([1] * (len(x_init.size()) - 2)))\n\n            x_init = scale_init_shape.expand_as(x_init)\\\n                * (x_init - m_init_shape.expand_as(x_init))\n            self.V_avg.copy_(self.V.data)\n            self.g_avg.copy_(self.g.data)\n            self.b_avg.copy_(self.b.data)\n            return x_init\n        else:\n            v, g, b = get_vars_maybe_avg(\n                self, [\'V\', \'g\', \'b\'], self.training,\n                polyak_decay=self.polyak_decay)\n            scalar = g / \\\n                torch.norm(v.transpose(0, 1).contiguous().view(\n                    self.out_channels, -1), 2, 1).squeeze(1)\n            w = scalar.view(self.in_channels, self.out_channels,\n                            *([1] * (len(v.size()) - 2))).expand_as(v) * v\n\n            x = F.conv_transpose2d(x, w, b, self.stride,\n                                   self.padding, self.output_padding,\n                                   self.groups)\n            return x\n'"
onmt/tests/__init__.py,0,b''
onmt/tests/test_attention.py,5,"b'""""""\nHere come the tests for attention types and their compatibility\n""""""\nimport unittest\nimport torch\nfrom torch.autograd import Variable\n\nimport onmt\n\n\nclass TestAttention(unittest.TestCase):\n\n    def test_masked_global_attention(self):\n\n        source_lengths = torch.IntTensor([7, 3, 5, 2])\n        # illegal_weights_mask = torch.ByteTensor([\n        #     [0, 0, 0, 0, 0, 0, 0],\n        #     [0, 0, 0, 1, 1, 1, 1],\n        #     [0, 0, 0, 0, 0, 1, 1],\n        #     [0, 0, 1, 1, 1, 1, 1]])\n\n        batch_size = source_lengths.size(0)\n        dim = 20\n\n        memory_bank = Variable(torch.randn(batch_size,\n                                           source_lengths.max(), dim))\n        hidden = Variable(torch.randn(batch_size, dim))\n\n        attn = onmt.modules.GlobalAttention(dim)\n\n        _, alignments = attn(hidden, memory_bank,\n                             memory_lengths=source_lengths)\n        # TODO: fix for pytorch 0.3\n        # illegal_weights = alignments.masked_select(illegal_weights_mask)\n\n        # self.assertEqual(0.0, illegal_weights.data.sum())\n'"
onmt/tests/test_audio_dataset.py,9,"b'# -*- coding: utf-8 -*-\nimport unittest\nfrom onmt.inputters.audio_dataset import AudioSeqField, AudioDataReader\n\nimport itertools\nimport os\nimport shutil\n\nimport torch\nimport torchaudio\n\nfrom onmt.tests.utils_for_tests import product_dict\n\n\nclass TestAudioField(unittest.TestCase):\n    INIT_CASES = list(product_dict(\n        pad_index=[0, 32],\n        batch_first=[False, True],\n        include_lengths=[True, False]))\n\n    PARAMS = list(product_dict(\n        batch_size=[1, 17],\n        max_len=[23],\n        full_length_seq=[0, 5, 16],\n        nfeats=[1, 5]))\n\n    @classmethod\n    def degenerate_case(cls, init_case, params):\n        if params[""batch_size""] < params[""full_length_seq""]:\n            return True\n        return False\n\n    @classmethod\n    def pad_inputs(cls, params):\n        lengths = torch.randint(1, params[""max_len""],\n                                (params[""batch_size""],)).tolist()\n        lengths[params[""full_length_seq""]] = params[""max_len""]\n        fake_input = [\n            torch.randn((params[""nfeats""], lengths[b]))\n            for b in range(params[""batch_size""])]\n        return fake_input, lengths\n\n    @classmethod\n    def numericalize_inputs(cls, init_case, params):\n        bs = params[""batch_size""]\n        max_len = params[""max_len""]\n        lengths = torch.randint(1, max_len, (bs,))\n        lengths[params[""full_length_seq""]] = max_len\n        nfeats = params[""nfeats""]\n        fake_input = torch.full(\n            (bs, 1, nfeats, max_len), init_case[""pad_index""])\n        for b in range(bs):\n            fake_input[b, :, :, :lengths[b]] = torch.randn(\n                (1, nfeats, lengths[b]))\n        if init_case[""include_lengths""]:\n            fake_input = (fake_input, lengths)\n        return fake_input, lengths\n\n    def test_pad_shape_and_lengths(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            if not self.degenerate_case(init_case, params):\n                field = AudioSeqField(**init_case)\n                fake_input, lengths = self.pad_inputs(params)\n                outp = field.pad(fake_input)\n                if init_case[""include_lengths""]:\n                    outp, _ = outp\n                expected_shape = (\n                    params[""batch_size""], 1, params[""nfeats""],\n                    params[""max_len""])\n                self.assertEqual(outp.shape, expected_shape)\n\n    def test_pad_returns_correct_lengths(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            if not self.degenerate_case(init_case, params) and \\\n                    init_case[""include_lengths""]:\n                field = AudioSeqField(**init_case)\n                fake_input, lengths = self.pad_inputs(params)\n                _, outp_lengths = field.pad(fake_input)\n                self.assertEqual(outp_lengths, lengths)\n\n    def test_pad_pads_right_places_and_uses_correct_index(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            if not self.degenerate_case(init_case, params):\n                field = AudioSeqField(**init_case)\n                fake_input, lengths = self.pad_inputs(params)\n                outp = field.pad(fake_input)\n                if init_case[""include_lengths""]:\n                    outp, _ = outp\n                for b in range(params[""batch_size""]):\n                    for s in range(lengths[b], params[""max_len""]):\n                        self.assertTrue(\n                            outp[b, :, :, s].allclose(\n                                torch.tensor(float(init_case[""pad_index""]))))\n\n    def test_numericalize_shape(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            if not self.degenerate_case(init_case, params):\n                field = AudioSeqField(**init_case)\n                fake_input, lengths = self.numericalize_inputs(\n                    init_case, params)\n                outp = field.numericalize(fake_input)\n                if init_case[""include_lengths""]:\n                    outp, _ = outp\n                if init_case[""batch_first""]:\n                    expected_shape = (\n                        params[""batch_size""], 1,\n                        params[""nfeats""], params[""max_len""])\n                else:\n                    expected_shape = (\n                        params[""max_len""], params[""batch_size""],\n                        1, params[""nfeats""])\n                self.assertEqual(expected_shape, outp.shape,\n                                 init_case.__str__())\n\n    def test_process_shape(self):\n        # tests pad and numericalize integration\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            if not self.degenerate_case(init_case, params):\n                field = AudioSeqField(**init_case)\n                fake_input, lengths = self.pad_inputs(params)\n                outp = field.process(fake_input)\n                if init_case[""include_lengths""]:\n                    outp, _ = outp\n                if init_case[""batch_first""]:\n                    expected_shape = (\n                        params[""batch_size""], 1,\n                        params[""nfeats""], params[""max_len""])\n                else:\n                    expected_shape = (\n                        params[""max_len""], params[""batch_size""],\n                        1, params[""nfeats""])\n                self.assertEqual(expected_shape, outp.shape,\n                                 init_case.__str__())\n\n    def test_process_lengths(self):\n        # tests pad and numericalize integration\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            if not self.degenerate_case(init_case, params):\n                if init_case[""include_lengths""]:\n                    field = AudioSeqField(**init_case)\n                    fake_input, lengths = self.pad_inputs(params)\n                    lengths = torch.tensor(lengths, dtype=torch.int)\n                    _, outp_lengths = field.process(fake_input)\n                    self.assertTrue(outp_lengths.eq(lengths).all())\n\n\nclass TestAudioDataReader(unittest.TestCase):\n    # this test touches the file system, so it could be considered an\n    # integration test\n    _THIS_DIR = os.path.dirname(os.path.abspath(__file__))\n    _AUDIO_DATA_DIRNAME = ""test_audio_data""\n    _AUDIO_DATA_DIR = os.path.join(_THIS_DIR, _AUDIO_DATA_DIRNAME)\n    _AUDIO_DATA_FMT = ""test_noise_{:d}.wav""\n    _AUDIO_DATA_PATH_FMT = os.path.join(_AUDIO_DATA_DIR, _AUDIO_DATA_FMT)\n\n    _AUDIO_LIST_DIR = ""test_audio_filenames""\n    # file to hold full paths to audio data\n    _AUDIO_LIST_PATHS_FNAME = ""test_files.txt""\n    _AUDIO_LIST_PATHS_PATH = os.path.join(\n        _AUDIO_LIST_DIR, _AUDIO_LIST_PATHS_FNAME)\n    # file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)\n    _AUDIO_LIST_FNAMES_FNAME = ""test_fnames.txt""\n    _AUDIO_LIST_FNAMES_PATH = os.path.join(\n        _AUDIO_LIST_DIR, _AUDIO_LIST_FNAMES_FNAME)\n\n    # it\'s ok if non-audio files co-exist with audio files in the data dir\n    _JUNK_FILE = os.path.join(\n        _AUDIO_DATA_DIR, ""this_is_junk.txt"")\n\n    _N_EXAMPLES = 20\n    _SAMPLE_RATE = 48000\n    _N_CHANNELS = 2\n\n    @classmethod\n    def setUpClass(cls):\n        if not os.path.exists(cls._AUDIO_DATA_DIR):\n            os.makedirs(cls._AUDIO_DATA_DIR)\n        if not os.path.exists(cls._AUDIO_LIST_DIR):\n            os.makedirs(cls._AUDIO_LIST_DIR)\n\n        with open(cls._JUNK_FILE, ""w"") as f:\n            f.write(""this is some garbage\\nShould have no impact."")\n\n        with open(cls._AUDIO_LIST_PATHS_PATH, ""w"") as f_list_fnames, \\\n                open(cls._AUDIO_LIST_FNAMES_PATH, ""w"") as f_list_paths:\n            lengths = torch.randint(int(.5e5), int(1.5e6), (cls._N_EXAMPLES,))\n            for i in range(cls._N_EXAMPLES):\n                # dividing gets the noise in [-1, 1]\n                white_noise = torch.randn((cls._N_CHANNELS, lengths[i])) / 10\n                f_path = cls._AUDIO_DATA_PATH_FMT.format(i)\n                torchaudio.save(f_path, white_noise, cls._SAMPLE_RATE)\n                f_name_short = cls._AUDIO_DATA_FMT.format(i)\n                f_list_fnames.write(f_name_short + ""\\n"")\n                f_list_paths.write(f_path + ""\\n"")\n\n    @classmethod\n    def tearDownClass(cls):\n        shutil.rmtree(cls._AUDIO_DATA_DIR)\n        shutil.rmtree(cls._AUDIO_LIST_DIR)\n\n    def test_read_from_dir_and_data_file_containing_filenames(self):\n        rdr = AudioDataReader(self._SAMPLE_RATE, window=""hamming"",\n                              window_size=0.02, window_stride=0.01)\n        i = 0  # initialize since there\'s a sanity check on i\n        for i, aud in enumerate(rdr.read(\n                self._AUDIO_LIST_FNAMES_PATH, ""src"", self._AUDIO_DATA_DIR)):\n            self.assertEqual(aud[""src""].shape[0], 481)\n            self.assertEqual(aud[""src_path""],\n                             self._AUDIO_DATA_PATH_FMT.format(i))\n        self.assertGreater(i, 0, ""No audio data was read."")\n\n    def test_read_from_dir_and_data_file_containing_paths(self):\n        rdr = AudioDataReader(self._SAMPLE_RATE, window=""hamming"",\n                              window_size=0.02, window_stride=0.01)\n        i = 0  # initialize since there\'s a sanity check on i\n        for i, aud in enumerate(rdr.read(\n                self._AUDIO_LIST_PATHS_PATH, ""src"", self._AUDIO_DATA_DIR)):\n            self.assertEqual(aud[""src""].shape[0], 481)\n            self.assertEqual(aud[""src_path""],\n                             self._AUDIO_DATA_FMT.format(i))\n        self.assertGreater(i, 0, ""No audio data was read."")\n'"
onmt/tests/test_beam_search.py,45,"b'import unittest\nfrom onmt.translate.beam_search import BeamSearch, GNMTGlobalScorer\n\nfrom copy import deepcopy\n\nimport torch\n\n\nclass GlobalScorerStub(object):\n    alpha = 0\n    beta = 0\n\n    def __init__(self):\n        self.length_penalty = lambda x, alpha: 1.\n        self.cov_penalty = lambda cov, beta: torch.zeros(\n            (1, cov.shape[-2]), device=cov.device, dtype=torch.float)\n        self.has_cov_pen = False\n        self.has_len_pen = False\n\n    def update_global_state(self, beam):\n        pass\n\n    def score(self, beam, scores):\n        return scores\n\n\nclass TestBeamSearch(unittest.TestCase):\n    BLOCKED_SCORE = -10e20\n\n    def test_advance_with_all_repeats_gets_blocked(self):\n        # all beams repeat (beam >= 1 repeat dummy scores)\n        beam_sz = 5\n        n_words = 100\n        repeat_idx = 47\n        ngram_repeat = 3\n        device_init = torch.zeros(1, 1)\n        for batch_sz in [1, 3]:\n            beam = BeamSearch(\n                beam_sz, batch_sz, 0, 1, 2, 2,\n                GlobalScorerStub(), 0, 30,\n                False, ngram_repeat, set(),\n                False, 0.)\n            beam.initialize(device_init, torch.randint(0, 30, (batch_sz,)))\n            for i in range(ngram_repeat + 4):\n                # predict repeat_idx over and over again\n                word_probs = torch.full(\n                    (batch_sz * beam_sz, n_words), -float(\'inf\'))\n                word_probs[0::beam_sz, repeat_idx] = 0\n\n                attns = torch.randn(1, batch_sz * beam_sz, 53)\n                beam.advance(word_probs, attns)\n\n                if i < ngram_repeat:\n                    # before repeat, scores are either 0 or -inf\n                    expected_scores = torch.tensor(\n                        [0] + [-float(\'inf\')] * (beam_sz - 1))\\\n                        .repeat(batch_sz, 1)\n                    self.assertTrue(beam.topk_log_probs.equal(expected_scores))\n                elif i % ngram_repeat == 0:\n                    # on repeat, `repeat_idx` score is BLOCKED_SCORE\n                    # (but it\'s still the best score, thus we have\n                    # [BLOCKED_SCORE, -inf, -inf, -inf, -inf]\n                    expected_scores = torch.tensor(\n                        [0] + [-float(\'inf\')] * (beam_sz - 1))\\\n                        .repeat(batch_sz, 1)\n                    expected_scores[:, 0] = self.BLOCKED_SCORE\n                    self.assertTrue(beam.topk_log_probs.equal(expected_scores))\n                else:\n                    # repetitions keeps maximizing score\n                    # index 0 has been blocked, so repeating=>+0.0 score\n                    # other indexes are -inf so repeating=>BLOCKED_SCORE\n                    # which is higher\n                    expected_scores = torch.tensor(\n                        [0] + [-float(\'inf\')] * (beam_sz - 1))\\\n                        .repeat(batch_sz, 1)\n                    expected_scores[:, :] = self.BLOCKED_SCORE\n                    expected_scores = torch.tensor(\n                        self.BLOCKED_SCORE).repeat(batch_sz, beam_sz)\n\n    def test_advance_with_some_repeats_gets_blocked(self):\n        # beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)\n        beam_sz = 5\n        n_words = 100\n        repeat_idx = 47\n        ngram_repeat = 3\n        no_repeat_score = -2.3\n        repeat_score = -0.1\n        device_init = torch.zeros(1, 1)\n        for batch_sz in [1, 3]:\n            beam = BeamSearch(\n                beam_sz, batch_sz, 0, 1, 2, 2,\n                GlobalScorerStub(), 0, 30,\n                False, ngram_repeat, set(),\n                False, 0.)\n            beam.initialize(device_init, torch.randint(0, 30, (batch_sz,)))\n            for i in range(ngram_repeat + 4):\n                # non-interesting beams are going to get dummy values\n                word_probs = torch.full(\n                    (batch_sz * beam_sz, n_words), -float(\'inf\'))\n                if i == 0:\n                    # on initial round, only predicted scores for beam 0\n                    # matter. Make two predictions. Top one will be repeated\n                    # in beam zero, second one will live on in beam 1.\n                    word_probs[0::beam_sz, repeat_idx] = repeat_score\n                    word_probs[0::beam_sz, repeat_idx +\n                               i + 1] = no_repeat_score\n                else:\n                    # predict the same thing in beam 0\n                    word_probs[0::beam_sz, repeat_idx] = 0\n                    # continue pushing around what beam 1 predicts\n                    word_probs[1::beam_sz, repeat_idx + i + 1] = 0\n                attns = torch.randn(1, batch_sz * beam_sz, 53)\n                beam.advance(word_probs, attns)\n                if i < ngram_repeat:\n                    self.assertFalse(\n                        beam.topk_log_probs[0::beam_sz].eq(\n                            self.BLOCKED_SCORE).any())\n                    self.assertFalse(\n                        beam.topk_log_probs[1::beam_sz].eq(\n                            self.BLOCKED_SCORE).any())\n                elif i == ngram_repeat:\n                    # now beam 0 dies (along with the others), beam 1 -> beam 0\n                    self.assertFalse(\n                        beam.topk_log_probs[:, 0].eq(\n                            self.BLOCKED_SCORE).any())\n\n                    expected = torch.full([batch_sz, beam_sz], float(""-inf""))\n                    expected[:, 0] = no_repeat_score\n                    expected[:, 1] = self.BLOCKED_SCORE\n                    self.assertTrue(\n                        beam.topk_log_probs[:, :].equal(expected))\n                else:\n                    # now beam 0 dies (along with the others), beam 1 -> beam 0\n                    self.assertFalse(\n                        beam.topk_log_probs[:, 0].eq(\n                            self.BLOCKED_SCORE).any())\n\n                    expected = torch.full([batch_sz, beam_sz], float(""-inf""))\n                    expected[:, 0] = no_repeat_score\n                    expected[:, 1:] = self.BLOCKED_SCORE\n                    self.assertTrue(\n                        beam.topk_log_probs.equal(expected))\n\n    def test_repeating_excluded_index_does_not_die(self):\n        # beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)\n        beam_sz = 5\n        n_words = 100\n        repeat_idx = 47  # will be repeated and should be blocked\n        repeat_idx_ignored = 7  # will be repeated and should not be blocked\n        ngram_repeat = 3\n        device_init = torch.zeros(1, 1)\n        for batch_sz in [1, 3]:\n            beam = BeamSearch(\n                beam_sz, batch_sz, 0, 1, 2, 2,\n                GlobalScorerStub(), 0, 30,\n                False, ngram_repeat, {repeat_idx_ignored},\n                False, 0.)\n            beam.initialize(device_init, torch.randint(0, 30, (batch_sz,)))\n            for i in range(ngram_repeat + 4):\n                # non-interesting beams are going to get dummy values\n                word_probs = torch.full(\n                    (batch_sz * beam_sz, n_words), -float(\'inf\'))\n                if i == 0:\n                    word_probs[0::beam_sz, repeat_idx] = -0.1\n                    word_probs[0::beam_sz, repeat_idx + i + 1] = -2.3\n                    word_probs[0::beam_sz, repeat_idx_ignored] = -5.0\n                else:\n                    # predict the same thing in beam 0\n                    word_probs[0::beam_sz, repeat_idx] = 0\n                    # continue pushing around what beam 1 predicts\n                    word_probs[1::beam_sz, repeat_idx + i + 1] = 0\n                    # predict the allowed-repeat again in beam 2\n                    word_probs[2::beam_sz, repeat_idx_ignored] = 0\n                attns = torch.randn(1, batch_sz * beam_sz, 53)\n                beam.advance(word_probs, attns)\n                if i < ngram_repeat:\n                    self.assertFalse(beam.topk_log_probs[:, 0].eq(\n                        self.BLOCKED_SCORE).any())\n                    self.assertFalse(beam.topk_log_probs[:, 1].eq(\n                        self.BLOCKED_SCORE).any())\n                    self.assertFalse(beam.topk_log_probs[:, 2].eq(\n                        self.BLOCKED_SCORE).any())\n                else:\n                    # now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1\n                    # and the rest die\n                    self.assertFalse(beam.topk_log_probs[:, 0].eq(\n                        self.BLOCKED_SCORE).any())\n                    # since all preds after i=0 are 0, we can check\n                    # that the beam is the correct idx by checking that\n                    # the curr score is the initial score\n                    self.assertTrue(beam.topk_log_probs[:, 0].eq(-2.3).all())\n                    self.assertFalse(beam.topk_log_probs[:, 1].eq(\n                        self.BLOCKED_SCORE).all())\n                    self.assertTrue(beam.topk_log_probs[:, 1].eq(-5.0).all())\n\n                    self.assertTrue(beam.topk_log_probs[:, 2].eq(\n                        self.BLOCKED_SCORE).all())\n\n    def test_doesnt_predict_eos_if_shorter_than_min_len(self):\n        # beam 0 will always predict EOS. The other beams will predict\n        # non-eos scores.\n        for batch_sz in [1, 3]:\n            beam_sz = 5\n            n_words = 100\n            _non_eos_idxs = [47, 51, 13, 88, 99]\n            valid_score_dist = torch.log_softmax(torch.tensor(\n                [6., 5., 4., 3., 2., 1.]), dim=0)\n            min_length = 5\n            eos_idx = 2\n            lengths = torch.randint(0, 30, (batch_sz,))\n            beam = BeamSearch(beam_sz, batch_sz, 0, 1, 2, 2,\n                              GlobalScorerStub(),\n                              min_length, 30, False, 0, set(),\n                              False, 0.)\n            device_init = torch.zeros(1, 1)\n            beam.initialize(device_init, lengths)\n            all_attns = []\n            for i in range(min_length + 4):\n                # non-interesting beams are going to get dummy values\n                word_probs = torch.full(\n                    (batch_sz * beam_sz, n_words), -float(\'inf\'))\n                if i == 0:\n                    # ""best"" prediction is eos - that should be blocked\n                    word_probs[0::beam_sz, eos_idx] = valid_score_dist[0]\n                    # include at least beam_sz predictions OTHER than EOS\n                    # that are greater than -1e20\n                    for j, score in zip(_non_eos_idxs, valid_score_dist[1:]):\n                        word_probs[0::beam_sz, j] = score\n                else:\n                    # predict eos in beam 0\n                    word_probs[0::beam_sz, eos_idx] = valid_score_dist[0]\n                    # provide beam_sz other good predictions\n                    for k, (j, score) in enumerate(\n                            zip(_non_eos_idxs, valid_score_dist[1:])):\n                        beam_idx = min(beam_sz - 1, k)\n                        word_probs[beam_idx::beam_sz, j] = score\n\n                attns = torch.randn(1, batch_sz * beam_sz, 53)\n                all_attns.append(attns)\n                beam.advance(word_probs, attns)\n                if i < min_length:\n                    expected_score_dist = \\\n                        (i + 1) * valid_score_dist[1:].unsqueeze(0)\n                    self.assertTrue(\n                        beam.topk_log_probs.allclose(\n                            expected_score_dist))\n                elif i == min_length:\n                    # now the top beam has ended and no others have\n                    self.assertTrue(beam.is_finished[:, 0].eq(1).all())\n                    self.assertTrue(beam.is_finished[:, 1:].eq(0).all())\n                else:  # i > min_length\n                    # not of interest, but want to make sure it keeps running\n                    # since only beam 0 terminates and n_best = 2\n                    pass\n\n    def test_beam_is_done_when_n_best_beams_eos_using_min_length(self):\n        # this is also a test that when block_ngram_repeat=0,\n        # repeating is acceptable\n        beam_sz = 5\n        batch_sz = 3\n        n_words = 100\n        _non_eos_idxs = [47, 51, 13, 88, 99]\n        valid_score_dist = torch.log_softmax(torch.tensor(\n            [6., 5., 4., 3., 2., 1.]), dim=0)\n        min_length = 5\n        eos_idx = 2\n        beam = BeamSearch(\n            beam_sz, batch_sz, 0, 1, 2, 2,\n            GlobalScorerStub(),\n            min_length, 30, False, 0, set(),\n            False, 0.)\n        device_init = torch.zeros(1, 1)\n        beam.initialize(device_init, torch.randint(0, 30, (batch_sz,)))\n        for i in range(min_length + 4):\n            # non-interesting beams are going to get dummy values\n            word_probs = torch.full(\n                (batch_sz * beam_sz, n_words), -float(\'inf\'))\n            if i == 0:\n                # ""best"" prediction is eos - that should be blocked\n                word_probs[0::beam_sz, eos_idx] = valid_score_dist[0]\n                # include at least beam_sz predictions OTHER than EOS\n                # that are greater than -1e20\n                for j, score in zip(_non_eos_idxs, valid_score_dist[1:]):\n                    word_probs[0::beam_sz, j] = score\n            elif i <= min_length:\n                # predict eos in beam 1\n                word_probs[1::beam_sz, eos_idx] = valid_score_dist[0]\n                # provide beam_sz other good predictions in other beams\n                for k, (j, score) in enumerate(\n                        zip(_non_eos_idxs, valid_score_dist[1:])):\n                    beam_idx = min(beam_sz - 1, k)\n                    word_probs[beam_idx::beam_sz, j] = score\n            else:\n                word_probs[0::beam_sz, eos_idx] = valid_score_dist[0]\n                word_probs[1::beam_sz, eos_idx] = valid_score_dist[0]\n                # provide beam_sz other good predictions in other beams\n                for k, (j, score) in enumerate(\n                        zip(_non_eos_idxs, valid_score_dist[1:])):\n                    beam_idx = min(beam_sz - 1, k)\n                    word_probs[beam_idx::beam_sz, j] = score\n\n            attns = torch.randn(1, batch_sz * beam_sz, 53)\n            beam.advance(word_probs, attns)\n            if i < min_length:\n                self.assertFalse(beam.done)\n            elif i == min_length:\n                # beam 1 dies on min_length\n                self.assertTrue(beam.is_finished[:, 1].all())\n                beam.update_finished()\n                self.assertFalse(beam.done)\n            else:  # i > min_length\n                # beam 0 dies on the step after beam 1 dies\n                self.assertTrue(beam.is_finished[:, 0].all())\n                beam.update_finished()\n                self.assertTrue(beam.done)\n\n    def test_beam_returns_attn_with_correct_length(self):\n        beam_sz = 5\n        batch_sz = 3\n        n_words = 100\n        _non_eos_idxs = [47, 51, 13, 88, 99]\n        valid_score_dist = torch.log_softmax(torch.tensor(\n            [6., 5., 4., 3., 2., 1.]), dim=0)\n        min_length = 5\n        eos_idx = 2\n        inp_lens = torch.randint(1, 30, (batch_sz,))\n        beam = BeamSearch(\n            beam_sz, batch_sz, 0, 1, 2, 2,\n            GlobalScorerStub(),\n            min_length, 30, True, 0, set(),\n            False, 0.)\n        device_init = torch.zeros(1, 1)\n        _, _, inp_lens, _ = beam.initialize(device_init, inp_lens)\n        # inp_lens is tiled in initialize, reassign to make attn match\n        for i in range(min_length + 2):\n            # non-interesting beams are going to get dummy values\n            word_probs = torch.full(\n                (batch_sz * beam_sz, n_words), -float(\'inf\'))\n            if i == 0:\n                # ""best"" prediction is eos - that should be blocked\n                word_probs[0::beam_sz, eos_idx] = valid_score_dist[0]\n                # include at least beam_sz predictions OTHER than EOS\n                # that are greater than -1e20\n                for j, score in zip(_non_eos_idxs, valid_score_dist[1:]):\n                    word_probs[0::beam_sz, j] = score\n            elif i <= min_length:\n                # predict eos in beam 1\n                word_probs[1::beam_sz, eos_idx] = valid_score_dist[0]\n                # provide beam_sz other good predictions in other beams\n                for k, (j, score) in enumerate(\n                        zip(_non_eos_idxs, valid_score_dist[1:])):\n                    beam_idx = min(beam_sz - 1, k)\n                    word_probs[beam_idx::beam_sz, j] = score\n            else:\n                word_probs[0::beam_sz, eos_idx] = valid_score_dist[0]\n                word_probs[1::beam_sz, eos_idx] = valid_score_dist[0]\n                # provide beam_sz other good predictions in other beams\n                for k, (j, score) in enumerate(\n                        zip(_non_eos_idxs, valid_score_dist[1:])):\n                    beam_idx = min(beam_sz - 1, k)\n                    word_probs[beam_idx::beam_sz, j] = score\n\n            attns = torch.randn(1, batch_sz * beam_sz, 53)\n            beam.advance(word_probs, attns)\n            if i < min_length:\n                self.assertFalse(beam.done)\n                # no top beams are finished yet\n                for b in range(batch_sz):\n                    self.assertEqual(beam.attention[b], [])\n            elif i == min_length:\n                # beam 1 dies on min_length\n                self.assertTrue(beam.is_finished[:, 1].all())\n                beam.update_finished()\n                self.assertFalse(beam.done)\n                # no top beams are finished yet\n                for b in range(batch_sz):\n                    self.assertEqual(beam.attention[b], [])\n            else:  # i > min_length\n                # beam 0 dies on the step after beam 1 dies\n                self.assertTrue(beam.is_finished[:, 0].all())\n                beam.update_finished()\n                self.assertTrue(beam.done)\n                # top beam is finished now so there are attentions\n                for b in range(batch_sz):\n                    # two beams are finished in each batch\n                    self.assertEqual(len(beam.attention[b]), 2)\n                    for k in range(2):\n                        # second dim is cut down to the non-padded src length\n                        self.assertEqual(beam.attention[b][k].shape[-1],\n                                         inp_lens[b])\n                    # first dim is equal to the time of death\n                    # (beam 0 died at current step - adjust for SOS)\n                    self.assertEqual(beam.attention[b][0].shape[0], i + 1)\n                    # (beam 1 died at last step - adjust for SOS)\n                    self.assertEqual(beam.attention[b][1].shape[0], i)\n                # behavior gets weird when beam is already done so just stop\n                break\n\n\nclass TestBeamSearchAgainstReferenceCase(unittest.TestCase):\n    # this is just test_beam.TestBeamAgainstReferenceCase repeated\n    # in each batch.\n    BEAM_SZ = 5\n    EOS_IDX = 2  # don\'t change this - all the scores would need updated\n    N_WORDS = 8  # also don\'t change for same reason\n    N_BEST = 3\n    DEAD_SCORE = -1e20\n    BATCH_SZ = 3\n    INP_SEQ_LEN = 53\n\n    def random_attn(self):\n        return torch.randn(1, self.BATCH_SZ * self.BEAM_SZ, self.INP_SEQ_LEN)\n\n    def init_step(self, beam, expected_len_pen):\n        # init_preds: [4, 3, 5, 6, 7] - no EOS\'s\n        init_scores = torch.log_softmax(torch.tensor(\n            [[0, 0, 0, 4, 5, 3, 2, 1]], dtype=torch.float), dim=1)\n        init_scores = deepcopy(init_scores.repeat(\n            self.BATCH_SZ * self.BEAM_SZ, 1))\n        new_scores = init_scores + beam.topk_log_probs.view(-1).unsqueeze(1)\n        expected_beam_scores, expected_preds_0 = new_scores \\\n            .view(self.BATCH_SZ, self.BEAM_SZ * self.N_WORDS) \\\n            .topk(self.BEAM_SZ, dim=-1)\n        beam.advance(deepcopy(init_scores), self.random_attn())\n        self.assertTrue(beam.topk_log_probs.allclose(expected_beam_scores))\n        self.assertTrue(beam.topk_ids.equal(expected_preds_0))\n        self.assertFalse(beam.is_finished.any())\n        self.assertFalse(beam.done)\n        return expected_beam_scores\n\n    def first_step(self, beam, expected_beam_scores, expected_len_pen):\n        # no EOS\'s yet\n        assert beam.is_finished.sum() == 0\n        scores_1 = torch.log_softmax(torch.tensor(\n            [[0, 0, 0, .3, 0, .51, .2, 0],\n             [0, 0, 1.5, 0, 0, 0, 0, 0],\n             [0, 0, 0, 0, .49, .48, 0, 0],\n             [0, 0, 0, .2, .2, .2, .2, .2],\n             [0, 0, 0, .2, .2, .2, .2, .2]]\n        ), dim=1)\n        scores_1 = scores_1.repeat(self.BATCH_SZ, 1)\n\n        beam.advance(deepcopy(scores_1), self.random_attn())\n\n        new_scores = scores_1 + expected_beam_scores.view(-1).unsqueeze(1)\n        expected_beam_scores, unreduced_preds = new_scores\\\n            .view(self.BATCH_SZ, self.BEAM_SZ * self.N_WORDS)\\\n            .topk(self.BEAM_SZ, -1)\n        expected_bptr_1 = unreduced_preds / self.N_WORDS\n        # [5, 3, 2, 6, 0], so beam 2 predicts EOS!\n        expected_preds_1 = unreduced_preds - expected_bptr_1 * self.N_WORDS\n        self.assertTrue(beam.topk_log_probs.allclose(expected_beam_scores))\n        self.assertTrue(beam.topk_scores.allclose(\n            expected_beam_scores / expected_len_pen))\n        self.assertTrue(beam.topk_ids.equal(expected_preds_1))\n        self.assertTrue(beam.current_backptr.equal(expected_bptr_1))\n        self.assertEqual(beam.is_finished.sum(), self.BATCH_SZ)\n        self.assertTrue(beam.is_finished[:, 2].all())  # beam 2 finished\n        beam.update_finished()\n        self.assertFalse(beam.top_beam_finished.any())\n        self.assertFalse(beam.done)\n        return expected_beam_scores\n\n    def second_step(self, beam, expected_beam_scores, expected_len_pen):\n        # assumes beam 2 finished on last step\n        scores_2 = torch.log_softmax(torch.tensor(\n            [[0, 0, 0, .3, 0, .51, .2, 0],\n             [0, 0, 0, 0, 0, 0, 0, 0],\n             [0, 0, 0, 0, 5000, .48, 0, 0],  # beam 2 shouldn\'t continue\n             [0, 0, 50, .2, .2, .2, .2, .2],  # beam 3 -> beam 0 should die\n             [0, 0, 0, .2, .2, .2, .2, .2]]\n        ), dim=1)\n        scores_2 = scores_2.repeat(self.BATCH_SZ, 1)\n\n        beam.advance(deepcopy(scores_2), self.random_attn())\n\n        # ended beam 2 shouldn\'t continue\n        expected_beam_scores[:, 2::self.BEAM_SZ] = self.DEAD_SCORE\n        new_scores = scores_2 + expected_beam_scores.view(-1).unsqueeze(1)\n        expected_beam_scores, unreduced_preds = new_scores\\\n            .view(self.BATCH_SZ, self.BEAM_SZ * self.N_WORDS)\\\n            .topk(self.BEAM_SZ, -1)\n        expected_bptr_2 = unreduced_preds / self.N_WORDS\n        # [2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!\n        expected_preds_2 = unreduced_preds - expected_bptr_2 * self.N_WORDS\n        # [-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ\n        self.assertTrue(beam.topk_log_probs.allclose(expected_beam_scores))\n        self.assertTrue(beam.topk_scores.allclose(\n            expected_beam_scores / expected_len_pen))\n        self.assertTrue(beam.topk_ids.equal(expected_preds_2))\n        self.assertTrue(beam.current_backptr.equal(expected_bptr_2))\n        # another beam is finished in all batches\n        self.assertEqual(beam.is_finished.sum(), self.BATCH_SZ)\n        # new beam 0 finished\n        self.assertTrue(beam.is_finished[:, 0].all())\n        # new beam 0 is old beam 3\n        self.assertTrue(expected_bptr_2[:, 0].eq(3).all())\n        beam.update_finished()\n        self.assertTrue(beam.top_beam_finished.all())\n        self.assertFalse(beam.done)\n        return expected_beam_scores\n\n    def third_step(self, beam, expected_beam_scores, expected_len_pen):\n        # assumes beam 0 finished on last step\n        scores_3 = torch.log_softmax(torch.tensor(\n            [[0, 0, 5000, 0, 5000, .51, .2, 0],  # beam 0 shouldn\'t cont\n             [0, 0, 0, 0, 0, 0, 0, 0],\n             [0, 0, 0, 0, 0, 5000, 0, 0],\n             [0, 0, 0, .2, .2, .2, .2, .2],\n             [0, 0, 50, 0, .2, .2, .2, .2]]  # beam 4 -> beam 1 should die\n        ), dim=1)\n        scores_3 = scores_3.repeat(self.BATCH_SZ, 1)\n\n        beam.advance(deepcopy(scores_3), self.random_attn())\n\n        expected_beam_scores[:, 0::self.BEAM_SZ] = self.DEAD_SCORE\n        new_scores = scores_3 + expected_beam_scores.view(-1).unsqueeze(1)\n        expected_beam_scores, unreduced_preds = new_scores\\\n            .view(self.BATCH_SZ, self.BEAM_SZ * self.N_WORDS)\\\n            .topk(self.BEAM_SZ, -1)\n        expected_bptr_3 = unreduced_preds / self.N_WORDS\n        # [5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!\n        expected_preds_3 = unreduced_preds - expected_bptr_3 * self.N_WORDS\n        self.assertTrue(beam.topk_log_probs.allclose(\n            expected_beam_scores))\n        self.assertTrue(beam.topk_scores.allclose(\n            expected_beam_scores / expected_len_pen))\n        self.assertTrue(beam.topk_ids.equal(expected_preds_3))\n        self.assertTrue(beam.current_backptr.equal(expected_bptr_3))\n        self.assertEqual(beam.is_finished.sum(), self.BATCH_SZ)\n        # new beam 1 finished\n        self.assertTrue(beam.is_finished[:, 1].all())\n        # new beam 1 is old beam 4\n        self.assertTrue(expected_bptr_3[:, 1].eq(4).all())\n        beam.update_finished()\n        self.assertTrue(beam.top_beam_finished.all())\n        self.assertTrue(beam.done)\n        return expected_beam_scores\n\n    def test_beam_advance_against_known_reference(self):\n        beam = BeamSearch(\n            self.BEAM_SZ, self.BATCH_SZ, 0, 1, 2, self.N_BEST,\n            GlobalScorerStub(),\n            0, 30, False, 0, set(),\n            False, 0.)\n        device_init = torch.zeros(1, 1)\n        beam.initialize(device_init, torch.randint(0, 30, (self.BATCH_SZ,)))\n        expected_beam_scores = self.init_step(beam, 1)\n        expected_beam_scores = self.first_step(beam, expected_beam_scores, 1)\n        expected_beam_scores = self.second_step(beam, expected_beam_scores, 1)\n        self.third_step(beam, expected_beam_scores, 1)\n\n\nclass TestBeamWithLengthPenalty(TestBeamSearchAgainstReferenceCase):\n    # this could be considered an integration test because it tests\n    # interactions between the GNMT scorer and the beam\n\n    def test_beam_advance_against_known_reference(self):\n        scorer = GNMTGlobalScorer(0.7, 0., ""avg"", ""none"")\n        beam = BeamSearch(\n            self.BEAM_SZ, self.BATCH_SZ, 0, 1, 2, self.N_BEST,\n            scorer,\n            0, 30, False, 0, set(),\n            False, 0.)\n        device_init = torch.zeros(1, 1)\n        beam.initialize(device_init, torch.randint(0, 30, (self.BATCH_SZ,)))\n        expected_beam_scores = self.init_step(beam, 1.)\n        expected_beam_scores = self.first_step(beam, expected_beam_scores, 3)\n        expected_beam_scores = self.second_step(beam, expected_beam_scores, 4)\n        self.third_step(beam, expected_beam_scores, 5)\n'"
onmt/tests/test_copy_generator.py,10,"b'import unittest\nfrom onmt.modules.copy_generator import CopyGenerator, CopyGeneratorLoss\n\nimport itertools\nfrom copy import deepcopy\n\nimport torch\nfrom torch.nn.functional import softmax\n\nfrom onmt.tests.utils_for_tests import product_dict\n\n\nclass TestCopyGenerator(unittest.TestCase):\n    INIT_CASES = list(product_dict(\n        input_size=[172],\n        output_size=[319],\n        pad_idx=[0, 39],\n    ))\n    PARAMS = list(product_dict(\n        batch_size=[1, 14],\n        max_seq_len=[23],\n        tgt_max_len=[50],\n        n_extra_words=[107]\n    ))\n\n    @classmethod\n    def dummy_inputs(cls, params, init_case):\n        hidden = torch.randn((params[""batch_size""] * params[""tgt_max_len""],\n                              init_case[""input_size""]))\n        attn = torch.randn((params[""batch_size""] * params[""tgt_max_len""],\n                            params[""max_seq_len""]))\n        src_map = torch.randn((params[""max_seq_len""], params[""batch_size""],\n                               params[""n_extra_words""]))\n        return hidden, attn, src_map\n\n    @classmethod\n    def expected_shape(cls, params, init_case):\n        return params[""tgt_max_len""] * params[""batch_size""], \\\n               init_case[""output_size""] + params[""n_extra_words""]\n\n    def test_copy_gen_forward_shape(self):\n        for params, init_case in itertools.product(\n                self.PARAMS, self.INIT_CASES):\n            cgen = CopyGenerator(**init_case)\n            dummy_in = self.dummy_inputs(params, init_case)\n            res = cgen(*dummy_in)\n            expected_shape = self.expected_shape(params, init_case)\n            self.assertEqual(res.shape, expected_shape, init_case.__str__())\n\n    def test_copy_gen_outp_has_no_prob_of_pad(self):\n        for params, init_case in itertools.product(\n                self.PARAMS, self.INIT_CASES):\n            cgen = CopyGenerator(**init_case)\n            dummy_in = self.dummy_inputs(params, init_case)\n            res = cgen(*dummy_in)\n            self.assertTrue(\n                res[:, init_case[""pad_idx""]].allclose(torch.tensor(0.0)))\n\n    def test_copy_gen_trainable_params_update(self):\n        for params, init_case in itertools.product(\n                self.PARAMS, self.INIT_CASES):\n            cgen = CopyGenerator(**init_case)\n            trainable_params = {n: p for n, p in cgen.named_parameters()\n                                if p.requires_grad}\n            assert len(trainable_params) > 0  # sanity check\n            old_weights = deepcopy(trainable_params)\n            dummy_in = self.dummy_inputs(params, init_case)\n            res = cgen(*dummy_in)\n            pretend_loss = res.sum()\n            pretend_loss.backward()\n            dummy_optim = torch.optim.SGD(trainable_params.values(), 1)\n            dummy_optim.step()\n            for param_name in old_weights.keys():\n                self.assertTrue(\n                    trainable_params[param_name]\n                    .ne(old_weights[param_name]).any(),\n                    param_name + "" "" + init_case.__str__())\n\n\nclass TestCopyGeneratorLoss(unittest.TestCase):\n    INIT_CASES = list(product_dict(\n        vocab_size=[172],\n        unk_index=[0, 39],\n        ignore_index=[1, 17],  # pad idx\n        force_copy=[True, False]\n    ))\n    PARAMS = list(product_dict(\n        batch_size=[1, 14],\n        tgt_max_len=[50],\n        n_extra_words=[107]\n    ))\n\n    @classmethod\n    def dummy_inputs(cls, params, init_case):\n        n_unique_src_words = 13\n        scores = torch.randn((params[""batch_size""] * params[""tgt_max_len""],\n                              init_case[""vocab_size""] + n_unique_src_words))\n        scores = softmax(scores, dim=1)\n        align = torch.randint(0, n_unique_src_words,\n                              (params[""batch_size""] * params[""tgt_max_len""],))\n        target = torch.randint(0, init_case[""vocab_size""],\n                               (params[""batch_size""] * params[""tgt_max_len""],))\n        target[0] = init_case[""unk_index""]\n        target[1] = init_case[""ignore_index""]\n        return scores, align, target\n\n    @classmethod\n    def expected_shape(cls, params, init_case):\n        return (params[""batch_size""] * params[""tgt_max_len""],)\n\n    def test_copy_loss_forward_shape(self):\n        for params, init_case in itertools.product(\n                self.PARAMS, self.INIT_CASES):\n            loss = CopyGeneratorLoss(**init_case)\n            dummy_in = self.dummy_inputs(params, init_case)\n            res = loss(*dummy_in)\n            expected_shape = self.expected_shape(params, init_case)\n            self.assertEqual(res.shape, expected_shape, init_case.__str__())\n\n    def test_copy_loss_ignore_index_is_ignored(self):\n        for params, init_case in itertools.product(\n                self.PARAMS, self.INIT_CASES):\n            loss = CopyGeneratorLoss(**init_case)\n            scores, align, target = self.dummy_inputs(params, init_case)\n            res = loss(scores, align, target)\n            should_be_ignored = (target == init_case[""ignore_index""]).nonzero()\n            assert len(should_be_ignored) > 0  # otherwise not testing anything\n            self.assertTrue(res[should_be_ignored].allclose(torch.tensor(0.0)))\n\n    def test_copy_loss_output_range_is_positive(self):\n        for params, init_case in itertools.product(\n                self.PARAMS, self.INIT_CASES):\n            loss = CopyGeneratorLoss(**init_case)\n            dummy_in = self.dummy_inputs(params, init_case)\n            res = loss(*dummy_in)\n            self.assertTrue((res >= 0).all())\n'"
onmt/tests/test_embeddings.py,5,"b'import unittest\nfrom onmt.modules.embeddings import Embeddings\n\nimport itertools\nfrom copy import deepcopy\n\nimport torch\n\nfrom onmt.tests.utils_for_tests import product_dict\n\n\nclass TestEmbeddings(unittest.TestCase):\n    INIT_CASES = list(product_dict(\n        word_vec_size=[172],\n        word_vocab_size=[319],\n        word_padding_idx=[17],\n        position_encoding=[False, True],\n        feat_merge=[""first"", ""concat"", ""sum"", ""mlp""],\n        feat_vec_exponent=[-1, 1.1, 0.7],\n        feat_vec_size=[0, 200],\n        feat_padding_idx=[[], [29], [0, 1]],\n        feat_vocab_sizes=[[], [39], [401, 39]],\n        dropout=[0, 0.5],\n        fix_word_vecs=[False, True]\n    ))\n    PARAMS = list(product_dict(\n        batch_size=[1, 14],\n        max_seq_len=[23]\n    ))\n\n    @classmethod\n    def case_is_degenerate(cls, case):\n        no_feats = len(case[""feat_vocab_sizes""]) == 0\n        if case[""feat_merge""] != ""first"" and no_feats:\n            return True\n        if case[""feat_merge""] == ""first"" and not no_feats:\n            return True\n        if case[""feat_merge""] == ""concat"" and case[""feat_vec_exponent""] != -1:\n            return True\n        if no_feats and case[""feat_vec_exponent""] != -1:\n            return True\n        if len(case[""feat_vocab_sizes""]) != len(case[""feat_padding_idx""]):\n            return True\n        if case[""feat_vec_size""] == 0 and case[""feat_vec_exponent""] <= 0:\n            return True\n        if case[""feat_merge""] == ""sum"":\n            if case[""feat_vec_exponent""] != -1:\n                return True\n            if case[""feat_vec_size""] != 0:\n                return True\n        if case[""feat_vec_size""] != 0 and case[""feat_vec_exponent""] != -1:\n            return True\n        return False\n\n    @classmethod\n    def cases(cls):\n        for case in cls.INIT_CASES:\n            if not cls.case_is_degenerate(case):\n                yield case\n\n    @classmethod\n    def dummy_inputs(cls, params, init_case):\n        max_seq_len = params[""max_seq_len""]\n        batch_size = params[""batch_size""]\n        fv_sizes = init_case[""feat_vocab_sizes""]\n        n_words = init_case[""word_vocab_size""]\n        voc_sizes = [n_words] + fv_sizes\n        pad_idxs = [init_case[""word_padding_idx""]] + \\\n            init_case[""feat_padding_idx""]\n        lengths = torch.randint(0, max_seq_len, (batch_size,))\n        lengths[0] = max_seq_len\n        inps = torch.empty((max_seq_len, batch_size, len(voc_sizes)),\n                           dtype=torch.long)\n        for f, (voc_size, pad_idx) in enumerate(zip(voc_sizes, pad_idxs)):\n            for b, len_ in enumerate(lengths):\n                inps[:len_, b, f] = torch.randint(0, voc_size-1, (len_,))\n                inps[len_:, b, f] = pad_idx\n        return inps\n\n    @classmethod\n    def expected_shape(cls, params, init_case):\n        wvs = init_case[""word_vec_size""]\n        fvs = init_case[""feat_vec_size""]\n        nf = len(init_case[""feat_vocab_sizes""])\n        size = wvs\n        if init_case[""feat_merge""] not in {""sum"", ""mlp""}:\n            size += nf * fvs\n        return params[""max_seq_len""], params[""batch_size""], size\n\n    def test_embeddings_forward_shape(self):\n        for params, init_case in itertools.product(self.PARAMS, self.cases()):\n            emb = Embeddings(**init_case)\n            dummy_in = self.dummy_inputs(params, init_case)\n            res = emb(dummy_in)\n            expected_shape = self.expected_shape(params, init_case)\n            self.assertEqual(res.shape, expected_shape, init_case.__str__())\n\n    def test_embeddings_trainable_params(self):\n        for params, init_case in itertools.product(self.PARAMS,\n                                                   self.cases()):\n            emb = Embeddings(**init_case)\n            trainable_params = {n: p for n, p in emb.named_parameters()\n                                if p.requires_grad}\n            # first check there\'s nothing unexpectedly not trainable\n            for key in emb.state_dict():\n                if key not in trainable_params:\n                    if key.endswith(""emb_luts.0.weight"") and \\\n                            init_case[""fix_word_vecs""]:\n                        # ok: word embeddings shouldn\'t be trainable\n                        # if word vecs are fixed\n                        continue\n                    if key.endswith("".pe.pe""):\n                        # ok: positional encodings shouldn\'t be trainable\n                        assert init_case[""position_encoding""]\n                        continue\n                    else:\n                        self.fail(""Param {:s} is unexpectedly not ""\n                                  ""trainable."".format(key))\n            # then check nothing unexpectedly trainable\n            if init_case[""fix_word_vecs""]:\n                self.assertFalse(\n                    any(trainable_param.endswith(""emb_luts.0.weight"")\n                        for trainable_param in trainable_params),\n                    ""Word embedding is trainable but word vecs are fixed."")\n            if init_case[""position_encoding""]:\n                self.assertFalse(\n                    any(trainable_p.endswith("".pe.pe"")\n                        for trainable_p in trainable_params),\n                    ""Positional encoding is trainable."")\n\n    def test_embeddings_trainable_params_update(self):\n        for params, init_case in itertools.product(self.PARAMS, self.cases()):\n            emb = Embeddings(**init_case)\n            trainable_params = {n: p for n, p in emb.named_parameters()\n                                if p.requires_grad}\n            if len(trainable_params) > 0:\n                old_weights = deepcopy(trainable_params)\n                dummy_in = self.dummy_inputs(params, init_case)\n                res = emb(dummy_in)\n                pretend_loss = res.sum()\n                pretend_loss.backward()\n                dummy_optim = torch.optim.SGD(trainable_params.values(), 1)\n                dummy_optim.step()\n                for param_name in old_weights.keys():\n                    self.assertTrue(\n                        trainable_params[param_name]\n                        .ne(old_weights[param_name]).any(),\n                        param_name + "" "" + init_case.__str__())\n'"
onmt/tests/test_greedy_search.py,25,"b'import unittest\nfrom onmt.translate.greedy_search import GreedySearch\n\nimport torch\n\n\nclass TestGreedySearch(unittest.TestCase):\n    BATCH_SZ = 3\n    INP_SEQ_LEN = 53\n    DEAD_SCORE = -1e20\n\n    BLOCKED_SCORE = -10e20\n\n    def test_doesnt_predict_eos_if_shorter_than_min_len(self):\n        # batch 0 will always predict EOS. The other batches will predict\n        # non-eos scores.\n        for batch_sz in [1, 3]:\n            n_words = 100\n            _non_eos_idxs = [47]\n            valid_score_dist = torch.log_softmax(torch.tensor(\n                [6., 5.]), dim=0)\n            min_length = 5\n            eos_idx = 2\n            lengths = torch.randint(0, 30, (batch_sz,))\n            samp = GreedySearch(\n                0, 1, 2, batch_sz, min_length,\n                False, set(), False, 30, 1., 1)\n            samp.initialize(torch.zeros(1), lengths)\n            all_attns = []\n            for i in range(min_length + 4):\n                word_probs = torch.full(\n                    (batch_sz, n_words), -float(\'inf\'))\n                # ""best"" prediction is eos - that should be blocked\n                word_probs[0, eos_idx] = valid_score_dist[0]\n                # include at least one prediction OTHER than EOS\n                # that is greater than -1e20\n                word_probs[0, _non_eos_idxs[0]] = valid_score_dist[1]\n                word_probs[1:, _non_eos_idxs[0] + i] = 0\n\n                attns = torch.randn(1, batch_sz, 53)\n                all_attns.append(attns)\n                samp.advance(word_probs, attns)\n                if i < min_length:\n                    self.assertTrue(\n                        samp.topk_scores[0].allclose(valid_score_dist[1]))\n                    self.assertTrue(\n                        samp.topk_scores[1:].eq(0).all())\n                elif i == min_length:\n                    # now batch 0 has ended and no others have\n                    self.assertTrue(samp.is_finished[0, :].eq(1).all())\n                    self.assertTrue(samp.is_finished[1:, 1:].eq(0).all())\n                else:  # i > min_length\n                    break\n\n    def test_returns_correct_scores_deterministic(self):\n        for batch_sz in [1, 13]:\n            for temp in [1., 3.]:\n                n_words = 100\n                _non_eos_idxs = [47, 51, 13, 88, 99]\n                valid_score_dist_1 = torch.log_softmax(torch.tensor(\n                    [6., 5., 4., 3., 2., 1.]), dim=0)\n                valid_score_dist_2 = torch.log_softmax(torch.tensor(\n                    [6., 1.]), dim=0)\n                eos_idx = 2\n                lengths = torch.randint(0, 30, (batch_sz,))\n                samp = GreedySearch(\n                    0, 1, 2, batch_sz, 0,\n                    False, set(), False, 30, temp, 1)\n                samp.initialize(torch.zeros(1), lengths)\n                # initial step\n                i = 0\n                word_probs = torch.full(\n                    (batch_sz, n_words), -float(\'inf\'))\n                # batch 0 dies on step 0\n                word_probs[0, eos_idx] = valid_score_dist_1[0]\n                # include at least one prediction OTHER than EOS\n                # that is greater than -1e20\n                word_probs[0, _non_eos_idxs] = valid_score_dist_1[1:]\n                word_probs[1:, _non_eos_idxs[0] + i] = 0\n\n                attns = torch.randn(1, batch_sz, 53)\n                samp.advance(word_probs, attns)\n                self.assertTrue(samp.is_finished[0].eq(1).all())\n                samp.update_finished()\n                self.assertEqual(\n                    samp.scores[0], [valid_score_dist_1[0] / temp])\n                if batch_sz == 1:\n                    self.assertTrue(samp.done)\n                    continue\n                else:\n                    self.assertFalse(samp.done)\n\n                # step 2\n                i = 1\n                word_probs = torch.full(\n                    (batch_sz - 1, n_words), -float(\'inf\'))\n                # (old) batch 8 dies on step 1\n                word_probs[7, eos_idx] = valid_score_dist_2[0]\n                word_probs[0:7, _non_eos_idxs[:2]] = valid_score_dist_2\n                word_probs[8:, _non_eos_idxs[:2]] = valid_score_dist_2\n\n                attns = torch.randn(1, batch_sz, 53)\n                samp.advance(word_probs, attns)\n\n                self.assertTrue(samp.is_finished[7].eq(1).all())\n                samp.update_finished()\n                self.assertEqual(\n                    samp.scores[8], [valid_score_dist_2[0] / temp])\n\n                # step 3\n                i = 2\n                word_probs = torch.full(\n                    (batch_sz - 2, n_words), -float(\'inf\'))\n                # everything dies\n                word_probs[:, eos_idx] = 0\n\n                attns = torch.randn(1, batch_sz, 53)\n                samp.advance(word_probs, attns)\n\n                self.assertTrue(samp.is_finished.eq(1).all())\n                samp.update_finished()\n                for b in range(batch_sz):\n                    if b != 0 and b != 8:\n                        self.assertEqual(samp.scores[b], [0])\n                self.assertTrue(samp.done)\n\n    def test_returns_correct_scores_non_deterministic(self):\n        for batch_sz in [1, 13]:\n            for temp in [1., 3.]:\n                n_words = 100\n                _non_eos_idxs = [47, 51, 13, 88, 99]\n                valid_score_dist_1 = torch.log_softmax(torch.tensor(\n                    [6., 5., 4., 3., 2., 1.]), dim=0)\n                valid_score_dist_2 = torch.log_softmax(torch.tensor(\n                    [6., 1.]), dim=0)\n                eos_idx = 2\n                lengths = torch.randint(0, 30, (batch_sz,))\n                samp = GreedySearch(\n                    0, 1, 2, batch_sz, 0,\n                    False, set(), False, 30, temp, 2)\n                samp.initialize(torch.zeros(1), lengths)\n                # initial step\n                i = 0\n                for _ in range(100):\n                    word_probs = torch.full(\n                        (batch_sz, n_words), -float(\'inf\'))\n                    # batch 0 dies on step 0\n                    word_probs[0, eos_idx] = valid_score_dist_1[0]\n                    # include at least one prediction OTHER than EOS\n                    # that is greater than -1e20\n                    word_probs[0, _non_eos_idxs] = valid_score_dist_1[1:]\n                    word_probs[1:, _non_eos_idxs[0] + i] = 0\n\n                    attns = torch.randn(1, batch_sz, 53)\n                    samp.advance(word_probs, attns)\n                    if samp.is_finished[0].eq(1).all():\n                        break\n                else:\n                    self.fail(""Batch 0 never ended (very unlikely but maybe ""\n                              ""due to stochasticisty. If so, please increase ""\n                              ""the range of the for-loop."")\n                samp.update_finished()\n                self.assertEqual(\n                    samp.scores[0], [valid_score_dist_1[0] / temp])\n                if batch_sz == 1:\n                    self.assertTrue(samp.done)\n                    continue\n                else:\n                    self.assertFalse(samp.done)\n\n                # step 2\n                i = 1\n                for _ in range(100):\n                    word_probs = torch.full(\n                        (batch_sz - 1, n_words), -float(\'inf\'))\n                    # (old) batch 8 dies on step 1\n                    word_probs[7, eos_idx] = valid_score_dist_2[0]\n                    word_probs[0:7, _non_eos_idxs[:2]] = valid_score_dist_2\n                    word_probs[8:, _non_eos_idxs[:2]] = valid_score_dist_2\n\n                    attns = torch.randn(1, batch_sz, 53)\n                    samp.advance(word_probs, attns)\n                    if samp.is_finished[7].eq(1).all():\n                        break\n                else:\n                    self.fail(""Batch 8 never ended (very unlikely but maybe ""\n                              ""due to stochasticisty. If so, please increase ""\n                              ""the range of the for-loop."")\n\n                samp.update_finished()\n                self.assertEqual(\n                    samp.scores[8], [valid_score_dist_2[0] / temp])\n\n                # step 3\n                i = 2\n                for _ in range(250):\n                    word_probs = torch.full(\n                        (samp.alive_seq.shape[0], n_words), -float(\'inf\'))\n                    # everything dies\n                    word_probs[:, eos_idx] = 0\n\n                    attns = torch.randn(1, batch_sz, 53)\n                    samp.advance(word_probs, attns)\n                    if samp.is_finished.any():\n                        samp.update_finished()\n                    if samp.is_finished.eq(1).all():\n                        break\n                else:\n                    self.fail(""All batches never ended (very unlikely but ""\n                              ""maybe due to stochasticisty. If so, please ""\n                              ""increase the range of the for-loop."")\n\n                for b in range(batch_sz):\n                    if b != 0 and b != 8:\n                        self.assertEqual(samp.scores[b], [0])\n                self.assertTrue(samp.done)\n'"
onmt/tests/test_image_dataset.py,2,"b'import unittest\nfrom onmt.inputters.image_dataset import ImageDataReader\n\nimport os\nimport shutil\n\nimport cv2\nimport numpy as np\nimport torch\n\n\nclass TestImageDataReader(unittest.TestCase):\n    # this test touches the file system, so it could be considered an\n    # integration test\n    _THIS_DIR = os.path.dirname(os.path.abspath(__file__))\n    _IMG_DATA_DIRNAME = ""test_image_data""\n    _IMG_DATA_DIR = os.path.join(_THIS_DIR, _IMG_DATA_DIRNAME)\n    _IMG_DATA_FMT = ""test_img_{:d}.png""\n    _IMG_DATA_PATH_FMT = os.path.join(_IMG_DATA_DIR, _IMG_DATA_FMT)\n\n    _IMG_LIST_DIR = ""test_image_filenames""\n    # file to hold full paths to image data\n    _IMG_LIST_PATHS_FNAME = ""test_files.txt""\n    _IMG_LIST_PATHS_PATH = os.path.join(\n        _IMG_LIST_DIR, _IMG_LIST_PATHS_FNAME)\n    # file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)\n    _IMG_LIST_FNAMES_FNAME = ""test_fnames.txt""\n    _IMG_LIST_FNAMES_PATH = os.path.join(\n        _IMG_LIST_DIR, _IMG_LIST_FNAMES_FNAME)\n\n    # it\'s ok if non-image files co-exist with image files in the data dir\n    _JUNK_FILE = os.path.join(\n        _IMG_DATA_DIR, ""this_is_junk.txt"")\n\n    _N_EXAMPLES = 20\n    _N_CHANNELS = 3\n\n    @classmethod\n    def setUpClass(cls):\n        if not os.path.exists(cls._IMG_DATA_DIR):\n            os.makedirs(cls._IMG_DATA_DIR)\n        if not os.path.exists(cls._IMG_LIST_DIR):\n            os.makedirs(cls._IMG_LIST_DIR)\n\n        with open(cls._JUNK_FILE, ""w"") as f:\n            f.write(""this is some garbage\\nShould have no impact."")\n\n        with open(cls._IMG_LIST_PATHS_PATH, ""w"") as f_list_fnames, \\\n                open(cls._IMG_LIST_FNAMES_PATH, ""w"") as f_list_paths:\n            cls.n_rows = torch.randint(30, 314, (cls._N_EXAMPLES,))\n            cls.n_cols = torch.randint(30, 314, (cls._N_EXAMPLES,))\n            for i in range(cls._N_EXAMPLES):\n                img = np.random.randint(\n                    0, 255, (cls.n_rows[i], cls.n_cols[i], cls._N_CHANNELS))\n                f_path = cls._IMG_DATA_PATH_FMT.format(i)\n                cv2.imwrite(f_path, img)\n                f_name_short = cls._IMG_DATA_FMT.format(i)\n                f_list_fnames.write(f_name_short + ""\\n"")\n                f_list_paths.write(f_path + ""\\n"")\n\n    @classmethod\n    def tearDownClass(cls):\n        shutil.rmtree(cls._IMG_DATA_DIR)\n        shutil.rmtree(cls._IMG_LIST_DIR)\n\n    def test_read_from_dir_and_data_file_containing_filenames(self):\n        rdr = ImageDataReader(channel_size=self._N_CHANNELS)\n        i = 0  # initialize since there\'s a sanity check on i\n        for i, img in enumerate(rdr.read(\n                self._IMG_LIST_FNAMES_PATH, ""src"", self._IMG_DATA_DIR)):\n            self.assertEqual(\n                img[""src""].shape,\n                (self._N_CHANNELS, self.n_rows[i], self.n_cols[i]))\n            self.assertEqual(img[""src_path""],\n                             self._IMG_DATA_PATH_FMT.format(i))\n        self.assertGreater(i, 0, ""No image data was read."")\n\n    def test_read_from_dir_and_data_file_containing_paths(self):\n        rdr = ImageDataReader(channel_size=self._N_CHANNELS)\n        i = 0  # initialize since there\'s a sanity check on i\n        for i, img in enumerate(rdr.read(\n                self._IMG_LIST_PATHS_PATH, ""src"", self._IMG_DATA_DIR)):\n            self.assertEqual(\n                img[""src""].shape,\n                (self._N_CHANNELS, self.n_rows[i], self.n_cols[i]))\n            self.assertEqual(img[""src_path""],\n                             self._IMG_DATA_FMT.format(i))\n        self.assertGreater(i, 0, ""No image data was read."")\n\n\nclass TestImageDataReader1Channel(TestImageDataReader):\n    _N_CHANNELS = 1\n'"
onmt/tests/test_models.py,20,"b'import copy\nimport unittest\nimport math\n\nimport torch\n\nimport onmt\nimport onmt.inputters\nimport onmt.opts\nfrom onmt.model_builder import build_embeddings, \\\n    build_encoder, build_decoder\nfrom onmt.encoders.image_encoder import ImageEncoder\nfrom onmt.encoders.audio_encoder import AudioEncoder\nfrom onmt.utils.parse import ArgumentParser\n\nparser = ArgumentParser(description=\'train.py\')\nonmt.opts.model_opts(parser)\nonmt.opts.train_opts(parser)\n\n# -data option is required, but not used in this test, so dummy.\nopt = parser.parse_known_args([\'-data\', \'dummy\'])[0]\n\n\nclass TestModel(unittest.TestCase):\n\n    def __init__(self, *args, **kwargs):\n        super(TestModel, self).__init__(*args, **kwargs)\n        self.opt = opt\n\n    def get_field(self):\n        src = onmt.inputters.get_fields(""text"", 0, 0)[""src""]\n        src.base_field.build_vocab([])\n        return src\n\n    def get_batch(self, source_l=3, bsize=1):\n        # len x batch x nfeat\n        test_src = torch.ones(source_l, bsize, 1).long()\n        test_tgt = torch.ones(source_l, bsize, 1).long()\n        test_length = torch.ones(bsize).fill_(source_l).long()\n        return test_src, test_tgt, test_length\n\n    def get_batch_image(self, tgt_l=3, bsize=1, h=15, w=17):\n        # batch x c x h x w\n        test_src = torch.ones(bsize, 3, h, w).float()\n        test_tgt = torch.ones(tgt_l, bsize, 1).long()\n        test_length = None\n        return test_src, test_tgt, test_length\n\n    def get_batch_audio(self, tgt_l=7, bsize=3, sample_rate=5500,\n                        window_size=0.03, t=37):\n        # batch x 1 x nfft x t\n        nfft = int(math.floor((sample_rate * window_size) / 2) + 1)\n        test_src = torch.ones(bsize, 1, nfft, t).float()\n        test_tgt = torch.ones(tgt_l, bsize, 1).long()\n        test_length = torch.ones(bsize).long().fill_(tgt_l)\n        return test_src, test_tgt, test_length\n\n    def embeddings_forward(self, opt, source_l=3, bsize=1):\n        \'\'\'\n        Tests if the embeddings works as expected\n\n        args:\n            opt: set of options\n            source_l: Length of generated input sentence\n            bsize: Batchsize of generated input\n        \'\'\'\n        word_field = self.get_field()\n        emb = build_embeddings(opt, word_field)\n        test_src, _, __ = self.get_batch(source_l=source_l, bsize=bsize)\n        if opt.decoder_type == \'transformer\':\n            input = torch.cat([test_src, test_src], 0)\n            res = emb(input)\n            compare_to = torch.zeros(source_l * 2, bsize,\n                                     opt.src_word_vec_size)\n        else:\n            res = emb(test_src)\n            compare_to = torch.zeros(source_l, bsize, opt.src_word_vec_size)\n\n        self.assertEqual(res.size(), compare_to.size())\n\n    def encoder_forward(self, opt, source_l=3, bsize=1):\n        \'\'\'\n        Tests if the encoder works as expected\n\n        args:\n            opt: set of options\n            source_l: Length of generated input sentence\n            bsize: Batchsize of generated input\n        \'\'\'\n        if opt.rnn_size > 0:\n            opt.enc_rnn_size = opt.rnn_size\n        word_field = self.get_field()\n        embeddings = build_embeddings(opt, word_field)\n        enc = build_encoder(opt, embeddings)\n\n        test_src, test_tgt, test_length = self.get_batch(source_l=source_l,\n                                                         bsize=bsize)\n\n        hidden_t, outputs, test_length = enc(test_src, test_length)\n\n        # Initialize vectors to compare size with\n        test_hid = torch.zeros(self.opt.enc_layers, bsize, opt.enc_rnn_size)\n        test_out = torch.zeros(source_l, bsize, opt.dec_rnn_size)\n\n        # Ensure correct sizes and types\n        self.assertEqual(test_hid.size(),\n                         hidden_t[0].size(),\n                         hidden_t[1].size())\n        self.assertEqual(test_out.size(), outputs.size())\n        self.assertEqual(type(outputs), torch.Tensor)\n\n    def nmtmodel_forward(self, opt, source_l=3, bsize=1):\n        """"""\n        Creates a nmtmodel with a custom opt function.\n        Forwards a testbatch and checks output size.\n\n        Args:\n            opt: Namespace with options\n            source_l: length of input sequence\n            bsize: batchsize\n        """"""\n        if opt.rnn_size > 0:\n            opt.enc_rnn_size = opt.rnn_size\n            opt.dec_rnn_size = opt.rnn_size\n        word_field = self.get_field()\n\n        embeddings = build_embeddings(opt, word_field)\n        enc = build_encoder(opt, embeddings)\n\n        embeddings = build_embeddings(opt, word_field, for_encoder=False)\n        dec = build_decoder(opt, embeddings)\n\n        model = onmt.models.model.NMTModel(enc, dec)\n\n        test_src, test_tgt, test_length = self.get_batch(source_l=source_l,\n                                                         bsize=bsize)\n        outputs, attn = model(test_src, test_tgt, test_length)\n        outputsize = torch.zeros(source_l - 1, bsize, opt.dec_rnn_size)\n        # Make sure that output has the correct size and type\n        self.assertEqual(outputs.size(), outputsize.size())\n        self.assertEqual(type(outputs), torch.Tensor)\n\n    def imagemodel_forward(self, opt, tgt_l=2, bsize=1, h=15, w=17):\n        """"""\n        Creates an image-to-text nmtmodel with a custom opt function.\n        Forwards a testbatch and checks output size.\n\n        Args:\n            opt: Namespace with options\n            source_l: length of input sequence\n            bsize: batchsize\n        """"""\n        if opt.encoder_type == \'transformer\' or opt.encoder_type == \'cnn\':\n            return\n\n        word_field = self.get_field()\n\n        enc = ImageEncoder(\n            opt.enc_layers, opt.brnn, opt.enc_rnn_size,\n            opt.dropout)\n\n        embeddings = build_embeddings(opt, word_field, for_encoder=False)\n        dec = build_decoder(opt, embeddings)\n\n        model = onmt.models.model.NMTModel(enc, dec)\n\n        test_src, test_tgt, test_length = self.get_batch_image(\n            h=h, w=w,\n            bsize=bsize,\n            tgt_l=tgt_l)\n        outputs, attn = model(test_src, test_tgt, test_length)\n        outputsize = torch.zeros(tgt_l - 1, bsize, opt.dec_rnn_size)\n        # Make sure that output has the correct size and type\n        self.assertEqual(outputs.size(), outputsize.size())\n        self.assertEqual(type(outputs), torch.Tensor)\n\n    def audiomodel_forward(self, opt, tgt_l=7, bsize=3, t=37):\n        """"""\n        Creates a speech-to-text nmtmodel with a custom opt function.\n        Forwards a testbatch and checks output size.\n\n        Args:\n            opt: Namespace with options\n            source_l: length of input sequence\n            bsize: batchsize\n        """"""\n        if opt.encoder_type == \'transformer\' or opt.encoder_type == \'cnn\':\n            return\n        if opt.rnn_type == \'SRU\':\n            return\n\n        word_field = self.get_field()\n\n        enc = AudioEncoder(opt.rnn_type, opt.enc_layers, opt.dec_layers,\n                           opt.brnn, opt.enc_rnn_size, opt.dec_rnn_size,\n                           opt.audio_enc_pooling, opt.dropout,\n                           opt.sample_rate, opt.window_size)\n\n        embeddings = build_embeddings(opt, word_field, for_encoder=False)\n        dec = build_decoder(opt, embeddings)\n\n        model = onmt.models.model.NMTModel(enc, dec)\n\n        test_src, test_tgt, test_length = self.get_batch_audio(\n            bsize=bsize,\n            sample_rate=opt.sample_rate,\n            window_size=opt.window_size,\n            t=t, tgt_l=tgt_l)\n        outputs, attn = model(test_src, test_tgt, test_length)\n        outputsize = torch.zeros(tgt_l - 1, bsize, opt.dec_rnn_size)\n        # Make sure that output has the correct size and type\n        self.assertEqual(outputs.size(), outputsize.size())\n        self.assertEqual(type(outputs), torch.Tensor)\n\n\ndef _add_test(param_setting, methodname):\n    """"""\n    Adds a Test to TestModel according to settings\n\n    Args:\n        param_setting: list of tuples of (param, setting)\n        methodname: name of the method that gets called\n    """"""\n\n    def test_method(self):\n        opt = copy.deepcopy(self.opt)\n        if param_setting:\n            for param, setting in param_setting:\n                setattr(opt, param, setting)\n        ArgumentParser.update_model_opts(opt)\n        getattr(self, methodname)(opt)\n    if param_setting:\n        name = \'test_\' + methodname + ""_"" + ""_"".join(\n            str(param_setting).split())\n    else:\n        name = \'test_\' + methodname + \'_standard\'\n    setattr(TestModel, name, test_method)\n    test_method.__name__ = name\n\n\n\'\'\'\nTEST PARAMETERS\n\'\'\'\nopt.brnn = False\n\ntest_embeddings = [[],\n                   [(\'decoder_type\', \'transformer\')]\n                   ]\n\nfor p in test_embeddings:\n    _add_test(p, \'embeddings_forward\')\n\ntests_encoder = [[],\n                 [(\'encoder_type\', \'mean\')],\n                 # [(\'encoder_type\', \'transformer\'),\n                 # (\'word_vec_size\', 16), (\'rnn_size\', 16)],\n                 []\n                 ]\n\nfor p in tests_encoder:\n    _add_test(p, \'encoder_forward\')\n\ntests_nmtmodel = [[(\'rnn_type\', \'GRU\')],\n                  [(\'layers\', 10)],\n                  [(\'input_feed\', 0)],\n                  [(\'decoder_type\', \'transformer\'),\n                   (\'encoder_type\', \'transformer\'),\n                   (\'src_word_vec_size\', 16),\n                   (\'tgt_word_vec_size\', 16),\n                   (\'rnn_size\', 16)],\n                  [(\'decoder_type\', \'transformer\'),\n                   (\'encoder_type\', \'transformer\'),\n                   (\'src_word_vec_size\', 16),\n                   (\'tgt_word_vec_size\', 16),\n                   (\'rnn_size\', 16),\n                   (\'position_encoding\', True)],\n                  [(\'coverage_attn\', True)],\n                  [(\'copy_attn\', True)],\n                  [(\'global_attention\', \'mlp\')],\n                  [(\'context_gate\', \'both\')],\n                  [(\'context_gate\', \'target\')],\n                  [(\'context_gate\', \'source\')],\n                  [(\'encoder_type\', ""brnn""),\n                   (\'brnn_merge\', \'sum\')],\n                  [(\'encoder_type\', ""brnn"")],\n                  [(\'decoder_type\', \'cnn\'),\n                   (\'encoder_type\', \'cnn\')],\n                  [(\'encoder_type\', \'rnn\'),\n                   (\'global_attention\', None)],\n                  [(\'encoder_type\', \'rnn\'),\n                   (\'global_attention\', None),\n                   (\'copy_attn\', True),\n                   (\'copy_attn_type\', \'general\')],\n                  [(\'encoder_type\', \'rnn\'),\n                   (\'global_attention\', \'mlp\'),\n                   (\'copy_attn\', True),\n                   (\'copy_attn_type\', \'general\')],\n                  [],\n                  ]\n\nif onmt.models.sru.check_sru_requirement():\n    #   """""" Only do SRU test if requirment is safisfied. """"""\n    # SRU doesn\'t support input_feed.\n    tests_nmtmodel.append([(\'rnn_type\', \'SRU\'), (\'input_feed\', 0)])\n\nfor p in tests_nmtmodel:\n    _add_test(p, \'nmtmodel_forward\')\n\nfor p in tests_nmtmodel:\n    _add_test(p, \'imagemodel_forward\')\n\nfor p in tests_nmtmodel:\n    p.append((\'sample_rate\', 5500))\n    p.append((\'window_size\', 0.03))\n    # when reasonable, set audio_enc_pooling to 2\n    for arg, val in p:\n        if arg == ""layers"" and int(val) > 2:\n            # Need lengths >= audio_enc_pooling**n_layers.\n            # That condition is unrealistic for large n_layers,\n            # so leave audio_enc_pooling at 1.\n            break\n    else:\n        p.append((\'audio_enc_pooling\', \'2\'))\n    _add_test(p, \'audiomodel_forward\')\n'"
onmt/tests/test_preprocess.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\nimport configargparse\nimport copy\nimport unittest\nimport glob\nimport os\nimport codecs\n\nimport onmt\nimport onmt.inputters\nimport onmt.opts\nimport onmt.bin.preprocess as preprocess\n\n\nparser = configargparse.ArgumentParser(description=\'preprocess.py\')\nonmt.opts.preprocess_opts(parser)\n\nSAVE_DATA_PREFIX = \'data/test_preprocess\'\n\ndefault_opts = [\n    \'-data_type\', \'text\',\n    \'-train_src\', \'data/src-train.txt\',\n    \'-train_tgt\', \'data/tgt-train.txt\',\n    \'-valid_src\', \'data/src-val.txt\',\n    \'-valid_tgt\', \'data/tgt-val.txt\',\n    \'-save_data\', SAVE_DATA_PREFIX\n]\n\nopt = parser.parse_known_args(default_opts)[0]\n\n\nclass TestData(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super(TestData, self).__init__(*args, **kwargs)\n        self.opt = opt\n\n    def dataset_build(self, opt):\n        fields = onmt.inputters.get_fields(""text"", 0, 0)\n\n        if hasattr(opt, \'src_vocab\') and len(opt.src_vocab) > 0:\n            with codecs.open(opt.src_vocab, \'w\', \'utf-8\') as f:\n                f.write(\'a\\nb\\nc\\nd\\ne\\nf\\n\')\n        if hasattr(opt, \'tgt_vocab\') and len(opt.tgt_vocab) > 0:\n            with codecs.open(opt.tgt_vocab, \'w\', \'utf-8\') as f:\n                f.write(\'a\\nb\\nc\\nd\\ne\\nf\\n\')\n\n        src_reader = onmt.inputters.str2reader[opt.data_type].from_opt(opt)\n        tgt_reader = onmt.inputters.str2reader[""text""].from_opt(opt)\n        align_reader = onmt.inputters.str2reader[""text""].from_opt(opt)\n        preprocess.build_save_dataset(\n            \'train\', fields, src_reader, tgt_reader, align_reader, opt)\n\n        preprocess.build_save_dataset(\n            \'valid\', fields, src_reader, tgt_reader, align_reader, opt)\n\n        # Remove the generated *pt files.\n        for pt in glob.glob(SAVE_DATA_PREFIX + \'*.pt\'):\n            os.remove(pt)\n        if hasattr(opt, \'src_vocab\') and os.path.exists(opt.src_vocab):\n            os.remove(opt.src_vocab)\n        if hasattr(opt, \'tgt_vocab\') and os.path.exists(opt.tgt_vocab):\n            os.remove(opt.tgt_vocab)\n\n\ndef _add_test(param_setting, methodname):\n    """"""\n    Adds a Test to TestData according to settings\n\n    Args:\n        param_setting: list of tuples of (param, setting)\n        methodname: name of the method that gets called\n    """"""\n\n    def test_method(self):\n        if param_setting:\n            opt = copy.deepcopy(self.opt)\n            for param, setting in param_setting:\n                setattr(opt, param, setting)\n        else:\n            opt = self.opt\n        getattr(self, methodname)(opt)\n    if param_setting:\n        name = \'test_\' + methodname + ""_"" + ""_"".join(\n            str(param_setting).split())\n    else:\n        name = \'test_\' + methodname + \'_standard\'\n    setattr(TestData, name, test_method)\n    test_method.__name__ = name\n\n\ntest_databuild = [[],\n                  [(\'src_vocab_size\', 1),\n                   (\'tgt_vocab_size\', 1)],\n                  [(\'src_vocab_size\', 10000),\n                   (\'tgt_vocab_size\', 10000)],\n                  [(\'src_seq_len\', 1)],\n                  [(\'src_seq_len\', 5000)],\n                  [(\'src_seq_length_trunc\', 1)],\n                  [(\'src_seq_length_trunc\', 5000)],\n                  [(\'tgt_seq_len\', 1)],\n                  [(\'tgt_seq_len\', 5000)],\n                  [(\'tgt_seq_length_trunc\', 1)],\n                  [(\'tgt_seq_length_trunc\', 5000)],\n                  [(\'shuffle\', 0)],\n                  [(\'lower\', True)],\n                  [(\'dynamic_dict\', True)],\n                  [(\'share_vocab\', True)],\n                  [(\'dynamic_dict\', True),\n                   (\'share_vocab\', True)],\n                  [(\'dynamic_dict\', True),\n                   (\'shard_size\', 500000)],\n                  [(\'src_vocab\', \'/tmp/src_vocab.txt\'),\n                   (\'tgt_vocab\', \'/tmp/tgt_vocab.txt\')],\n                  ]\n\nfor p in test_databuild:\n    _add_test(p, \'dataset_build\')\n\n# Test image preprocessing\ntest_databuild = [[],\n                  [(\'tgt_vocab_size\', 1)],\n                  [(\'tgt_vocab_size\', 10000)],\n                  [(\'tgt_seq_len\', 1)],\n                  [(\'tgt_seq_len\', 5000)],\n                  [(\'tgt_seq_length_trunc\', 1)],\n                  [(\'tgt_seq_length_trunc\', 5000)],\n                  [(\'shuffle\', 0)],\n                  [(\'lower\', True)],\n                  [(\'shard_size\', 5)],\n                  [(\'shard_size\', 50)],\n                  [(\'tgt_vocab\', \'/tmp/tgt_vocab.txt\')],\n                  ]\ntest_databuild_common = [(\'data_type\', \'img\'),\n                         (\'src_dir\', \'/tmp/im2text/images\'),\n                         (\'train_src\', [\'/tmp/im2text/src-train-head.txt\']),\n                         (\'train_tgt\', [\'/tmp/im2text/tgt-train-head.txt\']),\n                         (\'valid_src\', \'/tmp/im2text/src-val-head.txt\'),\n                         (\'valid_tgt\', \'/tmp/im2text/tgt-val-head.txt\'),\n                         ]\nfor p in test_databuild:\n    _add_test(p + test_databuild_common, \'dataset_build\')\n\n# Test audio preprocessing\ntest_databuild = [[],\n                  [(\'tgt_vocab_size\', 1)],\n                  [(\'tgt_vocab_size\', 10000)],\n                  [(\'src_seq_len\', 1)],\n                  [(\'src_seq_len\', 5000)],\n                  [(\'src_seq_length_trunc\', 3200)],\n                  [(\'src_seq_length_trunc\', 5000)],\n                  [(\'tgt_seq_len\', 1)],\n                  [(\'tgt_seq_len\', 5000)],\n                  [(\'tgt_seq_length_trunc\', 1)],\n                  [(\'tgt_seq_length_trunc\', 5000)],\n                  [(\'shuffle\', 0)],\n                  [(\'lower\', True)],\n                  [(\'shard_size\', 5)],\n                  [(\'shard_size\', 50)],\n                  [(\'tgt_vocab\', \'/tmp/tgt_vocab.txt\')],\n                  ]\ntest_databuild_common = [(\'data_type\', \'audio\'),\n                         (\'src_dir\', \'/tmp/speech/an4_dataset\'),\n                         (\'train_src\', [\'/tmp/speech/src-train-head.txt\']),\n                         (\'train_tgt\', [\'/tmp/speech/tgt-train-head.txt\']),\n                         (\'valid_src\', \'/tmp/speech/src-val-head.txt\'),\n                         (\'valid_tgt\', \'/tmp/speech/tgt-val-head.txt\'),\n                         (\'sample_rate\', 16000),\n                         (\'window_size\', 0.04),\n                         (\'window_stride\', 0.02),\n                         (\'window\', \'hamming\'),\n                         ]\nfor p in test_databuild:\n    _add_test(p + test_databuild_common, \'dataset_build\')\n'"
onmt/tests/test_simple.py,0,b'import onmt\n\n\ndef test_load():\n    onmt\n    pass\n'
onmt/tests/test_structured_attention.py,2,"b'import unittest\nfrom onmt.modules.structured_attention import MatrixTree\n\nimport torch\n\n\nclass TestStructuredAttention(unittest.TestCase):\n    def test_matrix_tree_marg_pdfs_sum_to_1(self):\n        dtree = MatrixTree()\n        q = torch.rand(1, 5, 5)\n        marg = dtree.forward(q)\n        self.assertTrue(\n            marg.sum(1).allclose(torch.tensor(1.0)))\n'"
onmt/tests/test_text_dataset.py,0,"b'import unittest\nfrom onmt.inputters.text_dataset import TextMultiField, TextDataReader\n\nimport itertools\nimport os\nfrom copy import deepcopy\n\nfrom torchtext.data import Field\n\nfrom onmt.tests.utils_for_tests import product_dict\n\n\nclass TestTextMultiField(unittest.TestCase):\n    INIT_CASES = list(product_dict(\n        base_name=[""base_field"", ""zbase_field""],\n        base_field=[Field],\n        feats_fields=[\n            [],\n            [(""a"", Field)],\n            [(""r"", Field), (""b"", Field)]]))\n\n    PARAMS = list(product_dict(\n        include_lengths=[False, True]))\n\n    @classmethod\n    def initialize_case(cls, init_case, params):\n        # initialize fields at the top of each unit test to prevent\n        # any undesired stateful effects\n        case = deepcopy(init_case)\n        case[""base_field""] = case[""base_field""](\n            include_lengths=params[""include_lengths""])\n        for i, (n, f_cls) in enumerate(case[""feats_fields""]):\n            case[""feats_fields""][i] = (n, f_cls(sequential=True))\n        return case\n\n    def test_process_shape(self):\n        dummy_input_bs_1 = [[\n                [""this"", ""is"", ""for"", ""the"", ""unittest""],\n                [""NOUN"", ""VERB"", ""PREP"", ""ART"", ""NOUN""],\n                ["""", """", """", """", ""MODULE""]]]\n        dummy_input_bs_5 = [\n                [[""this"", ""is"", ""for"", ""the"", ""unittest""],\n                 [""NOUN"", ""VERB"", ""PREP"", ""ART"", ""NOUN""],\n                 ["""", """", """", """", ""MODULE""]],\n                [[""batch"", ""2""],\n                 [""NOUN"", ""NUM""],\n                 ["""", """"]],\n                [[""batch"", ""3"", ""is"", ""the"", ""longest"", ""batch""],\n                 [""NOUN"", ""NUM"", ""VERB"", ""ART"", ""ADJ"", ""NOUN""],\n                 ["""", """", """", """", """", """"]],\n                [[""fourth"", ""batch""],\n                 [""ORD"", ""NOUN""],\n                 ["""", """"]],\n                [[""and"", ""another"", ""one""],\n                 [""CONJ"", ""?"", ""NUM""],\n                 ["""", """", """"]]]\n        for bs, max_len, dummy_input in [\n                (1, 5, dummy_input_bs_1), (5, 6, dummy_input_bs_5)]:\n            for init_case, params in itertools.product(\n                    self.INIT_CASES, self.PARAMS):\n                init_case = self.initialize_case(init_case, params)\n                mf = TextMultiField(**init_case)\n                fields = [init_case[""base_field""]] \\\n                    + [f for _, f in init_case[""feats_fields""]]\n                nfields = len(fields)\n                for i, f in enumerate(fields):\n                    all_sents = [b[i] for b in dummy_input]\n                    f.build_vocab(all_sents)\n                inp_only_desired_fields = [b[:nfields] for b in dummy_input]\n                data = mf.process(inp_only_desired_fields)\n                if params[""include_lengths""]:\n                    data, lengths = data\n                    self.assertEqual(lengths.shape, (bs,))\n                expected_shape = (max_len, bs, nfields)\n                self.assertEqual(data.shape, expected_shape)\n\n    def test_preprocess_shape(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            init_case = self.initialize_case(init_case, params)\n            mf = TextMultiField(**init_case)\n            sample_str = ""dummy input here .""\n            proc = mf.preprocess(sample_str)\n            self.assertEqual(len(proc), len(init_case[""feats_fields""]) + 1)\n\n    def test_base_field(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            init_case = self.initialize_case(init_case, params)\n            mf = TextMultiField(**init_case)\n            self.assertIs(mf.base_field, init_case[""base_field""])\n\n    def test_correct_n_fields(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            init_case = self.initialize_case(init_case, params)\n            mf = TextMultiField(**init_case)\n            self.assertEqual(len(mf.fields),\n                             len(init_case[""feats_fields""]) + 1)\n\n    def test_fields_order_correct(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            init_case = self.initialize_case(init_case, params)\n            mf = TextMultiField(**init_case)\n            fnames = [name for name, _ in init_case[""feats_fields""]]\n            correct_order = [init_case[""base_name""]] + list(sorted(fnames))\n            self.assertEqual([name for name, _ in mf.fields], correct_order)\n\n    def test_getitem_0_returns_correct_field(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            init_case = self.initialize_case(init_case, params)\n            mf = TextMultiField(**init_case)\n            self.assertEqual(mf[0][0], init_case[""base_name""])\n            self.assertIs(mf[0][1], init_case[""base_field""])\n\n    def test_getitem_nonzero_returns_correct_field(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            init_case = self.initialize_case(init_case, params)\n            mf = TextMultiField(**init_case)\n            fnames = [name for name, _ in init_case[""feats_fields""]]\n            if len(fnames) > 0:\n                ordered_names = list(sorted(fnames))\n                name2field = dict(init_case[""feats_fields""])\n                for i, name in enumerate(ordered_names, 1):\n                    expected_field = name2field[name]\n                    self.assertIs(mf[i][1], expected_field)\n\n    def test_getitem_has_correct_number_of_indexes(self):\n        for init_case, params in itertools.product(\n                self.INIT_CASES, self.PARAMS):\n            init_case = self.initialize_case(init_case, params)\n            mf = TextMultiField(**init_case)\n            nfields = len(init_case[""feats_fields""]) + 1\n            with self.assertRaises(IndexError):\n                mf[nfields]\n\n\nclass TestTextDataReader(unittest.TestCase):\n    def test_read(self):\n        strings = [\n            ""hello world"".encode(""utf-8""),\n            ""this\'s a string with punctuation ."".encode(""utf-8""),\n            ""ThIs Is A sTrInG wItH oDD CapitALIZAtion"".encode(""utf-8"")\n        ]\n        rdr = TextDataReader()\n        for i, ex in enumerate(rdr.read(strings, ""src"")):\n            self.assertEqual(ex[""src""], strings[i].decode(""utf-8""))\n\n\nclass TestTextDataReaderFromFS(unittest.TestCase):\n    # this test touches the file system, so it could be considered an\n    # integration test\n    STRINGS = [\n            ""hello world\\n"".encode(""utf-8""),\n            ""this\'s a string with punctuation . \\n"".encode(""utf-8""),\n            ""ThIs Is A sTrInG wItH oDD CapitALIZAtion\\n"".encode(""utf-8"")\n    ]\n    FILE_NAME = ""test_strings.txt""\n\n    @classmethod\n    def setUpClass(cls):\n        # write utf-8 bytes\n        with open(cls.FILE_NAME, ""wb"") as f:\n            for str_ in cls.STRINGS:\n                f.write(str_)\n\n    @classmethod\n    def tearDownClass(cls):\n        os.remove(cls.FILE_NAME)\n\n    def test_read(self):\n        rdr = TextDataReader()\n        for i, ex in enumerate(rdr.read(self.FILE_NAME, ""src"")):\n            self.assertEqual(ex[""src""], self.STRINGS[i].decode(""utf-8""))\n'"
onmt/tests/test_translation_server.py,5,"b'import unittest\nfrom onmt.translate.translation_server import ServerModel, TranslationServer\n\nimport os\nfrom six import string_types\nfrom textwrap import dedent\n\nimport torch\n\nfrom onmt.translate.translator import Translator\n\n\nTEST_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestServerModel(unittest.TestCase):\n    def test_deferred_loading_model_and_unload(self):\n        model_id = 0\n        opt = {""models"": [""test_model.pt""]}\n        model_root = TEST_DIR\n        sm = ServerModel(opt, model_id, model_root=model_root, load=False)\n        self.assertFalse(sm.loaded)\n        sm.load()\n        self.assertTrue(sm.loaded)\n        self.assertIsInstance(sm.translator, Translator)\n        sm.unload()\n        self.assertFalse(sm.loaded)\n\n    def test_load_model_on_init_and_unload(self):\n        model_id = 0\n        opt = {""models"": [""test_model.pt""]}\n        model_root = TEST_DIR\n        sm = ServerModel(opt, model_id, model_root=model_root, load=True)\n        self.assertTrue(sm.loaded)\n        self.assertIsInstance(sm.translator, Translator)\n        sm.unload()\n        self.assertFalse(sm.loaded)\n\n    def test_tokenizing_with_no_tokenizer_fails(self):\n        model_id = 0\n        opt = {""models"": [""test_model.pt""]}\n        model_root = TEST_DIR\n        sm = ServerModel(opt, model_id, model_root=model_root, load=True)\n        with self.assertRaises(ValueError):\n            sm.tokenize(""hello world"")\n\n    def test_detokenizing_with_no_tokenizer_fails(self):\n        model_id = 0\n        opt = {""models"": [""test_model.pt""]}\n        model_root = TEST_DIR\n        sm = ServerModel(opt, model_id, model_root=model_root, load=True)\n        with self.assertRaises(ValueError):\n            sm.detokenize(""hello world"")\n\n    if torch.cuda.is_available():\n        def test_moving_to_gpu_and_back(self):\n            torch.cuda.set_device(torch.device(""cuda"", 0))\n            model_id = 0\n            opt = {""models"": [""test_model.pt""]}\n            model_root = TEST_DIR\n            sm = ServerModel(opt, model_id, model_root=model_root, load=True)\n            for p in sm.translator.model.parameters():\n                self.assertEqual(p.device.type, ""cpu"")\n            sm.to_gpu()\n            for p in sm.translator.model.parameters():\n                self.assertEqual(p.device.type, ""cuda"")\n                self.assertEqual(p.device.index, 0)\n            sm.to_cpu()\n            for p in sm.translator.model.parameters():\n                self.assertEqual(p.device.type, ""cpu"")\n\n        def test_initialize_on_gpu_and_move_back(self):\n            torch.cuda.set_device(torch.device(""cuda"", 0))\n            model_id = 0\n            opt = {""models"": [""test_model.pt""], ""gpu"": 0}\n            model_root = TEST_DIR\n            sm = ServerModel(opt, model_id, model_root=model_root, load=True)\n            for p in sm.translator.model.parameters():\n                self.assertEqual(p.device.type, ""cuda"")\n                self.assertEqual(p.device.index, 0)\n            sm.to_gpu()\n            for p in sm.translator.model.parameters():\n                self.assertEqual(p.device.type, ""cuda"")\n                self.assertEqual(p.device.index, 0)\n            sm.to_cpu()\n            for p in sm.translator.model.parameters():\n                self.assertEqual(p.device.type, ""cpu"")\n\n        if torch.cuda.device_count() > 1:\n            def test_initialize_on_nonzero_gpu_and_back(self):\n                torch.cuda.set_device(torch.device(""cuda"", 1))\n                model_id = 0\n                opt = {""models"": [""test_model.pt""], ""gpu"": 1}\n                model_root = TEST_DIR\n                sm = ServerModel(opt, model_id, model_root=model_root,\n                                 load=True)\n                for p in sm.translator.model.parameters():\n                    self.assertEqual(p.device.type, ""cuda"")\n                    self.assertEqual(p.device.index, 1)\n                sm.to_gpu()\n                for p in sm.translator.model.parameters():\n                    self.assertEqual(p.device.type, ""cuda"")\n                    self.assertEqual(p.device.index, 1)\n                sm.to_cpu()\n                for p in sm.translator.model.parameters():\n                    self.assertEqual(p.device.type, ""cpu"")\n\n    def test_run(self):\n        model_id = 0\n        opt = {""models"": [""test_model.pt""]}\n        model_root = TEST_DIR\n        sm = ServerModel(opt, model_id, model_root=model_root, load=True)\n        inp = [{""src"": ""hello how are you today""},\n               {""src"": ""good morning to you .""}]\n        results, scores, n_best, time, aligns = sm.run(inp)\n        self.assertIsInstance(results, list)\n        for sentence_string in results:\n            self.assertIsInstance(sentence_string, string_types)\n        self.assertIsInstance(scores, list)\n        for elem in scores:\n            self.assertIsInstance(elem, float)\n        self.assertIsInstance(aligns, list)\n        for align_list in aligns:\n            for align_string in align_list:\n                if align_string is not None:\n                    self.assertIsInstance(align_string, string_types)\n        self.assertEqual(len(results), len(scores))\n        self.assertEqual(len(scores), len(inp) * n_best)\n        self.assertEqual(len(time), 1)\n        self.assertIsInstance(time, dict)\n        self.assertIn(""translation"", time)\n\n\nclass TestTranslationServer(unittest.TestCase):\n    # this could be considered an integration test because it touches\n    # the filesystem for the config file (and the models)\n\n    CFG_F = os.path.join(\n        TEST_DIR, ""test_translation_server_config_file.json"")\n\n    def tearDown(self):\n        if os.path.exists(self.CFG_F):\n            os.remove(self.CFG_F)\n\n    def write(self, cfg):\n        with open(self.CFG_F, ""w"") as f:\n            f.write(cfg)\n\n    CFG_NO_LOAD = dedent(""""""\\\n        {\n            ""models_root"": ""%s"",\n            ""models"": [\n                {\n                    ""id"": 100,\n                    ""model"": ""test_model.pt"",\n                    ""timeout"": -1,\n                    ""on_timeout"": ""to_cpu"",\n                    ""load"": false,\n                    ""opt"": {\n                        ""beam_size"": 5\n                    }\n                }\n            ]\n        }\n        """""" % TEST_DIR)\n\n    def test_start_without_initial_loading(self):\n        self.write(self.CFG_NO_LOAD)\n        sv = TranslationServer()\n        sv.start(self.CFG_F)\n        self.assertFalse(sv.models[100].loaded)\n        self.assertEqual(set(sv.models.keys()), {100})\n\n    CFG_LOAD = dedent(""""""\\\n        {\n            ""models_root"": ""%s"",\n            ""models"": [\n                {\n                    ""id"": 100,\n                    ""model"": ""test_model.pt"",\n                    ""timeout"": -1,\n                    ""on_timeout"": ""to_cpu"",\n                    ""load"": true,\n                    ""opt"": {\n                        ""beam_size"": 5\n                    }\n                }\n            ]\n        }\n        """""" % TEST_DIR)\n\n    def test_start_with_initial_loading(self):\n        self.write(self.CFG_LOAD)\n        sv = TranslationServer()\n        sv.start(self.CFG_F)\n        self.assertTrue(sv.models[100].loaded)\n        self.assertEqual(set(sv.models.keys()), {100})\n\n    CFG_2_MODELS = dedent(""""""\\\n        {\n            ""models_root"": ""%s"",\n            ""models"": [\n                {\n                    ""id"": 100,\n                    ""model"": ""test_model.pt"",\n                    ""timeout"": -1,\n                    ""on_timeout"": ""to_cpu"",\n                    ""load"": true,\n                    ""opt"": {\n                        ""beam_size"": 5\n                    }\n                },\n                {\n                    ""id"": 1000,\n                    ""model"": ""test_model2.pt"",\n                    ""timeout"": -1,\n                    ""on_timeout"": ""to_cpu"",\n                    ""load"": false,\n                    ""opt"": {\n                        ""beam_size"": 5\n                    }\n                }\n            ]\n        }\n        """""" % TEST_DIR)\n\n    def test_start_with_two_models(self):\n        self.write(self.CFG_2_MODELS)\n        sv = TranslationServer()\n        sv.start(self.CFG_F)\n        self.assertTrue(sv.models[100].loaded)\n        self.assertFalse(sv.models[1000].loaded)\n        self.assertEqual(set(sv.models.keys()), {100, 1000})\n'"
onmt/tests/utils_for_tests.py,0,"b'import itertools\n\n\ndef product_dict(**kwargs):\n    keys = kwargs.keys()\n    vals = kwargs.values()\n    for instance in itertools.product(*vals):\n        yield dict(zip(keys, instance))\n'"
onmt/translate/__init__.py,0,"b'"""""" Modules for translation """"""\nfrom onmt.translate.translator import Translator\nfrom onmt.translate.translation import Translation, TranslationBuilder\nfrom onmt.translate.beam_search import BeamSearch, GNMTGlobalScorer\nfrom onmt.translate.decode_strategy import DecodeStrategy\nfrom onmt.translate.greedy_search import GreedySearch\nfrom onmt.translate.penalties import PenaltyBuilder\nfrom onmt.translate.translation_server import TranslationServer, \\\n    ServerModelError\n\n__all__ = [\'Translator\', \'Translation\', \'BeamSearch\',\n           \'GNMTGlobalScorer\', \'TranslationBuilder\',\n           \'PenaltyBuilder\', \'TranslationServer\', \'ServerModelError\',\n           ""DecodeStrategy"", ""GreedySearch""]\n'"
onmt/translate/beam_search.py,20,"b'import torch\nfrom onmt.translate import penalties\nfrom onmt.translate.decode_strategy import DecodeStrategy\nfrom onmt.utils.misc import tile\n\nimport warnings\n\n\nclass BeamSearch(DecodeStrategy):\n    """"""Generation beam search.\n\n    Note that the attributes list is not exhaustive. Rather, it highlights\n    tensors to document their shape. (Since the state variables\' ""batch""\n    size decreases as beams finish, we denote this axis with a B rather than\n    ``batch_size``).\n\n    Args:\n        beam_size (int): Number of beams to use (see base ``parallel_paths``).\n        batch_size (int): See base.\n        pad (int): See base.\n        bos (int): See base.\n        eos (int): See base.\n        n_best (int): Don\'t stop until at least this many beams have\n            reached EOS.\n        global_scorer (onmt.translate.GNMTGlobalScorer): Scorer instance.\n        min_length (int): See base.\n        max_length (int): See base.\n        return_attention (bool): See base.\n        block_ngram_repeat (int): See base.\n        exclusion_tokens (set[int]): See base.\n\n    Attributes:\n        top_beam_finished (ByteTensor): Shape ``(B,)``.\n        _batch_offset (LongTensor): Shape ``(B,)``.\n        _beam_offset (LongTensor): Shape ``(batch_size x beam_size,)``.\n        alive_seq (LongTensor): See base.\n        topk_log_probs (FloatTensor): Shape ``(B x beam_size,)``. These\n            are the scores used for the topk operation.\n        memory_lengths (LongTensor): Lengths of encodings. Used for\n            masking attentions.\n        select_indices (LongTensor or NoneType): Shape\n            ``(B x beam_size,)``. This is just a flat view of the\n            ``_batch_index``.\n        topk_scores (FloatTensor): Shape\n            ``(B, beam_size)``. These are the\n            scores a sequence will receive if it finishes.\n        topk_ids (LongTensor): Shape ``(B, beam_size)``. These are the\n            word indices of the topk predictions.\n        _batch_index (LongTensor): Shape ``(B, beam_size)``.\n        _prev_penalty (FloatTensor or NoneType): Shape\n            ``(B, beam_size)``. Initialized to ``None``.\n        _coverage (FloatTensor or NoneType): Shape\n            ``(1, B x beam_size, inp_seq_len)``.\n        hypotheses (list[list[Tuple[Tensor]]]): Contains a tuple\n            of score (float), sequence (long), and attention (float or None).\n    """"""\n\n    def __init__(self, beam_size, batch_size, pad, bos, eos, n_best,\n                 global_scorer, min_length, max_length, return_attention,\n                 block_ngram_repeat, exclusion_tokens,\n                 stepwise_penalty, ratio):\n        super(BeamSearch, self).__init__(\n            pad, bos, eos, batch_size, beam_size, min_length,\n            block_ngram_repeat, exclusion_tokens, return_attention,\n            max_length)\n        # beam parameters\n        self.global_scorer = global_scorer\n        self.beam_size = beam_size\n        self.n_best = n_best\n        self.ratio = ratio\n\n        # result caching\n        self.hypotheses = [[] for _ in range(batch_size)]\n\n        # beam state\n        self.top_beam_finished = torch.zeros([batch_size], dtype=torch.uint8)\n        # BoolTensor was introduced in pytorch 1.2\n        try:\n            self.top_beam_finished = self.top_beam_finished.bool()\n        except AttributeError:\n            pass\n        self._batch_offset = torch.arange(batch_size, dtype=torch.long)\n\n        self.select_indices = None\n        self.done = False\n        # ""global state"" of the old beam\n        self._prev_penalty = None\n        self._coverage = None\n\n        self._stepwise_cov_pen = (\n            stepwise_penalty and self.global_scorer.has_cov_pen)\n        self._vanilla_cov_pen = (\n            not stepwise_penalty and self.global_scorer.has_cov_pen)\n        self._cov_pen = self.global_scorer.has_cov_pen\n\n    def initialize(self, memory_bank, src_lengths, src_map=None, device=None,\n                   target_prefix=None):\n        """"""Initialize for decoding.\n        Repeat src objects `beam_size` times.\n        """"""\n\n        def fn_map_state(state, dim):\n            return tile(state, self.beam_size, dim=dim)\n\n        if isinstance(memory_bank, tuple):\n            memory_bank = tuple(tile(x, self.beam_size, dim=1)\n                                for x in memory_bank)\n            mb_device = memory_bank[0].device\n        else:\n            memory_bank = tile(memory_bank, self.beam_size, dim=1)\n            mb_device = memory_bank.device\n        if src_map is not None:\n            src_map = tile(src_map, self.beam_size, dim=1)\n        if device is None:\n            device = mb_device\n\n        self.memory_lengths = tile(src_lengths, self.beam_size)\n        if target_prefix is not None:\n            target_prefix = tile(target_prefix, self.beam_size, dim=1)\n\n        super(BeamSearch, self).initialize(\n            memory_bank, self.memory_lengths, src_map, device, target_prefix)\n\n        self.best_scores = torch.full(\n            [self.batch_size], -1e10, dtype=torch.float, device=device)\n        self._beam_offset = torch.arange(\n            0, self.batch_size * self.beam_size, step=self.beam_size,\n            dtype=torch.long, device=device)\n        self.topk_log_probs = torch.tensor(\n            [0.0] + [float(""-inf"")] * (self.beam_size - 1), device=device\n        ).repeat(self.batch_size)\n        # buffers for the topk scores and \'backpointer\'\n        self.topk_scores = torch.empty((self.batch_size, self.beam_size),\n                                       dtype=torch.float, device=device)\n        self.topk_ids = torch.empty((self.batch_size, self.beam_size),\n                                    dtype=torch.long, device=device)\n        self._batch_index = torch.empty([self.batch_size, self.beam_size],\n                                        dtype=torch.long, device=device)\n        return fn_map_state, memory_bank, self.memory_lengths, src_map\n\n    @property\n    def current_predictions(self):\n        return self.alive_seq[:, -1]\n\n    @property\n    def current_backptr(self):\n        # for testing\n        return self.select_indices.view(self.batch_size, self.beam_size)\\\n            .fmod(self.beam_size)\n\n    @property\n    def batch_offset(self):\n        return self._batch_offset\n\n    def _pick(self, log_probs):\n        """"""Return token decision for a step.\n\n        Args:\n            log_probs (FloatTensor): (B, vocab_size)\n\n        Returns:\n            topk_scores (FloatTensor): (B, beam_size)\n            topk_ids (LongTensor): (B, beam_size)\n        """"""\n        vocab_size = log_probs.size(-1)\n        # maybe fix some prediction at this step by modifying log_probs\n        log_probs = self.target_prefixing(log_probs)\n\n        # Flatten probs into a list of possibilities.\n        curr_scores = log_probs.reshape(-1, self.beam_size * vocab_size)\n        topk_scores, topk_ids = torch.topk(curr_scores, self.beam_size, dim=-1)\n        return topk_scores, topk_ids\n\n    def advance(self, log_probs, attn):\n        vocab_size = log_probs.size(-1)\n\n        # using integer division to get an integer _B without casting\n        _B = log_probs.shape[0] // self.beam_size\n\n        if self._stepwise_cov_pen and self._prev_penalty is not None:\n            self.topk_log_probs += self._prev_penalty\n            self.topk_log_probs -= self.global_scorer.cov_penalty(\n                self._coverage + attn, self.global_scorer.beta).view(\n                _B, self.beam_size)\n\n        # force the output to be longer than self.min_length\n        step = len(self)\n        self.ensure_min_length(log_probs)\n\n        # Multiply probs by the beam probability.\n        log_probs += self.topk_log_probs.view(_B * self.beam_size, 1)\n\n        # if the sequence ends now, then the penalty is the current\n        # length + 1, to include the EOS token\n        length_penalty = self.global_scorer.length_penalty(\n            step + 1, alpha=self.global_scorer.alpha)\n\n        curr_scores = log_probs / length_penalty\n\n        # Avoid any direction that would repeat unwanted ngrams\n        self.block_ngram_repeats(curr_scores)\n\n        # Pick up candidate token by curr_scores\n        self.topk_scores, self.topk_ids = self._pick(curr_scores)\n\n        # Recover log probs.\n        # Length penalty is just a scalar. It doesn\'t matter if it\'s applied\n        # before or after the topk.\n        torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n\n        # Resolve beam origin and map to batch index flat representation.\n        torch.div(self.topk_ids, vocab_size, out=self._batch_index)\n        self._batch_index += self._beam_offset[:_B].unsqueeze(1)\n        self.select_indices = self._batch_index.view(_B * self.beam_size)\n        self.topk_ids.fmod_(vocab_size)  # resolve true word ids\n\n        # Append last prediction.\n        self.alive_seq = torch.cat(\n            [self.alive_seq.index_select(0, self.select_indices),\n             self.topk_ids.view(_B * self.beam_size, 1)], -1)\n\n        self.maybe_update_forbidden_tokens()\n\n        if self.return_attention or self._cov_pen:\n            current_attn = attn.index_select(1, self.select_indices)\n            if step == 1:\n                self.alive_attn = current_attn\n                # update global state (step == 1)\n                if self._cov_pen:  # coverage penalty\n                    self._prev_penalty = torch.zeros_like(self.topk_log_probs)\n                    self._coverage = current_attn\n            else:\n                self.alive_attn = self.alive_attn.index_select(\n                    1, self.select_indices)\n                self.alive_attn = torch.cat([self.alive_attn, current_attn], 0)\n                # update global state (step > 1)\n                if self._cov_pen:\n                    self._coverage = self._coverage.index_select(\n                        1, self.select_indices)\n                    self._coverage += current_attn\n                    self._prev_penalty = self.global_scorer.cov_penalty(\n                        self._coverage, beta=self.global_scorer.beta).view(\n                            _B, self.beam_size)\n\n        if self._vanilla_cov_pen:\n            # shape: (batch_size x beam_size, 1)\n            cov_penalty = self.global_scorer.cov_penalty(\n                self._coverage,\n                beta=self.global_scorer.beta)\n            self.topk_scores -= cov_penalty.view(_B, self.beam_size).float()\n\n        self.is_finished = self.topk_ids.eq(self.eos)\n        self.ensure_max_length()\n\n    def update_finished(self):\n        # Penalize beams that finished.\n        _B_old = self.topk_log_probs.shape[0]\n        step = self.alive_seq.shape[-1]  # 1 greater than the step in advance\n        self.topk_log_probs.masked_fill_(self.is_finished, -1e10)\n        # on real data (newstest2017) with the pretrained transformer,\n        # it\'s faster to not move this back to the original device\n        self.is_finished = self.is_finished.to(\'cpu\')\n        self.top_beam_finished |= self.is_finished[:, 0].eq(1)\n        predictions = self.alive_seq.view(_B_old, self.beam_size, step)\n        attention = (\n            self.alive_attn.view(\n                step - 1, _B_old, self.beam_size, self.alive_attn.size(-1))\n            if self.alive_attn is not None else None)\n        non_finished_batch = []\n        for i in range(self.is_finished.size(0)):  # Batch level\n            b = self._batch_offset[i]\n            finished_hyp = self.is_finished[i].nonzero().view(-1)\n            # Store finished hypotheses for this batch.\n            for j in finished_hyp:  # Beam level: finished beam j in batch i\n                if self.ratio > 0:\n                    s = self.topk_scores[i, j] / (step + 1)\n                    if self.best_scores[b] < s:\n                        self.best_scores[b] = s\n                self.hypotheses[b].append((\n                    self.topk_scores[i, j],\n                    predictions[i, j, 1:],  # Ignore start_token.\n                    attention[:, i, j, :self.memory_lengths[i]]\n                    if attention is not None else None))\n            # End condition is the top beam finished and we can return\n            # n_best hypotheses.\n            if self.ratio > 0:\n                pred_len = self.memory_lengths[i] * self.ratio\n                finish_flag = ((self.topk_scores[i, 0] / pred_len)\n                               <= self.best_scores[b]) or \\\n                    self.is_finished[i].all()\n            else:\n                finish_flag = self.top_beam_finished[i] != 0\n            if finish_flag and len(self.hypotheses[b]) >= self.n_best:\n                best_hyp = sorted(\n                    self.hypotheses[b], key=lambda x: x[0], reverse=True)\n                for n, (score, pred, attn) in enumerate(best_hyp):\n                    if n >= self.n_best:\n                        break\n                    self.scores[b].append(score)\n                    self.predictions[b].append(pred)  # ``(batch, n_best,)``\n                    self.attention[b].append(\n                        attn if attn is not None else [])\n            else:\n                non_finished_batch.append(i)\n        non_finished = torch.tensor(non_finished_batch)\n        # If all sentences are translated, no need to go further.\n        if len(non_finished) == 0:\n            self.done = True\n            return\n\n        _B_new = non_finished.shape[0]\n        # Remove finished batches for the next step.\n        self.top_beam_finished = self.top_beam_finished.index_select(\n            0, non_finished)\n        self._batch_offset = self._batch_offset.index_select(0, non_finished)\n        non_finished = non_finished.to(self.topk_ids.device)\n        self.topk_log_probs = self.topk_log_probs.index_select(0,\n                                                               non_finished)\n        self._batch_index = self._batch_index.index_select(0, non_finished)\n        self.select_indices = self._batch_index.view(_B_new * self.beam_size)\n        self.alive_seq = predictions.index_select(0, non_finished) \\\n            .view(-1, self.alive_seq.size(-1))\n        self.topk_scores = self.topk_scores.index_select(0, non_finished)\n        self.topk_ids = self.topk_ids.index_select(0, non_finished)\n        self.maybe_update_target_prefix(self.select_indices)\n        if self.alive_attn is not None:\n            inp_seq_len = self.alive_attn.size(-1)\n            self.alive_attn = attention.index_select(1, non_finished) \\\n                .view(step - 1, _B_new * self.beam_size, inp_seq_len)\n            if self._cov_pen:\n                self._coverage = self._coverage \\\n                    .view(1, _B_old, self.beam_size, inp_seq_len) \\\n                    .index_select(1, non_finished) \\\n                    .view(1, _B_new * self.beam_size, inp_seq_len)\n                if self._stepwise_cov_pen:\n                    self._prev_penalty = self._prev_penalty.index_select(\n                        0, non_finished)\n\n\nclass GNMTGlobalScorer(object):\n    """"""NMT re-ranking.\n\n    Args:\n       alpha (float): Length parameter.\n       beta (float):  Coverage parameter.\n       length_penalty (str): Length penalty strategy.\n       coverage_penalty (str): Coverage penalty strategy.\n\n    Attributes:\n        alpha (float): See above.\n        beta (float): See above.\n        length_penalty (callable): See :class:`penalties.PenaltyBuilder`.\n        coverage_penalty (callable): See :class:`penalties.PenaltyBuilder`.\n        has_cov_pen (bool): See :class:`penalties.PenaltyBuilder`.\n        has_len_pen (bool): See :class:`penalties.PenaltyBuilder`.\n    """"""\n\n    @classmethod\n    def from_opt(cls, opt):\n        return cls(\n            opt.alpha,\n            opt.beta,\n            opt.length_penalty,\n            opt.coverage_penalty)\n\n    def __init__(self, alpha, beta, length_penalty, coverage_penalty):\n        self._validate(alpha, beta, length_penalty, coverage_penalty)\n        self.alpha = alpha\n        self.beta = beta\n        penalty_builder = penalties.PenaltyBuilder(coverage_penalty,\n                                                   length_penalty)\n        self.has_cov_pen = penalty_builder.has_cov_pen\n        # Term will be subtracted from probability\n        self.cov_penalty = penalty_builder.coverage_penalty\n\n        self.has_len_pen = penalty_builder.has_len_pen\n        # Probability will be divided by this\n        self.length_penalty = penalty_builder.length_penalty\n\n    @classmethod\n    def _validate(cls, alpha, beta, length_penalty, coverage_penalty):\n        # these warnings indicate that either the alpha/beta\n        # forces a penalty to be a no-op, or a penalty is a no-op but\n        # the alpha/beta would suggest otherwise.\n        if length_penalty is None or length_penalty == ""none"":\n            if alpha != 0:\n                warnings.warn(""Non-default `alpha` with no length penalty. ""\n                              ""`alpha` has no effect."")\n        else:\n            # using some length penalty\n            if length_penalty == ""wu"" and alpha == 0.:\n                warnings.warn(""Using length penalty Wu with alpha==0 ""\n                              ""is equivalent to using length penalty none."")\n        if coverage_penalty is None or coverage_penalty == ""none"":\n            if beta != 0:\n                warnings.warn(""Non-default `beta` with no coverage penalty. ""\n                              ""`beta` has no effect."")\n        else:\n            # using some coverage penalty\n            if beta == 0.:\n                warnings.warn(""Non-default coverage penalty with beta==0 ""\n                              ""is equivalent to using coverage penalty none."")\n'"
onmt/translate/decode_strategy.py,8,"b'import torch\n\n\nclass DecodeStrategy(object):\n    """"""Base class for generation strategies.\n\n    Args:\n        pad (int): Magic integer in output vocab.\n        bos (int): Magic integer in output vocab.\n        eos (int): Magic integer in output vocab.\n        batch_size (int): Current batch size.\n        parallel_paths (int): Decoding strategies like beam search\n            use parallel paths. Each batch is repeated ``parallel_paths``\n            times in relevant state tensors.\n        min_length (int): Shortest acceptable generation, not counting\n            begin-of-sentence or end-of-sentence.\n        max_length (int): Longest acceptable sequence, not counting\n            begin-of-sentence (presumably there has been no EOS\n            yet if max_length is used as a cutoff).\n        block_ngram_repeat (int): Block beams where\n            ``block_ngram_repeat``-grams repeat.\n        exclusion_tokens (set[int]): If a gram contains any of these\n            tokens, it may repeat.\n        return_attention (bool): Whether to work with attention too. If this\n            is true, it is assumed that the decoder is attentional.\n\n    Attributes:\n        pad (int): See above.\n        bos (int): See above.\n        eos (int): See above.\n        predictions (list[list[LongTensor]]): For each batch, holds a\n            list of beam prediction sequences.\n        scores (list[list[FloatTensor]]): For each batch, holds a\n            list of scores.\n        attention (list[list[FloatTensor or list[]]]): For each\n            batch, holds a list of attention sequence tensors\n            (or empty lists) having shape ``(step, inp_seq_len)`` where\n            ``inp_seq_len`` is the length of the sample (not the max\n            length of all inp seqs).\n        alive_seq (LongTensor): Shape ``(B x parallel_paths, step)``.\n            This sequence grows in the ``step`` axis on each call to\n            :func:`advance()`.\n        is_finished (ByteTensor or NoneType): Shape\n            ``(B, parallel_paths)``. Initialized to ``None``.\n        alive_attn (FloatTensor or NoneType): If tensor, shape is\n            ``(step, B x parallel_paths, inp_seq_len)``, where ``inp_seq_len``\n            is the (max) length of the input sequence.\n        target_prefix (LongTensor or NoneType): If tensor, shape is\n            ``(B x parallel_paths, prefix_seq_len)``, where ``prefix_seq_len``\n            is the (max) length of the pre-fixed prediction.\n        min_length (int): See above.\n        max_length (int): See above.\n        block_ngram_repeat (int): See above.\n        exclusion_tokens (set[int]): See above.\n        return_attention (bool): See above.\n        done (bool): See above.\n    """"""\n\n    def __init__(self, pad, bos, eos, batch_size, parallel_paths,\n                 min_length, block_ngram_repeat, exclusion_tokens,\n                 return_attention, max_length):\n\n        # magic indices\n        self.pad = pad\n        self.bos = bos\n        self.eos = eos\n\n        self.batch_size = batch_size\n        self.parallel_paths = parallel_paths\n        # result caching\n        self.predictions = [[] for _ in range(batch_size)]\n        self.scores = [[] for _ in range(batch_size)]\n        self.attention = [[] for _ in range(batch_size)]\n\n        self.alive_attn = None\n\n        self.min_length = min_length\n        self.max_length = max_length\n\n        self.block_ngram_repeat = block_ngram_repeat\n        n_paths = batch_size * parallel_paths\n        self.forbidden_tokens = [dict() for _ in range(n_paths)]\n\n        self.exclusion_tokens = exclusion_tokens\n        self.return_attention = return_attention\n\n        self.done = False\n\n    def initialize(self, memory_bank, src_lengths, src_map=None, device=None,\n                   target_prefix=None):\n        """"""DecodeStrategy subclasses should override :func:`initialize()`.\n\n        `initialize` should be called before all actions.\n        used to prepare necessary ingredients for decode.\n        """"""\n        if device is None:\n            device = torch.device(\'cpu\')\n        self.alive_seq = torch.full(\n            [self.batch_size * self.parallel_paths, 1], self.bos,\n            dtype=torch.long, device=device)\n        self.is_finished = torch.zeros(\n            [self.batch_size, self.parallel_paths],\n            dtype=torch.uint8, device=device)\n        if target_prefix is not None:\n            seq_len, batch_size, n_feats = target_prefix.size()\n            assert batch_size == self.batch_size * self.parallel_paths,\\\n                ""forced target_prefix should\'ve extend to same number of path!""\n            target_prefix_words = target_prefix[:, :, 0].transpose(0, 1)\n            target_prefix = target_prefix_words[:, 1:]  # remove bos\n            # fix length constraint\n            prefix_non_pad = target_prefix.ne(self.pad).sum(dim=-1).tolist()\n            self.max_length += max(prefix_non_pad)\n            self.min_length += min(prefix_non_pad)\n        self.target_prefix = target_prefix  # NOTE: forced prefix words\n        return None, memory_bank, src_lengths, src_map\n\n    def __len__(self):\n        return self.alive_seq.shape[1]\n\n    def ensure_min_length(self, log_probs):\n        if len(self) <= self.min_length:\n            log_probs[:, self.eos] = -1e20\n\n    def ensure_max_length(self):\n        # add one to account for BOS. Don\'t account for EOS because hitting\n        # this implies it hasn\'t been found.\n        if len(self) == self.max_length + 1:\n            self.is_finished.fill_(1)\n\n    def block_ngram_repeats(self, log_probs):\n        """"""\n        We prevent the beam from going in any direction that would repeat any\n        ngram of size <block_ngram_repeat> more thant once.\n\n        The way we do it: we maintain a list of all ngrams of size\n        <block_ngram_repeat> that is updated each time the beam advances, and\n        manually put any token that would lead to a repeated ngram to 0.\n\n        This improves on the previous version\'s complexity:\n           - previous version\'s complexity: batch_size * beam_size * len(self)\n           - current version\'s complexity: batch_size * beam_size\n\n        This improves on the previous version\'s accuracy;\n           - Previous version blocks the whole beam, whereas here we only\n            block specific tokens.\n           - Before the translation would fail when all beams contained\n            repeated ngrams. This is sure to never happen here.\n        """"""\n\n        # we don\'t block nothing if the user doesn\'t want it\n        if self.block_ngram_repeat <= 0:\n            return\n\n        # we can\'t block nothing beam\'s too short\n        if len(self) < self.block_ngram_repeat:\n            return\n\n        n = self.block_ngram_repeat - 1\n        for path_idx in range(self.alive_seq.shape[0]):\n            # we check paths one by one\n\n            current_ngram = tuple(self.alive_seq[path_idx, -n:].tolist())\n            forbidden_tokens = self.forbidden_tokens[path_idx].get(\n                current_ngram, None)\n            if forbidden_tokens is not None:\n                log_probs[path_idx, list(forbidden_tokens)] = -10e20\n\n    def maybe_update_forbidden_tokens(self):\n        """"""We complete and reorder the list of forbidden_tokens""""""\n\n        # we don\'t forbid nothing if the user doesn\'t want it\n        if self.block_ngram_repeat <= 0:\n            return\n\n        # we can\'t forbid nothing if beam\'s too short\n        if len(self) < self.block_ngram_repeat:\n            return\n\n        n = self.block_ngram_repeat\n\n        forbidden_tokens = list()\n        for path_idx, seq in zip(self.select_indices, self.alive_seq):\n\n            # Reordering forbidden_tokens following beam selection\n            # We rebuild a dict to ensure we get the value and not the pointer\n            forbidden_tokens.append(\n                dict(self.forbidden_tokens[path_idx]))\n\n            # Grabing the newly selected tokens and associated ngram\n            current_ngram = tuple(seq[-n:].tolist())\n\n            # skip the blocking if any token in current_ngram is excluded\n            if set(current_ngram) & self.exclusion_tokens:\n                continue\n\n            forbidden_tokens[-1].setdefault(current_ngram[:-1], set())\n            forbidden_tokens[-1][current_ngram[:-1]].add(current_ngram[-1])\n\n        self.forbidden_tokens = forbidden_tokens\n\n    def target_prefixing(self, log_probs):\n        """"""Fix the first part of predictions with `self.target_prefix`.\n\n        Args:\n            log_probs (FloatTensor): logits of size ``(B, vocab_size)``.\n\n        Returns:\n            log_probs (FloatTensor): modified logits in ``(B, vocab_size)``.\n        """"""\n        _B, vocab_size = log_probs.size()\n        step = len(self)\n        if (self.target_prefix is not None and\n                step <= self.target_prefix.size(1)):\n            pick_idx = self.target_prefix[:, step - 1].tolist()  # (B)\n            pick_coo = [[path_i, pick] for path_i, pick in enumerate(pick_idx)\n                        if pick not in [self.eos, self.pad]]\n            if len(pick_coo) > 0:\n                pick_coo = torch.tensor(pick_coo).to(self.target_prefix)\n                pick_fill_value = 10000 * torch.ones(\n                    [pick_coo.size(0)], dtype=log_probs.dtype)\n                # pickups: Tensor where specified index were set to 1, others 0\n                pickups = torch.sparse_coo_tensor(\n                    pick_coo.t(), pick_fill_value,\n                    size=log_probs.size(), device=log_probs.device).to_dense()\n                # Adding pickups to log_probs making probabilities of\n                # specified index close to 1\n                log_probs += pickups\n        return log_probs\n\n    def maybe_update_target_prefix(self, select_index):\n        """"""We update / reorder `target_prefix` for alive path.""""""\n        if self.target_prefix is None:\n            return\n        # prediction step have surpass length of given target_prefix,\n        # no need to further change this attr\n        if len(self) > self.target_prefix.size(1):\n            return\n        self.target_prefix = self.target_prefix.index_select(0, select_index)\n\n    def advance(self, log_probs, attn):\n        """"""DecodeStrategy subclasses should override :func:`advance()`.\n\n        Advance is used to update ``self.alive_seq``, ``self.is_finished``,\n        and, when appropriate, ``self.alive_attn``.\n        """"""\n\n        raise NotImplementedError()\n\n    def update_finished(self):\n        """"""DecodeStrategy subclasses should override :func:`update_finished()`.\n\n        ``update_finished`` is used to update ``self.predictions``,\n        ``self.scores``, and other ""output"" attributes.\n        """"""\n\n        raise NotImplementedError()\n'"
onmt/translate/greedy_search.py,11,"b'import torch\n\nfrom onmt.translate.decode_strategy import DecodeStrategy\n\n\ndef sample_with_temperature(logits, sampling_temp, keep_topk):\n    """"""Select next tokens randomly from the top k possible next tokens.\n\n    Samples from a categorical distribution over the ``keep_topk`` words using\n    the category probabilities ``logits / sampling_temp``.\n\n    Args:\n        logits (FloatTensor): Shaped ``(batch_size, vocab_size)``.\n            These can be logits (``(-inf, inf)``) or log-probs (``(-inf, 0]``).\n            (The distribution actually uses the log-probabilities\n            ``logits - logits.logsumexp(-1)``, which equals the logits if\n            they are log-probabilities summing to 1.)\n        sampling_temp (float): Used to scale down logits. The higher the\n            value, the more likely it is that a non-max word will be\n            sampled.\n        keep_topk (int): This many words could potentially be chosen. The\n            other logits are set to have probability 0.\n\n    Returns:\n        (LongTensor, FloatTensor):\n\n        * topk_ids: Shaped ``(batch_size, 1)``. These are\n          the sampled word indices in the output vocab.\n        * topk_scores: Shaped ``(batch_size, 1)``. These\n          are essentially ``(logits / sampling_temp)[topk_ids]``.\n    """"""\n\n    if sampling_temp == 0.0 or keep_topk == 1:\n        # For temp=0.0, take the argmax to avoid divide-by-zero errors.\n        # keep_topk=1 is also equivalent to argmax.\n        topk_scores, topk_ids = logits.topk(1, dim=-1)\n        if sampling_temp > 0:\n            topk_scores /= sampling_temp\n    else:\n        logits = torch.div(logits, sampling_temp)\n\n        if keep_topk > 0:\n            top_values, top_indices = torch.topk(logits, keep_topk, dim=1)\n            kth_best = top_values[:, -1].view([-1, 1])\n            kth_best = kth_best.repeat([1, logits.shape[1]]).float()\n\n            # Set all logits that are not in the top-k to -10000.\n            # This puts the probabilities close to 0.\n            ignore = torch.lt(logits, kth_best)\n            logits = logits.masked_fill(ignore, -10000)\n\n        dist = torch.distributions.Multinomial(\n            logits=logits, total_count=1)\n        topk_ids = torch.argmax(dist.sample(), dim=1, keepdim=True)\n        topk_scores = logits.gather(dim=1, index=topk_ids)\n    return topk_ids, topk_scores\n\n\nclass GreedySearch(DecodeStrategy):\n    """"""Select next tokens randomly from the top k possible next tokens.\n\n    The ``scores`` attribute\'s lists are the score, after applying temperature,\n    of the final prediction (either EOS or the final token in the event\n    that ``max_length`` is reached)\n\n    Args:\n        pad (int): See base.\n        bos (int): See base.\n        eos (int): See base.\n        batch_size (int): See base.\n        min_length (int): See base.\n        max_length (int): See base.\n        block_ngram_repeat (int): See base.\n        exclusion_tokens (set[int]): See base.\n        return_attention (bool): See base.\n        max_length (int): See base.\n        sampling_temp (float): See\n            :func:`~onmt.translate.greedy_search.sample_with_temperature()`.\n        keep_topk (int): See\n            :func:`~onmt.translate.greedy_search.sample_with_temperature()`.\n    """"""\n\n    def __init__(self, pad, bos, eos, batch_size, min_length,\n                 block_ngram_repeat, exclusion_tokens, return_attention,\n                 max_length, sampling_temp, keep_topk):\n        assert block_ngram_repeat == 0\n        super(GreedySearch, self).__init__(\n            pad, bos, eos, batch_size, 1, min_length, block_ngram_repeat,\n            exclusion_tokens, return_attention, max_length)\n        self.sampling_temp = sampling_temp\n        self.keep_topk = keep_topk\n        self.topk_scores = None\n\n    def initialize(self, memory_bank, src_lengths, src_map=None, device=None,\n                   target_prefix=None):\n        """"""Initialize for decoding.""""""\n        fn_map_state = None\n\n        if isinstance(memory_bank, tuple):\n            mb_device = memory_bank[0].device\n        else:\n            mb_device = memory_bank.device\n        if device is None:\n            device = mb_device\n\n        self.memory_lengths = src_lengths\n        super(GreedySearch, self).initialize(\n            memory_bank, src_lengths, src_map, device, target_prefix)\n        self.select_indices = torch.arange(\n            self.batch_size, dtype=torch.long, device=device)\n        self.original_batch_idx = torch.arange(\n            self.batch_size, dtype=torch.long, device=device)\n        return fn_map_state, memory_bank, self.memory_lengths, src_map\n\n    @property\n    def current_predictions(self):\n        return self.alive_seq[:, -1]\n\n    @property\n    def batch_offset(self):\n        return self.select_indices\n\n    def _pick(self, log_probs):\n        """"""Function used to pick next tokens.\n\n        Args:\n            log_probs (FloatTensor): ``(batch_size, vocab_size)``.\n        """"""\n        # maybe fix some prediction at this step by modifying log_probs\n        log_probs = self.target_prefixing(log_probs)\n        topk_ids, topk_scores = sample_with_temperature(\n            log_probs, self.sampling_temp, self.keep_topk)\n        return topk_ids, topk_scores\n\n    def advance(self, log_probs, attn):\n        """"""Select next tokens randomly from the top k possible next tokens.\n\n        Args:\n            log_probs (FloatTensor): Shaped ``(batch_size, vocab_size)``.\n                These can be logits (``(-inf, inf)``) or log-probs\n                (``(-inf, 0]``). (The distribution actually uses the\n                log-probabilities ``logits - logits.logsumexp(-1)``,\n                which equals the logits if they are log-probabilities summing\n                to 1.)\n            attn (FloatTensor): Shaped ``(1, B, inp_seq_len)``.\n        """"""\n\n        self.ensure_min_length(log_probs)\n        self.block_ngram_repeats(log_probs)\n\n        topk_ids, self.topk_scores = self._pick(log_probs)\n\n        self.is_finished = topk_ids.eq(self.eos)\n\n        self.alive_seq = torch.cat([self.alive_seq, topk_ids], -1)\n        if self.return_attention:\n            if self.alive_attn is None:\n                self.alive_attn = attn\n            else:\n                self.alive_attn = torch.cat([self.alive_attn, attn], 0)\n        self.ensure_max_length()\n\n    def update_finished(self):\n        """"""Finalize scores and predictions.""""""\n        # shape: (sum(~ self.is_finished), 1)\n        finished_batches = self.is_finished.view(-1).nonzero()\n        for b in finished_batches.view(-1):\n            b_orig = self.original_batch_idx[b]\n            self.scores[b_orig].append(self.topk_scores[b, 0])\n            self.predictions[b_orig].append(self.alive_seq[b, 1:])\n            self.attention[b_orig].append(\n                self.alive_attn[:, b, :self.memory_lengths[b]]\n                if self.alive_attn is not None else [])\n        self.done = self.is_finished.all()\n        if self.done:\n            return\n        is_alive = ~self.is_finished.view(-1)\n        self.alive_seq = self.alive_seq[is_alive]\n        if self.alive_attn is not None:\n            self.alive_attn = self.alive_attn[:, is_alive]\n        self.select_indices = is_alive.nonzero().view(-1)\n        self.original_batch_idx = self.original_batch_idx[is_alive]\n        self.maybe_update_target_prefix(self.select_indices)\n'"
onmt/translate/penalties.py,4,"b'from __future__ import division\nimport torch\n\n\nclass PenaltyBuilder(object):\n    """"""Returns the Length and Coverage Penalty function for Beam Search.\n\n    Args:\n        length_pen (str): option name of length pen\n        cov_pen (str): option name of cov pen\n\n    Attributes:\n        has_cov_pen (bool): Whether coverage penalty is None (applying it\n            is a no-op). Note that the converse isn\'t true. Setting beta\n            to 0 should force coverage length to be a no-op.\n        has_len_pen (bool): Whether length penalty is None (applying it\n            is a no-op). Note that the converse isn\'t true. Setting alpha\n            to 1 should force length penalty to be a no-op.\n        coverage_penalty (callable[[FloatTensor, float], FloatTensor]):\n            Calculates the coverage penalty.\n        length_penalty (callable[[int, float], float]): Calculates\n            the length penalty.\n    """"""\n\n    def __init__(self, cov_pen, length_pen):\n        self.has_cov_pen = not self._pen_is_none(cov_pen)\n        self.coverage_penalty = self._coverage_penalty(cov_pen)\n        self.has_len_pen = not self._pen_is_none(length_pen)\n        self.length_penalty = self._length_penalty(length_pen)\n\n    @staticmethod\n    def _pen_is_none(pen):\n        return pen == ""none"" or pen is None\n\n    def _coverage_penalty(self, cov_pen):\n        if cov_pen == ""wu"":\n            return self.coverage_wu\n        elif cov_pen == ""summary"":\n            return self.coverage_summary\n        elif self._pen_is_none(cov_pen):\n            return self.coverage_none\n        else:\n            raise NotImplementedError(""No \'{:s}\' coverage penalty."".format(\n                cov_pen))\n\n    def _length_penalty(self, length_pen):\n        if length_pen == ""wu"":\n            return self.length_wu\n        elif length_pen == ""avg"":\n            return self.length_average\n        elif self._pen_is_none(length_pen):\n            return self.length_none\n        else:\n            raise NotImplementedError(""No \'{:s}\' length penalty."".format(\n                length_pen))\n\n    # Below are all the different penalty terms implemented so far.\n    # Subtract coverage penalty from topk log probs.\n    # Divide topk log probs by length penalty.\n\n    def coverage_wu(self, cov, beta=0.):\n        """"""GNMT coverage re-ranking score.\n\n        See ""Google\'s Neural Machine Translation System"" :cite:`wu2016google`.\n        ``cov`` is expected to be sized ``(*, seq_len)``, where ``*`` is\n        probably ``batch_size x beam_size`` but could be several\n        dimensions like ``(batch_size, beam_size)``. If ``cov`` is attention,\n        then the ``seq_len`` axis probably sums to (almost) 1.\n        """"""\n\n        penalty = -torch.min(cov, cov.clone().fill_(1.0)).log().sum(-1)\n        return beta * penalty\n\n    def coverage_summary(self, cov, beta=0.):\n        """"""Our summary penalty.""""""\n        penalty = torch.max(cov, cov.clone().fill_(1.0)).sum(-1)\n        penalty -= cov.size(-1)\n        return beta * penalty\n\n    def coverage_none(self, cov, beta=0.):\n        """"""Returns zero as penalty""""""\n        none = torch.zeros((1,), device=cov.device,\n                           dtype=torch.float)\n        if cov.dim() == 3:\n            none = none.unsqueeze(0)\n        return none\n\n    def length_wu(self, cur_len, alpha=0.):\n        """"""GNMT length re-ranking score.\n\n        See ""Google\'s Neural Machine Translation System"" :cite:`wu2016google`.\n        """"""\n\n        return ((5 + cur_len) / 6.0) ** alpha\n\n    def length_average(self, cur_len, alpha=0.):\n        """"""Returns the current sequence length.""""""\n        return cur_len\n\n    def length_none(self, cur_len, alpha=0.):\n        """"""Returns unmodified scores.""""""\n        return 1.0\n'"
onmt/translate/process_zh.py,0,"b'from pyhanlp import HanLP\nfrom snownlp import SnowNLP\nimport pkuseg\n\n\ndef wrap_str_func(func):\n    """"""\n    Wrapper to apply str function to the proper key of return_dict.\n    """"""\n    def wrapper(some_dict):\n        some_dict[""seg""] = [func(item) for item in some_dict[""seg""]]\n        return some_dict\n    return wrapper\n\n\n# Chinese segmentation\n@wrap_str_func\ndef zh_segmentator(line, server_model):\n    return "" "".join(pkuseg.pkuseg().cut(line))\n\n\n# Chinese simplify -> Chinese traditional standard\n@wrap_str_func\ndef zh_traditional_standard(line, server_model):\n    return HanLP.convertToTraditionalChinese(line)\n\n\n# Chinese simplify -> Chinese traditional (HongKong)\n@wrap_str_func\ndef zh_traditional_hk(line, server_model):\n    return HanLP.s2hk(line)\n\n\n# Chinese simplify -> Chinese traditional (Taiwan)\n@wrap_str_func\ndef zh_traditional_tw(line, server_model):\n    return HanLP.s2tw(line)\n\n\n# Chinese traditional -> Chinese simplify (v1)\n@wrap_str_func\ndef zh_simplify(line, server_model):\n    return HanLP.convertToSimplifiedChinese(line)\n\n\n# Chinese traditional -> Chinese simplify (v2)\n@wrap_str_func\ndef zh_simplify_v2(line, server_model):\n    return SnowNLP(line).han\n'"
onmt/translate/translation.py,1,"b'"""""" Translation main class """"""\nfrom __future__ import unicode_literals, print_function\n\nimport os\nimport torch\nfrom onmt.inputters.text_dataset import TextMultiField\nfrom onmt.utils.alignment import build_align_pharaoh\n\n\nclass TranslationBuilder(object):\n    """"""\n    Build a word-based translation from the batch output\n    of translator and the underlying dictionaries.\n\n    Replacement based on ""Addressing the Rare Word\n    Problem in Neural Machine Translation"" :cite:`Luong2015b`\n\n    Args:\n       data (onmt.inputters.Dataset): Data.\n       fields (List[Tuple[str, torchtext.data.Field]]): data fields\n       n_best (int): number of translations produced\n       replace_unk (bool): replace unknown words using attention\n       has_tgt (bool): will the batch have gold targets\n    """"""\n\n    def __init__(self, data, fields, n_best=1, replace_unk=False,\n                 has_tgt=False, phrase_table=""""):\n        self.data = data\n        self.fields = fields\n        self._has_text_src = isinstance(\n            dict(self.fields)[""src""], TextMultiField)\n        self.n_best = n_best\n        self.replace_unk = replace_unk\n        self.phrase_table_dict = {}\n        if phrase_table != """" and os.path.exists(phrase_table):\n            with open(phrase_table) as phrase_table_fd:\n                for line in phrase_table_fd:\n                    phrase_src, phrase_trg = line.rstrip(""\\n"").split(""|||"")\n                    self.phrase_table_dict[phrase_src] = phrase_trg\n        self.has_tgt = has_tgt\n\n    def _build_target_tokens(self, src, src_vocab, src_raw, pred, attn):\n        tgt_field = dict(self.fields)[""tgt""].base_field\n        vocab = tgt_field.vocab\n        tokens = []\n\n        for tok in pred:\n            if tok < len(vocab):\n                tokens.append(vocab.itos[tok])\n            else:\n                tokens.append(src_vocab.itos[tok - len(vocab)])\n            if tokens[-1] == tgt_field.eos_token:\n                tokens = tokens[:-1]\n                break\n        if self.replace_unk and attn is not None and src is not None:\n            for i in range(len(tokens)):\n                if tokens[i] == tgt_field.unk_token:\n                    _, max_index = attn[i][:len(src_raw)].max(0)\n                    tokens[i] = src_raw[max_index.item()]\n                    if self.phrase_table_dict:\n                        src_tok = src_raw[max_index.item()]\n                        if src_tok in self.phrase_table_dict:\n                            tokens[i] = self.phrase_table_dict[src_tok]\n        return tokens\n\n    def from_batch(self, translation_batch):\n        batch = translation_batch[""batch""]\n        assert(len(translation_batch[""gold_score""]) ==\n               len(translation_batch[""predictions""]))\n        batch_size = batch.batch_size\n\n        preds, pred_score, attn, align, gold_score, indices = list(zip(\n            *sorted(zip(translation_batch[""predictions""],\n                        translation_batch[""scores""],\n                        translation_batch[""attention""],\n                        translation_batch[""alignment""],\n                        translation_batch[""gold_score""],\n                        batch.indices.data),\n                    key=lambda x: x[-1])))\n\n        if not any(align):  # when align is a empty nested list\n            align = [None] * batch_size\n\n        # Sorting\n        inds, perm = torch.sort(batch.indices)\n        if self._has_text_src:\n            src = batch.src[0][:, :, 0].index_select(1, perm)\n        else:\n            src = None\n        tgt = batch.tgt[:, :, 0].index_select(1, perm) \\\n            if self.has_tgt else None\n\n        translations = []\n        for b in range(batch_size):\n            if self._has_text_src:\n                src_vocab = self.data.src_vocabs[inds[b]] \\\n                    if self.data.src_vocabs else None\n                src_raw = self.data.examples[inds[b]].src[0]\n            else:\n                src_vocab = None\n                src_raw = None\n            pred_sents = [self._build_target_tokens(\n                src[:, b] if src is not None else None,\n                src_vocab, src_raw,\n                preds[b][n],\n                align[b][n] if align[b] is not None else attn[b][n])\n                for n in range(self.n_best)]\n            gold_sent = None\n            if tgt is not None:\n                gold_sent = self._build_target_tokens(\n                    src[:, b] if src is not None else None,\n                    src_vocab, src_raw,\n                    tgt[1:, b] if tgt is not None else None, None)\n\n            translation = Translation(\n                src[:, b] if src is not None else None,\n                src_raw, pred_sents, attn[b], pred_score[b],\n                gold_sent, gold_score[b], align[b]\n            )\n            translations.append(translation)\n\n        return translations\n\n\nclass Translation(object):\n    """"""Container for a translated sentence.\n\n    Attributes:\n        src (LongTensor): Source word IDs.\n        src_raw (List[str]): Raw source words.\n        pred_sents (List[List[str]]): Words from the n-best translations.\n        pred_scores (List[List[float]]): Log-probs of n-best translations.\n        attns (List[FloatTensor]) : Attention distribution for each\n            translation.\n        gold_sent (List[str]): Words from gold translation.\n        gold_score (List[float]): Log-prob of gold translation.\n        word_aligns (List[FloatTensor]): Words Alignment distribution for\n            each translation.\n    """"""\n\n    __slots__ = [""src"", ""src_raw"", ""pred_sents"", ""attns"", ""pred_scores"",\n                 ""gold_sent"", ""gold_score"", ""word_aligns""]\n\n    def __init__(self, src, src_raw, pred_sents,\n                 attn, pred_scores, tgt_sent, gold_score, word_aligns):\n        self.src = src\n        self.src_raw = src_raw\n        self.pred_sents = pred_sents\n        self.attns = attn\n        self.pred_scores = pred_scores\n        self.gold_sent = tgt_sent\n        self.gold_score = gold_score\n        self.word_aligns = word_aligns\n\n    def log(self, sent_number):\n        """"""\n        Log translation.\n        """"""\n\n        msg = [\'\\nSENT {}: {}\\n\'.format(sent_number, self.src_raw)]\n\n        best_pred = self.pred_sents[0]\n        best_score = self.pred_scores[0]\n        pred_sent = \' \'.join(best_pred)\n        msg.append(\'PRED {}: {}\\n\'.format(sent_number, pred_sent))\n        msg.append(""PRED SCORE: {:.4f}\\n"".format(best_score))\n\n        if self.word_aligns is not None:\n            pred_align = self.word_aligns[0]\n            pred_align_pharaoh = build_align_pharaoh(pred_align)\n            pred_align_sent = \' \'.join(pred_align_pharaoh)\n            msg.append(""ALIGN: {}\\n"".format(pred_align_sent))\n\n        if self.gold_sent is not None:\n            tgt_sent = \' \'.join(self.gold_sent)\n            msg.append(\'GOLD {}: {}\\n\'.format(sent_number, tgt_sent))\n            msg.append((""GOLD SCORE: {:.4f}\\n"".format(self.gold_score)))\n        if len(self.pred_sents) > 1:\n            msg.append(\'\\nBEST HYP:\\n\')\n            for score, sent in zip(self.pred_scores, self.pred_sents):\n                msg.append(""[{:.4f}] {}\\n"".format(score, sent))\n\n        return """".join(msg)\n'"
onmt/translate/translation_server.py,4,"b'#!/usr/bin/env python\n""""""REST Translation server.""""""\nfrom __future__ import print_function\nimport codecs\nimport sys\nimport os\nimport time\nimport json\nimport threading\nimport re\nimport traceback\nimport importlib\nimport torch\nimport onmt.opts\n\nfrom itertools import islice\nfrom copy import deepcopy\n\nfrom onmt.utils.logging import init_logger\nfrom onmt.utils.misc import set_random_seed\nfrom onmt.utils.misc import check_model_config\nfrom onmt.utils.alignment import to_word_align\nfrom onmt.utils.parse import ArgumentParser\nfrom onmt.translate.translator import build_translator\n\n\ndef critical(func):\n    """"""Decorator for critical section (mutually exclusive code)""""""\n    def wrapper(server_model, *args, **kwargs):\n        if sys.version_info[0] == 3:\n            if not server_model.running_lock.acquire(True, 120):\n                raise ServerModelError(""Model %d running lock timeout""\n                                       % server_model.model_id)\n        else:\n            # semaphore doesn\'t have a timeout arg in Python 2.7\n            server_model.running_lock.acquire(True)\n        try:\n            o = func(server_model, *args, **kwargs)\n        except (Exception, RuntimeError):\n            server_model.running_lock.release()\n            raise\n        server_model.running_lock.release()\n        return o\n    return wrapper\n\n\nclass Timer:\n    def __init__(self, start=False):\n        self.stime = -1\n        self.prev = -1\n        self.times = {}\n        if start:\n            self.start()\n\n    def start(self):\n        self.stime = time.time()\n        self.prev = self.stime\n        self.times = {}\n\n    def tick(self, name=None, tot=False):\n        t = time.time()\n        if not tot:\n            elapsed = t - self.prev\n        else:\n            elapsed = t - self.stime\n        self.prev = t\n\n        if name is not None:\n            self.times[name] = elapsed\n        return elapsed\n\n\nclass ServerModelError(Exception):\n    pass\n\n\nclass CTranslate2Translator(object):\n    """"""\n    This class wraps the ctranslate2.Translator object to\n    reproduce the onmt.translate.translator API.\n    """"""\n\n    def __init__(self, model_path, device, device_index,\n                 batch_size, beam_size, n_best, preload=False):\n        import ctranslate2\n        self.translator = ctranslate2.Translator(\n            model_path,\n            device=device,\n            device_index=device_index,\n            inter_threads=1,\n            intra_threads=1,\n            compute_type=""default"")\n        self.batch_size = batch_size\n        self.beam_size = beam_size\n        self.n_best = n_best\n        if preload:\n            # perform a first request to initialize everything\n            dummy_translation = self.translate([""a""])\n            print(""Performed a dummy translation to initialize the model"",\n                  dummy_translation)\n            time.sleep(1)\n            self.translator.unload_model(to_cpu=True)\n\n    def translate(self, texts_to_translate, batch_size=8, **kwargs):\n        batch = [item.split("" "") for item in texts_to_translate]\n        preds = self.translator.translate_batch(\n            batch,\n            max_batch_size=self.batch_size,\n            beam_size=self.beam_size,\n            num_hypotheses=self.n_best\n        )\n        scores = [[item[""score""] for item in ex] for ex in preds]\n        predictions = [["" "".join(item[""tokens""]) for item in ex]\n                       for ex in preds]\n        return scores, predictions\n\n    def to_cpu(self):\n        self.translator.unload_model(to_cpu=True)\n\n    def to_gpu(self):\n        self.translator.load_model()\n\n\nclass TranslationServer(object):\n    def __init__(self):\n        self.models = {}\n        self.next_id = 0\n\n    def start(self, config_file):\n        """"""Read the config file and pre-/load the models.""""""\n        self.config_file = config_file\n        with open(self.config_file) as f:\n            self.confs = json.load(f)\n\n        self.models_root = self.confs.get(\'models_root\', \'./available_models\')\n        for i, conf in enumerate(self.confs[""models""]):\n            if ""models"" not in conf:\n                if ""model"" in conf:\n                    # backwards compatibility for confs\n                    conf[""models""] = [conf[""model""]]\n                else:\n                    raise ValueError(""""""Incorrect config file: missing \'models\'\n                                        parameter for model #%d"""""" % i)\n            check_model_config(conf, self.models_root)\n            kwargs = {\'timeout\': conf.get(\'timeout\', None),\n                      \'load\': conf.get(\'load\', None),\n                      \'preprocess_opt\': conf.get(\'preprocess\', None),\n                      \'tokenizer_opt\': conf.get(\'tokenizer\', None),\n                      \'postprocess_opt\': conf.get(\'postprocess\', None),\n                      \'on_timeout\': conf.get(\'on_timeout\', None),\n                      \'model_root\': conf.get(\'model_root\', self.models_root),\n                      \'ct2_model\': conf.get(\'ct2_model\', None)\n                      }\n            kwargs = {k: v for (k, v) in kwargs.items() if v is not None}\n            model_id = conf.get(""id"", None)\n            opt = conf[""opt""]\n            opt[""models""] = conf[""models""]\n            self.preload_model(opt, model_id=model_id, **kwargs)\n\n    def clone_model(self, model_id, opt, timeout=-1):\n        """"""Clone a model `model_id`.\n\n        Different options may be passed. If `opt` is None, it will use the\n        same set of options\n        """"""\n        if model_id in self.models:\n            if opt is None:\n                opt = self.models[model_id].user_opt\n            opt[""models""] = self.models[model_id].opt.models\n            return self.load_model(opt, timeout)\n        else:\n            raise ServerModelError(""No such model \'%s\'"" % str(model_id))\n\n    def load_model(self, opt, model_id=None, **model_kwargs):\n        """"""Load a model given a set of options\n        """"""\n        model_id = self.preload_model(opt, model_id=model_id, **model_kwargs)\n        load_time = self.models[model_id].load_time\n\n        return model_id, load_time\n\n    def preload_model(self, opt, model_id=None, **model_kwargs):\n        """"""Preloading the model: updating internal datastructure\n\n        It will effectively load the model if `load` is set\n        """"""\n        if model_id is not None:\n            if model_id in self.models.keys():\n                raise ValueError(""Model ID %d already exists"" % model_id)\n        else:\n            model_id = self.next_id\n            while model_id in self.models.keys():\n                model_id += 1\n            self.next_id = model_id + 1\n        print(""Pre-loading model %d"" % model_id)\n        model = ServerModel(opt, model_id, **model_kwargs)\n        self.models[model_id] = model\n\n        return model_id\n\n    def run(self, inputs):\n        """"""Translate `inputs`\n\n        We keep the same format as the Lua version i.e.\n        ``[{""id"": model_id, ""src"": ""sequence to translate""},{ ...}]``\n\n        We use inputs[0][""id""] as the model id\n        """"""\n\n        model_id = inputs[0].get(""id"", 0)\n        if model_id in self.models and self.models[model_id] is not None:\n            return self.models[model_id].run(inputs)\n        else:\n            print(""Error No such model \'%s\'"" % str(model_id))\n            raise ServerModelError(""No such model \'%s\'"" % str(model_id))\n\n    def unload_model(self, model_id):\n        """"""Manually unload a model.\n\n        It will free the memory and cancel the timer\n        """"""\n\n        if model_id in self.models and self.models[model_id] is not None:\n            self.models[model_id].unload()\n        else:\n            raise ServerModelError(""No such model \'%s\'"" % str(model_id))\n\n    def list_models(self):\n        """"""Return the list of available models\n        """"""\n        models = []\n        for _, model in self.models.items():\n            models += [model.to_dict()]\n        return models\n\n\nclass ServerModel(object):\n    """"""Wrap a model with server functionality.\n\n    Args:\n        opt (dict): Options for the Translator\n        model_id (int): Model ID\n        preprocess_opt (list): Options for preprocess processus or None\n                               (extend for CJK)\n        tokenizer_opt (dict): Options for the tokenizer or None\n        postprocess_opt (list): Options for postprocess processus or None\n                                (extend for CJK)\n        load (bool): whether to load the model during :func:`__init__()`\n        timeout (int): Seconds before running :func:`do_timeout()`\n            Negative values means no timeout\n        on_timeout (str): Options are [""to_cpu"", ""unload""]. Set what to do on\n            timeout (see :func:`do_timeout()`.)\n        model_root (str): Path to the model directory\n            it must contain the model and tokenizer file\n    """"""\n\n    def __init__(self, opt, model_id, preprocess_opt=None, tokenizer_opt=None,\n                 postprocess_opt=None, load=False, timeout=-1,\n                 on_timeout=""to_cpu"", model_root=""./"", ct2_model=None):\n        self.model_root = model_root\n        self.opt = self.parse_opt(opt)\n\n        self.model_id = model_id\n        self.preprocess_opt = preprocess_opt\n        self.tokenizers_opt = tokenizer_opt\n        self.postprocess_opt = postprocess_opt\n        self.timeout = timeout\n        self.on_timeout = on_timeout\n\n        self.ct2_model = os.path.join(model_root, ct2_model) \\\n            if ct2_model is not None else None\n\n        self.unload_timer = None\n        self.user_opt = opt\n        self.tokenizers = None\n\n        if len(self.opt.log_file) > 0:\n            log_file = os.path.join(model_root, self.opt.log_file)\n        else:\n            log_file = None\n        self.logger = init_logger(log_file=log_file,\n                                  log_file_level=self.opt.log_file_level,\n                                  rotate=True)\n\n        self.loading_lock = threading.Event()\n        self.loading_lock.set()\n        self.running_lock = threading.Semaphore(value=1)\n\n        set_random_seed(self.opt.seed, self.opt.cuda)\n\n        if self.preprocess_opt is not None:\n            self.logger.info(""Loading preprocessor"")\n            self.preprocessor = []\n\n            for function_path in self.preprocess_opt:\n                function = get_function_by_path(function_path)\n                self.preprocessor.append(function)\n\n        if self.tokenizers_opt is not None:\n            if ""src"" in self.tokenizers_opt and ""tgt"" in self.tokenizers_opt:\n                self.logger.info(""Loading src & tgt tokenizer"")\n                self.tokenizers = {\n                    \'src\': self.build_tokenizer(tokenizer_opt[\'src\']),\n                    \'tgt\': self.build_tokenizer(tokenizer_opt[\'tgt\'])\n                }\n            else:\n                self.logger.info(""Loading tokenizer"")\n                self.tokenizers_opt = {\n                    \'src\': tokenizer_opt,\n                    \'tgt\': tokenizer_opt\n                }\n                tokenizer = self.build_tokenizer(tokenizer_opt)\n                self.tokenizers = {\n                    \'src\': tokenizer,\n                    \'tgt\': tokenizer\n                }\n\n        if self.postprocess_opt is not None:\n            self.logger.info(""Loading postprocessor"")\n            self.postprocessor = []\n\n            for function_path in self.postprocess_opt:\n                function = get_function_by_path(function_path)\n                self.postprocessor.append(function)\n\n        if load:\n            self.load(preload=True)\n            self.stop_unload_timer()\n\n    def parse_opt(self, opt):\n        """"""Parse the option set passed by the user using `onmt.opts`\n\n       Args:\n           opt (dict): Options passed by the user\n\n       Returns:\n           opt (argparse.Namespace): full set of options for the Translator\n        """"""\n\n        prec_argv = sys.argv\n        sys.argv = sys.argv[:1]\n        parser = ArgumentParser()\n        onmt.opts.translate_opts(parser)\n\n        models = opt[\'models\']\n        if not isinstance(models, (list, tuple)):\n            models = [models]\n        opt[\'models\'] = [os.path.join(self.model_root, model)\n                         for model in models]\n        opt[\'src\'] = ""dummy_src""\n\n        for (k, v) in opt.items():\n            if k == \'models\':\n                sys.argv += [\'-model\']\n                sys.argv += [str(model) for model in v]\n            elif type(v) == bool:\n                sys.argv += [\'-%s\' % k]\n            else:\n                sys.argv += [\'-%s\' % k, str(v)]\n\n        opt = parser.parse_args()\n        ArgumentParser.validate_translate_opts(opt)\n        opt.cuda = opt.gpu > -1\n\n        sys.argv = prec_argv\n        return opt\n\n    @property\n    def loaded(self):\n        return hasattr(self, \'translator\')\n\n    def load(self, preload=False):\n        self.loading_lock.clear()\n\n        timer = Timer()\n        self.logger.info(""Loading model %d"" % self.model_id)\n        timer.start()\n\n        try:\n            if self.ct2_model is not None:\n                self.translator = CTranslate2Translator(\n                    self.ct2_model,\n                    device=""cuda"" if self.opt.cuda else ""cpu"",\n                    device_index=self.opt.gpu if self.opt.cuda else 0,\n                    batch_size=self.opt.batch_size,\n                    beam_size=self.opt.beam_size,\n                    n_best=self.opt.n_best,\n                    preload=preload)\n            else:\n                self.translator = build_translator(\n                    self.opt, report_score=False,\n                    out_file=codecs.open(os.devnull, ""w"", ""utf-8""))\n        except RuntimeError as e:\n            raise ServerModelError(""Runtime Error: %s"" % str(e))\n\n        timer.tick(""model_loading"")\n        self.load_time = timer.tick()\n        self.reset_unload_timer()\n        self.loading_lock.set()\n\n    @critical\n    def run(self, inputs):\n        """"""Translate `inputs` using this model\n\n        Args:\n            inputs (List[dict[str, str]]): [{""src"": ""...""},{""src"": ...}]\n\n        Returns:\n            result (list): translations\n            times (dict): containing times\n        """"""\n\n        self.stop_unload_timer()\n\n        timer = Timer()\n        timer.start()\n\n        self.logger.info(""Running translation using %d"" % self.model_id)\n\n        if not self.loading_lock.is_set():\n            self.logger.info(\n                ""Model #%d is being loaded by another thread, waiting""\n                % self.model_id)\n            if not self.loading_lock.wait(timeout=30):\n                raise ServerModelError(""Model %d loading timeout""\n                                       % self.model_id)\n\n        else:\n            if not self.loaded:\n                self.load()\n                timer.tick(name=""load"")\n            elif self.opt.cuda:\n                self.to_gpu()\n                timer.tick(name=""to_gpu"")\n\n        texts = []\n        head_spaces = []\n        tail_spaces = []\n        all_preprocessed = []\n        for i, inp in enumerate(inputs):\n            src = inp[\'src\']\n            whitespaces_before, whitespaces_after = """", """"\n            match_before = re.search(r\'^\\s+\', src)\n            match_after = re.search(r\'\\s+$\', src)\n            if match_before is not None:\n                whitespaces_before = match_before.group(0)\n            if match_after is not None:\n                whitespaces_after = match_after.group(0)\n            head_spaces.append(whitespaces_before)\n            # every segment becomes a dict for flexibility purposes\n            seg_dict = self.maybe_preprocess(inp)\n            all_preprocessed.append(seg_dict)\n            for seg, ref in zip(seg_dict[""seg""], seg_dict[""ref""]):\n                tok = self.maybe_tokenize(seg)\n                if ref is not None:\n                    ref = self.maybe_tokenize(ref, side=\'tgt\')\n                texts.append((tok, ref))\n            tail_spaces.append(whitespaces_after)\n\n        empty_indices = []\n        texts_to_translate, texts_ref = [], []\n        for i, (tok, ref_tok) in enumerate(texts):\n            if tok == """":\n                empty_indices.append(i)\n            else:\n                texts_to_translate.append(tok)\n                texts_ref.append(ref_tok)\n        if any([item is None for item in texts_ref]):\n            texts_ref = None\n\n        scores = []\n        predictions = []\n        if len(texts_to_translate) > 0:\n            try:\n                scores, predictions = self.translator.translate(\n                    texts_to_translate,\n                    tgt=texts_ref,\n                    batch_size=len(texts_to_translate)\n                    if self.opt.batch_size == 0\n                    else self.opt.batch_size)\n            except (RuntimeError, Exception) as e:\n                err = ""Error: %s"" % str(e)\n                self.logger.error(err)\n                self.logger.error(""repr(text_to_translate): ""\n                                  + repr(texts_to_translate))\n                self.logger.error(""model: #%s"" % self.model_id)\n                self.logger.error(""model opt: "" + str(self.opt.__dict__))\n                self.logger.error(traceback.format_exc())\n\n                raise ServerModelError(err)\n\n        timer.tick(name=""translation"")\n        self.logger.info(""""""Using model #%d\\t%d inputs\n               \\ttranslation time: %f"""""" % (self.model_id, len(texts),\n                                            timer.times[\'translation\']))\n        self.reset_unload_timer()\n\n        # NOTE: translator returns lists of `n_best` list\n        def flatten_list(_list): return sum(_list, [])\n        tiled_texts = [t for t in texts_to_translate\n                       for _ in range(self.opt.n_best)]\n        results = flatten_list(predictions)\n\n        def maybe_item(x): return x.item() if type(x) is torch.Tensor else x\n        scores = [maybe_item(score_tensor)\n                  for score_tensor in flatten_list(scores)]\n\n        results = [self.maybe_detokenize_with_align(result, src)\n                   for result, src in zip(results, tiled_texts)]\n\n        aligns = [align for _, align in results]\n        results = [tokens for tokens, _ in results]\n\n        # build back results with empty texts\n        for i in empty_indices:\n            j = i * self.opt.n_best\n            results = results[:j] + [""""] * self.opt.n_best + results[j:]\n            aligns = aligns[:j] + [None] * self.opt.n_best + aligns[j:]\n            scores = scores[:j] + [0] * self.opt.n_best + scores[j:]\n\n        rebuilt_segs, scores, aligns = self.rebuild_seg_packages(\n            all_preprocessed, results, scores, aligns, self.opt.n_best)\n\n        results = [self.maybe_postprocess(seg) for seg in rebuilt_segs]\n\n        head_spaces = [h for h in head_spaces for i in range(self.opt.n_best)]\n        tail_spaces = [h for h in tail_spaces for i in range(self.opt.n_best)]\n        results = ["""".join(items)\n                   for items in zip(head_spaces, results, tail_spaces)]\n\n        self.logger.info(""Translation Results: %d"", len(results))\n\n        return results, scores, self.opt.n_best, timer.times, aligns\n\n    def rebuild_seg_packages(self, all_preprocessed, results,\n                             scores, aligns, n_best):\n        """"""\n        Rebuild proper segment packages based on initial n_seg.\n        """"""\n        offset = 0\n        rebuilt_segs = []\n        avg_scores = []\n        merged_aligns = []\n        for i, seg_dict in enumerate(all_preprocessed):\n            n_seg = seg_dict[""n_seg""]\n            sub_results = results[n_best * offset: (offset + n_seg) * n_best]\n            sub_scores = scores[n_best * offset: (offset + n_seg) * n_best]\n            sub_aligns = aligns[n_best * offset: (offset + n_seg) * n_best]\n            for j in range(n_best):\n                _seg_dict = deepcopy(seg_dict)\n                _seg_dict[""seg""] = list(islice(sub_results, j, None, n_best))\n                rebuilt_segs.append(_seg_dict)\n                sub_sub_scores = list(islice(sub_scores, j, None, n_best))\n                avg_score = sum(sub_sub_scores)/n_seg if n_seg != 0 else 0\n                avg_scores.append(avg_score)\n                sub_sub_aligns = list(islice(sub_aligns, j, None, n_best))\n                merged_aligns.append(sub_sub_aligns)\n            offset += n_seg\n        return rebuilt_segs, avg_scores, merged_aligns\n\n    def do_timeout(self):\n        """"""Timeout function that frees GPU memory.\n\n        Moves the model to CPU or unloads it; depending on\n        attr`self.on_timemout` value\n        """"""\n\n        if self.on_timeout == ""unload"":\n            self.logger.info(""Timeout: unloading model %d"" % self.model_id)\n            self.unload()\n        if self.on_timeout == ""to_cpu"":\n            self.logger.info(""Timeout: sending model %d to CPU""\n                             % self.model_id)\n            self.to_cpu()\n\n    @critical\n    def unload(self):\n        self.logger.info(""Unloading model %d"" % self.model_id)\n        del self.translator\n        if self.opt.cuda:\n            torch.cuda.empty_cache()\n        self.stop_unload_timer()\n        self.unload_timer = None\n\n    def stop_unload_timer(self):\n        if self.unload_timer is not None:\n            self.unload_timer.cancel()\n\n    def reset_unload_timer(self):\n        if self.timeout < 0:\n            return\n\n        self.stop_unload_timer()\n        self.unload_timer = threading.Timer(self.timeout, self.do_timeout)\n        self.unload_timer.start()\n\n    def to_dict(self):\n        hide_opt = [""models"", ""src""]\n        d = {""model_id"": self.model_id,\n             ""opt"": {k: self.user_opt[k] for k in self.user_opt.keys()\n                     if k not in hide_opt},\n             ""models"": self.user_opt[""models""],\n             ""loaded"": self.loaded,\n             ""timeout"": self.timeout,\n             }\n        if self.tokenizers_opt is not None:\n            d[""tokenizer""] = self.tokenizers_opt\n        return d\n\n    @critical\n    def to_cpu(self):\n        """"""Move the model to CPU and clear CUDA cache.""""""\n        if type(self.translator) == CTranslate2Translator:\n            self.translator.to_cpu()\n        else:\n            self.translator.model.cpu()\n            if self.opt.cuda:\n                torch.cuda.empty_cache()\n\n    def to_gpu(self):\n        """"""Move the model to GPU.""""""\n        if type(self.translator) == CTranslate2Translator:\n            self.translator.to_gpu()\n        else:\n            torch.cuda.set_device(self.opt.gpu)\n            self.translator.model.cuda()\n\n    def maybe_preprocess(self, sequence):\n        """"""Preprocess the sequence (or not)\n\n        """"""\n        if sequence.get(""src"", None) is not None:\n            sequence = deepcopy(sequence)\n            sequence[""seg""] = [sequence[""src""].strip()]\n            sequence.pop(""src"")\n            sequence[""ref""] = [sequence.get(\'ref\', None)]\n            sequence[""n_seg""] = 1\n        if self.preprocess_opt is not None:\n            return self.preprocess(sequence)\n        return sequence\n\n    def preprocess(self, sequence):\n        """"""Preprocess a single sequence.\n\n        Args:\n            sequence (str): The sequence to preprocess.\n\n        Returns:\n            sequence (str): The preprocessed sequence.\n        """"""\n        if self.preprocessor is None:\n            raise ValueError(""No preprocessor loaded"")\n        for function in self.preprocessor:\n            sequence = function(sequence, self)\n        return sequence\n\n    def build_tokenizer(self, tokenizer_opt):\n        """"""Build tokenizer described by `tokenizer_opt`.""""""\n        if ""type"" not in tokenizer_opt:\n            raise ValueError(\n                ""Missing mandatory tokenizer option \'type\'"")\n\n        if tokenizer_opt[\'type\'] == \'sentencepiece\':\n            if ""model"" not in tokenizer_opt:\n                raise ValueError(\n                    ""Missing mandatory tokenizer option \'model\'"")\n            import sentencepiece as spm\n            tokenizer = spm.SentencePieceProcessor()\n            model_path = os.path.join(self.model_root,\n                                      tokenizer_opt[\'model\'])\n            tokenizer.Load(model_path)\n        elif tokenizer_opt[\'type\'] == \'pyonmttok\':\n            if ""params"" not in tokenizer_opt:\n                raise ValueError(\n                    ""Missing mandatory tokenizer option \'params\'"")\n            import pyonmttok\n            if tokenizer_opt[""mode""] is not None:\n                mode = tokenizer_opt[""mode""]\n            else:\n                mode = None\n            # load can be called multiple times: modify copy\n            tokenizer_params = dict(tokenizer_opt[""params""])\n            for key, value in tokenizer_opt[""params""].items():\n                if key.endswith(""path""):\n                    tokenizer_params[key] = os.path.join(\n                        self.model_root, value)\n            tokenizer = pyonmttok.Tokenizer(mode,\n                                            **tokenizer_params)\n        else:\n            raise ValueError(""Invalid value for tokenizer type"")\n        return tokenizer\n\n    def maybe_tokenize(self, sequence, side=\'src\'):\n        """"""Tokenize the sequence (or not).\n\n        Same args/returns as `tokenize`\n        """"""\n\n        if self.tokenizers_opt is not None:\n            return self.tokenize(sequence, side)\n        return sequence\n\n    def tokenize(self, sequence, side=\'src\'):\n        """"""Tokenize a single sequence.\n\n        Args:\n            sequence (str): The sequence to tokenize.\n\n        Returns:\n            tok (str): The tokenized sequence.\n        """"""\n\n        if self.tokenizers is None:\n            raise ValueError(""No tokenizer loaded"")\n\n        if self.tokenizers_opt[side][""type""] == ""sentencepiece"":\n            tok = self.tokenizers[side].EncodeAsPieces(sequence)\n            tok = "" "".join(tok)\n        elif self.tokenizers_opt[side][""type""] == ""pyonmttok"":\n            tok, _ = self.tokenizers[side].tokenize(sequence)\n            tok = "" "".join(tok)\n        return tok\n\n    def tokenizer_marker(self, side=\'src\'):\n        """"""Return marker used in `side` tokenizer.""""""\n        marker = None\n        if self.tokenizers_opt is not None:\n            tokenizer_type = self.tokenizers_opt[side].get(\'type\', None)\n            if tokenizer_type == ""pyonmttok"":\n                params = self.tokenizers_opt[side].get(\'params\', None)\n                if params is not None:\n                    if params.get(""joiner_annotate"", None) is not None:\n                        marker = \'joiner\'\n                    elif params.get(""spacer_annotate"", None) is not None:\n                        marker = \'spacer\'\n            elif tokenizer_type == ""sentencepiece"":\n                marker = \'spacer\'\n        return marker\n\n    def maybe_detokenize_with_align(self, sequence, src, side=\'tgt\'):\n        """"""De-tokenize (or not) the sequence (with alignment).\n\n        Args:\n            sequence (str): The sequence to detokenize, possible with\n                alignment seperate by ` ||| `.\n\n        Returns:\n            sequence (str): The detokenized sequence.\n            align (str): The alignment correspand to detokenized src/tgt\n                sorted or None if no alignment in output.\n        """"""\n        align = None\n        if self.opt.report_align:\n            # output contain alignment\n            sequence, align = sequence.split(\' ||| \')\n            if align != \'\':\n                align = self.maybe_convert_align(src, sequence, align)\n        sequence = self.maybe_detokenize(sequence, side)\n        return (sequence, align)\n\n    def maybe_detokenize(self, sequence, side=\'tgt\'):\n        """"""De-tokenize the sequence (or not)\n\n        Same args/returns as :func:`tokenize()`\n        """"""\n\n        if self.tokenizers_opt is not None and \'\'.join(sequence.split()) != \'\':\n            return self.detokenize(sequence, side)\n        return sequence\n\n    def detokenize(self, sequence, side=\'tgt\'):\n        """"""Detokenize a single sequence\n\n        Same args/returns as :func:`tokenize()`\n        """"""\n\n        if self.tokenizers is None:\n            raise ValueError(""No tokenizer loaded"")\n\n        if self.tokenizers_opt[side][""type""] == ""sentencepiece"":\n            detok = self.tokenizers[side].DecodePieces(sequence.split())\n        elif self.tokenizers_opt[side][""type""] == ""pyonmttok"":\n            detok = self.tokenizers[side].detokenize(sequence.split())\n\n        return detok\n\n    def maybe_convert_align(self, src, tgt, align):\n        """"""Convert alignment to match detokenized src/tgt (or not).\n\n        Args:\n            src (str): The tokenized source sequence.\n            tgt (str): The tokenized target sequence.\n            align (str): The alignment correspand to src/tgt pair.\n\n        Returns:\n            align (str): The alignment correspand to detokenized src/tgt.\n        """"""\n        if self.tokenizers_opt is not None:\n            src_marker = self.tokenizer_marker(side=\'src\')\n            tgt_marker = self.tokenizer_marker(side=\'tgt\')\n            if src_marker is None or tgt_marker is None:\n                raise ValueError(""To get decoded alignment, joiner/spacer ""\n                                 ""should be used in both side\'s tokenizer."")\n            elif \'\'.join(tgt.split()) != \'\':\n                align = to_word_align(src, tgt, align, src_marker, tgt_marker)\n        return align\n\n    def maybe_postprocess(self, sequence):\n        """"""Postprocess the sequence (or not)\n\n        """"""\n        if self.postprocess_opt is not None:\n            return self.postprocess(sequence)\n        else:\n            return sequence[""seg""][0]\n\n    def postprocess(self, sequence):\n        """"""Preprocess a single sequence.\n\n        Args:\n            sequence (str): The sequence to process.\n\n        Returns:\n            sequence (str): The postprocessed sequence.\n        """"""\n        if self.postprocessor is None:\n            raise ValueError(""No postprocessor loaded"")\n        for function in self.postprocessor:\n            sequence = function(sequence, self)\n        return sequence\n\n\ndef get_function_by_path(path, args=[], kwargs={}):\n    module_name = ""."".join(path.split(""."")[:-1])\n    function_name = path.split(""."")[-1]\n    try:\n        module = importlib.import_module(module_name)\n    except ValueError as e:\n        print(""Cannot import module \'%s\'"" % module_name)\n        raise e\n    function = getattr(module, function_name)\n    return function\n'"
onmt/translate/translator.py,8,"b'#!/usr/bin/env python\n"""""" Translator Class and builder """"""\nfrom __future__ import print_function\nimport codecs\nimport os\nimport time\nimport numpy as np\nfrom itertools import count, zip_longest\n\nimport torch\n\nimport onmt.model_builder\nimport onmt.inputters as inputters\nimport onmt.decoders.ensemble\nfrom onmt.translate.beam_search import BeamSearch\nfrom onmt.translate.greedy_search import GreedySearch\nfrom onmt.utils.misc import tile, set_random_seed, report_matrix\nfrom onmt.utils.alignment import extract_alignment, build_align_pharaoh\nfrom onmt.modules.copy_generator import collapse_copy_scores\n\n\ndef build_translator(opt, report_score=True, logger=None, out_file=None):\n    if out_file is None:\n        out_file = codecs.open(opt.output, \'w+\', \'utf-8\')\n\n    load_test_model = onmt.decoders.ensemble.load_test_model \\\n        if len(opt.models) > 1 else onmt.model_builder.load_test_model\n    fields, model, model_opt = load_test_model(opt)\n\n    scorer = onmt.translate.GNMTGlobalScorer.from_opt(opt)\n\n    translator = Translator.from_opt(\n        model,\n        fields,\n        opt,\n        model_opt,\n        global_scorer=scorer,\n        out_file=out_file,\n        report_align=opt.report_align,\n        report_score=report_score,\n        logger=logger\n    )\n    return translator\n\n\ndef max_tok_len(new, count, sofar):\n    """"""\n    In token batching scheme, the number of sequences is limited\n    such that the total number of src/tgt tokens (including padding)\n    in a batch <= batch_size\n    """"""\n    # Maintains the longest src and tgt length in the current batch\n    global max_src_in_batch  # this is a hack\n    # Reset current longest length at a new batch (count=1)\n    if count == 1:\n        max_src_in_batch = 0\n        # max_tgt_in_batch = 0\n    # Src: [<bos> w1 ... wN <eos>]\n    max_src_in_batch = max(max_src_in_batch, len(new.src[0]) + 2)\n    # Tgt: [w1 ... wM <eos>]\n    src_elements = count * max_src_in_batch\n    return src_elements\n\n\nclass Translator(object):\n    """"""Translate a batch of sentences with a saved model.\n\n    Args:\n        model (onmt.modules.NMTModel): NMT model to use for translation\n        fields (dict[str, torchtext.data.Field]): A dict\n            mapping each side to its list of name-Field pairs.\n        src_reader (onmt.inputters.DataReaderBase): Source reader.\n        tgt_reader (onmt.inputters.TextDataReader): Target reader.\n        gpu (int): GPU device. Set to negative for no GPU.\n        n_best (int): How many beams to wait for.\n        min_length (int): See\n            :class:`onmt.translate.decode_strategy.DecodeStrategy`.\n        max_length (int): See\n            :class:`onmt.translate.decode_strategy.DecodeStrategy`.\n        beam_size (int): Number of beams.\n        random_sampling_topk (int): See\n            :class:`onmt.translate.greedy_search.GreedySearch`.\n        random_sampling_temp (int): See\n            :class:`onmt.translate.greedy_search.GreedySearch`.\n        stepwise_penalty (bool): Whether coverage penalty is applied every step\n            or not.\n        dump_beam (bool): Debugging option.\n        block_ngram_repeat (int): See\n            :class:`onmt.translate.decode_strategy.DecodeStrategy`.\n        ignore_when_blocking (set or frozenset): See\n            :class:`onmt.translate.decode_strategy.DecodeStrategy`.\n        replace_unk (bool): Replace unknown token.\n        tgt_prefix (bool): Force the predictions begin with provided -tgt.\n        data_type (str): Source data type.\n        verbose (bool): Print/log every translation.\n        report_time (bool): Print/log total time/frequency.\n        copy_attn (bool): Use copy attention.\n        global_scorer (onmt.translate.GNMTGlobalScorer): Translation\n            scoring/reranking object.\n        out_file (TextIO or codecs.StreamReaderWriter): Output file.\n        report_score (bool) : Whether to report scores\n        logger (logging.Logger or NoneType): Logger.\n    """"""\n\n    def __init__(\n            self,\n            model,\n            fields,\n            src_reader,\n            tgt_reader,\n            gpu=-1,\n            n_best=1,\n            min_length=0,\n            max_length=100,\n            ratio=0.,\n            beam_size=30,\n            random_sampling_topk=1,\n            random_sampling_temp=1,\n            stepwise_penalty=None,\n            dump_beam=False,\n            block_ngram_repeat=0,\n            ignore_when_blocking=frozenset(),\n            replace_unk=False,\n            tgt_prefix=False,\n            phrase_table="""",\n            data_type=""text"",\n            verbose=False,\n            report_time=False,\n            copy_attn=False,\n            global_scorer=None,\n            out_file=None,\n            report_align=False,\n            report_score=True,\n            logger=None,\n            seed=-1):\n        self.model = model\n        self.fields = fields\n        tgt_field = dict(self.fields)[""tgt""].base_field\n        self._tgt_vocab = tgt_field.vocab\n        self._tgt_eos_idx = self._tgt_vocab.stoi[tgt_field.eos_token]\n        self._tgt_pad_idx = self._tgt_vocab.stoi[tgt_field.pad_token]\n        self._tgt_bos_idx = self._tgt_vocab.stoi[tgt_field.init_token]\n        self._tgt_unk_idx = self._tgt_vocab.stoi[tgt_field.unk_token]\n        self._tgt_vocab_len = len(self._tgt_vocab)\n\n        self._gpu = gpu\n        self._use_cuda = gpu > -1\n        self._dev = torch.device(""cuda"", self._gpu) \\\n            if self._use_cuda else torch.device(""cpu"")\n\n        self.n_best = n_best\n        self.max_length = max_length\n\n        self.beam_size = beam_size\n        self.random_sampling_temp = random_sampling_temp\n        self.sample_from_topk = random_sampling_topk\n\n        self.min_length = min_length\n        self.ratio = ratio\n        self.stepwise_penalty = stepwise_penalty\n        self.dump_beam = dump_beam\n        self.block_ngram_repeat = block_ngram_repeat\n        self.ignore_when_blocking = ignore_when_blocking\n        self._exclusion_idxs = {\n            self._tgt_vocab.stoi[t] for t in self.ignore_when_blocking}\n        self.src_reader = src_reader\n        self.tgt_reader = tgt_reader\n        self.replace_unk = replace_unk\n        if self.replace_unk and not self.model.decoder.attentional:\n            raise ValueError(\n                ""replace_unk requires an attentional decoder."")\n        self.tgt_prefix = tgt_prefix\n        self.phrase_table = phrase_table\n        self.data_type = data_type\n        self.verbose = verbose\n        self.report_time = report_time\n\n        self.copy_attn = copy_attn\n\n        self.global_scorer = global_scorer\n        if self.global_scorer.has_cov_pen and \\\n                not self.model.decoder.attentional:\n            raise ValueError(\n                ""Coverage penalty requires an attentional decoder."")\n        self.out_file = out_file\n        self.report_align = report_align\n        self.report_score = report_score\n        self.logger = logger\n\n        self.use_filter_pred = False\n        self._filter_pred = None\n\n        # for debugging\n        self.beam_trace = self.dump_beam != """"\n        self.beam_accum = None\n        if self.beam_trace:\n            self.beam_accum = {\n                ""predicted_ids"": [],\n                ""beam_parent_ids"": [],\n                ""scores"": [],\n                ""log_probs"": []}\n\n        set_random_seed(seed, self._use_cuda)\n\n    @classmethod\n    def from_opt(\n            cls,\n            model,\n            fields,\n            opt,\n            model_opt,\n            global_scorer=None,\n            out_file=None,\n            report_align=False,\n            report_score=True,\n            logger=None):\n        """"""Alternate constructor.\n\n        Args:\n            model (onmt.modules.NMTModel): See :func:`__init__()`.\n            fields (dict[str, torchtext.data.Field]): See\n                :func:`__init__()`.\n            opt (argparse.Namespace): Command line options\n            model_opt (argparse.Namespace): Command line options saved with\n                the model checkpoint.\n            global_scorer (onmt.translate.GNMTGlobalScorer): See\n                :func:`__init__()`..\n            out_file (TextIO or codecs.StreamReaderWriter): See\n                :func:`__init__()`.\n            report_align (bool) : See :func:`__init__()`.\n            report_score (bool) : See :func:`__init__()`.\n            logger (logging.Logger or NoneType): See :func:`__init__()`.\n        """"""\n\n        src_reader = inputters.str2reader[opt.data_type].from_opt(opt)\n        tgt_reader = inputters.str2reader[""text""].from_opt(opt)\n        return cls(\n            model,\n            fields,\n            src_reader,\n            tgt_reader,\n            gpu=opt.gpu,\n            n_best=opt.n_best,\n            min_length=opt.min_length,\n            max_length=opt.max_length,\n            ratio=opt.ratio,\n            beam_size=opt.beam_size,\n            random_sampling_topk=opt.random_sampling_topk,\n            random_sampling_temp=opt.random_sampling_temp,\n            stepwise_penalty=opt.stepwise_penalty,\n            dump_beam=opt.dump_beam,\n            block_ngram_repeat=opt.block_ngram_repeat,\n            ignore_when_blocking=set(opt.ignore_when_blocking),\n            replace_unk=opt.replace_unk,\n            tgt_prefix=opt.tgt_prefix,\n            phrase_table=opt.phrase_table,\n            data_type=opt.data_type,\n            verbose=opt.verbose,\n            report_time=opt.report_time,\n            copy_attn=model_opt.copy_attn,\n            global_scorer=global_scorer,\n            out_file=out_file,\n            report_align=report_align,\n            report_score=report_score,\n            logger=logger,\n            seed=opt.seed)\n\n    def _log(self, msg):\n        if self.logger:\n            self.logger.info(msg)\n        else:\n            print(msg)\n\n    def _gold_score(self, batch, memory_bank, src_lengths, src_vocabs,\n                    use_src_map, enc_states, batch_size, src):\n        if ""tgt"" in batch.__dict__:\n            gs = self._score_target(\n                batch, memory_bank, src_lengths, src_vocabs,\n                batch.src_map if use_src_map else None)\n            self.model.decoder.init_state(src, memory_bank, enc_states)\n        else:\n            gs = [0] * batch_size\n        return gs\n\n    def translate(\n            self,\n            src,\n            tgt=None,\n            src_dir=None,\n            batch_size=None,\n            batch_type=""sents"",\n            attn_debug=False,\n            align_debug=False,\n            phrase_table=""""):\n        """"""Translate content of ``src`` and get gold scores from ``tgt``.\n\n        Args:\n            src: See :func:`self.src_reader.read()`.\n            tgt: See :func:`self.tgt_reader.read()`.\n            src_dir: See :func:`self.src_reader.read()` (only relevant\n                for certain types of data).\n            batch_size (int): size of examples per mini-batch\n            attn_debug (bool): enables the attention logging\n            align_debug (bool): enables the word alignment logging\n\n        Returns:\n            (`list`, `list`)\n\n            * all_scores is a list of `batch_size` lists of `n_best` scores\n            * all_predictions is a list of `batch_size` lists\n                of `n_best` predictions\n        """"""\n\n        if batch_size is None:\n            raise ValueError(""batch_size must be set"")\n\n        if self.tgt_prefix and tgt is None:\n            raise ValueError(\'Prefix should be feed to tgt if -tgt_prefix.\')\n\n        src_data = {""reader"": self.src_reader, ""data"": src, ""dir"": src_dir}\n        tgt_data = {""reader"": self.tgt_reader, ""data"": tgt, ""dir"": None}\n        _readers, _data, _dir = inputters.Dataset.config(\n            [(\'src\', src_data), (\'tgt\', tgt_data)])\n\n        # corpus_id field is useless here\n        if self.fields.get(""corpus_id"", None) is not None:\n            self.fields.pop(\'corpus_id\')\n        data = inputters.Dataset(\n            self.fields, readers=_readers, data=_data, dirs=_dir,\n            sort_key=inputters.str2sortkey[self.data_type],\n            filter_pred=self._filter_pred\n        )\n\n        data_iter = inputters.OrderedIterator(\n            dataset=data,\n            device=self._dev,\n            batch_size=batch_size,\n            batch_size_fn=max_tok_len if batch_type == ""tokens"" else None,\n            train=False,\n            sort=False,\n            sort_within_batch=True,\n            shuffle=False\n        )\n\n        xlation_builder = onmt.translate.TranslationBuilder(\n            data, self.fields, self.n_best, self.replace_unk, tgt,\n            self.phrase_table\n        )\n\n        # Statistics\n        counter = count(1)\n        pred_score_total, pred_words_total = 0, 0\n        gold_score_total, gold_words_total = 0, 0\n\n        all_scores = []\n        all_predictions = []\n\n        start_time = time.time()\n\n        for batch in data_iter:\n            batch_data = self.translate_batch(\n                batch, data.src_vocabs, attn_debug\n            )\n            translations = xlation_builder.from_batch(batch_data)\n\n            for trans in translations:\n                all_scores += [trans.pred_scores[:self.n_best]]\n                pred_score_total += trans.pred_scores[0]\n                pred_words_total += len(trans.pred_sents[0])\n                if tgt is not None:\n                    gold_score_total += trans.gold_score\n                    gold_words_total += len(trans.gold_sent) + 1\n\n                n_best_preds = ["" "".join(pred)\n                                for pred in trans.pred_sents[:self.n_best]]\n                if self.report_align:\n                    align_pharaohs = [build_align_pharaoh(align) for align\n                                      in trans.word_aligns[:self.n_best]]\n                    n_best_preds_align = ["" "".join(align) for align\n                                          in align_pharaohs]\n                    n_best_preds = [pred + "" ||| "" + align\n                                    for pred, align in zip(\n                                        n_best_preds, n_best_preds_align)]\n                all_predictions += [n_best_preds]\n                self.out_file.write(\'\\n\'.join(n_best_preds) + \'\\n\')\n                self.out_file.flush()\n\n                if self.verbose:\n                    sent_number = next(counter)\n                    output = trans.log(sent_number)\n                    if self.logger:\n                        self.logger.info(output)\n                    else:\n                        os.write(1, output.encode(\'utf-8\'))\n\n                if attn_debug:\n                    preds = trans.pred_sents[0]\n                    preds.append(\'</s>\')\n                    attns = trans.attns[0].tolist()\n                    if self.data_type == \'text\':\n                        srcs = trans.src_raw\n                    else:\n                        srcs = [str(item) for item in range(len(attns[0]))]\n                    output = report_matrix(srcs, preds, attns)\n                    if self.logger:\n                        self.logger.info(output)\n                    else:\n                        os.write(1, output.encode(\'utf-8\'))\n\n                if align_debug:\n                    tgts = trans.pred_sents[0]\n                    align = trans.word_aligns[0].tolist()\n                    if self.data_type == \'text\':\n                        srcs = trans.src_raw\n                    else:\n                        srcs = [str(item) for item in range(len(align[0]))]\n                    output = report_matrix(srcs, tgts, align)\n                    if self.logger:\n                        self.logger.info(output)\n                    else:\n                        os.write(1, output.encode(\'utf-8\'))\n\n        end_time = time.time()\n\n        if self.report_score:\n            msg = self._report_score(\'PRED\', pred_score_total,\n                                     pred_words_total)\n            self._log(msg)\n            if tgt is not None:\n                msg = self._report_score(\'GOLD\', gold_score_total,\n                                         gold_words_total)\n                self._log(msg)\n\n        if self.report_time:\n            total_time = end_time - start_time\n            self._log(""Total translation time (s): %f"" % total_time)\n            self._log(""Average translation time (s): %f"" % (\n                total_time / len(all_predictions)))\n            self._log(""Tokens per second: %f"" % (\n                pred_words_total / total_time))\n\n        if self.dump_beam:\n            import json\n            json.dump(self.translator.beam_accum,\n                      codecs.open(self.dump_beam, \'w\', \'utf-8\'))\n        return all_scores, all_predictions\n\n    def _align_pad_prediction(self, predictions, bos, pad):\n        """"""\n        Padding predictions in batch and add BOS.\n\n        Args:\n            predictions (List[List[Tensor]]): `(batch, n_best,)`, for each src\n                sequence contain n_best tgt predictions all of which ended with\n                eos id.\n            bos (int): bos index to be used.\n            pad (int): pad index to be used.\n\n        Return:\n            batched_nbest_predict (torch.LongTensor): `(batch, n_best, tgt_l)`\n        """"""\n        dtype, device = predictions[0][0].dtype, predictions[0][0].device\n        flatten_tgt = [best.tolist() for bests in predictions\n                       for best in bests]\n        paded_tgt = torch.tensor(\n            list(zip_longest(*flatten_tgt, fillvalue=pad)),\n            dtype=dtype, device=device).T\n        bos_tensor = torch.full([paded_tgt.size(0), 1], bos,\n                                dtype=dtype, device=device)\n        full_tgt = torch.cat((bos_tensor, paded_tgt), dim=-1)\n        batched_nbest_predict = full_tgt.view(\n            len(predictions), -1, full_tgt.size(-1))  # (batch, n_best, tgt_l)\n        return batched_nbest_predict\n\n    def _align_forward(self, batch, predictions):\n        """"""\n        For a batch of input and its prediction, return a list of batch predict\n        alignment src indice Tensor in size ``(batch, n_best,)``.\n        """"""\n        # (0) add BOS and padding to tgt prediction\n        batch_tgt_idxs = self._align_pad_prediction(\n            predictions, bos=self._tgt_bos_idx, pad=self._tgt_pad_idx)\n        tgt_mask = (batch_tgt_idxs.eq(self._tgt_pad_idx) |\n                    batch_tgt_idxs.eq(self._tgt_eos_idx) |\n                    batch_tgt_idxs.eq(self._tgt_bos_idx))\n\n        n_best = batch_tgt_idxs.size(1)\n        # (1) Encoder forward.\n        src, enc_states, memory_bank, src_lengths = self._run_encoder(batch)\n\n        # (2) Repeat src objects `n_best` times.\n        # We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``\n        src = tile(src, n_best, dim=1)\n        enc_states = tile(enc_states, n_best, dim=1)\n        if isinstance(memory_bank, tuple):\n            memory_bank = tuple(tile(x, n_best, dim=1) for x in memory_bank)\n        else:\n            memory_bank = tile(memory_bank, n_best, dim=1)\n        src_lengths = tile(src_lengths, n_best)  # ``(batch * n_best,)``\n\n        # (3) Init decoder with n_best src,\n        self.model.decoder.init_state(src, memory_bank, enc_states)\n        # reshape tgt to ``(len, batch * n_best, nfeat)``\n        tgt = batch_tgt_idxs.view(-1, batch_tgt_idxs.size(-1)).T.unsqueeze(-1)\n        dec_in = tgt[:-1]  # exclude last target from inputs\n        _, attns = self.model.decoder(\n            dec_in, memory_bank, memory_lengths=src_lengths, with_align=True)\n\n        alignment_attn = attns[""align""]  # ``(B, tgt_len-1, src_len)``\n        # masked_select\n        align_tgt_mask = tgt_mask.view(-1, tgt_mask.size(-1))\n        prediction_mask = align_tgt_mask[:, 1:]  # exclude bos to match pred\n        # get aligned src id for each prediction\'s valid tgt tokens\n        alignement = extract_alignment(\n            alignment_attn, prediction_mask, src_lengths, n_best)\n        return alignement\n\n    def translate_batch(self, batch, src_vocabs, attn_debug):\n        """"""Translate a batch of sentences.""""""\n        with torch.no_grad():\n            if self.beam_size == 1:\n                decode_strategy = GreedySearch(\n                    pad=self._tgt_pad_idx,\n                    bos=self._tgt_bos_idx,\n                    eos=self._tgt_eos_idx,\n                    batch_size=batch.batch_size,\n                    min_length=self.min_length, max_length=self.max_length,\n                    block_ngram_repeat=self.block_ngram_repeat,\n                    exclusion_tokens=self._exclusion_idxs,\n                    return_attention=attn_debug or self.replace_unk,\n                    sampling_temp=self.random_sampling_temp,\n                    keep_topk=self.sample_from_topk)\n            else:\n                # TODO: support these blacklisted features\n                assert not self.dump_beam\n                decode_strategy = BeamSearch(\n                    self.beam_size,\n                    batch_size=batch.batch_size,\n                    pad=self._tgt_pad_idx,\n                    bos=self._tgt_bos_idx,\n                    eos=self._tgt_eos_idx,\n                    n_best=self.n_best,\n                    global_scorer=self.global_scorer,\n                    min_length=self.min_length, max_length=self.max_length,\n                    return_attention=attn_debug or self.replace_unk,\n                    block_ngram_repeat=self.block_ngram_repeat,\n                    exclusion_tokens=self._exclusion_idxs,\n                    stepwise_penalty=self.stepwise_penalty,\n                    ratio=self.ratio)\n            return self._translate_batch_with_strategy(batch, src_vocabs,\n                                                       decode_strategy)\n\n    def _run_encoder(self, batch):\n        src, src_lengths = batch.src if isinstance(batch.src, tuple) \\\n                           else (batch.src, None)\n\n        enc_states, memory_bank, src_lengths = self.model.encoder(\n            src, src_lengths)\n        if src_lengths is None:\n            assert not isinstance(memory_bank, tuple), \\\n                \'Ensemble decoding only supported for text data\'\n            src_lengths = torch.Tensor(batch.batch_size) \\\n                               .type_as(memory_bank) \\\n                               .long() \\\n                               .fill_(memory_bank.size(0))\n        return src, enc_states, memory_bank, src_lengths\n\n    def _decode_and_generate(\n            self,\n            decoder_in,\n            memory_bank,\n            batch,\n            src_vocabs,\n            memory_lengths,\n            src_map=None,\n            step=None,\n            batch_offset=None):\n        if self.copy_attn:\n            # Turn any copied words into UNKs.\n            decoder_in = decoder_in.masked_fill(\n                decoder_in.gt(self._tgt_vocab_len - 1), self._tgt_unk_idx\n            )\n\n        # Decoder forward, takes [tgt_len, batch, nfeats] as input\n        # and [src_len, batch, hidden] as memory_bank\n        # in case of inference tgt_len = 1, batch = beam times batch_size\n        # in case of Gold Scoring tgt_len = actual length, batch = 1 batch\n        dec_out, dec_attn = self.model.decoder(\n            decoder_in, memory_bank, memory_lengths=memory_lengths, step=step\n        )\n\n        # Generator forward.\n        if not self.copy_attn:\n            if ""std"" in dec_attn:\n                attn = dec_attn[""std""]\n            else:\n                attn = None\n            log_probs = self.model.generator(dec_out.squeeze(0))\n            # returns [(batch_size x beam_size) , vocab ] when 1 step\n            # or [ tgt_len, batch_size, vocab ] when full sentence\n        else:\n            attn = dec_attn[""copy""]\n            scores = self.model.generator(dec_out.view(-1, dec_out.size(2)),\n                                          attn.view(-1, attn.size(2)),\n                                          src_map)\n            # here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]\n            if batch_offset is None:\n                scores = scores.view(-1, batch.batch_size, scores.size(-1))\n                scores = scores.transpose(0, 1).contiguous()\n            else:\n                scores = scores.view(-1, self.beam_size, scores.size(-1))\n            scores = collapse_copy_scores(\n                scores,\n                batch,\n                self._tgt_vocab,\n                src_vocabs,\n                batch_dim=0,\n                batch_offset=batch_offset\n            )\n            scores = scores.view(decoder_in.size(0), -1, scores.size(-1))\n            log_probs = scores.squeeze(0).log()\n            # returns [(batch_size x beam_size) , vocab ] when 1 step\n            # or [ tgt_len, batch_size, vocab ] when full sentence\n        return log_probs, attn\n\n    def _translate_batch_with_strategy(\n            self,\n            batch,\n            src_vocabs,\n            decode_strategy):\n        """"""Translate a batch of sentences step by step using cache.\n\n        Args:\n            batch: a batch of sentences, yield by data iterator.\n            src_vocabs (list): list of torchtext.data.Vocab if can_copy.\n            decode_strategy (DecodeStrategy): A decode strategy to use for\n                generate translation step by step.\n\n        Returns:\n            results (dict): The translation results.\n        """"""\n        # (0) Prep the components of the search.\n        use_src_map = self.copy_attn\n        parallel_paths = decode_strategy.parallel_paths  # beam_size\n        batch_size = batch.batch_size\n\n        # (1) Run the encoder on the src.\n        src, enc_states, memory_bank, src_lengths = self._run_encoder(batch)\n        self.model.decoder.init_state(src, memory_bank, enc_states)\n\n        results = {\n            ""predictions"": None,\n            ""scores"": None,\n            ""attention"": None,\n            ""batch"": batch,\n            ""gold_score"": self._gold_score(\n                batch, memory_bank, src_lengths, src_vocabs, use_src_map,\n                enc_states, batch_size, src)}\n\n        # (2) prep decode_strategy. Possibly repeat src objects.\n        src_map = batch.src_map if use_src_map else None\n        target_prefix = batch.tgt if self.tgt_prefix else None\n        fn_map_state, memory_bank, memory_lengths, src_map = \\\n            decode_strategy.initialize(memory_bank, src_lengths, src_map,\n                                       target_prefix=target_prefix)\n        if fn_map_state is not None:\n            self.model.decoder.map_state(fn_map_state)\n\n        # (3) Begin decoding step by step:\n        for step in range(decode_strategy.max_length):\n            decoder_input = decode_strategy.current_predictions.view(1, -1, 1)\n\n            log_probs, attn = self._decode_and_generate(\n                decoder_input,\n                memory_bank,\n                batch,\n                src_vocabs,\n                memory_lengths=memory_lengths,\n                src_map=src_map,\n                step=step,\n                batch_offset=decode_strategy.batch_offset)\n\n            decode_strategy.advance(log_probs, attn)\n            any_finished = decode_strategy.is_finished.any()\n            if any_finished:\n                decode_strategy.update_finished()\n                if decode_strategy.done:\n                    break\n\n            select_indices = decode_strategy.select_indices\n\n            if any_finished:\n                # Reorder states.\n                if isinstance(memory_bank, tuple):\n                    memory_bank = tuple(x.index_select(1, select_indices)\n                                        for x in memory_bank)\n                else:\n                    memory_bank = memory_bank.index_select(1, select_indices)\n\n                memory_lengths = memory_lengths.index_select(0, select_indices)\n\n                if src_map is not None:\n                    src_map = src_map.index_select(1, select_indices)\n\n            if parallel_paths > 1 or any_finished:\n                self.model.decoder.map_state(\n                    lambda state, dim: state.index_select(dim, select_indices))\n\n        results[""scores""] = decode_strategy.scores\n        results[""predictions""] = decode_strategy.predictions\n        results[""attention""] = decode_strategy.attention\n        if self.report_align:\n            results[""alignment""] = self._align_forward(\n                batch, decode_strategy.predictions)\n        else:\n            results[""alignment""] = [[] for _ in range(batch_size)]\n        return results\n\n    def _score_target(self, batch, memory_bank, src_lengths,\n                      src_vocabs, src_map):\n        tgt = batch.tgt\n        tgt_in = tgt[:-1]\n\n        log_probs, attn = self._decode_and_generate(\n            tgt_in, memory_bank, batch, src_vocabs,\n            memory_lengths=src_lengths, src_map=src_map)\n\n        log_probs[:, :, self._tgt_pad_idx] = 0\n        gold = tgt[1:]\n        gold_scores = log_probs.gather(2, gold)\n        gold_scores = gold_scores.sum(dim=0).view(-1)\n\n        return gold_scores\n\n    def _report_score(self, name, score_total, words_total):\n        if words_total == 0:\n            msg = ""%s No words predicted"" % (name,)\n        else:\n            avg_score = score_total / words_total\n            ppl = np.exp(-score_total.item() / words_total)\n            msg = (""%s AVG SCORE: %.4f, %s PPL: %.4f"" % (\n                name, avg_score,\n                name, ppl))\n        return msg\n'"
onmt/utils/__init__.py,0,"b'""""""Module defining various utilities.""""""\nfrom onmt.utils.misc import split_corpus, aeq, use_gpu, set_random_seed\nfrom onmt.utils.alignment import make_batch_align_matrix\nfrom onmt.utils.report_manager import ReportMgr, build_report_manager\nfrom onmt.utils.statistics import Statistics\nfrom onmt.utils.optimizers import MultipleOptimizer, \\\n    Optimizer, AdaFactor\nfrom onmt.utils.earlystopping import EarlyStopping, scorers_from_opts\n\n__all__ = [""split_corpus"", ""aeq"", ""use_gpu"", ""set_random_seed"", ""ReportMgr"",\n           ""build_report_manager"", ""Statistics"",\n           ""MultipleOptimizer"", ""Optimizer"", ""AdaFactor"", ""EarlyStopping"",\n           ""scorers_from_opts"", ""make_batch_align_matrix""]\n'"
onmt/utils/alignment.py,4,"b'# -*- coding: utf-8 -*-\n\nimport torch\nfrom itertools import accumulate\n\n\ndef make_batch_align_matrix(index_tensor, size=None, normalize=False):\n    """"""\n    Convert a sparse index_tensor into a batch of alignment matrix,\n    with row normalize to the sum of 1 if set normalize.\n\n    Args:\n        index_tensor (LongTensor): ``(N, 3)`` of [batch_id, tgt_id, src_id]\n        size (List[int]): Size of the sparse tensor.\n        normalize (bool): if normalize the 2nd dim of resulting tensor.\n    """"""\n    n_fill, device = index_tensor.size(0), index_tensor.device\n    value_tensor = torch.ones([n_fill], dtype=torch.float)\n    dense_tensor = torch.sparse_coo_tensor(\n        index_tensor.t(), value_tensor, size=size, device=device).to_dense()\n    if normalize:\n        row_sum = dense_tensor.sum(-1, keepdim=True)  # sum by row(tgt)\n        # threshold on 1 to avoid div by 0\n        torch.nn.functional.threshold(row_sum, 1, 1, inplace=True)\n        dense_tensor.div_(row_sum)\n    return dense_tensor\n\n\ndef extract_alignment(align_matrix, tgt_mask, src_lens, n_best):\n    """"""\n    Extract a batched align_matrix into its src indice alignment lists,\n    with tgt_mask to filter out invalid tgt position as EOS/PAD.\n    BOS already excluded from tgt_mask in order to match prediction.\n\n    Args:\n        align_matrix (Tensor): ``(B, tgt_len, src_len)``,\n            attention head normalized by Softmax(dim=-1)\n        tgt_mask (BoolTensor): ``(B, tgt_len)``, True for EOS, PAD.\n        src_lens (LongTensor): ``(B,)``, containing valid src length\n        n_best (int): a value indicating number of parallel translation.\n        * B: denote flattened batch as B = batch_size * n_best.\n\n    Returns:\n        alignments (List[List[FloatTensor|None]]): ``(batch_size, n_best,)``,\n         containing valid alignment matrix (or None if blank prediction)\n         for each translation.\n    """"""\n    batch_size_n_best = align_matrix.size(0)\n    assert batch_size_n_best % n_best == 0\n\n    alignments = [[] for _ in range(batch_size_n_best // n_best)]\n\n    # treat alignment matrix one by one as each have different lengths\n    for i, (am_b, tgt_mask_b, src_len) in enumerate(\n            zip(align_matrix, tgt_mask, src_lens)):\n        valid_tgt = ~tgt_mask_b\n        valid_tgt_len = valid_tgt.sum()\n        if valid_tgt_len == 0:\n            # No alignment if not exist valid tgt token\n            valid_alignment = None\n        else:\n            # get valid alignment (sub-matrix from full paded aligment matrix)\n            am_valid_tgt = am_b.masked_select(valid_tgt.unsqueeze(-1)) \\\n                               .view(valid_tgt_len, -1)\n            valid_alignment = am_valid_tgt[:, :src_len]  # only keep valid src\n        alignments[i // n_best].append(valid_alignment)\n\n    return alignments\n\n\ndef build_align_pharaoh(valid_alignment):\n    """"""Convert valid alignment matrix to i-j (from 0) Pharaoh format pairs,\n    or empty list if it\'s None.\n    """"""\n    align_pairs = []\n    if isinstance(valid_alignment, torch.Tensor):\n        tgt_align_src_id = valid_alignment.argmax(dim=-1)\n\n        for tgt_id, src_id in enumerate(tgt_align_src_id.tolist()):\n            align_pairs.append(str(src_id) + ""-"" + str(tgt_id))\n        align_pairs.sort(key=lambda x: int(x.split(\'-\')[-1]))  # sort by tgt_id\n        align_pairs.sort(key=lambda x: int(x.split(\'-\')[0]))  # sort by src_id\n    return align_pairs\n\n\ndef to_word_align(src, tgt, subword_align, m_src=\'joiner\', m_tgt=\'joiner\'):\n    """"""Convert subword alignment to word alignment.\n\n    Args:\n        src (string): tokenized sentence in source language.\n        tgt (string): tokenized sentence in target language.\n        subword_align (string): align_pharaoh correspond to src-tgt.\n        m_src (string): tokenization mode used in src,\n            can be [""joiner"", ""spacer""].\n        m_tgt (string): tokenization mode used in tgt,\n            can be [""joiner"", ""spacer""].\n\n    Returns:\n        word_align (string): converted alignments correspand to\n            detokenized src-tgt.\n    """"""\n    assert m_src in [""joiner"", ""spacer""], ""Invalid value for argument m_src!""\n    assert m_tgt in [""joiner"", ""spacer""], ""Invalid value for argument m_tgt!""\n\n    src, tgt = src.strip().split(), tgt.strip().split()\n    subword_align = {(int(a), int(b)) for a, b in (x.split(""-"")\n                     for x in subword_align.split())}\n\n    src_map = (subword_map_by_spacer(src, marker=\'\xe2\x96\x81\') if m_src == \'spacer\'\n               else subword_map_by_joiner(src, marker=\'\xef\xbf\xad\'))\n\n    tgt_map = (subword_map_by_spacer(src, marker=\'\xe2\x96\x81\') if m_tgt == \'spacer\'\n               else subword_map_by_joiner(src, marker=\'\xef\xbf\xad\'))\n\n    word_align = list({""{}-{}"".format(src_map[a], tgt_map[b])\n                       for a, b in subword_align})\n    word_align.sort(key=lambda x: int(x.split(\'-\')[-1]))  # sort by tgt_id\n    word_align.sort(key=lambda x: int(x.split(\'-\')[0]))  # sort by src_id\n    return "" "".join(word_align)\n\n\ndef subword_map_by_joiner(subwords, marker=\'\xef\xbf\xad\'):\n    """"""Return word id for each subword token (annotate by joiner).""""""\n    flags = [0] * len(subwords)\n    for i, tok in enumerate(subwords):\n        if tok.endswith(marker):\n            flags[i] = 1\n        if tok.startswith(marker):\n            assert i >= 1 and flags[i-1] != 1, \\\n                ""Sentence `{}` not correct!"".format("" "".join(subwords))\n            flags[i-1] = 1\n    marker_acc = list(accumulate([0] + flags[:-1]))\n    word_group = [(i - maker_sofar) for i, maker_sofar\n                  in enumerate(marker_acc)]\n    return word_group\n\n\ndef subword_map_by_spacer(subwords, marker=\'\xe2\x96\x81\'):\n    """"""Return word id for each subword token (annotate by spacer).""""""\n    word_group = list(accumulate([int(marker in x) for x in subwords]))\n    if word_group[0] == 1:  # when dummy prefix is set\n        word_group = [item - 1 for item in word_group]\n    return word_group\n'"
onmt/utils/cnn_factory.py,4,"b'""""""\nImplementation of ""Convolutional Sequence to Sequence Learning""\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nimport onmt.modules\n\nSCALE_WEIGHT = 0.5 ** 0.5\n\n\ndef shape_transform(x):\n    """""" Tranform the size of the tensors to fit for conv input. """"""\n    return torch.unsqueeze(torch.transpose(x, 1, 2), 3)\n\n\nclass GatedConv(nn.Module):\n    """""" Gated convolution for CNN class """"""\n\n    def __init__(self, input_size, width=3, dropout=0.2, nopad=False):\n        super(GatedConv, self).__init__()\n        self.conv = onmt.modules.WeightNormConv2d(\n            input_size, 2 * input_size, kernel_size=(width, 1), stride=(1, 1),\n            padding=(width // 2 * (1 - nopad), 0))\n        init.xavier_uniform_(self.conv.weight, gain=(4 * (1 - dropout))**0.5)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x_var):\n        x_var = self.dropout(x_var)\n        x_var = self.conv(x_var)\n        out, gate = x_var.split(int(x_var.size(1) / 2), 1)\n        out = out * torch.sigmoid(gate)\n        return out\n\n\nclass StackedCNN(nn.Module):\n    """""" Stacked CNN class """"""\n\n    def __init__(self, num_layers, input_size, cnn_kernel_width=3,\n                 dropout=0.2):\n        super(StackedCNN, self).__init__()\n        self.dropout = dropout\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n        for _ in range(num_layers):\n            self.layers.append(\n                GatedConv(input_size, cnn_kernel_width, dropout))\n\n    def forward(self, x):\n        for conv in self.layers:\n            x = x + conv(x)\n            x *= SCALE_WEIGHT\n        return x\n'"
onmt/utils/distributed.py,10,"b'"""""" Pytorch Distributed utils\n    This piece of code was heavily inspired by the equivalent of Fairseq-py\n    https://github.com/pytorch/fairseq\n""""""\n\n\nfrom __future__ import print_function\n\nimport math\nimport pickle\nimport torch.distributed\n\nfrom onmt.utils.logging import logger\n\n\ndef is_master(opt, device_id):\n    return opt.gpu_ranks[device_id] == 0\n\n\ndef multi_init(opt, device_id):\n    dist_init_method = \'tcp://{master_ip}:{master_port}\'.format(\n        master_ip=opt.master_ip,\n        master_port=opt.master_port)\n    dist_world_size = opt.world_size\n    torch.distributed.init_process_group(\n        backend=opt.gpu_backend, init_method=dist_init_method,\n        world_size=dist_world_size, rank=opt.gpu_ranks[device_id])\n    gpu_rank = torch.distributed.get_rank()\n    if not is_master(opt, device_id):\n        logger.disabled = True\n\n    return gpu_rank\n\n\ndef all_reduce_and_rescale_tensors(tensors, rescale_denom,\n                                   buffer_size=10485760):\n    """"""All-reduce and rescale tensors in chunks of the specified size.\n\n    Args:\n        tensors: list of Tensors to all-reduce\n        rescale_denom: denominator for rescaling summed Tensors\n        buffer_size: all-reduce chunk size in bytes\n    """"""\n    # buffer size in bytes, determine equiv. # of elements based on data type\n    buffer_t = tensors[0].new(\n        math.ceil(buffer_size / tensors[0].element_size())).zero_()\n    buffer = []\n\n    def all_reduce_buffer():\n        # copy tensors into buffer_t\n        offset = 0\n        for t in buffer:\n            numel = t.numel()\n            buffer_t[offset:offset+numel].copy_(t.view(-1))\n            offset += numel\n\n        # all-reduce and rescale\n        torch.distributed.all_reduce(buffer_t[:offset])\n        buffer_t.div_(rescale_denom)\n\n        # copy all-reduced buffer back into tensors\n        offset = 0\n        for t in buffer:\n            numel = t.numel()\n            t.view(-1).copy_(buffer_t[offset:offset+numel])\n            offset += numel\n\n    filled = 0\n    for t in tensors:\n        sz = t.numel() * t.element_size()\n        if sz > buffer_size:\n            # tensor is bigger than buffer, all-reduce and rescale directly\n            torch.distributed.all_reduce(t)\n            t.div_(rescale_denom)\n        elif filled + sz > buffer_size:\n            # buffer is full, all-reduce and replace buffer with grad\n            all_reduce_buffer()\n            buffer = [t]\n            filled = sz\n        else:\n            # add tensor to buffer\n            buffer.append(t)\n            filled += sz\n\n    if len(buffer) > 0:\n        all_reduce_buffer()\n\n\ndef all_gather_list(data, max_size=4096):\n    """"""Gathers arbitrary data from all nodes into a list.""""""\n    world_size = torch.distributed.get_world_size()\n    if not hasattr(all_gather_list, \'_in_buffer\') or \\\n            max_size != all_gather_list._in_buffer.size():\n        all_gather_list._in_buffer = torch.cuda.ByteTensor(max_size)\n        all_gather_list._out_buffers = [\n            torch.cuda.ByteTensor(max_size)\n            for i in range(world_size)\n        ]\n    in_buffer = all_gather_list._in_buffer\n    out_buffers = all_gather_list._out_buffers\n\n    enc = pickle.dumps(data)\n    enc_size = len(enc)\n    if enc_size + 2 > max_size:\n        raise ValueError(\n            \'encoded data exceeds max_size: {}\'.format(enc_size + 2))\n    assert max_size < 255*256\n    in_buffer[0] = enc_size // 255  # this encoding works for max_size < 65k\n    in_buffer[1] = enc_size % 255\n    in_buffer[2:enc_size+2] = torch.ByteTensor(list(enc))\n\n    torch.distributed.all_gather(out_buffers, in_buffer.cuda())\n\n    results = []\n    for i in range(world_size):\n        out_buffer = out_buffers[i]\n        size = (255 * out_buffer[0].item()) + out_buffer[1].item()\n\n        bytes_list = bytes(out_buffer[2:size+2].tolist())\n        result = pickle.loads(bytes_list)\n        results.append(result)\n    return results\n'"
onmt/utils/earlystopping.py,0,"b'\nfrom enum import Enum\nfrom onmt.utils.logging import logger\n\n\nclass PatienceEnum(Enum):\n    IMPROVING = 0\n    DECREASING = 1\n    STOPPED = 2\n\n\nclass Scorer(object):\n    def __init__(self, best_score, name):\n        self.best_score = best_score\n        self.name = name\n\n    def is_improving(self, stats):\n        raise NotImplementedError()\n\n    def is_decreasing(self, stats):\n        raise NotImplementedError()\n\n    def update(self, stats):\n        self.best_score = self._caller(stats)\n\n    def __call__(self, stats, **kwargs):\n        return self._caller(stats)\n\n    def _caller(self, stats):\n        raise NotImplementedError()\n\n\nclass PPLScorer(Scorer):\n\n    def __init__(self):\n        super(PPLScorer, self).__init__(float(""inf""), ""ppl"")\n\n    def is_improving(self, stats):\n        return stats.ppl() < self.best_score\n\n    def is_decreasing(self, stats):\n        return stats.ppl() > self.best_score\n\n    def _caller(self, stats):\n        return stats.ppl()\n\n\nclass AccuracyScorer(Scorer):\n\n    def __init__(self):\n        super(AccuracyScorer, self).__init__(float(""-inf""), ""acc"")\n\n    def is_improving(self, stats):\n        return stats.accuracy() > self.best_score\n\n    def is_decreasing(self, stats):\n        return stats.accuracy() < self.best_score\n\n    def _caller(self, stats):\n        return stats.accuracy()\n\n\nDEFAULT_SCORERS = [PPLScorer(), AccuracyScorer()]\n\n\nSCORER_BUILDER = {\n    ""ppl"": PPLScorer,\n    ""accuracy"": AccuracyScorer\n}\n\n\ndef scorers_from_opts(opt):\n    if opt.early_stopping_criteria is None:\n        return DEFAULT_SCORERS\n    else:\n        scorers = []\n        for criterion in set(opt.early_stopping_criteria):\n            assert criterion in SCORER_BUILDER.keys(), \\\n                ""Criterion {} not found"".format(criterion)\n            scorers.append(SCORER_BUILDER[criterion]())\n        return scorers\n\n\nclass EarlyStopping(object):\n\n    def __init__(self, tolerance, scorers=DEFAULT_SCORERS):\n        """"""\n            Callable class to keep track of early stopping.\n\n            Args:\n                tolerance(int): number of validation steps without improving\n                scorer(fn): list of scorers to validate performance on dev\n        """"""\n\n        self.tolerance = tolerance\n        self.stalled_tolerance = self.tolerance\n        self.current_tolerance = self.tolerance\n        self.early_stopping_scorers = scorers\n        self.status = PatienceEnum.IMPROVING\n        self.current_step_best = 0\n\n    def __call__(self, valid_stats, step):\n        """"""\n            Update the internal state of early stopping mechanism, whether to\n        continue training or stop the train procedure.\n\n            Checks whether the scores from all pre-chosen scorers improved. If\n        every metric improve, then the status is switched to improving and the\n        tolerance is reset. If every metric deteriorate, then the status is\n        switched to decreasing and the tolerance is also decreased; if the\n        tolerance reaches 0, then the status is changed to stopped.\n        Finally, if some improved and others not, then it\'s considered stalled;\n        after tolerance number of stalled, the status is switched to stopped.\n\n        :param valid_stats: Statistics of dev set\n        """"""\n\n        if self.status == PatienceEnum.STOPPED:\n            # Don\'t do anything\n            return\n\n        if all([scorer.is_improving(valid_stats) for scorer\n                in self.early_stopping_scorers]):\n            self._update_increasing(valid_stats, step)\n\n        elif all([scorer.is_decreasing(valid_stats) for scorer\n                  in self.early_stopping_scorers]):\n            self._update_decreasing()\n\n        else:\n            self._update_stalled()\n\n    def _update_stalled(self):\n        self.stalled_tolerance -= 1\n\n        logger.info(\n            ""Stalled patience: {}/{}"".format(self.stalled_tolerance,\n                                             self.tolerance))\n\n        if self.stalled_tolerance == 0:\n            logger.info(\n                ""Training finished after stalled validations. Early Stop!""\n            )\n            self._log_best_step()\n\n        self._decreasing_or_stopped_status_update(self.stalled_tolerance)\n\n    def _update_increasing(self, valid_stats, step):\n        self.current_step_best = step\n        for scorer in self.early_stopping_scorers:\n            logger.info(\n                ""Model is improving {}: {:g} --> {:g}."".format(\n                    scorer.name, scorer.best_score, scorer(valid_stats))\n            )\n            # Update best score of each criteria\n            scorer.update(valid_stats)\n\n        # Reset tolerance\n        self.current_tolerance = self.tolerance\n        self.stalled_tolerance = self.tolerance\n\n        # Update current status\n        self.status = PatienceEnum.IMPROVING\n\n    def _update_decreasing(self):\n        # Decrease tolerance\n        self.current_tolerance -= 1\n\n        # Log\n        logger.info(\n            ""Decreasing patience: {}/{}"".format(self.current_tolerance,\n                                                self.tolerance)\n        )\n        # Log\n        if self.current_tolerance == 0:\n            logger.info(""Training finished after not improving. Early Stop!"")\n            self._log_best_step()\n\n        self._decreasing_or_stopped_status_update(self.current_tolerance)\n\n    def _log_best_step(self):\n        logger.info(""Best model found at step {}"".format(\n            self.current_step_best))\n\n    def _decreasing_or_stopped_status_update(self, tolerance):\n        self.status = PatienceEnum.DECREASING \\\n            if tolerance > 0 \\\n            else PatienceEnum.STOPPED\n\n    def is_improving(self):\n        return self.status == PatienceEnum.IMPROVING\n\n    def has_stopped(self):\n        return self.status == PatienceEnum.STOPPED\n'"
onmt/utils/logging.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nimport logging\nfrom logging.handlers import RotatingFileHandler\nlogger = logging.getLogger()\n\n\ndef init_logger(log_file=None, log_file_level=logging.NOTSET, rotate=False):\n    log_format = logging.Formatter(""[%(asctime)s %(levelname)s] %(message)s"")\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    logger.handlers = [console_handler]\n\n    if log_file and log_file != \'\':\n        if rotate:\n            file_handler = RotatingFileHandler(\n                log_file, maxBytes=1000000, backupCount=10)\n        else:\n            file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)\n        logger.addHandler(file_handler)\n\n    return logger\n'"
onmt/utils/loss.py,10,"b'""""""\nThis includes: LossComputeBase and the standard NMTLossCompute, and\n               sharded loss compute stuff.\n""""""\nfrom __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport onmt\nfrom onmt.modules.sparse_losses import SparsemaxLoss\nfrom onmt.modules.sparse_activations import LogSparsemax\n\n\ndef build_loss_compute(model, tgt_field, opt, train=True):\n    """"""\n    Returns a LossCompute subclass which wraps around an nn.Module subclass\n    (such as nn.NLLLoss) which defines the loss criterion. The LossCompute\n    object allows this loss to be computed in shards and passes the relevant\n    data to a Statistics object which handles training/validation logging.\n    Currently, the NMTLossCompute class handles all loss computation except\n    for when using a copy mechanism.\n    """"""\n    device = torch.device(""cuda"" if onmt.utils.misc.use_gpu(opt) else ""cpu"")\n\n    padding_idx = tgt_field.vocab.stoi[tgt_field.pad_token]\n    unk_idx = tgt_field.vocab.stoi[tgt_field.unk_token]\n\n    if opt.lambda_coverage != 0:\n        assert opt.coverage_attn, ""--coverage_attn needs to be set in "" \\\n            ""order to use --lambda_coverage != 0""\n\n    if opt.copy_attn:\n        criterion = onmt.modules.CopyGeneratorLoss(\n            len(tgt_field.vocab), opt.copy_attn_force,\n            unk_index=unk_idx, ignore_index=padding_idx\n        )\n    elif opt.label_smoothing > 0 and train:\n        criterion = LabelSmoothingLoss(\n            opt.label_smoothing, len(tgt_field.vocab), ignore_index=padding_idx\n        )\n    elif isinstance(model.generator[-1], LogSparsemax):\n        criterion = SparsemaxLoss(ignore_index=padding_idx, reduction=\'sum\')\n    else:\n        criterion = nn.NLLLoss(ignore_index=padding_idx, reduction=\'sum\')\n\n    # if the loss function operates on vectors of raw logits instead of\n    # probabilities, only the first part of the generator needs to be\n    # passed to the NMTLossCompute. At the moment, the only supported\n    # loss function of this kind is the sparsemax loss.\n    use_raw_logits = isinstance(criterion, SparsemaxLoss)\n    loss_gen = model.generator[0] if use_raw_logits else model.generator\n    if opt.copy_attn:\n        compute = onmt.modules.CopyGeneratorLossCompute(\n            criterion, loss_gen, tgt_field.vocab, opt.copy_loss_by_seqlength,\n            lambda_coverage=opt.lambda_coverage\n        )\n    else:\n        compute = NMTLossCompute(\n            criterion, loss_gen, lambda_coverage=opt.lambda_coverage,\n            lambda_align=opt.lambda_align)\n    compute.to(device)\n\n    return compute\n\n\nclass LossComputeBase(nn.Module):\n    """"""\n    Class for managing efficient loss computation. Handles\n    sharding next step predictions and accumulating multiple\n    loss computations\n\n    Users can implement their own loss computation strategy by making\n    subclass of this one.  Users need to implement the _compute_loss()\n    and make_shard_state() methods.\n\n    Args:\n        generator (:obj:`nn.Module`) :\n             module that maps the output of the decoder to a\n             distribution over the target vocabulary.\n        tgt_vocab (:obj:`Vocab`) :\n             torchtext vocab object representing the target output\n        normalzation (str): normalize by ""sents"" or ""tokens""\n    """"""\n\n    def __init__(self, criterion, generator):\n        super(LossComputeBase, self).__init__()\n        self.criterion = criterion\n        self.generator = generator\n\n    @property\n    def padding_idx(self):\n        return self.criterion.ignore_index\n\n    def _make_shard_state(self, batch, output, range_, attns=None):\n        """"""\n        Make shard state dictionary for shards() to return iterable\n        shards for efficient loss computation. Subclass must define\n        this method to match its own _compute_loss() interface.\n        Args:\n            batch: the current batch.\n            output: the predict output from the model.\n            range_: the range of examples for computing, the whole\n                    batch or a trunc of it?\n            attns: the attns dictionary returned from the model.\n        """"""\n        return NotImplementedError\n\n    def _compute_loss(self, batch, output, target, **kwargs):\n        """"""\n        Compute the loss. Subclass must define this method.\n\n        Args:\n\n            batch: the current batch.\n            output: the predict output from the model.\n            target: the validate target to compare output with.\n            **kwargs(optional): additional info for computing loss.\n        """"""\n        return NotImplementedError\n\n    def __call__(self,\n                 batch,\n                 output,\n                 attns,\n                 normalization=1.0,\n                 shard_size=0,\n                 trunc_start=0,\n                 trunc_size=None):\n        """"""Compute the forward loss, possibly in shards in which case this\n        method also runs the backward pass and returns ``None`` as the loss\n        value.\n\n        Also supports truncated BPTT for long sequences by taking a\n        range in the decoder output sequence to back propagate in.\n        Range is from `(trunc_start, trunc_start + trunc_size)`.\n\n        Note sharding is an exact efficiency trick to relieve memory\n        required for the generation buffers. Truncation is an\n        approximate efficiency trick to relieve the memory required\n        in the RNN buffers.\n\n        Args:\n          batch (batch) : batch of labeled examples\n          output (:obj:`FloatTensor`) :\n              output of decoder model `[tgt_len x batch x hidden]`\n          attns (dict) : dictionary of attention distributions\n              `[tgt_len x batch x src_len]`\n          normalization: Optional normalization factor.\n          shard_size (int) : maximum number of examples in a shard\n          trunc_start (int) : starting position of truncation window\n          trunc_size (int) : length of truncation window\n\n        Returns:\n            A tuple with the loss and a :obj:`onmt.utils.Statistics` instance.\n        """"""\n        if trunc_size is None:\n            trunc_size = batch.tgt.size(0) - trunc_start\n        trunc_range = (trunc_start, trunc_start + trunc_size)\n        shard_state = self._make_shard_state(batch, output, trunc_range, attns)\n        if shard_size == 0:\n            loss, stats = self._compute_loss(batch, **shard_state)\n            return loss / float(normalization), stats\n        batch_stats = onmt.utils.Statistics()\n        for shard in shards(shard_state, shard_size):\n            loss, stats = self._compute_loss(batch, **shard)\n            loss.div(float(normalization)).backward()\n            batch_stats.update(stats)\n        return None, batch_stats\n\n    def _stats(self, loss, scores, target):\n        """"""\n        Args:\n            loss (:obj:`FloatTensor`): the loss computed by the loss criterion.\n            scores (:obj:`FloatTensor`): a score for each possible output\n            target (:obj:`FloatTensor`): true targets\n\n        Returns:\n            :obj:`onmt.utils.Statistics` : statistics for this batch.\n        """"""\n        pred = scores.max(1)[1]\n        non_padding = target.ne(self.padding_idx)\n        num_correct = pred.eq(target).masked_select(non_padding).sum().item()\n        num_non_padding = non_padding.sum().item()\n        return onmt.utils.Statistics(loss.item(), num_non_padding, num_correct)\n\n    def _bottle(self, _v):\n        return _v.view(-1, _v.size(2))\n\n    def _unbottle(self, _v, batch_size):\n        return _v.view(-1, batch_size, _v.size(1))\n\n\nclass LabelSmoothingLoss(nn.Module):\n    """"""\n    With label smoothing,\n    KL-divergence between q_{smoothed ground truth prob.}(w)\n    and p_{prob. computed by model}(w) is minimized.\n    """"""\n    def __init__(self, label_smoothing, tgt_vocab_size, ignore_index=-100):\n        assert 0.0 < label_smoothing <= 1.0\n        self.ignore_index = ignore_index\n        super(LabelSmoothingLoss, self).__init__()\n\n        smoothing_value = label_smoothing / (tgt_vocab_size - 2)\n        one_hot = torch.full((tgt_vocab_size,), smoothing_value)\n        one_hot[self.ignore_index] = 0\n        self.register_buffer(\'one_hot\', one_hot.unsqueeze(0))\n\n        self.confidence = 1.0 - label_smoothing\n\n    def forward(self, output, target):\n        """"""\n        output (FloatTensor): batch_size x n_classes\n        target (LongTensor): batch_size\n        """"""\n        model_prob = self.one_hot.repeat(target.size(0), 1)\n        model_prob.scatter_(1, target.unsqueeze(1), self.confidence)\n        model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)\n\n        return F.kl_div(output, model_prob, reduction=\'sum\')\n\n\nclass NMTLossCompute(LossComputeBase):\n    """"""\n    Standard NMT Loss Computation.\n    """"""\n\n    def __init__(self, criterion, generator, normalization=""sents"",\n                 lambda_coverage=0.0, lambda_align=0.0):\n        super(NMTLossCompute, self).__init__(criterion, generator)\n        self.lambda_coverage = lambda_coverage\n        self.lambda_align = lambda_align\n\n    def _make_shard_state(self, batch, output, range_, attns=None):\n        shard_state = {\n            ""output"": output,\n            ""target"": batch.tgt[range_[0] + 1: range_[1], :, 0],\n        }\n        if self.lambda_coverage != 0.0:\n            coverage = attns.get(""coverage"", None)\n            std = attns.get(""std"", None)\n            assert attns is not None\n            assert std is not None, ""lambda_coverage != 0.0 requires "" \\\n                ""attention mechanism""\n            assert coverage is not None, ""lambda_coverage != 0.0 requires "" \\\n                ""coverage attention""\n\n            shard_state.update({\n                ""std_attn"": attns.get(""std""),\n                ""coverage_attn"": coverage\n            })\n        if self.lambda_align != 0.0:\n            # attn_align should be in (batch_size, pad_tgt_size, pad_src_size)\n            attn_align = attns.get(""align"", None)\n            # align_idx should be a Tensor in size([N, 3]), N is total number\n            # of align src-tgt pair in current batch, each as\n            # [\'sent_N\xc2\xb0_in_batch\', \'tgt_id+1\', \'src_id\'] (check AlignField)\n            align_idx = batch.align\n            assert attns is not None\n            assert attn_align is not None, ""lambda_align != 0.0 requires "" \\\n                ""alignement attention head""\n            assert align_idx is not None, ""lambda_align != 0.0 requires "" \\\n                ""provide guided alignement""\n            pad_tgt_size, batch_size, _ = batch.tgt.size()\n            pad_src_size = batch.src[0].size(0)\n            align_matrix_size = [batch_size, pad_tgt_size, pad_src_size]\n            ref_align = onmt.utils.make_batch_align_matrix(\n                align_idx, align_matrix_size, normalize=True)\n            # NOTE: tgt-src ref alignement that in range_ of shard\n            # (coherent with batch.tgt)\n            shard_state.update({\n                ""align_head"": attn_align,\n                ""ref_align"": ref_align[:, range_[0] + 1: range_[1], :]\n            })\n        return shard_state\n\n    def _compute_loss(self, batch, output, target, std_attn=None,\n                      coverage_attn=None, align_head=None, ref_align=None):\n\n        bottled_output = self._bottle(output)\n\n        scores = self.generator(bottled_output)\n        gtruth = target.view(-1)\n\n        loss = self.criterion(scores, gtruth)\n        if self.lambda_coverage != 0.0:\n            coverage_loss = self._compute_coverage_loss(\n                std_attn=std_attn, coverage_attn=coverage_attn)\n            loss += coverage_loss\n        if self.lambda_align != 0.0:\n            if align_head.dtype != loss.dtype:  # Fix FP16\n                align_head = align_head.to(loss.dtype)\n            if ref_align.dtype != loss.dtype:\n                ref_align = ref_align.to(loss.dtype)\n            align_loss = self._compute_alignement_loss(\n                align_head=align_head, ref_align=ref_align)\n            loss += align_loss\n        stats = self._stats(loss.clone(), scores, gtruth)\n\n        return loss, stats\n\n    def _compute_coverage_loss(self, std_attn, coverage_attn):\n        covloss = torch.min(std_attn, coverage_attn).sum()\n        covloss *= self.lambda_coverage\n        return covloss\n\n    def _compute_alignement_loss(self, align_head, ref_align):\n        """"""Compute loss between 2 partial alignment matrix.""""""\n        # align_head contains value in [0, 1) presenting attn prob,\n        # 0 was resulted by the context attention src_pad_mask\n        # So, the correspand position in ref_align should also be 0\n        # Therefore, clip align_head to > 1e-18 should be bias free.\n        align_loss = -align_head.clamp(min=1e-18).log().mul(ref_align).sum()\n        align_loss *= self.lambda_align\n        return align_loss\n\n\ndef filter_shard_state(state, shard_size=None):\n    for k, v in state.items():\n        if shard_size is None:\n            yield k, v\n\n        if v is not None:\n            v_split = []\n            if isinstance(v, torch.Tensor):\n                for v_chunk in torch.split(v, shard_size):\n                    v_chunk = v_chunk.data.clone()\n                    v_chunk.requires_grad = v.requires_grad\n                    v_split.append(v_chunk)\n            yield k, (v, v_split)\n\n\ndef shards(state, shard_size, eval_only=False):\n    """"""\n    Args:\n        state: A dictionary which corresponds to the output of\n               *LossCompute._make_shard_state(). The values for\n               those keys are Tensor-like or None.\n        shard_size: The maximum size of the shards yielded by the model.\n        eval_only: If True, only yield the state, nothing else.\n              Otherwise, yield shards.\n\n    Yields:\n        Each yielded shard is a dict.\n\n    Side effect:\n        After the last shard, this function does back-propagation.\n    """"""\n    if eval_only:\n        yield filter_shard_state(state)\n    else:\n        # non_none: the subdict of the state dictionary where the values\n        # are not None.\n        non_none = dict(filter_shard_state(state, shard_size))\n\n        # Now, the iteration:\n        # state is a dictionary of sequences of tensor-like but we\n        # want a sequence of dictionaries of tensors.\n        # First, unzip the dictionary into a sequence of keys and a\n        # sequence of tensor-like sequences.\n        keys, values = zip(*((k, [v_chunk for v_chunk in v_split])\n                             for k, (_, v_split) in non_none.items()))\n\n        # Now, yield a dictionary for each shard. The keys are always\n        # the same. values is a sequence of length #keys where each\n        # element is a sequence of length #shards. We want to iterate\n        # over the shards, not over the keys: therefore, the values need\n        # to be re-zipped by shard and then each shard can be paired\n        # with the keys.\n        for shard_tensors in zip(*values):\n            yield dict(zip(keys, shard_tensors))\n\n        # Assumed backprop\'d\n        variables = []\n        for k, (v, v_split) in non_none.items():\n            if isinstance(v, torch.Tensor) and state[k].requires_grad:\n                variables.extend(zip(torch.split(state[k], shard_size),\n                                     [v_chunk.grad for v_chunk in v_split]))\n        inputs, grads = zip(*variables)\n        torch.autograd.backward(inputs, grads)\n'"
onmt/utils/misc.py,9,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport random\nimport inspect\nfrom itertools import islice, repeat\nimport os\n\n\ndef split_corpus(path, shard_size, default=None):\n    """"""yield a `list` containing `shard_size` line of `path`,\n    or repeatly generate `default` if `path` is None.\n    """"""\n    if path is not None:\n        return _split_corpus(path, shard_size)\n    else:\n        return repeat(default)\n\n\ndef _split_corpus(path, shard_size):\n    """"""Yield a `list` containing `shard_size` line of `path`.\n    """"""\n    with open(path, ""rb"") as f:\n        if shard_size <= 0:\n            yield f.readlines()\n        else:\n            while True:\n                shard = list(islice(f, shard_size))\n                if not shard:\n                    break\n                yield shard\n\n\ndef aeq(*args):\n    """"""\n    Assert all arguments have the same value\n    """"""\n    arguments = (arg for arg in args)\n    first = next(arguments)\n    assert all(arg == first for arg in arguments), \\\n        ""Not all arguments have the same value: "" + str(args)\n\n\ndef sequence_mask(lengths, max_len=None):\n    """"""\n    Creates a boolean mask from sequence lengths.\n    """"""\n    batch_size = lengths.numel()\n    max_len = max_len or lengths.max()\n    return (torch.arange(0, max_len, device=lengths.device)\n            .type_as(lengths)\n            .repeat(batch_size, 1)\n            .lt(lengths.unsqueeze(1)))\n\n\ndef tile(x, count, dim=0):\n    """"""\n    Tiles x on dimension dim count times.\n    """"""\n    perm = list(range(len(x.size())))\n    if dim != 0:\n        perm[0], perm[dim] = perm[dim], perm[0]\n        x = x.permute(perm).contiguous()\n    out_size = list(x.size())\n    out_size[0] *= count\n    batch = x.size(0)\n    x = x.view(batch, -1) \\\n         .transpose(0, 1) \\\n         .repeat(count, 1) \\\n         .transpose(0, 1) \\\n         .contiguous() \\\n         .view(*out_size)\n    if dim != 0:\n        x = x.permute(perm).contiguous()\n    return x\n\n\ndef use_gpu(opt):\n    """"""\n    Creates a boolean if gpu used\n    """"""\n    return (hasattr(opt, \'gpu_ranks\') and len(opt.gpu_ranks) > 0) or \\\n        (hasattr(opt, \'gpu\') and opt.gpu > -1)\n\n\ndef set_random_seed(seed, is_cuda):\n    """"""Sets the random seed.""""""\n    if seed > 0:\n        torch.manual_seed(seed)\n        # this one is needed for torchtext random call (shuffled iterator)\n        # in multi gpu it ensures datasets are read in the same order\n        random.seed(seed)\n        # some cudnn methods can be random even after fixing the seed\n        # unless you tell it to be deterministic\n        torch.backends.cudnn.deterministic = True\n\n    if is_cuda and seed > 0:\n        # These ensure same initialization in multi gpu mode\n        torch.cuda.manual_seed(seed)\n\n\ndef generate_relative_positions_matrix(length, max_relative_positions,\n                                       cache=False):\n    """"""Generate the clipped relative positions matrix\n       for a given length and maximum relative positions""""""\n    if cache:\n        distance_mat = torch.arange(-length+1, 1, 1).unsqueeze(0)\n    else:\n        range_vec = torch.arange(length)\n        range_mat = range_vec.unsqueeze(-1).expand(-1, length).transpose(0, 1)\n        distance_mat = range_mat - range_mat.transpose(0, 1)\n    distance_mat_clipped = torch.clamp(distance_mat,\n                                       min=-max_relative_positions,\n                                       max=max_relative_positions)\n    # Shift values to be >= 0\n    final_mat = distance_mat_clipped + max_relative_positions\n    return final_mat\n\n\ndef relative_matmul(x, z, transpose):\n    """"""Helper function for relative positions attention.""""""\n    batch_size = x.shape[0]\n    heads = x.shape[1]\n    length = x.shape[2]\n    x_t = x.permute(2, 0, 1, 3)\n    x_t_r = x_t.reshape(length, heads * batch_size, -1)\n    if transpose:\n        z_t = z.transpose(1, 2)\n        x_tz_matmul = torch.matmul(x_t_r, z_t)\n    else:\n        x_tz_matmul = torch.matmul(x_t_r, z)\n    x_tz_matmul_r = x_tz_matmul.reshape(length, batch_size, heads, -1)\n    x_tz_matmul_r_t = x_tz_matmul_r.permute(1, 2, 0, 3)\n    return x_tz_matmul_r_t\n\n\ndef fn_args(fun):\n    """"""Returns the list of function arguments name.""""""\n    return inspect.getfullargspec(fun).args\n\n\ndef report_matrix(row_label, column_label, matrix):\n    header_format = ""{:>10.10} "" + ""{:>10.7} "" * len(row_label)\n    row_format = ""{:>10.10} "" + ""{:>10.7f} "" * len(row_label)\n    output = header_format.format("""", *row_label) + \'\\n\'\n    for word, row in zip(column_label, matrix):\n        max_index = row.index(max(row))\n        row_format = row_format.replace(\n            ""{:>10.7f} "", ""{:*>10.7f} "", max_index + 1)\n        row_format = row_format.replace(\n            ""{:*>10.7f} "", ""{:>10.7f} "", max_index)\n        output += row_format.format(word, *row) + \'\\n\'\n        row_format = ""{:>10.10} "" + ""{:>10.7f} "" * len(row_label)\n    return output\n\n\ndef check_model_config(model_config, root):\n    # we need to check the model path + any tokenizer path\n    for model in model_config[""models""]:\n        model_path = os.path.join(root, model)\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(\n                ""{} from model {} does not exist"".format(\n                    model_path, model_config[""id""]))\n    if ""tokenizer"" in model_config.keys():\n        if ""params"" in model_config[""tokenizer""].keys():\n            for k, v in model_config[""tokenizer""][""params""].items():\n                if k.endswith(""path""):\n                    tok_path = os.path.join(root, v)\n                    if not os.path.exists(tok_path):\n                        raise FileNotFoundError(\n                            ""{} from model {} does not exist"".format(\n                                tok_path, model_config[""id""]))\n'"
onmt/utils/optimizers.py,29,"b'"""""" Optimizers class """"""\nimport torch\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nimport operator\nimport functools\nfrom copy import copy\nfrom math import sqrt\nimport types\nimport importlib\nfrom onmt.utils.misc import fn_args\n\n\ndef build_torch_optimizer(model, opt):\n    """"""Builds the PyTorch optimizer.\n\n    We use the default parameters for Adam that are suggested by\n    the original paper https://arxiv.org/pdf/1412.6980.pdf\n    These values are also used by other established implementations,\n    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n    https://keras.io/optimizers/\n    Recently there are slightly different values used in the paper\n    ""Attention is all you need""\n    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n    was used there however, beta2=0.999 is still arguably the more\n    established value, so we use that here as well\n\n    Args:\n      model: The model to optimize.\n      opt. The dictionary of options.\n\n    Returns:\n      A ``torch.optim.Optimizer`` instance.\n    """"""\n    params = [p for p in model.parameters() if p.requires_grad]\n    betas = [opt.adam_beta1, opt.adam_beta2]\n    if opt.optim == \'sgd\':\n        optimizer = optim.SGD(params, lr=opt.learning_rate)\n    elif opt.optim == \'adagrad\':\n        optimizer = optim.Adagrad(\n            params,\n            lr=opt.learning_rate,\n            initial_accumulator_value=opt.adagrad_accumulator_init)\n    elif opt.optim == \'adadelta\':\n        optimizer = optim.Adadelta(params, lr=opt.learning_rate)\n    elif opt.optim == \'adafactor\':\n        optimizer = AdaFactor(\n            params,\n            non_constant_decay=True,\n            enable_factorization=True,\n            weight_decay=0)\n    elif opt.optim == \'adam\':\n        optimizer = optim.Adam(\n            params,\n            lr=opt.learning_rate,\n            betas=betas,\n            eps=1e-9)\n    elif opt.optim == \'sparseadam\':\n        dense = []\n        sparse = []\n        for name, param in model.named_parameters():\n            if not param.requires_grad:\n                continue\n            # TODO: Find a better way to check for sparse gradients.\n            if \'embed\' in name:\n                sparse.append(param)\n            else:\n                dense.append(param)\n        optimizer = MultipleOptimizer(\n            [optim.Adam(\n                dense,\n                lr=opt.learning_rate,\n                betas=betas,\n                eps=1e-8),\n             optim.SparseAdam(\n                 sparse,\n                 lr=opt.learning_rate,\n                 betas=betas,\n                 eps=1e-8)])\n    elif opt.optim == \'fusedadam\':\n        # we use here a FusedAdam() copy of an old Apex repo\n        optimizer = FusedAdam(\n            params,\n            lr=opt.learning_rate,\n            betas=betas)\n    else:\n        raise ValueError(\'Invalid optimizer type: \' + opt.optim)\n\n    if opt.model_dtype == \'fp16\':\n        import apex\n        if opt.optim != \'fusedadam\':\n            # In this case use the new AMP API from apex\n            loss_scale = ""dynamic"" if opt.loss_scale == 0 else opt.loss_scale\n            model, optimizer = apex.amp.initialize(\n                [model, model.generator],\n                optimizer,\n                opt_level=opt.apex_opt_level,\n                loss_scale=loss_scale,\n                keep_batchnorm_fp32=None)\n        else:\n            # In this case use the old FusedAdam with FP16_optimizer wrapper\n            static_loss_scale = opt.loss_scale\n            dynamic_loss_scale = opt.loss_scale == 0\n            optimizer = apex.contrib.optimizers.FP16_Optimizer(\n                optimizer,\n                static_loss_scale=static_loss_scale,\n                dynamic_loss_scale=dynamic_loss_scale)\n    return optimizer\n\n\ndef make_learning_rate_decay_fn(opt):\n    """"""Returns the learning decay function from options.""""""\n    if opt.decay_method == \'noam\':\n        return functools.partial(\n            noam_decay,\n            warmup_steps=opt.warmup_steps,\n            model_size=opt.rnn_size)\n    elif opt.decay_method == \'noamwd\':\n        return functools.partial(\n            noamwd_decay,\n            warmup_steps=opt.warmup_steps,\n            model_size=opt.rnn_size,\n            rate=opt.learning_rate_decay,\n            decay_steps=opt.decay_steps,\n            start_step=opt.start_decay_steps)\n    elif opt.decay_method == \'rsqrt\':\n        return functools.partial(\n            rsqrt_decay, warmup_steps=opt.warmup_steps)\n    elif opt.start_decay_steps is not None:\n        return functools.partial(\n            exponential_decay,\n            rate=opt.learning_rate_decay,\n            decay_steps=opt.decay_steps,\n            start_step=opt.start_decay_steps)\n\n\ndef noam_decay(step, warmup_steps, model_size):\n    """"""Learning rate schedule described in\n    https://arxiv.org/pdf/1706.03762.pdf.\n    """"""\n    return (\n        model_size ** (-0.5) *\n        min(step ** (-0.5), step * warmup_steps**(-1.5)))\n\n\ndef noamwd_decay(step, warmup_steps,\n                 model_size, rate, decay_steps, start_step=0):\n    """"""Learning rate schedule optimized for huge batches\n    """"""\n    return (\n        model_size ** (-0.5) *\n        min(step ** (-0.5), step * warmup_steps**(-1.5)) *\n        rate ** (max(step - start_step + decay_steps, 0) // decay_steps))\n\n\ndef exponential_decay(step, rate, decay_steps, start_step=0):\n    """"""A standard exponential decay, scaling the learning rate by :obj:`rate`\n    every :obj:`decay_steps` steps.\n    """"""\n    return rate ** (max(step - start_step + decay_steps, 0) // decay_steps)\n\n\ndef rsqrt_decay(step, warmup_steps):\n    """"""Decay based on the reciprocal of the step square root.""""""\n    return 1.0 / sqrt(max(step, warmup_steps))\n\n\nclass MultipleOptimizer(object):\n    """""" Implement multiple optimizers needed for sparse adam """"""\n\n    def __init__(self, op):\n        """""" ? """"""\n        self.optimizers = op\n\n    @property\n    def param_groups(self):\n        param_groups = []\n        for optimizer in self.optimizers:\n            param_groups.extend(optimizer.param_groups)\n        return param_groups\n\n    def zero_grad(self):\n        """""" ? """"""\n        for op in self.optimizers:\n            op.zero_grad()\n\n    def step(self):\n        """""" ? """"""\n        for op in self.optimizers:\n            op.step()\n\n    @property\n    def state(self):\n        """""" ? """"""\n        return {k: v for op in self.optimizers for k, v in op.state.items()}\n\n    def state_dict(self):\n        """""" ? """"""\n        return [op.state_dict() for op in self.optimizers]\n\n    def load_state_dict(self, state_dicts):\n        """""" ? """"""\n        assert len(state_dicts) == len(self.optimizers)\n        for i in range(len(state_dicts)):\n            self.optimizers[i].load_state_dict(state_dicts[i])\n\n\nclass Optimizer(object):\n    """"""\n    Controller class for optimization. Mostly a thin\n    wrapper for `optim`, but also useful for implementing\n    rate scheduling beyond what is currently available.\n    Also implements necessary methods for training RNNs such\n    as grad manipulations.\n    """"""\n\n    def __init__(self,\n                 optimizer,\n                 learning_rate,\n                 learning_rate_decay_fn=None,\n                 max_grad_norm=None):\n        """"""Initializes the controller.\n\n       Args:\n         optimizer: A ``torch.optim.Optimizer`` instance.\n         learning_rate: The initial learning rate.\n         learning_rate_decay_fn: An optional callable taking the current step\n           as argument and return a learning rate scaling factor.\n         max_grad_norm: Clip gradients to this global norm.\n        """"""\n        self._optimizer = optimizer\n        self._learning_rate = learning_rate\n        self._learning_rate_decay_fn = learning_rate_decay_fn\n        self._max_grad_norm = max_grad_norm or 0\n        self._training_step = 1\n        self._decay_step = 1\n        self._fp16 = None\n\n    @classmethod\n    def from_opt(cls, model, opt, checkpoint=None):\n        """"""Builds the optimizer from options.\n\n        Args:\n          cls: The ``Optimizer`` class to instantiate.\n          model: The model to optimize.\n          opt: The dict of user options.\n          checkpoint: An optional checkpoint to load states from.\n\n        Returns:\n          An ``Optimizer`` instance.\n        """"""\n        optim_opt = opt\n        optim_state_dict = None\n\n        if opt.train_from and checkpoint is not None:\n            optim = checkpoint[\'optim\']\n            ckpt_opt = checkpoint[\'opt\']\n            ckpt_state_dict = {}\n            if isinstance(optim, Optimizer):  # Backward compatibility.\n                ckpt_state_dict[\'training_step\'] = optim._step + 1\n                ckpt_state_dict[\'decay_step\'] = optim._step + 1\n                ckpt_state_dict[\'optimizer\'] = optim.optimizer.state_dict()\n            else:\n                ckpt_state_dict = optim\n\n            if opt.reset_optim == \'none\':\n                # Load everything from the checkpoint.\n                optim_opt = ckpt_opt\n                optim_state_dict = ckpt_state_dict\n            elif opt.reset_optim == \'all\':\n                # Build everything from scratch.\n                pass\n            elif opt.reset_optim == \'states\':\n                # Reset optimizer, keep options.\n                optim_opt = ckpt_opt\n                optim_state_dict = ckpt_state_dict\n                del optim_state_dict[\'optimizer\']\n            elif opt.reset_optim == \'keep_states\':\n                # Reset options, keep optimizer.\n                optim_state_dict = ckpt_state_dict\n\n        optimizer = cls(\n            build_torch_optimizer(model, optim_opt),\n            optim_opt.learning_rate,\n            learning_rate_decay_fn=make_learning_rate_decay_fn(optim_opt),\n            max_grad_norm=optim_opt.max_grad_norm)\n        if opt.model_dtype == ""fp16"":\n            if opt.optim == ""fusedadam"":\n                optimizer._fp16 = ""legacy""\n            else:\n                optimizer._fp16 = ""amp""\n        if optim_state_dict:\n            optimizer.load_state_dict(optim_state_dict)\n        return optimizer\n\n    @property\n    def training_step(self):\n        """"""The current training step.""""""\n        return self._training_step\n\n    def learning_rate(self):\n        """"""Returns the current learning rate.""""""\n        if self._learning_rate_decay_fn is None:\n            return self._learning_rate\n        scale = self._learning_rate_decay_fn(self._decay_step)\n        return scale * self._learning_rate\n\n    def state_dict(self):\n        return {\n            \'training_step\': self._training_step,\n            \'decay_step\': self._decay_step,\n            \'optimizer\': self._optimizer.state_dict()\n        }\n\n    def load_state_dict(self, state_dict):\n        self._training_step = state_dict[\'training_step\']\n        # State can be partially restored.\n        if \'decay_step\' in state_dict:\n            self._decay_step = state_dict[\'decay_step\']\n        if \'optimizer\' in state_dict:\n            self._optimizer.load_state_dict(state_dict[\'optimizer\'])\n\n    def zero_grad(self):\n        """"""Zero the gradients of optimized parameters.""""""\n        self._optimizer.zero_grad()\n\n    def backward(self, loss):\n        """"""Wrapper for backward pass. Some optimizer requires ownership of the\n        backward pass.""""""\n        if self._fp16 == ""amp"":\n            import apex\n            with apex.amp.scale_loss(loss, self._optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self._fp16 == ""legacy"":\n            kwargs = {}\n            if ""update_master_grads"" in fn_args(self._optimizer.backward):\n                kwargs[""update_master_grads""] = True\n            self._optimizer.backward(loss, **kwargs)\n        else:\n            loss.backward()\n\n    def step(self):\n        """"""Update the model parameters based on current gradients.\n\n        Optionally, will employ gradient modification or update learning\n        rate.\n        """"""\n        learning_rate = self.learning_rate()\n        if self._fp16 == ""legacy"":\n            if hasattr(self._optimizer, ""update_master_grads""):\n                self._optimizer.update_master_grads()\n            if hasattr(self._optimizer, ""clip_master_grads"") and \\\n               self._max_grad_norm > 0:\n                self._optimizer.clip_master_grads(self._max_grad_norm)\n\n        for group in self._optimizer.param_groups:\n            group[\'lr\'] = learning_rate\n            if self._fp16 is None and self._max_grad_norm > 0:\n                clip_grad_norm_(group[\'params\'], self._max_grad_norm)\n        self._optimizer.step()\n        self._decay_step += 1\n        self._training_step += 1\n\n# Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf\n# inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch\n\n\nclass AdaFactor(torch.optim.Optimizer):\n\n    def __init__(self, params, lr=None, beta1=0.9, beta2=0.999, eps1=1e-30,\n                 eps2=1e-3, cliping_threshold=1, non_constant_decay=True,\n                 enable_factorization=True, ams_grad=True, weight_decay=0):\n\n        enable_momentum = beta1 != 0\n\n        if non_constant_decay:\n            ams_grad = False\n\n        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps1=eps1,\n                        eps2=eps2, cliping_threshold=cliping_threshold,\n                        weight_decay=weight_decay, ams_grad=ams_grad,\n                        enable_factorization=enable_factorization,\n                        enable_momentum=enable_momentum,\n                        non_constant_decay=non_constant_decay)\n\n        super(AdaFactor, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdaFactor, self).__setstate__(state)\n\n    def _experimental_reshape(self, shape):\n        temp_shape = shape[2:]\n        if len(temp_shape) == 1:\n            new_shape = (shape[0], shape[1]*shape[2])\n        else:\n            tmp_div = len(temp_shape) // 2 + len(temp_shape) % 2\n            new_shape = (shape[0]*functools.reduce(operator.mul,\n                                                   temp_shape[tmp_div:], 1),\n                         shape[1]*functools.reduce(operator.mul,\n                                                   temp_shape[:tmp_div], 1))\n        return new_shape, copy(shape)\n\n    def _check_shape(self, shape):\n        \'\'\'\n        output1 - True - algorithm for matrix, False - vector;\n        output2 - need reshape\n        \'\'\'\n        if len(shape) > 2:\n            return True, True\n        elif len(shape) == 2:\n            return True, False\n        elif len(shape) == 2 and (shape[0] == 1 or shape[1] == 1):\n            return False, False\n        else:\n            return False, False\n\n    def _rms(self, x):\n        return sqrt(torch.mean(x.pow(2)))\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse \\\n                                       gradients, use SparseAdam instead\')\n\n                is_matrix, is_need_reshape = self._check_shape(grad.size())\n                new_shape = p.data.size()\n                if is_need_reshape and group[\'enable_factorization\']:\n                    new_shape, old_shape = \\\n                        self._experimental_reshape(p.data.size())\n                    grad = grad.view(new_shape)\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    if group[\'enable_momentum\']:\n                        state[\'exp_avg\'] = torch.zeros(new_shape,\n                                                       dtype=torch.float32,\n                                                       device=p.grad.device)\n\n                    if is_matrix and group[\'enable_factorization\']:\n                        state[\'exp_avg_sq_R\'] = \\\n                            torch.zeros((1, new_shape[1]),\n                                        dtype=torch.float32,\n                                        device=p.grad.device)\n                        state[\'exp_avg_sq_C\'] = \\\n                            torch.zeros((new_shape[0], 1),\n                                        dtype=torch.float32,\n                                        device=p.grad.device)\n                    else:\n                        state[\'exp_avg_sq\'] = torch.zeros(new_shape,\n                                                          dtype=torch.float32,\n                                                          device=p.grad.device)\n                    if group[\'ams_grad\']:\n                        state[\'exp_avg_sq_hat\'] = \\\n                            torch.zeros(new_shape, dtype=torch.float32,\n                                        device=p.grad.device)\n\n                if group[\'enable_momentum\']:\n                    exp_avg = state[\'exp_avg\']\n\n                if is_matrix and group[\'enable_factorization\']:\n                    exp_avg_sq_r = state[\'exp_avg_sq_R\']\n                    exp_avg_sq_c = state[\'exp_avg_sq_C\']\n                else:\n                    exp_avg_sq = state[\'exp_avg_sq\']\n\n                if group[\'ams_grad\']:\n                    exp_avg_sq_hat = state[\'exp_avg_sq_hat\']\n\n                state[\'step\'] += 1\n                lr_t = group[\'lr\']\n                lr_t *= max(group[\'eps2\'], self._rms(p.data))\n\n                if group[\'enable_momentum\']:\n                    if group[\'non_constant_decay\']:\n                        beta1_t = group[\'beta1\'] * \\\n                                  (1 - group[\'beta1\'] ** (state[\'step\'] - 1)) \\\n                                  / (1 - group[\'beta1\'] ** state[\'step\'])\n                    else:\n                        beta1_t = group[\'beta1\']\n                    exp_avg.mul_(beta1_t).add_(1 - beta1_t, grad)\n\n                if group[\'non_constant_decay\']:\n                    beta2_t = group[\'beta2\'] * \\\n                              (1 - group[\'beta2\'] ** (state[\'step\'] - 1)) / \\\n                              (1 - group[\'beta2\'] ** state[\'step\'])\n                else:\n                    beta2_t = group[\'beta2\']\n\n                if is_matrix and group[\'enable_factorization\']:\n                    exp_avg_sq_r.mul_(beta2_t). \\\n                        add_(1 - beta2_t, torch.sum(torch.mul(grad, grad).\n                                                    add_(group[\'eps1\']),\n                                                    dim=0, keepdim=True))\n                    exp_avg_sq_c.mul_(beta2_t). \\\n                        add_(1 - beta2_t, torch.sum(torch.mul(grad, grad).\n                                                    add_(group[\'eps1\']),\n                                                    dim=1, keepdim=True))\n                    v = torch.mul(exp_avg_sq_c,\n                                  exp_avg_sq_r).div_(torch.sum(exp_avg_sq_r))\n                else:\n                    exp_avg_sq.mul_(beta2_t). \\\n                        addcmul_(1 - beta2_t, grad, grad). \\\n                        add_((1 - beta2_t)*group[\'eps1\'])\n                    v = exp_avg_sq\n\n                g = grad\n                if group[\'enable_momentum\']:\n                    g = torch.div(exp_avg, 1 - beta1_t ** state[\'step\'])\n\n                if group[\'ams_grad\']:\n                    torch.max(exp_avg_sq_hat, v, out=exp_avg_sq_hat)\n                    v = exp_avg_sq_hat\n                    u = torch.div(g, (torch.div(v, 1 - beta2_t **\n                                  state[\'step\'])).sqrt().add_(group[\'eps1\']))\n                else:\n                    u = torch.div(g, v.sqrt())\n\n                u.div_(max(1, self._rms(u) / group[\'cliping_threshold\']))\n                p.data.add_(-lr_t * (u.view(old_shape) if is_need_reshape and\n                            group[\'enable_factorization\'] else u))\n\n                if group[\'weight_decay\'] != 0:\n                    p.data.add_(-group[\'weight_decay\'] * lr_t, p.data)\n\n        return loss\n\n\nclass FusedAdam(torch.optim.Optimizer):\n\n    """"""Implements Adam algorithm. Currently GPU-only.\n       Requires Apex to be installed via\n    ``python setup.py install --cuda_ext --cpp_ext``.\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square.\n            (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in FusedAdam!\n        eps_inside_sqrt (boolean, optional): in the \'update parameters\' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n    .. _Adam: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params,\n                 lr=1e-3, bias_correction=True,\n                 betas=(0.9, 0.999), eps=1e-8, eps_inside_sqrt=False,\n                 weight_decay=0., max_grad_norm=0., amsgrad=False):\n        global fused_adam_cuda\n        fused_adam_cuda = importlib.import_module(""fused_adam_cuda"")\n\n        if amsgrad:\n            raise RuntimeError(\'AMSGrad variant not supported.\')\n        defaults = dict(lr=lr, bias_correction=bias_correction,\n                        betas=betas, eps=eps, weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n        super(FusedAdam, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n\n    def step(self, closure=None, grads=None, output_params=None,\n             scale=1., grad_norms=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients.\n                (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None]*len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        if output_params is None:\n            output_params_group = [None]*len(self.param_groups)\n        elif isinstance(output_params, types.GeneratorType):\n            output_params_group = [output_params]\n        elif type(output_params[0]) != list:\n            output_params_group = [output_params]\n        else:\n            output_params_group = output_params\n\n        if grad_norms is None:\n            grad_norms = [None]*len(self.param_groups)\n\n        for group, grads_this_group, output_params_this_group, \\\n            grad_norm in zip(self.param_groups, grads_group,\n                             output_params_group, grad_norms):\n            if grads_this_group is None:\n                grads_this_group = [None]*len(group[\'params\'])\n            if output_params_this_group is None:\n                output_params_this_group = [None]*len(group[\'params\'])\n\n            # compute combined scale factor for this group\n            combined_scale = scale\n            if group[\'max_grad_norm\'] > 0:\n                # norm is in fact norm*scale\n                clip = ((grad_norm / scale) + 1e-6) / group[\'max_grad_norm\']\n                if clip > 1:\n                    combined_scale = clip * scale\n\n            bias_correction = 1 if group[\'bias_correction\'] else 0\n\n            for p, grad, output_param in zip(group[\'params\'],\n                                             grads_this_group,\n                                             output_params_this_group):\n                # note: p.grad should not ever be set for correct operation of\n                # mixed precision optimizer that sometimes sends None gradients\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'FusedAdam does not support sparse \\\n                                       gradients, please consider \\\n                                       SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                out_p = torch.tensor([], dtype=torch.float) if output_param \\\n                    is None else output_param\n                fused_adam_cuda.adam(p.data,\n                                     out_p,\n                                     exp_avg,\n                                     exp_avg_sq,\n                                     grad,\n                                     group[\'lr\'],\n                                     beta1,\n                                     beta2,\n                                     group[\'eps\'],\n                                     combined_scale,\n                                     state[\'step\'],\n                                     self.eps_mode,\n                                     bias_correction,\n                                     group[\'weight_decay\'])\n        return loss\n'"
onmt/utils/parse.py,1,"b'import configargparse as cfargparse\nimport os\n\nimport torch\n\nimport onmt.opts as opts\nfrom onmt.utils.logging import logger\n\n\nclass ArgumentParser(cfargparse.ArgumentParser):\n    def __init__(\n            self,\n            config_file_parser_class=cfargparse.YAMLConfigFileParser,\n            formatter_class=cfargparse.ArgumentDefaultsHelpFormatter,\n            **kwargs):\n        super(ArgumentParser, self).__init__(\n            config_file_parser_class=config_file_parser_class,\n            formatter_class=formatter_class,\n            **kwargs)\n\n    @classmethod\n    def defaults(cls, *args):\n        """"""Get default arguments added to a parser by all ``*args``.""""""\n        dummy_parser = cls()\n        for callback in args:\n            callback(dummy_parser)\n        defaults = dummy_parser.parse_known_args([])[0]\n        return defaults\n\n    @classmethod\n    def update_model_opts(cls, model_opt):\n        if model_opt.word_vec_size > 0:\n            model_opt.src_word_vec_size = model_opt.word_vec_size\n            model_opt.tgt_word_vec_size = model_opt.word_vec_size\n\n        if model_opt.layers > 0:\n            model_opt.enc_layers = model_opt.layers\n            model_opt.dec_layers = model_opt.layers\n\n        if model_opt.rnn_size > 0:\n            model_opt.enc_rnn_size = model_opt.rnn_size\n            model_opt.dec_rnn_size = model_opt.rnn_size\n\n        model_opt.brnn = model_opt.encoder_type == ""brnn""\n\n        if model_opt.copy_attn_type is None:\n            model_opt.copy_attn_type = model_opt.global_attention\n\n        if model_opt.alignment_layer is None:\n            model_opt.alignment_layer = -2\n            model_opt.lambda_align = 0.0\n            model_opt.full_context_alignment = False\n\n    @classmethod\n    def validate_model_opts(cls, model_opt):\n        assert model_opt.model_type in [""text"", ""img"", ""audio"", ""vec""], \\\n            ""Unsupported model type %s"" % model_opt.model_type\n\n        # this check is here because audio allows the encoder and decoder to\n        # be different sizes, but other model types do not yet\n        same_size = model_opt.enc_rnn_size == model_opt.dec_rnn_size\n        assert model_opt.model_type == \'audio\' or same_size, \\\n            ""The encoder and decoder rnns must be the same size for now""\n\n        assert model_opt.rnn_type != ""SRU"" or model_opt.gpu_ranks, \\\n            ""Using SRU requires -gpu_ranks set.""\n        if model_opt.share_embeddings:\n            if model_opt.model_type != ""text"":\n                raise AssertionError(\n                    ""--share_embeddings requires --model_type text."")\n        if model_opt.lambda_align > 0.0:\n            assert model_opt.decoder_type == \'transformer\', \\\n                ""Only transformer is supported to joint learn alignment.""\n            assert model_opt.alignment_layer < model_opt.dec_layers and \\\n                model_opt.alignment_layer >= -model_opt.dec_layers, \\\n                ""N\xc2\xb0 alignment_layer should be smaller than number of layers.""\n            logger.info(""Joint learn alignment at layer [{}] ""\n                        ""with {} heads in full_context \'{}\'."".format(\n                            model_opt.alignment_layer,\n                            model_opt.alignment_heads,\n                            model_opt.full_context_alignment))\n\n    @classmethod\n    def ckpt_model_opts(cls, ckpt_opt):\n        # Load default opt values, then overwrite with the opts in\n        # the checkpoint. That way, if there are new options added,\n        # the defaults are used.\n        opt = cls.defaults(opts.model_opts)\n        opt.__dict__.update(ckpt_opt.__dict__)\n        return opt\n\n    @classmethod\n    def validate_train_opts(cls, opt):\n        if opt.epochs:\n            raise AssertionError(\n                  ""-epochs is deprecated please use -train_steps."")\n        if opt.truncated_decoder > 0 and max(opt.accum_count) > 1:\n            raise AssertionError(""BPTT is not compatible with -accum > 1"")\n\n        if opt.gpuid:\n            raise AssertionError(\n                  ""gpuid is deprecated see world_size and gpu_ranks"")\n        if torch.cuda.is_available() and not opt.gpu_ranks:\n            logger.warn(""You have a CUDA device, should run with -gpu_ranks"")\n        if opt.world_size < len(opt.gpu_ranks):\n            raise AssertionError(\n                  ""parameter counts of -gpu_ranks must be less or equal ""\n                  ""than -world_size."")\n        if opt.world_size == len(opt.gpu_ranks) and \\\n                min(opt.gpu_ranks) > 0:\n            raise AssertionError(\n                  ""-gpu_ranks should have master(=0) rank ""\n                  ""unless -world_size is greater than len(gpu_ranks)."")\n        assert len(opt.data_ids) == len(opt.data_weights), \\\n            ""Please check -data_ids and -data_weights options!""\n\n        assert len(opt.dropout) == len(opt.dropout_steps), \\\n            ""Number of dropout values must match accum_steps values""\n\n        assert len(opt.attention_dropout) == len(opt.dropout_steps), \\\n            ""Number of attention_dropout values must match accum_steps values""\n\n    @classmethod\n    def validate_translate_opts(cls, opt):\n        if opt.beam_size != 1 and opt.random_sampling_topk != 1:\n            raise ValueError(\'Can either do beam search OR random sampling.\')\n\n    @classmethod\n    def validate_preprocess_args(cls, opt):\n        assert opt.max_shard_size == 0, \\\n            ""-max_shard_size is deprecated. Please use \\\n            -shard_size (number of examples) instead.""\n        assert opt.shuffle == 0, \\\n            ""-shuffle is not implemented. Please shuffle \\\n            your data before pre-processing.""\n\n        assert len(opt.train_src) == len(opt.train_tgt), \\\n            ""Please provide same number of src and tgt train files!""\n\n        assert len(opt.train_src) == len(opt.train_ids), \\\n            ""Please provide proper -train_ids for your data!""\n\n        for file in opt.train_src + opt.train_tgt:\n            assert os.path.isfile(file), ""Please check path of %s"" % file\n\n        if len(opt.train_align) == 1 and opt.train_align[0] is None:\n            opt.train_align = [None] * len(opt.train_src)\n        else:\n            assert len(opt.train_align) == len(opt.train_src), \\\n                ""Please provide same number of word alignment train \\\n                files as src/tgt!""\n            for file in opt.train_align:\n                assert os.path.isfile(file), ""Please check path of %s"" % file\n\n        assert not opt.valid_align or os.path.isfile(opt.valid_align), \\\n            ""Please check path of your valid alignment file!""\n\n        assert not opt.valid_src or os.path.isfile(opt.valid_src), \\\n            ""Please check path of your valid src file!""\n        assert not opt.valid_tgt or os.path.isfile(opt.valid_tgt), \\\n            ""Please check path of your valid tgt file!""\n\n        assert not opt.src_vocab or os.path.isfile(opt.src_vocab), \\\n            ""Please check path of your src vocab!""\n        assert not opt.tgt_vocab or os.path.isfile(opt.tgt_vocab), \\\n            ""Please check path of your tgt vocab!""\n'"
onmt/utils/report_manager.py,1,"b'"""""" Report manager utility """"""\nfrom __future__ import print_function\nimport time\nfrom datetime import datetime\n\nimport onmt\n\nfrom onmt.utils.logging import logger\n\n\ndef build_report_manager(opt, gpu_rank):\n    if opt.tensorboard and gpu_rank == 0:\n        from torch.utils.tensorboard import SummaryWriter\n        tensorboard_log_dir = opt.tensorboard_log_dir\n\n        if not opt.train_from:\n            tensorboard_log_dir += datetime.now().strftime(""/%b-%d_%H-%M-%S"")\n\n        writer = SummaryWriter(tensorboard_log_dir, comment=""Unmt"")\n    else:\n        writer = None\n\n    report_mgr = ReportMgr(opt.report_every, start_time=-1,\n                           tensorboard_writer=writer)\n    return report_mgr\n\n\nclass ReportMgrBase(object):\n    """"""\n    Report Manager Base class\n    Inherited classes should override:\n        * `_report_training`\n        * `_report_step`\n    """"""\n\n    def __init__(self, report_every, start_time=-1.):\n        """"""\n        Args:\n            report_every(int): Report status every this many sentences\n            start_time(float): manually set report start time. Negative values\n                means that you will need to set it later or use `start()`\n        """"""\n        self.report_every = report_every\n        self.start_time = start_time\n\n    def start(self):\n        self.start_time = time.time()\n\n    def log(self, *args, **kwargs):\n        logger.info(*args, **kwargs)\n\n    def report_training(self, step, num_steps, learning_rate,\n                        report_stats, multigpu=False):\n        """"""\n        This is the user-defined batch-level traing progress\n        report function.\n\n        Args:\n            step(int): current step count.\n            num_steps(int): total number of batches.\n            learning_rate(float): current learning rate.\n            report_stats(Statistics): old Statistics instance.\n        Returns:\n            report_stats(Statistics): updated Statistics instance.\n        """"""\n        if self.start_time < 0:\n            raise ValueError(""""""ReportMgr needs to be started\n                                (set \'start_time\' or use \'start()\'"""""")\n\n        if step % self.report_every == 0:\n            if multigpu:\n                report_stats = \\\n                    onmt.utils.Statistics.all_gather_stats(report_stats)\n            self._report_training(\n                step, num_steps, learning_rate, report_stats)\n            return onmt.utils.Statistics()\n        else:\n            return report_stats\n\n    def _report_training(self, *args, **kwargs):\n        """""" To be overridden """"""\n        raise NotImplementedError()\n\n    def report_step(self, lr, step, train_stats=None, valid_stats=None):\n        """"""\n        Report stats of a step\n\n        Args:\n            train_stats(Statistics): training stats\n            valid_stats(Statistics): validation stats\n            lr(float): current learning rate\n        """"""\n        self._report_step(\n            lr, step, train_stats=train_stats, valid_stats=valid_stats)\n\n    def _report_step(self, *args, **kwargs):\n        raise NotImplementedError()\n\n\nclass ReportMgr(ReportMgrBase):\n    def __init__(self, report_every, start_time=-1., tensorboard_writer=None):\n        """"""\n        A report manager that writes statistics on standard output as well as\n        (optionally) TensorBoard\n\n        Args:\n            report_every(int): Report status every this many sentences\n            tensorboard_writer(:obj:`tensorboard.SummaryWriter`):\n                The TensorBoard Summary writer to use or None\n        """"""\n        super(ReportMgr, self).__init__(report_every, start_time)\n        self.tensorboard_writer = tensorboard_writer\n\n    def maybe_log_tensorboard(self, stats, prefix, learning_rate, step):\n        if self.tensorboard_writer is not None:\n            stats.log_tensorboard(\n                prefix, self.tensorboard_writer, learning_rate, step)\n\n    def _report_training(self, step, num_steps, learning_rate,\n                         report_stats):\n        """"""\n        See base class method `ReportMgrBase.report_training`.\n        """"""\n        report_stats.output(step, num_steps,\n                            learning_rate, self.start_time)\n\n        self.maybe_log_tensorboard(report_stats,\n                                   ""progress"",\n                                   learning_rate,\n                                   step)\n        report_stats = onmt.utils.Statistics()\n\n        return report_stats\n\n    def _report_step(self, lr, step, train_stats=None, valid_stats=None):\n        """"""\n        See base class method `ReportMgrBase.report_step`.\n        """"""\n        if train_stats is not None:\n            self.log(\'Train perplexity: %g\' % train_stats.ppl())\n            self.log(\'Train accuracy: %g\' % train_stats.accuracy())\n\n            self.maybe_log_tensorboard(train_stats,\n                                       ""train"",\n                                       lr,\n                                       step)\n\n        if valid_stats is not None:\n            self.log(\'Validation perplexity: %g\' % valid_stats.ppl())\n            self.log(\'Validation accuracy: %g\' % valid_stats.accuracy())\n\n            self.maybe_log_tensorboard(valid_stats,\n                                       ""valid"",\n                                       lr,\n                                       step)\n'"
onmt/utils/rnn_factory.py,1,"b'""""""\n RNN tools\n""""""\nimport torch.nn as nn\nimport onmt.models\n\n\ndef rnn_factory(rnn_type, **kwargs):\n    """""" rnn factory, Use pytorch version when available. """"""\n    no_pack_padded_seq = False\n    if rnn_type == ""SRU"":\n        # SRU doesn\'t support PackedSequence.\n        no_pack_padded_seq = True\n        rnn = onmt.models.sru.SRU(**kwargs)\n    else:\n        rnn = getattr(nn, rnn_type)(**kwargs)\n    return rnn, no_pack_padded_seq\n'"
onmt/utils/statistics.py,1,"b'"""""" Statistics calculation utility """"""\nfrom __future__ import division\nimport time\nimport math\nimport sys\n\nfrom onmt.utils.logging import logger\n\n\nclass Statistics(object):\n    """"""\n    Accumulator for loss statistics.\n    Currently calculates:\n\n    * accuracy\n    * perplexity\n    * elapsed time\n    """"""\n\n    def __init__(self, loss=0, n_words=0, n_correct=0):\n        self.loss = loss\n        self.n_words = n_words\n        self.n_correct = n_correct\n        self.n_src_words = 0\n        self.start_time = time.time()\n\n    @staticmethod\n    def all_gather_stats(stat, max_size=4096):\n        """"""\n        Gather a `Statistics` object accross multiple process/nodes\n\n        Args:\n            stat(:obj:Statistics): the statistics object to gather\n                accross all processes/nodes\n            max_size(int): max buffer size to use\n\n        Returns:\n            `Statistics`, the update stats object\n        """"""\n        stats = Statistics.all_gather_stats_list([stat], max_size=max_size)\n        return stats[0]\n\n    @staticmethod\n    def all_gather_stats_list(stat_list, max_size=4096):\n        """"""\n        Gather a `Statistics` list accross all processes/nodes\n\n        Args:\n            stat_list(list([`Statistics`])): list of statistics objects to\n                gather accross all processes/nodes\n            max_size(int): max buffer size to use\n\n        Returns:\n            our_stats(list([`Statistics`])): list of updated stats\n        """"""\n        from torch.distributed import get_rank\n        from onmt.utils.distributed import all_gather_list\n\n        # Get a list of world_size lists with len(stat_list) Statistics objects\n        all_stats = all_gather_list(stat_list, max_size=max_size)\n\n        our_rank = get_rank()\n        our_stats = all_stats[our_rank]\n        for other_rank, stats in enumerate(all_stats):\n            if other_rank == our_rank:\n                continue\n            for i, stat in enumerate(stats):\n                our_stats[i].update(stat, update_n_src_words=True)\n        return our_stats\n\n    def update(self, stat, update_n_src_words=False):\n        """"""\n        Update statistics by suming values with another `Statistics` object\n\n        Args:\n            stat: another statistic object\n            update_n_src_words(bool): whether to update (sum) `n_src_words`\n                or not\n\n        """"""\n        self.loss += stat.loss\n        self.n_words += stat.n_words\n        self.n_correct += stat.n_correct\n\n        if update_n_src_words:\n            self.n_src_words += stat.n_src_words\n\n    def accuracy(self):\n        """""" compute accuracy """"""\n        return 100 * (self.n_correct / self.n_words)\n\n    def xent(self):\n        """""" compute cross entropy """"""\n        return self.loss / self.n_words\n\n    def ppl(self):\n        """""" compute perplexity """"""\n        return math.exp(min(self.loss / self.n_words, 100))\n\n    def elapsed_time(self):\n        """""" compute elapsed time """"""\n        return time.time() - self.start_time\n\n    def output(self, step, num_steps, learning_rate, start):\n        """"""Write out statistics to stdout.\n\n        Args:\n           step (int): current step\n           n_batch (int): total batches\n           start (int): start time of step.\n        """"""\n        t = self.elapsed_time()\n        step_fmt = ""%2d"" % step\n        if num_steps > 0:\n            step_fmt = ""%s/%5d"" % (step_fmt, num_steps)\n        logger.info(\n            (""Step %s; acc: %6.2f; ppl: %5.2f; xent: %4.2f; "" +\n             ""lr: %7.5f; %3.0f/%3.0f tok/s; %6.0f sec"")\n            % (step_fmt,\n               self.accuracy(),\n               self.ppl(),\n               self.xent(),\n               learning_rate,\n               self.n_src_words / (t + 1e-5),\n               self.n_words / (t + 1e-5),\n               time.time() - start))\n        sys.stdout.flush()\n\n    def log_tensorboard(self, prefix, writer, learning_rate, step):\n        """""" display statistics to tensorboard """"""\n        t = self.elapsed_time()\n        writer.add_scalar(prefix + ""/xent"", self.xent(), step)\n        writer.add_scalar(prefix + ""/ppl"", self.ppl(), step)\n        writer.add_scalar(prefix + ""/accuracy"", self.accuracy(), step)\n        writer.add_scalar(prefix + ""/tgtper"", self.n_words / t, step)\n        writer.add_scalar(prefix + ""/lr"", learning_rate, step)\n'"
