file_path,api_count,code
train_gra_transf_inpt5_new_dropout_2layerMLP_2nn4nnjnn_early_stop.py,12,"b'import argparse\nimport collections\nimport datetime\nimport os\nimport pickle\nimport time\nimport ipdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom dataloader.QuickdrawDataset4dict_2nn4nnjnn import *\nfrom network.gra_transf_inpt5_new_dropout_2layerMLP_3_adj_mtx import *\nfrom utils.AverageMeter import AverageMeter\nfrom utils.Logger import Logger\nfrom utils.accuracy import *\nfrom utils.EarlyStopping import *\nfrom tqdm import tqdm\n\n\n\n################################################\n# This python file contains four parts:\n#\n# Part 1. Argument Parser\n# Part 2. configurations:\n#                       Part 2-1. Basic configuration\n#                       Part 2-2. dataloader instantiation\n#                       Part 2-3. log configuration\n#                       Part 2-4. configurations for loss function, network, and optimizer\n# Part 3. \'train\' function\n# Part 4. \'validate\' function \n# Part 5. \'main\' function\n################################################\n\n\n# Part 1. Argument Parser\nparser = argparse.ArgumentParser(description=\'MGT_stage_1\')\nparser.add_argument(""--exp"", type=str, default=""train_gra_transf_inpt5_new_dropout_2layerMLP_2nn4nnjnn_early_stop_003"", help=""experiment"")\nparser.add_argument(""--train_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/train/"", help=""train_sketch_coordinate_dir"")\nparser.add_argument(""--val_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/val/"", help=""val_sketch_coordinate_dir"")\nparser.add_argument(""--test_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/test/"", help=""test_sketch_coordinate_dir"")\nparser.add_argument(""--sketch_list"", type=str, default=""./dataloader/tiny_train_set.txt"", help=""sketch_list_urls"")\nparser.add_argument(""--sketch_list_4_val"", type=str, default=""./dataloader/tiny_val_set.txt"", help=""sketch_list_urls_4_validation"")\nparser.add_argument(""--sketch_list_4_test"", type=str, default=""./dataloader/tiny_test_set.txt"", help=""sketch_list_urls_4_test"")\nparser.add_argument(""--batch_size"", type=int, default=192, help=""batch_size"")\nparser.add_argument(""--num_workers"", type=int, default=12, help=""num_workers"")\nparser.add_argument(\'--gpu\', type=str, default=""1"", help=\'choose GPU\')\n\n\nargs = parser.parse_args()\n\n\n# Part 2. configurations\n\n# Part 2-1. Basic configuration\nbasic_configs = collections.OrderedDict()\nbasic_configs[\'serial_number\'] = args.exp\nbasic_configs[\'random_seed\'] = int(time.time())\n_seed = basic_configs[\'random_seed\']\nrandom.seed(_seed)\nnp.random.seed(_seed)\ntorch.manual_seed(_seed)\ntorch.cuda.manual_seed(_seed)\ntorch.cuda.manual_seed_all(_seed)\nos.environ[\'PYTHONHASHSEED\'] = str(_seed)\nbasic_configs[\'learning_rate\'] = 0.00005\nbasic_configs[\'num_epochs\'] = 100\nbasic_configs[\'early_stopping_patience\'] = 10\n\n# 001\n#basic_configs[""lr_protocol""] = [(10,0.00005), (20,0.00005 * 0.7), (30,0.00005 * 0.7 * 0.7), (40,0.00005 * 0.7 * 0.7 * 0.7), (50,0.00005 * 0.7 * 0.7 * 0.7 * 0.7),    (60,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (70,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (80,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (90,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (100,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7)]\n\n# 002\nbasic_configs[""lr_protocol""] = [(10,0.00005), (20,0.00005 * 0.7), (30,0.00005 * 0.7 * 0.7), (40,0.00005 * 0.7 * 0.7 * 0.7), (50,0.00005 * 0.7 * 0.7 * 0.7 * 0.7),    (60,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (70,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (80,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (85,0.00005 * (0.7 ** 8)),(90,0.00005 * (0.7 ** 9)),(95,0.00005 * (0.7 ** 10)), (100,0.00005 * (0.7 ** 11))]\n\n\nbasic_configs[""display_step""] = 100\nlr_protocol = basic_configs[""lr_protocol""]\n\n\n# Part 2-2. dataloader instantiation\ndataloader_configs = collections.OrderedDict()\ndataloader_configs[\'train_coordinate_path_root\'] = args.train_coordinate_path_root\ndataloader_configs[\'val_coordinate_path_root\'] = args.val_coordinate_path_root\ndataloader_configs[\'test_coordinate_path_root\'] = args.test_coordinate_path_root\ndataloader_configs[\'sketch_list\'] = args.sketch_list\ndataloader_configs[\'sketch_list_4_val\'] = args.sketch_list_4_val\ndataloader_configs[\'sketch_list_4_test\'] = args.sketch_list_4_test\ndataloader_configs[\'batch_size\'] = args.batch_size\ndataloader_configs[\'num_workers\'] = args.num_workers\n\n\n\ndataloader_configs[\'data_dict_4_train\'] = \'./dataloader/tiny_train_dataset_dict.pickle\'\ndata_dict_4_train_f = open(dataloader_configs[\'data_dict_4_train\'], \'rb\')\ndata_dict_4_train = pickle.load(data_dict_4_train_f)  \n\ndataloader_configs[\'data_dict_4_validation\'] = \'./dataloader/tiny_val_dataset_dict.pickle\' \ndata_dict_4_validation_f = open(dataloader_configs[\'data_dict_4_validation\'], \'rb\')\ndata_dict_4_validation = pickle.load(data_dict_4_validation_f)  \n\n\ndataloader_configs[\'data_dict_4_test\'] = \'./dataloader/tiny_test_dataset_dict.pickle\' \ndata_dict_4_test_f = open(dataloader_configs[\'data_dict_4_test\'], \'rb\')\ndata_dict_4_test = pickle.load(data_dict_4_test_f)  \n\n#ipdb.set_trace()\n\n\n# create dataset\n# -----------------------------------------------------------------------------------------------------\n\ntrain_dataset = QuickdrawDataset_2nn4nnjnn(dataloader_configs[\'train_coordinate_path_root\'], dataloader_configs[\'sketch_list\'], data_dict_4_train)\ntrain_loader = DataLoader(train_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=True, num_workers=dataloader_configs[\'num_workers\'])\n\nval_dataset = QuickdrawDataset_2nn4nnjnn(dataloader_configs[\'val_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_val\'], data_dict_4_validation)\nval_loader = DataLoader(val_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\ntest_dataset = QuickdrawDataset_2nn4nnjnn(dataloader_configs[\'test_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_test\'], data_dict_4_test)\ntest_loader = DataLoader(test_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\n\n\n# Part 2-3. log configuration\nexp_dir = os.path.join(\'./experimental_results\', args.exp)\n\nexp_log_dir = os.path.join(exp_dir, ""log"")\nif not os.path.exists(exp_log_dir):\n    os.makedirs(exp_log_dir)\n\nexp_visual_dir = os.path.join(exp_dir, ""visual"")\nif not os.path.exists(exp_visual_dir):\n    os.makedirs(exp_visual_dir)\n\nexp_ckpt_dir = os.path.join(exp_dir, ""checkpoints"")\nif not os.path.exists(exp_ckpt_dir):\n    os.makedirs(exp_ckpt_dir)\n\nnow_str = datetime.datetime.now().__str__().replace(\' \', \'_\')\nwriter_path = os.path.join(exp_visual_dir, now_str)\nwriter = SummaryWriter(writer_path)\n\nlogger_path = os.path.join(exp_log_dir, now_str + "".log"")\nlogger = Logger(logger_path).get_logger()\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n\n\nlogger.info(""argument parser settings: {}"".format(args))\n\nlogger.info(""basic configuration settings: {}"".format(basic_configs))\n\n\n# Part 2-4. configurations for loss function, network, and optimizer\n\n\nloss_function = nn.CrossEntropyLoss()\n\n\nmax_val_acc = 0.0\nmax_val_acc_epoch = -1\n\n\n\nnetwork_configs=collections.OrderedDict()\n\nnetwork_configs[\'output_dim\']=345\nnetwork_configs[\'n_heads\']=8\nnetwork_configs[\'embed_dim\']=256\nnetwork_configs[\'n_layers\']=4\nnetwork_configs[\'feed_forward_hidden\']=4*network_configs[\'embed_dim\']\nnetwork_configs[\'normalization\']=\'batch\'\nnetwork_configs[\'dropout\']=0.25\nnetwork_configs[\'mlp_classifier_dropout\']=0.25\n\n\nlogger.info(""network configuration settings: {}"".format(network_configs))\n\n\nnet = make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=103, \n                 n_layers=network_configs[\'n_layers\'], n_heads=network_configs[\'n_heads\'], \n                 embed_dim=network_configs[\'embed_dim\'], feedforward_dim=network_configs[\'feed_forward_hidden\'], \n                 normalization=network_configs[\'normalization\'], dropout=network_configs[\'dropout\'], mlp_classifier_dropout=network_configs[\'mlp_classifier_dropout\'])\nnet = net.cuda()\n\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.00005)\n\n\n# Part 3. \'train\' function\ndef train_function(epoch):\n    training_loss = AverageMeter()\n    training_acc = AverageMeter()\n    net.train()\n\n    lr = next((lr for (max_epoch, lr) in lr_protocol if max_epoch > epoch), lr_protocol[-1][1])\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logger.info(""set learning rate to: {}"".format(lr))\n\n    for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, attention_mask_2, attention_mask_3, padding_mask, position_encoding) in enumerate(tqdm(train_loader, ascii=True)):\n\n        coordinate = coordinate.cuda()\n        label = label.cuda()\n        flag_bits = flag_bits.cuda()\n        stroke_len = stroke_len.cuda()\n        attention_mask = attention_mask.cuda()\n        attention_mask_2 = attention_mask_2.cuda()\n        attention_mask_3 = attention_mask_3.cuda()\n        padding_mask = padding_mask.cuda()\n        position_encoding = position_encoding.cuda()\n        \n        # Resize inputs\n        flag_bits.squeeze_(2)\n        position_encoding.squeeze_(2)\n        stroke_len.unsqueeze_(1)\n     \n        optimizer.zero_grad()\n\n        output = net(coordinate, flag_bits, position_encoding, attention_mask, attention_mask_2, attention_mask_3, padding_mask, stroke_len)\n        # ipdb.set_trace()\n\n    \n        batch_loss = loss_function(output, label)\n        \n\n        batch_loss.backward()\n\n        optimizer.step()\n\n        training_loss.update(batch_loss.item(), coordinate.size(0))\n        \n        training_acc.update(accuracy(output, label, topk = (1,))[0].item(), coordinate.size(0))\n\n        if (idx + 1) % basic_configs[""display_step""] == 0:\n            logger.info(\n                ""==> Iteration [{}][{}/{}]:"".format(epoch + 1, idx + 1, len(train_loader)))\n            logger.info(""current batch loss: {}"".format(\n                batch_loss.item()))\n            logger.info(""average loss: {}"".format(\n                training_loss.avg))\n            logger.info(""average acc: {}"".format(training_acc.avg))  \n\n    \n    logger.info(""Begin evaluating on validation set"")\n\n    validation_loss, validation_acc = validate_function(val_loader)\n    logger.info(""Begin evaluating on testing set"")\n    test_loss, test_acc = validate_function(test_loader)\n    \n    writer.add_scalars(""loss"", {\n        ""training_loss"":training_loss.avg,\n        ""validation_loss"":validation_loss.avg,\n        ""test_loss"":test_loss.avg\n        }, epoch+1)\n    writer.add_scalars(""acc"", {\n        ""training_acc"":training_acc.avg,\n        ""validation_acc"":validation_acc.avg,\n        ""test_acc"":test_acc.avg,\n        }, epoch+1)\n\n    return validation_acc\n\n\n# Part 4. \'validate\' function\ndef validate_function(data_loader):\n    validation_loss = AverageMeter()\n\n    validation_acc_1 = AverageMeter()\n    validation_acc_5 = AverageMeter()\n    validation_acc_10 = AverageMeter()\n\n    net.eval()\n    \n    timelist = list()\n      \n    with torch.no_grad():\n        for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, attention_mask_2, attention_mask_3, padding_mask, position_encoding) in enumerate(tqdm(data_loader, ascii=True)):\n            \n            coordinate = coordinate.cuda()\n            label = label.cuda()\n            flag_bits = flag_bits.cuda()\n            stroke_len = stroke_len.cuda()\n            attention_mask = attention_mask.cuda()\n            attention_mask_2 = attention_mask_2.cuda()\n            attention_mask_3 = attention_mask_3.cuda()\n            padding_mask = padding_mask.cuda()\n            position_encoding = position_encoding.cuda()\n\n            # Resize inputs\n            flag_bits.squeeze_(2)\n            position_encoding.squeeze_(2)\n            stroke_len.unsqueeze_(1)\n\n             \n            tic = time.time()\n            \n            output = net(coordinate, flag_bits, position_encoding, attention_mask, attention_mask_2, attention_mask_3, padding_mask, stroke_len)\n\n            timelist.append(time.time() - tic)\n\n            batch_loss = loss_function(output, label)\n            \n\n            validation_loss.update(batch_loss.item(), coordinate.size(0))\n            \n            \n            acc_1, acc_5, acc_10 = accuracy(output, label, topk = (1, 5, 10))\n            validation_acc_1.update(acc_1, coordinate.size(0))\n            validation_acc_5.update(acc_5, coordinate.size(0))\n            validation_acc_10.update(acc_10, coordinate.size(0))\n\n        logger.info(""==> Evaluation Result: "")\n        \n        logger.info(""loss: {}  acc@1: {} acc@5: {} acc@10: {}"".format(validation_loss.avg, validation_acc_1.avg, validation_acc_5.avg, validation_acc_10.avg))\n        logger.info(""Total inference time: {}s"".format(sum(timelist)))\n\n    return validation_loss, validation_acc_1\n\n\n\n\n# Part 5. \'main\' function\nif __name__ == \'__main__\':\n\n    logger.info(""Begin evaluating on validation set before training"")\n    validate_function(val_loader)\n    \n    logger.info(""training status: "")\n\n     \n    early_stopping = EarlyStopping(patience=basic_configs[\'early_stopping_patience\'], delta=0)\n\n    for epoch in range(basic_configs[\'num_epochs\']):\n        logger.info(""Begin training epoch {}"".format(epoch + 1))\n        validation_acc = train_function(epoch)\n\n        if validation_acc.avg > max_val_acc:\n            max_val_acc = validation_acc.avg\n            max_val_acc_epoch = epoch + 1\n\n         \n        early_stopping(validation_acc.avg)\n        logger.info(""Early stopping counter: {}"".format(early_stopping.counter))\n        logger.info(""Early stopping best_score: {}"".format(early_stopping.best_score))\n        logger.info(""Early stopping early_stop: {}"".format(early_stopping.early_stop))\n\n        if early_stopping.early_stop == True:\n            logger.info(""Early stopping after Epoch: {}"".format(epoch + 1))\n            break\n\n        \n        net_checkpoint_name = args.exp + ""_net_epoch"" + str(epoch + 1)\n        net_checkpoint_path = os.path.join(exp_ckpt_dir, net_checkpoint_name)\n        net_state = {""epoch"": epoch + 1,\n                     ""network"": net.state_dict()}\n        torch.save(net_state, net_checkpoint_path)\n\n    logger.info(""max_val_acc: {}  max_val_acc_epoch: {}"".format(max_val_acc, max_val_acc_epoch))    \n'"
train_gra_transf_inpt5_new_dropout_2layerMLP_4nn_early_stop.py,12,"b'import argparse\nimport collections\nimport datetime\nimport os\nimport pickle\nimport time\nimport ipdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom dataloader.QuickdrawDataset4dict_4nn import *\nfrom network.gra_transf_inpt5_new_dropout_2layerMLP import *\nfrom utils.AverageMeter import AverageMeter\nfrom utils.Logger import Logger\nfrom utils.accuracy import *\nfrom utils.EarlyStopping import *\nfrom tqdm import tqdm\n\n\n################################################\n# This python file contains four parts:\n#\n# Part 1. Argument Parser\n# Part 2. configurations:\n#                       Part 2-1. Basic configuration\n#                       Part 2-2. dataloader instantiation\n#                       Part 2-3. log configuration\n#                       Part 2-4. configurations for loss function, network, and optimizer\n# Part 3. \'train\' function\n# Part 4. \'validate\' function \n# Part 5. \'main\' function\n################################################\n\n\n# Part 1. Argument Parser\nparser = argparse.ArgumentParser(description=\'MGT_stage_1\')\nparser.add_argument(""--exp"", type=str, default=""train_gra_transf_inpt5_new_dropout_2layerMLP_4nn_early_stop_002"", help=""experiment"")\nparser.add_argument(""--train_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/train/"", help=""train_sketch_coordinate_dir"")\nparser.add_argument(""--val_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/val/"", help=""val_sketch_coordinate_dir"")\nparser.add_argument(""--test_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/test/"", help=""test_sketch_coordinate_dir"")\nparser.add_argument(""--sketch_list"", type=str, default=""./dataloader/tiny_train_set.txt"", help=""sketch_list_urls"")\nparser.add_argument(""--sketch_list_4_val"", type=str, default=""./dataloader/tiny_val_set.txt"", help=""sketch_list_urls_4_validation"")\nparser.add_argument(""--sketch_list_4_test"", type=str, default=""./dataloader/tiny_test_set.txt"", help=""sketch_list_urls_4_test"")\nparser.add_argument(""--batch_size"", type=int, default=512, help=""batch_size"")\nparser.add_argument(""--num_workers"", type=int, default=12, help=""num_workers"")\nparser.add_argument(\'--gpu\', type=str, default=""2"", help=\'choose GPU\')\n\n\n\nargs = parser.parse_args()\n\n\n\n# Part 2. configurations\n\n# Part 2-1. Basic configuration\nbasic_configs = collections.OrderedDict()\nbasic_configs[\'serial_number\'] = args.exp\nbasic_configs[\'random_seed\'] = int(time.time())\n_seed = basic_configs[\'random_seed\']\nrandom.seed(_seed)\nnp.random.seed(_seed)\ntorch.manual_seed(_seed)\ntorch.cuda.manual_seed(_seed)\ntorch.cuda.manual_seed_all(_seed)\nos.environ[\'PYTHONHASHSEED\'] = str(_seed)\nbasic_configs[\'learning_rate\'] = 0.00005\nbasic_configs[\'num_epochs\'] = 100\nbasic_configs[\'early_stopping_patience\'] = 10\n\n#  001\n# basic_configs[""lr_protocol""] = [(10,0.00005), (20,0.00005 * 0.7), (30,0.00005 * 0.7 * 0.7), (40,0.00005 * 0.7 * 0.7 * 0.7), (50,0.00005 * 0.7 * 0.7 * 0.7 * 0.7),    (60,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (70,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (80,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (90,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (100,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7)]\n\n# 002\nbasic_configs[""lr_protocol""] = [(10,0.00005), (20,0.00005 * 0.7), (30,0.00005 * 0.7 * 0.7), (40,0.00005 * 0.7 * 0.7 * 0.7), (50,0.00005 * 0.7 * 0.7 * 0.7 * 0.7), (60,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (70,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (75,0.00005 * (0.7 ** 7)),(80,0.00005 * (0.7 ** 8)), (85,0.00005 * (0.7 ** 9)),(90,0.00005 * (0.7 ** 10)),(95,0.00005 * (0.7 ** 11)),(100,0.00005 * (0.7 ** 12))]\n\n\nbasic_configs[""display_step""] = 100\nlr_protocol = basic_configs[""lr_protocol""]\n\n\n# Part 2-2. dataloader instantiation\ndataloader_configs = collections.OrderedDict()\ndataloader_configs[\'train_coordinate_path_root\'] = args.train_coordinate_path_root\ndataloader_configs[\'val_coordinate_path_root\'] = args.val_coordinate_path_root\ndataloader_configs[\'test_coordinate_path_root\'] = args.test_coordinate_path_root\ndataloader_configs[\'sketch_list\'] = args.sketch_list\ndataloader_configs[\'sketch_list_4_val\'] = args.sketch_list_4_val\ndataloader_configs[\'sketch_list_4_test\'] = args.sketch_list_4_test\ndataloader_configs[\'batch_size\'] = args.batch_size\ndataloader_configs[\'num_workers\'] = args.num_workers\n\n\n\ndataloader_configs[\'data_dict_4_train\'] = \'./dataloader/tiny_train_dataset_dict.pickle\'\ndata_dict_4_train_f = open(dataloader_configs[\'data_dict_4_train\'], \'rb\')\ndata_dict_4_train = pickle.load(data_dict_4_train_f)  \n\ndataloader_configs[\'data_dict_4_validation\'] = \'./dataloader/tiny_val_dataset_dict.pickle\' \ndata_dict_4_validation_f = open(dataloader_configs[\'data_dict_4_validation\'], \'rb\')\ndata_dict_4_validation = pickle.load(data_dict_4_validation_f)  \n\n\ndataloader_configs[\'data_dict_4_test\'] = \'./dataloader/tiny_test_dataset_dict.pickle\' \ndata_dict_4_test_f = open(dataloader_configs[\'data_dict_4_test\'], \'rb\')\ndata_dict_4_test = pickle.load(data_dict_4_test_f)  \n\n#ipdb.set_trace()\n\n\n# create dataset\n# -----------------------------------------------------------------------------------------------------\n\ntrain_dataset = QuickdrawDataset_4nn(dataloader_configs[\'train_coordinate_path_root\'], dataloader_configs[\'sketch_list\'], data_dict_4_train)\ntrain_loader = DataLoader(train_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=True, num_workers=dataloader_configs[\'num_workers\'])\n\nval_dataset = QuickdrawDataset_4nn(dataloader_configs[\'val_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_val\'], data_dict_4_validation)\nval_loader = DataLoader(val_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\ntest_dataset = QuickdrawDataset_4nn(dataloader_configs[\'test_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_test\'], data_dict_4_test)\ntest_loader = DataLoader(test_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\n\n\n# Part 2-3. log configuration\nexp_dir = os.path.join(\'./experimental_results\', args.exp)\n\nexp_log_dir = os.path.join(exp_dir, ""log"")\nif not os.path.exists(exp_log_dir):\n    os.makedirs(exp_log_dir)\n\nexp_visual_dir = os.path.join(exp_dir, ""visual"")\nif not os.path.exists(exp_visual_dir):\n    os.makedirs(exp_visual_dir)\n\nexp_ckpt_dir = os.path.join(exp_dir, ""checkpoints"")\nif not os.path.exists(exp_ckpt_dir):\n    os.makedirs(exp_ckpt_dir)\n\nnow_str = datetime.datetime.now().__str__().replace(\' \', \'_\')\nwriter_path = os.path.join(exp_visual_dir, now_str)\nwriter = SummaryWriter(writer_path)\n\nlogger_path = os.path.join(exp_log_dir, now_str + "".log"")\nlogger = Logger(logger_path).get_logger()\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n\n\nlogger.info(""argument parser settings: {}"".format(args))\n\nlogger.info(""basic configuration settings: {}"".format(basic_configs))\n\n\n\n# Part 2-4. configurations for loss function, network, and optimizer\n\n\nloss_function = nn.CrossEntropyLoss()\n\n\nmax_val_acc = 0.0\nmax_val_acc_epoch = -1\n\n\n\nnetwork_configs=collections.OrderedDict()\n\nnetwork_configs[\'output_dim\']=345\nnetwork_configs[\'n_heads\']=8\n\nnetwork_configs[\'embed_dim\']=256\n\n\n\nnetwork_configs[\'n_layers\']=4\nnetwork_configs[\'feed_forward_hidden\']=4*network_configs[\'embed_dim\']\nnetwork_configs[\'normalization\']=\'batch\'\n\nnetwork_configs[\'dropout\']=0.10\nnetwork_configs[\'mlp_classifier_dropout\']=0.10\n\n\nlogger.info(""network configuration settings: {}"".format(network_configs))\n\n\nnet = make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=103, \n                 n_layers=network_configs[\'n_layers\'], n_heads=network_configs[\'n_heads\'], \n                 embed_dim=network_configs[\'embed_dim\'], feedforward_dim=network_configs[\'feed_forward_hidden\'], \n                 normalization=network_configs[\'normalization\'], dropout=network_configs[\'dropout\'], mlp_classifier_dropout=network_configs[\'mlp_classifier_dropout\'])\nnet = net.cuda()\n\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.00005)\n\n\n# Part 3. \'train\' function\ndef train_function(epoch):\n    training_loss = AverageMeter()\n    training_acc = AverageMeter()\n    net.train()\n\n    lr = next((lr for (max_epoch, lr) in lr_protocol if max_epoch > epoch), lr_protocol[-1][1])\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logger.info(""set learning rate to: {}"".format(lr))\n\n    for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, padding_mask, position_encoding) in enumerate(tqdm(train_loader, ascii=True)):\n\n        coordinate = coordinate.cuda()\n        label = label.cuda()\n        flag_bits = flag_bits.cuda()\n        stroke_len = stroke_len.cuda()\n        attention_mask = attention_mask.cuda()\n        padding_mask = padding_mask.cuda()\n        position_encoding = position_encoding.cuda()\n        \n        # Resize inputs\n        flag_bits.squeeze_(2)\n        position_encoding.squeeze_(2)\n        stroke_len.unsqueeze_(1)\n     \n        optimizer.zero_grad()\n\n        output = net(coordinate, flag_bits, position_encoding, attention_mask, padding_mask, stroke_len)\n        # ipdb.set_trace()\n\n    \n        batch_loss = loss_function(output, label)\n        \n\n        batch_loss.backward()\n\n        optimizer.step()\n\n        training_loss.update(batch_loss.item(), coordinate.size(0))\n         \n        training_acc.update(accuracy(output, label, topk = (1,))[0].item(), coordinate.size(0))\n\n        if (idx + 1) % basic_configs[""display_step""] == 0:\n            logger.info(\n                ""==> Iteration [{}][{}/{}]:"".format(epoch + 1, idx + 1, len(train_loader)))\n            logger.info(""current batch loss: {}"".format(\n                batch_loss.item()))\n            logger.info(""average loss: {}"".format(\n                training_loss.avg))\n            logger.info(""average acc: {}"".format(training_acc.avg))  \n\n    \n    logger.info(""Begin evaluating on validation set"")\n\n    validation_loss, validation_acc = validate_function(val_loader)\n    logger.info(""Begin evaluating on testing set"")\n    test_loss, test_acc = validate_function(test_loader)\n    \n    writer.add_scalars(""loss"", {\n        ""training_loss"":training_loss.avg,\n        ""validation_loss"":validation_loss.avg,\n        ""test_loss"":test_loss.avg\n        }, epoch+1)\n    writer.add_scalars(""acc"", {\n        ""training_acc"":training_acc.avg,\n        ""validation_acc"":validation_acc.avg,\n        ""test_acc"":test_acc.avg,\n        }, epoch+1)\n\n    return validation_acc\n\n\n# Part 4. \'validate\' function\ndef validate_function(data_loader):\n    validation_loss = AverageMeter()\n\n    validation_acc_1 = AverageMeter()\n    validation_acc_5 = AverageMeter()\n    validation_acc_10 = AverageMeter()\n\n    net.eval()\n     \n    timelist = list()\n     \n    with torch.no_grad():\n        for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, padding_mask, position_encoding) in enumerate(tqdm(data_loader, ascii=True)):\n            \n            coordinate = coordinate.cuda()\n            label = label.cuda()\n            flag_bits = flag_bits.cuda()\n            stroke_len = stroke_len.cuda()\n            attention_mask = attention_mask.cuda()\n            padding_mask = padding_mask.cuda()\n            position_encoding = position_encoding.cuda()\n\n            # Resize inputs\n            flag_bits.squeeze_(2)\n            position_encoding.squeeze_(2)\n            stroke_len.unsqueeze_(1)\n\n             \n            tic = time.time()\n            \n            output = net(coordinate, flag_bits, position_encoding, attention_mask, padding_mask, stroke_len)\n\n            timelist.append(time.time() - tic)\n\n            batch_loss = loss_function(output, label)\n            \n\n            validation_loss.update(batch_loss.item(), coordinate.size(0))\n            \n            \n            acc_1, acc_5, acc_10 = accuracy(output, label, topk = (1, 5, 10))\n            validation_acc_1.update(acc_1, coordinate.size(0))\n            validation_acc_5.update(acc_5, coordinate.size(0))\n            validation_acc_10.update(acc_10, coordinate.size(0))\n\n        logger.info(""==> Evaluation Result: "")\n        \n        logger.info(""loss: {}  acc@1: {} acc@5: {} acc@10: {}"".format(validation_loss.avg, validation_acc_1.avg, validation_acc_5.avg, validation_acc_10.avg))\n        logger.info(""Total inference time: {}s"".format(sum(timelist)))\n\n    return validation_loss, validation_acc_1\n\n\n\n\n# Part 5. \'main\' function\nif __name__ == \'__main__\':\n\n    logger.info(""Begin evaluating on validation set before training"")\n    validate_function(val_loader)\n    \n    logger.info(""training status: "")\n\n    \n    early_stopping = EarlyStopping(patience=basic_configs[\'early_stopping_patience\'], delta=0)\n\n    for epoch in range(basic_configs[\'num_epochs\']):\n        logger.info(""Begin training epoch {}"".format(epoch + 1))\n        validation_acc = train_function(epoch)\n\n        if validation_acc.avg > max_val_acc:\n            max_val_acc = validation_acc.avg\n            max_val_acc_epoch = epoch + 1\n\n        \n        early_stopping(validation_acc.avg)\n        logger.info(""Early stopping counter: {}"".format(early_stopping.counter))\n        logger.info(""Early stopping best_score: {}"".format(early_stopping.best_score))\n        logger.info(""Early stopping early_stop: {}"".format(early_stopping.early_stop))\n\n        if early_stopping.early_stop == True:\n            logger.info(""Early stopping after Epoch: {}"".format(epoch + 1))\n            break\n\n        \n        net_checkpoint_name = args.exp + ""_net_epoch"" + str(epoch + 1)\n        net_checkpoint_path = os.path.join(exp_ckpt_dir, net_checkpoint_name)\n        net_state = {""epoch"": epoch + 1,\n                     ""network"": net.state_dict()}\n        torch.save(net_state, net_checkpoint_path)\n\n    logger.info(""max_val_acc: {}  max_val_acc_epoch: {}"".format(max_val_acc, max_val_acc_epoch))    \n'"
train_gra_transf_inpt5_new_dropout_2layerMLP_4nnjnn_early_stop.py,12,"b'import argparse\nimport collections\nimport datetime\nimport os\nimport pickle\nimport time\nimport ipdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom dataloader.QuickdrawDataset4dict_4nnjnn import *\nfrom network.gra_transf_inpt5_new_dropout_2layerMLP_2_adj_mtx import *\nfrom utils.AverageMeter import AverageMeter\nfrom utils.Logger import Logger\nfrom utils.accuracy import *\nfrom utils.EarlyStopping import *\nfrom tqdm import tqdm\n\n\n################################################\n# This python file contains four parts:\n#\n# Part 1. Argument Parser\n# Part 2. configurations:\n#                       Part 2-1. Basic configuration\n#                       Part 2-2. dataloader instantiation\n#                       Part 2-3. log configuration\n#                       Part 2-4. configurations for loss function, network, and optimizer\n# Part 3. \'train\' function\n# Part 4. \'validate\' function \n# Part 5. \'main\' function\n################################################\n\n\n# Part 1. Argument Parser\nparser = argparse.ArgumentParser(description=\'MGT_stage_1\')\nparser.add_argument(""--exp"", type=str, default=""train_gra_transf_inpt5_new_dropout_2layerMLP_4nnjnn_early_stop_001"", help=""experiment"")\nparser.add_argument(""--train_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/train/"", help=""train_sketch_coordinate_dir"")\nparser.add_argument(""--val_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/val/"", help=""val_sketch_coordinate_dir"")\nparser.add_argument(""--test_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/test/"", help=""test_sketch_coordinate_dir"")\nparser.add_argument(""--sketch_list"", type=str, default=""./dataloader/tiny_train_set.txt"", help=""sketch_list_urls"")\nparser.add_argument(""--sketch_list_4_val"", type=str, default=""./dataloader/tiny_val_set.txt"", help=""sketch_list_urls_4_validation"")\nparser.add_argument(""--sketch_list_4_test"", type=str, default=""./dataloader/tiny_test_set.txt"", help=""sketch_list_urls_4_test"")\nparser.add_argument(""--batch_size"", type=int, default=256, help=""batch_size"")\nparser.add_argument(""--num_workers"", type=int, default=12, help=""num_workers"")\nparser.add_argument(\'--gpu\', type=str, default=""1"", help=\'choose GPU\')\n\n\n\nargs = parser.parse_args()\n\n\n\n# Part 2. configurations\n\n# Part 2-1. Basic configuration\nbasic_configs = collections.OrderedDict()\nbasic_configs[\'serial_number\'] = args.exp\nbasic_configs[\'random_seed\'] = int(time.time())\n_seed = basic_configs[\'random_seed\']\nrandom.seed(_seed)\nnp.random.seed(_seed)\ntorch.manual_seed(_seed)\ntorch.cuda.manual_seed(_seed)\ntorch.cuda.manual_seed_all(_seed)\nos.environ[\'PYTHONHASHSEED\'] = str(_seed)\nbasic_configs[\'learning_rate\'] = 0.00005\nbasic_configs[\'num_epochs\'] = 100\nbasic_configs[\'early_stopping_patience\'] = 10\n\nbasic_configs[""lr_protocol""] = [(10,0.00005), (20,0.00005 * 0.7), (30,0.00005 * 0.7 * 0.7), (40,0.00005 * 0.7 * 0.7 * 0.7), (50,0.00005 * 0.7 * 0.7 * 0.7 * 0.7),    (60,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (70,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (80,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (90,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (100,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7)]\n\n\nbasic_configs[""display_step""] = 100\nlr_protocol = basic_configs[""lr_protocol""]\n\n\n# Part 2-2. dataloader instantiation\ndataloader_configs = collections.OrderedDict()\ndataloader_configs[\'train_coordinate_path_root\'] = args.train_coordinate_path_root\ndataloader_configs[\'val_coordinate_path_root\'] = args.val_coordinate_path_root\ndataloader_configs[\'test_coordinate_path_root\'] = args.test_coordinate_path_root\ndataloader_configs[\'sketch_list\'] = args.sketch_list\ndataloader_configs[\'sketch_list_4_val\'] = args.sketch_list_4_val\ndataloader_configs[\'sketch_list_4_test\'] = args.sketch_list_4_test\ndataloader_configs[\'batch_size\'] = args.batch_size\ndataloader_configs[\'num_workers\'] = args.num_workers\n\n\n\ndataloader_configs[\'data_dict_4_train\'] = \'./dataloader/tiny_train_dataset_dict.pickle\'\ndata_dict_4_train_f = open(dataloader_configs[\'data_dict_4_train\'], \'rb\')\ndata_dict_4_train = pickle.load(data_dict_4_train_f)  \n\ndataloader_configs[\'data_dict_4_validation\'] = \'./dataloader/tiny_val_dataset_dict.pickle\' \ndata_dict_4_validation_f = open(dataloader_configs[\'data_dict_4_validation\'], \'rb\')\ndata_dict_4_validation = pickle.load(data_dict_4_validation_f)  \n\n\ndataloader_configs[\'data_dict_4_test\'] = \'./dataloader/tiny_test_dataset_dict.pickle\' \ndata_dict_4_test_f = open(dataloader_configs[\'data_dict_4_test\'], \'rb\')\ndata_dict_4_test = pickle.load(data_dict_4_test_f)  \n\n#ipdb.set_trace()\n\n\n# create dataset\n# -----------------------------------------------------------------------------------------------------\n\ntrain_dataset = QuickdrawDataset_4nnjnn(dataloader_configs[\'train_coordinate_path_root\'], dataloader_configs[\'sketch_list\'], data_dict_4_train)\ntrain_loader = DataLoader(train_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=True, num_workers=dataloader_configs[\'num_workers\'])\n\nval_dataset = QuickdrawDataset_4nnjnn(dataloader_configs[\'val_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_val\'], data_dict_4_validation)\nval_loader = DataLoader(val_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\ntest_dataset = QuickdrawDataset_4nnjnn(dataloader_configs[\'test_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_test\'], data_dict_4_test)\ntest_loader = DataLoader(test_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\n\n\n# Part 2-3. log configuration\nexp_dir = os.path.join(\'./experimental_results\', args.exp)\n\nexp_log_dir = os.path.join(exp_dir, ""log"")\nif not os.path.exists(exp_log_dir):\n    os.makedirs(exp_log_dir)\n\nexp_visual_dir = os.path.join(exp_dir, ""visual"")\nif not os.path.exists(exp_visual_dir):\n    os.makedirs(exp_visual_dir)\n\nexp_ckpt_dir = os.path.join(exp_dir, ""checkpoints"")\nif not os.path.exists(exp_ckpt_dir):\n    os.makedirs(exp_ckpt_dir)\n\nnow_str = datetime.datetime.now().__str__().replace(\' \', \'_\')\nwriter_path = os.path.join(exp_visual_dir, now_str)\nwriter = SummaryWriter(writer_path)\n\nlogger_path = os.path.join(exp_log_dir, now_str + "".log"")\nlogger = Logger(logger_path).get_logger()\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n\n\nlogger.info(""argument parser settings: {}"".format(args))\n\nlogger.info(""basic configuration settings: {}"".format(basic_configs))\n\n\n\n# Part 2-4. configurations for loss function, network, and optimizer\n\n\nloss_function = nn.CrossEntropyLoss()\n\n\nmax_val_acc = 0.0\nmax_val_acc_epoch = -1\n\n\nnetwork_configs=collections.OrderedDict()\n\nnetwork_configs[\'output_dim\']=345\nnetwork_configs[\'n_heads\']=8\nnetwork_configs[\'embed_dim\']=256\nnetwork_configs[\'n_layers\']=4\nnetwork_configs[\'feed_forward_hidden\']=4*network_configs[\'embed_dim\']\nnetwork_configs[\'normalization\']=\'batch\'\nnetwork_configs[\'dropout\']=0.25\nnetwork_configs[\'mlp_classifier_dropout\']=0.25\n\n\nlogger.info(""network configuration settings: {}"".format(network_configs))\n\n\nnet = make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=103, \n                 n_layers=network_configs[\'n_layers\'], n_heads=network_configs[\'n_heads\'], \n                 embed_dim=network_configs[\'embed_dim\'], feedforward_dim=network_configs[\'feed_forward_hidden\'], \n                 normalization=network_configs[\'normalization\'], dropout=network_configs[\'dropout\'], mlp_classifier_dropout=network_configs[\'mlp_classifier_dropout\'])\nnet = net.cuda()\n\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.00005)\n\n\n# Part 3. \'train\' function\ndef train_function(epoch):\n    training_loss = AverageMeter()\n    training_acc = AverageMeter()\n    net.train()\n\n    lr = next((lr for (max_epoch, lr) in lr_protocol if max_epoch > epoch), lr_protocol[-1][1])\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logger.info(""set learning rate to: {}"".format(lr))\n\n    for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, attention_mask_2, padding_mask, position_encoding) in enumerate(tqdm(train_loader, ascii=True)):\n\n        coordinate = coordinate.cuda()\n        label = label.cuda()\n        flag_bits = flag_bits.cuda()\n        stroke_len = stroke_len.cuda()\n        attention_mask = attention_mask.cuda()\n        attention_mask_2 = attention_mask_2.cuda()\n        padding_mask = padding_mask.cuda()\n        position_encoding = position_encoding.cuda()\n        \n        # Resize inputs\n        flag_bits.squeeze_(2)\n        position_encoding.squeeze_(2)\n        stroke_len.unsqueeze_(1)\n     \n        optimizer.zero_grad()\n\n        output = net(coordinate, flag_bits, position_encoding, attention_mask, attention_mask_2, padding_mask, stroke_len)\n        # ipdb.set_trace()\n\n    \n        batch_loss = loss_function(output, label)\n        \n\n        batch_loss.backward()\n\n        optimizer.step()\n\n        training_loss.update(batch_loss.item(), coordinate.size(0))\n        \n        training_acc.update(accuracy(output, label, topk = (1,))[0].item(), coordinate.size(0))\n\n        if (idx + 1) % basic_configs[""display_step""] == 0:\n            logger.info(\n                ""==> Iteration [{}][{}/{}]:"".format(epoch + 1, idx + 1, len(train_loader)))\n            logger.info(""current batch loss: {}"".format(\n                batch_loss.item()))\n            logger.info(""average loss: {}"".format(\n                training_loss.avg))\n            logger.info(""average acc: {}"".format(training_acc.avg))  \n\n    \n    logger.info(""Begin evaluating on validation set"")\n\n    validation_loss, validation_acc = validate_function(val_loader)\n    logger.info(""Begin evaluating on testing set"")\n    test_loss, test_acc = validate_function(test_loader)\n    \n    writer.add_scalars(""loss"", {\n        ""training_loss"":training_loss.avg,\n        ""validation_loss"":validation_loss.avg,\n        ""test_loss"":test_loss.avg\n        }, epoch+1)\n    writer.add_scalars(""acc"", {\n        ""training_acc"":training_acc.avg,\n        ""validation_acc"":validation_acc.avg,\n        ""test_acc"":test_acc.avg,\n        }, epoch+1)\n\n    return validation_acc\n\n\n# Part 4. \'validate\' function\ndef validate_function(data_loader):\n    validation_loss = AverageMeter()\n\n    validation_acc_1 = AverageMeter()\n    validation_acc_5 = AverageMeter()\n    validation_acc_10 = AverageMeter()\n\n    net.eval()\n    \n    timelist = list()\n      \n    with torch.no_grad():\n        for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, attention_mask_2, padding_mask, position_encoding) in enumerate(tqdm(data_loader, ascii=True)):\n            \n            coordinate = coordinate.cuda()\n            label = label.cuda()\n            flag_bits = flag_bits.cuda()\n            stroke_len = stroke_len.cuda()\n            attention_mask = attention_mask.cuda()\n            attention_mask_2 = attention_mask_2.cuda()\n            padding_mask = padding_mask.cuda()\n            position_encoding = position_encoding.cuda()\n\n            # Resize inputs\n            flag_bits.squeeze_(2)\n            position_encoding.squeeze_(2)\n            stroke_len.unsqueeze_(1)\n\n            \n            tic = time.time()\n            \n            output = net(coordinate, flag_bits, position_encoding, attention_mask, attention_mask_2, padding_mask, stroke_len)\n\n            timelist.append(time.time() - tic)\n\n            batch_loss = loss_function(output, label)\n            \n\n            validation_loss.update(batch_loss.item(), coordinate.size(0))\n            \n            \n            acc_1, acc_5, acc_10 = accuracy(output, label, topk = (1, 5, 10))\n            validation_acc_1.update(acc_1, coordinate.size(0))\n            validation_acc_5.update(acc_5, coordinate.size(0))\n            validation_acc_10.update(acc_10, coordinate.size(0))\n\n        logger.info(""==> Evaluation Result: "")\n        \n        logger.info(""loss: {}  acc@1: {} acc@5: {} acc@10: {}"".format(validation_loss.avg, validation_acc_1.avg, validation_acc_5.avg, validation_acc_10.avg))\n        logger.info(""Total inference time: {}s"".format(sum(timelist)))\n\n    return validation_loss, validation_acc_1\n\n\n\n\n# Part 5. \'main\' function\nif __name__ == \'__main__\':\n\n    logger.info(""Begin evaluating on validation set before training"")\n    validate_function(val_loader)\n    \n    logger.info(""training status: "")\n\n    \n    early_stopping = EarlyStopping(patience=basic_configs[\'early_stopping_patience\'], delta=0)\n\n    for epoch in range(basic_configs[\'num_epochs\']):\n        logger.info(""Begin training epoch {}"".format(epoch + 1))\n        validation_acc = train_function(epoch)\n\n        if validation_acc.avg > max_val_acc:\n            max_val_acc = validation_acc.avg\n            max_val_acc_epoch = epoch + 1\n\n        \n        early_stopping(validation_acc.avg)\n        logger.info(""Early stopping counter: {}"".format(early_stopping.counter))\n        logger.info(""Early stopping best_score: {}"".format(early_stopping.best_score))\n        logger.info(""Early stopping early_stop: {}"".format(early_stopping.early_stop))\n\n        if early_stopping.early_stop == True:\n            logger.info(""Early stopping after Epoch: {}"".format(epoch + 1))\n            break\n\n        \n        net_checkpoint_name = args.exp + ""_net_epoch"" + str(epoch + 1)\n        net_checkpoint_path = os.path.join(exp_ckpt_dir, net_checkpoint_name)\n        net_state = {""epoch"": epoch + 1,\n                     ""network"": net.state_dict()}\n        torch.save(net_state, net_checkpoint_path)\n\n    logger.info(""max_val_acc: {}  max_val_acc_epoch: {}"".format(max_val_acc, max_val_acc_epoch))    \n'"
train_gra_transf_inpt5_new_dropout_2layerMLP_fully_connected_graph_early_stop.py,12,"b'import argparse\nimport collections\nimport datetime\nimport os\nimport pickle\nimport time\nimport ipdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom dataloader.QuickdrawDataset4dict_fully_connected_graph_attention_mask import *\nfrom network.gra_transf_inpt5_new_dropout_2layerMLP import *\nfrom utils.AverageMeter import AverageMeter\nfrom utils.Logger import Logger\nfrom utils.accuracy import *\nfrom utils.EarlyStopping import *\nfrom tqdm import tqdm\n\n\n################################################\n# This python file contains four parts:\n#\n# Part 1. Argument Parser\n# Part 2. configurations:\n#                       Part 2-1. Basic configuration\n#                       Part 2-2. dataloader instantiation\n#                       Part 2-3. log configuration\n#                       Part 2-4. configurations for loss function, network, and optimizer\n# Part 3. \'train\' function\n# Part 4. \'validate\' function \n# Part 5. \'main\' function\n################################################\n\n\n# Part 1. Argument Parser\nparser = argparse.ArgumentParser(description=\'MGT_stage_1\')\nparser.add_argument(""--exp"", type=str, default=""train_gra_transf_inpt5_new_dropout_2layerMLP_fully_connected_graph_early_stop_004"", help=""experiment"")\nparser.add_argument(""--train_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/train/"", help=""train_sketch_coordinate_dir"")\nparser.add_argument(""--val_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/val/"", help=""val_sketch_coordinate_dir"")\nparser.add_argument(""--test_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/test/"", help=""test_sketch_coordinate_dir"")\nparser.add_argument(""--sketch_list"", type=str, default=""./dataloader/tiny_train_set.txt"", help=""sketch_list_urls"")\nparser.add_argument(""--sketch_list_4_val"", type=str, default=""./dataloader/tiny_val_set.txt"", help=""sketch_list_urls_4_validation"")\nparser.add_argument(""--sketch_list_4_test"", type=str, default=""./dataloader/tiny_test_set.txt"", help=""sketch_list_urls_4_test"")\nparser.add_argument(""--batch_size"", type=int, default=512, help=""batch_size"")\nparser.add_argument(""--num_workers"", type=int, default=12, help=""num_workers"")\nparser.add_argument(\'--gpu\', type=str, default=""0"", help=\'choose GPU\')\n\n\nargs = parser.parse_args()\n\n\n# Part 2. configurations\n\n# Part 2-1. Basic configuration\nbasic_configs = collections.OrderedDict()\nbasic_configs[\'serial_number\'] = args.exp\nbasic_configs[\'random_seed\'] = int(time.time())\n_seed = basic_configs[\'random_seed\']\nrandom.seed(_seed)\nnp.random.seed(_seed)\ntorch.manual_seed(_seed)\ntorch.cuda.manual_seed(_seed)\ntorch.cuda.manual_seed_all(_seed)\nos.environ[\'PYTHONHASHSEED\'] = str(_seed)\nbasic_configs[\'learning_rate\'] = 0.00005\nbasic_configs[\'num_epochs\'] = 100\nbasic_configs[\'early_stopping_patience\'] = 10\n\n# 001\n#basic_configs[""lr_protocol""] = [(5,0.00005), (10,0.00005 * 0.7), (15,0.00005 * (0.7 ** 2)), (20,0.00005 * (0.7 ** 3)),(25,0.00005 * (0.7 ** 4)), (30,0.00005 * (0.7 ** 5)), (35,0.00005 * (0.7 ** 6)), (40,0.00005 * (0.7 ** 7)), (45,0.00005 * (0.7 ** 8)), (50,0.00005 * (0.7 ** 9)), (55,0.00005 * (0.7 ** 10)), (60,0.00005 * (0.7 ** 11)), (65,0.00005 * (0.7 ** 12)), (70,0.00005 * (0.7 ** 13)), (75,0.00005 * (0.7 ** 14)), (80,0.00005 * (0.7 ** 15)), (85,0.00005 * (0.7 ** 16)), (90,0.00005 * (0.7 ** 17)), (95,0.00005 * (0.7 ** 18)), (100,0.00005 * (0.7 ** 19))]\n\n# 004\nbasic_configs[""lr_protocol""] = [(10,0.00005), (20,0.00005 * 0.7), (30,0.00005 * 0.7 * 0.7), (40,0.00005 * 0.7 * 0.7 * 0.7), (50,0.00005 * 0.7 * 0.7 * 0.7 * 0.7),    (60,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (70,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (80,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (90,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7), (100,0.00005 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7 * 0.7)]\n\n\n\nbasic_configs[""display_step""] = 100\nlr_protocol = basic_configs[""lr_protocol""]\n\n\n# Part 2-2. dataloader instantiation\ndataloader_configs = collections.OrderedDict()\ndataloader_configs[\'train_coordinate_path_root\'] = args.train_coordinate_path_root\ndataloader_configs[\'val_coordinate_path_root\'] = args.val_coordinate_path_root\ndataloader_configs[\'test_coordinate_path_root\'] = args.test_coordinate_path_root\ndataloader_configs[\'sketch_list\'] = args.sketch_list\ndataloader_configs[\'sketch_list_4_val\'] = args.sketch_list_4_val\ndataloader_configs[\'sketch_list_4_test\'] = args.sketch_list_4_test\ndataloader_configs[\'batch_size\'] = args.batch_size\ndataloader_configs[\'num_workers\'] = args.num_workers\n\n\n\ndataloader_configs[\'data_dict_4_train\'] = \'./dataloader/tiny_train_dataset_dict.pickle\'\ndata_dict_4_train_f = open(dataloader_configs[\'data_dict_4_train\'], \'rb\')\ndata_dict_4_train = pickle.load(data_dict_4_train_f)  \n\ndataloader_configs[\'data_dict_4_validation\'] = \'./dataloader/tiny_val_dataset_dict.pickle\' \ndata_dict_4_validation_f = open(dataloader_configs[\'data_dict_4_validation\'], \'rb\')\ndata_dict_4_validation = pickle.load(data_dict_4_validation_f)  \n\n\ndataloader_configs[\'data_dict_4_test\'] = \'./dataloader/tiny_test_dataset_dict.pickle\' \ndata_dict_4_test_f = open(dataloader_configs[\'data_dict_4_test\'], \'rb\')\ndata_dict_4_test = pickle.load(data_dict_4_test_f)  \n\n#ipdb.set_trace()\n\n\n# create dataset\n# -----------------------------------------------------------------------------------------------------\n\ntrain_dataset = QuickdrawDataset_fully_connected_graph_attmask(dataloader_configs[\'train_coordinate_path_root\'], dataloader_configs[\'sketch_list\'], data_dict_4_train)\ntrain_loader = DataLoader(train_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=True, num_workers=dataloader_configs[\'num_workers\'])\n\nval_dataset = QuickdrawDataset_fully_connected_graph_attmask(dataloader_configs[\'val_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_val\'], data_dict_4_validation)\nval_loader = DataLoader(val_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\ntest_dataset = QuickdrawDataset_fully_connected_graph_attmask(dataloader_configs[\'test_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_test\'], data_dict_4_test)\ntest_loader = DataLoader(test_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\n\n\n# Part 2-3. log configuration\nexp_dir = os.path.join(\'./experimental_results\', args.exp)\n\nexp_log_dir = os.path.join(exp_dir, ""log"")\nif not os.path.exists(exp_log_dir):\n    os.makedirs(exp_log_dir)\n\nexp_visual_dir = os.path.join(exp_dir, ""visual"")\nif not os.path.exists(exp_visual_dir):\n    os.makedirs(exp_visual_dir)\n\nexp_ckpt_dir = os.path.join(exp_dir, ""checkpoints"")\nif not os.path.exists(exp_ckpt_dir):\n    os.makedirs(exp_ckpt_dir)\n\nnow_str = datetime.datetime.now().__str__().replace(\' \', \'_\')\nwriter_path = os.path.join(exp_visual_dir, now_str)\nwriter = SummaryWriter(writer_path)\n\nlogger_path = os.path.join(exp_log_dir, now_str + "".log"")\nlogger = Logger(logger_path).get_logger()\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n\n\nlogger.info(""argument parser settings: {}"".format(args))\n\nlogger.info(""basic configuration settings: {}"".format(basic_configs))\n\n\n\n# Part 2-4. configurations for loss function, network, and optimizer\n\n\nloss_function = nn.CrossEntropyLoss()\n\nmax_val_acc = 0.0\nmax_val_acc_epoch = -1\n\n\n\nnetwork_configs=collections.OrderedDict()\n\nnetwork_configs[\'output_dim\']=345\nnetwork_configs[\'n_heads\']=8\n\nnetwork_configs[\'embed_dim\']=256\n\n\nnetwork_configs[\'n_layers\']=4\nnetwork_configs[\'feed_forward_hidden\']=4*network_configs[\'embed_dim\']\nnetwork_configs[\'normalization\']=\'batch\'\n\nnetwork_configs[\'dropout\']=0.10\nnetwork_configs[\'mlp_classifier_dropout\']=0.10\n\n\nlogger.info(""network configuration settings: {}"".format(network_configs))\n\n\nnet = make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=103, \n                 n_layers=network_configs[\'n_layers\'], n_heads=network_configs[\'n_heads\'], \n                 embed_dim=network_configs[\'embed_dim\'], feedforward_dim=network_configs[\'feed_forward_hidden\'], \n                 normalization=network_configs[\'normalization\'], dropout=network_configs[\'dropout\'], mlp_classifier_dropout=network_configs[\'mlp_classifier_dropout\'])\nnet = net.cuda()\n\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.00005)\n\n\n# Part 3. \'train\' function\ndef train_function(epoch):\n    training_loss = AverageMeter()\n    training_acc = AverageMeter()\n    net.train()\n\n    lr = next((lr for (max_epoch, lr) in lr_protocol if max_epoch > epoch), lr_protocol[-1][1])\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logger.info(""set learning rate to: {}"".format(lr))\n\n    for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, padding_mask, position_encoding) in enumerate(tqdm(train_loader, ascii=True)):\n\n        coordinate = coordinate.cuda()\n        label = label.cuda()\n        flag_bits = flag_bits.cuda()\n        stroke_len = stroke_len.cuda()\n        attention_mask = attention_mask.cuda()\n        padding_mask = padding_mask.cuda()\n        position_encoding = position_encoding.cuda()\n        \n        # Resize inputs\n        flag_bits.squeeze_(2)\n        position_encoding.squeeze_(2)\n        stroke_len.unsqueeze_(1)\n     \n        optimizer.zero_grad()\n\n        output = net(coordinate, flag_bits, position_encoding, attention_mask, padding_mask, stroke_len)\n        # ipdb.set_trace()\n\n    \n        batch_loss = loss_function(output, label)\n        \n\n        batch_loss.backward()\n\n        optimizer.step()\n\n        training_loss.update(batch_loss.item(), coordinate.size(0))\n        \n        training_acc.update(accuracy(output, label, topk = (1,))[0].item(), coordinate.size(0))\n\n        if (idx + 1) % basic_configs[""display_step""] == 0:\n            logger.info(\n                ""==> Iteration [{}][{}/{}]:"".format(epoch + 1, idx + 1, len(train_loader)))\n            logger.info(""current batch loss: {}"".format(\n                batch_loss.item()))\n            logger.info(""average loss: {}"".format(\n                training_loss.avg))\n            logger.info(""average acc: {}"".format(training_acc.avg))  \n\n    \n    logger.info(""Begin evaluating on validation set"")\n\n    validation_loss, validation_acc = validate_function(val_loader)\n    logger.info(""Begin evaluating on testing set"")\n    test_loss, test_acc = validate_function(test_loader)\n    \n    writer.add_scalars(""loss"", {\n        ""training_loss"":training_loss.avg,\n        ""validation_loss"":validation_loss.avg,\n        ""test_loss"":test_loss.avg\n        }, epoch+1)\n    writer.add_scalars(""acc"", {\n        ""training_acc"":training_acc.avg,\n        ""validation_acc"":validation_acc.avg,\n        ""test_acc"":test_acc.avg,\n        }, epoch+1)\n\n    return validation_acc\n\n\n# Part 4. \'validate\' function\ndef validate_function(data_loader):\n    validation_loss = AverageMeter()\n\n    validation_acc_1 = AverageMeter()\n    validation_acc_5 = AverageMeter()\n    validation_acc_10 = AverageMeter()\n\n    net.eval()\n    \n    timelist = list()\n     \n    with torch.no_grad():\n        for idx, (coordinate, label, flag_bits, stroke_len, attention_mask, padding_mask, position_encoding) in enumerate(tqdm(data_loader, ascii=True)):\n            \n            coordinate = coordinate.cuda()\n            label = label.cuda()\n            flag_bits = flag_bits.cuda()\n            stroke_len = stroke_len.cuda()\n            attention_mask = attention_mask.cuda()\n            padding_mask = padding_mask.cuda()\n            position_encoding = position_encoding.cuda()\n\n            # Resize inputs\n            flag_bits.squeeze_(2)\n            position_encoding.squeeze_(2)\n            stroke_len.unsqueeze_(1)\n\n            \n            tic = time.time()\n            \n            output = net(coordinate, flag_bits, position_encoding, attention_mask, padding_mask, stroke_len)\n\n            timelist.append(time.time() - tic)\n\n            batch_loss = loss_function(output, label)\n            \n\n            validation_loss.update(batch_loss.item(), coordinate.size(0))\n            \n            \n            acc_1, acc_5, acc_10 = accuracy(output, label, topk = (1, 5, 10))\n            validation_acc_1.update(acc_1, coordinate.size(0))\n            validation_acc_5.update(acc_5, coordinate.size(0))\n            validation_acc_10.update(acc_10, coordinate.size(0))\n\n        logger.info(""==> Evaluation Result: "")\n        \n        logger.info(""loss: {}  acc@1: {} acc@5: {} acc@10: {}"".format(validation_loss.avg, validation_acc_1.avg, validation_acc_5.avg, validation_acc_10.avg))\n        logger.info(""Total inference time: {}s"".format(sum(timelist)))\n\n    return validation_loss, validation_acc_1\n\n\n\n\n# Part 5. \'main\' function\nif __name__ == \'__main__\':\n\n    logger.info(""Begin evaluating on validation set before training"")\n    validate_function(val_loader)\n    \n    logger.info(""training status: "")\n\n    \n    early_stopping = EarlyStopping(patience=basic_configs[\'early_stopping_patience\'], delta=0)\n\n    for epoch in range(basic_configs[\'num_epochs\']):\n        logger.info(""Begin training epoch {}"".format(epoch + 1))\n        validation_acc = train_function(epoch)\n\n        if validation_acc.avg > max_val_acc:\n            max_val_acc = validation_acc.avg\n            max_val_acc_epoch = epoch + 1\n\n        \n        early_stopping(validation_acc.avg)\n        logger.info(""Early stopping counter: {}"".format(early_stopping.counter))\n        logger.info(""Early stopping best_score: {}"".format(early_stopping.best_score))\n        logger.info(""Early stopping early_stop: {}"".format(early_stopping.early_stop))\n\n        if early_stopping.early_stop == True:\n            logger.info(""Early stopping after Epoch: {}"".format(epoch + 1))\n            break\n\n        \n        net_checkpoint_name = args.exp + ""_net_epoch"" + str(epoch + 1)\n        net_checkpoint_path = os.path.join(exp_ckpt_dir, net_checkpoint_name)\n        net_state = {""epoch"": epoch + 1,\n                     ""network"": net.state_dict()}\n        torch.save(net_state, net_checkpoint_path)\n\n    logger.info(""max_val_acc: {}  max_val_acc_epoch: {}"".format(max_val_acc, max_val_acc_epoch))    \n'"
dataloader/QuickdrawDataset.py,1,"b'import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\nclass QuickdrawDataset(data.Dataset):\n\n    def __init__(self, sketch_path_root, sketch_list, data_transforms=None):\n\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            self.sketch_urls = [os.path.join(sketch_path_root, sketch_url.strip().split(\' \')[\n                                             0]) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(\' \')[-1])\n                           for sketch_url in sketch_url_list]\n\n\n        self.data_transforms = data_transforms\n\n    def __len__(self):\n        return len(self.sketch_urls)\n\n    def __getitem__(self, item):\n\n        sketch_url = self.sketch_urls[item]\n\n        label = self.labels[item]\n        \n        \n        sketch = Image.open(sketch_url, \'r\')\n        \n\n        if self.data_transforms is not None:\n            try:\n                sketch = self.data_transforms(sketch)\n            except:\n                print(""Cannot transform sketch: {}"".format(sketch_url))\n\n        return sketch, label\n'"
dataloader/QuickdrawDataset4dict_2nn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\n\ndef produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n  \n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        if (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n\n    return adja_matr\n\n\n\n\n\n\n\n\n\n'''\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n'''\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_2nn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        # \n        attention_mask_2_neighbors = produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len)\n        \n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_2_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_2nn4nn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\n\ndef produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n  \n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        if (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n\n    return adja_matr\n\n\ndef produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n        # \n        if (flag_bits[1] == 100):\n            adja_matr[0][2] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n            # \n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\n                adja_matr[idx][idx - 2] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n            # \n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\n                adja_matr[idx][idx + 2] = 0\n\n    return adja_matr\n\n\n\n\n\n\n\n'''\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n'''\n\n\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_2nn4nn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        # \n        attention_mask_2_neighbors = produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len)\n\n        attention_mask_4_neighbors = produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len)\n\n        \n        \n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_2_neighbors, attention_mask_4_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_2nn4nn6nn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\ndef produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n  \n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        if (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n\n    return adja_matr\n\n\ndef produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n        # \n        if (flag_bits[1] == 100):\n            adja_matr[0][2] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n            # \n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\n                adja_matr[idx][idx - 2] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n            # \n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\n                adja_matr[idx][idx + 2] = 0\n\n    return adja_matr\n\n\ndef produce_adjacent_matrix_6_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n        # \n        if (flag_bits[1] == 100):\n            adja_matr[0][2] = 0\n            if (flag_bits[2] == 100):\n                adja_matr[0][3] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n            # \n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\n                adja_matr[idx][idx - 2] = 0\n                if (idx >= 3) and (flag_bits[idx - 3] == 100):\n                    adja_matr[idx][idx - 3] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n            # \n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\n                adja_matr[idx][idx + 2] = 0\n                if (idx <= (stroke_len - 4)) and (flag_bits[idx + 2] == 100):\n                    adja_matr[idx][idx + 3] = 0\n\n    return adja_matr\n\n\n\n\n'''\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n'''\n\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_2nn4nn6nn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        # \n        attention_mask_2_neighbors = produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len)\n\n        attention_mask_4_neighbors = produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len)\n\n        attention_mask_6_neighbors = produce_adjacent_matrix_6_neighbors(flag_bits, stroke_len)\n\n        \n        \n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_2_neighbors, attention_mask_4_neighbors, attention_mask_6_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_2nn4nnjnn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\ndef produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n  \n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        if (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n\n    return adja_matr\n\n\ndef produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n        # \n        if (flag_bits[1] == 100):\n            adja_matr[0][2] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n            # \n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\n                adja_matr[idx][idx - 2] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n            # \n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\n                adja_matr[idx][idx + 2] = 0\n\n    return adja_matr\n\n\ndef produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n    \n    adja_matr[0][0] = 0\n    adja_matr[0][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][0] = 0\n\n    assert flag_bits[0] == 100 or flag_bits[0] == 101\n\n    if (flag_bits[0] == 101) and stroke_len >= 2:\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n\n        assert flag_bits[idx] == 100 or flag_bits[idx] == 101\n\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 101):\n            adja_matr[idx][idx - 1] = 0\n\n        if (idx == stroke_len - 1):\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 101):\n            adja_matr[idx][idx + 1] = 0\n\n\n    return adja_matr\n\n\n\n\n'''\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n'''\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_2nn4nnjnn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        # \n        attention_mask_2_neighbors = produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len)\n\n        attention_mask_4_neighbors = produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len)\n\n        attention_mask_joint_neighbors = produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len)\n\n        \n        \n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_2_neighbors, attention_mask_4_neighbors, attention_mask_joint_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_2nnjnn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\n\ndef produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n  \n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        if (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n\n    return adja_matr\n\n\n\n\ndef produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n    \n    adja_matr[0][0] = 0\n    adja_matr[0][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][0] = 0\n\n    assert flag_bits[0] == 100 or flag_bits[0] == 101\n\n    if (flag_bits[0] == 101) and stroke_len >= 2:\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n\n        assert flag_bits[idx] == 100 or flag_bits[idx] == 101\n\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 101):\n            adja_matr[idx][idx - 1] = 0\n\n        if (idx == stroke_len - 1):\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 101):\n            adja_matr[idx][idx + 1] = 0\n\n\n    return adja_matr\n\n\n\n\n'''\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n'''\n\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_2nnjnn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        # \n        attention_mask_2_neighbors = produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len)\n\n        \n\n        attention_mask_joint_neighbors = produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len)\n\n        \n        \n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_2_neighbors, attention_mask_joint_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_4nn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\ndef produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n        # \n        if (flag_bits[1] == 100):\n            adja_matr[0][2] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n            # \n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\n                adja_matr[idx][idx - 2] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n            # \n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\n                adja_matr[idx][idx + 2] = 0\n\n    return adja_matr\n\n\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_4nn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        \n        \n        attention_mask_4_neighbors = produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len)\n        \n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_4_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_4nnjnn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\ndef produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n        # \n        if (flag_bits[1] == 100):\n            adja_matr[0][2] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n            # \n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\n                adja_matr[idx][idx - 2] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n            # \n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\n                adja_matr[idx][idx + 2] = 0\n\n    return adja_matr\n\n\ndef produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n    \n    adja_matr[0][0] = 0\n    adja_matr[0][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][0] = 0\n\n    assert flag_bits[0] == 100 or flag_bits[0] == 101\n\n    if (flag_bits[0] == 101) and stroke_len >= 2:\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n\n        assert flag_bits[idx] == 100 or flag_bits[idx] == 101\n\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 101):\n            adja_matr[idx][idx - 1] = 0\n\n        if (idx == stroke_len - 1):\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 101):\n            adja_matr[idx][idx + 1] = 0\n\n\n    return adja_matr\n\n\n\n\n'''\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n'''\n\n\n\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_4nnjnn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        # \n\n        attention_mask_4_neighbors = produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len)\n\n        attention_mask_joint_neighbors = produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len)\n\n        \n        \n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_4_neighbors, attention_mask_joint_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_6nn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\ndef produce_adjacent_matrix_6_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n\n    adja_matr[0][0] = 0\n    # TODO\n    if (flag_bits[0] == 100):\n        adja_matr[0][1] = 0\n        # \n        if (flag_bits[1] == 100):\n            adja_matr[0][2] = 0\n            if (flag_bits[2] == 100):\n                adja_matr[0][3] = 0\n\n\n    for idx in range(1, stroke_len):\n        #\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 100):\n            adja_matr[idx][idx - 1] = 0\n            # \n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\n                adja_matr[idx][idx - 2] = 0\n                if (idx >= 3) and (flag_bits[idx - 3] == 100):\n                    adja_matr[idx][idx - 3] = 0\n\n        if idx == stroke_len - 1:\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\n            adja_matr[idx][idx + 1] = 0\n            # \n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\n                adja_matr[idx][idx + 2] = 0\n                if (idx <= (stroke_len - 4)) and (flag_bits[idx + 2] == 100):\n                    adja_matr[idx][idx + 3] = 0\n\n    return adja_matr\n\n\n'''\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n'''\n\n\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_6nn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        \n        attention_mask_6_neighbors = produce_adjacent_matrix_6_neighbors(flag_bits, stroke_len)\n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_6_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_bigru.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\ndef generate_attention_mask(stroke_length):\n    attention_mask = np.zeros([100, 100], int)\n    attention_mask[stroke_length: , :] = -1e8\n    attention_mask[:, stroke_length : ] = -1e8\n    return attention_mask\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        \n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n    \n        attention_mask = generate_attention_mask(stroke_len)\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        \n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_fully_connected_graph_attention_mask.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\ndef generate_attention_mask(stroke_length):\n    attention_mask = np.zeros([100, 100], int)\n    \n    attention_mask[stroke_length: , :] = -1e8\n    attention_mask[:, stroke_length : ] = -1e8\n    return attention_mask\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_fully_connected_graph_attmask(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        #\n        attention_mask = generate_attention_mask(stroke_len)\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_fully_connected_stroke_attention_mask.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\n\ndef detect_ending_points(flag_bits, stroke_len):\n\n    assert flag_bits.shape == (100, 1)\n    ending_points_index_array = []\n    # \n    ending_points_index_array.append(-1)\n    \n    for idx in range(stroke_len):\n        # \n        if (flag_bits[idx] == 101):\n            ending_points_index_array.append(idx)\n\n    # \n    if stroke_len == 100:\n\n        assert flag_bits[99] == 100 or flag_bits[99] == 101\n\n        if flag_bits[99] == 100:\n            ending_points_index_array.append(99)\n\n    return ending_points_index_array\n\ndef produce_adjacent_matrix(ending_points_index_array):\n    adja_matr = np.zeros([100, 100], int)\n    \n    adja_matr[ : ][ : ] = -1e8\n    for idx in range(1, len(ending_points_index_array)):\n        start_index = ending_points_index_array[idx - 1] + 1\n        end_index = ending_points_index_array[idx]\n        # \n        if end_index == 99:\n            adja_matr[start_index : , start_index : ] = 0\n        else:\n            adja_matr[start_index : end_index + 1, start_index: end_index + 1] = 0\n\n    return adja_matr\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_fully_connected_stroke_attmask(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        #\n        ending_points_index_array = detect_ending_points(flag_bits, stroke_len)\n        attention_mask = produce_adjacent_matrix(ending_points_index_array)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_jnn.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\n\n\n\ndef produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len):\n    assert flag_bits.shape == (100, 1)\n    adja_matr = np.zeros([100, 100], int)\n    adja_matr[ : ][ : ] = -1e10\n    \n    adja_matr[0][0] = 0\n    adja_matr[0][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][stroke_len - 1] = 0\n    adja_matr[stroke_len - 1][0] = 0\n\n    assert flag_bits[0] == 100 or flag_bits[0] == 101\n\n    if (flag_bits[0] == 101) and stroke_len >= 2:\n        adja_matr[0][1] = 0\n\n\n    for idx in range(1, stroke_len):\n\n        assert flag_bits[idx] == 100 or flag_bits[idx] == 101\n\n        adja_matr[idx][idx] = 0\n\n        if (flag_bits[idx - 1] == 101):\n            adja_matr[idx][idx - 1] = 0\n\n        if (idx == stroke_len - 1):\n            break\n\n        # \n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 101):\n            adja_matr[idx][idx + 1] = 0\n\n\n    return adja_matr\n\n\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_jnn(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        \n        attention_mask_joint_neighbors = produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len)\n        # check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask_joint_neighbors, padding_mask, position_encoding)\n\n    \n"""
dataloader/QuickdrawDataset4dict_random_attention_mask.py,1,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport time\nimport numpy as np\n\n\n\ndef produce_adjacent_matrix_random(stroke_length, conn=0.15):\n    attention_mask = np.random.choice(a=[0, -1e8], size=[100, 100], p=[conn, 1-conn])\n    attention_mask[stroke_length: , :] = -1e8\n    attention_mask[:, stroke_length : ] = -1e8\n    #####\n    attention_mask = np.triu(attention_mask)\n    attention_mask += attention_mask.T - np.diag(attention_mask.diagonal())\n    \n    for i in range(stroke_length):\n        attention_mask[i, i] = 0\n    return attention_mask\n\n\n\ndef check_adjacent_matrix(adjacent_matrix, stroke_len):\n    assert adjacent_matrix.shape == (100, 100)\n    for idx in range(1, stroke_len):\n        assert adjacent_matrix[idx][idx - 1] == adjacent_matrix[idx - 1][idx]\n\n\n\ndef generate_padding_mask(stroke_length):\n    padding_mask = np.ones([100, 1], int)\n    padding_mask[stroke_length: , : ] = 0\n    return padding_mask\n\n\n\nclass QuickdrawDataset_random_attmask(data.Dataset):\n\n    def __init__(self, coordinate_path_root, sketch_list, data_dict, non_zero_ratio):\n        with open(sketch_list) as sketch_url_file:\n            sketch_url_list = sketch_url_file.readlines()\n            \n            self.coordinate_urls = [os.path.join(coordinate_path_root, (sketch_url.strip(\n            ).split(' ')[0]).replace('png', 'npy')) for sketch_url in sketch_url_list]\n            \n            self.labels = [int(sketch_url.strip().split(' ')[-1])\n                           for sketch_url in sketch_url_list]\n\n            self.data_dict = data_dict\n\n            self.non_zero_ratio = non_zero_ratio\n        \n\n    def __len__(self):\n        return len(self.coordinate_urls)\n\n    def __getitem__(self, item):\n        \n        \n        coordinate_url = self.coordinate_urls[item]\n        label = self.labels[item]\n        \n        #\n        coordinate, flag_bits, stroke_len = self.data_dict[coordinate_url]\n\n        #\n        \n        attention_mask = produce_adjacent_matrix_random(stroke_len, self.non_zero_ratio)\n        check_adjacent_matrix(attention_mask, stroke_len)\n\n        padding_mask = generate_padding_mask(stroke_len)\n\n        position_encoding = np.arange(100)\n        position_encoding.resize([100, 1])\n\n        if coordinate.dtype == 'object':\n            coordinate = coordinate[0]\n        \n        assert coordinate.shape == (100, 2)\n        \n        coordinate = coordinate.astype('float32') \n        # print(type(coordinate), type(label), type(flag_bits), type(stroke_len), type(attention_mask), type(padding_mask), type(position_encoding))\n        return (coordinate, label, flag_bits.astype('int'), stroke_len, attention_mask, padding_mask, position_encoding)\n\n    \n"""
dataloader/__init__.py,0,b'\n'
network/Bidirectional_GRU.py,5,"b""import os\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport ipdb\nimport matplotlib.pyplot as plt\nfrom torch.optim import lr_scheduler\n\n\n\n\n\n\n\nclass GRUNet(nn.Module):\n\n    def __init__(self, network_configs):\n\n        super(GRUNet, self).__init__()\n\n        self.coord_embed = nn.Linear(network_configs['coord_input_dim'], network_configs['embed_dim'], bias=False)\n        self.feat_embed = nn.Embedding(network_configs['feat_dict_size'], network_configs['embed_dim'])\n        self.hidden_size = network_configs['hidden_size']\n        self.gru = nn.GRU(input_size= network_configs['embed_dim'], hidden_size= network_configs['hidden_size'], num_layers= network_configs['num_layers'], batch_first=True, dropout=network_configs['dropout'], bidirectional=True)\n        self.out_layer = nn.Linear(network_configs['hidden_size'] * 2, network_configs['num_classes'])\n\n    def forward(self, coordinate, flag_bits, position_encoding):\n\n        x = self.coord_embed(coordinate) + self.feat_embed(flag_bits) + self.feat_embed(position_encoding)\n\n\n        \n        self.rnn_hidden_feature, h = self.gru(x)\n        featur = torch.cat(( self.rnn_hidden_feature[:, -1, : self.hidden_size], self.rnn_hidden_feature[:, -1, self.hidden_size: ]), 1)\n        x = self.out_layer(featur)\n        return x, featur\n"""
network/__init__.py,0,b'\n'
network/gra_transf_inpt5_new_dropout_2layerMLP.py,5,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .graph_transformer_layers_new_dropout import *\n\nimport ipdb\n\n\nclass GraphTransformerEncoder(nn.Module):\n    \n    def __init__(self, coord_input_dim, feat_input_dim, feat_dict_size, n_layers=6, n_heads=8, \n                 embed_dim=512, feedforward_dim=2048, normalization=\'batch\', dropout=0.1):         \n        \n        super(GraphTransformerEncoder, self).__init__()\n        \n        # Embedding/Input layers\n        self.coord_embed = nn.Linear(coord_input_dim, embed_dim, bias=False)\n        self.feat_embed = nn.Embedding(feat_dict_size, embed_dim)\n        #self.in_drop = nn.Dropout(dropout)\n        \n        # Transformer blocks\n        self.transformer_layers = nn.ModuleList([\n            GraphTransformerLayer(n_heads, embed_dim * 3, feedforward_dim, normalization, dropout) \n                for _ in range(n_layers)\n        ])\n\n    def forward(self, coord, flag, pos, attention_mask=None):\n        \n        # Embed inputs to embed_dim\n        #h = self.coord_embed(coord) + self.feat_embed(flag) + self.feat_embed(pos)\n        h = torch.cat((self.coord_embed(coord), self.feat_embed(flag)), dim=2)\n        h = torch.cat((h, self.feat_embed(pos)), dim=2)\n        #h = self.in_drop(h)\n        \n        # Perform n_layers of Graph Transformer blocks\n        for layer in self.transformer_layers:\n            h = layer(h, mask=attention_mask)\n        \n        return h\n    \n\n# modified on 2019 10 23.\nclass GraphTransformerClassifier(nn.Module):\n    \n    def __init__(self, n_classes, coord_input_dim, feat_input_dim, feat_dict_size, \n                 n_layers=6, n_heads=8, embed_dim=512, feedforward_dim=2048, \n                 normalization=\'batch\', dropout=0.1, mlp_classifier_dropout = 0.1):\n        \n        super(GraphTransformerClassifier, self).__init__()\n        \n        self.encoder = GraphTransformerEncoder(\n            coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n            n_heads, embed_dim, feedforward_dim, normalization, dropout)\n        \n        self.mlp_classifier = nn.Sequential(\n            nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(embed_dim * 3, feedforward_dim, bias=True),\n            nn.ReLU(),\n            # TODO\n            nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(feedforward_dim, feedforward_dim, bias=True),\n            nn.ReLU(),\n            #nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(feedforward_dim, n_classes, bias=True)\n        )\n        \n        # self.g1 = nn.Linear(embed_dim, embed_dim, bias=False)\n        # self.g2 = nn.Linear(embed_dim, embed_dim, bias=False)\n    \n    def forward(self, coord, flag, pos, attention_mask=None, \n                padding_mask=None, true_seq_length=None):\n        """"""\n        Args:\n            coord: Input coordinates (batch_size, seq_length, coord_input_dim)\n            # TODO feat: Input features (batch_size, seq_length, feat_input_dim)\n            attention_mask: Masks for attention computation (batch_size, seq_length, seq_length)\n                            Attention mask should contain -inf if attention is not possible \n                            (i.e. mask is a negative adjacency matrix)\n            padding_mask: Mask indicating padded elements in input (batch_size, seq_length)\n                          Padding mask element should be 1 if valid element, 0 if padding\n                          (i.e. mask is a boolean multiplicative mask)\n            true_seq_length: True sequence lengths for input (batch_size, )\n                             Used for computing true mean of node embeddings for graph embedding\n        \n        Returns:\n            logits: Un-normalized logits for class prediction (batch_size, n_classes)\n        """"""\n        \n        # Embed input sequence\n        h = self.encoder(coord, flag, pos, attention_mask)\n        \n        # h = torch.sigmoid(self.g1(h)) * self.g2(h)\n        \n        # Mask out padding embeddings to zero\n        if padding_mask is not None:\n            masked_h = h * padding_mask.type_as(h)\n            g = masked_h.sum(dim = 1)\n            # g = masked_h.sum(dim=1)/true_seq_length.type_as(h)\n            \n        else:\n            g = h.sum(dim=1)\n        \n        # Compute logits\n        logits = self.mlp_classifier(g)\n        \n        return logits\n \n    \ndef make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=104, \n               n_layers=6, n_heads=8, embed_dim=512, feedforward_dim=2048, \n               normalization=\'batch\', dropout=0.1, mlp_classifier_dropout = 0.1):\n    \n    model = GraphTransformerClassifier(\n        n_classes, coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n        n_heads, embed_dim, feedforward_dim, normalization, dropout, mlp_classifier_dropout)\n    \n    print(model)\n    nb_param = 0\n    for param in model.parameters():\n        nb_param += np.prod(list(param.data.size()))\n    print(\'Number of parameters: \', nb_param)\n\n    return model\n'"
network/gra_transf_inpt5_new_dropout_2layerMLP_2_adj_mtx.py,5,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .graph_transformer_layers_new_dropout_2_adj_mtx import *\n\nimport ipdb\n\n\n\nclass GraphTransformerEncoder(nn.Module):\n    \n    def __init__(self, coord_input_dim, feat_input_dim, feat_dict_size, n_layers=6, n_heads=8, \n                 embed_dim=512, feedforward_dim=2048, normalization=\'batch\', dropout=0.1):         \n        \n        super(GraphTransformerEncoder, self).__init__()\n        \n        # Embedding/Input layers\n        self.coord_embed = nn.Linear(coord_input_dim, embed_dim, bias=False)\n        self.feat_embed = nn.Embedding(feat_dict_size, embed_dim)\n        #self.in_drop = nn.Dropout(dropout)\n        \n        # Transformer blocks\n        self.transformer_layers = nn.ModuleList([\n            MultiGraphTransformerLayer(n_heads, embed_dim * 3, feedforward_dim, normalization, dropout) \n                for _ in range(n_layers)\n        ])\n\n    def forward(self, coord, flag, pos, attention_mask1=None, attention_mask2=None):\n        \n        # Embed inputs to embed_dim\n        #h = self.coord_embed(coord) + self.feat_embed(flag) + self.feat_embed(pos)\n        h = torch.cat((self.coord_embed(coord), self.feat_embed(flag)), dim=2)\n        h = torch.cat((h, self.feat_embed(pos)), dim=2)\n        #h = self.in_drop(h)\n        \n        # Perform n_layers of Graph Transformer blocks\n        for layer in self.transformer_layers:\n            h = layer(h, mask1=attention_mask1, mask2=attention_mask2)\n        \n        return h\n    \n\n\nclass GraphTransformerClassifier(nn.Module):\n    \n    def __init__(self, n_classes, coord_input_dim, feat_input_dim, feat_dict_size, \n                 n_layers=6, n_heads=8, embed_dim=512, feedforward_dim=2048, \n                 normalization=\'batch\', dropout=0.1, mlp_classifier_dropout = 0.1):\n        \n        super(GraphTransformerClassifier, self).__init__()\n        \n        self.encoder = GraphTransformerEncoder(\n            coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n            n_heads, embed_dim, feedforward_dim, normalization, dropout)\n        \n        self.mlp_classifier = nn.Sequential(\n            nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(embed_dim * 3, feedforward_dim, bias=True),\n            nn.ReLU(),\n            # TODO\n            nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(feedforward_dim, feedforward_dim, bias=True),\n            nn.ReLU(),\n            #nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(feedforward_dim, n_classes, bias=True)\n        )\n        \n        # self.g1 = nn.Linear(embed_dim, embed_dim, bias=False)\n        # self.g2 = nn.Linear(embed_dim, embed_dim, bias=False)\n    \n    def forward(self, coord, flag, pos, attention_mask1=None, attention_mask2=None,\n                padding_mask=None, true_seq_length=None):\n        """"""\n        Args:\n            coord: Input coordinates (batch_size, seq_length, coord_input_dim)\n            # TODO feat: Input features (batch_size, seq_length, feat_input_dim)\n            attention_mask: Masks for attention computation (batch_size, seq_length, seq_length)\n                            Attention mask should contain -inf if attention is not possible \n                            (i.e. mask is a negative adjacency matrix)\n            padding_mask: Mask indicating padded elements in input (batch_size, seq_length)\n                          Padding mask element should be 1 if valid element, 0 if padding\n                          (i.e. mask is a boolean multiplicative mask)\n            true_seq_length: True sequence lengths for input (batch_size, )\n                             Used for computing true mean of node embeddings for graph embedding\n        \n        Returns:\n            logits: Un-normalized logits for class prediction (batch_size, n_classes)\n        """"""\n        \n        # Embed input sequence\n        h = self.encoder(coord, flag, pos, attention_mask1, attention_mask2)\n        \n        # h = torch.sigmoid(self.g1(h)) * self.g2(h)\n        \n        # Mask out padding embeddings to zero\n        if padding_mask is not None:\n            masked_h = h * padding_mask.type_as(h)\n            g = masked_h.sum(dim = 1)\n            # g = masked_h.sum(dim=1)/true_seq_length.type_as(h)\n            \n        else:\n            g = h.sum(dim=1)\n        \n        # Compute logits\n        logits = self.mlp_classifier(g)\n        \n        return logits\n \n    \ndef make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=104, \n               n_layers=6, n_heads=8, embed_dim=512, feedforward_dim=2048, \n               normalization=\'batch\', dropout=0.1, mlp_classifier_dropout = 0.1):\n    \n    model = GraphTransformerClassifier(\n        n_classes, coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n        n_heads, embed_dim, feedforward_dim, normalization, dropout, mlp_classifier_dropout)\n    \n    print(model)\n    nb_param = 0\n    for param in model.parameters():\n        nb_param += np.prod(list(param.data.size()))\n    print(\'Number of parameters: \', nb_param)\n\n    return model\n'"
network/gra_transf_inpt5_new_dropout_2layerMLP_3_adj_mtx.py,5,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .graph_transformer_layers_new_dropout_3_adj_mtx import *\n\nimport ipdb\n\n\nclass GraphTransformerEncoder(nn.Module):\n    \n    def __init__(self, coord_input_dim, feat_input_dim, feat_dict_size, n_layers=6, n_heads=8, \n                 embed_dim=512, feedforward_dim=2048, normalization=\'batch\', dropout=0.1):         \n        \n        super(GraphTransformerEncoder, self).__init__()\n        \n        # Embedding/Input layers\n        self.coord_embed = nn.Linear(coord_input_dim, embed_dim, bias=False)\n        self.feat_embed = nn.Embedding(feat_dict_size, embed_dim)\n        #self.in_drop = nn.Dropout(dropout)\n        \n        # Transformer blocks\n        self.transformer_layers = nn.ModuleList([\n            MultiGraphTransformerLayer(n_heads, embed_dim * 3, feedforward_dim, normalization, dropout) \n                for _ in range(n_layers)\n        ])\n\n    def forward(self, coord, flag, pos, attention_mask1=None, attention_mask2=None, attention_mask3=None):\n        \n        # Embed inputs to embed_dim\n        #h = self.coord_embed(coord) + self.feat_embed(flag) + self.feat_embed(pos)\n        h = torch.cat((self.coord_embed(coord), self.feat_embed(flag)), dim=2)\n        h = torch.cat((h, self.feat_embed(pos)), dim=2)\n        #h = self.in_drop(h)\n        \n        # Perform n_layers of Graph Transformer blocks\n        for layer in self.transformer_layers:\n            h = layer(h, mask1=attention_mask1, mask2=attention_mask2, mask3=attention_mask3)\n        \n        return h\n    \n\n\nclass GraphTransformerClassifier(nn.Module):\n    \n    def __init__(self, n_classes, coord_input_dim, feat_input_dim, feat_dict_size, \n                 n_layers=6, n_heads=8, embed_dim=512, feedforward_dim=2048, \n                 normalization=\'batch\', dropout=0.1, mlp_classifier_dropout = 0.1):\n        \n        super(GraphTransformerClassifier, self).__init__()\n        \n        self.encoder = GraphTransformerEncoder(\n            coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n            n_heads, embed_dim, feedforward_dim, normalization, dropout)\n        \n        self.mlp_classifier = nn.Sequential(\n            nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(embed_dim * 3, feedforward_dim, bias=True),\n            nn.ReLU(),\n            # TODO\n            nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(feedforward_dim, feedforward_dim, bias=True),\n            nn.ReLU(),\n            #nn.Dropout(mlp_classifier_dropout),\n            nn.Linear(feedforward_dim, n_classes, bias=True)\n        )\n        \n        # self.g1 = nn.Linear(embed_dim, embed_dim, bias=False)\n        # self.g2 = nn.Linear(embed_dim, embed_dim, bias=False)\n    \n    def forward(self, coord, flag, pos, attention_mask1=None, attention_mask2=None, attention_mask3=None,\n                padding_mask=None, true_seq_length=None):\n        """"""\n        Args:\n            coord: Input coordinates (batch_size, seq_length, coord_input_dim)\n            # TODO feat: Input features (batch_size, seq_length, feat_input_dim)\n            attention_mask: Masks for attention computation (batch_size, seq_length, seq_length)\n                            Attention mask should contain -inf if attention is not possible \n                            (i.e. mask is a negative adjacency matrix)\n            padding_mask: Mask indicating padded elements in input (batch_size, seq_length)\n                          Padding mask element should be 1 if valid element, 0 if padding\n                          (i.e. mask is a boolean multiplicative mask)\n            true_seq_length: True sequence lengths for input (batch_size, )\n                             Used for computing true mean of node embeddings for graph embedding\n        \n        Returns:\n            logits: Un-normalized logits for class prediction (batch_size, n_classes)\n        """"""\n        \n        # Embed input sequence\n        h = self.encoder(coord, flag, pos, attention_mask1, attention_mask2, attention_mask3)\n        \n        # h = torch.sigmoid(self.g1(h)) * self.g2(h)\n        \n        # Mask out padding embeddings to zero\n        if padding_mask is not None:\n            masked_h = h * padding_mask.type_as(h)\n            g = masked_h.sum(dim = 1)\n            # g = masked_h.sum(dim=1)/true_seq_length.type_as(h)\n            \n        else:\n            g = h.sum(dim=1)\n        \n        # Compute logits\n        logits = self.mlp_classifier(g)\n        \n        return logits\n \n    \ndef make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=104, \n               n_layers=6, n_heads=8, embed_dim=512, feedforward_dim=2048, \n               normalization=\'batch\', dropout=0.1, mlp_classifier_dropout = 0.1):\n    \n    model = GraphTransformerClassifier(\n        n_classes, coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n        n_heads, embed_dim, feedforward_dim, normalization, dropout, mlp_classifier_dropout)\n    \n    print(model)\n    nb_param = 0\n    for param in model.parameters():\n        nb_param += np.prod(list(param.data.size()))\n    print(\'Number of parameters: \', nb_param)\n\n    return model\n'"
network/graph_attention_net.py,3,"b'import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .graph_transformer_layers_new_dropout import *\n\n\nclass GraphAttentionLayer(nn.Module):\n\n    def __init__(self, n_heads, embed_dim, feedforward_dim, \n                 normalization=\'batch\', dropout=0.1):\n        super(GraphAttentionLayer, self).__init__()\n        \n        self.self_attention = SkipConnection(\n            MultiHeadAttention(\n                    n_heads=n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim,\n                    dropout=dropout\n                )\n            )\n        self.norm = Normalization(embed_dim, normalization)\n        \n    def forward(self, input, mask):\n        h = F.relu(self.self_attention(input, mask=mask))\n        h = self.norm(h, mask=mask)\n        return h\n\n\nclass GraphAttentionEncoder(nn.Module):\n    \n    def __init__(self, coord_input_dim, feat_input_dim, feat_dict_size, n_layers=3, n_heads=8, \n                 embed_dim=256, feedforward_dim=1024, normalization=\'batch\', dropout=0.1):         \n        \n        super(GraphAttentionEncoder, self).__init__()\n        \n        # Embedding/Input layers\n        self.coord_embed = nn.Linear(coord_input_dim, embed_dim, bias=False)\n        self.feat_embed = nn.Embedding(feat_dict_size, embed_dim)\n        \n        # Transformer blocks\n        self.attention_layers = nn.ModuleList([\n            GraphAttentionLayer(n_heads, embed_dim * 3, feedforward_dim, normalization, dropout) \n                for _ in range(n_layers)\n        ])\n\n    def forward(self, coord, flag, pos, attention_mask=None):\n        \n        # Embed inputs to embed_dim\n        h = torch.cat((self.coord_embed(coord), self.feat_embed(flag), self.feat_embed(pos)), dim=2)\n        \n        # Perform n_layers of Graph Attention blocks\n        for layer in self.attention_layers:\n            h = layer(h, mask=attention_mask)\n        \n        return h\n\n\nclass GraphAttentionClassifier(nn.Module):\n    \n    def __init__(self, n_classes, coord_input_dim, feat_input_dim, feat_dict_size, \n                 n_layers=3, n_heads=8, embed_dim=256, feedforward_dim=1024, \n                 normalization=\'batch\', dropout=0.1):\n        \n        super(GraphAttentionClassifier, self).__init__()\n        \n        self.encoder = GraphAttentionEncoder(\n            coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n            n_heads, embed_dim, feedforward_dim, normalization, dropout)\n        \n        self.mlp_classifier = nn.Sequential(\n            nn.Linear(embed_dim * 3, feedforward_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(feedforward_dim, feedforward_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(feedforward_dim, n_classes, bias=True)\n        )\n    \n    def forward(self, coord, flag, pos, attention_mask=None,\n                padding_mask=None, true_seq_length=None):\n        """"""\n        Args:\n            coord: Input coordinates (batch_size, seq_length, coord_input_dim)\n            # TODO feat: Input features (batch_size, seq_length, feat_input_dim)\n            attention_mask: Masks for attention computation (batch_size, seq_length, seq_length)\n                            Attention mask should contain -inf if attention is not possible \n                            (i.e. mask is a negative adjacency matrix)\n            padding_mask: Mask indicating padded elements in input (batch_size, seq_length)\n                          Padding mask element should be 1 if valid element, 0 if padding\n                          (i.e. mask is a boolean multiplicative mask)\n            true_seq_length: True sequence lengths for input (batch_size, )\n                             Used for computing true mean of node embeddings for graph embedding\n        \n        Returns:\n            logits: Un-normalized logits for class prediction (batch_size, n_classes)\n        """"""\n        \n        # Embed input sequence\n        h = self.encoder(coord, flag, pos, attention_mask)\n                \n        # Mask out padding embeddings to zero\n        if padding_mask is not None:\n            masked_h = h * padding_mask.type_as(h)\n            g = masked_h.sum(dim = 1)\n            # g = masked_h.sum(dim=1)/true_seq_length.type_as(h)\n            \n        else:\n            g = h.sum(dim=1)\n        \n        # Compute logits\n        logits = self.mlp_classifier(g)\n        \n        return logits\n \n    \ndef make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=104, \n               n_layers=3, n_heads=8, embed_dim=256, feedforward_dim=1024, \n               normalization=\'batch\', dropout=0.1):\n    \n    model = GraphAttentionClassifier(\n        n_classes, coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n        n_heads, embed_dim, feedforward_dim, normalization, dropout)\n    \n    print(model)\n    nb_param = 0\n    for param in model.parameters():\n        nb_param += np.prod(list(param.data.size()))\n    print(\'Number of parameters: \', nb_param)\n\n    return model\n'"
network/graph_conv_net.py,3,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import nn\nimport math\n\n\nclass SkipConnection(nn.Module):\n\n    def __init__(self, module):\n        super(SkipConnection, self).__init__()\n        self.module = module\n\n    def forward(self, input, mask=None):\n        return input + self.module(input, mask=mask)\n    \n\nclass Normalization(nn.Module):\n\n    def __init__(self, embed_dim, normalization=\'layer\'):\n        super(Normalization, self).__init__()\n\n        self.normalizer = {\n            \'layer\': nn.LayerNorm(embed_dim, )\n            \'batch\': nn.BatchNorm1d(embed_dim, affine=True, track_running_stats=True),\n            \'instance\': nn.InstanceNorm1d(embed_dim, affine=True, track_running_stats=True)\n        }.get(normalization, None)\n\n        # Normalization by default initializes affine parameters \n        # with bias 0 and weight unif(0,1) which is too large!\n        self.init_parameters()\n\n    def init_parameters(self):\n        for name, param in self.named_parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, mask=None):\n        if isinstance(self.normalizer, nn.BatchNorm1d):\n            return self.normalizer(input.view(-1, input.size(-1))).view(*input.size())\n        elif isinstance(self.normalizer, nn.InstanceNorm1d):\n            return self.normalizer(input.permute(0, 2, 1)).permute(0, 2, 1)\n        else:\n            assert self.normalizer is None, ""Unknown normalizer type""\n            return input\n\n\nclass NodeFeatures(nn.Module):\n    """"""Convnet features for nodes\n    """"""\n    \n    def __init__(self, embed_dim, normalization=\'batch\', dropout=0.1):\n        super(NodeFeatures, self).__init__()\n        self.U = nn.Linear(embed_dim, embed_dim, True)\n        self.V = nn.Linear(embed_dim, embed_dim, True)\n        self.drop = nn.Dropout(dropout)\n        \n        self.init_parameters()\n        \n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, mask=None):\n        """"""\n        Args:\n            x: Node features (batch_size, num_nodes, embed_dim)\n        Returns:\n            x_new: Convolved node features (batch_size, num_nodes, embed_dim)\n        """"""\n        num_nodes, embed_dim = x.shape[1], x.shape[2]\n        \n        Ux = self.U(x)  # B x V x H\n        Vx = self.V(x)  # B x V x H\n        \n        # extend Vx from ""B x V x H"" to ""B x V x V x H""\n        Vx = Vx.unsqueeze(1).expand(-1, num_nodes, -1, -1)\n        if mask is not None:\n            mask = mask.unsqueeze(-1)\n            Vx = Vx * mask.type_as(Vx)\n            \n        x_new = Ux + torch.sum(Vx, dim=2)  # B x V x H\n        \n        x_new = F.relu(x_new)\n        x_new = self.drop(x_new)\n        \n        return x_new\n\n\nclass GraphConvNetLayer(nn.Module):\n    """"""Graph Convnet layer\n    """"""\n\n    def __init__(self, embed_dim, normalization=\'batch\', dropout=0.1):\n        super(GraphConvNetLayer, self).__init__()\n        self.node_feat = SkipConnection(\n            NodeFeatures(embed_dim, normalization, dropout)\n        )\n        self.norm = Normalization(embed_dim, normalization)\n\n    def forward(self, x, mask=None):\n        """"""\n        Args:\n            x: Node features (batch_size, num_nodes, embed_dim)\n        """"""\n        return self.norm(self.node_feat(x, mask=mask))\n\n\n\nclass GraphConvNetEncoder(nn.Module):\n    \n    def __init__(self, coord_input_dim, feat_input_dim, feat_dict_size, n_layers=3,\n                 embed_dim=256, normalization=\'batch\', dropout=0.1):         \n        \n        super(GraphConvNetEncoder, self).__init__()\n        \n        # Embedding/Input layers\n        self.coord_embed = nn.Linear(coord_input_dim, embed_dim, bias=False)\n        self.feat_embed = nn.Embedding(feat_dict_size, embed_dim)\n        \n        # GCN blocks\n        self.gcn_layers = nn.ModuleList([\n            GraphConvNetLayer(embed_dim * 3, normalization, dropout) \n                for _ in range(n_layers)\n        ])\n\n    def forward(self, coord, flag, pos, attention_mask=None, padding_mask=None):\n        \n        # Embed inputs to embed_dim\n        h = torch.cat((self.coord_embed(coord), self.feat_embed(flag), self.feat_embed(pos)), dim=2)\n        \n        # Perform n_layers of Graph ConvNet blocks\n        for layer in self.gcn_layers:\n            # Mask out padding embeddings to zero\n            if padding_mask is not None:\n                h = h * padding_mask.type_as(h)\n            \n            h = layer(h, mask=attention_mask)\n        \n        return h\n\n\nclass GraphConvNetClassifier(nn.Module):\n    \n    def __init__(self, n_classes, coord_input_dim, feat_input_dim, feat_dict_size, \n                 n_layers=3, embed_dim=256, feedforward_dim=1024, \n                 normalization=\'batch\', dropout=0.1):\n        \n        super(GraphConvNetClassifier, self).__init__()\n        \n        self.encoder = GraphConvNetEncoder(\n            coord_input_dim, feat_input_dim, feat_dict_size, \n            n_layers, embed_dim, normalization, dropout)\n        \n        self.mlp_classifier = nn.Sequential(\n            nn.Linear(embed_dim * 3, feedforward_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(feedforward_dim, feedforward_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(feedforward_dim, n_classes, bias=True)\n        )\n\n    def _mask_to_adj(A):\n        """"""Helper function to convert adjacency mask into adjacency matrix format.\n        1 --> connection, 0 --> no connection\n        """"""\n        A[A==0] = 1\n        A[A<0] = -1e10\n        return A\n\n    \n    def forward(self, coord, flag, pos, attention_mask=None,\n                padding_mask=None, true_seq_length=None):\n        """"""\n        Args:\n            coord: Input coordinates (batch_size, seq_length, coord_input_dim)\n            # TODO feat: Input features (batch_size, seq_length, feat_input_dim)\n            attention_mask: Masks for attention computation (batch_size, seq_length, seq_length)\n                            Attention mask should contain -inf if attention is not possible \n                            (i.e. mask is a negative adjacency matrix)\n            padding_mask: Mask indicating padded elements in input (batch_size, seq_length)\n                          Padding mask element should be 1 if valid element, 0 if padding\n                          (i.e. mask is a boolean multiplicative mask)\n            true_seq_length: True sequence lengths for input (batch_size, )\n                             Used for computing true mean of node embeddings for graph embedding\n        \n        Returns:\n            logits: Un-normalized logits for class prediction (batch_size, n_classes)\n        """"""\n        if attention_mask is not None:\n            attention_mask = self._mask_to_adj(attention_mask)\n\n        # Embed input sequence\n        h = self.encoder(coord, flag, pos, attention_mask, padding_mask)\n                \n        # Mask out padding embeddings to zero\n        if padding_mask is not None:\n            masked_h = h * padding_mask.type_as(h)\n            g = masked_h.sum(dim=1)\n            # g = masked_h.sum(dim=1)/true_seq_length.type_as(h)\n            \n        else:\n            g = h.sum(dim=1)\n        \n        # Compute logits\n        logits = self.mlp_classifier(g)\n        \n        return logits\n \n    \ndef make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=104, \n               n_layers=3, n_heads=8, embed_dim=256, feedforward_dim=1024, \n               normalization=\'batch\', dropout=0.1):\n    \n    model = GraphConvNetClassifier(\n        n_classes, coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n        embed_dim, feedforward_dim, normalization, dropout)\n    \n    print(model)\n    nb_param = 0\n    for param in model.parameters():\n        nb_param += np.prod(list(param.data.size()))\n    print(\'Number of parameters: \', nb_param)\n\n    return model\n\n'"
network/graph_mlp_net.py,3,"b'import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Normalization(nn.Module):\n\n    def __init__(self, embed_dim, normalization=\'batch\'):\n        super(Normalization, self).__init__()\n\n        normalizer_class = {\n            \'batch\': nn.BatchNorm1d,\n            \'instance\': nn.InstanceNorm1d\n        }.get(normalization, None)\n\n        self.normalizer = normalizer_class(embed_dim, affine=True)\n\n        # Normalization by default initializes affine parameters \n        # with bias 0 and weight unif(0,1) which is too large!\n        self.init_parameters()\n\n    def init_parameters(self):\n        for name, param in self.named_parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, mask=None):\n        if isinstance(self.normalizer, nn.BatchNorm1d):\n            return self.normalizer(input.view(-1, input.size(-1))).view(*input.size())\n        elif isinstance(self.normalizer, nn.InstanceNorm1d):\n            return self.normalizer(input.permute(0, 2, 1)).permute(0, 2, 1)\n        else:\n            assert self.normalizer is None, ""Unknown normalizer type""\n            return input\n\n\nclass GraphMLPLayer(nn.Module):\n\n    def __init__(self, embed_dim, dropout=0.1, normalization=\'batch\'):\n        super(GraphMLPLayer, self).__init__()\n        \n        self.sub_layers = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.norm = Normalization(embed_dim, normalization)\n        \n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n    \n    def forward(self, input):\n        return self.norm(self.sub_layers(input))\n\n\nclass GraphMLPEncoder(nn.Module):\n    \n    def __init__(self, coord_input_dim, feat_input_dim, feat_dict_size, \n                 n_layers=3, embed_dim=256, dropout=0.1):         \n        \n        super(GraphMLPEncoder, self).__init__()\n        \n        # Embedding/Input layers\n        self.coord_embed = nn.Linear(coord_input_dim, embed_dim, bias=False)\n        self.feat_embed = nn.Embedding(feat_dict_size, embed_dim)\n        \n        # MLP blocks\n        self.mlp_layers = nn.ModuleList([\n            GraphMLPLayer(embed_dim * 3, dropout) \n                for _ in range(n_layers)\n        ])\n        \n        self.init_parameters()\n        \n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, coord, flag, pos):\n        \n        # Embed inputs to embed_dim\n        h = torch.cat((self.coord_embed(coord), self.feat_embed(flag), self.feat_embed(pos)), dim=2)\n        \n        # Perform n_layers of Graph MLP blocks\n        for layer in self.mlp_layers:\n            h = layer(h)\n        \n        return h\n\n\nclass GraphMLPClassifier(nn.Module):\n    \n    def __init__(self, n_classes, coord_input_dim, feat_input_dim, feat_dict_size, \n                 n_layers=3, embed_dim=256, feedforward_dim=1024, dropout=0.1):\n        \n        super(GraphMLPClassifier, self).__init__()\n        \n        self.encoder = GraphMLPEncoder(\n            coord_input_dim, feat_input_dim, feat_dict_size, \n            n_layers, embed_dim, dropout)\n        \n        self.mlp_classifier = nn.Sequential(\n            nn.Linear(embed_dim * 3, feedforward_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(feedforward_dim, feedforward_dim, bias=True),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(feedforward_dim, n_classes, bias=True)\n        )\n    \n    def forward(self, coord, flag, pos, attention_mask=None, padding_mask=None, true_seq_length=None):\n        """"""\n        Args:\n            coord: Input coordinates (batch_size, seq_length, coord_input_dim)\n            # TODO feat: Input features (batch_size, seq_length, feat_input_dim)\n            attention_mask: Masks for attention computation (batch_size, seq_length, seq_length)\n                            Attention mask should contain -inf if attention is not possible \n                            (i.e. mask is a negative adjacency matrix)\n            padding_mask: Mask indicating padded elements in input (batch_size, seq_length)\n                          Padding mask element should be 1 if valid element, 0 if padding\n                          (i.e. mask is a boolean multiplicative mask)\n            true_seq_length: True sequence lengths for input (batch_size, )\n                             Used for computing true mean of node embeddings for graph embedding\n        \n        Returns:\n            logits: Un-normalized logits for class prediction (batch_size, n_classes)\n        """"""\n        \n        # Embed input sequence\n        h = self.encoder(coord, flag, pos)\n                \n        # Mask out padding embeddings to zero\n        if padding_mask is not None:\n            masked_h = h * padding_mask.type_as(h)\n            g = masked_h.sum(dim = 1)\n            # g = masked_h.sum(dim=1)/true_seq_length.type_as(h)\n            \n        else:\n            g = h.sum(dim=1)\n        \n        # Compute logits\n        logits = self.mlp_classifier(g)\n        \n        return logits\n \n    \ndef make_model(n_classes=345, coord_input_dim=2, feat_input_dim=2, feat_dict_size=104, \n               n_layers=3, n_heads=8, embed_dim=256, feedforward_dim=1024, \n               normalization=\'batch\', dropout=0.1):\n    \n    model = GraphMLPClassifier(\n        n_classes, coord_input_dim, feat_input_dim, feat_dict_size, n_layers, \n        embed_dim, feedforward_dim, dropout)\n    \n    print(model)\n    nb_param = 0\n    for param in model.parameters():\n        nb_param += np.prod(list(param.data.size()))\n    print(\'Number of parameters: \', nb_param)\n\n    return model\n'"
network/graph_transformer_layers_new_dropout.py,11,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import nn\nimport math\nimport ipdb\n\n\nclass SkipConnection(nn.Module):\n\n    def __init__(self, module):\n        super(SkipConnection, self).__init__()\n        self.module = module\n\n    def forward(self, input, mask=None):\n        return input + self.module(input, mask=mask)\n    \n\nclass Normalization(nn.Module):\n\n    def __init__(self, embed_dim, normalization=\'batch\'):\n        super(Normalization, self).__init__()\n\n        normalizer_class = {\n            \'batch\': nn.BatchNorm1d,\n            \'instance\': nn.InstanceNorm1d\n        }.get(normalization, None)\n\n        self.normalizer = normalizer_class(embed_dim, affine=True)\n\n        # Normalization by default initializes affine parameters \n        # with bias 0 and weight unif(0,1) which is too large!\n        self.init_parameters()\n\n    def init_parameters(self):\n        for name, param in self.named_parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, mask=None):\n        if isinstance(self.normalizer, nn.BatchNorm1d):\n            return self.normalizer(input.view(-1, input.size(-1))).view(*input.size())\n        elif isinstance(self.normalizer, nn.InstanceNorm1d):\n            return self.normalizer(input.permute(0, 2, 1)).permute(0, 2, 1)\n        else:\n            assert self.normalizer is None, ""Unknown normalizer type""\n            return input\n\n\nclass MultiHeadAttention(nn.Module):\n    \n    def __init__(self, n_heads, input_dim, embed_dim=None, \n                 val_dim=None, key_dim=None, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n\n        if val_dim is None:\n            assert embed_dim is not None, ""Provide either embed_dim or val_dim""\n            val_dim = embed_dim // n_heads\n        if key_dim is None:\n            key_dim = val_dim\n\n        self.n_heads = n_heads\n        self.input_dim = input_dim\n        self.embed_dim = embed_dim\n        self.val_dim = val_dim\n        self.key_dim = key_dim\n\n        self.norm_factor = 1 / math.sqrt(key_dim)\n\n        self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_val = nn.Parameter(torch.Tensor(n_heads, input_dim, val_dim))\n\n        if embed_dim is not None:\n            self.W_out = nn.Parameter(torch.Tensor(n_heads, key_dim, embed_dim))\n            \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, q, h=None, mask=None):\n        """"""\n        Args:\n            q: Input queries (batch_size, n_query, input_dim)\n            h: Input data (batch_size, graph_size, input_dim)\n            mask: Input attention mask (batch_size, n_query, graph_size)\n                  or viewable as that (i.e. can be 2 dim if n_query == 1);\n                  Mask should contain -inf if attention is not possible \n                  (i.e. mask is a negative adjacency matrix)\n        \n        Returns: \n            out: Updated data after attention (batch_size, graph_size, input_dim)\n        """"""\n        if h is None:\n            h = q  # compute self-attention\n\n        # h should be (batch_size, graph_size, input_dim)\n        batch_size, graph_size, input_dim = h.size()\n        n_query = q.size(1)\n        assert q.size(0) == batch_size\n        assert q.size(2) == input_dim\n        assert input_dim == self.input_dim, ""Wrong embedding dimension of input""\n\n        hflat = h.contiguous().view(-1, input_dim)\n        qflat = q.contiguous().view(-1, input_dim)\n\n        # last dimension can be different for keys and values\n        shp = (self.n_heads, batch_size, graph_size, -1)\n        shp_q = (self.n_heads, batch_size, n_query, -1)\n\n        # Calculate queries, (n_heads, n_query, graph_size, key/val_size)\n        dropt1_qflat = self.dropout_1(qflat)\n        Q = torch.matmul(dropt1_qflat, self.W_query).view(shp_q)\n\n        # Calculate keys and values (n_heads, batch_size, graph_size, key/val_size)\n        dropt2_hflat = self.dropout_2(hflat)\n        K = torch.matmul(dropt2_hflat, self.W_key).view(shp)\n\n        dropt3_hflat = self.dropout_3(hflat)\n        V = torch.matmul(dropt3_hflat, self.W_val).view(shp)\n\n        # Calculate compatibility (n_heads, batch_size, n_query, graph_size)\n        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n        \n        # Optionally apply mask to prevent attention\n        if mask is not None:\n            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n            compatibility = compatibility + mask.type_as(compatibility)\n\n        attn = F.softmax(compatibility, dim=-1)\n\n        heads = torch.matmul(attn, V)\n\n        out = torch.mm(\n            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.n_heads * self.val_dim),\n            self.W_out.view(-1, self.embed_dim)\n        ).view(batch_size, n_query, self.embed_dim)\n        \n        #out = self.drop(out)\n\n        return out\n        \n\nclass PositionWiseFeedforward(nn.Module):\n    \n    def __init__(self, embed_dim, feedforward_dim=512, dropout=0.1):\n        super(PositionWiseFeedforward, self).__init__()\n        # modified on 2019 10 23\n        self.sub_layers = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, embed_dim, bias=True),\n            nn.ReLU()\n        )\n        \n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n    \n    def forward(self, input, mask=None):\n        return self.sub_layers(input)\n\n\nclass GraphTransformerLayer(nn.Module):\n\n    def __init__(self, n_heads, embed_dim, feedforward_dim, \n                 normalization=\'batch\', dropout=0.1):\n        super(GraphTransformerLayer, self).__init__()\n        \n        self.self_attention = SkipConnection(\n            MultiHeadAttention(\n                    n_heads=n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim,\n                    dropout=dropout\n                )\n            )\n        self.norm1 = Normalization(embed_dim, normalization)\n        \n        self.positionwise_ff = SkipConnection(\n               PositionWiseFeedforward(\n                   embed_dim=embed_dim,\n                   feedforward_dim=feedforward_dim,\n                   dropout=dropout\n                )\n            )\n        self.norm2 = Normalization(embed_dim, normalization)\n        \n    def forward(self, input, mask):\n        h = self.self_attention(input, mask=mask)\n        h = self.norm1(h, mask=mask)\n        h = self.positionwise_ff(h, mask=mask)\n        h = self.norm2(h, mask=mask)\n        return h\n'"
network/graph_transformer_layers_new_dropout_2_adj_mtx.py,12,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import nn\nimport math\nimport ipdb\n\n\n\nclass SkipConnection(nn.Module):\n\n    def __init__(self, module):\n        super(SkipConnection, self).__init__()\n        self.module = module\n\n    def forward(self, input, mask=None):\n        return input + self.module(input, mask=mask)\n    \n\nclass Normalization(nn.Module):\n\n    def __init__(self, embed_dim, normalization=\'batch\'):\n        super(Normalization, self).__init__()\n\n        normalizer_class = {\n            \'batch\': nn.BatchNorm1d,\n            \'instance\': nn.InstanceNorm1d\n        }.get(normalization, None)\n\n        self.normalizer = normalizer_class(embed_dim, affine=True)\n\n        # Normalization by default initializes affine parameters \n        # with bias 0 and weight unif(0,1) which is too large!\n        self.init_parameters()\n\n    def init_parameters(self):\n        for name, param in self.named_parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, mask=None):\n        if isinstance(self.normalizer, nn.BatchNorm1d):\n            return self.normalizer(input.view(-1, input.size(-1))).view(*input.size())\n        elif isinstance(self.normalizer, nn.InstanceNorm1d):\n            return self.normalizer(input.permute(0, 2, 1)).permute(0, 2, 1)\n        else:\n            assert self.normalizer is None, ""Unknown normalizer type""\n            return input\n\n\nclass MultiHeadAttention(nn.Module):\n    \n    def __init__(self, n_heads, input_dim, embed_dim=None, \n                 val_dim=None, key_dim=None, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n\n        if val_dim is None:\n            assert embed_dim is not None, ""Provide either embed_dim or val_dim""\n            val_dim = embed_dim // n_heads\n        if key_dim is None:\n            key_dim = val_dim\n\n        self.n_heads = n_heads\n        self.input_dim = input_dim\n        self.embed_dim = embed_dim\n        self.val_dim = val_dim\n        self.key_dim = key_dim\n\n        self.norm_factor = 1 / math.sqrt(key_dim)\n\n        self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_val = nn.Parameter(torch.Tensor(n_heads, input_dim, val_dim))\n\n        if embed_dim is not None:\n            self.W_out = nn.Parameter(torch.Tensor(n_heads, key_dim, embed_dim))\n            \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, q, h=None, mask=None):\n        """"""\n        Args:\n            q: Input queries (batch_size, n_query, input_dim)\n            h: Input data (batch_size, graph_size, input_dim)\n            mask: Input attention mask (batch_size, n_query, graph_size)\n                  or viewable as that (i.e. can be 2 dim if n_query == 1);\n                  Mask should contain -inf if attention is not possible \n                  (i.e. mask is a negative adjacency matrix)\n        \n        Returns: \n            out: Updated data after attention (batch_size, graph_size, input_dim)\n        """"""\n        if h is None:\n            h = q  # compute self-attention\n\n        # h should be (batch_size, graph_size, input_dim)\n        batch_size, graph_size, input_dim = h.size()\n        n_query = q.size(1)\n        assert q.size(0) == batch_size\n        assert q.size(2) == input_dim\n        assert input_dim == self.input_dim, ""Wrong embedding dimension of input""\n\n        hflat = h.contiguous().view(-1, input_dim)\n        qflat = q.contiguous().view(-1, input_dim)\n\n        # last dimension can be different for keys and values\n        shp = (self.n_heads, batch_size, graph_size, -1)\n        shp_q = (self.n_heads, batch_size, n_query, -1)\n\n        # Calculate queries, (n_heads, n_query, graph_size, key/val_size)\n        dropt1_qflat = self.dropout_1(qflat)\n        Q = torch.matmul(dropt1_qflat, self.W_query).view(shp_q)\n\n        # Calculate keys and values (n_heads, batch_size, graph_size, key/val_size)\n        dropt2_hflat = self.dropout_2(hflat)\n        K = torch.matmul(dropt2_hflat, self.W_key).view(shp)\n\n        dropt3_hflat = self.dropout_3(hflat)\n        V = torch.matmul(dropt3_hflat, self.W_val).view(shp)\n\n        # Calculate compatibility (n_heads, batch_size, n_query, graph_size)\n        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n        \n        # Optionally apply mask to prevent attention\n        if mask is not None:\n            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n            compatibility = compatibility + mask.type_as(compatibility)\n\n        attn = F.softmax(compatibility, dim=-1)\n\n        heads = torch.matmul(attn, V)\n\n        out = torch.mm(\n            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.n_heads * self.val_dim),\n            self.W_out.view(-1, self.embed_dim)\n        ).view(batch_size, n_query, self.embed_dim)\n        \n        #out = self.drop(out)\n\n        return out\n        \n\nclass PositionWiseFeedforward(nn.Module):\n    \n    def __init__(self, embed_dim, feedforward_dim=512, dropout=0.1):\n        super(PositionWiseFeedforward, self).__init__()\n        # modified on 2019 10 23\n        self.sub_layers = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, embed_dim, bias=True),\n            nn.ReLU()\n        )\n        \n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n    \n    def forward(self, input, mask=None):\n        return self.sub_layers(input)\n\n\nclass MultiGraphTransformerLayer(nn.Module):\n\n    def __init__(self, n_heads, embed_dim, feedforward_dim, \n                 normalization=\'batch\', dropout=0.1):\n        super(MultiGraphTransformerLayer, self).__init__()\n        \n        self.self_attention1 = SkipConnection(\n            MultiHeadAttention(\n                    n_heads=n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim,\n                    dropout=dropout\n                )\n            )\n        self.self_attention2 = SkipConnection(\n            MultiHeadAttention(\n                    n_heads=n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim,\n                    dropout=dropout\n                )\n            )\n\n        # modified on 2019 10 24.\n        self.tmp_linear_layer = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim * 2, embed_dim, bias=True),\n            nn.ReLU(),\n        )\n\n        self.norm1 = Normalization(embed_dim, normalization)\n        \n        self.positionwise_ff = SkipConnection(\n               PositionWiseFeedforward(\n                   embed_dim=embed_dim,\n                   feedforward_dim=feedforward_dim,\n                   dropout=dropout\n                )\n            )\n        self.norm2 = Normalization(embed_dim, normalization)\n        \n    def forward(self, input, mask1, mask2):\n        #ipdb.set_trace()\n        h1 = self.self_attention1(input, mask=mask1)\n        h2 = self.self_attention2(input, mask=mask2)\n        hh = torch.cat((h1, h2), dim=2)\n        hh = self.tmp_linear_layer(hh)\n        #ipdb.set_trace()\n        hh = self.norm1(hh, mask=mask1)\n        hh = self.positionwise_ff(hh, mask=mask1)\n        hh = self.norm2(hh, mask=mask1)\n        return hh'"
network/graph_transformer_layers_new_dropout_3_adj_mtx.py,12,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import nn\nimport math\nimport ipdb\n\n\n\nclass SkipConnection(nn.Module):\n\n    def __init__(self, module):\n        super(SkipConnection, self).__init__()\n        self.module = module\n\n    def forward(self, input, mask=None):\n        return input + self.module(input, mask=mask)\n    \n\nclass Normalization(nn.Module):\n\n    def __init__(self, embed_dim, normalization=\'batch\'):\n        super(Normalization, self).__init__()\n\n        normalizer_class = {\n            \'batch\': nn.BatchNorm1d,\n            \'instance\': nn.InstanceNorm1d\n        }.get(normalization, None)\n\n        self.normalizer = normalizer_class(embed_dim, affine=True)\n\n        # Normalization by default initializes affine parameters \n        # with bias 0 and weight unif(0,1) which is too large!\n        self.init_parameters()\n\n    def init_parameters(self):\n        for name, param in self.named_parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, mask=None):\n        if isinstance(self.normalizer, nn.BatchNorm1d):\n            return self.normalizer(input.view(-1, input.size(-1))).view(*input.size())\n        elif isinstance(self.normalizer, nn.InstanceNorm1d):\n            return self.normalizer(input.permute(0, 2, 1)).permute(0, 2, 1)\n        else:\n            assert self.normalizer is None, ""Unknown normalizer type""\n            return input\n\n\nclass MultiHeadAttention(nn.Module):\n    \n    def __init__(self, n_heads, input_dim, embed_dim=None, \n                 val_dim=None, key_dim=None, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n\n        if val_dim is None:\n            assert embed_dim is not None, ""Provide either embed_dim or val_dim""\n            val_dim = embed_dim // n_heads\n        if key_dim is None:\n            key_dim = val_dim\n\n        self.n_heads = n_heads\n        self.input_dim = input_dim\n        self.embed_dim = embed_dim\n        self.val_dim = val_dim\n        self.key_dim = key_dim\n\n        self.norm_factor = 1 / math.sqrt(key_dim)\n\n        self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_val = nn.Parameter(torch.Tensor(n_heads, input_dim, val_dim))\n\n        if embed_dim is not None:\n            self.W_out = nn.Parameter(torch.Tensor(n_heads, key_dim, embed_dim))\n            \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, q, h=None, mask=None):\n        """"""\n        Args:\n            q: Input queries (batch_size, n_query, input_dim)\n            h: Input data (batch_size, graph_size, input_dim)\n            mask: Input attention mask (batch_size, n_query, graph_size)\n                  or viewable as that (i.e. can be 2 dim if n_query == 1);\n                  Mask should contain -inf if attention is not possible \n                  (i.e. mask is a negative adjacency matrix)\n        \n        Returns: \n            out: Updated data after attention (batch_size, graph_size, input_dim)\n        """"""\n        if h is None:\n            h = q  # compute self-attention\n\n        # h should be (batch_size, graph_size, input_dim)\n        batch_size, graph_size, input_dim = h.size()\n        n_query = q.size(1)\n        assert q.size(0) == batch_size\n        assert q.size(2) == input_dim\n        assert input_dim == self.input_dim, ""Wrong embedding dimension of input""\n\n        hflat = h.contiguous().view(-1, input_dim)\n        qflat = q.contiguous().view(-1, input_dim)\n\n        # last dimension can be different for keys and values\n        shp = (self.n_heads, batch_size, graph_size, -1)\n        shp_q = (self.n_heads, batch_size, n_query, -1)\n\n        # Calculate queries, (n_heads, n_query, graph_size, key/val_size)\n        dropt1_qflat = self.dropout_1(qflat)\n        Q = torch.matmul(dropt1_qflat, self.W_query).view(shp_q)\n\n        # Calculate keys and values (n_heads, batch_size, graph_size, key/val_size)\n        dropt2_hflat = self.dropout_2(hflat)\n        K = torch.matmul(dropt2_hflat, self.W_key).view(shp)\n\n        dropt3_hflat = self.dropout_3(hflat)\n        V = torch.matmul(dropt3_hflat, self.W_val).view(shp)\n\n        # Calculate compatibility (n_heads, batch_size, n_query, graph_size)\n        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n        \n        # Optionally apply mask to prevent attention\n        if mask is not None:\n            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n            compatibility = compatibility + mask.type_as(compatibility)\n\n        attn = F.softmax(compatibility, dim=-1)\n\n        heads = torch.matmul(attn, V)\n\n        out = torch.mm(\n            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.n_heads * self.val_dim),\n            self.W_out.view(-1, self.embed_dim)\n        ).view(batch_size, n_query, self.embed_dim)\n        \n        #out = self.drop(out)\n\n        return out\n        \n\nclass PositionWiseFeedforward(nn.Module):\n    \n    def __init__(self, embed_dim, feedforward_dim=512, dropout=0.1):\n        super(PositionWiseFeedforward, self).__init__()\n        # modified on 2019 10 23\n        self.sub_layers = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, embed_dim, bias=True),\n            nn.ReLU()\n        )\n        \n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n    \n    def forward(self, input, mask=None):\n        return self.sub_layers(input)\n\n\nclass MultiGraphTransformerLayer(nn.Module):\n\n    def __init__(self, n_heads, embed_dim, feedforward_dim, \n                 normalization=\'batch\', dropout=0.1):\n        super(MultiGraphTransformerLayer, self).__init__()\n        \n        self.self_attention1 = SkipConnection(\n            MultiHeadAttention(\n                    n_heads=n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim,\n                    dropout=dropout\n                )\n            )\n        self.self_attention2 = SkipConnection(\n            MultiHeadAttention(\n                    n_heads=n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim,\n                    dropout=dropout\n                )\n            )\n\n        self.self_attention3 = SkipConnection(\n            MultiHeadAttention(\n                    n_heads=n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim,\n                    dropout=dropout\n                )\n            )\n        # modified on 2019 10 26.\n        self.tmp_linear_layer = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim * 3, embed_dim, bias=True),\n            nn.ReLU(),\n        )\n\n        self.norm1 = Normalization(embed_dim, normalization)\n        \n        self.positionwise_ff = SkipConnection(\n               PositionWiseFeedforward(\n                   embed_dim=embed_dim,\n                   feedforward_dim=feedforward_dim,\n                   dropout=dropout\n                )\n            )\n        self.norm2 = Normalization(embed_dim, normalization)\n        \n    def forward(self, input, mask1, mask2, mask3):\n        #ipdb.set_trace()\n        h1 = self.self_attention1(input, mask=mask1)\n        h2 = self.self_attention2(input, mask=mask2)\n        h3 = self.self_attention3(input, mask=mask3)\n        hh = torch.cat((h1, h2, h3), dim=2)\n        hh = self.tmp_linear_layer(hh)\n        #ipdb.set_trace()\n        hh = self.norm1(hh, mask=mask1)\n        hh = self.positionwise_ff(hh, mask=mask1)\n        hh = self.norm2(hh, mask=mask1)\n        return hh'"
testcase/test_case_4_QuickdrawDataset4dict_2nn.py,0,"b'import numpy as np\r\nimport ipdb\r\n\r\n\r\ndef produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len):\r\n    assert flag_bits.shape == (10, 1)\r\n    adja_matr = np.zeros([10, 10], int)\r\n    # \r\n    adja_matr[ : ][ : ] = -1e10\r\n\r\n    adja_matr[0][0] = 0\r\n    # TODO\r\n    if (flag_bits[0] == 100):\r\n        adja_matr[0][1] = 0\r\n\r\n\r\n    for idx in range(1, stroke_len):\r\n        #\r\n        adja_matr[idx][idx] = 0\r\n\r\n        if (flag_bits[idx - 1] == 100):\r\n            adja_matr[idx][idx - 1] = 0\r\n\r\n        if idx == stroke_len - 1:\r\n            break\r\n\r\n        if (flag_bits[idx] == 100):\r\n            adja_matr[idx][idx + 1] = 0\r\n\r\n    return adja_matr\r\n\r\n\r\n\r\n\r\ndef stroke_length_detection(coordinate_array):\r\n\r\n    for i in range(len(coordinate_array)-1,-1,-1):\r\n        #ipdb.set_trace()\r\n\r\n        if ((coordinate_array[i] == np.array([0, 0, 0, 0])).all()):\r\n \r\n            return i\r\n            \r\n    return 10\r\n\r\n\r\ndef flag_bit_transfer(input_array):\r\n    out_array = np.zeros([10, 1], int)\r\n    assert input_array.shape == (10, 2)\r\n    for idx, bits in enumerate(input_array):\r\n        if ((bits == [1, 0]).all()):\r\n            out_array[idx] = 100\r\n        elif ((bits == [0, 1]).all()):\r\n            out_array[idx] = 101\r\n        else:\r\n            out_array[idx] = 102\r\n    \r\n    return out_array\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n# coordi_array = np.array([[1, 1, 0, 1], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\ncoordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 1, 0]], dtype=np.float32)\r\n\r\n\r\n\r\nstroke_len = stroke_length_detection(coordi_array)\r\nflag_bits = flag_bit_transfer(coordi_array[:, 2 : ])\r\nadja_matr = produce_adjacent_matrix_2_neighbors(flag_bits, stroke_len)\r\n\r\nipdb.set_trace()'"
testcase/test_case_4_QuickdrawDataset4dict_4nn.py,0,"b'import numpy as np\r\nimport ipdb\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len):\r\n    assert flag_bits.shape == (10, 1)\r\n    adja_matr = np.zeros([10, 10], int)\r\n    adja_matr[ : ][ : ] = -1e10\r\n\r\n    adja_matr[0][0] = 0\r\n    # TODO\r\n    if (flag_bits[0] == 100):\r\n        adja_matr[0][1] = 0\r\n        # 20191020\r\n        if (flag_bits[1] == 100):\r\n            adja_matr[0][2] = 0\r\n\r\n\r\n    for idx in range(1, stroke_len):\r\n        #\r\n        adja_matr[idx][idx] = 0\r\n\r\n        if (flag_bits[idx - 1] == 100):\r\n            adja_matr[idx][idx - 1] = 0\r\n            # 20191020\r\n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\r\n                adja_matr[idx][idx - 2] = 0\r\n\r\n        if idx == stroke_len - 1:\r\n            break\r\n\r\n        #\r\n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\r\n            adja_matr[idx][idx + 1] = 0\r\n            # \r\n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\r\n                adja_matr[idx][idx + 2] = 0\r\n\r\n    return adja_matr\r\n\r\n\r\n\r\ndef stroke_length_detection(coordinate_array):\r\n\r\n    for i in range(len(coordinate_array)-1,-1,-1):\r\n        #ipdb.set_trace()\r\n\r\n        if ((coordinate_array[i] == np.array([0, 0, 0, 0])).all()):\r\n \r\n            return i\r\n            \r\n    return 10\r\n\r\n\r\ndef flag_bit_transfer(input_array):\r\n    out_array = np.zeros([10, 1], int)\r\n    assert input_array.shape == (10, 2)\r\n    for idx, bits in enumerate(input_array):\r\n        if ((bits == [1, 0]).all()):\r\n            out_array[idx] = 100\r\n        elif ((bits == [0, 1]).all()):\r\n            out_array[idx] = 101\r\n        else:\r\n            out_array[idx] = 102\r\n    \r\n    return out_array\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 0, 1], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 1, 0], [9, 9, 0, 1], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\n\r\ncoordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 1, 0]], dtype=np.float32)\r\n\r\n\r\n\r\nstroke_len = stroke_length_detection(coordi_array)\r\nflag_bits = flag_bit_transfer(coordi_array[:, 2 : ])\r\n\r\nadja_matr = produce_adjacent_matrix_4_neighbors(flag_bits, stroke_len)\r\n\r\nx = adja_matr\r\n\r\nipdb.set_trace()'"
testcase/test_case_4_QuickdrawDataset4dict_6nn.py,0,"b'import numpy as np\r\nimport ipdb\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef produce_adjacent_matrix_6_neighbors(flag_bits, stroke_len):\r\n    assert flag_bits.shape == (10, 1)\r\n    adja_matr = np.zeros([10, 10], int)\r\n    adja_matr[ : ][ : ] = -1\r\n\r\n    adja_matr[0][0] = 0\r\n    # TODO\r\n    if (flag_bits[0] == 100):\r\n        adja_matr[0][1] = 0\r\n        # 20191020\r\n        if (flag_bits[1] == 100):\r\n            adja_matr[0][2] = 0\r\n            if (flag_bits[2] == 100):\r\n                adja_matr[0][3] = 0\r\n\r\n\r\n    for idx in range(1, stroke_len):\r\n        #\r\n        adja_matr[idx][idx] = 0\r\n\r\n        if (flag_bits[idx - 1] == 100):\r\n            adja_matr[idx][idx - 1] = 0\r\n            # 20191020\r\n            if (idx >= 2) and (flag_bits[idx - 2] == 100):\r\n                adja_matr[idx][idx - 2] = 0\r\n                if (idx >= 3) and (flag_bits[idx - 3] == 100):\r\n                    adja_matr[idx][idx - 3] = 0\r\n\r\n        if idx == stroke_len - 1:\r\n            break\r\n\r\n        # \r\n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 100):\r\n            adja_matr[idx][idx + 1] = 0\r\n            # \r\n            if (idx <= (stroke_len - 3)) and (flag_bits[idx + 1] == 100):\r\n                adja_matr[idx][idx + 2] = 0\r\n                if (idx <= (stroke_len - 4)) and (flag_bits[idx + 2] == 100):\r\n                    adja_matr[idx][idx + 3] = 0\r\n\r\n    return adja_matr\r\n\r\n\r\ndef stroke_length_detection(coordinate_array):\r\n\r\n    for i in range(len(coordinate_array)-1,-1,-1):\r\n        #ipdb.set_trace()\r\n\r\n        if ((coordinate_array[i] == np.array([0, 0, 0, 0])).all()):\r\n \r\n            return i\r\n            \r\n    return 10\r\n\r\n\r\ndef flag_bit_transfer(input_array):\r\n    out_array = np.zeros([10, 1], int)\r\n    assert input_array.shape == (10, 2)\r\n    for idx, bits in enumerate(input_array):\r\n        if ((bits == [1, 0]).all()):\r\n            out_array[idx] = 100\r\n        elif ((bits == [0, 1]).all()):\r\n            out_array[idx] = 101\r\n        else:\r\n            out_array[idx] = 102\r\n    \r\n    return out_array\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 1, 0], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 1, 0], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 0, 1], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 1, 0], [9, 9, 0, 1], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\n\r\ncoordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 1, 0]], dtype=np.float32)\r\n\r\n\r\n\r\nstroke_len = stroke_length_detection(coordi_array)\r\nflag_bits = flag_bit_transfer(coordi_array[:, 2 : ])\r\n\r\nadja_matr = produce_adjacent_matrix_6_neighbors(flag_bits, stroke_len)\r\n\r\nx = adja_matr\r\n\r\nipdb.set_trace()'"
testcase/test_case_4_QuickdrawDataset4dict_fully_connected_stroke_attention_mask.py,0,"b'import numpy as np\r\nimport ipdb\r\n\r\n\r\ndef detect_ending_points(flag_bits, stroke_len):\r\n\r\n    assert flag_bits.shape == (10, 1)\r\n    ending_points_index_array = []\r\n    # \r\n    ending_points_index_array.append(-1)\r\n    \r\n    for idx in range(stroke_len):\r\n        # \r\n        if (flag_bits[idx] == 101):\r\n            ending_points_index_array.append(idx)\r\n\r\n    # \r\n    if stroke_len == 10:\r\n\r\n        assert flag_bits[9] == 100 or flag_bits[9] == 101\r\n\r\n        if flag_bits[9] == 100:\r\n            ending_points_index_array.append(9)\r\n\r\n    return ending_points_index_array\r\n\r\ndef produce_adjacent_matrix(ending_points_index_array):\r\n    adja_matr = np.zeros([10, 10], int)\r\n    \r\n    adja_matr[ : ][ : ] = -1e10\r\n    for idx in range(1, len(ending_points_index_array)):\r\n        start_index = ending_points_index_array[idx - 1] + 1\r\n        end_index = ending_points_index_array[idx]\r\n        #\r\n        if end_index == 9:\r\n            adja_matr[start_index : , start_index : ] = 0\r\n        else:\r\n            adja_matr[start_index : end_index + 1, start_index: end_index + 1] = 0\r\n\r\n    return adja_matr\r\n\r\n\r\n\r\n####################\r\n\r\n\r\n\r\n\r\ndef stroke_length_detection(coordinate_array):\r\n\r\n    for i in range(len(coordinate_array)-1,-1,-1):\r\n        #ipdb.set_trace()\r\n\r\n        if ((coordinate_array[i] == np.array([0, 0, 0, 0])).all()):\r\n \r\n            return i\r\n            \r\n    return 10\r\n\r\n\r\ndef flag_bit_transfer(input_array):\r\n    out_array = np.zeros([10, 1], int)\r\n    assert input_array.shape == (10, 2)\r\n    for idx, bits in enumerate(input_array):\r\n        if ((bits == [1, 0]).all()):\r\n            out_array[idx] = 100\r\n        elif ((bits == [0, 1]).all()):\r\n            out_array[idx] = 101\r\n        else:\r\n            out_array[idx] = 102\r\n    \r\n    return out_array\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 0, 1], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\ncoordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 1, 0]], dtype=np.float32)\r\n\r\n\r\n\r\nstroke_len = stroke_length_detection(coordi_array)\r\nflag_bits = flag_bit_transfer(coordi_array[:, 2 : ])\r\n\r\n\r\n\r\nending_points_index_array = detect_ending_points(flag_bits, stroke_len)\r\nattention_mask = produce_adjacent_matrix(ending_points_index_array)\r\n\r\n\r\n\r\nipdb.set_trace()'"
testcase/test_case_4_QuickdrawDataset4dict_jnn.py,0,"b'import numpy as np\r\nimport ipdb\r\n\r\n\r\n\r\n\r\n\r\ndef produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len):\r\n    assert flag_bits.shape == (10, 1)\r\n    adja_matr = np.zeros([10, 10], int)\r\n    adja_matr[ : ][ : ] = -1\r\n    \r\n    adja_matr[0][0] = 0\r\n    adja_matr[0][stroke_len - 1] = 0\r\n    adja_matr[stroke_len - 1][stroke_len - 1] = 0\r\n    adja_matr[stroke_len - 1][0] = 0\r\n\r\n    assert flag_bits[0] == 100 or flag_bits[0] == 101\r\n\r\n    if (flag_bits[0] == 101) and stroke_len >= 2:\r\n        adja_matr[0][1] = 0\r\n\r\n\r\n    for idx in range(1, stroke_len):\r\n\r\n        assert flag_bits[idx] == 100 or flag_bits[idx] == 101\r\n\r\n        adja_matr[idx][idx] = 0\r\n\r\n        if (flag_bits[idx - 1] == 101):\r\n            adja_matr[idx][idx - 1] = 0\r\n\r\n        if (idx == stroke_len - 1):\r\n            break\r\n\r\n        # \r\n        if (idx <= (stroke_len - 2)) and (flag_bits[idx] == 101):\r\n            adja_matr[idx][idx + 1] = 0\r\n\r\n\r\n    return adja_matr\r\n\r\n\r\ndef stroke_length_detection(coordinate_array):\r\n\r\n    for i in range(len(coordinate_array)-1,-1,-1):\r\n        #ipdb.set_trace()\r\n\r\n        if ((coordinate_array[i] == np.array([0, 0, 0, 0])).all()):\r\n \r\n            return i\r\n            \r\n    return 10\r\n\r\n\r\ndef flag_bit_transfer(input_array):\r\n    out_array = np.zeros([10, 1], int)\r\n    assert input_array.shape == (10, 2)\r\n    for idx, bits in enumerate(input_array):\r\n        if ((bits == [1, 0]).all()):\r\n            out_array[idx] = 100\r\n        elif ((bits == [0, 1]).all()):\r\n            out_array[idx] = 101\r\n        else:\r\n            out_array[idx] = 102\r\n    \r\n    return out_array\r\n\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 0, 1], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [0, 0, 0, 0], [-1, -1, -1, -1]], dtype=np.float32)\r\n\r\n\r\n# coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\n#coordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 1, 0], [9, 9, 0, 1], [10, 10, 0, 1]], dtype=np.float32)\r\n\r\n\r\ncoordi_array = np.array([[1, 1, 1, 0], [2, 2, 1, 0], [3, 3, 0, 1], [4, 4, 1, 0], [5, 5, 1, 0], [6, 6, 1, 0], [7, 7, 1, 0], [8, 8, 0, 1], [9, 9, 1, 0], [10, 10, 1, 0]], dtype=np.float32)\r\n\r\n\r\n\r\nstroke_len = stroke_length_detection(coordi_array)\r\nflag_bits = flag_bit_transfer(coordi_array[:, 2 : ])\r\n\r\nadja_matr = produce_adjacent_matrix_joint_neighbors(flag_bits, stroke_len)\r\n\r\nx = adja_matr\r\n\r\nassert (x.T == x).all() == True\r\n\r\nipdb.set_trace()'"
testcase/test_case_4_QuickdrawDataset4dict_random_attention_mask.py,0,"b'import numpy as np\r\n\r\n\r\ndef produce_adjacent_matrix_random(stroke_length, conn=0.15):\r\n    attention_mask = np.random.choice(a=[0, 1], size=[7, 7], p=[conn, 1-conn])\r\n    attention_mask[stroke_length: , :] = 2\r\n    attention_mask[:, stroke_length : ] = 2\r\n    #####\r\n    attention_mask = np.triu(attention_mask)\r\n    attention_mask += attention_mask.T - np.diag(attention_mask.diagonal())\r\n    \r\n    for i in range(stroke_length):\r\n        attention_mask[i, i] = 0\r\n    return attention_mask\r\n\r\n\r\n\r\n\r\natt_msk = produce_adjacent_matrix_random(4, 0.5)\r\nprint(att_msk)\r\nprint(""----------------------"")\r\nprint((att_msk.T == att_msk).all())'"
utils/AverageMeter.py,0,"b'class AverageMeter(object):\r\n    """"""Computes and stores the average and current value""""""\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0.0\r\n        self.sum = 0.0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += float(val * n)\r\n        self.count += n\r\n        self.avg = self.sum / self.count'"
utils/EarlyStopping.py,0,"b'import numpy as np\n\n\nclass EarlyStopping:\n    \n    def __init__(self, patience=10, delta=0):\n        """"""\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 10\n            \n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        """"""\n        self.patience = patience\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n\n    def __call__(self, val_acc):\n\n        score = val_acc\n\n        if self.best_score is None:\n            self.best_score = score\n            \n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f\'EarlyStopping counter: {self.counter} out of {self.patience}\')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score            \n            self.counter = 0\n\n    \n'"
utils/Logger.py,0,"b'import logging\nimport os\n\n\nclass Logger(object):\n    """"""\n    set logger\n\n    """"""\n\n    def __init__(self, logger_path):\n        self.logger = logging.getLogger()\n        self.logger.setLevel(logging.DEBUG)\n        self.logfile = logging.FileHandler(logger_path)\n        #\n        self.logfile.setLevel(logging.DEBUG)\n        # formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n        formatter = logging.Formatter(\n            \'%(asctime)s -%(filename)s:%(lineno)s - %(levelname)s - %(message)s\')\n        self.logfile.setFormatter(formatter)\n        self.logdisplay = logging.StreamHandler()\n        #\n        self.logdisplay.setLevel(logging.DEBUG)\n        self.logdisplay.setFormatter(formatter)\n        self.logger.addHandler(self.logfile)\n        self.logger.addHandler(self.logdisplay)\n\n    def get_logger(self):\n        return self.logger\n'"
utils/__init__.py,0,b'\n'
utils/accuracy.py,0,"b'def accuracy(output, target, topk=(1,)):\r\n    """"""Computes the precision@k for the specified values of k""""""\r\n    maxk = max(topk)\r\n    batch_size = target.size(0)\r\n\r\n    _, pred = output.topk(maxk, 1, True, True)\r\n    pred = pred.t()\r\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n\r\n    res = []\r\n    for k in topk:\r\n        correct_k = correct[:k].view(-1).float().sum(0)\r\n        res.append(correct_k.mul_(100.0 / batch_size))\r\n    return res\r\n\r\n'"
baselines/cnn_baselines/__init__.py,0,b'\n'
baselines/cnn_baselines/train_inceptionv3.py,11,"b'import argparse\nimport collections\nimport datetime\n#import imp\nimport os\nimport pickle\nimport time\n#import lmdb\nimport ipdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport sys\nsys.path.append(""../.."")\nfrom dataloader.QuickdrawDataset import *\n\nfrom utils.AverageMeter import AverageMeter\nfrom utils.Logger import Logger\nfrom utils.accuracy import *\n\n\n\n\n\n\n\n\n################################################\n# This python file contains four parts:\n#\n# Part 1. Argument Parser\n# Part 2. configurations:\n#                       Part 2-1. Basic configuration\n#                       Part 2-2. dataloader instantiation\n#                       Part 2-3. log configuration\n#                       Part 2-4. configurations for loss function, network, and optimizer\n# Part 3. \'train\' function\n# Part 4. \'validate\' function \n# Part 5. \'main\' function\n################################################\n\n\n# Part 1. Argument Parser\nparser = argparse.ArgumentParser(description=\'inceptionv3_based_sketch_classifiction\')\nparser.add_argument(""--exp"", type=str, default=""inceptionv3_001"", help=""experiment"")\n# TODO\nparser.add_argument(""--train_picture_path_root"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_train_set/"", help=""train_sketch_picture_dir"")\nparser.add_argument(""--val_picture_path_root"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_val_set/"", help=""val_sketch_picture_dir"")\nparser.add_argument(""--test_picture_path_root"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_test_set/"", help=""test_sketch_picture_dir"")\nparser.add_argument(""--sketch_list"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_train_set.txt"", help=""sketch_list_urls"")\nparser.add_argument(""--sketch_list_4_val"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_val_set.txt"", help=""sketch_list_urls_4_validation"")\nparser.add_argument(""--sketch_list_4_test"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_test_set.txt"", help=""sketch_list_urls_4_test"")\nparser.add_argument(""--batch_size"", type=int, default=64, help=""batch_size"")\nparser.add_argument(""--num_workers"", type=int, default=8, help=""num_workers"")\nparser.add_argument(\'--gpu\', type=str, default=""0"", help=\'choose GPU\')\n# parser.add_argument(\'--gpu\', type=str, default=""0,1,2,3"", help=\'choose GPU\')\n\n\nargs = parser.parse_args()\n\n\n# Part 2. configurations\n\n# Part 2-1. Basic configuration\nbasic_configs = collections.OrderedDict()\nbasic_configs[\'serial_number\'] = args.exp\nbasic_configs[\'learning_rate\'] = 1e-2\nbasic_configs[\'num_epochs\'] = 17\nbasic_configs[""lr_protocol""] = [(10, 1e-2), (20, 1e-3), (30, 1e-4), (40, 1e-5)]\nbasic_configs[""display_step""] = 10\nlr_protocol = basic_configs[""lr_protocol""]\n\n\n# Part 2-2. dataloader instantiation\ndataloader_configs = collections.OrderedDict()\ndataloader_configs[\'train_picture_path_root\'] = args.train_picture_path_root\ndataloader_configs[\'val_picture_path_root\'] = args.val_picture_path_root\ndataloader_configs[\'test_picture_path_root\'] = args.test_picture_path_root\ndataloader_configs[\'sketch_list\'] = args.sketch_list\ndataloader_configs[\'sketch_list_4_val\'] = args.sketch_list_4_val\ndataloader_configs[\'sketch_list_4_test\'] = args.sketch_list_4_test\ndataloader_configs[\'batch_size\'] = args.batch_size\ndataloader_configs[\'num_workers\'] = args.num_workers\n \n\ntransform_train = transforms.Compose([\n    # transforms.RandomHorizontalFlip(),\n    transforms.Resize(299),\n    transforms.ToTensor()\n])\n\ntransform_val = transforms.Compose([\n    transforms.Resize(299),\n    transforms.ToTensor()\n])\n\n#ipdb.set_trace()\n\n\n# create dataset\n# -----------------------------------------------------------------------------------------------------\n\ntrain_dataset = QuickdrawDataset(dataloader_configs[\'train_picture_path_root\'], dataloader_configs[\'sketch_list\'], transform_train)\ntrain_loader = DataLoader(train_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=True, num_workers=dataloader_configs[\'num_workers\'])\n\nval_dataset = QuickdrawDataset(dataloader_configs[\'val_picture_path_root\'], dataloader_configs[\'sketch_list_4_val\'], transform_val)\nval_loader = DataLoader(val_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\ntest_dataset = QuickdrawDataset(dataloader_configs[\'test_picture_path_root\'], dataloader_configs[\'sketch_list_4_test\'], transform_val)\ntest_loader = DataLoader(test_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\n# Part 2-3. log configuration\nexp_dir = os.path.join(\'./experimental_results\', args.exp)\n\nexp_log_dir = os.path.join(exp_dir, ""log"")\nif not os.path.exists(exp_log_dir):\n    os.makedirs(exp_log_dir)\n\nexp_visual_dir = os.path.join(exp_dir, ""visual"")\nif not os.path.exists(exp_visual_dir):\n    os.makedirs(exp_visual_dir)\n\nexp_ckpt_dir = os.path.join(exp_dir, ""checkpoints"")\nif not os.path.exists(exp_ckpt_dir):\n    os.makedirs(exp_ckpt_dir)\n\nnow_str = datetime.datetime.now().__str__().replace(\' \', \'_\')\nwriter_path = os.path.join(exp_visual_dir, now_str)\nwriter = SummaryWriter(writer_path)\n\nlogger_path = os.path.join(exp_log_dir, now_str + "".log"")\nlogger = Logger(logger_path).get_logger()\n\n# TODO\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\nlogger.info(""basic configuration settings: {}"".format(basic_configs))\n#logger.info(""dataloader configuration settings: {}"".format(dataloader_configs))\n\n\n# Part 2-4. configurations for loss function, and optimizer\n\n# TODO\nloss_function = nn.CrossEntropyLoss()\n\nnet = models.inception_v3(num_classes = 345)\n\nlogger.info(""withOUT ImageNet pretraining!!!"")\nnet = net.cuda()\n# net = torch.nn.DataParallel(net, device_ids=[int(x) for x in args.gpu.split(\',\')]).cuda()\n\n# optimizer = torch.optim.SGD(net.parameters(), lr=basic_configs[\'learning_rate\'], momentum=0.9, weight_decay=5e-4)\n# TODO  change as RMSProb\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n\n\n# Part 3. \'train\' function\ndef train_function(epoch):\n    training_loss = AverageMeter()\n    training_acc = AverageMeter()\n    net.train()\n\n    lr = next((lr for (max_epoch, lr) in lr_protocol if max_epoch > epoch), lr_protocol[-1][1])\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logger.info(""set learning rate to: {}"".format(lr))\n    # TODO \n    for idx, (sketch, label) in enumerate(tqdm(train_loader, ascii=True)):\n\n        sketch = sketch.cuda()\n        label = label.cuda()\n     \n        optimizer.zero_grad()\n\n        \n        output = net(sketch)\n        \n        output = output[0]\n        # ipdb.set_trace()\n\n    \n        batch_loss = loss_function(output, label)\n\n        batch_loss.backward()\n\n        optimizer.step()\n\n        training_loss.update(batch_loss.item(), sketch.size(0))\n        \n        training_acc.update(accuracy(output, label, topk = (1,))[0].item(), sketch.size(0))\n\n        if (idx + 1) % basic_configs[""display_step""] == 0:\n            logger.info(\n                ""==> Iteration [{}][{}/{}]:"".format(epoch + 1, idx + 1, len(train_loader)))\n            logger.info(""current batch loss: {}"".format(\n                batch_loss.item()))\n            logger.info(""average loss: {}"".format(\n                training_loss.avg))\n            logger.info(""average acc: {}"".format(training_acc.avg))  \n\n    # TODO\n    logger.info(""Begin Evaluating"")\n\n    validation_loss, validation_acc = validate_function(val_loader)\n    test_loss, test_acc = validate_function(test_loader)\n    \n    writer.add_scalars(""loss"", {\n        ""training_loss"":training_loss.avg,\n        ""validation_loss"":validation_loss.avg,\n        ""test_loss"":test_loss.avg\n        }, epoch+1)\n    writer.add_scalars(""acc"", {\n        ""training_acc"":training_acc.avg,\n        ""validation_acc"":validation_acc.avg,\n        ""test_acc"":test_acc.avg\n        }, epoch+1)\n\n\n# Part 4. \'validate\' function\ndef validate_function(data_loader):\n    validation_loss = AverageMeter()\n    validation_acc = AverageMeter()\n    net.eval()\n    # TODO  \n    with torch.no_grad():\n        for idx, (sketch, label) in enumerate(tqdm(data_loader, ascii=True)):\n            \n            sketch = sketch.cuda()\n            label = label.cuda()\n        \n            \n            #output, _ = net(sketch)\n            output = net(sketch)\n\n            batch_loss = loss_function(output, label)\n\n            validation_loss.update(batch_loss.item(), sketch.size(0))\n            \n            validation_acc.update(accuracy(output, label, topk = (1,))[0].item(), sketch.size(0))\n\n        logger.info(""==> Evaluation Result: "")\n        logger.info(""loss: {}  acc:{}"".format(validation_loss.avg, validation_acc.avg))\n\n    return validation_loss, validation_acc\n\n\n\n\n\n# Part 5. \'main\' function\nif __name__ == \'__main__\':\n\n    logger.info(""Begin Evaluating before training"")\n    validate_function(val_loader)\n    \n    \n    logger.info(""training status: "")\n    for epoch in range(basic_configs[\'num_epochs\']):\n        logger.info(""Begin training epoch {}"".format(epoch + 1))\n        train_function(epoch)\n\n        net_checkpoint_name = args.exp + ""_net_epoch"" + str(epoch + 1)\n        net_checkpoint_path = os.path.join(exp_ckpt_dir, net_checkpoint_name)\n        net_state = {""epoch"": epoch + 1,\n                     ""network"": net.state_dict()}\n        torch.save(net_state, net_checkpoint_path)\n'"
baselines/cnn_baselines/train_mobilenetv2.py,11,"b'import argparse\nimport collections\nimport datetime\n#import imp\nimport os\nimport pickle\nimport time\n#import lmdb\nimport ipdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport sys\nsys.path.append(""../.."")\nfrom dataloader.QuickdrawDataset import *\nfrom utils.AverageMeter import AverageMeter\nfrom utils.Logger import Logger\nfrom utils.accuracy import *\n\n\n\n\n\n\n\n\n\n################################################\n# This python file contains four parts:\n#\n# Part 1. Argument Parser\n# Part 2. configurations:\n#                       Part 2-1. Basic configuration\n#                       Part 2-2. dataloader instantiation\n#                       Part 2-3. log configuration\n#                       Part 2-4. configurations for loss function, network, and optimizer\n# Part 3. \'train\' function\n# Part 4. \'validate\' function \n# Part 5. \'main\' function\n################################################\n\n\n# Part 1. Argument Parser\nparser = argparse.ArgumentParser(description=\'mobilenetv2_based_sketch_classifiction\')\nparser.add_argument(""--exp"", type=str, default=""mobilenetv2_001"", help=""experiment"")\n# TODO\nparser.add_argument(""--train_picture_path_root"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_train_set/"", help=""train_sketch_picture_dir"")\nparser.add_argument(""--val_picture_path_root"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_val_set/"", help=""val_sketch_picture_dir"")\nparser.add_argument(""--test_picture_path_root"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_test_set/"", help=""test_sketch_picture_dir"")\nparser.add_argument(""--sketch_list"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_train_set.txt"", help=""sketch_list_urls"")\nparser.add_argument(""--sketch_list_4_val"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_val_set.txt"", help=""sketch_list_urls_4_validation"")\nparser.add_argument(""--sketch_list_4_test"", type=str, default=""../../dataloader/data_4_cnnbaselines/tiny_test_set.txt"", help=""sketch_list_urls_4_test"")\nparser.add_argument(""--batch_size"", type=int, default=128, help=""batch_size"")\nparser.add_argument(""--num_workers"", type=int, default=8, help=""num_workers"")\nparser.add_argument(\'--gpu\', type=str, default=""1"", help=\'choose GPU\')\n# parser.add_argument(\'--gpu\', type=str, default=""0,1,2,3"", help=\'choose GPU\')\n\n\nargs = parser.parse_args()\n\n\n# Part 2. configurations\n\n# Part 2-1. Basic configuration\nbasic_configs = collections.OrderedDict()\nbasic_configs[\'serial_number\'] = args.exp\nbasic_configs[\'learning_rate\'] = 1e-3\nbasic_configs[\'num_epochs\'] = 50\n#basic_configs[""lr_protocol""] = [(30, 1e-2), (60, 1e-3), (90, 1e-4), (120, 1e-5)]\nbasic_configs[""lr_protocol""] = [(10, 1e-3), (20, (1e-3)*0.5), (30, (1e-3)*0.5*0.5), (40, (1e-3)*0.5*0.5*0.5), (50, (1e-3)*0.5*0.5*0.5*0.5)]\nbasic_configs[""display_step""] = 10\nlr_protocol = basic_configs[""lr_protocol""]\n\n\n# Part 2-2. dataloader instantiation\ndataloader_configs = collections.OrderedDict()\ndataloader_configs[\'train_picture_path_root\'] = args.train_picture_path_root\ndataloader_configs[\'val_picture_path_root\'] = args.val_picture_path_root\ndataloader_configs[\'test_picture_path_root\'] = args.test_picture_path_root\ndataloader_configs[\'sketch_list\'] = args.sketch_list\ndataloader_configs[\'sketch_list_4_val\'] = args.sketch_list_4_val\ndataloader_configs[\'sketch_list_4_test\'] = args.sketch_list_4_test\ndataloader_configs[\'batch_size\'] = args.batch_size\ndataloader_configs[\'num_workers\'] = args.num_workers\n \n\ntransform_train = transforms.Compose([\n    #transforms.RandomHorizontalFlip(),\n    transforms.ToTensor()\n])\n\ntransform_val = transforms.Compose([\n    transforms.ToTensor()\n])\n\n#ipdb.set_trace()\n\n\n# create dataset\n# -----------------------------------------------------------------------------------------------------\n\ntrain_dataset = QuickdrawDataset(dataloader_configs[\'train_picture_path_root\'], dataloader_configs[\'sketch_list\'], transform_train)\ntrain_loader = DataLoader(train_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=True, num_workers=dataloader_configs[\'num_workers\'])\n\nval_dataset = QuickdrawDataset(dataloader_configs[\'val_picture_path_root\'], dataloader_configs[\'sketch_list_4_val\'], transform_val)\nval_loader = DataLoader(val_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\ntest_dataset = QuickdrawDataset(dataloader_configs[\'test_picture_path_root\'], dataloader_configs[\'sketch_list_4_test\'], transform_val)\ntest_loader = DataLoader(test_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\n\n# Part 2-3. log configuration\nexp_dir = os.path.join(\'./experimental_results\', args.exp)\n\nexp_log_dir = os.path.join(exp_dir, ""log"")\nif not os.path.exists(exp_log_dir):\n    os.makedirs(exp_log_dir)\n\nexp_visual_dir = os.path.join(exp_dir, ""visual"")\nif not os.path.exists(exp_visual_dir):\n    os.makedirs(exp_visual_dir)\n\nexp_ckpt_dir = os.path.join(exp_dir, ""checkpoints"")\nif not os.path.exists(exp_ckpt_dir):\n    os.makedirs(exp_ckpt_dir)\n\nnow_str = datetime.datetime.now().__str__().replace(\' \', \'_\')\nwriter_path = os.path.join(exp_visual_dir, now_str)\nwriter = SummaryWriter(writer_path)\n\nlogger_path = os.path.join(exp_log_dir, now_str + "".log"")\nlogger = Logger(logger_path).get_logger()\n\n# TODO\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\nlogger.info(""basic configuration settings: {}"".format(basic_configs))\n#logger.info(""dataloader configuration settings: {}"".format(dataloader_configs))\n\n\n# Part 2-4. configurations for loss function, and optimizer\n\n# TODO\nloss_function = nn.CrossEntropyLoss()\n\nmax_val_acc = 0.0\nmax_val_acc_epoch = -1\n\nnet = models.mobilenet_v2(num_classes = 345)\nlogger.info(""withOUT ImageNet pretraining!!!"")\nnet = net.cuda()\n# net = torch.nn.DataParallel(net, device_ids=[int(x) for x in args.gpu.split(\',\')]).cuda()\n\n# optimizer = torch.optim.SGD(net.parameters(), lr=basic_configs[\'learning_rate\'], momentum=0.9, weight_decay=5e-4)\n# TODO  change as RMSProb\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n\n\n# Part 3. \'train\' function\ndef train_function(epoch):\n    training_loss = AverageMeter()\n    training_acc = AverageMeter()\n    net.train()\n\n    lr = next((lr for (max_epoch, lr) in lr_protocol if max_epoch > epoch), lr_protocol[-1][1])\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logger.info(""set learning rate to: {}"".format(lr))\n    # TODO \n    for idx, (sketch, label) in enumerate(tqdm(train_loader, ascii=True)):\n\n        sketch = sketch.cuda()\n        label = label.cuda()\n     \n        optimizer.zero_grad()\n\n        output = net(sketch)\n        # ipdb.set_trace()\n\n    \n        batch_loss = loss_function(output, label)\n\n        batch_loss.backward()\n\n        optimizer.step()\n\n        training_loss.update(batch_loss.item(), sketch.size(0))\n        \n        training_acc.update(accuracy(output, label, topk = (1,))[0].item(), sketch.size(0))\n\n        if (idx + 1) % basic_configs[""display_step""] == 0:\n            logger.info(\n                ""==> Iteration [{}][{}/{}]:"".format(epoch + 1, idx + 1, len(train_loader)))\n            logger.info(""current batch loss: {}"".format(\n                batch_loss.item()))\n            logger.info(""average loss: {}"".format(\n                training_loss.avg))\n            logger.info(""average acc: {}"".format(training_acc.avg))  \n\n    # TODO\n    logger.info(""Begin Evaluating"")\n\n    validation_loss, validation_acc = validate_function(val_loader)\n\n    test_loss, test_acc = validate_function(test_loader)\n    \n    writer.add_scalars(""loss"", {\n        ""training_loss"":training_loss.avg,\n        ""validation_loss"":validation_loss.avg,\n        ""test_loss"":test_loss.avg,\n        }, epoch+1)\n    writer.add_scalars(""acc"", {\n        ""training_acc"":training_acc.avg,\n        ""validation_acc"":validation_acc.avg,\n        ""test_acc"":test_acc.avg\n        }, epoch+1)\n\n    return validation_acc\n\n\n\n# Part 4. \'validate\' function\ndef validate_function(data_loader):\n    validation_loss = AverageMeter()\n    \n    validation_acc_1 = AverageMeter()\n    validation_acc_5 = AverageMeter()\n    validation_acc_10 = AverageMeter()\n    net.eval()\n    # TODO  \n    with torch.no_grad():\n        for idx, (sketch, label) in enumerate(tqdm(data_loader, ascii=True)):\n            \n            sketch = sketch.cuda()\n            label = label.cuda()\n        \n            \n            output = net(sketch)\n\n            batch_loss = loss_function(output, label)\n\n            validation_loss.update(batch_loss.item(), sketch.size(0))\n            \n            acc_1, acc_5, acc_10 = accuracy(output, label, topk = (1, 5, 10))\n            validation_acc_1.update(acc_1, sketch.size(0))\n            validation_acc_5.update(acc_5, sketch.size(0))\n            validation_acc_10.update(acc_10, sketch.size(0))\n\n        logger.info(""==> Testing Result: "")\n        logger.info(""loss: {}  acc@1: {} acc@5: {} acc@10: {}"".format(validation_loss.avg, validation_acc_1.avg, validation_acc_5.avg, validation_acc_10.avg))\n\n    return validation_loss, validation_acc_1\n\n\n\n\n\n# Part 5. \'main\' function\nif __name__ == \'__main__\':\n\n    logger.info(""Begin Evaluating before training"")\n    validate_function(val_loader)\n    \n    \n    logger.info(""training status: "")\n    for epoch in range(basic_configs[\'num_epochs\']):\n        logger.info(""Begin training epoch {}"".format(epoch + 1))\n\n        validation_acc = train_function(epoch)\n\n        if validation_acc.avg > max_val_acc:\n            max_val_acc = validation_acc.avg\n            max_val_acc_epoch = epoch + 1\n\n        logger.info(""max_val_acc: {}  max_val_acc_epoch: {}"".format(max_val_acc, max_val_acc_epoch))\n\n        net_checkpoint_name = args.exp + ""_net_epoch"" + str(epoch + 1)\n        net_checkpoint_path = os.path.join(exp_ckpt_dir, net_checkpoint_name)\n        net_state = {""epoch"": epoch + 1,\n                     ""network"": net.state_dict()}\n        torch.save(net_state, net_checkpoint_path)\n\n\n    logger.info(""max_val_acc: {}  max_val_acc_epoch: {}"".format(max_val_acc, max_val_acc_epoch))\n\n\n'"
baselines/gcn_baselines/__init__.py,0,b'\n'
baselines/rnn_baselines/__init__.py,0,b'\n'
baselines/rnn_baselines/train_bigru.py,9,"b'import argparse\nimport collections\nimport datetime\n#import imp\nimport os\nimport pickle\nimport time\n#import lmdb\nimport ipdb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.nn.modules.module import Module\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport sys\nsys.path.append(""../.."")\nfrom utils.AverageMeter import AverageMeter\nfrom utils.Logger import Logger\nfrom utils.accuracy import *\nfrom dataloader.QuickdrawDataset4dict_bigru import *\nfrom network.Bidirectional_GRU import *\n\n\n\n\n\n################################################\n# This python file contains five parts:\n#\n# Part 1. Argument Parser\n# Part 2. configurations:\n#                       Part 2-1. Basic configuration\n#                       Part 2-2. dataloader instantiation\n#                       Part 2-3. log configuration\n#                       Part 2-4. configurations for loss function, network, and optimizer\n# Part 3. \'train\' function\n# Part 4. \'validate\' function \n# Part 5. \'main\' function\n################################################\n\n\n# Part 1. Argument Parser\nparser = argparse.ArgumentParser(description=\'bigru_based_sketch_classifiction\')\nparser.add_argument(""--exp"", type=str, default=""bigru_001"", help=""experiment"")\n# TODO\nparser.add_argument(""--train_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/train/"", help=""train_sketch_coordinate_dir"")\nparser.add_argument(""--val_coordinate_path_root"", type=str, default=""/home/peng/dataset/tiny_quickdraw_coordinate/val/"", help=""val_sketch_coordinate_dir"")\nparser.add_argument(""--sketch_list"", type=str, default=""../../dataloader/tiny_train_set.txt"", help=""sketch_list_urls"")\nparser.add_argument(""--sketch_list_4_val"", type=str, default=""../../dataloader/tiny_val_set.txt"", help=""sketch_list_urls_4_validation"")\nparser.add_argument(""--batch_size"", type=int, default=256, help=""batch_size"")\nparser.add_argument(""--num_workers"", type=int, default=12, help=""num_workers"")\nparser.add_argument(\'--gpu\', type=str, default=""0"", help=\'choose GPU\')\n\n\nargs = parser.parse_args()\n\n\n# Part 2. configurations\n\n# Part 2-1. Basic configuration\nbasic_configs = collections.OrderedDict()\nbasic_configs[\'serial_number\'] = args.exp\nbasic_configs[\'learning_rate\'] = 1e-3\nbasic_configs[\'num_epochs\'] = 1000\nbasic_configs[""lr_protocol""] = [(30, 1e-3), (60, 0.001 * 0.9), (90, 0.001 * (0.9 ** 2)), (120, 0.001 * (0.9 ** 3)), (150, 0.001 * (0.9 ** 4)), (180, 0.001 * (0.9 ** 5)), (210, 1e-4), (240, 1e-5)]\nbasic_configs[""display_step""] = 10\nlr_protocol = basic_configs[""lr_protocol""]\n\n\n# Part 2-2. dataloader instantiation\ndataloader_configs = collections.OrderedDict()\ndataloader_configs[\'train_coordinate_path_root\'] = args.train_coordinate_path_root\ndataloader_configs[\'val_coordinate_path_root\'] = args.val_coordinate_path_root\ndataloader_configs[\'sketch_list\'] = args.sketch_list\ndataloader_configs[\'sketch_list_4_val\'] = args.sketch_list_4_val\ndataloader_configs[\'batch_size\'] = args.batch_size\ndataloader_configs[\'num_workers\'] = args.num_workers\n\ndataloader_configs[\'data_dict_4_train\'] = \'../../dataloader/tiny_train_dataset_dict.pickle\'\ndata_dict_4_train_f = open(dataloader_configs[\'data_dict_4_train\'], \'rb\')\ndata_dict_4_train = pickle.load(data_dict_4_train_f)  \n\ndataloader_configs[\'data_dict_4_validation\'] = \'../../dataloader/tiny_val_dataset_dict.pickle\' \ndata_dict_4_validation_f = open(dataloader_configs[\'data_dict_4_validation\'], \'rb\')\ndata_dict_4_validation = pickle.load(data_dict_4_validation_f)  \n\n#ipdb.set_trace()\n\n\n# create dataset\n# -----------------------------------------------------------------------------------------------------\n\ntrain_dataset = QuickdrawDataset(dataloader_configs[\'train_coordinate_path_root\'], dataloader_configs[\'sketch_list\'], data_dict_4_train)\ntrain_loader = DataLoader(train_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=True, num_workers=dataloader_configs[\'num_workers\'])\n\nval_dataset = QuickdrawDataset(dataloader_configs[\'val_coordinate_path_root\'], dataloader_configs[\'sketch_list_4_val\'], data_dict_4_validation)\nval_loader = DataLoader(val_dataset, batch_size=dataloader_configs[\'batch_size\'], shuffle=False, num_workers=dataloader_configs[\'num_workers\'])\n\n# Part 2-3. log configuration\nexp_dir = os.path.join(\'./experimental_results\', args.exp)\n\nexp_log_dir = os.path.join(exp_dir, ""log"")\nif not os.path.exists(exp_log_dir):\n    os.makedirs(exp_log_dir)\n\nexp_visual_dir = os.path.join(exp_dir, ""visual"")\nif not os.path.exists(exp_visual_dir):\n    os.makedirs(exp_visual_dir)\n\nexp_ckpt_dir = os.path.join(exp_dir, ""checkpoints"")\nif not os.path.exists(exp_ckpt_dir):\n    os.makedirs(exp_ckpt_dir)\n\nnow_str = datetime.datetime.now().__str__().replace(\' \', \'_\')\nwriter_path = os.path.join(exp_visual_dir, now_str)\nwriter = SummaryWriter(writer_path)\n\nlogger_path = os.path.join(exp_log_dir, now_str + "".log"")\nlogger = Logger(logger_path).get_logger()\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\nlogger.info(""basic configuration settings: {}"".format(basic_configs))\n\n\n# Part 2-4. configurations for loss function, and optimizer\n\nloss_function = nn.CrossEntropyLoss()\n\nnetwork_configs=collections.OrderedDict()\nnetwork_configs[\'coord_input_dim\']= 2\nnetwork_configs[\'embed_dim\']= 256\nnetwork_configs[\'feat_dict_size\']= 103\nnetwork_configs[\'hidden_size\']= 256\nnetwork_configs[\'num_layers\']= 5\nnetwork_configs[\'dropout\'] = 0.5\nnetwork_configs[\'num_classes\']= 345\n\n\nlogger.info(""network configuration settings: {}"".format(network_configs))\n\nnet = GRUNet(network_configs)\nnet = net.cuda()\n\n\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n\n\n# Part 3. \'train\' function\ndef train_function(epoch):\n    training_loss = AverageMeter()\n    training_acc = AverageMeter()\n    net.train()\n\n    lr = next((lr for (max_epoch, lr) in lr_protocol if max_epoch > epoch), lr_protocol[-1][1])\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    logger.info(""set learning rate to: {}"".format(lr))\n    \n    for idx, (coordinate, label, flag_bits, _, _, _, position_encoding) in enumerate(tqdm(train_loader, ascii=True)):\n\n        coordinate = coordinate.cuda()\n        label = label.cuda()\n        flag_bits = flag_bits.cuda()\n        position_encoding = position_encoding.cuda()\n        \n        \n        flag_bits.squeeze_(2)\n        position_encoding.squeeze_(2)\n     \n        optimizer.zero_grad()\n\n        output, _ = net(coordinate, flag_bits, position_encoding)\n        # ipdb.set_trace()\n\n    \n        batch_loss = loss_function(output, label)\n\n        batch_loss.backward()\n\n        optimizer.step()\n\n        training_loss.update(batch_loss.item(), coordinate.size(0))\n        \n        training_acc.update(accuracy(output, label, topk = (1,))[0].item(), coordinate.size(0))\n\n        if (idx + 1) % basic_configs[""display_step""] == 0:\n            logger.info(\n                ""==> Iteration [{}][{}/{}]:"".format(epoch + 1, idx + 1, len(train_loader)))\n            logger.info(""current batch loss: {}"".format(\n                batch_loss.item()))\n            logger.info(""average loss: {}"".format(\n                training_loss.avg))\n            logger.info(""average acc: {}"".format(training_acc.avg))  \n\n    \n    logger.info(""Begin Evaluating"")\n\n    validation_loss, validation_acc = validate_function()\n    \n    writer.add_scalars(""loss"", {\n        ""training_loss"":training_loss.avg,\n        ""validation_loss"":validation_loss.avg\n        }, epoch+1)\n    writer.add_scalars(""acc"", {\n        ""training_acc"":training_acc.avg,\n        ""validation_acc"":validation_acc.avg\n        }, epoch+1)\n\n\n# Part 4. \'validate\' function\ndef validate_function():\n    validation_loss = AverageMeter()\n    validation_acc = AverageMeter()\n    net.eval()\n      \n    with torch.no_grad():\n        for idx, (coordinate, label, flag_bits, _, _, _, position_encoding) in enumerate(tqdm(val_loader, ascii=True)):\n            \n            coordinate = coordinate.cuda()\n            label = label.cuda()\n            flag_bits = flag_bits.cuda()\n            position_encoding = position_encoding.cuda()\n\n            \n            flag_bits.squeeze_(2)\n            position_encoding.squeeze_(2)\n            \n            output, _ = net(coordinate, flag_bits, position_encoding)\n\n            batch_loss = loss_function(output, label)\n\n            validation_loss.update(batch_loss.item(), coordinate.size(0))\n            \n            validation_acc.update(accuracy(output, label, topk = (1,))[0].item(), coordinate.size(0))\n\n        logger.info(""==> Evaluation Result: "")\n        logger.info(""loss: {}  acc:{}"".format(validation_loss.avg, validation_acc.avg))\n\n    return validation_loss, validation_acc\n\n\n\n\n\n# Part 5. \'main\' function\nif __name__ == \'__main__\':\n\n    logger.info(""Begin Evaluating before training"")\n    validate_function()\n    \n    \n    logger.info(""training status: "")\n    for epoch in range(basic_configs[\'num_epochs\']):\n        logger.info(""Begin training epoch {}"".format(epoch + 1))\n        train_function(epoch)\n\n        net_checkpoint_name = args.exp + ""_net_epoch"" + str(epoch + 1)\n        net_checkpoint_path = os.path.join(exp_ckpt_dir, net_checkpoint_name)\n        net_state = {""epoch"": epoch + 1,\n                     ""network"": net.state_dict()}\n        torch.save(net_state, net_checkpoint_path)\n'"
