file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python 3.6\nimport os\nimport io\nimport re\nfrom setuptools import setup, find_packages\n\n\ndef read(*names, **kwargs):\n    with io.open(\n            os.path.join(os.path.dirname(__file__), *names),\n            encoding=kwargs.get(""encoding"", ""utf8"")) as fp:\n        return fp.read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\nwith open(\'README.md\') as f:\n    long_description = f.read()\n\nVERSION = find_version(\'torchnlp\', \'__init__.py\')\n\nsetup_info = dict(\n    # Metadata\n    name=\'pytorch-nlp\',\n    version=VERSION,\n    author=\'Michael Petrochuk\',\n    author_email=\'petrochukm@gmail.com\',\n    url=\'https://github.com/PetrochukM/PytorchNLP\',\n    description=\'Text utilities and datasets for PyTorch\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    license=\'BSD\',\n    install_requires=[\'numpy\', \'tqdm\'],\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n    keywords=\'pytorch nlp text torchtext torchnlp\',\n    python_requires=\'>=3.5\',\n\n    # Package info\n    packages=find_packages(exclude=[\'.vscode\', \'build_tools\', \'docs\', \'tests\']),\n    zip_safe=True)\n\nsetup(**setup_info)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/stable/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../torchnlp\'))\nimport torchnlp\n\n# -- Project information -----------------------------------------------------\n\nproject = \'PyTorch-NLP\'\ncopyright = \'2018, Michael Petrochuk\'\nauthor = \'Michael Petrochuk\'\n\n# The short X.Y version\nversion = torchnlp.__version__\n# The full version, including alpha/beta/rc tags\nrelease = torchnlp.__version__\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.napoleon\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = []\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# -- Options for HTML output -------------------------------------------------\n\nimport sphinx_rtd_theme\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\nhtml_logo = \'_static/img/logo.svg\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\ndef setup(app):\n    app.add_stylesheet(\'https://fonts.googleapis.com/css?family=Lato\')\n    app.add_stylesheet(\'css/pytorch_theme.css\')\n    app.add_stylesheet(\'css/pytorch_nlp_theme.css\')\n\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PyTorch-NLPdoc\'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'PyTorch-NLP.tex\', \'PyTorch-NLP Documentation\', \'Michael Petrochuk\', \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \'pytorch-nlp\', \'PyTorch-NLP Documentation\', [author], 1)]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'PyTorch-NLP\', \'PyTorch-NLP Documentation\', author, \'PyTorch-NLP\',\n     \'One line description of project.\', \'Miscellaneous\'),\n]\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n}\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n# -- A patch that prevents Sphinx from cross-referencing ivar tags -------\n# See http://stackoverflow.com/a/41184353/3343043\n\nfrom docutils import nodes\nfrom sphinx.util.docfields import TypedField\nfrom sphinx import addnodes\n\n\ndef patched_make_field(self, types, domain, items, **kw):\n    # `kw` catches `env=None` needed for newer sphinx while maintaining\n    #  backwards compatibility when passed along further down!\n\n    # type: (List, unicode, Tuple) -> nodes.field\n    def handle_item(fieldarg, content):\n        par = nodes.paragraph()\n        par += addnodes.literal_strong(\'\', fieldarg)  # Patch: this line added\n        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,\n        #                           addnodes.literal_strong))\n        if fieldarg in types:\n            par += nodes.Text(\' (\')\n            # NOTE: using .pop() here to prevent a single type node to be\n            # inserted twice into the doctree, which leads to\n            # inconsistencies later when references are resolved\n            fieldtype = types.pop(fieldarg)\n            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):\n                typename = u\'\'.join(n.astext() for n in fieldtype)\n                typename = typename.replace(\'int\', \'python:int\')\n                typename = typename.replace(\'long\', \'python:long\')\n                typename = typename.replace(\'float\', \'python:float\')\n                typename = typename.replace(\'type\', \'python:type\')\n                par.extend(\n                    self.make_xrefs(self.typerolename, domain, typename, addnodes.literal_emphasis,\n                                    **kw))\n            else:\n                par += fieldtype\n            par += nodes.Text(\')\')\n        par += nodes.Text(\' -- \')\n        par += content\n        return par\n\n    fieldname = nodes.field_name(\'\', self.label)\n    if len(items) == 1 and self.can_collapse:\n        fieldarg, content = items[0]\n        bodynode = handle_item(fieldarg, content)\n    else:\n        bodynode = self.list_type()\n        for fieldarg, content in items:\n            bodynode += nodes.list_item(\'\', handle_item(fieldarg, content))\n    fieldbody = nodes.field_body(\'\', bodynode)\n    return nodes.field(\'\', fieldname, fieldbody)\n\n\nTypedField.make_field = patched_make_field\n'"
tests/conftest.py,0,"b'import logging\nimport sys\n\nimport pytest\n\n\n# Add test environment variable before all tests\ndef pytest_sessionstart(session):\n    logging.basicConfig(\n        format=\'[%(asctime)s][%(processName)s][%(name)s][%(levelname)s] %(message)s\',\n        level=logging.INFO,\n        stream=sys.stdout)\n\n\ndef pytest_configure(config):\n    """"""Inject documentation.""""""\n    config.addinivalue_line(""markers"", ""capture_disabled: "")\n\n\n@pytest.hookimpl(hookwrapper=True)\ndef pytest_pyfunc_call(pyfuncitem):\n    if \'capture_disabled\' in pyfuncitem.keywords:\n        capmanager = pyfuncitem._request.config.pluginmanager.getplugin(\'capturemanager\')\n        capmanager.suspend_capture_item(pyfuncitem._request.node, ""call"", in_=True)\n\n        print(\'\')\n        print(\'\')\n        print(\'======= Capsys Disabled: {} ======= \'.format(pyfuncitem.obj.__name__))\n\n    try:\n        yield\n    finally:\n        if \'capture_disabled\' in pyfuncitem.keywords:\n            print(\'========= Capsys Enabled ========= \')\n            print(\'\')\n\n\n# REFERENCE: https://docs.pytest.org/en/latest/example/simple.html\n\n\ndef pytest_addoption(parser):\n    parser.addoption(""--runslow"", action=""store_true"", default=False, help=""run slow tests"")\n\n\ndef pytest_collection_modifyitems(config, items):\n    if config.getoption(""--runslow""):\n        # --runslow given in cli: do not skip slow tests\n        return\n    skip_slow = pytest.mark.skip(reason=""need --runslow option to run"")\n    for item in items:\n        if ""slow"" in item.keywords:\n            item.add_marker(skip_slow)\n'"
tests/test_download.py,0,"b""import urllib.request\n\nfrom tqdm import tqdm\n\nfrom torchnlp.download import _get_filename_from_url\nfrom torchnlp.download import _reporthook\n\n\ndef test_get_filename_from_url():\n    assert 'aclImdb_v1.tar.gz' in _get_filename_from_url(\n        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n    assert 'SimpleQuestions_v2.tgz' in _get_filename_from_url(\n        'https://www.dropbox.com/s/tohrsllcfy7rch4/SimpleQuestions_v2.tgz?raw=1')\n\n\ndef test_reporthook():\n    # Check that reporthook works with URLLIB\n    with tqdm(unit='B', unit_scale=True, miniters=1) as t:\n        urllib.request.urlretrieve('http://google.com', reporthook=_reporthook(t))\n"""
tests/test_random.py,3,"b'import random\n\nimport torch\nimport numpy\n\nfrom torchnlp.random import fork_rng\nfrom torchnlp.random import fork_rng_wrap\nfrom torchnlp.random import get_random_generator_state\nfrom torchnlp.random import set_random_generator_state\nfrom torchnlp.random import set_seed\n\n\ndef test_random_generator_state():\n    # TODO: Test `torch.cuda` random as well.\n    state = get_random_generator_state()\n    randint = random.randint(1, 2**31)\n    numpy_randint = numpy.random.randint(1, 2**31)\n    torch_randint = int(torch.randint(1, 2**31, (1,)))\n\n    set_random_generator_state(state)\n    post_randint = random.randint(1, 2**31)\n    post_numpy_randint = numpy.random.randint(1, 2**31)\n    post_torch_randint = int(torch.randint(1, 2**31, (1,)))\n\n    assert randint == post_randint\n    assert numpy_randint == post_numpy_randint\n    assert torch_randint == post_torch_randint\n\n\ndef test_set_seed__smoke_test():\n    set_seed(123)\n\n\ndef test_fork_rng_wrap():\n    set_seed(123)\n    pre_randint = [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n    @fork_rng_wrap()\n    def func():\n        random.randint(1, 2**31)\n\n    func()\n\n    post_randint = [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n    set_seed(123)\n    assert pre_randint != post_randint\n    assert pre_randint == [random.randint(1, 2**31), random.randint(1, 2**31)]\n    assert post_randint == [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n\ndef test_fork_rng():\n    set_seed(123)\n    pre_randint = [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n    with fork_rng(seed=123):\n        random.randint(1, 2**31)\n\n    post_randint = [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n    set_seed(123)\n    assert pre_randint != post_randint\n    assert pre_randint == [random.randint(1, 2**31), random.randint(1, 2**31)]\n    assert post_randint == [random.randint(1, 2**31), random.randint(1, 2**31)]\n'"
tests/test_utils.py,20,"b""from collections import namedtuple\nfrom functools import partial\nfrom unittest import mock\n\nimport pickle\n\nimport torch\n\nfrom torchnlp.encoders.text import stack_and_pad_tensors\nfrom torchnlp.utils import collate_tensors\nfrom torchnlp.utils import flatten_parameters\nfrom torchnlp.utils import get_tensors\nfrom torchnlp.utils import lengths_to_mask\nfrom torchnlp.utils import split_list\nfrom torchnlp.utils import tensors_to\nfrom torchnlp.utils import torch_equals_ignore_index\nfrom torchnlp.utils import get_total_parameters\n\n\nclass GetTensorsObjectMock(object):\n\n    class_attribute = torch.tensor([4, 5])\n\n    def __init__(self, recurse=True):\n        self.noise_int = 3\n        self.noise_str = 'abc'\n        self.instance_attribute = frozenset([torch.tensor([6, 7])])\n        if recurse:\n            self.object_ = GetTensorsObjectMock(recurse=False)\n\n    @property\n    def property_(self):\n        return torch.tensor([7, 8])\n\n\ndef test_get_tensors_list():\n    list_ = [torch.tensor([1, 2]), torch.tensor([2, 3])]\n    tensors = get_tensors(list_)\n    assert len(tensors) == 2\n\n\ndef test_get_tensors_dict():\n    list_ = [{'t': torch.tensor([1, 2])}, torch.tensor([2, 3])]\n    tensors = get_tensors(list_)\n    assert len(tensors) == 2\n\n\ndef test_get_tensors_tuple():\n    tuple_ = tuple([{'t': torch.tensor([1, 2])}, torch.tensor([2, 3])])\n    tensors = get_tensors(tuple_)\n    assert len(tensors) == 2\n\n\ndef test_get_tensors_object():\n    object_ = GetTensorsObjectMock()\n    tensors = get_tensors(object_)\n    assert len(tensors) == 5\n\n\ndef test_flatten_parameters():\n    rnn = torch.nn.LSTM(10, 20, 2)\n    rnn_pickle = pickle.dumps(rnn)\n    rnn2 = pickle.loads(rnn_pickle)\n    # Check that ``flatten_parameters`` works with a RNN module.\n    flatten_parameters(rnn2)\n\n\ndef test_get_total_parameters():\n    rnn = torch.nn.LSTM(10, 20, 2)\n    assert get_total_parameters(rnn) == 5920\n\n\ndef test_split_list():\n    assert split_list([1, 2, 3, 4, 5], (0.6, 0.4)) == [[1, 2, 3], [4, 5]]\n\n\ndef test_torch_equals_ignore_index():\n    source = torch.LongTensor([1, 2, 3])\n    target = torch.LongTensor([1, 2, 4])\n    assert torch_equals_ignore_index(source, target, ignore_index=3)\n    assert not torch_equals_ignore_index(source, target)\n\n\nTestTuple = namedtuple('TestTuple', ['t'])\n\n\ndef test_collate_tensors():\n\n    tensor = torch.Tensor(1)\n    collate_sequences = partial(collate_tensors, stack_tensors=stack_and_pad_tensors)\n    assert collate_sequences([tensor, tensor])[0].shape == (2, 1)\n    assert collate_sequences([[tensor], [tensor]])[0][0].shape == (2, 1)\n    assert collate_sequences([{'t': tensor}, {'t': tensor}])['t'][0].shape == (2, 1)\n    assert collate_sequences([TestTuple(t=tensor), TestTuple(t=tensor)]).t[0].shape == (2, 1)\n    assert collate_sequences(['test', 'test']) == ['test', 'test']\n\n\n@mock.patch('torch.is_tensor')\ndef test_tensors_to(mock_is_tensor):\n    TestTuple = namedtuple('TestTuple', ['t'])\n\n    mock_tensor = mock.Mock()\n    mock_is_tensor.side_effect = lambda m, **kwargs: m == mock_tensor\n    tensors_to(mock_tensor, device=torch.device('cpu'))\n    mock_tensor.to.called == 1\n    mock_tensor.to.reset_mock()\n\n    returned = tensors_to({'t': [mock_tensor]}, device=torch.device('cpu'))\n    mock_tensor.to.called == 1\n    mock_tensor.to.reset_mock()\n    assert isinstance(returned, dict)\n\n    returned = tensors_to([mock_tensor], device=torch.device('cpu'))\n    mock_tensor.to.called == 1\n    mock_tensor.to.reset_mock()\n    assert isinstance(returned, list)\n\n    returned = tensors_to(tuple([mock_tensor]), device=torch.device('cpu'))\n    mock_tensor.to.called == 1\n    mock_tensor.to.reset_mock()\n    assert isinstance(returned, tuple)\n\n    returned = tensors_to(TestTuple(t=mock_tensor), device=torch.device('cpu'))\n    mock_tensor.to.called == 1\n    mock_tensor.to.reset_mock()\n    assert isinstance(returned, TestTuple)\n\n\ndef test_lengths_to_mask():\n    assert lengths_to_mask([3]).sum() == 3\n    assert lengths_to_mask(torch.tensor(3)).sum() == 3\n    assert lengths_to_mask([1, 2, 3]).sum() == 6\n    assert lengths_to_mask([1, 2, 3])[0].sum() == 1\n    assert lengths_to_mask([1, 2, 3])[0][0].item() == 1\n    assert lengths_to_mask(torch.tensor([1, 2, 3]))[0][0].item() == 1\n    assert lengths_to_mask(torch.tensor([1.0, 2.0, 3.0]))[0][0].item() == 1\n"""
torchnlp/__init__.py,0,"b""__version__ = '0.5.0'\n"""
torchnlp/download.py,0,"b'from urllib.parse import urlparse\n\nimport logging\nimport os\nimport subprocess\nimport urllib.request\nimport zipfile\n\nfrom torchnlp._third_party.lazy_loader import LazyLoader\nfrom tqdm import tqdm\n\nrequests = LazyLoader(\'requests\', globals(), \'requests\')\n\nlogger = logging.getLogger(__name__)\n\n\ndef _reporthook(t):\n    """""" ``reporthook`` to use with ``urllib.request`` that prints the process of the download.\n\n    Uses ``tqdm`` for progress bar.\n\n    **Reference:**\n    https://github.com/tqdm/tqdm\n\n    Args:\n        t (tqdm.tqdm) Progress bar.\n\n    Example:\n        >>> with tqdm(unit=\'B\', unit_scale=True, miniters=1, desc=filename) as t:  # doctest: +SKIP\n        ...   urllib.request.urlretrieve(file_url, filename=full_path, reporthook=reporthook(t))\n    """"""\n    last_b = [0]\n\n    def inner(b=1, bsize=1, tsize=None):\n        """"""\n        Args:\n            b (int, optional): Number of blocks just transferred [default: 1].\n            bsize (int, optional): Size of each block (in tqdm units) [default: 1].\n            tsize (int, optional): Total size (in tqdm units). If [default: None] remains unchanged.\n        """"""\n        if tsize is not None:\n            t.total = tsize\n        t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n\n    return inner\n\n\ndef _download_file_from_drive(filename, url):  # pragma: no cover\n    """""" Download filename from google drive unless it\'s already in directory.\n\n    Args:\n        filename (str): Name of the file to download to (do nothing if it already exists).\n        url (str): URL to download from.\n    """"""\n    confirm_token = None\n\n    # Since the file is big, drive will scan it for virus and take it to a\n    # warning page. We find the confirm token on this page and append it to the\n    # URL to start the download process.\n    confirm_token = None\n    session = requests.Session()\n    response = session.get(url, stream=True)\n    for k, v in response.cookies.items():\n        if k.startswith(""download_warning""):\n            confirm_token = v\n\n    if confirm_token:\n        url = url + ""&confirm="" + confirm_token\n\n    logger.info(""Downloading %s to %s"" % (url, filename))\n\n    response = session.get(url, stream=True)\n    # Now begin the download.\n    chunk_size = 16 * 1024\n    with open(filename, ""wb"") as f:\n        for chunk in response.iter_content(chunk_size):\n            if chunk:\n                f.write(chunk)\n\n    # Print newline to clear the carriage return from the download progress\n    statinfo = os.stat(filename)\n    logger.info(""Successfully downloaded %s, %s bytes."" % (filename, statinfo.st_size))\n\n\ndef _maybe_extract(compressed_filename, directory, extension=None):\n    """""" Extract a compressed file to ``directory``.\n\n    Args:\n        compressed_filename (str): Compressed file.\n        directory (str): Extract to directory.\n        extension (str, optional): Extension of the file; Otherwise, attempts to extract extension\n            from the filename.\n    """"""\n    logger.info(\'Extracting {}\'.format(compressed_filename))\n\n    if extension is None:\n        basename = os.path.basename(compressed_filename)\n        extension = basename.split(\'.\', 1)[1]\n\n    if \'zip\' in extension:\n        with zipfile.ZipFile(compressed_filename, ""r"") as zip_:\n            zip_.extractall(directory)\n    elif \'tar.gz\' in extension or \'tgz\' in extension:\n        # `tar` is much faster than python\'s `tarfile` implementation\n        subprocess.call([\'tar\', \'-C\', directory, \'-zxvf\', compressed_filename])\n    elif \'tar\' in extension:\n        subprocess.call([\'tar\', \'-C\', directory, \'-xvf\', compressed_filename])\n\n    logger.info(\'Extracted {}\'.format(compressed_filename))\n\n\ndef _get_filename_from_url(url):\n    """""" Return a filename from a URL\n\n    Args:\n        url (str): URL to extract filename from\n\n    Returns:\n        (str): Filename in URL\n    """"""\n    parse = urlparse(url)\n    return os.path.basename(parse.path)\n\n\ndef download_file_maybe_extract(url, directory, filename=None, extension=None, check_files=[]):\n    """""" Download the file at ``url`` to ``directory``. Extract to ``directory`` if tar or zip.\n\n    Args:\n        url (str or Path): Url of file.\n        directory (str): Directory to download to.\n        filename (str, optional): Name of the file to download; Otherwise, a filename is extracted\n            from the url.\n        extension (str, optional): Extension of the file; Otherwise, attempts to extract extension\n            from the filename.\n        check_files (list of str or Path): Check if these files exist, ensuring the download\n            succeeded. If these files exist before the download, the download is skipped.\n\n    Returns:\n        (str): Filename of download file.\n\n    Raises:\n        ValueError: Error if one of the ``check_files`` are not found following the download.\n    """"""\n    if filename is None:\n        filename = _get_filename_from_url(url)\n\n    directory = str(directory)\n    filepath = os.path.join(directory, filename)\n    check_files = [os.path.join(directory, str(f)) for f in check_files]\n\n    if len(check_files) > 0 and _check_download(*check_files):\n        return filepath\n\n    if not os.path.isdir(directory):\n        os.makedirs(directory)\n\n    logger.info(\'Downloading {}\'.format(filename))\n\n    # Download\n    if \'drive.google.com\' in url:\n        _download_file_from_drive(filepath, url)\n    else:\n        with tqdm(unit=\'B\', unit_scale=True, miniters=1, desc=filename) as t:\n            urllib.request.urlretrieve(url, filename=filepath, reporthook=_reporthook(t))\n\n    _maybe_extract(compressed_filename=filepath, directory=directory, extension=extension)\n\n    if not _check_download(*check_files):\n        raise ValueError(\'[DOWNLOAD FAILED] `*check_files` not found\')\n\n    return filepath\n\n\ndef _check_download(*filepaths):\n    """""" Check if the downloaded files are found.\n\n    Args:\n        filepaths (list of str): Check if these filepaths exist\n\n    Returns:\n        (bool): Returns True if all filepaths exist\n    """"""\n    return all([os.path.isfile(filepath) for filepath in filepaths])\n\n\ndef download_files_maybe_extract(urls, directory, check_files=[]):\n    """""" Download the files at ``urls`` to ``directory``. Extract to ``directory`` if tar or zip.\n\n    Args:\n        urls (str): Url of files.\n        directory (str): Directory to download to.\n        check_files (list of str): Check if these files exist, ensuring the download succeeded.\n            If these files exist before the download, the download is skipped.\n\n    Raises:\n        ValueError: Error if one of the ``check_files`` are not found following the download.\n    """"""\n    check_files = [os.path.join(directory, f) for f in check_files]\n    if _check_download(*check_files):\n        return\n\n    for url in urls:\n        download_file_maybe_extract(url=url, directory=directory)\n\n    if not _check_download(*check_files):\n        raise ValueError(\'[DOWNLOAD FAILED] `*check_files` not found\')\n'"
torchnlp/random.py,12,"b'from collections import namedtuple\nfrom contextlib import contextmanager\n\nimport functools\nimport random\n\nimport numpy as np\nimport torch\n\nRandomGeneratorState = namedtuple(\'RandomGeneratorState\',\n                                  [\'random\', \'torch\', \'numpy\', \'torch_cuda\'])\n\n\ndef get_random_generator_state(cuda=torch.cuda.is_available()):\n    """""" Get the `torch`, `numpy` and `random` random generator state.\n\n    Args:\n        cuda (bool, optional): If `True` saves the `cuda` seed also. Note that getting and setting\n            the random generator state for CUDA can be quite slow if you have a lot of GPUs.\n\n    Returns:\n        RandomGeneratorState\n    """"""\n    return RandomGeneratorState(random.getstate(), torch.random.get_rng_state(),\n                                np.random.get_state(),\n                                torch.cuda.get_rng_state_all() if cuda else None)\n\n\ndef set_random_generator_state(state):\n    """""" Set the `torch`, `numpy` and `random` random generator state.\n\n    Args:\n        state (RandomGeneratorState)\n    """"""\n    random.setstate(state.random)\n    torch.random.set_rng_state(state.torch)\n    np.random.set_state(state.numpy)\n    if state.torch_cuda is not None and torch.cuda.is_available() and len(\n            state.torch_cuda) == torch.cuda.device_count():  # pragma: no cover\n        torch.cuda.set_rng_state_all(state.torch_cuda)\n\n\n@contextmanager\ndef fork_rng(seed=None, cuda=torch.cuda.is_available()):\n    """""" Forks the `torch`, `numpy` and `random` random generators, so that when you return, the\n    random generators are reset to the state that they were previously in.\n\n    Args:\n        seed (int or None, optional): If defined this sets the seed values for the random\n            generator fork. This is a convenience parameter.\n        cuda (bool, optional): If `True` saves the `cuda` seed also. Getting and setting the random\n            generator state can be quite slow if you have a lot of GPUs.\n    """"""\n    state = get_random_generator_state(cuda)\n    if seed is not None:\n        set_seed(seed, cuda)\n    try:\n        yield\n    finally:\n        set_random_generator_state(state)\n\n\ndef fork_rng_wrap(function=None, **kwargs):\n    """""" Decorator alias for `fork_rng`.\n    """"""\n    if not function:\n        return functools.partial(fork_rng_wrap, **kwargs)\n\n    @functools.wraps(function)\n    def wrapper():\n        with fork_rng(**kwargs):\n            return function()\n\n    return wrapper\n\n\ndef set_seed(seed, cuda=torch.cuda.is_available()):\n    """""" Set seed values for random generators.\n\n    Args:\n        seed (int): Value used as a seed.\n        cuda (bool, optional): If `True` sets the `cuda` seed also.\n    """"""\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if cuda:  # pragma: no cover\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n'"
torchnlp/utils.py,26,"b'import logging\nimport inspect\nimport collections\n\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_tensors(object_, seen=set()):\n    if torch.is_tensor(object_):\n        return [object_]\n\n    elif isinstance(object_, (str, float, int)) or id(object_) in seen:\n        return []\n\n    seen.add(id(object_))\n    tensors = set()\n\n    if isinstance(object_, collections.abc.Mapping):\n        for value in object_.values():\n            tensors.update(_get_tensors(value, seen))\n    elif isinstance(object_, collections.abc.Iterable):\n        for value in object_:\n            tensors.update(_get_tensors(value, seen))\n    else:\n        members = [\n            value for key, value in inspect.getmembers(object_)\n            if not isinstance(value, (collections.abc.Callable, type(None)))\n        ]\n        tensors.update(_get_tensors(members, seen))\n\n    return tensors\n\n\ndef get_tensors(object_):\n    """""" Get all tensors associated with ``object_``\n\n    Args:\n        object_ (any): Any object to look for tensors.\n\n    Returns:\n        (list of torch.tensor): List of tensors that are associated with ``object_``.\n    """"""\n    return _get_tensors(object_)\n\n\ndef sampler_to_iterator(dataset, sampler):\n    """""" Given a batch sampler or sampler returns examples instead of indices\n\n    Args:\n        dataset (torch.utils.data.Dataset): Dataset to sample from.\n        sampler (torch.utils.data.sampler.Sampler): Sampler over the dataset.\n\n    Returns:\n        generator over dataset examples\n    """"""\n    for sample in sampler:\n        if isinstance(sample, (list, tuple)):\n            # yield a batch\n            yield [dataset[i] for i in sample]\n        else:\n            # yield a single example\n            yield dataset[sample]\n\n\ndef flatten_parameters(model):\n    """""" ``flatten_parameters`` of a RNN model loaded from disk. """"""\n    model.apply(lambda m: m.flatten_parameters() if hasattr(m, \'flatten_parameters\') else None)\n\n\ndef split_list(list_, splits):\n    """""" Split ``list_`` using the ``splits`` ratio.\n\n    Args:\n        list_ (list): List to split.\n        splits (tuple): Tuple of decimals determining list splits summing up to 1.0.\n\n    Returns:\n        (list): Splits of the list.\n\n    Example:\n        >>> dataset = [1, 2, 3, 4, 5]\n        >>> split_list(dataset, splits=(.6, .2, .2))\n        [[1, 2, 3], [4], [5]]\n    """"""\n    assert sum(splits) == 1, \'Splits must sum to 1.0\'\n    splits = [round(s * len(list_)) for s in splits]\n    lists = []\n    for split in splits[:-1]:\n        lists.append(list_[:split])\n        list_ = list_[split:]\n    lists.append(list_)\n    return lists\n\n\ndef get_total_parameters(model):\n    """""" Return the total number of trainable parameters in ``model``.\n\n    Args:\n        model (torch.nn.Module)\n\n    Returns:\n        (int): The total number of trainable parameters in ``model``.\n    """"""\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef torch_equals_ignore_index(tensor, tensor_other, ignore_index=None):\n    """"""\n    Compute ``torch.equal`` with the optional mask parameter.\n\n    Args:\n        ignore_index (int, optional): Specifies a ``tensor`` index that is ignored.\n\n    Returns:\n        (bool) Returns ``True`` if target and prediction are equal.\n    """"""\n    if ignore_index is not None:\n        assert tensor.size() == tensor_other.size()\n        mask_arr = tensor.ne(ignore_index)\n        tensor = tensor.masked_select(mask_arr)\n        tensor_other = tensor_other.masked_select(mask_arr)\n\n    return torch.equal(tensor, tensor_other)\n\n\ndef is_namedtuple(object_):\n    return hasattr(object_, \'_asdict\') and isinstance(object_, tuple)\n\n\ndef lengths_to_mask(*lengths, **kwargs):\n    """""" Given a list of lengths, create a batch mask.\n\n    Example:\n        >>> lengths_to_mask([1, 2, 3])\n        tensor([[ True, False, False],\n                [ True,  True, False],\n                [ True,  True,  True]])\n        >>> lengths_to_mask([1, 2, 2], [1, 2, 2])\n        tensor([[[ True, False],\n                 [False, False]],\n        <BLANKLINE>\n                [[ True,  True],\n                 [ True,  True]],\n        <BLANKLINE>\n                [[ True,  True],\n                 [ True,  True]]])\n\n    Args:\n        *lengths (list of int or torch.Tensor)\n        **kwargs: Keyword arguments passed to ``torch.zeros`` upon initially creating the returned\n          tensor.\n\n    Returns:\n        torch.BoolTensor\n    """"""\n    # Squeeze to deal with random additional dimensions\n    lengths = [l.squeeze().tolist() if torch.is_tensor(l) else l for l in lengths]\n\n    # For cases where length is a scalar, this needs to convert it to a list.\n    lengths = [l if isinstance(l, list) else [l] for l in lengths]\n    assert all(len(l) == len(lengths[0]) for l in lengths)\n    batch_size = len(lengths[0])\n    other_dimensions = tuple([int(max(l)) for l in lengths])\n    mask = torch.zeros(batch_size, *other_dimensions, **kwargs)\n    for i, length in enumerate(zip(*tuple(lengths))):\n        mask[i][[slice(int(l)) for l in length]].fill_(1)\n    return mask.bool()\n\n\ndef collate_tensors(batch, stack_tensors=torch.stack):\n    """""" Collate a list of type ``k`` (dict, namedtuple, list, etc.) with tensors.\n\n    Inspired by:\n    https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py#L31\n\n    Args:\n        batch (list of k): List of rows of type ``k``.\n        stack_tensors (callable): Function to stack tensors into a batch.\n\n    Returns:\n        k: Collated batch of type ``k``.\n\n    Example use case:\n        This is useful with ``torch.utils.data.dataloader.DataLoader`` which requires a collate\n        function. Typically, when collating sequences you\'d set\n        ``collate_fn=partial(collate_tensors, stack_tensors=encoders.text.stack_and_pad_tensors)``.\n\n    Example:\n\n        >>> import torch\n        >>> batch = [\n        ...   { \'column_a\': torch.randn(5), \'column_b\': torch.randn(5) },\n        ...   { \'column_a\': torch.randn(5), \'column_b\': torch.randn(5) },\n        ... ]\n        >>> collated = collate_tensors(batch)\n        >>> {k: t.size() for (k, t) in collated.items()}\n        {\'column_a\': torch.Size([2, 5]), \'column_b\': torch.Size([2, 5])}\n    """"""\n    if all([torch.is_tensor(b) for b in batch]):\n        return stack_tensors(batch)\n    if (all([isinstance(b, dict) for b in batch]) and\n            all([b.keys() == batch[0].keys() for b in batch])):\n        return {key: collate_tensors([d[key] for d in batch], stack_tensors) for key in batch[0]}\n    elif all([is_namedtuple(b) for b in batch]):  # Handle ``namedtuple``\n        return batch[0].__class__(**collate_tensors([b._asdict() for b in batch], stack_tensors))\n    elif all([isinstance(b, list) for b in batch]):\n        # Handle list of lists such each list has some column to be batched, similar to:\n        # [[\'a\', \'b\'], [\'a\', \'b\']] \xe2\x86\x92 [[\'a\', \'a\'], [\'b\', \'b\']]\n        transposed = zip(*batch)\n        return [collate_tensors(samples, stack_tensors) for samples in transposed]\n    else:\n        return batch\n\n\ndef tensors_to(tensors, *args, **kwargs):\n    """""" Apply ``torch.Tensor.to`` to tensors in a generic data structure.\n\n    Inspired by:\n    https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py#L31\n\n    Args:\n        tensors (tensor, dict, list, namedtuple or tuple): Data structure with tensor values to\n            move.\n        *args: Arguments passed to ``torch.Tensor.to``.\n        **kwargs: Keyword arguments passed to ``torch.Tensor.to``.\n\n    Example use case:\n        This is useful as a complementary function to ``collate_tensors``. Following collating,\n        it\'s important to move your tensors to the appropriate device.\n\n    Returns:\n        The inputted ``tensors`` with ``torch.Tensor.to`` applied.\n\n    Example:\n\n        >>> import torch\n        >>> batch = [\n        ...   { \'column_a\': torch.randn(5), \'column_b\': torch.randn(5) },\n        ...   { \'column_a\': torch.randn(5), \'column_b\': torch.randn(5) },\n        ... ]\n        >>> tensors_to(batch, torch.device(\'cpu\'))  # doctest: +ELLIPSIS\n        [{\'column_a\': tensor(...}]\n    """"""\n    if torch.is_tensor(tensors):\n        return tensors.to(*args, **kwargs)\n    elif isinstance(tensors, dict):\n        return {k: tensors_to(v, *args, **kwargs) for k, v in tensors.items()}\n    elif hasattr(tensors, \'_asdict\') and isinstance(tensors, tuple):  # Handle ``namedtuple``\n        return tensors.__class__(**tensors_to(tensors._asdict(), *args, **kwargs))\n    elif isinstance(tensors, list):\n        return [tensors_to(t, *args, **kwargs) for t in tensors]\n    elif isinstance(tensors, tuple):\n        return tuple([tensors_to(t, *args, **kwargs) for t in tensors])\n    else:\n        return tensors\n\n\ndef identity(x):\n    return x\n'"
examples/awd-lstm-lm/main.py,14,"b'import argparse\nimport time\nimport math\nimport numpy as np\nimport torch\nimport model\n\nfrom utils import repackage_hidden\n\nparser = argparse.ArgumentParser(description=\'PyTorch PennTreeBank RNN/LSTM Language Model\')\nparser.add_argument(\n    \'--data\', type=str, default=\'penn_treebank_dataset\', help=\'the name of the dataset to load\')\nparser.add_argument(\n    \'--model\', type=str, default=\'LSTM\', help=\'type of recurrent net (LSTM, QRNN, GRU)\')\nparser.add_argument(\'--emsize\', type=int, default=400, help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=1150, help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=3, help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=30, help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.25, help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=8000, help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=80, metavar=\'N\', help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=70, help=\'sequence length\')\nparser.add_argument(\n    \'--dropout\', type=float, default=0.4, help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\n    \'--dropouth\', type=float, default=0.3, help=\'dropout for rnn layers (0 = no dropout)\')\nparser.add_argument(\n    \'--dropouti\',\n    type=float,\n    default=0.65,\n    help=\'dropout for input embedding layers (0 = no dropout)\')\nparser.add_argument(\n    \'--dropoute\',\n    type=float,\n    default=0.1,\n    help=\'dropout to remove words from embedding layer (0 = no dropout)\')\nparser.add_argument(\n    \'--wdrop\',\n    type=float,\n    default=0.5,\n    help=\'amount of weight dropout to apply to the RNN hidden to hidden matrix\')\nparser.add_argument(\'--seed\', type=int, default=1111, help=\'random seed\')\nparser.add_argument(\'--nonmono\', type=int, default=5, help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_true\', default=False, help=\'use CUDA\')\nparser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\', help=\'report interval\')\nrandomhash = \'\'.join(str(time.time()).split(\'.\'))\nparser.add_argument(\n    \'--save\', type=str, default=randomhash + \'.pt\', help=\'path to save the final model\')\nparser.add_argument(\n    \'--alpha\',\n    type=float,\n    default=2,\n    help=\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\')\nparser.add_argument(\n    \'--beta\',\n    type=float,\n    default=1,\n    help=\'beta slowness regularization applied on RNN activiation\' +\n    \' (beta = 0 means no regularization)\')\nparser.add_argument(\n    \'--wdecay\', type=float, default=1.2e-6, help=\'weight decay applied to all weights\')\nparser.add_argument(\'--resume\', type=str, default=\'\', help=\'path of model to resume\')\nparser.add_argument(\'--optimizer\', type=str, default=\'sgd\', help=\'optimizer to use (sgd, adam)\')\nparser.add_argument(\n    \'--when\',\n    nargs=""+"",\n    type=int,\n    default=[-1],\n    help=\'When (which epochs) to divide the learning rate by 10 - accepts multiple\')\nargs = parser.parse_args()\nargs.tied = True\n\n# Set the random seed manually for reproducibility.\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n###############################################################################\n# Load data\n###############################################################################\n\n\ndef model_save(fn):\n    with open(fn, \'wb\') as f:\n        torch.save([model, criterion, optimizer], f)\n\n\ndef model_load(fn):\n    global model, criterion, optimizer\n    with open(fn, \'rb\') as f:\n        model, criterion, optimizer = torch.load(f)\n\n\nfrom torchnlp import datasets\nfrom torchnlp.encoders import LabelEncoder\nfrom torchnlp.samplers import BPTTBatchSampler\n\nprint(\'Producing dataset...\')\ntrain, val, test = getattr(datasets, args.data)(train=True, dev=True, test=True)\n\nencoder = LabelEncoder(train + val + test)\n\ntrain_data = encoder.batch_encode(train)\nval_data = encoder.batch_encode(val)\ntest_data = encoder.batch_encode(test)\n\neval_batch_size = 10\ntest_batch_size = 1\n\ntrain_source_sampler, val_source_sampler, test_source_sampler = tuple(\n    [BPTTBatchSampler(d, args.bptt, args.batch_size, True, \'source\') for d in (train, val, test)])\n\ntrain_target_sampler, val_target_sampler, test_target_sampler = tuple(\n    [BPTTBatchSampler(d, args.bptt, args.batch_size, True, \'target\') for d in (train, val, test)])\n\n###############################################################################\n# Build the model\n###############################################################################\n\nfrom splitcross import SplitCrossEntropyLoss\ncriterion = None\n\nntokens = encoder.vocab_size\nmodel = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout,\n                       args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n###\nif args.resume:\n    print(\'Resuming model ...\')\n    model_load(args.resume)\n    optimizer.param_groups[0][\'lr\'] = args.lr\n    model.dropouti, model.dropouth, model.dropout, args.dropoute = (args.dropouti, args.dropouth,\n                                                                    args.dropout, args.dropoute)\n    if args.wdrop:\n        from weight_drop import WeightDrop\n        for rnn in model.rnns:\n            if type(rnn) == WeightDrop:\n                rnn.dropout = args.wdrop\n            elif rnn.zoneout > 0:\n                rnn.zoneout = args.wdrop\n###\nif not criterion:\n    splits = []\n    if ntokens > 500000:\n        # One Billion\n        # This produces fairly even matrix mults for the buckets:\n        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n        splits = [4200, 35000, 180000]\n    elif ntokens > 75000:\n        # WikiText-103\n        splits = [2800, 20000, 76000]\n    print(\'Using\', splits)\n    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n###\nif args.cuda:\n    model = model.cuda()\n    criterion = criterion.cuda()\n###\nparams = list(model.parameters()) + list(criterion.parameters())\ntotal_params = sum(\n    x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\nprint(\'Args:\', args)\nprint(\'Model total parameters:\', total_params)\n\n###############################################################################\n# Training code\n###############################################################################\n\n\ndef evaluate(data_source, source_sampler, target_sampler, batch_size=10):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    if args.model == \'QRNN\':\n        model.reset()\n    total_loss = 0\n    hidden = model.init_hidden(batch_size)\n\n    for source_sample, target_sample in zip(source_sampler, target_sampler):\n        model.train()\n        data = torch.stack([data_source[i] for i in source_sample])\n        targets = torch.stack([data_source[i] for i in target_sample]).view(-1)\n        with torch.no_grad():\n            output, hidden = model(data, hidden)\n        total_loss += len(data) * criterion(model.decoder.weight, model.decoder.bias, output,\n                                            targets).item()\n        hidden = repackage_hidden(hidden)\n    return total_loss / len(data_source)\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    if args.model == \'QRNN\':\n        model.reset()\n    total_loss = 0\n    start_time = time.time()\n    hidden = model.init_hidden(args.batch_size)\n    batch = 0\n    for source_sample, target_sample in zip(train_source_sampler, train_target_sampler):\n        model.train()\n        data = torch.stack([train_data[i] for i in source_sample]).t_().contiguous()\n        targets = torch.stack([train_data[i] for i in target_sample]).t_().contiguous().view(-1)\n\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n        hidden = repackage_hidden(hidden)\n        optimizer.zero_grad()\n\n        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h=True)\n        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n\n        loss = raw_loss\n        # Activiation Regularization\n        if args.alpha:\n            loss = loss + sum(\n                args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n        # Temporal Activation Regularization (slowness)\n        if args.beta:\n            loss = loss + sum(\n                args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        if args.clip:\n            torch.nn.utils.clip_grad_norm_(params, args.clip)\n        optimizer.step()\n\n        total_loss += raw_loss.item()\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss / args.log_interval\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | ms/batch {:5.2f} | \'\n                  \'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}\'.format(\n                      epoch, batch,\n                      len(train_source_sampler) // args.bptt,\n                      optimizer.param_groups[0][\'lr\'], elapsed * 1000 / args.log_interval, cur_loss,\n                      math.exp(cur_loss), cur_loss / math.log(2)))\n            total_loss = 0\n            start_time = time.time()\n        ###\n        batch += 1\n\n\n# Loop over epochs.\nlr = args.lr\nbest_val_loss = []\nstored_loss = 100000000\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    optimizer = None\n    # Ensure the optimizer is optimizing params, which includes both the model\'s weights as well as\n    # the criterion\'s weight (i.e. Adaptive Softmax)\n    if args.optimizer == \'sgd\':\n        optimizer = torch.optim.SGD(params, lr=args.lr, weight_decay=args.wdecay)\n    if args.optimizer == \'adam\':\n        optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.wdecay)\n    for epoch in range(1, args.epochs + 1):\n        epoch_start_time = time.time()\n        train()\n        if \'t0\' in optimizer.param_groups[0]:\n            tmp = {}\n            for prm in model.parameters():\n                tmp[prm] = prm.data.clone()\n                prm.data = optimizer.state[prm][\'ax\'].clone()\n\n            val_loss2 = evaluate(val_data, val_source_sampler, val_target_sampler)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \'\n                  \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n                      epoch, (time.time() - epoch_start_time), val_loss2, math.exp(val_loss2),\n                      val_loss2 / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss2 < stored_loss:\n                model_save(args.save)\n                print(\'Saving Averaged!\')\n                stored_loss = val_loss2\n\n            for prm in model.parameters():\n                prm.data = tmp[prm].clone()\n\n        else:\n            val_loss = evaluate(val_data, val_source_sampler, val_target_sampler, eval_batch_size)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \'\n                  \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n                      epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss),\n                      val_loss / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss < stored_loss:\n                model_save(args.save)\n                print(\'Saving model (new best validation)\')\n                stored_loss = val_loss\n\n            if args.optimizer == \'sgd\' and \'t0\' not in optimizer.param_groups[0] and (\n                    len(best_val_loss) > args.nonmono and\n                    val_loss > min(best_val_loss[:-args.nonmono])):\n                print(\'Switching to ASGD\')\n                optimizer = torch.optim.ASGD(\n                    model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n\n            if epoch in args.when:\n                print(\'Saving model before learning rate decreased\')\n                model_save(\'{}.e{}\'.format(args.save, epoch))\n                print(\'Dividing learning rate by 10\')\n                optimizer.param_groups[0][\'lr\'] /= 10.\n\n            best_val_loss.append(val_loss)\n\nexcept KeyboardInterrupt:\n    print(\'-\' * 89)\n    print(\'Exiting from training early\')\n\n# Load the best saved model.\nmodel_load(args.save)\n\n# Run on test data.\ntest_loss = evaluate(test_data, test_source_sampler, test_target_sampler, test_batch_size)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    test_loss, math.exp(test_loss), test_loss / math.log(2)))\nprint(\'=\' * 89)\n'"
examples/awd-lstm-lm/model.py,4,"b'import torch\nimport torch.nn as nn\n\nfrom torchnlp.nn import LockedDropout\nfrom torchnlp.nn import WeightDrop\n\n\nclass RNNModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a decoder.""""""\n\n    def __init__(self,\n                 rnn_type,\n                 ntoken,\n                 ninp,\n                 nhid,\n                 nlayers,\n                 dropout=0.5,\n                 dropouth=0.5,\n                 dropouti=0.5,\n                 dropoute=0.1,\n                 wdrop=0,\n                 tie_weights=False):\n        super(RNNModel, self).__init__()\n        self.emb_drop = LockedDropout(dropouti)\n        self.idrop = nn.Dropout(dropouti)\n        self.hdrop = LockedDropout(dropouth)\n        self.drop = LockedDropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        assert rnn_type in [\'LSTM\', \'QRNN\', \'GRU\'], \'RNN type is not supported\'\n        if rnn_type == \'LSTM\':\n            self.rnns = [\n                torch.nn.LSTM(\n                    ninp if l == 0 else nhid,\n                    nhid if l != nlayers - 1 else (ninp if tie_weights else nhid),\n                    1,\n                    dropout=0) for l in range(nlayers)\n            ]\n            if wdrop:\n                self.rnns = [WeightDrop(rnn, [\'weight_hh_l0\'], dropout=wdrop) for rnn in self.rnns]\n        if rnn_type == \'GRU\':\n            self.rnns = [\n                torch.nn.GRU(\n                    ninp if l == 0 else nhid, nhid if l != nlayers - 1 else ninp, 1, dropout=0)\n                for l in range(nlayers)\n            ]\n            if wdrop:\n                self.rnns = [WeightDrop(rnn, [\'weight_hh_l0\'], dropout=wdrop) for rnn in self.rnns]\n        elif rnn_type == \'QRNN\':\n            from torchqrnn import QRNNLayer\n            self.rnns = [\n                QRNNLayer(\n                    input_size=ninp if l == 0 else nhid,\n                    hidden_size=nhid if l != nlayers - 1 else (ninp if tie_weights else nhid),\n                    save_prev_x=True,\n                    zoneout=0,\n                    window=2 if l == 0 else 1,\n                    output_gate=True) for l in range(nlayers)\n            ]\n            for rnn in self.rnns:\n                rnn.linear = WeightDrop(rnn.linear, [\'weight\'], dropout=wdrop)\n        print(self.rnns)\n        self.rnns = torch.nn.ModuleList(self.rnns)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        # Optionally tie weights as in:\n        # ""Using the Output Embedding to Improve Language Models"" (Press & Wolf 2016)\n        # https://arxiv.org/abs/1608.05859\n        # and\n        # ""Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            #if nhid != ninp:\n            #    raise ValueError(\'When using the tied flag, nhid must be equal to emsize\')\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n\n        self.rnn_type = rnn_type\n        self.ninp = ninp\n        self.nhid = nhid\n        self.nlayers = nlayers\n        self.dropout = dropout\n        self.dropouti = dropouti\n        self.dropouth = dropouth\n        self.dropoute = dropoute\n        self.tie_weights = tie_weights\n\n    def reset(self):\n        if self.rnn_type == \'QRNN\':\n            [r.reset() for r in self.rnns]\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden, return_h=False):\n        #emb = self.idrop(emb)\n        emb = self.encoder(input)\n        emb = self.emb_drop(emb)\n\n        raw_output = emb\n        new_hidden = []\n        #raw_output, hidden = self.rnn(emb, hidden)\n        raw_outputs = []\n        outputs = []\n        for l, rnn in enumerate(self.rnns):\n            current_input = raw_output\n            raw_output, new_h = rnn(raw_output, hidden[l])\n            new_hidden.append(new_h)\n            raw_outputs.append(raw_output)\n            if l != self.nlayers - 1:\n                raw_output = self.hdrop(raw_output)\n                outputs.append(raw_output)\n        hidden = new_hidden\n\n        output = self.drop(raw_output)\n        outputs.append(output)\n\n        result = output.view(output.size(0) * output.size(1), output.size(2))\n        if return_h:\n            return result, hidden, raw_outputs, outputs\n        return result, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == \'LSTM\':\n            return [(weight.new_zeros(1, bsz, self.nhid if l != self.nlayers - 1 else\n                                      (self.ninp if self.tie_weights else self.nhid)),\n                     weight.new_zeros(1, bsz, self.nhid if l != self.nlayers - 1 else\n                                      (self.ninp if self.tie_weights else self.nhid)))\n                    for l in range(self.nlayers)]\n        elif self.rnn_type == \'QRNN\' or self.rnn_type == \'GRU\':\n            return [\n                weight.new_zeros(1, bsz, self.nhid\n                                 if l != self.nlayers - 1 else (self.ninp\n                                                                if self.tie_weights else self.nhid))\n                for l in range(self.nlayers)\n            ]\n'"
examples/awd-lstm-lm/splitcross.py,31,"b""from collections import defaultdict\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\n\nclass SplitCrossEntropyLoss(nn.Module):\n    r'''SplitCrossEntropyLoss calculates an approximate softmax'''\n    def __init__(self, hidden_size, splits, verbose=False):\n        # We assume splits is [0, split1, split2, N] where N >= |V|\n        # For example, a vocab of 1000 words may have splits [0] + [100, 500] + [inf]\n        super(SplitCrossEntropyLoss, self).__init__()\n        self.hidden_size = hidden_size\n        self.splits = [0] + splits + [100 * 1000000]\n        self.nsplits = len(self.splits) - 1\n        self.stats = defaultdict(list)\n        self.verbose = verbose\n        # Each of the splits that aren't in the head require a pretend token, we'll call them tombstones\n        # The probability given to this tombstone is the probability of selecting an item from the represented split\n        if self.nsplits > 1:\n            self.tail_vectors = nn.Parameter(torch.zeros(self.nsplits - 1, hidden_size))\n            self.tail_bias = nn.Parameter(torch.zeros(self.nsplits - 1))\n\n    def logprob(self, weight, bias, hiddens, splits=None, softmaxed_head_res=None, verbose=False):\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        if softmaxed_head_res is None:\n            start, end = self.splits[0], self.splits[1]\n            head_weight = None if end - start == 0 else weight[start:end]\n            head_bias = None if end - start == 0 else bias[start:end]\n            # We only add the tombstones if we have more than one split\n            if self.nsplits > 1:\n                head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n                head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n            # Perform the softmax calculation for the word vectors in the head for all splits\n            # We need to guard against empty splits as torch.cat does not like random lists\n            head_res = torch.nn.functional.linear(hiddens, head_weight, bias=head_bias)\n            softmaxed_head_res = torch.nn.functional.log_softmax(head_res)\n\n        if splits is None:\n            splits = list(range(self.nsplits))\n\n        results = []\n        running_offset = 0\n        for idx in splits:\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                results.append(softmaxed_head_res[:, :-(self.nsplits - 1)])\n\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                start, end = self.splits[idx], self.splits[idx + 1]\n                tail_weight = weight[start:end]\n                tail_bias = bias[start:end]\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = torch.nn.functional.linear(hiddens, tail_weight, bias=tail_bias)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = (softmaxed_head_res[:, -idx]).contiguous()\n                tail_entropy = torch.nn.functional.log_softmax(tail_res)\n                results.append(head_entropy.view(-1, 1) + tail_entropy)\n\n        if len(results) > 1:\n            return torch.cat(results, dim=1)\n        return results[0]\n\n    def split_on_targets(self, hiddens, targets):\n        # Split the targets into those in the head and in the tail\n        split_targets = []\n        split_hiddens = []\n\n        # Determine to which split each element belongs (for each start split value, add 1 if equal or greater)\n        # This method appears slower at least for WT-103 values for approx softmax\n        #masks = [(targets >= self.splits[idx]).view(1, -1) for idx in range(1, self.nsplits)]\n        #mask = torch.sum(torch.cat(masks, dim=0), dim=0)\n        ###\n        # This is equally fast for smaller splits as method below but scales linearly\n        mask = None\n        for idx in range(1, self.nsplits):\n            partial_mask = targets >= self.splits[idx]\n            mask = mask + partial_mask if mask is not None else partial_mask\n        ###\n        #masks = torch.stack([targets] * (self.nsplits - 1))\n        #mask = torch.sum(masks >= self.split_starts, dim=0)\n        for idx in range(self.nsplits):\n            # If there are no splits, avoid costly masked select\n            if self.nsplits == 1:\n                split_targets, split_hiddens = [targets], [hiddens]\n                continue\n            # If all the words are covered by earlier targets, we have empties so later stages don't freak out\n            if sum(len(t) for t in split_targets) == len(targets):\n                split_targets.append([])\n                split_hiddens.append([])\n                continue\n            # Are you in our split?\n            tmp_mask = mask == idx\n            split_targets.append(torch.masked_select(targets, tmp_mask))\n            split_hiddens.append(hiddens.masked_select(tmp_mask.unsqueeze(1).expand_as(hiddens)).view(-1, hiddens.size(1)))\n        return split_targets, split_hiddens\n\n    def forward(self, weight, bias, hiddens, targets, verbose=False):\n        if self.verbose or verbose:\n            for idx in sorted(self.stats):\n                print('{}: {}'.format(idx, int(np.mean(self.stats[idx]))), end=', ')\n            print()\n\n        total_loss = None\n        if len(hiddens.size()) > 2: hiddens = hiddens.view(-1, hiddens.size(2))\n\n        split_targets, split_hiddens = self.split_on_targets(hiddens, targets)\n\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        start, end = self.splits[0], self.splits[1]\n        head_weight = None if end - start == 0 else weight[start:end]\n        head_bias = None if end - start == 0 else bias[start:end]\n\n        # We only add the tombstones if we have more than one split\n        if self.nsplits > 1:\n            head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n            head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n        # Perform the softmax calculation for the word vectors in the head for all splits\n        # We need to guard against empty splits as torch.cat does not like random lists\n        combo = torch.cat([split_hiddens[i] for i in range(self.nsplits) if len(split_hiddens[i])])\n        ###\n        all_head_res = torch.nn.functional.linear(combo, head_weight, bias=head_bias)\n        softmaxed_all_head_res = torch.nn.functional.log_softmax(all_head_res)\n        if self.verbose or verbose:\n            self.stats[0].append(combo.size()[0] * head_weight.size()[0])\n\n        running_offset = 0\n        for idx in range(self.nsplits):\n            # If there are no targets for this split, continue\n            if len(split_targets[idx]) == 0: continue\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n                entropy = -torch.gather(softmaxed_head_res, dim=1, index=split_targets[idx].view(-1, 1))\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n\n                if self.verbose or verbose:\n                    start, end = self.splits[idx], self.splits[idx + 1]\n                    tail_weight = weight[start:end]\n                    self.stats[idx].append(split_hiddens[idx].size()[0] * tail_weight.size()[0])\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = self.logprob(weight, bias, split_hiddens[idx], splits=[idx], softmaxed_head_res=softmaxed_head_res)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = softmaxed_head_res[:, -idx]\n                # All indices are shifted - if the first split handles [0,...,499] then the 500th in the second split will be 0 indexed\n                indices = (split_targets[idx] - self.splits[idx]).view(-1, 1)\n                # Warning: if you don't squeeze, you get an N x 1 return, which acts oddly with broadcasting\n                tail_entropy = torch.gather(torch.nn.functional.log_softmax(tail_res), dim=1, index=indices).squeeze()\n                entropy = -(head_entropy + tail_entropy)\n            ###\n            running_offset += len(split_hiddens[idx])\n            total_loss = entropy.float().sum() if total_loss is None else total_loss + entropy.float().sum()\n\n        return (total_loss / len(targets)).type_as(weight)\n\n\nif __name__ == '__main__':\n    np.random.seed(42)\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n\n    V = 8\n    H = 10\n    N = 100\n    E = 10\n\n    embed = torch.nn.Embedding(V, H)\n    crit = SplitCrossEntropyLoss(hidden_size=H, splits=[V // 2])\n    bias = torch.nn.Parameter(torch.ones(V))\n    optimizer = torch.optim.SGD(list(embed.parameters()) + list(crit.parameters()), lr=1)\n\n    for _ in range(E):\n        prev = (torch.rand(N, 1) * 0.999 * V).int().long()\n        x = (torch.rand(N, 1) * 0.999 * V).int().long()\n        y = embed(prev).squeeze()\n        c = crit(embed.weight, bias, y, x.view(N))\n        print('Crit', c.exp().item())\n\n        logprobs = crit.logprob(embed.weight, bias, y[:2]).exp()\n        print(logprobs)\n        print(logprobs.sum(dim=1))\n\n        optimizer.zero_grad()\n        c.backward()\n        optimizer.step()\n"""
examples/awd-lstm-lm/utils.py,1,"b'import torch\n\n\ndef repackage_hidden(h):\n    """"""Wraps hidden states in new Tensors, to detach them from their history.""""""\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n'"
examples/snli/model.py,2,"b'import torch\nimport torch.nn as nn\n\n\nclass Bottle(nn.Module):\n\n    def forward(self, input):\n        if len(input.size()) <= 2:\n            return super(Bottle, self).forward(input)\n        size = input.size()[:2]\n        out = super(Bottle, self).forward(input.view(size[0] * size[1], -1))\n        return out.view(size[0], size[1], -1)\n\n\nclass Linear(Bottle, nn.Linear):\n    pass\n\n\nclass Encoder(nn.Module):\n\n    def __init__(self, config):\n        super(Encoder, self).__init__()\n        self.config = config\n        input_size = config.d_proj if config.projection else config.d_embed\n        self.rnn = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.d_hidden,\n            num_layers=config.n_layers,\n            dropout=config.dp_ratio,\n            bidirectional=config.birnn)\n\n    def forward(self, inputs):\n        batch_size = inputs.size()[1]\n        state_shape = self.config.n_cells, batch_size, self.config.d_hidden\n        h0 = c0 = inputs.detach().new_zeros(*state_shape)\n        outputs, (ht, ct) = self.rnn(inputs, (h0, c0))\n        return ht[-1] if not self.config.birnn else ht[-2:].transpose(0, 1).contiguous().view(\n            batch_size, -1)\n\n\nclass SNLIClassifier(nn.Module):\n\n    def __init__(self, config):\n        super(SNLIClassifier, self).__init__()\n        self.config = config\n        self.embed = nn.Embedding(config.n_embed, config.d_embed)\n        self.projection = Linear(config.d_embed, config.d_proj)\n        self.encoder = Encoder(config)\n        self.dropout = nn.Dropout(p=config.dp_ratio)\n        self.relu = nn.ReLU()\n        seq_in_size = 2 * config.d_hidden\n        if self.config.birnn:\n            seq_in_size *= 2\n        lin_config = [seq_in_size] * 2\n        self.out = nn.Sequential(\n            Linear(*lin_config), self.relu, self.dropout, Linear(*lin_config), self.relu,\n            self.dropout, Linear(*lin_config), self.relu, self.dropout,\n            Linear(seq_in_size, config.d_out))\n\n    def forward(self, premise, hypothesis):\n        prem_embed = self.embed(premise)\n        hypo_embed = self.embed(hypothesis)\n        if self.config.fix_emb:\n            prem_embed = prem_embed.detach()\n            hypo_embed = hypo_embed.detach()\n        if self.config.projection:\n            prem_embed = self.relu(self.projection(prem_embed))\n            hypo_embed = self.relu(self.projection(hypo_embed))\n        premise = self.encoder(prem_embed)\n        hypothesis = self.encoder(hypo_embed)\n        scores = self.out(torch.cat([premise, hypothesis], 1))\n        return scores'"
examples/snli/train.py,14,"b""from functools import partial\n\nimport glob\nimport itertools\nimport os\nimport time\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SequentialSampler\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\nfrom torchnlp.samplers import BucketBatchSampler\nfrom torchnlp.datasets import snli_dataset\nfrom torchnlp.encoders.text import WhitespaceEncoder\nfrom torchnlp.encoders import LabelEncoder\nfrom torchnlp import word_to_vector\n\nfrom model import SNLIClassifier\nfrom util import get_args, makedirs, collate_fn\n\nargs = get_args()\n\nif args.gpu >= 0:\n    torch.cuda.set_device(args.gpu)\n\n# load dataset\ntrain, dev, test = snli_dataset(train=True, dev=True, test=True)\n\n# Preprocess\nfor row in itertools.chain(train, dev, test):\n    row['premise'] = row['premise'].lower()\n    row['hypothesis'] = row['hypothesis'].lower()\n\n# Make Encoders\nsentence_corpus = [row['premise'] for row in itertools.chain(train, dev, test)]\nsentence_corpus += [row['hypothesis'] for row in itertools.chain(train, dev, test)]\nsentence_encoder = WhitespaceEncoder(sentence_corpus)\n\nlabel_corpus = [row['label'] for row in itertools.chain(train, dev, test)]\nlabel_encoder = LabelEncoder(label_corpus)\n\n# Encode\nfor row in itertools.chain(train, dev, test):\n    row['premise'] = sentence_encoder.encode(row['premise'])\n    row['hypothesis'] = sentence_encoder.encode(row['hypothesis'])\n    row['label'] = label_encoder.encode(row['label'])\n\nconfig = args\nconfig.n_embed = sentence_encoder.vocab_size\nconfig.d_out = label_encoder.vocab_size\nconfig.n_cells = config.n_layers\n\n# double the number of cells for bidirectional networks\nif config.birnn:\n    config.n_cells *= 2\n\nif args.resume_snapshot:\n    model = torch.load(\n        args.resume_snapshot, map_location=lambda storage, location: storage.cuda(args.gpu))\nelse:\n    model = SNLIClassifier(config)\n    if args.word_vectors:\n        # Load word vectors\n        word_vectors = word_to_vector.aliases[args.word_vectors]()\n        for i, token in enumerate(sentence_encoder.vocab):\n            model.embed.weight.data[i] = word_vectors[token]\n\n    if args.gpu >= 0:\n        model.cuda()\n\ncriterion = nn.CrossEntropyLoss()\nopt = optim.Adam(model.parameters(), lr=args.lr)\n\niterations = 0\nstart = time.time()\nbest_dev_acc = -1\nheader = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy'\ndev_log_template = ' '.join(\n    '{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'\n    .split(','))\nlog_template = ' '.join(\n    '{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\nmakedirs(args.save_path)\nprint(header)\n\nfor epoch in range(args.epochs):\n    n_correct, n_total = 0, 0\n\n    train_sampler = SequentialSampler(train)\n    train_batch_sampler = BucketBatchSampler(\n        train_sampler, args.batch_size, True, sort_key=lambda r: len(row['premise']))\n    train_iterator = DataLoader(\n        train,\n        batch_sampler=train_batch_sampler,\n        collate_fn=collate_fn,\n        pin_memory=torch.cuda.is_available(),\n        num_workers=0)\n    for batch_idx, (premise_batch, hypothesis_batch, label_batch) in enumerate(train_iterator):\n\n        # switch model to training mode, clear gradient accumulators\n        model.train()\n        torch.set_grad_enabled(True)\n        opt.zero_grad()\n\n        iterations += 1\n\n        # forward pass\n        answer = model(premise_batch, hypothesis_batch)\n\n        # calculate accuracy of predictions in the current batch\n        n_correct += (torch.max(answer, 1)[1].view(label_batch.size()) == label_batch).sum()\n        n_total += premise_batch.size()[1]\n        train_acc = 100. * n_correct / n_total\n\n        # calculate loss of the network output with respect to training labels\n        loss = criterion(answer, label_batch)\n\n        # backpropagate and update optimizer learning rate\n        loss.backward()\n        opt.step()\n\n        # checkpoint model periodically\n        if iterations % args.save_every == 0:\n            snapshot_prefix = os.path.join(args.save_path, 'snapshot')\n            snapshot_path = snapshot_prefix + '_acc_{:.4f}_loss_{:.6f}_iter_{}_model.pt'.format(\n                train_acc, loss.item(), iterations)\n            torch.save(model, snapshot_path)\n            for f in glob.glob(snapshot_prefix + '*'):\n                if f != snapshot_path:\n                    os.remove(f)\n\n        # evaluate performance on validation set periodically\n        if iterations % args.dev_every == 0:\n\n            # switch model to evaluation mode\n            model.eval()\n            torch.set_grad_enabled(False)\n\n            # calculate accuracy on validation set\n            n_dev_correct, dev_loss = 0, 0\n\n            dev_sampler = SequentialSampler(train)\n            dev_batch_sampler = BucketBatchSampler(\n                dev, args.batch_size, True, sort_key=lambda r: len(row['premise']))\n            dev_iterator = DataLoader(\n                dev,\n                batch_sampler=dev_batch_sampler,\n                collate_fn=partial(collate_fn, train=False),\n                pin_memory=torch.cuda.is_available(),\n                num_workers=0)\n            for dev_batch_idx, (premise_batch, hypothesis_batch,\n                                label_batch) in enumerate(dev_iterator):\n                answer = model(premise_batch, hypothesis_batch)\n                n_dev_correct += (torch.max(answer,\n                                            1)[1].view(label_batch.size()) == label_batch).sum()\n                dev_loss = criterion(answer, label_batch)\n            dev_acc = 100. * n_dev_correct / len(dev)\n\n            print(\n                dev_log_template.format(time.time() - start, epoch, iterations, 1 + batch_idx,\n                                        len(train_sampler),\n                                        100. * (1 + batch_idx) / len(train_sampler), loss.item(),\n                                        dev_loss.item(), train_acc, dev_acc))\n\n            # update best validation set accuracy\n            if dev_acc > best_dev_acc:\n\n                # found a model with better validation set accuracy\n\n                best_dev_acc = dev_acc\n                snapshot_prefix = os.path.join(args.save_path, 'best_snapshot')\n                snapshot_path = snapshot_prefix + '_devacc_{}_devloss_{}__iter_{}_model.pt'.format(\n                    dev_acc, dev_loss.item(), iterations)\n\n                # save model, delete previous 'best_snapshot' files\n                torch.save(model, snapshot_path)\n                for f in glob.glob(snapshot_prefix + '*'):\n                    if f != snapshot_path:\n                        os.remove(f)\n\n        elif iterations % args.log_every == 0:\n\n            # print progress message\n            print(\n                log_template.format(time.time() - start, epoch, iterations, 1 + batch_idx,\n                                    len(train_sampler), 100. * (1 + batch_idx) / len(train_sampler),\n                                    loss.item(), ' ' * 8, n_correct / n_total * 100, ' ' * 12))\n"""
examples/snli/util.py,1,"b'from argparse import ArgumentParser\n\nimport os\n\nimport torch\n\nfrom torchnlp.encoders.text import stack_and_pad_tensors\n\n\ndef makedirs(name):\n    """"""helper function for python 2 and 3 to call os.makedirs()\n       avoiding an error if the directory to be created already exists""""""\n\n    import os, errno\n\n    try:\n        os.makedirs(name)\n    except OSError as ex:\n        if ex.errno == errno.EEXIST and os.path.isdir(name):\n            # ignore existing directory\n            pass\n        else:\n            # a different error happened\n            raise\n\n\ndef get_args():\n    parser = ArgumentParser(description=\'PyTorch/torchtext SNLI example\')\n    parser.add_argument(\'--epochs\', type=int, default=50)\n    parser.add_argument(\'--batch_size\', type=int, default=128)\n    parser.add_argument(\'--d_embed\', type=int, default=100)\n    parser.add_argument(\'--d_proj\', type=int, default=300)\n    parser.add_argument(\'--d_hidden\', type=int, default=300)\n    parser.add_argument(\'--n_layers\', type=int, default=1)\n    parser.add_argument(\'--log_every\', type=int, default=50)\n    parser.add_argument(\'--lr\', type=float, default=.001)\n    parser.add_argument(\'--dev_every\', type=int, default=1000)\n    parser.add_argument(\'--save_every\', type=int, default=1000)\n    parser.add_argument(\'--dp_ratio\', type=int, default=0.2)\n    parser.add_argument(\'--no-bidirectional\', action=\'store_false\', dest=\'birnn\')\n    parser.add_argument(\'--preserve-case\', action=\'store_false\', dest=\'lower\')\n    parser.add_argument(\'--no-projection\', action=\'store_false\', dest=\'projection\')\n    parser.add_argument(\'--train_embed\', action=\'store_false\', dest=\'fix_emb\')\n    parser.add_argument(\'--gpu\', type=int, default=-1)\n    parser.add_argument(\'--save_path\', type=str, default=\'results\')\n    parser.add_argument(\n        \'--vector_cache\',\n        type=str,\n        default=os.path.join(os.getcwd(), \'.vector_cache/input_vectors.pt\'))\n    parser.add_argument(\'--word_vectors\', type=str, default=\'glove.6B.100d\')\n    parser.add_argument(\'--resume_snapshot\', type=str, default=\'\')\n    args = parser.parse_args()\n    return args\n\n\ndef collate_fn(batch, train=True):\n    """""" list of tensors to a batch tensors """"""\n    premise_batch, _ = stack_and_pad_tensors([row[\'premise\'] for row in batch])\n    hypothesis_batch, _ = stack_and_pad_tensors([row[\'hypothesis\'] for row in batch])\n    label_batch = torch.stack([row[\'label\'] for row in batch])\n\n    # PyTorch RNN requires batches to be transposed for speed and integration with CUDA\n    transpose = (lambda b: b.t_().squeeze(0).contiguous())\n\n    return (transpose(premise_batch), transpose(hypothesis_batch), transpose(label_batch))\n'"
tests/datasets/test_count.py,0,"b'from torchnlp.datasets import count_dataset\n\n\ndef test_count_dataset():\n    test_data = count_dataset(test=True)\n    train_data, dev_data, test_data = count_dataset(test=True, train=True, dev=True)\n    # Test if data generated is consistent\n    assert list(test_data) == list(test_data)\n    assert len(train_data) > 0\n    assert len(dev_data) > 0\n    assert len(test_data) > 0\n\n\ndef test_count_dataset_rows():\n    test_data = count_dataset(test=True, test_rows=100)\n    assert len(test_data) == 100\n'"
tests/datasets/test_imdb.py,0,"b'import os\nimport shutil\n\nimport mock\n\nfrom torchnlp.datasets import imdb_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_imdb_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, test = imdb_dataset(directory=directory, test=True, train=True)\n    assert len(train) > 0\n    assert len(test) > 0\n    test = sorted(test, key=lambda r: len(r[\'text\']))\n    assert test[0] == {\n        \'text\':\n            ""This movie was sadly under-promoted but proved to be truly exceptional. Entering "" +\n            ""the theatre I knew nothing about the film except that a friend wanted to see it."" +\n            ""<br /><br />I was caught off guard with the high quality of the film. I couldn\'t "" +\n            ""image Ashton Kutcher in a serious role, but his performance truly exemplified his "" +\n            ""character. This movie is exceptional and deserves our monetary support, unlike so "" +\n            ""many other movies. It does not come lightly for me to recommend any movie, but in "" +\n            ""this case I highly recommend that everyone see it.<br /><br />This films is Truly "" +\n            ""Exceptional!"",\n        \'sentiment\':\n            \'pos\'\n    }\n\n    # Clean up\n    shutil.rmtree(os.path.join(directory, \'aclImdb\'))\n'"
tests/datasets/test_iwslt.py,0,"b'import os\nimport shutil\n\nimport mock\n\nfrom torchnlp.datasets import iwslt_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\niwslt_directory = \'tests/_test_data/iwslt\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_iwslt_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = iwslt_dataset(directory=iwslt_directory, test=True, dev=True, train=True)\n    assert len(train) > 0\n    assert len(dev) > 0\n    assert len(test) > 0\n    train = sorted(train, key=lambda r: len(r[\'en\']))\n    assert train[0] == {\'en\': \'Thank you.\', \'de\': \'Danke.\'}\n\n    # Smoke test for iwslt_clean running twice\n    train, dev, test = iwslt_dataset(directory=iwslt_directory, test=True, dev=True, train=True)\n    train = sorted(train, key=lambda r: len(r[\'en\']))\n    assert train[0] == {\'en\': \'Thank you.\', \'de\': \'Danke.\'}\n\n    # Clean up\n    shutil.rmtree(os.path.join(iwslt_directory, \'en-de\'))\n'"
tests/datasets/test_multi30k.py,0,"b'import os\n\nimport mock\n\nfrom torchnlp.datasets import multi30k_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\nmulti30k_directory = \'tests/_test_data/multi30k\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_multi30k_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = multi30k_dataset(\n        directory=multi30k_directory, test=True, dev=True, train=True)\n    assert len(train) > 0\n    assert len(dev) > 0\n    assert len(test) > 0\n    assert train[0] == {\n        \'de\': \'Zwei junge wei\xc3\x9fe M\xc3\xa4nner sind im Freien in der N\xc3\xa4he vieler B\xc3\xbcsche.\',\n        \'en\': \'Two young, White males are outside near many bushes.\'\n    }\n\n    # Clean up\n    os.remove(os.path.join(multi30k_directory, \'train.en\'))\n    os.remove(os.path.join(multi30k_directory, \'train.de\'))\n    os.remove(os.path.join(multi30k_directory, \'test.en\'))\n    os.remove(os.path.join(multi30k_directory, \'test.de\'))\n    os.remove(os.path.join(multi30k_directory, \'val.en\'))\n    os.remove(os.path.join(multi30k_directory, \'val.de\'))\n'"
tests/datasets/test_penn_treebank.py,0,"b'import mock\n\nfrom torchnlp.datasets import penn_treebank_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/penn-treebank\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_penn_treebank_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = penn_treebank_dataset(\n        directory=directory, test=True, dev=True, train=True, check_files=[])\n    assert len(train) > 0\n    assert len(test) > 0\n    assert len(dev) > 0\n    assert train[0:10] == [\n        \'aer\', \'banknote\', \'berlitz\', \'calloway\', \'centrust\', \'cluett\', \'fromstein\', \'gitano\',\n        \'guterman\', \'hydro-quebec\'\n    ]\n'"
tests/datasets/test_reverse.py,0,"b'from torchnlp.datasets import reverse_dataset\n\n\ndef test_reverse_dataset():\n    test_data = reverse_dataset(test=True)\n    train_data, dev_data, test_data = reverse_dataset(test=True, train=True, dev=True)\n    # Test if data generated is consistent\n    assert list(test_data) == list(test_data)\n    assert len(train_data) > 0\n    assert len(dev_data) > 0\n    assert len(test_data) > 0\n\n\ndef test_reverse_dataset_rows():\n    test_data = reverse_dataset(test=True, test_rows=100)\n    assert len(test_data) == 100\n'"
tests/datasets/test_simple_qa.py,0,"b'import os\nimport shutil\n\nimport mock\nimport pytest\n\nfrom torchnlp.datasets import simple_qa_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/\'\n\n\n@pytest.mark.skip(reason=""Simple Questions dataset url sometimes returns 404."")\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_simple_qa_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = simple_qa_dataset(directory=directory, test=True, train=True, dev=True)\n    assert len(train) > 0\n    assert len(dev) > 0\n    assert len(test) > 0\n    assert dev[0] == {\n        \'question\': \'Who was the trump ocean club international hotel and tower named after\',\n        \'relation\': \'www.freebase.com/symbols/namesake/named_after\',\n        \'object\': \'www.freebase.com/m/0cqt90\',\n        \'subject\': \'www.freebase.com/m/0f3xg_\',\n    }\n\n    # Clean up\n    shutil.rmtree(os.path.join(directory, \'SimpleQuestions_v2\'))\n'"
tests/datasets/test_smt.py,0,"b'import os\nimport shutil\n\nimport mock\n\nfrom torchnlp.datasets import smt_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_smt_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = smt_dataset(directory=directory, test=True, dev=True, train=True)\n    assert len(train) > 0\n    assert len(dev) > 0\n    assert len(test) > 0\n    assert train[5] == {\n        \'text\':\n            ""Whether or not you \'re enlightened by any of Derrida \'s lectures on `` the other \'\' "" +\n            ""and `` the self , \'\' Derrida is an undeniably fascinating and playful fellow ."",\n        \'label\':\n            \'positive\'\n    }\n    train = smt_dataset(directory=directory, train=True, subtrees=True)\n    assert train[3] == {\'text\': \'Rock\', \'label\': \'neutral\'}\n\n    train = smt_dataset(directory=directory, train=True, subtrees=True, fine_grained=True)\n    assert train[4] == {\n        \'text\':\n            ""is destined to be the 21st Century \'s new `` Conan \'\' and that he \'s going to make a"" +\n            "" splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven"" +\n            "" Segal ."",\n        \'label\':\n            \'very positive\'\n    }\n\n    # Clean up\n    shutil.rmtree(os.path.join(directory, \'trees\'))\n'"
tests/datasets/test_snli.py,0,"b'import os\nimport shutil\n\nimport mock\n\nfrom torchnlp.datasets import snli_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_snli_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = snli_dataset(directory=directory, test=True, dev=True, train=True)\n    assert len(train) > 0\n    assert len(dev) > 0\n    assert len(test) > 0\n    assert train[0] == {\n        \'premise\':\n            \'A person on a horse jumps over a broken down airplane.\',\n        \'hypothesis\':\n            \'A person is training his horse for a competition.\',\n        \'label\':\n            \'neutral\',\n        \'premise_transitions\': [\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'reduce\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'reduce\', \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\',\n            \'reduce\', \'shift\', \'shift\', \'shift\', \'reduce\', \'shift\', \'reduce\'\n        ],\n        \'hypothesis_transitions\': [\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'reduce\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\',\n            \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\', \'reduce\', \'shift\',\n            \'shift\', \'shift\', \'reduce\', \'shift\', \'reduce\'\n        ]\n    }\n\n    # Clean up\n    shutil.rmtree(os.path.join(directory, \'snli_1.0\'))\n'"
tests/datasets/test_trec.py,0,"b'import mock\n\nfrom torchnlp.datasets import trec_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/trec\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_penn_treebank_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, test = trec_dataset(directory=directory, test=True, train=True, check_files=[])\n    assert len(train) > 0\n    assert len(test) > 0\n    assert train[:2] == [{\n        \'label\': \'DESC\',\n        \'text\': \'How did serfdom develop in and then leave Russia ?\'\n    }, {\n        \'label\': \'ENTY\',\n        \'text\': \'What films featured the character Popeye Doyle ?\'\n    }]\n\n    train = trec_dataset(directory=directory, train=True, check_files=[], fine_grained=True)\n    assert train[:2] == [{\n        \'label\': \'manner\',\n        \'text\': \'How did serfdom develop in and then leave Russia ?\'\n    }, {\n        \'label\': \'cremat\',\n        \'text\': \'What films featured the character Popeye Doyle ?\'\n    }]\n'"
tests/datasets/test_ud_pos.py,0,"b'import os\nimport shutil\n\nimport mock\n\nfrom torchnlp.datasets import ud_pos_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_ud_pos_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = ud_pos_dataset(directory=directory, test=True, dev=True, train=True)\n    assert len(train) > 0\n    assert len(dev) > 0\n    assert len(test) > 0\n    assert train[0] == {\n        \'tokens\': [\n            \'Al\', \'-\', \'Zaman\', \':\', \'American\', \'forces\', \'killed\', \'Shaikh\', \'Abdullah\', \'al\',\n            \'-\', \'Ani\', \',\', \'the\', \'preacher\', \'at\', \'the\', \'mosque\', \'in\', \'the\', \'town\', \'of\',\n            \'Qaim\', \',\', \'near\', \'the\', \'Syrian\', \'border\', \'.\'\n        ],\n        \'ud_tags\': [\n            \'PROPN\', \'PUNCT\', \'PROPN\', \'PUNCT\', \'ADJ\', \'NOUN\', \'VERB\', \'PROPN\', \'PROPN\', \'PROPN\',\n            \'PUNCT\', \'PROPN\', \'PUNCT\', \'DET\', \'NOUN\', \'ADP\', \'DET\', \'NOUN\', \'ADP\', \'DET\', \'NOUN\',\n            \'ADP\', \'PROPN\', \'PUNCT\', \'ADP\', \'DET\', \'ADJ\', \'NOUN\', \'PUNCT\'\n        ],\n        \'ptb_tags\': [\n            \'NNP\', \'HYPH\', \'NNP\', \':\', \'JJ\', \'NNS\', \'VBD\', \'NNP\', \'NNP\', \'NNP\', \'HYPH\', \'NNP\', \',\',\n            \'DT\', \'NN\', \'IN\', \'DT\', \'NN\', \'IN\', \'DT\', \'NN\', \'IN\', \'NNP\', \',\', \'IN\', \'DT\', \'JJ\',\n            \'NN\', \'.\'\n        ]\n    }\n\n    # Clean up\n    shutil.rmtree(os.path.join(directory, \'en-ud-v2\'))\n'"
tests/datasets/test_wikitext_2.py,0,"b'import os\nimport shutil\n\nimport mock\n\nfrom torchnlp.datasets import wikitext_2_dataset\nfrom tests.datasets.utils import urlretrieve_side_effect\n\ndirectory = \'tests/_test_data/\'\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_wikitext_2_dataset_row(mock_urlretrieve):\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = wikitext_2_dataset(directory=directory, test=True, dev=True, train=True)\n    assert len(train) > 0\n    assert len(test) > 0\n    assert len(dev) > 0\n    assert train[0:10] == [\n        \'</s>\', \'=\', \'Valkyria\', \'Chronicles\', \'III\', \'=\', \'</s>\', \'</s>\', \'Senj\xc5\x8d\', \'no\'\n    ]\n\n    # Clean up\n    shutil.rmtree(os.path.join(directory, \'wikitext-2\'))\n'"
tests/datasets/test_wmt.py,0,"b""import os\n\nimport mock\n\nfrom torchnlp.datasets import wmt_dataset\nfrom tests.datasets.utils import download_from_drive_side_effect\n\ndirectory = 'tests/_test_data/wmt16_en_de'\n\n\n@mock.patch('torchnlp.download._download_file_from_drive')\ndef test_wmt_dataset(mock_download_from_drive):\n    mock_download_from_drive.side_effect = download_from_drive_side_effect\n\n    # Check a row are parsed correctly\n    train, dev, test = wmt_dataset(directory=directory, test=True, dev=True, train=True)\n    assert len(train) > 0\n    assert len(test) > 0\n    assert len(dev) > 0\n    assert train[0] == {\n        'en': 'Res@@ um@@ ption of the session',\n        'de': 'Wiederaufnahme der Sitzungsperiode'\n    }\n\n    # Clean up\n    os.remove(os.path.join(directory, 'bpe.32000'))\n    os.remove(os.path.join(directory, 'newstest2013.tok.bpe.32000.en'))\n    os.remove(os.path.join(directory, 'newstest2013.tok.bpe.32000.de'))\n    os.remove(os.path.join(directory, 'newstest2014.tok.bpe.32000.en'))\n    os.remove(os.path.join(directory, 'newstest2014.tok.bpe.32000.de'))\n    os.remove(os.path.join(directory, 'train.tok.clean.bpe.32000.de'))\n    os.remove(os.path.join(directory, 'train.tok.clean.bpe.32000.en'))\n    os.remove(os.path.join(directory, 'vocab.bpe.32000'))\n"""
tests/datasets/test_zero_to_zero.py,0,"b'from torchnlp.datasets import zero_dataset\n\n\ndef test_zero_dataset():\n    test_data = zero_dataset(test=True)\n    train_data, dev_data, test_data = zero_dataset(test=True, train=True, dev=True)\n    # Test if data generated is consistent\n    assert list(test_data) == list(test_data)\n    assert len(train_data) > 0\n    assert len(dev_data) > 0\n    assert len(test_data) > 0\n\n\ndef test_zero_dataset_rows():\n    test_data = zero_dataset(test=True, test_rows=100)\n    assert len(test_data) == 100\n'"
tests/datasets/utils.py,0,"b'import urllib.request\n\n\n# Check the URL requested is valid\ndef urlretrieve_side_effect(url, **kwargs):\n    # TODO: Fix failure case if internet does not work\n    assert urllib.request.urlopen(url).getcode() == 200\n\n\n# Check the URL requested is valid\ndef download_from_drive_side_effect(filename, url, **kwargs):\n    # TODO: Fix failure case if internet does not work\n    assert urllib.request.urlopen(url).getcode() == 200\n'"
tests/encoders/test_encoder.py,0,"b""from torchnlp.encoders import Encoder\n\n\ndef test_encoder():\n    encoder = Encoder(enforce_reversible=True)\n    encoder.encode('this is a test')\n    encoder.decode('this is a test')\n"""
tests/encoders/test_label_encoder.py,4,"b""import pickle\n\nimport pytest\nimport torch\n\nfrom torchnlp.encoders import LabelEncoder\nfrom torchnlp.encoders.label_encoder import DEFAULT_UNKNOWN_TOKEN\n\n\n@pytest.fixture\ndef label_encoder():\n    sample = ['people/deceased_person/place_of_death', 'symbols/name_source/namesakes']\n    return LabelEncoder(sample)\n\n\ndef test_label_encoder_no_reserved():\n    sample = ['people/deceased_person/place_of_death', 'symbols/name_source/namesakes']\n    label_encoder = LabelEncoder(sample, reserved_labels=[], unknown_index=None)\n\n    label_encoder.encode('people/deceased_person/place_of_death')\n\n    # No ``unknown_index`` defined causes ``RuntimeError`` if an unknown label is used.\n    with pytest.raises(TypeError):\n        label_encoder.encode('symbols/namesake/named_after')\n\n\ndef test_label_encoder_enforce_reversible(label_encoder):\n    label_encoder.enforce_reversible = True\n\n    label_encoder.encode('people/deceased_person/place_of_death')\n    with pytest.raises(ValueError):\n        label_encoder.encode('symbols/namesake/named_after')\n\n    label_encoder.decode(torch.tensor(label_encoder.vocab_size - 1))\n    with pytest.raises(IndexError):\n        label_encoder.decode(torch.tensor(label_encoder.vocab_size))\n\n\ndef test_label_encoder_batch_encoding(label_encoder):\n    encoded = label_encoder.batch_encode(label_encoder.vocab)\n    assert torch.equal(encoded, torch.arange(label_encoder.vocab_size).view(-1))\n\n\ndef test_label_encoder_batch_dim(label_encoder):\n    encoded = label_encoder.batch_encode(label_encoder.vocab, dim=-1)\n    decoded = label_encoder.batch_decode(encoded, dim=-1)\n    assert decoded == label_encoder.vocab\n\n\ndef test_label_encoder_batch_decoding(label_encoder):\n    assert label_encoder.vocab == label_encoder.batch_decode(torch.arange(label_encoder.vocab_size))\n\n\ndef test_label_encoder_vocab(label_encoder):\n    assert len(label_encoder.vocab) == 3\n    assert len(label_encoder.vocab) == label_encoder.vocab_size\n\n\ndef test_label_encoder_unknown(label_encoder):\n    input_ = 'symbols/namesake/named_after'\n    output = label_encoder.encode(input_)\n    assert label_encoder.decode(output) == DEFAULT_UNKNOWN_TOKEN\n\n\ndef test_label_encoder_known(label_encoder):\n    input_ = 'symbols/namesake/named_after'\n    sample = ['people/deceased_person/place_of_death', 'symbols/name_source/namesakes']\n    sample.append(input_)\n    label_encoder = LabelEncoder(sample)\n    output = label_encoder.encode(input_)\n    assert label_encoder.decode(output) == input_\n\n\ndef test_label_encoder_is_pickleable(label_encoder):\n    pickle.dumps(label_encoder)\n"""
tests/metrics/test_accuracy.py,26,"b'import torch\n\nfrom torchnlp.metrics import get_accuracy\nfrom torchnlp.metrics import get_token_accuracy\n\n\ndef test_get_accuracy():\n    targets = torch.LongTensor([1, 2, 3, 4])\n    outputs = torch.LongTensor([1, 2, 3, 3])\n    accuracy, _, _ = get_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_token_accuracy():\n    targets = torch.LongTensor([1, 2, 3, 4])\n    outputs = torch.LongTensor([1, 2, 3, 3])\n    accuracy, _, _ = get_token_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_accuracy_2d_2d():\n    targets = torch.LongTensor([[1], [2], [3], [4]])\n    outputs = torch.LongTensor([[1], [2], [3], [3]])\n    accuracy, _, _ = get_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_token_accuracy_2d_2d():\n    targets = torch.LongTensor([[1], [2], [3], [4]])\n    outputs = torch.LongTensor([[1], [2], [3], [3]])\n    accuracy, _, _ = get_token_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_token_accuracy_2d_2d_2d_2d():\n    targets = torch.LongTensor([[1, 1], [2, 2], [3, 3]])\n    outputs = torch.LongTensor([[1, 1], [2, 3], [4, 4]])\n    accuracy, _, _ = get_token_accuracy(targets, outputs, ignore_index=3)\n    assert accuracy == 0.75\n\n\ndef test_get_accuracy_2d_3d():\n    targets = torch.LongTensor([[1, 1], [2, 2], [3, 3], [4, 4]])\n    outputs = torch.LongTensor([[[1, 1], [1, 1]], [[2, 2], [2, 2]], [[3, 3], [3, 3]],\n                                [[3, 3], [3, 3]]])\n    accuracy, _, _ = get_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_token_accuracy_2d_3d():\n    targets = torch.LongTensor([[1, 1], [2, 2], [3, 3], [4, 4]])\n    outputs = torch.LongTensor([[[1, 1], [1, 1]], [[2, 2], [2, 2]], [[3, 3], [3, 3]],\n                                [[3, 3], [3, 3]]])\n    accuracy, _, _ = get_token_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_accuracy_2d_3d_top_k():\n    targets = torch.LongTensor([[1, 1], [2, 2], [3, 3], [4, 4]])\n    outputs = torch.LongTensor([[[1, 1], [1, 1]], [[2, 2], [2, 2]], [[3, 3], [3, 3]],\n                                [[3, 3], [4, 4]]])\n    accuracy, _, _ = get_accuracy(targets, outputs, k=3)\n    assert accuracy == 1.0\n\n\ndef test_get_accuracy_1d_2d():\n    targets = torch.LongTensor([1, 2, 3, 4])\n    outputs = torch.LongTensor([[1], [2], [3], [3]])\n    accuracy, _, _ = get_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_token_accuracy_1d_2d():\n    targets = torch.LongTensor([1, 2, 3, 4])\n    outputs = torch.LongTensor([[1], [2], [3], [3]])\n    accuracy, _, _ = get_token_accuracy(targets, outputs)\n    assert accuracy == 0.75\n\n\ndef test_get_accuracy_1d_2d_top_k():\n    targets = torch.LongTensor([1, 2, 3, 4])\n    outputs = torch.LongTensor([[1, 1], [2, 2], [3, 3], [3, 4]])\n    accuracy, _, _ = get_accuracy(targets, outputs, k=3)\n    assert accuracy == 1.0\n\n\ndef test_get_accuracy_ignore_index():\n    targets = torch.LongTensor([1, 2, 3, 4])\n    outputs = torch.LongTensor([1, 2, 3, 3])\n    accuracy, _, _ = get_accuracy(targets, outputs, ignore_index=4)\n    assert accuracy == 1.0\n\n\ndef test_get_token_accuracy_ignore_index():\n    targets = torch.LongTensor([1, 2, 3, 4])\n    outputs = torch.LongTensor([1, 2, 3, 3])\n    accuracy, _, _ = get_token_accuracy(targets, outputs, ignore_index=4)\n    assert accuracy == 1.0\n'"
tests/metrics/test_bleu.py,0,"b'import numpy as np\n\nfrom torchnlp.metrics import get_moses_multi_bleu\n\n\n# TODO: Fix failure case if internet does not work\ndef test_get_moses_multi_bleu():\n    hypotheses = [""The brown fox jumps over the dog \xe7\xac\x91"", ""The brown fox jumps over the dog 2 \xe7\xac\x91""]\n    references = [\n        ""The quick brown fox jumps over the lazy dog \xe7\xac\x91"",\n        ""The quick brown fox jumps over the lazy dog \xe7\xac\x91""\n    ]\n    result = get_moses_multi_bleu(hypotheses, references, lowercase=False)\n    np.testing.assert_almost_equal(result, 46.51, decimal=2)\n\n\ndef test_get_moses_multi_bleu_lowercase():\n    hypotheses = [""The brown fox jumps over the dog \xe7\xac\x91"", ""The brown fox jumps over the dog 2 \xe7\xac\x91""]\n    references = [\n        ""The quick brown fox jumps over the lazy dog \xe7\xac\x91"",\n        ""The quick brown fox jumps over the lazy dog \xe7\xac\x91""\n    ]\n    result = get_moses_multi_bleu(hypotheses, references, lowercase=True)\n    np.testing.assert_almost_equal(result, 46.51, decimal=2)\n'"
tests/nn/__init__.py,0,b''
tests/nn/test_attention.py,4,"b'""""""\nAttention unit tests.\n\nNote: Blackbox testing of the attention.\n""""""\nimport random\nimport unittest\n\nimport torch\n\nfrom torchnlp.nn import Attention\nfrom tests.nn.utils import kwargs_product\n\n\nclass TestAttention(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        # Some random small constants to set up the hidden state sizes\n        self.batch_size = random.randint(1, 10)\n        self.dimensions = random.randint(1, 10)\n        self.output_seq_len = random.randint(1, 10)\n        self.input_seq_len = random.randint(1, 10)\n\n        # Constant randomly generated tensors\n        self.input_ = torch.FloatTensor(self.batch_size, self.output_seq_len,\n                                        self.dimensions).random_()\n        self.context = torch.FloatTensor(self.batch_size, self.input_seq_len,\n                                         self.dimensions).random_()\n\n    def _attentions(self, attention_type=[\'general\', \'dot\']):\n        """"""\n        Generate all possible instantiations of `Attention` to test\n        """"""\n        possible_params = {}\n        if attention_type:\n            possible_params[\'attention_type\'] = attention_type\n        for kwargs in kwargs_product(possible_params):\n            attention = Attention(self.dimensions, **kwargs)\n            for param in attention.parameters():\n                param.data.uniform_(-.1, .1)\n            yield attention, kwargs\n\n    def test_init(self):\n        Attention(self.dimensions)\n\n    def test_forward(self):\n        for attention, _ in self._attentions():\n            output, weights = attention.forward(self.input_, self.context)\n\n            # Check sizes\n            self.assertEqual(output.size(), (self.batch_size, self.output_seq_len, self.dimensions))\n            self.assertEqual(weights.size(),\n                             (self.batch_size, self.output_seq_len, self.input_seq_len))\n\n            # Check types\n            self.assertEqual(output.type(), \'torch.FloatTensor\')\n            self.assertEqual(weights.type(), \'torch.FloatTensor\')\n\n            batch = weights.tolist()\n            for queries in batch:\n                for query in queries:\n                    self.assertAlmostEqual(sum(query), 1.0, 3)\n'"
tests/nn/test_cnn_encoder.py,3,"b'from numpy.testing import assert_almost_equal\n\nimport numpy\nimport torch\nimport unittest\n\nfrom torchnlp.nn import CNNEncoder\n\n# from allennlp.nn import InitializerApplicator\n\n\nclass TestCNNEncoder(unittest.TestCase):\n\n    def test_get_dimension_is_correct(self):\n        encoder = CNNEncoder(embedding_dim=5, num_filters=4, ngram_filter_sizes=(3, 5))\n        assert encoder.get_output_dim() == 8\n        assert encoder.get_input_dim() == 5\n        encoder = CNNEncoder(\n            embedding_dim=5, num_filters=4, ngram_filter_sizes=(3, 5), output_dim=7)\n        assert encoder.get_output_dim() == 7\n        assert encoder.get_input_dim() == 5\n\n    def test_forward_does_correct_computation(self):\n        encoder = CNNEncoder(embedding_dim=2, num_filters=1, ngram_filter_sizes=(1, 2))\n        for param in encoder.parameters():\n            torch.nn.init.constant_(param, 1.)\n        input_tensor = torch.FloatTensor([[[.7, .8], [.1, 1.5]]])\n        encoder_output = encoder(input_tensor)\n        assert_almost_equal(\n            encoder_output.data.numpy(), numpy.asarray([[1.6 + 1.0, 3.1 + 1.0]]), decimal=6)\n\n    def test_forward_runs_with_larger_input(self):\n        encoder = CNNEncoder(\n            embedding_dim=7, num_filters=13, ngram_filter_sizes=(1, 2, 3, 4, 5), output_dim=30)\n        tensor = torch.rand(4, 8, 7)\n        assert encoder(tensor).size() == (4, 30)\n'"
tests/nn/test_lock_dropout.py,2,"b""import random\nimport unittest\n\nimport torch\nimport numpy as np\n\nfrom torchnlp.nn import LockedDropout\n\n\nclass TestLockedDropout(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        self.input_ = torch.FloatTensor(\n            random.randint(1, 10), random.randint(1, 10), random.randint(1, 10))\n        self.probability = random.random()\n\n    def test_init(self):\n        LockedDropout(self.probability)\n\n    def test_forward(self):\n        dropout = LockedDropout(self.probability)\n        output = dropout.forward(self.input_)\n\n        # Check sizes\n        self.assertEqual(output.size(), self.input_.size())\n\n        # Check types\n        self.assertEqual(output.type(), 'torch.FloatTensor')\n\n    def test_forward_eval(self):\n        dropout = LockedDropout(self.probability).eval()\n        output = dropout.forward(self.input_)\n\n        # Check sizes\n        np.equal(output.numpy(), self.input_.numpy())\n"""
tests/nn/test_weight_drop.py,7,"b""import torch\nimport pytest\n\nfrom torchnlp.nn.weight_drop import WeightDropLSTM\nfrom torchnlp.nn.weight_drop import WeightDropGRU\nfrom torchnlp.nn.weight_drop import WeightDropLinear\nfrom torchnlp.nn.weight_drop import WeightDrop\n\n\ndef test_weight_drop_linear():\n    # Input is (seq, batch, input)\n    x = torch.randn(2, 1, 10)\n\n    lin = WeightDropLinear(10, 10, weight_dropout=0.9)\n    run1 = [x.sum() for x in lin(x).data]\n    run2 = [x.sum() for x in lin(x).data]\n\n    assert run1[0] != run2[0]\n    assert run1[1] != run2[1]\n\n\ndef test_weight_drop_lstm():\n    input_ = torch.randn(2, 1, 10)\n\n    wd_lstm = WeightDropLSTM(10, 10, num_layers=2, weight_dropout=0.9)\n    run1 = [x.sum() for x in wd_lstm(input_)[0].data]\n    run2 = [x.sum() for x in wd_lstm(input_)[0].data]\n\n    # First time step, not influenced by hidden to hidden weights, should be equal\n    assert pytest.approx(run1[0].item()) == pytest.approx(run2[0].item())\n    # Second step should not\n    assert run1[1] != run2[1]\n\n\ndef test_weight_drop_gru():\n    input_ = torch.randn(2, 1, 10)\n\n    wd_lstm = WeightDropGRU(10, 10, num_layers=2, weight_dropout=0.9)\n    run1 = [x.sum() for x in wd_lstm(input_)[0].data]\n    run2 = [x.sum() for x in wd_lstm(input_)[0].data]\n\n    # First time step, not influenced by hidden to hidden weights, should be equal\n    assert pytest.approx(run1[0].item()) == pytest.approx(run2[0].item())\n    # Second step should not\n    assert run1[1] != run2[1]\n\n\ndef test_weight_drop():\n    input_ = torch.randn(2, 1, 10)\n\n    wd_lstm = WeightDrop(torch.nn.LSTM(10, 10), ['weight_hh_l0'], dropout=0.9)\n    run1 = [x.sum() for x in wd_lstm(input_)[0].data]\n    run2 = [x.sum() for x in wd_lstm(input_)[0].data]\n\n    # First time step, not influenced by hidden to hidden weights, should be equal\n    assert pytest.approx(run1[0].item()) == pytest.approx(run2[0].item())\n    # Second step should not\n    assert run1[1] != run2[1]\n\n\ndef test_weight_drop_zero():\n    input_ = torch.randn(2, 1, 10)\n\n    wd_lstm = WeightDrop(torch.nn.LSTM(10, 10), ['weight_hh_l0'], dropout=0.0)\n    run1 = [x.sum() for x in wd_lstm(input_)[0].data]\n    run2 = [x.sum() for x in wd_lstm(input_)[0].data]\n\n    # First time step, not influenced by hidden to hidden weights, should be equal\n    assert pytest.approx(run1[0].item()) == pytest.approx(run2[0].item())\n    # Second step should not\n    assert pytest.approx(run1[1].item()) == pytest.approx(run2[1].item())\n"""
tests/nn/utils.py,0,"b'from itertools import product\n\n\ndef kwargs_product(dict_):\n    """"""\n    Args:\n        dict_ (dict): Dict with possible kwargs values.\n\n    Returns:\n        (iterable) Iterable over all combinations of the dict of kwargs.\n\n    Usage:\n\n        >>> list(kwargs_product({ \'number\': [1,2], \'character\': \'ab\' }))\n        [{\'number\': 1, \'character\': \'a\'}, {\'number\': 1, \'character\': \'b\'}, \\\n{\'number\': 2, \'character\': \'a\'}, {\'number\': 2, \'character\': \'b\'}]\n\n    """"""\n    return (dict(zip(dict_, x)) for x in product(*dict_.values()))\n'"
tests/samplers/test_balanced_sampler.py,0,"b""from collections import Counter\n\nimport pickle\nimport pytest\nimport random\n\nfrom torchnlp.random import fork_rng_wrap\nfrom torchnlp.samplers import BalancedSampler\n\n\n# NOTE: `fork_rng_wrap` to ensure the tests never randomly fail due to an rare sampling.\n@fork_rng_wrap(seed=123)\ndef test_balanced_sampler():\n    data = ['a', 'a', 'b', 'b', 'b', 'c']\n    num_samples = 10000\n    sampler = BalancedSampler(data, replacement=True, num_samples=num_samples)\n    assert len(sampler) == num_samples\n    samples = [data[i] for i in sampler]\n    assert len(samples) == num_samples\n    counts = Counter(samples)\n    assert counts['a'] / num_samples == pytest.approx(.33, 0.2)\n    assert counts['b'] / num_samples == pytest.approx(.33, 0.2)\n    assert counts['c'] / num_samples == pytest.approx(.33, 0.2)\n\n\n@fork_rng_wrap(seed=123)\ndef test_balanced_sampler__weighted():\n    data = [('a', 0), ('a', 1), ('a', 2), ('b', 2), ('c', 1)]\n    num_samples = 10000\n    sampler = BalancedSampler(\n        data,\n        replacement=True,\n        num_samples=num_samples,\n        get_weight=lambda e: e[1],\n        get_class=lambda e: e[0])\n    samples = [data[i] for i in sampler]\n    counts = Counter(samples)\n    assert counts[('a', 2)] / num_samples == pytest.approx(.22, 0.2)\n    assert counts[('a', 1)] / num_samples == pytest.approx(.11, 0.2)\n    assert counts[('a', 0)] / num_samples == 0.0\n    assert counts[('b', 2)] / num_samples == pytest.approx(.33, 0.2)\n    assert counts[('c', 1)] / num_samples == pytest.approx(.33, 0.2)\n\n\n@fork_rng_wrap(seed=123)\ndef test_balanced_sampler__nondeterministic():\n    data = [random.randint(1, 100) for i in range(100)]\n    sampler = BalancedSampler(data)\n    samples = [data[i] for i in sampler]\n    new_samples = [data[i] for i in sampler]\n    assert new_samples != samples\n\n\ndef test_pickleable():\n    data_source = [1, 2, 3, 4, 5]\n    sampler = BalancedSampler(data_source)\n    pickle.dumps(sampler)\n"""
tests/samplers/test_bptt_batch_sampler.py,0,"b""import pickle\nimport string\n\nimport pytest\n\nfrom torchnlp.samplers import BPTTBatchSampler\nfrom torchnlp.utils import sampler_to_iterator\n\n\n@pytest.fixture\ndef alphabet():\n    return list(string.ascii_lowercase)\n\n\n@pytest.fixture\ndef sampler(alphabet):\n    return BPTTBatchSampler(alphabet, bptt_length=2, batch_size=4, drop_last=True)\n\n\ndef test_bptt_batch_sampler_drop_last(sampler, alphabet):\n    # Test samplers iterate over chunks similar to:\n    # https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L112\n    list_ = list(sampler_to_iterator(alphabet, sampler))\n    assert list_[0] == [['a', 'b'], ['g', 'h'], ['m', 'n'], ['s', 't']]\n    assert len(sampler) == len(list_)\n\n\ndef test_bptt_batch_sampler(alphabet):\n    sampler = BPTTBatchSampler(alphabet, bptt_length=2, batch_size=4, drop_last=False)\n    list_ = list(sampler_to_iterator(alphabet, sampler))\n    assert list_[0] == [['a', 'b'], ['h', 'i'], ['o', 'p'], ['u', 'v']]\n    assert len(sampler) == len(list_)\n\n\ndef test_bptt_batch_sampler_example():\n    sampler = BPTTBatchSampler(range(100), bptt_length=2, batch_size=3, drop_last=False)\n    assert list(sampler)[0] == [slice(0, 2), slice(34, 36), slice(67, 69)]\n\n    sampler = BPTTBatchSampler(\n        range(100), bptt_length=2, batch_size=3, drop_last=False, type_='target')\n    assert list(sampler)[0] == [slice(1, 3), slice(35, 37), slice(68, 70)]\n\n\ndef test_is_pickleable(sampler):\n    pickle.dumps(sampler)\n"""
tests/samplers/test_bptt_sampler.py,0,"b""import pickle\nimport random\n\nimport pytest\n\nfrom torchnlp.samplers import BPTTSampler\n\n\n@pytest.fixture\ndef sampler():\n    return BPTTSampler(range(5), 2)\n\n\ndef test_bptt_sampler_odd(sampler):\n    assert list(sampler) == [slice(0, 2), slice(2, 4)]\n    assert len(sampler) == 2\n\n\ndef test_bptt_sampler_even():\n    sampler = BPTTSampler(range(6), 2, type_='target')\n    assert list(sampler) == [slice(1, 3), slice(3, 5), slice(5, 6)]\n    assert len(sampler) == 3\n\n\ndef test_bptt_sampler_length():\n    for i in range(1, 1000):\n        sampler = BPTTSampler(range(i), random.randint(1, 17))\n        assert len(sampler) == len(list(sampler))\n\n\ndef test_is_pickleable(sampler):\n    pickle.dumps(sampler)\n"""
tests/samplers/test_bucket_batch_sampler.py,1,"b'import pickle\n\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom torchnlp.random import fork_rng_wrap\nfrom torchnlp.samplers import BucketBatchSampler\n\n\ndef test_bucket_batch_sampler_length():\n    data_source = [[1], [2], [3], [4], [5], [6]]\n    sort_key = lambda i: len(data_source[i])\n    batch_size = 2\n    sampler = SequentialSampler(data_source)\n    batch_sampler = BucketBatchSampler(\n        sampler,\n        batch_size=batch_size,\n        sort_key=sort_key,\n        drop_last=False,\n        bucket_size_multiplier=2)\n    batches = list(batch_sampler)\n    assert len(batches) == 3\n    assert len(batch_sampler) == 3\n\n\ndef test_bucket_batch_sampler_uneven_length():\n    data_source = [[1], [2], [3], [4], [5]]\n    sort_key = lambda i: len(data_source[i])\n    batch_size = 2\n    sampler = SequentialSampler(data_source)\n    batch_sampler = BucketBatchSampler(\n        sampler, batch_size, sort_key=sort_key, drop_last=False, bucket_size_multiplier=2)\n    batches = list(batch_sampler)\n    assert len(batches) == 3\n    assert len(batch_sampler) == 3\n    batch_sampler = BucketBatchSampler(\n        sampler, batch_size, sort_key=sort_key, drop_last=True, bucket_size_multiplier=2)\n    batches = list(batch_sampler)\n    assert len(batches) == 2\n    assert len(batch_sampler) == 2\n\n\ndef test_bucket_batch_sampler_sorted():\n    data_source = [[1], [2], [3], [4], [5]]\n    sort_key = lambda i: data_source[i]\n    batch_size = len(data_source)\n    sampler = SequentialSampler(data_source)\n    batches = list(\n        BucketBatchSampler(\n            sampler, batch_size, sort_key=sort_key, drop_last=False, bucket_size_multiplier=1))\n    for i, batch in enumerate(batches):\n        assert batch[0] == i\n\n\n@fork_rng_wrap(seed=123)\ndef test_bucket_batch_sampler():\n    sampler = SequentialSampler(list(range(10)))\n    batch_sampler = BucketBatchSampler(\n        sampler, batch_size=3, drop_last=False, bucket_size_multiplier=2)\n    assert len(batch_sampler) == 4\n    assert list(batch_sampler) == [[0, 1, 2], [3, 4, 5], [9], [6, 7, 8]]\n\n\ndef test_bucket_batch_sampler__drop_last():\n    sampler = SequentialSampler(list(range(10)))\n    batch_sampler = BucketBatchSampler(\n        sampler, batch_size=3, drop_last=True, bucket_size_multiplier=2)\n    assert len(batch_sampler) == 3\n    assert len(list(iter(batch_sampler))) == 3\n\n\ndef test_pickleable():\n    sampler = SequentialSampler(list(range(10)))\n    batch_sampler = BucketBatchSampler(\n        sampler, batch_size=2, drop_last=False, bucket_size_multiplier=2)\n    pickle.dumps(batch_sampler)\n'"
tests/samplers/test_deterministic_sampler.py,0,"b'import random\nimport pickle\n\nfrom torchnlp.random import fork_rng\nfrom torchnlp.random import set_seed\nfrom torchnlp.samplers import BalancedSampler\nfrom torchnlp.samplers import DeterministicSampler\n\n\ndef test_deterministic_sampler__nondeterministic_iter():\n    with fork_rng(seed=123):\n        data = [random.randint(1, 100) for i in range(100)]\n\n    sampler = DeterministicSampler(BalancedSampler(data), random_seed=123)\n    assert len(sampler) == len(data)\n    samples = [data[i] for i in sampler]\n    assert samples[:10] == [3, 35, 99, 43, 67, 82, 66, 68, 100, 14]\n\n    # NOTE: Each iteration is new sample from `sampler`; however, the entire sequence of iterations\n    # is deterministic based on the `random_seed=123`\n    new_samples = [data[i] for i in sampler]\n    assert samples != new_samples\n\n\ndef test_deterministic_sampler__nondeterministic_next():\n\n    class _Sampler():\n\n        def __iter__(self):\n            for _ in range(100):\n                yield random.randint(1, 100)\n\n    sampler = DeterministicSampler(_Sampler(), random_seed=123)\n    assert list(sampler)[:10] == [7, 35, 12, 99, 53, 35, 14, 5, 49, 69]\n\n\ndef test_deterministic_sampler__side_effects():\n    """""" Ensure that the sampler does not affect random generation after it\'s finished. """"""\n    set_seed(123)\n    pre_randint = [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n    sampler = DeterministicSampler(list(range(10)), random_seed=123)\n    list(iter(sampler))\n\n    post_randint = [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n    set_seed(123)\n    assert pre_randint == [random.randint(1, 2**31), random.randint(1, 2**31)]\n    assert post_randint == [random.randint(1, 2**31), random.randint(1, 2**31)]\n\n\ndef test_pickleable():\n    data_source = [1, 2, 3, 4, 5]\n    sampler = DeterministicSampler(data_source, random_seed=123)\n    pickle.dumps(sampler)\n'"
tests/samplers/test_distributed_batch_sampler.py,2,"b'import pickle\n\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom torchnlp.samplers import DistributedBatchSampler\n\n\ndef test_distributed_batch_sampler():\n    sampler = SequentialSampler(list(range(15)))\n    batch_sampler = BatchSampler(sampler, 10, False)\n\n    distributed_sampler = DistributedBatchSampler(batch_sampler, num_replicas=4, rank=0)\n    assert list(distributed_sampler) == [[0, 4, 8], [10, 14]]\n    assert len(distributed_sampler) == 2\n\n    distributed_sampler = DistributedBatchSampler(batch_sampler, num_replicas=4, rank=1)\n    assert list(distributed_sampler) == [[1, 5, 9], [11]]\n    assert len(distributed_sampler) == 2\n\n    distributed_sampler = DistributedBatchSampler(batch_sampler, num_replicas=4, rank=2)\n    assert list(distributed_sampler) == [[2, 6], [12]]\n    assert len(distributed_sampler) == 2\n\n    distributed_sampler = DistributedBatchSampler(batch_sampler, num_replicas=4, rank=3)\n    assert list(distributed_sampler) == [[3, 7], [13]]\n    assert len(distributed_sampler) == 2\n\n\ndef test_pickleable():\n    sampler = SequentialSampler(list(range(15)))\n    batch_sampler = BatchSampler(sampler, 10, False)\n    sampler = DistributedBatchSampler(batch_sampler, num_replicas=1, rank=0)\n    pickle.dumps(sampler)\n'"
tests/samplers/test_distributed_sampler.py,1,"b'import pickle\n\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom torchnlp.samplers import DistributedSampler\n\n\ndef test_distributed_batch_sampler():\n    sampler = SequentialSampler(list(range(15)))\n\n    distributed_sampler = DistributedSampler(sampler, num_replicas=3, rank=0)\n    assert list(distributed_sampler) == [0, 3, 6, 9, 12]\n\n    distributed_sampler = DistributedSampler(sampler, num_replicas=3, rank=1)\n    assert list(distributed_sampler) == [1, 4, 7, 10, 13]\n\n    distributed_sampler = DistributedSampler(sampler, num_replicas=3, rank=2)\n    assert list(distributed_sampler) == [2, 5, 8, 11, 14]\n\n\ndef test_pickleable():\n    sampler = SequentialSampler(list(range(15)))\n    sampler = DistributedSampler(sampler, num_replicas=3, rank=2)\n    pickle.dumps(sampler)\n'"
tests/samplers/test_noisy_sorted_sampler.py,0,"b'import pickle\n\nfrom torchnlp.samplers import NoisySortedSampler\n\n\ndef test_noisy_sorted_sampler():\n    data_source = [1, 2, 3, 4, 5, 6]\n    indexes = list(NoisySortedSampler(data_source))\n    assert len(indexes) == len(data_source)\n\n\ndef test_noisy_sorted_sampler_sorted():\n    data_source = [1, 2, 3, 4, 5, 6]\n    indexes = list(NoisySortedSampler(data_source, get_noise=lambda e: 0.0))\n    assert len(indexes) == len(data_source)\n    for i, j in enumerate(indexes):\n        assert i == j\n\n\ndef test_noisy_sorted_sampler_sort_key_noise():\n    data_source = [2, 6, 10]\n    # `sort_key_noise` does not affect values 2, 6, 10\n    indexes = list(NoisySortedSampler(data_source, get_noise=lambda e: e * 0.25))\n    for i, j in enumerate(indexes):\n        assert i == j\n\n\ndef test_pickleable():\n    data_source = [1, 2, 3, 4, 5, 6]\n    sampler = NoisySortedSampler(data_source)\n    pickle.dumps(sampler)\n'"
tests/samplers/test_oom_batch_sampler.py,3,"b'from functools import partial\n\nimport pickle\n\nimport torch\n\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom torchnlp.samplers import get_number_of_elements\nfrom torchnlp.samplers import OomBatchSampler\n\n\ndef test_oom_batch_sampler():\n    data = list(range(-4, 14))\n    sampler = SequentialSampler(data)\n    batch_sampler = BatchSampler(sampler, 4, False)\n    oom_sampler = OomBatchSampler(batch_sampler, lambda i: data[i], num_batches=3)\n    list_ = list(oom_sampler)\n    # The largest batches are first\n    assert [data[i] for i in list_[0]] == [8, 9, 10, 11]\n    assert [data[i] for i in list_[1]] == [12, 13]\n    assert [data[i] for i in list_[2]] == [4, 5, 6, 7]\n    assert len(list_) == 5\n\n\ndef test_get_number_of_elements():\n    assert get_number_of_elements([torch.randn(5, 5), torch.randn(4, 4)]) == 41\n\n\ndef get_index(data, i):\n    return data[i]\n\n\ndef test_pickleable():\n    data = list(range(-4, 14))\n    sampler = SequentialSampler(data)\n    batch_sampler = BatchSampler(sampler, 4, False)\n    get_data = partial(get_index, data)\n    oom_sampler = OomBatchSampler(batch_sampler, get_data, num_batches=3)\n    pickle.dumps(oom_sampler)\n'"
tests/samplers/test_repeat_sampler.py,0,b'import pickle\n\nfrom torchnlp.samplers import RepeatSampler\n\n\ndef test_repeat_sampler():\n    sampler = RepeatSampler([1])\n    iterator = iter(sampler)\n    assert next(iterator) == 1\n    assert next(iterator) == 1\n    assert next(iterator) == 1\n\n\ndef test_pickleable():\n    sampler = RepeatSampler([1])\n    pickle.dumps(sampler)\n'
tests/samplers/test_sorted_sampler.py,0,"b'import pickle\n\nfrom torchnlp.samplers import SortedSampler\n\n\ndef test_sorted_sampler():\n    data_source = [[1], [2], [3], [4], [5], [6]]\n    sort_key = lambda r: r[0]\n    indexes = list(SortedSampler(data_source, sort_key=sort_key))\n    assert len(indexes) == len(data_source)\n    for i, j in enumerate(indexes):\n        assert i == j\n\n\ndef test_pickleable():\n    data_source = [[1], [2], [3], [4], [5], [6]]\n    sampler = SortedSampler(data_source)\n    pickle.dumps(sampler)\n'"
tests/word_to_vector/__init__.py,0,b''
tests/word_to_vector/test_bpemb.py,0,"b'import os\nimport mock\n\nfrom tests.word_to_vector.utils import urlretrieve_side_effect\nfrom torchnlp.word_to_vector import BPEmb\n\n\n@mock.patch(\'urllib.request.urlretrieve\')\ndef test_bpemb(mock_urlretrieve):\n    directory = \'tests/_test_data/bpemb/\'\n\n    # Make sure URL has a 200 status\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Attempt to parse a subset of BPEmb\n    vectors = BPEmb(cache=directory)\n\n    # Our test data only contains a subset of 5 tokens\n    assert len(vectors) == 5\n\n    # Embedding dimensionalty should be 300 by default\n    assert len(vectors[\'\xe2\x96\x81the\']) == 300\n\n    # Test implementation of __contains()__\n    assert \'\xe2\x96\x81the\' in vectors\n\n    # Test with the unknown characters\n    assert len(vectors[\'\xe6\xbc\xa2\xe5\xad\x97\']) == 300\n\n    # Clean up\n    os.remove(os.path.join(directory, \'en.wiki.bpe.op50000.d300.w2v.txt.pt\'))\n\n\ndef test_unsupported_language():\n    error_class = None\n    error_message = \'\'\n\n    try:\n        BPEmb(language=\'python\')\n    except Exception as e:\n        error_class = e.__class__\n        error_message = str(e)\n\n    assert error_class is ValueError\n    assert error_message.startswith(""Language \'python\' not supported."")\n\n\ndef test_unsupported_dim():\n    error_class = None\n    error_message = \'\'\n\n    try:\n        BPEmb(dim=42)\n    except Exception as e:\n        error_class = e.__class__\n        error_message = str(e)\n\n    assert error_class is ValueError\n    assert error_message.startswith(""Embedding dimensionality of \'42\' not "" ""supported."")\n\n\ndef test_unsupported_merge_ops():\n    error_class = None\n    error_message = \'\'\n\n    try:\n        BPEmb(merge_ops=42)\n    except Exception as e:\n        error_class = e.__class__\n        error_message = str(e)\n\n    assert error_class is ValueError\n    assert error_message.startswith(""Number of \'42\' merge operations not "" ""supported."")\n'"
tests/word_to_vector/test_char_n_gram.py,0,"b'import os\nimport mock\n\nfrom torchnlp.word_to_vector import CharNGram\nfrom torchnlp.encoders.text import DEFAULT_UNKNOWN_TOKEN\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_charngram_100d(mock_urlretrieve):\n    directory = \'tests/_test_data/char_n_gram/\'\n\n    # Make sure URL has a 200 status\n    # TODO: Skip for now due to SSL failure.\n    # mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Attempt to parse a subset of CharNGram\n    vectors = CharNGram(cache=directory)\n    assert len(vectors[\'e\']) == 100\n\n    # Test with the unknown characters\n    assert len(vectors[\'\xe6\xbc\xa2\xe5\xad\x97\']) == 100\n    assert len(vectors[DEFAULT_UNKNOWN_TOKEN]) == 100\n\n    # Clean up\n    os.remove(directory + \'charNgram.txt.pt\')\n    os.remove(directory + \'charNgram.txt\')\n'"
tests/word_to_vector/test_fast_text.py,0,"b""import os\nimport mock\n\nfrom torchnlp.word_to_vector import FastText\nfrom tests.word_to_vector.utils import urlretrieve_side_effect\n\n\n@mock.patch('urllib.request.urlretrieve')\ndef test_fasttext_simple(mock_urlretrieve):\n    directory = 'tests/_test_data/fast_text/'\n\n    # Make sure URL has a 200 status\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Attempt to parse a subset of FastText\n    vectors = FastText(language='simple', cache=directory)\n    assert len(vectors['the']) == 300\n    assert len(vectors) > 1\n\n    # Test cache and `is_include`\n    vectors = FastText(language='simple', is_include=lambda w: w == 'the', cache=directory)\n    assert 'the' in vectors.token_to_index\n    assert len(vectors) == 1\n\n    # Test implementation of __contains()__\n    assert 'the' in vectors\n\n    # Test with the unknown characters\n    assert len(vectors['\xe6\xbc\xa2\xe5\xad\x97']) == 300\n\n    # Clean up\n    os.remove(os.path.join(directory, 'wiki.simple.vec.pt'))\n\n\n@mock.patch('urllib.request.urlretrieve')\ndef test_fasttext_list_arguments(mock_urlretrieve):\n    directory = 'tests/_test_data/fast_text/'\n\n    # Make sure URL has a 200 status\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Load subset of FastText\n    vectors = FastText(language='simple', cache=directory)\n\n    # Test implementation of __getitem()__ for token list and tuple\n    list(vectors[['the', 'of']].shape) == [2, 300]\n    list(vectors[('the', 'of')].shape) == [2, 300]\n\n    # Clean up\n    os.remove(os.path.join(directory, 'wiki.simple.vec.pt'))\n\n\n@mock.patch('urllib.request.urlretrieve')\ndef test_fasttext_non_list_or_tuple_raises_type_error(mock_urlretrieve):\n    directory = 'tests/_test_data/fast_text/'\n\n    # Make sure URL has a 200 status\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Load subset of FastText\n    vectors = FastText(language='simple', cache=directory)\n\n    # Test implementation of __getitem()__ for invalid type\n    error_class = None\n\n    try:\n        vectors[None]\n    except Exception as e:\n        error_class = e.__class__\n\n    assert error_class is TypeError\n\n    # Clean up\n    os.remove(os.path.join(directory, 'wiki.simple.vec.pt'))\n\n\n@mock.patch('urllib.request.urlretrieve')\ndef test_aligned_fasttext(mock_urlretrieve):\n    directory = 'tests/_test_data/fast_text/'\n\n    # Make sure URL has a 200 status\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Parse the aligned FastText embeddings\n    vectors = FastText(aligned=True, cache=directory)\n\n    # Assert the embeddings' dimensionality\n    assert len(vectors['the']) == 300\n    # Our test file contains only five words to keep the file size small\n    assert len(vectors) == 5\n\n    # Clean up\n    os.remove(os.path.join(directory, 'wiki.en.align.vec.pt'))\n"""
tests/word_to_vector/test_glove.py,0,"b'import os\nimport mock\n\nfrom torchnlp.word_to_vector import GloVe\nfrom tests.word_to_vector.utils import urlretrieve_side_effect\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_glove_6b_50(mock_urlretrieve):\n    directory = \'tests/_test_data/glove/\'\n\n    # Make sure URL has a 200 status\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Attempt to parse a subset of GloVe\n    vectors = GloVe(name=""6B"", dim=""50"", cache=directory)\n    assert len(vectors[\'the\']) == 50\n\n    # Test with the unknown characters\n    assert len(vectors[\'\xe6\xbc\xa2\xe5\xad\x97\']) == 50\n\n    # Clean up\n    os.remove(directory + \'glove.6B.50d.txt.pt\')\n\n\n@mock.patch(""urllib.request.urlretrieve"")\ndef test_glove_twitter_6b_25(mock_urlretrieve):\n    directory = \'tests/_test_data/glove/\'\n\n    # Make sure URL has a 200 status\n    mock_urlretrieve.side_effect = urlretrieve_side_effect\n\n    # Attempt to parse a subset of GloVe\n    vectors = GloVe(name=""twitter.27B"", dim=""25"", cache=directory)\n    assert len(vectors[\'the\']) == 25\n\n    # Test with the unknown characters\n    assert len(vectors[\'\xe6\xbc\xa2\xe5\xad\x97\']) == 25\n\n    # Clean up\n    os.remove(directory + \'glove.twitter.27B.25d.txt.pt\')\n'"
tests/word_to_vector/utils.py,0,"b'import urllib.request\n\n\n# Check the URL requested is valid\ndef urlretrieve_side_effect(url, **kwargs):\n    assert urllib.request.urlopen(url).getcode() == 200\n'"
torchnlp/_third_party/__init__.py,0,b''
torchnlp/_third_party/lazy_loader.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" A LazyLoader class. """"""\n\nimport importlib\nimport types\n\n\nclass LazyLoader(types.ModuleType):\n    """"""\n    Lazily import a module, mainly to avoid pulling in large dependencies. `contrib`, and\n    `ffmpeg` are examples of modules that are large and not always needed, and this allows them to\n    only be loaded when they are used.\n    """"""\n\n    def __init__(self, local_name, parent_module_globals, name):\n        self._local_name = local_name\n        self._parent_module_globals = parent_module_globals\n\n        super(LazyLoader, self).__init__(name)\n\n    def _load(self):\n        """""" Load the module and insert it into the parent\'s globals. """"""\n\n        # Import the target module and insert it into the parent\'s namespace\n        module = importlib.import_module(self.__name__)\n        self._parent_module_globals[self._local_name] = module\n\n        # Update this object\'s dict so that if someone keeps a reference to the\n        #   LazyLoader, lookups are efficient (__getattr__ is only called on lookups\n        #   that fail).\n        self.__dict__.update(module.__dict__)\n\n        return module\n\n    def __call__(self, *args, **kwargs):\n        module = self._load()\n        return module(*args, **kwargs)\n\n    def __getattr__(self, item):\n        module = self._load()\n        return getattr(module, item)\n\n    def __dir__(self):\n        module = self._load()\n        return dir(module)\n'"
torchnlp/_third_party/weighted_random_sampler.py,4,"b'import torch\n\nfrom torch.utils.data.sampler import Sampler\nfrom torch._six import int_classes as _int_classes\n\n\nclass WeightedRandomSampler(Sampler):\n\n    def __init__(self, weights, num_samples=None, replacement=True):\n        # NOTE: Adapted `WeightedRandomSampler` to accept `num_samples=0` and `num_samples=None`.\n        if num_samples is None:\n            num_samples = len(weights)\n        if not isinstance(num_samples, _int_classes) or isinstance(num_samples, bool) or \\\n                num_samples < 0:\n            raise ValueError(""num_samples should be a positive integer ""\n                             ""value, but got num_samples={}"".format(num_samples))\n        if not isinstance(replacement, bool):\n            raise ValueError(""replacement should be a boolean value, but got ""\n                             ""replacement={}"".format(replacement))\n        self.weights = torch.as_tensor(weights, dtype=torch.double)\n        self.num_samples = num_samples\n        self.replacement = replacement\n\n    def __iter__(self):\n        if self.num_samples == 0:\n            return iter([])\n\n        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())\n\n    def __len__(self):\n        return self.num_samples\n'"
torchnlp/datasets/__init__.py,0,"b""from torchnlp.datasets.reverse import reverse_dataset\nfrom torchnlp.datasets.count import count_dataset\nfrom torchnlp.datasets.zero import zero_dataset\nfrom torchnlp.datasets.simple_qa import simple_qa_dataset\nfrom torchnlp.datasets.imdb import imdb_dataset\nfrom torchnlp.datasets.wikitext_2 import wikitext_2_dataset\nfrom torchnlp.datasets.penn_treebank import penn_treebank_dataset\nfrom torchnlp.datasets.ud_pos import ud_pos_dataset\nfrom torchnlp.datasets.snli import snli_dataset\nfrom torchnlp.datasets.trec import trec_dataset\nfrom torchnlp.datasets.multi30k import multi30k_dataset\nfrom torchnlp.datasets.iwslt import iwslt_dataset\nfrom torchnlp.datasets.wmt import wmt_dataset\nfrom torchnlp.datasets.smt import smt_dataset\n\n__all__ = [\n    'wmt_dataset',\n    'iwslt_dataset',\n    'multi30k_dataset',\n    'snli_dataset',\n    'simple_qa_dataset',\n    'imdb_dataset',\n    'wikitext_2_dataset',\n    'penn_treebank_dataset',\n    'ud_pos_dataset',\n    'trec_dataset',\n    'reverse_dataset',\n    'count_dataset',\n    'zero_dataset',\n    'smt_dataset',\n]\n"""
torchnlp/datasets/count.py,0,"b'import random\n\n\ndef count_dataset(train=False,\n                  dev=False,\n                  test=False,\n                  train_rows=10000,\n                  dev_rows=1000,\n                  test_rows=1000,\n                  seq_max_length=10):\n    """"""\n    Load the Count dataset.\n\n    The Count dataset is a simple task of counting the number of integers in a sequence. This\n    dataset is useful for testing implementations of sequence to label models.\n\n    Args:\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_rows (int, optional): Number of training rows to generate.\n        dev_rows (int, optional): Number of development rows to generate.\n        test_rows (int, optional): Number of test rows to generate.\n        seq_max_length (int, optional): Maximum sequence length.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.random import set_seed\n        >>> set_seed(321)\n        >>>\n        >>> from torchnlp.datasets import count_dataset\n        >>> train = count_dataset(train=True)\n        >>> train[0:2]\n        [{\'numbers\': \'6 2 5 8 7\', \'count\': \'5\'}, {\'numbers\': \'3 9 7 6 6 7\', \'count\': \'6\'}]\n    """"""\n    ret = []\n    for is_requested, n_rows in [(train, train_rows), (dev, dev_rows), (test, test_rows)]:\n        rows = []\n        for i in range(n_rows):\n            length = random.randint(1, seq_max_length)\n            seq = []\n            for _ in range(length):\n                seq.append(str(random.randint(0, 9)))\n            input_ = \' \'.join(seq)\n            rows.append({\'numbers\': input_, \'count\': str(length)})\n\n        # NOTE: Given that `random.randint` is deterministic with the same `random_seed` we need\n        # to allow the random generator to create the train, dev and test dataset in order.\n        # Otherwise, `reverse(train=True)` and `reverse(test=True)` would share the first 1000 rows.\n        if not is_requested:\n            continue\n\n        ret.append(rows)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/imdb.py,0,"b'import os\nimport glob\n\nfrom torchnlp.download import download_file_maybe_extract\n\n\ndef imdb_dataset(directory=\'data/\',\n                 train=False,\n                 test=False,\n                 train_directory=\'train\',\n                 test_directory=\'test\',\n                 extracted_name=\'aclImdb\',\n                 check_files=[\'aclImdb/README\'],\n                 url=\'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\',\n                 sentiments=[\'pos\', \'neg\']):\n    """"""\n    Load the IMDB dataset (Large Movie Review Dataset v1.0).\n\n    This is a dataset for binary sentiment classification containing substantially more data than\n    previous benchmark datasets. Provided a set of 25,000 highly polar movie reviews for\n    training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text\n    and already processed bag of words formats are provided.\n\n    Note:\n        The order examples are returned is not guaranteed due to ``iglob``.\n\n    **Reference:** http://ai.stanford.edu/~amaas/data/sentiment/\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_directory (str, optional): The directory of the training split.\n        test_directory (str, optional): The directory of the test split.\n        extracted_name (str, optional): Name of the extracted dataset directory.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset ``tar.gz`` file.\n        sentiments (list of str, optional): Sentiments to load from the dataset.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import imdb_dataset  # doctest: +SKIP\n        >>> train = imdb_dataset(train=True)  # doctest: +SKIP\n        >>> train[0:2]  # doctest: +SKIP\n        [{\n          \'text\': \'For a movie that gets no respect there sure are a lot of memorable quotes...\',\n          \'sentiment\': \'pos\'\n        }, {\n          \'text\': \'Bizarre horror movie filled with famous faces but stolen by Cristina Raines...\',\n          \'sentiment\': \'pos\'\n        }]\n    """"""\n    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [\n        dir_ for (requested, dir_) in [(train, train_directory), (test, test_directory)]\n        if requested\n    ]\n    for split_directory in splits:\n        full_path = os.path.join(directory, extracted_name, split_directory)\n        examples = []\n        for sentiment in sentiments:\n            for filename in glob.iglob(os.path.join(full_path, sentiment, \'*.txt\')):\n                with open(filename, \'r\', encoding=""utf-8"") as f:\n                    text = f.readline()\n                examples.append({\n                    \'text\': text,\n                    \'sentiment\': sentiment,\n                })\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/iwslt.py,0,"b'import os\nimport xml.etree.ElementTree as ElementTree\nimport io\nimport glob\n\nfrom torchnlp.download import download_file_maybe_extract\n\n\ndef iwslt_dataset(\n        directory=\'data/iwslt/\',\n        train=False,\n        dev=False,\n        test=False,\n        language_extensions=[\'en\', \'de\'],\n        train_filename=\'{source}-{target}/train.{source}-{target}.{lang}\',\n        dev_filename=\'{source}-{target}/IWSLT16.TED.tst2013.{source}-{target}.{lang}\',\n        test_filename=\'{source}-{target}/IWSLT16.TED.tst2014.{source}-{target}.{lang}\',\n        check_files=[\'{source}-{target}/train.tags.{source}-{target}.{source}\'],\n        url=\'https://wit3.fbk.eu/archive/2016-01/texts/{source}/{target}/{source}-{target}.tgz\'):\n    """"""\n    Load the International Workshop on Spoken Language Translation (IWSLT) 2017 translation dataset.\n\n    In-domain training, development and evaluation sets were supplied through the website of the\n    WIT3 project, while out-of-domain training data were linked in the workshop\xe2\x80\x99s website. With\n    respect to edition 2016 of the evaluation campaign, some of the talks added to the TED\n    repository during the last year have been used to define the evaluation sets (tst2017), while\n    the remaining new talks have been included in the training sets.\n\n    The English data that participants were asked to recognize and translate consists in part of\n    TED talks as in the years before, and in part of real-life lectures and talks that have been\n    mainly recorded in lecture halls at KIT and Carnegie Mellon University. TED talks are\n    challenging due to their variety in topics, but are very benign as they are very thoroughly\n    rehearsed and planned, leading to easy to recognize and translate language.\n\n    Note:\n        The order examples are returned is not guaranteed due to ``iglob``.\n\n    References:\n      * http://workshop2017.iwslt.org/downloads/iwslt2017_proceeding_v2.pdf\n      * http://workshop2017.iwslt.org/\n\n    **Citation:**\n    M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated\n    Talks. In Proc. of EAMT, pp. 261-268, Trento, Italy.\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the dev split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        language_extensions (:class:`list` of :class:`str`): Two language extensions\n            [\'en\'|\'de\'|\'it\'|\'ni\'|\'ro\'] to load.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the dev split.\n        test_filename (str, optional): The filename of the test split.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset file.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import iwslt_dataset  # doctest: +SKIP\n        >>> train = iwslt_dataset(train=True)  # doctest: +SKIP\n        >>> train[:2]  # doctest: +SKIP\n        [{\n          \'en\': ""David Gallo: This is Bill Lange. I\'m Dave Gallo."",\n          \'de\': \'David Gallo: Das ist Bill Lange. Ich bin Dave Gallo.\'\n        }, {\n          \'en\': ""And we\'re going to tell you some stories from the sea here in video."",\n          \'de\': \'Wir werden Ihnen einige Geschichten \xc3\xbcber das Meer in Videoform erz\xc3\xa4hlen.\'\n        }]\n    """"""\n    if len(language_extensions) != 2:\n        raise ValueError(""`language_extensions` must be two language extensions ""\n                         ""[\'en\'|\'de\'|\'it\'|\'ni\'|\'ro\'] to load."")\n\n    # Format Filenames\n    source, target = tuple(language_extensions)\n    check_files = [s.format(source=source, target=target) for s in check_files]\n    url = url.format(source=source, target=target)\n\n    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n\n    iwslt_clean(os.path.join(directory, \'{source}-{target}\'.format(source=source, target=target)))\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        examples = []\n        for extension in language_extensions:\n            path = os.path.join(directory,\n                                filename.format(lang=extension, source=source, target=target))\n            with open(path, \'r\', encoding=\'utf-8\') as f:\n                language_specific_examples = [l.strip() for l in f]\n\n            if len(examples) == 0:\n                examples = [{} for _ in range(len(language_specific_examples))]\n            for i, example in enumerate(language_specific_examples):\n                examples[i][extension] = example\n\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n\n\ndef iwslt_clean(directory):\n    # Thanks to torchtext for this snippet:\n    # https://github.com/pytorch/text/blob/ea64e1d28c794ed6ffc0a5c66651c33e2f57f01f/torchtext/datasets/translation.py#L152\n    for xml_filename in glob.iglob(os.path.join(directory, \'*.xml\')):\n        txt_filename = os.path.splitext(xml_filename)[0]\n        if os.path.isfile(txt_filename):\n            continue\n\n        with io.open(txt_filename, mode=\'w\', encoding=\'utf-8\') as f:\n            root = ElementTree.parse(xml_filename).getroot()[0]\n            for doc in root.findall(\'doc\'):\n                for element in doc.findall(\'seg\'):\n                    f.write(element.text.strip() + \'\\n\')\n\n    xml_tags = [\n        \'<url\', \'<keywords\', \'<talkid\', \'<description\', \'<reviewer\', \'<translator\', \'<title\',\n        \'<speaker\'\n    ]\n    for original_filename in glob.iglob(os.path.join(directory, \'train.tags*\')):\n        txt_filename = original_filename.replace(\'.tags\', \'\')\n        if os.path.isfile(txt_filename):\n            continue\n\n        with io.open(txt_filename, mode=\'w\', encoding=\'utf-8\') as txt_file, \\\n                io.open(original_filename, mode=\'r\', encoding=\'utf-8\') as original_file:\n            for line in original_file:\n                if not any(tag in line for tag in xml_tags):\n                    txt_file.write(line.strip() + \'\\n\')\n'"
torchnlp/datasets/multi30k.py,0,"b'import os\n\nfrom torchnlp.download import download_files_maybe_extract\n\n\ndef multi30k_dataset(directory=\'data/multi30k/\',\n                     train=False,\n                     dev=False,\n                     test=False,\n                     train_filename=\'train\',\n                     dev_filename=\'val\',\n                     test_filename=\'test\',\n                     check_files=[\'train.de\', \'val.de\'],\n                     urls=[\n                         \'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\',\n                         \'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\',\n                         \'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz\'\n                     ]):\n    """"""\n    Load the WMT 2016 machine translation dataset.\n\n    As a translation task, this task consists in translating English sentences that describe an\n    image into German, given the English sentence itself. As training and development data, we\n    provide 29,000 and 1,014 triples respectively, each containing an English source sentence, its\n    German human translation. As test data, we provide a new set of 1,000 tuples containing an\n    English description.\n\n    Status:\n        Host ``www.quest.dcs.shef.ac.uk`` forgot to update their SSL\n        certificate; therefore, this dataset does not download securely.\n\n    References:\n        * http://www.statmt.org/wmt16/multimodal-task.html\n        * http://shannon.cs.illinois.edu/DenotationGraph/\n\n    **Citation**\n    ::\n\n        @article{elliott-EtAl:2016:VL16,\n            author    = {{Elliott}, D. and {Frank}, S. and {Sima\'an}, K. and {Specia}, L.},\n            title     = {Multi30K: Multilingual English-German Image Descriptions},\n            booktitle = {Proceedings of the 5th Workshop on Vision and Language},\n            year      = {2016},\n            pages     = {70--74},\n            year      = 2016\n        }\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the dev split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_directory (str, optional): The directory of the training split.\n        dev_directory (str, optional): The directory of the dev split.\n        test_directory (str, optional): The directory of the test split.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        urls (str, optional): URLs to download.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import multi30k_dataset  # doctest: +SKIP\n        >>> train = multi30k_dataset(train=True)  # doctest: +SKIP\n        >>> train[:2]  # doctest: +SKIP\n        [{\n          \'en\': \'Two young, White males are outside near many bushes.\',\n          \'de\': \'Zwei junge wei\xc3\x9fe M\xc3\xa4nner sind im Freien in der N\xc3\xa4he vieler B\xc3\xbcsche.\'\n        }, {\n          \'en\': \'Several men in hard hatsare operating a giant pulley system.\',\n          \'de\': \'Mehrere M\xc3\xa4nner mit Schutzhelmen bedienen ein Antriebsradsystem.\'\n        }]\n    """"""\n    download_files_maybe_extract(urls=urls, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n\n    for filename in splits:\n        examples = []\n\n        en_path = os.path.join(directory, filename + \'.en\')\n        de_path = os.path.join(directory, filename + \'.de\')\n        en_file = [l.strip() for l in open(en_path, \'r\', encoding=\'utf-8\')]\n        de_file = [l.strip() for l in open(de_path, \'r\', encoding=\'utf-8\')]\n        assert len(en_file) == len(de_file)\n        for i in range(len(en_file)):\n            if en_file[i] != \'\' and de_file[i] != \'\':\n                examples.append({\'en\': en_file[i], \'de\': de_file[i]})\n\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/penn_treebank.py,0,"b'import os\nimport io\n\nfrom torchnlp.download import download_files_maybe_extract\nfrom torchnlp.encoders.text import DEFAULT_EOS_TOKEN\nfrom torchnlp.encoders.text import DEFAULT_UNKNOWN_TOKEN\n\n\ndef penn_treebank_dataset(\n        directory=\'data/penn-treebank\',\n        train=False,\n        dev=False,\n        test=False,\n        train_filename=\'ptb.train.txt\',\n        dev_filename=\'ptb.valid.txt\',\n        test_filename=\'ptb.test.txt\',\n        check_files=[\'ptb.train.txt\'],\n        urls=[\n            \'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\',\n            \'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt\',\n            \'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt\'\n        ],\n        unknown_token=DEFAULT_UNKNOWN_TOKEN,\n        eos_token=DEFAULT_EOS_TOKEN):\n    """"""\n    Load the Penn Treebank dataset.\n\n    This is the Penn Treebank Project: Release 2 CDROM, featuring a million words of 1989 Wall\n    Street Journal material.\n\n    **Reference:** https://catalog.ldc.upenn.edu/LDC99T42\n\n    **Citation:**\n    Marcus, Mitchell P., Marcinkiewicz, Mary Ann & Santorini, Beatrice (1993).\n    Building a Large Annotated Corpus of English: The Penn Treebank\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the development split.\n        test_filename (str, optional): The filename of the test split.\n        name (str, optional): Name of the dataset directory.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        urls (str, optional): URLs to download.\n        unknown_token (str, optional): Token to use for unknown words.\n        eos_token (str, optional): Token to use at the end of sentences.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import penn_treebank_dataset  # doctest: +SKIP\n        >>> train = penn_treebank_dataset(train=True)  # doctest: +SKIP\n        >>> train[:10]  # doctest: +SKIP\n        [\'aer\', \'banknote\', \'berlitz\', \'calloway\', \'centrust\', \'cluett\', \'fromstein\', \'gitano\',\n        \'guterman\', \'hydro-quebec\']\n    """"""\n    download_files_maybe_extract(urls=urls, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        full_path = os.path.join(directory, filename)\n        text = []\n        with io.open(full_path, encoding=\'utf-8\') as f:\n            for line in f:\n                text.extend(line.replace(\'<unk>\', unknown_token).split())\n                text.append(eos_token)\n        ret.append(text)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/reverse.py,0,"b'import random\n\n\ndef reverse_dataset(train=False,\n                    dev=False,\n                    test=False,\n                    train_rows=10000,\n                    dev_rows=1000,\n                    test_rows=1000,\n                    seq_max_length=10):\n    """"""\n    Load the Reverse dataset.\n\n    The Reverse dataset is a simple task of reversing a list of numbers. This dataset is useful\n    for testing implementations of sequence to sequence models.\n\n    Args:\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_rows (int, optional): Number of training rows to generate.\n        dev_rows (int, optional): Number of development rows to generate.\n        test_rows (int, optional): Number of test rows to generate.\n        seq_max_length (int, optional): Maximum sequence length.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.random import set_seed\n        >>> set_seed(321)\n        >>>\n        >>> from torchnlp.datasets import reverse_dataset\n        >>> train = reverse_dataset(train=True)\n        >>> train[0:1]\n        [{\'source\': \'6 2 5 8 7\', \'target\': \'7 8 5 2 6\'}]\n    """"""\n    ret = []\n    for is_requested, n_rows in [(train, train_rows), (dev, dev_rows), (test, test_rows)]:\n        rows = []\n        for i in range(n_rows):\n            length = random.randint(1, seq_max_length)\n            seq = []\n            for _ in range(length):\n                seq.append(str(random.randint(0, 9)))\n            input_ = \' \'.join(seq)\n            output = \' \'.join(reversed(seq))\n            rows.append({\'source\': input_, \'target\': output})\n\n        # NOTE: Given that `random.randint` is deterministic with the same `random_seed` we need\n        # to allow the random generator to create the train, dev and test dataset in order.\n        # Otherwise, `reverse(train=True)` and `reverse(test=True)` would share the first 1000 rows.\n        if not is_requested:\n            continue\n\n        ret.append(rows)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/simple_qa.py,0,"b'import os\n\nfrom torchnlp._third_party.lazy_loader import LazyLoader\n\npd = LazyLoader(\'pd\', globals(), \'pandas\')\n\nfrom torchnlp.download import download_file_maybe_extract\n\n\ndef simple_qa_dataset(\n    directory=\'data/\',\n    train=False,\n    dev=False,\n    test=False,\n    extracted_name=\'SimpleQuestions_v2\',\n    train_filename=\'annotated_fb_data_train.txt\',\n    dev_filename=\'annotated_fb_data_valid.txt\',\n    test_filename=\'annotated_fb_data_test.txt\',\n    check_files=[\'SimpleQuestions_v2/annotated_fb_data_train.txt\'],\n    url=\'https://www.dropbox.com/s/tohrsllcfy7rch4/SimpleQuestions_v2.tgz?raw=1\',\n):  # pragma: no cover\n    """"""\n    Load the SimpleQuestions dataset.\n\n    Single-relation factoid questions (simple questions) are common in many settings\n    (e.g. Microsoft\xe2\x80\x99s search query logs and WikiAnswers questions). The SimpleQuestions dataset is\n    one of the most commonly used benchmarks for studying single-relation factoid questions.\n\n    **Reference:**\n    https://research.fb.com/publications/large-scale-simple-question-answering-with-memory-networks/\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        extracted_name (str, optional): Name of the extracted dataset directory.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the development split.\n        test_filename (str, optional): The filename of the test split.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset `tar.gz` file.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import simple_qa_dataset  # doctest: +SKIP\n        >>> train = simple_qa_dataset(train=True)  # doctest: +SKIP\n        SimpleQuestions_v2.tgz:  15%|\xe2\x96\x8f| 62.3M/423M [00:09<00:41, 8.76MB/s]\n        >>> train[0:2]  # doctest: +SKIP\n        [{\n          \'question\': \'what is the book e about\',\n          \'relation\': \'www.freebase.com/book/written_work/subjects\',\n          \'object\': \'www.freebase.com/m/01cj3p\',\n          \'subject\': \'www.freebase.com/m/04whkz5\'\n        }, {\n          \'question\': \'to what release does the release track cardiac arrest come from\',\n          \'relation\': \'www.freebase.com/music/release_track/release\',\n          \'object\': \'www.freebase.com/m/0sjc7c1\',\n          \'subject\': \'www.freebase.com/m/0tp2p24\'\n        }]\n    """"""\n    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        full_path = os.path.join(directory, extracted_name, filename)\n        data = pd.read_csv(\n            full_path, header=None, sep=\'\\t\', names=[\'subject\', \'relation\', \'object\', \'question\'])\n        ret.append([{\n            \'question\': row[\'question\'],\n            \'relation\': row[\'relation\'],\n            \'object\': row[\'object\'],\n            \'subject\': row[\'subject\'],\n        } for _, row in data.iterrows()])\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/smt.py,0,"b'import os\nimport io\n\nfrom torchnlp.download import download_file_maybe_extract\n\n\ndef get_label_str(label, fine_grained=False):\n    pre = \'very \' if fine_grained else \'\'\n    return {\n        \'0\': pre + \'negative\',\n        \'1\': \'negative\',\n        \'2\': \'neutral\',\n        \'3\': \'positive\',\n        \'4\': pre + \'positive\',\n        None: None\n    }[label]\n\n\ndef parse_tree(data, subtrees=False, fine_grained=False):\n    # https://github.com/pytorch/text/blob/6476392a801f51794c90378dd23489578896c6f2/torchtext/data/example.py#L56\n    try:\n        from nltk.tree import Tree\n    except ImportError:\n        print(""Please install NLTK. "" ""See the docs at http://nltk.org for more information."")\n        raise\n    tree = Tree.fromstring(data)\n\n    if subtrees:\n        return [{\n            \'text\': \' \'.join(t.leaves()),\n            \'label\': get_label_str(t.label(), fine_grained=fine_grained)\n        } for t in tree.subtrees()]\n\n    return {\n        \'text\': \' \'.join(tree.leaves()),\n        \'label\': get_label_str(tree.label(), fine_grained=fine_grained)\n    }\n\n\ndef smt_dataset(directory=\'data/\',\n                train=False,\n                dev=False,\n                test=False,\n                train_filename=\'train.txt\',\n                dev_filename=\'dev.txt\',\n                test_filename=\'test.txt\',\n                extracted_name=\'trees\',\n                check_files=[\'trees/train.txt\'],\n                url=\'http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\',\n                fine_grained=False,\n                subtrees=False):\n    """"""\n    Load the Stanford Sentiment Treebank dataset.\n\n    Semantic word spaces have been very useful but cannot express the meaning of longer phrases in\n    a principled way. Further progress towards understanding compositionality in tasks such as\n    sentiment detection requires richer supervised training and evaluation resources and more\n    powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes\n    fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and\n    presents new challenges for sentiment compositionality.\n\n    **Reference**:\n    https://nlp.stanford.edu/sentiment/index.html\n\n    **Citation:**\n    Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning,\n    Andrew Y. Ng and Christopher Potts. Recursive Deep Models for Semantic Compositionality Over a\n    Sentiment Treebank\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the development split.\n        test_filename (str, optional): The filename of the test split.\n        extracted_name (str, optional): Name of the extracted dataset directory.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset `tar.gz` file.\n        subtrees (bool, optional): Whether to include sentiment-tagged subphrases in addition to\n            complete examples.\n        fine_grained (bool, optional): Whether to use 5-class instead of 3-class labeling.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import smt_dataset  # doctest: +SKIP\n        >>> train = smt_dataset(train=True)  # doctest: +SKIP\n        >>> train[5]  # doctest: +SKIP\n        {\n          \'text\': ""Whether or not you \'re enlightened by any of Derrida \'s lectures on ..."",\n          \'label\': \'positive\'\n        }\n    """"""\n    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        full_path = os.path.join(directory, extracted_name, filename)\n        examples = []\n        with io.open(full_path, encoding=\'utf-8\') as f:\n            for line in f:\n                line = line.strip()\n                if subtrees:\n                    examples.extend(parse_tree(line, subtrees=subtrees, fine_grained=fine_grained))\n                else:\n                    examples.append(parse_tree(line, subtrees=subtrees, fine_grained=fine_grained))\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/snli.py,0,"b'import os\nimport io\n\nimport json\n\nfrom torchnlp.download import download_file_maybe_extract\n\n\ndef snli_dataset(directory=\'data/\',\n                 train=False,\n                 dev=False,\n                 test=False,\n                 train_filename=\'snli_1.0_train.jsonl\',\n                 dev_filename=\'snli_1.0_dev.jsonl\',\n                 test_filename=\'snli_1.0_test.jsonl\',\n                 extracted_name=\'snli_1.0\',\n                 check_files=[\'snli_1.0/snli_1.0_train.jsonl\'],\n                 url=\'http://nlp.stanford.edu/projects/snli/snli_1.0.zip\'):\n    """"""\n    Load the Stanford Natural Language Inference (SNLI) dataset.\n\n    The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs\n    manually labeled for balanced classification with the labels entailment, contradiction, and\n    neutral, supporting the task of natural language inference (NLI), also known as recognizing\n    textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating\n    representational systems for text, especially including those induced by representation\n    learning methods, as well as a resource for developing NLP models of any kind.\n\n    **Reference:** https://nlp.stanford.edu/projects/snli/\n\n    **Citation:**\n    Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large\n    annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference\n    on Empirical Methods in Natural Language Processing (EMNLP).\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the development split.\n        test_filename (str, optional): The filename of the test split.\n        extracted_name (str, optional): Name of the extracted dataset directory.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset `tar.gz` file.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import snli_dataset  # doctest: +SKIP\n        >>> train = snli_dataset(train=True)  # doctest: +SKIP\n        >>> train[0]  # doctest: +SKIP\n        {\n          \'premise\': \'Kids are on a amusement ride.\',\n          \'hypothesis\': \'A car is broke down on the side of the road.\',\n          \'label\': \'contradiction\',\n          \'premise_transitions\': [\'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', ...],\n          \'hypothesis_transitions\': [\'shift\', \'shift\', \'shift\', \'shift\', \'shift\', \'shift\', ...],\n        }\n    """"""\n    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n\n    get_transitions = lambda parse: [\'reduce\' if t == \')\' else \'shift\' for t in parse if t != \'(\']\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        full_path = os.path.join(directory, extracted_name, filename)\n        examples = []\n        with io.open(full_path, encoding=\'utf-8\') as f:\n            for line in f:\n                line = line.strip()\n                line = json.loads(line)\n                examples.append({\n                    \'premise\': line[\'sentence1\'],\n                    \'hypothesis\': line[\'sentence2\'],\n                    \'label\': line[\'gold_label\'],\n                    \'premise_transitions\': get_transitions(line[\'sentence1_binary_parse\']),\n                    \'hypothesis_transitions\': get_transitions(line[\'sentence2_binary_parse\'])\n                })\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/trec.py,0,"b'import os\n\nfrom torchnlp.download import download_files_maybe_extract\n\n\ndef trec_dataset(directory=\'data/trec/\',\n                 train=False,\n                 test=False,\n                 train_filename=\'train_5500.label\',\n                 test_filename=\'TREC_10.label\',\n                 check_files=[\'train_5500.label\'],\n                 urls=[\n                     \'http://cogcomp.org/Data/QA/QC/train_5500.label\',\n                     \'http://cogcomp.org/Data/QA/QC/TREC_10.label\'\n                 ],\n                 fine_grained=False):\n    """"""\n    Load the Text REtrieval Conference (TREC) Question Classification dataset.\n\n    TREC dataset contains 5500 labeled questions in training set and another 500 for test set. The\n    dataset has 6 labels, 50 level-2 labels. Average length of each sentence is 10, vocabulary size\n    of 8700.\n\n    References:\n        * https://nlp.stanford.edu/courses/cs224n/2004/may-steinberg-project.pdf\n        * http://cogcomp.org/Data/QA/QC/\n        * http://www.aclweb.org/anthology/C02-1150\n\n    **Citation:**\n    Xin Li, Dan Roth, Learning Question Classifiers. COLING\'02, Aug., 2002.\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_filename (str, optional): The filename of the training split.\n        test_filename (str, optional): The filename of the test split.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        urls (str, optional): URLs to download.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import trec_dataset  # doctest: +SKIP\n        >>> train = trec_dataset(train=True)  # doctest: +SKIP\n        >>> train[:2]  # doctest: +SKIP\n        [{\n          \'label\': \'DESC\',\n          \'text\': \'How did serfdom develop in and then leave Russia ?\'\n        }, {\n          \'label\': \'ENTY\',\n          \'text\': \'What films featured the character Popeye Doyle ?\'\n        }]\n    """"""\n    download_files_maybe_extract(urls=urls, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [(train, train_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        full_path = os.path.join(directory, filename)\n        examples = []\n        for line in open(full_path, \'rb\'):\n            # there is one non-ASCII byte: sisterBADBYTEcity; replaced with space\n            label, _, text = line.replace(b\'\\xf0\', b\' \').strip().decode().partition(\' \')\n            label, _, label_fine = label.partition(\':\')\n            if fine_grained:\n                examples.append({\'label\': label_fine, \'text\': text})\n            else:\n                examples.append({\'label\': label, \'text\': text})\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/ud_pos.py,0,"b'import os\nimport io\n\nfrom torchnlp.download import download_file_maybe_extract\n\n\ndef ud_pos_dataset(directory=\'data/\',\n                   train=False,\n                   dev=False,\n                   test=False,\n                   train_filename=\'en-ud-tag.v2.train.txt\',\n                   dev_filename=\'en-ud-tag.v2.dev.txt\',\n                   test_filename=\'en-ud-tag.v2.test.txt\',\n                   extracted_name=\'en-ud-v2\',\n                   check_files=[\'en-ud-v2/en-ud-tag.v2.train.txt\'],\n                   url=\'https://bitbucket.org/sivareddyg/public/downloads/en-ud-v2.zip\'):\n    """"""\n    Load the Universal Dependencies - English Dependency Treebank dataset.\n\n    Corpus of sentences annotated using Universal Dependencies annotation. The corpus comprises\n    254,830 words and 16,622 sentences, taken from various web media including weblogs, newsgroups,\n    emails, reviews, and Yahoo! answers.\n\n    References:\n        * http://universaldependencies.org/\n        * https://github.com/UniversalDependencies/UD_English\n\n    **Citation:**\n    Natalia Silveira and Timothy Dozat and Marie-Catherine de Marneffe and Samuel Bowman and\n    Miriam Connor and John Bauer and Christopher D. Manning (2014).\n    A Gold Standard Dependency Corpus for {E}nglish\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the development split.\n        test_filename (str, optional): The filename of the test split.\n        extracted_name (str, optional): Name of the extracted dataset directory.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset `tar.gz` file.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import ud_pos_dataset  # doctest: +SKIP\n        >>> train = ud_pos_dataset(train=True)  # doctest: +SKIP\n        >>> train[17]  # doctest: +SKIP\n        {\n          \'tokens\': [\'Guerrillas\', \'killed\', \'an\', \'engineer\', \',\', \'Asi\', \'Ali\', \',\', \'from\',\n                     \'Tikrit\', \'.\'],\n          \'ud_tags\': [\'NOUN\', \'VERB\', \'DET\', \'NOUN\', \'PUNCT\', \'PROPN\', \'PROPN\', \'PUNCT\', \'ADP\',\n                      \'PROPN\', \'PUNCT\'],\n          \'ptb_tags\': [\'NNS\', \'VBD\', \'DT\', \'NN\', \',\', \'NNP\', \'NNP\', \',\', \'IN\', \'NNP\', \'.\']\n        }\n    """"""\n    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        full_path = os.path.join(directory, extracted_name, filename)\n        examples = []\n        with io.open(full_path, encoding=\'utf-8\') as f:\n            sentence = {\'tokens\': [], \'ud_tags\': [], \'ptb_tags\': []}\n            for line in f:\n                line = line.strip()\n                if line == \'\' and len(sentence[\'tokens\']) > 0:\n                    examples.append(sentence)\n                    sentence = {\'tokens\': [], \'ud_tags\': [], \'ptb_tags\': []}\n                elif line != \'\':\n                    token, ud_tag, ptb_tag = tuple(line.split(\'\\t\'))\n                    sentence[\'tokens\'].append(token)\n                    sentence[\'ud_tags\'].append(ud_tag)\n                    sentence[\'ptb_tags\'].append(ptb_tag)\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/wikitext_2.py,0,"b'import os\nimport io\n\nfrom torchnlp.download import download_file_maybe_extract\nfrom torchnlp.encoders.text import DEFAULT_EOS_TOKEN\nfrom torchnlp.encoders.text import DEFAULT_UNKNOWN_TOKEN\n\n\ndef wikitext_2_dataset(\n        directory=\'data/\',\n        train=False,\n        dev=False,\n        test=False,\n        train_filename=\'wiki.train.tokens\',\n        dev_filename=\'wiki.valid.tokens\',\n        test_filename=\'wiki.test.tokens\',\n        extracted_name=\'wikitext-2\',\n        check_files=[\'wikitext-2/wiki.train.tokens\'],\n        url=\'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\',\n        unknown_token=DEFAULT_UNKNOWN_TOKEN,\n        eos_token=DEFAULT_EOS_TOKEN):\n    """"""\n    Load the WikiText-2 dataset.\n\n    The WikiText language modeling dataset is a collection of over 100 million tokens extracted\n    from the set of verified Good and Featured articles on Wikipedia. The dataset is available\n    under the Creative Commons Attribution-ShareAlike License.\n\n    **Reference:**\n    https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the development split.\n        test_filename (str, optional): The filename of the test split.\n        extracted_name (str, optional): Name of the extracted dataset directory.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset `tar.gz` file.\n        unknown_token (str, optional): Token to use for unknown words.\n        eos_token (str, optional): Token to use at the end of sentences.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import wikitext_2_dataset  # doctest: +SKIP\n        >>> train = wikitext_2_dataset(train=True)  # doctest: +SKIP\n        >>> train[:10]  # doctest: +SKIP\n        [\'</s>\', \'=\', \'Valkyria\', \'Chronicles\', \'III\', \'=\', \'</s>\', \'</s>\', \'Senj\xc5\x8d\', \'no\']\n    """"""\n    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n    for filename in splits:\n        full_path = os.path.join(directory, extracted_name, filename)\n        text = []\n        with io.open(full_path, encoding=\'utf-8\') as f:\n            for line in f:\n                text.extend(line.replace(\'<unk>\', unknown_token).split())\n                text.append(eos_token)\n        ret.append(text)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/wmt.py,0,"b'import os\n\nfrom torchnlp.download import download_file_maybe_extract\n\n\ndef wmt_dataset(directory=\'data/wmt16_en_de\',\n                train=False,\n                dev=False,\n                test=False,\n                train_filename=\'train.tok.clean.bpe.32000\',\n                dev_filename=\'newstest2013.tok.bpe.32000\',\n                test_filename=\'newstest2014.tok.bpe.32000\',\n                check_files=[\'train.tok.clean.bpe.32000.en\'],\n                url=\'https://drive.google.com/uc?export=download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8\'):\n    """"""\n    The Workshop on Machine Translation (WMT) 2014 English-German dataset.\n\n    Initially this dataset was preprocessed by Google Brain. Though this download contains test sets\n    from 2015 and 2016, the train set differs slightly from WMT 2015 and 2016 and significantly from\n    WMT 2017.\n\n    The provided data is mainly taken from version 7 of the Europarl corpus, which is freely\n    available. Note that this the same data as last year, since Europarl is not anymore translted\n    across all 23 official European languages. Additional training data is taken from the new News\n    Commentary corpus. There are about 50 million words of training data per language from the\n    Europarl corpus and 3 million words from the News Commentary corpus.\n\n    A new data resource from 2013 is the Common Crawl corpus which was collected from web sources.\n    Each parallel corpus comes with a annotation file that gives the source of each sentence pair.\n\n    References:\n        * https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/translate_ende.py # noqa: E501\n        * http://www.statmt.org/wmt14/translation-task.html\n\n    Args:\n        directory (str, optional): Directory to cache the dataset.\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the dev split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_filename (str, optional): The filename of the training split.\n        dev_filename (str, optional): The filename of the dev split.\n        test_filename (str, optional): The filename of the test split.\n        check_files (str, optional): Check if these files exist, then this download was successful.\n        url (str, optional): URL of the dataset `tar.gz` file.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import wmt_dataset  # doctest: +SKIP\n        >>> train = wmt_dataset(train=True)  # doctest: +SKIP\n        >>> train[:2]  # doctest: +SKIP\n        [{\n          \'en\': \'Res@@ um@@ ption of the session\',\n          \'de\': \'Wiederaufnahme der Sitzungsperiode\'\n        }, {\n          \'en\': \'I declare resumed the session of the European Parliament ad@@ jour@@ ned on...\'\n          \'de\': \'Ich erkl\xc3\xa4r@@ e die am Freitag , dem 17. Dezember unterbro@@ ch@@ ene...\'\n        }]\n    """"""\n    download_file_maybe_extract(\n        url=url, directory=directory, check_files=check_files, filename=\'wmt16_en_de.tar.gz\')\n\n    ret = []\n    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n    splits = [f for (requested, f) in splits if requested]\n\n    for filename in splits:\n        examples = []\n\n        en_path = os.path.join(directory, filename + \'.en\')\n        de_path = os.path.join(directory, filename + \'.de\')\n        en_file = [l.strip() for l in open(en_path, \'r\', encoding=\'utf-8\')]\n        de_file = [l.strip() for l in open(de_path, \'r\', encoding=\'utf-8\')]\n        assert len(en_file) == len(de_file)\n        for i in range(len(en_file)):\n            if en_file[i] != \'\' and de_file[i] != \'\':\n                examples.append({\'en\': en_file[i], \'de\': de_file[i]})\n\n        ret.append(examples)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/datasets/zero.py,0,"b'def zero_dataset(train=False, dev=False, test=False, train_rows=256, dev_rows=64, test_rows=64):\n    """"""\n    Load the Zero dataset.\n\n    The Zero dataset is a simple task of predicting zero from zero. This dataset is useful for\n    integration testing. The extreme simplicity of the dataset allows for models to learn the task\n    quickly allowing for quick end-to-end testing.\n\n    Args:\n        train (bool, optional): If to load the training split of the dataset.\n        dev (bool, optional): If to load the development split of the dataset.\n        test (bool, optional): If to load the test split of the dataset.\n        train_rows (int, optional): Number of training rows to generate.\n        dev_rows (int, optional): Number of development rows to generate.\n        test_rows (int, optional): Number of test rows to generate.\n\n    Returns:\n        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n        Returns between one and all dataset splits (train, dev and test) depending on if their\n        respective boolean argument is ``True``.\n\n    Example:\n        >>> from torchnlp.datasets import zero_dataset\n        >>> train = zero_dataset(train=True)\n        >>> train[0:2]\n        [{\'source\': \'0\', \'target\': \'0\'}, {\'source\': \'0\', \'target\': \'0\'}]\n    """"""\n    ret = []\n    for is_requested, n_rows in [(train, train_rows), (dev, dev_rows), (test, test_rows)]:\n        if not is_requested:\n            continue\n        rows = [{\'source\': str(0), \'target\': str(0)} for i in range(n_rows)]\n        ret.append(rows)\n\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return tuple(ret)\n'"
torchnlp/encoders/__init__.py,0,"b""from torchnlp.encoders.encoder import Encoder\nfrom torchnlp.encoders.label_encoder import LabelEncoder\n\n__all__ = [\n    'Encoder',\n    'LabelEncoder',\n]\n"""
torchnlp/encoders/encoder.py,0,"b'class Encoder(object):\n    """"""\n    Base class for a encoder employing an identity function.\n\n    Args:\n        enforce_reversible (bool, optional): Check for reversibility on ``Encoder.encode`` and\n          ``Encoder.decode``. Formally, reversible means:\n          ``Encoder.decode(Encoder.encode(object_)) == object_``.\n    """"""\n\n    def __init__(self, enforce_reversible=False):\n        self.enforce_reversible = enforce_reversible\n\n    def encode(self, object_):\n        """""" Encodes an object.\n\n        Args:\n            object_ (object): Object to encode.\n\n        Returns:\n            object: Encoding of the object.\n        """"""\n        if self.enforce_reversible:\n            self.enforce_reversible = False\n            encoded_decoded = self.decode(self.encode(object_))\n            self.enforce_reversible = True\n            if encoded_decoded != object_:\n                raise ValueError(\'Encoding is not reversible for ""%s""\' % object_)\n\n        return object_\n\n    def batch_encode(self, iterator, *args, **kwargs):\n        """"""\n        Args:\n            batch (list): Batch of objects to encode.\n            *args: Arguments passed to ``encode``.\n            **kwargs: Keyword arguments passed to ``encode``.\n\n        Returns:\n            list: Batch of encoded objects.\n        """"""\n        return [self.encode(object_, *args, **kwargs) for object_ in iterator]\n\n    def decode(self, encoded):\n        """""" Decodes an object.\n\n        Args:\n            object_ (object): Encoded object.\n\n        Returns:\n            object: Object decoded.\n        """"""\n        if self.enforce_reversible:\n            self.enforce_reversible = False\n            decoded_encoded = self.encode(self.decode(encoded))\n            self.enforce_reversible = True\n            if decoded_encoded != encoded:\n                raise ValueError(\'Decoding is not reversible for ""%s""\' % encoded)\n\n        return encoded\n\n    def batch_decode(self, iterator, *args, **kwargs):\n        """"""\n        Args:\n            iterator (list): Batch of encoded objects.\n            *args: Arguments passed to ``decode``.\n            **kwargs: Keyword arguments passed to ``decode``.\n\n        Returns:\n            list: Batch of decoded objects.\n        """"""\n        return [self.decode(encoded, *args, **kwargs) for encoded in iterator]\n'"
torchnlp/encoders/label_encoder.py,6,"b'from collections import Counter\n\nfrom torchnlp.encoders.encoder import Encoder\n\nimport torch\n\nDEFAULT_UNKNOWN_TOKEN = \'<unk>\'\nDEFAULT_RESERVED = [DEFAULT_UNKNOWN_TOKEN]\n\n\nclass LabelEncoder(Encoder):\n    """""" Encodes an label via a dictionary.\n\n    Args:\n        sample (list of strings): Sample of data used to build encoding dictionary.\n        min_occurrences (int, optional): Minimum number of occurrences for a label to be added to\n          the encoding dictionary.\n        reserved_labels (list, optional): List of reserved labels inserted in the beginning of the\n          dictionary.\n        unknown_index (int, optional): The unknown label is used to encode unseen labels. This is\n          the index that label resides at.\n        **kwargs: Keyword arguments passed onto ``Encoder``.\n\n    Example:\n\n        >>> samples = [\'label_a\', \'label_b\']\n        >>> encoder = LabelEncoder(samples, reserved_labels=[\'unknown\'], unknown_index=0)\n        >>> encoder.encode(\'label_a\')\n        tensor(1)\n        >>> encoder.decode(encoder.encode(\'label_a\'))\n        \'label_a\'\n        >>> encoder.encode(\'label_c\')\n        tensor(0)\n        >>> encoder.decode(encoder.encode(\'label_c\'))\n        \'unknown\'\n        >>> encoder.vocab\n        [\'unknown\', \'label_a\', \'label_b\']\n    """"""\n\n    def __init__(self,\n                 sample,\n                 min_occurrences=1,\n                 reserved_labels=DEFAULT_RESERVED,\n                 unknown_index=DEFAULT_RESERVED.index(DEFAULT_UNKNOWN_TOKEN),\n                 **kwargs):\n        super().__init__(**kwargs)\n\n        if unknown_index and unknown_index >= len(reserved_labels):\n            raise ValueError(\'The `unknown_index` if provided must be also `reserved`.\')\n\n        self.unknown_index = unknown_index\n        self.tokens = Counter(sample)\n        self.index_to_token = reserved_labels.copy()\n        self.token_to_index = {token: index for index, token in enumerate(reserved_labels)}\n        for token, count in self.tokens.items():\n            if count >= min_occurrences:\n                self.index_to_token.append(token)\n                self.token_to_index[token] = len(self.index_to_token) - 1\n\n    @property\n    def vocab(self):\n        """"""\n        Returns:\n            list: List of labels in the dictionary.\n        """"""\n        return self.index_to_token\n\n    @property\n    def vocab_size(self):\n        """"""\n        Returns:\n            int: Number of labels in the dictionary.\n        """"""\n        return len(self.vocab)\n\n    def encode(self, label):\n        """""" Encodes a ``label``.\n\n        Args:\n            label (object): Label to encode.\n\n        Returns:\n            torch.Tensor: Encoding of the label.\n        """"""\n        label = super().encode(label)\n\n        return torch.tensor(self.token_to_index.get(label, self.unknown_index), dtype=torch.long)\n\n    def batch_encode(self, iterator, *args, dim=0, **kwargs):\n        """"""\n        Args:\n            iterator (iterator): Batch of labels to encode.\n            *args: Arguments passed to ``Encoder.batch_encode``.\n            dim (int, optional): Dimension along which to concatenate tensors.\n            **kwargs: Keyword arguments passed to ``Encoder.batch_encode``.\n\n        Returns:\n            torch.Tensor: Tensor of encoded labels.\n        """"""\n        return torch.stack(super().batch_encode(iterator, *args, **kwargs), dim=dim)\n\n    def decode(self, encoded):\n        """""" Decodes ``encoded`` label.\n\n        Args:\n            encoded (torch.Tensor): Encoded label.\n\n        Returns:\n            object: Label decoded from ``encoded``.\n        """"""\n        encoded = super().decode(encoded)\n\n        if encoded.numel() > 1:\n            raise ValueError(\n                \'``decode`` decodes one label at a time, use ``batch_decode`` instead.\')\n\n        return self.index_to_token[encoded.squeeze().item()]\n\n    def batch_decode(self, tensor, *args, dim=0, **kwargs):\n        """"""\n        Args:\n            tensor (torch.Tensor): Batch of tensors.\n            *args: Arguments passed to ``Encoder.batch_decode``.\n            dim (int, optional): Dimension along which to split tensors.\n            **kwargs: Keyword arguments passed to ``Encoder.batch_decode``.\n\n        Returns:\n            list: Batch of decoded labels.\n        """"""\n        return super().batch_decode([t.squeeze(0) for t in tensor.split(1, dim=dim)])\n'"
torchnlp/metrics/__init__.py,0,"b""from torchnlp.metrics.accuracy import get_accuracy\nfrom torchnlp.metrics.accuracy import get_token_accuracy\nfrom torchnlp.metrics.bleu import get_moses_multi_bleu\n\n# TODO: Use `sklearn.metrics` for a `confusion_matrix` implemented with ignore_index\n# TODO: Use `sklearn.metrics` for a `recall` implemented with ignore_index\n# TODO: Use `sklearn.metrics` for a `precision` implemented with ignore_index\n# TODO: Use `sklearn.metrics` for a `f1` implemented with ignore_index\n# TODO: Implement perplexity\n# TODO: Implement rogue metric\n\n__all__ = ['get_accuracy', 'get_token_accuracy', 'get_moses_multi_bleu']\n"""
torchnlp/metrics/accuracy.py,17,"b'import torch\n\nfrom torchnlp.utils import torch_equals_ignore_index\n\nis_scalar = lambda t: torch.is_tensor(t) and len(t.size()) == 0\n\n\ndef get_accuracy(targets, outputs, k=1, ignore_index=None):\n    """""" Get the accuracy top-k accuracy between two tensors.\n\n    Args:\n      targets (1 - 2D :class:`torch.Tensor`): Target or true vector against which to measure\n          saccuracy\n      outputs (1 - 3D :class:`torch.Tensor`): Prediction or output vector\n      ignore_index (int, optional): Specifies a target index that is ignored\n\n    Returns:\n      :class:`tuple` consisting of accuracy (:class:`float`), number correct (:class:`int`) and\n      total (:class:`int`)\n\n    Example:\n\n        >>> import torch\n        >>> from torchnlp.metrics import get_accuracy\n        >>> targets = torch.LongTensor([1, 2, 3, 4, 5])\n        >>> outputs = torch.LongTensor([1, 2, 2, 3, 5])\n        >>> accuracy, n_correct, n_total = get_accuracy(targets, outputs, ignore_index=3)\n        >>> accuracy\n        0.8\n        >>> n_correct\n        4\n        >>> n_total\n        5\n    """"""\n    n_correct = 0.0\n    for target, output in zip(targets, outputs):\n        if not torch.is_tensor(target) or is_scalar(target):\n            target = torch.LongTensor([target])\n\n        if not torch.is_tensor(output) or is_scalar(output):\n            output = torch.LongTensor([[output]])\n\n        predictions = output.topk(k=min(k, len(output)), dim=0)[0]\n        for prediction in predictions:\n            if torch_equals_ignore_index(\n                    target.squeeze(), prediction.squeeze(), ignore_index=ignore_index):\n                n_correct += 1\n                break\n\n    return n_correct / len(targets), int(n_correct), len(targets)\n\n\ndef get_token_accuracy(targets, outputs, ignore_index=None):\n    """""" Get the accuracy token accuracy between two tensors.\n\n    Args:\n      targets (1 - 2D :class:`torch.Tensor`): Target or true vector against which to measure\n          saccuracy\n      outputs (1 - 3D :class:`torch.Tensor`): Prediction or output vector\n      ignore_index (int, optional): Specifies a target index that is ignored\n\n    Returns:\n      :class:`tuple` consisting of accuracy (:class:`float`), number correct (:class:`int`) and\n      total (:class:`int`)\n\n    Example:\n\n        >>> import torch\n        >>> from torchnlp.metrics import get_token_accuracy\n        >>> targets = torch.LongTensor([[1, 1], [2, 2], [3, 3]])\n        >>> outputs = torch.LongTensor([[1, 1], [2, 3], [4, 4]])\n        >>> accuracy, n_correct, n_total = get_token_accuracy(targets, outputs, ignore_index=3)\n        >>> accuracy\n        0.75\n        >>> n_correct\n        3.0\n        >>> n_total\n        4.0\n     """"""\n    n_correct = 0.0\n    n_total = 0.0\n    for target, output in zip(targets, outputs):\n        if not torch.is_tensor(target) or is_scalar(target):\n            target = torch.LongTensor([target])\n\n        if not torch.is_tensor(output) or is_scalar(output):\n            output = torch.LongTensor([[output]])\n\n        if len(target.size()) != len(output.size()):\n            prediction = output.max(dim=0)[0].view(-1)\n        else:\n            prediction = output\n\n        if ignore_index is not None:\n            mask = target.ne(ignore_index)\n            n_correct += prediction.eq(target).masked_select(mask).sum().item()\n            n_total += mask.sum().item()\n        else:\n            n_total += len(target)\n            n_correct += prediction.eq(target).sum().item()\n\n    return n_correct / n_total, n_correct, n_total\n'"
torchnlp/metrics/bleu.py,0,"b'# coding=utf-8\n# Copyright 2017 The Tensor2Tensor Authors.\n# Copyright 2017 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\nimport subprocess\nimport tempfile\nimport logging\n\nfrom torchnlp._third_party.lazy_loader import LazyLoader\n\nimport numpy as np\n\nsix = LazyLoader(\'six\', globals(), \'six\')\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_moses_multi_bleu(hypotheses, references, lowercase=False):\n    """"""Get the BLEU score using the moses `multi-bleu.perl` script.\n\n    **Script:**\n    https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl\n\n    Args:\n      hypotheses (list of str): List of predicted values\n      references (list of str): List of target values\n      lowercase (bool): If true, pass the ""-lc"" flag to the `multi-bleu.perl` script\n\n    Returns:\n      (:class:`np.float32`) The BLEU score as a float32 value.\n\n    Example:\n\n      >>> hypotheses = [\n      ...   ""The brown fox jumps over the dog \xe7\xac\x91"",\n      ...   ""The brown fox jumps over the dog 2 \xe7\xac\x91""\n      ... ]\n      >>> references = [\n      ...   ""The quick brown fox jumps over the lazy dog \xe7\xac\x91"",\n      ...   ""The quick brown fox jumps over the lazy dog \xe7\xac\x91""\n      ... ]\n      >>> get_moses_multi_bleu(hypotheses, references, lowercase=True)\n      46.51\n    """"""\n    if isinstance(hypotheses, list):\n        hypotheses = np.array(hypotheses)\n    if isinstance(references, list):\n        references = np.array(references)\n\n    if np.size(hypotheses) == 0:\n        return np.float32(0.0)\n\n    # Get MOSES multi-bleu script\n    try:\n        multi_bleu_path, _ = six.moves.urllib.request.urlretrieve(\n            ""https://raw.githubusercontent.com/moses-smt/mosesdecoder/""\n            ""master/scripts/generic/multi-bleu.perl"")\n        os.chmod(multi_bleu_path, 0o755)\n    except:\n        logger.warning(""Unable to fetch multi-bleu.perl script"")\n        return None\n\n    # Dump hypotheses and references to tempfiles\n    hypothesis_file = tempfile.NamedTemporaryFile()\n    hypothesis_file.write(""\\n"".join(hypotheses).encode(""utf-8""))\n    hypothesis_file.write(b""\\n"")\n    hypothesis_file.flush()\n    reference_file = tempfile.NamedTemporaryFile()\n    reference_file.write(""\\n"".join(references).encode(""utf-8""))\n    reference_file.write(b""\\n"")\n    reference_file.flush()\n\n    # Calculate BLEU using multi-bleu script\n    with open(hypothesis_file.name, ""r"") as read_pred:\n        bleu_cmd = [multi_bleu_path]\n        if lowercase:\n            bleu_cmd += [""-lc""]\n        bleu_cmd += [reference_file.name]\n        try:\n            bleu_out = subprocess.check_output(bleu_cmd, stdin=read_pred, stderr=subprocess.STDOUT)\n            bleu_out = bleu_out.decode(""utf-8"")\n            bleu_score = re.search(r""BLEU = (.+?),"", bleu_out).group(1)\n            bleu_score = float(bleu_score)\n            bleu_score = np.float32(bleu_score)\n        except subprocess.CalledProcessError as error:\n            if error.output is not None:\n                logger.warning(""multi-bleu.perl script returned non-zero exit code"")\n                logger.warning(error.output)\n            bleu_score = None\n\n    # Close temp files\n    hypothesis_file.close()\n    reference_file.close()\n\n    return bleu_score\n'"
torchnlp/nn/__init__.py,0,"b""from torchnlp.nn.attention import Attention\nfrom torchnlp.nn.lock_dropout import LockedDropout\nfrom torchnlp.nn.weight_drop import WeightDropGRU\nfrom torchnlp.nn.weight_drop import WeightDropLSTM\nfrom torchnlp.nn.weight_drop import WeightDropLinear\nfrom torchnlp.nn.weight_drop import WeightDrop\nfrom torchnlp.nn.cnn_encoder import CNNEncoder\n\n__all__ = [\n    'LockedDropout',\n    'Attention',\n    'CNNEncoder',\n    'WeightDrop',\n    'WeightDropGRU',\n    'WeightDropLSTM',\n    'WeightDropLinear',\n]\n"""
torchnlp/nn/attention.py,12,"b'import torch\nimport torch.nn as nn\n\n\nclass Attention(nn.Module):\n    """""" Applies attention mechanism on the `context` using the `query`.\n\n    **Thank you** to IBM for their initial implementation of :class:`Attention`. Here is\n    their `License\n    <https://github.com/IBM/pytorch-seq2seq/blob/master/LICENSE>`__.\n\n    Args:\n        dimensions (int): Dimensionality of the query and context.\n        attention_type (str, optional): How to compute the attention score:\n\n            * dot: :math:`score(H_j,q) = H_j^T q`\n            * general: :math:`score(H_j, q) = H_j^T W_a q`\n\n    Example:\n\n         >>> attention = Attention(256)\n         >>> query = torch.randn(5, 1, 256)\n         >>> context = torch.randn(5, 5, 256)\n         >>> output, weights = attention(query, context)\n         >>> output.size()\n         torch.Size([5, 1, 256])\n         >>> weights.size()\n         torch.Size([5, 1, 5])\n    """"""\n\n    def __init__(self, dimensions, attention_type=\'general\'):\n        super(Attention, self).__init__()\n\n        if attention_type not in [\'dot\', \'general\']:\n            raise ValueError(\'Invalid attention type selected.\')\n\n        self.attention_type = attention_type\n        if self.attention_type == \'general\':\n            self.linear_in = nn.Linear(dimensions, dimensions, bias=False)\n\n        self.linear_out = nn.Linear(dimensions * 2, dimensions, bias=False)\n        self.softmax = nn.Softmax(dim=-1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, query, context):\n        """"""\n        Args:\n            query (:class:`torch.FloatTensor` [batch size, output length, dimensions]): Sequence of\n                queries to query the context.\n            context (:class:`torch.FloatTensor` [batch size, query length, dimensions]): Data\n                overwhich to apply the attention mechanism.\n\n        Returns:\n            :class:`tuple` with `output` and `weights`:\n            * **output** (:class:`torch.LongTensor` [batch size, output length, dimensions]):\n              Tensor containing the attended features.\n            * **weights** (:class:`torch.FloatTensor` [batch size, output length, query length]):\n              Tensor containing attention weights.\n        """"""\n        batch_size, output_len, dimensions = query.size()\n        query_len = context.size(1)\n\n        if self.attention_type == ""general"":\n            query = query.reshape(batch_size * output_len, dimensions)\n            query = self.linear_in(query)\n            query = query.reshape(batch_size, output_len, dimensions)\n\n        # TODO: Include mask on PADDING_INDEX?\n\n        # (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->\n        # (batch_size, output_len, query_len)\n        attention_scores = torch.bmm(query, context.transpose(1, 2).contiguous())\n\n        # Compute weights across every context sequence\n        attention_scores = attention_scores.view(batch_size * output_len, query_len)\n        attention_weights = self.softmax(attention_scores)\n        attention_weights = attention_weights.view(batch_size, output_len, query_len)\n\n        # (batch_size, output_len, query_len) * (batch_size, query_len, dimensions) ->\n        # (batch_size, output_len, dimensions)\n        mix = torch.bmm(attention_weights, context)\n\n        # concat -> (batch_size * output_len, 2*dimensions)\n        combined = torch.cat((mix, query), dim=2)\n        combined = combined.view(batch_size * output_len, 2 * dimensions)\n\n        # Apply linear_out on every 2nd dimension of concat\n        # output -> (batch_size, output_len, dimensions)\n        output = self.linear_out(combined).view(batch_size, output_len, dimensions)\n        output = self.tanh(output)\n\n        return output, attention_weights\n'"
torchnlp/nn/cnn_encoder.py,9,"b'""""""\nLICENSE: https://github.com/allenai/allennlp/blob/master/LICENSE\n""""""\n\nimport torch\nfrom torch.nn import Conv1d, Linear, ReLU\n\n\nclass CNNEncoder(torch.nn.Module):\n    """""" A combination of multiple convolution layers and max pooling layers.\n\n    The CNN has one convolution layer for each ngram filter size. Each convolution operation gives\n    out a vector of size num_filters. The number of times a convolution layer will be used\n    is ``num_tokens - ngram_size + 1``. The corresponding maxpooling layer aggregates all these\n    outputs from the convolution layer and outputs the max.\n\n    This operation is repeated for every ngram size passed, and consequently the dimensionality of\n    the output after maxpooling is ``len(ngram_filter_sizes) * num_filters``.  This then gets\n    (optionally) projected down to a lower dimensional output, specified by ``output_dim``.\n\n    We then use a fully connected layer to project in back to the desired output_dim.  For more\n    details, refer to ""A Sensitivity Analysis of (and Practitioners\xe2\x80\x99 Guide to) Convolutional Neural\n    Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.\n\n    **Thank you** to AI2 for their initial implementation of :class:`CNNEncoder`. Here is\n    their `License\n    <https://github.com/allenai/allennlp/blob/master/LICENSE>`__.\n\n    Args:\n        embedding_dim (int): This is the input dimension to the encoder.  We need this because we\n          can\'t do shape inference in pytorch, and we need to know what size filters to construct\n          in the CNN.\n        num_filters (int): This is the output dim for each convolutional layer, which is the number\n          of ""filters"" learned by that layer.\n        ngram_filter_sizes (:class:`tuple` of :class:`int`, optional): This specifies both the\n          number of convolutional layers we will create and their sizes. The default of\n          ``(2, 3, 4, 5)`` will have four convolutional layers, corresponding to encoding ngrams of\n          size 2 to 5 with some number of filters.\n        conv_layer_activation (torch.nn.Module, optional): Activation to use after the convolution\n          layers.\n        output_dim (int or None, optional) : After doing convolutions and pooling, we\'ll project the\n          collected features into a vector of this size.  If this value is ``None``, we will just\n          return the result of the max pooling, giving an output of shape\n          ``len(ngram_filter_sizes) * num_filters``.\n    """"""\n\n    def __init__(self,\n                 embedding_dim,\n                 num_filters,\n                 ngram_filter_sizes=(2, 3, 4, 5),\n                 conv_layer_activation=ReLU(),\n                 output_dim=None):\n        super(CNNEncoder, self).__init__()\n        self._embedding_dim = embedding_dim\n        self._num_filters = num_filters\n        self._ngram_filter_sizes = ngram_filter_sizes\n        self._activation = conv_layer_activation\n        self._output_dim = output_dim\n\n        self._convolution_layers = [\n            Conv1d(\n                in_channels=self._embedding_dim,\n                out_channels=self._num_filters,\n                kernel_size=ngram_size) for ngram_size in self._ngram_filter_sizes\n        ]\n        for i, conv_layer in enumerate(self._convolution_layers):\n            self.add_module(\'conv_layer_%d\' % i, conv_layer)\n\n        maxpool_output_dim = self._num_filters * len(self._ngram_filter_sizes)\n        if self._output_dim:\n            self.projection_layer = Linear(maxpool_output_dim, self._output_dim)\n        else:\n            self.projection_layer = None\n            self._output_dim = maxpool_output_dim\n\n    def get_input_dim(self):\n        return self._embedding_dim\n\n    def get_output_dim(self):\n        return self._output_dim\n\n    def forward(self, tokens, mask=None):\n        """"""\n        Args:\n            tokens (:class:`torch.FloatTensor` [batch_size, num_tokens, input_dim]): Sequence\n                matrix to encode.\n            mask (:class:`torch.FloatTensor`): Broadcastable matrix to `tokens` used as a mask.\n        Returns:\n            (:class:`torch.FloatTensor` [batch_size, output_dim]): Encoding of sequence.\n        """"""\n        if mask is not None:\n            tokens = tokens * mask.unsqueeze(-1).float()\n\n        # Our input is expected to have shape `(batch_size, num_tokens, embedding_dim)`.  The\n        # convolution layers expect input of shape `(batch_size, in_channels, sequence_length)`,\n        # where the conv layer `in_channels` is our `embedding_dim`.  We thus need to transpose the\n        # tensor first.\n        tokens = torch.transpose(tokens, 1, 2)\n        # Each convolution layer returns output of size `(batch_size, num_filters, pool_length)`,\n        # where `pool_length = num_tokens - ngram_size + 1`.  We then do an activation function,\n        # then do max pooling over each filter for the whole input sequence.  Because our max\n        # pooling is simple, we just use `torch.max`.  The resultant tensor of has shape\n        # `(batch_size, num_conv_layers * num_filters)`, which then gets projected using the\n        # projection layer, if requested.\n\n        filter_outputs = []\n        for i in range(len(self._convolution_layers)):\n            convolution_layer = getattr(self, \'conv_layer_{}\'.format(i))\n            filter_outputs.append(self._activation(convolution_layer(tokens)).max(dim=2)[0])\n\n        # Now we have a list of `num_conv_layers` tensors of shape `(batch_size, num_filters)`.\n        # Concatenating them gives us a tensor of shape\n        # `(batch_size, num_filters * num_conv_layers)`.\n        maxpool_output = torch.cat(\n            filter_outputs, dim=1) if len(filter_outputs) > 1 else filter_outputs[0]\n\n        if self.projection_layer:\n            result = self.projection_layer(maxpool_output)\n        else:\n            result = maxpool_output\n        return result\n'"
torchnlp/nn/lock_dropout.py,2,"b'# BSD 3-Clause License\n\n# Copyright (c) 2017,\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# Original implementation:\n# https://github.com/salesforce/awd-lstm-lm/blob/master/locked_dropout.py\n\nimport torch.nn as nn\n\n\nclass LockedDropout(nn.Module):\n    """""" LockedDropout applies the same dropout mask to every time step.\n\n    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n    their `License\n    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n\n    Args:\n        p (float): Probability of an element in the dropout mask to be zeroed.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n        super().__init__()\n\n    def forward(self, x):\n        """"""\n        Args:\n            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n                apply dropout too.\n        """"""\n        if not self.training or not self.p:\n            return x\n        x = x.clone()\n        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n        mask = mask.div_(1 - self.p)\n        mask = mask.expand_as(x)\n        return x * mask\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n            + \'p=\' + str(self.p) + \')\'\n'"
torchnlp/nn/weight_drop.py,15,"b'from torch.nn import Parameter\n\nimport torch\n\n\ndef _weight_drop(module, weights, dropout):\n    """"""\n    Helper for `WeightDrop`.\n    """"""\n\n    for name_w in weights:\n        w = getattr(module, name_w)\n        del module._parameters[name_w]\n        module.register_parameter(name_w + \'_raw\', Parameter(w))\n\n    original_module_forward = module.forward\n\n    def forward(*args, **kwargs):\n        for name_w in weights:\n            raw_w = getattr(module, name_w + \'_raw\')\n            w = torch.nn.functional.dropout(raw_w, p=dropout, training=module.training)\n            setattr(module, name_w, w)\n\n        return original_module_forward(*args, **kwargs)\n\n    setattr(module, \'forward\', forward)\n\n\nclass WeightDrop(torch.nn.Module):\n    """"""\n    The weight-dropped module applies recurrent regularization through a DropConnect mask on the\n    hidden-to-hidden recurrent weights.\n\n    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n    their `License\n    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n\n    Args:\n        module (:class:`torch.nn.Module`): Containing module.\n        weights (:class:`list` of :class:`str`): Names of the module weight parameters to apply a\n          dropout too.\n        dropout (float): The probability a weight will be dropped.\n\n    Example:\n\n        >>> from torchnlp.nn import WeightDrop\n        >>> import torch\n        >>>\n        >>> torch.manual_seed(123)\n        <torch._C.Generator object ...\n        >>>\n        >>> gru = torch.nn.GRUCell(2, 2)\n        >>> weights = [\'weight_hh\']\n        >>> weight_drop_gru = WeightDrop(gru, weights, dropout=0.9)\n        >>>\n        >>> input_ = torch.randn(3, 2)\n        >>> hidden_state = torch.randn(3, 2)\n        >>> weight_drop_gru(input_, hidden_state)\n        tensor(... grad_fn=<AddBackward0>)\n    """"""\n\n    def __init__(self, module, weights, dropout=0.0):\n        super(WeightDrop, self).__init__()\n        _weight_drop(module, weights, dropout)\n        self.forward = module.forward\n\n\nclass WeightDropLSTM(torch.nn.LSTM):\n    """"""\n    Wrapper around :class:`torch.nn.LSTM` that adds ``weight_dropout`` named argument.\n\n    Args:\n        weight_dropout (float): The probability a weight will be dropped.\n    """"""\n\n    def __init__(self, *args, weight_dropout=0.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        weights = [\'weight_hh_l\' + str(i) for i in range(self.num_layers)]\n        _weight_drop(self, weights, weight_dropout)\n\n\nclass WeightDropGRU(torch.nn.GRU):\n    """"""\n    Wrapper around :class:`torch.nn.GRU` that adds ``weight_dropout`` named argument.\n\n    Args:\n        weight_dropout (float): The probability a weight will be dropped.\n    """"""\n\n    def __init__(self, *args, weight_dropout=0.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        weights = [\'weight_hh_l\' + str(i) for i in range(self.num_layers)]\n        _weight_drop(self, weights, weight_dropout)\n\n\nclass WeightDropLinear(torch.nn.Linear):\n    """"""\n    Wrapper around :class:`torch.nn.Linear` that adds ``weight_dropout`` named argument.\n\n    Args:\n        weight_dropout (float): The probability a weight will be dropped.\n    """"""\n\n    def __init__(self, *args, weight_dropout=0.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        weights = [\'weight\']\n        _weight_drop(self, weights, weight_dropout)\n'"
torchnlp/samplers/__init__.py,0,"b""from torchnlp.samplers.balanced_sampler import BalancedSampler\nfrom torchnlp.samplers.bptt_batch_sampler import BPTTBatchSampler\nfrom torchnlp.samplers.bptt_sampler import BPTTSampler\nfrom torchnlp.samplers.bucket_batch_sampler import BucketBatchSampler\nfrom torchnlp.samplers.deterministic_sampler import DeterministicSampler\nfrom torchnlp.samplers.distributed_batch_sampler import DistributedBatchSampler\nfrom torchnlp.samplers.distributed_sampler import DistributedSampler\nfrom torchnlp.samplers.noisy_sorted_sampler import NoisySortedSampler\nfrom torchnlp.samplers.oom_batch_sampler import get_number_of_elements\nfrom torchnlp.samplers.oom_batch_sampler import OomBatchSampler\nfrom torchnlp.samplers.repeat_sampler import RepeatSampler\nfrom torchnlp.samplers.sorted_sampler import SortedSampler\n\n__all__ = [\n    'BalancedSampler',\n    'BPTTBatchSampler',\n    'BPTTSampler',\n    'BucketBatchSampler',\n    'DeterministicSampler',\n    'DistributedBatchSampler',\n    'DistributedSampler',\n    'get_number_of_elements',\n    'NoisySortedSampler',\n    'OomBatchSampler',\n    'RepeatSampler',\n    'SortedSampler',\n]\n"""
torchnlp/samplers/balanced_sampler.py,0,"b'from torchnlp._third_party.weighted_random_sampler import WeightedRandomSampler\n\nfrom torchnlp.utils import identity\n\n\nclass BalancedSampler(WeightedRandomSampler):\n    """""" Weighted sampler with respect for an element\'s class.\n\n    Args:\n        data (iterable)\n        get_class (callable, optional): Get the class of an item relative to the entire dataset.\n        get_weight (callable, optional): Define a weight for each item other than one.\n        kwargs: Additional key word arguments passed onto `WeightedRandomSampler`.\n\n    Example:\n        >>> from torchnlp.samplers import DeterministicSampler\n        >>>\n        >>> data = [\'a\', \'b\', \'c\'] + [\'c\'] * 100\n        >>> sampler = BalancedSampler(data, num_samples=3)\n        >>> sampler = DeterministicSampler(sampler, random_seed=12)\n        >>> [data[i] for i in sampler]\n        [\'c\', \'b\', \'a\']\n    """"""\n\n    def __init__(self, data_source, get_class=identity, get_weight=lambda x: 1, **kwargs):\n        classified = [get_class(item) for item in data_source]\n        weighted = [float(get_weight(item)) for item in data_source]\n        class_totals = {\n            k: sum([w for c, w in zip(classified, weighted) if k == c]) for k in set(classified)\n        }\n        weights = [w / class_totals[c] if w > 0 else 0.0 for c, w in zip(classified, weighted)]\n        super().__init__(weights=weights, **kwargs)\n'"
torchnlp/samplers/bptt_batch_sampler.py,1,"b'import math\n\nfrom torch.utils.data.sampler import Sampler\n\nfrom torchnlp.samplers.bptt_sampler import BPTTSampler\n\n\nclass BPTTBatchSampler(Sampler):\n    """""" Samples sequentially a batch of source and target slices of size ``bptt_length``.\n\n    Typically, such a sampler, is used for language modeling training with backpropagation through\n    time (BPTT).\n\n    **Reference:**\n    https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L61\n\n    Args:\n        data (iterable)\n        bptt_length (int): Length of the slice.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if its size would be\n            less than ``batch_size``.\n        type_ (str, optional): Type of batch [\'source\'|\'target\'] to load where a target batch is one\n            timestep ahead.\n\n    Example:\n        >>> sampler = BPTTBatchSampler(range(100), bptt_length=2, batch_size=3, drop_last=False)\n        >>> list(sampler)[0] # First Batch\n        [slice(0, 2, None), slice(34, 36, None), slice(67, 69, None)]\n    """"""\n\n    def __init__(self, data, bptt_length, batch_size, drop_last, type_=\'source\'):\n        self.data = data\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n        # For each row in the batch, we iterate over a chunk of size `chunk_size`\n        # Our chunks are similar to the columns in this PyTorch example:\n        # https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L61\n        chunk_sizes = [math.floor(len(data) / batch_size)] * batch_size\n\n        # Distribute the remaining elements to some chunks\n        if not self.drop_last:\n            remainder = len(data) - sum(chunk_sizes)\n            for i in range(remainder):\n                chunk_sizes[i] += 1\n\n        self.samplers = [{\n            \'offset\': sum(chunk_sizes[:i]),\n            \'sampler\': BPTTSampler(range(chunk_sizes[i]), bptt_length, type_=type_)\n        } for i in range(batch_size)]\n\n    def __iter__(self):\n        # Samplers iterate over chunks similar to:\n        # https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L112\n        self.iterators = [iter(value[\'sampler\']) for value in self.samplers]\n        while True:\n            batch = []\n            for i, iterator in enumerate(self.iterators):\n                try:\n                    # Adjust the sampler indices to the offset\n                    offset = self.samplers[i][\'offset\']\n                    slice_ = next(iterator)\n                    batch.append(slice(slice_.start + offset, slice_.stop + offset))\n                except StopIteration:\n                    pass\n\n            # Samplers are all empty\n            if (len(batch) == 0):\n                break\n\n            yield batch\n\n    def __len__(self):\n        return len(self.samplers[0][\'sampler\'])\n'"
torchnlp/samplers/bptt_sampler.py,1,"b'import math\n\nfrom torch.utils.data.sampler import Sampler\n\n\nclass BPTTSampler(Sampler):\n    """""" Samples sequentially source and target slices of size ``bptt_length``.\n\n    Typically, such a sampler, is used for language modeling training with backpropagation through\n    time (BPTT).\n\n    **Reference:**\n    https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L122\n\n    Args:\n        data (iterable): Iterable data.\n        bptt_length (int): Length of the slice.\n        type_ (str, optional): Type of slice [\'source\'|\'target\'] to load where a target slice is one\n            timestep ahead\n\n    Example:\n        >>> from torchnlp.samplers import BPTTSampler\n        >>> list(BPTTSampler(range(5), 2))\n        [slice(0, 2, None), slice(2, 4, None)]\n    """"""\n\n    def __init__(self, data, bptt_length, type_=\'source\'):\n        self.data = data\n        self.bptt_length = bptt_length\n        self.type = type_\n\n    def __iter__(self):\n        for i in range(0, len(self.data) - 1, self.bptt_length):\n            seq_length = min(self.bptt_length, len(self.data) - 1 - i)\n            if self.type == \'source\':\n                yield slice(i, i + seq_length)\n            if self.type == \'target\':\n                yield slice(i + 1, i + 1 + seq_length)\n\n    def __len__(self):\n        return math.ceil((len(self.data) - 1) / self.bptt_length)\n'"
torchnlp/samplers/bucket_batch_sampler.py,4,"b'import math\n\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nfrom torchnlp.samplers.sorted_sampler import SortedSampler\nfrom torchnlp.utils import identity\n\n\nclass BucketBatchSampler(BatchSampler):\n    """""" `BucketBatchSampler` toggles between `sampler` batches and sorted batches.\n\n    Typically, the `sampler` will be a `RandomSampler` allowing the user to toggle between\n    random batches and sorted batches. A larger `bucket_size_multiplier` is more sorted and vice\n    versa.\n\n    Background:\n        ``BucketBatchSampler`` is similar to a ``BucketIterator`` found in popular libraries like\n        ``AllenNLP`` and ``torchtext``. A ``BucketIterator`` pools together examples with a similar\n        size length to reduce the padding required for each batch while maintaining some noise\n        through bucketing.\n\n        **AllenNLP Implementation:**\n        https://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/bucket_iterator.py\n\n        **torchtext Implementation:**\n        https://github.com/pytorch/text/blob/master/torchtext/data/iterator.py#L225\n\n    Args:\n        sampler (torch.data.utils.sampler.Sampler):\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If `True` the sampler will drop the last batch if its size would be less\n            than `batch_size`.\n        sort_key (callable, optional): Callable to specify a comparison key for sorting.\n        bucket_size_multiplier (int, optional): Buckets are of size\n            `batch_size * bucket_size_multiplier`.\n\n    Example:\n        >>> from torchnlp.random import set_seed\n        >>> set_seed(123)\n        >>>\n        >>> from torch.utils.data.sampler import SequentialSampler\n        >>> sampler = SequentialSampler(list(range(10)))\n        >>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=False))\n        [[6, 7, 8], [0, 1, 2], [3, 4, 5], [9]]\n        >>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    """"""\n\n    def __init__(self,\n                 sampler,\n                 batch_size,\n                 drop_last,\n                 sort_key=identity,\n                 bucket_size_multiplier=100):\n        super().__init__(sampler, batch_size, drop_last)\n        self.sort_key = sort_key\n        self.bucket_sampler = BatchSampler(sampler,\n                                           min(batch_size * bucket_size_multiplier, len(sampler)),\n                                           False)\n\n    def __iter__(self):\n        for bucket in self.bucket_sampler:\n            sorted_sampler = SortedSampler(bucket, self.sort_key)\n            for batch in SubsetRandomSampler(\n                    list(BatchSampler(sorted_sampler, self.batch_size, self.drop_last))):\n                yield [bucket[i] for i in batch]\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return math.ceil(len(self.sampler) / self.batch_size)\n'"
torchnlp/samplers/deterministic_sampler.py,3,"b'from contextlib import contextmanager\n\nfrom torch.utils.data.sampler import Sampler\n\nimport torch\n\nfrom torchnlp.random import fork_rng\nfrom torchnlp.random import get_random_generator_state\nfrom torchnlp.random import set_random_generator_state\nfrom torchnlp.random import set_seed\n\n\nclass DeterministicSampler(Sampler):\n    """""" Maintains a random state such that `sampler` returns the same output every process.\n\n    Args:\n        sampler (torch.data.utils.sampler.Sampler)\n        random_seed (int)\n        cuda (bool, optional): If `True` this sampler forks the random state of CUDA as well.\n    """"""\n\n    def __init__(self, sampler, random_seed, cuda=torch.cuda.is_available()):\n        self.sampler = sampler\n        self.rng_state = None\n        self.random_seed = random_seed\n        self.cuda = cuda\n\n    @contextmanager\n    def _fork_rng(self):\n        with fork_rng(cuda=self.cuda):\n            if self.rng_state is not None:\n                set_random_generator_state(self.rng_state)\n            else:\n                set_seed(self.random_seed)\n\n            try:\n                yield\n            finally:\n                self.rng_state = get_random_generator_state(cuda=self.cuda)\n\n    def __iter__(self):\n        with self._fork_rng():\n            iterator = iter(self.sampler)\n\n        while True:\n            try:\n                with self._fork_rng():\n                    sample = next(iterator)\n                yield sample\n            except StopIteration:\n                break\n\n    def __len__(self):\n        return len(self.sampler)\n'"
torchnlp/samplers/distributed_batch_sampler.py,4,"b'from torch.utils.data.sampler import BatchSampler\n\nfrom torchnlp.samplers.distributed_sampler import DistributedSampler\n\n\nclass DistributedBatchSampler(BatchSampler):\n    """""" `BatchSampler` wrapper that distributes across each batch multiple workers.\n\n    Args:\n        batch_sampler (torch.utils.data.sampler.BatchSampler)\n        num_replicas (int, optional): Number of processes participating in distributed training.\n        rank (int, optional): Rank of the current process within num_replicas.\n\n    Example:\n        >>> from torch.utils.data.sampler import BatchSampler\n        >>> from torch.utils.data.sampler import SequentialSampler\n        >>> sampler = SequentialSampler(list(range(12)))\n        >>> batch_sampler = BatchSampler(sampler, batch_size=4, drop_last=False)\n        >>>\n        >>> list(DistributedBatchSampler(batch_sampler, num_replicas=2, rank=0))\n        [[0, 2], [4, 6], [8, 10]]\n        >>> list(DistributedBatchSampler(batch_sampler, num_replicas=2, rank=1))\n        [[1, 3], [5, 7], [9, 11]]\n    """"""\n\n    def __init__(self, batch_sampler, **kwargs):\n        self.batch_sampler = batch_sampler\n        self.kwargs = kwargs\n\n    def __iter__(self):\n        for batch in self.batch_sampler:\n            yield list(DistributedSampler(batch, **self.kwargs))\n\n    def __len__(self):\n        return len(self.batch_sampler)\n'"
torchnlp/samplers/distributed_sampler.py,5,"b'from torch.utils.data.sampler import Sampler\n\nimport torch\n\n\nclass DistributedSampler(Sampler):\n    """""" Iterable wrapper that distributes data across multiple workers.\n\n    Args:\n        iterable (iterable)\n        num_replicas (int, optional): Number of processes participating in distributed training.\n        rank (int, optional): Rank of the current process within ``num_replicas``.\n\n    Example:\n        >>> list(DistributedSampler(range(10), num_replicas=2, rank=0))\n        [0, 2, 4, 6, 8]\n        >>> list(DistributedSampler(range(10), num_replicas=2, rank=1))\n        [1, 3, 5, 7, 9]\n    """"""\n\n    def __init__(self, iterable, num_replicas=None, rank=None):\n        self.iterable = iterable\n        self.num_replicas = num_replicas\n        self.rank = rank\n\n        if num_replicas is None or rank is None:  # pragma: no cover\n            if not torch.distributed.is_initialized():\n                raise RuntimeError(\'Requires `torch.distributed` to be initialized.\')\n\n            self.num_replicas = (\n                torch.distributed.get_world_size() if num_replicas is None else num_replicas)\n            self.rank = torch.distributed.get_rank() if rank is None else rank\n\n        if self.rank >= self.num_replicas:\n            raise IndexError(\'`rank` must be smaller than the `num_replicas`.\')\n\n    def __iter__(self):\n        return iter(\n            [e for i, e in enumerate(self.iterable) if (i - self.rank) % self.num_replicas == 0])\n\n    def __len__(self):\n        return len(self.iterable)\n'"
torchnlp/samplers/noisy_sorted_sampler.py,1,"b'import random\n\nfrom torch.utils.data.sampler import Sampler\n\nfrom torchnlp.utils import identity\n\n\ndef _uniform_noise(_):\n    return random.uniform(-1, 1)\n\n\nclass NoisySortedSampler(Sampler):\n    """""" Samples elements sequentially with noise.\n\n    **Background**\n\n        ``NoisySortedSampler`` is similar to a ``BucketIterator`` found in popular libraries like\n        `AllenNLP` and `torchtext`. A ``BucketIterator`` pools together examples with a similar size\n        length to reduce the padding required for each batch. ``BucketIterator`` also includes the\n        ability to add noise to the pooling.\n\n        **AllenNLP Implementation:**\n        https://github.com/allenai/allennlp/blob/e125a490b71b21e914af01e70e9b00b165d64dcd/allennlp/data/iterators/bucket_iterator.py\n\n        **torchtext Implementation:**\n        https://github.com/pytorch/text/blob/master/torchtext/data/iterator.py#L225\n\n    Args:\n        data (iterable): Data to sample from.\n        sort_key (callable): Specifies a function of one argument that is used to extract a\n            numerical comparison key from each list element.\n        get_noise (callable): Noise added to each numerical ``sort_key``.\n\n    Example:\n        >>> from torchnlp.random import set_seed\n        >>> set_seed(123)\n        >>>\n        >>> import random\n        >>> get_noise = lambda i: round(random.uniform(-1, 1))\n        >>> list(NoisySortedSampler(range(10), sort_key=lambda i: i, get_noise=get_noise))\n        [0, 1, 2, 3, 5, 4, 6, 7, 9, 8]\n    """"""\n\n    def __init__(self, data, sort_key=identity, get_noise=_uniform_noise):\n        super().__init__(data)\n        self.data = data\n        self.sort_key = sort_key\n        self.get_noise = get_noise\n\n    def __iter__(self):\n        zip_ = []\n        for i, row in enumerate(self.data):\n            value = self.get_noise(row) + self.sort_key(row)\n            zip_.append(tuple([i, value]))\n        zip_ = sorted(zip_, key=lambda r: r[1])\n        return iter([item[0] for item in zip_])\n\n    def __len__(self):\n        return len(self.data)\n'"
torchnlp/samplers/oom_batch_sampler.py,2,"b'import heapq\n\nfrom torch.utils.data.sampler import BatchSampler\n\nfrom torchnlp.utils import get_tensors\n\n\ndef get_number_of_elements(object_):\n    """""" Get the sum of the number of elements in all tensors stored in `object_`.\n\n    This is particularly useful for sampling the largest objects based on tensor size like in:\n    `OomBatchSampler.__init__.get_item_size`.\n\n    Args:\n        object (any)\n\n    Returns:\n        (int): The number of elements in the `object_`.\n    """"""\n    return sum([t.numel() for t in get_tensors(object_)])\n\n\nclass OomBatchSampler(BatchSampler):\n    """""" Out-of-memory (OOM) batch sampler wraps `batch_sampler` to sample the `num_batches` largest\n    batches first in attempt to cause any potential OOM errors early.\n\n    Credits:\n    https://github.com/allenai/allennlp/blob/3d100d31cc8d87efcf95c0b8d162bfce55c64926/allennlp/data/iterators/bucket_iterator.py#L43\n\n    Args:\n        batch_sampler (torch.utils.data.sampler.BatchSampler)\n        get_item_size (callable): Measure the size of an item given it\'s index `int`.\n        num_batches (int, optional): The number of the large batches to move to the beginning of the\n            iteration.\n    """"""\n\n    def __init__(self, batch_sampler, get_item_size, num_batches=5):\n        self.batch_sampler = batch_sampler\n        self.get_item_size = get_item_size\n        self.num_batches = num_batches\n\n    def __iter__(self):\n        batches = list(iter(self.batch_sampler))\n        largest_batches = heapq.nlargest(\n            self.num_batches,\n            range(len(batches)),\n            key=lambda i: sum([self.get_item_size(j) for j in batches[i]]))\n        move_to_front = [batches[i] for i in largest_batches]\n        [batches.pop(i) for i in sorted(largest_batches, reverse=True)]\n        batches[0:0] = move_to_front\n        return iter(batches)\n\n    def __len__(self):\n        return len(self.batch_sampler)\n'"
torchnlp/samplers/repeat_sampler.py,2,"b'from torch.utils.data.sampler import Sampler\n\n\nclass RepeatSampler(Sampler):\n    """""" Sampler that repeats forever.\n\n    Background:\n        The repeat sampler can be used with the ``DataLoader`` with option to re-use worker\n        processes. Learn more here: https://github.com/pytorch/pytorch/issues/15849\n\n    Args:\n        sampler (torch.data.utils.sampler.Sampler)\n    """"""\n\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)\n'"
torchnlp/samplers/sorted_sampler.py,1,"b'from torch.utils.data.sampler import Sampler\n\nfrom torchnlp.utils import identity\n\n\nclass SortedSampler(Sampler):\n    """""" Samples elements sequentially, always in the same order.\n\n    Args:\n        data (iterable): Iterable data.\n        sort_key (callable): Specifies a function of one argument that is used to extract a\n            numerical comparison key from each list element.\n\n    Example:\n        >>> list(SortedSampler(range(10), sort_key=lambda i: -i))\n        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n    """"""\n\n    def __init__(self, data, sort_key=identity):\n        super().__init__(data)\n        self.data = data\n        self.sort_key = sort_key\n        zip_ = [(i, self.sort_key(row)) for i, row in enumerate(self.data)]\n        zip_ = sorted(zip_, key=lambda r: r[1])\n        self.sorted_indexes = [item[0] for item in zip_]\n\n    def __iter__(self):\n        return iter(self.sorted_indexes)\n\n    def __len__(self):\n        return len(self.data)\n'"
torchnlp/word_to_vector/__init__.py,0,"b""from torchnlp.word_to_vector.char_n_gram import CharNGram\nfrom torchnlp.word_to_vector.bpemb import BPEmb\nfrom torchnlp.word_to_vector.fast_text import FastText\nfrom torchnlp.word_to_vector.glove import GloVe\nfrom torchnlp.word_to_vector.aliases import aliases\n\n__all__ = ['GloVe', 'BPEmb', 'FastText', 'CharNGram', 'aliases']\n"""
torchnlp/word_to_vector/aliases.py,0,"b'from functools import partial\n\nfrom torchnlp.word_to_vector.char_n_gram import CharNGram\nfrom torchnlp.word_to_vector.fast_text import FastText\nfrom torchnlp.word_to_vector.glove import GloVe\n\naliases = {\n    ""charngram.100d"": partial(CharNGram),\n    ""fasttext.en.300d"": partial(FastText, language=""en""),\n    ""fasttext.simple.300d"": partial(FastText, language=""simple""),\n    ""glove.42B.300d"": partial(GloVe, name=""42B"", dim=""300""),\n    ""glove.840B.300d"": partial(GloVe, name=""840B"", dim=""300""),\n    ""glove.twitter.27B.25d"": partial(GloVe, name=""twitter.27B"", dim=""25""),\n    ""glove.twitter.27B.50d"": partial(GloVe, name=""twitter.27B"", dim=""50""),\n    ""glove.twitter.27B.100d"": partial(GloVe, name=""twitter.27B"", dim=""100""),\n    ""glove.twitter.27B.200d"": partial(GloVe, name=""twitter.27B"", dim=""200""),\n    ""glove.6B.50d"": partial(GloVe, name=""6B"", dim=""50""),\n    ""glove.6B.100d"": partial(GloVe, name=""6B"", dim=""100""),\n    ""glove.6B.200d"": partial(GloVe, name=""6B"", dim=""200""),\n    ""glove.6B.300d"": partial(GloVe, name=""6B"", dim=""300"")\n}\n'"
torchnlp/word_to_vector/bpemb.py,1,"b'from torchnlp.word_to_vector.pretrained_word_vectors import _PretrainedWordVectors\n\n# List of all 275 supported languages from http://cosyne.h-its.org/bpemb/data/\nSUPPORTED_LANGUAGES = [\n    \'ab\', \'ace\', \'ady\', \'af\', \'ak\', \'als\', \'am\', \'an\', \'ang\', \'ar\', \'arc\', \'arz\', \'as\', \'ast\',\n    \'atj\', \'av\', \'ay\', \'az\', \'azb\', \'ba\', \'bar\', \'bcl\', \'be\', \'bg\', \'bi\', \'bjn\', \'bm\', \'bn\', \'bo\',\n    \'bpy\', \'br\', \'bs\', \'bug\', \'bxr\', \'ca\', \'cdo\', \'ce\', \'ceb\', \'ch\', \'chr\', \'chy\', \'ckb\', \'co\',\n    \'cr\', \'crh\', \'cs\', \'csb\', \'cu\', \'cv\', \'cy\', \'da\', \'de\', \'din\', \'diq\', \'dsb\', \'dty\', \'dv\', \'dz\',\n    \'ee\', \'el\', \'en\', \'eo\', \'es\', \'et\', \'eu\', \'ext\', \'fa\', \'ff\', \'fi\', \'fj\', \'fo\', \'fr\', \'frp\',\n    \'frr\', \'fur\', \'fy\', \'ga\', \'gag\', \'gan\', \'gd\', \'gl\', \'glk\', \'gn\', \'gom\', \'got\', \'gu\', \'gv\', \'ha\',\n    \'hak\', \'haw\', \'he\', \'hi\', \'hif\', \'hr\', \'hsb\', \'ht\', \'hu\', \'hy\', \'ia\', \'id\', \'ie\', \'ig\', \'ik\',\n    \'ilo\', \'io\', \'is\', \'it\', \'iu\', \'ja\', \'jam\', \'jbo\', \'jv\', \'ka\', \'kaa\', \'kab\', \'kbd\', \'kbp\', \'kg\',\n    \'ki\', \'kk\', \'kl\', \'km\', \'kn\', \'ko\', \'koi\', \'krc\', \'ks\', \'ksh\', \'ku\', \'kv\', \'kw\', \'ky\', \'la\',\n    \'lad\', \'lb\', \'lbe\', \'lez\', \'lg\', \'li\', \'lij\', \'lmo\', \'ln\', \'lo\', \'lrc\', \'lt\', \'ltg\', \'lv\',\n    \'mai\', \'mdf\', \'mg\', \'mh\', \'mhr\', \'mi\', \'min\', \'mk\', \'ml\', \'mn\', \'mr\', \'mrj\', \'ms\', \'mt\', \'mwl\',\n    \'my\', \'myv\', \'mzn\', \'na\', \'nap\', \'nds\', \'ne\', \'new\', \'ng\', \'nl\', \'nn\', \'no\', \'nov\', \'nrm\',\n    \'nso\', \'nv\', \'ny\', \'oc\', \'olo\', \'om\', \'or\', \'os\', \'pa\', \'pag\', \'pam\', \'pap\', \'pcd\', \'pdc\',\n    \'pfl\', \'pi\', \'pih\', \'pl\', \'pms\', \'pnb\', \'pnt\', \'ps\', \'pt\', \'qu\', \'rm\', \'rmy\', \'rn\', \'ro\', \'ru\',\n    \'rue\', \'rw\', \'sa\', \'sah\', \'sc\', \'scn\', \'sco\', \'sd\', \'se\', \'sg\', \'sh\', \'si\', \'sk\', \'sl\', \'sm\',\n    \'sn\', \'so\', \'sq\', \'sr\', \'srn\', \'ss\', \'st\', \'stq\', \'su\', \'sv\', \'sw\', \'szl\', \'ta\', \'tcy\', \'te\',\n    \'tet\', \'tg\', \'th\', \'ti\', \'tk\', \'tl\', \'tn\', \'to\', \'tpi\', \'tr\', \'ts\', \'tt\', \'tum\', \'tw\', \'ty\',\n    \'tyv\', \'udm\', \'ug\', \'uk\', \'ur\', \'uz\', \'ve\', \'vec\', \'vep\', \'vi\', \'vls\', \'vo\', \'wa\', \'war\', \'wo\',\n    \'wuu\', \'xal\', \'xh\', \'xmf\', \'yi\', \'yo\', \'za\', \'zea\', \'zh\', \'zu\'\n]\n\n# All supported vector dimensionalities for which embeddings were trained\nSUPPORTED_DIMS = [25, 50, 100, 200, 300]\n\n# All supported number of merge operations for which embeddings were trained\nSUPPORTED_MERGE_OPS = [1000, 3000, 5000, 10000, 25000, 50000, 100000, 200000]\n\n\nclass BPEmb(_PretrainedWordVectors):\n    """"""\n    Byte-Pair Encoding (BPE) embeddings trained on Wikipedia for 275 languages\n\n    A collection of pre-trained subword unit embeddings in 275 languages, based\n    on Byte-Pair Encoding (BPE). In an evaluation using fine-grained entity typing as testbed,\n    BPEmb performs competitively, and for some languages better than alternative subword\n    approaches, while requiring vastly fewer resources and no tokenization.\n\n    References:\n        * https://arxiv.org/abs/1710.02187\n        * https://github.com/bheinzerling/bpemb\n\n    Args:\n        language (str, optional): Language of the corpus on which the embeddings\n            have been trained\n        dim (int, optional): Dimensionality of the embeddings\n        merge_ops (int, optional): Number of merge operations used by the\n            tokenizer\n\n    Example:\n        >>> from torchnlp.word_to_vector import BPEmb  # doctest: +SKIP\n        >>> vectors = BPEmb(dim=25)  # doctest: +SKIP\n        >>> subwords = ""\xe2\x96\x81mel ford shire"".split()  # doctest: +SKIP\n        >>> vectors[subwords]  # doctest: +SKIP\n        Columns 0 to 9\n        -0.5859 -0.1803  0.2623 -0.6052  0.0194 -0.2795  0.2716 -0.2957 -0.0492\n        1.0934\n         0.3848 -0.2412  1.0599 -0.8588 -1.2596 -0.2534 -0.5704  0.2168 -0.1718\n        1.2675\n         1.4407 -0.0996  1.2239 -0.5085 -0.7542 -0.9628 -1.7177  0.0618 -0.4025\n        1.0405\n        ...\n        Columns 20 to 24\n        -0.0022  0.4820 -0.5156 -0.0564  0.4300\n         0.0355 -0.2257  0.1323  0.6053 -0.8878\n        -0.0167 -0.3686  0.9666  0.2497 -1.2239\n        [torch.FloatTensor of size 3x25]\n    """"""\n    url_base = \'http://cosyne.h-its.org/bpemb/data/{language}/\'\n    file_name = \'{language}.wiki.bpe.op{merge_ops}.d{dim}.w2v.txt\'\n    zip_extension = \'.tar.gz\'\n\n    def __init__(self, language=\'en\', dim=300, merge_ops=50000, **kwargs):\n        # Check if all parameters are valid\n        if language not in SUPPORTED_LANGUAGES:\n            raise ValueError((""Language \'%s\' not supported. Use one of the ""\n                              ""following options instead:\\n%s"") % (language, SUPPORTED_LANGUAGES))\n        if dim not in SUPPORTED_DIMS:\n            raise ValueError(\n                (""Embedding dimensionality of \'%d\' not supported. ""\n                 ""Use one of the following options instead:\\n%s"") % (dim, SUPPORTED_DIMS))\n        if merge_ops not in SUPPORTED_MERGE_OPS:\n            raise ValueError((""Number of \'%d\' merge operations not supported. ""\n                              ""Use one of the following options instead:\\n%s"") %\n                             (merge_ops, SUPPORTED_MERGE_OPS))\n\n        format_map = {\'language\': language, \'merge_ops\': merge_ops, \'dim\': dim}\n\n        # Assemble file name to locally store embeddings under\n        name = self.file_name.format_map(format_map)\n        # Assemble URL to download the embeddings form\n        url = (\n            self.url_base.format_map(format_map) + self.file_name.format_map(format_map) +\n            self.zip_extension)\n\n        super(BPEmb, self).__init__(name, url=url, **kwargs)\n'"
torchnlp/word_to_vector/char_n_gram.py,2,"b'# BSD 3-Clause License\n\n# Copyright (c) James Bradbury and Soumith Chintala 2016,\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport torch\n\nfrom torchnlp.word_to_vector.pretrained_word_vectors import _PretrainedWordVectors\n\n\nclass CharNGram(_PretrainedWordVectors):\n    """""" Character n-gram is a character-based compositional model to embed textual sequences.\n\n    Character n-gram embeddings are trained by the same Skip-gram objective. The final character\n    embedding is the average of the unique character n-gram embeddings of wt. For example, the\n    character n-grams (n = 1, 2, 3) of the word \xe2\x80\x9cCat\xe2\x80\x9d are {C, a, t, #B#C, Ca, at, t#E#, #B#Ca, Cat,\n    at#E#}, where \xe2\x80\x9c#B#\xe2\x80\x9d and \xe2\x80\x9c#E#\xe2\x80\x9d represent the beginning and the end of each word, respectively.\n    Using the character embeddings efficiently provides morphological features. Each word is\n    subsequently represented as xt, the concatenation of its corresponding word and character\n    embeddings shared across the tasks.\n\n    **Reference:**\n    http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/\n\n    Args:\n        cache (str, optional): directory for cached vectors\n        unk_init (callback, optional): by default, initialize out-of-vocabulary word vectors\n            to zero vectors; can be any function that takes in a Tensor and\n            returns a Tensor of the same size\n        is_include (callable, optional): callable returns True if to include a token in memory\n            vectors cache; some of these embedding files are gigantic so filtering it can cut\n            down on the memory usage. We do not cache on disk if ``is_include`` is defined.\n\n    Example:\n        >>> from torchnlp.word_to_vector import CharNGram  # doctest: +SKIP\n        >>> vectors = CharNGram()  # doctest: +SKIP\n        >>> vectors[\'hello\']  # doctest: +SKIP\n        -1.7494\n        0.6242\n        ...\n        -0.6202\n        2.0928\n        [torch.FloatTensor of size 100]\n    """"""\n\n    name = \'charNgram.txt\'\n    url = (\'https://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/\'\n           \'jmt_pre-trained_embeddings.tar.gz\')\n\n    def __init__(self, **kwargs):\n        super(CharNGram, self).__init__(self.name, url=self.url, **kwargs)\n\n    def __getitem__(self, token):\n        vector = torch.Tensor(self.dim).zero_()\n        # These literals need to be coerced to unicode for Python 2 compatibility\n        # when we try to join them with read ngrams from the files.\n        chars = [\'#BEGIN#\'] + list(token) + [\'#END#\']\n        num_vectors = 0\n        for n in [2, 3, 4]:\n            end = len(chars) - n + 1\n            grams = [chars[i:(i + n)] for i in range(end)]\n            for gram in grams:\n                gram_key = \'{}gram-{}\'.format(n, \'\'.join(gram))\n                if gram_key in self.token_to_index:\n                    vector += self.vectors[self.token_to_index[gram_key]]\n                    num_vectors += 1\n        if num_vectors > 0:\n            vector /= num_vectors\n        else:\n            vector = self.unk_init(vector)\n        return vector\n'"
torchnlp/word_to_vector/fast_text.py,1,"b'# BSD 3-Clause License\n\n# Copyright (c) James Bradbury and Soumith Chintala 2016,\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport os\n\nfrom torchnlp.word_to_vector.pretrained_word_vectors import _PretrainedWordVectors\n\n\nclass FastText(_PretrainedWordVectors):\n    """""" Enriched word vectors with subword information from Facebook\'s AI Research (FAIR) lab.\n\n    A approach based on the skipgram model, where each word is represented as a bag of character\n    n-grams. A vector representation is associated to each character n-gram; words being\n    represented as the sum of these representations.\n\n    References:\n        * https://arxiv.org/abs/1607.04606\n        * https://fasttext.cc/\n        * https://arxiv.org/abs/1710.04087\n\n    Args:\n        language (str): language of the vectors\n        aligned (bool): if True: use multilingual embeddings where words with\n            the same meaning share (approximately) the same position in the\n            vector space across languages. if False: use regular FastText\n            embeddings. All available languages can be found under\n            https://github.com/facebookresearch/MUSE#multilingual-word-embeddings\n        cache (str, optional): directory for cached vectors\n        unk_init (callback, optional): by default, initialize out-of-vocabulary word vectors\n            to zero vectors; can be any function that takes in a Tensor and\n            returns a Tensor of the same size\n        is_include (callable, optional): callable returns True if to include a token in memory\n            vectors cache; some of these embedding files are gigantic so filtering it can cut\n            down on the memory usage. We do not cache on disk if ``is_include`` is defined.\n\n    Example:\n        >>> from torchnlp.word_to_vector import FastText  # doctest: +SKIP\n        >>> vectors = FastText()  # doctest: +SKIP\n        >>> vectors[\'hello\']  # doctest: +SKIP\n        -0.1595\n        -0.1826\n        ...\n        0.2492\n        0.0654\n        [torch.FloatTensor of size 300]\n    """"""\n    url_base = \'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec\'\n    aligned_url_base = \'https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.{}.align.vec\'\n\n    def __init__(self, language=""en"", aligned=False, **kwargs):\n        if aligned:\n            url = self.aligned_url_base.format(language)\n        else:\n            url = self.url_base.format(language)\n        name = os.path.basename(url)\n        super(FastText, self).__init__(name, url=url, **kwargs)\n'"
torchnlp/word_to_vector/glove.py,1,"b'# BSD 3-Clause License\n\n# Copyright (c) James Bradbury and Soumith Chintala 2016,\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nfrom torchnlp.word_to_vector.pretrained_word_vectors import _PretrainedWordVectors\n\n\nclass GloVe(_PretrainedWordVectors):\n    """"""Word vectors derived from word-word co-occurrence statistics from a corpus by Stanford.\n\n    GloVe is essentially a log-bilinear model with a weighted least-squares objective. The main\n    intuition underlying the model is the simple observation that ratios of word-word co-occurrence\n    probabilities have the potential for encoding some form of meaning.\n\n    **Reference:**\n    https://nlp.stanford.edu/projects/glove/\n\n    Args:\n        name (str): name of the GloVe vectors (\'840B\', \'twitter.27B\', \'6B\', \'42B\')\n        cache (str, optional): directory for cached vectors\n        unk_init (callback, optional): by default, initialize out-of-vocabulary word vectors\n            to zero vectors; can be any function that takes in a Tensor and\n            returns a Tensor of the same size\n        is_include (callable, optional): callable returns True if to include a token in memory\n            vectors cache; some of these embedding files are gigantic so filtering it can cut\n            down on the memory usage. We do not cache on disk if ``is_include`` is defined.\n\n    Example:\n        >>> from torchnlp.word_to_vector import GloVe  # doctest: +SKIP\n        >>> vectors = GloVe()  # doctest: +SKIP\n        >>> vectors[\'hello\']  # doctest: +SKIP\n        -1.7494\n        0.6242\n        ...\n        -0.6202\n        2.0928\n        [torch.FloatTensor of size 100]\n    """"""\n\n    url = {\n        \'42B\': \'http://nlp.stanford.edu/data/glove.42B.300d.zip\',\n        \'840B\': \'http://nlp.stanford.edu/data/glove.840B.300d.zip\',\n        \'twitter.27B\': \'http://nlp.stanford.edu/data/glove.twitter.27B.zip\',\n        \'6B\': \'http://nlp.stanford.edu/data/glove.6B.zip\',\n    }\n\n    def __init__(self, name=\'840B\', dim=300, **kwargs):\n        url = self.url[name]\n        name = \'glove.{}.{}d.txt\'.format(name, str(dim))\n        super(GloVe, self).__init__(name, url=url, **kwargs)\n'"
torchnlp/word_to_vector/pretrained_word_vectors.py,7,"b'# BSD 3-Clause License\n\n# Copyright (c) James Bradbury and Soumith Chintala 2016,\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nfrom __future__ import unicode_literals\n\nimport io\nimport logging\nimport os\n\nfrom torchnlp._third_party.lazy_loader import LazyLoader\nfrom tqdm import tqdm\n\nimport torch\nsix = LazyLoader(\'six\', globals(), \'six\')\n\nfrom torchnlp.download import download_file_maybe_extract\n\nlogger = logging.getLogger(__name__)\n\n\nclass _PretrainedWordVectors(object):\n    """""" _PretrainedWordVectors handles downloading, caching and storing pretrained embeddings.\n\n    Args:\n        name (str): name of the file that contains the vectors\n        cache (str, optional): directory for cached vectors\n        url (str or None, optional): url for download if vectors not found in cache\n        unk_init (callback, optional): by default, initialize out-of-vocabulary word vectors\n            to zero vectors; can be any function that takes in a Tensor and\n            returns a Tensor of the same size\n        is_include (callable, optional): callable returns True if to include a token in memory\n            vectors cache; some of these embedding files are gigantic so filtering it can cut\n            down on the memory usage. We do not cache on disk if ``is_include`` is defined.\n    """"""\n\n    def __init__(self,\n                 name,\n                 cache=\'.word_vectors_cache\',\n                 url=None,\n                 unk_init=torch.Tensor.zero_,\n                 is_include=None):\n        self.unk_init = unk_init\n        self.is_include = is_include\n        self.name = name\n        self.cache(name, cache, url=url)\n\n    def __contains__(self, token):\n        return token in self.token_to_index\n\n    def _get_token_vector(self, token):\n        """"""Return embedding for token or for UNK if token not in vocabulary""""""\n        if token in self.token_to_index:\n            return self.vectors[self.token_to_index[token]]\n        else:\n            return self.unk_init(torch.Tensor(self.dim))\n\n    def __getitem__(self, tokens):\n        if isinstance(tokens, list) or isinstance(tokens, tuple):\n            vector_list = [self._get_token_vector(token) for token in tokens]\n            return torch.stack(vector_list)\n        elif isinstance(tokens, str):\n            token = tokens\n            return self._get_token_vector(token)\n        else:\n            raise TypeError(""\'__getitem__\' method can only be used with types""\n                            ""\'str\', \'list\', or \'tuple\' as parameter"")\n\n    def __len__(self):\n        return len(self.vectors)\n\n    def __str__(self):\n        return self.name\n\n    def cache(self, name, cache, url=None):\n        if os.path.isfile(name):\n            path = name\n            path_pt = os.path.join(cache, os.path.basename(name)) + \'.pt\'\n        else:\n            path = os.path.join(cache, name)\n            path_pt = path + \'.pt\'\n\n        if not os.path.isfile(path_pt) or self.is_include is not None:\n            if url:\n                download_file_maybe_extract(url=url, directory=cache, check_files=[name])\n\n            if not os.path.isfile(path):\n                raise RuntimeError(\'no vectors found at {}\'.format(path))\n\n            index_to_token, vectors, dim = [], None, None\n\n            # Try to read the whole file with utf-8 encoding.\n            binary_lines = False\n            try:\n                with io.open(path, encoding=""utf8"") as f:\n                    lines = [line for line in f]\n            # If there are malformed lines, read in binary mode\n            # and manually decode each word from utf-8\n            except:\n                logger.warning(""Could not read {} as UTF8 file, ""\n                               ""reading file as bytes and skipping ""\n                               ""words with malformed UTF8."".format(path))\n                with open(path, \'rb\') as f:\n                    lines = [line for line in f]\n                binary_lines = True\n\n            logger.info(""Loading vectors from {}"".format(path))\n            for line in tqdm(lines, total=len(lines)):\n                # Explicitly splitting on "" "" is important, so we don\'t\n                # get rid of Unicode non-breaking spaces in the vectors.\n                entries = line.rstrip().split(b"" "" if binary_lines else "" "")\n\n                word, entries = entries[0], entries[1:]\n                if dim is None and vectors is None and len(entries) > 1:\n                    dim = len(entries)\n                    vectors = torch.empty(len(lines), dim, dtype=torch.float)\n                elif len(entries) == 1:\n                    logger.warning(""Skipping token {} with 1-dimensional ""\n                                   ""vector {}; likely a header"".format(word, entries))\n                    continue\n                elif dim != len(entries):\n                    raise RuntimeError(""Vector for token {} has {} dimensions, but previously ""\n                                       ""read vectors have {} dimensions. All vectors must have ""\n                                       ""the same number of dimensions."".format(\n                                           word, len(entries), dim))\n\n                if binary_lines:\n                    try:\n                        if isinstance(word, six.binary_type):\n                            word = word.decode(\'utf-8\')\n                    except:\n                        logger.info(""Skipping non-UTF8 token {}"".format(repr(word)))\n                        continue\n\n                if self.is_include is not None and not self.is_include(word):\n                    continue\n\n                vectors[len(index_to_token)] = torch.tensor([float(x) for x in entries])\n                index_to_token.append(word)\n\n            self.index_to_token = index_to_token\n            self.token_to_index = {word: i for i, word in enumerate(index_to_token)}\n            self.vectors = vectors[:len(index_to_token)]\n            self.dim = dim\n            logger.info(\'Saving vectors to {}\'.format(path_pt))\n            if not os.path.exists(cache):\n                os.makedirs(cache)\n            torch.save((self.index_to_token, self.token_to_index, self.vectors, self.dim), path_pt)\n        else:\n            logger.info(\'Loading vectors from {}\'.format(path_pt))\n            self.index_to_token, self.token_to_index, self.vectors, self.dim = torch.load(path_pt)\n'"
tests/encoders/text/test_character_encoder.py,0,"b""import pickle\n\nimport pytest\n\nfrom torchnlp.encoders.text import CharacterEncoder\nfrom torchnlp.encoders.text import DEFAULT_RESERVED_TOKENS\nfrom torchnlp.encoders.text import DEFAULT_UNKNOWN_TOKEN\nfrom torchnlp.encoders.text import DEFAULT_UNKNOWN_INDEX\n\n\n@pytest.fixture\ndef sample():\n    return ['The quick brown fox jumps over the lazy dog']\n\n\n@pytest.fixture\ndef encoder(sample):\n    return CharacterEncoder(sample)\n\n\ndef test_character_encoder(encoder, sample):\n    input_ = 'english-language pangram'\n    output = encoder.encode(input_)\n    assert encoder.vocab_size == len(set(list(sample[0]))) + len(DEFAULT_RESERVED_TOKENS)\n    assert len(output) == len(input_)\n    assert encoder.decode(output) == input_.replace('-', DEFAULT_UNKNOWN_TOKEN)\n\n\ndef test_character_encoder__enforce_reversible(encoder):\n    encoder.enforce_reversible = True\n\n    with pytest.raises(ValueError):\n        encoder.decode(encoder.encode('english-language pangram'))\n\n    encoder.decode(encoder.encode('english language pangram'))\n\n    encoded = encoder.encode('english language pangram')\n    encoded[7] = DEFAULT_UNKNOWN_INDEX\n    with pytest.raises(ValueError):\n        encoder.decode(encoded)\n\n\ndef test_character_encoder_batch(encoder):\n    input_ = 'english-language pangram'\n    longer_input_ = 'english-language pangram pangram'\n    encoded, lengths = encoder.batch_encode([input_, longer_input_])\n    assert encoded.shape[0] == 2\n    assert len(lengths) == 2\n    decoded = encoder.batch_decode(encoded, lengths=lengths)\n    assert decoded[0] == input_.replace('-', DEFAULT_UNKNOWN_TOKEN)\n    assert decoded[1] == longer_input_.replace('-', DEFAULT_UNKNOWN_TOKEN)\n\n\ndef test_character_encoder_min_occurrences(sample):\n    encoder = CharacterEncoder(sample, min_occurrences=10)\n    input_ = 'English-language pangram'\n    output = encoder.encode(input_)\n    assert encoder.decode(output) == ''.join([DEFAULT_UNKNOWN_TOKEN] * len(input_))\n\n\ndef test_is_pickleable(encoder):\n    pickle.dumps(encoder)\n"""
tests/encoders/text/test_delimiter_encoder.py,0,"b""import pickle\n\nimport pytest\n\nfrom torchnlp.encoders.text import DelimiterEncoder\nfrom torchnlp.encoders.text import DEFAULT_UNKNOWN_TOKEN\nfrom torchnlp.encoders.text import DEFAULT_EOS_TOKEN\n\n\n@pytest.fixture\ndef encoder():\n    sample = ['people/deceased_person/place_of_death', 'symbols/name_source/namesakes']\n    return DelimiterEncoder('/', sample, append_eos=True)\n\n\ndef test_delimiter_encoder(encoder):\n    input_ = 'symbols/namesake/named_after'\n    output = encoder.encode(input_)\n    assert encoder.decode(output) == '/'.join(\n        ['symbols', DEFAULT_UNKNOWN_TOKEN, DEFAULT_UNKNOWN_TOKEN, DEFAULT_EOS_TOKEN])\n\n\ndef test_is_pickleable(encoder):\n    pickle.dumps(encoder)\n"""
tests/encoders/text/test_moses_encoder.py,0,"b'import pickle\nimport sys\n\nimport pytest\n\nfrom torchnlp.encoders.text import MosesEncoder\n\n\n@pytest.fixture\ndef input_():\n    return (""This ain\'t funny. It\'s actually hillarious, yet double Ls. | [] < > [ ] & "" +\n            ""You\'re gonna shake it off? Don\'t?"")\n\n\n@pytest.fixture\ndef encoder(input_):\n    return MosesEncoder([input_])\n\n\n@pytest.mark.skipif(\n    sys.version_info >= (3, 7), reason=""Running NLTK moses with Python 3.7 halts on travis."")\ndef test_moses_encoder(encoder, input_):\n    # TEST adapted from example in http://www.nltk.org/_modules/nltk/tokenize/moses.html\n    expected_tokens = [\n        \'This\', \'ain\', \'&apos;t\', \'funny\', \'.\', \'It\', \'&apos;s\', \'actually\', \'hillarious\', \',\',\n        \'yet\', \'double\', \'Ls\', \'.\', \'&#124;\', \'&#91;\', \'&#93;\', \'&lt;\', \'&gt;\', \'&#91;\', \'&#93;\',\n        \'&amp;\', \'You\', \'&apos;re\', \'gonna\', \'shake\', \'it\', \'off\', \'?\', \'Don\', \'&apos;t\', \'?\'\n    ]\n    expected_decode = (""This ain\'t funny. It\'s actually hillarious, yet double Ls. | [] < > [] & "" +\n                       ""You\'re gonna shake it off? Don\'t?"")\n    tokens = encoder.encode(input_)\n    assert [encoder.index_to_token[i] for i in tokens] == expected_tokens\n    assert encoder.decode(tokens) == expected_decode\n\n\n@pytest.mark.skipif(\n    sys.version_info >= (3, 7), reason=""Running NLTK moses with Python 3.7 halts on travis."")\ndef test_is_pickleable(encoder):\n    pickle.dumps(encoder)\n'"
tests/encoders/text/test_spacy_encoder.py,0,"b'import pickle\n\nimport pytest\n\nfrom torchnlp.encoders.text import SpacyEncoder\n\n\n@pytest.fixture\ndef input_():\n    return (\'This is a sentence\')\n\n\n@pytest.fixture\ndef encoder(input_):\n    return SpacyEncoder([input_])\n\n\ndef test_spacy_encoder(encoder, input_):\n    tokens = encoder.encode(input_)\n    assert encoder.decode(tokens) == input_\n\n\ndef test_spacy_encoder_issue_44():\n    # https://github.com/PetrochukM/PyTorch-NLP/issues/44\n    encoder = SpacyEncoder([""This ain\'t funny.""])\n    assert \'ai\' in encoder.vocab\n    assert \'n\\\'t\' in encoder.vocab\n\n\ndef test_spacy_encoder_batch(encoder, input_):\n    tokens, _ = encoder.batch_encode([input_, input_])\n    assert encoder.decode(tokens[0]) == input_\n    assert encoder.decode(tokens[1]) == input_\n\n\ndef test_spacy_encoder_not_installed_language():\n    error_message = \'\'\n    try:\n        SpacyEncoder([], language=\'fr\')\n    except Exception as e:\n        error_message = str(e)\n\n    assert error_message.startswith(""Language \'fr\' not found."")\n\n\ndef test_spacy_encoder_unsupported_language():\n    error_message = \'\'\n    try:\n        SpacyEncoder([], language=\'python\')\n    except Exception as e:\n        error_message = str(e)\n\n    assert error_message.startswith(""No tokenizer available for language "" + ""\'python\'."")\n\n\ndef test_is_pickleable(encoder):\n    pickle.dumps(encoder)\n'"
tests/encoders/text/test_static_tokenizer_encoder.py,1,"b""import pickle\n\nimport pytest\nimport torch\n\nfrom torchnlp.encoders.text import StaticTokenizerEncoder\n\n\n@pytest.fixture\ndef input_():\n    return 'This is a sentence'\n\n\n@pytest.fixture\ndef encoder(input_):\n    return StaticTokenizerEncoder([input_])\n\n\ndef test_static_tokenizer_encoder__empty(encoder):\n    tokens = encoder.encode('')\n    assert tokens.dtype == torch.long\n    assert encoder.decode(tokens) == ''\n\n\ndef test_static_tokenizer_encoder(encoder, input_):\n    tokens = encoder.encode(input_)\n    assert encoder.decode(tokens) == input_\n\n\ndef test_static_tokenizer_encoder_batch(encoder, input_):\n    batched_input = [input_, input_]\n    encoded, lengths = encoder.batch_encode(batched_input)\n    assert encoder.batch_decode(encoded, lengths=lengths) == batched_input\n\n\ndef test_is_pickleable(encoder):\n    pickle.dumps(encoder)\n"""
tests/encoders/text/test_subword_encoder.py,0,"b'import pickle\n\nimport pytest\n\nfrom torchnlp.encoders.text import SubwordEncoder\nfrom torchnlp.encoders.text import DEFAULT_EOS_INDEX\n\n\nclass TestSubwordEncoder:\n\n    @pytest.fixture(scope=\'module\')\n    def corpus(self):\n        return [\n            ""One morning I shot an elephant in my pajamas. How he got in my pajamas, I don\'t"",\n            \'know.\', \'\', \'Groucho Marx\',\n            ""I haven\'t slept for 10 days... because that would be too long."", \'\', \'Mitch Hedberg\'\n        ]\n\n    @pytest.fixture\n    def encoder(self, corpus):\n        return SubwordEncoder(corpus, target_vocab_size=86, min_occurrences=2, max_occurrences=6)\n\n    def test_build_vocab_target_size(self, encoder):\n        # NOTE: ``target_vocab_size`` is approximate; therefore, it may not be exactly the target\n        # size\n        assert len(encoder.vocab) == 86\n\n    def test_encode(self, encoder):\n        input_ = \'This has UPPER CASE letters that are out of alphabet\'\n        assert encoder.decode(encoder.encode(input_)) == input_\n\n    def test_eos(self, corpus):\n        encoder = SubwordEncoder(corpus, append_eos=True)\n        input_ = \'This is a sentence\'\n        assert encoder.encode(input_)[-1] == DEFAULT_EOS_INDEX\n\n    def test_is_pickleable(self, encoder):\n        pickle.dumps(encoder)\n'"
tests/encoders/text/test_subword_tokenizer.py,0,"b'import unittest\nimport collections\nimport pickle\nimport random\nimport mock\n\nimport six\n\nfrom torchnlp.encoders.text import subword_text_tokenizer\nfrom torchnlp.encoders.text.subword_text_tokenizer import encode\nfrom torchnlp.encoders.text.subword_text_tokenizer import decode\nfrom torchnlp.encoders.text.subword_text_tokenizer import _escape_token\nfrom torchnlp.encoders.text.subword_text_tokenizer import _unescape_token\nfrom torchnlp.encoders.text.subword_text_tokenizer import _ESCAPE_CHARS\nfrom torchnlp.encoders.text.subword_text_tokenizer import SubwordTextTokenizer\n\n\nclass TestTokenCounts(unittest.TestCase):\n\n    def setUp(self):\n        self.corpus = [\n            ""One morning I shot an elephant in my pajamas. How he got in my pajamas, I don\'t"",\n            \'know.\', \'\', \'Groucho Marx\',\n            ""I haven\'t slept for 10 days... because that would be too long."", \'\', \'Mitch Hedberg\'\n        ]\n\n    def test_token_counts(self):\n        token_counts = SubwordTextTokenizer._count_tokens(self.corpus)\n        expected = {\n            u""\'"": 2,\n            u""."": 2,\n            u"". "": 1,\n            u""... "": 1,\n            u""Groucho"": 1,\n            u""Marx"": 1,\n            u""Mitch"": 1,\n            u""Hedberg"": 1,\n            u""I"": 3,\n            u""in"": 2,\n            u""my"": 2,\n            u""pajamas"": 2,\n        }\n        self.assertDictContainsSubset(expected, token_counts)\n\n\nclass EncodeDecodeTest(unittest.TestCase):\n\n    def test_encode(self):\n        self.assertListEqual([u""Dude"", u"" - "", u""that"", u""\'"", u""s"", u""so"", u""cool"", u"".""],\n                             encode(u""Dude - that\'s so cool.""))\n        self.assertListEqual([u""\xc5\x81ukasz"", u""est"", u""n\xc3\xa9"", u""en"", u""1981"", u"".""],\n                             encode(u""\xc5\x81ukasz est n\xc3\xa9 en 1981.""))\n        self.assertListEqual([u"" "", u""Spaces"", u""at"", u""the"", u""ends"", u"" ""],\n                             encode(u"" Spaces at the ends ""))\n        self.assertListEqual([u""802"", u""."", u""11b""], encode(u""802.11b""))\n        self.assertListEqual([u""two"", u"". \\n"", u""lines""], encode(u""two. \\nlines""))\n\n    def test_decode(self):\n        self.assertEqual(u""Dude - that\'s so cool."",\n                         decode([u""Dude"", u"" - "", u""that"", u""\'"", u""s"", u""so"", u""cool"", u"".""]))\n\n    def test_invertibility_on_random_strings(self):\n        for _ in range(1000):\n            s = u"""".join(six.unichr(random.randint(0, 65535)) for _ in range(10))\n            self.assertEqual(s, decode(encode(s)))\n\n\nclass EscapeUnescapeTokenTest(unittest.TestCase):\n\n    def test_escape_token(self):\n        escaped = _escape_token(\'Foo! Bar.\\nunder_score back\\\\slash\',\n                                set(\'abcdefghijklmnopqrstuvwxyz .\\n\') | _ESCAPE_CHARS)\n\n        self.assertEqual(\'\\\\70;oo\\\\33; \\\\66;ar.\\\\10;under\\\\uscore back\\\\\\\\slash_\', escaped)\n\n    def test_unescape_token(self):\n        unescaped = _unescape_token(\'\\\\70;oo\\\\33; \\\\66;ar.\\\\10;under\\\\uscore back\\\\\\\\slash_\')\n\n        self.assertEqual(\'Foo! Bar.\\nunder_score back\\\\slash\', unescaped)\n\n\nclass SubwordTextTokenizerTest(unittest.TestCase):\n\n    def test_encode_decode(self):\n        corpus = (\'This is a corpus of text that provides a bunch of tokens from which \'\n                  \'to build a vocabulary. It will be used when strings are encoded \'\n                  \'with a SubwordTextTokenizer subclass. The encoder was coded by a coder.\')\n        alphabet = set(corpus) ^ {\' \'}\n\n        original = \'This is a coded sentence encoded by the SubwordTextTokenizer.\'\n\n        encoder = SubwordTextTokenizer.build_to_target_size_from_corpus([corpus, original],\n                                                                        target_size=100,\n                                                                        min_val=2,\n                                                                        max_val=10)\n\n        # Encoding should be reversible.\n        encoded = encoder.encode(original)\n        decoded = encoder.decode(encoded)\n        self.assertEqual(original, decoded)\n\n        # The substrings coded and coder are frequent enough in the corpus that\n        # they should appear in the vocabulary even though they are substrings\n        # of other included strings.\n        subtoken_strings = encoded\n        self.assertIn(\'encoded_\', subtoken_strings)\n        self.assertIn(\'coded_\', subtoken_strings)\n        self.assertIn(\'SubwordTextTokenizer_\', encoder._all_subtoken_strings)\n        self.assertIn(\'coder_\', encoder._all_subtoken_strings)\n\n        # Every character in the corpus should be in the encoder\'s alphabet and\n        # its subtoken vocabulary.\n        self.assertTrue(alphabet.issubset(encoder._alphabet))\n        for a in alphabet:\n            self.assertIn(a, encoder._all_subtoken_strings)\n\n    def test_unicode(self):\n        corpus = \'Cat emoticons. \\U0001F638 \\U0001F639 \\U0001F63A \\U0001F63B\'\n        token_counts = collections.Counter(corpus.split(\' \'))\n\n        encoder = SubwordTextTokenizer.build_to_target_size_from_token_counts(\n            100, token_counts, 2, 10)\n\n        self.assertIn(\'\\U0001F638\', encoder._alphabet)\n        self.assertIn(\'\\U0001F63B\', encoder._all_subtoken_strings)\n\n    def test_small_vocab(self):\n        corpus = \'The quick brown fox jumps over the lazy dog\'\n        token_counts = collections.Counter(corpus.split(\' \'))\n        alphabet = set(corpus) ^ {\' \'}\n\n        encoder = SubwordTextTokenizer.build_to_target_size_from_token_counts(\n            10, token_counts, 2, 10)\n\n        # All vocabulary elements are in the alphabet and subtoken strings even\n        # if we requested a smaller vocabulary to assure all expected strings\n        # are encodable.\n        self.assertTrue(alphabet.issubset(encoder._alphabet))\n        for a in alphabet:\n            self.assertIn(a, encoder._all_subtoken_strings)\n\n    def test_encodable_when_not_in_alphabet(self):\n        corpus = \'the quick brown fox jumps over the lazy dog\'\n        token_counts = collections.Counter(corpus.split(\' \'))\n\n        encoder = SubwordTextTokenizer.build_to_target_size_from_token_counts(\n            100, token_counts, 2, 10)\n        original = \'This has UPPER CASE letters that are out of alphabet\'\n\n        # Early versions could have an infinite loop when breaking into subtokens\n        # if there was any out-of-alphabet characters in the encoded string.\n        encoded = encoder.encode(original)\n        decoded = encoder.decode(encoded)\n\n        self.assertEqual(original, decoded)\n        encoded_str = \'\'.join(encoded)\n        self.assertIn(\'\\\\84;\', encoded_str)\n\n    @mock.patch.object(subword_text_tokenizer, \'_ESCAPE_CHARS\', new=set(\'\\\\_;13579\'))\n    def test_raises_exception_when_not_encodable(self):\n        corpus = \'the quick brown fox jumps over the lazy dog\'\n        token_counts = collections.Counter(corpus.split(\' \'))\n\n        # Deliberately exclude some required encoding chars from the alphabet\n        # and token list, making some strings unencodable.\n        encoder = SubwordTextTokenizer.build_to_target_size_from_token_counts(\n            100, token_counts, 2, 10)\n        original = \'This has UPPER CASE letters that are out of alphabet\'\n\n        # Previously there was a bug which produced an infinite loop in this case.\n        with self.assertRaises(AssertionError):\n            encoder.encode(original)\n\n\ndef test_is_pickleable():\n    tokenizer = SubwordTextTokenizer()\n    pickle.dumps(tokenizer)\n'"
tests/encoders/text/test_text_encoder.py,6,"b""import pytest\nimport torch\n\nfrom torchnlp.encoders.text import DEFAULT_PADDING_INDEX\nfrom torchnlp.encoders.text import stack_and_pad_tensors\nfrom torchnlp.encoders.text import pad_tensor\n\n\ndef test_pad_tensor():\n    padded = pad_tensor(torch.LongTensor([1, 2, 3]), 5, DEFAULT_PADDING_INDEX)\n    assert padded.tolist() == [1, 2, 3, DEFAULT_PADDING_INDEX, DEFAULT_PADDING_INDEX]\n\n\ndef test_pad_tensor_multiple_dim():\n    padded = pad_tensor(torch.LongTensor(1, 2, 3), 5, DEFAULT_PADDING_INDEX)\n    assert padded.size() == (5, 2, 3)\n    assert padded[1].sum().item() == pytest.approx(0)\n\n\ndef test_pad_tensor_multiple_dim_float_tensor():\n    padded = pad_tensor(torch.FloatTensor(778, 80), 804, DEFAULT_PADDING_INDEX)\n    assert padded.size() == (804, 80)\n    assert padded[-1].sum().item() == pytest.approx(0)\n    assert padded.type() == 'torch.FloatTensor'\n\n\ndef test_stack_and_pad_tensors():\n    batch = [torch.LongTensor([1, 2, 3]), torch.LongTensor([1, 2]), torch.LongTensor([1])]\n    padded, lengths = stack_and_pad_tensors(batch, DEFAULT_PADDING_INDEX)\n    padded = [r.tolist() for r in padded]\n    assert padded == [[1, 2, 3], [1, 2, DEFAULT_PADDING_INDEX],\n                      [1, DEFAULT_PADDING_INDEX, DEFAULT_PADDING_INDEX]]\n    assert lengths.tolist() == [3, 2, 1]\n\n\ndef test_stack_and_pad_tensors__dim():\n    batch_size = 3\n    batch = [torch.LongTensor([1, 2, 3, 4]), torch.LongTensor([1, 2, 3]), torch.LongTensor([1, 2])]\n    padded, lengths = stack_and_pad_tensors(batch, DEFAULT_PADDING_INDEX, dim=1)\n    assert padded.shape == (4, batch_size)\n    assert lengths.shape == (1, batch_size)\n    assert lengths.tolist() == [[4, 3, 2]]\n    assert padded.tolist() == [[1, 1, 1], [2, 2, 2], [3, 3, DEFAULT_PADDING_INDEX],\n                               [4, DEFAULT_PADDING_INDEX, DEFAULT_PADDING_INDEX]]\n"""
tests/encoders/text/test_treebank_encoder.py,0,"b'import pickle\n\nimport pytest\n\nfrom torchnlp.encoders.text import TreebankEncoder\n\n\n@pytest.fixture\ndef input_():\n    return \'\'\'Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.\'\'\'\n\n\n@pytest.fixture\ndef encoder(input_):\n    return TreebankEncoder([input_])\n\n\ndef test_treebank_encoder(encoder, input_):\n    # TEST adapted from example in http://www.nltk.org/_modules/nltk/tokenize/treebank.html\n    expected_tokens = [\n        \'Good\', \'muffins\', \'cost\', \'$\', \'3.88\', \'in\', \'New\', \'York.\', \'Please\', \'buy\', \'me\', \'two\',\n        \'of\', \'them.\', \'Thanks\', \'.\'\n    ]\n    expected_decode = ""Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.""\n    tokens = encoder.encode(input_)\n    assert [encoder.index_to_token[i] for i in tokens] == expected_tokens\n    assert encoder.decode(tokens) == expected_decode\n\n\ndef test_is_pickleable(encoder):\n    pickle.dumps(encoder)\n'"
tests/encoders/text/test_word_encoder.py,0,"b""import pickle\n\nimport pytest\n\nfrom torchnlp.encoders.text import WhitespaceEncoder\n\n\n@pytest.fixture\ndef input_():\n    return 'This is a sentence'\n\n\n@pytest.fixture\ndef encoder(input_):\n    return WhitespaceEncoder([input_])\n\n\ndef test_whitespace_encoder(encoder, input_):\n    tokens = encoder.encode(input_)\n    assert encoder.decode(tokens) == input_\n\n\ndef test_is_pickleable(encoder):\n    pickle.dumps(encoder)\n"""
torchnlp/encoders/text/__init__.py,0,"b""from torchnlp.encoders.text.character_encoder import CharacterEncoder\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_COPY_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_COPY_TOKEN\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_EOS_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_EOS_TOKEN\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_TOKEN\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_RESERVED_TOKENS\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_SOS_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_SOS_TOKEN\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_UNKNOWN_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_UNKNOWN_TOKEN\nfrom torchnlp.encoders.text.delimiter_encoder import DelimiterEncoder\nfrom torchnlp.encoders.text.moses_encoder import MosesEncoder\nfrom torchnlp.encoders.text.spacy_encoder import SpacyEncoder\nfrom torchnlp.encoders.text.static_tokenizer_encoder import StaticTokenizerEncoder\nfrom torchnlp.encoders.text.subword_encoder import SubwordEncoder\nfrom torchnlp.encoders.text.text_encoder import BatchedSequences\nfrom torchnlp.encoders.text.text_encoder import pad_tensor\nfrom torchnlp.encoders.text.text_encoder import stack_and_pad_tensors\nfrom torchnlp.encoders.text.text_encoder import TextEncoder\nfrom torchnlp.encoders.text.treebank_encoder import TreebankEncoder\nfrom torchnlp.encoders.text.whitespace_encoder import WhitespaceEncoder\n\n__all__ = [\n    'CharacterEncoder', 'DEFAULT_COPY_INDEX', 'DEFAULT_COPY_TOKEN', 'DEFAULT_EOS_INDEX',\n    'DEFAULT_EOS_TOKEN', 'DEFAULT_PADDING_INDEX', 'DEFAULT_PADDING_TOKEN',\n    'DEFAULT_RESERVED_TOKENS', 'DEFAULT_SOS_INDEX', 'DEFAULT_SOS_TOKEN', 'DEFAULT_UNKNOWN_INDEX',\n    'DEFAULT_UNKNOWN_TOKEN', 'DelimiterEncoder', 'MosesEncoder', 'pad_tensor',\n    'stack_and_pad_tensors', 'TextEncoder', 'SpacyEncoder', 'StaticTokenizerEncoder',\n    'SubwordEncoder', 'TreebankEncoder', 'WhitespaceEncoder', 'BatchedSequences'\n]\n"""
torchnlp/encoders/text/character_encoder.py,0,"b'from torchnlp.encoders.text.static_tokenizer_encoder import StaticTokenizerEncoder\n\n\ndef _tokenize(s):\n    return list(s)\n\n\ndef _detokenize(s):\n    return \'\'.join(s)\n\n\nclass CharacterEncoder(StaticTokenizerEncoder):\n    """""" Encodes text into a tensor by splitting the text into individual characters.\n\n    Args:\n        **args: Arguments passed onto ``StaticTokenizerEncoder.__init__``.\n        **kwargs: Keyword arguments passed onto ``StaticTokenizerEncoder.__init__``.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        if \'tokenize\' in kwargs:\n            raise TypeError(\'``CharacterEncoder`` does not take keyword argument ``tokenize``.\')\n\n        if \'detokenize\' in kwargs:\n            raise TypeError(\'``CharacterEncoder`` does not take keyword argument ``detokenize``.\')\n\n        super().__init__(*args, tokenize=_tokenize, detokenize=_detokenize, **kwargs)\n'"
torchnlp/encoders/text/default_reserved_tokens.py,0,"b'"""""" Default reserved tokens\n\nThese tokens are used as global default parameters in this library; therefore, any method using\nthese will be consistent but also allow for customization.\n""""""\nDEFAULT_PADDING_INDEX = 0\nDEFAULT_UNKNOWN_INDEX = 1\nDEFAULT_EOS_INDEX = 2\nDEFAULT_SOS_INDEX = 3\nDEFAULT_COPY_INDEX = 4\nDEFAULT_PADDING_TOKEN = \'<pad>\'\nDEFAULT_UNKNOWN_TOKEN = \'<unk>\'\nDEFAULT_EOS_TOKEN = \'</s>\'\nDEFAULT_SOS_TOKEN = \'<s>\'\nDEFAULT_COPY_TOKEN = \'<copy>\'\nDEFAULT_RESERVED_TOKENS = [\n    DEFAULT_PADDING_TOKEN, DEFAULT_UNKNOWN_TOKEN, DEFAULT_EOS_TOKEN, DEFAULT_SOS_TOKEN,\n    DEFAULT_COPY_TOKEN\n]\n\nDEFAULT_RESERVED_INDEX_TO_TOKEN = DEFAULT_RESERVED_TOKENS\nDEFAULT_RESERVED_TOKEN_TO_INDEX = {\n    token: index for index, token in enumerate(DEFAULT_RESERVED_INDEX_TO_TOKEN)\n}\n'"
torchnlp/encoders/text/delimiter_encoder.py,0,"b'from functools import partial\n\nfrom torchnlp.encoders.text.static_tokenizer_encoder import StaticTokenizerEncoder\n\n\ndef _tokenize(s, delimiter):\n    return s.split(delimiter) if len(s) > 0 else []\n\n\ndef _detokenize(s, delimiter):\n    return delimiter.join(s)\n\n\nclass DelimiterEncoder(StaticTokenizerEncoder):\n    """""" Encodes text into a tensor by splitting the text using a delimiter.\n\n    Args:\n        delimiter (string): Delimiter used with ``string.split``.\n        **args: Arguments passed onto ``StaticTokenizerEncoder.__init__``.\n        **kwargs: Keyword arguments passed onto ``StaticTokenizerEncoder.__init__``.\n\n    Example:\n\n        >>> encoder = DelimiterEncoder(\'|\', [\'token_a|token_b\', \'token_c\'])\n        >>> encoder.encode(\'token_a|token_c\')\n        tensor([5, 7])\n        >>> encoder.vocab\n        [\'<pad>\', \'<unk>\', \'</s>\', \'<s>\', \'<copy>\', \'token_a\', \'token_b\', \'token_c\']\n\n    """"""\n\n    def __init__(self, delimiter, *args, **kwargs):\n        if \'tokenize\' in kwargs:\n            raise TypeError(\'``DelimiterEncoder`` does not take keyword argument ``tokenize``.\')\n\n        if \'detokenize\' in kwargs:\n            raise TypeError(\'``DelimiterEncoder`` does not take keyword argument ``detokenize``.\')\n\n        self.delimiter = delimiter\n\n        super().__init__(\n            *args,\n            tokenize=partial(_tokenize, delimiter=self.delimiter),\n            detokenize=partial(_detokenize, delimiter=self.delimiter),\n            **kwargs)\n'"
torchnlp/encoders/text/moses_encoder.py,0,"b'from functools import partial\n\nfrom torchnlp.encoders.text.static_tokenizer_encoder import StaticTokenizerEncoder\n\n\nclass MosesEncoder(StaticTokenizerEncoder):\n    """""" Encodes the text using the Moses tokenizer.\n\n    **Tokenizer Reference:**\n    http://www.nltk.org/_modules/nltk/tokenize/moses.html\n\n    Args:\n        **args: Arguments passed onto ``StaticTokenizerEncoder.__init__``.\n        **kwargs: Keyword arguments passed onto ``StaticTokenizerEncoder.__init__``.\n\n    NOTE: The `doctest` is skipped because running NLTK moses with Python 3.7\'s pytest halts on\n    travis.\n\n    Example:\n\n        >>> encoder = MosesEncoder([""This ain\'t funny."", ""Don\'t?""]) # doctest: +SKIP\n        >>> encoder.encode(""This ain\'t funny."") # doctest: +SKIP\n        tensor([5, 6, 7, 8, 9])\n        >>> encoder.vocab # doctest: +SKIP\n        [\'<pad>\', \'<unk>\', \'</s>\', \'<s>\', \'<copy>\', \'This\', \'ain\', \'&apos;t\', \'funny\', \'.\', \\\n\'Don\', \'?\']\n        >>> encoder.decode(encoder.encode(""This ain\'t funny."")) # doctest: +SKIP\n        ""This ain\'t funny.""\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        if \'tokenize\' in kwargs:\n            raise TypeError(\'``MosesEncoder`` does not take keyword argument ``tokenize``.\')\n\n        if \'detokenize\' in kwargs:\n            raise TypeError(\'``MosesEncoder`` does not take keyword argument ``detokenize``.\')\n\n        try:\n            from sacremoses import MosesTokenizer\n            from sacremoses import MosesDetokenizer\n        except ImportError:\n            print(""Please install SacreMoses. ""\n                  ""See the docs at https://github.com/alvations/sacremoses for more information."")\n            raise\n\n        super().__init__(\n            *args,\n            tokenize=MosesTokenizer().tokenize,\n            detokenize=partial(MosesDetokenizer().detokenize, return_str=True),\n            **kwargs)\n'"
torchnlp/encoders/text/spacy_encoder.py,0,"b'from functools import partial\n\nfrom torchnlp.encoders.text.static_tokenizer_encoder import StaticTokenizerEncoder\n\n\ndef _tokenize(s, tokenizer):\n    return [w.text for w in tokenizer(s)]\n\n\nclass SpacyEncoder(StaticTokenizerEncoder):\n    """""" Encodes the text using spaCy\'s tokenizer.\n\n    **Tokenizer Reference:**\n    https://spacy.io/api/tokenizer\n\n    Args:\n        **args: Arguments passed onto ``StaticTokenizerEncoder.__init__``.\n        language (string, optional): Language to use for parsing. Accepted values\n            are \'en\', \'de\', \'es\', \'pt\', \'fr\', \'it\', \'nl\' and \'xx\'.\n            For details see https://spacy.io/models/#available-models\n        **kwargs: Keyword arguments passed onto ``StaticTokenizerEncoder.__init__``.\n    Example:\n\n        >>> encoder = SpacyEncoder([""This ain\'t funny."", ""Don\'t?""])\n        >>> encoder.encode(""This ain\'t funny."")\n        tensor([5, 6, 7, 8, 9])\n        >>> encoder.vocab\n        [\'<pad>\', \'<unk>\', \'</s>\', \'<s>\', \'<copy>\', \'This\', \'ai\', ""n\'t"", \'funny\', \'.\', \'Do\', \'?\']\n        >>> encoder.decode(encoder.encode(""This ain\'t funny.""))\n        ""This ai n\'t funny .""\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        if \'tokenize\' in kwargs:\n            raise TypeError(\'``SpacyEncoder`` does not take keyword argument ``tokenize``.\')\n\n        try:\n            import spacy\n        except ImportError:\n            print(""Please install spaCy: "" ""`pip install spacy`"")\n            raise\n\n        # Use English as default when no language was specified\n        language = kwargs.get(\'language\', \'en\')\n\n        # All languages supported by spaCy can be found here:\n        #   https://spacy.io/models/#available-models\n        supported_languages = [\'en\', \'de\', \'es\', \'pt\', \'fr\', \'it\', \'nl\', \'xx\']\n\n        if language in supported_languages:\n            # Load the spaCy language model if it has been installed\n            try:\n                self.spacy = spacy.load(language, disable=[\'parser\', \'tagger\', \'ner\'])\n            except OSError:\n                raise ValueError((""Language \'{0}\' not found. Install using ""\n                                  ""spaCy: `python -m spacy download {0}`"").format(language))\n        else:\n            raise ValueError(\n                (""No tokenizer available for language \'%s\'. "" + ""Currently supported are %s"") %\n                (language, supported_languages))\n\n        super().__init__(*args, tokenize=partial(_tokenize, tokenizer=self.spacy), **kwargs)\n\n    def batch_encode(self, sequences):\n        # Batch tokenization is handled by ``self.spacy.pipe``\n        original = self.tokenize\n        self.tokenize = lambda sequence: [token.text for token in sequence]\n        return_ = super().batch_encode(self.spacy.pipe(sequences))\n        self.tokenize = original\n        return return_\n'"
torchnlp/encoders/text/static_tokenizer_encoder.py,3,"b'from collections import Counter\nfrom collections.abc import Iterable\n\nimport torch\n\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_EOS_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_RESERVED_TOKENS\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_UNKNOWN_INDEX\nfrom torchnlp.encoders.text.text_encoder import TextEncoder\n\n\ndef _tokenize(s):\n    return s.split()\n\n\ndef _detokenize(t):\n    return \' \'.join(t)\n\n\nclass StaticTokenizerEncoder(TextEncoder):\n    """""" Encodes a text sequence using a static tokenizer.\n\n    Args:\n        sample (collections.abc.Iterable): Sample of data used to build encoding dictionary.\n        min_occurrences (int, optional): Minimum number of occurrences for a token to be added to\n          the encoding dictionary.\n        tokenize (callable): :class:`callable` to tokenize a sequence.\n        detokenize (callable): :class:`callable` to detokenize a sequence.\n        append_eos (bool, optional): If ``True`` append EOS token onto the end to the encoded\n          vector.\n        reserved_tokens (list of str, optional): List of reserved tokens inserted in the beginning\n            of the dictionary.\n        eos_index (int, optional): The eos token is used to encode the end of a sequence. This is\n          the index that token resides at.\n        unknown_index (int, optional): The unknown token is used to encode unseen tokens. This is\n          the index that token resides at.\n        padding_index (int, optional): The unknown token is used to encode sequence padding. This is\n          the index that token resides at.\n        **kwargs: Keyword arguments passed onto ``TextEncoder.__init__``.\n\n    Example:\n\n        >>> sample = [""This ain\'t funny."", ""Don\'t?""]\n        >>> encoder = StaticTokenizerEncoder(sample, tokenize=lambda s: s.split())\n        >>> encoder.encode(""This ain\'t funny."")\n        tensor([5, 6, 7])\n        >>> encoder.vocab\n        [\'<pad>\', \'<unk>\', \'</s>\', \'<s>\', \'<copy>\', \'This\', ""ain\'t"", \'funny.\', ""Don\'t?""]\n        >>> encoder.decode(encoder.encode(""This ain\'t funny.""))\n        ""This ain\'t funny.""\n\n    """"""\n\n    def __init__(self,\n                 sample,\n                 min_occurrences=1,\n                 append_eos=False,\n                 tokenize=_tokenize,\n                 detokenize=_detokenize,\n                 reserved_tokens=DEFAULT_RESERVED_TOKENS,\n                 eos_index=DEFAULT_EOS_INDEX,\n                 unknown_index=DEFAULT_UNKNOWN_INDEX,\n                 padding_index=DEFAULT_PADDING_INDEX,\n                 **kwargs):\n        super().__init__(**kwargs)\n\n        if not isinstance(sample, Iterable):\n            raise TypeError(\'Sample must be a `collections.abc.Iterable`.\')\n\n        self.eos_index = eos_index\n        self.unknown_index = unknown_index\n        self.padding_index = padding_index\n        self.reserved_tokens = reserved_tokens\n        self.tokenize = tokenize\n        self.detokenize = detokenize\n        self.append_eos = append_eos\n        self.tokens = Counter()\n\n        for sequence in sample:\n            self.tokens.update(self.tokenize(sequence))\n\n        self.index_to_token = reserved_tokens.copy()\n        self.token_to_index = {token: index for index, token in enumerate(reserved_tokens)}\n        for token, count in self.tokens.items():\n            if count >= min_occurrences:\n                self.index_to_token.append(token)\n                self.token_to_index[token] = len(self.index_to_token) - 1\n\n    @property\n    def vocab(self):\n        """"""\n        Returns:\n            list: List of tokens in the dictionary.\n        """"""\n        return self.index_to_token\n\n    @property\n    def vocab_size(self):\n        """"""\n        Returns:\n            int: Number of tokens in the dictionary.\n        """"""\n        return len(self.vocab)\n\n    def encode(self, sequence):\n        """""" Encodes a ``sequence``.\n\n        Args:\n            sequence (str): String ``sequence`` to encode.\n\n        Returns:\n            torch.Tensor: Encoding of the ``sequence``.\n        """"""\n        sequence = super().encode(sequence)\n        sequence = self.tokenize(sequence)\n        vector = [self.token_to_index.get(token, self.unknown_index) for token in sequence]\n        if self.append_eos:\n            vector.append(self.eos_index)\n        return torch.tensor(vector, dtype=torch.long)\n\n    def decode(self, encoded):\n        """""" Decodes a tensor into a sequence.\n\n        Args:\n            encoded (torch.Tensor): Encoded sequence.\n\n        Returns:\n            str: Sequence decoded from ``encoded``.\n        """"""\n        encoded = super().decode(encoded)\n        tokens = [self.index_to_token[index] for index in encoded]\n        return self.detokenize(tokens)\n'"
torchnlp/encoders/text/subword_encoder.py,3,"b'import torch\n\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_EOS_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_RESERVED_TOKENS\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_UNKNOWN_INDEX\nfrom torchnlp.encoders.text.text_encoder import TextEncoder\nfrom torchnlp.encoders.text.subword_text_tokenizer import SubwordTextTokenizer\n\n\nclass SubwordEncoder(TextEncoder):\n    """""" Invertibly encoding text using a limited vocabulary.\n\n    Applies Googles Tensor2Tensor ``SubwordTextTokenizer`` that invertibly encodes a native string\n    as a\n    sequence of subtokens from a limited vocabulary. In order to build the vocabulary, it uses\n    recursive binary search to find a minimum token count `x`\n    (s.t. `min_occurrences` <= `x` <= `max_occurrences`) that most closely matches the\n    `target_size`.\n\n    **Tokenizer Reference:**\n    https://github.com/tensorflow/tensor2tensor/blob/8bdecbe434d93cb1e79c0489df20fee2d5a37dc2/tensor2tensor/data_generators/text_encoder.py#L389\n\n    Args:\n        sample (list): Sample of data used to build encoding dictionary.\n        append_eos (bool, optional): If ``True`` append EOS token onto the end to the encoded\n            vector.\n        target_vocab_size (int, optional): Desired size of vocab.\n        min_occurrences (int, optional): Lower bound for the minimum token count.\n        max_occurrences (int, optional): Upper bound for the minimum token count.\n        reserved_tokens (list of str, optional): List of reserved tokens inserted in the beginning\n            of the dictionary.\n        eos_index (int, optional): The eos token is used to encode the end of a sequence. This is\n          the index that token resides at.\n        unknown_index (int, optional): The unknown token is used to encode unseen tokens. This is\n          the index that token resides at.\n        padding_index (int, optional): The padding token is used to encode sequence padding. This is\n          the index that token resides at.\n        **kwargs: Keyword arguments passed onto ``TextEncoder.__init__``.\n    """"""\n\n    def __init__(self,\n                 sample,\n                 append_eos=False,\n                 target_vocab_size=None,\n                 min_occurrences=1,\n                 max_occurrences=1e3,\n                 reserved_tokens=DEFAULT_RESERVED_TOKENS,\n                 eos_index=DEFAULT_EOS_INDEX,\n                 unknown_index=DEFAULT_UNKNOWN_INDEX,\n                 padding_index=DEFAULT_PADDING_INDEX,\n                 **kwargs):\n        super().__init__(**kwargs)\n\n        self.append_eos = append_eos\n        self.eos_index = eos_index\n        self.unknown_index = unknown_index\n        self.reserved_tokens = reserved_tokens\n        self.padding_index = padding_index\n\n        if target_vocab_size is None:\n            self.tokenizer = SubwordTextTokenizer()\n            self.tokenizer.build_from_corpus(sample, min_count=min_occurrences)\n        else:\n\n            target_vocab_size -= len(reserved_tokens)\n            self.tokenizer = SubwordTextTokenizer.build_to_target_size_from_corpus(\n                sample,\n                target_size=target_vocab_size,\n                min_val=min_occurrences,\n                max_val=max_occurrences)\n\n        self.index_to_token = reserved_tokens.copy()\n        self.token_to_index = {token: index for index, token in enumerate(reserved_tokens)}\n        for token in self.tokenizer.vocab:\n            self.index_to_token.append(token)\n            self.token_to_index[token] = len(self.index_to_token) - 1\n\n    @property\n    def vocab(self):\n        """"""\n        Returns:\n            list: List of tokens in the dictionary.\n        """"""\n        return self.index_to_token\n\n    @property\n    def vocab_size(self):\n        """"""\n        Returns:\n            int: Number of tokens in the dictionary.\n        """"""\n        return len(self.vocab)\n\n    def encode(self, sequence):\n        """""" Encodes a ``sequence``.\n\n        Args:\n            sequence (str): String ``sequence`` to encode.\n\n        Returns:\n            torch.Tensor: Encoding of the ``sequence``.\n        """"""\n        sequence = super().encode(sequence)\n        sequence = self.tokenizer.encode(sequence)\n        vector = [self.token_to_index.get(token, self.unknown_index) for token in sequence]\n        if self.append_eos:\n            vector.append(self.eos_index)\n        return torch.tensor(vector, dtype=torch.long)\n\n    def decode(self, encoded):\n        """""" Decodes a tensor into a sequence.\n\n        Args:\n            encoded (torch.Tensor): Encoded sequence.\n\n        Returns:\n            str: Sequence decoded from ``encoded``.\n        """"""\n        encoded = super().decode(encoded)\n        return self.tokenizer.decode([self.index_to_token[index] for index in encoded])\n'"
torchnlp/encoders/text/subword_text_tokenizer.py,0,"b'# coding=utf-8\n# Copyright 2017 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom functools import lru_cache\n\nimport collections\nimport logging\nimport re\nimport sys\nimport unicodedata\n\n# Dependency imports\nfrom torchnlp._third_party.lazy_loader import LazyLoader\n\nsix = LazyLoader(\'six\', globals(), \'six\')\n\nlogger = logging.getLogger(__name__)\n\n\n@lru_cache()\ndef get_alphanumeric_char_set():\n    """""" This set contains all letter and number characters. """"""\n    return set(\n        six.unichr(i)\n        for i in six.moves.xrange(sys.maxunicode)\n        if (unicodedata.category(six.unichr(i)).startswith(""L"") or\n            unicodedata.category(six.unichr(i)).startswith(""N"")))\n\n\n# Regular expression for unescaping token strings.\n# \'\\u\' is converted to \'_\'\n# \'\\\\\' is converted to \'\\\'\n# \'\\213;\' is converted to unichr(213)\n_UNESCAPE_REGEX = re.compile(r""\\\\u|\\\\\\\\|\\\\([0-9]+);"")\n_ESCAPE_CHARS = set(u""\\\\_u;0123456789"")\n\n\n# Conversion between Unicode and UTF-8, if required (on Python2)\ndef native_to_unicode(s):\n    if six.PY2:\n        return s if isinstance(s, unicode) else s.decode(""utf8"")  # noqa: F821\n    else:\n        return s\n\n\ndef unicode_to_native(s):\n    if six.PY2:\n        return s.encode(""utf-8"")\n    else:\n        return s\n\n\ndef encode(text):\n    """"""\n    Encode a unicode string as a list of tokens.\n    Args:\n      text: a unicode string\n    Returns:\n      a list of tokens as Unicode strings\n    """"""\n    if not text:\n        return []\n    ret = []\n    token_start = 0\n    # Classify each character in the input string\n    is_alnum = [c in get_alphanumeric_char_set() for c in text]\n    for pos in six.moves.xrange(1, len(text)):\n        if is_alnum[pos] != is_alnum[pos - 1]:\n            token = text[token_start:pos]\n            if token != u"" "" or token_start == 0:\n                ret.append(token)\n            token_start = pos\n    final_token = text[token_start:]\n    ret.append(final_token)\n    return ret\n\n\ndef decode(tokens):\n    """"""\n    Decode a list of tokens to a unicode string.\n    Args:\n      tokens: a list of Unicode strings\n    Returns:\n      a unicode string\n    """"""\n    token_is_alnum = [t[0] in get_alphanumeric_char_set() for t in tokens]\n    ret = []\n    for i, token in enumerate(tokens):\n        if i > 0 and token_is_alnum[i - 1] and token_is_alnum[i]:\n            ret.append(u"" "")\n        ret.append(token)\n    return """".join(ret)\n\n\ndef _escape_token(token, alphabet):\n    """"""\n    Escape away underscores and OOV characters and append \'_\'.\n    This allows the token to be experessed as the concatenation of a list\n    of subtokens from the vocabulary. The underscore acts as a sentinel\n    which allows us to invertibly concatenate multiple such lists.\n    Args:\n      token: A unicode string to be escaped.\n      alphabet: A set of all characters in the vocabulary\'s alphabet.\n    Returns:\n      escaped_token: An escaped unicode string.\n    Raises:\n      ValueError: If the provided token is not unicode.\n    """"""\n    if not isinstance(token, six.text_type):\n        raise ValueError(""Expected string type for token, got %s"" % type(token))\n\n    token = token.replace(u""\\\\"", u""\\\\\\\\"").replace(u""_"", u""\\\\u"")\n    ret = [c if c in alphabet and c != u""\\n"" else r""\\%d;"" % ord(c) for c in token]\n    return u"""".join(ret) + ""_""\n\n\ndef _unescape_token(escaped_token):\n    """"""\n    Inverse of _escape_token().\n    Args:\n      escaped_token: a unicode string\n    Returns:\n      token: a unicode string\n    """"""\n\n    def match(m):\n        if m.group(1) is None:\n            return u""_"" if m.group(0) == u""\\\\u"" else u""\\\\""\n\n        try:\n            return six.unichr(int(m.group(1)))\n        except (ValueError, OverflowError):\n            return """"\n\n    trimmed = escaped_token[:-1] if escaped_token.endswith(""_"") else escaped_token\n    return _UNESCAPE_REGEX.sub(match, trimmed)\n\n\nclass SubwordTextTokenizer(object):\n    """""" Class for invertibly encoding text using a limited vocabulary.\n\n    Invertibly encodes a native string as a sequence of subtokens from a limited\n    vocabulary.\n\n    A SubwordTextTokenizer is built from a corpus (so it is tailored to the text in\n    the corpus), and stored to a file. See text_encoder_build_subword.py.\n    It can then be loaded and used to encode/decode any text.\n\n    Encoding has four phases:\n        1.  Tokenize into a list of tokens.  Each token is a unicode string of either\n            all alphanumeric characters or all non-alphanumeric characters.  We drop\n            tokens consisting of a single space that are between two alphanumeric\n            tokens.\n        2.  Escape each token.  This escapes away special and out-of-vocabulary\n            characters, and makes sure that each token ends with an underscore, and\n            has no other underscores.\n        3.  Represent each escaped token as a the concatenation of a list of subtokens\n            from the limited vocabulary.  Subtoken selection is done greedily from\n            beginning to end.  That is, we construct the list in order, always picking\n            the longest subtoken in our vocabulary that matches a prefix of the\n            remaining portion of the encoded token.\n        4.  Concatenate these lists.  This concatenation is invertible due to the\n            fact that the trailing underscores indicate when one list is finished.\n    """"""\n\n    def __init__(self):\n        """"""Initialize and read from a file, if provided.""""""\n        self._alphabet = set()\n\n    def encode(self, raw_text):\n        """"""Converts a native string to a list of subtoken.\n\n        Args:\n          raw_text: a native string.\n        Returns:\n          a list of integers in the range [0, vocab_size)\n        """"""\n        return self._tokens_to_subtoken(encode(native_to_unicode(raw_text)))\n\n    def decode(self, subtokens):\n        """"""Converts a sequence of subtoken to a native string.\n\n        Args:\n          subtokens: a list of integers in the range [0, vocab_size)\n        Returns:\n          a native string\n        """"""\n        return unicode_to_native(decode(self._subtoken_to_tokens(subtokens)))\n\n    @property\n    def vocab(self):\n        return self._all_subtoken_strings\n\n    @property\n    def vocab_size(self):\n        return len(self._all_subtoken_strings)\n\n    def _tokens_to_subtoken(self, tokens):\n        """""" Converts a list of tokens to a list of subtoken.\n\n        Args:\n          tokens: a list of strings.\n        Returns:\n          a list of integers in the range [0, vocab_size)\n        """"""\n        ret = []\n        for token in tokens:\n            ret.extend(\n                self._escaped_token_to_subtoken_strings(_escape_token(token, self._alphabet)))\n        return ret\n\n    def _subtoken_to_tokens(self, subtokens):\n        """""" Converts a list of subtoken to a list of tokens.\n\n        Args:\n          subtokens: a list of integers in the range [0, vocab_size)\n\n        Returns:\n          a list of strings.\n        """"""\n        concatenated = """".join(subtokens)\n        split = concatenated.split(""_"")\n        return [_unescape_token(t + ""_"") for t in split if t]\n\n    def _escaped_token_to_subtoken_strings(self, escaped_token):\n        """""" Converts an escaped token string to a list of subtoken strings.\n\n        Args:\n          escaped_token: An escaped token as a unicode string.\n        Returns:\n          A list of subtokens as unicode strings.\n        """"""\n        # NOTE: This algorithm is greedy; it won\'t necessarily produce the ""best""\n        # list of subtokens.\n        ret = []\n        start = 0\n        token_len = len(escaped_token)\n        while start < token_len:\n            for end in six.moves.xrange(min(token_len, start + self._max_subtoken_len), start, -1):\n                subtoken = escaped_token[start:end]\n                if subtoken in self._all_subtoken_strings:\n                    ret.append(subtoken)\n                    start = end\n                    break\n\n            else:  # Did not break\n                # If there is no possible encoding of the escaped token then one of the\n                # characters in the token is not in the alphabet. This should be\n                # impossible and would be indicative of a bug.\n                assert False, ""Token substring not found in subtoken vocabulary.""\n\n        return ret\n\n    @classmethod\n    def _count_tokens(cls, *sources):\n        token_counts = collections.Counter()\n        for corpus in sources:\n            for text in corpus:\n                token_counts.update(encode(text))\n        return token_counts\n\n    @classmethod\n    def build_to_target_size_from_corpus(cls,\n                                         *args,\n                                         target_size=32000,\n                                         min_val=1,\n                                         max_val=1e3,\n                                         num_iterations=4):\n        token_counts = SubwordTextTokenizer._count_tokens(*args)\n        return SubwordTextTokenizer.build_to_target_size_from_token_counts(\n            target_size, token_counts, min_val, max_val, num_iterations)\n\n    @classmethod\n    def build_to_target_size_from_token_counts(cls,\n                                               target_size,\n                                               token_counts,\n                                               min_val,\n                                               max_val,\n                                               num_iterations=4):\n        """"""Builds a SubwordTextTokenizer that has `vocab_size` near `target_size`.\n\n        Uses simple recursive binary search to find a minimum token count that most\n        closely matches the `target_size`.\n\n        Args:\n          target_size: Desired vocab_size to approximate.\n          token_counts: A dictionary of token counts, mapping string to int.\n          min_val: An integer; lower bound for the minimum token count.\n          max_val: An integer; upper bound for the minimum token count.\n          num_iterations: An integer; how many iterations of refinement.\n\n        Returns:\n          A SubwordTextTokenizer instance.\n\n        Raises:\n          ValueError: If `min_val` is greater than `max_val`.\n        """"""\n        if min_val > max_val:\n            raise ValueError(""Lower bound for the minimum token count ""\n                             ""is greater than the upper bound."")\n\n        def bisect(min_val, max_val):\n            """"""Bisection to find the right size.""""""\n            present_count = (max_val + min_val) // 2\n            logger.info(""Trying min_count %d"" % present_count)\n            subtokenizer = cls()\n            subtokenizer.build_from_token_counts(token_counts, present_count, num_iterations)\n            logger.info(""min_count %d attained a %d vocab_size"", present_count,\n                        subtokenizer.vocab_size)\n\n            # If min_val == max_val, we can\'t do any better than this.\n            if subtokenizer.vocab_size == target_size or min_val >= max_val:\n                return subtokenizer\n\n            if subtokenizer.vocab_size > target_size:\n                other_subtokenizer = bisect(present_count + 1, max_val)\n            else:\n                other_subtokenizer = bisect(min_val, present_count - 1)\n\n            if other_subtokenizer is None:\n                return subtokenizer\n\n            if (abs(other_subtokenizer.vocab_size - target_size) <\n                    abs(subtokenizer.vocab_size - target_size)):\n                return other_subtokenizer\n            return subtokenizer\n\n        return bisect(min_val, max_val)\n\n    def build_from_corpus(self, *corpuses, min_count=1, num_iterations=4):\n        token_counts = SubwordTextTokenizer._count_tokens(*corpuses)\n        return self.build_from_token_counts(token_counts, min_count, num_iterations)\n\n    def build_from_token_counts(self, token_counts, min_count, num_iterations=4):\n        """"""Train a SubwordTextTokenizer based on a dictionary of word counts.\n\n        Args:\n          token_counts: a dictionary of Unicode strings to int.\n          min_count: an integer - discard subtokens with lower counts.\n          num_iterations: an integer; how many iterations of refinement.\n        """"""\n        self._init_alphabet_from_tokens(six.iterkeys(token_counts))\n\n        # Bootstrap the initial list of subtokens with the characters from the\n        # alphabet plus the escaping characters.\n        self._init_subtokens_from_list(list(self._alphabet))\n\n        # We build iteratively.  On each iteration, we segment all the words,\n        # then count the resulting potential subtokens, keeping the ones\n        # with high enough counts for our new vocabulary.\n        if min_count < 1:\n            min_count = 1\n        for i in six.moves.xrange(num_iterations):\n\n            # Collect all substrings of the encoded token that break along current\n            # subtoken boundaries.\n            subtoken_counts = collections.defaultdict(int)\n            for token, count in six.iteritems(token_counts):\n                escaped_token = _escape_token(token, self._alphabet)\n                subtokens = self._escaped_token_to_subtoken_strings(escaped_token)\n                start = 0\n                for subtoken in subtokens:\n                    for end in six.moves.xrange(start + 1, len(escaped_token) + 1):\n                        new_subtoken = escaped_token[start:end]\n                        subtoken_counts[new_subtoken] += count\n                    start += len(subtoken)\n\n            # Array of sets of candidate subtoken strings, by length.\n            len_to_subtoken_strings = []\n            for subtoken_string, count in six.iteritems(subtoken_counts):\n                lsub = len(subtoken_string)\n                if count >= min_count:\n                    while len(len_to_subtoken_strings) <= lsub:\n                        len_to_subtoken_strings.append(set())\n                    len_to_subtoken_strings[lsub].add(subtoken_string)\n\n            # Consider the candidates longest to shortest, so that if we accept\n            # a longer subtoken string, we can decrement the counts of its\n            # prefixes.\n            new_subtoken_strings = []\n            for lsub in six.moves.xrange(len(len_to_subtoken_strings) - 1, 0, -1):\n                subtoken_strings = len_to_subtoken_strings[lsub]\n                for subtoken_string in subtoken_strings:\n                    count = subtoken_counts[subtoken_string]\n                    if count >= min_count:\n                        # Exclude alphabet tokens here, as they must be included later,\n                        # explicitly, regardless of count.\n                        if subtoken_string not in self._alphabet:\n                            new_subtoken_strings.append((count, subtoken_string))\n                        for l in six.moves.xrange(1, lsub):\n                            subtoken_counts[subtoken_string[:l]] -= count\n\n            # Include the alphabet explicitly to guarantee all strings are\n            # encodable.\n            new_subtoken_strings.extend((subtoken_counts.get(a, 0), a) for a in self._alphabet)\n            new_subtoken_strings.sort(reverse=True)\n\n            # Reinitialize to the candidate vocabulary.\n            self._init_subtokens_from_list([subtoken for _, subtoken in new_subtoken_strings])\n\n    def _init_subtokens_from_list(self, subtoken_strings):\n        """"""Initialize token information from a list of subtoken strings.""""""\n        # we remember the maximum length of any subtoken to avoid having to\n        # check arbitrarily long strings.\n        self._all_subtoken_strings = set([s for s in subtoken_strings if s])\n        self._max_subtoken_len = max([len(s) for s in subtoken_strings])\n\n    def _init_alphabet_from_tokens(self, tokens):\n        """"""Initialize alphabet from an iterable of token or subtoken strings.""""""\n        # Include all characters from all tokens in the alphabet to guarantee that\n        # any token can be encoded. Additionally, include all escaping\n        # characters.\n        self._alphabet = {c for token in tokens for c in token}\n        self._alphabet |= _ESCAPE_CHARS\n'"
torchnlp/encoders/text/text_encoder.py,11,"b'from collections import namedtuple\n\nimport torch\n\nfrom torchnlp.encoders import Encoder\nfrom torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\n\n\ndef pad_tensor(tensor, length, padding_index=DEFAULT_PADDING_INDEX):\n    """""" Pad a ``tensor`` to ``length`` with ``padding_index``.\n\n    Args:\n        tensor (torch.Tensor [n, ...]): Tensor to pad.\n        length (int): Pad the ``tensor`` up to ``length``.\n        padding_index (int, optional): Index to pad tensor with.\n\n    Returns\n        (torch.Tensor [length, ...]) Padded Tensor.\n    """"""\n    n_padding = length - tensor.shape[0]\n    assert n_padding >= 0\n    if n_padding == 0:\n        return tensor\n    padding = tensor.new(n_padding, *tensor.shape[1:]).fill_(padding_index)\n    return torch.cat((tensor, padding), dim=0)\n\n\nBatchedSequences = namedtuple(\'BatchedSequences\', [\'tensor\', \'lengths\'])\n\n\ndef stack_and_pad_tensors(batch, padding_index=DEFAULT_PADDING_INDEX, dim=0):\n    """""" Pad a :class:`list` of ``tensors`` (``batch``) with ``padding_index``.\n\n    Args:\n        batch (:class:`list` of :class:`torch.Tensor`): Batch of tensors to pad.\n        padding_index (int, optional): Index to pad tensors with.\n        dim (int, optional): Dimension on to which to concatenate the batch of tensors.\n\n    Returns\n        BatchedSequences(torch.Tensor, torch.Tensor): Padded tensors and original lengths of\n            tensors.\n    """"""\n    lengths = [tensor.shape[0] for tensor in batch]\n    max_len = max(lengths)\n    padded = [pad_tensor(tensor, max_len, padding_index) for tensor in batch]\n    lengths = torch.tensor(lengths, dtype=torch.long)\n    padded = torch.stack(padded, dim=dim).contiguous()\n    for _ in range(dim):\n        lengths = lengths.unsqueeze(0)\n\n    return BatchedSequences(padded, lengths)\n\n\nclass TextEncoder(Encoder):\n\n    def decode(self, encoded):\n        """""" Decodes an object.\n\n        Args:\n            object_ (object): Encoded object.\n\n        Returns:\n            object: Object decoded.\n        """"""\n        if self.enforce_reversible:\n            self.enforce_reversible = False\n            decoded_encoded = self.encode(self.decode(encoded))\n            self.enforce_reversible = True\n            if not torch.equal(decoded_encoded, encoded):\n                raise ValueError(\'Decoding is not reversible for ""%s""\' % encoded)\n\n        return encoded\n\n    def batch_encode(self, iterator, *args, dim=0, **kwargs):\n        """"""\n        Args:\n            iterator (iterator): Batch of text to encode.\n            *args: Arguments passed onto ``Encoder.__init__``.\n            dim (int, optional): Dimension along which to concatenate tensors.\n            **kwargs: Keyword arguments passed onto ``Encoder.__init__``.\n\n        Returns\n            torch.Tensor, torch.Tensor: Encoded and padded batch of sequences; Original lengths of\n                sequences.\n        """"""\n        return stack_and_pad_tensors(\n            super().batch_encode(iterator), padding_index=self.padding_index, dim=dim)\n\n    def batch_decode(self, tensor, lengths, dim=0, *args, **kwargs):\n        """"""\n        Args:\n            batch (list of :class:`torch.Tensor`): Batch of encoded sequences.\n            lengths (torch.Tensor): Original lengths of sequences.\n            dim (int, optional): Dimension along which to split tensors.\n            *args: Arguments passed to ``decode``.\n            **kwargs: Key word arguments passed to ``decode``.\n\n        Returns:\n            list: Batch of decoded sequences.\n        """"""\n        return super().batch_decode(\n            [t.squeeze(0)[:l] for t, l in zip(tensor.split(1, dim=dim), lengths)])\n'"
torchnlp/encoders/text/treebank_encoder.py,0,"b'from torchnlp.encoders.text.static_tokenizer_encoder import StaticTokenizerEncoder\n\n\nclass TreebankEncoder(StaticTokenizerEncoder):\n    """""" Encodes text using the Treebank tokenizer.\n\n    **Tokenizer Reference:**\n    http://www.nltk.org/_modules/nltk/tokenize/treebank.html\n\n    Args:\n        **args: Arguments passed onto ``StaticTokenizerEncoder.__init__``.\n        **kwargs: Keyword arguments passed onto ``StaticTokenizerEncoder.__init__``.\n\n    Example:\n\n        >>> encoder = TreebankEncoder([""This ain\'t funny."", ""Don\'t?""])\n        >>> encoder.encode(""This ain\'t funny."")\n        tensor([5, 6, 7, 8, 9])\n        >>> encoder.vocab\n        [\'<pad>\', \'<unk>\', \'</s>\', \'<s>\', \'<copy>\', \'This\', \'ai\', ""n\'t"", \'funny\', \'.\', \'Do\', \'?\']\n        >>> encoder.decode(encoder.encode(""This ain\'t funny.""))\n        ""This ain\'t funny.""\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        if \'tokenize\' in kwargs:\n            raise TypeError(\'``TreebankEncoder`` does not take keyword argument ``tokenize``.\')\n\n        if \'detokenize\' in kwargs:\n            raise TypeError(\'``TreebankEncoder`` does not take keyword argument ``detokenize``.\')\n\n        try:\n            import nltk\n\n            # Required for moses\n            nltk.download(\'perluniprops\')\n            nltk.download(\'nonbreaking_prefixes\')\n\n            from nltk.tokenize.treebank import TreebankWordTokenizer\n            from nltk.tokenize.treebank import TreebankWordDetokenizer\n        except ImportError:\n            print(""Please install NLTK. "" ""See the docs at http://nltk.org for more information."")\n            raise\n\n        super().__init__(\n            *args,\n            tokenize=TreebankWordTokenizer().tokenize,\n            detokenize=TreebankWordDetokenizer().detokenize,\n            **kwargs)\n'"
torchnlp/encoders/text/whitespace_encoder.py,0,"b'from torchnlp.encoders.text.delimiter_encoder import DelimiterEncoder\n\n\nclass WhitespaceEncoder(DelimiterEncoder):\n    """""" Encodes a text by splitting on whitespace.\n\n    Args:\n        **args: Arguments passed onto ``DelimiterEncoder.__init__``.\n        **kwargs: Keyword arguments passed onto ``DelimiterEncoder.__init__``.\n\n    Example:\n\n        >>> encoder = WhitespaceEncoder([""This ain\'t funny."", ""Don\'t?""])\n        >>> encoder.encode(""This ain\'t funny."")\n        tensor([5, 6, 7])\n        >>> encoder.vocab\n        [\'<pad>\', \'<unk>\', \'</s>\', \'<s>\', \'<copy>\', \'This\', ""ain\'t"", \'funny.\', ""Don\'t?""]\n        >>> encoder.decode(encoder.encode(""This ain\'t funny.""))\n        ""This ain\'t funny.""\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(\' \', *args, **kwargs)\n'"
