file_path,api_count,code
cli/setup.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""setup.py: setuptools control.""""""\n\nimport warnings\nimport re\nfrom setuptools import setup, find_packages\nimport sys\nimport os\n\n#if sys.version_info.major == 2:\n#    from urllib3 import disable_warnings\n#    disable_warnings()\n\n#with warnings.catch_warnings():\n#    if not sys.version_info.major == 3:\n    #    sys.exit(""\\n \\\n#        print(\'*********************************************************************\')\n#        print(\'* The CLI is primarly tested for Python 3 but Python 2 should work. *\')\n#        print(\'* If you see issues with Python 2, Please email help@pipeline.ai    *\')\n#        print(\'*********************************************************************\')\n\n# NOTE: changed this to use version.py as a single location to set version for all python modules\nwith warnings.catch_warnings():\n    version = re.search(\n            \'^__version__\\s*=\\s*""(.*)""\',\n            open(\'cli_pipeline/cli_pipeline.py\').read(),\n            re.M\n        ).group(1)\n\n\n# Get the long description from the relevant file\n#with warnings.catch_warnings():\n#    with open(\'README.rst\') as f:\n#        long_description = f.read()\n\nwith warnings.catch_warnings():\n    requirements_path = \'./requirements.txt\'\n    requirements_path = os.path.normpath(requirements_path)\n    requirements_path = os.path.expandvars(requirements_path)\n    requirements_path = os.path.expanduser(requirements_path)\n    requirements_path = os.path.abspath(requirements_path)\n\n    with open(requirements_path) as f:\n        requirements = [line.rstrip() for line in f.readlines()]\n\nwith warnings.catch_warnings():\n    setup(\n        include_package_data=True,\n        name=""cli-pipeline"",\n        packages=[""cli_pipeline""],\n        entry_points={\n            ""console_scripts"": [\n                \'pipeline = cli_pipeline.cli_pipeline:_main\',\n            ]\n        },\n        keywords=\'machine learning artificial intelligence model training deployment optimization\',\n        version=version,\n        license=\'Apache 2.0\',\n        description=""PipelineAI CLI"",\n        classifiers=[\n            \'Development Status :: 5 - Production/Stable\',\n\n            # Indicate who your project is intended for\n            \'Intended Audience :: Developers\',\n            \'Topic :: Software Development :: Build Tools\',\n\n            # Specify the Python versions you support here. In particular, ensure\n            # that you indicate whether you support Python 2, Python 3 or both.\n            \'Programming Language :: Python :: 2\',\n            \'Programming Language :: Python :: 2.7\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n            \'Programming Language :: Python :: 3.7\',\n        ],\n        # long_description=""%s\\n\\nRequirements:\\n%s"" % (long_description, requirements),\n        author=""PipelineAI"",\n        author_email=""contact@pipeline.ai"",\n        url=""https://pipeline.ai"",\n        install_requires=requirements,\n        dependency_links=[],\n        package=find_packages(exclude=[\'concurrent\', \'concurrent.*\', \'*.concurrent.*\']),\n        python_requires=\'>=2\',\n        package_data={\n            # IF YOU MAKE CHANGES BELOW, MAKE SURE YOU UPDATE `MANFIEST.in` WITH THE SAME CHANGES\n            \'templates\': [\'templates/**/*.template\']\n        },\n    )\n'"
stream/tensorflow_kafka_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib.kafka.python.ops import kafka_dataset_ops\nfrom tensorflow.python.data.ops import iterator_ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.platform import test\n\n#\n# (Tested on Confluent 4.0.0 and Python 3)\n#\n# Setup Kafka topic\n# kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\n# Create sample binary message\n# echo -e ""D0\\nD1\\nD2\\nD3\\nD4\\nD5\\nD6\\nD7\\nD8\\nD9"" > ./test_message.bin\'\n\n# Put binary message on Kafka topic\n# kafka-console-producer --topic test --broker-list 127.0.0.1:9092 < ./test_message.bin\n#\nclass KafkaDatasetTest(test.TestCase):\n\n  def setUp(self):\n    # The Kafka server has to be setup before the test\n    # and tear down after the test manually.\n    # The docker engine has to be installed.\n    #\n    # To setup the Kafka server:\n    # $ bash kafka_test.sh start kafka\n    #\n    # To team down the Kafka server:\n    # $ bash kafka_test.sh stop kafka\n    pass\n\n  def testKafkaDataset(self):\n    topics = array_ops.placeholder(dtypes.string, shape=[None])\n    num_epochs = array_ops.placeholder(dtypes.int64, shape=[])\n    batch_size = array_ops.placeholder(dtypes.int64, shape=[])\n\n    repeat_dataset = kafka_dataset_ops.KafkaDataset(\n        servers=""stream-mnist-a:9092"", topics=topics, group=""test"", eof=True).repeat(num_epochs)\n    batch_dataset = repeat_dataset.batch(batch_size)\n\n    iterator = iterator_ops.Iterator.from_structure(batch_dataset.output_types)\n    init_op = iterator.make_initializer(repeat_dataset)\n    init_batch_op = iterator.make_initializer(batch_dataset)\n    get_next = iterator.get_next()\n\n    with self.test_session() as sess:\n      # Basic test: read from topic 0.\n      sess.run(init_op, feed_dict={topics: [""test:0:0:4""], num_epochs: 1})\n      for i in range(5):\n        self.assertEqual(str(""D"" + str(i)).encode(\'utf-8\'), sess.run(get_next))\n      with self.assertRaises(errors.OutOfRangeError):\n        sess.run(get_next)\n\n      # Basic test: read from topic 1.\n      sess.run(init_op, feed_dict={topics: [""test:0:5:-1""], num_epochs: 1})\n      for i in range(5):\n        self.assertEqual(str(""D"" + str(i + 5)).encode(\'utf-8\'), sess.run(get_next))\n      with self.assertRaises(errors.OutOfRangeError):\n        sess.run(get_next)\n\n      # Basic test: read from both topics.\n      sess.run(\n          init_op,\n          feed_dict={\n              topics: [""test:0:0:4"", ""test:0:5:-1""],\n              num_epochs: 1\n          })\n      for j in range(2):\n        for i in range(5):\n          self.assertEqual(str(""D"" + str(i + j * 5)).encode(\'utf-8\'), sess.run(get_next))\n      with self.assertRaises(errors.OutOfRangeError):\n        sess.run(get_next)\n\n      # Test repeated iteration through both files.\n      sess.run(\n          init_op,\n          feed_dict={\n              topics: [""test:0:0:4"", ""test:0:5:-1""],\n              num_epochs: 10\n          })\n      for _ in range(10):\n        for j in range(2):\n          for i in range(5):\n            self.assertEqual(str(""D"" + str(i + j * 5)).encode(\'utf-8\'), sess.run(get_next))\n      with self.assertRaises(errors.OutOfRangeError):\n        sess.run(get_next)\n\n      # Test batched and repeated iteration through both files.\n      sess.run(\n          init_batch_op,\n          feed_dict={\n              topics: [""test:0:0:4"", ""test:0:5:-1""],\n              num_epochs: 10,\n              batch_size: 5\n          })\n      for _ in range(10):\n        self.assertAllEqual([str(""D"" + str(i)).encode(\'utf-8\') for i in range(5)],\n                            sess.run(get_next))\n        self.assertAllEqual([str(""D"" + str(i + 5)).encode(\'utf-8\') for i in range(5)],\n                            sess.run(get_next))\n\n\nif __name__ == ""__main__"":\n  test.main()\n'"
cli/cli_pipeline/__init__.py,0,b''
cli/cli_pipeline/cli_pipeline.py,0,"b'# -*- coding: utf-8 -*-\n\n__version__ = ""1.5.330""\n\nimport base64 as _base64\nimport glob as _glob\nimport json as _json\nimport logging as _logging\nimport os as _os\nimport re as _re\nfrom os import environ as _environ\nimport subprocess as _subprocess\nimport sys as _sys\nimport tarfile as _tarfile\nimport time as _time\nimport warnings as _warnings\nfrom datetime import datetime as _datetime\nfrom inspect import getmembers as _getmembers, isfunction as _isfunction\nfrom pprint import pprint as _pprint\nimport tempfile as _tempfile\nfrom distutils import dir_util as _dir_util\n\nimport boto3 as _boto3\nfrom botocore.exceptions import ClientError as _ClientError\nimport fire as _fire\nimport jinja2 as _jinja2\nimport kubernetes.client as _kubeclient\nimport kubernetes.config as _kubeconfig\nimport requests as _requests\n\n# 200 OK\n# Standard response for successful HTTP requests.\n# The actual response will depend on the request method used.\n# In a GET request, the response will contain an entity corresponding\n# to the requested resource. In a POST request, the response will\n# contain an entity describing or containing the result of the action.\n_HTTP_STATUS_SUCCESS_OK = 200\n\n# 201 Created\n# The request has been fulfilled, resulting in the creation of a new resource.\n_HTTP_STATUS_SUCCESS_CREATED = 201\n\n# 400 Bad Request\n# The server cannot or will not process the request due to an apparent client error\n# (e.g., malformed request syntax, size too large, invalid request message framing,\n# or deceptive request routing).\n_HTTP_STATUS_CLIENT_ERROR_BAD_REQUEST = 400\n\n# 401 Unauthorized (RFC 7235)\n# Similar to 403 Forbidden, but specifically for use when authentication is required\n# and has failed or has not yet been provided. The response must include a\n# WWW-Authenticate header field containing a challenge applicable to the requested resource.\n# See Basic access authentication and Digest access authentication.\n# [34] 401 semantically means ""unauthenticated"",[35]\n# i.e. the user does not have the necessary credentials.\n# Note: Some sites issue HTTP 401 when an IP address is banned from the website\n# (usually the website domain) and that specific address is refused permission to access a website.\n_HTTP_STATUS_CLIENT_ERROR_UNAUTHORIZED = 401\n\n# 403 Forbidden\n# The request was valid, but the server is refusing action.\n# The user might not have the necessary permissions for a resource,\n# or may need an account of some sort.\n_HTTP_STATUS_CLIENT_ERROR_FORBIDDEN = 403\n\n# 500 Internal Server Error\n# A generic error message, given when an unexpected condition was encountered\n# and no more specific message is suitable.\n_HTTP_STATUS_SERVER_ERROR_INTERNAL_SERVER_ERROR = 500\n\n# 501 Not Implemented\n# The server either does not recognize the request method, or it lacks the ability\n# to fulfil the request. Usually this implies future availability\n# (e.g., a new feature of a web-service API)\n_HTTP_STATUS_SERVER_ERROR_NOT_IMPLEMENTED = 501\n\n_invalid_input_az_09_regex_pattern = _re.compile(\'[^a-z0-9]\')\n\n_logger = _logging.getLogger()\n_logger.setLevel(_logging.WARNING)\n_logging.getLogger(""urllib3"").setLevel(_logging.WARNING)\n_logging.getLogger(\'kubernetes.client.rest\').setLevel(_logging.WARNING)\n\n_ch = _logging.StreamHandler(_sys.stdout)\n_ch.setLevel(_logging.DEBUG)\n_formatter = _logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n_ch.setFormatter(_formatter)\n_logger.addHandler(_ch)\n\n_http_mode = False\n\n_default_resource_template_name = \'default\'\n_default_overwrite = True\n\n_job_subdir_name = \'job\'\n_function_subdir_name = \'function\'\n_model_subdir_name = \'model\'\n_stream_subdir_name = \'stream\'\n_train_subdir_name = \'model\'\n_default_type = \'tensorflow\'\n\nif _sys.version_info.major == 3:\n    from urllib3 import disable_warnings as _disable_warnings\n    _disable_warnings()\n\n_dockerfile_template_registry = {\n    \'predict\': ([\'docker/predict-server-local-dockerfile.template\'], []),\n    \'job\': ([\'docker/job-server-local-dockerfile.template\'], []),\n    \'stream\': ([\'docker/stream-server-local-dockerfile.template\'], []),\n    \'train\': ([\'docker/train-server-local-dockerfile.template\'], []),\n}\n\n_kube_deploy_template_registry = {\n    \'predict\': ([\'yaml/predict-deploy.yaml.template\'], []),\n    \'job\': ([\'yaml/job-deploy.yaml.template\'], []),\n    \'stream\': ([\'yaml/stream-deploy.yaml.template\'], []),\n    \'train\': ([\'yaml/train-deploy.yaml.template\'], []),\n}\n_kube_ingress_template_registry = {\n    \'predict\': ([\'yaml/predict-ingress.yaml.template\'], []),\n    \'job\': ([\'yaml/job-ingress.yaml.template\'], []),\n    \'stream\': ([\'yaml/stream-ingress.yaml.template\'], []),\n    \'train\': ([\'yaml/train-ingress.yaml.template\'], []),\n}\n_kube_svc_template_registry = {\n    \'predict\': ([\'yaml/predict-svc.yaml.template\'], []),\n    \'job\': ([\'yaml/job-svc.yaml.template\'], []),\n    \'stream\': ([\'yaml/stream-svc.yaml.template\'], []),\n    \'train\': ([\'yaml/train-svc.yaml.template\'], []),\n}\n_kube_routerules_template_registry = {\n    \'predict\': ([\'yaml/predict-routerules.yaml.template\'], []),\n    \'job\': ([\'yaml/job-routerules.yaml.template\'], []),\n    \'stream\': ([\'yaml/stream-routerules.yaml.template\'], []),\n    \'train\': ([\'yaml/train-routerules.yaml.template\'], []),\n}\n_kube_autoscale_template_registry = {\n    \'predict\': ([\'yaml/predict-autoscale.yaml.template\'], []),\n    \'job\': ([\'yaml/job-autoscale.yaml.template\'], []),\n    \'stream\': ([\'yaml/stream-autoscale.yaml.template\'], []),\n    \'train\': ([\'yaml/train-autoscale.yaml.template\'], []),\n}\n\n_pipeline_api_version = \'v1\'\n\n_default_pipeline_templates_path = _os.path.normpath(_os.path.join(_os.path.dirname(__file__), \'templates\'))\n_default_pipeline_services_path = _os.path.normpath(_os.path.join(_os.path.dirname(__file__), \'services\'))\n\n_default_image_registry_url = \'docker.io\'\n_default_image_registry_repo = \'pipelineai\'\n\n_default_ecr_image_registry_url = \'954636985443.dkr.ecr.us-west-2.amazonaws.com\'\n_default_ecr_image_registry_repo = \'pipelineai\'\n\n_default_image_registry_job_namespace = \'job\'\n_default_image_registry_predict_namespace = \'predict\'\n_default_image_registry_stream_namespace = \'stream\'\n_default_image_registry_train_namespace = \'train\'\n_default_image_registry_base_tag = \'1.5.0\'\n\n_default_model_chip = \'cpu\'\n\n_default_build_type = \'docker\'\n_default_build_context_path = \'.\'\n\n_default_namespace = \'default\'\n\n_pipelineai_dockerhub_cpu_image_list = [\n    \'predict-cpu\',\n    \'ubuntu-16.04-cpu\',\n    \'train-cpu\',\n    \'stream-cpu\'\n]\n\n# These are always additive to the CPU images ^^\n_pipelineai_dockerhub_gpu_image_list = [\n    \'predict-gpu\',\n    \'ubuntu-16.04-gpu\',\n    \'train-gpu\',\n    \'stream-gpu\'\n]\n\n_pipelineai_ecr_cpu_image_list = [\n    \'predict-cpu\',\n    \'ubuntu-16.04-cpu\',\n    \'train-cpu\',\n    \'stream-cpu\',\n    \'notebook-cpu\',\n    \'metastore-2.1.1\',\n    \'dashboard-hystrix\',\n    \'dashboard-turbine\',\n    \'prometheus\',\n    \'grafana\',\n    \'admin\',\n    \'api\',\n    \'logging-elasticsearch-6.3.0\',\n    \'logging-kibana-oss-6.3.0\',\n    \'logging-fluentd-kubernetes-v1.2.2-debian-elasticsearch\'\n]\n\n# These are always additive to the CPU images ^^\n_pipelineai_ecr_gpu_image_list = [\n    \'predict-gpu\',\n    \'ubuntu-16.04-gpu\',\n    \'train-gpu\',\n    \'stream-gpu\',\n    \'notebook-gpu\'\n]\n\n# These are free-form, but could be locked down to 0.7.1\n#  However, they work with the _other_image_list below\n_istio_image_list = [\n    \'docker.io/istio/proxy_init:0.7.1\',\n    \'docker.io/istio/proxy:0.7.1\',\n    \'docker.io/istio/istio-ca:0.7.1\',\n    \'docker.io/istio/mixer:0.7.1\',\n    \'docker.io/istio/pilot:0.7.1\',\n    \'docker.io/istio/servicegraph:0.7.1\'\n]\n\n#_vizier_image_list = [\n#    \'docker.io/mysql:8.0.3\',\n#    \'docker.io/katib/vizier-core:v0.1.2-alpha\',\n#    \'docker.io/katib/earlystopping-medianstopping:v0.1.2-alpha\',\n#    \'docker.io/katib/suggestion-bayesianoptimization:v0.1.2-alpha\',\n#    \'docker.io/katib/suggestion-grid:v0.1.2-alpha\',\n#    \'docker.io/katib/suggestion-hyperband:v0.1.1-alpha\',\n#    \'docker.io/katib/suggestion-random:v0.1.2-alpha\',\n#]\n\n_other_image_list = [\n    \'docker.io/prom/statsd-exporter:v0.5.0\',\n    \'k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3\',\n    \'gcr.io/google_containers/heapster:v1.4.0\',\n    \'gcr.io/google_containers/addon-resizer:2.0\',\n    \'docker.io/jaegertracing/all-in-one:1.6.0\',\n#    \'docker.io/mitdbg/modeldb-frontend:latest\',\n]\n\n_PIPELINE_API_BASE_PATH = \'/admin/api/c/v1\'\n\n# TODO:  LOCK THIS DOWN TO \'.tar.gz\'\n_ALLOWED_EXTENSIONS = set([\'tar\', \'gz\', \'tar.gz\'])\n\n_DEFAULT_PIPELINE_TEMPLATES_PATH = _os.path.normpath(\n    _os.path.join(_os.path.dirname(__file__), \'templates\'))\n\n_PIPELINE_CHIP_LIST = [\'cpu\', \'gpu\', \'tpu\']\n_PIPELINE_RUNTIME_LIST = [\'bash\', \'caffe\', \'cpp\', \'jvm\', \'nginx\', \'nodejs\', \'onnx\', \'python\', \'tflite\', \'tfserving\']\n_PIPELINE_RESOURCE_TYPE_LIST = [\'job\', \'model\', \'stream\', \'train\']\n_PIPELINE_KUBERNETES_RESOURCE_TYPE_LIST = [\'autoscale\', \'deploy\', \'svc\', \'ingress\', \'routerules\']\n_PIPELINE_SUPPORTED_KUBERNETES_RESOURCE_TYPE_LIST = [\'deploy\', \'svc\', \'ingress\', \'routerules\']\n# 1800s (30 minutes) to handle long running transactions\n_DEFAULT_REQUEST_TIMEOUT_SECONDS = 1800\n_DEFAULT_SUBPROCESS_TIMEOUT_SECONDS = 1800\n\n_PIPELINE_RESOURCE_TYPE_CONFIG_DICT = {\n    \'resource_types\': [\'job\', \'model\', \'stream\', \'train\'],\n    \'job\': {\n        \'chip\': _default_model_chip,\n        \'namespace\': \'job\',\n        \'subdir_name\': _job_subdir_name,\n        \'image_registry_namespace\': _default_image_registry_job_namespace,\n        \'image_registry_url\': _default_image_registry_url,\n        \'image_registry_repo\': _default_image_registry_repo,\n        \'image_registry_base_tag\': _default_image_registry_base_tag,\n        \'kube_resource_type_list\': [t for t in _PIPELINE_KUBERNETES_RESOURCE_TYPE_LIST if t not in [\'autoscale\', \'ingress\', \'routerules\']],\n    },\n    \'model\': {\n        \'chip\': _default_model_chip,\n        \'namespace\': _default_namespace,\n        \'subdir_name\': _model_subdir_name,\n        \'image_registry_namespace\': _default_image_registry_predict_namespace,\n        \'image_registry_url\': _default_image_registry_url,\n        \'image_registry_repo\': _default_image_registry_repo,\n        \'image_registry_base_tag\': _default_image_registry_base_tag,\n        \'kube_resource_type_list\': [t for t in _PIPELINE_KUBERNETES_RESOURCE_TYPE_LIST if t not in [\'autoscale\']],\n    },\n    \'stream\': {\n        \'chip\': _default_model_chip,\n        \'namespace\': _default_namespace,\n        \'subdir_name\': _stream_subdir_name,\n        \'image_registry_namespace\': _default_image_registry_stream_namespace,\n        \'image_registry_url\': _default_image_registry_url,\n        \'image_registry_repo\': _default_image_registry_repo,\n        \'image_registry_base_tag\': _default_image_registry_base_tag,\n        \'kube_resource_type_list\': [t for t in _PIPELINE_KUBERNETES_RESOURCE_TYPE_LIST if t not in [\'autoscale\']],\n    },\n    \'train\': {\n        \'chip\': _default_model_chip,\n        \'namespace\': _default_namespace,\n        \'subdir_name\': _train_subdir_name,\n        \'image_registry_namespace\': _default_image_registry_train_namespace,\n        \'image_registry_url\': _default_image_registry_url,\n        \'image_registry_repo\': _default_image_registry_repo,\n        \'image_registry_base_tag\': _default_image_registry_base_tag,\n        \'kube_resource_type_list\': [t for t in _PIPELINE_KUBERNETES_RESOURCE_TYPE_LIST if t not in [\'autoscale\', \'svc\', \'ingress\', \'routerules\']],\n    },\n}\n\n# TODO:  Convert this to work in API\n# service name must be no more than 63 characters\n#          // at this point in the workflow run_id is not available yet so reduce 63 by 8 to 55\n#          // limit name, tag and runtime to 35 characters to account for\n#          // 7 characters for predict prefix\n#          // 2 characters for two delimiting dashes ""-"", one between predict and name and one between name and tag\n#          // 8 characters for user_id\n#          // 8 characters for run_id - not available yet\n#          // 3 characters for chip\n\n\ndef _exec_cmd(cmd, throw_on_error=True, env=None, stream_output=False, cwd=None, cmd_stdin=None,\n             **kwargs):\n    """"""\n    Runs a command as a child process.\n    A convenience wrapper for running a command from a Python script.\n    Keyword arguments:\n    cmd -- the command to run, as a list of strings\n    throw_on_error -- if true, raises an Exception if the exit code of the program is nonzero\n    env -- additional environment variables to be defined when running the child process\n    cwd -- working directory for child process\n    stream_output -- if true, does not capture standard output and error; if false, captures these\n      streams and returns them\n    cmd_stdin -- if specified, passes the specified string as stdin to the child process.\n    Note on the return value: If stream_output is true, then only the exit code is returned. If\n    stream_output is false, then a tuple of the exit code, standard output and standard error is\n    returned.\n    """"""\n    cmd_env = _os.environ.copy()\n    if env:\n        cmd_env.update(env)\n\n    if stream_output:\n        child = _subprocess.Popen(cmd, env=cmd_env, cwd=cwd, universal_newlines=True,\n                                 stdin=_subprocess.PIPE, **kwargs)\n        child.communicate(cmd_stdin)\n        exit_code = child.wait()\n        if throw_on_error and exit_code is not 0:\n            raise Exception(""Non-zero exitcode: %s"" % (exit_code))\n        return exit_code\n    else:\n        child = _subprocess.Popen(\n            cmd, env=cmd_env, stdout=_subprocess.PIPE, stdin=_subprocess.PIPE, stderr=_subprocess.PIPE,\n            cwd=cwd, universal_newlines=True, **kwargs)\n        (stdout, stderr) = child.communicate(cmd_stdin)\n        exit_code = child.wait()\n        if throw_on_error and exit_code is not 0:\n            raise Exception(""Non-zero exit code: %s\\n\\nSTDOUT:\\n%s\\n\\nSTDERR:%s"" %\n                                        (exit_code, stdout, stderr))\n        return exit_code, stdout, stderr\n\n\n# TODO: this should be restricted to just Git repos and not S3 and stuff like that\n_GIT_URI_REGEX = _re.compile(r""^[^/]*:"")\n\n\ndef _parse_subdirectory(uri):\n    # Parses a uri and returns the uri and subdirectory as separate values.\n    # Uses \'#\' as a delimiter.\n    subdirectory = \'\'\n    parsed_uri = uri\n    if \'#\' in uri:\n        subdirectory = uri[uri.find(\'#\')+1:]\n        parsed_uri = uri[:uri.find(\'#\')]\n    if subdirectory and \'.\' in subdirectory:\n        raise Exception(""\'.\' is not allowed in project subdirectory paths."")\n    return parsed_uri, subdirectory\n\n\ndef _is_valid_branch_name(work_dir, version):\n    """"""\n    Returns True if the ``version`` is the name of a branch in a Git project.\n    ``work_dir`` must be the working directory in a git repo.\n    """"""\n    if version is not None:\n        from git import Repo\n        from git.exc import GitCommandError\n        repo = Repo(work_dir, search_parent_directories=True)\n        try:\n            return repo.git.rev_parse(""--verify"", ""refs/heads/%s"" % version) is not \'\'\n        except GitCommandError:\n            return False\n    return False\n\n\ndef _expand_uri(uri):\n    if _is_local_uri(uri):\n        return _os.path.abspath(uri)\n    return uri\n\n\ndef _is_local_uri(uri):\n    """"""Returns True if the passed-in URI should be interpreted as a path on the local filesystem.""""""\n    return not _GIT_URI_REGEX.match(uri)\n\n\ndef _fetch_project(uri, force_tempdir, version=None, git_username=None, git_password=None):\n    """"""\n    Fetch a project into a local directory, returning the path to the local project directory.\n    :param force_tempdir: If True, will fetch the project into a temporary directory. Otherwise,\n                          will fetch Git projects into a temporary directory but simply return the\n                          path of local projects (i.e. perform a no-op for local projects).\n    """"""\n    parsed_uri, subdirectory = _parse_subdirectory(uri)\n    use_temp_dst_dir = force_tempdir or not _is_local_uri(parsed_uri)\n    dst_dir = _tempfile.mkdtemp() if use_temp_dst_dir else parsed_uri\n    if use_temp_dst_dir:\n        print(""=== Fetching project from %s into %s ==="" % (uri, dst_dir))\n    if _is_local_uri(uri):\n        if version is not None:\n            raise Exception(""Setting a version is only supported for Git project URIs"")\n        if use_temp_dst_dir:\n            _dir_util.copy_tree(src=parsed_uri, dst=dst_dir)\n    else:\n        assert _GIT_URI_REGEX.match(parsed_uri), ""Non-local URI %s should be a Git URI"" % parsed_uri\n        _fetch_git_repo(parsed_uri, version, dst_dir, git_username, git_password)\n    res = _os.path.abspath(_os.path.join(dst_dir, subdirectory))\n    if not _os.path.exists(res):\n        raise Exception(""Could not find subdirectory %s of %s"" % (subdirectory, dst_dir))\n    return res\n\n\ndef _fetch_git_repo(uri, version, dst_dir, git_username, git_password):\n    """"""\n    Clone the git repo at ``uri`` into ``dst_dir``, checking out commit ``version`` (or defaulting\n    to the head commit of the repository\'s master branch if version is unspecified).\n    If ``git_username`` and ``git_password`` are specified, uses them to authenticate while fetching\n    the repo. Otherwise, assumes authentication parameters are specified by the environment,\n    e.g. by a Git credential helper.\n    """"""\n    # We defer importing git until the last moment, because the import requires that the git\n    # executable is availble on the PATH, so we only want to fail if we actually need it.\n    import git\n    repo = git.Repo.init(dst_dir)\n    origin = repo.create_remote(""origin"", uri)\n    git_args = [git_username, git_password]\n    if not (all(arg is not None for arg in git_args) or all(arg is None for arg in git_args)):\n        raise Exception(""Either both or neither of git_username and git_password must be ""\n                                 ""specified."")\n    if git_username:\n        git_credentials = ""url=%s\\nusername=%s\\npassword=%s"" % (uri, git_username, git_password)\n        repo.git.config(""--local"", ""credential.helper"", ""cache"")\n        _exec_cmd(cmd=[""git"", ""credential-cache"", ""store""], cwd=dst_dir,\n                         cmd_stdin=git_credentials)\n    origin.fetch()\n    if version is not None:\n        try:\n            repo.git.checkout(version)\n        except git.exc.GitCommandError as e:\n            raise Exception(""Unable to checkout version \'%s\' of git repo %s""\n                                     ""- please ensure that the version exists in the repo. ""\n                                     ""Error: %s"" % (version, uri, e))\n    else:\n        repo.create_head(""master"", origin.refs.master)\n        repo.heads.master.checkout()\n\n\ndef _dict_print(n, d):\n    # TODO:  Check for sensitive info and REDACT - or leave out completely.\n    print(\'%s:\' % n)\n    print(_json.dumps(d, sort_keys=True, indent=4, separators=(\', \', \': \')))\n    print(\' \')\n\n\ndef _validate_chips(chip_list):\n    """"""\n    Validate hardware chip is supported by PipelineAI\n\n    :param chip_list:   List of one or more hardware chip(s)\n                            Valid values are cpu, gpu, tpu\n\n    :return:            bool True when chips in the list are supported\n                        else ValueError\n    """"""\n    for chip in chip_list:\n        if chip not in _PIPELINE_CHIP_LIST:\n            raise ValueError(\'chip %s is not supported.  Supported chips: %s\' %\n                             (chip, _PIPELINE_CHIP_LIST))\n\n    return True\n\n\ndef _validate_runtimes(runtime_list):\n    """"""\n    Validate serving runtime is supported by PipelineAI\n\n    :param runtime_list:    List of one or more runtime(s)\n                                Valid values are\n                                bash, caffe, cpp, jvm, nginx, nodejs,\n                                onnx, python, tflite, tfserving\n\n    :return:            bool True when all runtimes in the list are supported\n                        else ValueError\n    """"""\n    for runtime in runtime_list:\n        if runtime not in _PIPELINE_RUNTIME_LIST:\n            raise ValueError(\'runtime %s is not supported.  Supported runtimes: %s\' %\n                             (runtime, _PIPELINE_RUNTIME_LIST))\n\n    return True\n\n\ndef _get_api_url(host, endpoint):\n    path_with_endpoint = _os.path.join(_PIPELINE_API_BASE_PATH, endpoint)\n    # Stripping / and \\ because the PIPELNIE_BASE_PATH already has this\n    host = host.rstrip(\'/\\\\ \')\n\n    if host.startswith(\'http\'):\n        url = \'%s%s\' % (host, path_with_endpoint)\n    else:\n        # default to https://\n        url = \'https://%s%s\' % (host, path_with_endpoint)\n\n    return url\n\n\ndef _get_resource_config(resource_type):\n    """"""\n    Get resource meta data and configuration.\n\n    :param resource_type:   Type of resource (job, function, model, stream)\n\n    :return:                dict containing meta data about the resource_type when it exists, else {}\n    """"""\n    return dict(_PIPELINE_RESOURCE_TYPE_CONFIG_DICT.get(resource_type, {}))\n\n\ndef _get_resource_image_registry_namespace(resource_type):\n    """"""\n    Get image_registry_namespace by resource type.\n\n    :param resource_type:   Type of resource (job, function, model, stream)\n\n    :return:                str default name prefix by resource type when resource type is defined, else None\n    """"""\n    if resource_type in _PIPELINE_RESOURCE_TYPE_CONFIG_DICT:\n        resource = dict(_PIPELINE_RESOURCE_TYPE_CONFIG_DICT.get(resource_type))\n        return resource.get(\'image_registry_namespace\', None)\n    else:\n        return None\n\n\ndef _get_resource_name(user_id, name):\n    """"""\n    Get resource name by resource type.\n\n    :param str user_id:     PipelineAI 8 character user id that uniquely\n                                identifies the user that created the resource\n                                for super users this user_id is not\n                                always the current user\n                                for non-super users this user_id is always\n                                the current user\n    :param str name:        User defined name for the resource\n\n    :return:                resource name - PipelineAI name (with user_id prefix)\n    """"""\n    resource_name = user_id + name\n\n    return resource_name\n\n\ndef _get_resource_tag(\n    resource_type,\n    tag,\n    runtime,\n    chip,\n    resource_id=None,\n):\n    """"""\n    Get resource tag by resource type.\n\n    IMPORTANT:\n        resource_id is optional to enable calling this method before the training resource has been created\n        resource_tag will be incomplete (missing resource_id appended to the end) when resource_id is NOT supplied\n\n    :param resource_type:   Type of resource (job, function, model, stream) - currently not used but here to enable generic method\n    :param tag:             User defined tag for the resource\n    :param runtime:         Runtime used to serve the resource  Valid values: python, tfserving, tflight\n    :param chip:            Type of hardware chip used to server the resource\n    :param resource_id:     (optional) Id that uniquely identifies the trained resource\n\n    :return:                resource tag\n    """"""\n\n    resource_tag = tag + runtime + chip\n    if resource_id:\n        resource_tag += resource_id\n\n    return resource_tag\n\n\ndef _get_resource_base_image_default(resource_type):\n    """"""\n    Get default base image name by resource type.\n\n    :param resource_type:   Type of resource (job, function, model, stream)\n\n    :return:                default base image name\n    """"""\n    resource_config = _get_resource_config(resource_type)\n    base_image_default = \'%s/%s/%s-%s:%s\' % (\n        resource_config[\'image_registry_url\'],\n        resource_config[\'image_registry_repo\'],\n        resource_config[\'image_registry_namespace\'],\n        resource_config[\'chip\'],\n        resource_config[\'image_registry_base_tag\']\n    )\n    return base_image_default\n\n\ndef resource_optimize_and_train(\n    api_token,\n    host,\n    user_id,\n    resource_type,\n    name,\n    tag,\n    resource_subtype,\n    runtime_list,\n    chip_list,\n    resource_id,\n    kube_resource_type_list=None,\n    namespace=None,\n    build_type=None,\n    build_context_path=None,\n    squash=False,\n    no_cache=False,\n    http_proxy=None,\n    https_proxy=None,\n    image_registry_url=None,\n    image_registry_repo=None,\n    image_registry_namespace=None,\n    image_registry_base_tag=None,\n    image_registry_base_chip=None,\n    pipeline_templates_path=None,\n    stream_logger_url=None,\n    stream_logger_topic=None,\n    stream_input_url=None,\n    stream_input_topic=None,\n    stream_output_url=None,\n    stream_output_topic=None,\n    stream_enable_mqtt=False,\n    stream_enable_kafka_rest_api=False,\n    input_host_path=None,\n    output_host_path=None,\n    master_replicas=1,\n    worker_replicas=1,\n    ps_replicas=1,\n    master_memory=\'4Gi\',\n    worker_memory=\'4Gi\',\n    ps_memory=\'4Gi\',\n    master_cpu=\'500m\',\n    worker_cpu=\'500m\',\n    ps_cpu=\'500m\',\n    master_gpu=\'0\',\n    worker_gpu=\'0\',\n    ps_gpu=\'0\',\n    train_args=\'\',\n    verify=False,\n    cert=None,\n    timeout=None\n):\n    """"""\n    Initialize, build and create one or more kubernetes resources.\n\n    :param str host:                                PipelineAI host server dns name\n    :param str user_id:                             PipelineAI 8 character user id that uniquely\n                                                        identifies the user that created the resource\n                                                        for super users this user_id is not\n                                                        always the current user\n                                                        for non-super users this user_id is always\n                                                        the current user\n    :param str resource_type:                       Type of resource (job, function, model, stream)\n    :param str name:                                User defined name for the resource\n    :param str tag:                                 User defined tag for the resource\n    :param str resource_subtype:                    Framework type, tensorflow or pmml for models,\n                                                        kafka or mqtt for stream\n    :param list runtime_list:                       List of one or more runtime(s) that should be used\n                                                        to serve the resource\n                                                        Valid values are python, tfserving, tflite\n    :param list chip_list:                          List of one or more hardware chip(s) that should be\n                                                        used to serve the resource\n                                                        Valid values are [cpu, gpu, tpu]\n    :param str resource_id:                         Id that uniquely identifies the trained resource\n    :param list kube_resource_type_list:            (Optional) List of strings containing the kubernetes resource\n                                                        type names to generate yaml files for.\n                                                        valid values are [deploy, svc, ingress, routerules]\n    :param str build_type:                          (Optional)\n    :param str build_context_path:                  (Optional)\n    :param str namespace:                           (Optional) Namespace provides a scope for names. Names of\n                                                        resources need to be unique within namespace,\n                                                        but not across namespaces.\n    :param bool squash:                             (Optional) docker context\n    :param bool no_cache:                           (Optional) docker context\n    :param str http_proxy:                          (Optional) docker context\n    :param str https_proxy:                         (Optional) docker context\n    :param str image_registry_url:                  (Optional) docker context\n    :param str image_registry_repo:                 (Optional) docker context\n    :param str image_registry_namespace:            (Optional) docker context - image name prefix\n    :param str image_registry_base_tag:             (Optional) docker context\n    :param str image_registry_base_chip:            (Optional) docker context\n    :param str pipeline_templates_path:             (Optional) directory path to PipelineAI yaml templates\n    :param str stream_logger_url:                   (Optional)\n    :param str stream_logger_topic:                 (Optional)\n    :param str stream_input_url:                    (Optional)\n    :param str stream_input_topic:                  (Optional)\n    :param str stream_output_url:                   (Optional)\n    :param str stream_output_topic:                 (Optional)\n    :param str stream_enable_mqtt:                  (Optional) bool Default: False\n    :param str stream_enable_kafka_rest_api:        (Optional) bool Default: False\n    :param str input_host_path:                     (Optional) train context\n    :param int master_replicas:                     (Optional) train context\n    :param str output_host_path:                    (Optional) train context\n    :param int ps_replicas:                         (Optional) train context\n    :param str train_args:                          (Optional) train context\n    :param int worker_replicas:                     (Optional) train context\n    :param bool verify:                             (optional) Either a boolean, in which case it\n                                                        controls whether we verify the server\'s\n                                                        TLS certificate, or a string, in which case\n                                                        it must be a path to a CA bundle to use.\n                                                        Defaults to ``False``.\n    :param tuple cert:                              (optional) if String, path to ssl client cert file\n                                                        (.pem). If Tuple, (\'cert\', \'key\') pair.\n    :param int timeout:                             subprocess command timeout in seconds\n\n    Examples:\n\n    Train all kubernetes resources on cpu chip served by python and tfserving runtimes::\n\n        pipeline resource-optimize-and-train --host community.cloud.pipeline.ai --user-id <YOUR-USER-ID> --resource-type model --name mnist --tag <YOUR-TAG-NAME> --resource-subtype tensorflow --runtime-list \\[python,tfserving\\] --chip-list \\[cpu\\] --resource-id <YOUR-RESOURCE-ID> --kube-resource-type-list \\[deploy,svc,ingress,routerules\\]\n\n    Train all kubernetes resources on cpu and gpu chips served by tflite runtime::\n\n        pipeline resource-optimize-and-train --host community.cloud.pipeline.ai --user-id <YOUR-USER-NAME> --resource-type model --name mnist --tag <YOUR-TAG-NAME> --resource-subtype tensorflow --runtime-list \\[tflite\\] --chip-list \\[cpu,gpu\\] --resource-id <YOUR-RESOURCE-ID> --kube-resource-type-list \\[deploy,svc,ingress,routerules\\]\n\n    :rtype:                                         list\n    :return:                                        list containing the file names of the generated\n                                                        kubernetes yaml files.\n    """"""\n\n    print(\'\')\n    print(\'Started...\')\n    print(\'\')\n\n    name = _validate_and_prep_name(name)\n    tag = _validate_and_prep_tag(tag)\n    resource_config = _get_resource_config(resource_type)\n    if not namespace:\n        namespace = resource_config[\'namespace\']\n    if not image_registry_namespace:\n        image_registry_namespace = resource_config[\'image_registry_namespace\']\n    if not timeout:\n        timeout = _DEFAULT_SUBPROCESS_TIMEOUT_SECONDS\n\n    _validate_chips(chip_list)\n    _validate_runtimes(runtime_list)\n\n    return_dict = dict()\n    status_list = list()\n    status_code_list = list()\n\n    if not kube_resource_type_list:\n        kube_resource_type_list = resource_config[\'kube_resource_type_list\'] \n\n    endpoint = \'resource-optimize-and-train\'\n    url = _get_api_url(host, endpoint)\n    json_body = {\n        \'user_id\': user_id,\n        \'resource_type\': resource_type,\n        \'name\': name,\n        \'tag\': tag,\n        \'resource_subtype\': resource_subtype,\n        \'runtime_list\': runtime_list,\n        \'chip_list\': chip_list,\n        \'resource_id\': resource_id,\n        \'kube_resource_type_list\': kube_resource_type_list,\n        \'namespace\': namespace,\n        \'build_type\': build_type,\n        \'build_context_path\': build_context_path,\n        \'squash\': squash,\n        \'no_cache\': no_cache,\n        \'http_proxy\': http_proxy,\n        \'https_proxy\': https_proxy,\n        \'image_registry_url\': image_registry_url,\n        \'image_registry_repo\': image_registry_repo,\n        \'image_registry_namespace\': image_registry_namespace,\n        \'image_registry_base_tag\': image_registry_base_tag,\n        \'image_registry_base_chip\': image_registry_base_chip,\n        \'pipeline_templates_path\': pipeline_templates_path,\n        \'stream_logger_url\': stream_logger_url,\n        \'stream_logger_topic\': stream_logger_topic,\n        \'stream_input_url\': stream_input_url,\n        \'stream_input_topic\': stream_input_topic,\n        \'stream_output_url\': stream_output_url,\n        \'stream_output_topic\': stream_output_topic,\n        \'stream_enable_mqtt\': stream_enable_mqtt,\n        \'stream_enable_kafka_rest_api\': stream_enable_kafka_rest_api,\n        \'input_host_path\': input_host_path,\n        \'output_host_path\': output_host_path,\n        \'master_replicas\': master_replicas,\n        \'worker_replicas\': worker_replicas,\n        \'ps_replicas\': ps_replicas,\n        \'master_memory\': master_memory,\n        \'worker_memory\': worker_memory,\n        \'ps_memory\': ps_memory,\n        \'master_cpu\': master_cpu,\n        \'worker_cpu\': worker_cpu,\n        \'ps_cpu\': ps_cpu,\n        \'master_gpu\': master_gpu,\n        \'worker_gpu\': worker_gpu,\n        \'ps_gpu\': ps_gpu,\n        \'train_args\': train_args,\n    }\n\n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.post(\n        headers=headers,\n        url=url,\n        json=json_body,\n        verify=verify,\n        cert=cert,\n        timeout=timeout\n    )\n\n    status_code = response.status_code\n    status_code_list.append(status_code)\n\n    if status_code > _HTTP_STATUS_SUCCESS_CREATED:\n        status_list.append(\'incomplete\')\n        return_dict[\'error_message\'] = \'%s %s\' % (endpoint, status_code)\n    else:\n        status_list.append(\'complete\')\n        return_dict[endpoint] = response.json()\n\n    status = max(status_list)\n    status_code = max(status_code_list)\n    return_dict.update({\'status\': status})\n\n    print(\'\')\n    print(\'...Completed\')\n    print(\'\')\n\n    return return_dict, status_code\n\n\ndef resource_optimize_and_deploy(\n    api_token,\n    host,\n    user_id,\n    resource_type,\n    name,\n    tag,\n    resource_subtype,\n    runtime_list,\n    chip_list,\n    resource_id,\n    kube_resource_type_list=None,\n    namespace=None,\n    build_type=None,\n    build_context_path=None,\n    squash=False,\n    no_cache=False,\n    http_proxy=None,\n    https_proxy=None,\n    image_registry_url=None,\n    image_registry_repo=None,\n    image_registry_namespace=None,\n    image_registry_base_tag=None,\n    image_registry_base_chip=None,\n    pipeline_templates_path=None,\n    stream_logger_url=None,\n    stream_logger_topic=None,\n    stream_input_url=None,\n    stream_input_topic=None,\n    stream_output_url=None,\n    stream_output_topic=None,\n    stream_enable_mqtt=False,\n    stream_enable_kafka_rest_api=False,\n#    input_host_path=None,\n#    master_replicas=1,\n#    output_host_path=None,\n#    ps_replicas=1,\n#    train_args=None,\n#    train_host_path=None,\n#    worker_replicas=1,\n#    target_core_util_percentage=\'50\',\n#    min_replicas=\'1\',\n#    max_replicas=\'2\',\n    predict_memory_limit=\'4Gi\',\n    predict_cpu_limit=\'1000m\',\n    predict_gpu_limit=\'0\',\n    resource_split_tag_and_weight_dict=None,\n    resource_shadow_tag_list=None,\n    new_route=True,\n    verify=False,\n    cert=None,\n    timeout=None\n):\n    """"""\n    Initialize, build and create one or more kubernetes resources.\n\n    :param str host:                                PipelineAI host server dns name\n    :param str user_id:                             PipelineAI 8 character user id that uniquely\n                                                        identifies the user that created the resource\n                                                        for super users this user_id is not\n                                                        always the current user\n                                                        for non-super users this user_id is always\n                                                        the current user\n    :param str resource_type:                       Type of resource (job, function, model, stream)\n    :param str name:                                User defined name for the resource\n    :param str tag:                                 User defined tag for the resource\n    :param str resource_subtype:                    Framework type, tensorflow or pmml for models,\n                                                        kafka or mqtt for stream\n    :param list runtime_list:                       List of one or more runtime(s) that should be used\n                                                        to serve the resource\n                                                        Valid values are python, tfserving, tflite\n    :param list chip_list:                          List of one or more hardware chip(s) that should be\n                                                        used to serve the resource\n                                                        Valid values are [cpu, gpu, tpu]\n    :param str resource_id:                         Id that uniquely identifies the trained resource\n    :param list kube_resource_type_list:            (Optional) List of strings containing the kubernetes resource\n                                                        type names to generate yaml files for.\n                                                        valid values are [deploy, svc, ingress, routerules]\n    :param str build_type:                          (Optional)\n    :param str build_context_path:                  (Optional)\n    :param str namespace:                           (Optional) Namespace provides a scope for names. Names of\n                                                        resources need to be unique within namespace,\n                                                        but not across namespaces.\n    :param str target_core_util_percentage:         (Optional) autoscaling core cpu utilization percentage\n    :param str min_replicas:                        (Optional) autoscaling min replicas\n    :param str max_replicas:                        (Optional) autoscaling max replicas\n    :param bool squash:                             (Optional) docker context\n    :param bool no_cache:                           (Optional) docker context\n    :param str http_proxy:                          (Optional) docker context\n    :param str https_proxy:                         (Optional) docker context\n    :param str image_registry_url:                  (Optional) docker context\n    :param str image_registry_repo:                 (Optional) docker context\n    :param str image_registry_namespace:            (Optional) docker context - image name prefix\n    :param str image_registry_base_tag:             (Optional) docker context\n    :param str image_registry_base_chip:            (Optional) docker context\n    :param str pipeline_templates_path:             (Optional) directory path to PipelineAI yaml templates\n    :param str stream_logger_url:                   (Optional)\n    :param str stream_logger_topic:                 (Optional)\n    :param str stream_input_url:                    (Optional)\n    :param str stream_input_topic:                  (Optional)\n    :param str stream_output_url:                   (Optional)\n    :param str stream_output_topic:                 (Optional)\n    :param str stream_enable_mqtt:                  (Optional) bool Default: False\n    :param str stream_enable_kafka_rest_api:        (Optional) bool Default: False\n    :param str input_host_path:                     (Optional) train context\n    :param int master_replicas:                     (Optional) train context\n    :param str output_host_path:                    (Optional) train context\n    :param int ps_replicas:                         (Optional) train context\n    :param str train_args:                          (Optional) train context\n    :param str train_host_path:                     (Optional) train context\n    :param int worker_replicas:                     (Optional) train context\n    :param dict resource_split_tag_and_weight_dict: (Optional) routerules context\n                                                        Example: dict(a:100, b:0, c:0)\n    :param list resource_shadow_tag_list:           (Optional) routerules context\n                                                        Example: [b,c] Note: must set b and c to traffic\n                                                        split 0 above\n    :param bool new_route:                          (Optional) Set to True (default) to create a\n                                                        default routerule for a new route with 0 traffic\n                                                        Set to False when updating an existing routerule\n    :param bool verify:                             (optional) Either a boolean, in which case it\n                                                        controls whether we verify the server\'s\n                                                        TLS certificate, or a string, in which case\n                                                        it must be a path to a CA bundle to use.\n                                                        Defaults to ``False``.\n    :param tuple cert:                              (optional) if String, path to ssl client cert file\n                                                        (.pem). If Tuple, (\'cert\', \'key\') pair.\n    :param int timeout:                             subprocess command timeout in seconds\n\n    Examples:\n\n    Deploy all kubernetes resources on cpu chip served by python and tfserving runtimes::\n\n    pipeline resource-optimize-and-deploy --host community.cloud.pipeline.ai --user-id <YOUR-USER-ID> --resource-type model --name mnist --tag <YOUR-TAG-NAME> --resource-subtype tensorflow --runtime-list \\[python,tfserving\\] --chip-list \\[cpu\\] --resource-id <YOUR-RESOURCE-ID> --kube-resource-type-list \\[deploy,svc,ingress,routerules\\]\n\n    Deploy all kubernetes resources on cpu and gpu chips served by tflite runtime::\n\n    pipeline resource-optimize-and-deploy --host community.cloud.pipeline.ai --user-id <YOUR-USER-NAME> --resource-type model --name mnist --tag <YOUR-TAG-NAME> --resource-subtype tensorflow --runtime-list \\[tflite\\] --chip-list \\[cpu,gpu\\] --resource-id <YOUR-RESOURCE-ID> --kube-resource-type-list \\[deploy,svc,ingress,routerules\\]\n\n    :rtype:                                         list\n    :return:                                        list containing the file names of the generated\n                                                        kubernetes yaml files.\n    """"""\n\n    print(\'\')\n    print(\'Started...\')\n    print(\'\')\n\n    name = _validate_and_prep_name(name)\n    tag = _validate_and_prep_tag(tag)\n    resource_config = _get_resource_config(resource_type)\n    if not namespace:\n        namespace = resource_config[\'namespace\']\n    if not image_registry_namespace:\n        image_registry_namespace = resource_config[\'image_registry_namespace\']\n    if not timeout:\n        timeout = _DEFAULT_SUBPROCESS_TIMEOUT_SECONDS\n\n    _validate_chips(chip_list)\n    _validate_runtimes(runtime_list)\n\n    return_dict = dict()\n    status_list = list()\n    status_code_list = list()\n\n    if not kube_resource_type_list:\n        kube_resource_type_list = resource_config[\'kube_resource_type_list\'] \n\n    # get existing routes and shadowing when the user does not define them\n    # this is required or existing routes and shadowing will be removed\n    if (\n        not isinstance(resource_split_tag_and_weight_dict, dict)\n        and \'routerules\' in kube_resource_type_list\n    ):\n        endpoint = \'resource-kube-routes\'\n        url = _get_api_url(host, endpoint)\n\n        resource_name = _get_resource_name(user_id, name)\n\n        params = {\n            \'user_id\': user_id,\n            \'resource_type\': resource_type,\n            \'resource_name\': resource_name,\n            \'namespace\': namespace,\n            \'image_registry_namespace\': image_registry_namespace\n        }\n\n        headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n        response = _requests.get(\n            headers=headers,\n            url=url,\n            params=params,\n            verify=verify,\n            cert=cert,\n            timeout=timeout\n        )\n\n        status_code = response.status_code\n        if status_code > _HTTP_STATUS_SUCCESS_OK:\n            return_dict[\'error_message\'] = \'%s %s\' % (endpoint, status_code)\n        else:\n            kube_routes = response.json()\n            return_dict[endpoint] = kube_routes\n\n            resource_split_tag_and_weight_dict = dict()\n            resource_shadow_tag_list = list()\n            routes_dict = kube_routes[\'routes\']\n\n            for k, _ in routes_dict.items():\n                resource_tag_dict = routes_dict[k]\n                resource_split_tag_and_weight_dict[k] = resource_tag_dict[\'split\']\n                if resource_tag_dict.get(\'shadow\', False) is True:\n                    resource_shadow_tag_list.append(k)\n\n            status_list.append(\'complete\')\n            status_code_list.append(status_code)\n\n    endpoint = \'resource-optimize-and-deploy\'\n    url = _get_api_url(host, endpoint)\n    json_body = {\n        \'user_id\': user_id,\n        \'resource_type\': resource_type,\n        \'name\': name,\n        \'tag\': tag,\n        \'resource_subtype\': resource_subtype,\n        \'runtime_list\': runtime_list,\n        \'chip_list\': chip_list,\n        \'resource_id\': resource_id,\n        \'kube_resource_type_list\': kube_resource_type_list,\n        \'namespace\': namespace,\n#        \'target_core_util_percentage\': target_core_util_percentage,\n#        \'min_replicas\': min_replicas,\n#        \'max_replicas\': max_replicas,\n        \'build_type\': build_type,\n        \'build_context_path\': build_context_path,\n        \'squash\': squash,\n        \'no_cache\': no_cache,\n        \'http_proxy\': http_proxy,\n        \'https_proxy\': https_proxy,\n        \'image_registry_url\': image_registry_url,\n        \'image_registry_repo\': image_registry_repo,\n        \'image_registry_namespace\': image_registry_namespace,\n        \'image_registry_base_tag\': image_registry_base_tag,\n        \'image_registry_base_chip\': image_registry_base_chip,\n        \'pipeline_templates_path\': pipeline_templates_path,\n        \'stream_logger_url\': stream_logger_url,\n        \'stream_logger_topic\': stream_logger_topic,\n        \'stream_input_url\': stream_input_url,\n        \'stream_input_topic\': stream_input_topic,\n        \'stream_output_url\': stream_output_url,\n        \'stream_output_topic\': stream_output_topic,\n        \'stream_enable_mqtt\': stream_enable_mqtt,\n        \'stream_enable_kafka_rest_api\': stream_enable_kafka_rest_api,\n#        \'input_host_path\': input_host_path,\n#        \'master_replicas\': master_replicas,\n#        \'output_host_path\': output_host_path,\n#        \'ps_replicas\': ps_replicas,\n#        \'train_args\': train_args,\n#        \'train_host_path\': train_host_path,\n#        \'worker_replicas\': worker_replicas,\n        \'predict_memory_limit\': predict_memory_limit,\n        \'predict_cpu_limit\': predict_cpu_limit,\n        \'predict_gpu_limit\': predict_gpu_limit,\n        \'resource_split_tag_and_weight_dict\': resource_split_tag_and_weight_dict,\n        \'resource_shadow_tag_list\': resource_shadow_tag_list,\n        \'new_route\': new_route,\n    }\n \n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.post(\n        headers=headers,\n        url=url,\n        json=json_body,\n        verify=verify,\n        cert=cert,\n        timeout=timeout\n    )\n\n    status_code = response.status_code\n    status_code_list.append(status_code)\n\n    if status_code > _HTTP_STATUS_SUCCESS_CREATED:\n        status_list.append(\'incomplete\')\n        return_dict[\'error_message\'] = \'%s %s\' % (endpoint, status_code)\n    else:\n        status_list.append(\'complete\')\n        return_dict[endpoint] = response.json()\n\n    status = max(status_list)\n    status_code = max(status_code_list)\n    return_dict.update({\'status\': status})\n\n    print(\'\')\n    print(\'...Completed\')\n    print(\'\')\n\n    return return_dict, status_code\n\n\n# pipeline resource-routes-get --user-id 83f05e58 \\\n#                              --api-token <api-token> \\\n#                              --host community.cloud.pipeline.ai \\\n#                              --name mnist \\\n#                              --resource-type=model\ndef resource_routes_get(\n    api_token,\n    host,\n    user_id,\n    resource_type,\n    name,\n    verify=False,\n    cert=None,\n    timeout=1800,\n    namespace=None,\n    image_registry_namespace=None):\n\n    name = _validate_and_prep_name(name)\n\n    resource_config = _get_resource_config(resource_type)\n    if not namespace:\n        namespace = resource_config[\'namespace\']\n    if not image_registry_namespace:\n        image_registry_namespace = resource_config[\'image_registry_namespace\']\n\n    endpoint = \'resource-kube-routes\'\n    url = _get_api_url(host, endpoint)\n\n    resource_name = _get_resource_name(user_id, name)\n\n    params = {\n            \'user_id\': user_id,\n            \'resource_type\': resource_type,\n            \'resource_name\': resource_name,\n            \'namespace\': namespace,\n            \'image_registry_namespace\': image_registry_namespace\n    }\n\n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.get(\n            headers=headers,\n            url=url,\n            params=params,\n            verify=verify,\n            cert=cert,\n            timeout=timeout\n    )\n\n    kube_routes = response.json()\n\n    return_dict = {} \n\n    status_code = response.status_code\n    if status_code > _HTTP_STATUS_SUCCESS_OK:\n        return_dict[\'error_message\'] = \'%s %s\' % (endpoint, status_code)\n    else:\n        resource_split_tag_and_weight_dict = dict()\n        resource_shadow_tag_list = list()\n\n        routes_dict = kube_routes[\'routes\']\n\n        for k, _ in routes_dict.items():\n            resource_tag_dict = routes_dict[k]\n            resource_split_tag_and_weight_dict[k] = resource_tag_dict[\'split\']\n            if resource_tag_dict.get(\'shadow\', False) is True:\n                resource_shadow_tag_list.append(k)\n\n        return_dict[\'split_dict\'] = resource_split_tag_and_weight_dict\n        return_dict[\'shadow_list\'] = resource_shadow_tag_list\n\n    return_dict[\'status_code\'] = status_code\n    return_dict[\'status\'] = \'complete\'\n    return_dict[\'user_id\'] = user_id\n    return_dict[\'resource_type\'] = resource_type\n    return_dict[\'resource_name\'] = resource_name\n    return_dict[\'namespace\'] = namespace\n    return_dict[\'image_registry_namespace\'] = image_registry_namespace\n\n    return _json.dumps(return_dict)\n\n\n# pipeline resource-routes-set --user-id 83f05e58 --api-token <api-token> --host community.cloud.pipeline.ai --name transfer --resource-type=model  --resource-split-tag-and-weight-dict=\'{""v1pythoncpu672c6296"": 40, ""v2pythoncpu2b2634f7"": 60, ""v3pythoncpuf26e3e46"": 0}\' --resource-shadow-tag-list=\'[""v3pythoncpuf26e3e46""]\'\n#\ndef resource_routes_set(\n    api_token,\n    host,\n    user_id,\n    resource_type,\n    name,\n    resource_split_tag_and_weight_dict,\n    resource_shadow_tag_list,\n    verify=False,\n    cert=None,\n    timeout=1800,\n    namespace=None,\n    image_registry_namespace=None):\n\n    name = _validate_and_prep_name(name)\n\n    resource_config = _get_resource_config(resource_type)\n    if not namespace:\n        namespace = resource_config[\'namespace\']\n    if not image_registry_namespace:\n        image_registry_namespace = resource_config[\'image_registry_namespace\']\n\n    endpoint = \'resource-kube-route\'\n    url = _get_api_url(host, endpoint)\n\n    resource_name = _get_resource_name(user_id, name)\n\n    json_body = {\n            \'user_id\': user_id,\n            \'resource_type\': resource_type,\n            \'name\': name,\n            \'resource_name\': resource_name,\n            \'resource_split_tag_and_weight_dict\': resource_split_tag_and_weight_dict,\n            \'resource_shadow_tag_list\':resource_shadow_tag_list,\n            \'namespace\': namespace,\n            \'image_registry_namespace\': image_registry_namespace\n    }\n\n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.post(\n            headers=headers,\n            url=url,\n            json=json_body,\n            verify=verify,\n            cert=cert,\n            timeout=timeout\n    )\n\n    return_dict = {}\n\n    status_code = response.status_code\n    if status_code > _HTTP_STATUS_SUCCESS_CREATED:\n        return_dict[\'error_message\'] = \'%s %s\' % (endpoint, status_code)\n\n    return_dict[\'status_code\'] = status_code\n    return_dict[\'status\'] = \'complete\'\n    return_dict[\'split_dict\'] = resource_split_tag_and_weight_dict\n    return_dict[\'shadow_list\'] = resource_shadow_tag_list\n    return_dict[\'user_id\'] = user_id\n    return_dict[\'resource_type\'] = resource_type\n    return_dict[\'resource_name\'] = resource_name\n    return_dict[\'namespace\'] = namespace\n    return_dict[\'image_registry_namespace\'] = image_registry_namespace\n\n    return _json.dumps(return_dict)\n\n\n# pipeline resource-scale --user-id 83f05e58 \\\n#                         --api-token <api-token> \\\n#                         --host community.cloud.pipeline.ai \\\n#                         --name mnist \\\n#                         --resource-type=model\ndef resource_scale(\n    api_token,\n    host,\n    user_id,\n    resource_type,\n    name,\n    resource_tag,\n    replicas,\n    verify=False,\n    cert=None,\n    timeout=1800,\n    namespace=None,\n    image_registry_namespace=None):\n\n    name = _validate_and_prep_name(name)\n\n    resource_config = _get_resource_config(resource_type)\n    if not namespace:\n        namespace = resource_config[\'namespace\']\n    if not image_registry_namespace:\n        image_registry_namespace = resource_config[\'image_registry_namespace\']\n\n    endpoint = \'resource-kube-scale\'\n    url = _get_api_url(host, endpoint)\n\n    resource_name = _get_resource_name(user_id, name)\n\n    json_body = {\n            \'user_id\': user_id,\n            \'resource_type\': resource_type,\n            \'resource_name\': resource_name,\n            \'resource_tag\': resource_tag,\n            \'replicas\': replicas,\n            \'namespace\': namespace,\n            \'image_registry_namespace\': image_registry_namespace\n    }\n\n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.post(\n            headers=headers,\n            url=url,\n            json=json_body,\n            verify=verify,\n            cert=cert,\n            timeout=timeout\n    )\n\n    return_dict = {}\n\n    status_code = response.status_code\n    if status_code > _HTTP_STATUS_SUCCESS_CREATED:\n        return_dict[\'error_message\'] = \'%s %s\' % (endpoint, status_code)\n\n    return_dict[\'status_code\'] = status_code\n    return_dict[\'status\'] = \'complete\'\n    return_dict[\'user_id\'] = user_id\n    return_dict[\'resource_type\'] = resource_type\n    return_dict[\'name\'] = name\n    return_dict[\'resource_name\'] = resource_name\n    return_dict[\'resource_tag\'] = resource_tag\n    return_dict[\'replicas\'] = replicas\n    return_dict[\'namespace\'] = namespace\n    return_dict[\'image_registry_namespace\'] = image_registry_namespace\n\n    return _json.dumps(return_dict)\n\n\ndef resource_upload(\n    api_token,\n    host,\n    user_id,\n    resource_type,\n    resource_subtype,\n    name,\n    tag,\n    path,\n    template=None,\n    verify=False,\n    cert=None,\n    overwrite=True,\n    timeout=1800\n    ):\n    """"""\n    Upload source code.\n\n    * Compress resource source code into a tar archive.\n    * Create required directories and generate deployment and service resource definitions.\n    * Receive resource source code - or trained binary (ie. tensorflow SavedModel binary)\n      from client as a tar archive then uncompress and extract on the PipelineAI server.\n    * Initialize training resource\n\n    :param str host:             PipelineAI host server dns name\n    :param str user_id:          PipelineAI 8 character user id that uniquely\n                                    identifies the user that created the resource\n                                    for super users this user_id is not\n                                    always the current user\n                                    for non-super users this user_id is always\n                                    the current user\n    :param str resource_type:    Type of resource (job, function, model, stream)\n    :param str resource_subtype: Framework type, tensorflow or pmml for models,\n                                    kafka or mqtt for stream\n    :param str name:             User defined name for the resource\n    :param str tag:              User defined tag for the resource\n    :param str path:             Caller\'s local hard drive directory path containing the\n                                    source code to upload\n    :param str template:         (optional)\n    :param bool verify:          (optional) Either a boolean, in which case it\n                                    controls whether we verify the server\'s\n                                    TLS certificate, or a string, in which case\n                                    it must be a path to a CA bundle to use.\n                                    Defaults to ``False``\n    :param tuple cert:           (optional) if String, path to ssl client cert file\n                                    (.pem). If Tuple, (\'cert\', \'key\') pair.\n    :param bool overwrite:       (optional) set to True to overwrite existing resource\n                                    set to False to fail when resource already exists\n                                    Defaults to ``True``\n    :param int timeout:          (optional) subprocess command timeout in seconds\n                                    Defaults to 1800\n\n    :return:                     string summary\n\n    Example::\n\n    pipeline resource-upload --api-token <YOUR-API-TOKEN> --host community.cloud.pipeline.ai --user-id <YOUR-USER-ID> --resource-type model --resource-subtype tensorflow --name mnist --tag <YOUR-TAG-NAME> --path ./model\n\n    """"""\n    print(\'\')\n    print(\'...Started\')\n    print(\'\')\n\n    # TODO:  Remove user_id parameter and replace with OAuth authenticate to\n    #        retrieve clientId (first 8 of user hash)\n    if not timeout:\n        timeout = _DEFAULT_REQUEST_TIMEOUT_SECONDS\n    if not template:\n        template = _default_resource_template_name\n\n    name = _validate_and_prep_name(name)\n    tag = _validate_and_prep_tag(tag)\n\n    if _is_base64_encoded(path):\n        path = _decode_base64(path)\n\n    path = _os.path.expandvars(path)\n    path = _os.path.expanduser(path)\n    path = _os.path.normpath(path)\n    absolute_path = _os.path.abspath(path)\n\n    return_dict = {}\n\n    # *********** resource_archive_tar ********************************\n    print(\'Packaging New Resource for PipelineAI...\')\n\n#    # TODO:  Revisit this as we need to preserve the user\'s MLproject file \n#    # if the model directory contains a MLproject file, it must be excluded from the archive\n#    # because MLproject has to be generated from a yaml template so the model name matches\n#    # the values supplied by the user and does not reuse an existing resource name and/or tag\n#    exclude_file_list = [\'MLproject\']\n\n#    exclude_file_list = []\n\n    if _os.path.exists(absolute_path):\n        archive_path = model_archive_tar(\n            name, \n            tag, \n            absolute_path, \n            #exclude_file_list=exclude_file_list\n        )\n    else:\n        print(""Path \'%s\' does not exist."" % absolute_path)\n        return\n\n    # *********** resource_source_init ********************************\n    print(\'Preparing PipelineAI for the New Resource...\')\n    endpoint = \'resource-source-init\'\n    url = _get_api_url(host, endpoint)\n\n    body = {\n        \'user_id\': user_id,\n        \'resource_type\': resource_type,\n        \'resource_subtype\': resource_subtype,\n        \'name\': name,\n        \'tag\': tag,\n        \'template_name\': template,\n        \'overwrite\': overwrite\n    }\n\n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.post(\n        headers=headers,\n        url=url,\n        json=body,\n        verify=verify,\n        cert=cert,\n        timeout=timeout\n    )\n\n    response.raise_for_status()\n\n    return_dict[endpoint] = response.json()\n    # _dict_print(endpoint, return_dict[endpoint])\n\n    # *********** resource-archive-receive ********************************\n    print(\'Sending New Resource to PipelineAI...\')\n    endpoint = \'resource-archive-receive\'\n    url = _get_api_url(host, endpoint)\n    files = {\'file\': open(archive_path, \'rb\')}\n\n    form_data = {\n        \'user_id\': user_id,\n        \'resource_type\': resource_type,\n        \'resource_subtype\': resource_subtype,\n        \'name\': name,\n        \'tag\': tag,\n        \'overwrite\': overwrite\n    }\n\n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.post(\n        headers=headers,\n        url=url,\n        data=form_data,\n        files=files,\n        verify=verify,\n        cert=cert,\n        timeout=timeout\n    )\n\n    if _os.path.exists(archive_path):\n        _os.remove(archive_path)\n\n    status_code = response.status_code\n    if status_code > _HTTP_STATUS_SUCCESS_CREATED:\n        return_dict[\'error_message\'] = \'%s %s\' % (endpoint, status_code)\n        return return_dict, status_code\n    else:\n        return_dict[endpoint] = response.json()\n        # _dict_print(endpoint, return_dict[endpoint])\n\n    # *********** resource-source-add ********************************\n    print(\'Initializing Resource...\')\n    endpoint = \'resource-source-add\'\n    url = _get_api_url(host, endpoint)\n    body = {\n        \'user_id\': user_id,\n        \'resource_type\': resource_type,\n        \'resource_subtype\': resource_subtype,\n        \'name\': name,\n        \'tag\': tag,\n        \'timeout\': timeout\n    }\n\n    headers = {\'Authorization\': \'Bearer %s\' % api_token}\n\n    response = _requests.post(\n        headers=headers,\n        url=url,\n        json=body,\n        verify=verify,\n        cert=cert,\n        timeout=timeout\n    )\n\n    response.raise_for_status()\n    resource_source_add_dict = response.json()\n\n    resource_id = resource_source_add_dict.get(\'resource_id\', None)\n    experiment_id = resource_source_add_dict.get(\'experiment_id\', None)\n    experiment_name = resource_source_add_dict.get(\'experiment_name\', None)\n\n#    runtime_list = \',\'.join(resource_source_add_dict.get(\'runtime_list\', []))\n    return_dict[endpoint] = resource_source_add_dict\n\n#    kubernetes_resource_type_list = \',\'.join(_PIPELINE_SUPPORTED_KUBERNETES_RESOURCE_TYPE_LIST)\n\n    response_dict = {}\n    if host:\n        host = host.rstrip(\'/\\\\ \')\n        response_dict[\'comments\'] = \'Navigate to %s to optimize, deploy, validate, and explain your models in live production.\' % host\n        response_dict[\'host\'] = host\n\n    if tag:\n        response_dict[\'tag\'] = tag\n\n    if name:\n        response_dict[\'name\'] = name\n\n    if status_code:\n        response_dict[\'status_code\'] = status_code\n        if status_code == 201:\n            response_dict[\'status\'] = \'Success\'\n\n    if resource_id:\n        response_dict[\'resource_id\'] = resource_id\n\n    if experiment_id:\n        response_dict[\'experiment_id\'] = experiment_id\n \n    if experiment_name:\n        response_dict[\'experiment_name\'] = experiment_name \n\n    return _json.dumps(response_dict)\n\n\n# TODO:  This is too cryptic.  Try to simplify as following:\n#          1) use ""[cpu]"" instead of \\[cpu\\]\n#          2) don\'t require kube-resource-type-list (this is too specific to the internals of our system)\n\n#    CLI:  pipeline resource-optimize-and_deploy\n\n#    Example:\n#        pipeline resource-optimize-and-deploy --host %s --user-id %s --resource-type model --name %s --tag %s --resource-subtype %s --runtime-list \\[%s\\] --chip-list \\[cpu\\] --resource-id %s --kube-resource-type-list \\[%s\\]\n\n#     \'\'\' % (host, host, user_id, name, tag, resource_subtype, runtime_list, resource_id))\n\n    # def test(\n    #     name: str,\n    #     host: str,\n    #     port: int=80,\n    #     path: str=\'../input/predict/test_request.json\',\n    #     concurrency: int=10,\n    #     request_mime_type: str=\'application/json\',\n    #     response_mime_type: str=\'application/json\',\n    #     timeout_seconds: int=1200\n    # ):\n\n    #     name = self.validate_and_prep_name(name)\n\n    #     if self.is_base64_encoded(path):\n    #         path = self.decode_base64(path)\n\n    #     #  TODO: Check if path is secure using securefile or some such\n    #     path = os.path.expandvars(path)\n    #     path = os.path.expanduser(path)\n    #     path = os.path.normpath(path)\n    #     absolute_path = os.path.abspath(path)\n\n    #     print(\'Sample data path: %s\' % absolute_path)\n\n    #     test_url = \'https://%s:%s/predict/%s/invoke\' % (host, port, name)\n\n    #     self.predict_http_test(\n    #         endpoint_url=test_url,\n    #         test_request_path=absolute_path,\n    #         test_request_concurrency=concurrency,\n    #         test_request_mime_type=request_mime_type,\n    #         test_response_mime_type=response_mime_type,\n    #         test_request_timeout_seconds=timeout_seconds\n    #     )\n\n\n# If no image_registry_url, image_registry_repo, or tag are given,\n# we assume that each element in image_list contains all 3, so we just pull it as is\ndef _sync_registry(image_list,\n                   tag=None,\n                   image_registry_url=None,\n                   image_registry_repo=None):\n\n    if tag and image_registry_url and image_registry_repo:\n        for image in image_list:\n            cmd = \'docker pull %s/%s/%s:%s\' % (image_registry_url, image_registry_repo, image, tag)\n            print(cmd)\n            print("""")\n            # TODO:  return check_output\n            _subprocess.call(cmd, shell=True)\n    else:\n        for image in image_list:\n            cmd = \'docker pull %s\' % image\n            print(cmd)\n            print("""")\n            # TODO:  return check_output\n            _subprocess.call(cmd, shell=True)\n\n\ndef env_registry_sync(tag,\n                      chip=_default_model_chip,\n                      image_registry_url=_default_image_registry_url,\n                      image_registry_repo=_default_image_registry_repo):\n\n    # Do GPU first because it\'s more specialized\n    if chip == \'gpu\':\n        _sync_registry(_pipelineai_dockerhub_gpu_image_list,\n                       tag,\n                       image_registry_url,\n                       image_registry_repo)\n\n    _sync_registry(_pipelineai_dockerhub_cpu_image_list,\n                   tag,\n                   image_registry_url,\n                   image_registry_repo)\n    # TODO:  Return http/json\n\n\ndef env_registry_tag(from_image_registry_url,\n                     from_image_registry_repo,\n                     from_image,\n                     from_tag,\n                     to_image_registry_url,\n                     to_image_registry_repo,\n                     to_image,\n                     to_tag): \n\n    cmd = \'docker tag %s/%s/%s:%s %s/%s/%s:%s\' % (from_image_registry_url, from_image_registry_repo, from_image, from_tag, to_image_registry_url, to_image_registry_repo, to_image, to_tag) \n    print(cmd)\n    _subprocess.call(cmd, shell=True)\n    print("""")    \n\n    \ndef _env_registry_fulltag(from_image_registry_url,\n                          from_image_registry_repo,\n                          from_tag,\n                          to_image_registry_url,\n                          to_image_registry_repo,\n                          to_tag,\n                          chip=_default_model_chip):\n\n    for image in _pipelineai_ecr_cpu_image_list:\n        env_registry_tag(from_image_registry_url=from_image_registry_url,\n                         from_image_registry_repo=from_image_registry_repo,\n                         from_image=image,\n                         from_tag=from_tag,\n                         to_image_registry_url=to_image_registry_url,\n                         to_image_registry_repo=to_image_registry_repo,\n                         to_image=image,\n                         to_tag=to_tag)\n\n    if chip == \'gpu\':\n        for image in _pipelineai_ecr_gpu_image_list:\n            env_registry_tag(from_image_registry_url=from_image_registry_url,\n                             from_image_registry_repo=from_image_registry_repo,\n                             from_image=image,\n                             from_tag=from_tag,\n                             to_image_registry_url=to_image_registry_url,\n                             to_image_registry_repo=to_image_registry_repo,\n                             to_image=image,\n                             to_tag=to_tag)\n\ndef env_registry_push(image_registry_url,\n                      image_registry_repo,\n                      image,\n                      tag):\n\n    cmd = \'docker push %s/%s/%s:%s\' % (image_registry_url, image_registry_repo, image, tag)\n    print(cmd)\n    _subprocess.call(cmd, shell=True)\n    print("""")\n\n\ndef _env_registry_fullpush(image_registry_url,\n                           image_registry_repo,\n                           tag,\n                           chip=_default_model_chip):\n\n    for image in _pipelineai_ecr_cpu_image_list:\n        env_registry_push(image_registry_url=image_registry_url,\n                          image_registry_repo=image_registry_repo,\n                          image=image,\n                          tag=tag)\n\n    if chip == \'gpu\':\n        for image in _pipelineai_ecr_gpu_image_list:\n            env_registry_push(image_registry_url=image_registry_url,\n                              image_registry_repo=image_registry_repo,\n                              image=image,\n                              tag=tag)\n\n\ndef _env_registry_fullsync(tag,\n                           chip=_default_model_chip,\n                           image_registry_url=_default_image_registry_url,\n                           image_registry_repo=_default_image_registry_repo,\n                           private_image_registry_url=_default_ecr_image_registry_url,\n                           private_image_registry_repo=_default_ecr_image_registry_repo):\n\n    env_registry_sync(tag,\n                      chip,\n                      image_registry_url,\n                      image_registry_repo)\n\n    _sync_registry(_istio_image_list)\n#    _sync_registry(_vizier_image_list)\n    _sync_registry(_other_image_list)\n\n    _sync_registry(_pipelineai_ecr_cpu_image_list,\n                   tag,\n                   private_image_registry_url,\n                   private_image_registry_repo)\n\n    if chip == \'gpu\':\n        _sync_registry(_pipelineai_ecr_gpu_image_list,\n                       tag,\n                       private_image_registry_url,\n                       private_image_registry_repo)\n\n\n    # TODO:  warn about not being whitelisted for private repos.  contact@pipeline.ai\n    # TODO:  return http/json\n\n\ndef help():\n    print(""Available commands:"")\n    this_module = _sys.modules[__name__]\n    functions = [o[0] for o in _getmembers(this_module) if _isfunction(o[1])]\n    functions = [function.replace(\'_\', \'-\') for function in functions if not function.startswith(\'_\')]\n    functions = sorted(functions)\n    print(""\\n"".join(functions))\n\n\ndef version():\n    build_context_path = _os.path.expandvars(_default_build_context_path)\n    build_context_path = _os.path.expanduser(build_context_path)\n    build_context_path = _os.path.abspath(build_context_path)\n    build_context_path = _os.path.normpath(build_context_path)\n\n    train_base_image_default = \'%s/%s/%s-%s:%s\' % (_default_image_registry_url, _default_image_registry_repo, _default_image_registry_train_namespace, _default_model_chip, _default_image_registry_base_tag)\n    predict_base_image_default = \'%s/%s/%s-%s:%s\' % (_default_image_registry_url, _default_image_registry_repo, _default_image_registry_predict_namespace, _default_model_chip, _default_image_registry_base_tag)\n\n    generated_path = \'~/.pipelineai/cluster/yaml\'\n    generated_path = _os.path.expandvars(generated_path)\n    generated_path = _os.path.expanduser(generated_path)\n    generated_path = _os.path.abspath(generated_path)\n    generated_path = _os.path.normpath(generated_path)\n\n    return_dict = {\n        ""cli_version"": __version__,\n        ""api_version"": _pipeline_api_version,\n        ""build_type_default"": _default_build_type,\n        ""build_context_path"": build_context_path,\n#        ""build_context_path_default"": _default_build_context_path,\n        ""train_base_image_default"": train_base_image_default,\n        ""predict_base_image_default"": predict_base_image_default,\n        ""templates_path_default"": _default_pipeline_templates_path,\n        ""generated_templates_path"": generated_path\n    }\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef _get_default_model_runtime(model_type):\n    model_runtime = \'python\'\n\n    if model_type in [\'keras\', \'python\', \'scikit\', \'pytorch\', \'xgboost\']:\n        model_runtime = \'python\'\n\n    if model_type in [\'java\', \'pmml\', \'spark\']:\n        model_runtime = \'jvm\'\n\n    if model_type in [\'tensorflow\']:\n        model_runtime = \'tfserving\'\n\n    if model_type in [\'caffe\', \'cpp\']:\n        model_runtime = \'cpp\'\n\n    if model_type in [\'mxnet\', \'onnx\']:\n        model_runtime = \'onnx\'\n\n    if model_type in [\'javascript\', \'tensorflowjs\']:\n        model_runtime = \'nginx\'\n\n    if model_type in [\'nodejs\']:\n        model_runtime = \'nodejs\'\n\n    if model_type in [\'bash\']:\n        model_runtime = \'bash\'\n\n    return model_runtime\n\n\n# Make sure model_tag is DNS compliant since this may be used as a DNS hostname.\n# We might also want to remove \'-\' and \'_\', etc.\ndef _validate_and_prep_tag(tag):\n    if type(tag) != str:\n        tag = str(tag)\n    tag = tag.lower()\n    return _invalid_input_az_09_regex_pattern.sub(\'\', tag)\n\n\n# Make sure model_name is DNS compliant since this may be used as a DNS hostname.\n# We might also want to remove \'-\' and \'_\', etc.\ndef _validate_and_prep_name(name):\n    if type(name) != str:\n        name = str(name)\n    name = name.lower()\n    return _invalid_input_az_09_regex_pattern.sub(\'\', name)\n\n\ndef _validate_and_prep_resource_split_tag_and_weight_dict(model_split_tag_and_weight_dict):\n    model_weight_total = 0\n    for tag, _ in model_split_tag_and_weight_dict.items():\n        tag = _validate_and_prep_tag(tag)\n        model_weight = int(model_split_tag_and_weight_dict[tag])\n        model_weight_total += model_weight\n\n    if model_weight_total != 100:\n        raise ValueError(""Total of \'%s\' for weights \'%s\' does not equal 100 as expected."" % (model_weight_total, model_split_tag_and_weight_dict))\n\n    return\n\n\ndef _safe_get_istio_ingress_nodeport(namespace):\n    try:\n        istio_ingress_nodeport = _get_istio_ingress_nodeport(namespace)\n    except Exception:\n        istio_ingress_nodeport = \'<ingress-controller-nodeport>\'\n    return istio_ingress_nodeport\n\n\ndef _safe_get_istio_ingress_ip(namespace):\n    try:\n        istio_ingress_ip = _get_istio_ingress_ip(namespace)\n    except Exception:\n        istio_ingress_ip = \'<ingress-controller-ip>\'\n    return istio_ingress_ip\n\n\ndef predict_kube_endpoint(model_name,\n                          namespace=None,\n                          image_registry_namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1_beta1 = _kubeclient.ExtensionsV1beta1Api()\n\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        endpoint_url = _get_model_kube_endpoint(model_name=model_name,\n                                                namespace=namespace,\n                                                image_registry_namespace=image_registry_namespace)\n\n        response = kubeclient_v1_beta1.list_namespaced_deployment(namespace=namespace,\n                                                                  include_uninitialized=True,\n                                                                  watch=False,\n                                                                  limit=1000,\n                                                                  pretty=False)\n\n        deployments = response.items\n        model_variant_list = [deployment.metadata.name for deployment in deployments\n                               if \'%s-%s\' % (image_registry_namespace, model_name) in deployment.metadata.name]\n\n    return_dict = {""endpoint_url"": endpoint_url,\n                   ""model_variants"": model_variant_list}\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef predict_kube_endpoints(\n    namespace=None,\n    image_registry_namespace=None\n):\n    """"""\n\n    :param namespace:\n    :param image_registry_namespace:\n    :return:\n    """"""\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    _kubeconfig.load_kube_config()\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n\n        endpoint_list = _get_all_model_endpoints(\n            namespace=namespace,\n            image_registry_namespace=image_registry_namespace\n        )\n\n        return_dict = {""endpoints"": endpoint_list}\n\n        if _http_mode:\n            return return_dict\n            # return _jsonify(return_dict)\n        else:\n            return return_dict\n\n\ndef _get_sage_endpoint_url(model_name,\n                           model_region,\n                           image_registry_namespace=None):\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    return \'https://runtime.sagemaker.%s.amazonaws.com/endpoints/%s-%s/invocations\' % (model_region, image_registry_namespace, model_name)\n\n\ndef predict_kube_connect(model_name,\n                         model_tag,\n                         local_port=None,\n                         service_port=None,\n                         namespace=None,\n                         image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    _service_connect(service_name=service_name,\n                     namespace=namespace,\n                     local_port=local_port,\n                     service_port=service_port)\n\n\ndef _service_connect(service_name,\n                     namespace=None,\n                     local_port=None,\n                     service_port=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    pod = _get_pod_by_service_name(service_name=service_name)\n    if not pod:\n        print("""")\n        print(""Service \'%s\' is not running."" % service_name)\n        print("""")\n        return\n    if not service_port:\n        svc = _get_svc_by_service_name(service_name=service_name)\n        if not svc:\n            print("""")\n            print(""Service \'%s\' proxy port cannot be found."" % service_name)\n            print("""")\n            return\n        service_port = svc.spec.ports[0].target_port\n\n    if not local_port:\n        print("""")\n        print(""Proxying local port \'<randomly-chosen>\' to app \'%s\' port \'%s\' using pod \'%s\' in namespace \'%s\'."" % (service_port, service_name, pod.metadata.name, namespace))\n        print("""")\n        print(""If you break out of this terminal, your proxy session will end."")\n        print("""")\n        print(""Use \'http://127.0.0.1:<randomly-chosen>\' to access app \'%s\' on port \'%s\' in namespace \'%s\'."" % (service_name, service_port, namespace))\n        print("""")\n        cmd = \'kubectl port-forward %s :%s --namespace=%s\' % (pod.metadata.name, service_port, namespace)\n        print(cmd)\n        print("""")\n    else:\n        print("""")\n        print(""Proxying local port \'%s\' to app \'%s\' port \'%s\' using pod \'%s\' in namespace \'%s\'."" % (local_port, service_port, service_name, pod.metadata.name, namespace))\n        print("""")\n        print(""If you break out of this terminal, your proxy session will end."")\n        print("""")\n        print(""Use \'http://127.0.0.1:%s\' to access app \'%s\' on port \'%s\' in namespace \'%s\'."" % (local_port, service_name, service_port, namespace))\n        print("""")\n        cmd = \'kubectl port-forward %s %s:%s --namespace=%s\' % (pod.metadata.name, local_port, service_port, namespace)\n        print(cmd)\n        print("""")\n\n    _subprocess.call(cmd, shell=True)\n    print("""")\n\n\ndef _create_predict_server_Dockerfile(model_name,\n                                      model_tag,\n                                      model_path,\n                                      model_type,\n                                      model_runtime,\n                                      model_chip,\n                                      stream_logger_url,\n                                      stream_logger_topic,\n                                      stream_input_url,\n                                      stream_input_topic,\n                                      stream_output_url,\n                                      stream_output_topic,\n                                      image_registry_url,\n                                      image_registry_repo,\n                                      image_registry_namespace,\n                                      image_registry_base_tag,\n                                      image_registry_base_chip,\n                                      pipeline_templates_path):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    context = {\n               \'PIPELINE_RESOURCE_NAME\': model_name,\n               \'PIPELINE_RESOURCE_TAG\': model_tag,\n               \'PIPELINE_RESOURCE_PATH\': model_path,\n               \'PIPELINE_RESOURCE_TYPE\': \'model\',\n               \'PIPELINE_RESOURCE_SUBTYPE\': model_type,\n               \'PIPELINE_NAME\': model_name,\n               \'PIPELINE_TAG\': model_tag,\n               \'PIPELINE_RUNTIME\': model_runtime,\n               \'PIPELINE_CHIP\': model_chip,\n               \'PIPELINE_STREAM_LOGGER_URL\': stream_logger_url,\n               \'PIPELINE_STREAM_LOGGER_TOPIC\': stream_logger_topic,\n               \'PIPELINE_STREAM_INPUT_URL\': stream_input_url,\n               \'PIPELINE_STREAM_INPUT_TOPIC\': stream_input_topic,\n               \'PIPELINE_STREAM_OUTPUT_URL\': stream_output_url,\n               \'PIPELINE_STREAM_OUTPUT_TOPIC\': stream_output_topic,\n               \'PIPELINE_IMAGE_REGISTRY_URL\': image_registry_url,\n               \'PIPELINE_IMAGE_REGISTRY_REPO\': image_registry_repo,\n               \'PIPELINE_IMAGE_REGISTRY_NAMESPACE\': image_registry_namespace,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_TAG\': image_registry_base_tag,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_CHIP\': image_registry_base_chip,\n              }\n\n    model_predict_cpu_Dockerfile_templates_path = _os.path.normpath(_os.path.join(pipeline_templates_path, _dockerfile_template_registry[\'predict\'][0][0]))\n    path, filename = _os.path.split(model_predict_cpu_Dockerfile_templates_path)\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    # Reminder to me that we can write this file anywhere (pipelineai/models, pipelineai/models/.../model\n    #   since we\'re always passing the model_path when we build the docker image with this Dockerfile\n    rendered_Dockerfile = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-%s-%s-%s-Dockerfile\' % (image_registry_namespace, model_name, model_tag, model_type, model_runtime, model_chip))\n    with open(rendered_Dockerfile, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_Dockerfile))\n\n    return rendered_Dockerfile\n\n\ndef predict_server_describe(model_name,\n                            model_tag,\n                            namespace=None,\n                            image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    return _service_describe(service_name=service_name,\n                             namespace=namespace)\n\n\ndef _is_base64_encoded(data):\n    try:\n        data = data.encode(\'utf-8\')\n    except:\n        pass\n\n    try:\n        if _base64.b64encode(_base64.b64decode(data)) == data:\n            return True\n    except:\n        pass\n\n    return False\n\n\ndef _decode_base64(data,\n                   encoding=\'utf-8\'):\n    return _base64.b64decode(data).decode(encoding)\n\n\ndef _encode_base64(data,\n                   encoding=\'utf-8\'):\n    return _base64.b64encode(data.encode(encoding))\n\n\ndef env_kube_activate(namespace):\n    cmd = \'kubectl config set-context $(kubectl config current-context) --namespace=%s\' % namespace\n    print(cmd)\n    _subprocess.call(cmd, shell=True)\n    print("""")\n    cmd = \'kubectl config view | grep namespace\'\n    print(cmd)\n    _subprocess.call(cmd, shell=True)\n    print("""")\n\n\n#  Note:  model_path must contain the pipeline_conda_environment.yaml file\ndef env_conda_activate(model_name,\n                       model_tag,\n                       model_path=\'.\'):\n\n    model_path = _os.path.expandvars(model_path)\n    model_path = _os.path.expanduser(model_path)\n    model_path = _os.path.abspath(model_path)\n    model_path = _os.path.normpath(model_path)\n\n    print(\'Looking for %s/pipeline_conda_environment.yaml\' % model_path)\n\n    # TODO:  Check if exists.  If so, warn the user as new packages in pipeline_conda_environment.yaml\n    #        will not be picked up after the initial environment creation.\n    cmd = \'bash -c ""source activate root && conda env update --name %s-%s -f %s/pipeline_conda_environment.yaml --prune --verbose""\' % (model_name, model_tag, model_path)\n    print(cmd)\n    _subprocess.call(cmd, shell=True)\n    print("""")\n    cmd = \'bash -c ""source activate %s-%s""\' % (model_name, model_tag)\n    print(cmd)\n    _subprocess.call(cmd, shell=True)\n    print("""")\n    return cmd\n\n\n# model_name: mnist\n# model_tag: gpu\n# model_path: tensorflow/mnist-gpu/\n# model_type: tensorflow\n# model_runtime: tfserving\n# model_chip: gpu\n#\ndef predict_server_build(model_name,\n                         model_tag,\n                         model_type,\n                         model_path, # relative to models/ ie. ./tensorflow/mnist/\n                         model_runtime=None,\n                         model_chip=None,\n                         squash=False,\n                         no_cache=False,\n                         http_proxy=None,\n                         https_proxy=None,\n                         stream_logger_url=None,\n                         stream_logger_topic=None,\n                         stream_input_url=None,\n                         stream_input_topic=None,\n                         stream_output_url=None,\n                         stream_output_topic=None,\n                         build_type=None,\n                         build_context_path=None,\n                         image_registry_url=None,\n                         image_registry_repo=None,\n                         image_registry_namespace=None,\n                         image_registry_base_tag=None,\n                         image_registry_base_chip=None,\n                         pipeline_templates_path=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not model_chip:\n        model_chip = _default_model_chip\n\n    if not model_runtime:\n        model_runtime = _get_default_model_runtime(model_type)\n\n    if not build_type:\n        build_type = _default_build_type\n\n    if not build_context_path:\n        build_context_path = _default_build_context_path\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    if not image_registry_base_tag:\n        image_registry_base_tag = _default_image_registry_base_tag\n\n    if not image_registry_base_chip:\n        image_registry_base_chip = model_chip\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    build_context_path = _os.path.expandvars(build_context_path)\n    build_context_path = _os.path.expanduser(build_context_path)\n    build_context_path = _os.path.abspath(build_context_path)\n    build_context_path = _os.path.normpath(build_context_path)\n\n    pipeline_templates_path = _os.path.expandvars(pipeline_templates_path)\n    pipeline_templates_path = _os.path.expanduser(pipeline_templates_path)\n    pipeline_templates_path = _os.path.abspath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n    # All these paths must be in the same dir or this won\'t work - be careful where you start the server or build from.\n    pipeline_templates_path = _os.path.relpath(pipeline_templates_path, build_context_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n\n    if _is_base64_encoded(model_path):\n        model_path = _decode_base64(model_path)\n\n\n#    work_dir = _fetch_project(uri=uri, force_tempdir=False, version=version,\n#                              git_username=git_username, git_password=git_password)\n\n    if not _is_local_uri(model_path):\n        # TODO:  add these args in the cli \n        version = None\n        git_username = None\n        git_password = None\n        model_path = _fetch_project(uri=model_path, force_tempdir=False, version=version,\n                                    git_username=git_username, git_password=git_password)\n\n    model_path = _os.path.expandvars(model_path)\n    model_path = _os.path.expanduser(model_path)\n    model_path = _os.path.normpath(model_path)\n    model_path = _os.path.abspath(model_path)\n    model_path = _os.path.relpath(model_path, build_context_path)\n    model_path = _os.path.normpath(model_path)\n\n    if build_type == \'docker\':\n        generated_Dockerfile = _create_predict_server_Dockerfile(model_name=model_name,\n                                                                 model_tag=model_tag,\n                                                                 model_path=model_path,\n                                                                 model_type=model_type,\n                                                                 model_runtime=model_runtime,\n                                                                 model_chip=model_chip,\n                                                                 stream_logger_url=stream_logger_url,\n                                                                 stream_logger_topic=stream_logger_topic,\n                                                                 stream_input_url=stream_input_url,\n                                                                 stream_input_topic=stream_input_topic,\n                                                                 stream_output_url=stream_output_url,\n                                                                 stream_output_topic=stream_output_topic,\n                                                                 image_registry_url=image_registry_url,\n                                                                 image_registry_repo=image_registry_repo,\n                                                                 image_registry_namespace=image_registry_namespace,\n                                                                 image_registry_base_tag=image_registry_base_tag,\n                                                                 image_registry_base_chip=image_registry_base_chip,\n                                                                 pipeline_templates_path=pipeline_templates_path)\n\n        if http_proxy:\n            http_proxy_build_arg_snippet = \'--build-arg HTTP_PROXY=%s\' % http_proxy\n        else:\n            http_proxy_build_arg_snippet = \'\'\n\n        if https_proxy:\n            https_proxy_build_arg_snippet = \'--build-arg HTTPS_PROXY=%s\' % https_proxy\n        else:\n            https_proxy_build_arg_snippet = \'\'\n\n        if no_cache:\n            no_cache = \'--no-cache\'\n        else:\n            no_cache = \'\'\n\n        if squash:\n            squash = \'--squash\'\n        else:\n            squash = \'\'\n\n        print("""")\n        # TODO: Narrow the build_context_path (difference between model_path and current path?)\n        cmd = \'docker build --network=host %s %s %s %s -t %s/%s/%s-%s:%s -f %s %s\' % (no_cache, squash, http_proxy_build_arg_snippet, https_proxy_build_arg_snippet, image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag, generated_Dockerfile, model_path)\n\n        print(cmd)\n        print("""")\n        _subprocess.call(cmd, shell=True)\n    else:\n        return_dict = {""status"": ""incomplete"",\n                       ""error_message"": ""Build type \'%s\' not found"" % build_type}\n\n#        if _http_mode:\n#            return _jsonify(return_dict)\n        if _http_mode:\n            return return_dict\n            # return _jsonify(return_dict)\n        else:\n            return return_dict\n\n    return_dict = {""status"": ""complete"",\n                   ""cmd"": ""%s"" % cmd,\n                   ""model_variant"": ""%s-%s-%s"" % (image_registry_namespace, model_name, model_tag),\n                   ""image"": ""%s/%s/%s-%s:%s"" % (image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag),\n                   ""model_path"": model_path}\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef _create_predict_kube_Kubernetes_yaml(model_name,\n                                         model_tag,\n                                         model_chip=None,\n                                         namespace=None,\n                                         stream_logger_url=None,\n                                         stream_logger_topic=None,\n                                         stream_input_url=None,\n                                         stream_input_topic=None,\n                                         stream_output_url=None,\n                                         stream_output_topic=None,\n#                                         target_core_util_percentage=\'50\',\n#                                         min_replicas=\'1\',\n#                                         max_replicas=\'2\',\n                                         image_registry_url=None,\n                                         image_registry_repo=None,\n                                         image_registry_namespace=None,\n                                         image_registry_base_tag=None,\n                                         image_registry_base_chip=None,\n                                         pipeline_templates_path=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    if not image_registry_base_tag:\n        image_registry_base_tag = _default_image_registry_base_tag\n\n    if not model_chip:\n        model_chip = _default_model_chip\n\n    if not image_registry_base_chip:\n        image_registry_base_chip = model_chip\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    pipeline_templates_path = _os.path.expandvars(pipeline_templates_path)\n    pipeline_templates_path = _os.path.expanduser(pipeline_templates_path)\n    pipeline_templates_path = _os.path.abspath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n\n    context = {\'PIPELINE_NAMESPACE\': namespace,\n               \'PIPELINE_NAME\': model_name,\n               \'PIPELINE_TAG\': model_tag,\n               \'PIPELINE_CHIP\': model_chip,\n               \'PIPELINE_RESOURCE_NAME\': model_name,\n               \'PIPELINE_RESOURCE_TAG\': model_tag,\n               \'PIPELINE_STREAM_LOGGER_URL\': stream_logger_url,\n               \'PIPELINE_STREAM_LOGGER_TOPIC\': stream_logger_topic,\n               \'PIPELINE_STREAM_INPUT_URL\': stream_input_url,\n               \'PIPELINE_STREAM_INPUT_TOPIC\': stream_input_topic,\n               \'PIPELINE_STREAM_OUTPUT_URL\': stream_output_url,\n               \'PIPELINE_STREAM_OUTPUT_TOPIC\': stream_output_topic,\n#               \'PIPELINE_TARGET_CORE_UTIL_PERCENTAGE\': target_core_util_percentage,\n#               \'PIPELINE_MIN_REPLICAS\': min_replicas,\n#               \'PIPELINE_MAX_REPLICAS\': max_replicas,\n               \'PIPELINE_IMAGE_REGISTRY_URL\': image_registry_url,\n               \'PIPELINE_IMAGE_REGISTRY_REPO\': image_registry_repo,\n               \'PIPELINE_IMAGE_REGISTRY_NAMESPACE\': image_registry_namespace,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_TAG\': image_registry_base_tag,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_CHIP\': image_registry_base_chip,\n              }\n\n    rendered_filenames = []\n\n    if model_chip == \'gpu\':\n        model_router_deploy_yaml_templates_path = _os.path.normpath(_os.path.join(pipeline_templates_path, _kube_deploy_template_registry[\'predict\'][0][0]))\n        path, filename = _os.path.split(model_router_deploy_yaml_templates_path)\n        rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n        rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-%s-deploy.yaml\' % (image_registry_namespace, model_name, model_tag, model_chip))\n        with open(rendered_filename, \'wt\') as fh:\n            fh.write(rendered)\n            print(""\'%s\' => \'%s\'"" % (filename, rendered_filename))\n            rendered_filenames += [rendered_filename]\n    else:\n        model_router_deploy_yaml_templates_path = _os.path.normpath(_os.path.join(pipeline_templates_path, _kube_deploy_template_registry[\'predict\'][0][0]))\n        path, filename = _os.path.split(model_router_deploy_yaml_templates_path)\n        rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n        rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-%s-deploy.yaml\' % (image_registry_namespace, model_name, model_tag, model_chip))\n        with open(rendered_filename, \'wt\') as fh:\n            fh.write(rendered)\n            print(""\'%s\' => \'%s\'"" % (filename, rendered_filename))\n            rendered_filenames += [rendered_filename]\n\n    model_router_ingress_yaml_templates_path = _os.path.normpath(_os.path.join(pipeline_templates_path, _kube_ingress_template_registry[\'predict\'][0][0]))\n    path, filename = _os.path.split(model_router_ingress_yaml_templates_path)\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-ingress.yaml\' % (image_registry_namespace, model_name))\n    with open(rendered_filename, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'"" % (filename, rendered_filename))\n        rendered_filenames += [rendered_filename]\n\n    # routerules template is handled later do not generate it here\n\n    model_router_svc_yaml_templates_path = _os.path.normpath(_os.path.join(pipeline_templates_path, _kube_svc_template_registry[\'predict\'][0][0]))\n    path, filename = _os.path.split(model_router_svc_yaml_templates_path)\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-svc.yaml\' % (image_registry_namespace, model_name))\n    with open(rendered_filename, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'"" % (filename, rendered_filename))\n        rendered_filenames += [rendered_filename]\n\n    model_router_autoscale_yaml_templates_path = _os.path.normpath(_os.path.join(pipeline_templates_path, _kube_autoscale_template_registry[\'predict\'][0][0]))\n    path, filename = _os.path.split(model_router_autoscale_yaml_templates_path)\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-autoscale.yaml\' % (image_registry_namespace, model_name, model_tag))\n    with open(rendered_filename, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'"" % (filename, rendered_filename))\n        rendered_filenames += [rendered_filename]\n\n    return rendered_filenames\n\n\ndef predict_server_shell(model_name,\n                         model_tag,\n                         image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    cmd = \'docker exec -it %s bash\' % container_name\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n\ndef predict_server_register(model_name,\n                            model_tag,\n                            image_registry_url=None,\n                            image_registry_repo=None,\n                            image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    registry_type = ""docker""\n    registry_coordinates = \'%s/%s/%s-%s:%s\' % (image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag)\n\n    cmd = \'docker push %s\' % registry_coordinates\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n    return_dict = {""status"": ""complete"",\n                   ""model_name"": model_name,\n                   ""model_tag"": model_tag,\n                   ""image_registry_url"": image_registry_url,\n                   ""image_registry_repo"": image_registry_repo,\n                   ""image_registry_namespace"": image_registry_namespace,\n                   ""registry_type"": registry_type,\n                   ""registry_coordinates"": registry_coordinates\n                  }\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef predict_server_pull(model_name,\n                        model_tag,\n                        image_registry_url=None,\n                        image_registry_repo=None,\n                        image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    cmd = \'docker pull %s/%s/%s-%s:%s\' % (image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag)\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n\ndef predict_server_start(model_name,\n                         model_tag,\n                         image_registry_url=None,\n                         image_registry_repo=None,\n                         image_registry_namespace=None,\n                         single_server_only=\'true\',\n                         enable_stream_predictions=\'false\',\n                         stream_logger_url=None,\n                         stream_logger_topic=None,\n                         stream_input_url=None,\n                         stream_input_topic=None,\n                         stream_output_url=None,\n                         stream_output_topic=None,\n                         predict_port=\'8080\',\n                         prometheus_port=\'9090\',\n                         grafana_port=\'3000\',\n                         predict_memory_limit=None,\n                         start_cmd=\'docker\',\n                         start_cmd_extra_args=\'\'):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    if not stream_logger_topic:\n        stream_logger_topic = \'%s-%s-logger\' % (model_name, model_tag)\n\n    if not stream_input_topic:\n        stream_input_topic = \'%s-%s-input\' % (model_name, model_tag)\n\n    if not stream_output_topic:\n        stream_output_topic = \'%s-%s-output\' % (model_name, model_tag)\n\n    # Trying to avoid this:\n    #   WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\n    #\n    # https://docs.docker.com/config/containers/resource_constraints/#limit-a-containers-access-to-memory\n    #\n    if not predict_memory_limit:\n        predict_memory_limit = \'\'\n    else:\n        predict_memory_limit = \'--memory=%s --memory-swap=%s\' % (predict_memory_limit, predict_memory_limit)\n\n    # Note: We added `serve` to mimic AWS SageMaker and encourage ENTRYPOINT vs CMD as detailed here:\n    #       https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html\n    cmd = \'%s run -itd -p %s:8080 -p %s:9090 -p %s:3000 -e PIPELINE_SINGLE_SERVER_ONLY=%s -e PIPELINE_ENABLE_STREAM_PREDICTIONS=%s -e PIPELINE_STREAM_LOGGER_URL=%s -e PIPELINE_STREAM_LOGGER_TOPIC=%s -e PIPELINE_STREAM_INPUT_URL=%s -e PIPELINE_STREAM_INPUT_TOPIC=%s -e PIPELINE_STREAM_OUTPUT_URL=%s -e PIPELINE_STREAM_OUTPUT_TOPIC=%s --name=%s %s %s %s/%s/%s-%s:%s serve\' % (start_cmd, predict_port, prometheus_port, grafana_port, single_server_only, enable_stream_predictions, stream_logger_url, stream_logger_topic, stream_input_url, stream_input_topic, stream_output_url, stream_output_topic, container_name, predict_memory_limit, start_cmd_extra_args, image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag)\n    print("""")\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n    print("""")\n    print(""==> IGNORE ANY \'WARNING\' ABOVE.  IT\'S WORKING OK!!"")\n    print("""")\n    print(""Container start: %s"" % container_name)\n    print("""")\n    print(""==> Use \'pipeline predict-server-logs --model-name=%s --model-tag=%s\' to see logs."" % (model_name, model_tag))\n    print("""")\n\n\ndef predict_server_stop(model_name,\n                        model_tag,\n                        image_registry_namespace=None,\n                        stop_cmd=\'docker\'):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n    print("""")\n    cmd = \'%s rm -f %s\' % (stop_cmd, container_name)\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n\ndef predict_server_logs(model_name,\n                        model_tag,\n                        image_registry_namespace=None,\n                        logs_cmd=\'docker\'):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n    print("""")\n    cmd = \'%s logs -f %s\' % (logs_cmd, container_name)\n    print(cmd)\n    print("""")\n\n    _subprocess.call(cmd, shell=True)\n\n\ndef _filter_tar(tarinfo):\n    ignore_list = []\n    for ignore in ignore_list:\n        if ignore in tarinfo.name:\n            return None\n\n    return tarinfo\n\n\ndef predict_server_tar(model_name,\n                       model_tag,\n                       model_path,\n                       tar_path=\'.\',\n                       filemode=\'w\',\n                       compression=\'gz\'):\n\n    return model_archive_tar(model_name=model_name,\n                             model_tag=model_tag,\n                             model_path=model_path,\n                             tar_path=tar_path,\n                             filemode=filemode,\n                             compression=compression)\n\n\ndef model_archive_tar(\n    model_name,\n    model_tag,\n    model_path,\n    tar_path=\'.\',\n    filemode=\'w\',\n    compression=\'gz\',\n):\n    """"""\n\n    :param str model_name:          User defined name for the resource\n    :param str model_tag:           User defined tag for the resource\n    :param str model_path:          Caller\'s local hard drive directory path containing the\n                                        source code to archive\n    :param str tar_path:            directory path where the archive should be created\n                                        Defaults to ``.```\n    :param str filemode:            file open mode\n                                        Defaults to ``w``\n    :param str compression:         archive compression mode\n                                        Defaults to ``gz``\n    :return:                        str path to the tar.gz archive created\n    """"""\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    model_path = _os.path.expandvars(model_path)\n    model_path = _os.path.expanduser(model_path)\n    model_path = _os.path.abspath(model_path)\n    model_path = _os.path.normpath(model_path)\n\n    tar_path = _os.path.expandvars(tar_path)\n    tar_path = _os.path.expanduser(tar_path)\n    tar_path = _os.path.abspath(tar_path)\n    tar_path = _os.path.normpath(tar_path)\n\n    tar_filename = \'%s-%s.tar.gz\' % (model_name, model_tag)\n    tar_path = _os.path.join(tar_path, tar_filename)\n\n    exclude_file_list = []\n\n    pipeline_ignore_path = _os.path.join(model_path, \'pipeline_ignore\')\n    if _os.path.isfile(pipeline_ignore_path):\n        with open(pipeline_ignore_path) as f:\n            content = f.readlines()\n            # you may also want to remove whitespace characters like `\\n` at the end of each line\n            exclude_file_list = [x.strip() for x in content]\n\n    def exclude_file(filename):\n        # iterate through exclude_file_list and exclude the given filename if it contains something in this list\n        excluded_filenames = [exclude_filename for exclude_filename in exclude_file_list if exclude_filename in filename]\n        return excluded_filenames\n\n    with _tarfile.open(tar_path, \'%s:%s\' % (filemode, compression)) as tar:\n        tar.add(model_path, arcname=_model_subdir_name, exclude=exclude_file, filter=_filter_tar)\n\n    return tar_path\n\n\ndef predict_server_untar(model_name,\n                         model_tag,\n                         model_path,\n                         untar_path=\'.\',\n                         untar_filename=None,\n                         filemode=\'w\',\n                         compression=\'gz\'):\n\n    return model_archive_untar(model_name=model_name,\n                               model_tag=model_tag,\n                               model_path=model_path,\n                               untar_path=untar_path,\n                               untar_filename=untar_filename,\n                               filemode=filemode,\n                               compression=compression)\n\n\ndef model_archive_untar(model_name,\n                        model_tag,\n                        model_path,\n                        untar_path=\'.\',\n                        untar_filename=None,\n                        filemode=\'r\',\n                        compression=\'gz\'):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    model_path = _os.path.expandvars(model_path)\n    model_path = _os.path.expanduser(model_path)\n    model_path = _os.path.abspath(model_path)\n    model_path = _os.path.normpath(model_path)\n\n    untar_path = _os.path.expandvars(untar_path)\n    untar_path = _os.path.expanduser(untar_path)\n    untar_path = _os.path.abspath(untar_path)\n    untar_path = _os.path.normpath(untar_path)\n\n    #print(""Untar_path: %s"" % untar_path)\n    if not untar_filename:\n        untar_filename = \'%s-%s.tar.gz\' % (model_name, model_tag)\n\n    full_untar_path = _os.path.join(untar_path, untar_filename)\n\n    with _tarfile.open(full_untar_path, \'%s:%s\' % (filemode, compression)) as tar:\n        tar.extractall(model_path)\n\n    return untar_path\n\n\n# TODO:  LOCK THIS DOWN TO \'.tar.gz\'\n_ALLOWED_EXTENSIONS = set([\'tar\', \'gz\', \'tar.gz\'])\n\n\n# def predict_kube_start(model_name,\n#                        model_tag,\n#                        model_chip=None,\n#                        namespace=None,\n#                        stream_logger_url=None,\n#                        stream_logger_topic=None,\n#                        stream_input_url=None,\n#                        stream_input_topic=None,\n#                        stream_output_url=None,\n#                        stream_output_topic=None,\n# #                       target_core_util_percentage=\'50\',\n# #                       min_replicas=\'1\',\n# #                       max_replicas=\'2\',\n#                        image_registry_url=None,\n#                        image_registry_repo=None,\n#                        image_registry_namespace=None,\n#                        image_registry_base_tag=None,\n#                        image_registry_base_chip=None,\n#                        pipeline_templates_path=None):\n\n#     model_name = _validate_and_prep_name(model_name)\n#     model_tag = _validate_and_prep_tag(model_tag)\n\n#     if not namespace:\n#         namespace = _default_namespace\n\n#     if not image_registry_url:\n#         image_registry_url = _default_image_registry_url\n\n#     if not image_registry_repo:\n#         image_registry_repo = _default_image_registry_repo\n\n#     if not image_registry_namespace:\n#         image_registry_namespace = _default_image_registry_predict_namespace\n\n#     if not image_registry_base_tag:\n#         image_registry_base_tag = _default_image_registry_base_tag\n\n#     if not model_chip:\n#         model_chip = _default_model_chip\n\n#     if not image_registry_base_chip:\n#         image_registry_base_chip = model_chip\n\n#     if not pipeline_templates_path:\n#         pipeline_templates_path = _default_pipeline_templates_path\n\n#     rendered_yamls = _create_predict_kube_Kubernetes_yaml(\n#                                       model_name=model_name,\n#                                       model_tag=model_tag,\n#                                       model_chip=model_chip,\n#                                       namespace=namespace,\n#                                       stream_logger_url=stream_logger_url,\n#                                       stream_logger_topic=stream_logger_topic,\n#                                       stream_input_url=stream_input_url,\n#                                       stream_input_topic=stream_input_topic,\n#                                       stream_output_url=stream_output_url,\n#                                       stream_output_topic=stream_output_topic,\n# #                                      target_core_util_percentage=target_core_util_percentage,\n# #                                      min_replicas=min_replicas,\n# #                                      max_replicas=max_replicas,\n#                                       image_registry_url=image_registry_url,\n#                                       image_registry_repo=image_registry_repo,\n#                                       image_registry_namespace=image_registry_namespace,\n#                                       image_registry_base_tag=image_registry_base_tag,\n#                                       image_registry_base_chip=image_registry_base_chip,\n#                                       pipeline_templates_path=pipeline_templates_path)\n\n#     for rendered_yaml in rendered_yamls:\n#         # For now, only handle \'-deploy\' and \'-svc\' and \'-ingress\' (not autoscale or routerules)\n#         if (\'-stream-deploy\' not in rendered_yaml and \'-stream-svc\' not in rendered_yaml) and (\'-deploy\' in rendered_yaml or \'-svc\' in rendered_yaml or \'-ingress\' in rendered_yaml):\n#             _istio_apply(yaml_path=rendered_yaml,\n#                          namespace=namespace)\n\n# # TODO:  Either fix this - or change to use gateway vs. ingress\n# #    endpoint_url = _get_model_kube_endpoint(model_name=model_name,\n# #                                            namespace=namespace,\n# #                                            image_registry_namespace=image_registry_namespace)\n\n# #    endpoint_url = endpoint_url.rstrip(\'/\')\n\n#     return_dict = {\n#         ""status"": ""complete"",\n#         ""model_name"": model_name,\n#         ""model_tag"": model_tag,\n# #        ""endpoint_url"": endpoint_url,\n# #        ""comments"": ""The `endpoint_url` is an internal IP to the ingress controller. No traffic will be allowed until you enable traffic to this endpoint using `pipeline predict-kube-route`. This extra routing step is intentional.""\n#     }\n\n#     if _http_mode:\n#         return return_dict\n#         # return _jsonify(return_dict)\n#     else:\n#         return return_dict\n\n#     response = _requests.get(url=endpoint_url,\n#                              headers=accept_headers,\n#                              timeout=timeout_seconds)\n\n#     if response.text:\n#         print("""")\n#         _pprint(response.text)\n\n#     # Consume messages from topic\n#     endpoint_url = \'%s/consumers/%s/instances/%s/records\' % (stream_url, stream_consumer_name, stream_consumer_name)\n#     print(endpoint_url)\n#     response = _requests.get(url=endpoint_url,\n#                              headers=accept_headers,\n#                              timeout=timeout_seconds)\n\n#     messages = response.text\n\n#     if response.text:\n#         print("""")\n#         _pprint(response.text)\n\n#     # Remove consumer subscription from topic\n#     endpoint_url = \'%s/consumers/%s/instances/%s\' % (stream_url, stream_consumer_name, stream_consumer_name)\n#     endpoint_url = endpoint_url.rstrip(\'/\')\n#     print(endpoint_url)\n#     response = _requests.delete(url=endpoint_url,\n#                                 headers=content_type_headers,\n#                                 timeout=timeout_seconds)\n\n#     if response.text:\n#         print("""")\n#         _pprint(response.text)\n\n#     return messages\n\n\n# def stream_kube_consume(model_name,\n#                         model_tag,\n#                         stream_topic,\n#                         stream_consumer_name=None,\n#                         stream_offset=None,\n#                         namespace=None,\n#                         image_registry_namespace=None,\n#                         timeout_seconds=1200):\n\n#     if not namespace:\n#         namespace = _default_namespace\n\n#     if not stream_offset:\n#         stream_offset = ""earliest""\n\n#     if not image_registry_namespace:\n#         image_registry_namespace = _default_image_registry_stream_namespace\n\n#     service_name = ""%s-%s-%s"" % (image_registry_namespace, model_name, model_tag)\n#     stream_url = _get_cluster_service(service_name=service_name,\n#                                       namespace=namespace)\n\n#     stream_url = stream_url.rstrip(\'/\')\n\n#     stream_url = \'http://%s/stream/%s/%s\' % (stream_url, model_name, model_tag)\n\n#     if not stream_consumer_name:\n#         stream_consumer_name = \'%s-%s-%s\' % (model_name, model_tag, stream_topic)\n\n#     stream_http_consume(stream_url=stream_url,\n#                         stream_topic=stream_topic,\n#                         stream_consumer_name=stream_consumer_name,\n#                         stream_offset=stream_offset,\n#                         namespace=namespace,\n#                         image_registry_namespace=image_registry_namespace,\n#                         timeout_seconds=timeout_seconds)\n\n\n# def predict_stream_test(model_name,\n#                         model_tag,\n#                         test_request_path,\n#                         stream_input_topic=None,\n#                         namespace=None,\n#                         image_registry_namespace=None,\n#                         test_request_concurrency=1,\n#                         test_request_mime_type=\'application/json\',\n#                         test_response_mime_type=\'application/json\',\n#                         test_request_timeout_seconds=1200):\n\n#     stream_kube_produce(model_name=model_name,\n#                         model_tag=model_tag,\n#                         test_request_path=test_request_path,\n#                         stream_input_topic=stream_input_topic,\n#                         namespace=namespace,\n#                         image_registry_namespace=image_registry_namespace,\n#                         test_request_concurrency=test_request_concurrency,\n#                         test_request_mime_type=test_request_mime_type,\n#                         test_response_mime_type=test_response_mime_type,\n#                         test_request_timeout_seconds=test_request_timeout_seconds)\n\n\n# def stream_http_produce(endpoint_url,\n#                         test_request_path,\n#                         test_request_concurrency=1,\n#                         test_request_timeout_seconds=1200):\n\n#     endpoint_url = endpoint_url.rstrip(\'/\')\n\n#     print("""")\n#     print(""Producing messages for endpoint_url \'%s\'."" % endpoint_url)\n#     print("""")\n\n#     accept_and_content_type_headers = {""Accept"": ""application/vnd.kafka.v2+json"", ""Content-Type"": ""application/vnd.kafka.json.v2+json""}\n\n#     with open(test_request_path, \'rt\') as fh:\n#         model_input_text = fh.read()\n\n#     body = \'{""records"": [{""value"":%s}]}\' % model_input_text\n\n#     response = _requests.post(url=endpoint_url,\n#                               headers=accept_and_content_type_headers,\n#                               data=body.encode(\'utf-8\'),\n#                               timeout=test_request_timeout_seconds)\n\n#     return_dict = {""status"": ""complete"",\n#                    ""endpoint_url"": endpoint_url,\n#                    ""headers"": accept_and_content_type_headers,\n#                    ""timeout"": test_request_timeout_seconds,\n#                    ""test_request_path"": test_request_path,\n#                    ""test_request_concurrency"": test_request_concurrency,\n#                    ""body"": body,\n#                    ""response"": response,\n#                   }\n\n#     if _http_mode:\n#         return return_dict\n#         # return _jsonify(return_dict)\n#     else:\n#         return return_dict\n\n\n# def stream_kube_produce(model_name,\n#                         model_tag,\n#                         test_request_path,\n#                         stream_topic=None,\n#                         namespace=None,\n#                         image_registry_namespace=None,\n#                         test_request_concurrency=1,\n#                         test_request_mime_type=\'application/json\',\n#                         test_response_mime_type=\'application/json\',\n#                         test_request_timeout_seconds=1200):\n\n#     if not namespace:\n#         namespace = _default_namespace\n\n#     if not image_registry_namespace:\n#         image_registry_namespace = _default_image_registry_stream_namespace\n\n#     if not stream_topic:\n#         stream_topic = \'%s-%s-input\' % (model_name, model_tag)\n\n#     service_name = ""%s-%s-%s"" % (image_registry_namespace, model_name, model_tag)\n\n#     stream_url = _get_cluster_service(service_name=service_name,\n#                                       namespace=namespace)\n\n#     stream_url = stream_url.rstrip(\'/\')\n\n#     stream_url = \'http://%s/stream/%s/%s\' % (stream_url, model_name, model_tag)\n\n#     stream_url = stream_url.rstrip(\'/\')\n\n#     endpoint_url = \'%s/topics/%s\' % (stream_url, stream_topic)\n\n#     endpoint_url = endpoint_url.rstrip(\'/\')\n\n#     # TODO: Enrich return_dict with model_name and model_tag and stream_url and stream_topic\n#     # TODO:  The following method returns json.\n#     #        Enrich this json response with `model_name`, `model_tag`, `stream_url`, and `stream_topic`\n#     return stream_http_produce(endpoint_url=endpoint_url,\n#                                test_request_path=test_request_path,\n#                                test_request_concurrency=test_request_concurrency,\n#                                test_request_mime_type=test_request_mime_type,\n#                                test_response_mime_type=test_response_mime_type,\n#                                test_request_timeout_seconds=test_request_timeout_seconds)\n\n\ndef predict_server_test(endpoint_url,\n                        test_request_path,\n                        test_request_concurrency=1,\n                        test_request_mime_type=\'application/json\',\n                        test_response_mime_type=\'application/json\',\n                        test_request_timeout_seconds=1200):\n\n    from concurrent.futures import ThreadPoolExecutor\n\n    endpoint_url = endpoint_url.rstrip(\'/\')\n\n    with ThreadPoolExecutor(max_workers=test_request_concurrency) as executor:\n        for _ in range(test_request_concurrency):\n            executor.submit(_predict_http_test(endpoint_url=endpoint_url,\n                                               test_request_path=test_request_path,\n                                               test_request_mime_type=test_request_mime_type,\n                                               test_response_mime_type=test_response_mime_type,\n                                               test_request_timeout_seconds=test_request_timeout_seconds))\n\n\n# def predict_kube_test(model_name,\n#                       test_request_path,\n#                       image_registry_namespace=None,\n#                       namespace=None,\n#                       test_request_concurrency=1,\n#                       test_request_mime_type=\'application/json\',\n#                       test_response_mime_type=\'application/json\',\n#                       test_request_timeout_seconds=1200):\n\n#     if not namespace:\n#         namespace = _default_namespace\n\n#     if not image_registry_namespace:\n#         image_registry_namespace = _default_image_registry_predict_namespace\n\n#     if _is_base64_encoded(test_request_path):\n#         test_request_path = _decode_base64(test_request_path)\n\n#     endpoint_url = _get_model_kube_endpoint(model_name=model_name,\n#                                             namespace=namespace,\n#                                             image_registry_namespace=image_registry_namespace)\n\n#     endpoint_url = endpoint_url.rstrip(\'/\')\n\n#     # This is required to get around the limitation of istio managing only 1 load balancer\n#     # See here for more details: https://github.com/istio/istio/issues/1752\n#     # If this gets fixed, we can relax the -routerules.yaml and -ingress.yaml in the templates dir\n#     #   (we\'ll no longer need to scope by model_name)\n\n#     from concurrent.futures import ThreadPoolExecutor\n\n#     with ThreadPoolExecutor(max_workers=test_request_concurrency) as executor:\n#         for _ in range(test_request_concurrency):\n#             executor.submit(_predict_http_test(endpoint_url=endpoint_url,\n#                                                test_request_path=test_request_path,\n#                                                test_request_mime_type=test_request_mime_type,\n#                                                test_response_mime_type=test_response_mime_type,\n#                                                test_request_timeout_seconds=test_request_timeout_seconds))\n#     return_dict = {""status"": ""complete"",\n#                    ""model_name"": model_name,\n#                    ""endpoint_url"": endpoint_url,\n#                    ""test_request_path"": test_request_path,\n#                    ""test_request_concurrency"": test_request_concurrency}\n\n#     if _http_mode:\n#         return return_dict\n#         # return _jsonify(return_dict)\n#     else:\n#         return return_dict\n\n\ndef predict_http_test(endpoint_url,\n                      test_request_path,\n                      test_request_concurrency=1,\n                      test_request_mime_type=\'application/json\',\n                      test_response_mime_type=\'application/json\',\n                      test_request_timeout_seconds=1200):\n\n    from concurrent.futures import ThreadPoolExecutor\n\n    endpoint_url = endpoint_url.rstrip(\'/\')\n\n    with ThreadPoolExecutor(max_workers=test_request_concurrency) as executor:\n        for _ in range(test_request_concurrency):\n            executor.submit(_predict_http_test(endpoint_url=endpoint_url,\n                                               test_request_path=test_request_path,\n                                               test_request_mime_type=test_request_mime_type,\n                                               test_response_mime_type=test_response_mime_type,\n                                               test_request_timeout_seconds=test_request_timeout_seconds))\n\n\ndef _predict_http_test(endpoint_url,\n                       test_request_path,\n                       test_request_mime_type=\'application/json\',\n                       test_response_mime_type=\'application/json\',\n                       test_request_timeout_seconds=1200):\n\n    test_request_path = _os.path.expandvars(test_request_path)\n    test_request_path = _os.path.expanduser(test_request_path)\n    test_request_path = _os.path.abspath(test_request_path)\n    test_request_path = _os.path.normpath(test_request_path)\n\n    full_endpoint_url = endpoint_url.rstrip(\'/\')\n    print("""")\n    print(""Predicting with file \'%s\' using \'%s\'"" % (test_request_path, full_endpoint_url))\n    print("""")\n\n    with open(test_request_path, \'rb\') as fh:\n        model_input_binary = fh.read()\n\n    headers = {\'Content-type\': test_request_mime_type, \'Accept\': test_response_mime_type}\n\n    begin_time = _datetime.now()\n    response = _requests.post(url=full_endpoint_url,\n                              headers=headers,\n                              data=model_input_binary,\n                              timeout=test_request_timeout_seconds)\n    end_time = _datetime.now()\n\n    if response.text:\n        print("""")\n        _pprint(response.text)\n\n    print(""Status: %s"" % response.status_code)\n\n    total_time = end_time - begin_time\n    print("""")\n    print(""Request time: %s milliseconds"" % (total_time.microseconds / 1000))\n    print("""")\n\n    return_dict = {""status"": ""complete"",\n                   ""endpoint_url"": full_endpoint_url,\n                   ""test_request_path"": test_request_path}\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef predict_sage_test(model_name,\n                      test_request_path,\n                      image_registry_namespace=None,\n                      test_request_concurrency=1,\n                      test_request_mime_type=\'application/json\',\n                      test_response_mime_type=\'application/json\',\n                      test_request_timeout_seconds=1200):\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    from concurrent.futures import ThreadPoolExecutor\n\n    with ThreadPoolExecutor(max_workers=test_request_concurrency) as executor:\n        for _ in range(test_request_concurrency):\n            executor.submit(_test_single_prediction_sage(\n                                          model_name=model_name,\n                                          test_request_path=test_request_path,\n                                          image_registry_namespace=image_registry_namespace,\n                                          test_request_mime_type=test_request_mime_type,\n                                          test_response_mime_type=test_response_mime_type))\n                                          # test_request_timeout_seconds=test_request_timeout_seconds))\n\n\ndef _test_single_prediction_sage(model_name,\n                                 test_request_path,\n                                 image_registry_namespace,\n                                 test_request_mime_type=\'application/json\',\n                                 test_response_mime_type=\'application/json\'):\n\n    test_request_path = _os.path.expandvars(test_request_path)\n    test_request_path = _os.path.expanduser(test_request_path)\n    test_request_path = _os.path.abspath(test_request_path)\n    test_request_path = _os.path.normpath(test_request_path)\n\n    print("""")\n    print(""Predicting with file \'%s\' using endpoint \'%s-%s\'"" % (test_request_path, image_registry_namespace, model_name))\n\n    with open(test_request_path, \'rb\') as fh:\n        model_input_binary = fh.read()\n\n    begin_time = _datetime.now()\n    body = model_input_binary.decode(\'utf-8\')\n    print(""Sending body: %s"" % body)\n    sagemaker_client = _boto3.client(\'runtime.sagemaker\')\n    response = sagemaker_client.invoke_endpoint(\n                                          EndpointName=\'%s-%s\' % (image_registry_namespace, model_name),\n                                          Body=model_input_binary,\n                                          ContentType=test_request_mime_type,\n                                          Accept=test_response_mime_type)\n    end_time = _datetime.now()\n\n    if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n        print("""")\n        print(""Variant: \'%s\'"" % response[\'InvokedProductionVariant\'])\n        print("""")\n        _pprint(response[\'Body\'].read().decode(\'utf-8\'))\n\n        print("""")\n    else:\n        return\n\n    total_time = end_time - begin_time\n    print(""Request time: %s milliseconds"" % (total_time.microseconds / 1000))\n    print("""")\n\n\ndef predict_sage_stop(model_name,\n                      image_registry_namespace=None):\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    sagemaker_client = _boto3.client(\'sagemaker\')\n\n    # Remove Endpoint\n    try:\n        begin_time = _datetime.now()\n        sagemaker_client.delete_endpoint(EndpointName=\'%s-%s\' % (image_registry_namespace, model_name))\n        end_time = _datetime.now()\n        total_time = end_time - begin_time\n        print(""Time: %s milliseconds"" % (total_time.microseconds / 1000))\n        print("""")\n    except _ClientError:\n        pass\n\n    print(""Stopped endpoint: %s-%s"" % (image_registry_namespace, model_name))\n\n    # Remove Endpoint Config\n    try:\n        begin_time = _datetime.now()\n        sagemaker_client.delete_endpoint_config(EndpointConfigName=\'%s-%s\' % (image_registry_namespace, model_name))\n        end_time = _datetime.now()\n\n        total_time = end_time - begin_time\n        print(""Time: %s milliseconds"" % (total_time.microseconds / 1000))\n        print("""")\n    except _ClientError:\n        pass\n\n    print(""Stopped endpoint config: %s-%s"" % (image_registry_namespace, model_name))\n    print("""")\n\n\ndef predict_sage_describe(model_name,\n                          image_registry_namespace=None):\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    begin_time = _datetime.now()\n    sagemaker_client = _boto3.client(\'sagemaker\')\n    response = sagemaker_client.describe_endpoint(EndpointName=\'%s-%s\' % (image_registry_namespace, model_name))\n    end_time = _datetime.now()\n\n    total_time = end_time - begin_time\n    model_region = \'UNKNOWN_REGION\'\n    print("""")\n    if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n        status = response[\'EndpointStatus\']\n        print(""Endpoint Status: \'%s\'"" % status)\n\n        endpoint_arn = response[\'EndpointArn\']\n        print("""")\n        print(""EndpointArn: \'%s\'"" % endpoint_arn)\n        model_region = endpoint_arn.split(\':\')[3]\n        endpoint_url = _get_sage_endpoint_url(model_name=model_name,\n                                              model_region=model_region,\n                                              image_registry_namespace=image_registry_namespace)\n        print(""Endpoint Url: \'%s\'"" % endpoint_url)\n        print("""")\n        print(""Request time: %s milliseconds"" % (total_time.microseconds / 1000))\n        print("""")\n\n\ndef _get_pod_by_service_name(service_name):\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1 = _kubeclient.CoreV1Api()\n\n    found = False\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        response = kubeclient_v1.list_pod_for_all_namespaces(watch=False, pretty=True)\n        pods = response.items\n        for pod in pods:\n            if service_name in pod.metadata.name:\n                found = True\n                break\n    if found:\n        return pod\n    else:\n        return None\n\n\ndef _get_svc_by_service_name(service_name):\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1 = _kubeclient.CoreV1Api()\n\n    found = False\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        response = kubeclient_v1.list_service_for_all_namespaces(watch=False,\n                                                                 pretty=True)\n        services = response.items\n        for svc in services:\n            if service_name in svc.metadata.name:\n                found = True\n                break\n    if found:\n        return svc\n    else:\n        return None\n\n\ndef predict_kube_shell(model_name,\n                       model_tag,\n                       namespace=None,\n                       image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    container_name = \'%s-%s\' % (image_registry_namespace, model_name)\n\n    _service_shell(service_name=service_name,\n                   container_name=container_name,\n                   namespace=namespace)\n\n\ndef _service_shell(service_name,\n                   container_name=None,\n                   namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1 = _kubeclient.CoreV1Api()\n\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        response = kubeclient_v1.list_pod_for_all_namespaces(watch=False,\n                                                             pretty=True)\n        pods = response.items\n        for pod in pods:\n            if service_name in pod.metadata.name:\n                break\n        print("""")\n        print(""Connecting to \'%s\'"" % pod.metadata.name)\n        print("""")\n\n        if container_name:\n            cmd = ""kubectl exec -it %s -c %s bash"" % (pod.metadata.name, container_name)\n        else:\n            cmd = ""kubectl exec -it %s bash"" % pod.metadata.name\n\n        _subprocess.call(cmd, shell=True)\n\n        print("""")\n\n\ndef predict_kube_logs(model_name,\n                      model_tag,\n                      namespace=None,\n                      image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n    container_name = \'%s-%s\' % (image_registry_namespace, model_name)\n\n    _service_logs(service_name=service_name,\n                  container_name=container_name,\n                  namespace=namespace)\n\n\ndef _service_logs(service_name,\n                  container_name=None,\n                  namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1 = _kubeclient.CoreV1Api()\n\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        response = kubeclient_v1.list_pod_for_all_namespaces(watch=False,\n                                                             pretty=True)\n        found = False\n        pods = response.items\n        for pod in pods:\n            if service_name in pod.metadata.name:\n                found = True\n                break\n        if found:\n            print("""")\n            print(""Tailing logs on \'%s\'."" % pod.metadata.name)\n            print("""")\n            if container_name:\n                cmd = ""kubectl logs -f %s -c %s --namespace=%s"" % (pod.metadata.name, container_name, namespace)\n            else:\n                cmd = ""kubectl logs -f %s --namespace=%s"" % (pod.metadata.name, namespace)\n            print(cmd)\n            print("""")\n            _subprocess.call(cmd, shell=True)\n            print("""")\n        else:\n            print("""")\n            print(""Service \'%s\' is not running."" % service_name)\n            print("""")\n\n\ndef _service_describe(service_name,\n                      namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1 = _kubeclient.CoreV1Api()\n\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        response = kubeclient_v1.list_pod_for_all_namespaces(watch=False,\n                                                             pretty=True)\n        pods = response.items\n        for pod in pods:\n            if service_name in pod.metadata.name:\n                break\n        print("""")\n        print(""Connecting to \'%s\'"" % pod.metadata.name)\n        print("""")\n        cmd = ""kubectl get pod %s --namespace=%s -o json"" % (pod.metadata.name, namespace)\n        service_describe_bytes = _subprocess.check_output(cmd, shell=True)\n\n        return service_describe_bytes.decode(\'utf-8\')\n\n\ndef predict_kube_scale(model_name,\n                       model_tag,\n                       replicas,\n                       namespace=None,\n                       image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    _service_scale(service_name=service_name,\n                   replicas=replicas,\n                   namespace=namespace)\n\n    return_dict = {""status"": ""complete"",\n                   ""model_name"": model_name,\n                   ""model_tag"": model_tag,\n                   ""replicas"": replicas}\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef predict_kube_autoscale(model_name,\n                           model_tag,\n                           cpu_percent,\n                           min_replicas,\n                           max_replicas,\n                           namespace=None,\n                           image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    # TODO:  make sure resources/requests/cpu has been set to something in the yaml\n    #        ie. istioctl kube-inject -f helloworld.yaml -o helloworld-istio.yaml\n    #        then manually edit as follows:\n    #\n    #  resources:\n    #    limits:\n    #      cpu: 1000m\n    #    requests:\n    #      cpu: 100m\n\n    cmd = ""kubectl autoscale deployment %s-%s-%s --cpu-percent=%s --min=%s --max=%s --namespace=%s"" % (image_registry_namespace, model_name, model_tag, cpu_percent, min_replicas, max_replicas, namespace)\n    print("""")\n    print(""Running \'%s\'."" % cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n    cmd = ""kubectl get hpa""\n    print("""")\n    print(""Running \'%s\'."" % cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n    print("""")\n\n    return_dict = {""status"": ""complete"",\n                   ""model_name"": model_name,\n                   ""model_tag"": model_tag,\n                   ""cpu_percent"": cpu_percent,\n                   ""min_replcias"": min_replicas,\n                   ""max_replicas"": max_replicas}\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef predict_kube_describe(model_name,\n                          model_tag,\n                          namespace=None,\n                          image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    return _service_describe(service_name=service_name,\n                             namespace=namespace)\n\n\ndef _service_scale(service_name,\n                   replicas,\n                   namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1_beta1 = _kubeclient.ExtensionsV1beta1Api()\n\n    # TODO:  Filter by given `namespace`\n    #        I believe there is a new method list_deployment_for_namespace() or some such\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        response = kubeclient_v1_beta1.list_deployment_for_all_namespaces(watch=False,\n                                                                          pretty=True)\n        found = False\n        deployments = response.items\n        for deploy in deployments:\n            if service_name in deploy.metadata.name:\n                found = True\n                break\n        if found:\n            print("""")\n            print(""Scaling service \'%s\' to \'%s\' replicas."" % (deploy.metadata.name, replicas))\n            print("""")\n            cmd = ""kubectl scale deploy %s --replicas=%s --namespace=%s"" % (deploy.metadata.name, replicas, namespace)\n            print(""Running \'%s\'."" % cmd)\n            print("""")\n            _subprocess.call(cmd, shell=True)\n            print("""")\n        else:\n            print("""")\n            print(""Service \'%s\' is not running."" % service_name)\n            print("""")\n\n\ndef _kube_apply(yaml_path,\n                namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    yaml_path = _os.path.normpath(yaml_path)\n\n    cmd = ""kubectl apply --namespace %s -f %s"" % (namespace, yaml_path)\n    _kube(cmd=cmd)\n\n\ndef _kube_create(yaml_path,\n                 namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    yaml_path = _os.path.normpath(yaml_path)\n\n    cmd = ""kubectl create --namespace %s -f %s --save-config --record"" % (namespace, yaml_path)\n    _kube(cmd=cmd)\n\n\ndef _kube_delete(yaml_path,\n                 namespace=None):\n\n    yaml_path = _os.path.normpath(yaml_path)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    cmd = ""kubectl delete --namespace %s -f %s"" % (namespace, yaml_path)\n    _kube(cmd=cmd)\n\n\ndef _kube( cmd):\n    print("""")\n    print(""Running \'%s\'."" % cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n    print("""")\n\n\ndef _predict_kube_routes(\n    model_name=None,\n    namespace=None,\n    image_registry_namespace=None\n):\n\n    route_context = \'\'\n    if model_name:\n        if not image_registry_namespace:\n            image_registry_namespace = _default_image_registry_predict_namespace\n        route_context = \'%s-%s\' % (image_registry_namespace, model_name)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    route_dict = dict()\n    status = ""incomplete""\n    cmd = ""kubectl get routerule %s-invoke --namespace=%s -o json"" % (route_context, namespace)\n\n    try:\n\n        routes = _subprocess.check_output(cmd, shell=True)\n        spec = _json.loads(routes.decode(\'utf-8\'))[\'spec\']\n\n        for route in spec.get(\'route\', []):\n            route_dict[route[\'labels\'][\'tag\']] = {\n                \'split\': route[\'weight\'],\n                \'shadow\': True if (spec.get(\'mirror\', None) and route[\'labels\'][\'tag\'] in spec[\'mirror\'][\'labels\'][\'tag\']) else False\n            }\n        status = ""complete""\n    except Exception as exc:\n        print(str(exc))\n\n    return_dict = {\n        ""status"": status,\n        ""routes"": route_dict\n    }\n\n    return return_dict\n\n\ndef _get_model_kube_endpoint(model_name,\n                             namespace,\n                             image_registry_namespace):\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1_beta1 = _kubeclient.ExtensionsV1beta1Api()\n\n    ingress_name = \'%s-%s\' % (image_registry_namespace, model_name)\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        ingress = kubeclient_v1_beta1.read_namespaced_ingress(name=ingress_name,\n                                                              namespace=namespace)\n\n        endpoint = None\n        if ingress.status.load_balancer.ingress and len(ingress.status.load_balancer.ingress) > 0:\n            if (ingress.status.load_balancer.ingress[0].hostname):\n                endpoint = ingress.status.load_balancer.ingress[0].hostname\n            if (ingress.status.load_balancer.ingress[0].ip):\n                endpoint = ingress.status.load_balancer.ingress[0].ip\n\n        if not endpoint:\n            try:\n                istio_ingress_nodeport = _get_istio_ingress_nodeport(namespace)\n            except Exception:\n                istio_ingress_nodeport = \'<ingress-controller-nodeport>\'\n\n            try:\n                istio_ingress_ip = _get_istio_ingress_ip(namespace)\n            except Exception:\n                istio_ingress_ip = \'<ingress-controller-ip>\'\n\n            endpoint = \'%s:%s\' % (istio_ingress_ip, istio_ingress_nodeport)\n\n        path = ingress.spec.rules[0].http.paths[0].path\n\n        endpoint = \'http://%s%s\' % (endpoint, path)\n        endpoint = endpoint.replace("".*"", ""invoke"")\n\n        return endpoint\n\n\ndef _get_istio_ingress_nodeport(namespace):\n    cmd = ""kubectl get svc -n %s istio-ingress -o jsonpath=\'{.spec.ports[0].nodePort}\'"" % namespace\n    istio_ingress_nodeport_bytes = _subprocess.check_output(cmd, shell=True)\n    return istio_ingress_nodeport_bytes.decode(\'utf-8\')\n\n\ndef _get_istio_ingress_ip(namespace):\n    cmd = ""kubectl -n %s get po -l istio=ingress -o jsonpath=\'{.items[0].status.hostIP}\'"" % namespace\n    istio_ingress_nodeport_bytes = _subprocess.check_output(cmd, shell=True)\n    return istio_ingress_nodeport_bytes.decode(\'utf-8\')\n\n\n# TODO: Filter ingresses using image_registry_namespace (\'predict-\')\n# Note:  This is used by multiple functions, so double-check before making changes here\ndef _get_all_model_endpoints(namespace,\n                             image_registry_namespace=_default_image_registry_predict_namespace):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1_beta1 = _kubeclient.ExtensionsV1beta1Api()\n\n    endpoint_list = []\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        ingresses = kubeclient_v1_beta1.list_namespaced_ingress(namespace=namespace)\n        for ingress in ingresses.items:\n            endpoint = None\n            if ingress.status.load_balancer.ingress and len(ingress.status.load_balancer.ingress) > 0:\n                if (ingress.status.load_balancer.ingress[0].hostname):\n                    endpoint = ingress.status.load_balancer.ingress[0].hostname\n                if (ingress.status.load_balancer.ingress[0].ip):\n                    endpoint = ingress.status.load_balancer.ingress[0].ip\n\n            if not endpoint:\n                try:\n                    istio_ingress_nodeport = _get_istio_ingress_nodeport(namespace)\n                except Exception:\n                    istio_ingress_nodeport = \'<ingress-controller-nodeport>\'\n\n                try:\n                    istio_ingress_ip = _get_istio_ingress_ip(namespace)\n                except Exception:\n                    istio_ingress_ip = \'<ingress-controller-ip>\'\n\n                endpoint = \'%s:%s\' % (istio_ingress_ip, istio_ingress_nodeport)\n\n            path = ingress.spec.rules[0].http.paths[0].path\n            endpoint = \'http://%s%s\' % (endpoint, path)\n            endpoint = endpoint.replace("".*"", ""invoke"")\n            endpoint_list += [endpoint]\n\n    return endpoint_list\n\n\ndef _get_cluster_service(service_name,\n                         namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1 = _kubeclient.CoreV1Api()\n\n    endpoint = None\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n        service = kubeclient_v1.read_namespaced_service(name=service_name,\n                                                        namespace=namespace)\n\n        # TODO: What about port? defaults to 80 for ingress controller, but what about non-ingress-controller?\n        if service.status.load_balancer.ingress and len(service.status.load_balancer.ingress) > 0:\n            if (service.status.load_balancer.ingress[0].hostname):\n                endpoint = service.status.load_balancer.ingress[0].hostname\n            if (service.status.load_balancer.ingress[0].ip):\n                endpoint = service.status.load_balancer.ingress[0].ip\n\n        if not endpoint:\n            try:\n                istio_ingress_nodeport = _get_istio_ingress_nodeport(namespace)\n            except Exception:\n                istio_ingress_nodeport = \'<ingress-controller-nodeport>\'\n\n            try:\n                istio_ingress_ip = _get_istio_ingress_ip(namespace)\n            except Exception:\n                istio_ingress_ip = \'<ingress-controller-ip>\'\n\n            endpoint = \'%s:%s\' % (istio_ingress_ip, istio_ingress_nodeport)\n\n    return endpoint\n\n\ndef _istio_apply(yaml_path,\n                 namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    yaml_path = _os.path.normpath(yaml_path)\n\n    cmd = ""istioctl kube-inject -i %s -f %s"" % (namespace, yaml_path)\n    print("""")\n    print(""Running \'%s\'."" % cmd)\n    print("""")\n    new_yaml_bytes = _subprocess.check_output(cmd, shell=True)\n    new_yaml_path = \'%s-istio\' % yaml_path\n    with open(new_yaml_path, \'wt\') as fh:\n        fh.write(new_yaml_bytes.decode(\'utf-8\'))\n        print(""\'%s\' => \'%s\'"" % (yaml_path, new_yaml_path))\n    print("""")\n\n    cmd = ""kubectl apply --namespace %s -f %s"" % (namespace, new_yaml_path)\n    print("""")\n    print(""Running \'%s\'."" % cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n    print("""")\n\n\ndef predict_kube_route(\n    model_name,\n    model_split_tag_and_weight_dict,\n    model_shadow_tag_list,\n    pipeline_templates_path=None,\n    image_registry_namespace=None,\n    namespace=None\n):\n    """"""\n    Route and shadow traffic across model variant services.\n\n    Examples:\n    {""cpu"":50, ""gpu"":50}\n    {""cpu"":1, ""gpu"":99}\n    {""025"":99, ""050"":1}\n    {""025"":50, ""050"":50}\n    {""025"":1, ""050"":99}\n    split: {""a"":100, ""b"":0}\n    shadow: [""b""]\n\n    :param model_name:\n    :param model_split_tag_and_weight_dict: Example: # \'{""a"":100, ""b"":0, ""c"":0}\'\n    :param model_shadow_tag_list: Example: \'[b,c]\' Note: must set b and c to traffic split 0 above\n    :param pipeline_templates_path:\n    :param image_registry_namespace:\n    :param namespace:\n    :return:\n    """"""\n\n    model_name = _validate_and_prep_name(model_name)\n\n    if type(model_split_tag_and_weight_dict) is str:\n        model_split_tag_and_weight_dict = _base64.b64decode(model_split_tag_and_weight_dict)\n        model_split_tag_and_weight_dict = _json.loads(model_split_tag_and_weight_dict)\n\n    if type(model_shadow_tag_list) is str:\n        model_shadow_tag_list = _base64.b64decode(model_shadow_tag_list)\n        # strip \'[\' and \']\' and split on comma\n        model_shadow_tag_list = model_shadow_tag_list.decode(\'utf-8\')\n        model_shadow_tag_list = model_shadow_tag_list.strip()\n        model_shadow_tag_list = model_shadow_tag_list.lstrip(\'[\')\n        model_shadow_tag_list = model_shadow_tag_list.rstrip(\']\')\n        if \',\' in model_shadow_tag_list:\n            model_shadow_tag_list = model_shadow_tag_list.split(\',\')\n            model_shadow_tag_list = [tag.strip() for tag in model_shadow_tag_list]\n            model_shadow_tag_list = [tag.strip(""\\"""") for tag in model_shadow_tag_list]\n        else:\n            model_shadow_tag_list = model_shadow_tag_list.strip(""\\"""")\n            if model_shadow_tag_list:\n                model_shadow_tag_list = [model_shadow_tag_list]\n            else:\n                model_shadow_tag_list = []\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    try:\n        _validate_and_prep_resource_split_tag_and_weight_dict(model_split_tag_and_weight_dict)\n    except ValueError as ve:\n        return_dict = {\n            ""status"": ""incomplete"",\n            ""error_message"": ve\n        }\n\n        if _http_mode:\n            return return_dict\n            # return _jsonify(return_dict)\n        else:\n            return return_dict\n\n    for model_tag in model_shadow_tag_list:\n        error_message = \'\'\'\n        Model variants targeted for traffic-shadow must also bet set to\n        0 percent traffic-split as follows: \n        --model-split-tag-and-weight-dict=\\\'{""%s"":0,...}\\\'.\\\' % model_tag\n        \'\'\'\n        try:\n            if int(model_split_tag_and_weight_dict[model_tag]) != 0:\n                return_dict = {\n                    ""status"": ""incomplete"",\n                    ""error_message"": error_message\n                }\n                if _http_mode:\n                    return return_dict\n                    # return _jsonify(return_dict)\n                else:\n                    return return_dict\n        except KeyError:\n            return_dict = {\n                ""status"": ""incomplete"",\n                ""error_message"": error_message\n            }\n            if _http_mode:\n                return return_dict\n                # return _jsonify(return_dict)\n            else:\n                return return_dict\n\n    model_shadow_tag_list = [_validate_and_prep_tag(model_tag) for model_tag in model_shadow_tag_list]\n    model_split_tag_list = [_validate_and_prep_tag(model_tag) for model_tag in model_split_tag_and_weight_dict.keys()]\n    model_split_weight_list = list(model_split_tag_and_weight_dict.values())\n    context = {\n        \'PIPELINE_NAMESPACE\': namespace,\n        \'PIPELINE_IMAGE_REGISTRY_NAMESPACE\': image_registry_namespace,\n        \'PIPELINE_RESOURCE_NAME\': model_name,\n        \'PIPELINE_RESOURCE_SPLIT_TAG_LIST\': model_split_tag_list,\n        \'PIPELINE_RESOURCE_SPLIT_WEIGHT_LIST\': model_split_weight_list,\n        \'PIPELINE_RESOURCE_NUM_SPLIT_TAGS_AND_WEIGHTS\': len(model_split_tag_list),\n        \'PIPELINE_RESOURCE_SHADOW_TAG_LIST\': model_shadow_tag_list,\n        \'PIPELINE_RESOURCE_NUM_SHADOW_TAGS\': len(model_shadow_tag_list)\n    }\n\n    model_router_routerules_yaml_templates_path = _os.path.normpath(_os.path.join(\n        pipeline_templates_path,\n        _kube_routerules_template_registry[\'predict\'][0][0])\n    )\n    path, filename = _os.path.split(model_router_routerules_yaml_templates_path)\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n\n    # Operating systems limit the length of file names\n    # Code below is commented because the generated yaml file name gets too long and raises\n    # OSError: [Errno 36] File name too long\n    # split_tag_weight_filename_snippet = \'split\'\n    # for idx in range(len(model_split_tag_list)):\n    #     split_tag_weight_filename_snippet = \'%s-%s-%s\' % (split_tag_weight_filename_snippet, model_split_tag_list[idx], model_split_weight_list[idx])\n    # split_tag_weight_filename_snippet = split_tag_weight_filename_snippet.lstrip(\'-\')\n    # split_tag_weight_filename_snippet = split_tag_weight_filename_snippet.rstrip(\'-\')\n    # shadow_tag_filename_snippet = \'shadow\'\n    # for idx in range(len(model_shadow_tag_list)):\n    #     shadow_tag_filename_snippet = \'%s-%s\' % (shadow_tag_filename_snippet, model_shadow_tag_list[idx])\n    # shadow_tag_filename_snippet = shadow_tag_filename_snippet.lstrip(\'-\')\n    # shadow_tag_filename_snippet = shadow_tag_filename_snippet.rstrip(\'-\')\n    # rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-%s-router-routerules.yaml\' % (image_registry_namespace, model_name, split_tag_weight_filename_snippet, shadow_tag_filename_snippet))\n\n    # refactoring rendered_filename\n    # removing shadow_tag_filename_snippet and split_tag_weight_filename_snippet\n    # to resolve OSError: [Errno 36] File name too long\n    # refactored naming convention is limited to model name to match the\n    # identifier being used to group, compare and route model variants\n    rendered_filename = _os.path.normpath(\n        \'.pipeline-generated-%s-%s-router-routerules.yaml\'\n        % (image_registry_namespace, model_name)\n    )\n    with open(rendered_filename, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_filename))\n    _kube_apply(rendered_filename, namespace)\n\n    return_dict = {\n        ""status"": ""complete"",\n        ""model_split_tag_and_weight_dict"": model_split_tag_and_weight_dict,\n        ""model_shadow_tag_list"": model_shadow_tag_list\n    }\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\n# ie. http://localhost:32000/predict-kube-stop/mnist/a\n\ndef predict_kube_stop(model_name,\n                      model_tag,\n                      namespace=None,\n                      image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n    _service_stop(service_name=service_name,\n                  namespace=namespace)\n\n    # TODO:  Also remove from ingress\n\n    return_dict = {""status"": ""complete"",\n                   ""model_name"": model_name,\n                   ""model_tag"": model_tag}\n\n    if _http_mode:\n        return return_dict\n        # return _jsonify(return_dict)\n    else:\n        return return_dict\n\n\ndef _service_stop(service_name,\n                  namespace=None):\n\n    if not namespace:\n        namespace = _default_namespace\n\n    _kubeconfig.load_kube_config()\n    kubeclient_v1 = _kubeclient.CoreV1Api()\n    kubeclient_v1_beta1 = _kubeclient.ExtensionsV1beta1Api()\n\n    with _warnings.catch_warnings():\n        _warnings.simplefilter(""ignore"")\n\n        # Remove deployment\n        response = kubeclient_v1_beta1.list_deployment_for_all_namespaces(watch=False, pretty=True)\n        found = False\n        deployments = response.items\n        for deploy in deployments:\n            if service_name in deploy.metadata.name:\n                found = True\n                break\n        if found:\n            print("""")\n            print(""Deleting \'%s\' deployment."" % deploy.metadata.name)\n            print("""")\n            cmd = ""kubectl delete deploy %s --namespace %s"" % (deploy.metadata.name, namespace)\n            print(""Running \'%s\'."" % cmd)\n            print("""")\n            _subprocess.call(cmd, shell=True)\n            print("""")\n\n        # Remove service\n        response = kubeclient_v1.list_service_for_all_namespaces(watch=False, pretty=True)\n        found = False\n        deployments = response.items\n        for deploy in deployments:\n            if service_name in deploy.metadata.name:\n                found = True\n                break\n        if found:\n            print(""Deleting \'%s\' service."" % deploy.metadata.name)\n            print("""")\n            cmd = ""kubectl delete svc %s --namespace %s"" % (deploy.metadata.name, namespace)\n            print(""Running \'%s\'."" % cmd)\n            print("""")\n            _subprocess.call(cmd, shell=True)\n            print("""")\n\n\ndef train_server_pull(model_name,\n                      model_tag,\n                      image_registry_url=None,\n                      image_registry_repo=None,\n                      image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    cmd = \'docker pull %s/%s/%s-%s:%s\' % (image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag)\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n\ndef train_server_register(model_name,\n                          model_tag,\n                          image_registry_url=None,\n                          image_registry_repo=None,\n                          image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    cmd = \'docker push %s/%s/%s-%s:%s\' % (image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag)\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n\ndef train_server_logs(model_name,\n                      model_tag,\n                      image_registry_namespace=None,\n                      logs_cmd=\'docker\'):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n    print("""")\n    cmd = \'%s logs -f %s\' % (logs_cmd, container_name)\n    print(cmd)\n    print("""")\n\n    _subprocess.call(cmd, shell=True)\n\n\ndef train_server_shell(model_name,\n                       model_tag,\n                       image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    cmd = \'docker exec -it %s bash\' % container_name\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n\ndef _create_train_server_Dockerfile(model_name,\n                                    model_tag,\n                                    model_path,\n                                    model_type,\n                                    model_runtime,\n                                    model_chip,\n                                    stream_logger_url,\n                                    stream_logger_topic,\n                                    stream_input_url,\n                                    stream_input_topic,\n                                    stream_output_url,\n                                    stream_output_topic,\n                                    image_registry_url,\n                                    image_registry_repo,\n                                    image_registry_namespace,\n                                    image_registry_base_tag,\n                                    image_registry_base_chip,\n                                    pipeline_templates_path):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    print("""")\n    print(""Using templates in \'%s\'."" % pipeline_templates_path)\n    print(""(Specify --pipeline-templates-path if the templates live elsewhere.)"")\n    print("""")\n\n    context = {\n               \'PIPELINE_RESOURCE_NAME\': model_name,\n               \'PIPELINE_RESOURCE_TAG\': model_tag,\n               \'PIPELINE_RESOURCE_PATH\': model_path,\n               \'PIPELINE_RESOURCE_TYPE\': \'model\',\n               \'PIPELINE_RESOURCE_SUBTYPE\': model_type,\n               \'PIPELINE_NAME\': model_name,\n               \'PIPELINE_TAG\': model_tag,\n               \'PIPELINE_RUNTIME\': model_runtime,\n               \'PIPELINE_CHIP\': model_chip,\n               \'PIPELINE_STREAM_LOGGER_URL\': stream_logger_url,\n               \'PIPELINE_STREAM_LOGGER_TOPIC\': stream_logger_topic,\n               \'PIPELINE_STREAM_INPUT_URL\': stream_input_url,\n               \'PIPELINE_STREAM_INPUT_TOPIC\': stream_input_topic,\n               \'PIPELINE_STREAM_OUTPUT_URL\': stream_output_url,\n               \'PIPELINE_STREAM_OUTPUT_TOPIC\': stream_output_topic,\n               \'PIPELINE_IMAGE_REGISTRY_URL\': image_registry_url,\n               \'PIPELINE_IMAGE_REGISTRY_REPO\': image_registry_repo,\n               \'PIPELINE_IMAGE_REGISTRY_NAMESPACE\': image_registry_namespace,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_TAG\': image_registry_base_tag,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_CHIP\': image_registry_base_chip,\n              }\n\n    model_train_cpu_Dockerfile_templates_path = _os.path.normpath(_os.path.join(pipeline_templates_path, _dockerfile_template_registry[\'train\'][0][0]))\n    path, filename = _os.path.split(model_train_cpu_Dockerfile_templates_path)\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-Dockerfile\' % (image_registry_namespace, model_name, model_tag))\n    with open(rendered_filename, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_filename))\n\n    return rendered_filename\n\n\n#\n# model_name: mnist\n# model_tag: gpu\n# model_path: tensorflow/mnist-gpu/model/\n# model_type: tensorflow\n# model_runtime: tfserving\n# model_chip: gpu\n#\ndef train_server_build(model_name,\n                       model_tag,\n                       model_path,\n                       model_type,\n                       model_runtime=None,\n                       model_chip=None,\n                       http_proxy=None,\n                       https_proxy=None,\n                       stream_logger_url=None,\n                       stream_logger_topic=None,\n                       stream_input_url=None,\n                       stream_input_topic=None,\n                       stream_output_url=None,\n                       stream_output_topic=None,\n                       build_type=None,\n                       build_context_path=None,\n                       image_registry_url=None,\n                       image_registry_repo=None,\n                       image_registry_namespace=None,\n                       image_registry_base_tag=None,\n                       image_registry_base_chip=None,\n                       pipeline_templates_path=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not model_runtime:\n        model_runtime = _get_default_model_runtime(model_type)\n\n    if not model_chip:\n        model_chip = _default_model_chip\n\n    if not build_type:\n        build_type = _default_build_type\n\n    if not build_context_path:\n        build_context_path = _default_build_context_path\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    if not image_registry_base_tag:\n        image_registry_base_tag = _default_image_registry_base_tag\n\n    if not image_registry_base_chip:\n        image_registry_base_chip = model_chip\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    build_context_path = _os.path.normpath(build_context_path)\n    build_context_path = _os.path.expandvars(build_context_path)\n    build_context_path = _os.path.expanduser(build_context_path)\n    build_context_path = _os.path.normpath(build_context_path)\n    build_context_path = _os.path.abspath(build_context_path)\n    build_context_path = _os.path.normpath(build_context_path)\n\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.expandvars(pipeline_templates_path)\n    pipeline_templates_path = _os.path.expanduser(pipeline_templates_path)\n    pipeline_templates_path = _os.path.abspath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.relpath(pipeline_templates_path, build_context_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n\n    model_path = _os.path.normpath(model_path)\n    model_path = _os.path.expandvars(model_path)\n    model_path = _os.path.expanduser(model_path)\n    model_path = _os.path.abspath(model_path)\n    model_path = _os.path.normpath(model_path)\n    model_path = _os.path.relpath(model_path, build_context_path)\n    model_path = _os.path.normpath(model_path)\n\n    if build_type == \'docker\':\n        generated_Dockerfile = _create_train_server_Dockerfile(model_name=model_name,\n                                                               model_tag=model_tag,\n                                                               model_path=model_path,\n                                                               model_type=model_type,\n                                                               model_runtime=model_runtime,\n                                                               model_chip=model_chip,\n                                                               stream_logger_url=stream_logger_url,\n                                                               stream_logger_topic=stream_logger_topic,\n                                                               stream_input_url=stream_input_url,\n                                                               stream_input_topic=stream_input_topic,\n                                                               stream_output_url=stream_output_url,\n                                                               stream_output_topic=stream_output_topic,\n                                                               image_registry_url=image_registry_url,\n                                                               image_registry_repo=image_registry_repo,\n                                                               image_registry_namespace=image_registry_namespace,\n                                                               image_registry_base_tag=image_registry_base_tag,\n                                                               image_registry_base_chip=image_registry_base_chip,\n                                                               pipeline_templates_path=pipeline_templates_path)\n\n        if http_proxy:\n            http_proxy_build_arg_snippet = \'--build-arg HTTP_PROXY=%s\' % http_proxy\n        else:\n            http_proxy_build_arg_snippet = \'\'\n\n        if https_proxy:\n            https_proxy_build_arg_snippet = \'--build-arg HTTPS_PROXY=%s\' % https_proxy\n        else:\n            https_proxy_build_arg_snippet = \'\'\n\n        cmd = \'docker build --network=host %s %s -t %s/%s/%s-%s:%s -f %s %s\' % (http_proxy_build_arg_snippet, https_proxy_build_arg_snippet, image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag, generated_Dockerfile, model_path)\n\n        print(cmd)\n        print("""")\n        _subprocess.call(cmd, shell=True)\n    else:\n        print(""Build type \'%s\' not found."" % build_type)\n\n\ndef train_server_start(model_name,\n                       model_tag,\n                       input_host_path,\n                       output_host_path,\n#                       train_host_path,\n                       train_args,\n                       single_server_only=\'true\',\n                       stream_logger_url=None,\n                       stream_logger_topic=None,\n                       stream_input_url=None,\n                       stream_input_topic=None,\n                       stream_output_url=None,\n                       stream_output_topic=None,\n                       train_memory_limit=None,\n                       image_registry_url=None,\n                       image_registry_repo=None,\n                       image_registry_namespace=None,\n                       start_cmd=\'docker\',\n                       start_cmd_extra_args=\'\'):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if _is_base64_encoded(input_host_path):\n        input_host_path = _decode_base64(input_host_path)\n    input_host_path = _os.path.expandvars(input_host_path)\n    input_host_path = _os.path.expanduser(input_host_path)\n    input_host_path = _os.path.normpath(input_host_path)\n    input_host_path = _os.path.abspath(input_host_path)\n\n    if _is_base64_encoded(output_host_path):\n        output_host_path = _decode_base64(output_host_path)\n    output_host_path = _os.path.expandvars(output_host_path)\n    output_host_path = _os.path.expanduser(output_host_path)\n    output_host_path = _os.path.normpath(output_host_path)\n    output_host_path = _os.path.abspath(output_host_path)\n\n#    if _is_base64_encoded(train_host_path):\n#        train_host_path = _decode_base64(train_host_path)\n#    train_host_path = _os.path.expandvars(train_host_path)\n#    train_host_path = _os.path.expanduser(train_host_path)\n#    train_host_path = _os.path.normpath(train_host_path)\n#    train_host_path = _os.path.abspath(train_host_path)\n\n    if _is_base64_encoded(train_args):\n        train_args = _decode_base64(train_args)\n    # Note:  train_args are not currently expanded, so they are handled as is\n    #        in other words, don\'t expect ~ to become /Users/cfregly/..., etc like the above paths\n    #        the logic below isn\'t working properly.  it creates the following in the Docker cmd:\n    #        -e PIPELINE_TRAIN_ARGS=""/Users/cfregly/pipelineai/models/tensorflow/mnist-v3/model/--train_epochs=2 --batch_size=100\n    # train_args = _os.path.expandvars(train_args)\n    # train_args = _os.path.expanduser(train_args)\n    # train_args = _os.path.normpath(train_args)\n    # train_args = _os.path.abspath(train_args)\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    # Trying to avoid this:\n    #   WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\n    #\n    # https://docs.docker.com/config/containers/resource_constraints/#limit-a-containers-access-to-memory\n    #\n    if not train_memory_limit:\n        train_memory_limit = \'\'\n    else:\n        train_memory_limit = \'--memory=%s --memory-swap=%s\' % (train_memory_limit, train_memory_limit)\n\n    # environment == local, task type == worker, and no cluster definition\n    tf_config_local_run = \'\\\'{\\""environment\\"": \\""local\\"", \\""task\\"":{\\""type\\"": \\""worker\\""}}\\\'\'\n\n    # Note:  We added `train` to mimic AWS SageMaker and encourage ENTRYPOINT vs CMD per https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html\n    # /opt/ml/input/data/{training|validation|testing} per https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    # Note:  The %s:<paths> below must match the paths in templates/docker/train-server-local-dockerfile.template\n    # Any changes to these paths must be sync\'d with train-server-local-dockerfile.template, train-cluster.yaml.template, and train-cluster-gpu.yaml.template\n    # Also, /opt/ml/model is already burned into the Docker image at this point, so we can\'t specify it from the outside.  (This is by design.)\n    cmd = \'%s run -itd -p 2222:2222 -p 6006:6006 -e PIPELINE_SINGLE_SERVER_ONLY=%s -e PIPELINE_STREAM_LOGGER_URL=%s -e PIPELINE_STREAM_LOGGER_TOPIC=%s -e PIPELINE_STREAM_INPUT_URL=%s -e PIPELINE_STREAM_INPUT_TOPIC=%s -e PIPELINE_STREAM_OUTPUT_URL=%s -e PIPELINE_STREAM_OUTPUT_TOPIC=%s -e TF_CONFIG=%s -e PIPELINE_TRAIN_ARGS=""%s"" -v %s:/opt/ml/input/ -v %s:/opt/ml/output/ --name=%s %s %s %s/%s/%s-%s:%s train\' % (start_cmd, single_server_only, stream_logger_url, stream_logger_topic, stream_input_url, stream_input_topic, stream_output_url, stream_output_topic, tf_config_local_run, train_args, input_host_path, output_host_path, container_name, train_memory_limit, start_cmd_extra_args, image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag)\n    print("""")\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n    print("""")\n    print(""==> IGNORE ANY \'WARNING\' ABOVE.  IT\'S WORKING OK!!"")\n    print("""")\n    print(""Container start: %s"" % container_name)\n    print("""")\n    print(""==> Use \'pipeline train-server-logs --model-name=%s --model-tag=%s\' to see the container logs."" % (model_name, model_tag))\n    print("""")\n\n\ndef train_server_stop(model_name,\n                      model_tag,\n                      image_registry_namespace=None,\n                      stop_cmd=\'docker\'):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    container_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n    print("""")\n    cmd = \'%s rm -f %s\' % (stop_cmd, container_name)\n    print(cmd)\n    print("""")\n    _subprocess.call(cmd, shell=True)\n\n\ndef _create_train_kube_yaml(model_name,\n                            model_tag,\n                            input_host_path,\n                            output_host_path,\n #                           train_host_path,\n                            model_chip,\n                            train_args,\n                            stream_logger_url,\n                            stream_logger_topic,\n                            stream_input_url,\n                            stream_input_topic,\n                            stream_output_url,\n                            stream_output_topic,\n                            master_replicas,\n                            ps_replicas,\n                            worker_replicas,\n                            image_registry_url,\n                            image_registry_repo,\n                            image_registry_namespace,\n                            image_registry_base_tag,\n                            image_registry_base_chip,\n                            pipeline_templates_path,\n                            namespace):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    context = {\n               \'PIPELINE_NAMESPACE\': namespace,\n               \'PIPELINE_RESOURCE_NAME\': model_name,\n               \'PIPELINE_RESOURCE_TAG\': model_tag,\n               \'PIPELINE_NAME\': model_name,\n               \'PIPELINE_TAG\': model_tag,\n               \'PIPELINE_CHIP\': model_chip,\n               \'PIPELINE_TRAIN_ARGS\': train_args,\n               \'PIPELINE_INPUT_HOST_PATH\': input_host_path,\n               \'PIPELINE_OUTPUT_HOST_PATH\': output_host_path,\n#               \'PIPELINE_TRAIN_HOST_PATH\': train_host_path,\n               \'PIPELINE_STREAM_LOGGER_URL\': stream_logger_url,\n               \'PIPELINE_STREAM_LOGGER_TOPIC\': stream_logger_topic,\n               \'PIPELINE_STREAM_INPUT_URL\': stream_input_url,\n               \'PIPELINE_STREAM_INPUT_TOPIC\': stream_input_topic,\n               \'PIPELINE_STREAM_OUTPUT_URL\': stream_output_url,\n               \'PIPELINE_STREAM_OUTPUT_TOPIC\': stream_output_topic,\n               \'PIPELINE_MASTER_REPLICAS\': int(master_replicas),\n               \'PIPELINE_PS_REPLICAS\': int(ps_replicas),\n               \'PIPELINE_WORKER_REPLICAS\': int(worker_replicas),\n               \'PIPELINE_IMAGE_REGISTRY_URL\': image_registry_url,\n               \'PIPELINE_IMAGE_REGISTRY_REPO\': image_registry_repo,\n               \'PIPELINE_IMAGE_REGISTRY_NAMESPACE\': image_registry_namespace,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_TAG\': image_registry_base_tag,\n               \'PIPELINE_IMAGE_REGISTRY_BASE_CHIP\': image_registry_base_chip,\n               }\n\n    if model_chip == \'gpu\':\n        predict_clustered_template = _os.path.normpath(_os.path.join(pipeline_templates_path, _kube_deploy_template_registry[\'train\'][0][0]))\n        path, filename = _os.path.split(predict_clustered_template)\n        rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n        rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-%s.yaml\' % (image_registry_namespace, model_name, model_tag, model_chip))\n        with open(rendered_filename, \'wt\') as fh:\n            fh.write(rendered)\n    else:\n        predict_clustered_template = _os.path.normpath(_os.path.join(pipeline_templates_path, _kube_deploy_template_registry[\'train\'][0][0]))\n        path, filename = _os.path.split(predict_clustered_template)\n        rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n        rendered_filename = _os.path.normpath(\'.pipeline-generated-%s-%s-%s-%s.yaml\' % (image_registry_namespace, model_name, model_tag, model_chip))\n        with open(rendered_filename, \'wt\') as fh:\n            fh.write(rendered)\n\n    print(""\'%s\' => \'%s\'."" % (filename, rendered_filename))\n\n    return rendered_filename\n\n\ndef train_kube_connect(model_name,\n                       model_tag,\n                       local_port=None,\n                       service_port=None,\n                       namespace=None,\n                       image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    _service_connect(service_name=service_name,\n                     namespace=namespace,\n                     local_port=local_port,\n                     service_port=service_port)\n\n\ndef train_kube_describe(model_name,\n                        model_tag,\n                        namespace=None,\n                        image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    return _service_describe(service_name=service_name,\n                             namespace=namespace)\n\n\ndef train_kube_shell(model_name,\n                     model_tag,\n                     namespace=None,\n                     image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    _service_shell(service_name=service_name,\n                   namespace=namespace)\n\n\ndef train_kube_start(model_name,\n                     model_tag,\n                     input_host_path,\n                     output_host_path,\n#                     train_host_path,\n                     train_args,\n                     model_chip=None,\n                     master_replicas=1,\n                     ps_replicas=1,\n                     worker_replicas=1,\n                     stream_logger_url=None,\n                     stream_logger_topic=None,\n                     stream_input_url=None,\n                     stream_input_topic=None,\n                     stream_output_url=None,\n                     stream_output_topic=None,\n                     image_registry_url=None,\n                     image_registry_repo=None,\n                     image_registry_namespace=None,\n                     image_registry_base_tag=None,\n                     image_registry_base_chip=None,\n                     pipeline_templates_path=None,\n                     namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not model_chip:\n        model_chip = _default_model_chip\n\n    print(input_host_path)\n\n    if _is_base64_encoded(input_host_path):\n        input_host_path = _decode_base64(input_host_path)\n    input_host_path = _os.path.expandvars(input_host_path)\n    input_host_path = _os.path.expanduser(input_host_path)\n    input_host_path = _os.path.normpath(input_host_path)\n    input_host_path = _os.path.abspath(input_host_path)\n\n    if _is_base64_encoded(output_host_path):\n        output_host_path = _decode_base64(output_host_path)\n    output_host_path = _os.path.expandvars(output_host_path)\n    output_host_path = _os.path.expanduser(output_host_path)\n    output_host_path = _os.path.normpath(output_host_path)\n    output_host_path = _os.path.abspath(output_host_path)\n\n#    if _is_base64_encoded(train_host_path):\n#        train_host_path = _decode_base64(train_host_path)\n#    train_host_path = _os.path.expandvars(train_host_path)\n#    train_host_path = _os.path.expanduser(train_host_path)\n#    train_host_path = _os.path.normpath(train_host_path)\n#    train_host_path = _os.path.abspath(train_host_path)\n\n    if _is_base64_encoded(train_args):\n        train_args = _decode_base64(train_args)\n    train_args = _os.path.expandvars(train_args)\n    train_args = _os.path.expanduser(train_args)\n    train_args = _os.path.normpath(train_args)\n    train_args = _os.path.abspath(train_args)\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    if not image_registry_base_tag:\n        image_registry_base_tag = _default_image_registry_base_tag\n\n    if not image_registry_base_chip:\n        image_registry_base_chip = model_chip\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    pipeline_templates_path = _os.path.expandvars(pipeline_templates_path)\n    pipeline_templates_path = _os.path.expanduser(pipeline_templates_path)\n    pipeline_templates_path = _os.path.abspath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    generated_yaml_path = _create_train_kube_yaml(model_name=model_name,\n                                                  model_tag=model_tag,\n                                                  model_chip=model_chip,\n                                                  input_host_path=input_host_path,\n                                                  output_host_path=output_host_path,\n#                                                  train_host_path=train_host_path,\n                                                  train_args=train_args,\n                                                  stream_logger_url=stream_logger_url,\n                                                  stream_logger_topic=stream_logger_topic,\n                                                  stream_input_url=stream_input_url,\n                                                  stream_input_topic=stream_input_topic,\n                                                  stream_output_url=stream_output_url,\n                                                  stream_output_topic=stream_output_topic,\n                                                  master_replicas=master_replicas,\n                                                  ps_replicas=ps_replicas,\n                                                  worker_replicas=worker_replicas,\n                                                  image_registry_url=image_registry_url,\n                                                  image_registry_repo=image_registry_repo,\n                                                  image_registry_namespace=image_registry_namespace,\n                                                  image_registry_base_tag=image_registry_base_tag,\n                                                  image_registry_base_chip=image_registry_base_chip,\n                                                  pipeline_templates_path=pipeline_templates_path,\n                                                  namespace=namespace)\n\n    generated_yaml_path = _os.path.normpath(generated_yaml_path)\n\n    # For now, only handle \'-deploy\' and \'-svc\' yaml\'s\n    _kube_apply(yaml_path=generated_yaml_path,\n                namespace=namespace)\n\n\ndef train_kube_stop(model_name,\n                    model_tag,\n                    namespace=None,\n                    image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    _service_stop(service_name=service_name,\n                         namespace=namespace)\n\n\ndef train_kube_logs(model_name,\n                    model_tag,\n                    namespace=None,\n                    image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    _service_logs(service_name=service_name,\n                         namespace=namespace)\n\n\ndef train_kube_scale(model_name,\n                     model_tag,\n                     replicas,\n                     namespace=None,\n                     image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_train_namespace\n\n    service_name = \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag)\n\n    _service_scale(service_name=service_name,\n                   replicas=replicas,\n                   namespace=namespace)\n\n\ndef predict_sage_start(model_name,\n                       model_tag,\n                       aws_iam_arn,\n                       namespace=None,\n                       image_registry_url=None,\n                       image_registry_repo=None,\n                       image_registry_namespace=None,\n                       image_registry_base_tag=None,\n                       pipeline_templates_path=None):\n\n    model_name = _validate_and_prep_name(model_name)\n    model_tag = _validate_and_prep_tag(model_tag)\n\n    if not namespace:\n        namespace = _default_namespace\n\n    if not image_registry_url:\n        image_registry_url = _default_image_registry_url\n\n    if not image_registry_repo:\n        image_registry_repo = _default_image_registry_repo\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    if not image_registry_base_tag:\n        image_registry_base_tag = _default_image_registry_base_tag\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    # Create Model\n    begin_time = _datetime.now()\n\n    sagemaker_admin_client = _boto3.client(\'sagemaker\')\n    response = sagemaker_admin_client.create_model(\n        ModelName=\'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag),\n        PrimaryContainer={\n            \'ContainerHostname\': \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag),\n            \'Image\': \'%s/%s/%s-%s:%s\' % (image_registry_url, image_registry_repo, image_registry_namespace, model_name, model_tag),\n            \'Environment\': {\n            }\n        },\n        ExecutionRoleArn=\'%s\' % aws_iam_arn,\n        Tags=[\n            {\n                \'Key\': \'PIPELINE_RESOURCE_NAME\',\n                \'Value\': \'%s\' % model_name\n            },\n            {\n                \'Key\': \'PIPELINE_RESOURCE_TAG\',\n                \'Value\': \'%s\' % model_tag\n            },\n#            {\n#                \'Key\': \'PIPELINE_RESOURCE_SUBTYPE\',\n#                \'Value\': \'%s\' % model_type\n#            },\n#            {\n#                \'Key\': \'PIPELINE_RUNTIME\',\n#                \'Value\': \'%s\' % model_runtime\n#            },\n#            {\n#                \'Key\': \'PIPELINE_CHIP\',\n#                \'Value\': \'%s\' % model_chip\n#            },\n        ]\n    )\n\n    model_region = \'UNKNOWN_REGION\'\n    if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n        model_arn = response[\'ModelArn\']\n        print("""")\n        print(""ModelArn: \'%s\'"" % model_arn)\n        model_region = model_arn.split(\':\')[3]\n        print("""")\n    else:\n        return\n\n    end_time = _datetime.now()\n\n    total_time = end_time - begin_time\n    print("""")\n    print(""Request time: %s milliseconds"" % (total_time.microseconds / 1000))\n    print("""")\n\n    return _get_sage_endpoint_url(model_name=model_name,\n                                  model_region=model_region,\n                                  image_registry_namespace=image_registry_namespace)\n\n# TODO:  Verify that this works now that AWS SageMaker has fixed a bug\n#\n#   aws sagemaker update-endpoint-weights-and-capacities --endpoint-name=arn:aws:sagemaker:us-west-2:954636985443:endpoint-config/predict-mnist --desired-weights-and-capacities=\'[{""VariantName"": ""predict-mnist-gpu"", ""DesiredWeight"": 100, ""DesiredInstanceCount"": 1}]\'\n#\n#   aws sagemaker update-endpoint-weights-and-capacities --endpoint-name=arn:aws:sagemaker:us-west-2:954636985443:endpoint-config/predict-mnist --desired-weights-and-capacities=VariantName=predict-mnist-gpu,DesiredWeight=100,DesiredInstanceCount=1\n#\ndef predict_sage_route(model_name,\n                       aws_instance_type_dict,\n                       model_split_tag_and_weight_dict,\n                       pipeline_templates_path=None,\n                       image_registry_namespace=None):\n\n    model_name = _validate_and_prep_name(model_name)\n\n    # Instance Types:\n    #   \'ml.c4.2xlarge\'|\'ml.c4.8xlarge\'|\'ml.c4.xlarge\'|\'ml.c5.2xlarge\'|\'ml.c5.9xlarge\'|\'ml.c5.xlarge\'|\'ml.m4.xlarge\'|\'ml.p2.xlarge\'|\'ml.p3.2xlarge\'|\'ml.t2.medium\',\n    if type(aws_instance_type_dict) is str:\n        aws_instance_type_dict = _base64.b64decode(aws_instance_type_dict)\n        aws_instance_type_dict = _json.loads(aws_instance_type_dict)\n\n    if type(model_split_tag_and_weight_dict) is str:\n        model_split_tag_and_weight_dict = _base64.b64decode(model_split_tag_and_weight_dict)\n        model_split_tag_and_weight_dict = _json.loads(model_split_tag_and_weight_dict)\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    try:\n        _validate_and_prep_resource_split_tag_and_weight_dict(model_split_tag_and_weight_dict)\n    except ValueError as ve:\n        return_dict = {""status"": ""incomplete"",\n                       ""error_message"": ve}\n\n        if _http_mode:\n            return return_dict\n            # return _jsonify(return_dict)\n        else:\n            return return_dict\n\n    model_tag_list = [ _validate_and_prep_tag(model_tag) for model_tag in model_split_tag_and_weight_dict.keys() ]\n\n    sagemaker_admin_client = _boto3.client(\'sagemaker\')\n\n    begin_time = _datetime.now()\n\n    if not _get_sage_endpoint_config(model_name):\n        # Create Endpoint Configuration\n        tag_weight_dict_list = []\n\n        for model_tag in model_tag_list:\n            tag_weight_dict = {\n            \'VariantName\': \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag),\n            \'ModelName\': \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag),\n            \'InitialInstanceCount\': 1,\n            \'InstanceType\': \'%s\' % aws_instance_type_dict[model_tag],\n            \'InitialVariantWeight\': model_split_tag_and_weight_dict[model_tag],\n            }\n\n            tag_weight_dict_list += [tag_weight_dict]\n\n        print(tag_weight_dict_list)\n\n        response = sagemaker_admin_client.create_endpoint_config(\n            EndpointConfigName=\'%s-%s\' % (image_registry_namespace, model_name),\n            ProductionVariants=tag_weight_dict_list,\n            Tags=[\n            {\n                \'Key\': \'PIPELINE_NAME\',\n                \'Value\': \'%s\' % model_name\n            },\n            {\n                \'Key\': \'PIPELINE_TAG\',\n                \'Value\': \'%s\' % model_tag\n            },\n            {\n                \'Key\': \'PIPELINE_RESOURCE_NAME\',\n                \'Value\': \'%s\' % model_name\n            },\n            {\n                \'Key\': \'PIPELINE_RESOURCE_TAG\',\n                \'Value\': \'%s\' % model_tag\n            },\n            ]\n        )\n\n        if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n            print("""")\n            print(""EndpointConfigArn: \'%s\'"" % response[\'EndpointConfigArn\'])\n            print("""")\n        else:\n            return\n    else:\n        tag_weight_dict_list = []\n\n        for model_tag in model_tag_list:\n            tag_weight_dict = {\n            \'VariantName\': \'%s-%s-%s\' % (image_registry_namespace, model_name, model_tag),\n            \'DesiredWeight\': model_split_tag_and_weight_dict[model_tag],\n            \'DesiredInstanceCount\': 1\n            }\n\n            tag_weight_dict_list += [tag_weight_dict]\n\n        print(tag_weight_dict_list)\n\n        response = sagemaker_admin_client.update_endpoint_weights_and_capacities(\n            EndpointName=\'%s-%s\' % (image_registry_namespace, model_name),\n            DesiredWeightsAndCapacities=tag_weight_dict_list,\n        )\n\n        if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n            print("""")\n            print(""EndpointArn: \'%s\'"" % response[\'EndpointArn\'])\n            print("""")\n        else:\n            print(response[\'ResponseMetadata\'][\'HTTPStatusCode\'])\n            return\n\n    if not _get_sage_endpoint(model_name):\n        # Create Endpoint (Models + Endpoint Configuration)\n        response = sagemaker_admin_client.create_endpoint(\n            EndpointName=\'%s-%s\' % (image_registry_namespace, model_name),\n            EndpointConfigName=\'%s-%s\' % (image_registry_namespace, model_name),\n            Tags=[\n            {\n                \'Key\': \'PIPELINE_NAME\',\n                \'Value\': \'%s\' % model_name\n            },\n            {\n                \'Key\': \'PIPELINE_TAG\',\n                \'Value\': \'%s\' % model_tag\n            },\n            {\n                \'Key\': \'PIPELINE_RESOURCE_NAME\',\n                \'Value\': \'%s\' % model_name\n            },\n            {\n                \'Key\': \'PIPELINE_RESOURCE_TAG\',\n                \'Value\': \'%s\' % model_tag\n            },\n            ]\n        )\n\n        if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n            print("""")\n            print(""EndpointArn: \'%s\'"" % response[\'EndpointArn\'])\n            print("""")\n        else:\n            return\n\n    end_time = _datetime.now()\n\n    total_time = end_time - begin_time\n\n    print("""")\n    print(""Request time: %s milliseconds"" % (total_time.microseconds / 1000))\n    print("""")\n\n\ndef _get_sage_endpoint_config(model_name,\n                              image_registry_namespace=None):\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    sagemaker_admin_client = _boto3.client(\'sagemaker\')\n\n    begin_time = _datetime.now()\n\n    try:\n        response = sagemaker_admin_client.describe_endpoint_config(\n            EndpointConfigName=\'%s-%s\' % (image_registry_namespace, model_name),\n        )\n    except _ClientError:\n        return None\n\n    end_time = _datetime.now()\n\n    total_time = end_time - begin_time\n\n    if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n        print("""")\n        print(""EndpointConfigArn: \'%s\'"" % response[\'EndpointConfigArn\'])\n        print("""")\n    else:\n        print(response[\'ResponseMetadata\'][\'HTTPStatusCode\'])\n        return\n\n    print("""")\n    print(""Request time: %s milliseconds"" % (total_time.microseconds / 1000))\n    print("""")\n\n    return response[\'EndpointConfigArn\']\n\n\ndef _get_sage_endpoint(model_name,\n                       image_registry_namespace=None):\n\n    if not image_registry_namespace:\n        image_registry_namespace = _default_image_registry_predict_namespace\n\n    sagemaker_admin_client = _boto3.client(\'sagemaker\')\n\n    begin_time = _datetime.now()\n\n    try:\n        response = sagemaker_admin_client.describe_endpoint(\n            EndpointName=\'%s-%s\' % (image_registry_namespace, model_name),\n        )\n    except _ClientError:\n        return None\n\n    end_time = _datetime.now()\n\n    total_time = end_time - begin_time\n\n    model_region = \'UNKNOWN_REGION\'\n    if response and response[\'ResponseMetadata\'][\'HTTPStatusCode\'] == 200:\n        print("""")\n        model_arn = response[\'EndpointArn\']\n        print(""EndpointArn: \'%s\'"" % model_arn)\n        model_region = model_arn.split(\':\')[3]\n    else:\n        print(response[\'ResponseMetadata\'][\'HTTPStatusCode\'])\n        return\n\n    print(""Request time: %s milliseconds"" % (total_time.microseconds / 1000))\n    print("""")\n\n    return _get_sage_endpoint_url(model_name,\n                                  model_region,\n                                  image_registry_namespace)\n\n\ndef cluster_kube_uninstall(tag,\n                           ingress_type=\'nodeport\',\n                           chip=_default_model_chip,\n                           pipeline_templates_path=None,\n                           dry_run=False):\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    generated_path = \'~/.pipelineai/cluster/yaml\'\n    generated_path = _os.path.expandvars(generated_path)\n    generated_path = _os.path.expanduser(generated_path)\n    generated_path = _os.path.abspath(generated_path)\n    generated_path = _os.path.normpath(generated_path)\n\n    cmd = """"""\n# Admin\nkubectl delete deploy admin \nkubectl delete svc admin \n\n# Api / MLflow\nkubectl delete deploy api \nkubectl delete svc api \n\n# Notebook\nkubectl delete deploy notebook-%s \nkubectl delete svc notebook-%s\n\n# Hystrix\nkubectl delete deploy dashboard-hystrix \nkubectl delete svc dashboard-hystrix\n\n# Turbine (Part 1)\nkubectl delete deploy dashboard-turbine \nkubectl delete svc dashboard-turbine \n\n# Istio\nkubectl delete -f %s/.generated-istio-%s-1.0.5.yaml\nsleep 10\n\nkubectl delete -f %s/.generated-pipelineai-gateway.yaml\nsleep 10\n\nkubectl delete -f %s/.generated-virtualservice-admin.yaml\nkubectl delete -f %s/.generated-virtualservice-api.yaml\nkubectl delete -f %s/.generated-virtualservice-airflow.yaml\nkubectl delete -f %s/.generated-virtualservice-notebook-%s.yaml\nkubectl delete -f %s/.generated-virtualservice-hystrix.yaml\nkubectl delete -f %s/.generated-virtualservice-turbine.yaml\n\n# Remove Airflow (Requires pipelineai-cluster-admin rolebinding)\n#helm delete --purge airflow\n\n# Turbine (Part 2)\nkubectl delete clusterrolebinding pipelineai-serviceaccounts-view \n\n# Remove ability to create pods and istio assets\n# Can only delete this after removing Airflow\n#kubectl delete clusterrolebinding pipelineai-cluster-admin\n\n# Kafka\n#helm delete --purge kafka\n\n#kubectl delete -f %s/cluster/yaml/kafka/kafka-rest-svc.yaml\n#kubectl delete -f %s/cluster/yaml/kafka/kafka-svc.yaml\n"""""" % (\n       chip,\n       chip, \n       generated_path,\n       ingress_type,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       chip,\n       generated_path,\n       generated_path,\n       pipeline_templates_path,\n       pipeline_templates_path,\n    )\n\n    if not dry_run:\n        print(cmd)\n        response_bytes = _subprocess.check_output(cmd, shell=True)\n        return response_bytes.decode(\'utf-8\')\n    else:\n        return cmd\n\n\ndef cluster_kube_install(tag,\n                         image_registry_url,\n                         image_registry_username=\'\',\n                         image_registry_password=\'\',\n                         ingress_type=\'nodeport\',\n                         ui_gateway=\'gateway\',\n                         api_gateway=\'gateway\',\n                         namespace=\'default\',\n                         users_storage_gb=\'10Gi\',\n                         users_root_path=\'/mnt/pipelineai/users\',\n                         chip=_default_model_chip,\n                         pipeline_templates_path=None,\n                         dry_run=False):\n\n    # lowercase the ingress_type to match the file in the template\n    ingress_type = ingress_type.lower()\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    pipeline_templates_path = _os.path.expandvars(pipeline_templates_path)\n    pipeline_templates_path = _os.path.expanduser(pipeline_templates_path)\n    pipeline_templates_path = _os.path.abspath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n\n    context = {\n               \'PIPELINE_NAMESPACE\': namespace,\n               \'PIPELINE_UI_GATEWAY\': ui_gateway,\n               \'PIPELINE_API_GATEWAY\': api_gateway,\n               \'PIPELINE_USERS_STORAGE_GB\': users_storage_gb,\n               \'PIPELINE_USERS_ROOT_PATH\': users_root_path,\n               \'PIPELINE_IMAGE_REGISTRY_URL\': image_registry_url,\n               \'PIPELINE_IMAGE_REGISTRY_USERNAME\': image_registry_username,\n               \'PIPELINE_IMAGE_REGISTRY_PASSWORD\': image_registry_password,\n              }\n\n    if not pipeline_templates_path:\n        pipeline_templates_path = _default_pipeline_templates_path\n\n    pipeline_templates_path = _os.path.expandvars(pipeline_templates_path)\n    pipeline_templates_path = _os.path.expanduser(pipeline_templates_path)\n    pipeline_templates_path = _os.path.abspath(pipeline_templates_path)\n    pipeline_templates_path = _os.path.normpath(pipeline_templates_path)\n\n    generated_path = \'~/.pipelineai/\'\n    generated_path = _os.path.expandvars(generated_path)\n    generated_path = _os.path.expanduser(generated_path)\n    generated_path = _os.path.abspath(generated_path)\n    generated_path = _os.path.normpath(generated_path)\n\n    # Note: This isn\'t python3 compat (exist_ok)\n    # _os.makedirs(generated_path, exist_ok=True)\n\n    try: \n        _os.makedirs(generated_path)\n        _os.makedirs(_os.path.join(generated_path, \'cluster/yaml/\'))\n        _os.makedirs(_os.path.join(generated_path, \'cluster/config/\'))\n    except OSError:\n        if not _os.path.isdir(generated_path):\n            raise\n        else:\n            pass\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/config/\'))\n    filename = \'10-kubeadm.conf\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/config/10-kubeadm.conf\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/config/\'))\n    filename = \'kubeadm-init.yaml\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/config/kubeadm-init.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/storage/\'))\n    filename = \'openebs-storageclass.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-openebs-storageclass.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/istio/\'))\n    filename = \'pipelineai-gateway.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-pipelineai-gateway.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/istio/\'))\n    filename = \'virtualservice-airflow.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-virtualservice-airflow.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/istio/\'))\n    filename = \'virtualservice-mlflow.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-virtualservice-mlflow.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/istio/\'))\n    filename = \'virtualservice-grafana.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-virtualservice-grafana.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/istio/\'))\n    filename = \'crds.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-crds.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/istio/\'))\n    filename = \'istio-noauth.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-istio-noauth.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/airflow/\'))\n    filename = \'airflow-deploy.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-airflow-deploy.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/airflow/\'))\n    filename = \'airflow-svc.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-airflow-svc.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/mlflow/\'))\n    filename = \'mlflow-deploy.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-mlflow-deploy.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/mlflow/\'))\n    filename = \'mlflow-svc.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-mlflow-svc.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/redis/\'))\n    filename = \'redis-master-deploy.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-redis-master-deploy.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/redis/\'))\n    filename = \'redis-master-svc.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-redis-master-svc.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/mysql/\'))\n    filename = \'mysql-master-deploy.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-mysql-master-deploy.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/mysql/\'))\n    filename = \'mysql-master-svc.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-mysql-master-svc.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/storage/\'))\n    filename = \'users-kubeflow-pvc.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-users-kubeflow-pvc.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    path = _os.path.normpath(_os.path.join(pipeline_templates_path, \'cluster/yaml/storage/\'))\n    filename = \'tfevent-volume-pvc.yaml.template\'\n    rendered = _jinja2.Environment(loader=_jinja2.FileSystemLoader(path)).get_template(filename).render(context)\n    rendered_path = _os.path.join(generated_path, \'cluster/yaml/.generated-tfevent-volume-pvc.yaml\')\n    with open(rendered_path, \'wt\') as fh:\n        fh.write(rendered)\n        print(""\'%s\' => \'%s\'."" % (filename, rendered_path))\n\n    cmd = """"""\n# StorageClass\nkubectl create -f %s/cluster/yaml/.generated-openebs-storageclass.yaml\nsleep 3\n\n# PVC\nkubectl create -f %s/cluster/yaml/.generated-users-kubeflow-pvc.yaml\nkubectl create -f %s/cluster/yaml/.generated-tfevent-volume-pvc.yaml\n\n# MySql \nkubectl create -f %s/cluster/yaml/.generated-mysql-master-svc.yaml\nkubectl create -f %s/cluster/yaml/.generated-mysql-master-deploy.yaml\n\n# Redis\nkubectl create -f %s/cluster/yaml/.generated-redis-master-svc.yaml\nkubectl create -f %s/cluster/yaml/.generated-redis-master-deploy.yaml\n\n# Airflow\nkubectl create -f %s/cluster/yaml/.generated-airflow-svc.yaml\nkubectl create -f %s/cluster/yaml/.generated-airflow-deploy.yaml\n\n# MLflow\nkubectl create -f %s/cluster/yaml/.generated-mlflow-svc.yaml\nkubectl create -f %s/cluster/yaml/.generated-mlflow-deploy.yaml\n\n# Istio\nkubectl create -f %s/cluster/yaml/.generated-crds.yaml\nkubectl create -f %s/cluster/yaml/.generated-istio-noauth.yaml\nkubectl create -f %s/cluster/yaml/.generated-pipelineai-gateway.yaml\nkubectl create -f %s/cluster/yaml/.generated-virtualservice-airflow.yaml\nkubectl create -f %s/cluster/yaml/.generated-virtualservice-mlflow.yaml\nkubectl create -f %s/cluster/yaml/.generated-virtualservice-grafana.yaml\n"""""" % (\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,\n       generated_path,       \n       generated_path,\n       generated_path,\n       generated_path,\n)\n\n    if not dry_run:\n        print(cmd)\n        response_bytes = _subprocess.check_output(cmd, shell=True)\n        return response_bytes.decode(\'utf-8\')\n    else:\n        return cmd\n\n\ndef _get_short_user_id(user_id):\n    import hashlib\n    unique_id = user_id.split(\'|\').pop().encode()\n    return hashlib.sha512(unique_id).hexdigest()[0:8]\n\n\ndef _main():\n    #  WARNING:\n    #      the global variables below DO NOT WORK\n    #      the values are only available within this main(), not the code above\n    global _http_mode\n\n    print(_sys.argv)\n\n    if len(_sys.argv) == 1:\n        return help()\n    else:\n        _http_mode = False\n        _fire.Fire()\n\n\nif __name__ == \'__main__\':\n    _main()\n'"
cli/cli_pipeline/image_to_pixels.py,0,"b""import numpy as np\nfrom PIL import Image\nimport scipy\n\n# ------------- .png to pixels ----------------------------------------------------------------------------------\n#\n# convert a .png image to a list of pixel values\n#   mode = P (8-bit pixels, mapped to any other mode using a color palette)\n#\n# ------------- .png to pixels ----------------------------------------------------------------------------------\nprint('''\n.png to pixels\n\n      mode = P (8-bit pixels, mapped to any other mode using a color palette)\n\n''')\nimg_name = 'example5.png'\nimg = Image.open(img_name).convert('P')\nWIDTH, HEIGHT = img.size\ndata = list(img.getdata()) # convert image data to a list of integers\nprint('''\nimg_name: {}\nWIDTH: {}\nHEIGHT: {}\nimg.mode: {}\nimg.size: {}\nimage.data: {}\n\n'''.format(img_name, WIDTH, HEIGHT, img.mode, img.size, data))\n\n# convert image data list of integer pixel values to 2D list that can be displayed as an image\ndata = [data[offset:offset+WIDTH] for offset in range(0, WIDTH*HEIGHT, WIDTH)]\n\n# At this point the image's pixels are all in memory and can be accessed\n# individually using data[row][col].\n\nprint('Display the 2D list of pixel values as an image')\nfor row in data:\n    print(' '.join('{:3}'.format(value) for value in row))\n\n\n# ------------- pixels to .png ----------------------------------------------------------------------------------\n#\n# convert a list of pixel values to a .png image\n#   mode = P (8-bit pixels, mapped to any other mode using a color palette)\n#\n# ------------- pixels to .png ----------------------------------------------------------------------------------\nprint('''\n\n\n\npixels to .png\n        mode = P (8-bit pixels, mapped to any other mode using a color palette)\n\n''')\nimage_name = '7.png'\n# mnist standard image format is 28x28\nWIDTH = 28\nHEIGHT = 28\n# image data as a list of integers\n# x = '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 18 18 18 126 136 175 26 166 255 247 127 0 0 0 0 0 0 0 0 0 0 0 0 30 36 94 154 170 253 253 253 253 253 225 172 253 242 195 64 0 0 0 0 0 0 0 0 0 0 0 49 238 253 253 253 253 253 253 253 253 251 93 82 82 56 39 0 0 0 0 0 0 0 0 0 0 0 0 18 219 253 253 253 253 253 198 182 247 241 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 80 156 107 253 253 205 11 0 43 154 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 14 1 154 253 90 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 139 253 190 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 190 253 70 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 35 241 225 160 108 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 81 240 253 253 119 25 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 45 186 253 253 150 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16 93 252 253 187 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 249 253 249 64 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 46 130 183 253 253 207 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 39 148 229 253 253 253 250 182 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 24 114 221 253 253 253 253 201 78 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 23 66 213 253 253 253 253 198 81 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18 171 219 253 253 253 253 195 80 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 55 172 226 253 253 253 253 244 133 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 136 253 253 253 212 135 132 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'\n# image_data = list(map(int, x.split()))\nimage_data = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,150,238,254,255,237,150,150,225,161,221,203,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,66,146,253,253,253,253,253,253,253,253,253,253,253,173,0,0,0,0,0,0,0,0,0,0,0,0,0,140,251,253,253,178,114,114,114,114,114,114,167,253,253,154,0,0,0,0,0,0,0,0,0,0,0,12,84,240,253,248,170,28,0,0,0,0,0,0,90,253,250,90,0,0,0,0,0,0,0,0,0,0,10,129,226,253,235,128,0,0,0,0,0,0,0,8,188,253,190,0,0,0,0,0,0,0,0,0,0,0,56,250,253,246,98,0,0,0,0,0,0,0,0,76,243,234,100,0,0,0,0,0,0,0,0,0,0,0,185,253,248,44,0,0,0,0,0,0,0,0,34,245,253,95,0,0,0,0,0,0,0,0,0,0,0,0,69,187,87,0,0,0,0,0,0,0,0,22,164,253,223,63,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,55,247,253,85,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,230,253,184,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,145,253,241,43,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,57,250,253,206,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,135,253,253,40,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,105,251,248,108,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,68,226,253,180,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,40,233,253,205,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,189,253,240,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,110,253,253,109,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,242,253,159,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,43,198,228,24,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\njson_data = [image_data[offset:offset+WIDTH] for offset in range(0, WIDTH*HEIGHT, WIDTH)]\nimage = Image.fromarray(np.asarray(json_data, dtype=np.uint32))\nimage.show()\n# json_data = []\n\n# mode = P (8-bit pixels, mapped to any other mode using a color palette)\nimage_mode = 'P'\nimage_size = (WIDTH, HEIGHT)\nprint('''\nimage_name: {}\nWIDTH: {}\nHEIGHT: {}\nimg.mode: {}\nimg.size: {}\nimage.data: {}\n\n'''.format(image_name, WIDTH, HEIGHT, image_mode, image_size, image_data))\n\n# image_out = Image.new(image_mode, image_size)\n# image_out.putdata(json_data)\n# image_out.save(image_name)\n\n# json_data = [json_data[offset:offset+WIDTH] for offset in range(0, WIDTH*HEIGHT, WIDTH)]\n\nprint('Display the 2D list of pixel values as an image')\nfor row in json_data:\n    print(' '.join('{:3}'.format(value) for value in row))\n"""
kubeflow/airflow-dags/taxi_pipeline.py,0,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Chicago taxi example using TFX.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport logging\nimport os\nfrom tfx.components.evaluator.component import Evaluator\nfrom tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\nfrom tfx.components.example_validator.component import ExampleValidator\nfrom tfx.components.model_validator.component import ModelValidator\nfrom tfx.components.pusher.component import Pusher\nfrom tfx.components.schema_gen.component import SchemaGen\nfrom tfx.components.statistics_gen.component import StatisticsGen\nfrom tfx.components.trainer.component import Trainer\nfrom tfx.components.transform.component import Transform\nfrom tfx.orchestration import pipeline\nfrom tfx.orchestration.airflow.airflow_runner import AirflowDAGRunner\nfrom tfx.proto import evaluator_pb2\nfrom tfx.proto import pusher_pb2\nfrom tfx.proto import trainer_pb2\nfrom tfx.utils.dsl_utils import csv_input\n\n# This example assumes that the taxi data is stored in ~/taxi/data and the\n# taxi utility function is in ~/taxi.  Feel free to customize this as needed.\n_taxi_root = \'/mnt/pipelineai/users/airflow-dags\' \n_data_root = os.path.join(_taxi_root, \'data/taxi_data\')\n# Python module file to inject customized logic into the TFX components. The\n# Transform and Trainer both require user-defined functions to run successfully.\n_taxi_module_file = os.path.join(_taxi_root, \'taxi_utils.py\')\n# Path which can be listened to by the model server.  Pusher will output the\n# trained model here.\n_serving_model_dir = os.path.join(_taxi_root, \'saved_models/taxi\')\n\n# Directory and data locations.  This example assumes all of the chicago taxi\n# example code and metadata library is relative to $HOME, but you can store\n# these files anywhere on your local filesystem.\n_tfx_root = os.path.join(_taxi_root, \'tfx\')\n_pipeline_root = os.path.join(_tfx_root, \'pipelines\')\n_metadata_db_root = os.path.join(_tfx_root, \'metadata\')\n_log_root = os.path.join(_tfx_root, \'logs\')\n\n# Airflow-specific configs; these will be passed directly to airflow\n_airflow_config = {\n    \'schedule_interval\': None,\n    \'start_date\': datetime.datetime(2019, 1, 1),\n}\n\n# Logging overrides\nlogger_overrides = {\'log_root\': _log_root, \'log_level\': logging.INFO}\n\n\ndef _create_pipeline():\n  """"""Implements the chicago taxi pipeline with TFX.""""""\n  examples = csv_input(_data_root)\n\n  # Brings data into the pipeline or otherwise joins/converts training data.\n  example_gen = CsvExampleGen(input_base=examples)\n\n  # Computes statistics over data for visualization and example validation.\n  statistics_gen = StatisticsGen(input_data=example_gen.outputs.examples)\n\n  # Generates schema based on statistics files.\n  infer_schema = SchemaGen(stats=statistics_gen.outputs.output)\n\n  # Performs anomaly detection based on statistics and data schema.\n  validate_stats = ExampleValidator(\n      stats=statistics_gen.outputs.output, schema=infer_schema.outputs.output)\n\n  # Performs transformations and feature engineering in training and serving.\n  transform = Transform(\n      input_data=example_gen.outputs.examples,\n      schema=infer_schema.outputs.output,\n      module_file=_taxi_module_file)\n\n  # Uses user-provided Python function that implements a model using TF-Learn.\n  trainer = Trainer(\n      module_file=_taxi_module_file,\n      transformed_examples=transform.outputs.transformed_examples,\n      schema=infer_schema.outputs.output,\n      transform_output=transform.outputs.transform_output,\n      train_args=trainer_pb2.TrainArgs(num_steps=10000),\n      eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n\n  # Uses TFMA to compute a evaluation statistics over features of a model.\n  model_analyzer = Evaluator(\n      examples=example_gen.outputs.examples,\n      model_exports=trainer.outputs.output,\n      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[\n          evaluator_pb2.SingleSlicingSpec(\n              column_for_slicing=[\'trip_start_hour\'])\n      ]))\n\n  # Performs quality validation of a candidate model (compared to a baseline).\n  model_validator = ModelValidator(\n      examples=example_gen.outputs.examples, model=trainer.outputs.output)\n\n  # Checks whether the model passed the validation steps and pushes the model\n  # to a file destination if check passed.\n  pusher = Pusher(\n      model_export=trainer.outputs.output,\n      model_blessing=model_validator.outputs.blessing,\n      push_destination=pusher_pb2.PushDestination(\n          filesystem=pusher_pb2.PushDestination.Filesystem(\n              base_directory=_serving_model_dir)))\n\n  return pipeline.Pipeline(\n      pipeline_name=\'taxi\',\n      pipeline_root=_pipeline_root,\n      components=[\n          example_gen, statistics_gen, infer_schema, validate_stats, transform,\n          trainer, model_analyzer, model_validator, pusher\n      ],\n      enable_cache=True,\n      metadata_db_root=_metadata_db_root,\n      additional_pipeline_args={\'logger_args\': logger_overrides},\n  )\n\n\nairflow_pipeline = AirflowDAGRunner(_airflow_config).run(_create_pipeline())\n'"
kubeflow/airflow-dags/taxi_utils.py,0,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Python source file include taxi pipeline functions and necesasry utils.\n\nFor a TFX pipeline to successfully run, a preprocessing_fn and a\n_build_estimator function needs to be provided.  This file contains both.\n\nThis file is equivalent to examples/chicago_taxi/trainer/model.py and\nexamples/chicago_taxi/preprocess.py.\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\nimport tensorflow_transform as tft\nfrom tensorflow_transform.tf_metadata import schema_utils\n\n# Categorical features are assumed to each have a maximum value in the dataset.\n_MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n\n_CATEGORICAL_FEATURE_KEYS = [\n    \'trip_start_hour\', \'trip_start_day\', \'trip_start_month\',\n    \'pickup_census_tract\', \'dropoff_census_tract\', \'pickup_community_area\',\n    \'dropoff_community_area\'\n]\n\n_DENSE_FLOAT_FEATURE_KEYS = [\'trip_miles\', \'fare\', \'trip_seconds\']\n\n# Number of buckets used by tf.transform for encoding each feature.\n_FEATURE_BUCKET_COUNT = 10\n\n_BUCKET_FEATURE_KEYS = [\n    \'pickup_latitude\', \'pickup_longitude\', \'dropoff_latitude\',\n    \'dropoff_longitude\'\n]\n\n# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n_VOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n_OOV_SIZE = 10\n\n_VOCAB_FEATURE_KEYS = [\n    \'payment_type\',\n    \'company\',\n]\n\n# Keys\n_LABEL_KEY = \'tips\'\n_FARE_KEY = \'fare\'\n\n\ndef _transformed_name(key):\n  return key + \'_xf\'\n\n\ndef _transformed_names(keys):\n  return [_transformed_name(key) for key in keys]\n\n\n# Tf.Transform considers these features as ""raw""\ndef _get_raw_feature_spec(schema):\n  return schema_utils.schema_as_feature_spec(schema).feature_spec\n\n\ndef _gzip_reader_fn():\n  """"""Small utility returning a record reader that can read gzip\'ed files.""""""\n  return tf.TFRecordReader(\n      options=tf.python_io.TFRecordOptions(\n          compression_type=tf.python_io.TFRecordCompressionType.GZIP))\n\n\ndef _fill_in_missing(x):\n  """"""Replace missing values in a SparseTensor.\n\n  Fills in missing values of `x` with \'\' or 0, and converts to a dense tensor.\n\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  """"""\n  default_value = \'\' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\n\ndef preprocessing_fn(inputs):\n  """"""tf.transform\'s callback function for preprocessing inputs.\n\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n\n  Returns:\n    Map from string feature key to transformed feature operations.\n  """"""\n  outputs = {}\n  for key in _DENSE_FLOAT_FEATURE_KEYS:\n    # Preserve this feature as a dense float, setting nan\'s to the mean.\n    outputs[_transformed_name(key)] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]))\n\n  for key in _VOCAB_FEATURE_KEYS:\n    # Build a vocabulary for this feature.\n    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n        _fill_in_missing(inputs[key]),\n        top_k=_VOCAB_SIZE,\n        num_oov_buckets=_OOV_SIZE)\n\n  for key in _BUCKET_FEATURE_KEYS:\n    outputs[_transformed_name(key)] = tft.bucketize(\n        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\n\n  for key in _CATEGORICAL_FEATURE_KEYS:\n    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_transformed_name(_LABEL_KEY)] = tf.where(\n      tf.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was > 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n\ndef _build_estimator(config, hidden_units=None, warm_start_from=None):\n  """"""Build an estimator for predicting the tipping behavior of taxi riders.\n\n  Args:\n    config: tf.contrib.learn.RunConfig defining the runtime environment for the\n      estimator (including model_dir).\n    hidden_units: [int], the layer sizes of the DNN (input layer first)\n    warm_start_from: Optional directory to warm start from.\n\n  Returns:\n    A dict of the following:\n      - estimator: The estimator that will be used for training and eval.\n      - train_spec: Spec for training.\n      - eval_spec: Spec for eval.\n      - eval_input_receiver_fn: Input function for eval.\n  """"""\n  real_valued_columns = [\n      tf.feature_column.numeric_column(key, shape=())\n      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n  ]\n  categorical_columns = [\n      tf.feature_column.categorical_column_with_identity(\n          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n      for key in _transformed_names(_VOCAB_FEATURE_KEYS)\n  ]\n  categorical_columns += [\n      tf.feature_column.categorical_column_with_identity(\n          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n      for key in _transformed_names(_BUCKET_FEATURE_KEYS)\n  ]\n  categorical_columns += [\n      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n          key,\n          num_buckets=num_buckets,\n          default_value=0) for key, num_buckets in zip(\n              _transformed_names(_CATEGORICAL_FEATURE_KEYS),\n              _MAX_CATEGORICAL_FEATURE_VALUES)\n  ]\n  return tf.estimator.DNNLinearCombinedClassifier(\n      config=config,\n      linear_feature_columns=categorical_columns,\n      dnn_feature_columns=real_valued_columns,\n      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n      warm_start_from=warm_start_from)\n\n\ndef _example_serving_receiver_fn(tf_transform_output, schema):\n  """"""Build the serving in inputs.\n\n  Args:\n    tf_transform_output: A TFTransformOutput.\n    schema: the schema of the input data.\n\n  Returns:\n    Tensorflow graph which parses examples, applying tf-transform to them.\n  """"""\n  raw_feature_spec = _get_raw_feature_spec(schema)\n  raw_feature_spec.pop(_LABEL_KEY)\n\n  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n      raw_feature_spec, default_batch_size=None)\n  serving_input_receiver = raw_input_fn()\n\n  transformed_features = tf_transform_output.transform_raw_features(\n      serving_input_receiver.features)\n\n  return tf.estimator.export.ServingInputReceiver(\n      transformed_features, serving_input_receiver.receiver_tensors)\n\n\ndef _eval_input_receiver_fn(tf_transform_output, schema):\n  """"""Build everything needed for the tf-model-analysis to run the model.\n\n  Args:\n    tf_transform_output: A TFTransformOutput.\n    schema: the schema of the input data.\n\n  Returns:\n    EvalInputReceiver function, which contains:\n      - Tensorflow graph which parses raw untransformed features, applies the\n        tf-transform preprocessing operators.\n      - Set of raw, untransformed features.\n      - Label against which predictions will be compared.\n  """"""\n  # Notice that the inputs are raw features, not transformed features here.\n  raw_feature_spec = _get_raw_feature_spec(schema)\n\n  serialized_tf_example = tf.placeholder(\n      dtype=tf.string, shape=[None], name=\'input_example_tensor\')\n\n  # Add a parse_example operator to the tensorflow graph, which will parse\n  # raw, untransformed, tf examples.\n  features = tf.parse_example(serialized_tf_example, raw_feature_spec)\n\n  # Now that we have our raw examples, process them through the tf-transform\n  # function computed during the preprocessing step.\n  transformed_features = tf_transform_output.transform_raw_features(\n      features)\n\n  # The key name MUST be \'examples\'.\n  receiver_tensors = {\'examples\': serialized_tf_example}\n\n  # NOTE: Model is driven by transformed features (since training works on the\n  # materialized output of TFT, but slicing will happen on raw features.\n  features.update(transformed_features)\n\n  return tfma.export.EvalInputReceiver(\n      features=features,\n      receiver_tensors=receiver_tensors,\n      labels=transformed_features[_transformed_name(_LABEL_KEY)])\n\n\ndef _input_fn(filenames, tf_transform_output, batch_size=200):\n  """"""Generates features and labels for training or evaluation.\n\n  Args:\n    filenames: [str] list of CSV files to read data from.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: int First dimension size of the Tensors returned by input_fn\n\n  Returns:\n    A (features, indices) tuple where features is a dictionary of\n      Tensors, and indices is a single Tensor of label indices.\n  """"""\n  transformed_feature_spec = (\n      tf_transform_output.transformed_feature_spec().copy())\n\n  transformed_features = tf.contrib.learn.io.read_batch_features(\n      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n\n  # We pop the label because we do not want to use it as a feature while we\'re\n  # training.\n  return transformed_features, transformed_features.pop(\n      _transformed_name(_LABEL_KEY))\n\n\n# TFX will call this function\ndef trainer_fn(hparams, schema):\n  """"""Build the estimator using the high level API.\n\n  Args:\n    hparams: Holds hyperparameters used to train the model as name/value pairs.\n    schema: Holds the schema of the training examples.\n\n  Returns:\n    A dict of the following:\n      - estimator: The estimator that will be used for training and eval.\n      - train_spec: Spec for training.\n      - eval_spec: Spec for eval.\n      - eval_input_receiver_fn: Input function for eval.\n  """"""\n  # Number of nodes in the first layer of the DNN\n  first_dnn_layer_size = 100\n  num_dnn_layers = 4\n  dnn_decay_factor = 0.7\n\n  train_batch_size = 40\n  eval_batch_size = 40\n\n  tf_transform_output = tft.TFTransformOutput(hparams.transform_output)\n\n  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n      hparams.train_files,\n      tf_transform_output,\n      batch_size=train_batch_size)\n\n  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n      hparams.eval_files,\n      tf_transform_output,\n      batch_size=eval_batch_size)\n\n  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda\n      train_input_fn,\n      max_steps=hparams.train_steps)\n\n  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda\n      tf_transform_output, schema)\n\n  exporter = tf.estimator.FinalExporter(\'chicago-taxi\', serving_receiver_fn)\n  eval_spec = tf.estimator.EvalSpec(\n      eval_input_fn,\n      steps=hparams.eval_steps,\n      exporters=[exporter],\n      name=\'chicago-taxi-eval\')\n\n  run_config = tf.estimator.RunConfig(\n      save_checkpoints_steps=999, keep_checkpoint_max=1)\n\n  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n\n  estimator = _build_estimator(\n      # Construct layers sizes with exponetial decay\n      hidden_units=[\n          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n          for i in range(num_dnn_layers)\n      ],\n      config=run_config,\n      warm_start_from=hparams.warm_start_from)\n\n  # Create an input receiver for TFMA processing\n  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda\n      tf_transform_output, schema)\n\n  return {\n      \'estimator\': estimator,\n      \'train_spec\': train_spec,\n      \'eval_spec\': eval_spec,\n      \'eval_input_receiver_fn\': receiver_fn\n  }\n'"
libs/pipeline_logger/setup.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""setup.py: setuptools control.""""""\n\nimport re\nfrom setuptools import setup\n\n#import sys\n#if not sys.version_info[0] == 3:\n#    print(""\\n \\\n#    sys.exit(""\\n \\\n#              ****************************************************************\\n \\\n#              * The CLI has only been tested with Python 3+ at this time.    *\\n \\\n#              * Report any issues with Python 2 by emailing help@pipeline.io *\\n \\\n#              ****************************************************************\\n"")\n\nversion = re.search(\n    \'^__version__\\s*=\\s*""(.*)""\',\n    open(\'pipeline_logger/__init__.py\').read(),\n    re.M\n    ).group(1)\n\n# Get the long description from the relevant file\nwith open(\'README.rst\', encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nwith open(\'requirements.txt\', encoding=\'utf-8\') as f:\n    requirements = [line.rstrip() for line in f.readlines()]\n\nsetup(\n    name = ""pipeline-logger"",\n    packages = [""pipeline_logger""],\n    version = version,\n    description = ""PipelineAI Logger"",\n    long_description = ""%s\\n\\nRequirements:\\n%s"" % (long_description, requirements),\n    author = ""Chris Fregly"",\n    author_email = ""github@pipeline.ai"",\n    url = ""https://github.com/PipelineAI/pip/"",\n    install_requires=requirements,\n    dependency_links=[\n    ]\n)\n'"
libs/pipeline_logger/test.py,0,"b'from pipeline_logger import log\nimport logging\nimport json\nfrom pipeline_logger.kafka_handler import KafkaHandler\nfrom pipeline_logger.tensorboard_handler import TensorboardHandler\n\n_logger = logging.getLogger(\'test_logger\')\n_logger.setLevel(logging.INFO)\n_logger_stream_handler = logging.StreamHandler()\n_logger_stream_handler.setLevel(logging.INFO)\n_logger.addHandler(_logger_stream_handler)\n\n_logger_kafka_handler = KafkaHandler(host_list=\'localhost:9092\', topic=\'prediction-inputs\')\n_logger.addHandler(_logger_kafka_handler)\n\n_logger_tensorboard_handler = TensorboardHandler(logdir_path=\'/tmp/pipeline_logger.tensorboard_handler\')\n_logger.addHandler(_logger_tensorboard_handler)\n\n@log(logger=_logger, \n     labels={\'a_label_key\': \'a_label_value\'})\ndef test_log_inputs_and_outputs(arg1: int, arg2: int):\n    return arg1 + arg2\n\ntest_log_inputs_and_outputs(4, 5)\ntest_log_inputs_and_outputs(4, 5)\ntest_log_inputs_and_outputs(5, 4)\n\ndef _custom_inputs_fn(*args):\n    return hash(\'%s:%s\' % (args[0], args[1]))\n\ndef _custom_outputs_fn(outputs):\n    return hash(outputs)\n\n@log(custom_inputs_fn=_custom_inputs_fn, \n     custom_outputs_fn=_custom_outputs_fn, \n     logger=_logger, \n     labels={\'a_label_key\': \'a_label_value\'})\ndef test_log_inputs_and_outputs_custom_fn(arg1: int, arg2: int):\n    return arg1 + arg2\n\ntest_log_inputs_and_outputs_custom_fn(4, 5)\n\ndef _custom_inputs_json_fn(*args):\n    return hash(json.dumps(json.loads(args[0]), sort_keys=True))\n\n@log(custom_inputs_fn=_custom_inputs_json_fn, \n     custom_outputs_fn=_custom_outputs_fn,\n     logger=_logger, \n     labels={\'a_label_key\': \'a_label_value\'})\ndef test_log_inputs_and_outputs_json_custom_fn(args: str):\n    args_json = json.loads(args)\n    arg1 = args_json[\'arg1\']\n    arg2 = args_json[\'arg2\']\n    return arg1 + arg2\n\ntest_log_inputs_and_outputs_json_custom_fn(\'{""arg1"":4,""arg2"":5}\')\ntest_log_inputs_and_outputs_json_custom_fn(\'{""arg2"":5,""arg1"":4}\')\n'"
libs/pipeline_model/setup.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""setup.py: setuptools control.""""""\n\nimport re\nfrom setuptools import setup\n\n#import sys\n#if not sys.version_info[0] == 3:\n#    print(""\\n \\\n#    sys.exit(""\\n \\\n#              ****************************************************************\\n \\\n#              * The CLI has only been tested with Python 3+ at this time.    *\\n \\\n#              * Report any issues with Python 2 by emailing help@pipeline.io *\\n \\\n#              ****************************************************************\\n"")\n\nversion = re.search(\n    \'^__version__\\s*=\\s*""(.*)""\',\n    open(\'pipeline_model/__init__.py\').read(),\n    re.M\n    ).group(1)\n\n# Get the long description from the relevant file\nwith open(\'README.rst\', encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nwith open(\'requirements.txt\', encoding=\'utf-8\') as f:\n    requirements = [line.rstrip() for line in f.readlines()]\n\nsetup(\n    name = ""pipeline-model"",\n    packages = [""pipeline_model"", ""tensorflow"", ""tensorflow_serving""],\n    version = version,\n    description = ""PipelineAI Model"",\n    long_description = ""%s\\n\\nRequirements:\\n%s"" % (long_description, requirements),\n    author = ""Chris Fregly"",\n    author_email = ""github@pipeline.ai"",\n    url = ""https://github.com/pipelineai/pip/"",\n    install_requires=requirements,\n    dependency_links=[\n    ]\n)\n'"
libs/pipeline_model/test.py,0,"b""from pipeline_model import TensorFlowServingModel\nfrom tensorflow.core.framework import tensor_shape_pb2, tensor_pb2, types_pb2\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\n\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = 'blah'\nrequest.model_spec.signature_name = 'blah'\nprint(request)\n\n# create TensorProto object for a request\ndims = [tensor_shape_pb2.TensorShapeProto.Dim(size=1)]\nprint(dims)\n\ntensor_shape_proto = tensor_shape_pb2.TensorShapeProto(dim=dims)\nprint(tensor_shape_proto)\n\ntensor_proto = tensor_pb2.TensorProto(\n  dtype=types_pb2.DT_STRING,\n  tensor_shape=tensor_shape_proto,\n  string_val=[b'blah'])\nprint(tensor_proto)\n\n# put data into TensorProto and copy them into the request object\nrequest.inputs['blah'].CopyFrom(tensor_proto)\nprint(request)\n\n# call prediction\n#result = stub.Predict(request, 60.0)  # 60 secs timeout\n"""
libs/pipeline_monitor/setup.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""setup.py: setuptools control.""""""\n\nimport re\nfrom setuptools import setup\n\n#import sys\n#if not sys.version_info[0] == 3:\n#    print(""\\n \\\n#    sys.exit(""\\n \\\n#              ****************************************************************\\n \\\n#              * The CLI has only been tested with Python 3+ at this time.    *\\n \\\n#              * Report any issues with Python 2 by emailing help@pipeline.io *\\n \\\n#              ****************************************************************\\n"")\n\nversion = re.search(\n    \'^__version__\\s*=\\s*""(.*)""\',\n    open(\'pipeline_monitor/__init__.py\').read(),\n    re.M\n    ).group(1)\n\n# Get the long description from the relevant file\nwith open(\'README.rst\', encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nwith open(\'requirements.txt\', encoding=\'utf-8\') as f:\n    requirements = [line.rstrip() for line in f.readlines()]\n\nsetup(\n    name = ""pipeline-monitor"",\n    packages = [""pipeline_monitor""],\n    version = version,\n    description = ""PipelineAI Monitor"",\n    long_description = ""%s\\n\\nRequirements:\\n%s"" % (long_description, requirements),\n    author = ""Chris Fregly"",\n    author_email = ""github@pipeline.ai"",\n    url = ""https://github.com/PipelineAI/"",\n    install_requires=requirements,\n    dependency_links=[\n    ]\n)\n'"
libs/pipeline_monitor/test.py,0,"b'from pipeline_monitor import prometheus_monitor as monitor\n\n_labels= {\'a_label_key\':\'a_label_value\'}\n\n@monitor(labels=_labels, name=""test_monitor"")\ndef test_log_inputs_and_outputs(arg1: int, arg2: int):\n    return arg1 + arg2\n\ntest_log_inputs_and_outputs(4, 5)\n'"
libs/pipeline_runtime/setup.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""setup.py: setuptools control.""""""\n\nimport re\nfrom setuptools import setup\n\n#import sys\n#if not sys.version_info[0] == 3:\n#    print(""\\n \\\n#    sys.exit(""\\n \\\n#              ****************************************************************\\n \\\n#              * The CLI has only been tested with Python 3+ at this time.    *\\n \\\n#              * Report any issues with Python 2 by emailing help@pipeline.io *\\n \\\n#              ****************************************************************\\n"")\n\nversion = re.search(\n    \'^__version__\\s*=\\s*""(.*)""\',\n    open(\'pipeline_runtime/__init__.py\').read(),\n    re.M\n    ).group(1)\n\n# Get the long description from the relevant file\nwith open(\'README.rst\', encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nwith open(\'requirements.txt\', encoding=\'utf-8\') as f:\n    requirements = [line.rstrip() for line in f.readlines()]\n\nsetup(\n    name = ""pipeline-runtime"",\n    packages = [""pipeline_runtime""],\n    version = version,\n    description = ""PipelineAI Runtime"",\n    long_description = ""%s\\n\\nRequirements:\\n%s"" % (long_description, requirements),\n    author = ""Chris Fregly"",\n    author_email = ""github@pipeline.ai"",\n    url = ""https://github.com/PipelineAI/"",\n    install_requires=requirements,\n    dependency_links=[\n    ]\n)\n'"
libs/pipeline_runtime/test.py,0,b'from pipeline_runtime import *\n'
predict/mxnet/setup_mms.py,0,"b'from mms.model_loader import ModelLoader\nimport json\nimport sys\n\n# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 (the ""License"").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# or in the ""license"" file accompanying this file. This file is distributed\n# on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\n\ndef write_models_to_file(models=None):\n    """"""\n    Read model\'s metadata returned from the ModelLoader and write it into a file to be read from the\n    gunicorn workers\n    :param models: models metadata (service name, model dir, manifest file ...)\n    :return: void\n    """"""\n    mxnet_model_metadata_file = ""/mxnet_model_server/.models""\n    if models is None:\n        sys.exit(1)\n\n    print(""INFO: Writing models metadata..."")\n    with (open(mxnet_model_metadata_file, \'w\')) as fp:\n        json.dump(models, fp)\n\n\ndef main():\n    """"""\n    This API downloads and extracts the models before instantiating the MMS\n    :return:\n    """"""\n\n    model_list = []\n    models = \' \'\n    models = models.join(sys.argv[1:])\n\n    print(""INFO: Getting model files {}\\n"".format(models))\n    model_list += models.split(\' \')\n    models_dict = ModelLoader.load(dict(s.split(\'=\') for s in model_list))\n    write_models_to_file(models_dict)\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
predict/mxnet/wsgi.py,0,"b'# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 (the ""License"").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# or in the ""license"" file accompanying this file. This file is distributed\n# on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\n\nfrom mms import mxnet_model_server\nimport sys\nimport json\nimport os\nfrom mms.arg_parser import ArgParser\n\nmms_arg_header = \'MMS Argument\'\nargs = []\nfound_mms_args = 0\n\nmms_config_path = os.environ[\'MXNET_MODEL_SERVER_CONFIG\']\nis_gpu_image = os.environ[\'MXNET_MODEL_SERVER_GPU_IMAGE\']\n\n\ndef read_models_from_file():\n    """"""\n    This method reads the models meta-data from a local file. This is used for MMS in\n    :return: models meta-data\n    """"""\n    mxnet_model_metadata_file = ""/mxnet_model_server/.models""\n    models = json.load(open(mxnet_model_metadata_file))\n    return models\n\n\ntry:\n    mms_config_file = open(mms_config_path)\nexcept IOError as e:\n    sys.exit(""ERROR: File %s could not be located"" % mms_config_path)\nelse:\n    print(""INFO: Successfully read config file %s.."" % mms_config_path)\n    with mms_config_file:\n        try:\n            content = mms_config_file.readlines()\n            content = [line.rstrip() for line in content]\n\n            for i, line in enumerate(content):\n                line = line.lstrip()\n                if line.startswith(\'#\') or line.startswith(\'$\'):\n                    continue\n                if line.startswith(\'[\'):\n                    found_mms_args = 1 if mms_arg_header.lower() in line.lower() else 0\n                if found_mms_args is 1:\n                    if line.startswith(\'--\') and content[i+1] != \'optional\':\n                        args.append(line)\n                        if line.startswith(\'--model\'):\n                            args += content[i + 1].split(\' \')\n                        else:\n                            args.append(content[i + 1])\n        except Exception as e:\n            sys.exit(""ERROR: Cannot read the open file."")\n\nif is_gpu_image == 1:\n    args.append(\'--gpu\')\n    args.append(int(os.environ[\'gpu_id\']))\n    os.environ[\'gpu_id\'] = str(int(os.environ[\'gpu_id\']) + 1)\n\n# Extract the model\'s metadata from the file\nmodels = read_models_from_file()\n# Parse the arguments\narguments = ArgParser.extract_args(args)\n# Instantiate the MMS object to start the MMS app\nserver = mxnet_model_server.MMS(args=arguments, models=models)\napplication = server.create_app()\n'"
spark/scripts/hdfs_test.py,0,"b'import tensorflow as tf\n\nfilename_queue = tf.train.string_input_producer([\n  ""hdfs://hdfs-namenode:9000/hdfs/file1.csv"",\n  ""hdfs://hdfs-namenode:9000/hdfs/file2.csv"",\n])\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\n# Default values, in case of empty columns. Also specifies the type of the\n# decoded result.\nrecord_defaults = [[1], [1], [1], [1], [1]]\ncol1, col2, col3, col4, col5 = tf.decode_csv(\n    value, record_defaults=record_defaults)\nfeatures = tf.stack([col1, col2, col3, col4])\n\nwith tf.Session() as sess:\n  # Start populating the filename queue.\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  for i in range(1200):\n    # Retrieve a single instance:\n    example, label = sess.run([features, col5])\n    print(example, label)\n\n  coord.request_stop()\n  coord.join(threads)\n'"
train/scripts/hdfs_test.py,0,"b'import tensorflow as tf\n\nfilename_queue = tf.train.string_input_producer([\n  ""hdfs://hdfs-namenode:9000/hdfs/file1.csv"",\n  ""hdfs://hdfs-namenode:9000/hdfs/file2.csv"",\n])\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\n# Default values, in case of empty columns. Also specifies the type of the\n# decoded result.\nrecord_defaults = [[1], [1], [1], [1], [1]]\ncol1, col2, col3, col4, col5 = tf.decode_csv(\n    value, record_defaults=record_defaults)\nfeatures = tf.stack([col1, col2, col3, col4])\n\nwith tf.Session() as sess:\n  # Start populating the filename queue.\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  for i in range(1200):\n    # Retrieve a single instance:\n    example, label = sess.run([features, col5])\n    print(example, label)\n\n  coord.request_stop()\n  coord.join(threads)\n'"
kubeflow/kubeflow-pipelines/basic/artifact_location.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import dsl\nfrom kubernetes.client import V1SecretKeySelector\n\n\n@dsl.pipeline(\n    name=""custom_artifact_location_pipeline"",\n    description=""""""A pipeline to demonstrate how to configure the artifact \n    location for all the ops in the pipeline."""""",\n)\ndef custom_artifact_location(\n    tag: str, namespace: str = ""kubeflow"", bucket: str = ""mybucket""\n):\n\n    # configures artifact location\n    pipeline_artifact_location = dsl.ArtifactLocation.s3(\n        bucket=bucket,\n        endpoint=""minio-service.%s:9000"" % namespace,  # parameterize minio-service endpoint\n        insecure=True,\n        access_key_secret=V1SecretKeySelector(name=""minio"", key=""accesskey""),\n        secret_key_secret={""name"": ""minio"", ""key"": ""secretkey""},  # accepts dict also\n    )\n\n    # set pipeline level artifact location\n    dsl.get_pipeline_conf().set_artifact_location(pipeline_artifact_location)\n\n    # artifacts in this op are stored to endpoint `minio-service.<namespace>:9000`\n    op = dsl.ContainerOp(name=""foo"", image=""busybox:%s"" % tag)\n'"
kubeflow/kubeflow-pipelines/basic/condition.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_num_op(low, high):\n    """"""Generate a random number between low and high.""""""\n    return dsl.ContainerOp(\n        name=\'Generate random number\',\n        image=\'python:alpine3.6\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'python -c ""import random; print(random.randint($0, $1))"" | tee $2\', str(low), str(high), \'/tmp/output\'],\n        file_outputs={\'output\': \'/tmp/output\'}\n    )\n\n\ndef flip_coin_op():\n    """"""Flip a coin and output heads or tails randomly.""""""\n    return dsl.ContainerOp(\n        name=\'Flip coin\',\n        image=\'python:alpine3.6\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'python -c ""import random; result = \\\'heads\\\' if random.randint(0,1) == 0 \'\n                  \'else \\\'tails\\\'; print(result)"" | tee /tmp/output\'],\n        file_outputs={\'output\': \'/tmp/output\'}\n    )\n\n\ndef print_op(msg):\n    """"""Print a message.""""""\n    return dsl.ContainerOp(\n        name=\'Print\',\n        image=\'alpine:3.6\',\n        command=[\'echo\', msg],\n    )\n    \n\n@dsl.pipeline(\n    name=\'Conditional execution pipeline\',\n    description=\'Shows how to use dsl.Condition().\'\n)\ndef flipcoin_pipeline():\n    flip = flip_coin_op()\n    with dsl.Condition(flip.output == \'heads\'):\n        random_num_head = random_num_op(0, 9)\n        with dsl.Condition(random_num_head.output > 5):\n            print_op(\'heads and %s > 5!\' % random_num_head.output)\n        with dsl.Condition(random_num_head.output <= 5):\n            print_op(\'heads and %s <= 5!\' % random_num_head.output)\n\n    with dsl.Condition(flip.output == \'tails\'):\n        random_num_tail = random_num_op(10, 19)\n        with dsl.Condition(random_num_tail.output > 15):\n            print_op(\'tails and %s > 15!\' % random_num_tail.output)\n        with dsl.Condition(random_num_tail.output <= 15):\n            print_op(\'tails and %s <= 15!\' % random_num_tail.output)\n\n\nif __name__ == \'__main__\':\n    kfp.compiler.Compiler().compile(flipcoin_pipeline, __file__ + \'.zip\')\n'"
kubeflow/kubeflow-pipelines/basic/exit_handler.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name=\'GCS - Download\',\n        image=\'google/cloud-sdk:216.0.0\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'gsutil cat $0 | tee $1\', url, \'/tmp/results.txt\'],\n        file_outputs={\n            \'data\': \'/tmp/results.txt\',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name=\'echo\',\n        image=\'library/bash:4.4.23\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'echo ""$0""\', text]\n    )\n\n\n@dsl.pipeline(\n    name=\'Exit Handler\',\n    description=\'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).\'\n)\ndef download_and_print(url=\'gs://ml-pipeline-playground/shakespeare1.txt\'):\n    """"""A sample pipeline showing exit handler.""""""\n\n    exit_task = echo_op(\'exit!\')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == \'__main__\':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + \'.zip\')\n'"
kubeflow/kubeflow-pipelines/basic/immediate_value.py,0,"b'#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n  name=\'Immediate Value\',\n  description=\'A pipeline with parameter values hard coded\'\n)\ndef immediate_value_pipeline():\n  # ""url"" is a pipeline parameter with value being hard coded.\n  # It is useful in case for some component you want to hard code a parameter instead\n  # of exposing it as a pipeline parameter.\n  url=dsl.PipelineParam(name=\'url\', value=\'gs://ml-pipeline-playground/shakespeare1.txt\')\n  op1 = dsl.ContainerOp(\n     name=\'download\',\n     image=\'google/cloud-sdk:216.0.0\',\n     command=[\'sh\', \'-c\'],\n     arguments=[\'gsutil cat %s | tee /tmp/results.txt\' % url],\n     file_outputs={\'downloaded\': \'/tmp/results.txt\'})\n  op2 = dsl.ContainerOp(\n     name=\'echo\',\n     image=\'library/bash:4.4.23\',\n     command=[\'sh\', \'-c\'],\n     arguments=[\'echo %s\' % op1.output])\n\nif __name__ == \'__main__\':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(immediate_value_pipeline, __file__ + \'.zip\')\n'"
kubeflow/kubeflow-pipelines/basic/parallel_join.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name=\'GCS - Download\',\n        image=\'google/cloud-sdk:216.0.0\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'gsutil cat $0 | tee $1\', url, \'/tmp/results.txt\'],\n        file_outputs={\n            \'data\': \'/tmp/results.txt\',\n        }\n    )\n\n\ndef echo2_op(text1, text2):\n    return dsl.ContainerOp(\n        name=\'echo\',\n        image=\'library/bash:4.4.23\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'echo ""Text 1: $0""; echo ""Text 2: $1""\', text1, text2]\n    )\n\n\n@dsl.pipeline(\n  name=\'Parallel pipeline\',\n  description=\'Download two messages in parallel and prints the concatenated result.\'\n)\ndef download_and_join(\n    url1=\'gs://ml-pipeline-playground/shakespeare1.txt\',\n    url2=\'gs://ml-pipeline-playground/shakespeare2.txt\'\n):\n    """"""A three-step pipeline with first two running in parallel.""""""\n\n    download1_task = gcs_download_op(url1)\n    download2_task = gcs_download_op(url2)\n\n    echo_task = echo2_op(download1_task.output, download2_task.output)\n\nif __name__ == \'__main__\':\n    kfp.compiler.Compiler().compile(download_and_join, __file__ + \'.zip\')\n'"
kubeflow/kubeflow-pipelines/basic/pipeline_transformers.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_failure_op(exit_codes):\n  """"""A component that fails randomly.""""""\n  return dsl.ContainerOp(\n      name=\'random_failure\',\n      image=\'python:alpine3.6\',\n      command=[\'python\', \'-c\'],\n      arguments=[\'import random; import sys; exit_code = random.choice(sys.argv[1].split("","")); print(exit_code); sys.exit(exit_code)\', exit_codes]\n  )\n\ndef add_retry(op):\n  op.set_retry(5)\n  return op\n\n@dsl.pipeline(\n    name=\'Retry random failures\',\n    description=\'The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).\'\n)\ndef retry_sample_pipeline():\n  op1 = random_failure_op(\'0,1,2,3\')\n  op2 = random_failure_op(\'0,1\')\n  dsl.get_pipeline_conf().add_op_transformer(add_retry)\n\nif __name__ == \'__main__\':\n  kfp.compiler.Compiler().compile(retry_sample_pipeline, __file__ + \'.zip\')\n'"
kubeflow/kubeflow-pipelines/basic/recursion.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef flip_coin_op():\n    """"""Flip a coin and output heads or tails randomly.""""""\n    return dsl.ContainerOp(\n        name=\'Flip coin\',\n        image=\'python:alpine3.6\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'python -c ""import random; result = \\\'heads\\\' if random.randint(0,1) == 0 \'\n                  \'else \\\'tails\\\'; print(result)"" | tee /tmp/output\'],\n        file_outputs={\'output\': \'/tmp/output\'}\n    )\n\n\ndef print_op(msg):\n    """"""Print a message.""""""\n    return dsl.ContainerOp(\n        name=\'Print\',\n        image=\'alpine:3.6\',\n        command=[\'echo\', msg],\n    )\n\n\n# Use the dsl.graph_component to decorate pipeline functions that can be\n# recursively called.\n@dsl.graph_component\ndef flip_component(flip_result):\n    print_flip = print_op(flip_result)\n    flipA = flip_coin_op().after(print_flip)\n    with dsl.Condition(flipA.output == \'heads\'):\n        # When the flip_component is called recursively, the flipA.output\n        # from inside the graph component will be passed to the next flip_component\n        # as the input whereas the flip_result in the current graph component\n        # comes from the flipA.output in the flipcoin function.\n        flip_component(flipA.output)\n\n\n@dsl.pipeline(\n    name=\'Recursive loop pipeline\',\n    description=\'Shows how to create recursive loops.\'\n)\ndef flipcoin():\n    first_flip = flip_coin_op()\n    flip_loop = flip_component(first_flip.output)\n    # flip_loop is a graph_component with the outputs field\n    # filled with the returned dictionary.\n    print_op(\'cool, it is over.\').after(flip_loop)\n\n\nif __name__ == \'__main__\':\n    kfp.compiler.Compiler().compile(flipcoin, __file__ + \'.tar.gz\')\n'"
kubeflow/kubeflow-pipelines/basic/retry.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_failure_op(exit_codes):\n    """"""A component that fails randomly.""""""\n    return dsl.ContainerOp(\n        name=\'random_failure\',\n        image=\'python:alpine3.6\',\n        command=[\'python\', \'-c\'],\n        arguments=[\'import random; import sys; exit_code = random.choice(sys.argv[1].split("","")); print(exit_code); sys.exit(exit_code)\', exit_codes]\n    )\n\n\n@dsl.pipeline(\n    name=\'Retry random failures\',\n    description=\'The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).\'\n)\ndef retry_sample_pipeline():\n    op1 = random_failure_op(\'0,1,2,3\').set_retry(10)\n    op2 = random_failure_op(\'0,1\').set_retry(5)\n\n\nif __name__ == \'__main__\':\n    kfp.compiler.Compiler().compile(retry_sample_pipeline, __file__ + \'.zip\')\n'"
kubeflow/kubeflow-pipelines/basic/sequential.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name=\'GCS - Download\',\n        image=\'google/cloud-sdk:216.0.0\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'gsutil cat $0 | tee $1\', url, \'/tmp/results.txt\'],\n        file_outputs={\n            \'data\': \'/tmp/results.txt\',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name=\'echo\',\n        image=\'library/bash:4.4.23\',\n        command=[\'sh\', \'-c\'],\n        arguments=[\'echo ""$0""\', text]\n    )\n\n@dsl.pipeline(\n    name=\'Sequential pipeline\',\n    description=\'A pipeline with two sequential steps.\'\n)\ndef sequential_pipeline(url=\'gs://ml-pipeline-playground/shakespeare1.txt\'):\n    """"""A pipeline with two sequential steps.""""""\n\n    download_task = gcs_download_op(url)\n    echo_task = echo_op(download_task.output)\n\nif __name__ == \'__main__\':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + \'.zip\')\n'"
kubeflow/kubeflow-pipelines/basic/sidecar.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n    name=""pipeline_with_sidecar"", \n    description=""A pipeline that demonstrates how to add a sidecar to an operation.""\n)\ndef pipeline_with_sidecar(sleep_ms: int = 10):\n\n    # sidecar with sevice that reply ""hello world"" to any GET request\n    echo = dsl.Sidecar(\n        name=""echo"",\n        image=""hashicorp/http-echo:latest"",\n        args=[\'-text=""hello world""\'],\n    )\n\n    # container op with sidecar\n    op1 = dsl.ContainerOp(\n        name=""download"",\n        image=""busybox:latest"",\n        command=[""sh"", ""-c""],\n        arguments=[\n            ""sleep %s; wget localhost:5678 -O /tmp/results.txt"" % sleep_ms\n        ],  # sleep for X sec and call the sidecar and save results to output\n        sidecars=[echo],\n        file_outputs={""downloaded"": ""/tmp/results.txt""},\n    )\n\n    op2 = dsl.ContainerOp(\n        name=""echo"",\n        image=""library/bash"",\n        command=[""sh"", ""-c""],\n        arguments=[""echo %s"" % op1.output],  # print out content of op1 output\n    )\n'"
kubeflow/kubeflow-pipelines/fairing/fairing_tf.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Trains and Evaluates the MNIST network using a feed dictionary.""""""\nimport os\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.examples.tutorials.mnist import mnist\n\nimport fairing\n\nINPUT_DATA_DIR = \'/tmp/tensorflow/mnist/input_data/\'\nMAX_STEPS = 2000\nBATCH_SIZE = 100\nLEARNING_RATE = 0.3\nHIDDEN_1 = 128\nHIDDEN_2 = 32\n\n# HACK: Ideally we would want to have a unique subpath for each instance of the job, but since we can\'t\n# we are instead appending HOSTNAME to the logdir\nLOG_DIR = os.path.join(os.getenv(\'TEST_TMPDIR\', \'/tmp\'),\n                       \'tensorflow/mnist/logs/fully_connected_feed/\', os.getenv(\'HOSTNAME\', \'\'))\nMODEL_DIR = os.path.join(LOG_DIR, \'model.ckpt\')\n\n\nclass TensorflowModel():\n    def train(self, **kwargs):\n        tf.logging.set_verbosity(tf.logging.ERROR)\n        self.data_sets = input_data.read_data_sets(INPUT_DATA_DIR)\n        self.images_placeholder = tf.placeholder(\n            tf.float32, shape=(BATCH_SIZE, mnist.IMAGE_PIXELS))\n        self.labels_placeholder = tf.placeholder(tf.int32, shape=(BATCH_SIZE))\n\n        logits = mnist.inference(self.images_placeholder,\n                                 HIDDEN_1,\n                                 HIDDEN_2)\n\n        self.loss = mnist.loss(logits, self.labels_placeholder)\n        self.train_op = mnist.training(self.loss, LEARNING_RATE)\n        self.summary = tf.summary.merge_all()\n        init = tf.global_variables_initializer()\n        self.sess = tf.Session()\n        self.summary_writer = tf.summary.FileWriter(LOG_DIR, self.sess.graph)\n        self.sess.run(init)\n\n        data_set = self.data_sets.train\n        for step in xrange(MAX_STEPS):\n            images_feed, labels_feed = data_set.next_batch(BATCH_SIZE, False)\n            feed_dict = {\n                self.images_placeholder: images_feed,\n                self.labels_placeholder: labels_feed,\n            }\n\n            _, loss_value = self.sess.run([self.train_op, self.loss],\n                                     feed_dict=feed_dict)\n            if step % 100 == 0:\n                print(""At step {}, loss = {}"".format(step, loss_value))\n                summary_str = self.sess.run(self.summary, feed_dict=feed_dict)\n                self.summary_writer.add_summary(summary_str, step)\n                self.summary_writer.flush()\n\n\nif __name__ == \'__main__\':\n    fairing.config.set_builder(name=\'cluster\')\n    fairing.config.set_model(TensorflowModel())\n    fairing.config.run()\n'"
kubeflow/kubeflow-pipelines/fairing/fairing_xgboost.py,0,"b'\n# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport logging\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom xgboost import XGBRegressor\nimport urllib.request\n\n\nTRAINING_URL=""https://raw.githubusercontent.com/kubeflow/examples/master/xgboost_ames_housing/ames_dataset/train.csv""\nTRAINING_FILE=""train.csv""\n\nESTIMATORS=1000\nLEARNING_RATE=0.1\nTEST_FRACTION_SIZE=0.25\nEARLY_STOPPING_ROUNDS=50\n\ndef run_training_and_eval():\n    (train_X, train_y), (test_X, test_y) = read_input()\n    model = train_model(train_X,\n                        train_y,\n                        test_X,\n                        test_y,\n                        ESTIMATORS,\n                        LEARNING_RATE)\n\n    eval_model(model, test_X, test_y)\n\ndef download(url, file_name):\n    with urllib.request.urlopen(url) as response, open(file_name, ""wb"") as file:\n        file.write(response.read())\n\ndef read_input(test_size=TEST_FRACTION_SIZE):\n    """"""Read input data and split it into train and test.""""""\n    download(TRAINING_URL, TRAINING_FILE)\n    data = pd.read_csv(TRAINING_FILE)\n    data.dropna(axis=0, subset=[\'SalePrice\'], inplace=True)\n\n    y = data.SalePrice\n    X = data.drop([\'SalePrice\'], axis=1).select_dtypes(exclude=[\'object\'])\n\n    train_X, test_X, train_y, test_y = train_test_split(X.values,\n                                                        y.values,\n                                                        test_size=test_size,\n                                                        shuffle=False)\n\n    imputer = Imputer()\n    train_X = imputer.fit_transform(train_X)\n    test_X = imputer.transform(test_X)\n\n    return (train_X, train_y), (test_X, test_y)\n\ndef train_model(train_X,\n                train_y,\n                test_X,\n                test_y,\n                n_estimators,\n                learning_rate):\n    """"""Train the model using XGBRegressor.""""""\n    model = XGBRegressor(n_estimators=n_estimators,\n                      learning_rate=learning_rate)\n\n    model.fit(train_X,\n              train_y,\n              early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n              eval_set=[(test_X, test_y)])\n\n    logging.info(""Best RMSE on eval: %.2f with %d rounds"",\n                 model.best_score,\n                 model.best_iteration+1)\n    return model\n\ndef eval_model(model, test_X, test_y):\n    """"""Evaluate the model performance.""""""\n    predictions = model.predict(test_X)\n    logging.info(""mean_absolute_error=%.2f"", mean_absolute_error(predictions, test_y))\n\n\nimport fairing\nfairing.config.set_builder(\'append\', base_image=\'tensorflow/tensorflow:1.13.1-py3\', registry=\'pipelineai/fairing_xgboost\', push=False)\n#fairing.config.set_deployer(\'job\')\nrun_training_and_eval = fairing.config.fn(run_training_and_eval)\nrun_training_and_eval()\n'"
kubeflow/kubeflow-pipelines/minio/minio.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import dsl\nfrom kubernetes.client import V1SecretKeySelector\n\n\n@dsl.pipeline(\n    name=""custom_artifact_location_pipeline"",\n    description=""""""A pipeline to demonstrate how to configure the artifact \n    location for all the ops in the pipeline."""""",\n)\ndef custom_artifact_location(\n        tag: str = ""latest"", \n        namespace: str = ""kubeflow"", \n        bucket: str = ""mybucket""\n):\n\n    # configures artifact location\n    pipeline_artifact_location = dsl.ArtifactLocation.s3(\n        bucket=bucket,\n        endpoint=""minio-service.%s:9000"" % namespace,  # parameterize minio-service endpoint\n        insecure=True,\n        access_key_secret=V1SecretKeySelector(name=""mlpipeline-minio-artifact"", key=""accesskey""),\n        secret_key_secret={""name"": ""mlpipeline-minio-artifact"", ""key"": ""secretkey""},  # accepts dict also\n    )\n\n    # set pipeline level artifact location\n    dsl.get_pipeline_conf().set_artifact_location(pipeline_artifact_location)\n\n    # artifacts in this op are stored to endpoint `minio-service.<namespace>:9000`\n    op = dsl.ContainerOp(name=""foo"", image=""busybox:%s"" % tag)\n'"
kubeflow/kubeflow-pipelines/taxi/preprocessing.py,0,"b'#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Preprocessor applying tf.transform to the chicago_taxi data.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport tensorflow_transform as transform\n\n# Categorical features are assumed to each have a maximum value in the dataset.\nMAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\nCATEGORICAL_FEATURE_KEYS = [\n    \'trip_start_hour\',\n    \'trip_start_day\',\n    \'trip_start_month\'\n]\n\nDENSE_FLOAT_FEATURE_KEYS = [\n    \'trip_miles\',\n    \'fare\',\n    \'trip_seconds\'\n]\n\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nBUCKET_FEATURE_KEYS = [\n    \'pickup_latitude\', \'pickup_longitude\', \'dropoff_latitude\',\n    \'dropoff_longitude\'\n]\n\n# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\nOOV_SIZE = 10\n\nVOCAB_FEATURE_KEYS = [\n    \'pickup_census_tract\',\n    \'dropoff_census_tract\',\n    \'payment_type\',\n    \'company\',\n    \'pickup_community_area\',\n    \'dropoff_community_area\'\n]\nLABEL_KEY = \'tips\'\nFARE_KEY = \'fare\'\n\n\ndef preprocess(inputs):\n  """"""tf.transform\'s callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  """"""\n  outputs = {}\n  for key in DENSE_FLOAT_FEATURE_KEYS:\n    # Preserve this feature as a dense float, setting nan\'s to the mean.\n    outputs[key] = transform.scale_to_z_score(inputs[key])\n\n  for key in VOCAB_FEATURE_KEYS:\n    # Build a vocabulary for this feature.\n    if inputs[key].dtype == tf.string:\n        vocab_tensor = inputs[key]\n    else:\n        vocab_tensor = tf.as_string(inputs[key])\n    outputs[key] = transform.string_to_int(\n        vocab_tensor, vocab_filename=\'vocab_\' + key,\n        top_k=VOCAB_SIZE, num_oov_buckets=OOV_SIZE)\n\n  for key in BUCKET_FEATURE_KEYS:\n    outputs[key] = transform.bucketize(inputs[key], FEATURE_BUCKET_COUNT)\n\n  for key in CATEGORICAL_FEATURE_KEYS:\n    outputs[key] = tf.to_int64(inputs[key])\n\n  taxi_fare = inputs[FARE_KEY]\n  taxi_tip = inputs[LABEL_KEY]\n  # Test if the tip was > 20% of the fare.\n  tip_threshold = tf.multiply(taxi_fare, tf.constant(0.2))\n  outputs[LABEL_KEY] = tf.logical_and(\n      tf.logical_not(tf.is_nan(taxi_fare)),\n      tf.greater(taxi_tip, tip_threshold))\n\n  return outputs\n\n\ndef get_feature_columns(transformed_data_dir):\n  """"""Callback that returns a list of feature columns for building a tf.estimator.\n  Args:\n    transformed_data_dir: The GCS directory holding the output of the tft transformation.\n  Returns:\n    A list of tf.feature_column.\n  """"""\n  return (\n    [tf.feature_column.numeric_column(key, shape=()) for key in DENSE_FLOAT_FEATURE_KEYS] +\n    [tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_identity(\n            key, num_buckets=VOCAB_SIZE+OOV_SIZE))\n     for key in VOCAB_FEATURE_KEYS] +\n    [tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_identity(\n            key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0))\n     for key in BUCKET_FEATURE_KEYS] +\n    [tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_identity(\n            key, num_buckets=num_buckets, default_value=0))\n     for key, num_buckets in zip(CATEGORICAL_FEATURE_KEYS, MAX_CATEGORICAL_FEATURE_VALUES)])\n'"
kubeflow/kubeflow-pipelines/taxi/taxi-cab-classification-pipeline.py,0,"b'#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import gcp\nfrom kfp import onprem\n\nplatform = \'onprem\'\n\ndataflow_tf_data_validation_op  = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/dataflow/tfdv/component.yaml\')\ndataflow_tf_transform_op        = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/dataflow/tft/component.yaml\')\ntf_train_op                     = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/kubeflow/dnntrainer/component.yaml\')\ndataflow_tf_model_analyze_op    = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/dataflow/tfma/component.yaml\')\ndataflow_tf_predict_op          = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/dataflow/predict/component.yaml\')\n\nconfusion_matrix_op             = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/local/confusion_matrix/component.yaml\')\nroc_op                          = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/local/roc/component.yaml\')\n\nkubeflow_deploy_op              = components.load_component_from_url(\'https://raw.githubusercontent.com/kubeflow/pipelines/74d8e592174ae90175f66c3c00ba76a835cfba6d/components/kubeflow/deployer/component.yaml\')\n\n@dsl.pipeline(\n  name=\'TFX Taxi Cab Classification Pipeline Example\',\n  description=\'Example pipeline that does classification with model analysis based on a public BigQuery dataset.\'\n)\ndef taxi_cab_classification(\n#    output=\'minio://minio-service:9000/blah/\',\n#    output=\'gs://pipelineai-kubeflow/blah\',\n    output=\'/mnt\',\n    project=\'taxi-cab-classification-pipeline\',\n#    column_names=\'gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json\',\n    column_names=\'/mnt/kubeflow-pipelines/taxi/column-names.json\',\n    key_columns=\'trip_start_timestamp\',\n#    train=\'gs://ml-pipeline-playground/tfx/taxi-cab-classification/train.csv\',\n    train=\'/mnt/kubeflow-pipelines/taxi/train.csv\',\n#    evaluation=\'gs://ml-pipeline-playground/tfx/taxi-cab-classification/eval.csv\',\n    evaluation=\'/mnt/kubeflow-pipelines/taxi/eval.csv\',\n    mode=\'local\',\n#    preprocess_module=\'gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py\',\n    preprocess_module=\'/mnt/kubeflow-pipelines/taxi/preprocessing.py\',\n    learning_rate=0.1,\n    hidden_layer_size=\'1500\',\n    steps=3000,\n    analyze_slice_column=\'trip_start_hour\'\n):\n    # Note:  This needs to be gs:// if we want ROC and Confusion Matrix to work\n    output_template = \'gs://pipelineai-kubeflow/taxi/{{workflow.uid}}/{{pod.name}}/data\'\n#    output_template = \'s3://mybucket\' + \'/{{workflow.uid}}/{{pod.name}}/data\'\n    # output_template = str(output) + \'/{{workflow.uid}}/{{pod.name}}/data\'\n    target_lambda = """"""lambda x: (x[\'target\'] > x[\'fare\'] * 0.2)""""""\n    target_class_lambda = """"""lambda x: 1 if (x[\'target\'] > x[\'fare\'] * 0.2) else 0""""""\n\n    tf_server_name = \'taxi-cab-classification-model-{{workflow.uid}}\'\n\n#    pipeline_artifact_location = dsl.ArtifactLocation.s3(\n#        bucket=\'mybucket\',\n#        endpoint=""minio-service.kubeflow:9000"",  # parameterize minio-service endpoint\n#        insecure=True,\n#        access_key_secret={""name"": ""mlpipeline-minio-artifact"","" key"": ""accesskey""},\n#        secret_key_secret={""name"": ""mlpipeline-minio-artifact"", ""key"": ""secretkey""},  # accepts dict also\n#    )\n#    # set pipeline level artifact location\n#    dsl.get_pipeline_conf().set_artifact_location(pipeline_artifact_location)\n\n#    if platform != \'GCP\':\n#        vop = dsl.VolumeOp(\n#            name=""create_pvc"",\n#            resource_name=""pipeline-pvc"",\n#            modes=dsl.VOLUME_MODE_RWO,\n#            size=""1Gi""\n#        )\n    \n#        checkout = dsl.ContainerOp(\n#            name=""checkout"",\n#            image=""alpine/git:latest"",\n#            command=[""git"", ""clone"", ""https://github.com/kubeflow/pipelines.git"", str(output) + ""/pipelines""],\n##        ).apply(onprem.mount_pvc(vop.outputs[""name""], \'local-storage\', output))\n#        ).apply(onprem.mount_pvc(\'users-pvc\', \'local-storage\', output))\n#        checkout.after(vop)\n\n    validation = dataflow_tf_data_validation_op(\n        inference_data=train,\n        validation_data=evaluation,\n        column_names=column_names,\n        key_columns=key_columns,\n        gcp_project=project,\n        run_mode=mode,\n        validation_output=output_template,\n    )\n#    if platform != \'GCP\':\n#        validation.after(checkout)\n\n    preprocess = dataflow_tf_transform_op(\n        training_data_file_pattern=train,\n        evaluation_data_file_pattern=evaluation,\n        schema=validation.outputs[\'schema\'],\n        gcp_project=project,\n        run_mode=mode,\n        preprocessing_module=preprocess_module,\n        transformed_data_dir=output_template\n    )\n\n    training = tf_train_op(\n        transformed_data_dir=preprocess.output,\n        schema=validation.outputs[\'schema\'],\n        learning_rate=learning_rate,\n        hidden_layer_size=hidden_layer_size,\n        steps=steps,\n        target=\'tips\',\n        preprocessing_module=preprocess_module,\n        training_output_dir=output_template\n    )\n\n    analysis = dataflow_tf_model_analyze_op(\n        model=training.output,\n        evaluation_data=evaluation,\n        schema=validation.outputs[\'schema\'],\n        gcp_project=project,\n        run_mode=mode,\n        slice_columns=analyze_slice_column,\n        analysis_results_dir=output_template\n    )\n\n    prediction = dataflow_tf_predict_op(\n        data_file_pattern=evaluation,\n        schema=validation.outputs[\'schema\'],\n        target_column=\'tips\',\n        model=training.output,\n        run_mode=mode,\n        gcp_project=project,\n        predictions_dir=output_template\n    )\n\n    cm = confusion_matrix_op(\n        predictions=prediction.output,\n        target_lambda=target_lambda,\n        output_dir=output_template\n    )\n\n    roc = roc_op(\n        predictions_dir=prediction.output,\n        target_lambda=target_class_lambda,\n        output_dir=output_template\n    )\n\n    if platform == \'GCP\':\n        deploy = kubeflow_deploy_op(\n            model_dir=str(training.output) + \'/export/export\',\n            server_name=tf_server_name\n        )\n    else:\n        deploy = kubeflow_deploy_op(\n            cluster_name=project,\n            model_dir=str(training.output) + \'/export/export\',\n            pvc_name=\'users-pvc\',\n#            pvc_name=vop.outputs[""name""],\n            server_name=tf_server_name,\n            service_type=\'NodePort\',\n        )\n\n    steps = [validation, preprocess, training, analysis, prediction, cm, roc, deploy]\n    for step in steps:\n        if platform == \'GCP\':\n            step.apply(gcp.use_gcp_secret(\'user-gcp-sa\'))\n        else:\n            step.apply(onprem.mount_pvc(\'users-pvc\', \'local-storage\', output))\n#            step.apply(onprem.mount_pvc(vop.outputs[""name""], \'local-storage\', output))\n\n\nif __name__ == \'__main__\':\n    kfp.compiler.Compiler().compile(taxi_cab_classification, __file__ + \'.zip\')\n'"
kubeflow/pach-pipelines/code/IssueSummarization.py,0,"b'""""""Generates predictions using a stored model.\n\nUses trained model files to generate a prediction.\n""""""\n\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport dill as dpickle\nfrom keras.models import load_model\nfrom seq2seq_utils import Seq2Seq_Inference\n\nclass IssueSummarization(object):\n\n  def __init__(self):\n    body_pp_file = os.getenv(\'BODY_PP_FILE\', \'body_preprocessor.dpkl\')\n    print(\'body_pp file {0}\'.format(body_pp_file))\n    with open(body_pp_file, \'rb\') as body_file:\n      body_pp = dpickle.load(body_file)\n\n    title_pp_file = os.getenv(\'TITLE_PP_FILE\', \'title_preprocessor.dpkl\')\n    print(\'title_pp file {0}\'.format(title_pp_file))\n    with open(title_pp_file, \'rb\') as title_file:\n      title_pp = dpickle.load(title_file)\n\n    model_file = os.getenv(\'MODEL_FILE\', \'output_model.h5\')\n    print(\'model file {0}\'.format(model_file))\n    self.model = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n                                   decoder_preprocessor=title_pp,\n                                   seq2seq_model=load_model(model_file))\n\n  def predict(self, input_text, feature_names): # pylint: disable=unused-argument\n    return np.asarray([[self.model.generate_issue_title(body[0])[1]] for body in input_text])\n'"
kubeflow/pach-pipelines/code/prediction.py,0,"b'import argparse\nimport keras\nimport pandas as pd\nfrom seq2seq_utils import load_text_processor\nfrom seq2seq_utils import Seq2Seq_Inference\n\n# Parsing flags.\nparser = argparse.ArgumentParser()\nparser.add_argument(""--input_model_h5"")\nparser.add_argument(""--input_body_preprocessor_dpkl"")\nparser.add_argument(""--input_title_preprocessor_dpkl"")\nparser.add_argument(""--input_testdf_csv"")\nparser.add_argument(""--input_prediction_count"", type=int, default=50)\nargs = parser.parse_args()\nprint(args)\n\n# Read data.\ntestdf = pd.read_csv(args.input_testdf_csv)\n\n# Load model, preprocessors.\nseq2seq_Model = keras.models.load_model(args.input_model_h5)\nnum_encoder_tokens, body_pp = load_text_processor(args.input_body_preprocessor_dpkl)\nnum_decoder_tokens, title_pp = load_text_processor(args.input_title_preprocessor_dpkl)\n\n# Prepare inference.\nseq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n                                 decoder_preprocessor=title_pp,\n                                 seq2seq_model=seq2seq_Model)\n\n# Output predictions for n random rows in the test set.\nseq2seq_inf.demo_model_predictions(n=args.input_prediction_count, issue_df=testdf)\n'"
kubeflow/pach-pipelines/code/preprocess_data_for_deep_learning.py,0,"b'import argparse\nimport dill as dpickle\nimport numpy as np\nfrom ktext.preprocess import processor\nimport pandas as pd\n\n# Parsing flags.\nparser = argparse.ArgumentParser()\nparser.add_argument(""--input_traindf_csv"")\nparser.add_argument(""--output_body_preprocessor_dpkl"")\nparser.add_argument(""--output_title_preprocessor_dpkl"")\nparser.add_argument(""--output_train_title_vecs_npy"")\nparser.add_argument(""--output_train_body_vecs_npy"")\nargs = parser.parse_args()\nprint(args)\n\n# Read data.\ntraindf = pd.read_csv(args.input_traindf_csv)\ntrain_body_raw = traindf.body.tolist()\ntrain_title_raw = traindf.issue_title.tolist()\n\n# Clean, tokenize, and apply padding / truncating such that each document\n# length = 70. Also, retain only the top 8,000 words in the vocabulary and set\n# the remaining words to 1 which will become common index for rare words.\nbody_pp = processor(keep_n=8000, padding_maxlen=70)\ntrain_body_vecs = body_pp.fit_transform(train_body_raw)\n\nprint(\'Example original body:\', train_body_raw[0])\nprint(\'Example body after pre-processing:\', train_body_vecs[0])\n\n# Instantiate a text processor for the titles, with some different parameters.\ntitle_pp = processor(append_indicators=True, keep_n=4500,\n                     padding_maxlen=12, padding=\'post\')\n\n# process the title data\ntrain_title_vecs = title_pp.fit_transform(train_title_raw)\n\nprint(\'Example original title:\', train_title_raw[0])\nprint(\'Example title after pre-processing:\', train_title_vecs[0])\n\n# Save the preprocessor.\nwith open(args.output_body_preprocessor_dpkl, \'wb\') as f:\n  dpickle.dump(body_pp, f, protocol=2)\n\nwith open(args.output_title_preprocessor_dpkl, \'wb\') as f:\n  dpickle.dump(title_pp, f, protocol=2)\n\n# Save the processed data.\nnp.save(args.output_train_title_vecs_npy, train_title_vecs)\nnp.save(args.output_train_body_vecs_npy, train_body_vecs)\n'"
kubeflow/pach-pipelines/code/process_data.py,0,"b'import argparse\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Parsing flags.\nparser = argparse.ArgumentParser()\nparser.add_argument(""--input_csv"")\nparser.add_argument(""--sample_size"", type=int, default=2000000)\nparser.add_argument(""--output_traindf_csv"")\nparser.add_argument(""--output_testdf_csv"")\nargs = parser.parse_args()\nprint(args)\n\npd.set_option(\'display.max_colwidth\', 500)\n\n# Read in data sample 2M rows (for speed of tutorial)\ntraindf, testdf = train_test_split(pd.read_csv(args.input_csv).sample(n=args.sample_size),\n                                   test_size=.10)\n\n# Print stats about the shape of the data.\nprint(\'Train: {:,} rows {:,} columns\'.format(traindf.shape[0], traindf.shape[1]))\nprint(\'Test: {:,} rows {:,} columns\'.format(testdf.shape[0], testdf.shape[1]))\n\n# Store output as CSV.\ntraindf.to_csv(args.output_traindf_csv)\ntestdf.to_csv(args.output_testdf_csv)\n'"
kubeflow/pach-pipelines/code/recommend.py,0,"b'import argparse\nimport keras\nimport pandas as pd\nfrom seq2seq_utils import load_text_processor\nfrom seq2seq_utils import Seq2Seq_Inference\n\n# Parsing flags.\nparser = argparse.ArgumentParser()\nparser.add_argument(""--input_csv"")\nparser.add_argument(""--input_model_h5"")\nparser.add_argument(""--input_body_preprocessor_dpkl"")\nparser.add_argument(""--input_title_preprocessor_dpkl"")\nparser.add_argument(""--input_testdf_csv"")\nparser.add_argument(""--input_topic_number"", type=int, default=1)\nargs = parser.parse_args()\nprint(args)\n\n# Read data.\nall_data_df = pd.read_csv(args.input_csv)\ntestdf = pd.read_csv(args.input_testdf_csv)\n\n# Load model, preprocessors.\nnum_encoder_tokens, body_pp = load_text_processor(args.input_body_preprocessor_dpkl)\nnum_decoder_tokens, title_pp = load_text_processor(args.input_title_preprocessor_dpkl)\nseq2seq_Model = keras.models.load_model(args.input_model_h5)\n\n# Prepare the recommender.\nall_data_bodies = all_data_df[\'body\'].tolist()\nall_data_vectorized = body_pp.transform_parallel(all_data_bodies)\nseq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n                                    decoder_preprocessor=title_pp,\n                                    seq2seq_model=seq2seq_Model)\nrecsys_annoyobj = seq2seq_inf_rec.prepare_recommender(all_data_vectorized, all_data_df)\n\n# Output recommendations for n topics.\nseq2seq_inf_rec.demo_model_predictions(n=args.input_topic_number, issue_df=testdf, threshold=1)\n'"
kubeflow/pach-pipelines/code/seq2seq_utils.py,0,"b'import logging\nimport dill as dpickle\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom IPython.display import SVG, display\nfrom keras import backend as K\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.utils.vis_utils import model_to_dot\nfrom annoy import AnnoyIndex\nfrom tqdm import tqdm, tqdm_notebook\nfrom nltk.translate.bleu_score import corpus_bleu\n\n\ndef load_text_processor(fname=\'title_pp.dpkl\'):\n  """"""\n  Load preprocessors from disk.\n\n  Parameters\n  ----------\n  fname: str\n    file name of ktext.proccessor object\n\n  Returns\n  -------\n  num_tokens : int\n    size of vocabulary loaded into ktext.processor\n  pp : ktext.processor\n    the processor you are trying to load\n\n  Typical Usage:\n  -------------\n\n  num_decoder_tokens, title_pp = load_text_processor(fname=\'title_pp.dpkl\')\n  num_encoder_tokens, body_pp = load_text_processor(fname=\'body_pp.dpkl\')\n\n  """"""\n  # Load files from disk\n  with open(fname, \'rb\') as f:\n    pp = dpickle.load(f)\n\n  num_tokens = max(pp.id2token.keys()) + 1\n  print(\'Size of vocabulary for {}: {}\'.format(fname, num_tokens))\n  return num_tokens, pp\n\n\ndef load_decoder_inputs(decoder_np_vecs=\'train_title_vecs.npy\'):\n  """"""\n  Load decoder inputs.\n\n  Parameters\n  ----------\n  decoder_np_vecs : str\n    filename of serialized numpy.array of decoder input (issue title)\n\n  Returns\n  -------\n  decoder_input_data : numpy.array\n    The data fed to the decoder as input during training for teacher forcing.\n    This is the same as `decoder_np_vecs` except the last position.\n  decoder_target_data : numpy.array\n    The data that the decoder data is trained to generate (issue title).\n    Calculated by sliding `decoder_np_vecs` one position forward.\n\n  """"""\n  vectorized_title = np.load(decoder_np_vecs)\n  # For Decoder Input, you don\'t need the last word as that is only for prediction\n  # when we are training using Teacher Forcing.\n  decoder_input_data = vectorized_title[:, :-1]\n\n  # Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n  decoder_target_data = vectorized_title[:, 1:]\n\n  print(\'Shape of decoder input: {}\'.format(decoder_input_data.shape))\n  print(\'Shape of decoder target: {}\'.format(decoder_target_data.shape))\n  return decoder_input_data, decoder_target_data\n\n\ndef load_encoder_inputs(encoder_np_vecs=\'train_body_vecs.npy\'):\n  """"""\n  Load variables & data that are inputs to encoder.\n\n  Parameters\n  ----------\n  encoder_np_vecs : str\n    filename of serialized numpy.array of encoder input (issue title)\n\n  Returns\n  -------\n  encoder_input_data : numpy.array\n    The issue body\n  doc_length : int\n    The standard document length of the input for the encoder after padding\n    the shape of this array will be (num_examples, doc_length)\n\n  """"""\n  vectorized_body = np.load(encoder_np_vecs)\n  # Encoder input is simply the body of the issue text\n  encoder_input_data = vectorized_body\n  doc_length = encoder_input_data.shape[1]\n  print(\'Shape of encoder input: {}\'.format(encoder_input_data.shape))\n  return encoder_input_data, doc_length\n\n\ndef viz_model_architecture(model):\n  """"""Visualize model architecture in Jupyter notebook.""""""\n  display(SVG(model_to_dot(model).create(prog=\'dot\', format=\'svg\')))\n\n\ndef free_gpu_mem():\n  """"""Attempt to free gpu memory.""""""\n  K.get_session().close()\n  cfg = K.tf.ConfigProto()\n  cfg.gpu_options.allow_growth = True\n  K.set_session(K.tf.Session(config=cfg))\n\n\ndef test_gpu():\n  """"""Run a toy computation task in tensorflow to test GPU.""""""\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  session = tf.Session(config=config)\n  hello = tf.constant(\'Hello, TensorFlow!\')\n  print(session.run(hello))\n\n\ndef plot_model_training_history(history_object):\n  """"""Plots model train vs. validation loss.""""""\n  plt.title(\'model accuracy\')\n  plt.ylabel(\'accuracy\')\n  plt.xlabel(\'epoch\')\n  plt.plot(history_object.history[\'loss\'])\n  plt.plot(history_object.history[\'val_loss\'])\n  plt.legend([\'train\', \'test\'], loc=\'upper left\')\n  plt.show()\n\n\ndef extract_encoder_model(model):\n  """"""\n  Extract the encoder from the original Sequence to Sequence Model.\n\n  Returns a keras model object that has one input (body of issue) and one\n  output (encoding of issue, which is the last hidden state).\n\n  Input:\n  -----\n  model: keras model object\n\n  Returns:\n  -----\n  keras model object\n\n  """"""\n  encoder_model = model.get_layer(\'Encoder-Model\')\n  return encoder_model\n\n\ndef extract_decoder_model(model):\n  """"""\n  Extract the decoder from the original model.\n\n  Inputs:\n  ------\n  model: keras model object\n\n  Returns:\n  -------\n  A Keras model object with the following inputs and outputs:\n\n  Inputs of Keras Model That Is Returned:\n  1: the embedding index for the last predicted word or the <Start> indicator\n  2: the last hidden state, or in the case of the first word the hidden state from the encoder\n\n  Outputs of Keras Model That Is Returned:\n  1.  Prediction (class probabilities) for the next word\n  2.  The hidden state of the decoder, to be fed back into the decoder at the next time step\n\n  Implementation Notes:\n  ----------------------\n  Must extract relevant layers and reconstruct part of the computation graph\n  to allow for different inputs as we are not going to use teacher forcing at\n  inference time.\n\n  """"""\n  # the latent dimension is the same throughout the architecture so we are going to\n  # cheat and grab the latent dimension of the embedding because that is the same as what is\n  # output from the decoder\n  latent_dim = model.get_layer(\'Decoder-Word-Embedding\').output_shape[-1]\n\n  # Reconstruct the input into the decoder\n  decoder_inputs = model.get_layer(\'Decoder-Input\').input\n  dec_emb = model.get_layer(\'Decoder-Word-Embedding\')(decoder_inputs)\n  dec_bn = model.get_layer(\'Decoder-Batchnorm-1\')(dec_emb)\n\n  # Instead of setting the intial state from the encoder and forgetting about it, during inference\n  # we are not doing teacher forcing, so we will have to have a feedback loop from predictions back\n  # into the GRU, thus we define this input layer for the state so we can add this capability\n  gru_inference_state_input = Input(shape=(latent_dim,), name=\'hidden_state_input\')\n\n  # we need to reuse the weights that is why we are getting this\n  # If you inspect the decoder GRU that we created for training, it will take as input\n  # 2 tensors -> (1) is the embedding layer output for the teacher forcing\n  #                  (which will now be the last step\'s prediction, and will be _start_ on the\n  #                  first time step)\n  #              (2) is the state, which we will initialize with the encoder on the first time step\n  #              but then grab the state after the first prediction and feed that back in again.\n  gru_out, gru_state_out = model.get_layer(\'Decoder-GRU\')([dec_bn, gru_inference_state_input])\n\n  # Reconstruct dense layers\n  dec_bn2 = model.get_layer(\'Decoder-Batchnorm-2\')(gru_out)\n  dense_out = model.get_layer(\'Final-Output-Dense\')(dec_bn2)\n  decoder_model = Model([decoder_inputs, gru_inference_state_input],\n                        [dense_out, gru_state_out])\n  return decoder_model\n\n\nclass Seq2Seq_Inference(object):\n\n  # pylint: disable=too-many-instance-attributes\n\n  def __init__(self,\n               encoder_preprocessor,\n               decoder_preprocessor,\n               seq2seq_model):\n\n    self.pp_body = encoder_preprocessor\n    self.pp_title = decoder_preprocessor\n    self.seq2seq_model = seq2seq_model\n    self.encoder_model = extract_encoder_model(seq2seq_model)\n    self.decoder_model = extract_decoder_model(seq2seq_model)\n    self.default_max_len_title = self.pp_title.padding_maxlen\n    self.nn = None\n    self.rec_df = None\n\n  def generate_issue_title(self,\n                           raw_input_text,\n                           max_len_title=None):\n    """"""\n    Use the seq2seq model to generate a title given the body of an issue.\n\n    Inputs\n    ------\n    raw_input: str\n        The body of the issue text as an input string\n\n    max_len_title: int (optional)\n        The maximum length of the title the model will generate\n\n    """"""\n    if max_len_title is None:\n      max_len_title = self.default_max_len_title\n    # get the encoder\'s features for the decoder\n    raw_tokenized = self.pp_body.transform([raw_input_text])\n    body_encoding = self.encoder_model.predict(raw_tokenized)\n    # we want to save the encoder\'s embedding before its updated by decoder\n    #   because we can use that as an embedding for other tasks.\n    original_body_encoding = body_encoding\n    state_value = np.array(self.pp_title.token2id[\'_start_\']).reshape(1, 1)\n\n    decoded_sentence = []\n    stop_condition = False\n    while not stop_condition:\n      preds, st = self.decoder_model.predict([state_value, body_encoding])\n\n      # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n      # Argmax will return the integer index corresponding to the\n      #  prediction + 2 b/c we chopped off first two\n      pred_idx = np.argmax(preds[:, :, 2:]) + 2\n\n      # retrieve word from index prediction\n      pred_word_str = self.pp_title.id2token[pred_idx]\n\n      if pred_word_str == \'_end_\' or len(decoded_sentence) >= max_len_title:\n        stop_condition = True\n        break\n      decoded_sentence.append(pred_word_str)\n\n      # update the decoder for the next word\n      body_encoding = st\n      state_value = np.array(pred_idx).reshape(1, 1)\n\n    return original_body_encoding, \' \'.join(decoded_sentence)\n\n\n  def print_example(self,\n                    i,\n                    body_text,\n                    title_text,\n                    url,\n                    threshold):\n    """"""\n    Prints an example of the model\'s prediction for manual inspection.\n    """"""\n    if i:\n      print(\'\\n\\n==============================================\')\n      print(\'============== Example # {} =================\\n\'.format(i))\n\n    if url:\n      print(url)\n\n    print(""Issue Body:\\n {} \\n"".format(body_text))\n\n    if title_text:\n      print(""Original Title:\\n {}"".format(title_text))\n\n    emb, gen_title = self.generate_issue_title(body_text)\n    print(""\\n****** Machine Generated Title (Prediction) ******:\\n {}"".format(gen_title))\n\n    if self.nn:\n      # return neighbors and distances\n      n, d = self.nn.get_nns_by_vector(emb.flatten(), n=4,\n                                       include_distances=True)\n      neighbors = n[1:]\n      dist = d[1:]\n\n      if min(dist) <= threshold:\n        cols = [\'issue_url\', \'issue_title\', \'body\']\n        dfcopy = self.rec_df.iloc[neighbors][cols].copy(deep=True)\n        dfcopy[\'dist\'] = dist\n        similar_issues_df = dfcopy.query(\'dist <= {}\'.format(threshold))\n\n        print(""\\n**** Similar Issues (using encoder embedding) ****:\\n"")\n        display(similar_issues_df)\n\n\n  def demo_model_predictions(self,\n                             n,\n                             issue_df,\n                             threshold=1):\n    """"""\n    Pick n random Issues and display predictions.\n\n    Input:\n    ------\n    n : int\n      Number of issues to display from issue_df\n    issue_df : pandas DataFrame\n      DataFrame that contains two columns: `body` and `issue_title`.\n    threshold : float\n      distance threshold for recommendation of similar issues.\n\n    Returns:\n    --------\n    None\n      Prints the original issue body and the model\'s prediction.\n    """"""\n    # Extract body and title from DF\n    body_text = issue_df.body.tolist()\n    title_text = issue_df.issue_title.tolist()\n    url = issue_df.issue_url.tolist()\n\n    demo_list = np.random.randint(low=1, high=len(body_text), size=n)\n    for i in demo_list:\n      self.print_example(i,\n                         body_text=body_text[i],\n                         title_text=title_text[i],\n                         url=url[i],\n                         threshold=threshold)\n\n  def prepare_recommender(self, vectorized_array, original_df):\n    """"""\n    Use the annoy library to build recommender\n\n    Parameters\n    ----------\n    vectorized_array : List[List[int]]\n      This is the list of list of integers that represents your corpus\n      that is fed into the seq2seq model for training.\n    original_df : pandas.DataFrame\n      This is the original dataframe that has the columns\n      [\'issue_url\', \'issue_title\', \'body\']\n\n    Returns\n    -------\n    annoy.AnnoyIndex  object (see https://github.com/spotify/annoy)\n    """"""\n    self.rec_df = original_df\n    emb = self.encoder_model.predict(x=vectorized_array,\n                                     batch_size=vectorized_array.shape[0]//200)\n\n    f = emb.shape[1]\n    self.nn = AnnoyIndex(f)\n    logging.warning(\'Adding embeddings\')\n    for i in tqdm(range(len(emb))):\n      self.nn.add_item(i, emb[i])\n    logging.warning(\'Building trees for similarity lookup.\')\n    self.nn.build(50)\n    return self.nn\n\n  def set_recsys_data(self, original_df):\n    self.rec_df = original_df\n\n  def set_recsys_annoyobj(self, annoyobj):\n    self.nn = annoyobj\n\n  def evaluate_model(self, holdout_bodies, holdout_titles):\n    """"""\n    Method for calculating BLEU Score.\n\n    Parameters\n    ----------\n    holdout_bodies : List[str]\n      These are the issue bodies that we want to summarize\n    holdout_titles : List[str]\n      This is the ground truth we are trying to predict --> issue titles\n\n    Returns\n    -------\n    bleu : float\n      The BLEU Score\n\n    """"""\n    actual, predicted = list(), list()\n    assert len(holdout_bodies) == len(holdout_titles)\n    num_examples = len(holdout_bodies)\n\n    logging.warning(\'Generating predictions.\')\n    # step over the whole set TODO: parallelize this\n    for i in tqdm_notebook(range(num_examples)):\n      _, yhat = self.generate_issue_title(holdout_bodies[i])\n\n      actual.append(self.pp_title.process_text([holdout_titles[i]])[0])\n      predicted.append(self.pp_title.process_text([yhat])[0])\n\n    # calculate BLEU score\n    logging.warning(\'Calculating BLEU.\')\n    #must be careful with nltk api for corpus_bleu!,\n    # expects List[List[List[str]]] for ground truth, using List[List[str]] will give you\n    # erroneous results.\n    bleu = corpus_bleu([[a] for a in actual], predicted)\n    return bleu\n'"
kubeflow/pach-pipelines/code/train.py,0,"b'import argparse\nimport numpy as np\nfrom keras.callbacks import CSVLogger, ModelCheckpoint\nfrom keras.layers import Input, GRU, Dense, Embedding, BatchNormalization\nfrom keras.models import Model\nfrom keras import optimizers\nfrom seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n\n# Parsing flags.\nparser = argparse.ArgumentParser()\nparser.add_argument(""--input_body_preprocessor_dpkl"")\nparser.add_argument(""--input_title_preprocessor_dpkl"")\nparser.add_argument(""--input_train_title_vecs_npy"")\nparser.add_argument(""--input_train_body_vecs_npy"")\nparser.add_argument(""--output_model_h5"")\nparser.add_argument(""--learning_rate"", default=""0.001"")\nargs = parser.parse_args()\nprint(args)\n\nlearning_rate = float(args.learning_rate)\n\nencoder_input_data, doc_length = load_encoder_inputs(args.input_train_body_vecs_npy)\ndecoder_input_data, decoder_target_data = load_decoder_inputs(args.input_train_title_vecs_npy)\n\nnum_encoder_tokens, body_pp = load_text_processor(args.input_body_preprocessor_dpkl)\nnum_decoder_tokens, title_pp = load_text_processor(args.input_title_preprocessor_dpkl)\n\n# Arbitrarly set latent dimension for embedding and hidden units\nlatent_dim = 300\n\n###############\n# Encoder Model.\n###############\nencoder_inputs = Input(shape=(doc_length,), name=\'Encoder-Input\')\n\n# Word embeding for encoder (ex: Issue Body)\nx = Embedding(num_encoder_tokens,\n              latent_dim,\n              name=\'Body-Word-Embedding\',\n              mask_zero=False)(encoder_inputs)\nx = BatchNormalization(name=\'Encoder-Batchnorm-1\')(x)\n\n# We do not need the `encoder_output` just the hidden state.\n_, state_h = GRU(latent_dim, return_state=True, name=\'Encoder-Last-GRU\')(x)\n\n# Encapsulate the encoder as a separate entity so we can just\n# encode without decoding if we want to.\nencoder_model = Model(inputs=encoder_inputs, outputs=state_h, name=\'Encoder-Model\')\n\nseq2seq_encoder_out = encoder_model(encoder_inputs)\n\n################\n# Decoder Model.\n################\ndecoder_inputs = Input(shape=(None,), name=\'Decoder-Input\')  # for teacher forcing\n\n# Word Embedding For Decoder (ex: Issue Titles)\ndec_emb = Embedding(num_decoder_tokens,\n                    latent_dim,\n                    name=\'Decoder-Word-Embedding\',\n                    mask_zero=False)(decoder_inputs)\ndec_bn = BatchNormalization(name=\'Decoder-Batchnorm-1\')(dec_emb)\n\n# Set up the decoder, using `decoder_state_input` as initial state.\ndecoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name=\'Decoder-GRU\')\ndecoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\nx = BatchNormalization(name=\'Decoder-Batchnorm-2\')(decoder_gru_output)\n\n# Dense layer for prediction\ndecoder_dense = Dense(num_decoder_tokens, activation=\'softmax\', name=\'Final-Output-Dense\')\ndecoder_outputs = decoder_dense(x)\n\n################\n# Seq2Seq Model.\n################\n\nseq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nseq2seq_Model.compile(optimizer=optimizers.Nadam(lr=learning_rate),\n                      loss=\'sparse_categorical_crossentropy\')\n\nseq2seq_Model.summary()\n\nscript_name_base = \'tutorial_seq2seq\'\ncsv_logger = CSVLogger(\'{:}.log\'.format(script_name_base))\nmodel_checkpoint = ModelCheckpoint(\n    \'{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5\'.format(script_name_base), save_best_only=True)\n\nbatch_size = 1200\nepochs = 7\nhistory = seq2seq_Model.fit([encoder_input_data, decoder_input_data],\n                            np.expand_dims(decoder_target_data, -1),\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            validation_split=0.12,\n                            callbacks=[csv_logger, model_checkpoint])\n\n#############\n# Save model.\n#############\nseq2seq_Model.save(args.output_model_h5)\n'"
libs/pipeline_logger/pipeline_logger/__init__.py,0,"b'from logging import Logger\nfrom datetime import datetime\nimport json\nfrom typing import Callable\n\n__version__ = ""1.0.2""\n\n# TODO:  Handle batched inputs and outputs (using above custom fn\'s - match inputs to outputs!)\n# TODO:  Add Monitors around these calls!!\n\nclass log(object):\n\n    def __init__(self,\n                 labels: dict,\n                 logger: Logger,\n                 custom_inputs_fn: Callable=None,\n                 custom_outputs_fn: Callable=None):\n\n        self._labels = labels\n        self._logger = logger\n        self._custom_inputs_fn = custom_inputs_fn\n        self._custom_outputs_fn = custom_outputs_fn\n\n\n    def __call__(self, function):\n\n        def wrapped_function(*args: bytes):\n            log_dict = {\n                        \'log_labels\': self._labels,\n                        \'log_inputs\': str(args),\n                       }            \n \n            if self._custom_inputs_fn:\n                 custom_inputs = self._custom_inputs_fn(*args),\n                 log_dict[\'log_custom_inputs\'] = custom_inputs\n\n            outputs = function(*args)\n\n            log_dict[\'log_outputs\'] = outputs\n\n            if self._custom_outputs_fn:\n                custom_outputs = self._custom_outputs_fn(outputs)\n                log_dict[\'log_custom_outputs\'] = custom_outputs\n\n            self._logger.info(json.dumps(log_dict))\n\n            return outputs \n\n        return wrapped_function\n'"
libs/pipeline_logger/pipeline_logger/kafka_handler.py,0,"b'from kafka.client import SimpleClient\nfrom kafka.producer import SimpleProducer, KeyedProducer\nimport logging\n\n\nclass KafkaHandler(logging.Handler):\n\n    def __init__(self, host_list, topic, **kwargs):\n        logging.Handler.__init__(self)\n\n        self.kafka_client = SimpleClient(host_list)\n        self.key = kwargs.get(""key"", None)\n        self.kafka_topic_name = topic\n\n        if not self.key:\n            self.producer = SimpleProducer(self.kafka_client, **kwargs)\n        else:\n            self.producer = KeyedProducer(self.kafka_client, **kwargs)\n\n    def emit(self, record):\n        # drop kafka logging to avoid infinite recursion\n        if record.name == \'kafka\':\n            return\n        try:\n            # use default formatting\n            msg = self.format(record)\n            if isinstance(msg, str):\n                msg = msg.encode(""utf-8"")\n\n            # produce message\n            if not self.key:\n                self.producer.send_messages(self.kafka_topic_name, msg)\n            else:\n                self.producer.send_messages(self.kafka_topic_name, self.key, msg)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\n    def close(self):\n        try:\n            if self.producer:\n                self.producer.stop()\n        except:\n            pass\n\n        logging.Handler.close(self)\n'"
libs/pipeline_logger/pipeline_logger/tensorboard_handler.py,0,"b'# Details here:  https://github.com/TeamHG-Memex/tensorboard_logger/tree/0.0.4\nfrom tensorboard_logger import configure, log_value, Logger\n\nclass TensorBoardHandler(logging.Handler):\n\n    def __init__(self, logdir_path, flush_secs=5, **kwargs):\n        logging.Handler.__init__(self)\n        self.key = kwargs.get(""key"", None)\n        self.tensorboard_logger = tensorboard_logger.Logger(logdir_path, flush_secs)\n\n     \n    def emit(self, record: float, **kwargs):\n        step = kwargs.get(""step"", None)\n        try:\n            if not self.key:\n                self.tensorboard_logger.log_value(\'default\', record, step)\n            else:\n                self.tensorboard_logger.log_value(self.key, record, step)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\n\n    def close(self):\n        logging.Handler.close(self)\n'"
libs/pipeline_model/pipeline_model/__init__.py,0,"b'from grpc.beta import implementations\n#import tensorflow as tf\n# These are generated from the TF serving source \n#   or copied from various places such as the following:\n#     https://github.com/Vetal1977/tf_serving_example\n#     https://github.com/tobegit3hub/tensorflow_template_application\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\nfrom tensorflow.core.framework import tensor_pb2, tensor_shape_pb2, types_pb2\n\n__version__ = ""1.0.8""\n\n# TODO:  Add convenience methods for the following techniques:\n#   https://towardsdatascience.com/tensorflow-serving-client-make-it-slimmer-and-faster-b3e5f71208fb\n#   https://medium.com/@stianlindpetlund/tensorflow-serving-101-pt-2-682eaf7469e7\n#   https://github.com/davyzhang/dict-to-protobuf\n#  \n# TODO:  Add mock tensorflow serving server described here: \n#    https://medium.com/@stianlindpetlund/tensorflow-serving-101-pt-2-682eaf7469e7\n\nclass TensorFlowServingModel():\n   \n# TODO:  Don\'t expose any of these...\n#        (They\'re all internal to the model server runtime.) \n    def __init__(self,\n                 host: str,\n                 port: int,\n                 model_name: str,\n                 model_signature_name=None,\n                 timeout_seconds=5.0): # 5 second timeout\n\n        self._host = host\n        self._port = port\n        self._model_name = model_name\n        self._model_signature_name = model_signature_name\n        self._timeout_seconds = timeout_seconds\n\n\n    def predict(self,\n                input_string_to_tensor_dict):\n\n        channel = implementations.insecure_channel(self._host,\n                                                   self._port)\n\n        stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n       \n        # Transform input str::nparray dict into TensorFlow PredictRequest/tensors\n        tf_request = predict_pb2.PredictRequest()\n        tf_request.model_spec.name = self._model_name\n        if self._model_signature_name:\n            tf_request.model_spec.signature_name = self._model_signature_name\n        # We assume only a single version per model is running in this model server.\n        # tf_request.model_spec.version.value = ...\n\n        for input_str_key, input_tensor in input_string_to_tensor_dict.items():\n            tf_request.inputs[input_str_key].CopyFrom(input_tensor)\n\n        # Call TensorFlow Serving Predict\n        response = stub.Predict(tf_request, self._timeout_seconds)\n\n        # Return tensor dict\n        output_string_to_tensor_dict = {}\n        for output_str_key, _ in response.outputs.items():\n            output_string_to_tensor_dict[output_str_key] = repsonse.outputs[output_str_key]\n               #tf.make_ndarray(response.outputs[output_str_key])\n\n        return output_string_to_tensor_dict\n'"
libs/pipeline_model/tensorflow/__init__.py,0,b''
libs/pipeline_model/tensorflow_serving/__init__.py,0,b''
libs/pipeline_monitor/pipeline_monitor/__init__.py,0,"b'from prometheus_client import CollectorRegistry, generate_latest, start_http_server, Summary, Counter, Histogram, Gauge\nfrom timeit import default_timer\n\n__version__ = ""1.0.1""\n\nprometheus_monitor_registry = CollectorRegistry()\n\nclass prometheus_monitor(object):\n\n    _instances = {}\n\n    def __new__(cls, labels: dict, name: str):\n        if name in cls._instances:\n            return cls._instances[name]\n        instance = super().__new__(cls)\n        instance._labels = labels\n        instance._name = name\n        instance._counter = Counter(\'%s_counter\' % instance._name, instance._name, list(instance._labels.keys()))\n        instance._summary = Summary(\'%s_summary\' % instance._name, instance._name, list(instance._labels.keys()))\n        prometheus_monitor_registry.register(instance._counter)\n        prometheus_monitor_registry.register(instance._summary)        \n        cls._instances[name] = instance\n        return instance\n\n    def __enter__(self, *args, **kwargs):\n        self._counter.labels(*list(self._labels.values())).inc()\n        self._start = default_timer()\n\n    def __exit__(self, *args, **kwargs):\n        self._summary.labels(*list(self._labels.values())).observe(max(default_timer() - self._start, 0))\n\n    # Interface for decorator\n    def __call__(self, function, *args):\n\n        def wrapped_function(*args):\n            with self:\n                return function(*args)\n\n        return wrapped_function\n'"
libs/pipeline_runtime/pipeline_runtime/__init__.py,0,"b'__version__ = ""1.0.10""\n'"
kubeflow/kubeflow-pipelines/nlp/train_pipeline/nlp_pipeline.py,0,"b'\nimport kfp.dsl as dsl\nimport yaml\nfrom kubernetes import client as k8s\n\n\n@dsl.pipeline(\n  name=\'NLP\',\n  description=\'A pipeline demonstrating reproducible steps for NLP\'\n)\ndef nlp_pipeline(\n        csv_url=""https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv"",\n        csv_encoding=""ISO-8859-1"",\n        features_column=""BODY"",\n        labels_column=""REMOVED"",\n        raw_text_path=\'/mnt/text.data\',\n        labels_path=\'/mnt/labels.data\',\n        clean_text_path=\'/mnt/clean.data\',\n        spacy_tokens_path=\'/mnt/tokens.data\',\n        tfidf_vectors_path=\'/mnt/tfidf.data\',\n        lr_prediction_path=\'/mnt/prediction.data\',\n        tfidf_model_path=\'/mnt/tfidf.model\',\n        lr_model_path=\'/mnt/lr.model\',\n        lr_c_param=0.1,\n        tfidf_max_features=10000,\n        tfidf_ngram_range=3,\n        batch_size=\'100\'):\n    """"""\n    Pipeline \n    """"""\n    vop = dsl.VolumeOp(\n      name=\'my-pvc\',\n      resource_name=""my-pvc"",\n      modes=[""ReadWriteMany""],\n      size=""1Gi""\n    )\n\n    download_step = dsl.ContainerOp(\n        name=\'data_downloader\',\n        image=\'data_downloader:0.1\',\n        command=""python"",\n        arguments=[\n            ""/microservice/pipeline_step.py"",\n            ""--labels-path"", labels_path,\n            ""--features-path"", raw_text_path,\n            ""--csv-url"", csv_url,\n            ""--csv-encoding"", csv_encoding,\n            ""--features-column"", features_column,\n            ""--labels-column"", labels_column\n        ],\n        pvolumes={""/mnt"": vop.volume}\n    )\n\n    clean_step = dsl.ContainerOp(\n        name=\'clean_text\',\n        image=\'clean_text_transformer:0.1\',\n        command=""python"",\n        arguments=[\n            ""/microservice/pipeline_step.py"",\n            ""--in-path"", raw_text_path,\n            ""--out-path"", clean_text_path,\n        ],\n        pvolumes={""/mnt"": download_step.pvolume}\n    )\n\n    tokenize_step = dsl.ContainerOp(\n        name=\'tokenize\',\n        image=\'spacy_tokenizer:0.1\',\n        command=""python"",\n        arguments=[\n            ""/microservice/pipeline_step.py"",\n            ""--in-path"", clean_text_path,\n            ""--out-path"", spacy_tokens_path,\n        ],\n        pvolumes={""/mnt"": clean_step.pvolume}\n    )\n\n    vectorize_step = dsl.ContainerOp(\n        name=\'vectorize\',\n        image=\'tfidf_vectorizer:0.1\',\n        command=""python"",\n        arguments=[\n            ""/microservice/pipeline_step.py"",\n            ""--in-path"", spacy_tokens_path,\n            ""--out-path"", tfidf_vectors_path,\n            ""--max-features"", tfidf_max_features,\n            ""--ngram-range"", tfidf_ngram_range,\n            ""--action"", ""train"",\n            ""--model-path"", tfidf_model_path,\n        ],\n        pvolumes={""/mnt"": tokenize_step.pvolume}\n    )\n\n    predict_step = dsl.ContainerOp(\n        name=\'predictor\',\n        image=\'lr_text_classifier:0.1\',\n        command=""python"",\n        arguments=[\n            ""/microservice/pipeline_step.py"",\n            ""--in-path"", tfidf_vectors_path,\n            ""--labels-path"", labels_path,\n            ""--out-path"", lr_prediction_path,\n            ""--c-param"", lr_c_param,\n            ""--action"", ""train"",\n            ""--model-path"", lr_model_path,\n        ],\n        pvolumes={""/mnt"": vectorize_step.pvolume}\n    )\n\n    try:\n        seldon_config = yaml.load(open(""../deploy_pipeline/seldon_production_pipeline.yaml""))\n    except:\n        # If this file is run from the project core directory \n        seldon_config = yaml.load(open(""deploy_pipeline/seldon_production_pipeline.yaml""))\n\n    deploy_step = dsl.ResourceOp(\n        name=""seldondeploy"",\n        k8s_resource=seldon_config,\n        attribute_outputs={""name"": ""{.metadata.name}""})\n\n    deploy_step.after(predict_step)\n\nif __name__ == \'__main__\':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(nlp_pipeline, __file__ + \'.tar.gz\')\n'"
libs/pipeline_model/tensorflow/core/__init__.py,0,b''
libs/pipeline_model/tensorflow_serving/apis/__init__.py,0,b''
libs/pipeline_model/tensorflow_serving/apis/classification_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/classification.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow_serving.apis import input_pb2 as tensorflow__serving_dot_apis_dot_input__pb2\nfrom tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/classification.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n,tensorflow_serving/apis/classification.proto\\x12\\x12tensorflow.serving\\x1a#tensorflow_serving/apis/input.proto\\x1a#tensorflow_serving/apis/model.proto\\""%\\n\\x05\\x43lass\\x12\\r\\n\\x05label\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05score\\x18\\x02 \\x01(\\x02\\""=\\n\\x0f\\x43lassifications\\x12*\\n\\x07\\x63lasses\\x18\\x01 \\x03(\\x0b\\x32\\x19.tensorflow.serving.Class\\""T\\n\\x14\\x43lassificationResult\\x12<\\n\\x0f\\x63lassifications\\x18\\x01 \\x03(\\x0b\\x32#.tensorflow.serving.Classifications\\""t\\n\\x15\\x43lassificationRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12(\\n\\x05input\\x18\\x02 \\x01(\\x0b\\x32\\x19.tensorflow.serving.Input\\""R\\n\\x16\\x43lassificationResponse\\x12\\x38\\n\\x06result\\x18\\x01 \\x01(\\x0b\\x32(.tensorflow.serving.ClassificationResultB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_input__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_CLASS = _descriptor.Descriptor(\n  name=\'Class\',\n  full_name=\'tensorflow.serving.Class\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'tensorflow.serving.Class.label\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'tensorflow.serving.Class.score\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=142,\n  serialized_end=179,\n)\n\n\n_CLASSIFICATIONS = _descriptor.Descriptor(\n  name=\'Classifications\',\n  full_name=\'tensorflow.serving.Classifications\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'classes\', full_name=\'tensorflow.serving.Classifications.classes\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=181,\n  serialized_end=242,\n)\n\n\n_CLASSIFICATIONRESULT = _descriptor.Descriptor(\n  name=\'ClassificationResult\',\n  full_name=\'tensorflow.serving.ClassificationResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'classifications\', full_name=\'tensorflow.serving.ClassificationResult.classifications\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=244,\n  serialized_end=328,\n)\n\n\n_CLASSIFICATIONREQUEST = _descriptor.Descriptor(\n  name=\'ClassificationRequest\',\n  full_name=\'tensorflow.serving.ClassificationRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.ClassificationRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorflow.serving.ClassificationRequest.input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=330,\n  serialized_end=446,\n)\n\n\n_CLASSIFICATIONRESPONSE = _descriptor.Descriptor(\n  name=\'ClassificationResponse\',\n  full_name=\'tensorflow.serving.ClassificationResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'result\', full_name=\'tensorflow.serving.ClassificationResponse.result\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=448,\n  serialized_end=530,\n)\n\n_CLASSIFICATIONS.fields_by_name[\'classes\'].message_type = _CLASS\n_CLASSIFICATIONRESULT.fields_by_name[\'classifications\'].message_type = _CLASSIFICATIONS\n_CLASSIFICATIONREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_CLASSIFICATIONREQUEST.fields_by_name[\'input\'].message_type = tensorflow__serving_dot_apis_dot_input__pb2._INPUT\n_CLASSIFICATIONRESPONSE.fields_by_name[\'result\'].message_type = _CLASSIFICATIONRESULT\nDESCRIPTOR.message_types_by_name[\'Class\'] = _CLASS\nDESCRIPTOR.message_types_by_name[\'Classifications\'] = _CLASSIFICATIONS\nDESCRIPTOR.message_types_by_name[\'ClassificationResult\'] = _CLASSIFICATIONRESULT\nDESCRIPTOR.message_types_by_name[\'ClassificationRequest\'] = _CLASSIFICATIONREQUEST\nDESCRIPTOR.message_types_by_name[\'ClassificationResponse\'] = _CLASSIFICATIONRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nClass = _reflection.GeneratedProtocolMessageType(\'Class\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASS,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Class)\n  ))\n_sym_db.RegisterMessage(Class)\n\nClassifications = _reflection.GeneratedProtocolMessageType(\'Classifications\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONS,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Classifications)\n  ))\n_sym_db.RegisterMessage(Classifications)\n\nClassificationResult = _reflection.GeneratedProtocolMessageType(\'ClassificationResult\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONRESULT,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ClassificationResult)\n  ))\n_sym_db.RegisterMessage(ClassificationResult)\n\nClassificationRequest = _reflection.GeneratedProtocolMessageType(\'ClassificationRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREQUEST,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ClassificationRequest)\n  ))\n_sym_db.RegisterMessage(ClassificationRequest)\n\nClassificationResponse = _reflection.GeneratedProtocolMessageType(\'ClassificationResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONRESPONSE,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ClassificationResponse)\n  ))\n_sym_db.RegisterMessage(ClassificationResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/classification_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow_serving/apis/get_model_metadata_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/get_model_metadata.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import any_pb2 as google_dot_protobuf_dot_any__pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2 as tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2\nfrom tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/get_model_metadata.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow_serving/apis/get_model_metadata.proto\\x12\\x12tensorflow.serving\\x1a\\x19google/protobuf/any.proto\\x1a)tensorflow/core/protobuf/meta_graph.proto\\x1a#tensorflow_serving/apis/model.proto\\""\\xae\\x01\\n\\x0fSignatureDefMap\\x12L\\n\\rsignature_def\\x18\\x01 \\x03(\\x0b\\x32\\x35.tensorflow.serving.SignatureDefMap.SignatureDefEntry\\x1aM\\n\\x11SignatureDefEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x18.tensorflow.SignatureDef:\\x02\\x38\\x01\\""d\\n\\x17GetModelMetadataRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12\\x16\\n\\x0emetadata_field\\x18\\x02 \\x03(\\t\\""\\xe2\\x01\\n\\x18GetModelMetadataResponse\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12L\\n\\x08metadata\\x18\\x02 \\x03(\\x0b\\x32:.tensorflow.serving.GetModelMetadataResponse.MetadataEntry\\x1a\\x45\\n\\rMetadataEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12#\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x14.google.protobuf.Any:\\x02\\x38\\x01\\x42\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY = _descriptor.Descriptor(\n  name=\'SignatureDefEntry\',\n  full_name=\'tensorflow.serving.SignatureDefMap.SignatureDefEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.SignatureDefMap.SignatureDefEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.SignatureDefMap.SignatureDefEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=277,\n  serialized_end=354,\n)\n\n_SIGNATUREDEFMAP = _descriptor.Descriptor(\n  name=\'SignatureDefMap\',\n  full_name=\'tensorflow.serving.SignatureDefMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'signature_def\', full_name=\'tensorflow.serving.SignatureDefMap.signature_def\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SIGNATUREDEFMAP_SIGNATUREDEFENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=180,\n  serialized_end=354,\n)\n\n\n_GETMODELMETADATAREQUEST = _descriptor.Descriptor(\n  name=\'GetModelMetadataRequest\',\n  full_name=\'tensorflow.serving.GetModelMetadataRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.GetModelMetadataRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'metadata_field\', full_name=\'tensorflow.serving.GetModelMetadataRequest.metadata_field\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=356,\n  serialized_end=456,\n)\n\n\n_GETMODELMETADATARESPONSE_METADATAENTRY = _descriptor.Descriptor(\n  name=\'MetadataEntry\',\n  full_name=\'tensorflow.serving.GetModelMetadataResponse.MetadataEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.GetModelMetadataResponse.MetadataEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.GetModelMetadataResponse.MetadataEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=616,\n  serialized_end=685,\n)\n\n_GETMODELMETADATARESPONSE = _descriptor.Descriptor(\n  name=\'GetModelMetadataResponse\',\n  full_name=\'tensorflow.serving.GetModelMetadataResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.GetModelMetadataResponse.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'metadata\', full_name=\'tensorflow.serving.GetModelMetadataResponse.metadata\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_GETMODELMETADATARESPONSE_METADATAENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=459,\n  serialized_end=685,\n)\n\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2._SIGNATUREDEF\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY.containing_type = _SIGNATUREDEFMAP\n_SIGNATUREDEFMAP.fields_by_name[\'signature_def\'].message_type = _SIGNATUREDEFMAP_SIGNATUREDEFENTRY\n_GETMODELMETADATAREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_GETMODELMETADATARESPONSE_METADATAENTRY.fields_by_name[\'value\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_GETMODELMETADATARESPONSE_METADATAENTRY.containing_type = _GETMODELMETADATARESPONSE\n_GETMODELMETADATARESPONSE.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_GETMODELMETADATARESPONSE.fields_by_name[\'metadata\'].message_type = _GETMODELMETADATARESPONSE_METADATAENTRY\nDESCRIPTOR.message_types_by_name[\'SignatureDefMap\'] = _SIGNATUREDEFMAP\nDESCRIPTOR.message_types_by_name[\'GetModelMetadataRequest\'] = _GETMODELMETADATAREQUEST\nDESCRIPTOR.message_types_by_name[\'GetModelMetadataResponse\'] = _GETMODELMETADATARESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSignatureDefMap = _reflection.GeneratedProtocolMessageType(\'SignatureDefMap\', (_message.Message,), dict(\n\n  SignatureDefEntry = _reflection.GeneratedProtocolMessageType(\'SignatureDefEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SIGNATUREDEFMAP_SIGNATUREDEFENTRY,\n    __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.SignatureDefMap.SignatureDefEntry)\n    ))\n  ,\n  DESCRIPTOR = _SIGNATUREDEFMAP,\n  __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.SignatureDefMap)\n  ))\n_sym_db.RegisterMessage(SignatureDefMap)\n_sym_db.RegisterMessage(SignatureDefMap.SignatureDefEntry)\n\nGetModelMetadataRequest = _reflection.GeneratedProtocolMessageType(\'GetModelMetadataRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _GETMODELMETADATAREQUEST,\n  __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.GetModelMetadataRequest)\n  ))\n_sym_db.RegisterMessage(GetModelMetadataRequest)\n\nGetModelMetadataResponse = _reflection.GeneratedProtocolMessageType(\'GetModelMetadataResponse\', (_message.Message,), dict(\n\n  MetadataEntry = _reflection.GeneratedProtocolMessageType(\'MetadataEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _GETMODELMETADATARESPONSE_METADATAENTRY,\n    __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.GetModelMetadataResponse.MetadataEntry)\n    ))\n  ,\n  DESCRIPTOR = _GETMODELMETADATARESPONSE,\n  __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.GetModelMetadataResponse)\n  ))\n_sym_db.RegisterMessage(GetModelMetadataResponse)\n_sym_db.RegisterMessage(GetModelMetadataResponse.MetadataEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY.has_options = True\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_GETMODELMETADATARESPONSE_METADATAENTRY.has_options = True\n_GETMODELMETADATARESPONSE_METADATAENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/get_model_metadata_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow_serving/apis/inference_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/inference.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom tensorflow_serving.apis import input_pb2 as tensorflow__serving_dot_apis_dot_input__pb2\nfrom tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\nfrom tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/inference.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\\'tensorflow_serving/apis/inference.proto\\x12\\x12tensorflow.serving\\x1a,tensorflow_serving/apis/classification.proto\\x1a#tensorflow_serving/apis/input.proto\\x1a#tensorflow_serving/apis/model.proto\\x1a(tensorflow_serving/apis/regression.proto\\""W\\n\\rInferenceTask\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12\\x13\\n\\x0bmethod_name\\x18\\x02 \\x01(\\t\\""\\xdc\\x01\\n\\x0fInferenceResult\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12I\\n\\x15\\x63lassification_result\\x18\\x02 \\x01(\\x0b\\x32(.tensorflow.serving.ClassificationResultH\\x00\\x12\\x41\\n\\x11regression_result\\x18\\x03 \\x01(\\x0b\\x32$.tensorflow.serving.RegressionResultH\\x00\\x42\\x08\\n\\x06result\\""s\\n\\x15MultiInferenceRequest\\x12\\x30\\n\\x05tasks\\x18\\x01 \\x03(\\x0b\\x32!.tensorflow.serving.InferenceTask\\x12(\\n\\x05input\\x18\\x02 \\x01(\\x0b\\x32\\x19.tensorflow.serving.Input\\""N\\n\\x16MultiInferenceResponse\\x12\\x34\\n\\x07results\\x18\\x01 \\x03(\\x0b\\x32#.tensorflow.serving.InferenceResultB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_classification__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_input__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_regression__pb2.DESCRIPTOR,])\n\n\n\n\n_INFERENCETASK = _descriptor.Descriptor(\n  name=\'InferenceTask\',\n  full_name=\'tensorflow.serving.InferenceTask\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.InferenceTask.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'method_name\', full_name=\'tensorflow.serving.InferenceTask.method_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=225,\n  serialized_end=312,\n)\n\n\n_INFERENCERESULT = _descriptor.Descriptor(\n  name=\'InferenceResult\',\n  full_name=\'tensorflow.serving.InferenceResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.InferenceResult.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'classification_result\', full_name=\'tensorflow.serving.InferenceResult.classification_result\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'regression_result\', full_name=\'tensorflow.serving.InferenceResult.regression_result\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'result\', full_name=\'tensorflow.serving.InferenceResult.result\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=315,\n  serialized_end=535,\n)\n\n\n_MULTIINFERENCEREQUEST = _descriptor.Descriptor(\n  name=\'MultiInferenceRequest\',\n  full_name=\'tensorflow.serving.MultiInferenceRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tasks\', full_name=\'tensorflow.serving.MultiInferenceRequest.tasks\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorflow.serving.MultiInferenceRequest.input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=537,\n  serialized_end=652,\n)\n\n\n_MULTIINFERENCERESPONSE = _descriptor.Descriptor(\n  name=\'MultiInferenceResponse\',\n  full_name=\'tensorflow.serving.MultiInferenceResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'results\', full_name=\'tensorflow.serving.MultiInferenceResponse.results\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=654,\n  serialized_end=732,\n)\n\n_INFERENCETASK.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_INFERENCERESULT.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_INFERENCERESULT.fields_by_name[\'classification_result\'].message_type = tensorflow__serving_dot_apis_dot_classification__pb2._CLASSIFICATIONRESULT\n_INFERENCERESULT.fields_by_name[\'regression_result\'].message_type = tensorflow__serving_dot_apis_dot_regression__pb2._REGRESSIONRESULT\n_INFERENCERESULT.oneofs_by_name[\'result\'].fields.append(\n  _INFERENCERESULT.fields_by_name[\'classification_result\'])\n_INFERENCERESULT.fields_by_name[\'classification_result\'].containing_oneof = _INFERENCERESULT.oneofs_by_name[\'result\']\n_INFERENCERESULT.oneofs_by_name[\'result\'].fields.append(\n  _INFERENCERESULT.fields_by_name[\'regression_result\'])\n_INFERENCERESULT.fields_by_name[\'regression_result\'].containing_oneof = _INFERENCERESULT.oneofs_by_name[\'result\']\n_MULTIINFERENCEREQUEST.fields_by_name[\'tasks\'].message_type = _INFERENCETASK\n_MULTIINFERENCEREQUEST.fields_by_name[\'input\'].message_type = tensorflow__serving_dot_apis_dot_input__pb2._INPUT\n_MULTIINFERENCERESPONSE.fields_by_name[\'results\'].message_type = _INFERENCERESULT\nDESCRIPTOR.message_types_by_name[\'InferenceTask\'] = _INFERENCETASK\nDESCRIPTOR.message_types_by_name[\'InferenceResult\'] = _INFERENCERESULT\nDESCRIPTOR.message_types_by_name[\'MultiInferenceRequest\'] = _MULTIINFERENCEREQUEST\nDESCRIPTOR.message_types_by_name[\'MultiInferenceResponse\'] = _MULTIINFERENCERESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nInferenceTask = _reflection.GeneratedProtocolMessageType(\'InferenceTask\', (_message.Message,), dict(\n  DESCRIPTOR = _INFERENCETASK,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceTask)\n  ))\n_sym_db.RegisterMessage(InferenceTask)\n\nInferenceResult = _reflection.GeneratedProtocolMessageType(\'InferenceResult\', (_message.Message,), dict(\n  DESCRIPTOR = _INFERENCERESULT,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceResult)\n  ))\n_sym_db.RegisterMessage(InferenceResult)\n\nMultiInferenceRequest = _reflection.GeneratedProtocolMessageType(\'MultiInferenceRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _MULTIINFERENCEREQUEST,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceRequest)\n  ))\n_sym_db.RegisterMessage(MultiInferenceRequest)\n\nMultiInferenceResponse = _reflection.GeneratedProtocolMessageType(\'MultiInferenceResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _MULTIINFERENCERESPONSE,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceResponse)\n  ))\n_sym_db.RegisterMessage(MultiInferenceResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/inference_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow_serving/apis/input_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/input.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.example import example_pb2 as tensorflow_dot_core_dot_example_dot_example__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/input.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n#tensorflow_serving/apis/input.proto\\x12\\x12tensorflow.serving\\x1a%tensorflow/core/example/example.proto\\""4\\n\\x0b\\x45xampleList\\x12%\\n\\x08\\x65xamples\\x18\\x01 \\x03(\\x0b\\x32\\x13.tensorflow.Example\\""e\\n\\x16\\x45xampleListWithContext\\x12%\\n\\x08\\x65xamples\\x18\\x01 \\x03(\\x0b\\x32\\x13.tensorflow.Example\\x12$\\n\\x07\\x63ontext\\x18\\x02 \\x01(\\x0b\\x32\\x13.tensorflow.Example\\""\\xa1\\x01\\n\\x05Input\\x12;\\n\\x0c\\x65xample_list\\x18\\x01 \\x01(\\x0b\\x32\\x1f.tensorflow.serving.ExampleListB\\x02(\\x01H\\x00\\x12S\\n\\x19\\x65xample_list_with_context\\x18\\x02 \\x01(\\x0b\\x32*.tensorflow.serving.ExampleListWithContextB\\x02(\\x01H\\x00\\x42\\x06\\n\\x04kindB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_example_dot_example__pb2.DESCRIPTOR,])\n\n\n\n\n_EXAMPLELIST = _descriptor.Descriptor(\n  name=\'ExampleList\',\n  full_name=\'tensorflow.serving.ExampleList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'examples\', full_name=\'tensorflow.serving.ExampleList.examples\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=98,\n  serialized_end=150,\n)\n\n\n_EXAMPLELISTWITHCONTEXT = _descriptor.Descriptor(\n  name=\'ExampleListWithContext\',\n  full_name=\'tensorflow.serving.ExampleListWithContext\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'examples\', full_name=\'tensorflow.serving.ExampleListWithContext.examples\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'context\', full_name=\'tensorflow.serving.ExampleListWithContext.context\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=152,\n  serialized_end=253,\n)\n\n\n_INPUT = _descriptor.Descriptor(\n  name=\'Input\',\n  full_name=\'tensorflow.serving.Input\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'example_list\', full_name=\'tensorflow.serving.Input.example_list\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'example_list_with_context\', full_name=\'tensorflow.serving.Input.example_list_with_context\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'kind\', full_name=\'tensorflow.serving.Input.kind\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=256,\n  serialized_end=417,\n)\n\n_EXAMPLELIST.fields_by_name[\'examples\'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE\n_EXAMPLELISTWITHCONTEXT.fields_by_name[\'examples\'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE\n_EXAMPLELISTWITHCONTEXT.fields_by_name[\'context\'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE\n_INPUT.fields_by_name[\'example_list\'].message_type = _EXAMPLELIST\n_INPUT.fields_by_name[\'example_list_with_context\'].message_type = _EXAMPLELISTWITHCONTEXT\n_INPUT.oneofs_by_name[\'kind\'].fields.append(\n  _INPUT.fields_by_name[\'example_list\'])\n_INPUT.fields_by_name[\'example_list\'].containing_oneof = _INPUT.oneofs_by_name[\'kind\']\n_INPUT.oneofs_by_name[\'kind\'].fields.append(\n  _INPUT.fields_by_name[\'example_list_with_context\'])\n_INPUT.fields_by_name[\'example_list_with_context\'].containing_oneof = _INPUT.oneofs_by_name[\'kind\']\nDESCRIPTOR.message_types_by_name[\'ExampleList\'] = _EXAMPLELIST\nDESCRIPTOR.message_types_by_name[\'ExampleListWithContext\'] = _EXAMPLELISTWITHCONTEXT\nDESCRIPTOR.message_types_by_name[\'Input\'] = _INPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nExampleList = _reflection.GeneratedProtocolMessageType(\'ExampleList\', (_message.Message,), dict(\n  DESCRIPTOR = _EXAMPLELIST,\n  __module__ = \'tensorflow_serving.apis.input_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ExampleList)\n  ))\n_sym_db.RegisterMessage(ExampleList)\n\nExampleListWithContext = _reflection.GeneratedProtocolMessageType(\'ExampleListWithContext\', (_message.Message,), dict(\n  DESCRIPTOR = _EXAMPLELISTWITHCONTEXT,\n  __module__ = \'tensorflow_serving.apis.input_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ExampleListWithContext)\n  ))\n_sym_db.RegisterMessage(ExampleListWithContext)\n\nInput = _reflection.GeneratedProtocolMessageType(\'Input\', (_message.Message,), dict(\n  DESCRIPTOR = _INPUT,\n  __module__ = \'tensorflow_serving.apis.input_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Input)\n  ))\n_sym_db.RegisterMessage(Input)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_INPUT.fields_by_name[\'example_list\'].has_options = True\n_INPUT.fields_by_name[\'example_list\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\'))\n_INPUT.fields_by_name[\'example_list_with_context\'].has_options = True\n_INPUT.fields_by_name[\'example_list_with_context\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/input_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow_serving/apis/model_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/model.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/model.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n#tensorflow_serving/apis/model.proto\\x12\\x12tensorflow.serving\\x1a\\x1egoogle/protobuf/wrappers.proto\\""_\\n\\tModelSpec\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12,\\n\\x07version\\x18\\x02 \\x01(\\x0b\\x32\\x1b.google.protobuf.Int64Value\\x12\\x16\\n\\x0esignature_name\\x18\\x03 \\x01(\\tB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_wrappers__pb2.DESCRIPTOR,])\n\n\n\n\n_MODELSPEC = _descriptor.Descriptor(\n  name=\'ModelSpec\',\n  full_name=\'tensorflow.serving.ModelSpec\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.serving.ModelSpec.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.serving.ModelSpec.version\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'signature_name\', full_name=\'tensorflow.serving.ModelSpec.signature_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=91,\n  serialized_end=186,\n)\n\n_MODELSPEC.fields_by_name[\'version\'].message_type = google_dot_protobuf_dot_wrappers__pb2._INT64VALUE\nDESCRIPTOR.message_types_by_name[\'ModelSpec\'] = _MODELSPEC\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nModelSpec = _reflection.GeneratedProtocolMessageType(\'ModelSpec\', (_message.Message,), dict(\n  DESCRIPTOR = _MODELSPEC,\n  __module__ = \'tensorflow_serving.apis.model_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ModelSpec)\n  ))\n_sym_db.RegisterMessage(ModelSpec)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/model_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow_serving/apis/predict_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/predict.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nfrom tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/predict.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow_serving/apis/predict.proto\\x12\\x12tensorflow.serving\\x1a&tensorflow/core/framework/tensor.proto\\x1a#tensorflow_serving/apis/model.proto\\""\\xe2\\x01\\n\\x0ePredictRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12>\\n\\x06inputs\\x18\\x02 \\x03(\\x0b\\x32..tensorflow.serving.PredictRequest.InputsEntry\\x12\\x15\\n\\routput_filter\\x18\\x03 \\x03(\\t\\x1a\\x46\\n\\x0bInputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\""\\x9d\\x01\\n\\x0fPredictResponse\\x12\\x41\\n\\x07outputs\\x18\\x01 \\x03(\\x0b\\x32\\x30.tensorflow.serving.PredictResponse.OutputsEntry\\x1aG\\n\\x0cOutputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\x42\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_PREDICTREQUEST_INPUTSENTRY = _descriptor.Descriptor(\n  name=\'InputsEntry\',\n  full_name=\'tensorflow.serving.PredictRequest.InputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=295,\n  serialized_end=365,\n)\n\n_PREDICTREQUEST = _descriptor.Descriptor(\n  name=\'PredictRequest\',\n  full_name=\'tensorflow.serving.PredictRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.PredictRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inputs\', full_name=\'tensorflow.serving.PredictRequest.inputs\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_filter\', full_name=\'tensorflow.serving.PredictRequest.output_filter\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTREQUEST_INPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=139,\n  serialized_end=365,\n)\n\n\n_PREDICTRESPONSE_OUTPUTSENTRY = _descriptor.Descriptor(\n  name=\'OutputsEntry\',\n  full_name=\'tensorflow.serving.PredictResponse.OutputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=454,\n  serialized_end=525,\n)\n\n_PREDICTRESPONSE = _descriptor.Descriptor(\n  name=\'PredictResponse\',\n  full_name=\'tensorflow.serving.PredictResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'outputs\', full_name=\'tensorflow.serving.PredictResponse.outputs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTRESPONSE_OUTPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=368,\n  serialized_end=525,\n)\n\n_PREDICTREQUEST_INPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTREQUEST_INPUTSENTRY.containing_type = _PREDICTREQUEST\n_PREDICTREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_PREDICTREQUEST.fields_by_name[\'inputs\'].message_type = _PREDICTREQUEST_INPUTSENTRY\n_PREDICTRESPONSE_OUTPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTRESPONSE_OUTPUTSENTRY.containing_type = _PREDICTRESPONSE\n_PREDICTRESPONSE.fields_by_name[\'outputs\'].message_type = _PREDICTRESPONSE_OUTPUTSENTRY\nDESCRIPTOR.message_types_by_name[\'PredictRequest\'] = _PREDICTREQUEST\nDESCRIPTOR.message_types_by_name[\'PredictResponse\'] = _PREDICTRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nPredictRequest = _reflection.GeneratedProtocolMessageType(\'PredictRequest\', (_message.Message,), dict(\n\n  InputsEntry = _reflection.GeneratedProtocolMessageType(\'InputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTREQUEST_INPUTSENTRY,\n    __module__ = \'tensorflow_serving.apis.predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest.InputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTREQUEST,\n  __module__ = \'tensorflow_serving.apis.predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest)\n  ))\n_sym_db.RegisterMessage(PredictRequest)\n_sym_db.RegisterMessage(PredictRequest.InputsEntry)\n\nPredictResponse = _reflection.GeneratedProtocolMessageType(\'PredictResponse\', (_message.Message,), dict(\n\n  OutputsEntry = _reflection.GeneratedProtocolMessageType(\'OutputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTRESPONSE_OUTPUTSENTRY,\n    __module__ = \'tensorflow_serving.apis.predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse.OutputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTRESPONSE,\n  __module__ = \'tensorflow_serving.apis.predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse)\n  ))\n_sym_db.RegisterMessage(PredictResponse)\n_sym_db.RegisterMessage(PredictResponse.OutputsEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_PREDICTREQUEST_INPUTSENTRY.has_options = True\n_PREDICTREQUEST_INPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_PREDICTRESPONSE_OUTPUTSENTRY.has_options = True\n_PREDICTRESPONSE_OUTPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/predict_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow_serving/apis/prediction_service_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/prediction_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom tensorflow_serving.apis import get_model_metadata_pb2 as tensorflow__serving_dot_apis_dot_get__model__metadata__pb2\nfrom tensorflow_serving.apis import inference_pb2 as tensorflow__serving_dot_apis_dot_inference__pb2\nfrom tensorflow_serving.apis import predict_pb2 as tensorflow__serving_dot_apis_dot_predict__pb2\nfrom tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/prediction_service.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow_serving/apis/prediction_service.proto\\x12\\x12tensorflow.serving\\x1a,tensorflow_serving/apis/classification.proto\\x1a\\x30tensorflow_serving/apis/get_model_metadata.proto\\x1a\\\'tensorflow_serving/apis/inference.proto\\x1a%tensorflow_serving/apis/predict.proto\\x1a(tensorflow_serving/apis/regression.proto2\\xfc\\x03\\n\\x11PredictionService\\x12\\x61\\n\\x08\\x43lassify\\x12).tensorflow.serving.ClassificationRequest\\x1a*.tensorflow.serving.ClassificationResponse\\x12X\\n\\x07Regress\\x12%.tensorflow.serving.RegressionRequest\\x1a&.tensorflow.serving.RegressionResponse\\x12R\\n\\x07Predict\\x12\\"".tensorflow.serving.PredictRequest\\x1a#.tensorflow.serving.PredictResponse\\x12g\\n\\x0eMultiInference\\x12).tensorflow.serving.MultiInferenceRequest\\x1a*.tensorflow.serving.MultiInferenceResponse\\x12m\\n\\x10GetModelMetadata\\x12+.tensorflow.serving.GetModelMetadataRequest\\x1a,.tensorflow.serving.GetModelMetadataResponseB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_classification__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_inference__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_predict__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_regression__pb2.DESCRIPTOR,])\n\n\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\n\n\n  class PredictionServiceStub(object):\n    """"""open source marker; do not remove\n    PredictionService provides access to machine-learned models loaded by\n    model_servers.\n    """"""\n\n    def __init__(self, channel):\n      """"""Constructor.\n\n      Args:\n        channel: A grpc.Channel.\n      """"""\n      self.Classify = channel.unary_unary(\n          \'/tensorflow.serving.PredictionService/Classify\',\n          request_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.SerializeToString,\n          response_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.FromString,\n          )\n      self.Regress = channel.unary_unary(\n          \'/tensorflow.serving.PredictionService/Regress\',\n          request_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.SerializeToString,\n          response_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.FromString,\n          )\n      self.Predict = channel.unary_unary(\n          \'/tensorflow.serving.PredictionService/Predict\',\n          request_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.SerializeToString,\n          response_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.FromString,\n          )\n      self.MultiInference = channel.unary_unary(\n          \'/tensorflow.serving.PredictionService/MultiInference\',\n          request_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.SerializeToString,\n          response_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.FromString,\n          )\n      self.GetModelMetadata = channel.unary_unary(\n          \'/tensorflow.serving.PredictionService/GetModelMetadata\',\n          request_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.SerializeToString,\n          response_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.FromString,\n          )\n\n\n  class PredictionServiceServicer(object):\n    """"""open source marker; do not remove\n    PredictionService provides access to machine-learned models loaded by\n    model_servers.\n    """"""\n\n    def Classify(self, request, context):\n      """"""Classify.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def Regress(self, request, context):\n      """"""Regress.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def Predict(self, request, context):\n      """"""Predict -- provides access to loaded TensorFlow model.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def MultiInference(self, request, context):\n      """"""MultiInference API for multi-headed models.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def GetModelMetadata(self, request, context):\n      """"""GetModelMetadata - provides access to metadata for loaded models.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n\n  def add_PredictionServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n        \'Classify\': grpc.unary_unary_rpc_method_handler(\n            servicer.Classify,\n            request_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.FromString,\n            response_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.SerializeToString,\n        ),\n        \'Regress\': grpc.unary_unary_rpc_method_handler(\n            servicer.Regress,\n            request_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.FromString,\n            response_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.SerializeToString,\n        ),\n        \'Predict\': grpc.unary_unary_rpc_method_handler(\n            servicer.Predict,\n            request_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.FromString,\n            response_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.SerializeToString,\n        ),\n        \'MultiInference\': grpc.unary_unary_rpc_method_handler(\n            servicer.MultiInference,\n            request_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.FromString,\n            response_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.SerializeToString,\n        ),\n        \'GetModelMetadata\': grpc.unary_unary_rpc_method_handler(\n            servicer.GetModelMetadata,\n            request_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.FromString,\n            response_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.SerializeToString,\n        ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n        \'tensorflow.serving.PredictionService\', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))\n\n\n  class BetaPredictionServiceServicer(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""open source marker; do not remove\n    PredictionService provides access to machine-learned models loaded by\n    model_servers.\n    """"""\n    def Classify(self, request, context):\n      """"""Classify.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def Regress(self, request, context):\n      """"""Regress.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def Predict(self, request, context):\n      """"""Predict -- provides access to loaded TensorFlow model.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def MultiInference(self, request, context):\n      """"""MultiInference API for multi-headed models.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def GetModelMetadata(self, request, context):\n      """"""GetModelMetadata - provides access to metadata for loaded models.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\n  class BetaPredictionServiceStub(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""open source marker; do not remove\n    PredictionService provides access to machine-learned models loaded by\n    model_servers.\n    """"""\n    def Classify(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Classify.\n      """"""\n      raise NotImplementedError()\n    Classify.future = None\n    def Regress(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Regress.\n      """"""\n      raise NotImplementedError()\n    Regress.future = None\n    def Predict(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Predict -- provides access to loaded TensorFlow model.\n      """"""\n      raise NotImplementedError()\n    Predict.future = None\n    def MultiInference(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""MultiInference API for multi-headed models.\n      """"""\n      raise NotImplementedError()\n    MultiInference.future = None\n    def GetModelMetadata(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""GetModelMetadata - provides access to metadata for loaded models.\n      """"""\n      raise NotImplementedError()\n    GetModelMetadata.future = None\n\n\n  def beta_create_PredictionService_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_deserializers = {\n      (\'tensorflow.serving.PredictionService\', \'Classify\'): tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.FromString,\n      (\'tensorflow.serving.PredictionService\', \'GetModelMetadata\'): tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.FromString,\n      (\'tensorflow.serving.PredictionService\', \'MultiInference\'): tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.FromString,\n      (\'tensorflow.serving.PredictionService\', \'Predict\'): tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.FromString,\n      (\'tensorflow.serving.PredictionService\', \'Regress\'): tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.FromString,\n    }\n    response_serializers = {\n      (\'tensorflow.serving.PredictionService\', \'Classify\'): tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'GetModelMetadata\'): tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'MultiInference\'): tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'Predict\'): tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'Regress\'): tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.SerializeToString,\n    }\n    method_implementations = {\n      (\'tensorflow.serving.PredictionService\', \'Classify\'): face_utilities.unary_unary_inline(servicer.Classify),\n      (\'tensorflow.serving.PredictionService\', \'GetModelMetadata\'): face_utilities.unary_unary_inline(servicer.GetModelMetadata),\n      (\'tensorflow.serving.PredictionService\', \'MultiInference\'): face_utilities.unary_unary_inline(servicer.MultiInference),\n      (\'tensorflow.serving.PredictionService\', \'Predict\'): face_utilities.unary_unary_inline(servicer.Predict),\n      (\'tensorflow.serving.PredictionService\', \'Regress\'): face_utilities.unary_unary_inline(servicer.Regress),\n    }\n    server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n    return beta_implementations.server(method_implementations, options=server_options)\n\n\n  def beta_create_PredictionService_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_serializers = {\n      (\'tensorflow.serving.PredictionService\', \'Classify\'): tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'GetModelMetadata\'): tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'MultiInference\'): tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'Predict\'): tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.SerializeToString,\n      (\'tensorflow.serving.PredictionService\', \'Regress\'): tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.SerializeToString,\n    }\n    response_deserializers = {\n      (\'tensorflow.serving.PredictionService\', \'Classify\'): tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.FromString,\n      (\'tensorflow.serving.PredictionService\', \'GetModelMetadata\'): tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.FromString,\n      (\'tensorflow.serving.PredictionService\', \'MultiInference\'): tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.FromString,\n      (\'tensorflow.serving.PredictionService\', \'Predict\'): tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.FromString,\n      (\'tensorflow.serving.PredictionService\', \'Regress\'): tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.FromString,\n    }\n    cardinalities = {\n      \'Classify\': cardinality.Cardinality.UNARY_UNARY,\n      \'GetModelMetadata\': cardinality.Cardinality.UNARY_UNARY,\n      \'MultiInference\': cardinality.Cardinality.UNARY_UNARY,\n      \'Predict\': cardinality.Cardinality.UNARY_UNARY,\n      \'Regress\': cardinality.Cardinality.UNARY_UNARY,\n    }\n    stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n    return beta_implementations.dynamic_stub(channel, \'tensorflow.serving.PredictionService\', cardinalities, options=stub_options)\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/prediction_service_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom tensorflow_serving.apis import get_model_metadata_pb2 as tensorflow__serving_dot_apis_dot_get__model__metadata__pb2\nfrom tensorflow_serving.apis import inference_pb2 as tensorflow__serving_dot_apis_dot_inference__pb2\nfrom tensorflow_serving.apis import predict_pb2 as tensorflow__serving_dot_apis_dot_predict__pb2\nfrom tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nclass PredictionServiceStub(object):\n  """"""open source marker; do not remove\n  PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Classify = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Classify\',\n        request_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.FromString,\n        )\n    self.Regress = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Regress\',\n        request_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.FromString,\n        )\n    self.Predict = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Predict\',\n        request_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.FromString,\n        )\n    self.MultiInference = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/MultiInference\',\n        request_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.FromString,\n        )\n    self.GetModelMetadata = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/GetModelMetadata\',\n        request_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.FromString,\n        )\n\n\nclass PredictionServiceServicer(object):\n  """"""open source marker; do not remove\n  PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def Classify(self, request, context):\n    """"""Classify.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Regress(self, request, context):\n    """"""Regress.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Predict(self, request, context):\n    """"""Predict -- provides access to loaded TensorFlow model.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def MultiInference(self, request, context):\n    """"""MultiInference API for multi-headed models.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def GetModelMetadata(self, request, context):\n    """"""GetModelMetadata - provides access to metadata for loaded models.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_PredictionServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Classify\': grpc.unary_unary_rpc_method_handler(\n          servicer.Classify,\n          request_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.SerializeToString,\n      ),\n      \'Regress\': grpc.unary_unary_rpc_method_handler(\n          servicer.Regress,\n          request_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.SerializeToString,\n      ),\n      \'Predict\': grpc.unary_unary_rpc_method_handler(\n          servicer.Predict,\n          request_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.SerializeToString,\n      ),\n      \'MultiInference\': grpc.unary_unary_rpc_method_handler(\n          servicer.MultiInference,\n          request_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.SerializeToString,\n      ),\n      \'GetModelMetadata\': grpc.unary_unary_rpc_method_handler(\n          servicer.GetModelMetadata,\n          request_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.serving.PredictionService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
libs/pipeline_model/tensorflow_serving/apis/regression_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/regression.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow_serving.apis import input_pb2 as tensorflow__serving_dot_apis_dot_input__pb2\nfrom tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/regression.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(tensorflow_serving/apis/regression.proto\\x12\\x12tensorflow.serving\\x1a#tensorflow_serving/apis/input.proto\\x1a#tensorflow_serving/apis/model.proto\\""\\x1b\\n\\nRegression\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x02\\""G\\n\\x10RegressionResult\\x12\\x33\\n\\x0bregressions\\x18\\x01 \\x03(\\x0b\\x32\\x1e.tensorflow.serving.Regression\\""p\\n\\x11RegressionRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12(\\n\\x05input\\x18\\x02 \\x01(\\x0b\\x32\\x19.tensorflow.serving.Input\\""J\\n\\x12RegressionResponse\\x12\\x34\\n\\x06result\\x18\\x01 \\x01(\\x0b\\x32$.tensorflow.serving.RegressionResultB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_input__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_REGRESSION = _descriptor.Descriptor(\n  name=\'Regression\',\n  full_name=\'tensorflow.serving.Regression\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.Regression.value\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=138,\n  serialized_end=165,\n)\n\n\n_REGRESSIONRESULT = _descriptor.Descriptor(\n  name=\'RegressionResult\',\n  full_name=\'tensorflow.serving.RegressionResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'regressions\', full_name=\'tensorflow.serving.RegressionResult.regressions\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=167,\n  serialized_end=238,\n)\n\n\n_REGRESSIONREQUEST = _descriptor.Descriptor(\n  name=\'RegressionRequest\',\n  full_name=\'tensorflow.serving.RegressionRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.RegressionRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorflow.serving.RegressionRequest.input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=240,\n  serialized_end=352,\n)\n\n\n_REGRESSIONRESPONSE = _descriptor.Descriptor(\n  name=\'RegressionResponse\',\n  full_name=\'tensorflow.serving.RegressionResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'result\', full_name=\'tensorflow.serving.RegressionResponse.result\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=354,\n  serialized_end=428,\n)\n\n_REGRESSIONRESULT.fields_by_name[\'regressions\'].message_type = _REGRESSION\n_REGRESSIONREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_REGRESSIONREQUEST.fields_by_name[\'input\'].message_type = tensorflow__serving_dot_apis_dot_input__pb2._INPUT\n_REGRESSIONRESPONSE.fields_by_name[\'result\'].message_type = _REGRESSIONRESULT\nDESCRIPTOR.message_types_by_name[\'Regression\'] = _REGRESSION\nDESCRIPTOR.message_types_by_name[\'RegressionResult\'] = _REGRESSIONRESULT\nDESCRIPTOR.message_types_by_name[\'RegressionRequest\'] = _REGRESSIONREQUEST\nDESCRIPTOR.message_types_by_name[\'RegressionResponse\'] = _REGRESSIONRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nRegression = _reflection.GeneratedProtocolMessageType(\'Regression\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSION,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Regression)\n  ))\n_sym_db.RegisterMessage(Regression)\n\nRegressionResult = _reflection.GeneratedProtocolMessageType(\'RegressionResult\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSIONRESULT,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.RegressionResult)\n  ))\n_sym_db.RegisterMessage(RegressionResult)\n\nRegressionRequest = _reflection.GeneratedProtocolMessageType(\'RegressionRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSIONREQUEST,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.RegressionRequest)\n  ))\n_sym_db.RegisterMessage(RegressionRequest)\n\nRegressionResponse = _reflection.GeneratedProtocolMessageType(\'RegressionResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSIONRESPONSE,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.RegressionResponse)\n  ))\n_sym_db.RegisterMessage(RegressionResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow_serving/apis/regression_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_tests/__init__.py,0,b''
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_tests/test_pipeline.py,0,"b'import click\nfrom click.testing import CliRunner\nimport dill\nimport numpy as np\n\nfrom pipeline_steps.data_downloader.pipeline_step import run_pipeline as download_cli\nfrom pipeline_steps.clean_text.pipeline_step import run_pipeline as clean_cli\nfrom pipeline_steps.spacy_tokenize.pipeline_step import run_pipeline as spacy_cli\nfrom pipeline_steps.tfidf_vectorizer.pipeline_step import run_pipeline as tfidf_cli\nfrom pipeline_steps.lr_text_classifier.pipeline_step import run_pipeline as lr_cli\n\nimport sys\nsys.path.append("".."")\n\ndef test_pipeline():\n\n    runner = CliRunner()\n    \n    with runner.isolated_filesystem():\n\n        # # Test Data Downloader\n        # result = runner.invoke(download_cli, [\n        #     \'--labels-path\', \'labels.data\',\n        #     \'--features-path\', \'raw_text.data\'])\n\n        # with open(\'raw_text.data\', ""rb"") as f:\n        #     assert f\n        # with open(\'labels.data\', ""rb"") as f:\n        #     assert f\n\n        # Creating test data\n        with open(\'raw_text.data\', \'wb\') as f:\n            raw_arr = np.array([\n                ""hello this is a test"", \n                ""another sentence to process""])\n            dill.dump(raw_arr, f)\n\n        # Test Clean text transformer\n        result = runner.invoke(clean_cli, [\n            \'--in-path\', \'raw_text.data\',\n            \'--out-path\', \'clean_text.data\'])\n\n        with open(\'clean_text.data\', ""rb"") as f:\n            clean_arr = dill.load(f)\n            assert all(raw_arr == clean_arr)\n\n        # Test spacy tokenizer\n        result = runner.invoke(spacy_cli, [\n            \'--in-path\', \'clean_text.data\',\n            \'--out-path\', \'tokenized_text.data\'])\n\n        with open(\'tokenized_text.data\', ""rb"") as f:\n            tokenized_arr = dill.load(f)\n            expected_array = np.array([\n                [""hello"", ""this"", ""be"", ""a"", ""test""],\n                [""another"", ""sentence"", ""to"", ""process""]\n            ])\n            assert all(tokenized_arr == expected_array)\n\n        # Test tfidf vectorizer\n        result = runner.invoke(tfidf_cli, [\n            \'--in-path\', \'tokenized_text.data\',\n            \'--out-path\', \'tfidf_vectors.data\',\n            \'--max-features\', ""10"",\n            \'--ngram-range\', ""3"",\n            \'--action\', \'train\',\n            \'--model-path\', \'tfidf.model\'])\n\n        with open(""tfidf_vectors.data"", ""rb"") as f:\n            tfidf_vectors = dill.load(f)\n            assert tfidf_vectors.shape == (2,10)\n\n        with open(""tfidf.model"", ""rb"") as f:\n            assert f\n\n        # Test lr model\n\n        with open(""labels.data"", ""wb"") as f:\n            labels = np.array([0,1])\n            dill.dump(labels, f)\n\n        result = runner.invoke(lr_cli, [\n            \'--in-path\', \'tfidf_vectors.data\',\n            \'--labels-path\', \'labels.data\',\n            \'--out-path\', \'prediction.data\',\n            \'--c-param\', ""0.1"",\n            \'--action\', \'train\',\n            \'--model-path\', \'lr_text.model\'])\n\n        with open(""prediction.data"", ""rb"") as f:\n            assert f\n\n        with open(""lr_text.model"", ""rb"") as f:\n            assert f\n\n'"
libs/pipeline_model/tensorflow/core/example/__init__.py,0,b''
libs/pipeline_model/tensorflow/core/example/example_parser_configuration_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/example/example_parser_configuration.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/example/example_parser_configuration.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n:tensorflow/core/example/example_parser_configuration.proto\\x12\\ntensorflow\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a&tensorflow/core/framework/tensor.proto\\x1a%tensorflow/core/framework/types.proto\\""\\xa3\\x01\\n\\x12VarLenFeatureProto\\x12#\\n\\x05\\x64type\\x18\\x01 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12!\\n\\x19values_output_tensor_name\\x18\\x02 \\x01(\\t\\x12\\""\\n\\x1aindices_output_tensor_name\\x18\\x03 \\x01(\\t\\x12!\\n\\x19shapes_output_tensor_name\\x18\\x04 \\x01(\\t\\""\\xbb\\x01\\n\\x14\\x46ixedLenFeatureProto\\x12#\\n\\x05\\x64type\\x18\\x01 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12+\\n\\x05shape\\x18\\x02 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\x12.\\n\\rdefault_value\\x18\\x03 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto\\x12!\\n\\x19values_output_tensor_name\\x18\\x04 \\x01(\\t\\""\\x9a\\x01\\n\\x14\\x46\\x65\\x61tureConfiguration\\x12=\\n\\x11\\x66ixed_len_feature\\x18\\x01 \\x01(\\x0b\\x32 .tensorflow.FixedLenFeatureProtoH\\x00\\x12\\x39\\n\\x0fvar_len_feature\\x18\\x02 \\x01(\\x0b\\x32\\x1e.tensorflow.VarLenFeatureProtoH\\x00\\x42\\x08\\n\\x06\\x63onfig\\""\\xbe\\x01\\n\\x1a\\x45xampleParserConfiguration\\x12K\\n\\x0b\\x66\\x65\\x61ture_map\\x18\\x01 \\x03(\\x0b\\x32\\x36.tensorflow.ExampleParserConfiguration.FeatureMapEntry\\x1aS\\n\\x0f\\x46\\x65\\x61tureMapEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12/\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32 .tensorflow.FeatureConfiguration:\\x02\\x38\\x01\\x42?\\n\\x16org.tensorflow.exampleB ExampleParserConfigurationProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n\n\n\n\n_VARLENFEATUREPROTO = _descriptor.Descriptor(\n  name=\'VarLenFeatureProto\',\n  full_name=\'tensorflow.VarLenFeatureProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.VarLenFeatureProto.dtype\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'values_output_tensor_name\', full_name=\'tensorflow.VarLenFeatureProto.values_output_tensor_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'indices_output_tensor_name\', full_name=\'tensorflow.VarLenFeatureProto.indices_output_tensor_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shapes_output_tensor_name\', full_name=\'tensorflow.VarLenFeatureProto.shapes_output_tensor_name\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=200,\n  serialized_end=363,\n)\n\n\n_FIXEDLENFEATUREPROTO = _descriptor.Descriptor(\n  name=\'FixedLenFeatureProto\',\n  full_name=\'tensorflow.FixedLenFeatureProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.FixedLenFeatureProto.dtype\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.FixedLenFeatureProto.shape\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'default_value\', full_name=\'tensorflow.FixedLenFeatureProto.default_value\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'values_output_tensor_name\', full_name=\'tensorflow.FixedLenFeatureProto.values_output_tensor_name\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=366,\n  serialized_end=553,\n)\n\n\n_FEATURECONFIGURATION = _descriptor.Descriptor(\n  name=\'FeatureConfiguration\',\n  full_name=\'tensorflow.FeatureConfiguration\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'fixed_len_feature\', full_name=\'tensorflow.FeatureConfiguration.fixed_len_feature\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'var_len_feature\', full_name=\'tensorflow.FeatureConfiguration.var_len_feature\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'config\', full_name=\'tensorflow.FeatureConfiguration.config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=556,\n  serialized_end=710,\n)\n\n\n_EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY = _descriptor.Descriptor(\n  name=\'FeatureMapEntry\',\n  full_name=\'tensorflow.ExampleParserConfiguration.FeatureMapEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.ExampleParserConfiguration.FeatureMapEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.ExampleParserConfiguration.FeatureMapEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=820,\n  serialized_end=903,\n)\n\n_EXAMPLEPARSERCONFIGURATION = _descriptor.Descriptor(\n  name=\'ExampleParserConfiguration\',\n  full_name=\'tensorflow.ExampleParserConfiguration\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'feature_map\', full_name=\'tensorflow.ExampleParserConfiguration.feature_map\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=713,\n  serialized_end=903,\n)\n\n_VARLENFEATUREPROTO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_FIXEDLENFEATUREPROTO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_FIXEDLENFEATUREPROTO.fields_by_name[\'shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_FIXEDLENFEATUREPROTO.fields_by_name[\'default_value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_FEATURECONFIGURATION.fields_by_name[\'fixed_len_feature\'].message_type = _FIXEDLENFEATUREPROTO\n_FEATURECONFIGURATION.fields_by_name[\'var_len_feature\'].message_type = _VARLENFEATUREPROTO\n_FEATURECONFIGURATION.oneofs_by_name[\'config\'].fields.append(\n  _FEATURECONFIGURATION.fields_by_name[\'fixed_len_feature\'])\n_FEATURECONFIGURATION.fields_by_name[\'fixed_len_feature\'].containing_oneof = _FEATURECONFIGURATION.oneofs_by_name[\'config\']\n_FEATURECONFIGURATION.oneofs_by_name[\'config\'].fields.append(\n  _FEATURECONFIGURATION.fields_by_name[\'var_len_feature\'])\n_FEATURECONFIGURATION.fields_by_name[\'var_len_feature\'].containing_oneof = _FEATURECONFIGURATION.oneofs_by_name[\'config\']\n_EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY.fields_by_name[\'value\'].message_type = _FEATURECONFIGURATION\n_EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY.containing_type = _EXAMPLEPARSERCONFIGURATION\n_EXAMPLEPARSERCONFIGURATION.fields_by_name[\'feature_map\'].message_type = _EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY\nDESCRIPTOR.message_types_by_name[\'VarLenFeatureProto\'] = _VARLENFEATUREPROTO\nDESCRIPTOR.message_types_by_name[\'FixedLenFeatureProto\'] = _FIXEDLENFEATUREPROTO\nDESCRIPTOR.message_types_by_name[\'FeatureConfiguration\'] = _FEATURECONFIGURATION\nDESCRIPTOR.message_types_by_name[\'ExampleParserConfiguration\'] = _EXAMPLEPARSERCONFIGURATION\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nVarLenFeatureProto = _reflection.GeneratedProtocolMessageType(\'VarLenFeatureProto\', (_message.Message,), dict(\n  DESCRIPTOR = _VARLENFEATUREPROTO,\n  __module__ = \'tensorflow.core.example.example_parser_configuration_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.VarLenFeatureProto)\n  ))\n_sym_db.RegisterMessage(VarLenFeatureProto)\n\nFixedLenFeatureProto = _reflection.GeneratedProtocolMessageType(\'FixedLenFeatureProto\', (_message.Message,), dict(\n  DESCRIPTOR = _FIXEDLENFEATUREPROTO,\n  __module__ = \'tensorflow.core.example.example_parser_configuration_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.FixedLenFeatureProto)\n  ))\n_sym_db.RegisterMessage(FixedLenFeatureProto)\n\nFeatureConfiguration = _reflection.GeneratedProtocolMessageType(\'FeatureConfiguration\', (_message.Message,), dict(\n  DESCRIPTOR = _FEATURECONFIGURATION,\n  __module__ = \'tensorflow.core.example.example_parser_configuration_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.FeatureConfiguration)\n  ))\n_sym_db.RegisterMessage(FeatureConfiguration)\n\nExampleParserConfiguration = _reflection.GeneratedProtocolMessageType(\'ExampleParserConfiguration\', (_message.Message,), dict(\n\n  FeatureMapEntry = _reflection.GeneratedProtocolMessageType(\'FeatureMapEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY,\n    __module__ = \'tensorflow.core.example.example_parser_configuration_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.ExampleParserConfiguration.FeatureMapEntry)\n    ))\n  ,\n  DESCRIPTOR = _EXAMPLEPARSERCONFIGURATION,\n  __module__ = \'tensorflow.core.example.example_parser_configuration_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ExampleParserConfiguration)\n  ))\n_sym_db.RegisterMessage(ExampleParserConfiguration)\n_sym_db.RegisterMessage(ExampleParserConfiguration.FeatureMapEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\026org.tensorflow.exampleB ExampleParserConfigurationProtosP\\001\\370\\001\\001\'))\n_EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY.has_options = True\n_EXAMPLEPARSERCONFIGURATION_FEATUREMAPENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/example/example_parser_configuration_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/example/example_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/example/example.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.example import feature_pb2 as tensorflow_dot_core_dot_example_dot_feature__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/example/example.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow/core/example/example.proto\\x12\\ntensorflow\\x1a%tensorflow/core/example/feature.proto\\""1\\n\\x07\\x45xample\\x12&\\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x01(\\x0b\\x32\\x14.tensorflow.Features\\""i\\n\\x0fSequenceExample\\x12%\\n\\x07\\x63ontext\\x18\\x01 \\x01(\\x0b\\x32\\x14.tensorflow.Features\\x12/\\n\\rfeature_lists\\x18\\x02 \\x01(\\x0b\\x32\\x18.tensorflow.FeatureListsB,\\n\\x16org.tensorflow.exampleB\\rExampleProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_example_dot_feature__pb2.DESCRIPTOR,])\n\n\n\n\n_EXAMPLE = _descriptor.Descriptor(\n  name=\'Example\',\n  full_name=\'tensorflow.Example\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'features\', full_name=\'tensorflow.Example.features\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=92,\n  serialized_end=141,\n)\n\n\n_SEQUENCEEXAMPLE = _descriptor.Descriptor(\n  name=\'SequenceExample\',\n  full_name=\'tensorflow.SequenceExample\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'context\', full_name=\'tensorflow.SequenceExample.context\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feature_lists\', full_name=\'tensorflow.SequenceExample.feature_lists\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=143,\n  serialized_end=248,\n)\n\n_EXAMPLE.fields_by_name[\'features\'].message_type = tensorflow_dot_core_dot_example_dot_feature__pb2._FEATURES\n_SEQUENCEEXAMPLE.fields_by_name[\'context\'].message_type = tensorflow_dot_core_dot_example_dot_feature__pb2._FEATURES\n_SEQUENCEEXAMPLE.fields_by_name[\'feature_lists\'].message_type = tensorflow_dot_core_dot_example_dot_feature__pb2._FEATURELISTS\nDESCRIPTOR.message_types_by_name[\'Example\'] = _EXAMPLE\nDESCRIPTOR.message_types_by_name[\'SequenceExample\'] = _SEQUENCEEXAMPLE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nExample = _reflection.GeneratedProtocolMessageType(\'Example\', (_message.Message,), dict(\n  DESCRIPTOR = _EXAMPLE,\n  __module__ = \'tensorflow.core.example.example_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.Example)\n  ))\n_sym_db.RegisterMessage(Example)\n\nSequenceExample = _reflection.GeneratedProtocolMessageType(\'SequenceExample\', (_message.Message,), dict(\n  DESCRIPTOR = _SEQUENCEEXAMPLE,\n  __module__ = \'tensorflow.core.example.example_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.SequenceExample)\n  ))\n_sym_db.RegisterMessage(SequenceExample)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\026org.tensorflow.exampleB\\rExampleProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/example/example_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/example/feature_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/example/feature.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/example/feature.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow/core/example/feature.proto\\x12\\ntensorflow\\""\\x1a\\n\\tBytesList\\x12\\r\\n\\x05value\\x18\\x01 \\x03(\\x0c\\""\\x1e\\n\\tFloatList\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x02\\x42\\x02\\x10\\x01\\""\\x1e\\n\\tInt64List\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\""\\x98\\x01\\n\\x07\\x46\\x65\\x61ture\\x12+\\n\\nbytes_list\\x18\\x01 \\x01(\\x0b\\x32\\x15.tensorflow.BytesListH\\x00\\x12+\\n\\nfloat_list\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.FloatListH\\x00\\x12+\\n\\nint64_list\\x18\\x03 \\x01(\\x0b\\x32\\x15.tensorflow.Int64ListH\\x00\\x42\\x06\\n\\x04kind\\""\\x83\\x01\\n\\x08\\x46\\x65\\x61tures\\x12\\x32\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32!.tensorflow.Features.FeatureEntry\\x1a\\x43\\n\\x0c\\x46\\x65\\x61tureEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\""\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x13.tensorflow.Feature:\\x02\\x38\\x01\\""3\\n\\x0b\\x46\\x65\\x61tureList\\x12$\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32\\x13.tensorflow.Feature\\""\\x9c\\x01\\n\\x0c\\x46\\x65\\x61tureLists\\x12?\\n\\x0c\\x66\\x65\\x61ture_list\\x18\\x01 \\x03(\\x0b\\x32).tensorflow.FeatureLists.FeatureListEntry\\x1aK\\n\\x10\\x46\\x65\\x61tureListEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.FeatureList:\\x02\\x38\\x01\\x42,\\n\\x16org.tensorflow.exampleB\\rFeatureProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_BYTESLIST = _descriptor.Descriptor(\n  name=\'BytesList\',\n  full_name=\'tensorflow.BytesList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.BytesList.value\', index=0,\n      number=1, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=53,\n  serialized_end=79,\n)\n\n\n_FLOATLIST = _descriptor.Descriptor(\n  name=\'FloatList\',\n  full_name=\'tensorflow.FloatList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.FloatList.value\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=81,\n  serialized_end=111,\n)\n\n\n_INT64LIST = _descriptor.Descriptor(\n  name=\'Int64List\',\n  full_name=\'tensorflow.Int64List\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.Int64List.value\', index=0,\n      number=1, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=113,\n  serialized_end=143,\n)\n\n\n_FEATURE = _descriptor.Descriptor(\n  name=\'Feature\',\n  full_name=\'tensorflow.Feature\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'bytes_list\', full_name=\'tensorflow.Feature.bytes_list\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'float_list\', full_name=\'tensorflow.Feature.float_list\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'int64_list\', full_name=\'tensorflow.Feature.int64_list\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'kind\', full_name=\'tensorflow.Feature.kind\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=146,\n  serialized_end=298,\n)\n\n\n_FEATURES_FEATUREENTRY = _descriptor.Descriptor(\n  name=\'FeatureEntry\',\n  full_name=\'tensorflow.Features.FeatureEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.Features.FeatureEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.Features.FeatureEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=365,\n  serialized_end=432,\n)\n\n_FEATURES = _descriptor.Descriptor(\n  name=\'Features\',\n  full_name=\'tensorflow.Features\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'feature\', full_name=\'tensorflow.Features.feature\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FEATURES_FEATUREENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=301,\n  serialized_end=432,\n)\n\n\n_FEATURELIST = _descriptor.Descriptor(\n  name=\'FeatureList\',\n  full_name=\'tensorflow.FeatureList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'feature\', full_name=\'tensorflow.FeatureList.feature\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=434,\n  serialized_end=485,\n)\n\n\n_FEATURELISTS_FEATURELISTENTRY = _descriptor.Descriptor(\n  name=\'FeatureListEntry\',\n  full_name=\'tensorflow.FeatureLists.FeatureListEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.FeatureLists.FeatureListEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.FeatureLists.FeatureListEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=569,\n  serialized_end=644,\n)\n\n_FEATURELISTS = _descriptor.Descriptor(\n  name=\'FeatureLists\',\n  full_name=\'tensorflow.FeatureLists\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'feature_list\', full_name=\'tensorflow.FeatureLists.feature_list\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FEATURELISTS_FEATURELISTENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=488,\n  serialized_end=644,\n)\n\n_FEATURE.fields_by_name[\'bytes_list\'].message_type = _BYTESLIST\n_FEATURE.fields_by_name[\'float_list\'].message_type = _FLOATLIST\n_FEATURE.fields_by_name[\'int64_list\'].message_type = _INT64LIST\n_FEATURE.oneofs_by_name[\'kind\'].fields.append(\n  _FEATURE.fields_by_name[\'bytes_list\'])\n_FEATURE.fields_by_name[\'bytes_list\'].containing_oneof = _FEATURE.oneofs_by_name[\'kind\']\n_FEATURE.oneofs_by_name[\'kind\'].fields.append(\n  _FEATURE.fields_by_name[\'float_list\'])\n_FEATURE.fields_by_name[\'float_list\'].containing_oneof = _FEATURE.oneofs_by_name[\'kind\']\n_FEATURE.oneofs_by_name[\'kind\'].fields.append(\n  _FEATURE.fields_by_name[\'int64_list\'])\n_FEATURE.fields_by_name[\'int64_list\'].containing_oneof = _FEATURE.oneofs_by_name[\'kind\']\n_FEATURES_FEATUREENTRY.fields_by_name[\'value\'].message_type = _FEATURE\n_FEATURES_FEATUREENTRY.containing_type = _FEATURES\n_FEATURES.fields_by_name[\'feature\'].message_type = _FEATURES_FEATUREENTRY\n_FEATURELIST.fields_by_name[\'feature\'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name[\'value\'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name[\'feature_list\'].message_type = _FEATURELISTS_FEATURELISTENTRY\nDESCRIPTOR.message_types_by_name[\'BytesList\'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name[\'FloatList\'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name[\'Int64List\'] = _INT64LIST\nDESCRIPTOR.message_types_by_name[\'Feature\'] = _FEATURE\nDESCRIPTOR.message_types_by_name[\'Features\'] = _FEATURES\nDESCRIPTOR.message_types_by_name[\'FeatureList\'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name[\'FeatureLists\'] = _FEATURELISTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nBytesList = _reflection.GeneratedProtocolMessageType(\'BytesList\', (_message.Message,), dict(\n  DESCRIPTOR = _BYTESLIST,\n  __module__ = \'tensorflow.core.example.feature_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.BytesList)\n  ))\n_sym_db.RegisterMessage(BytesList)\n\nFloatList = _reflection.GeneratedProtocolMessageType(\'FloatList\', (_message.Message,), dict(\n  DESCRIPTOR = _FLOATLIST,\n  __module__ = \'tensorflow.core.example.feature_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.FloatList)\n  ))\n_sym_db.RegisterMessage(FloatList)\n\nInt64List = _reflection.GeneratedProtocolMessageType(\'Int64List\', (_message.Message,), dict(\n  DESCRIPTOR = _INT64LIST,\n  __module__ = \'tensorflow.core.example.feature_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.Int64List)\n  ))\n_sym_db.RegisterMessage(Int64List)\n\nFeature = _reflection.GeneratedProtocolMessageType(\'Feature\', (_message.Message,), dict(\n  DESCRIPTOR = _FEATURE,\n  __module__ = \'tensorflow.core.example.feature_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.Feature)\n  ))\n_sym_db.RegisterMessage(Feature)\n\nFeatures = _reflection.GeneratedProtocolMessageType(\'Features\', (_message.Message,), dict(\n\n  FeatureEntry = _reflection.GeneratedProtocolMessageType(\'FeatureEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FEATURES_FEATUREENTRY,\n    __module__ = \'tensorflow.core.example.feature_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.Features.FeatureEntry)\n    ))\n  ,\n  DESCRIPTOR = _FEATURES,\n  __module__ = \'tensorflow.core.example.feature_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.Features)\n  ))\n_sym_db.RegisterMessage(Features)\n_sym_db.RegisterMessage(Features.FeatureEntry)\n\nFeatureList = _reflection.GeneratedProtocolMessageType(\'FeatureList\', (_message.Message,), dict(\n  DESCRIPTOR = _FEATURELIST,\n  __module__ = \'tensorflow.core.example.feature_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.FeatureList)\n  ))\n_sym_db.RegisterMessage(FeatureList)\n\nFeatureLists = _reflection.GeneratedProtocolMessageType(\'FeatureLists\', (_message.Message,), dict(\n\n  FeatureListEntry = _reflection.GeneratedProtocolMessageType(\'FeatureListEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FEATURELISTS_FEATURELISTENTRY,\n    __module__ = \'tensorflow.core.example.feature_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.FeatureLists.FeatureListEntry)\n    ))\n  ,\n  DESCRIPTOR = _FEATURELISTS,\n  __module__ = \'tensorflow.core.example.feature_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.FeatureLists)\n  ))\n_sym_db.RegisterMessage(FeatureLists)\n_sym_db.RegisterMessage(FeatureLists.FeatureListEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\026org.tensorflow.exampleB\\rFeatureProtosP\\001\\370\\001\\001\'))\n_FLOATLIST.fields_by_name[\'value\'].has_options = True\n_FLOATLIST.fields_by_name[\'value\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_INT64LIST.fields_by_name[\'value\'].has_options = True\n_INT64LIST.fields_by_name[\'value\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_FEATURES_FEATUREENTRY.has_options = True\n_FEATURES_FEATUREENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_FEATURELISTS_FEATURELISTENTRY.has_options = True\n_FEATURELISTS_FEATURELISTENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/example/feature_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/__init__.py,0,b''
libs/pipeline_model/tensorflow/core/framework/allocation_description_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/allocation_description.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/allocation_description.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n6tensorflow/core/framework/allocation_description.proto\\x12\\ntensorflow\\""\\xa3\\x01\\n\\x15\\x41llocationDescription\\x12\\x17\\n\\x0frequested_bytes\\x18\\x01 \\x01(\\x03\\x12\\x17\\n\\x0f\\x61llocated_bytes\\x18\\x02 \\x01(\\x03\\x12\\x16\\n\\x0e\\x61llocator_name\\x18\\x03 \\x01(\\t\\x12\\x15\\n\\rallocation_id\\x18\\x04 \\x01(\\x03\\x12\\x1c\\n\\x14has_single_reference\\x18\\x05 \\x01(\\x08\\x12\\x0b\\n\\x03ptr\\x18\\x06 \\x01(\\x04\\x42<\\n\\x18org.tensorflow.frameworkB\\x1b\\x41llocationDescriptionProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_ALLOCATIONDESCRIPTION = _descriptor.Descriptor(\n  name=\'AllocationDescription\',\n  full_name=\'tensorflow.AllocationDescription\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'requested_bytes\', full_name=\'tensorflow.AllocationDescription.requested_bytes\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocated_bytes\', full_name=\'tensorflow.AllocationDescription.allocated_bytes\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocator_name\', full_name=\'tensorflow.AllocationDescription.allocator_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocation_id\', full_name=\'tensorflow.AllocationDescription.allocation_id\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'has_single_reference\', full_name=\'tensorflow.AllocationDescription.has_single_reference\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ptr\', full_name=\'tensorflow.AllocationDescription.ptr\', index=5,\n      number=6, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=71,\n  serialized_end=234,\n)\n\nDESCRIPTOR.message_types_by_name[\'AllocationDescription\'] = _ALLOCATIONDESCRIPTION\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAllocationDescription = _reflection.GeneratedProtocolMessageType(\'AllocationDescription\', (_message.Message,), dict(\n  DESCRIPTOR = _ALLOCATIONDESCRIPTION,\n  __module__ = \'tensorflow.core.framework.allocation_description_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.AllocationDescription)\n  ))\n_sym_db.RegisterMessage(AllocationDescription)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\033AllocationDescriptionProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/allocation_description_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/api_def_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/api_def.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/api_def.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\\'tensorflow/core/framework/api_def.proto\\x12\\ntensorflow\\x1a*tensorflow/core/framework/attr_value.proto\\""\\x93\\x05\\n\\x06\\x41piDef\\x12\\x15\\n\\rgraph_op_name\\x18\\x01 \\x01(\\t\\x12\\x31\\n\\nvisibility\\x18\\x02 \\x01(\\x0e\\x32\\x1d.tensorflow.ApiDef.Visibility\\x12-\\n\\x08\\x65ndpoint\\x18\\x03 \\x03(\\x0b\\x32\\x1b.tensorflow.ApiDef.Endpoint\\x12&\\n\\x06in_arg\\x18\\x04 \\x03(\\x0b\\x32\\x16.tensorflow.ApiDef.Arg\\x12\\\'\\n\\x07out_arg\\x18\\x05 \\x03(\\x0b\\x32\\x16.tensorflow.ApiDef.Arg\\x12\\x11\\n\\targ_order\\x18\\x0b \\x03(\\t\\x12%\\n\\x04\\x61ttr\\x18\\x06 \\x03(\\x0b\\x32\\x17.tensorflow.ApiDef.Attr\\x12\\x0f\\n\\x07summary\\x18\\x07 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x08 \\x01(\\t\\x12\\x1a\\n\\x12\\x64\\x65scription_prefix\\x18\\t \\x01(\\t\\x12\\x1a\\n\\x12\\x64\\x65scription_suffix\\x18\\n \\x01(\\t\\x1a\\x35\\n\\x08\\x45ndpoint\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x1b\\n\\x13\\x64\\x65precation_version\\x18\\x02 \\x01(\\x05\\x1a;\\n\\x03\\x41rg\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\trename_to\\x18\\x02 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x03 \\x01(\\t\\x1aj\\n\\x04\\x41ttr\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\trename_to\\x18\\x02 \\x01(\\t\\x12,\\n\\rdefault_value\\x18\\x03 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValue\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x04 \\x01(\\t\\""G\\n\\nVisibility\\x12\\x16\\n\\x12\\x44\\x45\\x46\\x41ULT_VISIBILITY\\x10\\x00\\x12\\x0b\\n\\x07VISIBLE\\x10\\x01\\x12\\x08\\n\\x04SKIP\\x10\\x02\\x12\\n\\n\\x06HIDDEN\\x10\\x03\\"")\\n\\x07\\x41piDefs\\x12\\x1e\\n\\x02op\\x18\\x01 \\x03(\\x0b\\x32\\x12.tensorflow.ApiDefB-\\n\\x18org.tensorflow.frameworkB\\x0c\\x41piDefProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_attr__value__pb2.DESCRIPTOR,])\n\n\n\n_APIDEF_VISIBILITY = _descriptor.EnumDescriptor(\n  name=\'Visibility\',\n  full_name=\'tensorflow.ApiDef.Visibility\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT_VISIBILITY\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'VISIBLE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SKIP\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HIDDEN\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=688,\n  serialized_end=759,\n)\n_sym_db.RegisterEnumDescriptor(_APIDEF_VISIBILITY)\n\n\n_APIDEF_ENDPOINT = _descriptor.Descriptor(\n  name=\'Endpoint\',\n  full_name=\'tensorflow.ApiDef.Endpoint\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.ApiDef.Endpoint.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'deprecation_version\', full_name=\'tensorflow.ApiDef.Endpoint.deprecation_version\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=464,\n  serialized_end=517,\n)\n\n_APIDEF_ARG = _descriptor.Descriptor(\n  name=\'Arg\',\n  full_name=\'tensorflow.ApiDef.Arg\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.ApiDef.Arg.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rename_to\', full_name=\'tensorflow.ApiDef.Arg.rename_to\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorflow.ApiDef.Arg.description\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=519,\n  serialized_end=578,\n)\n\n_APIDEF_ATTR = _descriptor.Descriptor(\n  name=\'Attr\',\n  full_name=\'tensorflow.ApiDef.Attr\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.ApiDef.Attr.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rename_to\', full_name=\'tensorflow.ApiDef.Attr.rename_to\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'default_value\', full_name=\'tensorflow.ApiDef.Attr.default_value\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorflow.ApiDef.Attr.description\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=580,\n  serialized_end=686,\n)\n\n_APIDEF = _descriptor.Descriptor(\n  name=\'ApiDef\',\n  full_name=\'tensorflow.ApiDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'graph_op_name\', full_name=\'tensorflow.ApiDef.graph_op_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'visibility\', full_name=\'tensorflow.ApiDef.visibility\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'endpoint\', full_name=\'tensorflow.ApiDef.endpoint\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'in_arg\', full_name=\'tensorflow.ApiDef.in_arg\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'out_arg\', full_name=\'tensorflow.ApiDef.out_arg\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'arg_order\', full_name=\'tensorflow.ApiDef.arg_order\', index=5,\n      number=11, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attr\', full_name=\'tensorflow.ApiDef.attr\', index=6,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'summary\', full_name=\'tensorflow.ApiDef.summary\', index=7,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorflow.ApiDef.description\', index=8,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description_prefix\', full_name=\'tensorflow.ApiDef.description_prefix\', index=9,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description_suffix\', full_name=\'tensorflow.ApiDef.description_suffix\', index=10,\n      number=10, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_APIDEF_ENDPOINT, _APIDEF_ARG, _APIDEF_ATTR, ],\n  enum_types=[\n    _APIDEF_VISIBILITY,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=100,\n  serialized_end=759,\n)\n\n\n_APIDEFS = _descriptor.Descriptor(\n  name=\'ApiDefs\',\n  full_name=\'tensorflow.ApiDefs\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'tensorflow.ApiDefs.op\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=761,\n  serialized_end=802,\n)\n\n_APIDEF_ENDPOINT.containing_type = _APIDEF\n_APIDEF_ARG.containing_type = _APIDEF\n_APIDEF_ATTR.fields_by_name[\'default_value\'].message_type = tensorflow_dot_core_dot_framework_dot_attr__value__pb2._ATTRVALUE\n_APIDEF_ATTR.containing_type = _APIDEF\n_APIDEF.fields_by_name[\'visibility\'].enum_type = _APIDEF_VISIBILITY\n_APIDEF.fields_by_name[\'endpoint\'].message_type = _APIDEF_ENDPOINT\n_APIDEF.fields_by_name[\'in_arg\'].message_type = _APIDEF_ARG\n_APIDEF.fields_by_name[\'out_arg\'].message_type = _APIDEF_ARG\n_APIDEF.fields_by_name[\'attr\'].message_type = _APIDEF_ATTR\n_APIDEF_VISIBILITY.containing_type = _APIDEF\n_APIDEFS.fields_by_name[\'op\'].message_type = _APIDEF\nDESCRIPTOR.message_types_by_name[\'ApiDef\'] = _APIDEF\nDESCRIPTOR.message_types_by_name[\'ApiDefs\'] = _APIDEFS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nApiDef = _reflection.GeneratedProtocolMessageType(\'ApiDef\', (_message.Message,), dict(\n\n  Endpoint = _reflection.GeneratedProtocolMessageType(\'Endpoint\', (_message.Message,), dict(\n    DESCRIPTOR = _APIDEF_ENDPOINT,\n    __module__ = \'tensorflow.core.framework.api_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.ApiDef.Endpoint)\n    ))\n  ,\n\n  Arg = _reflection.GeneratedProtocolMessageType(\'Arg\', (_message.Message,), dict(\n    DESCRIPTOR = _APIDEF_ARG,\n    __module__ = \'tensorflow.core.framework.api_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.ApiDef.Arg)\n    ))\n  ,\n\n  Attr = _reflection.GeneratedProtocolMessageType(\'Attr\', (_message.Message,), dict(\n    DESCRIPTOR = _APIDEF_ATTR,\n    __module__ = \'tensorflow.core.framework.api_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.ApiDef.Attr)\n    ))\n  ,\n  DESCRIPTOR = _APIDEF,\n  __module__ = \'tensorflow.core.framework.api_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ApiDef)\n  ))\n_sym_db.RegisterMessage(ApiDef)\n_sym_db.RegisterMessage(ApiDef.Endpoint)\n_sym_db.RegisterMessage(ApiDef.Arg)\n_sym_db.RegisterMessage(ApiDef.Attr)\n\nApiDefs = _reflection.GeneratedProtocolMessageType(\'ApiDefs\', (_message.Message,), dict(\n  DESCRIPTOR = _APIDEFS,\n  __module__ = \'tensorflow.core.framework.api_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ApiDefs)\n  ))\n_sym_db.RegisterMessage(ApiDefs)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\014ApiDefProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/api_def_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/attr_value_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/attr_value.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/attr_value.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n*tensorflow/core/framework/attr_value.proto\\x12\\ntensorflow\\x1a&tensorflow/core/framework/tensor.proto\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a%tensorflow/core/framework/types.proto\\""\\xa6\\x04\\n\\tAttrValue\\x12\\x0b\\n\\x01s\\x18\\x02 \\x01(\\x0cH\\x00\\x12\\x0b\\n\\x01i\\x18\\x03 \\x01(\\x03H\\x00\\x12\\x0b\\n\\x01\\x66\\x18\\x04 \\x01(\\x02H\\x00\\x12\\x0b\\n\\x01\\x62\\x18\\x05 \\x01(\\x08H\\x00\\x12$\\n\\x04type\\x18\\x06 \\x01(\\x0e\\x32\\x14.tensorflow.DataTypeH\\x00\\x12-\\n\\x05shape\\x18\\x07 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProtoH\\x00\\x12)\\n\\x06tensor\\x18\\x08 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProtoH\\x00\\x12/\\n\\x04list\\x18\\x01 \\x01(\\x0b\\x32\\x1f.tensorflow.AttrValue.ListValueH\\x00\\x12(\\n\\x04\\x66unc\\x18\\n \\x01(\\x0b\\x32\\x18.tensorflow.NameAttrListH\\x00\\x12\\x15\\n\\x0bplaceholder\\x18\\t \\x01(\\tH\\x00\\x1a\\xe9\\x01\\n\\tListValue\\x12\\t\\n\\x01s\\x18\\x02 \\x03(\\x0c\\x12\\r\\n\\x01i\\x18\\x03 \\x03(\\x03\\x42\\x02\\x10\\x01\\x12\\r\\n\\x01\\x66\\x18\\x04 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\r\\n\\x01\\x62\\x18\\x05 \\x03(\\x08\\x42\\x02\\x10\\x01\\x12&\\n\\x04type\\x18\\x06 \\x03(\\x0e\\x32\\x14.tensorflow.DataTypeB\\x02\\x10\\x01\\x12+\\n\\x05shape\\x18\\x07 \\x03(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\x12\\\'\\n\\x06tensor\\x18\\x08 \\x03(\\x0b\\x32\\x17.tensorflow.TensorProto\\x12&\\n\\x04\\x66unc\\x18\\t \\x03(\\x0b\\x32\\x18.tensorflow.NameAttrListB\\x07\\n\\x05value\\""\\x92\\x01\\n\\x0cNameAttrList\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x30\\n\\x04\\x61ttr\\x18\\x02 \\x03(\\x0b\\x32\\"".tensorflow.NameAttrList.AttrEntry\\x1a\\x42\\n\\tAttrEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12$\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValue:\\x02\\x38\\x01\\x42\\x30\\n\\x18org.tensorflow.frameworkB\\x0f\\x41ttrValueProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n\n\n\n\n_ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n  name=\'ListValue\',\n  full_name=\'tensorflow.AttrValue.ListValue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'s\', full_name=\'tensorflow.AttrValue.ListValue.s\', index=0,\n      number=2, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'i\', full_name=\'tensorflow.AttrValue.ListValue.i\', index=1,\n      number=3, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'f\', full_name=\'tensorflow.AttrValue.ListValue.f\', index=2,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'b\', full_name=\'tensorflow.AttrValue.ListValue.b\', index=3,\n      number=5, type=8, cpp_type=7, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorflow.AttrValue.ListValue.type\', index=4,\n      number=6, type=14, cpp_type=8, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.AttrValue.ListValue.shape\', index=5,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.AttrValue.ListValue.tensor\', index=6,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'func\', full_name=\'tensorflow.AttrValue.ListValue.func\', index=7,\n      number=9, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=492,\n  serialized_end=725,\n)\n\n_ATTRVALUE = _descriptor.Descriptor(\n  name=\'AttrValue\',\n  full_name=\'tensorflow.AttrValue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'s\', full_name=\'tensorflow.AttrValue.s\', index=0,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'i\', full_name=\'tensorflow.AttrValue.i\', index=1,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'f\', full_name=\'tensorflow.AttrValue.f\', index=2,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'b\', full_name=\'tensorflow.AttrValue.b\', index=3,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorflow.AttrValue.type\', index=4,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.AttrValue.shape\', index=5,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.AttrValue.tensor\', index=6,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'list\', full_name=\'tensorflow.AttrValue.list\', index=7,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'func\', full_name=\'tensorflow.AttrValue.func\', index=8,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'placeholder\', full_name=\'tensorflow.AttrValue.placeholder\', index=9,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ATTRVALUE_LISTVALUE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'value\', full_name=\'tensorflow.AttrValue.value\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=184,\n  serialized_end=734,\n)\n\n\n_NAMEATTRLIST_ATTRENTRY = _descriptor.Descriptor(\n  name=\'AttrEntry\',\n  full_name=\'tensorflow.NameAttrList.AttrEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.NameAttrList.AttrEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.NameAttrList.AttrEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=817,\n  serialized_end=883,\n)\n\n_NAMEATTRLIST = _descriptor.Descriptor(\n  name=\'NameAttrList\',\n  full_name=\'tensorflow.NameAttrList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.NameAttrList.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attr\', full_name=\'tensorflow.NameAttrList.attr\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_NAMEATTRLIST_ATTRENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=737,\n  serialized_end=883,\n)\n\n_ATTRVALUE_LISTVALUE.fields_by_name[\'type\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_ATTRVALUE_LISTVALUE.fields_by_name[\'shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_ATTRVALUE_LISTVALUE.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_ATTRVALUE_LISTVALUE.fields_by_name[\'func\'].message_type = _NAMEATTRLIST\n_ATTRVALUE_LISTVALUE.containing_type = _ATTRVALUE\n_ATTRVALUE.fields_by_name[\'type\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_ATTRVALUE.fields_by_name[\'shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_ATTRVALUE.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_ATTRVALUE.fields_by_name[\'list\'].message_type = _ATTRVALUE_LISTVALUE\n_ATTRVALUE.fields_by_name[\'func\'].message_type = _NAMEATTRLIST\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'s\'])\n_ATTRVALUE.fields_by_name[\'s\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'i\'])\n_ATTRVALUE.fields_by_name[\'i\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'f\'])\n_ATTRVALUE.fields_by_name[\'f\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'b\'])\n_ATTRVALUE.fields_by_name[\'b\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'type\'])\n_ATTRVALUE.fields_by_name[\'type\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'shape\'])\n_ATTRVALUE.fields_by_name[\'shape\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'tensor\'])\n_ATTRVALUE.fields_by_name[\'tensor\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'list\'])\n_ATTRVALUE.fields_by_name[\'list\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'func\'])\n_ATTRVALUE.fields_by_name[\'func\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'placeholder\'])\n_ATTRVALUE.fields_by_name[\'placeholder\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_NAMEATTRLIST_ATTRENTRY.fields_by_name[\'value\'].message_type = _ATTRVALUE\n_NAMEATTRLIST_ATTRENTRY.containing_type = _NAMEATTRLIST\n_NAMEATTRLIST.fields_by_name[\'attr\'].message_type = _NAMEATTRLIST_ATTRENTRY\nDESCRIPTOR.message_types_by_name[\'AttrValue\'] = _ATTRVALUE\nDESCRIPTOR.message_types_by_name[\'NameAttrList\'] = _NAMEATTRLIST\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAttrValue = _reflection.GeneratedProtocolMessageType(\'AttrValue\', (_message.Message,), dict(\n\n  ListValue = _reflection.GeneratedProtocolMessageType(\'ListValue\', (_message.Message,), dict(\n    DESCRIPTOR = _ATTRVALUE_LISTVALUE,\n    __module__ = \'tensorflow.core.framework.attr_value_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.AttrValue.ListValue)\n    ))\n  ,\n  DESCRIPTOR = _ATTRVALUE,\n  __module__ = \'tensorflow.core.framework.attr_value_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.AttrValue)\n  ))\n_sym_db.RegisterMessage(AttrValue)\n_sym_db.RegisterMessage(AttrValue.ListValue)\n\nNameAttrList = _reflection.GeneratedProtocolMessageType(\'NameAttrList\', (_message.Message,), dict(\n\n  AttrEntry = _reflection.GeneratedProtocolMessageType(\'AttrEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _NAMEATTRLIST_ATTRENTRY,\n    __module__ = \'tensorflow.core.framework.attr_value_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.NameAttrList.AttrEntry)\n    ))\n  ,\n  DESCRIPTOR = _NAMEATTRLIST,\n  __module__ = \'tensorflow.core.framework.attr_value_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.NameAttrList)\n  ))\n_sym_db.RegisterMessage(NameAttrList)\n_sym_db.RegisterMessage(NameAttrList.AttrEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\017AttrValueProtosP\\001\\370\\001\\001\'))\n_ATTRVALUE_LISTVALUE.fields_by_name[\'i\'].has_options = True\n_ATTRVALUE_LISTVALUE.fields_by_name[\'i\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_ATTRVALUE_LISTVALUE.fields_by_name[\'f\'].has_options = True\n_ATTRVALUE_LISTVALUE.fields_by_name[\'f\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_ATTRVALUE_LISTVALUE.fields_by_name[\'b\'].has_options = True\n_ATTRVALUE_LISTVALUE.fields_by_name[\'b\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_ATTRVALUE_LISTVALUE.fields_by_name[\'type\'].has_options = True\n_ATTRVALUE_LISTVALUE.fields_by_name[\'type\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_NAMEATTRLIST_ATTRENTRY.has_options = True\n_NAMEATTRLIST_ATTRENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/attr_value_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/cost_graph_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/cost_graph.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/cost_graph.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n*tensorflow/core/framework/cost_graph.proto\\x12\\ntensorflow\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a%tensorflow/core/framework/types.proto\\""\\xc5\\x05\\n\\x0c\\x43ostGraphDef\\x12+\\n\\x04node\\x18\\x01 \\x03(\\x0b\\x32\\x1d.tensorflow.CostGraphDef.Node\\x1a\\x87\\x05\\n\\x04Node\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x02 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x03 \\x01(\\x05\\x12;\\n\\ninput_info\\x18\\x04 \\x03(\\x0b\\x32\\\'.tensorflow.CostGraphDef.Node.InputInfo\\x12=\\n\\x0boutput_info\\x18\\x05 \\x03(\\x0b\\x32(.tensorflow.CostGraphDef.Node.OutputInfo\\x12\\x1d\\n\\x15temporary_memory_size\\x18\\x06 \\x01(\\x03\\x12\\x1d\\n\\x15host_temp_memory_size\\x18\\n \\x01(\\x03\\x12\\x1f\\n\\x17\\x64\\x65vice_temp_memory_size\\x18\\x0b \\x01(\\x03\\x12#\\n\\x1bhost_persistent_memory_size\\x18\\x0c \\x01(\\x03\\x12%\\n\\x1d\\x64\\x65vice_persistent_memory_size\\x18\\x10 \\x01(\\x03\\x12\\x14\\n\\x0c\\x63ompute_cost\\x18\\t \\x01(\\x03\\x12\\x14\\n\\x0c\\x63ompute_time\\x18\\x0e \\x01(\\x03\\x12\\x13\\n\\x0bmemory_time\\x18\\x0f \\x01(\\x03\\x12\\x10\\n\\x08is_final\\x18\\x07 \\x01(\\x08\\x12\\x15\\n\\rcontrol_input\\x18\\x08 \\x03(\\x05\\x1a;\\n\\tInputInfo\\x12\\x16\\n\\x0epreceding_node\\x18\\x01 \\x01(\\x05\\x12\\x16\\n\\x0epreceding_port\\x18\\x02 \\x01(\\x05\\x1a\\x86\\x01\\n\\nOutputInfo\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x18\\n\\x10\\x61lias_input_port\\x18\\x02 \\x01(\\x03\\x12+\\n\\x05shape\\x18\\x03 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\x12#\\n\\x05\\x64type\\x18\\x04 \\x01(\\x0e\\x32\\x14.tensorflow.DataTypeB0\\n\\x18org.tensorflow.frameworkB\\x0f\\x43ostGraphProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n\n\n\n\n_COSTGRAPHDEF_NODE_INPUTINFO = _descriptor.Descriptor(\n  name=\'InputInfo\',\n  full_name=\'tensorflow.CostGraphDef.Node.InputInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'preceding_node\', full_name=\'tensorflow.CostGraphDef.Node.InputInfo.preceding_node\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'preceding_port\', full_name=\'tensorflow.CostGraphDef.Node.InputInfo.preceding_port\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=657,\n  serialized_end=716,\n)\n\n_COSTGRAPHDEF_NODE_OUTPUTINFO = _descriptor.Descriptor(\n  name=\'OutputInfo\',\n  full_name=\'tensorflow.CostGraphDef.Node.OutputInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'size\', full_name=\'tensorflow.CostGraphDef.Node.OutputInfo.size\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alias_input_port\', full_name=\'tensorflow.CostGraphDef.Node.OutputInfo.alias_input_port\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.CostGraphDef.Node.OutputInfo.shape\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.CostGraphDef.Node.OutputInfo.dtype\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=719,\n  serialized_end=853,\n)\n\n_COSTGRAPHDEF_NODE = _descriptor.Descriptor(\n  name=\'Node\',\n  full_name=\'tensorflow.CostGraphDef.Node\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.CostGraphDef.Node.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device\', full_name=\'tensorflow.CostGraphDef.Node.device\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'tensorflow.CostGraphDef.Node.id\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_info\', full_name=\'tensorflow.CostGraphDef.Node.input_info\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_info\', full_name=\'tensorflow.CostGraphDef.Node.output_info\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'temporary_memory_size\', full_name=\'tensorflow.CostGraphDef.Node.temporary_memory_size\', index=5,\n      number=6, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'host_temp_memory_size\', full_name=\'tensorflow.CostGraphDef.Node.host_temp_memory_size\', index=6,\n      number=10, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_temp_memory_size\', full_name=\'tensorflow.CostGraphDef.Node.device_temp_memory_size\', index=7,\n      number=11, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'host_persistent_memory_size\', full_name=\'tensorflow.CostGraphDef.Node.host_persistent_memory_size\', index=8,\n      number=12, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_persistent_memory_size\', full_name=\'tensorflow.CostGraphDef.Node.device_persistent_memory_size\', index=9,\n      number=16, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'compute_cost\', full_name=\'tensorflow.CostGraphDef.Node.compute_cost\', index=10,\n      number=9, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'compute_time\', full_name=\'tensorflow.CostGraphDef.Node.compute_time\', index=11,\n      number=14, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_time\', full_name=\'tensorflow.CostGraphDef.Node.memory_time\', index=12,\n      number=15, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_final\', full_name=\'tensorflow.CostGraphDef.Node.is_final\', index=13,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'control_input\', full_name=\'tensorflow.CostGraphDef.Node.control_input\', index=14,\n      number=8, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_COSTGRAPHDEF_NODE_INPUTINFO, _COSTGRAPHDEF_NODE_OUTPUTINFO, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=206,\n  serialized_end=853,\n)\n\n_COSTGRAPHDEF = _descriptor.Descriptor(\n  name=\'CostGraphDef\',\n  full_name=\'tensorflow.CostGraphDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node\', full_name=\'tensorflow.CostGraphDef.node\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_COSTGRAPHDEF_NODE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=144,\n  serialized_end=853,\n)\n\n_COSTGRAPHDEF_NODE_INPUTINFO.containing_type = _COSTGRAPHDEF_NODE\n_COSTGRAPHDEF_NODE_OUTPUTINFO.fields_by_name[\'shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_COSTGRAPHDEF_NODE_OUTPUTINFO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_COSTGRAPHDEF_NODE_OUTPUTINFO.containing_type = _COSTGRAPHDEF_NODE\n_COSTGRAPHDEF_NODE.fields_by_name[\'input_info\'].message_type = _COSTGRAPHDEF_NODE_INPUTINFO\n_COSTGRAPHDEF_NODE.fields_by_name[\'output_info\'].message_type = _COSTGRAPHDEF_NODE_OUTPUTINFO\n_COSTGRAPHDEF_NODE.containing_type = _COSTGRAPHDEF\n_COSTGRAPHDEF.fields_by_name[\'node\'].message_type = _COSTGRAPHDEF_NODE\nDESCRIPTOR.message_types_by_name[\'CostGraphDef\'] = _COSTGRAPHDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nCostGraphDef = _reflection.GeneratedProtocolMessageType(\'CostGraphDef\', (_message.Message,), dict(\n\n  Node = _reflection.GeneratedProtocolMessageType(\'Node\', (_message.Message,), dict(\n\n    InputInfo = _reflection.GeneratedProtocolMessageType(\'InputInfo\', (_message.Message,), dict(\n      DESCRIPTOR = _COSTGRAPHDEF_NODE_INPUTINFO,\n      __module__ = \'tensorflow.core.framework.cost_graph_pb2\'\n      # @@protoc_insertion_point(class_scope:tensorflow.CostGraphDef.Node.InputInfo)\n      ))\n    ,\n\n    OutputInfo = _reflection.GeneratedProtocolMessageType(\'OutputInfo\', (_message.Message,), dict(\n      DESCRIPTOR = _COSTGRAPHDEF_NODE_OUTPUTINFO,\n      __module__ = \'tensorflow.core.framework.cost_graph_pb2\'\n      # @@protoc_insertion_point(class_scope:tensorflow.CostGraphDef.Node.OutputInfo)\n      ))\n    ,\n    DESCRIPTOR = _COSTGRAPHDEF_NODE,\n    __module__ = \'tensorflow.core.framework.cost_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.CostGraphDef.Node)\n    ))\n  ,\n  DESCRIPTOR = _COSTGRAPHDEF,\n  __module__ = \'tensorflow.core.framework.cost_graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CostGraphDef)\n  ))\n_sym_db.RegisterMessage(CostGraphDef)\n_sym_db.RegisterMessage(CostGraphDef.Node)\n_sym_db.RegisterMessage(CostGraphDef.Node.InputInfo)\n_sym_db.RegisterMessage(CostGraphDef.Node.OutputInfo)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\017CostGraphProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/cost_graph_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/device_attributes_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/device_attributes.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/device_attributes.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n1tensorflow/core/framework/device_attributes.proto\\x12\\ntensorflow\\"" \\n\\x0e\\x44\\x65viceLocality\\x12\\x0e\\n\\x06\\x62us_id\\x18\\x01 \\x01(\\x05\\""\\xac\\x01\\n\\x10\\x44\\x65viceAttributes\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65vice_type\\x18\\x02 \\x01(\\t\\x12\\x14\\n\\x0cmemory_limit\\x18\\x04 \\x01(\\x03\\x12,\\n\\x08locality\\x18\\x05 \\x01(\\x0b\\x32\\x1a.tensorflow.DeviceLocality\\x12\\x13\\n\\x0bincarnation\\x18\\x06 \\x01(\\x06\\x12\\x1c\\n\\x14physical_device_desc\\x18\\x07 \\x01(\\tB7\\n\\x18org.tensorflow.frameworkB\\x16\\x44\\x65viceAttributesProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_DEVICELOCALITY = _descriptor.Descriptor(\n  name=\'DeviceLocality\',\n  full_name=\'tensorflow.DeviceLocality\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'bus_id\', full_name=\'tensorflow.DeviceLocality.bus_id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=65,\n  serialized_end=97,\n)\n\n\n_DEVICEATTRIBUTES = _descriptor.Descriptor(\n  name=\'DeviceAttributes\',\n  full_name=\'tensorflow.DeviceAttributes\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.DeviceAttributes.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_type\', full_name=\'tensorflow.DeviceAttributes.device_type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_limit\', full_name=\'tensorflow.DeviceAttributes.memory_limit\', index=2,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'locality\', full_name=\'tensorflow.DeviceAttributes.locality\', index=3,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'incarnation\', full_name=\'tensorflow.DeviceAttributes.incarnation\', index=4,\n      number=6, type=6, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'physical_device_desc\', full_name=\'tensorflow.DeviceAttributes.physical_device_desc\', index=5,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=100,\n  serialized_end=272,\n)\n\n_DEVICEATTRIBUTES.fields_by_name[\'locality\'].message_type = _DEVICELOCALITY\nDESCRIPTOR.message_types_by_name[\'DeviceLocality\'] = _DEVICELOCALITY\nDESCRIPTOR.message_types_by_name[\'DeviceAttributes\'] = _DEVICEATTRIBUTES\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nDeviceLocality = _reflection.GeneratedProtocolMessageType(\'DeviceLocality\', (_message.Message,), dict(\n  DESCRIPTOR = _DEVICELOCALITY,\n  __module__ = \'tensorflow.core.framework.device_attributes_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeviceLocality)\n  ))\n_sym_db.RegisterMessage(DeviceLocality)\n\nDeviceAttributes = _reflection.GeneratedProtocolMessageType(\'DeviceAttributes\', (_message.Message,), dict(\n  DESCRIPTOR = _DEVICEATTRIBUTES,\n  __module__ = \'tensorflow.core.framework.device_attributes_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeviceAttributes)\n  ))\n_sym_db.RegisterMessage(DeviceAttributes)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\026DeviceAttributesProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/device_attributes_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/function_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/function.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\nfrom tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\nfrom tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/function.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(tensorflow/core/framework/function.proto\\x12\\ntensorflow\\x1a*tensorflow/core/framework/attr_value.proto\\x1a(tensorflow/core/framework/node_def.proto\\x1a&tensorflow/core/framework/op_def.proto\\""j\\n\\x12\\x46unctionDefLibrary\\x12)\\n\\x08\\x66unction\\x18\\x01 \\x03(\\x0b\\x32\\x17.tensorflow.FunctionDef\\x12)\\n\\x08gradient\\x18\\x02 \\x03(\\x0b\\x32\\x17.tensorflow.GradientDef\\""\\xaa\\x02\\n\\x0b\\x46unctionDef\\x12$\\n\\tsignature\\x18\\x01 \\x01(\\x0b\\x32\\x11.tensorflow.OpDef\\x12/\\n\\x04\\x61ttr\\x18\\x05 \\x03(\\x0b\\x32!.tensorflow.FunctionDef.AttrEntry\\x12%\\n\\x08node_def\\x18\\x03 \\x03(\\x0b\\x32\\x13.tensorflow.NodeDef\\x12-\\n\\x03ret\\x18\\x04 \\x03(\\x0b\\x32 .tensorflow.FunctionDef.RetEntry\\x1a\\x42\\n\\tAttrEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12$\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValue:\\x02\\x38\\x01\\x1a*\\n\\x08RetEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\"";\\n\\x0bGradientDef\\x12\\x15\\n\\rfunction_name\\x18\\x01 \\x01(\\t\\x12\\x15\\n\\rgradient_func\\x18\\x02 \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x0e\\x46unctionProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_attr__value__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_node__def__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_op__def__pb2.DESCRIPTOR,])\n\n\n\n\n_FUNCTIONDEFLIBRARY = _descriptor.Descriptor(\n  name=\'FunctionDefLibrary\',\n  full_name=\'tensorflow.FunctionDefLibrary\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'function\', full_name=\'tensorflow.FunctionDefLibrary.function\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gradient\', full_name=\'tensorflow.FunctionDefLibrary.gradient\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=182,\n  serialized_end=288,\n)\n\n\n_FUNCTIONDEF_ATTRENTRY = _descriptor.Descriptor(\n  name=\'AttrEntry\',\n  full_name=\'tensorflow.FunctionDef.AttrEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.FunctionDef.AttrEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.FunctionDef.AttrEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=479,\n  serialized_end=545,\n)\n\n_FUNCTIONDEF_RETENTRY = _descriptor.Descriptor(\n  name=\'RetEntry\',\n  full_name=\'tensorflow.FunctionDef.RetEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.FunctionDef.RetEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.FunctionDef.RetEntry.value\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=547,\n  serialized_end=589,\n)\n\n_FUNCTIONDEF = _descriptor.Descriptor(\n  name=\'FunctionDef\',\n  full_name=\'tensorflow.FunctionDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'signature\', full_name=\'tensorflow.FunctionDef.signature\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attr\', full_name=\'tensorflow.FunctionDef.attr\', index=1,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'node_def\', full_name=\'tensorflow.FunctionDef.node_def\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ret\', full_name=\'tensorflow.FunctionDef.ret\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FUNCTIONDEF_ATTRENTRY, _FUNCTIONDEF_RETENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=291,\n  serialized_end=589,\n)\n\n\n_GRADIENTDEF = _descriptor.Descriptor(\n  name=\'GradientDef\',\n  full_name=\'tensorflow.GradientDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'function_name\', full_name=\'tensorflow.GradientDef.function_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gradient_func\', full_name=\'tensorflow.GradientDef.gradient_func\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=591,\n  serialized_end=650,\n)\n\n_FUNCTIONDEFLIBRARY.fields_by_name[\'function\'].message_type = _FUNCTIONDEF\n_FUNCTIONDEFLIBRARY.fields_by_name[\'gradient\'].message_type = _GRADIENTDEF\n_FUNCTIONDEF_ATTRENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_attr__value__pb2._ATTRVALUE\n_FUNCTIONDEF_ATTRENTRY.containing_type = _FUNCTIONDEF\n_FUNCTIONDEF_RETENTRY.containing_type = _FUNCTIONDEF\n_FUNCTIONDEF.fields_by_name[\'signature\'].message_type = tensorflow_dot_core_dot_framework_dot_op__def__pb2._OPDEF\n_FUNCTIONDEF.fields_by_name[\'attr\'].message_type = _FUNCTIONDEF_ATTRENTRY\n_FUNCTIONDEF.fields_by_name[\'node_def\'].message_type = tensorflow_dot_core_dot_framework_dot_node__def__pb2._NODEDEF\n_FUNCTIONDEF.fields_by_name[\'ret\'].message_type = _FUNCTIONDEF_RETENTRY\nDESCRIPTOR.message_types_by_name[\'FunctionDefLibrary\'] = _FUNCTIONDEFLIBRARY\nDESCRIPTOR.message_types_by_name[\'FunctionDef\'] = _FUNCTIONDEF\nDESCRIPTOR.message_types_by_name[\'GradientDef\'] = _GRADIENTDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nFunctionDefLibrary = _reflection.GeneratedProtocolMessageType(\'FunctionDefLibrary\', (_message.Message,), dict(\n  DESCRIPTOR = _FUNCTIONDEFLIBRARY,\n  __module__ = \'tensorflow.core.framework.function_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.FunctionDefLibrary)\n  ))\n_sym_db.RegisterMessage(FunctionDefLibrary)\n\nFunctionDef = _reflection.GeneratedProtocolMessageType(\'FunctionDef\', (_message.Message,), dict(\n\n  AttrEntry = _reflection.GeneratedProtocolMessageType(\'AttrEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FUNCTIONDEF_ATTRENTRY,\n    __module__ = \'tensorflow.core.framework.function_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.FunctionDef.AttrEntry)\n    ))\n  ,\n\n  RetEntry = _reflection.GeneratedProtocolMessageType(\'RetEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FUNCTIONDEF_RETENTRY,\n    __module__ = \'tensorflow.core.framework.function_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.FunctionDef.RetEntry)\n    ))\n  ,\n  DESCRIPTOR = _FUNCTIONDEF,\n  __module__ = \'tensorflow.core.framework.function_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.FunctionDef)\n  ))\n_sym_db.RegisterMessage(FunctionDef)\n_sym_db.RegisterMessage(FunctionDef.AttrEntry)\n_sym_db.RegisterMessage(FunctionDef.RetEntry)\n\nGradientDef = _reflection.GeneratedProtocolMessageType(\'GradientDef\', (_message.Message,), dict(\n  DESCRIPTOR = _GRADIENTDEF,\n  __module__ = \'tensorflow.core.framework.function_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.GradientDef)\n  ))\n_sym_db.RegisterMessage(GradientDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\016FunctionProtosP\\001\\370\\001\\001\'))\n_FUNCTIONDEF_ATTRENTRY.has_options = True\n_FUNCTIONDEF_ATTRENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_FUNCTIONDEF_RETENTRY.has_options = True\n_FUNCTIONDEF_RETENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/function_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/graph_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/graph.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\nfrom tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2\nfrom tensorflow.core.framework import versions_pb2 as tensorflow_dot_core_dot_framework_dot_versions__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/graph.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow/core/framework/graph.proto\\x12\\ntensorflow\\x1a(tensorflow/core/framework/node_def.proto\\x1a(tensorflow/core/framework/function.proto\\x1a(tensorflow/core/framework/versions.proto\\""\\x9d\\x01\\n\\x08GraphDef\\x12!\\n\\x04node\\x18\\x01 \\x03(\\x0b\\x32\\x13.tensorflow.NodeDef\\x12(\\n\\x08versions\\x18\\x04 \\x01(\\x0b\\x32\\x16.tensorflow.VersionDef\\x12\\x13\\n\\x07version\\x18\\x03 \\x01(\\x05\\x42\\x02\\x18\\x01\\x12/\\n\\x07library\\x18\\x02 \\x01(\\x0b\\x32\\x1e.tensorflow.FunctionDefLibraryB,\\n\\x18org.tensorflow.frameworkB\\x0bGraphProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_node__def__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_function__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_versions__pb2.DESCRIPTOR,])\n\n\n\n\n_GRAPHDEF = _descriptor.Descriptor(\n  name=\'GraphDef\',\n  full_name=\'tensorflow.GraphDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node\', full_name=\'tensorflow.GraphDef.node\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'versions\', full_name=\'tensorflow.GraphDef.versions\', index=1,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.GraphDef.version\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'library\', full_name=\'tensorflow.GraphDef.library\', index=3,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=180,\n  serialized_end=337,\n)\n\n_GRAPHDEF.fields_by_name[\'node\'].message_type = tensorflow_dot_core_dot_framework_dot_node__def__pb2._NODEDEF\n_GRAPHDEF.fields_by_name[\'versions\'].message_type = tensorflow_dot_core_dot_framework_dot_versions__pb2._VERSIONDEF\n_GRAPHDEF.fields_by_name[\'library\'].message_type = tensorflow_dot_core_dot_framework_dot_function__pb2._FUNCTIONDEFLIBRARY\nDESCRIPTOR.message_types_by_name[\'GraphDef\'] = _GRAPHDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nGraphDef = _reflection.GeneratedProtocolMessageType(\'GraphDef\', (_message.Message,), dict(\n  DESCRIPTOR = _GRAPHDEF,\n  __module__ = \'tensorflow.core.framework.graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.GraphDef)\n  ))\n_sym_db.RegisterMessage(GraphDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\013GraphProtosP\\001\\370\\001\\001\'))\n_GRAPHDEF.fields_by_name[\'version\'].has_options = True\n_GRAPHDEF.fields_by_name[\'version\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/graph_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/graph_transfer_info_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/graph_transfer_info.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/graph_transfer_info.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n3tensorflow/core/framework/graph_transfer_info.proto\\x12\\ntensorflow\\x1a%tensorflow/core/framework/types.proto\\""\\xab\\t\\n\\x11GraphTransferInfo\\x12\\x39\\n\\tnode_info\\x18\\x01 \\x03(\\x0b\\x32&.tensorflow.GraphTransferInfo.NodeInfo\\x12\\x44\\n\\x0f\\x63onst_node_info\\x18\\x02 \\x03(\\x0b\\x32+.tensorflow.GraphTransferInfo.ConstNodeInfo\\x12\\x44\\n\\x0fnode_input_info\\x18\\x03 \\x03(\\x0b\\x32+.tensorflow.GraphTransferInfo.NodeInputInfo\\x12\\x46\\n\\x10node_output_info\\x18\\x04 \\x03(\\x0b\\x32,.tensorflow.GraphTransferInfo.NodeOutputInfo\\x12O\\n\\x15graph_input_node_info\\x18\\x05 \\x03(\\x0b\\x32\\x30.tensorflow.GraphTransferInfo.GraphInputNodeInfo\\x12Q\\n\\x16graph_output_node_info\\x18\\x06 \\x03(\\x0b\\x32\\x31.tensorflow.GraphTransferInfo.GraphOutputNodeInfo\\x12>\\n\\x0b\\x64\\x65stination\\x18\\x07 \\x01(\\x0e\\x32).tensorflow.GraphTransferInfo.Destination\\x1a\\x31\\n\\tNodeInput\\x12\\x0f\\n\\x07node_id\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0boutput_port\\x18\\x02 \\x01(\\x05\\x1a\\x8e\\x01\\n\\x08NodeInfo\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07node_id\\x18\\x02 \\x01(\\x05\\x12\\x11\\n\\ttype_name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\tsoc_op_id\\x18\\x04 \\x01(\\x05\\x12\\x12\\n\\npadding_id\\x18\\x05 \\x01(\\x05\\x12\\x13\\n\\x0binput_count\\x18\\x06 \\x01(\\x05\\x12\\x14\\n\\x0coutput_count\\x18\\x07 \\x01(\\x05\\x1ap\\n\\rConstNodeInfo\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07node_id\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05shape\\x18\\x03 \\x03(\\x03\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x04 \\x01(\\x0c\\x12#\\n\\x05\\x64type\\x18\\x05 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x1a]\\n\\rNodeInputInfo\\x12\\x0f\\n\\x07node_id\\x18\\x01 \\x01(\\x05\\x12;\\n\\nnode_input\\x18\\x02 \\x03(\\x0b\\x32\\\'.tensorflow.GraphTransferInfo.NodeInput\\x1a\\x38\\n\\x0eNodeOutputInfo\\x12\\x0f\\n\\x07node_id\\x18\\x01 \\x01(\\x05\\x12\\x15\\n\\rmax_byte_size\\x18\\x02 \\x03(\\x05\\x1aV\\n\\x12GraphInputNodeInfo\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05shape\\x18\\x02 \\x03(\\x03\\x12#\\n\\x05\\x64type\\x18\\x03 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x1aW\\n\\x13GraphOutputNodeInfo\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05shape\\x18\\x02 \\x03(\\x03\\x12#\\n\\x05\\x64type\\x18\\x03 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\""#\\n\\x0b\\x44\\x65stination\\x12\\x07\\n\\x03NOP\\x10\\x00\\x12\\x0b\\n\\x07HEXAGON\\x10\\x01\\x42\\x37\\n\\x18org.tensorflow.frameworkB\\x16GraphTransferInfoProtoP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n\n\n\n_GRAPHTRANSFERINFO_DESTINATION = _descriptor.EnumDescriptor(\n  name=\'Destination\',\n  full_name=\'tensorflow.GraphTransferInfo.Destination\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'NOP\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HEXAGON\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1267,\n  serialized_end=1302,\n)\n_sym_db.RegisterEnumDescriptor(_GRAPHTRANSFERINFO_DESTINATION)\n\n\n_GRAPHTRANSFERINFO_NODEINPUT = _descriptor.Descriptor(\n  name=\'NodeInput\',\n  full_name=\'tensorflow.GraphTransferInfo.NodeInput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_id\', full_name=\'tensorflow.GraphTransferInfo.NodeInput.node_id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_port\', full_name=\'tensorflow.GraphTransferInfo.NodeInput.output_port\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=627,\n  serialized_end=676,\n)\n\n_GRAPHTRANSFERINFO_NODEINFO = _descriptor.Descriptor(\n  name=\'NodeInfo\',\n  full_name=\'tensorflow.GraphTransferInfo.NodeInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.GraphTransferInfo.NodeInfo.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'node_id\', full_name=\'tensorflow.GraphTransferInfo.NodeInfo.node_id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type_name\', full_name=\'tensorflow.GraphTransferInfo.NodeInfo.type_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'soc_op_id\', full_name=\'tensorflow.GraphTransferInfo.NodeInfo.soc_op_id\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'padding_id\', full_name=\'tensorflow.GraphTransferInfo.NodeInfo.padding_id\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_count\', full_name=\'tensorflow.GraphTransferInfo.NodeInfo.input_count\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_count\', full_name=\'tensorflow.GraphTransferInfo.NodeInfo.output_count\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=679,\n  serialized_end=821,\n)\n\n_GRAPHTRANSFERINFO_CONSTNODEINFO = _descriptor.Descriptor(\n  name=\'ConstNodeInfo\',\n  full_name=\'tensorflow.GraphTransferInfo.ConstNodeInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.GraphTransferInfo.ConstNodeInfo.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'node_id\', full_name=\'tensorflow.GraphTransferInfo.ConstNodeInfo.node_id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.GraphTransferInfo.ConstNodeInfo.shape\', index=2,\n      number=3, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'tensorflow.GraphTransferInfo.ConstNodeInfo.data\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.GraphTransferInfo.ConstNodeInfo.dtype\', index=4,\n      number=5, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=823,\n  serialized_end=935,\n)\n\n_GRAPHTRANSFERINFO_NODEINPUTINFO = _descriptor.Descriptor(\n  name=\'NodeInputInfo\',\n  full_name=\'tensorflow.GraphTransferInfo.NodeInputInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_id\', full_name=\'tensorflow.GraphTransferInfo.NodeInputInfo.node_id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'node_input\', full_name=\'tensorflow.GraphTransferInfo.NodeInputInfo.node_input\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=937,\n  serialized_end=1030,\n)\n\n_GRAPHTRANSFERINFO_NODEOUTPUTINFO = _descriptor.Descriptor(\n  name=\'NodeOutputInfo\',\n  full_name=\'tensorflow.GraphTransferInfo.NodeOutputInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_id\', full_name=\'tensorflow.GraphTransferInfo.NodeOutputInfo.node_id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_byte_size\', full_name=\'tensorflow.GraphTransferInfo.NodeOutputInfo.max_byte_size\', index=1,\n      number=2, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1032,\n  serialized_end=1088,\n)\n\n_GRAPHTRANSFERINFO_GRAPHINPUTNODEINFO = _descriptor.Descriptor(\n  name=\'GraphInputNodeInfo\',\n  full_name=\'tensorflow.GraphTransferInfo.GraphInputNodeInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.GraphTransferInfo.GraphInputNodeInfo.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.GraphTransferInfo.GraphInputNodeInfo.shape\', index=1,\n      number=2, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.GraphTransferInfo.GraphInputNodeInfo.dtype\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1090,\n  serialized_end=1176,\n)\n\n_GRAPHTRANSFERINFO_GRAPHOUTPUTNODEINFO = _descriptor.Descriptor(\n  name=\'GraphOutputNodeInfo\',\n  full_name=\'tensorflow.GraphTransferInfo.GraphOutputNodeInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.GraphTransferInfo.GraphOutputNodeInfo.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.GraphTransferInfo.GraphOutputNodeInfo.shape\', index=1,\n      number=2, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.GraphTransferInfo.GraphOutputNodeInfo.dtype\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1178,\n  serialized_end=1265,\n)\n\n_GRAPHTRANSFERINFO = _descriptor.Descriptor(\n  name=\'GraphTransferInfo\',\n  full_name=\'tensorflow.GraphTransferInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_info\', full_name=\'tensorflow.GraphTransferInfo.node_info\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'const_node_info\', full_name=\'tensorflow.GraphTransferInfo.const_node_info\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'node_input_info\', full_name=\'tensorflow.GraphTransferInfo.node_input_info\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'node_output_info\', full_name=\'tensorflow.GraphTransferInfo.node_output_info\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_input_node_info\', full_name=\'tensorflow.GraphTransferInfo.graph_input_node_info\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_output_node_info\', full_name=\'tensorflow.GraphTransferInfo.graph_output_node_info\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'destination\', full_name=\'tensorflow.GraphTransferInfo.destination\', index=6,\n      number=7, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_GRAPHTRANSFERINFO_NODEINPUT, _GRAPHTRANSFERINFO_NODEINFO, _GRAPHTRANSFERINFO_CONSTNODEINFO, _GRAPHTRANSFERINFO_NODEINPUTINFO, _GRAPHTRANSFERINFO_NODEOUTPUTINFO, _GRAPHTRANSFERINFO_GRAPHINPUTNODEINFO, _GRAPHTRANSFERINFO_GRAPHOUTPUTNODEINFO, ],\n  enum_types=[\n    _GRAPHTRANSFERINFO_DESTINATION,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=107,\n  serialized_end=1302,\n)\n\n_GRAPHTRANSFERINFO_NODEINPUT.containing_type = _GRAPHTRANSFERINFO\n_GRAPHTRANSFERINFO_NODEINFO.containing_type = _GRAPHTRANSFERINFO\n_GRAPHTRANSFERINFO_CONSTNODEINFO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_GRAPHTRANSFERINFO_CONSTNODEINFO.containing_type = _GRAPHTRANSFERINFO\n_GRAPHTRANSFERINFO_NODEINPUTINFO.fields_by_name[\'node_input\'].message_type = _GRAPHTRANSFERINFO_NODEINPUT\n_GRAPHTRANSFERINFO_NODEINPUTINFO.containing_type = _GRAPHTRANSFERINFO\n_GRAPHTRANSFERINFO_NODEOUTPUTINFO.containing_type = _GRAPHTRANSFERINFO\n_GRAPHTRANSFERINFO_GRAPHINPUTNODEINFO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_GRAPHTRANSFERINFO_GRAPHINPUTNODEINFO.containing_type = _GRAPHTRANSFERINFO\n_GRAPHTRANSFERINFO_GRAPHOUTPUTNODEINFO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_GRAPHTRANSFERINFO_GRAPHOUTPUTNODEINFO.containing_type = _GRAPHTRANSFERINFO\n_GRAPHTRANSFERINFO.fields_by_name[\'node_info\'].message_type = _GRAPHTRANSFERINFO_NODEINFO\n_GRAPHTRANSFERINFO.fields_by_name[\'const_node_info\'].message_type = _GRAPHTRANSFERINFO_CONSTNODEINFO\n_GRAPHTRANSFERINFO.fields_by_name[\'node_input_info\'].message_type = _GRAPHTRANSFERINFO_NODEINPUTINFO\n_GRAPHTRANSFERINFO.fields_by_name[\'node_output_info\'].message_type = _GRAPHTRANSFERINFO_NODEOUTPUTINFO\n_GRAPHTRANSFERINFO.fields_by_name[\'graph_input_node_info\'].message_type = _GRAPHTRANSFERINFO_GRAPHINPUTNODEINFO\n_GRAPHTRANSFERINFO.fields_by_name[\'graph_output_node_info\'].message_type = _GRAPHTRANSFERINFO_GRAPHOUTPUTNODEINFO\n_GRAPHTRANSFERINFO.fields_by_name[\'destination\'].enum_type = _GRAPHTRANSFERINFO_DESTINATION\n_GRAPHTRANSFERINFO_DESTINATION.containing_type = _GRAPHTRANSFERINFO\nDESCRIPTOR.message_types_by_name[\'GraphTransferInfo\'] = _GRAPHTRANSFERINFO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nGraphTransferInfo = _reflection.GeneratedProtocolMessageType(\'GraphTransferInfo\', (_message.Message,), dict(\n\n  NodeInput = _reflection.GeneratedProtocolMessageType(\'NodeInput\', (_message.Message,), dict(\n    DESCRIPTOR = _GRAPHTRANSFERINFO_NODEINPUT,\n    __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo.NodeInput)\n    ))\n  ,\n\n  NodeInfo = _reflection.GeneratedProtocolMessageType(\'NodeInfo\', (_message.Message,), dict(\n    DESCRIPTOR = _GRAPHTRANSFERINFO_NODEINFO,\n    __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo.NodeInfo)\n    ))\n  ,\n\n  ConstNodeInfo = _reflection.GeneratedProtocolMessageType(\'ConstNodeInfo\', (_message.Message,), dict(\n    DESCRIPTOR = _GRAPHTRANSFERINFO_CONSTNODEINFO,\n    __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo.ConstNodeInfo)\n    ))\n  ,\n\n  NodeInputInfo = _reflection.GeneratedProtocolMessageType(\'NodeInputInfo\', (_message.Message,), dict(\n    DESCRIPTOR = _GRAPHTRANSFERINFO_NODEINPUTINFO,\n    __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo.NodeInputInfo)\n    ))\n  ,\n\n  NodeOutputInfo = _reflection.GeneratedProtocolMessageType(\'NodeOutputInfo\', (_message.Message,), dict(\n    DESCRIPTOR = _GRAPHTRANSFERINFO_NODEOUTPUTINFO,\n    __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo.NodeOutputInfo)\n    ))\n  ,\n\n  GraphInputNodeInfo = _reflection.GeneratedProtocolMessageType(\'GraphInputNodeInfo\', (_message.Message,), dict(\n    DESCRIPTOR = _GRAPHTRANSFERINFO_GRAPHINPUTNODEINFO,\n    __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo.GraphInputNodeInfo)\n    ))\n  ,\n\n  GraphOutputNodeInfo = _reflection.GeneratedProtocolMessageType(\'GraphOutputNodeInfo\', (_message.Message,), dict(\n    DESCRIPTOR = _GRAPHTRANSFERINFO_GRAPHOUTPUTNODEINFO,\n    __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo.GraphOutputNodeInfo)\n    ))\n  ,\n  DESCRIPTOR = _GRAPHTRANSFERINFO,\n  __module__ = \'tensorflow.core.framework.graph_transfer_info_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.GraphTransferInfo)\n  ))\n_sym_db.RegisterMessage(GraphTransferInfo)\n_sym_db.RegisterMessage(GraphTransferInfo.NodeInput)\n_sym_db.RegisterMessage(GraphTransferInfo.NodeInfo)\n_sym_db.RegisterMessage(GraphTransferInfo.ConstNodeInfo)\n_sym_db.RegisterMessage(GraphTransferInfo.NodeInputInfo)\n_sym_db.RegisterMessage(GraphTransferInfo.NodeOutputInfo)\n_sym_db.RegisterMessage(GraphTransferInfo.GraphInputNodeInfo)\n_sym_db.RegisterMessage(GraphTransferInfo.GraphOutputNodeInfo)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\026GraphTransferInfoProtoP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/graph_transfer_info_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/iterator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/iterator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/iterator.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(tensorflow/core/framework/iterator.proto\\x12\\ntensorflow\\""6\\n\\x15IteratorStateMetadata\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04keys\\x18\\x02 \\x03(\\tB*\\n\\x13org.tensorflow.utilB\\x0eIteratorProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_ITERATORSTATEMETADATA = _descriptor.Descriptor(\n  name=\'IteratorStateMetadata\',\n  full_name=\'tensorflow.IteratorStateMetadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.IteratorStateMetadata.version\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keys\', full_name=\'tensorflow.IteratorStateMetadata.keys\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=56,\n  serialized_end=110,\n)\n\nDESCRIPTOR.message_types_by_name[\'IteratorStateMetadata\'] = _ITERATORSTATEMETADATA\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nIteratorStateMetadata = _reflection.GeneratedProtocolMessageType(\'IteratorStateMetadata\', (_message.Message,), dict(\n  DESCRIPTOR = _ITERATORSTATEMETADATA,\n  __module__ = \'tensorflow.core.framework.iterator_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.IteratorStateMetadata)\n  ))\n_sym_db.RegisterMessage(IteratorStateMetadata)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\023org.tensorflow.utilB\\016IteratorProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/iterator_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/kernel_def_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/kernel_def.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/kernel_def.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n*tensorflow/core/framework/kernel_def.proto\\x12\\ntensorflow\\x1a*tensorflow/core/framework/attr_value.proto\\""\\xdd\\x01\\n\\tKernelDef\\x12\\n\\n\\x02op\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65vice_type\\x18\\x02 \\x01(\\t\\x12\\x38\\n\\nconstraint\\x18\\x03 \\x03(\\x0b\\x32$.tensorflow.KernelDef.AttrConstraint\\x12\\x17\\n\\x0fhost_memory_arg\\x18\\x04 \\x03(\\t\\x12\\r\\n\\x05label\\x18\\x05 \\x01(\\t\\x1aM\\n\\x0e\\x41ttrConstraint\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12-\\n\\x0e\\x61llowed_values\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValueB0\\n\\x18org.tensorflow.frameworkB\\x0fKernelDefProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_attr__value__pb2.DESCRIPTOR,])\n\n\n\n\n_KERNELDEF_ATTRCONSTRAINT = _descriptor.Descriptor(\n  name=\'AttrConstraint\',\n  full_name=\'tensorflow.KernelDef.AttrConstraint\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.KernelDef.AttrConstraint.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allowed_values\', full_name=\'tensorflow.KernelDef.AttrConstraint.allowed_values\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=247,\n  serialized_end=324,\n)\n\n_KERNELDEF = _descriptor.Descriptor(\n  name=\'KernelDef\',\n  full_name=\'tensorflow.KernelDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'tensorflow.KernelDef.op\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_type\', full_name=\'tensorflow.KernelDef.device_type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'constraint\', full_name=\'tensorflow.KernelDef.constraint\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'host_memory_arg\', full_name=\'tensorflow.KernelDef.host_memory_arg\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'tensorflow.KernelDef.label\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_KERNELDEF_ATTRCONSTRAINT, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=103,\n  serialized_end=324,\n)\n\n_KERNELDEF_ATTRCONSTRAINT.fields_by_name[\'allowed_values\'].message_type = tensorflow_dot_core_dot_framework_dot_attr__value__pb2._ATTRVALUE\n_KERNELDEF_ATTRCONSTRAINT.containing_type = _KERNELDEF\n_KERNELDEF.fields_by_name[\'constraint\'].message_type = _KERNELDEF_ATTRCONSTRAINT\nDESCRIPTOR.message_types_by_name[\'KernelDef\'] = _KERNELDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nKernelDef = _reflection.GeneratedProtocolMessageType(\'KernelDef\', (_message.Message,), dict(\n\n  AttrConstraint = _reflection.GeneratedProtocolMessageType(\'AttrConstraint\', (_message.Message,), dict(\n    DESCRIPTOR = _KERNELDEF_ATTRCONSTRAINT,\n    __module__ = \'tensorflow.core.framework.kernel_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.KernelDef.AttrConstraint)\n    ))\n  ,\n  DESCRIPTOR = _KERNELDEF,\n  __module__ = \'tensorflow.core.framework.kernel_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.KernelDef)\n  ))\n_sym_db.RegisterMessage(KernelDef)\n_sym_db.RegisterMessage(KernelDef.AttrConstraint)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\017KernelDefProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/kernel_def_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/log_memory_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/log_memory.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_description_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__description__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/log_memory.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n*tensorflow/core/framework/log_memory.proto\\x12\\ntensorflow\\x1a\\x32tensorflow/core/framework/tensor_description.proto\\""0\\n\\rMemoryLogStep\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\x12\\x0e\\n\\x06handle\\x18\\x02 \\x01(\\t\\""p\\n\\x19MemoryLogTensorAllocation\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\x12\\x13\\n\\x0bkernel_name\\x18\\x02 \\x01(\\t\\x12-\\n\\x06tensor\\x18\\x03 \\x01(\\x0b\\x32\\x1d.tensorflow.TensorDescription\\""L\\n\\x1bMemoryLogTensorDeallocation\\x12\\x15\\n\\rallocation_id\\x18\\x01 \\x01(\\x03\\x12\\x16\\n\\x0e\\x61llocator_name\\x18\\x02 \\x01(\\t\\""{\\n\\x15MemoryLogTensorOutput\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\x12\\x13\\n\\x0bkernel_name\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05index\\x18\\x03 \\x01(\\x05\\x12-\\n\\x06tensor\\x18\\x04 \\x01(\\x0b\\x32\\x1d.tensorflow.TensorDescription\\""\\x8b\\x01\\n\\x16MemoryLogRawAllocation\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\x12\\x11\\n\\toperation\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tnum_bytes\\x18\\x03 \\x01(\\x03\\x12\\x0b\\n\\x03ptr\\x18\\x04 \\x01(\\x04\\x12\\x15\\n\\rallocation_id\\x18\\x05 \\x01(\\x03\\x12\\x16\\n\\x0e\\x61llocator_name\\x18\\x06 \\x01(\\t\\""\\x7f\\n\\x18MemoryLogRawDeallocation\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\x12\\x11\\n\\toperation\\x18\\x02 \\x01(\\t\\x12\\x15\\n\\rallocation_id\\x18\\x03 \\x01(\\x03\\x12\\x16\\n\\x0e\\x61llocator_name\\x18\\x04 \\x01(\\t\\x12\\x10\\n\\x08\\x64\\x65\\x66\\x65rred\\x18\\x05 \\x01(\\x08\\x42\\x30\\n\\x18org.tensorflow.frameworkB\\x0fLogMemoryProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__description__pb2.DESCRIPTOR,])\n\n\n\n\n_MEMORYLOGSTEP = _descriptor.Descriptor(\n  name=\'MemoryLogStep\',\n  full_name=\'tensorflow.MemoryLogStep\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.MemoryLogStep.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'handle\', full_name=\'tensorflow.MemoryLogStep.handle\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=110,\n  serialized_end=158,\n)\n\n\n_MEMORYLOGTENSORALLOCATION = _descriptor.Descriptor(\n  name=\'MemoryLogTensorAllocation\',\n  full_name=\'tensorflow.MemoryLogTensorAllocation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.MemoryLogTensorAllocation.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_name\', full_name=\'tensorflow.MemoryLogTensorAllocation.kernel_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.MemoryLogTensorAllocation.tensor\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=160,\n  serialized_end=272,\n)\n\n\n_MEMORYLOGTENSORDEALLOCATION = _descriptor.Descriptor(\n  name=\'MemoryLogTensorDeallocation\',\n  full_name=\'tensorflow.MemoryLogTensorDeallocation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'allocation_id\', full_name=\'tensorflow.MemoryLogTensorDeallocation.allocation_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocator_name\', full_name=\'tensorflow.MemoryLogTensorDeallocation.allocator_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=274,\n  serialized_end=350,\n)\n\n\n_MEMORYLOGTENSOROUTPUT = _descriptor.Descriptor(\n  name=\'MemoryLogTensorOutput\',\n  full_name=\'tensorflow.MemoryLogTensorOutput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.MemoryLogTensorOutput.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_name\', full_name=\'tensorflow.MemoryLogTensorOutput.kernel_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'tensorflow.MemoryLogTensorOutput.index\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.MemoryLogTensorOutput.tensor\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=352,\n  serialized_end=475,\n)\n\n\n_MEMORYLOGRAWALLOCATION = _descriptor.Descriptor(\n  name=\'MemoryLogRawAllocation\',\n  full_name=\'tensorflow.MemoryLogRawAllocation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.MemoryLogRawAllocation.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'operation\', full_name=\'tensorflow.MemoryLogRawAllocation.operation\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_bytes\', full_name=\'tensorflow.MemoryLogRawAllocation.num_bytes\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ptr\', full_name=\'tensorflow.MemoryLogRawAllocation.ptr\', index=3,\n      number=4, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocation_id\', full_name=\'tensorflow.MemoryLogRawAllocation.allocation_id\', index=4,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocator_name\', full_name=\'tensorflow.MemoryLogRawAllocation.allocator_name\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=478,\n  serialized_end=617,\n)\n\n\n_MEMORYLOGRAWDEALLOCATION = _descriptor.Descriptor(\n  name=\'MemoryLogRawDeallocation\',\n  full_name=\'tensorflow.MemoryLogRawDeallocation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.MemoryLogRawDeallocation.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'operation\', full_name=\'tensorflow.MemoryLogRawDeallocation.operation\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocation_id\', full_name=\'tensorflow.MemoryLogRawDeallocation.allocation_id\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocator_name\', full_name=\'tensorflow.MemoryLogRawDeallocation.allocator_name\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'deferred\', full_name=\'tensorflow.MemoryLogRawDeallocation.deferred\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=619,\n  serialized_end=746,\n)\n\n_MEMORYLOGTENSORALLOCATION.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__description__pb2._TENSORDESCRIPTION\n_MEMORYLOGTENSOROUTPUT.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__description__pb2._TENSORDESCRIPTION\nDESCRIPTOR.message_types_by_name[\'MemoryLogStep\'] = _MEMORYLOGSTEP\nDESCRIPTOR.message_types_by_name[\'MemoryLogTensorAllocation\'] = _MEMORYLOGTENSORALLOCATION\nDESCRIPTOR.message_types_by_name[\'MemoryLogTensorDeallocation\'] = _MEMORYLOGTENSORDEALLOCATION\nDESCRIPTOR.message_types_by_name[\'MemoryLogTensorOutput\'] = _MEMORYLOGTENSOROUTPUT\nDESCRIPTOR.message_types_by_name[\'MemoryLogRawAllocation\'] = _MEMORYLOGRAWALLOCATION\nDESCRIPTOR.message_types_by_name[\'MemoryLogRawDeallocation\'] = _MEMORYLOGRAWDEALLOCATION\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMemoryLogStep = _reflection.GeneratedProtocolMessageType(\'MemoryLogStep\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYLOGSTEP,\n  __module__ = \'tensorflow.core.framework.log_memory_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MemoryLogStep)\n  ))\n_sym_db.RegisterMessage(MemoryLogStep)\n\nMemoryLogTensorAllocation = _reflection.GeneratedProtocolMessageType(\'MemoryLogTensorAllocation\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYLOGTENSORALLOCATION,\n  __module__ = \'tensorflow.core.framework.log_memory_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MemoryLogTensorAllocation)\n  ))\n_sym_db.RegisterMessage(MemoryLogTensorAllocation)\n\nMemoryLogTensorDeallocation = _reflection.GeneratedProtocolMessageType(\'MemoryLogTensorDeallocation\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYLOGTENSORDEALLOCATION,\n  __module__ = \'tensorflow.core.framework.log_memory_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MemoryLogTensorDeallocation)\n  ))\n_sym_db.RegisterMessage(MemoryLogTensorDeallocation)\n\nMemoryLogTensorOutput = _reflection.GeneratedProtocolMessageType(\'MemoryLogTensorOutput\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYLOGTENSOROUTPUT,\n  __module__ = \'tensorflow.core.framework.log_memory_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MemoryLogTensorOutput)\n  ))\n_sym_db.RegisterMessage(MemoryLogTensorOutput)\n\nMemoryLogRawAllocation = _reflection.GeneratedProtocolMessageType(\'MemoryLogRawAllocation\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYLOGRAWALLOCATION,\n  __module__ = \'tensorflow.core.framework.log_memory_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MemoryLogRawAllocation)\n  ))\n_sym_db.RegisterMessage(MemoryLogRawAllocation)\n\nMemoryLogRawDeallocation = _reflection.GeneratedProtocolMessageType(\'MemoryLogRawDeallocation\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYLOGRAWDEALLOCATION,\n  __module__ = \'tensorflow.core.framework.log_memory_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MemoryLogRawDeallocation)\n  ))\n_sym_db.RegisterMessage(MemoryLogRawDeallocation)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\017LogMemoryProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/log_memory_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/node_def_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/node_def.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/node_def.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(tensorflow/core/framework/node_def.proto\\x12\\ntensorflow\\x1a*tensorflow/core/framework/attr_value.proto\\""\\xb3\\x01\\n\\x07NodeDef\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02op\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05input\\x18\\x03 \\x03(\\t\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x04 \\x01(\\t\\x12+\\n\\x04\\x61ttr\\x18\\x05 \\x03(\\x0b\\x32\\x1d.tensorflow.NodeDef.AttrEntry\\x1a\\x42\\n\\tAttrEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12$\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValue:\\x02\\x38\\x01\\x42*\\n\\x18org.tensorflow.frameworkB\\tNodeProtoP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_attr__value__pb2.DESCRIPTOR,])\n\n\n\n\n_NODEDEF_ATTRENTRY = _descriptor.Descriptor(\n  name=\'AttrEntry\',\n  full_name=\'tensorflow.NodeDef.AttrEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.NodeDef.AttrEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.NodeDef.AttrEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=214,\n  serialized_end=280,\n)\n\n_NODEDEF = _descriptor.Descriptor(\n  name=\'NodeDef\',\n  full_name=\'tensorflow.NodeDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.NodeDef.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'tensorflow.NodeDef.op\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorflow.NodeDef.input\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device\', full_name=\'tensorflow.NodeDef.device\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attr\', full_name=\'tensorflow.NodeDef.attr\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_NODEDEF_ATTRENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=101,\n  serialized_end=280,\n)\n\n_NODEDEF_ATTRENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_attr__value__pb2._ATTRVALUE\n_NODEDEF_ATTRENTRY.containing_type = _NODEDEF\n_NODEDEF.fields_by_name[\'attr\'].message_type = _NODEDEF_ATTRENTRY\nDESCRIPTOR.message_types_by_name[\'NodeDef\'] = _NODEDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nNodeDef = _reflection.GeneratedProtocolMessageType(\'NodeDef\', (_message.Message,), dict(\n\n  AttrEntry = _reflection.GeneratedProtocolMessageType(\'AttrEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _NODEDEF_ATTRENTRY,\n    __module__ = \'tensorflow.core.framework.node_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.NodeDef.AttrEntry)\n    ))\n  ,\n  DESCRIPTOR = _NODEDEF,\n  __module__ = \'tensorflow.core.framework.node_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.NodeDef)\n  ))\n_sym_db.RegisterMessage(NodeDef)\n_sym_db.RegisterMessage(NodeDef.AttrEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\tNodeProtoP\\001\\370\\001\\001\'))\n_NODEDEF_ATTRENTRY.has_options = True\n_NODEDEF_ATTRENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/node_def_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/op_def_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/op_def.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/op_def.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n&tensorflow/core/framework/op_def.proto\\x12\\ntensorflow\\x1a*tensorflow/core/framework/attr_value.proto\\x1a%tensorflow/core/framework/types.proto\\""\\xb8\\x05\\n\\x05OpDef\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12+\\n\\tinput_arg\\x18\\x02 \\x03(\\x0b\\x32\\x18.tensorflow.OpDef.ArgDef\\x12,\\n\\noutput_arg\\x18\\x03 \\x03(\\x0b\\x32\\x18.tensorflow.OpDef.ArgDef\\x12\\\'\\n\\x04\\x61ttr\\x18\\x04 \\x03(\\x0b\\x32\\x19.tensorflow.OpDef.AttrDef\\x12.\\n\\x0b\\x64\\x65precation\\x18\\x08 \\x01(\\x0b\\x32\\x19.tensorflow.OpDeprecation\\x12\\x0f\\n\\x07summary\\x18\\x05 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x06 \\x01(\\t\\x12\\x16\\n\\x0eis_commutative\\x18\\x12 \\x01(\\x08\\x12\\x14\\n\\x0cis_aggregate\\x18\\x10 \\x01(\\x08\\x12\\x13\\n\\x0bis_stateful\\x18\\x11 \\x01(\\x08\\x12\\""\\n\\x1a\\x61llows_uninitialized_input\\x18\\x13 \\x01(\\x08\\x1a\\x9f\\x01\\n\\x06\\x41rgDef\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x02 \\x01(\\t\\x12\\""\\n\\x04type\\x18\\x03 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12\\x11\\n\\ttype_attr\\x18\\x04 \\x01(\\t\\x12\\x13\\n\\x0bnumber_attr\\x18\\x05 \\x01(\\t\\x12\\x16\\n\\x0etype_list_attr\\x18\\x06 \\x01(\\t\\x12\\x0e\\n\\x06is_ref\\x18\\x10 \\x01(\\x08\\x1a\\xbd\\x01\\n\\x07\\x41ttrDef\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x02 \\x01(\\t\\x12,\\n\\rdefault_value\\x18\\x03 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValue\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x04 \\x01(\\t\\x12\\x13\\n\\x0bhas_minimum\\x18\\x05 \\x01(\\x08\\x12\\x0f\\n\\x07minimum\\x18\\x06 \\x01(\\x03\\x12-\\n\\x0e\\x61llowed_values\\x18\\x07 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValue\\""5\\n\\rOpDeprecation\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0b\\x65xplanation\\x18\\x02 \\x01(\\t\\""\\\'\\n\\x06OpList\\x12\\x1d\\n\\x02op\\x18\\x01 \\x03(\\x0b\\x32\\x11.tensorflow.OpDefB,\\n\\x18org.tensorflow.frameworkB\\x0bOpDefProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_attr__value__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n\n\n\n\n_OPDEF_ARGDEF = _descriptor.Descriptor(\n  name=\'ArgDef\',\n  full_name=\'tensorflow.OpDef.ArgDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.OpDef.ArgDef.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorflow.OpDef.ArgDef.description\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorflow.OpDef.ArgDef.type\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type_attr\', full_name=\'tensorflow.OpDef.ArgDef.type_attr\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'number_attr\', full_name=\'tensorflow.OpDef.ArgDef.number_attr\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type_list_attr\', full_name=\'tensorflow.OpDef.ArgDef.type_list_attr\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_ref\', full_name=\'tensorflow.OpDef.ArgDef.is_ref\', index=6,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=483,\n  serialized_end=642,\n)\n\n_OPDEF_ATTRDEF = _descriptor.Descriptor(\n  name=\'AttrDef\',\n  full_name=\'tensorflow.OpDef.AttrDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.OpDef.AttrDef.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorflow.OpDef.AttrDef.type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'default_value\', full_name=\'tensorflow.OpDef.AttrDef.default_value\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorflow.OpDef.AttrDef.description\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'has_minimum\', full_name=\'tensorflow.OpDef.AttrDef.has_minimum\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'minimum\', full_name=\'tensorflow.OpDef.AttrDef.minimum\', index=5,\n      number=6, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allowed_values\', full_name=\'tensorflow.OpDef.AttrDef.allowed_values\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=645,\n  serialized_end=834,\n)\n\n_OPDEF = _descriptor.Descriptor(\n  name=\'OpDef\',\n  full_name=\'tensorflow.OpDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.OpDef.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_arg\', full_name=\'tensorflow.OpDef.input_arg\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_arg\', full_name=\'tensorflow.OpDef.output_arg\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attr\', full_name=\'tensorflow.OpDef.attr\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'deprecation\', full_name=\'tensorflow.OpDef.deprecation\', index=4,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'summary\', full_name=\'tensorflow.OpDef.summary\', index=5,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorflow.OpDef.description\', index=6,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_commutative\', full_name=\'tensorflow.OpDef.is_commutative\', index=7,\n      number=18, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_aggregate\', full_name=\'tensorflow.OpDef.is_aggregate\', index=8,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_stateful\', full_name=\'tensorflow.OpDef.is_stateful\', index=9,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allows_uninitialized_input\', full_name=\'tensorflow.OpDef.allows_uninitialized_input\', index=10,\n      number=19, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_OPDEF_ARGDEF, _OPDEF_ATTRDEF, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=138,\n  serialized_end=834,\n)\n\n\n_OPDEPRECATION = _descriptor.Descriptor(\n  name=\'OpDeprecation\',\n  full_name=\'tensorflow.OpDeprecation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.OpDeprecation.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'explanation\', full_name=\'tensorflow.OpDeprecation.explanation\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=836,\n  serialized_end=889,\n)\n\n\n_OPLIST = _descriptor.Descriptor(\n  name=\'OpList\',\n  full_name=\'tensorflow.OpList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'tensorflow.OpList.op\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=891,\n  serialized_end=930,\n)\n\n_OPDEF_ARGDEF.fields_by_name[\'type\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_OPDEF_ARGDEF.containing_type = _OPDEF\n_OPDEF_ATTRDEF.fields_by_name[\'default_value\'].message_type = tensorflow_dot_core_dot_framework_dot_attr__value__pb2._ATTRVALUE\n_OPDEF_ATTRDEF.fields_by_name[\'allowed_values\'].message_type = tensorflow_dot_core_dot_framework_dot_attr__value__pb2._ATTRVALUE\n_OPDEF_ATTRDEF.containing_type = _OPDEF\n_OPDEF.fields_by_name[\'input_arg\'].message_type = _OPDEF_ARGDEF\n_OPDEF.fields_by_name[\'output_arg\'].message_type = _OPDEF_ARGDEF\n_OPDEF.fields_by_name[\'attr\'].message_type = _OPDEF_ATTRDEF\n_OPDEF.fields_by_name[\'deprecation\'].message_type = _OPDEPRECATION\n_OPLIST.fields_by_name[\'op\'].message_type = _OPDEF\nDESCRIPTOR.message_types_by_name[\'OpDef\'] = _OPDEF\nDESCRIPTOR.message_types_by_name[\'OpDeprecation\'] = _OPDEPRECATION\nDESCRIPTOR.message_types_by_name[\'OpList\'] = _OPLIST\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nOpDef = _reflection.GeneratedProtocolMessageType(\'OpDef\', (_message.Message,), dict(\n\n  ArgDef = _reflection.GeneratedProtocolMessageType(\'ArgDef\', (_message.Message,), dict(\n    DESCRIPTOR = _OPDEF_ARGDEF,\n    __module__ = \'tensorflow.core.framework.op_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.OpDef.ArgDef)\n    ))\n  ,\n\n  AttrDef = _reflection.GeneratedProtocolMessageType(\'AttrDef\', (_message.Message,), dict(\n    DESCRIPTOR = _OPDEF_ATTRDEF,\n    __module__ = \'tensorflow.core.framework.op_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.OpDef.AttrDef)\n    ))\n  ,\n  DESCRIPTOR = _OPDEF,\n  __module__ = \'tensorflow.core.framework.op_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.OpDef)\n  ))\n_sym_db.RegisterMessage(OpDef)\n_sym_db.RegisterMessage(OpDef.ArgDef)\n_sym_db.RegisterMessage(OpDef.AttrDef)\n\nOpDeprecation = _reflection.GeneratedProtocolMessageType(\'OpDeprecation\', (_message.Message,), dict(\n  DESCRIPTOR = _OPDEPRECATION,\n  __module__ = \'tensorflow.core.framework.op_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.OpDeprecation)\n  ))\n_sym_db.RegisterMessage(OpDeprecation)\n\nOpList = _reflection.GeneratedProtocolMessageType(\'OpList\', (_message.Message,), dict(\n  DESCRIPTOR = _OPLIST,\n  __module__ = \'tensorflow.core.framework.op_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.OpList)\n  ))\n_sym_db.RegisterMessage(OpList)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\013OpDefProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/op_def_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/op_gen_overrides_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/op_gen_overrides.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/op_gen_overrides.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow/core/framework/op_gen_overrides.proto\\x12\\ntensorflow\\x1a*tensorflow/core/framework/attr_value.proto\\""\\xa7\\x03\\n\\rOpGenOverride\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04skip\\x18\\x02 \\x01(\\x08\\x12\\x0c\\n\\x04hide\\x18\\x03 \\x01(\\x08\\x12\\x11\\n\\trename_to\\x18\\x04 \\x01(\\t\\x12\\r\\n\\x05\\x61lias\\x18\\x05 \\x03(\\t\\x12;\\n\\x0c\\x61ttr_default\\x18\\x06 \\x03(\\x0b\\x32%.tensorflow.OpGenOverride.AttrDefault\\x12\\x35\\n\\x0b\\x61ttr_rename\\x18\\x07 \\x03(\\x0b\\x32 .tensorflow.OpGenOverride.Rename\\x12\\x36\\n\\x0cinput_rename\\x18\\x08 \\x03(\\x0b\\x32 .tensorflow.OpGenOverride.Rename\\x12\\x37\\n\\routput_rename\\x18\\t \\x03(\\x0b\\x32 .tensorflow.OpGenOverride.Rename\\x1a\\x41\\n\\x0b\\x41ttrDefault\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12$\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.AttrValue\\x1a\\""\\n\\x06Rename\\x12\\x0c\\n\\x04\\x66rom\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02to\\x18\\x02 \\x01(\\t\\""7\\n\\x0eOpGenOverrides\\x12%\\n\\x02op\\x18\\x01 \\x03(\\x0b\\x32\\x19.tensorflow.OpGenOverrideb\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_attr__value__pb2.DESCRIPTOR,])\n\n\n\n\n_OPGENOVERRIDE_ATTRDEFAULT = _descriptor.Descriptor(\n  name=\'AttrDefault\',\n  full_name=\'tensorflow.OpGenOverride.AttrDefault\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.OpGenOverride.AttrDefault.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.OpGenOverride.AttrDefault.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=431,\n  serialized_end=496,\n)\n\n_OPGENOVERRIDE_RENAME = _descriptor.Descriptor(\n  name=\'Rename\',\n  full_name=\'tensorflow.OpGenOverride.Rename\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'from\', full_name=\'tensorflow.OpGenOverride.Rename.from\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'to\', full_name=\'tensorflow.OpGenOverride.Rename.to\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=498,\n  serialized_end=532,\n)\n\n_OPGENOVERRIDE = _descriptor.Descriptor(\n  name=\'OpGenOverride\',\n  full_name=\'tensorflow.OpGenOverride\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.OpGenOverride.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'skip\', full_name=\'tensorflow.OpGenOverride.skip\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hide\', full_name=\'tensorflow.OpGenOverride.hide\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rename_to\', full_name=\'tensorflow.OpGenOverride.rename_to\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alias\', full_name=\'tensorflow.OpGenOverride.alias\', index=4,\n      number=5, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attr_default\', full_name=\'tensorflow.OpGenOverride.attr_default\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attr_rename\', full_name=\'tensorflow.OpGenOverride.attr_rename\', index=6,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_rename\', full_name=\'tensorflow.OpGenOverride.input_rename\', index=7,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_rename\', full_name=\'tensorflow.OpGenOverride.output_rename\', index=8,\n      number=9, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_OPGENOVERRIDE_ATTRDEFAULT, _OPGENOVERRIDE_RENAME, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=109,\n  serialized_end=532,\n)\n\n\n_OPGENOVERRIDES = _descriptor.Descriptor(\n  name=\'OpGenOverrides\',\n  full_name=\'tensorflow.OpGenOverrides\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'tensorflow.OpGenOverrides.op\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=534,\n  serialized_end=589,\n)\n\n_OPGENOVERRIDE_ATTRDEFAULT.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_attr__value__pb2._ATTRVALUE\n_OPGENOVERRIDE_ATTRDEFAULT.containing_type = _OPGENOVERRIDE\n_OPGENOVERRIDE_RENAME.containing_type = _OPGENOVERRIDE\n_OPGENOVERRIDE.fields_by_name[\'attr_default\'].message_type = _OPGENOVERRIDE_ATTRDEFAULT\n_OPGENOVERRIDE.fields_by_name[\'attr_rename\'].message_type = _OPGENOVERRIDE_RENAME\n_OPGENOVERRIDE.fields_by_name[\'input_rename\'].message_type = _OPGENOVERRIDE_RENAME\n_OPGENOVERRIDE.fields_by_name[\'output_rename\'].message_type = _OPGENOVERRIDE_RENAME\n_OPGENOVERRIDES.fields_by_name[\'op\'].message_type = _OPGENOVERRIDE\nDESCRIPTOR.message_types_by_name[\'OpGenOverride\'] = _OPGENOVERRIDE\nDESCRIPTOR.message_types_by_name[\'OpGenOverrides\'] = _OPGENOVERRIDES\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nOpGenOverride = _reflection.GeneratedProtocolMessageType(\'OpGenOverride\', (_message.Message,), dict(\n\n  AttrDefault = _reflection.GeneratedProtocolMessageType(\'AttrDefault\', (_message.Message,), dict(\n    DESCRIPTOR = _OPGENOVERRIDE_ATTRDEFAULT,\n    __module__ = \'tensorflow.core.framework.op_gen_overrides_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.OpGenOverride.AttrDefault)\n    ))\n  ,\n\n  Rename = _reflection.GeneratedProtocolMessageType(\'Rename\', (_message.Message,), dict(\n    DESCRIPTOR = _OPGENOVERRIDE_RENAME,\n    __module__ = \'tensorflow.core.framework.op_gen_overrides_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.OpGenOverride.Rename)\n    ))\n  ,\n  DESCRIPTOR = _OPGENOVERRIDE,\n  __module__ = \'tensorflow.core.framework.op_gen_overrides_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.OpGenOverride)\n  ))\n_sym_db.RegisterMessage(OpGenOverride)\n_sym_db.RegisterMessage(OpGenOverride.AttrDefault)\n_sym_db.RegisterMessage(OpGenOverride.Rename)\n\nOpGenOverrides = _reflection.GeneratedProtocolMessageType(\'OpGenOverrides\', (_message.Message,), dict(\n  DESCRIPTOR = _OPGENOVERRIDES,\n  __module__ = \'tensorflow.core.framework.op_gen_overrides_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.OpGenOverrides)\n  ))\n_sym_db.RegisterMessage(OpGenOverrides)\n\n\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/op_gen_overrides_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/reader_base_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/reader_base.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/reader_base.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n+tensorflow/core/framework/reader_base.proto\\x12\\ntensorflow\\""r\\n\\x0fReaderBaseState\\x12\\x14\\n\\x0cwork_started\\x18\\x01 \\x01(\\x03\\x12\\x15\\n\\rwork_finished\\x18\\x02 \\x01(\\x03\\x12\\x1c\\n\\x14num_records_produced\\x18\\x03 \\x01(\\x03\\x12\\x14\\n\\x0c\\x63urrent_work\\x18\\x04 \\x01(\\x0c\\x42\\x31\\n\\x18org.tensorflow.frameworkB\\x10ReaderBaseProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_READERBASESTATE = _descriptor.Descriptor(\n  name=\'ReaderBaseState\',\n  full_name=\'tensorflow.ReaderBaseState\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'work_started\', full_name=\'tensorflow.ReaderBaseState.work_started\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'work_finished\', full_name=\'tensorflow.ReaderBaseState.work_finished\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_records_produced\', full_name=\'tensorflow.ReaderBaseState.num_records_produced\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'current_work\', full_name=\'tensorflow.ReaderBaseState.current_work\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=59,\n  serialized_end=173,\n)\n\nDESCRIPTOR.message_types_by_name[\'ReaderBaseState\'] = _READERBASESTATE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nReaderBaseState = _reflection.GeneratedProtocolMessageType(\'ReaderBaseState\', (_message.Message,), dict(\n  DESCRIPTOR = _READERBASESTATE,\n  __module__ = \'tensorflow.core.framework.reader_base_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ReaderBaseState)\n  ))\n_sym_db.RegisterMessage(ReaderBaseState)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\020ReaderBaseProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/reader_base_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/remote_fused_graph_execute_info_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/remote_fused_graph_execute_info.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import graph_pb2 as tensorflow_dot_core_dot_framework_dot_graph__pb2\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/remote_fused_graph_execute_info.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n?tensorflow/core/framework/remote_fused_graph_execute_info.proto\\x12\\ntensorflow\\x1a%tensorflow/core/framework/graph.proto\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a%tensorflow/core/framework/types.proto\\""\\xf2\\x04\\n\\x1bRemoteFusedGraphExecuteInfo\\x12*\\n\\x0cremote_graph\\x18\\x01 \\x01(\\x0b\\x32\\x14.tensorflow.GraphDef\\x12\\x1d\\n\\x15graph_input_node_name\\x18\\x02 \\x03(\\t\\x12\\x1e\\n\\x16graph_output_node_name\\x18\\x03 \\x03(\\t\\x12\\x15\\n\\rexecutor_name\\x18\\x04 \\x01(\\t\\x12&\\n\\x1eserialized_executor_parameters\\x18\\x05 \\x01(\\x0c\\x12\\x66\\n default_graph_input_tensor_shape\\x18\\x06 \\x03(\\x0b\\x32<.tensorflow.RemoteFusedGraphExecuteInfo.TensorShapeTypeProto\\x12g\\n!default_graph_output_tensor_shape\\x18\\x07 \\x03(\\x0b\\x32<.tensorflow.RemoteFusedGraphExecuteInfo.TensorShapeTypeProto\\x1ah\\n\\x14TensorShapeTypeProto\\x12#\\n\\x05\\x64type\\x18\\x01 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12+\\n\\x05shape\\x18\\x02 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\""n\\n\\x08NodeType\\x12\\n\\n\\x06UNUSED\\x10\\x00\\x12\\x0f\\n\\x0bGRAPH_INPUT\\x10\\x01\\x12\\x10\\n\\x0cGRAPH_OUTPUT\\x10\\x02\\x12\\x0e\\n\\nFUSED_NODE\\x10\\x03\\x12\\x10\\n\\x0c\\x42ORDER_INPUT\\x10\\x04\\x12\\x11\\n\\rBORDER_OUTPUT\\x10\\x05\\x42\\x41\\n\\x18org.tensorflow.frameworkB RemoteFusedGraphExecuteInfoProtoP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_graph__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n\n\n\n_REMOTEFUSEDGRAPHEXECUTEINFO_NODETYPE = _descriptor.EnumDescriptor(\n  name=\'NodeType\',\n  full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.NodeType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'UNUSED\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'GRAPH_INPUT\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'GRAPH_OUTPUT\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FUSED_NODE\', index=3, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BORDER_INPUT\', index=4, number=4,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BORDER_OUTPUT\', index=5, number=5,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=720,\n  serialized_end=830,\n)\n_sym_db.RegisterEnumDescriptor(_REMOTEFUSEDGRAPHEXECUTEINFO_NODETYPE)\n\n\n_REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO = _descriptor.Descriptor(\n  name=\'TensorShapeTypeProto\',\n  full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.TensorShapeTypeProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.TensorShapeTypeProto.dtype\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.TensorShapeTypeProto.shape\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=614,\n  serialized_end=718,\n)\n\n_REMOTEFUSEDGRAPHEXECUTEINFO = _descriptor.Descriptor(\n  name=\'RemoteFusedGraphExecuteInfo\',\n  full_name=\'tensorflow.RemoteFusedGraphExecuteInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'remote_graph\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.remote_graph\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_input_node_name\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.graph_input_node_name\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_output_node_name\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.graph_output_node_name\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'executor_name\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.executor_name\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'serialized_executor_parameters\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.serialized_executor_parameters\', index=4,\n      number=5, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'default_graph_input_tensor_shape\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.default_graph_input_tensor_shape\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'default_graph_output_tensor_shape\', full_name=\'tensorflow.RemoteFusedGraphExecuteInfo.default_graph_output_tensor_shape\', index=6,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO, ],\n  enum_types=[\n    _REMOTEFUSEDGRAPHEXECUTEINFO_NODETYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=204,\n  serialized_end=830,\n)\n\n_REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO.fields_by_name[\'shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO.containing_type = _REMOTEFUSEDGRAPHEXECUTEINFO\n_REMOTEFUSEDGRAPHEXECUTEINFO.fields_by_name[\'remote_graph\'].message_type = tensorflow_dot_core_dot_framework_dot_graph__pb2._GRAPHDEF\n_REMOTEFUSEDGRAPHEXECUTEINFO.fields_by_name[\'default_graph_input_tensor_shape\'].message_type = _REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO\n_REMOTEFUSEDGRAPHEXECUTEINFO.fields_by_name[\'default_graph_output_tensor_shape\'].message_type = _REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO\n_REMOTEFUSEDGRAPHEXECUTEINFO_NODETYPE.containing_type = _REMOTEFUSEDGRAPHEXECUTEINFO\nDESCRIPTOR.message_types_by_name[\'RemoteFusedGraphExecuteInfo\'] = _REMOTEFUSEDGRAPHEXECUTEINFO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nRemoteFusedGraphExecuteInfo = _reflection.GeneratedProtocolMessageType(\'RemoteFusedGraphExecuteInfo\', (_message.Message,), dict(\n\n  TensorShapeTypeProto = _reflection.GeneratedProtocolMessageType(\'TensorShapeTypeProto\', (_message.Message,), dict(\n    DESCRIPTOR = _REMOTEFUSEDGRAPHEXECUTEINFO_TENSORSHAPETYPEPROTO,\n    __module__ = \'tensorflow.core.framework.remote_fused_graph_execute_info_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.RemoteFusedGraphExecuteInfo.TensorShapeTypeProto)\n    ))\n  ,\n  DESCRIPTOR = _REMOTEFUSEDGRAPHEXECUTEINFO,\n  __module__ = \'tensorflow.core.framework.remote_fused_graph_execute_info_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RemoteFusedGraphExecuteInfo)\n  ))\n_sym_db.RegisterMessage(RemoteFusedGraphExecuteInfo)\n_sym_db.RegisterMessage(RemoteFusedGraphExecuteInfo.TensorShapeTypeProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB RemoteFusedGraphExecuteInfoProtoP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/remote_fused_graph_execute_info_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/resource_handle_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/resource_handle.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/resource_handle.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\""r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_RESOURCEHANDLEPROTO = _descriptor.Descriptor(\n  name=\'ResourceHandleProto\',\n  full_name=\'tensorflow.ResourceHandleProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'device\', full_name=\'tensorflow.ResourceHandleProto.device\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'container\', full_name=\'tensorflow.ResourceHandleProto.container\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.ResourceHandleProto.name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hash_code\', full_name=\'tensorflow.ResourceHandleProto.hash_code\', index=3,\n      number=4, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maybe_type_name\', full_name=\'tensorflow.ResourceHandleProto.maybe_type_name\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=63,\n  serialized_end=177,\n)\n\nDESCRIPTOR.message_types_by_name[\'ResourceHandleProto\'] = _RESOURCEHANDLEPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nResourceHandleProto = _reflection.GeneratedProtocolMessageType(\'ResourceHandleProto\', (_message.Message,), dict(\n  DESCRIPTOR = _RESOURCEHANDLEPROTO,\n  __module__ = \'tensorflow.core.framework.resource_handle_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ResourceHandleProto)\n  ))\n_sym_db.RegisterMessage(ResourceHandleProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\016ResourceHandleP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/resource_handle_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/step_stats_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/step_stats.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import allocation_description_pb2 as tensorflow_dot_core_dot_framework_dot_allocation__description__pb2\nfrom tensorflow.core.framework import tensor_description_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__description__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/step_stats.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n*tensorflow/core/framework/step_stats.proto\\x12\\ntensorflow\\x1a\\x36tensorflow/core/framework/allocation_description.proto\\x1a\\x32tensorflow/core/framework/tensor_description.proto\\""=\\n\\x10\\x41llocationRecord\\x12\\x14\\n\\x0c\\x61lloc_micros\\x18\\x01 \\x01(\\x03\\x12\\x13\\n\\x0b\\x61lloc_bytes\\x18\\x02 \\x01(\\x03\\""\\xc4\\x01\\n\\x13\\x41llocatorMemoryUsed\\x12\\x16\\n\\x0e\\x61llocator_name\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0btotal_bytes\\x18\\x02 \\x01(\\x03\\x12\\x12\\n\\npeak_bytes\\x18\\x03 \\x01(\\x03\\x12\\x12\\n\\nlive_bytes\\x18\\x04 \\x01(\\x03\\x12\\x38\\n\\x12\\x61llocation_records\\x18\\x06 \\x03(\\x0b\\x32\\x1c.tensorflow.AllocationRecord\\x12\\x1e\\n\\x16\\x61llocator_bytes_in_use\\x18\\x05 \\x01(\\x03\\""U\\n\\nNodeOutput\\x12\\x0c\\n\\x04slot\\x18\\x01 \\x01(\\x05\\x12\\x39\\n\\x12tensor_description\\x18\\x03 \\x01(\\x0b\\x32\\x1d.tensorflow.TensorDescription\\""\\xef\\x01\\n\\x0bMemoryStats\\x12\\x1d\\n\\x15host_temp_memory_size\\x18\\x01 \\x01(\\x03\\x12\\x1f\\n\\x17\\x64\\x65vice_temp_memory_size\\x18\\x02 \\x01(\\x03\\x12#\\n\\x1bhost_persistent_memory_size\\x18\\x03 \\x01(\\x03\\x12%\\n\\x1d\\x64\\x65vice_persistent_memory_size\\x18\\x04 \\x01(\\x03\\x12(\\n host_persistent_tensor_alloc_ids\\x18\\x05 \\x03(\\x03\\x12*\\n\\""device_persistent_tensor_alloc_ids\\x18\\x06 \\x03(\\x03\\""\\x9b\\x03\\n\\rNodeExecStats\\x12\\x11\\n\\tnode_name\\x18\\x01 \\x01(\\t\\x12\\x18\\n\\x10\\x61ll_start_micros\\x18\\x02 \\x01(\\x03\\x12\\x1b\\n\\x13op_start_rel_micros\\x18\\x03 \\x01(\\x03\\x12\\x19\\n\\x11op_end_rel_micros\\x18\\x04 \\x01(\\x03\\x12\\x1a\\n\\x12\\x61ll_end_rel_micros\\x18\\x05 \\x01(\\x03\\x12/\\n\\x06memory\\x18\\x06 \\x03(\\x0b\\x32\\x1f.tensorflow.AllocatorMemoryUsed\\x12&\\n\\x06output\\x18\\x07 \\x03(\\x0b\\x32\\x16.tensorflow.NodeOutput\\x12\\x16\\n\\x0etimeline_label\\x18\\x08 \\x01(\\t\\x12\\x18\\n\\x10scheduled_micros\\x18\\t \\x01(\\x03\\x12\\x11\\n\\tthread_id\\x18\\n \\x01(\\r\\x12<\\n\\x11referenced_tensor\\x18\\x0b \\x03(\\x0b\\x32!.tensorflow.AllocationDescription\\x12-\\n\\x0cmemory_stats\\x18\\x0c \\x01(\\x0b\\x32\\x17.tensorflow.MemoryStats\\""P\\n\\x0f\\x44\\x65viceStepStats\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12-\\n\\nnode_stats\\x18\\x02 \\x03(\\x0b\\x32\\x19.tensorflow.NodeExecStats\\"";\\n\\tStepStats\\x12.\\n\\tdev_stats\\x18\\x01 \\x03(\\x0b\\x32\\x1b.tensorflow.DeviceStepStatsB0\\n\\x18org.tensorflow.frameworkB\\x0fStepStatsProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_allocation__description__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__description__pb2.DESCRIPTOR,])\n\n\n\n\n_ALLOCATIONRECORD = _descriptor.Descriptor(\n  name=\'AllocationRecord\',\n  full_name=\'tensorflow.AllocationRecord\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'alloc_micros\', full_name=\'tensorflow.AllocationRecord.alloc_micros\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alloc_bytes\', full_name=\'tensorflow.AllocationRecord.alloc_bytes\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=166,\n  serialized_end=227,\n)\n\n\n_ALLOCATORMEMORYUSED = _descriptor.Descriptor(\n  name=\'AllocatorMemoryUsed\',\n  full_name=\'tensorflow.AllocatorMemoryUsed\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'allocator_name\', full_name=\'tensorflow.AllocatorMemoryUsed.allocator_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'total_bytes\', full_name=\'tensorflow.AllocatorMemoryUsed.total_bytes\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'peak_bytes\', full_name=\'tensorflow.AllocatorMemoryUsed.peak_bytes\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'live_bytes\', full_name=\'tensorflow.AllocatorMemoryUsed.live_bytes\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocation_records\', full_name=\'tensorflow.AllocatorMemoryUsed.allocation_records\', index=4,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocator_bytes_in_use\', full_name=\'tensorflow.AllocatorMemoryUsed.allocator_bytes_in_use\', index=5,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=230,\n  serialized_end=426,\n)\n\n\n_NODEOUTPUT = _descriptor.Descriptor(\n  name=\'NodeOutput\',\n  full_name=\'tensorflow.NodeOutput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'slot\', full_name=\'tensorflow.NodeOutput.slot\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor_description\', full_name=\'tensorflow.NodeOutput.tensor_description\', index=1,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=428,\n  serialized_end=513,\n)\n\n\n_MEMORYSTATS = _descriptor.Descriptor(\n  name=\'MemoryStats\',\n  full_name=\'tensorflow.MemoryStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'host_temp_memory_size\', full_name=\'tensorflow.MemoryStats.host_temp_memory_size\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_temp_memory_size\', full_name=\'tensorflow.MemoryStats.device_temp_memory_size\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'host_persistent_memory_size\', full_name=\'tensorflow.MemoryStats.host_persistent_memory_size\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_persistent_memory_size\', full_name=\'tensorflow.MemoryStats.device_persistent_memory_size\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'host_persistent_tensor_alloc_ids\', full_name=\'tensorflow.MemoryStats.host_persistent_tensor_alloc_ids\', index=4,\n      number=5, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_persistent_tensor_alloc_ids\', full_name=\'tensorflow.MemoryStats.device_persistent_tensor_alloc_ids\', index=5,\n      number=6, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=516,\n  serialized_end=755,\n)\n\n\n_NODEEXECSTATS = _descriptor.Descriptor(\n  name=\'NodeExecStats\',\n  full_name=\'tensorflow.NodeExecStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_name\', full_name=\'tensorflow.NodeExecStats.node_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'all_start_micros\', full_name=\'tensorflow.NodeExecStats.all_start_micros\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'op_start_rel_micros\', full_name=\'tensorflow.NodeExecStats.op_start_rel_micros\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'op_end_rel_micros\', full_name=\'tensorflow.NodeExecStats.op_end_rel_micros\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'all_end_rel_micros\', full_name=\'tensorflow.NodeExecStats.all_end_rel_micros\', index=4,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory\', full_name=\'tensorflow.NodeExecStats.memory\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output\', full_name=\'tensorflow.NodeExecStats.output\', index=6,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'timeline_label\', full_name=\'tensorflow.NodeExecStats.timeline_label\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scheduled_micros\', full_name=\'tensorflow.NodeExecStats.scheduled_micros\', index=8,\n      number=9, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'thread_id\', full_name=\'tensorflow.NodeExecStats.thread_id\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'referenced_tensor\', full_name=\'tensorflow.NodeExecStats.referenced_tensor\', index=10,\n      number=11, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_stats\', full_name=\'tensorflow.NodeExecStats.memory_stats\', index=11,\n      number=12, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=758,\n  serialized_end=1169,\n)\n\n\n_DEVICESTEPSTATS = _descriptor.Descriptor(\n  name=\'DeviceStepStats\',\n  full_name=\'tensorflow.DeviceStepStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'device\', full_name=\'tensorflow.DeviceStepStats.device\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'node_stats\', full_name=\'tensorflow.DeviceStepStats.node_stats\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1171,\n  serialized_end=1251,\n)\n\n\n_STEPSTATS = _descriptor.Descriptor(\n  name=\'StepStats\',\n  full_name=\'tensorflow.StepStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dev_stats\', full_name=\'tensorflow.StepStats.dev_stats\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1253,\n  serialized_end=1312,\n)\n\n_ALLOCATORMEMORYUSED.fields_by_name[\'allocation_records\'].message_type = _ALLOCATIONRECORD\n_NODEOUTPUT.fields_by_name[\'tensor_description\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__description__pb2._TENSORDESCRIPTION\n_NODEEXECSTATS.fields_by_name[\'memory\'].message_type = _ALLOCATORMEMORYUSED\n_NODEEXECSTATS.fields_by_name[\'output\'].message_type = _NODEOUTPUT\n_NODEEXECSTATS.fields_by_name[\'referenced_tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_allocation__description__pb2._ALLOCATIONDESCRIPTION\n_NODEEXECSTATS.fields_by_name[\'memory_stats\'].message_type = _MEMORYSTATS\n_DEVICESTEPSTATS.fields_by_name[\'node_stats\'].message_type = _NODEEXECSTATS\n_STEPSTATS.fields_by_name[\'dev_stats\'].message_type = _DEVICESTEPSTATS\nDESCRIPTOR.message_types_by_name[\'AllocationRecord\'] = _ALLOCATIONRECORD\nDESCRIPTOR.message_types_by_name[\'AllocatorMemoryUsed\'] = _ALLOCATORMEMORYUSED\nDESCRIPTOR.message_types_by_name[\'NodeOutput\'] = _NODEOUTPUT\nDESCRIPTOR.message_types_by_name[\'MemoryStats\'] = _MEMORYSTATS\nDESCRIPTOR.message_types_by_name[\'NodeExecStats\'] = _NODEEXECSTATS\nDESCRIPTOR.message_types_by_name[\'DeviceStepStats\'] = _DEVICESTEPSTATS\nDESCRIPTOR.message_types_by_name[\'StepStats\'] = _STEPSTATS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAllocationRecord = _reflection.GeneratedProtocolMessageType(\'AllocationRecord\', (_message.Message,), dict(\n  DESCRIPTOR = _ALLOCATIONRECORD,\n  __module__ = \'tensorflow.core.framework.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.AllocationRecord)\n  ))\n_sym_db.RegisterMessage(AllocationRecord)\n\nAllocatorMemoryUsed = _reflection.GeneratedProtocolMessageType(\'AllocatorMemoryUsed\', (_message.Message,), dict(\n  DESCRIPTOR = _ALLOCATORMEMORYUSED,\n  __module__ = \'tensorflow.core.framework.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.AllocatorMemoryUsed)\n  ))\n_sym_db.RegisterMessage(AllocatorMemoryUsed)\n\nNodeOutput = _reflection.GeneratedProtocolMessageType(\'NodeOutput\', (_message.Message,), dict(\n  DESCRIPTOR = _NODEOUTPUT,\n  __module__ = \'tensorflow.core.framework.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.NodeOutput)\n  ))\n_sym_db.RegisterMessage(NodeOutput)\n\nMemoryStats = _reflection.GeneratedProtocolMessageType(\'MemoryStats\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYSTATS,\n  __module__ = \'tensorflow.core.framework.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MemoryStats)\n  ))\n_sym_db.RegisterMessage(MemoryStats)\n\nNodeExecStats = _reflection.GeneratedProtocolMessageType(\'NodeExecStats\', (_message.Message,), dict(\n  DESCRIPTOR = _NODEEXECSTATS,\n  __module__ = \'tensorflow.core.framework.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.NodeExecStats)\n  ))\n_sym_db.RegisterMessage(NodeExecStats)\n\nDeviceStepStats = _reflection.GeneratedProtocolMessageType(\'DeviceStepStats\', (_message.Message,), dict(\n  DESCRIPTOR = _DEVICESTEPSTATS,\n  __module__ = \'tensorflow.core.framework.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeviceStepStats)\n  ))\n_sym_db.RegisterMessage(DeviceStepStats)\n\nStepStats = _reflection.GeneratedProtocolMessageType(\'StepStats\', (_message.Message,), dict(\n  DESCRIPTOR = _STEPSTATS,\n  __module__ = \'tensorflow.core.framework.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.StepStats)\n  ))\n_sym_db.RegisterMessage(StepStats)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\017StepStatsProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/step_stats_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/summary_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/summary.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/summary.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\\'tensorflow/core/framework/summary.proto\\x12\\ntensorflow\\x1a&tensorflow/core/framework/tensor.proto\\""\\\'\\n\\x12SummaryDescription\\x12\\x11\\n\\ttype_hint\\x18\\x01 \\x01(\\t\\""\\x87\\x01\\n\\x0eHistogramProto\\x12\\x0b\\n\\x03min\\x18\\x01 \\x01(\\x01\\x12\\x0b\\n\\x03max\\x18\\x02 \\x01(\\x01\\x12\\x0b\\n\\x03num\\x18\\x03 \\x01(\\x01\\x12\\x0b\\n\\x03sum\\x18\\x04 \\x01(\\x01\\x12\\x13\\n\\x0bsum_squares\\x18\\x05 \\x01(\\x01\\x12\\x18\\n\\x0c\\x62ucket_limit\\x18\\x06 \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x12\\n\\x06\\x62ucket\\x18\\x07 \\x03(\\x01\\x42\\x02\\x10\\x01\\""\\xb5\\x01\\n\\x0fSummaryMetadata\\x12;\\n\\x0bplugin_data\\x18\\x01 \\x01(\\x0b\\x32&.tensorflow.SummaryMetadata.PluginData\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x02 \\x01(\\t\\x12\\x1b\\n\\x13summary_description\\x18\\x03 \\x01(\\t\\x1a\\x32\\n\\nPluginData\\x12\\x13\\n\\x0bplugin_name\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\x0c\\""\\xde\\x04\\n\\x07Summary\\x12(\\n\\x05value\\x18\\x01 \\x03(\\x0b\\x32\\x19.tensorflow.Summary.Value\\x1aX\\n\\x05Image\\x12\\x0e\\n\\x06height\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05width\\x18\\x02 \\x01(\\x05\\x12\\x12\\n\\ncolorspace\\x18\\x03 \\x01(\\x05\\x12\\x1c\\n\\x14\\x65ncoded_image_string\\x18\\x04 \\x01(\\x0c\\x1a}\\n\\x05\\x41udio\\x12\\x13\\n\\x0bsample_rate\\x18\\x01 \\x01(\\x02\\x12\\x14\\n\\x0cnum_channels\\x18\\x02 \\x01(\\x03\\x12\\x15\\n\\rlength_frames\\x18\\x03 \\x01(\\x03\\x12\\x1c\\n\\x14\\x65ncoded_audio_string\\x18\\x04 \\x01(\\x0c\\x12\\x14\\n\\x0c\\x63ontent_type\\x18\\x05 \\x01(\\t\\x1a\\xcf\\x02\\n\\x05Value\\x12\\x11\\n\\tnode_name\\x18\\x07 \\x01(\\t\\x12\\x0b\\n\\x03tag\\x18\\x01 \\x01(\\t\\x12-\\n\\x08metadata\\x18\\t \\x01(\\x0b\\x32\\x1b.tensorflow.SummaryMetadata\\x12\\x16\\n\\x0csimple_value\\x18\\x02 \\x01(\\x02H\\x00\\x12&\\n\\x1cobsolete_old_style_histogram\\x18\\x03 \\x01(\\x0cH\\x00\\x12*\\n\\x05image\\x18\\x04 \\x01(\\x0b\\x32\\x19.tensorflow.Summary.ImageH\\x00\\x12+\\n\\x05histo\\x18\\x05 \\x01(\\x0b\\x32\\x1a.tensorflow.HistogramProtoH\\x00\\x12*\\n\\x05\\x61udio\\x18\\x06 \\x01(\\x0b\\x32\\x19.tensorflow.Summary.AudioH\\x00\\x12)\\n\\x06tensor\\x18\\x08 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProtoH\\x00\\x42\\x07\\n\\x05valueB.\\n\\x18org.tensorflow.frameworkB\\rSummaryProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,])\n\n\n\n\n_SUMMARYDESCRIPTION = _descriptor.Descriptor(\n  name=\'SummaryDescription\',\n  full_name=\'tensorflow.SummaryDescription\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type_hint\', full_name=\'tensorflow.SummaryDescription.type_hint\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=95,\n  serialized_end=134,\n)\n\n\n_HISTOGRAMPROTO = _descriptor.Descriptor(\n  name=\'HistogramProto\',\n  full_name=\'tensorflow.HistogramProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min\', full_name=\'tensorflow.HistogramProto.min\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max\', full_name=\'tensorflow.HistogramProto.max\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num\', full_name=\'tensorflow.HistogramProto.num\', index=2,\n      number=3, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sum\', full_name=\'tensorflow.HistogramProto.sum\', index=3,\n      number=4, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sum_squares\', full_name=\'tensorflow.HistogramProto.sum_squares\', index=4,\n      number=5, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bucket_limit\', full_name=\'tensorflow.HistogramProto.bucket_limit\', index=5,\n      number=6, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'bucket\', full_name=\'tensorflow.HistogramProto.bucket\', index=6,\n      number=7, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=137,\n  serialized_end=272,\n)\n\n\n_SUMMARYMETADATA_PLUGINDATA = _descriptor.Descriptor(\n  name=\'PluginData\',\n  full_name=\'tensorflow.SummaryMetadata.PluginData\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'plugin_name\', full_name=\'tensorflow.SummaryMetadata.PluginData.plugin_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'content\', full_name=\'tensorflow.SummaryMetadata.PluginData.content\', index=1,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=406,\n  serialized_end=456,\n)\n\n_SUMMARYMETADATA = _descriptor.Descriptor(\n  name=\'SummaryMetadata\',\n  full_name=\'tensorflow.SummaryMetadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'plugin_data\', full_name=\'tensorflow.SummaryMetadata.plugin_data\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'tensorflow.SummaryMetadata.display_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'summary_description\', full_name=\'tensorflow.SummaryMetadata.summary_description\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SUMMARYMETADATA_PLUGINDATA, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=275,\n  serialized_end=456,\n)\n\n\n_SUMMARY_IMAGE = _descriptor.Descriptor(\n  name=\'Image\',\n  full_name=\'tensorflow.Summary.Image\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'tensorflow.Summary.Image.height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'tensorflow.Summary.Image.width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'colorspace\', full_name=\'tensorflow.Summary.Image.colorspace\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encoded_image_string\', full_name=\'tensorflow.Summary.Image.encoded_image_string\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=512,\n  serialized_end=600,\n)\n\n_SUMMARY_AUDIO = _descriptor.Descriptor(\n  name=\'Audio\',\n  full_name=\'tensorflow.Summary.Audio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sample_rate\', full_name=\'tensorflow.Summary.Audio.sample_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_channels\', full_name=\'tensorflow.Summary.Audio.num_channels\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'length_frames\', full_name=\'tensorflow.Summary.Audio.length_frames\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encoded_audio_string\', full_name=\'tensorflow.Summary.Audio.encoded_audio_string\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'content_type\', full_name=\'tensorflow.Summary.Audio.content_type\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=602,\n  serialized_end=727,\n)\n\n_SUMMARY_VALUE = _descriptor.Descriptor(\n  name=\'Value\',\n  full_name=\'tensorflow.Summary.Value\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_name\', full_name=\'tensorflow.Summary.Value.node_name\', index=0,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tag\', full_name=\'tensorflow.Summary.Value.tag\', index=1,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'metadata\', full_name=\'tensorflow.Summary.Value.metadata\', index=2,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'simple_value\', full_name=\'tensorflow.Summary.Value.simple_value\', index=3,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'obsolete_old_style_histogram\', full_name=\'tensorflow.Summary.Value.obsolete_old_style_histogram\', index=4,\n      number=3, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image\', full_name=\'tensorflow.Summary.Value.image\', index=5,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'histo\', full_name=\'tensorflow.Summary.Value.histo\', index=6,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'audio\', full_name=\'tensorflow.Summary.Value.audio\', index=7,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.Summary.Value.tensor\', index=8,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'value\', full_name=\'tensorflow.Summary.Value.value\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=730,\n  serialized_end=1065,\n)\n\n_SUMMARY = _descriptor.Descriptor(\n  name=\'Summary\',\n  full_name=\'tensorflow.Summary\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.Summary.value\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SUMMARY_IMAGE, _SUMMARY_AUDIO, _SUMMARY_VALUE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=459,\n  serialized_end=1065,\n)\n\n_SUMMARYMETADATA_PLUGINDATA.containing_type = _SUMMARYMETADATA\n_SUMMARYMETADATA.fields_by_name[\'plugin_data\'].message_type = _SUMMARYMETADATA_PLUGINDATA\n_SUMMARY_IMAGE.containing_type = _SUMMARY\n_SUMMARY_AUDIO.containing_type = _SUMMARY\n_SUMMARY_VALUE.fields_by_name[\'metadata\'].message_type = _SUMMARYMETADATA\n_SUMMARY_VALUE.fields_by_name[\'image\'].message_type = _SUMMARY_IMAGE\n_SUMMARY_VALUE.fields_by_name[\'histo\'].message_type = _HISTOGRAMPROTO\n_SUMMARY_VALUE.fields_by_name[\'audio\'].message_type = _SUMMARY_AUDIO\n_SUMMARY_VALUE.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_SUMMARY_VALUE.containing_type = _SUMMARY\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'simple_value\'])\n_SUMMARY_VALUE.fields_by_name[\'simple_value\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'obsolete_old_style_histogram\'])\n_SUMMARY_VALUE.fields_by_name[\'obsolete_old_style_histogram\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'image\'])\n_SUMMARY_VALUE.fields_by_name[\'image\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'histo\'])\n_SUMMARY_VALUE.fields_by_name[\'histo\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'audio\'])\n_SUMMARY_VALUE.fields_by_name[\'audio\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'tensor\'])\n_SUMMARY_VALUE.fields_by_name[\'tensor\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY.fields_by_name[\'value\'].message_type = _SUMMARY_VALUE\nDESCRIPTOR.message_types_by_name[\'SummaryDescription\'] = _SUMMARYDESCRIPTION\nDESCRIPTOR.message_types_by_name[\'HistogramProto\'] = _HISTOGRAMPROTO\nDESCRIPTOR.message_types_by_name[\'SummaryMetadata\'] = _SUMMARYMETADATA\nDESCRIPTOR.message_types_by_name[\'Summary\'] = _SUMMARY\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSummaryDescription = _reflection.GeneratedProtocolMessageType(\'SummaryDescription\', (_message.Message,), dict(\n  DESCRIPTOR = _SUMMARYDESCRIPTION,\n  __module__ = \'tensorflow.core.framework.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.SummaryDescription)\n  ))\n_sym_db.RegisterMessage(SummaryDescription)\n\nHistogramProto = _reflection.GeneratedProtocolMessageType(\'HistogramProto\', (_message.Message,), dict(\n  DESCRIPTOR = _HISTOGRAMPROTO,\n  __module__ = \'tensorflow.core.framework.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.HistogramProto)\n  ))\n_sym_db.RegisterMessage(HistogramProto)\n\nSummaryMetadata = _reflection.GeneratedProtocolMessageType(\'SummaryMetadata\', (_message.Message,), dict(\n\n  PluginData = _reflection.GeneratedProtocolMessageType(\'PluginData\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARYMETADATA_PLUGINDATA,\n    __module__ = \'tensorflow.core.framework.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.SummaryMetadata.PluginData)\n    ))\n  ,\n  DESCRIPTOR = _SUMMARYMETADATA,\n  __module__ = \'tensorflow.core.framework.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.SummaryMetadata)\n  ))\n_sym_db.RegisterMessage(SummaryMetadata)\n_sym_db.RegisterMessage(SummaryMetadata.PluginData)\n\nSummary = _reflection.GeneratedProtocolMessageType(\'Summary\', (_message.Message,), dict(\n\n  Image = _reflection.GeneratedProtocolMessageType(\'Image\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARY_IMAGE,\n    __module__ = \'tensorflow.core.framework.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.Summary.Image)\n    ))\n  ,\n\n  Audio = _reflection.GeneratedProtocolMessageType(\'Audio\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARY_AUDIO,\n    __module__ = \'tensorflow.core.framework.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.Summary.Audio)\n    ))\n  ,\n\n  Value = _reflection.GeneratedProtocolMessageType(\'Value\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARY_VALUE,\n    __module__ = \'tensorflow.core.framework.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.Summary.Value)\n    ))\n  ,\n  DESCRIPTOR = _SUMMARY,\n  __module__ = \'tensorflow.core.framework.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.Summary)\n  ))\n_sym_db.RegisterMessage(Summary)\n_sym_db.RegisterMessage(Summary.Image)\n_sym_db.RegisterMessage(Summary.Audio)\n_sym_db.RegisterMessage(Summary.Value)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\rSummaryProtosP\\001\\370\\001\\001\'))\n_HISTOGRAMPROTO.fields_by_name[\'bucket_limit\'].has_options = True\n_HISTOGRAMPROTO.fields_by_name[\'bucket_limit\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_HISTOGRAMPROTO.fields_by_name[\'bucket\'].has_options = True\n_HISTOGRAMPROTO.fields_by_name[\'bucket\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/summary_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/tensor_description_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/tensor_description.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import allocation_description_pb2 as tensorflow_dot_core_dot_framework_dot_allocation__description__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/tensor_description.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n2tensorflow/core/framework/tensor_description.proto\\x12\\ntensorflow\\x1a%tensorflow/core/framework/types.proto\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a\\x36tensorflow/core/framework/allocation_description.proto\\""\\xa8\\x01\\n\\x11TensorDescription\\x12#\\n\\x05\\x64type\\x18\\x01 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12+\\n\\x05shape\\x18\\x02 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\x12\\x41\\n\\x16\\x61llocation_description\\x18\\x04 \\x01(\\x0b\\x32!.tensorflow.AllocationDescriptionB8\\n\\x18org.tensorflow.frameworkB\\x17TensorDescriptionProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_allocation__description__pb2.DESCRIPTOR,])\n\n\n\n\n_TENSORDESCRIPTION = _descriptor.Descriptor(\n  name=\'TensorDescription\',\n  full_name=\'tensorflow.TensorDescription\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.TensorDescription.dtype\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.TensorDescription.shape\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocation_description\', full_name=\'tensorflow.TensorDescription.allocation_description\', index=2,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=208,\n  serialized_end=376,\n)\n\n_TENSORDESCRIPTION.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_TENSORDESCRIPTION.fields_by_name[\'shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_TENSORDESCRIPTION.fields_by_name[\'allocation_description\'].message_type = tensorflow_dot_core_dot_framework_dot_allocation__description__pb2._ALLOCATIONDESCRIPTION\nDESCRIPTOR.message_types_by_name[\'TensorDescription\'] = _TENSORDESCRIPTION\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTensorDescription = _reflection.GeneratedProtocolMessageType(\'TensorDescription\', (_message.Message,), dict(\n  DESCRIPTOR = _TENSORDESCRIPTION,\n  __module__ = \'tensorflow.core.framework.tensor_description_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TensorDescription)\n  ))\n_sym_db.RegisterMessage(TensorDescription)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\027TensorDescriptionProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/tensor_description_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/tensor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/tensor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/tensor.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n&tensorflow/core/framework/tensor.proto\\x12\\ntensorflow\\x1a/tensorflow/core/framework/resource_handle.proto\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a%tensorflow/core/framework/types.proto\\""\\x8c\\x04\\n\\x0bTensorProto\\x12#\\n\\x05\\x64type\\x18\\x01 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12\\x32\\n\\x0ctensor_shape\\x18\\x02 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\x12\\x16\\n\\x0eversion_number\\x18\\x03 \\x01(\\x05\\x12\\x16\\n\\x0etensor_content\\x18\\x04 \\x01(\\x0c\\x12\\x14\\n\\x08half_val\\x18\\r \\x03(\\x05\\x42\\x02\\x10\\x01\\x12\\x15\\n\\tfloat_val\\x18\\x05 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x16\\n\\ndouble_val\\x18\\x06 \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x13\\n\\x07int_val\\x18\\x07 \\x03(\\x05\\x42\\x02\\x10\\x01\\x12\\x12\\n\\nstring_val\\x18\\x08 \\x03(\\x0c\\x12\\x18\\n\\x0cscomplex_val\\x18\\t \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x15\\n\\tint64_val\\x18\\n \\x03(\\x03\\x42\\x02\\x10\\x01\\x12\\x14\\n\\x08\\x62ool_val\\x18\\x0b \\x03(\\x08\\x42\\x02\\x10\\x01\\x12\\x18\\n\\x0c\\x64\\x63omplex_val\\x18\\x0c \\x03(\\x01\\x42\\x02\\x10\\x01\\x12<\\n\\x13resource_handle_val\\x18\\x0e \\x03(\\x0b\\x32\\x1f.tensorflow.ResourceHandleProto\\x12\\x37\\n\\x0bvariant_val\\x18\\x0f \\x03(\\x0b\\x32\\"".tensorflow.VariantTensorDataProto\\x12\\x16\\n\\nuint32_val\\x18\\x10 \\x03(\\rB\\x02\\x10\\x01\\x12\\x16\\n\\nuint64_val\\x18\\x11 \\x03(\\x04\\x42\\x02\\x10\\x01\\""g\\n\\x16VariantTensorDataProto\\x12\\x11\\n\\ttype_name\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08metadata\\x18\\x02 \\x01(\\x0c\\x12(\\n\\x07tensors\\x18\\x03 \\x03(\\x0b\\x32\\x17.tensorflow.TensorProtoB-\\n\\x18org.tensorflow.frameworkB\\x0cTensorProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_resource__handle__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n\n\n\n\n_TENSORPROTO = _descriptor.Descriptor(\n  name=\'TensorProto\',\n  full_name=\'tensorflow.TensorProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.TensorProto.dtype\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor_shape\', full_name=\'tensorflow.TensorProto.tensor_shape\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'version_number\', full_name=\'tensorflow.TensorProto.version_number\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor_content\', full_name=\'tensorflow.TensorProto.tensor_content\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'half_val\', full_name=\'tensorflow.TensorProto.half_val\', index=4,\n      number=13, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'float_val\', full_name=\'tensorflow.TensorProto.float_val\', index=5,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'double_val\', full_name=\'tensorflow.TensorProto.double_val\', index=6,\n      number=6, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'int_val\', full_name=\'tensorflow.TensorProto.int_val\', index=7,\n      number=7, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'string_val\', full_name=\'tensorflow.TensorProto.string_val\', index=8,\n      number=8, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scomplex_val\', full_name=\'tensorflow.TensorProto.scomplex_val\', index=9,\n      number=9, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'int64_val\', full_name=\'tensorflow.TensorProto.int64_val\', index=10,\n      number=10, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'bool_val\', full_name=\'tensorflow.TensorProto.bool_val\', index=11,\n      number=11, type=8, cpp_type=7, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'dcomplex_val\', full_name=\'tensorflow.TensorProto.dcomplex_val\', index=12,\n      number=12, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'resource_handle_val\', full_name=\'tensorflow.TensorProto.resource_handle_val\', index=13,\n      number=14, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'variant_val\', full_name=\'tensorflow.TensorProto.variant_val\', index=14,\n      number=15, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'uint32_val\', full_name=\'tensorflow.TensorProto.uint32_val\', index=15,\n      number=16, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'uint64_val\', full_name=\'tensorflow.TensorProto.uint64_val\', index=16,\n      number=17, type=4, cpp_type=4, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=189,\n  serialized_end=713,\n)\n\n\n_VARIANTTENSORDATAPROTO = _descriptor.Descriptor(\n  name=\'VariantTensorDataProto\',\n  full_name=\'tensorflow.VariantTensorDataProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type_name\', full_name=\'tensorflow.VariantTensorDataProto.type_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'metadata\', full_name=\'tensorflow.VariantTensorDataProto.metadata\', index=1,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensors\', full_name=\'tensorflow.VariantTensorDataProto.tensors\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=715,\n  serialized_end=818,\n)\n\n_TENSORPROTO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_TENSORPROTO.fields_by_name[\'tensor_shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_TENSORPROTO.fields_by_name[\'resource_handle_val\'].message_type = tensorflow_dot_core_dot_framework_dot_resource__handle__pb2._RESOURCEHANDLEPROTO\n_TENSORPROTO.fields_by_name[\'variant_val\'].message_type = _VARIANTTENSORDATAPROTO\n_VARIANTTENSORDATAPROTO.fields_by_name[\'tensors\'].message_type = _TENSORPROTO\nDESCRIPTOR.message_types_by_name[\'TensorProto\'] = _TENSORPROTO\nDESCRIPTOR.message_types_by_name[\'VariantTensorDataProto\'] = _VARIANTTENSORDATAPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTensorProto = _reflection.GeneratedProtocolMessageType(\'TensorProto\', (_message.Message,), dict(\n  DESCRIPTOR = _TENSORPROTO,\n  __module__ = \'tensorflow.core.framework.tensor_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TensorProto)\n  ))\n_sym_db.RegisterMessage(TensorProto)\n\nVariantTensorDataProto = _reflection.GeneratedProtocolMessageType(\'VariantTensorDataProto\', (_message.Message,), dict(\n  DESCRIPTOR = _VARIANTTENSORDATAPROTO,\n  __module__ = \'tensorflow.core.framework.tensor_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.VariantTensorDataProto)\n  ))\n_sym_db.RegisterMessage(VariantTensorDataProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\014TensorProtosP\\001\\370\\001\\001\'))\n_TENSORPROTO.fields_by_name[\'half_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'half_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'float_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'float_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'double_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'double_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'int_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'int_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'scomplex_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'scomplex_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'int64_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'int64_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'bool_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'bool_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'dcomplex_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'dcomplex_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'uint32_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'uint32_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_TENSORPROTO.fields_by_name[\'uint64_val\'].has_options = True\n_TENSORPROTO.fields_by_name[\'uint64_val\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/tensor_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/tensor_shape_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/tensor_shape.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/tensor_shape.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\""z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n  name=\'Dim\',\n  full_name=\'tensorflow.TensorShapeProto.Dim\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'size\', full_name=\'tensorflow.TensorShapeProto.Dim.size\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.TensorShapeProto.Dim.name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=149,\n  serialized_end=182,\n)\n\n_TENSORSHAPEPROTO = _descriptor.Descriptor(\n  name=\'TensorShapeProto\',\n  full_name=\'tensorflow.TensorShapeProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dim\', full_name=\'tensorflow.TensorShapeProto.dim\', index=0,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'unknown_rank\', full_name=\'tensorflow.TensorShapeProto.unknown_rank\', index=1,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TENSORSHAPEPROTO_DIM, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=60,\n  serialized_end=182,\n)\n\n_TENSORSHAPEPROTO_DIM.containing_type = _TENSORSHAPEPROTO\n_TENSORSHAPEPROTO.fields_by_name[\'dim\'].message_type = _TENSORSHAPEPROTO_DIM\nDESCRIPTOR.message_types_by_name[\'TensorShapeProto\'] = _TENSORSHAPEPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTensorShapeProto = _reflection.GeneratedProtocolMessageType(\'TensorShapeProto\', (_message.Message,), dict(\n\n  Dim = _reflection.GeneratedProtocolMessageType(\'Dim\', (_message.Message,), dict(\n    DESCRIPTOR = _TENSORSHAPEPROTO_DIM,\n    __module__ = \'tensorflow.core.framework.tensor_shape_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.TensorShapeProto.Dim)\n    ))\n  ,\n  DESCRIPTOR = _TENSORSHAPEPROTO,\n  __module__ = \'tensorflow.core.framework.tensor_shape_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TensorShapeProto)\n  ))\n_sym_db.RegisterMessage(TensorShapeProto)\n_sym_db.RegisterMessage(TensorShapeProto.Dim)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\021TensorShapeProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/tensor_shape_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/tensor_slice_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/tensor_slice.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/tensor_slice.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n,tensorflow/core/framework/tensor_slice.proto\\x12\\ntensorflow\\""\\x80\\x01\\n\\x10TensorSliceProto\\x12\\x33\\n\\x06\\x65xtent\\x18\\x01 \\x03(\\x0b\\x32#.tensorflow.TensorSliceProto.Extent\\x1a\\x37\\n\\x06\\x45xtent\\x12\\r\\n\\x05start\\x18\\x01 \\x01(\\x03\\x12\\x10\\n\\x06length\\x18\\x02 \\x01(\\x03H\\x00\\x42\\x0c\\n\\nhas_lengthB2\\n\\x18org.tensorflow.frameworkB\\x11TensorSliceProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_TENSORSLICEPROTO_EXTENT = _descriptor.Descriptor(\n  name=\'Extent\',\n  full_name=\'tensorflow.TensorSliceProto.Extent\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'start\', full_name=\'tensorflow.TensorSliceProto.Extent.start\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'length\', full_name=\'tensorflow.TensorSliceProto.Extent.length\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'has_length\', full_name=\'tensorflow.TensorSliceProto.Extent.has_length\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=134,\n  serialized_end=189,\n)\n\n_TENSORSLICEPROTO = _descriptor.Descriptor(\n  name=\'TensorSliceProto\',\n  full_name=\'tensorflow.TensorSliceProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'extent\', full_name=\'tensorflow.TensorSliceProto.extent\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TENSORSLICEPROTO_EXTENT, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=61,\n  serialized_end=189,\n)\n\n_TENSORSLICEPROTO_EXTENT.containing_type = _TENSORSLICEPROTO\n_TENSORSLICEPROTO_EXTENT.oneofs_by_name[\'has_length\'].fields.append(\n  _TENSORSLICEPROTO_EXTENT.fields_by_name[\'length\'])\n_TENSORSLICEPROTO_EXTENT.fields_by_name[\'length\'].containing_oneof = _TENSORSLICEPROTO_EXTENT.oneofs_by_name[\'has_length\']\n_TENSORSLICEPROTO.fields_by_name[\'extent\'].message_type = _TENSORSLICEPROTO_EXTENT\nDESCRIPTOR.message_types_by_name[\'TensorSliceProto\'] = _TENSORSLICEPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTensorSliceProto = _reflection.GeneratedProtocolMessageType(\'TensorSliceProto\', (_message.Message,), dict(\n\n  Extent = _reflection.GeneratedProtocolMessageType(\'Extent\', (_message.Message,), dict(\n    DESCRIPTOR = _TENSORSLICEPROTO_EXTENT,\n    __module__ = \'tensorflow.core.framework.tensor_slice_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.TensorSliceProto.Extent)\n    ))\n  ,\n  DESCRIPTOR = _TENSORSLICEPROTO,\n  __module__ = \'tensorflow.core.framework.tensor_slice_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TensorSliceProto)\n  ))\n_sym_db.RegisterMessage(TensorSliceProto)\n_sym_db.RegisterMessage(TensorSliceProto.Extent)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\021TensorSliceProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/tensor_slice_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/types_pb2.py,0,"b""# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/types.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='tensorflow/core/framework/types.proto',\n  package='tensorflow',\n  syntax='proto3',\n  serialized_pb=_b('\\n%tensorflow/core/framework/types.proto\\x12\\ntensorflow*\\xaa\\x06\\n\\x08\\x44\\x61taType\\x12\\x0e\\n\\nDT_INVALID\\x10\\x00\\x12\\x0c\\n\\x08\\x44T_FLOAT\\x10\\x01\\x12\\r\\n\\tDT_DOUBLE\\x10\\x02\\x12\\x0c\\n\\x08\\x44T_INT32\\x10\\x03\\x12\\x0c\\n\\x08\\x44T_UINT8\\x10\\x04\\x12\\x0c\\n\\x08\\x44T_INT16\\x10\\x05\\x12\\x0b\\n\\x07\\x44T_INT8\\x10\\x06\\x12\\r\\n\\tDT_STRING\\x10\\x07\\x12\\x10\\n\\x0c\\x44T_COMPLEX64\\x10\\x08\\x12\\x0c\\n\\x08\\x44T_INT64\\x10\\t\\x12\\x0b\\n\\x07\\x44T_BOOL\\x10\\n\\x12\\x0c\\n\\x08\\x44T_QINT8\\x10\\x0b\\x12\\r\\n\\tDT_QUINT8\\x10\\x0c\\x12\\r\\n\\tDT_QINT32\\x10\\r\\x12\\x0f\\n\\x0b\\x44T_BFLOAT16\\x10\\x0e\\x12\\r\\n\\tDT_QINT16\\x10\\x0f\\x12\\x0e\\n\\nDT_QUINT16\\x10\\x10\\x12\\r\\n\\tDT_UINT16\\x10\\x11\\x12\\x11\\n\\rDT_COMPLEX128\\x10\\x12\\x12\\x0b\\n\\x07\\x44T_HALF\\x10\\x13\\x12\\x0f\\n\\x0b\\x44T_RESOURCE\\x10\\x14\\x12\\x0e\\n\\nDT_VARIANT\\x10\\x15\\x12\\r\\n\\tDT_UINT32\\x10\\x16\\x12\\r\\n\\tDT_UINT64\\x10\\x17\\x12\\x10\\n\\x0c\\x44T_FLOAT_REF\\x10\\x65\\x12\\x11\\n\\rDT_DOUBLE_REF\\x10\\x66\\x12\\x10\\n\\x0c\\x44T_INT32_REF\\x10g\\x12\\x10\\n\\x0c\\x44T_UINT8_REF\\x10h\\x12\\x10\\n\\x0c\\x44T_INT16_REF\\x10i\\x12\\x0f\\n\\x0b\\x44T_INT8_REF\\x10j\\x12\\x11\\n\\rDT_STRING_REF\\x10k\\x12\\x14\\n\\x10\\x44T_COMPLEX64_REF\\x10l\\x12\\x10\\n\\x0c\\x44T_INT64_REF\\x10m\\x12\\x0f\\n\\x0b\\x44T_BOOL_REF\\x10n\\x12\\x10\\n\\x0c\\x44T_QINT8_REF\\x10o\\x12\\x11\\n\\rDT_QUINT8_REF\\x10p\\x12\\x11\\n\\rDT_QINT32_REF\\x10q\\x12\\x13\\n\\x0f\\x44T_BFLOAT16_REF\\x10r\\x12\\x11\\n\\rDT_QINT16_REF\\x10s\\x12\\x12\\n\\x0e\\x44T_QUINT16_REF\\x10t\\x12\\x11\\n\\rDT_UINT16_REF\\x10u\\x12\\x15\\n\\x11\\x44T_COMPLEX128_REF\\x10v\\x12\\x0f\\n\\x0b\\x44T_HALF_REF\\x10w\\x12\\x13\\n\\x0f\\x44T_RESOURCE_REF\\x10x\\x12\\x12\\n\\x0e\\x44T_VARIANT_REF\\x10y\\x12\\x11\\n\\rDT_UINT32_REF\\x10z\\x12\\x11\\n\\rDT_UINT64_REF\\x10{B,\\n\\x18org.tensorflow.frameworkB\\x0bTypesProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\n)\n\n_DATATYPE = _descriptor.EnumDescriptor(\n  name='DataType',\n  full_name='tensorflow.DataType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='DT_INVALID', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_FLOAT', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_DOUBLE', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT32', index=3, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT8', index=4, number=4,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT16', index=5, number=5,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT8', index=6, number=6,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_STRING', index=7, number=7,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX64', index=8, number=8,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT64', index=9, number=9,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BOOL', index=10, number=10,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT8', index=11, number=11,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT8', index=12, number=12,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT32', index=13, number=13,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BFLOAT16', index=14, number=14,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT16', index=15, number=15,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT16', index=16, number=16,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT16', index=17, number=17,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX128', index=18, number=18,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_HALF', index=19, number=19,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_RESOURCE', index=20, number=20,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_VARIANT', index=21, number=21,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT32', index=22, number=22,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT64', index=23, number=23,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_FLOAT_REF', index=24, number=101,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_DOUBLE_REF', index=25, number=102,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT32_REF', index=26, number=103,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT8_REF', index=27, number=104,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT16_REF', index=28, number=105,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT8_REF', index=29, number=106,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_STRING_REF', index=30, number=107,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX64_REF', index=31, number=108,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT64_REF', index=32, number=109,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BOOL_REF', index=33, number=110,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT8_REF', index=34, number=111,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT8_REF', index=35, number=112,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT32_REF', index=36, number=113,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BFLOAT16_REF', index=37, number=114,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT16_REF', index=38, number=115,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT16_REF', index=39, number=116,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT16_REF', index=40, number=117,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX128_REF', index=41, number=118,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_HALF_REF', index=42, number=119,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_RESOURCE_REF', index=43, number=120,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_VARIANT_REF', index=44, number=121,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT32_REF', index=45, number=122,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT64_REF', index=46, number=123,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=54,\n  serialized_end=864,\n)\n_sym_db.RegisterEnumDescriptor(_DATATYPE)\n\nDataType = enum_type_wrapper.EnumTypeWrapper(_DATATYPE)\nDT_INVALID = 0\nDT_FLOAT = 1\nDT_DOUBLE = 2\nDT_INT32 = 3\nDT_UINT8 = 4\nDT_INT16 = 5\nDT_INT8 = 6\nDT_STRING = 7\nDT_COMPLEX64 = 8\nDT_INT64 = 9\nDT_BOOL = 10\nDT_QINT8 = 11\nDT_QUINT8 = 12\nDT_QINT32 = 13\nDT_BFLOAT16 = 14\nDT_QINT16 = 15\nDT_QUINT16 = 16\nDT_UINT16 = 17\nDT_COMPLEX128 = 18\nDT_HALF = 19\nDT_RESOURCE = 20\nDT_VARIANT = 21\nDT_UINT32 = 22\nDT_UINT64 = 23\nDT_FLOAT_REF = 101\nDT_DOUBLE_REF = 102\nDT_INT32_REF = 103\nDT_UINT8_REF = 104\nDT_INT16_REF = 105\nDT_INT8_REF = 106\nDT_STRING_REF = 107\nDT_COMPLEX64_REF = 108\nDT_INT64_REF = 109\nDT_BOOL_REF = 110\nDT_QINT8_REF = 111\nDT_QUINT8_REF = 112\nDT_QINT32_REF = 113\nDT_BFLOAT16_REF = 114\nDT_QINT16_REF = 115\nDT_QUINT16_REF = 116\nDT_UINT16_REF = 117\nDT_COMPLEX128_REF = 118\nDT_HALF_REF = 119\nDT_RESOURCE_REF = 120\nDT_VARIANT_REF = 121\nDT_UINT32_REF = 122\nDT_UINT64_REF = 123\n\n\nDESCRIPTOR.enum_types_by_name['DataType'] = _DATATYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\\n\\030org.tensorflow.frameworkB\\013TypesProtosP\\001\\370\\001\\001'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n"""
libs/pipeline_model/tensorflow/core/framework/types_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/variable_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/variable.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/variable.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(tensorflow/core/framework/variable.proto\\x12\\ntensorflow\\""\\xc1\\x01\\n\\x0bVariableDef\\x12\\x15\\n\\rvariable_name\\x18\\x01 \\x01(\\t\\x12\\x1a\\n\\x12initial_value_name\\x18\\x06 \\x01(\\t\\x12\\x18\\n\\x10initializer_name\\x18\\x02 \\x01(\\t\\x12\\x15\\n\\rsnapshot_name\\x18\\x03 \\x01(\\t\\x12\\x39\\n\\x13save_slice_info_def\\x18\\x04 \\x01(\\x0b\\x32\\x1c.tensorflow.SaveSliceInfoDef\\x12\\x13\\n\\x0bis_resource\\x18\\x05 \\x01(\\x08\\""`\\n\\x10SaveSliceInfoDef\\x12\\x11\\n\\tfull_name\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nfull_shape\\x18\\x02 \\x03(\\x03\\x12\\x12\\n\\nvar_offset\\x18\\x03 \\x03(\\x03\\x12\\x11\\n\\tvar_shape\\x18\\x04 \\x03(\\x03\\x42/\\n\\x18org.tensorflow.frameworkB\\x0eVariableProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_VARIABLEDEF = _descriptor.Descriptor(\n  name=\'VariableDef\',\n  full_name=\'tensorflow.VariableDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'variable_name\', full_name=\'tensorflow.VariableDef.variable_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'initial_value_name\', full_name=\'tensorflow.VariableDef.initial_value_name\', index=1,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'initializer_name\', full_name=\'tensorflow.VariableDef.initializer_name\', index=2,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_name\', full_name=\'tensorflow.VariableDef.snapshot_name\', index=3,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'save_slice_info_def\', full_name=\'tensorflow.VariableDef.save_slice_info_def\', index=4,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_resource\', full_name=\'tensorflow.VariableDef.is_resource\', index=5,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=57,\n  serialized_end=250,\n)\n\n\n_SAVESLICEINFODEF = _descriptor.Descriptor(\n  name=\'SaveSliceInfoDef\',\n  full_name=\'tensorflow.SaveSliceInfoDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'full_name\', full_name=\'tensorflow.SaveSliceInfoDef.full_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'full_shape\', full_name=\'tensorflow.SaveSliceInfoDef.full_shape\', index=1,\n      number=2, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'var_offset\', full_name=\'tensorflow.SaveSliceInfoDef.var_offset\', index=2,\n      number=3, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'var_shape\', full_name=\'tensorflow.SaveSliceInfoDef.var_shape\', index=3,\n      number=4, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=252,\n  serialized_end=348,\n)\n\n_VARIABLEDEF.fields_by_name[\'save_slice_info_def\'].message_type = _SAVESLICEINFODEF\nDESCRIPTOR.message_types_by_name[\'VariableDef\'] = _VARIABLEDEF\nDESCRIPTOR.message_types_by_name[\'SaveSliceInfoDef\'] = _SAVESLICEINFODEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nVariableDef = _reflection.GeneratedProtocolMessageType(\'VariableDef\', (_message.Message,), dict(\n  DESCRIPTOR = _VARIABLEDEF,\n  __module__ = \'tensorflow.core.framework.variable_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.VariableDef)\n  ))\n_sym_db.RegisterMessage(VariableDef)\n\nSaveSliceInfoDef = _reflection.GeneratedProtocolMessageType(\'SaveSliceInfoDef\', (_message.Message,), dict(\n  DESCRIPTOR = _SAVESLICEINFODEF,\n  __module__ = \'tensorflow.core.framework.variable_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.SaveSliceInfoDef)\n  ))\n_sym_db.RegisterMessage(SaveSliceInfoDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\016VariableProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/variable_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/framework/versions_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/framework/versions.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/framework/versions.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(tensorflow/core/framework/versions.proto\\x12\\ntensorflow\\""K\\n\\nVersionDef\\x12\\x10\\n\\x08producer\\x18\\x01 \\x01(\\x05\\x12\\x14\\n\\x0cmin_consumer\\x18\\x02 \\x01(\\x05\\x12\\x15\\n\\rbad_consumers\\x18\\x03 \\x03(\\x05\\x42/\\n\\x18org.tensorflow.frameworkB\\x0eVersionsProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_VERSIONDEF = _descriptor.Descriptor(\n  name=\'VersionDef\',\n  full_name=\'tensorflow.VersionDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'producer\', full_name=\'tensorflow.VersionDef.producer\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_consumer\', full_name=\'tensorflow.VersionDef.min_consumer\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bad_consumers\', full_name=\'tensorflow.VersionDef.bad_consumers\', index=2,\n      number=3, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=56,\n  serialized_end=131,\n)\n\nDESCRIPTOR.message_types_by_name[\'VersionDef\'] = _VERSIONDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nVersionDef = _reflection.GeneratedProtocolMessageType(\'VersionDef\', (_message.Message,), dict(\n  DESCRIPTOR = _VERSIONDEF,\n  __module__ = \'tensorflow.core.framework.versions_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.VersionDef)\n  ))\n_sym_db.RegisterMessage(VersionDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\016VersionsProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/framework/versions_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/__init__.py,0,b''
libs/pipeline_model/tensorflow/core/protobuf/cluster_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/cluster.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/cluster.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n&tensorflow/core/protobuf/cluster.proto\\x12\\ntensorflow\\""r\\n\\x06JobDef\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12,\\n\\x05tasks\\x18\\x02 \\x03(\\x0b\\x32\\x1d.tensorflow.JobDef.TasksEntry\\x1a,\\n\\nTasksEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\""-\\n\\nClusterDef\\x12\\x1f\\n\\x03job\\x18\\x01 \\x03(\\x0b\\x32\\x12.tensorflow.JobDefB0\\n\\x1aorg.tensorflow.distruntimeB\\rClusterProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_JOBDEF_TASKSENTRY = _descriptor.Descriptor(\n  name=\'TasksEntry\',\n  full_name=\'tensorflow.JobDef.TasksEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.JobDef.TasksEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.JobDef.TasksEntry.value\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=124,\n  serialized_end=168,\n)\n\n_JOBDEF = _descriptor.Descriptor(\n  name=\'JobDef\',\n  full_name=\'tensorflow.JobDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.JobDef.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tasks\', full_name=\'tensorflow.JobDef.tasks\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_JOBDEF_TASKSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=54,\n  serialized_end=168,\n)\n\n\n_CLUSTERDEF = _descriptor.Descriptor(\n  name=\'ClusterDef\',\n  full_name=\'tensorflow.ClusterDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'job\', full_name=\'tensorflow.ClusterDef.job\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=170,\n  serialized_end=215,\n)\n\n_JOBDEF_TASKSENTRY.containing_type = _JOBDEF\n_JOBDEF.fields_by_name[\'tasks\'].message_type = _JOBDEF_TASKSENTRY\n_CLUSTERDEF.fields_by_name[\'job\'].message_type = _JOBDEF\nDESCRIPTOR.message_types_by_name[\'JobDef\'] = _JOBDEF\nDESCRIPTOR.message_types_by_name[\'ClusterDef\'] = _CLUSTERDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nJobDef = _reflection.GeneratedProtocolMessageType(\'JobDef\', (_message.Message,), dict(\n\n  TasksEntry = _reflection.GeneratedProtocolMessageType(\'TasksEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _JOBDEF_TASKSENTRY,\n    __module__ = \'tensorflow.core.protobuf.cluster_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.JobDef.TasksEntry)\n    ))\n  ,\n  DESCRIPTOR = _JOBDEF,\n  __module__ = \'tensorflow.core.protobuf.cluster_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.JobDef)\n  ))\n_sym_db.RegisterMessage(JobDef)\n_sym_db.RegisterMessage(JobDef.TasksEntry)\n\nClusterDef = _reflection.GeneratedProtocolMessageType(\'ClusterDef\', (_message.Message,), dict(\n  DESCRIPTOR = _CLUSTERDEF,\n  __module__ = \'tensorflow.core.protobuf.cluster_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ClusterDef)\n  ))\n_sym_db.RegisterMessage(ClusterDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\032org.tensorflow.distruntimeB\\rClusterProtosP\\001\\370\\001\\001\'))\n_JOBDEF_TASKSENTRY.has_options = True\n_JOBDEF_TASKSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/cluster_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/config_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/config.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import cost_graph_pb2 as tensorflow_dot_core_dot_framework_dot_cost__graph__pb2\nfrom tensorflow.core.framework import graph_pb2 as tensorflow_dot_core_dot_framework_dot_graph__pb2\nfrom tensorflow.core.framework import step_stats_pb2 as tensorflow_dot_core_dot_framework_dot_step__stats__pb2\nfrom tensorflow.core.protobuf import debug_pb2 as tensorflow_dot_core_dot_protobuf_dot_debug__pb2\nfrom tensorflow.core.protobuf import cluster_pb2 as tensorflow_dot_core_dot_protobuf_dot_cluster__pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2 as tensorflow_dot_core_dot_protobuf_dot_rewriter__config__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/config.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow/core/protobuf/config.proto\\x12\\ntensorflow\\x1a*tensorflow/core/framework/cost_graph.proto\\x1a%tensorflow/core/framework/graph.proto\\x1a*tensorflow/core/framework/step_stats.proto\\x1a$tensorflow/core/protobuf/debug.proto\\x1a&tensorflow/core/protobuf/cluster.proto\\x1a.tensorflow/core/protobuf/rewriter_config.proto\\""\\xcd\\x03\\n\\nGPUOptions\\x12\\\'\\n\\x1fper_process_gpu_memory_fraction\\x18\\x01 \\x01(\\x01\\x12\\x16\\n\\x0e\\x61llocator_type\\x18\\x02 \\x01(\\t\\x12\\x1f\\n\\x17\\x64\\x65\\x66\\x65rred_deletion_bytes\\x18\\x03 \\x01(\\x03\\x12\\x14\\n\\x0c\\x61llow_growth\\x18\\x04 \\x01(\\x08\\x12\\x1b\\n\\x13visible_device_list\\x18\\x05 \\x01(\\t\\x12\\""\\n\\x1apolling_active_delay_usecs\\x18\\x06 \\x01(\\x05\\x12$\\n\\x1cpolling_inactive_delay_msecs\\x18\\x07 \\x01(\\x05\\x12\\x1c\\n\\x14\\x66orce_gpu_compatible\\x18\\x08 \\x01(\\x08\\x12\\x39\\n\\x0c\\x65xperimental\\x18\\t \\x01(\\x0b\\x32#.tensorflow.GPUOptions.Experimental\\x1a\\x86\\x01\\n\\x0c\\x45xperimental\\x12K\\n\\x0fvirtual_devices\\x18\\x01 \\x03(\\x0b\\x32\\x32.tensorflow.GPUOptions.Experimental.VirtualDevices\\x1a)\\n\\x0eVirtualDevices\\x12\\x17\\n\\x0fmemory_limit_mb\\x18\\x01 \\x03(\\x02\\""\\x85\\x03\\n\\x10OptimizerOptions\\x12+\\n#do_common_subexpression_elimination\\x18\\x01 \\x01(\\x08\\x12\\x1b\\n\\x13\\x64o_constant_folding\\x18\\x02 \\x01(\\x08\\x12$\\n\\x1cmax_folded_constant_in_bytes\\x18\\x06 \\x01(\\x03\\x12\\x1c\\n\\x14\\x64o_function_inlining\\x18\\x04 \\x01(\\x08\\x12\\x35\\n\\topt_level\\x18\\x03 \\x01(\\x0e\\x32\\"".tensorflow.OptimizerOptions.Level\\x12\\x45\\n\\x10global_jit_level\\x18\\x05 \\x01(\\x0e\\x32+.tensorflow.OptimizerOptions.GlobalJitLevel\\"" \\n\\x05Level\\x12\\x06\\n\\x02L1\\x10\\x00\\x12\\x0f\\n\\x02L0\\x10\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\""C\\n\\x0eGlobalJitLevel\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\x10\\n\\x03OFF\\x10\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x12\\x08\\n\\x04ON_1\\x10\\x01\\x12\\x08\\n\\x04ON_2\\x10\\x02\\""\\xee\\x02\\n\\x0cGraphOptions\\x12\\x1e\\n\\x16\\x65nable_recv_scheduling\\x18\\x02 \\x01(\\x08\\x12\\x37\\n\\x11optimizer_options\\x18\\x03 \\x01(\\x0b\\x32\\x1c.tensorflow.OptimizerOptions\\x12\\x18\\n\\x10\\x62uild_cost_model\\x18\\x04 \\x01(\\x03\\x12\\x1e\\n\\x16\\x62uild_cost_model_after\\x18\\t \\x01(\\x03\\x12\\x14\\n\\x0cinfer_shapes\\x18\\x05 \\x01(\\x08\\x12\\x1a\\n\\x12place_pruned_graph\\x18\\x06 \\x01(\\x08\\x12 \\n\\x18\\x65nable_bfloat16_sendrecv\\x18\\x07 \\x01(\\x08\\x12\\x15\\n\\rtimeline_step\\x18\\x08 \\x01(\\x05\\x12\\x33\\n\\x0frewrite_options\\x18\\n \\x01(\\x0b\\x32\\x1a.tensorflow.RewriterConfigJ\\x04\\x08\\x01\\x10\\x02R%skip_common_subexpression_elimination\\""A\\n\\x15ThreadPoolOptionProto\\x12\\x13\\n\\x0bnum_threads\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0bglobal_name\\x18\\x02 \\x01(\\t\\""2\\n\\nRPCOptions\\x12$\\n\\x1cuse_rpc_for_inprocess_master\\x18\\x01 \\x01(\\x08\\""\\x9d\\x05\\n\\x0b\\x43onfigProto\\x12>\\n\\x0c\\x64\\x65vice_count\\x18\\x01 \\x03(\\x0b\\x32(.tensorflow.ConfigProto.DeviceCountEntry\\x12$\\n\\x1cintra_op_parallelism_threads\\x18\\x02 \\x01(\\x05\\x12$\\n\\x1cinter_op_parallelism_threads\\x18\\x05 \\x01(\\x05\\x12\\x1f\\n\\x17use_per_session_threads\\x18\\t \\x01(\\x08\\x12G\\n\\x1csession_inter_op_thread_pool\\x18\\x0c \\x03(\\x0b\\x32!.tensorflow.ThreadPoolOptionProto\\x12\\x18\\n\\x10placement_period\\x18\\x03 \\x01(\\x05\\x12\\x16\\n\\x0e\\x64\\x65vice_filters\\x18\\x04 \\x03(\\t\\x12+\\n\\x0bgpu_options\\x18\\x06 \\x01(\\x0b\\x32\\x16.tensorflow.GPUOptions\\x12\\x1c\\n\\x14\\x61llow_soft_placement\\x18\\x07 \\x01(\\x08\\x12\\x1c\\n\\x14log_device_placement\\x18\\x08 \\x01(\\x08\\x12/\\n\\rgraph_options\\x18\\n \\x01(\\x0b\\x32\\x18.tensorflow.GraphOptions\\x12\\x1f\\n\\x17operation_timeout_in_ms\\x18\\x0b \\x01(\\x03\\x12+\\n\\x0brpc_options\\x18\\r \\x01(\\x0b\\x32\\x16.tensorflow.RPCOptions\\x12+\\n\\x0b\\x63luster_def\\x18\\x0e \\x01(\\x0b\\x32\\x16.tensorflow.ClusterDef\\x12\\x1d\\n\\x15isolate_session_state\\x18\\x0f \\x01(\\x08\\x1a\\x32\\n\\x10\\x44\\x65viceCountEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\""\\xd1\\x02\\n\\nRunOptions\\x12\\x36\\n\\x0btrace_level\\x18\\x01 \\x01(\\x0e\\x32!.tensorflow.RunOptions.TraceLevel\\x12\\x15\\n\\rtimeout_in_ms\\x18\\x02 \\x01(\\x03\\x12\\x1c\\n\\x14inter_op_thread_pool\\x18\\x03 \\x01(\\x05\\x12\\x1f\\n\\x17output_partition_graphs\\x18\\x05 \\x01(\\x08\\x12/\\n\\rdebug_options\\x18\\x06 \\x01(\\x0b\\x32\\x18.tensorflow.DebugOptions\\x12*\\n\\""report_tensor_allocations_upon_oom\\x18\\x07 \\x01(\\x08\\""R\\n\\nTraceLevel\\x12\\x0c\\n\\x08NO_TRACE\\x10\\x00\\x12\\x12\\n\\x0eSOFTWARE_TRACE\\x10\\x01\\x12\\x12\\n\\x0eHARDWARE_TRACE\\x10\\x02\\x12\\x0e\\n\\nFULL_TRACE\\x10\\x03J\\x04\\x08\\x04\\x10\\x05\\""\\x96\\x01\\n\\x0bRunMetadata\\x12)\\n\\nstep_stats\\x18\\x01 \\x01(\\x0b\\x32\\x15.tensorflow.StepStats\\x12,\\n\\ncost_graph\\x18\\x02 \\x01(\\x0b\\x32\\x18.tensorflow.CostGraphDef\\x12.\\n\\x10partition_graphs\\x18\\x03 \\x03(\\x0b\\x32\\x14.tensorflow.GraphDefB-\\n\\x18org.tensorflow.frameworkB\\x0c\\x43onfigProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_cost__graph__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_graph__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_step__stats__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_debug__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_cluster__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_rewriter__config__pb2.DESCRIPTOR,])\n\n\n\n_OPTIMIZEROPTIONS_LEVEL = _descriptor.EnumDescriptor(\n  name=\'Level\',\n  full_name=\'tensorflow.OptimizerOptions.Level\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'L1\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'L0\', index=1, number=-1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1059,\n  serialized_end=1091,\n)\n_sym_db.RegisterEnumDescriptor(_OPTIMIZEROPTIONS_LEVEL)\n\n_OPTIMIZEROPTIONS_GLOBALJITLEVEL = _descriptor.EnumDescriptor(\n  name=\'GlobalJitLevel\',\n  full_name=\'tensorflow.OptimizerOptions.GlobalJitLevel\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'OFF\', index=1, number=-1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ON_1\', index=2, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ON_2\', index=3, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1093,\n  serialized_end=1160,\n)\n_sym_db.RegisterEnumDescriptor(_OPTIMIZEROPTIONS_GLOBALJITLEVEL)\n\n_RUNOPTIONS_TRACELEVEL = _descriptor.EnumDescriptor(\n  name=\'TraceLevel\',\n  full_name=\'tensorflow.RunOptions.TraceLevel\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'NO_TRACE\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SOFTWARE_TRACE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HARDWARE_TRACE\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FULL_TRACE\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2572,\n  serialized_end=2654,\n)\n_sym_db.RegisterEnumDescriptor(_RUNOPTIONS_TRACELEVEL)\n\n\n_GPUOPTIONS_EXPERIMENTAL_VIRTUALDEVICES = _descriptor.Descriptor(\n  name=\'VirtualDevices\',\n  full_name=\'tensorflow.GPUOptions.Experimental.VirtualDevices\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'memory_limit_mb\', full_name=\'tensorflow.GPUOptions.Experimental.VirtualDevices.memory_limit_mb\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=727,\n  serialized_end=768,\n)\n\n_GPUOPTIONS_EXPERIMENTAL = _descriptor.Descriptor(\n  name=\'Experimental\',\n  full_name=\'tensorflow.GPUOptions.Experimental\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'virtual_devices\', full_name=\'tensorflow.GPUOptions.Experimental.virtual_devices\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_GPUOPTIONS_EXPERIMENTAL_VIRTUALDEVICES, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=634,\n  serialized_end=768,\n)\n\n_GPUOPTIONS = _descriptor.Descriptor(\n  name=\'GPUOptions\',\n  full_name=\'tensorflow.GPUOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'per_process_gpu_memory_fraction\', full_name=\'tensorflow.GPUOptions.per_process_gpu_memory_fraction\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allocator_type\', full_name=\'tensorflow.GPUOptions.allocator_type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'deferred_deletion_bytes\', full_name=\'tensorflow.GPUOptions.deferred_deletion_bytes\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allow_growth\', full_name=\'tensorflow.GPUOptions.allow_growth\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'visible_device_list\', full_name=\'tensorflow.GPUOptions.visible_device_list\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'polling_active_delay_usecs\', full_name=\'tensorflow.GPUOptions.polling_active_delay_usecs\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'polling_inactive_delay_msecs\', full_name=\'tensorflow.GPUOptions.polling_inactive_delay_msecs\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_gpu_compatible\', full_name=\'tensorflow.GPUOptions.force_gpu_compatible\', index=7,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'experimental\', full_name=\'tensorflow.GPUOptions.experimental\', index=8,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_GPUOPTIONS_EXPERIMENTAL, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=307,\n  serialized_end=768,\n)\n\n\n_OPTIMIZEROPTIONS = _descriptor.Descriptor(\n  name=\'OptimizerOptions\',\n  full_name=\'tensorflow.OptimizerOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'do_common_subexpression_elimination\', full_name=\'tensorflow.OptimizerOptions.do_common_subexpression_elimination\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'do_constant_folding\', full_name=\'tensorflow.OptimizerOptions.do_constant_folding\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_folded_constant_in_bytes\', full_name=\'tensorflow.OptimizerOptions.max_folded_constant_in_bytes\', index=2,\n      number=6, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'do_function_inlining\', full_name=\'tensorflow.OptimizerOptions.do_function_inlining\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'opt_level\', full_name=\'tensorflow.OptimizerOptions.opt_level\', index=4,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'global_jit_level\', full_name=\'tensorflow.OptimizerOptions.global_jit_level\', index=5,\n      number=5, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _OPTIMIZEROPTIONS_LEVEL,\n    _OPTIMIZEROPTIONS_GLOBALJITLEVEL,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=771,\n  serialized_end=1160,\n)\n\n\n_GRAPHOPTIONS = _descriptor.Descriptor(\n  name=\'GraphOptions\',\n  full_name=\'tensorflow.GraphOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'enable_recv_scheduling\', full_name=\'tensorflow.GraphOptions.enable_recv_scheduling\', index=0,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'optimizer_options\', full_name=\'tensorflow.GraphOptions.optimizer_options\', index=1,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'build_cost_model\', full_name=\'tensorflow.GraphOptions.build_cost_model\', index=2,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'build_cost_model_after\', full_name=\'tensorflow.GraphOptions.build_cost_model_after\', index=3,\n      number=9, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'infer_shapes\', full_name=\'tensorflow.GraphOptions.infer_shapes\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'place_pruned_graph\', full_name=\'tensorflow.GraphOptions.place_pruned_graph\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'enable_bfloat16_sendrecv\', full_name=\'tensorflow.GraphOptions.enable_bfloat16_sendrecv\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'timeline_step\', full_name=\'tensorflow.GraphOptions.timeline_step\', index=7,\n      number=8, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rewrite_options\', full_name=\'tensorflow.GraphOptions.rewrite_options\', index=8,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1163,\n  serialized_end=1529,\n)\n\n\n_THREADPOOLOPTIONPROTO = _descriptor.Descriptor(\n  name=\'ThreadPoolOptionProto\',\n  full_name=\'tensorflow.ThreadPoolOptionProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_threads\', full_name=\'tensorflow.ThreadPoolOptionProto.num_threads\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'global_name\', full_name=\'tensorflow.ThreadPoolOptionProto.global_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1531,\n  serialized_end=1596,\n)\n\n\n_RPCOPTIONS = _descriptor.Descriptor(\n  name=\'RPCOptions\',\n  full_name=\'tensorflow.RPCOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'use_rpc_for_inprocess_master\', full_name=\'tensorflow.RPCOptions.use_rpc_for_inprocess_master\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1598,\n  serialized_end=1648,\n)\n\n\n_CONFIGPROTO_DEVICECOUNTENTRY = _descriptor.Descriptor(\n  name=\'DeviceCountEntry\',\n  full_name=\'tensorflow.ConfigProto.DeviceCountEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.ConfigProto.DeviceCountEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.ConfigProto.DeviceCountEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2270,\n  serialized_end=2320,\n)\n\n_CONFIGPROTO = _descriptor.Descriptor(\n  name=\'ConfigProto\',\n  full_name=\'tensorflow.ConfigProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'device_count\', full_name=\'tensorflow.ConfigProto.device_count\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'intra_op_parallelism_threads\', full_name=\'tensorflow.ConfigProto.intra_op_parallelism_threads\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inter_op_parallelism_threads\', full_name=\'tensorflow.ConfigProto.inter_op_parallelism_threads\', index=2,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_per_session_threads\', full_name=\'tensorflow.ConfigProto.use_per_session_threads\', index=3,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'session_inter_op_thread_pool\', full_name=\'tensorflow.ConfigProto.session_inter_op_thread_pool\', index=4,\n      number=12, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'placement_period\', full_name=\'tensorflow.ConfigProto.placement_period\', index=5,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_filters\', full_name=\'tensorflow.ConfigProto.device_filters\', index=6,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gpu_options\', full_name=\'tensorflow.ConfigProto.gpu_options\', index=7,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'allow_soft_placement\', full_name=\'tensorflow.ConfigProto.allow_soft_placement\', index=8,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'log_device_placement\', full_name=\'tensorflow.ConfigProto.log_device_placement\', index=9,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_options\', full_name=\'tensorflow.ConfigProto.graph_options\', index=10,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'operation_timeout_in_ms\', full_name=\'tensorflow.ConfigProto.operation_timeout_in_ms\', index=11,\n      number=11, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rpc_options\', full_name=\'tensorflow.ConfigProto.rpc_options\', index=12,\n      number=13, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cluster_def\', full_name=\'tensorflow.ConfigProto.cluster_def\', index=13,\n      number=14, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'isolate_session_state\', full_name=\'tensorflow.ConfigProto.isolate_session_state\', index=14,\n      number=15, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_CONFIGPROTO_DEVICECOUNTENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1651,\n  serialized_end=2320,\n)\n\n\n_RUNOPTIONS = _descriptor.Descriptor(\n  name=\'RunOptions\',\n  full_name=\'tensorflow.RunOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'trace_level\', full_name=\'tensorflow.RunOptions.trace_level\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'timeout_in_ms\', full_name=\'tensorflow.RunOptions.timeout_in_ms\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inter_op_thread_pool\', full_name=\'tensorflow.RunOptions.inter_op_thread_pool\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_partition_graphs\', full_name=\'tensorflow.RunOptions.output_partition_graphs\', index=3,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_options\', full_name=\'tensorflow.RunOptions.debug_options\', index=4,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'report_tensor_allocations_upon_oom\', full_name=\'tensorflow.RunOptions.report_tensor_allocations_upon_oom\', index=5,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _RUNOPTIONS_TRACELEVEL,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2323,\n  serialized_end=2660,\n)\n\n\n_RUNMETADATA = _descriptor.Descriptor(\n  name=\'RunMetadata\',\n  full_name=\'tensorflow.RunMetadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_stats\', full_name=\'tensorflow.RunMetadata.step_stats\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cost_graph\', full_name=\'tensorflow.RunMetadata.cost_graph\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'partition_graphs\', full_name=\'tensorflow.RunMetadata.partition_graphs\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2663,\n  serialized_end=2813,\n)\n\n_GPUOPTIONS_EXPERIMENTAL_VIRTUALDEVICES.containing_type = _GPUOPTIONS_EXPERIMENTAL\n_GPUOPTIONS_EXPERIMENTAL.fields_by_name[\'virtual_devices\'].message_type = _GPUOPTIONS_EXPERIMENTAL_VIRTUALDEVICES\n_GPUOPTIONS_EXPERIMENTAL.containing_type = _GPUOPTIONS\n_GPUOPTIONS.fields_by_name[\'experimental\'].message_type = _GPUOPTIONS_EXPERIMENTAL\n_OPTIMIZEROPTIONS.fields_by_name[\'opt_level\'].enum_type = _OPTIMIZEROPTIONS_LEVEL\n_OPTIMIZEROPTIONS.fields_by_name[\'global_jit_level\'].enum_type = _OPTIMIZEROPTIONS_GLOBALJITLEVEL\n_OPTIMIZEROPTIONS_LEVEL.containing_type = _OPTIMIZEROPTIONS\n_OPTIMIZEROPTIONS_GLOBALJITLEVEL.containing_type = _OPTIMIZEROPTIONS\n_GRAPHOPTIONS.fields_by_name[\'optimizer_options\'].message_type = _OPTIMIZEROPTIONS\n_GRAPHOPTIONS.fields_by_name[\'rewrite_options\'].message_type = tensorflow_dot_core_dot_protobuf_dot_rewriter__config__pb2._REWRITERCONFIG\n_CONFIGPROTO_DEVICECOUNTENTRY.containing_type = _CONFIGPROTO\n_CONFIGPROTO.fields_by_name[\'device_count\'].message_type = _CONFIGPROTO_DEVICECOUNTENTRY\n_CONFIGPROTO.fields_by_name[\'session_inter_op_thread_pool\'].message_type = _THREADPOOLOPTIONPROTO\n_CONFIGPROTO.fields_by_name[\'gpu_options\'].message_type = _GPUOPTIONS\n_CONFIGPROTO.fields_by_name[\'graph_options\'].message_type = _GRAPHOPTIONS\n_CONFIGPROTO.fields_by_name[\'rpc_options\'].message_type = _RPCOPTIONS\n_CONFIGPROTO.fields_by_name[\'cluster_def\'].message_type = tensorflow_dot_core_dot_protobuf_dot_cluster__pb2._CLUSTERDEF\n_RUNOPTIONS.fields_by_name[\'trace_level\'].enum_type = _RUNOPTIONS_TRACELEVEL\n_RUNOPTIONS.fields_by_name[\'debug_options\'].message_type = tensorflow_dot_core_dot_protobuf_dot_debug__pb2._DEBUGOPTIONS\n_RUNOPTIONS_TRACELEVEL.containing_type = _RUNOPTIONS\n_RUNMETADATA.fields_by_name[\'step_stats\'].message_type = tensorflow_dot_core_dot_framework_dot_step__stats__pb2._STEPSTATS\n_RUNMETADATA.fields_by_name[\'cost_graph\'].message_type = tensorflow_dot_core_dot_framework_dot_cost__graph__pb2._COSTGRAPHDEF\n_RUNMETADATA.fields_by_name[\'partition_graphs\'].message_type = tensorflow_dot_core_dot_framework_dot_graph__pb2._GRAPHDEF\nDESCRIPTOR.message_types_by_name[\'GPUOptions\'] = _GPUOPTIONS\nDESCRIPTOR.message_types_by_name[\'OptimizerOptions\'] = _OPTIMIZEROPTIONS\nDESCRIPTOR.message_types_by_name[\'GraphOptions\'] = _GRAPHOPTIONS\nDESCRIPTOR.message_types_by_name[\'ThreadPoolOptionProto\'] = _THREADPOOLOPTIONPROTO\nDESCRIPTOR.message_types_by_name[\'RPCOptions\'] = _RPCOPTIONS\nDESCRIPTOR.message_types_by_name[\'ConfigProto\'] = _CONFIGPROTO\nDESCRIPTOR.message_types_by_name[\'RunOptions\'] = _RUNOPTIONS\nDESCRIPTOR.message_types_by_name[\'RunMetadata\'] = _RUNMETADATA\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nGPUOptions = _reflection.GeneratedProtocolMessageType(\'GPUOptions\', (_message.Message,), dict(\n\n  Experimental = _reflection.GeneratedProtocolMessageType(\'Experimental\', (_message.Message,), dict(\n\n    VirtualDevices = _reflection.GeneratedProtocolMessageType(\'VirtualDevices\', (_message.Message,), dict(\n      DESCRIPTOR = _GPUOPTIONS_EXPERIMENTAL_VIRTUALDEVICES,\n      __module__ = \'tensorflow.core.protobuf.config_pb2\'\n      # @@protoc_insertion_point(class_scope:tensorflow.GPUOptions.Experimental.VirtualDevices)\n      ))\n    ,\n    DESCRIPTOR = _GPUOPTIONS_EXPERIMENTAL,\n    __module__ = \'tensorflow.core.protobuf.config_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.GPUOptions.Experimental)\n    ))\n  ,\n  DESCRIPTOR = _GPUOPTIONS,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.GPUOptions)\n  ))\n_sym_db.RegisterMessage(GPUOptions)\n_sym_db.RegisterMessage(GPUOptions.Experimental)\n_sym_db.RegisterMessage(GPUOptions.Experimental.VirtualDevices)\n\nOptimizerOptions = _reflection.GeneratedProtocolMessageType(\'OptimizerOptions\', (_message.Message,), dict(\n  DESCRIPTOR = _OPTIMIZEROPTIONS,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.OptimizerOptions)\n  ))\n_sym_db.RegisterMessage(OptimizerOptions)\n\nGraphOptions = _reflection.GeneratedProtocolMessageType(\'GraphOptions\', (_message.Message,), dict(\n  DESCRIPTOR = _GRAPHOPTIONS,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.GraphOptions)\n  ))\n_sym_db.RegisterMessage(GraphOptions)\n\nThreadPoolOptionProto = _reflection.GeneratedProtocolMessageType(\'ThreadPoolOptionProto\', (_message.Message,), dict(\n  DESCRIPTOR = _THREADPOOLOPTIONPROTO,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ThreadPoolOptionProto)\n  ))\n_sym_db.RegisterMessage(ThreadPoolOptionProto)\n\nRPCOptions = _reflection.GeneratedProtocolMessageType(\'RPCOptions\', (_message.Message,), dict(\n  DESCRIPTOR = _RPCOPTIONS,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RPCOptions)\n  ))\n_sym_db.RegisterMessage(RPCOptions)\n\nConfigProto = _reflection.GeneratedProtocolMessageType(\'ConfigProto\', (_message.Message,), dict(\n\n  DeviceCountEntry = _reflection.GeneratedProtocolMessageType(\'DeviceCountEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _CONFIGPROTO_DEVICECOUNTENTRY,\n    __module__ = \'tensorflow.core.protobuf.config_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.ConfigProto.DeviceCountEntry)\n    ))\n  ,\n  DESCRIPTOR = _CONFIGPROTO,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ConfigProto)\n  ))\n_sym_db.RegisterMessage(ConfigProto)\n_sym_db.RegisterMessage(ConfigProto.DeviceCountEntry)\n\nRunOptions = _reflection.GeneratedProtocolMessageType(\'RunOptions\', (_message.Message,), dict(\n  DESCRIPTOR = _RUNOPTIONS,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RunOptions)\n  ))\n_sym_db.RegisterMessage(RunOptions)\n\nRunMetadata = _reflection.GeneratedProtocolMessageType(\'RunMetadata\', (_message.Message,), dict(\n  DESCRIPTOR = _RUNMETADATA,\n  __module__ = \'tensorflow.core.protobuf.config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RunMetadata)\n  ))\n_sym_db.RegisterMessage(RunMetadata)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\014ConfigProtosP\\001\\370\\001\\001\'))\n_CONFIGPROTO_DEVICECOUNTENTRY.has_options = True\n_CONFIGPROTO_DEVICECOUNTENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/config_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/control_flow_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/control_flow.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/control_flow.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n+tensorflow/core/protobuf/control_flow.proto\\x12\\ntensorflow\\""\\x96\\x01\\n\\tValuesDef\\x12\\x0e\\n\\x06values\\x18\\x01 \\x03(\\t\\x12\\x42\\n\\x0f\\x65xternal_values\\x18\\x02 \\x03(\\x0b\\x32).tensorflow.ValuesDef.ExternalValuesEntry\\x1a\\x35\\n\\x13\\x45xternalValuesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\""\\x88\\x01\\n\\x0e\\x43ondContextDef\\x12\\x14\\n\\x0c\\x63ontext_name\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tpred_name\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\npivot_name\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06\\x62ranch\\x18\\x04 \\x01(\\x05\\x12)\\n\\nvalues_def\\x18\\x05 \\x01(\\x0b\\x32\\x15.tensorflow.ValuesDef\\""\\xb9\\x02\\n\\x0fWhileContextDef\\x12\\x14\\n\\x0c\\x63ontext_name\\x18\\x01 \\x01(\\t\\x12\\x1b\\n\\x13parallel_iterations\\x18\\x02 \\x01(\\x05\\x12\\x11\\n\\tback_prop\\x18\\x03 \\x01(\\x08\\x12\\x13\\n\\x0bswap_memory\\x18\\x04 \\x01(\\x08\\x12\\x12\\n\\npivot_name\\x18\\x05 \\x01(\\t\\x12\\x1b\\n\\x13pivot_for_pred_name\\x18\\x06 \\x01(\\t\\x12\\x1b\\n\\x13pivot_for_body_name\\x18\\x07 \\x01(\\t\\x12\\x17\\n\\x0floop_exit_names\\x18\\x08 \\x03(\\t\\x12\\x18\\n\\x10loop_enter_names\\x18\\n \\x03(\\t\\x12)\\n\\nvalues_def\\x18\\t \\x01(\\x0b\\x32\\x15.tensorflow.ValuesDef\\x12\\x1f\\n\\x17maximum_iterations_name\\x18\\x0b \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11\\x43ontrolFlowProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_VALUESDEF_EXTERNALVALUESENTRY = _descriptor.Descriptor(\n  name=\'ExternalValuesEntry\',\n  full_name=\'tensorflow.ValuesDef.ExternalValuesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.ValuesDef.ExternalValuesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.ValuesDef.ExternalValuesEntry.value\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=157,\n  serialized_end=210,\n)\n\n_VALUESDEF = _descriptor.Descriptor(\n  name=\'ValuesDef\',\n  full_name=\'tensorflow.ValuesDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'values\', full_name=\'tensorflow.ValuesDef.values\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'external_values\', full_name=\'tensorflow.ValuesDef.external_values\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_VALUESDEF_EXTERNALVALUESENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=60,\n  serialized_end=210,\n)\n\n\n_CONDCONTEXTDEF = _descriptor.Descriptor(\n  name=\'CondContextDef\',\n  full_name=\'tensorflow.CondContextDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'context_name\', full_name=\'tensorflow.CondContextDef.context_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pred_name\', full_name=\'tensorflow.CondContextDef.pred_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pivot_name\', full_name=\'tensorflow.CondContextDef.pivot_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'branch\', full_name=\'tensorflow.CondContextDef.branch\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'values_def\', full_name=\'tensorflow.CondContextDef.values_def\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=213,\n  serialized_end=349,\n)\n\n\n_WHILECONTEXTDEF = _descriptor.Descriptor(\n  name=\'WhileContextDef\',\n  full_name=\'tensorflow.WhileContextDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'context_name\', full_name=\'tensorflow.WhileContextDef.context_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'parallel_iterations\', full_name=\'tensorflow.WhileContextDef.parallel_iterations\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'back_prop\', full_name=\'tensorflow.WhileContextDef.back_prop\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'swap_memory\', full_name=\'tensorflow.WhileContextDef.swap_memory\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pivot_name\', full_name=\'tensorflow.WhileContextDef.pivot_name\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pivot_for_pred_name\', full_name=\'tensorflow.WhileContextDef.pivot_for_pred_name\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pivot_for_body_name\', full_name=\'tensorflow.WhileContextDef.pivot_for_body_name\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loop_exit_names\', full_name=\'tensorflow.WhileContextDef.loop_exit_names\', index=7,\n      number=8, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loop_enter_names\', full_name=\'tensorflow.WhileContextDef.loop_enter_names\', index=8,\n      number=10, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'values_def\', full_name=\'tensorflow.WhileContextDef.values_def\', index=9,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maximum_iterations_name\', full_name=\'tensorflow.WhileContextDef.maximum_iterations_name\', index=10,\n      number=11, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=352,\n  serialized_end=665,\n)\n\n_VALUESDEF_EXTERNALVALUESENTRY.containing_type = _VALUESDEF\n_VALUESDEF.fields_by_name[\'external_values\'].message_type = _VALUESDEF_EXTERNALVALUESENTRY\n_CONDCONTEXTDEF.fields_by_name[\'values_def\'].message_type = _VALUESDEF\n_WHILECONTEXTDEF.fields_by_name[\'values_def\'].message_type = _VALUESDEF\nDESCRIPTOR.message_types_by_name[\'ValuesDef\'] = _VALUESDEF\nDESCRIPTOR.message_types_by_name[\'CondContextDef\'] = _CONDCONTEXTDEF\nDESCRIPTOR.message_types_by_name[\'WhileContextDef\'] = _WHILECONTEXTDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nValuesDef = _reflection.GeneratedProtocolMessageType(\'ValuesDef\', (_message.Message,), dict(\n\n  ExternalValuesEntry = _reflection.GeneratedProtocolMessageType(\'ExternalValuesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _VALUESDEF_EXTERNALVALUESENTRY,\n    __module__ = \'tensorflow.core.protobuf.control_flow_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.ValuesDef.ExternalValuesEntry)\n    ))\n  ,\n  DESCRIPTOR = _VALUESDEF,\n  __module__ = \'tensorflow.core.protobuf.control_flow_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ValuesDef)\n  ))\n_sym_db.RegisterMessage(ValuesDef)\n_sym_db.RegisterMessage(ValuesDef.ExternalValuesEntry)\n\nCondContextDef = _reflection.GeneratedProtocolMessageType(\'CondContextDef\', (_message.Message,), dict(\n  DESCRIPTOR = _CONDCONTEXTDEF,\n  __module__ = \'tensorflow.core.protobuf.control_flow_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CondContextDef)\n  ))\n_sym_db.RegisterMessage(CondContextDef)\n\nWhileContextDef = _reflection.GeneratedProtocolMessageType(\'WhileContextDef\', (_message.Message,), dict(\n  DESCRIPTOR = _WHILECONTEXTDEF,\n  __module__ = \'tensorflow.core.protobuf.control_flow_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.WhileContextDef)\n  ))\n_sym_db.RegisterMessage(WhileContextDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\021ControlFlowProtosP\\001\\370\\001\\001\'))\n_VALUESDEF_EXTERNALVALUESENTRY.has_options = True\n_VALUESDEF_EXTERNALVALUESENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/control_flow_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/debug_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/debug.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/debug.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n$tensorflow/core/protobuf/debug.proto\\x12\\ntensorflow\\""\\x8e\\x01\\n\\x10\\x44\\x65\\x62ugTensorWatch\\x12\\x11\\n\\tnode_name\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0boutput_slot\\x18\\x02 \\x01(\\x05\\x12\\x11\\n\\tdebug_ops\\x18\\x03 \\x03(\\t\\x12\\x12\\n\\ndebug_urls\\x18\\x04 \\x03(\\t\\x12+\\n#tolerate_debug_op_creation_failures\\x18\\x05 \\x01(\\x08\\""b\\n\\x0c\\x44\\x65\\x62ugOptions\\x12=\\n\\x17\\x64\\x65\\x62ug_tensor_watch_opts\\x18\\x04 \\x03(\\x0b\\x32\\x1c.tensorflow.DebugTensorWatch\\x12\\x13\\n\\x0bglobal_step\\x18\\n \\x01(\\x03\\""j\\n\\x12\\x44\\x65\\x62uggedSourceFile\\x12\\x0c\\n\\x04host\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tfile_path\\x18\\x02 \\x01(\\t\\x12\\x15\\n\\rlast_modified\\x18\\x03 \\x01(\\x03\\x12\\r\\n\\x05\\x62ytes\\x18\\x04 \\x01(\\x03\\x12\\r\\n\\x05lines\\x18\\x05 \\x03(\\t\\""K\\n\\x13\\x44\\x65\\x62uggedSourceFiles\\x12\\x34\\n\\x0csource_files\\x18\\x01 \\x03(\\x0b\\x32\\x1e.tensorflow.DebuggedSourceFileB,\\n\\x18org.tensorflow.frameworkB\\x0b\\x44\\x65\\x62ugProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_DEBUGTENSORWATCH = _descriptor.Descriptor(\n  name=\'DebugTensorWatch\',\n  full_name=\'tensorflow.DebugTensorWatch\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_name\', full_name=\'tensorflow.DebugTensorWatch.node_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_slot\', full_name=\'tensorflow.DebugTensorWatch.output_slot\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_ops\', full_name=\'tensorflow.DebugTensorWatch.debug_ops\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_urls\', full_name=\'tensorflow.DebugTensorWatch.debug_urls\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tolerate_debug_op_creation_failures\', full_name=\'tensorflow.DebugTensorWatch.tolerate_debug_op_creation_failures\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=53,\n  serialized_end=195,\n)\n\n\n_DEBUGOPTIONS = _descriptor.Descriptor(\n  name=\'DebugOptions\',\n  full_name=\'tensorflow.DebugOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'debug_tensor_watch_opts\', full_name=\'tensorflow.DebugOptions.debug_tensor_watch_opts\', index=0,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'global_step\', full_name=\'tensorflow.DebugOptions.global_step\', index=1,\n      number=10, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=197,\n  serialized_end=295,\n)\n\n\n_DEBUGGEDSOURCEFILE = _descriptor.Descriptor(\n  name=\'DebuggedSourceFile\',\n  full_name=\'tensorflow.DebuggedSourceFile\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'host\', full_name=\'tensorflow.DebuggedSourceFile.host\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'file_path\', full_name=\'tensorflow.DebuggedSourceFile.file_path\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'last_modified\', full_name=\'tensorflow.DebuggedSourceFile.last_modified\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bytes\', full_name=\'tensorflow.DebuggedSourceFile.bytes\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lines\', full_name=\'tensorflow.DebuggedSourceFile.lines\', index=4,\n      number=5, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=297,\n  serialized_end=403,\n)\n\n\n_DEBUGGEDSOURCEFILES = _descriptor.Descriptor(\n  name=\'DebuggedSourceFiles\',\n  full_name=\'tensorflow.DebuggedSourceFiles\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source_files\', full_name=\'tensorflow.DebuggedSourceFiles.source_files\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=405,\n  serialized_end=480,\n)\n\n_DEBUGOPTIONS.fields_by_name[\'debug_tensor_watch_opts\'].message_type = _DEBUGTENSORWATCH\n_DEBUGGEDSOURCEFILES.fields_by_name[\'source_files\'].message_type = _DEBUGGEDSOURCEFILE\nDESCRIPTOR.message_types_by_name[\'DebugTensorWatch\'] = _DEBUGTENSORWATCH\nDESCRIPTOR.message_types_by_name[\'DebugOptions\'] = _DEBUGOPTIONS\nDESCRIPTOR.message_types_by_name[\'DebuggedSourceFile\'] = _DEBUGGEDSOURCEFILE\nDESCRIPTOR.message_types_by_name[\'DebuggedSourceFiles\'] = _DEBUGGEDSOURCEFILES\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nDebugTensorWatch = _reflection.GeneratedProtocolMessageType(\'DebugTensorWatch\', (_message.Message,), dict(\n  DESCRIPTOR = _DEBUGTENSORWATCH,\n  __module__ = \'tensorflow.core.protobuf.debug_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DebugTensorWatch)\n  ))\n_sym_db.RegisterMessage(DebugTensorWatch)\n\nDebugOptions = _reflection.GeneratedProtocolMessageType(\'DebugOptions\', (_message.Message,), dict(\n  DESCRIPTOR = _DEBUGOPTIONS,\n  __module__ = \'tensorflow.core.protobuf.debug_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DebugOptions)\n  ))\n_sym_db.RegisterMessage(DebugOptions)\n\nDebuggedSourceFile = _reflection.GeneratedProtocolMessageType(\'DebuggedSourceFile\', (_message.Message,), dict(\n  DESCRIPTOR = _DEBUGGEDSOURCEFILE,\n  __module__ = \'tensorflow.core.protobuf.debug_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DebuggedSourceFile)\n  ))\n_sym_db.RegisterMessage(DebuggedSourceFile)\n\nDebuggedSourceFiles = _reflection.GeneratedProtocolMessageType(\'DebuggedSourceFiles\', (_message.Message,), dict(\n  DESCRIPTOR = _DEBUGGEDSOURCEFILES,\n  __module__ = \'tensorflow.core.protobuf.debug_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DebuggedSourceFiles)\n  ))\n_sym_db.RegisterMessage(DebuggedSourceFiles)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\013DebugProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/debug_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/device_properties_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/device_properties.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/device_properties.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow/core/protobuf/device_properties.proto\\x12\\ntensorflow\\""\\x90\\x03\\n\\x10\\x44\\x65viceProperties\\x12\\x0c\\n\\x04type\\x18\\x01 \\x01(\\t\\x12\\x0e\\n\\x06vendor\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05model\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\tfrequency\\x18\\x04 \\x01(\\x03\\x12\\x11\\n\\tnum_cores\\x18\\x05 \\x01(\\x03\\x12\\x42\\n\\x0b\\x65nvironment\\x18\\x06 \\x03(\\x0b\\x32-.tensorflow.DeviceProperties.EnvironmentEntry\\x12\\x15\\n\\rnum_registers\\x18\\x07 \\x01(\\x03\\x12\\x15\\n\\rl1_cache_size\\x18\\x08 \\x01(\\x03\\x12\\x15\\n\\rl2_cache_size\\x18\\t \\x01(\\x03\\x12\\x15\\n\\rl3_cache_size\\x18\\n \\x01(\\x03\\x12-\\n%shared_memory_size_per_multiprocessor\\x18\\x0b \\x01(\\x03\\x12\\x13\\n\\x0bmemory_size\\x18\\x0c \\x01(\\x03\\x12\\x11\\n\\tbandwidth\\x18\\r \\x01(\\x03\\x1a\\x32\\n\\x10\\x45nvironmentEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\""M\\n\\x0bNamedDevice\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x30\\n\\nproperties\\x18\\x02 \\x01(\\x0b\\x32\\x1c.tensorflow.DevicePropertiesB\\x1b\\x42\\x16\\x44\\x65vicePropertiesProtos\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_DEVICEPROPERTIES_ENVIRONMENTENTRY = _descriptor.Descriptor(\n  name=\'EnvironmentEntry\',\n  full_name=\'tensorflow.DeviceProperties.EnvironmentEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.DeviceProperties.EnvironmentEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.DeviceProperties.EnvironmentEntry.value\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=415,\n  serialized_end=465,\n)\n\n_DEVICEPROPERTIES = _descriptor.Descriptor(\n  name=\'DeviceProperties\',\n  full_name=\'tensorflow.DeviceProperties\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorflow.DeviceProperties.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'vendor\', full_name=\'tensorflow.DeviceProperties.vendor\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'model\', full_name=\'tensorflow.DeviceProperties.model\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'frequency\', full_name=\'tensorflow.DeviceProperties.frequency\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_cores\', full_name=\'tensorflow.DeviceProperties.num_cores\', index=4,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'environment\', full_name=\'tensorflow.DeviceProperties.environment\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_registers\', full_name=\'tensorflow.DeviceProperties.num_registers\', index=6,\n      number=7, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'l1_cache_size\', full_name=\'tensorflow.DeviceProperties.l1_cache_size\', index=7,\n      number=8, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'l2_cache_size\', full_name=\'tensorflow.DeviceProperties.l2_cache_size\', index=8,\n      number=9, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'l3_cache_size\', full_name=\'tensorflow.DeviceProperties.l3_cache_size\', index=9,\n      number=10, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shared_memory_size_per_multiprocessor\', full_name=\'tensorflow.DeviceProperties.shared_memory_size_per_multiprocessor\', index=10,\n      number=11, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_size\', full_name=\'tensorflow.DeviceProperties.memory_size\', index=11,\n      number=12, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bandwidth\', full_name=\'tensorflow.DeviceProperties.bandwidth\', index=12,\n      number=13, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_DEVICEPROPERTIES_ENVIRONMENTENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=65,\n  serialized_end=465,\n)\n\n\n_NAMEDDEVICE = _descriptor.Descriptor(\n  name=\'NamedDevice\',\n  full_name=\'tensorflow.NamedDevice\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.NamedDevice.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'properties\', full_name=\'tensorflow.NamedDevice.properties\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=467,\n  serialized_end=544,\n)\n\n_DEVICEPROPERTIES_ENVIRONMENTENTRY.containing_type = _DEVICEPROPERTIES\n_DEVICEPROPERTIES.fields_by_name[\'environment\'].message_type = _DEVICEPROPERTIES_ENVIRONMENTENTRY\n_NAMEDDEVICE.fields_by_name[\'properties\'].message_type = _DEVICEPROPERTIES\nDESCRIPTOR.message_types_by_name[\'DeviceProperties\'] = _DEVICEPROPERTIES\nDESCRIPTOR.message_types_by_name[\'NamedDevice\'] = _NAMEDDEVICE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nDeviceProperties = _reflection.GeneratedProtocolMessageType(\'DeviceProperties\', (_message.Message,), dict(\n\n  EnvironmentEntry = _reflection.GeneratedProtocolMessageType(\'EnvironmentEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DEVICEPROPERTIES_ENVIRONMENTENTRY,\n    __module__ = \'tensorflow.core.protobuf.device_properties_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.DeviceProperties.EnvironmentEntry)\n    ))\n  ,\n  DESCRIPTOR = _DEVICEPROPERTIES,\n  __module__ = \'tensorflow.core.protobuf.device_properties_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeviceProperties)\n  ))\n_sym_db.RegisterMessage(DeviceProperties)\n_sym_db.RegisterMessage(DeviceProperties.EnvironmentEntry)\n\nNamedDevice = _reflection.GeneratedProtocolMessageType(\'NamedDevice\', (_message.Message,), dict(\n  DESCRIPTOR = _NAMEDDEVICE,\n  __module__ = \'tensorflow.core.protobuf.device_properties_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.NamedDevice)\n  ))\n_sym_db.RegisterMessage(NamedDevice)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'B\\026DevicePropertiesProtos\\370\\001\\001\'))\n_DEVICEPROPERTIES_ENVIRONMENTENTRY.has_options = True\n_DEVICEPROPERTIES_ENVIRONMENTENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/device_properties_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/master_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/master.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import device_attributes_pb2 as tensorflow_dot_core_dot_framework_dot_device__attributes__pb2\nfrom tensorflow.core.framework import graph_pb2 as tensorflow_dot_core_dot_framework_dot_graph__pb2\nfrom tensorflow.core.lib.core import error_codes_pb2 as tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2\nfrom tensorflow.core.protobuf import config_pb2 as tensorflow_dot_core_dot_protobuf_dot_config__pb2\nfrom tensorflow.core.protobuf import named_tensor_pb2 as tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/master.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow/core/protobuf/master.proto\\x12\\ntensorflow\\x1a\\x31tensorflow/core/framework/device_attributes.proto\\x1a%tensorflow/core/framework/graph.proto\\x1a*tensorflow/core/lib/core/error_codes.proto\\x1a%tensorflow/core/protobuf/config.proto\\x1a+tensorflow/core/protobuf/named_tensor.proto\\""x\\n\\x14\\x43reateSessionRequest\\x12\\\'\\n\\tgraph_def\\x18\\x01 \\x01(\\x0b\\x32\\x14.tensorflow.GraphDef\\x12\\\'\\n\\x06\\x63onfig\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.ConfigProto\\x12\\x0e\\n\\x06target\\x18\\x03 \\x01(\\t\\""F\\n\\x15\\x43reateSessionResponse\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\x12\\x15\\n\\rgraph_version\\x18\\x02 \\x01(\\x03\\""v\\n\\x14\\x45xtendSessionRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\tgraph_def\\x18\\x02 \\x01(\\x0b\\x32\\x14.tensorflow.GraphDef\\x12\\x1d\\n\\x15\\x63urrent_graph_version\\x18\\x03 \\x01(\\x03\\""2\\n\\x15\\x45xtendSessionResponse\\x12\\x19\\n\\x11new_graph_version\\x18\\x04 \\x01(\\x03\\""\\xdf\\x01\\n\\x0eRunStepRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\x12*\\n\\x04\\x66\\x65\\x65\\x64\\x18\\x02 \\x03(\\x0b\\x32\\x1c.tensorflow.NamedTensorProto\\x12\\r\\n\\x05\\x66\\x65tch\\x18\\x03 \\x03(\\t\\x12\\x0e\\n\\x06target\\x18\\x04 \\x03(\\t\\x12\\\'\\n\\x07options\\x18\\x05 \\x01(\\x0b\\x32\\x16.tensorflow.RunOptions\\x12\\x1a\\n\\x12partial_run_handle\\x18\\x06 \\x01(\\t\\x12%\\n\\x1dstore_errors_in_response_body\\x18\\x07 \\x01(\\x08\\""\\xb5\\x01\\n\\x0fRunStepResponse\\x12,\\n\\x06tensor\\x18\\x01 \\x03(\\x0b\\x32\\x1c.tensorflow.NamedTensorProto\\x12)\\n\\x08metadata\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.RunMetadata\\x12+\\n\\x0bstatus_code\\x18\\x03 \\x01(\\x0e\\x32\\x16.tensorflow.error.Code\\x12\\x1c\\n\\x14status_error_message\\x18\\x04 \\x01(\\t\\""]\\n\\x16PartialRunSetupRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04\\x66\\x65\\x65\\x64\\x18\\x02 \\x03(\\t\\x12\\r\\n\\x05\\x66\\x65tch\\x18\\x03 \\x03(\\t\\x12\\x0e\\n\\x06target\\x18\\x04 \\x03(\\t\\""5\\n\\x17PartialRunSetupResponse\\x12\\x1a\\n\\x12partial_run_handle\\x18\\x01 \\x01(\\t\\""-\\n\\x13\\x43loseSessionRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\""\\x16\\n\\x14\\x43loseSessionResponse\\""9\\n\\x0cResetRequest\\x12\\x11\\n\\tcontainer\\x18\\x01 \\x03(\\t\\x12\\x16\\n\\x0e\\x64\\x65vice_filters\\x18\\x02 \\x03(\\t\\""\\x0f\\n\\rResetResponse\\"",\\n\\x12ListDevicesRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\""~\\n\\x13ListDevicesResponse\\x12\\x32\\n\\x0clocal_device\\x18\\x01 \\x03(\\x0b\\x32\\x1c.tensorflow.DeviceAttributes\\x12\\x33\\n\\rremote_device\\x18\\x02 \\x03(\\x0b\\x32\\x1c.tensorflow.DeviceAttributesB;\\n\\x1aorg.tensorflow.distruntimeB\\x18\\x44istributedRuntimeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_device__attributes__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_graph__pb2.DESCRIPTOR,tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_config__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2.DESCRIPTOR,])\n\n\n\n\n_CREATESESSIONREQUEST = _descriptor.Descriptor(\n  name=\'CreateSessionRequest\',\n  full_name=\'tensorflow.CreateSessionRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'graph_def\', full_name=\'tensorflow.CreateSessionRequest.graph_def\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'config\', full_name=\'tensorflow.CreateSessionRequest.config\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target\', full_name=\'tensorflow.CreateSessionRequest.target\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=271,\n  serialized_end=391,\n)\n\n\n_CREATESESSIONRESPONSE = _descriptor.Descriptor(\n  name=\'CreateSessionResponse\',\n  full_name=\'tensorflow.CreateSessionResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.CreateSessionResponse.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_version\', full_name=\'tensorflow.CreateSessionResponse.graph_version\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=393,\n  serialized_end=463,\n)\n\n\n_EXTENDSESSIONREQUEST = _descriptor.Descriptor(\n  name=\'ExtendSessionRequest\',\n  full_name=\'tensorflow.ExtendSessionRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.ExtendSessionRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_def\', full_name=\'tensorflow.ExtendSessionRequest.graph_def\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'current_graph_version\', full_name=\'tensorflow.ExtendSessionRequest.current_graph_version\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=465,\n  serialized_end=583,\n)\n\n\n_EXTENDSESSIONRESPONSE = _descriptor.Descriptor(\n  name=\'ExtendSessionResponse\',\n  full_name=\'tensorflow.ExtendSessionResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'new_graph_version\', full_name=\'tensorflow.ExtendSessionResponse.new_graph_version\', index=0,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=585,\n  serialized_end=635,\n)\n\n\n_RUNSTEPREQUEST = _descriptor.Descriptor(\n  name=\'RunStepRequest\',\n  full_name=\'tensorflow.RunStepRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.RunStepRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feed\', full_name=\'tensorflow.RunStepRequest.feed\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fetch\', full_name=\'tensorflow.RunStepRequest.fetch\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target\', full_name=\'tensorflow.RunStepRequest.target\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'options\', full_name=\'tensorflow.RunStepRequest.options\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'partial_run_handle\', full_name=\'tensorflow.RunStepRequest.partial_run_handle\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'store_errors_in_response_body\', full_name=\'tensorflow.RunStepRequest.store_errors_in_response_body\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=638,\n  serialized_end=861,\n)\n\n\n_RUNSTEPRESPONSE = _descriptor.Descriptor(\n  name=\'RunStepResponse\',\n  full_name=\'tensorflow.RunStepResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.RunStepResponse.tensor\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'metadata\', full_name=\'tensorflow.RunStepResponse.metadata\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'status_code\', full_name=\'tensorflow.RunStepResponse.status_code\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'status_error_message\', full_name=\'tensorflow.RunStepResponse.status_error_message\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=864,\n  serialized_end=1045,\n)\n\n\n_PARTIALRUNSETUPREQUEST = _descriptor.Descriptor(\n  name=\'PartialRunSetupRequest\',\n  full_name=\'tensorflow.PartialRunSetupRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.PartialRunSetupRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feed\', full_name=\'tensorflow.PartialRunSetupRequest.feed\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fetch\', full_name=\'tensorflow.PartialRunSetupRequest.fetch\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target\', full_name=\'tensorflow.PartialRunSetupRequest.target\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1047,\n  serialized_end=1140,\n)\n\n\n_PARTIALRUNSETUPRESPONSE = _descriptor.Descriptor(\n  name=\'PartialRunSetupResponse\',\n  full_name=\'tensorflow.PartialRunSetupResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'partial_run_handle\', full_name=\'tensorflow.PartialRunSetupResponse.partial_run_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1142,\n  serialized_end=1195,\n)\n\n\n_CLOSESESSIONREQUEST = _descriptor.Descriptor(\n  name=\'CloseSessionRequest\',\n  full_name=\'tensorflow.CloseSessionRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.CloseSessionRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1197,\n  serialized_end=1242,\n)\n\n\n_CLOSESESSIONRESPONSE = _descriptor.Descriptor(\n  name=\'CloseSessionResponse\',\n  full_name=\'tensorflow.CloseSessionResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1244,\n  serialized_end=1266,\n)\n\n\n_RESETREQUEST = _descriptor.Descriptor(\n  name=\'ResetRequest\',\n  full_name=\'tensorflow.ResetRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'container\', full_name=\'tensorflow.ResetRequest.container\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_filters\', full_name=\'tensorflow.ResetRequest.device_filters\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1268,\n  serialized_end=1325,\n)\n\n\n_RESETRESPONSE = _descriptor.Descriptor(\n  name=\'ResetResponse\',\n  full_name=\'tensorflow.ResetResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1327,\n  serialized_end=1342,\n)\n\n\n_LISTDEVICESREQUEST = _descriptor.Descriptor(\n  name=\'ListDevicesRequest\',\n  full_name=\'tensorflow.ListDevicesRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.ListDevicesRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1344,\n  serialized_end=1388,\n)\n\n\n_LISTDEVICESRESPONSE = _descriptor.Descriptor(\n  name=\'ListDevicesResponse\',\n  full_name=\'tensorflow.ListDevicesResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'local_device\', full_name=\'tensorflow.ListDevicesResponse.local_device\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'remote_device\', full_name=\'tensorflow.ListDevicesResponse.remote_device\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1390,\n  serialized_end=1516,\n)\n\n_CREATESESSIONREQUEST.fields_by_name[\'graph_def\'].message_type = tensorflow_dot_core_dot_framework_dot_graph__pb2._GRAPHDEF\n_CREATESESSIONREQUEST.fields_by_name[\'config\'].message_type = tensorflow_dot_core_dot_protobuf_dot_config__pb2._CONFIGPROTO\n_EXTENDSESSIONREQUEST.fields_by_name[\'graph_def\'].message_type = tensorflow_dot_core_dot_framework_dot_graph__pb2._GRAPHDEF\n_RUNSTEPREQUEST.fields_by_name[\'feed\'].message_type = tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2._NAMEDTENSORPROTO\n_RUNSTEPREQUEST.fields_by_name[\'options\'].message_type = tensorflow_dot_core_dot_protobuf_dot_config__pb2._RUNOPTIONS\n_RUNSTEPRESPONSE.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2._NAMEDTENSORPROTO\n_RUNSTEPRESPONSE.fields_by_name[\'metadata\'].message_type = tensorflow_dot_core_dot_protobuf_dot_config__pb2._RUNMETADATA\n_RUNSTEPRESPONSE.fields_by_name[\'status_code\'].enum_type = tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2._CODE\n_LISTDEVICESRESPONSE.fields_by_name[\'local_device\'].message_type = tensorflow_dot_core_dot_framework_dot_device__attributes__pb2._DEVICEATTRIBUTES\n_LISTDEVICESRESPONSE.fields_by_name[\'remote_device\'].message_type = tensorflow_dot_core_dot_framework_dot_device__attributes__pb2._DEVICEATTRIBUTES\nDESCRIPTOR.message_types_by_name[\'CreateSessionRequest\'] = _CREATESESSIONREQUEST\nDESCRIPTOR.message_types_by_name[\'CreateSessionResponse\'] = _CREATESESSIONRESPONSE\nDESCRIPTOR.message_types_by_name[\'ExtendSessionRequest\'] = _EXTENDSESSIONREQUEST\nDESCRIPTOR.message_types_by_name[\'ExtendSessionResponse\'] = _EXTENDSESSIONRESPONSE\nDESCRIPTOR.message_types_by_name[\'RunStepRequest\'] = _RUNSTEPREQUEST\nDESCRIPTOR.message_types_by_name[\'RunStepResponse\'] = _RUNSTEPRESPONSE\nDESCRIPTOR.message_types_by_name[\'PartialRunSetupRequest\'] = _PARTIALRUNSETUPREQUEST\nDESCRIPTOR.message_types_by_name[\'PartialRunSetupResponse\'] = _PARTIALRUNSETUPRESPONSE\nDESCRIPTOR.message_types_by_name[\'CloseSessionRequest\'] = _CLOSESESSIONREQUEST\nDESCRIPTOR.message_types_by_name[\'CloseSessionResponse\'] = _CLOSESESSIONRESPONSE\nDESCRIPTOR.message_types_by_name[\'ResetRequest\'] = _RESETREQUEST\nDESCRIPTOR.message_types_by_name[\'ResetResponse\'] = _RESETRESPONSE\nDESCRIPTOR.message_types_by_name[\'ListDevicesRequest\'] = _LISTDEVICESREQUEST\nDESCRIPTOR.message_types_by_name[\'ListDevicesResponse\'] = _LISTDEVICESRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nCreateSessionRequest = _reflection.GeneratedProtocolMessageType(\'CreateSessionRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CREATESESSIONREQUEST,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CreateSessionRequest)\n  ))\n_sym_db.RegisterMessage(CreateSessionRequest)\n\nCreateSessionResponse = _reflection.GeneratedProtocolMessageType(\'CreateSessionResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _CREATESESSIONRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CreateSessionResponse)\n  ))\n_sym_db.RegisterMessage(CreateSessionResponse)\n\nExtendSessionRequest = _reflection.GeneratedProtocolMessageType(\'ExtendSessionRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _EXTENDSESSIONREQUEST,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ExtendSessionRequest)\n  ))\n_sym_db.RegisterMessage(ExtendSessionRequest)\n\nExtendSessionResponse = _reflection.GeneratedProtocolMessageType(\'ExtendSessionResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _EXTENDSESSIONRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ExtendSessionResponse)\n  ))\n_sym_db.RegisterMessage(ExtendSessionResponse)\n\nRunStepRequest = _reflection.GeneratedProtocolMessageType(\'RunStepRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _RUNSTEPREQUEST,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RunStepRequest)\n  ))\n_sym_db.RegisterMessage(RunStepRequest)\n\nRunStepResponse = _reflection.GeneratedProtocolMessageType(\'RunStepResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _RUNSTEPRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RunStepResponse)\n  ))\n_sym_db.RegisterMessage(RunStepResponse)\n\nPartialRunSetupRequest = _reflection.GeneratedProtocolMessageType(\'PartialRunSetupRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _PARTIALRUNSETUPREQUEST,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.PartialRunSetupRequest)\n  ))\n_sym_db.RegisterMessage(PartialRunSetupRequest)\n\nPartialRunSetupResponse = _reflection.GeneratedProtocolMessageType(\'PartialRunSetupResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _PARTIALRUNSETUPRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.PartialRunSetupResponse)\n  ))\n_sym_db.RegisterMessage(PartialRunSetupResponse)\n\nCloseSessionRequest = _reflection.GeneratedProtocolMessageType(\'CloseSessionRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CLOSESESSIONREQUEST,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CloseSessionRequest)\n  ))\n_sym_db.RegisterMessage(CloseSessionRequest)\n\nCloseSessionResponse = _reflection.GeneratedProtocolMessageType(\'CloseSessionResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _CLOSESESSIONRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CloseSessionResponse)\n  ))\n_sym_db.RegisterMessage(CloseSessionResponse)\n\nResetRequest = _reflection.GeneratedProtocolMessageType(\'ResetRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _RESETREQUEST,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ResetRequest)\n  ))\n_sym_db.RegisterMessage(ResetRequest)\n\nResetResponse = _reflection.GeneratedProtocolMessageType(\'ResetResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _RESETRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ResetResponse)\n  ))\n_sym_db.RegisterMessage(ResetResponse)\n\nListDevicesRequest = _reflection.GeneratedProtocolMessageType(\'ListDevicesRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _LISTDEVICESREQUEST,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ListDevicesRequest)\n  ))\n_sym_db.RegisterMessage(ListDevicesRequest)\n\nListDevicesResponse = _reflection.GeneratedProtocolMessageType(\'ListDevicesResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _LISTDEVICESRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.master_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ListDevicesResponse)\n  ))\n_sym_db.RegisterMessage(ListDevicesResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\032org.tensorflow.distruntimeB\\030DistributedRuntimeProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/master_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/master_service_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/master_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.protobuf import master_pb2 as tensorflow_dot_core_dot_protobuf_dot_master__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/master_service.proto\',\n  package=\'tensorflow.grpc\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n-tensorflow/core/protobuf/master_service.proto\\x12\\x0ftensorflow.grpc\\x1a%tensorflow/core/protobuf/master.proto2\\xbc\\x04\\n\\rMasterService\\x12T\\n\\rCreateSession\\x12 .tensorflow.CreateSessionRequest\\x1a!.tensorflow.CreateSessionResponse\\x12T\\n\\rExtendSession\\x12 .tensorflow.ExtendSessionRequest\\x1a!.tensorflow.ExtendSessionResponse\\x12Z\\n\\x0fPartialRunSetup\\x12\\"".tensorflow.PartialRunSetupRequest\\x1a#.tensorflow.PartialRunSetupResponse\\x12\\x42\\n\\x07RunStep\\x12\\x1a.tensorflow.RunStepRequest\\x1a\\x1b.tensorflow.RunStepResponse\\x12Q\\n\\x0c\\x43loseSession\\x12\\x1f.tensorflow.CloseSessionRequest\\x1a .tensorflow.CloseSessionResponse\\x12N\\n\\x0bListDevices\\x12\\x1e.tensorflow.ListDevicesRequest\\x1a\\x1f.tensorflow.ListDevicesResponse\\x12<\\n\\x05Reset\\x12\\x18.tensorflow.ResetRequest\\x1a\\x19.tensorflow.ResetResponseB3\\n\\x1aorg.tensorflow.distruntimeB\\x13MasterServiceProtosP\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_protobuf_dot_master__pb2.DESCRIPTOR,])\n\n\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\032org.tensorflow.distruntimeB\\023MasterServiceProtosP\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\n\n\n  class MasterServiceStub(object):\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    MasterService defines a TensorFlow service with which a client can\n    interact to execute a distributed TensorFlow computation.\n\n    A master service keeps track of multiple ""master sessions"". Each\n    session encapsulates a computation graph and its associated state,\n    and typically corresponds to a single ""client session"" (e.g. a\n    `tensorflow::Session` instance).\n\n    A session is responsible for the following:\n    * assigning each node to a device (locally or remotely) using a\n    placement algorithm. This may make decisions based on collected\n    statistics from the workers in the system (e.g., memory usage,\n    bandwidth consumption, etc.)\n\n    * inserting intermediate nodes and edges to support cross-device\n    and cross-process data flows and resource management.\n\n    * issuing commands to workers to execute the subgraphs associated\n    with those workers.\n\n    Typically, a client carries out an iterative computation\n    (e.g. training) by invoking RPCs against the master in a\n    client-side loop. The client first creates a client session that\n    connects to a particular master (using gRPC for example). The\n    master creates a corresponding master session that is hosted on\n    the master and caches state between the client\'s invocations.\n\n    After the session is established, the master returns an opaque\n    handle to the client that can be used to associate the client and\n    master sessions.\n\n    The client may send an initial graph to the master in the\n    CreateSession call, and add nodes to the graph using ExtendSession.\n\n    The most frequent operation a master is ""RunStep"", which implements\n    the `Session::Run()` API. It supports feeding in arguments,\n    executing a dataflow computation, and fetching arguments.\n\n    Finally, when the client no longer needs the session, it should\n    close the session by invoking CloseSession, which allows the master\n    to reclaim resources associated with the session. The master may\n    implement a garbage collection scheme that closes sessions that\n    have been inactive for some time.\n\n    For example, the following pseudo-code illustrates how a client\n    interacts with a master:\n\n    stub = NewStub(""/job:mnist/replica:0/task:0"")\n    {handle} = stub->CreateSession({graph_def})\n    do {\n    stub->RunStep({handle, {feeds}, {fetches}})\n    // The client can evaluate a predicate locally, based on the\n    // result of `fetches`, to determine whether to terminate. For\n    // example, it might fetch the loss and evaluate whether it is less\n    // than some threshold.\n    } while (!should_stop({fetches}));\n    stub->CloseSession({handle})\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n\n    def __init__(self, channel):\n      """"""Constructor.\n\n      Args:\n        channel: A grpc.Channel.\n      """"""\n      self.CreateSession = channel.unary_unary(\n          \'/tensorflow.grpc.MasterService/CreateSession\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionResponse.FromString,\n          )\n      self.ExtendSession = channel.unary_unary(\n          \'/tensorflow.grpc.MasterService/ExtendSession\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionResponse.FromString,\n          )\n      self.PartialRunSetup = channel.unary_unary(\n          \'/tensorflow.grpc.MasterService/PartialRunSetup\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupResponse.FromString,\n          )\n      self.RunStep = channel.unary_unary(\n          \'/tensorflow.grpc.MasterService/RunStep\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepResponse.FromString,\n          )\n      self.CloseSession = channel.unary_unary(\n          \'/tensorflow.grpc.MasterService/CloseSession\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionResponse.FromString,\n          )\n      self.ListDevices = channel.unary_unary(\n          \'/tensorflow.grpc.MasterService/ListDevices\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesResponse.FromString,\n          )\n      self.Reset = channel.unary_unary(\n          \'/tensorflow.grpc.MasterService/Reset\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetResponse.FromString,\n          )\n\n\n  class MasterServiceServicer(object):\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    MasterService defines a TensorFlow service with which a client can\n    interact to execute a distributed TensorFlow computation.\n\n    A master service keeps track of multiple ""master sessions"". Each\n    session encapsulates a computation graph and its associated state,\n    and typically corresponds to a single ""client session"" (e.g. a\n    `tensorflow::Session` instance).\n\n    A session is responsible for the following:\n    * assigning each node to a device (locally or remotely) using a\n    placement algorithm. This may make decisions based on collected\n    statistics from the workers in the system (e.g., memory usage,\n    bandwidth consumption, etc.)\n\n    * inserting intermediate nodes and edges to support cross-device\n    and cross-process data flows and resource management.\n\n    * issuing commands to workers to execute the subgraphs associated\n    with those workers.\n\n    Typically, a client carries out an iterative computation\n    (e.g. training) by invoking RPCs against the master in a\n    client-side loop. The client first creates a client session that\n    connects to a particular master (using gRPC for example). The\n    master creates a corresponding master session that is hosted on\n    the master and caches state between the client\'s invocations.\n\n    After the session is established, the master returns an opaque\n    handle to the client that can be used to associate the client and\n    master sessions.\n\n    The client may send an initial graph to the master in the\n    CreateSession call, and add nodes to the graph using ExtendSession.\n\n    The most frequent operation a master is ""RunStep"", which implements\n    the `Session::Run()` API. It supports feeding in arguments,\n    executing a dataflow computation, and fetching arguments.\n\n    Finally, when the client no longer needs the session, it should\n    close the session by invoking CloseSession, which allows the master\n    to reclaim resources associated with the session. The master may\n    implement a garbage collection scheme that closes sessions that\n    have been inactive for some time.\n\n    For example, the following pseudo-code illustrates how a client\n    interacts with a master:\n\n    stub = NewStub(""/job:mnist/replica:0/task:0"")\n    {handle} = stub->CreateSession({graph_def})\n    do {\n    stub->RunStep({handle, {feeds}, {fetches}})\n    // The client can evaluate a predicate locally, based on the\n    // result of `fetches`, to determine whether to terminate. For\n    // example, it might fetch the loss and evaluate whether it is less\n    // than some threshold.\n    } while (!should_stop({fetches}));\n    stub->CloseSession({handle})\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n\n    def CreateSession(self, request, context):\n      """"""Creates a session.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def ExtendSession(self, request, context):\n      """"""Extends a session.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def PartialRunSetup(self, request, context):\n      """"""Prepares future partial run calls.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def RunStep(self, request, context):\n      """"""Drives the graph computation.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def CloseSession(self, request, context):\n      """"""Closes a session.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def ListDevices(self, request, context):\n      """"""List the devices usable by the master.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def Reset(self, request, context):\n      """"""Close and abandon all existing sessions.  Ongoing computations\n      will no longer affect fresh ones via the resources in containers listed in\n      the ResetRequest.  See ResetRequest for more details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n\n  def add_MasterServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n        \'CreateSession\': grpc.unary_unary_rpc_method_handler(\n            servicer.CreateSession,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionResponse.SerializeToString,\n        ),\n        \'ExtendSession\': grpc.unary_unary_rpc_method_handler(\n            servicer.ExtendSession,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionResponse.SerializeToString,\n        ),\n        \'PartialRunSetup\': grpc.unary_unary_rpc_method_handler(\n            servicer.PartialRunSetup,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupResponse.SerializeToString,\n        ),\n        \'RunStep\': grpc.unary_unary_rpc_method_handler(\n            servicer.RunStep,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepResponse.SerializeToString,\n        ),\n        \'CloseSession\': grpc.unary_unary_rpc_method_handler(\n            servicer.CloseSession,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionResponse.SerializeToString,\n        ),\n        \'ListDevices\': grpc.unary_unary_rpc_method_handler(\n            servicer.ListDevices,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesResponse.SerializeToString,\n        ),\n        \'Reset\': grpc.unary_unary_rpc_method_handler(\n            servicer.Reset,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetResponse.SerializeToString,\n        ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n        \'tensorflow.grpc.MasterService\', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))\n\n\n  class BetaMasterServiceServicer(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    MasterService defines a TensorFlow service with which a client can\n    interact to execute a distributed TensorFlow computation.\n\n    A master service keeps track of multiple ""master sessions"". Each\n    session encapsulates a computation graph and its associated state,\n    and typically corresponds to a single ""client session"" (e.g. a\n    `tensorflow::Session` instance).\n\n    A session is responsible for the following:\n    * assigning each node to a device (locally or remotely) using a\n    placement algorithm. This may make decisions based on collected\n    statistics from the workers in the system (e.g., memory usage,\n    bandwidth consumption, etc.)\n\n    * inserting intermediate nodes and edges to support cross-device\n    and cross-process data flows and resource management.\n\n    * issuing commands to workers to execute the subgraphs associated\n    with those workers.\n\n    Typically, a client carries out an iterative computation\n    (e.g. training) by invoking RPCs against the master in a\n    client-side loop. The client first creates a client session that\n    connects to a particular master (using gRPC for example). The\n    master creates a corresponding master session that is hosted on\n    the master and caches state between the client\'s invocations.\n\n    After the session is established, the master returns an opaque\n    handle to the client that can be used to associate the client and\n    master sessions.\n\n    The client may send an initial graph to the master in the\n    CreateSession call, and add nodes to the graph using ExtendSession.\n\n    The most frequent operation a master is ""RunStep"", which implements\n    the `Session::Run()` API. It supports feeding in arguments,\n    executing a dataflow computation, and fetching arguments.\n\n    Finally, when the client no longer needs the session, it should\n    close the session by invoking CloseSession, which allows the master\n    to reclaim resources associated with the session. The master may\n    implement a garbage collection scheme that closes sessions that\n    have been inactive for some time.\n\n    For example, the following pseudo-code illustrates how a client\n    interacts with a master:\n\n    stub = NewStub(""/job:mnist/replica:0/task:0"")\n    {handle} = stub->CreateSession({graph_def})\n    do {\n    stub->RunStep({handle, {feeds}, {fetches}})\n    // The client can evaluate a predicate locally, based on the\n    // result of `fetches`, to determine whether to terminate. For\n    // example, it might fetch the loss and evaluate whether it is less\n    // than some threshold.\n    } while (!should_stop({fetches}));\n    stub->CloseSession({handle})\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n    def CreateSession(self, request, context):\n      """"""Creates a session.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def ExtendSession(self, request, context):\n      """"""Extends a session.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def PartialRunSetup(self, request, context):\n      """"""Prepares future partial run calls.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def RunStep(self, request, context):\n      """"""Drives the graph computation.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def CloseSession(self, request, context):\n      """"""Closes a session.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def ListDevices(self, request, context):\n      """"""List the devices usable by the master.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def Reset(self, request, context):\n      """"""Close and abandon all existing sessions.  Ongoing computations\n      will no longer affect fresh ones via the resources in containers listed in\n      the ResetRequest.  See ResetRequest for more details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\n  class BetaMasterServiceStub(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    MasterService defines a TensorFlow service with which a client can\n    interact to execute a distributed TensorFlow computation.\n\n    A master service keeps track of multiple ""master sessions"". Each\n    session encapsulates a computation graph and its associated state,\n    and typically corresponds to a single ""client session"" (e.g. a\n    `tensorflow::Session` instance).\n\n    A session is responsible for the following:\n    * assigning each node to a device (locally or remotely) using a\n    placement algorithm. This may make decisions based on collected\n    statistics from the workers in the system (e.g., memory usage,\n    bandwidth consumption, etc.)\n\n    * inserting intermediate nodes and edges to support cross-device\n    and cross-process data flows and resource management.\n\n    * issuing commands to workers to execute the subgraphs associated\n    with those workers.\n\n    Typically, a client carries out an iterative computation\n    (e.g. training) by invoking RPCs against the master in a\n    client-side loop. The client first creates a client session that\n    connects to a particular master (using gRPC for example). The\n    master creates a corresponding master session that is hosted on\n    the master and caches state between the client\'s invocations.\n\n    After the session is established, the master returns an opaque\n    handle to the client that can be used to associate the client and\n    master sessions.\n\n    The client may send an initial graph to the master in the\n    CreateSession call, and add nodes to the graph using ExtendSession.\n\n    The most frequent operation a master is ""RunStep"", which implements\n    the `Session::Run()` API. It supports feeding in arguments,\n    executing a dataflow computation, and fetching arguments.\n\n    Finally, when the client no longer needs the session, it should\n    close the session by invoking CloseSession, which allows the master\n    to reclaim resources associated with the session. The master may\n    implement a garbage collection scheme that closes sessions that\n    have been inactive for some time.\n\n    For example, the following pseudo-code illustrates how a client\n    interacts with a master:\n\n    stub = NewStub(""/job:mnist/replica:0/task:0"")\n    {handle} = stub->CreateSession({graph_def})\n    do {\n    stub->RunStep({handle, {feeds}, {fetches}})\n    // The client can evaluate a predicate locally, based on the\n    // result of `fetches`, to determine whether to terminate. For\n    // example, it might fetch the loss and evaluate whether it is less\n    // than some threshold.\n    } while (!should_stop({fetches}));\n    stub->CloseSession({handle})\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n    def CreateSession(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Creates a session.\n      """"""\n      raise NotImplementedError()\n    CreateSession.future = None\n    def ExtendSession(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Extends a session.\n      """"""\n      raise NotImplementedError()\n    ExtendSession.future = None\n    def PartialRunSetup(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Prepares future partial run calls.\n      """"""\n      raise NotImplementedError()\n    PartialRunSetup.future = None\n    def RunStep(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Drives the graph computation.\n      """"""\n      raise NotImplementedError()\n    RunStep.future = None\n    def CloseSession(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Closes a session.\n      """"""\n      raise NotImplementedError()\n    CloseSession.future = None\n    def ListDevices(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""List the devices usable by the master.\n      """"""\n      raise NotImplementedError()\n    ListDevices.future = None\n    def Reset(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Close and abandon all existing sessions.  Ongoing computations\n      will no longer affect fresh ones via the resources in containers listed in\n      the ResetRequest.  See ResetRequest for more details.\n      """"""\n      raise NotImplementedError()\n    Reset.future = None\n\n\n  def beta_create_MasterService_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_deserializers = {\n      (\'tensorflow.grpc.MasterService\', \'CloseSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionRequest.FromString,\n      (\'tensorflow.grpc.MasterService\', \'CreateSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionRequest.FromString,\n      (\'tensorflow.grpc.MasterService\', \'ExtendSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionRequest.FromString,\n      (\'tensorflow.grpc.MasterService\', \'ListDevices\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesRequest.FromString,\n      (\'tensorflow.grpc.MasterService\', \'PartialRunSetup\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupRequest.FromString,\n      (\'tensorflow.grpc.MasterService\', \'Reset\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetRequest.FromString,\n      (\'tensorflow.grpc.MasterService\', \'RunStep\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepRequest.FromString,\n    }\n    response_serializers = {\n      (\'tensorflow.grpc.MasterService\', \'CloseSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionResponse.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'CreateSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionResponse.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'ExtendSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionResponse.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'ListDevices\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesResponse.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'PartialRunSetup\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupResponse.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'Reset\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetResponse.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'RunStep\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepResponse.SerializeToString,\n    }\n    method_implementations = {\n      (\'tensorflow.grpc.MasterService\', \'CloseSession\'): face_utilities.unary_unary_inline(servicer.CloseSession),\n      (\'tensorflow.grpc.MasterService\', \'CreateSession\'): face_utilities.unary_unary_inline(servicer.CreateSession),\n      (\'tensorflow.grpc.MasterService\', \'ExtendSession\'): face_utilities.unary_unary_inline(servicer.ExtendSession),\n      (\'tensorflow.grpc.MasterService\', \'ListDevices\'): face_utilities.unary_unary_inline(servicer.ListDevices),\n      (\'tensorflow.grpc.MasterService\', \'PartialRunSetup\'): face_utilities.unary_unary_inline(servicer.PartialRunSetup),\n      (\'tensorflow.grpc.MasterService\', \'Reset\'): face_utilities.unary_unary_inline(servicer.Reset),\n      (\'tensorflow.grpc.MasterService\', \'RunStep\'): face_utilities.unary_unary_inline(servicer.RunStep),\n    }\n    server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n    return beta_implementations.server(method_implementations, options=server_options)\n\n\n  def beta_create_MasterService_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_serializers = {\n      (\'tensorflow.grpc.MasterService\', \'CloseSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionRequest.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'CreateSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionRequest.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'ExtendSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionRequest.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'ListDevices\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesRequest.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'PartialRunSetup\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupRequest.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'Reset\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetRequest.SerializeToString,\n      (\'tensorflow.grpc.MasterService\', \'RunStep\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepRequest.SerializeToString,\n    }\n    response_deserializers = {\n      (\'tensorflow.grpc.MasterService\', \'CloseSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionResponse.FromString,\n      (\'tensorflow.grpc.MasterService\', \'CreateSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionResponse.FromString,\n      (\'tensorflow.grpc.MasterService\', \'ExtendSession\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionResponse.FromString,\n      (\'tensorflow.grpc.MasterService\', \'ListDevices\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesResponse.FromString,\n      (\'tensorflow.grpc.MasterService\', \'PartialRunSetup\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupResponse.FromString,\n      (\'tensorflow.grpc.MasterService\', \'Reset\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetResponse.FromString,\n      (\'tensorflow.grpc.MasterService\', \'RunStep\'): tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepResponse.FromString,\n    }\n    cardinalities = {\n      \'CloseSession\': cardinality.Cardinality.UNARY_UNARY,\n      \'CreateSession\': cardinality.Cardinality.UNARY_UNARY,\n      \'ExtendSession\': cardinality.Cardinality.UNARY_UNARY,\n      \'ListDevices\': cardinality.Cardinality.UNARY_UNARY,\n      \'PartialRunSetup\': cardinality.Cardinality.UNARY_UNARY,\n      \'Reset\': cardinality.Cardinality.UNARY_UNARY,\n      \'RunStep\': cardinality.Cardinality.UNARY_UNARY,\n    }\n    stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n    return beta_implementations.dynamic_stub(channel, \'tensorflow.grpc.MasterService\', cardinalities, options=stub_options)\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/master_service_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom tensorflow.core.protobuf import master_pb2 as tensorflow_dot_core_dot_protobuf_dot_master__pb2\n\n\nclass MasterServiceStub(object):\n  """"""//////////////////////////////////////////////////////////////////////////////\n\n  MasterService defines a TensorFlow service with which a client can\n  interact to execute a distributed TensorFlow computation.\n\n  A master service keeps track of multiple ""master sessions"". Each\n  session encapsulates a computation graph and its associated state,\n  and typically corresponds to a single ""client session"" (e.g. a\n  `tensorflow::Session` instance).\n\n  A session is responsible for the following:\n  * assigning each node to a device (locally or remotely) using a\n  placement algorithm. This may make decisions based on collected\n  statistics from the workers in the system (e.g., memory usage,\n  bandwidth consumption, etc.)\n\n  * inserting intermediate nodes and edges to support cross-device\n  and cross-process data flows and resource management.\n\n  * issuing commands to workers to execute the subgraphs associated\n  with those workers.\n\n  Typically, a client carries out an iterative computation\n  (e.g. training) by invoking RPCs against the master in a\n  client-side loop. The client first creates a client session that\n  connects to a particular master (using gRPC for example). The\n  master creates a corresponding master session that is hosted on\n  the master and caches state between the client\'s invocations.\n\n  After the session is established, the master returns an opaque\n  handle to the client that can be used to associate the client and\n  master sessions.\n\n  The client may send an initial graph to the master in the\n  CreateSession call, and add nodes to the graph using ExtendSession.\n\n  The most frequent operation a master is ""RunStep"", which implements\n  the `Session::Run()` API. It supports feeding in arguments,\n  executing a dataflow computation, and fetching arguments.\n\n  Finally, when the client no longer needs the session, it should\n  close the session by invoking CloseSession, which allows the master\n  to reclaim resources associated with the session. The master may\n  implement a garbage collection scheme that closes sessions that\n  have been inactive for some time.\n\n  For example, the following pseudo-code illustrates how a client\n  interacts with a master:\n\n  stub = NewStub(""/job:mnist/replica:0/task:0"")\n  {handle} = stub->CreateSession({graph_def})\n  do {\n  stub->RunStep({handle, {feeds}, {fetches}})\n  // The client can evaluate a predicate locally, based on the\n  // result of `fetches`, to determine whether to terminate. For\n  // example, it might fetch the loss and evaluate whether it is less\n  // than some threshold.\n  } while (!should_stop({fetches}));\n  stub->CloseSession({handle})\n\n  //////////////////////////////////////////////////////////////////////////////\n\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.CreateSession = channel.unary_unary(\n        \'/tensorflow.grpc.MasterService/CreateSession\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionResponse.FromString,\n        )\n    self.ExtendSession = channel.unary_unary(\n        \'/tensorflow.grpc.MasterService/ExtendSession\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionResponse.FromString,\n        )\n    self.PartialRunSetup = channel.unary_unary(\n        \'/tensorflow.grpc.MasterService/PartialRunSetup\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupResponse.FromString,\n        )\n    self.RunStep = channel.unary_unary(\n        \'/tensorflow.grpc.MasterService/RunStep\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepResponse.FromString,\n        )\n    self.CloseSession = channel.unary_unary(\n        \'/tensorflow.grpc.MasterService/CloseSession\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionResponse.FromString,\n        )\n    self.ListDevices = channel.unary_unary(\n        \'/tensorflow.grpc.MasterService/ListDevices\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesResponse.FromString,\n        )\n    self.Reset = channel.unary_unary(\n        \'/tensorflow.grpc.MasterService/Reset\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetResponse.FromString,\n        )\n\n\nclass MasterServiceServicer(object):\n  """"""//////////////////////////////////////////////////////////////////////////////\n\n  MasterService defines a TensorFlow service with which a client can\n  interact to execute a distributed TensorFlow computation.\n\n  A master service keeps track of multiple ""master sessions"". Each\n  session encapsulates a computation graph and its associated state,\n  and typically corresponds to a single ""client session"" (e.g. a\n  `tensorflow::Session` instance).\n\n  A session is responsible for the following:\n  * assigning each node to a device (locally or remotely) using a\n  placement algorithm. This may make decisions based on collected\n  statistics from the workers in the system (e.g., memory usage,\n  bandwidth consumption, etc.)\n\n  * inserting intermediate nodes and edges to support cross-device\n  and cross-process data flows and resource management.\n\n  * issuing commands to workers to execute the subgraphs associated\n  with those workers.\n\n  Typically, a client carries out an iterative computation\n  (e.g. training) by invoking RPCs against the master in a\n  client-side loop. The client first creates a client session that\n  connects to a particular master (using gRPC for example). The\n  master creates a corresponding master session that is hosted on\n  the master and caches state between the client\'s invocations.\n\n  After the session is established, the master returns an opaque\n  handle to the client that can be used to associate the client and\n  master sessions.\n\n  The client may send an initial graph to the master in the\n  CreateSession call, and add nodes to the graph using ExtendSession.\n\n  The most frequent operation a master is ""RunStep"", which implements\n  the `Session::Run()` API. It supports feeding in arguments,\n  executing a dataflow computation, and fetching arguments.\n\n  Finally, when the client no longer needs the session, it should\n  close the session by invoking CloseSession, which allows the master\n  to reclaim resources associated with the session. The master may\n  implement a garbage collection scheme that closes sessions that\n  have been inactive for some time.\n\n  For example, the following pseudo-code illustrates how a client\n  interacts with a master:\n\n  stub = NewStub(""/job:mnist/replica:0/task:0"")\n  {handle} = stub->CreateSession({graph_def})\n  do {\n  stub->RunStep({handle, {feeds}, {fetches}})\n  // The client can evaluate a predicate locally, based on the\n  // result of `fetches`, to determine whether to terminate. For\n  // example, it might fetch the loss and evaluate whether it is less\n  // than some threshold.\n  } while (!should_stop({fetches}));\n  stub->CloseSession({handle})\n\n  //////////////////////////////////////////////////////////////////////////////\n\n  """"""\n\n  def CreateSession(self, request, context):\n    """"""Creates a session.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def ExtendSession(self, request, context):\n    """"""Extends a session.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def PartialRunSetup(self, request, context):\n    """"""Prepares future partial run calls.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def RunStep(self, request, context):\n    """"""Drives the graph computation.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def CloseSession(self, request, context):\n    """"""Closes a session.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def ListDevices(self, request, context):\n    """"""List the devices usable by the master.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Reset(self, request, context):\n    """"""Close and abandon all existing sessions.  Ongoing computations\n    will no longer affect fresh ones via the resources in containers listed in\n    the ResetRequest.  See ResetRequest for more details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_MasterServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'CreateSession\': grpc.unary_unary_rpc_method_handler(\n          servicer.CreateSession,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CreateSessionResponse.SerializeToString,\n      ),\n      \'ExtendSession\': grpc.unary_unary_rpc_method_handler(\n          servicer.ExtendSession,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ExtendSessionResponse.SerializeToString,\n      ),\n      \'PartialRunSetup\': grpc.unary_unary_rpc_method_handler(\n          servicer.PartialRunSetup,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.PartialRunSetupResponse.SerializeToString,\n      ),\n      \'RunStep\': grpc.unary_unary_rpc_method_handler(\n          servicer.RunStep,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.RunStepResponse.SerializeToString,\n      ),\n      \'CloseSession\': grpc.unary_unary_rpc_method_handler(\n          servicer.CloseSession,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.CloseSessionResponse.SerializeToString,\n      ),\n      \'ListDevices\': grpc.unary_unary_rpc_method_handler(\n          servicer.ListDevices,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ListDevicesResponse.SerializeToString,\n      ),\n      \'Reset\': grpc.unary_unary_rpc_method_handler(\n          servicer.Reset,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_master__pb2.ResetResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.grpc.MasterService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
libs/pipeline_model/tensorflow/core/protobuf/meta_graph_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/meta_graph.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import any_pb2 as google_dot_protobuf_dot_any__pb2\nfrom tensorflow.core.framework import graph_pb2 as tensorflow_dot_core_dot_framework_dot_graph__pb2\nfrom tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\nfrom tensorflow.core.protobuf import saver_pb2 as tensorflow_dot_core_dot_protobuf_dot_saver__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/meta_graph.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n)tensorflow/core/protobuf/meta_graph.proto\\x12\\ntensorflow\\x1a\\x19google/protobuf/any.proto\\x1a%tensorflow/core/framework/graph.proto\\x1a&tensorflow/core/framework/op_def.proto\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a%tensorflow/core/framework/types.proto\\x1a$tensorflow/core/protobuf/saver.proto\\""\\xe3\\x05\\n\\x0cMetaGraphDef\\x12;\\n\\rmeta_info_def\\x18\\x01 \\x01(\\x0b\\x32$.tensorflow.MetaGraphDef.MetaInfoDef\\x12\\\'\\n\\tgraph_def\\x18\\x02 \\x01(\\x0b\\x32\\x14.tensorflow.GraphDef\\x12\\\'\\n\\tsaver_def\\x18\\x03 \\x01(\\x0b\\x32\\x14.tensorflow.SaverDef\\x12\\x43\\n\\x0e\\x63ollection_def\\x18\\x04 \\x03(\\x0b\\x32+.tensorflow.MetaGraphDef.CollectionDefEntry\\x12\\x41\\n\\rsignature_def\\x18\\x05 \\x03(\\x0b\\x32*.tensorflow.MetaGraphDef.SignatureDefEntry\\x12\\x30\\n\\x0e\\x61sset_file_def\\x18\\x06 \\x03(\\x0b\\x32\\x18.tensorflow.AssetFileDef\\x1a\\xe9\\x01\\n\\x0bMetaInfoDef\\x12\\x1a\\n\\x12meta_graph_version\\x18\\x01 \\x01(\\t\\x12,\\n\\x10stripped_op_list\\x18\\x02 \\x01(\\x0b\\x32\\x12.tensorflow.OpList\\x12&\\n\\x08\\x61ny_info\\x18\\x03 \\x01(\\x0b\\x32\\x14.google.protobuf.Any\\x12\\x0c\\n\\x04tags\\x18\\x04 \\x03(\\t\\x12\\x1a\\n\\x12tensorflow_version\\x18\\x05 \\x01(\\t\\x12\\x1e\\n\\x16tensorflow_git_version\\x18\\x06 \\x01(\\t\\x12\\x1e\\n\\x16stripped_default_attrs\\x18\\x07 \\x01(\\x08\\x1aO\\n\\x12\\x43ollectionDefEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12(\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x19.tensorflow.CollectionDef:\\x02\\x38\\x01\\x1aM\\n\\x11SignatureDefEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x18.tensorflow.SignatureDef:\\x02\\x38\\x01\\""\\xdf\\x03\\n\\rCollectionDef\\x12\\x37\\n\\tnode_list\\x18\\x01 \\x01(\\x0b\\x32\\"".tensorflow.CollectionDef.NodeListH\\x00\\x12\\x39\\n\\nbytes_list\\x18\\x02 \\x01(\\x0b\\x32#.tensorflow.CollectionDef.BytesListH\\x00\\x12\\x39\\n\\nint64_list\\x18\\x03 \\x01(\\x0b\\x32#.tensorflow.CollectionDef.Int64ListH\\x00\\x12\\x39\\n\\nfloat_list\\x18\\x04 \\x01(\\x0b\\x32#.tensorflow.CollectionDef.FloatListH\\x00\\x12\\x35\\n\\x08\\x61ny_list\\x18\\x05 \\x01(\\x0b\\x32!.tensorflow.CollectionDef.AnyListH\\x00\\x1a\\x19\\n\\x08NodeList\\x12\\r\\n\\x05value\\x18\\x01 \\x03(\\t\\x1a\\x1a\\n\\tBytesList\\x12\\r\\n\\x05value\\x18\\x01 \\x03(\\x0c\\x1a\\x1e\\n\\tInt64List\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\x1a\\x1e\\n\\tFloatList\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x02\\x42\\x02\\x10\\x01\\x1a.\\n\\x07\\x41nyList\\x12#\\n\\x05value\\x18\\x01 \\x03(\\x0b\\x32\\x14.google.protobuf.AnyB\\x06\\n\\x04kind\\""\\xa0\\x02\\n\\nTensorInfo\\x12\\x0e\\n\\x04name\\x18\\x01 \\x01(\\tH\\x00\\x12\\x36\\n\\ncoo_sparse\\x18\\x04 \\x01(\\x0b\\x32 .tensorflow.TensorInfo.CooSparseH\\x00\\x12#\\n\\x05\\x64type\\x18\\x02 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12\\x32\\n\\x0ctensor_shape\\x18\\x03 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\x1a\\x65\\n\\tCooSparse\\x12\\x1a\\n\\x12values_tensor_name\\x18\\x01 \\x01(\\t\\x12\\x1b\\n\\x13indices_tensor_name\\x18\\x02 \\x01(\\t\\x12\\x1f\\n\\x17\\x64\\x65nse_shape_tensor_name\\x18\\x03 \\x01(\\tB\\n\\n\\x08\\x65ncoding\\""\\xa0\\x02\\n\\x0cSignatureDef\\x12\\x34\\n\\x06inputs\\x18\\x01 \\x03(\\x0b\\x32$.tensorflow.SignatureDef.InputsEntry\\x12\\x36\\n\\x07outputs\\x18\\x02 \\x03(\\x0b\\x32%.tensorflow.SignatureDef.OutputsEntry\\x12\\x13\\n\\x0bmethod_name\\x18\\x03 \\x01(\\t\\x1a\\x45\\n\\x0bInputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12%\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x16.tensorflow.TensorInfo:\\x02\\x38\\x01\\x1a\\x46\\n\\x0cOutputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12%\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x16.tensorflow.TensorInfo:\\x02\\x38\\x01\\""M\\n\\x0c\\x41ssetFileDef\\x12+\\n\\x0btensor_info\\x18\\x01 \\x01(\\x0b\\x32\\x16.tensorflow.TensorInfo\\x12\\x10\\n\\x08\\x66ilename\\x18\\x02 \\x01(\\tB0\\n\\x18org.tensorflow.frameworkB\\x0fMetaGraphProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_graph__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_op__def__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_saver__pb2.DESCRIPTOR,])\n\n\n\n\n_METAGRAPHDEF_METAINFODEF = _descriptor.Descriptor(\n  name=\'MetaInfoDef\',\n  full_name=\'tensorflow.MetaGraphDef.MetaInfoDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta_graph_version\', full_name=\'tensorflow.MetaGraphDef.MetaInfoDef.meta_graph_version\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stripped_op_list\', full_name=\'tensorflow.MetaGraphDef.MetaInfoDef.stripped_op_list\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'any_info\', full_name=\'tensorflow.MetaGraphDef.MetaInfoDef.any_info\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tags\', full_name=\'tensorflow.MetaGraphDef.MetaInfoDef.tags\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensorflow_version\', full_name=\'tensorflow.MetaGraphDef.MetaInfoDef.tensorflow_version\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensorflow_git_version\', full_name=\'tensorflow.MetaGraphDef.MetaInfoDef.tensorflow_git_version\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stripped_default_attrs\', full_name=\'tensorflow.MetaGraphDef.MetaInfoDef.stripped_default_attrs\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=633,\n  serialized_end=866,\n)\n\n_METAGRAPHDEF_COLLECTIONDEFENTRY = _descriptor.Descriptor(\n  name=\'CollectionDefEntry\',\n  full_name=\'tensorflow.MetaGraphDef.CollectionDefEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.MetaGraphDef.CollectionDefEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.MetaGraphDef.CollectionDefEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=868,\n  serialized_end=947,\n)\n\n_METAGRAPHDEF_SIGNATUREDEFENTRY = _descriptor.Descriptor(\n  name=\'SignatureDefEntry\',\n  full_name=\'tensorflow.MetaGraphDef.SignatureDefEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.MetaGraphDef.SignatureDefEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.MetaGraphDef.SignatureDefEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=949,\n  serialized_end=1026,\n)\n\n_METAGRAPHDEF = _descriptor.Descriptor(\n  name=\'MetaGraphDef\',\n  full_name=\'tensorflow.MetaGraphDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta_info_def\', full_name=\'tensorflow.MetaGraphDef.meta_info_def\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_def\', full_name=\'tensorflow.MetaGraphDef.graph_def\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'saver_def\', full_name=\'tensorflow.MetaGraphDef.saver_def\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'collection_def\', full_name=\'tensorflow.MetaGraphDef.collection_def\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'signature_def\', full_name=\'tensorflow.MetaGraphDef.signature_def\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'asset_file_def\', full_name=\'tensorflow.MetaGraphDef.asset_file_def\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_METAGRAPHDEF_METAINFODEF, _METAGRAPHDEF_COLLECTIONDEFENTRY, _METAGRAPHDEF_SIGNATUREDEFENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=287,\n  serialized_end=1026,\n)\n\n\n_COLLECTIONDEF_NODELIST = _descriptor.Descriptor(\n  name=\'NodeList\',\n  full_name=\'tensorflow.CollectionDef.NodeList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.CollectionDef.NodeList.value\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1335,\n  serialized_end=1360,\n)\n\n_COLLECTIONDEF_BYTESLIST = _descriptor.Descriptor(\n  name=\'BytesList\',\n  full_name=\'tensorflow.CollectionDef.BytesList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.CollectionDef.BytesList.value\', index=0,\n      number=1, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1362,\n  serialized_end=1388,\n)\n\n_COLLECTIONDEF_INT64LIST = _descriptor.Descriptor(\n  name=\'Int64List\',\n  full_name=\'tensorflow.CollectionDef.Int64List\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.CollectionDef.Int64List.value\', index=0,\n      number=1, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1390,\n  serialized_end=1420,\n)\n\n_COLLECTIONDEF_FLOATLIST = _descriptor.Descriptor(\n  name=\'FloatList\',\n  full_name=\'tensorflow.CollectionDef.FloatList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.CollectionDef.FloatList.value\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1422,\n  serialized_end=1452,\n)\n\n_COLLECTIONDEF_ANYLIST = _descriptor.Descriptor(\n  name=\'AnyList\',\n  full_name=\'tensorflow.CollectionDef.AnyList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.CollectionDef.AnyList.value\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1454,\n  serialized_end=1500,\n)\n\n_COLLECTIONDEF = _descriptor.Descriptor(\n  name=\'CollectionDef\',\n  full_name=\'tensorflow.CollectionDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_list\', full_name=\'tensorflow.CollectionDef.node_list\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bytes_list\', full_name=\'tensorflow.CollectionDef.bytes_list\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'int64_list\', full_name=\'tensorflow.CollectionDef.int64_list\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'float_list\', full_name=\'tensorflow.CollectionDef.float_list\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'any_list\', full_name=\'tensorflow.CollectionDef.any_list\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_COLLECTIONDEF_NODELIST, _COLLECTIONDEF_BYTESLIST, _COLLECTIONDEF_INT64LIST, _COLLECTIONDEF_FLOATLIST, _COLLECTIONDEF_ANYLIST, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'kind\', full_name=\'tensorflow.CollectionDef.kind\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=1029,\n  serialized_end=1508,\n)\n\n\n_TENSORINFO_COOSPARSE = _descriptor.Descriptor(\n  name=\'CooSparse\',\n  full_name=\'tensorflow.TensorInfo.CooSparse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'values_tensor_name\', full_name=\'tensorflow.TensorInfo.CooSparse.values_tensor_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'indices_tensor_name\', full_name=\'tensorflow.TensorInfo.CooSparse.indices_tensor_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dense_shape_tensor_name\', full_name=\'tensorflow.TensorInfo.CooSparse.dense_shape_tensor_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1686,\n  serialized_end=1787,\n)\n\n_TENSORINFO = _descriptor.Descriptor(\n  name=\'TensorInfo\',\n  full_name=\'tensorflow.TensorInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.TensorInfo.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'coo_sparse\', full_name=\'tensorflow.TensorInfo.coo_sparse\', index=1,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.TensorInfo.dtype\', index=2,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor_shape\', full_name=\'tensorflow.TensorInfo.tensor_shape\', index=3,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TENSORINFO_COOSPARSE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'encoding\', full_name=\'tensorflow.TensorInfo.encoding\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=1511,\n  serialized_end=1799,\n)\n\n\n_SIGNATUREDEF_INPUTSENTRY = _descriptor.Descriptor(\n  name=\'InputsEntry\',\n  full_name=\'tensorflow.SignatureDef.InputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.SignatureDef.InputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.SignatureDef.InputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1949,\n  serialized_end=2018,\n)\n\n_SIGNATUREDEF_OUTPUTSENTRY = _descriptor.Descriptor(\n  name=\'OutputsEntry\',\n  full_name=\'tensorflow.SignatureDef.OutputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.SignatureDef.OutputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.SignatureDef.OutputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2020,\n  serialized_end=2090,\n)\n\n_SIGNATUREDEF = _descriptor.Descriptor(\n  name=\'SignatureDef\',\n  full_name=\'tensorflow.SignatureDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'inputs\', full_name=\'tensorflow.SignatureDef.inputs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'outputs\', full_name=\'tensorflow.SignatureDef.outputs\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'method_name\', full_name=\'tensorflow.SignatureDef.method_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SIGNATUREDEF_INPUTSENTRY, _SIGNATUREDEF_OUTPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1802,\n  serialized_end=2090,\n)\n\n\n_ASSETFILEDEF = _descriptor.Descriptor(\n  name=\'AssetFileDef\',\n  full_name=\'tensorflow.AssetFileDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tensor_info\', full_name=\'tensorflow.AssetFileDef.tensor_info\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'filename\', full_name=\'tensorflow.AssetFileDef.filename\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2092,\n  serialized_end=2169,\n)\n\n_METAGRAPHDEF_METAINFODEF.fields_by_name[\'stripped_op_list\'].message_type = tensorflow_dot_core_dot_framework_dot_op__def__pb2._OPLIST\n_METAGRAPHDEF_METAINFODEF.fields_by_name[\'any_info\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_METAGRAPHDEF_METAINFODEF.containing_type = _METAGRAPHDEF\n_METAGRAPHDEF_COLLECTIONDEFENTRY.fields_by_name[\'value\'].message_type = _COLLECTIONDEF\n_METAGRAPHDEF_COLLECTIONDEFENTRY.containing_type = _METAGRAPHDEF\n_METAGRAPHDEF_SIGNATUREDEFENTRY.fields_by_name[\'value\'].message_type = _SIGNATUREDEF\n_METAGRAPHDEF_SIGNATUREDEFENTRY.containing_type = _METAGRAPHDEF\n_METAGRAPHDEF.fields_by_name[\'meta_info_def\'].message_type = _METAGRAPHDEF_METAINFODEF\n_METAGRAPHDEF.fields_by_name[\'graph_def\'].message_type = tensorflow_dot_core_dot_framework_dot_graph__pb2._GRAPHDEF\n_METAGRAPHDEF.fields_by_name[\'saver_def\'].message_type = tensorflow_dot_core_dot_protobuf_dot_saver__pb2._SAVERDEF\n_METAGRAPHDEF.fields_by_name[\'collection_def\'].message_type = _METAGRAPHDEF_COLLECTIONDEFENTRY\n_METAGRAPHDEF.fields_by_name[\'signature_def\'].message_type = _METAGRAPHDEF_SIGNATUREDEFENTRY\n_METAGRAPHDEF.fields_by_name[\'asset_file_def\'].message_type = _ASSETFILEDEF\n_COLLECTIONDEF_NODELIST.containing_type = _COLLECTIONDEF\n_COLLECTIONDEF_BYTESLIST.containing_type = _COLLECTIONDEF\n_COLLECTIONDEF_INT64LIST.containing_type = _COLLECTIONDEF\n_COLLECTIONDEF_FLOATLIST.containing_type = _COLLECTIONDEF\n_COLLECTIONDEF_ANYLIST.fields_by_name[\'value\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_COLLECTIONDEF_ANYLIST.containing_type = _COLLECTIONDEF\n_COLLECTIONDEF.fields_by_name[\'node_list\'].message_type = _COLLECTIONDEF_NODELIST\n_COLLECTIONDEF.fields_by_name[\'bytes_list\'].message_type = _COLLECTIONDEF_BYTESLIST\n_COLLECTIONDEF.fields_by_name[\'int64_list\'].message_type = _COLLECTIONDEF_INT64LIST\n_COLLECTIONDEF.fields_by_name[\'float_list\'].message_type = _COLLECTIONDEF_FLOATLIST\n_COLLECTIONDEF.fields_by_name[\'any_list\'].message_type = _COLLECTIONDEF_ANYLIST\n_COLLECTIONDEF.oneofs_by_name[\'kind\'].fields.append(\n  _COLLECTIONDEF.fields_by_name[\'node_list\'])\n_COLLECTIONDEF.fields_by_name[\'node_list\'].containing_oneof = _COLLECTIONDEF.oneofs_by_name[\'kind\']\n_COLLECTIONDEF.oneofs_by_name[\'kind\'].fields.append(\n  _COLLECTIONDEF.fields_by_name[\'bytes_list\'])\n_COLLECTIONDEF.fields_by_name[\'bytes_list\'].containing_oneof = _COLLECTIONDEF.oneofs_by_name[\'kind\']\n_COLLECTIONDEF.oneofs_by_name[\'kind\'].fields.append(\n  _COLLECTIONDEF.fields_by_name[\'int64_list\'])\n_COLLECTIONDEF.fields_by_name[\'int64_list\'].containing_oneof = _COLLECTIONDEF.oneofs_by_name[\'kind\']\n_COLLECTIONDEF.oneofs_by_name[\'kind\'].fields.append(\n  _COLLECTIONDEF.fields_by_name[\'float_list\'])\n_COLLECTIONDEF.fields_by_name[\'float_list\'].containing_oneof = _COLLECTIONDEF.oneofs_by_name[\'kind\']\n_COLLECTIONDEF.oneofs_by_name[\'kind\'].fields.append(\n  _COLLECTIONDEF.fields_by_name[\'any_list\'])\n_COLLECTIONDEF.fields_by_name[\'any_list\'].containing_oneof = _COLLECTIONDEF.oneofs_by_name[\'kind\']\n_TENSORINFO_COOSPARSE.containing_type = _TENSORINFO\n_TENSORINFO.fields_by_name[\'coo_sparse\'].message_type = _TENSORINFO_COOSPARSE\n_TENSORINFO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_TENSORINFO.fields_by_name[\'tensor_shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_TENSORINFO.oneofs_by_name[\'encoding\'].fields.append(\n  _TENSORINFO.fields_by_name[\'name\'])\n_TENSORINFO.fields_by_name[\'name\'].containing_oneof = _TENSORINFO.oneofs_by_name[\'encoding\']\n_TENSORINFO.oneofs_by_name[\'encoding\'].fields.append(\n  _TENSORINFO.fields_by_name[\'coo_sparse\'])\n_TENSORINFO.fields_by_name[\'coo_sparse\'].containing_oneof = _TENSORINFO.oneofs_by_name[\'encoding\']\n_SIGNATUREDEF_INPUTSENTRY.fields_by_name[\'value\'].message_type = _TENSORINFO\n_SIGNATUREDEF_INPUTSENTRY.containing_type = _SIGNATUREDEF\n_SIGNATUREDEF_OUTPUTSENTRY.fields_by_name[\'value\'].message_type = _TENSORINFO\n_SIGNATUREDEF_OUTPUTSENTRY.containing_type = _SIGNATUREDEF\n_SIGNATUREDEF.fields_by_name[\'inputs\'].message_type = _SIGNATUREDEF_INPUTSENTRY\n_SIGNATUREDEF.fields_by_name[\'outputs\'].message_type = _SIGNATUREDEF_OUTPUTSENTRY\n_ASSETFILEDEF.fields_by_name[\'tensor_info\'].message_type = _TENSORINFO\nDESCRIPTOR.message_types_by_name[\'MetaGraphDef\'] = _METAGRAPHDEF\nDESCRIPTOR.message_types_by_name[\'CollectionDef\'] = _COLLECTIONDEF\nDESCRIPTOR.message_types_by_name[\'TensorInfo\'] = _TENSORINFO\nDESCRIPTOR.message_types_by_name[\'SignatureDef\'] = _SIGNATUREDEF\nDESCRIPTOR.message_types_by_name[\'AssetFileDef\'] = _ASSETFILEDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMetaGraphDef = _reflection.GeneratedProtocolMessageType(\'MetaGraphDef\', (_message.Message,), dict(\n\n  MetaInfoDef = _reflection.GeneratedProtocolMessageType(\'MetaInfoDef\', (_message.Message,), dict(\n    DESCRIPTOR = _METAGRAPHDEF_METAINFODEF,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.MetaGraphDef.MetaInfoDef)\n    ))\n  ,\n\n  CollectionDefEntry = _reflection.GeneratedProtocolMessageType(\'CollectionDefEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _METAGRAPHDEF_COLLECTIONDEFENTRY,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.MetaGraphDef.CollectionDefEntry)\n    ))\n  ,\n\n  SignatureDefEntry = _reflection.GeneratedProtocolMessageType(\'SignatureDefEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _METAGRAPHDEF_SIGNATUREDEFENTRY,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.MetaGraphDef.SignatureDefEntry)\n    ))\n  ,\n  DESCRIPTOR = _METAGRAPHDEF,\n  __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.MetaGraphDef)\n  ))\n_sym_db.RegisterMessage(MetaGraphDef)\n_sym_db.RegisterMessage(MetaGraphDef.MetaInfoDef)\n_sym_db.RegisterMessage(MetaGraphDef.CollectionDefEntry)\n_sym_db.RegisterMessage(MetaGraphDef.SignatureDefEntry)\n\nCollectionDef = _reflection.GeneratedProtocolMessageType(\'CollectionDef\', (_message.Message,), dict(\n\n  NodeList = _reflection.GeneratedProtocolMessageType(\'NodeList\', (_message.Message,), dict(\n    DESCRIPTOR = _COLLECTIONDEF_NODELIST,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.CollectionDef.NodeList)\n    ))\n  ,\n\n  BytesList = _reflection.GeneratedProtocolMessageType(\'BytesList\', (_message.Message,), dict(\n    DESCRIPTOR = _COLLECTIONDEF_BYTESLIST,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.CollectionDef.BytesList)\n    ))\n  ,\n\n  Int64List = _reflection.GeneratedProtocolMessageType(\'Int64List\', (_message.Message,), dict(\n    DESCRIPTOR = _COLLECTIONDEF_INT64LIST,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.CollectionDef.Int64List)\n    ))\n  ,\n\n  FloatList = _reflection.GeneratedProtocolMessageType(\'FloatList\', (_message.Message,), dict(\n    DESCRIPTOR = _COLLECTIONDEF_FLOATLIST,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.CollectionDef.FloatList)\n    ))\n  ,\n\n  AnyList = _reflection.GeneratedProtocolMessageType(\'AnyList\', (_message.Message,), dict(\n    DESCRIPTOR = _COLLECTIONDEF_ANYLIST,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.CollectionDef.AnyList)\n    ))\n  ,\n  DESCRIPTOR = _COLLECTIONDEF,\n  __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CollectionDef)\n  ))\n_sym_db.RegisterMessage(CollectionDef)\n_sym_db.RegisterMessage(CollectionDef.NodeList)\n_sym_db.RegisterMessage(CollectionDef.BytesList)\n_sym_db.RegisterMessage(CollectionDef.Int64List)\n_sym_db.RegisterMessage(CollectionDef.FloatList)\n_sym_db.RegisterMessage(CollectionDef.AnyList)\n\nTensorInfo = _reflection.GeneratedProtocolMessageType(\'TensorInfo\', (_message.Message,), dict(\n\n  CooSparse = _reflection.GeneratedProtocolMessageType(\'CooSparse\', (_message.Message,), dict(\n    DESCRIPTOR = _TENSORINFO_COOSPARSE,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.TensorInfo.CooSparse)\n    ))\n  ,\n  DESCRIPTOR = _TENSORINFO,\n  __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TensorInfo)\n  ))\n_sym_db.RegisterMessage(TensorInfo)\n_sym_db.RegisterMessage(TensorInfo.CooSparse)\n\nSignatureDef = _reflection.GeneratedProtocolMessageType(\'SignatureDef\', (_message.Message,), dict(\n\n  InputsEntry = _reflection.GeneratedProtocolMessageType(\'InputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SIGNATUREDEF_INPUTSENTRY,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.SignatureDef.InputsEntry)\n    ))\n  ,\n\n  OutputsEntry = _reflection.GeneratedProtocolMessageType(\'OutputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SIGNATUREDEF_OUTPUTSENTRY,\n    __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.SignatureDef.OutputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _SIGNATUREDEF,\n  __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.SignatureDef)\n  ))\n_sym_db.RegisterMessage(SignatureDef)\n_sym_db.RegisterMessage(SignatureDef.InputsEntry)\n_sym_db.RegisterMessage(SignatureDef.OutputsEntry)\n\nAssetFileDef = _reflection.GeneratedProtocolMessageType(\'AssetFileDef\', (_message.Message,), dict(\n  DESCRIPTOR = _ASSETFILEDEF,\n  __module__ = \'tensorflow.core.protobuf.meta_graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.AssetFileDef)\n  ))\n_sym_db.RegisterMessage(AssetFileDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\017MetaGraphProtosP\\001\\370\\001\\001\'))\n_METAGRAPHDEF_COLLECTIONDEFENTRY.has_options = True\n_METAGRAPHDEF_COLLECTIONDEFENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_METAGRAPHDEF_SIGNATUREDEFENTRY.has_options = True\n_METAGRAPHDEF_SIGNATUREDEFENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_COLLECTIONDEF_INT64LIST.fields_by_name[\'value\'].has_options = True\n_COLLECTIONDEF_INT64LIST.fields_by_name[\'value\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_COLLECTIONDEF_FLOATLIST.fields_by_name[\'value\'].has_options = True\n_COLLECTIONDEF_FLOATLIST.fields_by_name[\'value\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_SIGNATUREDEF_INPUTSENTRY.has_options = True\n_SIGNATUREDEF_INPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_SIGNATUREDEF_OUTPUTSENTRY.has_options = True\n_SIGNATUREDEF_OUTPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/meta_graph_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/named_tensor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/named_tensor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/named_tensor.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n+tensorflow/core/protobuf/named_tensor.proto\\x12\\ntensorflow\\x1a&tensorflow/core/framework/tensor.proto\\""I\\n\\x10NamedTensorProto\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\x06tensor\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProtoB2\\n\\x18org.tensorflow.frameworkB\\x11NamedTensorProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,])\n\n\n\n\n_NAMEDTENSORPROTO = _descriptor.Descriptor(\n  name=\'NamedTensorProto\',\n  full_name=\'tensorflow.NamedTensorProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.NamedTensorProto.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.NamedTensorProto.tensor\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=99,\n  serialized_end=172,\n)\n\n_NAMEDTENSORPROTO.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\nDESCRIPTOR.message_types_by_name[\'NamedTensorProto\'] = _NAMEDTENSORPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nNamedTensorProto = _reflection.GeneratedProtocolMessageType(\'NamedTensorProto\', (_message.Message,), dict(\n  DESCRIPTOR = _NAMEDTENSORPROTO,\n  __module__ = \'tensorflow.core.protobuf.named_tensor_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.NamedTensorProto)\n  ))\n_sym_db.RegisterMessage(NamedTensorProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\021NamedTensorProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/named_tensor_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/queue_runner_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/queue_runner.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.lib.core import error_codes_pb2 as tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/queue_runner.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n+tensorflow/core/protobuf/queue_runner.proto\\x12\\ntensorflow\\x1a*tensorflow/core/lib/core/error_codes.proto\\""\\xaa\\x01\\n\\x0eQueueRunnerDef\\x12\\x12\\n\\nqueue_name\\x18\\x01 \\x01(\\t\\x12\\x17\\n\\x0f\\x65nqueue_op_name\\x18\\x02 \\x03(\\t\\x12\\x15\\n\\rclose_op_name\\x18\\x03 \\x01(\\t\\x12\\x16\\n\\x0e\\x63\\x61ncel_op_name\\x18\\x04 \\x01(\\t\\x12<\\n\\x1cqueue_closed_exception_types\\x18\\x05 \\x03(\\x0e\\x32\\x16.tensorflow.error.CodeB2\\n\\x18org.tensorflow.frameworkB\\x11QueueRunnerProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2.DESCRIPTOR,])\n\n\n\n\n_QUEUERUNNERDEF = _descriptor.Descriptor(\n  name=\'QueueRunnerDef\',\n  full_name=\'tensorflow.QueueRunnerDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'queue_name\', full_name=\'tensorflow.QueueRunnerDef.queue_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'enqueue_op_name\', full_name=\'tensorflow.QueueRunnerDef.enqueue_op_name\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'close_op_name\', full_name=\'tensorflow.QueueRunnerDef.close_op_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cancel_op_name\', full_name=\'tensorflow.QueueRunnerDef.cancel_op_name\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'queue_closed_exception_types\', full_name=\'tensorflow.QueueRunnerDef.queue_closed_exception_types\', index=4,\n      number=5, type=14, cpp_type=8, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=104,\n  serialized_end=274,\n)\n\n_QUEUERUNNERDEF.fields_by_name[\'queue_closed_exception_types\'].enum_type = tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2._CODE\nDESCRIPTOR.message_types_by_name[\'QueueRunnerDef\'] = _QUEUERUNNERDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nQueueRunnerDef = _reflection.GeneratedProtocolMessageType(\'QueueRunnerDef\', (_message.Message,), dict(\n  DESCRIPTOR = _QUEUERUNNERDEF,\n  __module__ = \'tensorflow.core.protobuf.queue_runner_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.QueueRunnerDef)\n  ))\n_sym_db.RegisterMessage(QueueRunnerDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\021QueueRunnerProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/queue_runner_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/rewriter_config_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/rewriter_config.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/rewriter_config.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n.tensorflow/core/protobuf/rewriter_config.proto\\x12\\ntensorflow\\"";\\n\\x13\\x41utoParallelOptions\\x12\\x0e\\n\\x06\\x65nable\\x18\\x01 \\x01(\\x08\\x12\\x14\\n\\x0cnum_replicas\\x18\\x02 \\x01(\\x05\\""\\xb2\\x05\\n\\x0eRewriterConfig\\x12;\\n\\x10layout_optimizer\\x18\\x01 \\x01(\\x0e\\x32!.tensorflow.RewriterConfig.Toggle\\x12;\\n\\x10\\x63onstant_folding\\x18\\x03 \\x01(\\x0e\\x32!.tensorflow.RewriterConfig.Toggle\\x12\\x42\\n\\x17\\x61rithmetic_optimization\\x18\\x07 \\x01(\\x0e\\x32!.tensorflow.RewriterConfig.Toggle\\x12\\x42\\n\\x17\\x64\\x65pendency_optimization\\x18\\x08 \\x01(\\x0e\\x32!.tensorflow.RewriterConfig.Toggle\\x12\\x1d\\n\\x15\\x64isable_model_pruning\\x18\\x02 \\x01(\\x08\\x12\\x42\\n\\x13memory_optimization\\x18\\x04 \\x01(\\x0e\\x32%.tensorflow.RewriterConfig.MemOptType\\x12\\x30\\n(memory_optimizer_target_node_name_prefix\\x18\\x06 \\x01(\\t\\x12\\x36\\n\\rauto_parallel\\x18\\x05 \\x01(\\x0b\\x32\\x1f.tensorflow.AutoParallelOptions\\x12\\x12\\n\\noptimizers\\x18\\x64 \\x03(\\t\\""6\\n\\x06Toggle\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\x06\\n\\x02ON\\x10\\x01\\x12\\x07\\n\\x03OFF\\x10\\x02\\x12\\x0e\\n\\nAGGRESSIVE\\x10\\x03\\""\\x84\\x01\\n\\nMemOptType\\x12\\x13\\n\\x0f\\x44\\x45\\x46\\x41ULT_MEM_OPT\\x10\\x00\\x12\\x0e\\n\\nNO_MEM_OPT\\x10\\x01\\x12\\n\\n\\x06MANUAL\\x10\\x02\\x12\\x17\\n\\x13SWAPPING_HEURISTICS\\x10\\x04\\x12\\x1c\\n\\x18RECOMPUTATION_HEURISTICS\\x10\\x05\\x12\\x0e\\n\\nHEURISTICS\\x10\\x03\\x42\\x35\\n\\x18org.tensorflow.frameworkB\\x14RewriterConfigProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n_REWRITERCONFIG_TOGGLE = _descriptor.EnumDescriptor(\n  name=\'Toggle\',\n  full_name=\'tensorflow.RewriterConfig.Toggle\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ON\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'OFF\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AGGRESSIVE\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=625,\n  serialized_end=679,\n)\n_sym_db.RegisterEnumDescriptor(_REWRITERCONFIG_TOGGLE)\n\n_REWRITERCONFIG_MEMOPTTYPE = _descriptor.EnumDescriptor(\n  name=\'MemOptType\',\n  full_name=\'tensorflow.RewriterConfig.MemOptType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT_MEM_OPT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NO_MEM_OPT\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MANUAL\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SWAPPING_HEURISTICS\', index=3, number=4,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RECOMPUTATION_HEURISTICS\', index=4, number=5,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HEURISTICS\', index=5, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=682,\n  serialized_end=814,\n)\n_sym_db.RegisterEnumDescriptor(_REWRITERCONFIG_MEMOPTTYPE)\n\n\n_AUTOPARALLELOPTIONS = _descriptor.Descriptor(\n  name=\'AutoParallelOptions\',\n  full_name=\'tensorflow.AutoParallelOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'enable\', full_name=\'tensorflow.AutoParallelOptions.enable\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_replicas\', full_name=\'tensorflow.AutoParallelOptions.num_replicas\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=62,\n  serialized_end=121,\n)\n\n\n_REWRITERCONFIG = _descriptor.Descriptor(\n  name=\'RewriterConfig\',\n  full_name=\'tensorflow.RewriterConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'layout_optimizer\', full_name=\'tensorflow.RewriterConfig.layout_optimizer\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'constant_folding\', full_name=\'tensorflow.RewriterConfig.constant_folding\', index=1,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'arithmetic_optimization\', full_name=\'tensorflow.RewriterConfig.arithmetic_optimization\', index=2,\n      number=7, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dependency_optimization\', full_name=\'tensorflow.RewriterConfig.dependency_optimization\', index=3,\n      number=8, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'disable_model_pruning\', full_name=\'tensorflow.RewriterConfig.disable_model_pruning\', index=4,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_optimization\', full_name=\'tensorflow.RewriterConfig.memory_optimization\', index=5,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_optimizer_target_node_name_prefix\', full_name=\'tensorflow.RewriterConfig.memory_optimizer_target_node_name_prefix\', index=6,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'auto_parallel\', full_name=\'tensorflow.RewriterConfig.auto_parallel\', index=7,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'optimizers\', full_name=\'tensorflow.RewriterConfig.optimizers\', index=8,\n      number=100, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _REWRITERCONFIG_TOGGLE,\n    _REWRITERCONFIG_MEMOPTTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=124,\n  serialized_end=814,\n)\n\n_REWRITERCONFIG.fields_by_name[\'layout_optimizer\'].enum_type = _REWRITERCONFIG_TOGGLE\n_REWRITERCONFIG.fields_by_name[\'constant_folding\'].enum_type = _REWRITERCONFIG_TOGGLE\n_REWRITERCONFIG.fields_by_name[\'arithmetic_optimization\'].enum_type = _REWRITERCONFIG_TOGGLE\n_REWRITERCONFIG.fields_by_name[\'dependency_optimization\'].enum_type = _REWRITERCONFIG_TOGGLE\n_REWRITERCONFIG.fields_by_name[\'memory_optimization\'].enum_type = _REWRITERCONFIG_MEMOPTTYPE\n_REWRITERCONFIG.fields_by_name[\'auto_parallel\'].message_type = _AUTOPARALLELOPTIONS\n_REWRITERCONFIG_TOGGLE.containing_type = _REWRITERCONFIG\n_REWRITERCONFIG_MEMOPTTYPE.containing_type = _REWRITERCONFIG\nDESCRIPTOR.message_types_by_name[\'AutoParallelOptions\'] = _AUTOPARALLELOPTIONS\nDESCRIPTOR.message_types_by_name[\'RewriterConfig\'] = _REWRITERCONFIG\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAutoParallelOptions = _reflection.GeneratedProtocolMessageType(\'AutoParallelOptions\', (_message.Message,), dict(\n  DESCRIPTOR = _AUTOPARALLELOPTIONS,\n  __module__ = \'tensorflow.core.protobuf.rewriter_config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.AutoParallelOptions)\n  ))\n_sym_db.RegisterMessage(AutoParallelOptions)\n\nRewriterConfig = _reflection.GeneratedProtocolMessageType(\'RewriterConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _REWRITERCONFIG,\n  __module__ = \'tensorflow.core.protobuf.rewriter_config_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RewriterConfig)\n  ))\n_sym_db.RegisterMessage(RewriterConfig)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\024RewriterConfigProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/rewriter_config_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/saved_model_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/saved_model.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.protobuf import meta_graph_pb2 as tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/saved_model.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n*tensorflow/core/protobuf/saved_model.proto\\x12\\ntensorflow\\x1a)tensorflow/core/protobuf/meta_graph.proto\\""_\\n\\nSavedModel\\x12\\""\\n\\x1asaved_model_schema_version\\x18\\x01 \\x01(\\x03\\x12-\\n\\x0bmeta_graphs\\x18\\x02 \\x03(\\x0b\\x32\\x18.tensorflow.MetaGraphDefB1\\n\\x18org.tensorflow.frameworkB\\x10SavedModelProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2.DESCRIPTOR,])\n\n\n\n\n_SAVEDMODEL = _descriptor.Descriptor(\n  name=\'SavedModel\',\n  full_name=\'tensorflow.SavedModel\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'saved_model_schema_version\', full_name=\'tensorflow.SavedModel.saved_model_schema_version\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'meta_graphs\', full_name=\'tensorflow.SavedModel.meta_graphs\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=101,\n  serialized_end=196,\n)\n\n_SAVEDMODEL.fields_by_name[\'meta_graphs\'].message_type = tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2._METAGRAPHDEF\nDESCRIPTOR.message_types_by_name[\'SavedModel\'] = _SAVEDMODEL\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSavedModel = _reflection.GeneratedProtocolMessageType(\'SavedModel\', (_message.Message,), dict(\n  DESCRIPTOR = _SAVEDMODEL,\n  __module__ = \'tensorflow.core.protobuf.saved_model_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.SavedModel)\n  ))\n_sym_db.RegisterMessage(SavedModel)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\030org.tensorflow.frameworkB\\020SavedModelProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/saved_model_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/saver_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/saver.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/saver.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n$tensorflow/core/protobuf/saver.proto\\x12\\ntensorflow\\""\\x9e\\x02\\n\\x08SaverDef\\x12\\x1c\\n\\x14\\x66ilename_tensor_name\\x18\\x01 \\x01(\\t\\x12\\x18\\n\\x10save_tensor_name\\x18\\x02 \\x01(\\t\\x12\\x17\\n\\x0frestore_op_name\\x18\\x03 \\x01(\\t\\x12\\x13\\n\\x0bmax_to_keep\\x18\\x04 \\x01(\\x05\\x12\\x0f\\n\\x07sharded\\x18\\x05 \\x01(\\x08\\x12%\\n\\x1dkeep_checkpoint_every_n_hours\\x18\\x06 \\x01(\\x02\\x12=\\n\\x07version\\x18\\x07 \\x01(\\x0e\\x32,.tensorflow.SaverDef.CheckpointFormatVersion\\""5\\n\\x17\\x43heckpointFormatVersion\\x12\\n\\n\\x06LEGACY\\x10\\x00\\x12\\x06\\n\\x02V1\\x10\\x01\\x12\\x06\\n\\x02V2\\x10\\x02\\x42\\\'\\n\\x13org.tensorflow.utilB\\x0bSaverProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n_SAVERDEF_CHECKPOINTFORMATVERSION = _descriptor.EnumDescriptor(\n  name=\'CheckpointFormatVersion\',\n  full_name=\'tensorflow.SaverDef.CheckpointFormatVersion\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'LEGACY\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'V1\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'V2\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=286,\n  serialized_end=339,\n)\n_sym_db.RegisterEnumDescriptor(_SAVERDEF_CHECKPOINTFORMATVERSION)\n\n\n_SAVERDEF = _descriptor.Descriptor(\n  name=\'SaverDef\',\n  full_name=\'tensorflow.SaverDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'filename_tensor_name\', full_name=\'tensorflow.SaverDef.filename_tensor_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'save_tensor_name\', full_name=\'tensorflow.SaverDef.save_tensor_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'restore_op_name\', full_name=\'tensorflow.SaverDef.restore_op_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_to_keep\', full_name=\'tensorflow.SaverDef.max_to_keep\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sharded\', full_name=\'tensorflow.SaverDef.sharded\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keep_checkpoint_every_n_hours\', full_name=\'tensorflow.SaverDef.keep_checkpoint_every_n_hours\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.SaverDef.version\', index=6,\n      number=7, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SAVERDEF_CHECKPOINTFORMATVERSION,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=53,\n  serialized_end=339,\n)\n\n_SAVERDEF.fields_by_name[\'version\'].enum_type = _SAVERDEF_CHECKPOINTFORMATVERSION\n_SAVERDEF_CHECKPOINTFORMATVERSION.containing_type = _SAVERDEF\nDESCRIPTOR.message_types_by_name[\'SaverDef\'] = _SAVERDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSaverDef = _reflection.GeneratedProtocolMessageType(\'SaverDef\', (_message.Message,), dict(\n  DESCRIPTOR = _SAVERDEF,\n  __module__ = \'tensorflow.core.protobuf.saver_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.SaverDef)\n  ))\n_sym_db.RegisterMessage(SaverDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\023org.tensorflow.utilB\\013SaverProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/saver_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/tensor_bundle_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/tensor_bundle.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nfrom tensorflow.core.framework import tensor_slice_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__slice__pb2\nfrom tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\nfrom tensorflow.core.framework import versions_pb2 as tensorflow_dot_core_dot_framework_dot_versions__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/tensor_bundle.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n,tensorflow/core/protobuf/tensor_bundle.proto\\x12\\ntensorflow\\x1a,tensorflow/core/framework/tensor_shape.proto\\x1a,tensorflow/core/framework/tensor_slice.proto\\x1a%tensorflow/core/framework/types.proto\\x1a(tensorflow/core/framework/versions.proto\\""\\xb1\\x01\\n\\x11\\x42undleHeaderProto\\x12\\x12\\n\\nnum_shards\\x18\\x01 \\x01(\\x05\\x12<\\n\\nendianness\\x18\\x02 \\x01(\\x0e\\x32(.tensorflow.BundleHeaderProto.Endianness\\x12\\\'\\n\\x07version\\x18\\x03 \\x01(\\x0b\\x32\\x16.tensorflow.VersionDef\\""!\\n\\nEndianness\\x12\\n\\n\\x06LITTLE\\x10\\x00\\x12\\x07\\n\\x03\\x42IG\\x10\\x01\\""\\xd2\\x01\\n\\x10\\x42undleEntryProto\\x12#\\n\\x05\\x64type\\x18\\x01 \\x01(\\x0e\\x32\\x14.tensorflow.DataType\\x12+\\n\\x05shape\\x18\\x02 \\x01(\\x0b\\x32\\x1c.tensorflow.TensorShapeProto\\x12\\x10\\n\\x08shard_id\\x18\\x03 \\x01(\\x05\\x12\\x0e\\n\\x06offset\\x18\\x04 \\x01(\\x03\\x12\\x0c\\n\\x04size\\x18\\x05 \\x01(\\x03\\x12\\x0e\\n\\x06\\x63rc32c\\x18\\x06 \\x01(\\x07\\x12,\\n\\x06slices\\x18\\x07 \\x03(\\x0b\\x32\\x1c.tensorflow.TensorSliceProtoB.\\n\\x13org.tensorflow.utilB\\x12TensorBundleProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__slice__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_versions__pb2.DESCRIPTOR,])\n\n\n\n_BUNDLEHEADERPROTO_ENDIANNESS = _descriptor.EnumDescriptor(\n  name=\'Endianness\',\n  full_name=\'tensorflow.BundleHeaderProto.Endianness\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'LITTLE\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BIG\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=378,\n  serialized_end=411,\n)\n_sym_db.RegisterEnumDescriptor(_BUNDLEHEADERPROTO_ENDIANNESS)\n\n\n_BUNDLEHEADERPROTO = _descriptor.Descriptor(\n  name=\'BundleHeaderProto\',\n  full_name=\'tensorflow.BundleHeaderProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_shards\', full_name=\'tensorflow.BundleHeaderProto.num_shards\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'endianness\', full_name=\'tensorflow.BundleHeaderProto.endianness\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.BundleHeaderProto.version\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _BUNDLEHEADERPROTO_ENDIANNESS,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=234,\n  serialized_end=411,\n)\n\n\n_BUNDLEENTRYPROTO = _descriptor.Descriptor(\n  name=\'BundleEntryProto\',\n  full_name=\'tensorflow.BundleEntryProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorflow.BundleEntryProto.dtype\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorflow.BundleEntryProto.shape\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shard_id\', full_name=\'tensorflow.BundleEntryProto.shard_id\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'offset\', full_name=\'tensorflow.BundleEntryProto.offset\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'size\', full_name=\'tensorflow.BundleEntryProto.size\', index=4,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crc32c\', full_name=\'tensorflow.BundleEntryProto.crc32c\', index=5,\n      number=6, type=7, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slices\', full_name=\'tensorflow.BundleEntryProto.slices\', index=6,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=414,\n  serialized_end=624,\n)\n\n_BUNDLEHEADERPROTO.fields_by_name[\'endianness\'].enum_type = _BUNDLEHEADERPROTO_ENDIANNESS\n_BUNDLEHEADERPROTO.fields_by_name[\'version\'].message_type = tensorflow_dot_core_dot_framework_dot_versions__pb2._VERSIONDEF\n_BUNDLEHEADERPROTO_ENDIANNESS.containing_type = _BUNDLEHEADERPROTO\n_BUNDLEENTRYPROTO.fields_by_name[\'dtype\'].enum_type = tensorflow_dot_core_dot_framework_dot_types__pb2._DATATYPE\n_BUNDLEENTRYPROTO.fields_by_name[\'shape\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_BUNDLEENTRYPROTO.fields_by_name[\'slices\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__slice__pb2._TENSORSLICEPROTO\nDESCRIPTOR.message_types_by_name[\'BundleHeaderProto\'] = _BUNDLEHEADERPROTO\nDESCRIPTOR.message_types_by_name[\'BundleEntryProto\'] = _BUNDLEENTRYPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nBundleHeaderProto = _reflection.GeneratedProtocolMessageType(\'BundleHeaderProto\', (_message.Message,), dict(\n  DESCRIPTOR = _BUNDLEHEADERPROTO,\n  __module__ = \'tensorflow.core.protobuf.tensor_bundle_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.BundleHeaderProto)\n  ))\n_sym_db.RegisterMessage(BundleHeaderProto)\n\nBundleEntryProto = _reflection.GeneratedProtocolMessageType(\'BundleEntryProto\', (_message.Message,), dict(\n  DESCRIPTOR = _BUNDLEENTRYPROTO,\n  __module__ = \'tensorflow.core.protobuf.tensor_bundle_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.BundleEntryProto)\n  ))\n_sym_db.RegisterMessage(BundleEntryProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\023org.tensorflow.utilB\\022TensorBundleProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/tensor_bundle_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/tensorflow_server_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/tensorflow_server.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.protobuf import config_pb2 as tensorflow_dot_core_dot_protobuf_dot_config__pb2\nfrom tensorflow.core.protobuf import cluster_pb2 as tensorflow_dot_core_dot_protobuf_dot_cluster__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/tensorflow_server.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow/core/protobuf/tensorflow_server.proto\\x12\\ntensorflow\\x1a%tensorflow/core/protobuf/config.proto\\x1a&tensorflow/core/protobuf/cluster.proto\\""\\xa5\\x01\\n\\tServerDef\\x12\\\'\\n\\x07\\x63luster\\x18\\x01 \\x01(\\x0b\\x32\\x16.tensorflow.ClusterDef\\x12\\x10\\n\\x08job_name\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\ntask_index\\x18\\x03 \\x01(\\x05\\x12\\x37\\n\\x16\\x64\\x65\\x66\\x61ult_session_config\\x18\\x04 \\x01(\\x0b\\x32\\x17.tensorflow.ConfigProto\\x12\\x10\\n\\x08protocol\\x18\\x05 \\x01(\\tB/\\n\\x1aorg.tensorflow.distruntimeB\\x0cServerProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_protobuf_dot_config__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_cluster__pb2.DESCRIPTOR,])\n\n\n\n\n_SERVERDEF = _descriptor.Descriptor(\n  name=\'ServerDef\',\n  full_name=\'tensorflow.ServerDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'cluster\', full_name=\'tensorflow.ServerDef.cluster\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'job_name\', full_name=\'tensorflow.ServerDef.job_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'task_index\', full_name=\'tensorflow.ServerDef.task_index\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'default_session_config\', full_name=\'tensorflow.ServerDef.default_session_config\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'protocol\', full_name=\'tensorflow.ServerDef.protocol\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=144,\n  serialized_end=309,\n)\n\n_SERVERDEF.fields_by_name[\'cluster\'].message_type = tensorflow_dot_core_dot_protobuf_dot_cluster__pb2._CLUSTERDEF\n_SERVERDEF.fields_by_name[\'default_session_config\'].message_type = tensorflow_dot_core_dot_protobuf_dot_config__pb2._CONFIGPROTO\nDESCRIPTOR.message_types_by_name[\'ServerDef\'] = _SERVERDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nServerDef = _reflection.GeneratedProtocolMessageType(\'ServerDef\', (_message.Message,), dict(\n  DESCRIPTOR = _SERVERDEF,\n  __module__ = \'tensorflow.core.protobuf.tensorflow_server_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ServerDef)\n  ))\n_sym_db.RegisterMessage(ServerDef)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\032org.tensorflow.distruntimeB\\014ServerProtosP\\001\\370\\001\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/tensorflow_server_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/worker_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/worker.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import any_pb2 as google_dot_protobuf_dot_any__pb2\nfrom tensorflow.core.framework import cost_graph_pb2 as tensorflow_dot_core_dot_framework_dot_cost__graph__pb2\nfrom tensorflow.core.framework import step_stats_pb2 as tensorflow_dot_core_dot_framework_dot_step__stats__pb2\nfrom tensorflow.core.framework import device_attributes_pb2 as tensorflow_dot_core_dot_framework_dot_device__attributes__pb2\nfrom tensorflow.core.framework import graph_pb2 as tensorflow_dot_core_dot_framework_dot_graph__pb2\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nfrom tensorflow.core.lib.core import error_codes_pb2 as tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2\nfrom tensorflow.core.protobuf import config_pb2 as tensorflow_dot_core_dot_protobuf_dot_config__pb2\nfrom tensorflow.core.protobuf import debug_pb2 as tensorflow_dot_core_dot_protobuf_dot_debug__pb2\nfrom tensorflow.core.protobuf import named_tensor_pb2 as tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2\nfrom tensorflow.core.protobuf import tensorflow_server_pb2 as tensorflow_dot_core_dot_protobuf_dot_tensorflow__server__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/worker.proto\',\n  package=\'tensorflow\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow/core/protobuf/worker.proto\\x12\\ntensorflow\\x1a\\x19google/protobuf/any.proto\\x1a*tensorflow/core/framework/cost_graph.proto\\x1a*tensorflow/core/framework/step_stats.proto\\x1a\\x31tensorflow/core/framework/device_attributes.proto\\x1a%tensorflow/core/framework/graph.proto\\x1a&tensorflow/core/framework/tensor.proto\\x1a*tensorflow/core/lib/core/error_codes.proto\\x1a%tensorflow/core/protobuf/config.proto\\x1a$tensorflow/core/protobuf/debug.proto\\x1a+tensorflow/core/protobuf/named_tensor.proto\\x1a\\x30tensorflow/core/protobuf/tensorflow_server.proto\\""\\x12\\n\\x10GetStatusRequest\\""L\\n\\x11GetStatusResponse\\x12\\x37\\n\\x11\\x64\\x65vice_attributes\\x18\\x01 \\x03(\\x0b\\x32\\x1c.tensorflow.DeviceAttributes\\""~\\n\\x1a\\x43reateWorkerSessionRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\x12)\\n\\nserver_def\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.ServerDef\\x12\\x1d\\n\\x15isolate_session_state\\x18\\x03 \\x01(\\x08\\""\\x1d\\n\\x1b\\x43reateWorkerSessionResponse\\""4\\n\\x1a\\x44\\x65leteWorkerSessionRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\""\\x1d\\n\\x1b\\x44\\x65leteWorkerSessionResponse\\""\\xd7\\x01\\n\\x14RegisterGraphRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\tgraph_def\\x18\\x02 \\x01(\\x0b\\x32\\x14.tensorflow.GraphDef\\x12\\x1c\\n\\x10has_control_flow\\x18\\x03 \\x01(\\x08\\x42\\x02\\x18\\x01\\x12/\\n\\rgraph_options\\x18\\x04 \\x01(\\x0b\\x32\\x18.tensorflow.GraphOptions\\x12/\\n\\rdebug_options\\x18\\x05 \\x01(\\x0b\\x32\\x18.tensorflow.DebugOptions\\""-\\n\\x15RegisterGraphResponse\\x12\\x14\\n\\x0cgraph_handle\\x18\\x01 \\x01(\\t\\""F\\n\\x16\\x44\\x65registerGraphRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x02 \\x01(\\t\\x12\\x14\\n\\x0cgraph_handle\\x18\\x01 \\x01(\\t\\""\\x19\\n\\x17\\x44\\x65registerGraphResponse\\""&\\n\\x11\\x43leanupAllRequest\\x12\\x11\\n\\tcontainer\\x18\\x01 \\x03(\\t\\""\\x14\\n\\x12\\x43leanupAllResponse\\""\\x8a\\x01\\n\\x0c\\x45xecutorOpts\\x12\\x14\\n\\x0crecord_costs\\x18\\x01 \\x01(\\x08\\x12\\x17\\n\\x0frecord_timeline\\x18\\x03 \\x01(\\x08\\x12\\x1f\\n\\x17record_partition_graphs\\x18\\x04 \\x01(\\x08\\x12*\\n\\""report_tensor_allocations_upon_oom\\x18\\x05 \\x01(\\x08\\""\\x93\\x02\\n\\x0fRunGraphRequest\\x12\\x16\\n\\x0esession_handle\\x18\\x08 \\x01(\\t\\x12\\x14\\n\\x0cgraph_handle\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07step_id\\x18\\x02 \\x01(\\x03\\x12+\\n\\texec_opts\\x18\\x05 \\x01(\\x0b\\x32\\x18.tensorflow.ExecutorOpts\\x12*\\n\\x04send\\x18\\x03 \\x03(\\x0b\\x32\\x1c.tensorflow.NamedTensorProto\\x12\\x10\\n\\x08recv_key\\x18\\x04 \\x03(\\t\\x12\\x12\\n\\nis_partial\\x18\\x06 \\x01(\\x08\\x12\\x1b\\n\\x13is_last_partial_run\\x18\\x07 \\x01(\\x08\\x12%\\n\\x1dstore_errors_in_response_body\\x18\\t \\x01(\\x08\\""\\x91\\x02\\n\\x10RunGraphResponse\\x12*\\n\\x04recv\\x18\\x01 \\x03(\\x0b\\x32\\x1c.tensorflow.NamedTensorProto\\x12)\\n\\nstep_stats\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.StepStats\\x12,\\n\\ncost_graph\\x18\\x03 \\x01(\\x0b\\x32\\x18.tensorflow.CostGraphDef\\x12-\\n\\x0fpartition_graph\\x18\\x04 \\x03(\\x0b\\x32\\x14.tensorflow.GraphDef\\x12+\\n\\x0bstatus_code\\x18\\x05 \\x01(\\x0e\\x32\\x16.tensorflow.error.Code\\x12\\x1c\\n\\x14status_error_message\\x18\\x06 \\x01(\\t\\""&\\n\\x13\\x43leanupGraphRequest\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\""\\x16\\n\\x14\\x43leanupGraphResponse\\""\\xe7\\x01\\n\\x11RecvTensorRequest\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\x12\\x16\\n\\x0erendezvous_key\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06\\x64ma_ok\\x18\\x03 \\x01(\\x08\\x12\\x33\\n\\x0f\\x63lient_locality\\x18\\x04 \\x01(\\x0b\\x32\\x1a.tensorflow.DeviceLocality\\x12\\x33\\n\\x0fserver_locality\\x18\\x05 \\x01(\\x0b\\x32\\x1a.tensorflow.DeviceLocality\\x12/\\n\\x11transport_options\\x18\\x06 \\x01(\\x0b\\x32\\x14.google.protobuf.Any\\""\\x9a\\x01\\n\\x12RecvTensorResponse\\x12\\\'\\n\\x06tensor\\x18\\x01 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto\\x12\\x0f\\n\\x07is_dead\\x18\\x02 \\x01(\\x08\\x12\\x19\\n\\x11send_start_micros\\x18\\x03 \\x01(\\x03\\x12/\\n\\x11transport_options\\x18\\x04 \\x01(\\x0b\\x32\\x14.google.protobuf.Any\\""K\\n\\x0eLoggingRequest\\x12\\x13\\n\\x0brpc_logging\\x18\\x01 \\x01(\\x08\\x12\\r\\n\\x05\\x63lear\\x18\\x02 \\x01(\\x08\\x12\\x15\\n\\rfetch_step_id\\x18\\x03 \\x03(\\x03\\""N\\n\\x10LabeledStepStats\\x12\\x0f\\n\\x07step_id\\x18\\x01 \\x01(\\x03\\x12)\\n\\nstep_stats\\x18\\x02 \\x01(\\x0b\\x32\\x15.tensorflow.StepStats\\""=\\n\\x0fLoggingResponse\\x12*\\n\\x04step\\x18\\x01 \\x03(\\x0b\\x32\\x1c.tensorflow.LabeledStepStats\\""\\xab\\x01\\n\\tTraceOpts\\x12\\x10\\n\\x08\\x64uration\\x18\\x01 \\x01(\\x01\\x12\\x19\\n\\x11use_step_profiler\\x18\\x02 \\x01(\\x08\\x12\\x1b\\n\\x13use_kernel_profiler\\x18\\x03 \\x01(\\x08\\x12\\x1d\\n\\x15use_extended_profiler\\x18\\x04 \\x01(\\x08\\x12\\x18\\n\\x10use_gpu_profiler\\x18\\x05 \\x01(\\x08\\x12\\x1b\\n\\x13use_sample_profiler\\x18\\x06 \\x01(\\x08\\""8\\n\\x0eTracingRequest\\x12&\\n\\x07options\\x18\\x01 \\x01(\\x0b\\x32\\x15.tensorflow.TraceOpts\\""\\x11\\n\\x0fTracingResponseB/\\n\\x1aorg.tensorflow.distruntimeB\\x0cWorkerProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_cost__graph__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_step__stats__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_device__attributes__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_graph__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_config__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_debug__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_tensorflow__server__pb2.DESCRIPTOR,])\n\n\n\n\n_GETSTATUSREQUEST = _descriptor.Descriptor(\n  name=\'GetStatusRequest\',\n  full_name=\'tensorflow.GetStatusRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=514,\n  serialized_end=532,\n)\n\n\n_GETSTATUSRESPONSE = _descriptor.Descriptor(\n  name=\'GetStatusResponse\',\n  full_name=\'tensorflow.GetStatusResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'device_attributes\', full_name=\'tensorflow.GetStatusResponse.device_attributes\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=534,\n  serialized_end=610,\n)\n\n\n_CREATEWORKERSESSIONREQUEST = _descriptor.Descriptor(\n  name=\'CreateWorkerSessionRequest\',\n  full_name=\'tensorflow.CreateWorkerSessionRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.CreateWorkerSessionRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'server_def\', full_name=\'tensorflow.CreateWorkerSessionRequest.server_def\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'isolate_session_state\', full_name=\'tensorflow.CreateWorkerSessionRequest.isolate_session_state\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=612,\n  serialized_end=738,\n)\n\n\n_CREATEWORKERSESSIONRESPONSE = _descriptor.Descriptor(\n  name=\'CreateWorkerSessionResponse\',\n  full_name=\'tensorflow.CreateWorkerSessionResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=740,\n  serialized_end=769,\n)\n\n\n_DELETEWORKERSESSIONREQUEST = _descriptor.Descriptor(\n  name=\'DeleteWorkerSessionRequest\',\n  full_name=\'tensorflow.DeleteWorkerSessionRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.DeleteWorkerSessionRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=771,\n  serialized_end=823,\n)\n\n\n_DELETEWORKERSESSIONRESPONSE = _descriptor.Descriptor(\n  name=\'DeleteWorkerSessionResponse\',\n  full_name=\'tensorflow.DeleteWorkerSessionResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=825,\n  serialized_end=854,\n)\n\n\n_REGISTERGRAPHREQUEST = _descriptor.Descriptor(\n  name=\'RegisterGraphRequest\',\n  full_name=\'tensorflow.RegisterGraphRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.RegisterGraphRequest.session_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_def\', full_name=\'tensorflow.RegisterGraphRequest.graph_def\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'has_control_flow\', full_name=\'tensorflow.RegisterGraphRequest.has_control_flow\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'graph_options\', full_name=\'tensorflow.RegisterGraphRequest.graph_options\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_options\', full_name=\'tensorflow.RegisterGraphRequest.debug_options\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=857,\n  serialized_end=1072,\n)\n\n\n_REGISTERGRAPHRESPONSE = _descriptor.Descriptor(\n  name=\'RegisterGraphResponse\',\n  full_name=\'tensorflow.RegisterGraphResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'graph_handle\', full_name=\'tensorflow.RegisterGraphResponse.graph_handle\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1074,\n  serialized_end=1119,\n)\n\n\n_DEREGISTERGRAPHREQUEST = _descriptor.Descriptor(\n  name=\'DeregisterGraphRequest\',\n  full_name=\'tensorflow.DeregisterGraphRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.DeregisterGraphRequest.session_handle\', index=0,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_handle\', full_name=\'tensorflow.DeregisterGraphRequest.graph_handle\', index=1,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1121,\n  serialized_end=1191,\n)\n\n\n_DEREGISTERGRAPHRESPONSE = _descriptor.Descriptor(\n  name=\'DeregisterGraphResponse\',\n  full_name=\'tensorflow.DeregisterGraphResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1193,\n  serialized_end=1218,\n)\n\n\n_CLEANUPALLREQUEST = _descriptor.Descriptor(\n  name=\'CleanupAllRequest\',\n  full_name=\'tensorflow.CleanupAllRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'container\', full_name=\'tensorflow.CleanupAllRequest.container\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1220,\n  serialized_end=1258,\n)\n\n\n_CLEANUPALLRESPONSE = _descriptor.Descriptor(\n  name=\'CleanupAllResponse\',\n  full_name=\'tensorflow.CleanupAllResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1260,\n  serialized_end=1280,\n)\n\n\n_EXECUTOROPTS = _descriptor.Descriptor(\n  name=\'ExecutorOpts\',\n  full_name=\'tensorflow.ExecutorOpts\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'record_costs\', full_name=\'tensorflow.ExecutorOpts.record_costs\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'record_timeline\', full_name=\'tensorflow.ExecutorOpts.record_timeline\', index=1,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'record_partition_graphs\', full_name=\'tensorflow.ExecutorOpts.record_partition_graphs\', index=2,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'report_tensor_allocations_upon_oom\', full_name=\'tensorflow.ExecutorOpts.report_tensor_allocations_upon_oom\', index=3,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1283,\n  serialized_end=1421,\n)\n\n\n_RUNGRAPHREQUEST = _descriptor.Descriptor(\n  name=\'RunGraphRequest\',\n  full_name=\'tensorflow.RunGraphRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_handle\', full_name=\'tensorflow.RunGraphRequest.session_handle\', index=0,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_handle\', full_name=\'tensorflow.RunGraphRequest.graph_handle\', index=1,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.RunGraphRequest.step_id\', index=2,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exec_opts\', full_name=\'tensorflow.RunGraphRequest.exec_opts\', index=3,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'send\', full_name=\'tensorflow.RunGraphRequest.send\', index=4,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'recv_key\', full_name=\'tensorflow.RunGraphRequest.recv_key\', index=5,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_partial\', full_name=\'tensorflow.RunGraphRequest.is_partial\', index=6,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_last_partial_run\', full_name=\'tensorflow.RunGraphRequest.is_last_partial_run\', index=7,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'store_errors_in_response_body\', full_name=\'tensorflow.RunGraphRequest.store_errors_in_response_body\', index=8,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1424,\n  serialized_end=1699,\n)\n\n\n_RUNGRAPHRESPONSE = _descriptor.Descriptor(\n  name=\'RunGraphResponse\',\n  full_name=\'tensorflow.RunGraphResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'recv\', full_name=\'tensorflow.RunGraphResponse.recv\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'step_stats\', full_name=\'tensorflow.RunGraphResponse.step_stats\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cost_graph\', full_name=\'tensorflow.RunGraphResponse.cost_graph\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'partition_graph\', full_name=\'tensorflow.RunGraphResponse.partition_graph\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'status_code\', full_name=\'tensorflow.RunGraphResponse.status_code\', index=4,\n      number=5, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'status_error_message\', full_name=\'tensorflow.RunGraphResponse.status_error_message\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1702,\n  serialized_end=1975,\n)\n\n\n_CLEANUPGRAPHREQUEST = _descriptor.Descriptor(\n  name=\'CleanupGraphRequest\',\n  full_name=\'tensorflow.CleanupGraphRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.CleanupGraphRequest.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1977,\n  serialized_end=2015,\n)\n\n\n_CLEANUPGRAPHRESPONSE = _descriptor.Descriptor(\n  name=\'CleanupGraphResponse\',\n  full_name=\'tensorflow.CleanupGraphResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2017,\n  serialized_end=2039,\n)\n\n\n_RECVTENSORREQUEST = _descriptor.Descriptor(\n  name=\'RecvTensorRequest\',\n  full_name=\'tensorflow.RecvTensorRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.RecvTensorRequest.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rendezvous_key\', full_name=\'tensorflow.RecvTensorRequest.rendezvous_key\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dma_ok\', full_name=\'tensorflow.RecvTensorRequest.dma_ok\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'client_locality\', full_name=\'tensorflow.RecvTensorRequest.client_locality\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'server_locality\', full_name=\'tensorflow.RecvTensorRequest.server_locality\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'transport_options\', full_name=\'tensorflow.RecvTensorRequest.transport_options\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2042,\n  serialized_end=2273,\n)\n\n\n_RECVTENSORRESPONSE = _descriptor.Descriptor(\n  name=\'RecvTensorResponse\',\n  full_name=\'tensorflow.RecvTensorResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorflow.RecvTensorResponse.tensor\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_dead\', full_name=\'tensorflow.RecvTensorResponse.is_dead\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'send_start_micros\', full_name=\'tensorflow.RecvTensorResponse.send_start_micros\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'transport_options\', full_name=\'tensorflow.RecvTensorResponse.transport_options\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2276,\n  serialized_end=2430,\n)\n\n\n_LOGGINGREQUEST = _descriptor.Descriptor(\n  name=\'LoggingRequest\',\n  full_name=\'tensorflow.LoggingRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'rpc_logging\', full_name=\'tensorflow.LoggingRequest.rpc_logging\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'clear\', full_name=\'tensorflow.LoggingRequest.clear\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fetch_step_id\', full_name=\'tensorflow.LoggingRequest.fetch_step_id\', index=2,\n      number=3, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2432,\n  serialized_end=2507,\n)\n\n\n_LABELEDSTEPSTATS = _descriptor.Descriptor(\n  name=\'LabeledStepStats\',\n  full_name=\'tensorflow.LabeledStepStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_id\', full_name=\'tensorflow.LabeledStepStats.step_id\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'step_stats\', full_name=\'tensorflow.LabeledStepStats.step_stats\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2509,\n  serialized_end=2587,\n)\n\n\n_LOGGINGRESPONSE = _descriptor.Descriptor(\n  name=\'LoggingResponse\',\n  full_name=\'tensorflow.LoggingResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step\', full_name=\'tensorflow.LoggingResponse.step\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2589,\n  serialized_end=2650,\n)\n\n\n_TRACEOPTS = _descriptor.Descriptor(\n  name=\'TraceOpts\',\n  full_name=\'tensorflow.TraceOpts\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'duration\', full_name=\'tensorflow.TraceOpts.duration\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_step_profiler\', full_name=\'tensorflow.TraceOpts.use_step_profiler\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_kernel_profiler\', full_name=\'tensorflow.TraceOpts.use_kernel_profiler\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_extended_profiler\', full_name=\'tensorflow.TraceOpts.use_extended_profiler\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_gpu_profiler\', full_name=\'tensorflow.TraceOpts.use_gpu_profiler\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_sample_profiler\', full_name=\'tensorflow.TraceOpts.use_sample_profiler\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2653,\n  serialized_end=2824,\n)\n\n\n_TRACINGREQUEST = _descriptor.Descriptor(\n  name=\'TracingRequest\',\n  full_name=\'tensorflow.TracingRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'options\', full_name=\'tensorflow.TracingRequest.options\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2826,\n  serialized_end=2882,\n)\n\n\n_TRACINGRESPONSE = _descriptor.Descriptor(\n  name=\'TracingResponse\',\n  full_name=\'tensorflow.TracingResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2884,\n  serialized_end=2901,\n)\n\n_GETSTATUSRESPONSE.fields_by_name[\'device_attributes\'].message_type = tensorflow_dot_core_dot_framework_dot_device__attributes__pb2._DEVICEATTRIBUTES\n_CREATEWORKERSESSIONREQUEST.fields_by_name[\'server_def\'].message_type = tensorflow_dot_core_dot_protobuf_dot_tensorflow__server__pb2._SERVERDEF\n_REGISTERGRAPHREQUEST.fields_by_name[\'graph_def\'].message_type = tensorflow_dot_core_dot_framework_dot_graph__pb2._GRAPHDEF\n_REGISTERGRAPHREQUEST.fields_by_name[\'graph_options\'].message_type = tensorflow_dot_core_dot_protobuf_dot_config__pb2._GRAPHOPTIONS\n_REGISTERGRAPHREQUEST.fields_by_name[\'debug_options\'].message_type = tensorflow_dot_core_dot_protobuf_dot_debug__pb2._DEBUGOPTIONS\n_RUNGRAPHREQUEST.fields_by_name[\'exec_opts\'].message_type = _EXECUTOROPTS\n_RUNGRAPHREQUEST.fields_by_name[\'send\'].message_type = tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2._NAMEDTENSORPROTO\n_RUNGRAPHRESPONSE.fields_by_name[\'recv\'].message_type = tensorflow_dot_core_dot_protobuf_dot_named__tensor__pb2._NAMEDTENSORPROTO\n_RUNGRAPHRESPONSE.fields_by_name[\'step_stats\'].message_type = tensorflow_dot_core_dot_framework_dot_step__stats__pb2._STEPSTATS\n_RUNGRAPHRESPONSE.fields_by_name[\'cost_graph\'].message_type = tensorflow_dot_core_dot_framework_dot_cost__graph__pb2._COSTGRAPHDEF\n_RUNGRAPHRESPONSE.fields_by_name[\'partition_graph\'].message_type = tensorflow_dot_core_dot_framework_dot_graph__pb2._GRAPHDEF\n_RUNGRAPHRESPONSE.fields_by_name[\'status_code\'].enum_type = tensorflow_dot_core_dot_lib_dot_core_dot_error__codes__pb2._CODE\n_RECVTENSORREQUEST.fields_by_name[\'client_locality\'].message_type = tensorflow_dot_core_dot_framework_dot_device__attributes__pb2._DEVICELOCALITY\n_RECVTENSORREQUEST.fields_by_name[\'server_locality\'].message_type = tensorflow_dot_core_dot_framework_dot_device__attributes__pb2._DEVICELOCALITY\n_RECVTENSORREQUEST.fields_by_name[\'transport_options\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_RECVTENSORRESPONSE.fields_by_name[\'tensor\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_RECVTENSORRESPONSE.fields_by_name[\'transport_options\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_LABELEDSTEPSTATS.fields_by_name[\'step_stats\'].message_type = tensorflow_dot_core_dot_framework_dot_step__stats__pb2._STEPSTATS\n_LOGGINGRESPONSE.fields_by_name[\'step\'].message_type = _LABELEDSTEPSTATS\n_TRACINGREQUEST.fields_by_name[\'options\'].message_type = _TRACEOPTS\nDESCRIPTOR.message_types_by_name[\'GetStatusRequest\'] = _GETSTATUSREQUEST\nDESCRIPTOR.message_types_by_name[\'GetStatusResponse\'] = _GETSTATUSRESPONSE\nDESCRIPTOR.message_types_by_name[\'CreateWorkerSessionRequest\'] = _CREATEWORKERSESSIONREQUEST\nDESCRIPTOR.message_types_by_name[\'CreateWorkerSessionResponse\'] = _CREATEWORKERSESSIONRESPONSE\nDESCRIPTOR.message_types_by_name[\'DeleteWorkerSessionRequest\'] = _DELETEWORKERSESSIONREQUEST\nDESCRIPTOR.message_types_by_name[\'DeleteWorkerSessionResponse\'] = _DELETEWORKERSESSIONRESPONSE\nDESCRIPTOR.message_types_by_name[\'RegisterGraphRequest\'] = _REGISTERGRAPHREQUEST\nDESCRIPTOR.message_types_by_name[\'RegisterGraphResponse\'] = _REGISTERGRAPHRESPONSE\nDESCRIPTOR.message_types_by_name[\'DeregisterGraphRequest\'] = _DEREGISTERGRAPHREQUEST\nDESCRIPTOR.message_types_by_name[\'DeregisterGraphResponse\'] = _DEREGISTERGRAPHRESPONSE\nDESCRIPTOR.message_types_by_name[\'CleanupAllRequest\'] = _CLEANUPALLREQUEST\nDESCRIPTOR.message_types_by_name[\'CleanupAllResponse\'] = _CLEANUPALLRESPONSE\nDESCRIPTOR.message_types_by_name[\'ExecutorOpts\'] = _EXECUTOROPTS\nDESCRIPTOR.message_types_by_name[\'RunGraphRequest\'] = _RUNGRAPHREQUEST\nDESCRIPTOR.message_types_by_name[\'RunGraphResponse\'] = _RUNGRAPHRESPONSE\nDESCRIPTOR.message_types_by_name[\'CleanupGraphRequest\'] = _CLEANUPGRAPHREQUEST\nDESCRIPTOR.message_types_by_name[\'CleanupGraphResponse\'] = _CLEANUPGRAPHRESPONSE\nDESCRIPTOR.message_types_by_name[\'RecvTensorRequest\'] = _RECVTENSORREQUEST\nDESCRIPTOR.message_types_by_name[\'RecvTensorResponse\'] = _RECVTENSORRESPONSE\nDESCRIPTOR.message_types_by_name[\'LoggingRequest\'] = _LOGGINGREQUEST\nDESCRIPTOR.message_types_by_name[\'LabeledStepStats\'] = _LABELEDSTEPSTATS\nDESCRIPTOR.message_types_by_name[\'LoggingResponse\'] = _LOGGINGRESPONSE\nDESCRIPTOR.message_types_by_name[\'TraceOpts\'] = _TRACEOPTS\nDESCRIPTOR.message_types_by_name[\'TracingRequest\'] = _TRACINGREQUEST\nDESCRIPTOR.message_types_by_name[\'TracingResponse\'] = _TRACINGRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nGetStatusRequest = _reflection.GeneratedProtocolMessageType(\'GetStatusRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _GETSTATUSREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.GetStatusRequest)\n  ))\n_sym_db.RegisterMessage(GetStatusRequest)\n\nGetStatusResponse = _reflection.GeneratedProtocolMessageType(\'GetStatusResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _GETSTATUSRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.GetStatusResponse)\n  ))\n_sym_db.RegisterMessage(GetStatusResponse)\n\nCreateWorkerSessionRequest = _reflection.GeneratedProtocolMessageType(\'CreateWorkerSessionRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CREATEWORKERSESSIONREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CreateWorkerSessionRequest)\n  ))\n_sym_db.RegisterMessage(CreateWorkerSessionRequest)\n\nCreateWorkerSessionResponse = _reflection.GeneratedProtocolMessageType(\'CreateWorkerSessionResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _CREATEWORKERSESSIONRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CreateWorkerSessionResponse)\n  ))\n_sym_db.RegisterMessage(CreateWorkerSessionResponse)\n\nDeleteWorkerSessionRequest = _reflection.GeneratedProtocolMessageType(\'DeleteWorkerSessionRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _DELETEWORKERSESSIONREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeleteWorkerSessionRequest)\n  ))\n_sym_db.RegisterMessage(DeleteWorkerSessionRequest)\n\nDeleteWorkerSessionResponse = _reflection.GeneratedProtocolMessageType(\'DeleteWorkerSessionResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _DELETEWORKERSESSIONRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeleteWorkerSessionResponse)\n  ))\n_sym_db.RegisterMessage(DeleteWorkerSessionResponse)\n\nRegisterGraphRequest = _reflection.GeneratedProtocolMessageType(\'RegisterGraphRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _REGISTERGRAPHREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RegisterGraphRequest)\n  ))\n_sym_db.RegisterMessage(RegisterGraphRequest)\n\nRegisterGraphResponse = _reflection.GeneratedProtocolMessageType(\'RegisterGraphResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _REGISTERGRAPHRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RegisterGraphResponse)\n  ))\n_sym_db.RegisterMessage(RegisterGraphResponse)\n\nDeregisterGraphRequest = _reflection.GeneratedProtocolMessageType(\'DeregisterGraphRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _DEREGISTERGRAPHREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeregisterGraphRequest)\n  ))\n_sym_db.RegisterMessage(DeregisterGraphRequest)\n\nDeregisterGraphResponse = _reflection.GeneratedProtocolMessageType(\'DeregisterGraphResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _DEREGISTERGRAPHRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.DeregisterGraphResponse)\n  ))\n_sym_db.RegisterMessage(DeregisterGraphResponse)\n\nCleanupAllRequest = _reflection.GeneratedProtocolMessageType(\'CleanupAllRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CLEANUPALLREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CleanupAllRequest)\n  ))\n_sym_db.RegisterMessage(CleanupAllRequest)\n\nCleanupAllResponse = _reflection.GeneratedProtocolMessageType(\'CleanupAllResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _CLEANUPALLRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CleanupAllResponse)\n  ))\n_sym_db.RegisterMessage(CleanupAllResponse)\n\nExecutorOpts = _reflection.GeneratedProtocolMessageType(\'ExecutorOpts\', (_message.Message,), dict(\n  DESCRIPTOR = _EXECUTOROPTS,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.ExecutorOpts)\n  ))\n_sym_db.RegisterMessage(ExecutorOpts)\n\nRunGraphRequest = _reflection.GeneratedProtocolMessageType(\'RunGraphRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _RUNGRAPHREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RunGraphRequest)\n  ))\n_sym_db.RegisterMessage(RunGraphRequest)\n\nRunGraphResponse = _reflection.GeneratedProtocolMessageType(\'RunGraphResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _RUNGRAPHRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RunGraphResponse)\n  ))\n_sym_db.RegisterMessage(RunGraphResponse)\n\nCleanupGraphRequest = _reflection.GeneratedProtocolMessageType(\'CleanupGraphRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CLEANUPGRAPHREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CleanupGraphRequest)\n  ))\n_sym_db.RegisterMessage(CleanupGraphRequest)\n\nCleanupGraphResponse = _reflection.GeneratedProtocolMessageType(\'CleanupGraphResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _CLEANUPGRAPHRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.CleanupGraphResponse)\n  ))\n_sym_db.RegisterMessage(CleanupGraphResponse)\n\nRecvTensorRequest = _reflection.GeneratedProtocolMessageType(\'RecvTensorRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _RECVTENSORREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RecvTensorRequest)\n  ))\n_sym_db.RegisterMessage(RecvTensorRequest)\n\nRecvTensorResponse = _reflection.GeneratedProtocolMessageType(\'RecvTensorResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _RECVTENSORRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.RecvTensorResponse)\n  ))\n_sym_db.RegisterMessage(RecvTensorResponse)\n\nLoggingRequest = _reflection.GeneratedProtocolMessageType(\'LoggingRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _LOGGINGREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.LoggingRequest)\n  ))\n_sym_db.RegisterMessage(LoggingRequest)\n\nLabeledStepStats = _reflection.GeneratedProtocolMessageType(\'LabeledStepStats\', (_message.Message,), dict(\n  DESCRIPTOR = _LABELEDSTEPSTATS,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.LabeledStepStats)\n  ))\n_sym_db.RegisterMessage(LabeledStepStats)\n\nLoggingResponse = _reflection.GeneratedProtocolMessageType(\'LoggingResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _LOGGINGRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.LoggingResponse)\n  ))\n_sym_db.RegisterMessage(LoggingResponse)\n\nTraceOpts = _reflection.GeneratedProtocolMessageType(\'TraceOpts\', (_message.Message,), dict(\n  DESCRIPTOR = _TRACEOPTS,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TraceOpts)\n  ))\n_sym_db.RegisterMessage(TraceOpts)\n\nTracingRequest = _reflection.GeneratedProtocolMessageType(\'TracingRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _TRACINGREQUEST,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TracingRequest)\n  ))\n_sym_db.RegisterMessage(TracingRequest)\n\nTracingResponse = _reflection.GeneratedProtocolMessageType(\'TracingResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _TRACINGRESPONSE,\n  __module__ = \'tensorflow.core.protobuf.worker_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.TracingResponse)\n  ))\n_sym_db.RegisterMessage(TracingResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\032org.tensorflow.distruntimeB\\014WorkerProtosP\\001\\370\\001\\001\'))\n_REGISTERGRAPHREQUEST.fields_by_name[\'has_control_flow\'].has_options = True\n_REGISTERGRAPHREQUEST.fields_by_name[\'has_control_flow\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/worker_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
libs/pipeline_model/tensorflow/core/protobuf/worker_service_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow/core/protobuf/worker_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.protobuf import worker_pb2 as tensorflow_dot_core_dot_protobuf_dot_worker__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow/core/protobuf/worker_service.proto\',\n  package=\'tensorflow.grpc\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n-tensorflow/core/protobuf/worker_service.proto\\x12\\x0ftensorflow.grpc\\x1a%tensorflow/core/protobuf/worker.proto2\\x99\\x07\\n\\rWorkerService\\x12H\\n\\tGetStatus\\x12\\x1c.tensorflow.GetStatusRequest\\x1a\\x1d.tensorflow.GetStatusResponse\\x12\\x66\\n\\x13\\x43reateWorkerSession\\x12&.tensorflow.CreateWorkerSessionRequest\\x1a\\\'.tensorflow.CreateWorkerSessionResponse\\x12\\x66\\n\\x13\\x44\\x65leteWorkerSession\\x12&.tensorflow.DeleteWorkerSessionRequest\\x1a\\\'.tensorflow.DeleteWorkerSessionResponse\\x12T\\n\\rRegisterGraph\\x12 .tensorflow.RegisterGraphRequest\\x1a!.tensorflow.RegisterGraphResponse\\x12Z\\n\\x0f\\x44\\x65registerGraph\\x12\\"".tensorflow.DeregisterGraphRequest\\x1a#.tensorflow.DeregisterGraphResponse\\x12\\x45\\n\\x08RunGraph\\x12\\x1b.tensorflow.RunGraphRequest\\x1a\\x1c.tensorflow.RunGraphResponse\\x12Q\\n\\x0c\\x43leanupGraph\\x12\\x1f.tensorflow.CleanupGraphRequest\\x1a .tensorflow.CleanupGraphResponse\\x12K\\n\\nCleanupAll\\x12\\x1d.tensorflow.CleanupAllRequest\\x1a\\x1e.tensorflow.CleanupAllResponse\\x12M\\n\\nRecvTensor\\x12\\x1d.tensorflow.RecvTensorRequest\\x1a\\x1e.tensorflow.RecvTensorResponse\\""\\x00\\x12\\x42\\n\\x07Logging\\x12\\x1a.tensorflow.LoggingRequest\\x1a\\x1b.tensorflow.LoggingResponse\\x12\\x42\\n\\x07Tracing\\x12\\x1a.tensorflow.TracingRequest\\x1a\\x1b.tensorflow.TracingResponseB3\\n\\x1aorg.tensorflow.distruntimeB\\x13WorkerServiceProtosP\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DESCRIPTOR,])\n\n\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\032org.tensorflow.distruntimeB\\023WorkerServiceProtosP\\001\'))\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\n\n\n  class WorkerServiceStub(object):\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    WorkerService defines a TensorFlow service that executes dataflow\n    graphs on a set of local devices, on behalf of a MasterService.\n\n    A worker service keeps track of multiple ""registered graphs"". Each\n    registered graph is a subgraph of a client\'s graph, corresponding to\n    only the nodes that should execute on this worker (and any\n    additional nodes necessary for inter-process communication using\n    the `RecvTensor` method).\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n\n    def __init__(self, channel):\n      """"""Constructor.\n\n      Args:\n        channel: A grpc.Channel.\n      """"""\n      self.GetStatus = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/GetStatus\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusResponse.FromString,\n          )\n      self.CreateWorkerSession = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/CreateWorkerSession\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionResponse.FromString,\n          )\n      self.DeleteWorkerSession = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/DeleteWorkerSession\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionResponse.FromString,\n          )\n      self.RegisterGraph = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/RegisterGraph\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphResponse.FromString,\n          )\n      self.DeregisterGraph = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/DeregisterGraph\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphResponse.FromString,\n          )\n      self.RunGraph = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/RunGraph\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphResponse.FromString,\n          )\n      self.CleanupGraph = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/CleanupGraph\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphResponse.FromString,\n          )\n      self.CleanupAll = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/CleanupAll\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllResponse.FromString,\n          )\n      self.RecvTensor = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/RecvTensor\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorResponse.FromString,\n          )\n      self.Logging = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/Logging\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingResponse.FromString,\n          )\n      self.Tracing = channel.unary_unary(\n          \'/tensorflow.grpc.WorkerService/Tracing\',\n          request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingRequest.SerializeToString,\n          response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingResponse.FromString,\n          )\n\n\n  class WorkerServiceServicer(object):\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    WorkerService defines a TensorFlow service that executes dataflow\n    graphs on a set of local devices, on behalf of a MasterService.\n\n    A worker service keeps track of multiple ""registered graphs"". Each\n    registered graph is a subgraph of a client\'s graph, corresponding to\n    only the nodes that should execute on this worker (and any\n    additional nodes necessary for inter-process communication using\n    the `RecvTensor` method).\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n\n    def GetStatus(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def CreateWorkerSession(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def DeleteWorkerSession(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def RegisterGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def DeregisterGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def RunGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def CleanupGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def CleanupAll(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def RecvTensor(self, request, context):\n      """"""See worker.proto for details.\n      RecvTensor Method\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def Logging(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def Tracing(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n\n  def add_WorkerServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n        \'GetStatus\': grpc.unary_unary_rpc_method_handler(\n            servicer.GetStatus,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusResponse.SerializeToString,\n        ),\n        \'CreateWorkerSession\': grpc.unary_unary_rpc_method_handler(\n            servicer.CreateWorkerSession,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionResponse.SerializeToString,\n        ),\n        \'DeleteWorkerSession\': grpc.unary_unary_rpc_method_handler(\n            servicer.DeleteWorkerSession,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionResponse.SerializeToString,\n        ),\n        \'RegisterGraph\': grpc.unary_unary_rpc_method_handler(\n            servicer.RegisterGraph,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphResponse.SerializeToString,\n        ),\n        \'DeregisterGraph\': grpc.unary_unary_rpc_method_handler(\n            servicer.DeregisterGraph,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphResponse.SerializeToString,\n        ),\n        \'RunGraph\': grpc.unary_unary_rpc_method_handler(\n            servicer.RunGraph,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphResponse.SerializeToString,\n        ),\n        \'CleanupGraph\': grpc.unary_unary_rpc_method_handler(\n            servicer.CleanupGraph,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphResponse.SerializeToString,\n        ),\n        \'CleanupAll\': grpc.unary_unary_rpc_method_handler(\n            servicer.CleanupAll,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllResponse.SerializeToString,\n        ),\n        \'RecvTensor\': grpc.unary_unary_rpc_method_handler(\n            servicer.RecvTensor,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorResponse.SerializeToString,\n        ),\n        \'Logging\': grpc.unary_unary_rpc_method_handler(\n            servicer.Logging,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingResponse.SerializeToString,\n        ),\n        \'Tracing\': grpc.unary_unary_rpc_method_handler(\n            servicer.Tracing,\n            request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingRequest.FromString,\n            response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingResponse.SerializeToString,\n        ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n        \'tensorflow.grpc.WorkerService\', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))\n\n\n  class BetaWorkerServiceServicer(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    WorkerService defines a TensorFlow service that executes dataflow\n    graphs on a set of local devices, on behalf of a MasterService.\n\n    A worker service keeps track of multiple ""registered graphs"". Each\n    registered graph is a subgraph of a client\'s graph, corresponding to\n    only the nodes that should execute on this worker (and any\n    additional nodes necessary for inter-process communication using\n    the `RecvTensor` method).\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n    def GetStatus(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def CreateWorkerSession(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def DeleteWorkerSession(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def RegisterGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def DeregisterGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def RunGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def CleanupGraph(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def CleanupAll(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def RecvTensor(self, request, context):\n      """"""See worker.proto for details.\n      RecvTensor Method\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def Logging(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def Tracing(self, request, context):\n      """"""See worker.proto for details.\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\n  class BetaWorkerServiceStub(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""//////////////////////////////////////////////////////////////////////////////\n\n    WorkerService defines a TensorFlow service that executes dataflow\n    graphs on a set of local devices, on behalf of a MasterService.\n\n    A worker service keeps track of multiple ""registered graphs"". Each\n    registered graph is a subgraph of a client\'s graph, corresponding to\n    only the nodes that should execute on this worker (and any\n    additional nodes necessary for inter-process communication using\n    the `RecvTensor` method).\n\n    //////////////////////////////////////////////////////////////////////////////\n\n    """"""\n    def GetStatus(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    GetStatus.future = None\n    def CreateWorkerSession(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    CreateWorkerSession.future = None\n    def DeleteWorkerSession(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    DeleteWorkerSession.future = None\n    def RegisterGraph(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    RegisterGraph.future = None\n    def DeregisterGraph(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    DeregisterGraph.future = None\n    def RunGraph(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    RunGraph.future = None\n    def CleanupGraph(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    CleanupGraph.future = None\n    def CleanupAll(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    CleanupAll.future = None\n    def RecvTensor(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      RecvTensor Method\n      """"""\n      raise NotImplementedError()\n    RecvTensor.future = None\n    def Logging(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    Logging.future = None\n    def Tracing(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""See worker.proto for details.\n      """"""\n      raise NotImplementedError()\n    Tracing.future = None\n\n\n  def beta_create_WorkerService_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_deserializers = {\n      (\'tensorflow.grpc.WorkerService\', \'CleanupAll\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'CleanupGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'CreateWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'DeleteWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'DeregisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'GetStatus\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'Logging\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'RecvTensor\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'RegisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'RunGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphRequest.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'Tracing\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingRequest.FromString,\n    }\n    response_serializers = {\n      (\'tensorflow.grpc.WorkerService\', \'CleanupAll\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'CleanupGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'CreateWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'DeleteWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'DeregisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'GetStatus\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'Logging\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'RecvTensor\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'RegisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'RunGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphResponse.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'Tracing\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingResponse.SerializeToString,\n    }\n    method_implementations = {\n      (\'tensorflow.grpc.WorkerService\', \'CleanupAll\'): face_utilities.unary_unary_inline(servicer.CleanupAll),\n      (\'tensorflow.grpc.WorkerService\', \'CleanupGraph\'): face_utilities.unary_unary_inline(servicer.CleanupGraph),\n      (\'tensorflow.grpc.WorkerService\', \'CreateWorkerSession\'): face_utilities.unary_unary_inline(servicer.CreateWorkerSession),\n      (\'tensorflow.grpc.WorkerService\', \'DeleteWorkerSession\'): face_utilities.unary_unary_inline(servicer.DeleteWorkerSession),\n      (\'tensorflow.grpc.WorkerService\', \'DeregisterGraph\'): face_utilities.unary_unary_inline(servicer.DeregisterGraph),\n      (\'tensorflow.grpc.WorkerService\', \'GetStatus\'): face_utilities.unary_unary_inline(servicer.GetStatus),\n      (\'tensorflow.grpc.WorkerService\', \'Logging\'): face_utilities.unary_unary_inline(servicer.Logging),\n      (\'tensorflow.grpc.WorkerService\', \'RecvTensor\'): face_utilities.unary_unary_inline(servicer.RecvTensor),\n      (\'tensorflow.grpc.WorkerService\', \'RegisterGraph\'): face_utilities.unary_unary_inline(servicer.RegisterGraph),\n      (\'tensorflow.grpc.WorkerService\', \'RunGraph\'): face_utilities.unary_unary_inline(servicer.RunGraph),\n      (\'tensorflow.grpc.WorkerService\', \'Tracing\'): face_utilities.unary_unary_inline(servicer.Tracing),\n    }\n    server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n    return beta_implementations.server(method_implementations, options=server_options)\n\n\n  def beta_create_WorkerService_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_serializers = {\n      (\'tensorflow.grpc.WorkerService\', \'CleanupAll\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'CleanupGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'CreateWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'DeleteWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'DeregisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'GetStatus\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'Logging\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'RecvTensor\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'RegisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'RunGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphRequest.SerializeToString,\n      (\'tensorflow.grpc.WorkerService\', \'Tracing\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingRequest.SerializeToString,\n    }\n    response_deserializers = {\n      (\'tensorflow.grpc.WorkerService\', \'CleanupAll\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'CleanupGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'CreateWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'DeleteWorkerSession\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'DeregisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'GetStatus\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'Logging\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'RecvTensor\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'RegisterGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'RunGraph\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphResponse.FromString,\n      (\'tensorflow.grpc.WorkerService\', \'Tracing\'): tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingResponse.FromString,\n    }\n    cardinalities = {\n      \'CleanupAll\': cardinality.Cardinality.UNARY_UNARY,\n      \'CleanupGraph\': cardinality.Cardinality.UNARY_UNARY,\n      \'CreateWorkerSession\': cardinality.Cardinality.UNARY_UNARY,\n      \'DeleteWorkerSession\': cardinality.Cardinality.UNARY_UNARY,\n      \'DeregisterGraph\': cardinality.Cardinality.UNARY_UNARY,\n      \'GetStatus\': cardinality.Cardinality.UNARY_UNARY,\n      \'Logging\': cardinality.Cardinality.UNARY_UNARY,\n      \'RecvTensor\': cardinality.Cardinality.UNARY_UNARY,\n      \'RegisterGraph\': cardinality.Cardinality.UNARY_UNARY,\n      \'RunGraph\': cardinality.Cardinality.UNARY_UNARY,\n      \'Tracing\': cardinality.Cardinality.UNARY_UNARY,\n    }\n    stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n    return beta_implementations.dynamic_stub(channel, \'tensorflow.grpc.WorkerService\', cardinalities, options=stub_options)\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
libs/pipeline_model/tensorflow/core/protobuf/worker_service_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom tensorflow.core.protobuf import worker_pb2 as tensorflow_dot_core_dot_protobuf_dot_worker__pb2\n\n\nclass WorkerServiceStub(object):\n  """"""//////////////////////////////////////////////////////////////////////////////\n\n  WorkerService defines a TensorFlow service that executes dataflow\n  graphs on a set of local devices, on behalf of a MasterService.\n\n  A worker service keeps track of multiple ""registered graphs"". Each\n  registered graph is a subgraph of a client\'s graph, corresponding to\n  only the nodes that should execute on this worker (and any\n  additional nodes necessary for inter-process communication using\n  the `RecvTensor` method).\n\n  //////////////////////////////////////////////////////////////////////////////\n\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.GetStatus = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/GetStatus\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusResponse.FromString,\n        )\n    self.CreateWorkerSession = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/CreateWorkerSession\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionResponse.FromString,\n        )\n    self.DeleteWorkerSession = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/DeleteWorkerSession\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionResponse.FromString,\n        )\n    self.RegisterGraph = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/RegisterGraph\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphResponse.FromString,\n        )\n    self.DeregisterGraph = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/DeregisterGraph\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphResponse.FromString,\n        )\n    self.RunGraph = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/RunGraph\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphResponse.FromString,\n        )\n    self.CleanupGraph = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/CleanupGraph\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphResponse.FromString,\n        )\n    self.CleanupAll = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/CleanupAll\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllResponse.FromString,\n        )\n    self.RecvTensor = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/RecvTensor\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorResponse.FromString,\n        )\n    self.Logging = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/Logging\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingResponse.FromString,\n        )\n    self.Tracing = channel.unary_unary(\n        \'/tensorflow.grpc.WorkerService/Tracing\',\n        request_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingRequest.SerializeToString,\n        response_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingResponse.FromString,\n        )\n\n\nclass WorkerServiceServicer(object):\n  """"""//////////////////////////////////////////////////////////////////////////////\n\n  WorkerService defines a TensorFlow service that executes dataflow\n  graphs on a set of local devices, on behalf of a MasterService.\n\n  A worker service keeps track of multiple ""registered graphs"". Each\n  registered graph is a subgraph of a client\'s graph, corresponding to\n  only the nodes that should execute on this worker (and any\n  additional nodes necessary for inter-process communication using\n  the `RecvTensor` method).\n\n  //////////////////////////////////////////////////////////////////////////////\n\n  """"""\n\n  def GetStatus(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def CreateWorkerSession(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeleteWorkerSession(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def RegisterGraph(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeregisterGraph(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def RunGraph(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def CleanupGraph(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def CleanupAll(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def RecvTensor(self, request, context):\n    """"""See worker.proto for details.\n    RecvTensor Method\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Logging(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Tracing(self, request, context):\n    """"""See worker.proto for details.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_WorkerServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'GetStatus\': grpc.unary_unary_rpc_method_handler(\n          servicer.GetStatus,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.GetStatusResponse.SerializeToString,\n      ),\n      \'CreateWorkerSession\': grpc.unary_unary_rpc_method_handler(\n          servicer.CreateWorkerSession,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CreateWorkerSessionResponse.SerializeToString,\n      ),\n      \'DeleteWorkerSession\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeleteWorkerSession,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeleteWorkerSessionResponse.SerializeToString,\n      ),\n      \'RegisterGraph\': grpc.unary_unary_rpc_method_handler(\n          servicer.RegisterGraph,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RegisterGraphResponse.SerializeToString,\n      ),\n      \'DeregisterGraph\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeregisterGraph,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.DeregisterGraphResponse.SerializeToString,\n      ),\n      \'RunGraph\': grpc.unary_unary_rpc_method_handler(\n          servicer.RunGraph,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RunGraphResponse.SerializeToString,\n      ),\n      \'CleanupGraph\': grpc.unary_unary_rpc_method_handler(\n          servicer.CleanupGraph,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupGraphResponse.SerializeToString,\n      ),\n      \'CleanupAll\': grpc.unary_unary_rpc_method_handler(\n          servicer.CleanupAll,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.CleanupAllResponse.SerializeToString,\n      ),\n      \'RecvTensor\': grpc.unary_unary_rpc_method_handler(\n          servicer.RecvTensor,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.RecvTensorResponse.SerializeToString,\n      ),\n      \'Logging\': grpc.unary_unary_rpc_method_handler(\n          servicer.Logging,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.LoggingResponse.SerializeToString,\n      ),\n      \'Tracing\': grpc.unary_unary_rpc_method_handler(\n          servicer.Tracing,\n          request_deserializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingRequest.FromString,\n          response_serializer=tensorflow_dot_core_dot_protobuf_dot_worker__pb2.TracingResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.grpc.WorkerService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
predict/src/main/python/kafka.old/wordcount_client.py,0,"b'from confluent_kafka import Producer\n\np = Producer({\'bootstrap.servers\': \'localhost:9092\'})\ntopic = \'prediction-inputs\'\nsome_data_source = [""a b c"", ""a b"", ""a""]\nfor data in some_data_source:\n    print(""producing {} to {}"".format(data, topic))\n    p.produce(topic, data.encode(\'utf-8\'))\np.flush()\n'"
predict/src/main/python/kafka.old/wordcount_stream.py,0,"b'import logging\nimport time\nimport sys\nimport collections\n\nfrom winton_kafka_streams.processor import BaseProcessor, TopologyBuilder\nimport winton_kafka_streams.kafka_config as kafka_config\nimport winton_kafka_streams.kafka_streams as kafka_streams\n\nlog = logging.getLogger(__name__)\n\n# An example implementation of word count,\n# showing where punctuate can be useful\nclass WordCount(BaseProcessor):\n\n    def initialise(self, _name, _context):\n        super().initialise(_name, _context)\n        self.word_counts = collections.Counter()\n        # dirty_words tracks what words have changed since the last punctuate\n        self.dirty_words = set()\n        # output updated counts every 10 seconds\n        self.context.schedule(1.)\n\n    def process(self, key, value):\n        words = value.decode(\'utf-8\').split()\n        log.debug(f\'words list ({words})\')\n        self.word_counts.update(words)\n        self.dirty_words |= set(words)\n\n    def punctuate(self, timestamp):\n        for word in self.dirty_words:\n            count = str(self.word_counts[word])\n            log.debug(f\'Forwarding to sink ({word}, {count})\')\n            self.context.forward(word, count)\n        self.dirty_words = set()\n\n\ndef run(config_file):\n    kafka_config.read_local_config(config_file)\n\n    # Can also directly set config variables inline in Python\n    #kafka_config.KEY_SERDE = MySerde\n    with TopologyBuilder() as topology_builder:\n        topology_builder. \\\n            source(\'inputs\', [\'prediction-inputs\']). \\\n            processor(\'count\', WordCount, \'inputs\'). \\\n            sink(\'outputs\', \'prediction-outputs\', \'count\')\n\n    wks = kafka_streams.KafkaStreams(topology_builder, kafka_config)\n    wks.start()\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        wks.close()\n\n\nif __name__ == \'__main__\':\n    import argparse\n\n    parser = argparse.ArgumentParser(description=""Debug runner for Python Kafka Streams"")\n    parser.add_argument(\'--config-file\', \'-c\',\n                        help=""Local configuration - will override internal defaults"",\n                        default=\'config.properties\')\n    parser.add_argument(\'--verbose\', \'-v\',\n                        help=""Increase versbosity (repeat to increase level)"",\n                        action=\'count\', default=0)\n    args = parser.parse_args()\n\n    #levels = {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}\n    #level = levels.get(args.verbose, logging.DEBUG)\n    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n    run(args.config_file)\n'"
predict/src/main/python/kafka.old/ws_kafka_topic_stream.py,0,"b'#!/usr/bin/env python3\n\nimport os\nimport logging\nimport tornado.httpserver\nimport tornado.websocket\nimport tornado.ioloop\nimport tornado.web\nfrom tornado.ioloop import PeriodicCallback\nfrom tornado.options import define, options\nfrom random import randint #Random generator\nfrom urllib.parse import urlparse\nfrom confluent_kafka import Consumer, KafkaError\n\nLOGGER = logging.getLogger(__name__)\nLOGGER.setLevel(logging.ERROR)\n\nCH = logging.StreamHandler()\nCH.setLevel(logging.ERROR)\nLOGGER.addHandler(CH)\n\nTORNADO_ACCESS_LOGGER = logging.getLogger(\'tornado.access\')\nTORNADO_ACCESS_LOGGER.setLevel(logging.ERROR)\n\nTORNADO_APPLICATION_LOGGER = logging.getLogger(\'tornado.application\')\nTORNADO_APPLICATION_LOGGER.setLevel(logging.ERROR)\n\nTORNADO_GENERAL_LOGGER = logging.getLogger(\'tornado.general\')\nTORNADO_GENERAL_LOGGER.setLevel(logging.ERROR)\n\ndefine(\'PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT\', default=\'\', help=\'tornado http websocket kafka server listen port\', type=int)\ndefine(\'PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL\', default=\'\', help=\'interval to update from kafka topic\', type=int)\n\nclass HealthzHandler(tornado.web.RequestHandler):\n\n    @tornado.web.asynchronous\n    def get(self):\n        try:\n            self.set_status(200, None)\n            self.add_header(\'Content-Type\', \'text/plain\')\n            self.finish()\n        except Exception as e:\n            logging.exception(\'HealthzHandler.get: Exception {0}\'.format(str(e)))\n\n\nclass WSKafkaHandler(tornado.websocket.WebSocketHandler):\n    #check_origin fixes an error 403 with Tornado\n    #http://stackoverflow.com/questions/24851207/tornado-403-get-warning-when-opening-websocket\n    def check_origin(self, origin):\n        CORS_ORIGINS = [\'127.0.0.1\', \'localhost\', \'pipeline.ai\', \'community.pipeline.ai\']\n        parsed_origin = urlparse(origin)\n        # parsed_origin.netloc.lower() gives localhost:3333\n        return (parsed_origin.hostname in CORS_ORIGINS) or parsed_origin.hostname.endswith(\'.pipeline.ai\')\n\n    def open(self, topic_name):\n    #Send message periodic via socket upon a time interval\n        # To consume latest messages and auto-commit offsets\n        #self.consumer = KafkaConsumer(topic_name, bootstrap_servers=[\'localhost:9092\'])\n        self.consumer = Consumer({\'bootstrap.servers\': \'localhost:9092\'})\n        self.consumer.subscribe([\'topic_name\'])\n\n        self.callback = PeriodicCallback(self.send_values, options.PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL)\n        self.callback.start()\n\n    def send_values(self):\n        #Generates random values to send via websocket\n        #self.write_message(str(randint(1,10)) + \';\' + str(randint(1,10)) + \';\' + str(randint(1,10)) + \';\' + str(randint(1,10)))\n        for record in self.consumer:\n            # message value and key are raw bytes -- decode if necessary!\n            # e.g., for unicode: `message.value.decode(\'utf-8\')`\n            #print (""%s:%d:%d: key=%s value=%s"" % (message.topic, message.partition,\n            #                                      message.offset, message.key,\n            #                                      message.value))\n            try:\n                value = record.value.decode(\'utf-8\')\n                self.write_message(value)\n            except Exception as e:\n                logging.exception(\'WSHandler.send_values: Exception {0}\'.format(str(e)))\n                self.write_message(\'Error {0}\'.format(str(e)))\n\n    def on_message(self, message):\n        pass\n\n    def on_close(self):\n        self.callback.stop()\n\n\nclass Application(tornado.web.Application):\n    def __init__(self):\n        handlers = [\n            (r\'/stream/kafka/(.*)\', tornado.web.StaticFileHandler,\n                dict(default_filename=\'index.html\',\n                     path=os.path.join(os.path.dirname(__file__), \'static\'))),\n            (r\'/stream/ws/kafka/([a-zA-Z\\-0-9\\.:,_]+)\', WSKafkaHandler),\n            (r\'/healthz\', HealthzHandler),\n        ]\n        tornado.web.Application.__init__(self, handlers,)\n\n    def fallback(self):\n        LOGGER.warn(\'Model Server Application fallback: {0}\'.format(self))\n        return \'Fallback!\'\n\n\ndef main():\n    try:\n        tornado.options.parse_command_line()\n        if not options.PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT or not options.PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL:\n            LOGGER.error(\'--PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT and --PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL must be set\')\n            return\n\n        LOGGER.info(\'WebSocket Kafka Server main: begin start tornado-based http server port {0}\'.format(options.PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT))\n        http_server = tornado.httpserver.HTTPServer(Application())\n        http_server.listen(options.PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT)\n        LOGGER.info(\'WebSocket Kafka Server main: complete start tornado-based http server port {0}\'.format(options.PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL))\n\n        tornado.ioloop.IOLoop.current().start()\n        print(\'...Python-based WebSocket Kafka Server Started!\')\n    except Exception as e:\n        LOGGER.info(\'ws_kafka_topic_stream.main: Exception {0}\'.format(str(e)))\n        logging.exception(\'ws_kafka_topic_stream.main: Exception {0}\'.format(str(e)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
predict/src/main/python/model_server/model_kafka_server_python.py,0,"b""#!/usr/bin/env python3\n\nimport importlib\nimport os, subprocess\nimport logging\nfrom pipeline_monitor import prometheus_monitor as monitor\nfrom pipeline_monitor import prometheus_monitor_registry as monitor_registry\nfrom prometheus_client import CollectorRegistry, generate_latest, start_http_server, Summary, Counter, Histogram, Gauge\n\nfrom confluent_kafka import Consumer, KafkaError\n\n#define('PIPELINE_RESOURCE_NAME', default='', help='model name', type=str)\n#define('PIPELINE_RESOURCE_TAG', default='', help='model tag', type=str)\n#define('PIPELINE_RESOURCE_SUBTYPE', default='', help='model type', type=str)\n#define('PIPELINE_RUNTIME', default='', help='model runtime', type=str)\n#define('PIPELINE_CHIP', default='', help='model chip', type=str)\n#define('PIPELINE_RESOURCE_PATH', default='', help='model path', type=str)\n\n#define('PIPELINE_STREAM_LOGGER_URL', default='', help='model stream logger url', type=str)\n#define('PIPELINE_STREAM_LOGGER_TOPIC', default='', help='model stream logger topic', type=str)\n#define('PIPELINE_STREAM_INPUT_URL', default='', help='model stream input url', type=str)\n#define('PIPELINE_STREAM_INPUT_TOPIC', default='', help='model stream input topic', type=str)\n#define('PIPELINE_STREAM_OUTPUT_URL', default='', help='model stream output url', type=str)\n#define('PIPELINE_STREAM_OUTPUT_TOPIC', default='', help='model stream output topic', type=str)\n\n_logger = logging.getLogger(__name__)\n_logger .setLevel(logging.INFO)\n\ndef main():\n    try:\n#        if not (    options.PIPELINE_RESOURCE_NAME\n#                and options.PIPELINE_RESOURCE_TAG\n#                and options.PIPELINE_RESOURCE_SUBTYPE\n#                and options.PIPELINE_RUNTIME\n#                and options.PIPELINE_CHIP\n#                and options.PIPELINE_RESOURCE_PATH\n#                and options.PIPELINE_STREAM_LOGGER_URL\n#                and options.PIPELINE_STREAM_LOGGER_TOPIC\n#                and options.PIPELINE_STREAM_INPUT_URL\n#                and options.PIPELINE_STREAM_INPUT_TOPIC\n#                and options.PIPELINE_STREAM_OUTPUT_URL\n#                and options.PIPELINE_STREAM_OUTPUT_TOPIC):\n\n#            _logger.error('--PIPELINE_RESOURCE_NAME \\\n#                           --PIPELINE_RESOURCE_TAG \\\n#                           --PIPELINE_RESOURCE_SUBTYPE \\\n#                           --PIPELINE_RUNTIME \\\n#                           --PIPELINE_CHIP \\\n#                           --PIPELINE_RESOURCE_PATH \\\n#                           --PIPELINE_STREAM_LOGGER_URL \\\n#                           --PIPELINE_STREAM_LOGGER_TOPIC \\\n#                           --PIPELINE_STREAM_INPUT_URL \\\n#                           --PIPELINE_STREAM_INPUT_TOPIC \\\n#                           --PIPELINE_STREAM_OUTPUT_URL \\\n#                           --PIPELINE_STREAM_OUTPUT_TOPIC \\\n#                           must be set')\n#            return\n\n#        _logger.info('Model Kafka Server main: begin start kafka based consumer listening for stream logger url {0}, stream logger topic {1}, stream input url {2}, stream input topic {3}, and stream output url {4} and stream output topic {5}'.format(options.PIPELINE_STREAM_LOGGER_URL, options.PIPELINE_STREAM_LOGGER_TOPIC, options.PIPELINE_STREAM_INPUT_URL, options.PIPELINE_STREAM_INPUT_TOPIC, options.PIPELINE_STREAM_OUTPUT_URL, options.PIPELINE_STREAM_OUTPUT_TOPIC))\n\n        settings = {\n            'bootstrap.servers': 'community.pipeline.ai:31092',\n            # 'bootstrap.servers': options.PIPELINE_STREAM_INPUT_URL,\n            'group.id': 'consumer-1',\n            'client.id': 'client-1',\n            'enable.auto.commit': True,\n            'session.timeout.ms': 6000,\n            'default.topic.config': {'auto.offset.reset': 'smallest'}\n        }  \n    except Exception as e:\n        _logger.info('model_kafka_server_python.main: Exception {0}'.format(str(e)))\n        _logger.exception('model_kafka_server_python.main: Exception {0}'.format(str(e)))\n        return\n\n\n    try:\n        consumer = Consumer(settings)\n        consumer.subscribe(['input'])\n        # consumer.subscribe([options.PIPELINE_STREAM_INPUT_TOPIC])\n\n        while True:\n            msg = consumer.poll(0.1)\n            if msg is None:\n                continue\n            elif not msg.error():\n                print('Received message: {0}'.format(msg.value()))\n\n                # TODO:  model.predict()\n\n                # TODO:  Write prediction to `PIPELINE_STREAM_OUTPUT_URL`:`PIPELINE_STREAM_OUTPUT_TOPIC`\n            elif msg.error().code() == KafkaError._PARTITION_EOF:\n                print('End of partition reached {0}/{1}'\n                      .format(msg.topic(), msg.partition()))\n            else:\n                print('Error occured: {0}'.format(msg.error().str()))\n    except Exception as e:\n        _logger.info('model_kafka_server_python.main: Exception {0}'.format(str(e)))\n        _logger.exception('model_kafka_server_python.main: Exception {0}'.format(str(e)))\n    finally:\n        if consumer:\n            consumer.close()\n\n\nif __name__ == '__main__':\n    main()\n"""
predict/src/main/python/model_server/model_server_python.py,0,"b'#!/usr/bin/env python3\n\nimport importlib\nimport os, subprocess\nimport logging\nimport tornado.ioloop\nimport tornado.options\nimport tornado.web\nimport tornado.httpserver\nfrom tornado.options import define, options\nfrom pipeline_monitor import prometheus_monitor as monitor\nfrom pipeline_monitor import prometheus_monitor_registry as monitor_registry\nfrom prometheus_client import CollectorRegistry, generate_latest, start_http_server, Summary, Counter, Histogram, Gauge\n\ndefine(\'PIPELINE_NAME\', default=\'\', help=\'name\', type=str)\ndefine(\'PIPELINE_TAG\', default=\'\', help=\'tag\', type=str)\ndefine(\'PIPELINE_RUNTIME\', default=\'\', help=\'runtime\', type=str)\ndefine(\'PIPELINE_CHIP\', default=\'\', help=\'chip\', type=str)\ndefine(\'PIPELINE_RESOURCE_NAME\', default=\'\', help=\'resource name\', type=str)\ndefine(\'PIPELINE_RESOURCE_TAG\', default=\'\', help=\'resource tag\', type=str)\ndefine(\'PIPELINE_RESOURCE_TYPE\', default=\'\', help=\'resource type\', type=str)\ndefine(\'PIPELINE_RESOURCE_SUBTYPE\', default=\'\', help=\'resource subtype\', type=str)\n\ndefine(\'PIPELINE_RESOURCE_PATH\', default=\'\', help=\'model path\', type=str)\ndefine(\'PIPELINE_RESOURCE_SERVER_PORT\', default=\'\', help=\'tornado http server listen port\', type=int)\ndefine(\'PIPELINE_RESOURCE_SERVER_TENSORFLOW_SERVING_PORT\', default=\'\', help=\'port to run the prometheus http metrics server on\', type=int)\n\n# Create a metric to track time spent and requests made.\nREQUEST_TIME_SUMMARY = Summary(\'request_processing_time\', \'Model Server: Time Spent Processing Request\', \n   [\'method\', \'name\', \'tag\', \'runtime\', \'chip\', \'resource_name\', \'resource_tag\', \'resource_type\', \'resource_subtype\'])\n\nREQUESTS_IN_PROGRESS_GAUGE = Gauge(\'inprogress_requests\', \'Model Server: Requests Currently In Progress\', \n   [\'method\', \'name\', \'tag\', \'runtime\', \'chip\', \'resource_name\', \'resource_tag\', \'resource_type\', \'resource_subtype\'])\n\nREQUEST_COUNTER = Counter(\'http_requests_total\', \'Model Server: Total Http Request Count Since Last Process Restart\', \n   [\'method\', \'name\', \'tag\', \'runtime\', \'chip\', \'resource_name\', \'resource_tag\', \'resource_type\', \'resource_subtype\'])\n\nEXCEPTION_COUNTER = Counter(\'exceptions_total\', \'Model Server: Total Exceptions\', \n   [\'method\', \'name\', \'tag\', \'runtime\', \'chip\', \'resource_name\', \'resource_tag\', \'resource_type\', \'resource_subtype\'])\n\n\nmonitor_registry.register(REQUEST_TIME_SUMMARY)\nmonitor_registry.register(REQUESTS_IN_PROGRESS_GAUGE)\nmonitor_registry.register(REQUEST_COUNTER)\nmonitor_registry.register(EXCEPTION_COUNTER)\n\n_logger = logging.getLogger(__name__)\n_logger .setLevel(logging.INFO)\n\n_stream_handler = logging.StreamHandler()\n_stream_handler.setLevel(logging.INFO)\n_logger.addHandler(_stream_handler)\n\nTORNADO_ACCESS_LOGGER = logging.getLogger(\'tornado.access\')\nTORNADO_ACCESS_LOGGER.setLevel(logging.ERROR)\n\nTORNADO_APPLICATION_LOGGER = logging.getLogger(\'tornado.application\')\nTORNADO_APPLICATION_LOGGER.setLevel(logging.ERROR)\n\nTORNADO_GENERAL_LOGGER = logging.getLogger(\'tornado.general\')\nTORNADO_GENERAL_LOGGER.setLevel(logging.ERROR)\n\n\nclass StatusHandler(tornado.web.RequestHandler):\n\n    @tornado.web.asynchronous\n    def get(self):\n        try:\n            self.set_status(200, None)\n            self.add_header(\'Content-Type\', \'text/plain\')\n            self.write(\'{""status"":""OK""}\')\n            self.finish()\n        except Exception as e:\n            _logger.exception(\'Status.get: Exception {0}\'.format(str(e)))\n\n\nclass MetricsHandler(tornado.web.RequestHandler):\n\n    def metrics(self):\n        return generate_latest(monitor_registry)\n\n    @tornado.web.asynchronous\n    def get(self):\n        try:\n            self.set_status(200, None)\n            self.add_header(\'Content-Type\', \'text/plain\')\n            self.write(self.metrics())\n            self.finish()\n        except Exception as e:\n            _logger.exception(\'MetricsHandler.get: Exception {0}\'.format(str(e)))\n\n\nclass ModelPredictPython3Handler(tornado.web.RequestHandler):\n\n    _method = \'invoke\'\n    _key_list = [\n        os.environ[\'PIPELINE_NAME\'], \n\tos.environ[\'PIPELINE_TAG\'], \n\tos.environ[\'PIPELINE_RUNTIME\'], \n \tos.environ[\'PIPELINE_CHIP\'], \n\tos.environ[\'PIPELINE_RESOURCE_NAME\'], \n\tos.environ[\'PIPELINE_RESOURCE_TAG\'], \n\tos.environ[\'PIPELINE_RESOURCE_TYPE\'], \n\tos.environ[\'PIPELINE_RESOURCE_SUBTYPE\']\n    ]\n    _key = \'/\'.join(_key_list)\n    _runtime = os.environ[\'PIPELINE_RUNTIME\']\n\n    try:\n        _invokable = importlib.import_module(\'pipeline_invoke_%s\' % _runtime)\n        _logger.info(\'Installing invoke bundle and updating environment: complete\')\n    except Exception as e:\n        message = \'ModelPredictPython3Handler: Exception - {0} Error {1}\'.format(_key, str(e))\n        _logger.info(message)\n        _logger.exception(message)\n\n    @tornado.web.asynchronous\n    def post(self):\n        with EXCEPTION_COUNTER.labels(ModelPredictPython3Handler._method, *ModelPredictPython3Handler._key_list).count_exceptions(), \\\n          REQUESTS_IN_PROGRESS_GAUGE.labels(ModelPredictPython3Handler._method, *ModelPredictPython3Handler._key_list).track_inprogress():\n            try:\n                REQUEST_COUNTER.labels(ModelPredictPython3Handler._method, *ModelPredictPython3Handler._key_list).inc()\n\n                # https://github.com/PipelineAI/product-private/issues/94\n                # TODO:  the headers need to pass through the JVM layer, as well. \n                #        See PredictionService.scala for more TODO\'s.\n                # TODO:  https://github.com/istio/istio.github.io/blob/a6fae1b3683e20e40a718f0bd2332350d8c7a87a/_docs/tasks/telemetry/distributed-tracing.md\n                #incoming_headers = [ \'x-request-id\',\n                #                     \'x-b3-traceid\',\n                #                     \'x-b3-spanid\',\n                #                     \'x-b3-parentspanid\',\n                #                     \'x-b3-sampled\',\n                #                     \'x-b3-flags\',\n                #                     \'x-ot-span-context\'\n                #]\n                #for ihdr in incoming_headers:\n                #    val = self.request.headers.get(ihdr)\n                #    if val is not None:\n                #        headers[ihdr] = val\n                \n                response = ModelPredictPython3Handler._invokable.invoke(self.request.body)\n            except Exception as e:\n                response = \'ModelPredictPython3Handler.post: Exception - {0} Error {1}\'.format(ModelPredictPython3Handler._key, str(e))\n                _logger.info(response)\n                _logger.exception(response)\n            finally:\n                self.write(response)\n                self.finish()\n\n\nclass ModelServerApplication(tornado.web.Application):\n    def __init__(self):\n        handlers = [\n            (r\'/ping\', StatusHandler),\n            (r\'/metrics\', MetricsHandler),\n            (r\'/\', ModelPredictPython3Handler),\n        ]\n        tornado.web.Application.__init__(self, handlers, debug=False, autoreload=False)\n\n    def fallback(self):\n        _logger.warn(\'Model Server Application fallback: {0}\'.format(self))\n        return \'fallback!\'\n\napp = ModelServerApplication()\n\ndef main():\n    try:\n        tornado.options.parse_command_line()\n\n        if not (\n                    options.PIPELINE_NAME\n                and options.PIPELINE_TAG\n                and options.PIPELINE_RUNTIME\n                and options.PIPELINE_CHIP\n\t\tand options.PIPELINE_RESOURCE_NAME\n                and options.PIPELINE_RESOURCE_TAG\n                and options.PIPELINE_RESOURCE_TYPE\n                and options.PIPELINE_RESOURCE_SUBTYPE\n                and options.PIPELINE_RESOURCE_PATH\n                and options.PIPELINE_RESOURCE_SERVER_PORT\n                and options.PIPELINE_RESOURCE_SERVER_TENSORFLOW_SERVING_PORT):\n            _logger.error(\'--PIPELINE_NAME \\\n                           --PIPELINE_TAG \\\n                           --PIPELINE_RUNTIME \\\n                           --PIPELINE_CHIP \\\n                           --PIPELINE_RESOURCE_NAME \\\n                           --PIPELINE_RESOURCE_TAG \\\n                           --PIPELINE_RESOURCE_TYPE \\\n                           --PIPELINE_RESOURCE_SUBTYPE \\\n                           --PIPELINE_RESOURCE_PATH \\\n                           --PIPELINE_RESOURCE_SERVER_PORT \\\n                           --PIPELINE_RESOURCE_SERVER_TENSORFLOW_SERVING_PORT \\\n                           must be set\')\n            return\n\n        _logger.info(\'Model Server main: begin start tornado-based http server port {0}\'.format(options.PIPELINE_RESOURCE_SERVER_PORT))\n        \n        model_server_python_module_path = os.path.dirname(os.path.realpath(__file__))\n        cmd = \'gunicorn --bind 0.0.0.0:%s --pythonpath %s -k tornado model_server_python:app\' % (options.PIPELINE_RESOURCE_SERVER_PORT, model_server_python_module_path)\n        subprocess.call(cmd, shell=True)\n    except Exception as e:\n        _logger.info(\'model_server_python.main: Exception {0}\'.format(str(e)))\n        _logger.exception(\'model_server_python.main: Exception {0}\'.format(str(e)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/build_image.py,0,"b'# -*- coding: utf-8 -*-\n""""""Script to build images.\n\nFor example,\npython build_image.py --tf_version=1.6 --platform=gpu tf_serving\n""""""\nimport argparse\nimport datetime\nfrom itertools import chain\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nimport yaml\n\n\ndef run(command,\n        cwd=None,\n        env=None,\n        polling_interval=datetime.timedelta(seconds=1)):\n  """"""Run a subprocess.\n  Copied from kubeflow/test so it\'s easier to run locally.\n  TODO(lunkai): refactor to dedup.\n  Any subprocess output is emitted through the logging modules.\n  Returns:\n    output: A string containing the output.\n  """"""\n  logging.info(""Running: %s \\ncwd=%s"", "" "".join(command), cwd)\n\n  if not env:\n    env = os.environ\n  else:\n    keys = sorted(env.keys())\n\n    lines = []\n    for k in keys:\n      lines.append(""{0}={1}"".format(k, env[k]))\n    logging.info(""Running: Environment:\\n%s"", ""\\n"".join(lines))\n\n  process = subprocess.Popen(\n      command,\n      cwd=cwd,\n      env=env,\n      stdout=subprocess.PIPE,\n      stderr=subprocess.STDOUT)\n\n  logging.info(""Subprocess output:\\n"")\n  output = []\n  while process.poll() is None:\n    process.stdout.flush()\n    for line in iter(process.stdout.readline, b\'\'):\n      output.append(line.strip())\n      logging.info(line.strip())\n\n    time.sleep(polling_interval.total_seconds())\n\n  process.stdout.flush()\n  for line in iter(process.stdout.readline, b\'\'):\n    output.append(line.strip())\n    logging.info(line.strip())\n\n  if process.returncode != 0:\n    raise subprocess.CalledProcessError(\n        process.returncode, ""cmd: {0} exited with code {1}"".format(\n            "" "".join(command), process.returncode), ""\\n"".join(output))\n\n  return ""\\n"".join(output)\n\n\ndef wait_for_docker_daemon(timeout=60):\n  """"""Waiting for docker daemon to be ready. This is needed in DinD scenario.""""""\n  start_time = time.time()\n  while time.time() - start_time < timeout:\n    try:\n      subprocess.check_call([""docker"", ""ps""])\n    except subprocess.CalledProcessError:\n      time.sleep(5)\n    # Daemon ready.\n    logging.info(""docker daemon ready.\\n"")\n    return\n  # Timeout.\n  logging.error(""Timeout waiting for docker daemon\\n"")\n  # TODO(lunkai): use TimeoutError when we use py3.\n  raise RuntimeError\n\n\ndef get_build_args(config):\n  """"""\n  Make the list of params for docker build from config.\n\n  For example, if the config is {""a"": 1, ""b"": 2}\n  This should return\n  [""--build-arg"", ""a=1"", ""--build-arg"", ""b=2""]\n  """"""\n  config_list = [key + ""="" + val for key, val in config.items()]\n  return list(chain.from_iterable([[""--build-arg"", x] for x in config_list]))\n\n\ndef get_config(context_dir, version):\n  """"""Returns a dict of configuration from the version-config file.""""""\n  config_file = os.path.join(context_dir, ""versions"", version,\n                             ""version-config.json"")\n  with open(config_file) as f:\n    config = yaml.load(f)\n  return config\n\n\ndef build_tf_serving(args):\n  wait_for_docker_daemon()\n  dir_path = os.path.dirname(os.path.realpath(__file__))\n  context_dir = os.path.join(dir_path, ""k8s-model-server/images"")\n  version = args.tf_version if args.platform == ""cpu"" else args.tf_version + ""gpu""\n\n  config = get_config(context_dir, version)\n  build_args = get_build_args(config)\n  image_name = ""{}/tensorflow-serving-{}:{}"".format(args.registry, version,\n                                                    args.tag)\n\n  command = list(\n      chain(\n          [""docker"", ""build"", ""--pull""], build_args,\n          [""-t"", image_name, ""-f"", ""Dockerfile.{}"".format(args.platform), "".""]))\n  run(command, cwd=context_dir)\n\n  if args.push_gcr:\n    run([\n        ""gcloud"", ""auth"", ""activate-service-account"", ""--key-file"",\n        os.environ[\'GOOGLE_APPLICATION_CREDENTIALS\']\n    ])\n    run([""gcloud"", ""docker"", ""--"", ""push"", image_name])\n\n\ndef build_tf_notebook(args):\n  wait_for_docker_daemon()\n  dir_path = os.path.dirname(os.path.realpath(__file__))\n  context_dir = os.path.join(dir_path, ""tensorflow-notebook-image"")\n  version = args.tf_version if args.platform == ""cpu"" else args.tf_version + ""gpu""\n\n  config = get_config(context_dir, version)\n  build_args = get_build_args(config)\n  image_name = ""{}/tensorflow-{}-notebook-{}:{}"".format(\n      args.registry, args.tf_version, args.platform, args.tag)\n\n  command = list(\n      chain([""docker"", ""build"", ""--pull""], build_args,\n            [""-t"", image_name, ""-f"", ""Dockerfile"", "".""]))\n  run(command, cwd=context_dir)\n\n  if args.push_gcr:\n    run([\n        ""gcloud"", ""auth"", ""activate-service-account"", ""--key-file"",\n        os.environ[\'GOOGLE_APPLICATION_CREDENTIALS\']\n    ])\n    run([""gcloud"", ""docker"", ""--"", ""push"", image_name])\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  subparsers = parser.add_subparsers()\n\n  parser.add_argument(\n      ""--registry"",\n      default=""gcr.io/kubeflow-images-public"",\n      help=""The registry of the image"")\n  parser.add_argument(""--tag"", default=""latest"", help=""The image tag"")\n  parser.add_argument(""--tf_version"", default=""1.6"", help=""Tensorflow version"")\n  parser.add_argument(""--platform"", default=""cpu"", help=""cpu or gpu"")\n  parser.add_argument(\n      ""--push_gcr"",\n      action=\'store_true\',\n      default=False,\n      help=""Whether to push the image after building."")\n\n  parser_tf_serving = subparsers.add_parser(""tf_serving"")\n  parser_tf_serving.set_defaults(func=build_tf_serving)\n\n  parser_tf_notebook = subparsers.add_parser(""tf_notebook"")\n  parser_tf_notebook.set_defaults(func=build_tf_notebook)\n\n  args = parser.parse_args()\n  args.func(args)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/hack/convert_manifest_to_jsonnet.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple utility to convert a manifest to corresponding jsonnet.""""""\nimport argparse\nimport json\nimport yaml\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser(description=""Convert manifest."")\n  parser.add_argument(\n      ""--manifest"",\n      type=str,\n      required=True,\n  )\n\n  args = parser.parse_args()\n\n  with open(args.manifest) as hf:\n    manifest = hf.read()\n\n  components = manifest.split(""---"")\n\n  index = 0\n  for c in components:\n    t = c.strip()\n    if not t:\n      continue\n\n    d = yaml.load(t)\n    j = json.dumps(d, indent=2, sort_keys=True)\n    print(""component_{0}:: {1}, \\n"".format(index, j))\n    index += 1\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/kubeflow/generate_docs.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport glob\nimport os\nimport subprocess\n\nif __name__ == ""__main__"":\n  this_dir = os.path.dirname(__file__)\n\n  GOPATH = os.getenv(""GOPATH"")\n  doc_gen = os.path.join(GOPATH, ""bin/doc-gen"")\n  for f in os.listdir(this_dir):\n    full_dir = os.path.join(this_dir, f)\n    if not os.path.isdir(f):\n      continue\n    prototypes = glob.glob(os.path.join(full_dir, ""prototypes/*.jsonnet""))\n\n    command = [doc_gen, os.path.join(full_dir, ""parts.yaml"")]\n    command.extend(prototypes)\n    with open(os.path.join(full_dir, ""README.md""), ""w"") as hout:\n      subprocess.check_call(command, stdout=hout)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/releasing/add_image_shas.py,0,"b'# -*- coding: utf-8 -*-\n""""""The script uses a regex to identify images in GCR and add entries for them to\nimage_tags.yaml\n""""""\n\nimport argparse\nimport logging\nimport re\nimport json\nimport yaml\n\nfrom kubeflow.testing import util\n\n\ndef main(unparsed_args=None):  # pylint: disable=too-many-locals\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  # create the top-level parser\n  parser = argparse.ArgumentParser(description=""Get Images by regex"")\n\n  parser.add_argument(\n      ""--pattern"",\n      default="""",\n      type=str,\n      help=""Regex pattern e.g. .*tensorflow.*notebook.*:v20180619.*"")\n\n  parser.add_argument(\n      ""--images_file"",\n      default=""image_tags.yaml"",\n      type=str,\n      help=""Yaml file containing the tags to attach."")\n\n  parser.add_argument(\n      ""--repository"",\n      default=None,\n      type=str,\n      help=""GCR repository name (optional)."")\n\n  args = parser.parse_args()\n\n  with open(args.images_file) as hf:\n    config = yaml.load(hf)\n\n  existing_images = {}\n\n  for image in config[""images""]:\n    existing_images[image[""name""]] = {}\n    for v in image[""versions""]:\n      existing_images[image[""name""]][v[""digest""]] = v\n\n  list_images_cmd = [\n      ""gcloud"", ""--project=kubeflow-images-public"", ""container"", ""images"",\n      ""list"", ""--format=json""\n  ]\n  # By default gcloud uses gcr.io/[project] as the repository. However for\n  # images like katib, we may need to specify the repository as\n  # gcr.io/[project]/katib.\n  if args.repository:\n    list_images_cmd.append(""--repository="" + args.repository)\n  raw_images = util.run(list_images_cmd)\n\n  all_images = json.loads(raw_images)\n  name_pattern, tag_pattern = args.pattern.split("":"")\n\n  name_re = re.compile(name_pattern)\n  tag_re = re.compile(tag_pattern)\n\n  matching = []\n  for image in all_images:\n    if not name_re.match(image[""name""]):\n      continue\n    logging.info(""Matching image: %s"", image[""name""])\n    matching.append(image)\n\n  # For each image ist all tags and find the matching ones\n  images_to_add = {}\n  for image in matching:\n    raw_tags = util.run([\n        ""gcloud"", ""--project=kubeflow-images-public"", ""container"", ""images"",\n        ""list-tags"", image[""name""], ""--format=json""\n    ])\n\n    tags = json.loads(raw_tags)\n\n    for info in tags:\n      for t in info[""tags""]:\n        if tag_re.match(t):\n          is_match = True\n          versions = images_to_add.get(image[""name""], {})\n          versions[info[""digest""]] = info\n          images_to_add[image[""name""]] = versions\n\n  # Merge in any missing versions\n  for name, versions in images_to_add.iteritems():\n    if name not in existing_images:\n      existing_images[name] = {}\n\n    for v in versions.itervalues():\n      if v[""digest""] in existing_images[name]:\n        logging.info(""Image %s sha %s already defined."", name, v[""digest""])\n      else:\n        logging.info(""Image %s adding sha %s"", name, v[""digest""])\n        existing_images[name][v[""digest""]] = v\n\n  # Convert to the expected output\n  output = {}\n  output[""images""] = []\n\n  names = sorted(existing_images.keys())\n  for name in names:\n    versions = existing_images[name]\n    new_image = {}\n    new_image[""name""] = name\n    new_image[""versions""] = []\n    for v in versions.itervalues():\n      new_image[""versions""].append(v)\n\n    output[""images""].append(new_image)\n\n  with open(args.images_file, ""w"") as hf:\n    hf.write(yaml.safe_dump(output, default_flow_style=False))\n  logging.info(""Done."")\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/releasing/add_image_tag.py,0,"b'# -*- coding: utf-8 -*-\n""""""This script adds or moves a tag in image_tags.yaml\n\nThis script doesn\'t actually update the images. For that you need to call\napply_image_tags using image_tags.yaml\n\nThe script looks for images matching a regex and will add a tag to that image.\nIf that tag is already on an existing version of the image it is removed.\n\nExample:\npython add_image_tag.py --pattern=.*tensorflow.*1.*notebook.*:v20180619.* \\\n  --tag=v0.2.0\n\nThis would add the tag v0.2.0 to images matching the pattern and remove it from\nany existing images.\n""""""\n\nimport argparse\nimport logging\nimport re\nimport yaml\n\nfrom kubeflow.testing import util\n\n\ndef main(unparsed_args=None):  # pylint: disable=too-many-locals\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  # create the top-level parser\n  parser = argparse.ArgumentParser(description=""Apply tags to file"")\n\n  parser.add_argument(\n      ""--images_file"",\n      default=""image_tags.yaml"",\n      type=str,\n      help=""Yaml file containing the tags to attach."")\n\n  parser.add_argument(\n      ""--pattern"",\n      default="""",\n      type=str,\n      help=(""Regex pattern e.g. .*tensorflow.*notebook.*:v20180619.* ""\n            ""to select the images to apply.""))\n\n  parser.add_argument(""--tag"", default="""", type=str, help=""The tag to apply"")\n\n  args = parser.parse_args()\n\n  with open(args.images_file) as hf:\n    config = yaml.load(hf)\n\n  if not config:\n    raise ValueError(""No images could be load from %s"" % args.images_file)\n  name_pattern, tag_pattern = args.pattern.split("":"")\n  name_re = re.compile(name_pattern)\n  tag_re = re.compile(tag_pattern)\n\n  for image in config[""images""]:\n    name = image[""name""]\n    if not name_re.match(name):\n      continue\n\n    # Loop over all the images and see if the supplied tag is already\n    # mapped to an image and which version to add the label to.\n    # The index of the version to add the tag to.\n    new_index = []\n    existing_index = []\n    for v_index, v in enumerate(image[""versions""]):\n      for tag in v[""tags""]:\n        if tag == args.tag:\n          existing_index.append(v_index)\n\n        if tag_re.match(tag):\n          new_index.append(v_index)\n\n    if len(existing_index) > 1:\n      logging.error(""Multiple images %s had tag %s"", name, args.tag)\n\n    # TODO(jlewi)\n    if existing_index and not new_index:\n      logging.error(""Not moving tag for image %s because no images matched %s"",\n                    name, args.pattern)\n      existing_index = []\n    for e in existing_index:\n      image[""versions""][e][""tags""].remove(args.tag)\n\n      logging.info(""Image %s removing tag from sha %s"", name,\n                   image[""versions""][e][""digest""])\n\n    if len(new_index) > 1:\n      raise ValueError(""Image {0} had {1} images match {2}"".format(\n          name, len(new_index, args.pattern)))\n\n    if new_index:\n      v = image[""versions""][new_index[0]]\n      logging.info(""Image %s adding tag from sha %s"", name, v[""digest""])\n      v[""tags""].append(args.tag)\n\n  with open(args.images_file, ""w"") as hf:\n    hf.write(yaml.safe_dump(config, default_flow_style=False))\n  logging.info(""Done."")\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/releasing/apply_image_tags.py,0,"b'# -*- coding: utf-8 -*-\n""""""Apply the image tags as defined in image_tags.yaml""""""\n\nimport argparse\nimport logging\nimport os\nimport re\nimport yaml\n\nfrom kubeflow.testing import util\n\n\ndef main(unparsed_args=None):  # pylint: disable=too-many-locals\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  # create the top-level parser\n  parser = argparse.ArgumentParser(description=""Apply tags to file"")\n\n  parser.add_argument(\n      ""--images_file"",\n      default=""image_tags.yaml"",\n      type=str,\n      help=""Yaml file containing the tags to attach."")\n\n  parser.add_argument(\n      ""--pattern"",\n      default="""",\n      type=str,\n      help=(""Regex pattern e.g. .*tensorflow.*notebook.*:v20180619.* ""\n            ""to select the images to apply.""))\n  args = parser.parse_args()\n\n  if not os.path.exists(args.images_file):\n    raise ValueError(""Missing image tags file: {0}"".format(args.images_file))\n\n  with open(args.images_file) as hf:\n    config = yaml.load(hf)\n\n  name_pattern, tag_pattern = args.pattern.split("":"")\n  name_re = re.compile(name_pattern)\n  tag_re = re.compile(tag_pattern)\n\n  for image in config[""images""]:\n    name = image[""name""]\n    if not name_re.match(name):\n      continue\n    for v in image[""versions""]:\n      for tag in v[""tags""]:\n        if not tag_re.match(tag):\n          continue\n        source = name + ""@"" + v[""digest""]\n        dest = name + "":"" + tag\n        util.run([\n            ""gcloud"", ""container"", ""images"", ""add-tag"", ""--quiet"", source, dest\n        ])\n\n  logging.info(""Done."")\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/releasing/sync_images.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""This script synchronizes Docker image in image_tags.yaml to your own registry\n\nExample:\npython sync_images.py --registry registry.aliyuncs.com\n\nThis would sync up the docker images for kubeflow releasing to your own docker\nregistry.\n""""""\n\nimport sys\nimport os\nimport getopt\nimport time\nimport dateutil.parser\nimport re\nimport yaml\nimport logging\nimport argparse\nimport subprocess\n\n\ndef normalize_repo(repo):\n  repo_names = repo.split(\'/\', 3)\n  if len(repo_names) == 1:\n    repo_names = [\'docker.io\', \'library\', repo_names[0]]\n  if len(repo_names) == 2:\n    repo_names = [\'docker.io\', repo_names[0], repo_names[1]]\n  if len(repo_names) == 4:\n    # gcr.io/kubeflow-images-public/katib/tfevent-metrics-collector:v0.4.0\n    # -> gcr.io/katib/tfevent-metrics-collector:v0.4.0\n    repo_names = [repo_names[0], repo_names[2], repo_names[3]]\n  return repo_names\n\n\ndef main(unparsed_args=None):  # pylint: disable=too-many-locals\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  # create the top-level parser\n  parser = argparse.ArgumentParser(\n      description=""sync up the kubeflow docker images to your own registry"")\n\n  parser.add_argument(\n      ""--images_file"",\n      default=""image_tags.yaml"",\n      type=str,\n      help=""Yaml file containing the tags to sync up."")\n\n  parser.add_argument(\n      ""--registry"",\n      default=""registry.aliyuncs.com"",\n      type=str,\n      help=(""docker registry e.g. registry.aliyuncs.com""))\n\n  args = parser.parse_args()\n\n  with open(args.images_file) as hf:\n    config = yaml.load(hf)\n\n  if not config:\n    raise ValueError(""No images could be load from %s"" % args.images_file)\n\n  # Loop over all the images and sync to your registry\n\n  for image in config[""images""]:\n    name = image[""name""]\n    for v in image[""versions""]:\n      for tag in v[""tags""]:\n        repo_names = normalize_repo(name)\n        source = name + "":"" + tag\n        registry = args.registry\n        namespace = repo_names[1]\n        newName = repo_names[2]\n        new_repo_name = registry + \'/\' + namespace + \'/\' + newName\n        dest = new_repo_name + "":"" + tag\n        logging.info(""Sync up the image %s to %s"", source, dest)\n        logging.info(""Pulling %s"", source)\n        rc = subprocess.call([""docker"", ""pull"", source])\n        if rc != 0:\n          logging.info(""Failed to Pull %s"", source)\n          continue\n        logging.info(""Tagging the image %s to %s"", source, dest)\n        rc = subprocess.call([""docker"", ""tag"", source, dest])\n        if rc != 0:\n          logging.info(""Failed to tag the image %s to %s"", source, dest)\n          continue\n        logging.info(""Push %s"", dest)\n        rc = subprocess.call([""docker"", ""push"", dest])\n        if rc != 0:\n          logging.info(""Failed to push the image %s"", dest)\n          continue\n  logging.info(""Done."")\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/scripts/update_prototype.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis script updates parameters values in jsonnet configs.\nExample\n  python update_prototype --file=all.jsonnet \\\n    --values=tfJobImage=gcr.io/kubeflow/tf-operator:v20180301\nwill change lines with a format like\n// @optionalParam tfJobImage string gcr.io/kubeflow-images-public/tf_operator:v20180226-403 The image for the TfJob controller.  # noqa: E501\nand\ntfJobImage:: ""gcr.io/kubeflow-images-public/tf_operator:v20180226-403"",\nto\n// @optionalParam tfJobImage string gcr.io/kubeflow/tf-operator:v20180301\nand\ntfJobImage:: ""gcr.io/kubeflow/tf-operator:v20180301"",\nrespectively.\n""""""\nimport argparse\nimport os\nimport re\n\n\n# TODO(jlewi): This code is now in\n# py/kubeflow/kubeflow/ci/update_jupdate_jupyter_web_app.py\ndef main():\n  parser = argparse.ArgumentParser(\n      description=""Update ksonnet prototypes parameters\' values"")\n  parser.add_argument(\n      ""--file"", action=""store"", dest=""file"", help=""Prototype file name"")\n  parser.add_argument(\n      ""--values"",\n      action=""store"",\n      dest=""values"",\n      help=""Comma separated param=value pairs. Ex.: a=b,c=1"")\n  args = parser.parse_args()\n\n  if not os.path.exists(args.file):\n    raise IOError(""File "" + args.file + "" not found!"")\n\n  regexps = {}\n  for pair in args.values.split("",""):\n    if ""="" not in pair:\n      raise Exception((""Malformed --values. Values pairs must contain =, e.g. ""\n                       ""param=value""))\n    param, value = pair.split(""="")\n    r = re.compile(r""([ \\t]*"" + param + "":+ ?\\""?)[^\\"",]+(\\""?,?)"")\n    v = r""\\g<1>"" + value + r""\\2""\n    regexps[param] = (r, v, value)\n\n  with open(args.file) as f:\n    prototype = f.read().split(""\\n"")\n  replacements = 0\n  for i, line in enumerate(prototype):\n    for param in regexps.keys():\n      if param not in line:\n        continue\n      if line.startswith(""//""):\n        prototype[i] = re.sub(\n            r""(// @\\w+ )"" + param + r""( \\w+ )[^ ]+(.*)"",  # noqa: W605\n            r""\\g<1>"" + param + r""\\2"" + regexps[param][2] + r""\\3"",\n            line)\n        replacements += 1\n        continue\n      prototype[i] = re.sub(regexps[param][0], regexps[param][1], line)\n      if line != prototype[i]:\n        replacements += 1\n  if replacements == 0:\n    raise Exception(\n        ""No replacements made, are you sure you specified correct param?"")\n  if replacements < len(regexps):\n    raise Warning(""Made less replacements then number of params. Typo?"")\n  temp_file = args.file + "".tmp""\n  with open(temp_file, ""w"") as w:\n    w.write(""\\n"".join(prototype))\n  os.rename(temp_file, args.file)\n  print(""Successfully made %d replacements"" % replacements)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/scripts/upgrade_ks_app.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport argparse\nimport logging\nimport os.path\nimport shutil\nimport subprocess\nimport yaml\n\n# The path used by the bootstrapper\nBOOTSTRAPPER_REGISTRY = ""/opt/registries/kubeflow/kubeflow""\n\n# The current release of Kubeflow. This should be upgraded on every release.\nCURRENT_RELEASE = ""github.com/kubeflow/kubeflow/tree/v0.2.0-rc.1/kubeflow""\n\n# The default name for the registry.\nDEFAULT_REGISTRY_NAME = ""kubeflow""\n\n\ndef main():\n  logging.basicConfig(\n      level=logging.INFO,\n      format=""%(levelname)s|%(asctime)s %(message)s"",\n      datefmt=""%Y-%m-%dT%H:%M:%S"",\n  )\n  logging.getLogger().setLevel(logging.INFO)\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--app_dir"",\n      default=os.getcwd(),\n      type=str,\n      help=""The directory of the ksonnet app."")\n  parser.add_argument(\n      ""--registry"",\n      default=CURRENT_RELEASE,\n      type=str,\n      help=(\n          ""The Kubeflow registry to use. This can be a GitHub link like ""\n          ""{0} that points at a specific version of the registry. ""\n          ""To specify the name of the registry in your ksonnet app ""\n          ""you can use the from <name>=<registry URL>"").format(CURRENT_RELEASE))\n\n  args = parser.parse_args()\n\n  if ""="" in args.registry:\n    registry_name, registry_url = args.registry.split(""="", 1)\n  else:\n    registry_name = DEFAULT_REGISTRY_NAME\n    registry_url = args.registry\n\n  app_dir = args.app_dir\n  logging.info(""Processing app: %s"", app_dir)\n  with open(os.path.join(app_dir, ""app.yaml""), ""r"") as f:\n    app = yaml.load(f)\n\n  registries = app[\'registries\']\n  libraries = app[\'libraries\']\n\n  for name in registries.iterkeys():\n    if name != registry_name:\n      logging.info(""Skipping registry %s"", name)\n      continue\n\n    if registries[name][""uri""].startswith(""file""):\n      # File registries are not stored in .ksonnet\n      # TODO(jlewi): This messes with bootstrapper because we might want  to\n      # switch from using the file URI to using the git location.\n      logging.info(""Skipping registry %s because it is a file URI"" % name)\n      continue\n    target = os.path.join(app_dir, "".ksonnet/registries"", name)\n    if not os.path.exists(target):\n      logging.info(""Warning directory %s doesn\'t exist; skipping"" % target)\n      continue\n    shutil.rmtree(target)\n\n  libs_to_remove = []\n  for name in libraries.iterkeys():\n    lib_registry = libraries[name][""registry""]\n    if lib_registry != registry_name:\n      continue\n    libs_to_remove.append(name)\n    target = os.path.join(app_dir, ""vendor"", lib_registry, name)\n    if not os.path.exists(target):\n      logging.info(""Directory does not exist: %s"", target)\n      continue\n    shutil.rmtree(target)\n\n  # Remove the registry from app.yaml\n  if registry_name in app[""registries""]:\n    del app[""registries""][registry_name]\n\n  # Remove libraries from this registry\n  for lib in libs_to_remove:\n    del app[""libraries""][lib]\n\n  with open(\'app.yaml\', \'w\') as f:\n    yaml.dump(app, f, default_flow_style=False)\n\n  logging.info(""Adding registry %s point at %s"", registry_name, registry_url)\n  subprocess.call([\'ks\', \'registry\', \'add\', registry_name, registry_url])\n\n  for name in libs_to_remove:\n    package = ""{0}/{1}"".format(registry_name, name)\n    logging.info(""Installing package %s"", package)\n    subprocess.call([\'ks\', \'pkg\', \'install\', package])\n\n\nif __name__ == ""__main__"":\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/__init__.py,0,"b'# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/deploy_kubeflow.py,0,"b'# -*- coding: utf-8 -*-\n""""""Deploy Kubeflow and wait for it to be deployed.\n\nTODO(jlewi): This script is outdated. Its no longer used for GKE.\nIt is still used by minikube. For minikube we should be using kfctl.sh\n""""""\nimport argparse\nimport logging\nimport os\n\nimport yaml\nfrom kubernetes.config import kube_config\n# TODO(jlewi): We should be using absolute imports always.\n# So it should be from testing import deploy_utils because testing\n# is the top level python package.\nfrom . import deploy_utils\nfrom kubeflow.testing import test_helper\nfrom kubeflow.testing import util  # pylint: disable=no-name-in-module\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--namespace"", default=None, type=str, help=(""The namespace to use.""))\n  parser.add_argument(\n      ""--as_gcloud_user"",\n      dest=""as_gcloud_user"",\n      action=""store_true"",\n      help=(""Impersonate the user corresponding to the gcloud ""\n            ""command with kubectl and ks.""))\n  parser.add_argument(\n      ""--no-as_gcloud_user"", dest=""as_gcloud_user"", action=""store_false"")\n  parser.set_defaults(as_gcloud_user=False)\n  parser.add_argument(\n      ""--github_token"",\n      default=None,\n      type=str,\n      help=(""The GitHub API token to use. This is needed since ksonnet uses the""\n            "" GitHub API and without it we get rate limited. For more info see:""\n            "" https://github.com/ksonnet/ksonnet/blob/master/docs""\n            ""/troubleshooting.md. Can also be set using environment variable ""\n            ""GITHUB_TOKEN.""))\n  parser.set_defaults(as_gcloud_user=False)\n\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef deploy_kubeflow(test_case):\n  """"""Deploy Kubeflow.""""""\n  args = parse_args()\n  test_dir = test_case.test_suite.test_dir\n  namespace = args.namespace\n  api_client = deploy_utils.create_k8s_client()\n  app_dir = deploy_utils.setup_kubeflow_ks_app(test_dir, namespace,\n                                               args.github_token, api_client)\n\n  # ks generate tf-job-operator tf-job-operator\n  # TODO(jlewi): We don\'t need to generate a core component if we are\n  # just deploying TFServing. Might be better to refactor this code.\n  # Deploy Kubeflow\n  util.run([\n      ""ks"",\n      ""generate"",\n      ""tf-job-operator"",\n      ""tf-job-operator"",\n  ],\n           cwd=app_dir)\n\n  util.run([\n      ""ks"",\n      ""generate"",\n      ""pytorch-operator"",\n      ""pytorch-operator"",\n  ],\n           cwd=app_dir)\n\n  util.run([\n      ""ks"",\n      ""generate"",\n      ""jupyter"",\n      ""jupyter"",\n  ], cwd=app_dir)\n\n  util.run([\n      ""ks"",\n      ""generate"",\n      ""katib"",\n      ""katib"",\n  ], cwd=app_dir)\n\n  apply_command = [\n      ""ks"",\n      ""apply"",\n      ""default"",\n      ""-c"",\n      ""tf-job-operator"",\n      ""-c"",\n      ""pytorch-operator"",\n      ""-c"",\n      ""jupyter"",\n      ""-c"",\n      ""katib"",\n  ]\n\n  if args.as_gcloud_user:\n    account = deploy_utils.get_gcp_identity()\n    logging.info(""Impersonate %s"", account)\n\n    # If we don\'t use --as to impersonate the service account then we\n    # observe RBAC errors when doing certain operations. The problem appears\n    # to be that we end up using the in cluster config (e.g. pod service account)\n    # and not the GCP service account which has more privileges.\n    apply_command.append(""--as="" + account)\n  util.run(apply_command, cwd=app_dir)\n\n  # Verify that Jupyter is actually deployed.\n  jupyter_name = ""jupyter""\n  logging.info(""Verifying TfHub started."")\n  util.wait_for_statefulset(api_client, namespace, jupyter_name)\n\n  # Verify that core components are actually deployed.\n  deployment_names = [\n      ""tf-job-operator"", ""pytorch-operator"", ""studyjob-controller""\n  ]\n  for deployment_name in deployment_names:\n    logging.info(""Verifying that %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, namespace, deployment_name)\n\n\ndef main():\n  test_case = test_helper.TestCase(\n      name=\'deploy_kubeflow\', test_func=deploy_kubeflow)\n  test_suite = test_helper.init(name=\'deploy_kubeflow\', test_cases=[test_case])\n  test_suite.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/deploy_utils.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport datetime\nimport json\nimport logging\nimport os\nimport shutil\nimport ssl\nimport tempfile\nimport time\nimport uuid\n\nimport requests\nimport yaml\nfrom googleapiclient import discovery, errors\nfrom kubernetes import client as k8s_client\nfrom kubernetes.client import rest\nfrom kubernetes.config import kube_config\nfrom oauth2client.client import GoogleCredentials\n\nfrom kubeflow.testing import test_util, util  # pylint: disable=no-name-in-module  # noqa: E501\nfrom testing import vm_util\n\n\ndef get_gcp_identity():\n  identity = util.run([""gcloud"", ""config"", ""get-value"", ""account""])\n  logging.info(""Current GCP account: %s"", identity)\n  return identity\n\n\ndef create_k8s_client():\n  # We need to load the kube config so that we can have credentials to\n  # talk to the APIServer.\n  util.load_kube_config(persist_config=False)\n\n  # Create an API client object to talk to the K8s master.\n  api_client = k8s_client.ApiClient()\n\n  return api_client\n\n\ndef _setup_test(api_client, run_label):\n  """"""Create the namespace for the test.\n\n  Returns:\n    test_dir: The local test directory.\n  """"""\n\n  api = k8s_client.CoreV1Api(api_client)\n  namespace = k8s_client.V1Namespace()\n  namespace.api_version = ""v1""\n  namespace.kind = ""Namespace""\n  namespace.metadata = k8s_client.V1ObjectMeta(\n      name=run_label, labels={\n          ""app"": ""kubeflow-e2e-test"",\n      })\n\n  try:\n    logging.info(""Creating namespace %s"", namespace.metadata.name)\n    namespace = api.create_namespace(namespace)\n    logging.info(""Namespace %s created."", namespace.metadata.name)\n  except rest.ApiException as e:\n    if e.status == 409:\n      logging.info(""Namespace %s already exists."", namespace.metadata.name)\n    else:\n      raise\n\n  return namespace\n\n\ndef setup_kubeflow_ks_app(dir, namespace, github_token, api_client):\n  """"""Create a ksonnet app for Kubeflow""""""\n  util.makedirs(dir)\n\n  logging.info(""Using test directory: %s"", dir)\n\n  namespace_name = namespace\n\n  namespace = _setup_test(api_client, namespace_name)\n  logging.info(""Using namespace: %s"", namespace)\n  if github_token:\n    logging.info(""Setting GITHUB_TOKEN to %s."", github_token)\n    # Set a GITHUB_TOKEN so that we don\'t rate limited by GitHub;\n    # see: https://github.com/ksonnet/ksonnet/issues/233\n    os.environ[""GITHUB_TOKEN""] = github_token\n\n  if not os.getenv(""GITHUB_TOKEN""):\n    logging.warning(""GITHUB_TOKEN not set; you will probably hit Github API ""\n                    ""limits."")\n  # Initialize a ksonnet app.\n  app_name = ""kubeflow-test-"" + uuid.uuid4().hex[0:4]\n  util.run([\n      ""ks"",\n      ""init"",\n      app_name,\n  ], cwd=dir)\n\n  app_dir = os.path.join(dir, app_name)\n\n  # Set the default namespace.\n  util.run([""ks"", ""env"", ""set"", ""default"", ""--namespace="" + namespace_name],\n           cwd=app_dir)\n\n  kubeflow_registry = ""github.com/kubeflow/kubeflow/tree/master/kubeflow""\n  util.run([""ks"", ""registry"", ""add"", ""kubeflow"", kubeflow_registry],\n           cwd=app_dir)\n\n  # Install required packages\n  packages = [\n      ""kubeflow/common"", ""kubeflow/gcp"", ""kubeflow/jupyter"",\n      ""kubeflow/tf-serving"", ""kubeflow/tf-job"", ""kubeflow/tf-training"",\n      ""kubeflow/pytorch-job"", ""kubeflow/argo"", ""kubeflow/katib""\n  ]\n\n  # Instead of installing packages we edit the app.yaml file directly for p in\n  # packages:\n  # util.run([""ks"", ""pkg"", ""install"", p], cwd=app_dir)\n  app_file = os.path.join(app_dir, ""app.yaml"")\n  with open(app_file) as f:\n    app_yaml = yaml.load(f)\n\n  libraries = {}\n  for pkg in packages:\n    pkg = pkg.split(""/"")[1]\n    libraries[pkg] = {\n        \'gitVersion\': {\n            \'commitSha\': \'fake\',\n            \'refSpec\': \'fake\'\n        },\n        \'name\': pkg,\n        \'registry\': ""kubeflow""\n    }\n  app_yaml[\'libraries\'] = libraries\n\n  with open(app_file, ""w"") as f:\n    yaml.dump(app_yaml, f)\n\n  # Create vendor directory with a symlink to the src so that we use the code\n  # at the desired commit.\n  target_dir = os.path.join(app_dir, ""vendor"", ""kubeflow"")\n\n  REPO_ORG = ""kubeflow""\n  REPO_NAME = ""kubeflow""\n  REGISTRY_PATH = ""kubeflow""\n  source = os.path.join(dir, ""src"", REPO_ORG, REPO_NAME, REGISTRY_PATH)\n  logging.info(""Creating link %s -> %s"", target_dir, source)\n  os.symlink(source, target_dir)\n\n  return app_dir\n\n\ndef log_operation_status(operation):\n  """"""A callback to use with wait_for_operation.""""""\n  name = operation.get(""name"", """")\n  status = operation.get(""status"", """")\n  logging.info(""Operation %s status %s"", name, status)\n\n\ndef wait_for_operation(client,\n                       project,\n                       op_id,\n                       timeout=datetime.timedelta(hours=1),\n                       polling_interval=datetime.timedelta(seconds=5),\n                       status_callback=log_operation_status):\n  """"""Wait for the specified operation to complete.\n\n  Args:\n    client: Client for the API that owns the operation.\n    project: project\n    op_id: Operation id.\n    timeout: A datetime.timedelta expressing the amount of time to wait before\n      giving up.\n    polling_interval: A datetime.timedelta to represent the amount of time to\n      wait between requests polling for the operation status.\n\n  Returns:\n    op: The final operation.\n\n  Raises:\n    TimeoutError: if we timeout waiting for the operation to complete.\n  """"""\n  endtime = datetime.datetime.now() + timeout\n  while True:\n    try:\n      op = client.operations().get(project=project, operation=op_id).execute()\n\n      if status_callback:\n        status_callback(op)\n\n      status = op.get(""status"", """")\n      # Need to handle other status\'s\n      if status == ""DONE"":\n        return op\n    except ssl.SSLError as e:\n      logging.error(""Ignoring error %s"", e)\n    if datetime.datetime.now() > endtime:\n      raise TimeoutError(\n          ""Timed out waiting for op: {0} to complete."".format(op_id))\n    time.sleep(polling_interval.total_seconds())\n\n  # Linter complains if we don\'t have a return here even though its unreachable\n  return None\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/gcp_util.py,0,"b'import argparse\nimport base64\nimport datetime\nimport logging\nimport os\nimport errno\nimport shutil\nimport subprocess\nimport tempfile\nimport threading\nfrom functools import partial\nfrom multiprocessing import Process\nfrom time import sleep\nfrom google.auth.transport.requests import Request\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\n\nimport requests\nimport yaml\nimport google.auth\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nfrom retrying import retry\n\nIAM_SCOPE = ""https://www.googleapis.com/auth/iam""\nOAUTH_TOKEN_URI = ""https://www.googleapis.com/oauth2/v4/token""\n\ndef get_service_account_credentials(client_id_key):\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(credentials, google.oauth2.credentials.Credentials):\n    raise Exception(""make_iap_request is only supported for service ""\n                    ""accounts."")\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  credentials.refresh(Request())\n\n  signer_email = credentials.service_account_email\n  if isinstance(credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    signer = google.auth.iam.Signer(Request(), credentials, signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer\n  # and email acquired from the bootstrap credentials.\n  return google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={""target_audience"": may_get_env_var(client_id_key)})\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      ""assertion"": service_account_jwt,\n      ""grant_type"": google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[""id_token""]\n\ndef may_get_env_var(name):\n  env_val = os.getenv(name)\n  if env_val:\n    logging.info(""%s is set"" % name)\n    return env_val\n  else:\n    raise Exception(""%s not set"" % name)\n\ndef endpoint_is_ready(url, wait_min=15):\n  """"""\n  Checks if the kubeflow endpoint is ready.\n\n  Args:\n    url: The url endpoint\n  Returns:\n    True if the url is ready\n  """"""\n  service_account_credentials = get_service_account_credentials(""CLIENT_ID"")\n\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  # Wait up to 30 minutes for IAP access test.\n  num_req = 0\n  end_time = datetime.datetime.now() + datetime.timedelta(\n      minutes=wait_min)\n  while datetime.datetime.now() < end_time:\n    sleep(10)\n    num_req += 1\n    logging.info(""Trying url: %s"", url)\n    try:\n      resp = requests.request(\n          ""GET"",\n          url,\n          headers={\n              ""Authorization"":\n              ""Bearer {}"".format(google_open_id_connect_token)\n          },\n          verify=False)\n      logging.info(resp.text)\n      if resp.status_code == 200:\n        logging.info(""IAP is ready for %s!"", url)\n        return True\n      else:\n        logging.info(\n            ""%s: IAP not ready, request number: %s"" % (url, num_req))\n    except Exception as e:\n      logging.info(""%s: IAP not ready, exception caught %s, request number: %s"" %\n                   (url, str(e), num_req))\n  return False\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/get_gke_credentials.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport logging\nimport os\nimport yaml\nfrom kubeflow.testing import test_helper, util\nfrom kubernetes.config import kube_config\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--cluster"",\n      default=None,\n      type=str,\n      help=(\n          ""The name of the cluster. If not set assumes the script is running in""\n          "" a cluster and uses that cluster.""))\n  parser.add_argument(\n      ""--zone"",\n      default=""us-east1-d"",\n      type=str,\n      help=""The zone for the cluster."")\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=""The project to use."")\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef get_gke_credentials(test_case):\n  """"""Configure kubeconfig to talk to the supplied GKE cluster.""""""\n  args = parse_args()\n  util.maybe_activate_service_account()\n  config_file = os.path.expanduser(kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n  logging.info(""Using Kubernetes config file: %s"", config_file)\n  project = args.project\n  cluster_name = args.cluster\n  zone = args.zone\n  logging.info(""Using cluster: %s in project: %s in zone: %s"", cluster_name,\n               project, zone)\n  # Print out config to help debug issues with accounts and\n  # credentials.\n  util.run([""gcloud"", ""config"", ""list""])\n  util.configure_kubectl(project, zone, cluster_name)\n\n  # We want to modify the KUBECONFIG file to remove the gcloud commands\n  # for any users that are authenticating using service accounts.\n  # This will allow the script to be truly headless and not require gcloud.\n  # More importantly, kubectl will properly attach auth.info scope so that\n  # RBAC rules can be applied to the email and not the id.\n  # See https://github.com/kubernetes/kubernetes/pull/58141\n  #\n  # TODO(jlewi): We might want to check GOOGLE_APPLICATION_CREDENTIALS\n  # to see whether we are actually using a service account. If we aren\'t\n  # using a service account then we might not want to delete the gcloud\n  # commands.\n  logging.info(""Modifying kubeconfig %s"", config_file)\n  with open(config_file, ""r"") as hf:\n    config = yaml.load(hf)\n\n  for user in config[""users""]:\n    auth_provider = user.get(""user"", {}).get(""auth-provider"", {})\n    if auth_provider.get(""name"") != ""gcp"":\n      continue\n    logging.info(""Modifying user %s which has gcp auth provider"", user[""name""])\n    if ""config"" in auth_provider:\n      logging.info(""Deleting config from user %s"", user[""name""])\n      del auth_provider[""config""]\n\n      # This is a hack because the python client library will complain\n      # about an invalid config if there is no config field.\n      #\n      # It looks like the code checks here but that doesn\'t seem to work\n      # https://github.com/kubernetes-client/python-base/blob/master/config/kube_config.py#L209\n      auth_provider[""config""] = {\n          ""dummy"": ""dummy"",\n      }\n  logging.info(""Writing update kubeconfig:\\n %s"", yaml.dump(config))\n  with open(config_file, ""w"") as hf:\n    yaml.dump(config, hf)\n\n\ndef main():\n  test_case = test_helper.TestCase(\n      name=\'get_gke_credentials\', test_func=get_gke_credentials)\n  test_suite = test_helper.init(\n      name=\'get_gke_credentials\', test_cases=[test_case])\n  test_suite.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/katib_studyjob_test.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nLaunch a simple katib studyjob and verify that it runs.\n\nTODO(ricliu): This code shares a lot in common with the e2etest for tf-operator.\nConsider merging the common code into a CRD library. There are only some minor\ndifferences - for example TFJob Status has a list of job conditions, whereas\nKatib Studyjob status only shows the most recent condition.\n""""""\n\nimport argparse\nimport datetime\nimport logging\nimport multiprocessing\nimport os\nimport re\nimport subprocess\nimport time\n\nfrom kubernetes import client as k8s_client\nfrom kubeflow.testing import test_helper, util\nfrom retrying import retry\n\nNAMESPACE = ""default""\nSTUDY_JOB_GROUP = ""kubeflow.org""\nSTUDY_JOB_PLURAL = ""studyjobs""\nSTUDY_JOB_KIND = ""StudyJob""\nTIMEOUT = 120\n\n\n# TODO: TimeoutError is a built in exception in python3 so we can\n# delete this when we go to Python3.\nclass TimeoutError(Exception):  # pylint: disable=redefined-builtin\n  """"""An error indicating an operation timed out.""""""\n\n\nclass JobTimeoutError(TimeoutError):\n  """"""An error indicating the job timed out.\n    The job spec/status can be found in .job.\n  """"""\n\n  def __init__(self, message, job):\n    super(JobTimeoutError, self).__init__(message)\n    self.job = job\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--src_dir"", default="""", type=str, help=""The kubeflow src directory"")\n  parser.add_argument(\n      ""--studyjob_version"",\n      default=""v1alpha1"",\n      type=str,\n      help=""Which katib study job version to use"")\n  args, _ = parser.parse_known_args()\n  return args\n\n\n@retry(stop_max_attempt_number=3)\ndef create_app_and_job(args, namespace, name):\n  try:\n    util.run([\n        ""ks"", ""init"", ""katib-app"", ""--skip-default-registries"",\n        ""--namespace="" + namespace\n    ])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the app already exists. This is a sign the a previous\n    # attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  os.chdir(""katib-app"")\n  try:\n    util.run([""ks"", ""registry"", ""add"", ""kubeflow"", args.src_dir + ""/kubeflow""])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the registry has already been added.\n    # This is a sign the a previous attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  try:\n    util.run([""ks"", ""pkg"", ""install"", ""kubeflow/examples""])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the package has already been added.\n    # This is a sign the a previous attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  if args.studyjob_version == ""v1alpha1"":\n    prototype_name = ""katib-studyjob-test-v1alpha1""\n  else:\n    raise ValueError(\n        ""Unrecognized value for studyjob_version: %s"" % args.studyjob_version)\n\n  util.run([""ks"", ""generate"", prototype_name, name])\n  util.run([""ks"", ""apply"", ""default"", ""-c"", ""katib-studyjob-test""])\n\n\n@retry(wait_fixed=10000, stop_max_attempt_number=20)\ndef log_status(study_job):\n  """"""A callback to use with wait_for_job.""""""\n  condition = study_job.get(""status"", {}).get(""condition"")\n  logging.info(""Job %s in namespace %s; uid=%s; condition=%s"",\n               study_job.get(""metadata"", {}).get(""name""),\n               study_job.get(""metadata"", {}).get(""namespace""),\n               study_job.get(""metadata"", {}).get(""uid""), condition)\n\n\n# This is a modification of\n# https://github.com/kubeflow/tf-operator/blob/master/py/tf_job_client.py#L119.\n# pylint: disable=too-many-arguments\ndef wait_for_condition(client,\n                       namespace,\n                       name,\n                       expected_condition,\n                       version=""v1alpha1"",\n                       timeout=datetime.timedelta(minutes=10),\n                       polling_interval=datetime.timedelta(seconds=30),\n                       status_callback=None):\n  """"""Waits until any of the specified conditions occur.\n  Args:\n    client: K8s api client.\n    namespace: namespace for the job.\n    name: Name of the job.\n    expected_condition: A list of conditions. Function waits until any of the\n      supplied conditions is reached.\n    timeout: How long to wait for the job.\n    polling_interval: How often to poll for the status of the job.\n    status_callback: (Optional): Callable. If supplied this callable is\n      invoked after we poll the job. Callable takes a single argument which is\n      the job.\n  """"""\n  crd_api = k8s_client.CustomObjectsApi(client)\n  end_time = datetime.datetime.now() + timeout\n  while True:\n    # By setting async_req=True ApiClient returns multiprocessing.pool.AsyncResult\n    # If we don\'t set async_req=True then it could potentially block\n    # forever.\n    thread = crd_api.get_namespaced_custom_object(\n        STUDY_JOB_GROUP,\n        version,\n        namespace,\n        STUDY_JOB_PLURAL,\n        name,\n        async_req=True)\n\n    # Try to get the result but timeout.\n    results = None\n    try:\n      results = thread.get(TIMEOUT)\n    except multiprocessing.TimeoutError:\n      logging.error(""Timeout trying to get StudyJob."")\n    except Exception as e:\n      logging.error(\n          ""There was a problem waiting for StudyJob %s.%s; Exception; %s"", name,\n          name, e)\n      raise\n\n    if results:\n      if status_callback:\n        status_callback(results)\n\n      condition = results.get(""status"", {}).get(""condition"")\n      if condition in expected_condition:\n        return results\n\n    if datetime.datetime.now() + polling_interval > end_time:\n      raise JobTimeoutError(\n          ""Timeout waiting for job {0} in namespace {1} to enter one of the ""\n          ""conditions {2}."".format(name, namespace, expected_condition),\n          results)\n\n    time.sleep(polling_interval.seconds)\n\n  # Linter complains if we don\'t have a return statement even though\n  # this code is unreachable.\n  return None\n\n\ndef test_katib(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n  namespace = NAMESPACE\n  name = ""katib-studyjob-test""\n\n  util.load_kube_config()\n  api_client = k8s_client.ApiClient()\n  create_app_and_job(args, namespace, name)\n  try:\n    wait_for_condition(\n        api_client, namespace, name, [""Running""], status_callback=log_status)\n    logging.info(""StudyJob launched successfully"")\n  except Exception as e:\n    logging.error(""Test failed waiting for job; %s"", e)\n    test_case.add_failure_info(e.message)\n\n\nif __name__ == ""__main__"":\n  test_case = test_helper.TestCase(name=""test_katib"", test_func=test_katib)\n  test_suite = test_helper.init(name=""test_katib"", test_cases=[test_case])\n  test_suite.run()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/run_with_retry.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nrun_with_retry runs the given binary with the given number of retries\n\nThis is intended primary for retrying bash scripts. Ideally, we sould use\nargo\'s retryStrategy, but there is a bug in it\'s implementation:\nhttps://github.com/argoproj/argo/issues/885\n\nExample:\n  python run_with_retry --retries=5 -- bash my_flaky_script.sh\n\nThis runs bash my_flaky_script.sh upto 5 times till it succeeds\n""""""\nimport argparse\nfrom kubeflow.testing import test_helper, util\nfrom retrying import retry\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--retries"", required=True, type=int, help=""The number of retries."")\n\n  parser.add_argument(\'remaining_args\', nargs=argparse.REMAINDER)\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef run_with_retry(_):\n  """"""Deploy Kubeflow.""""""\n  args = parse_args()\n\n  @retry(stop_max_attempt_number=args.retries)\n  def run():\n    util.run(args.remaining_args[1:])\n\n  run()\n\n\ndef main():\n  test_case = test_helper.TestCase(\n      name=\'run_with_retry\', test_func=run_with_retry)\n  test_suite = test_helper.init(name=\'run_with_retry\', test_cases=[test_case])\n  test_suite.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/test_deploy.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Test deploying Kubeflow.\n\nRequirements:\n  This project assumes the py directory in github.com/kubeflow/tf-operator corresponds\n  to a top level Python package on the Python path.\n\n  TODO(jlewi): Come up with a better story for how we reuse the py package\n  in kubeflow/tf-operator. We should probably turn that into a legit Python pip\n  package that is built and released as part of the kubeflow/tf-operator project.\n""""""\n\nimport argparse\nimport datetime\nimport json\nimport logging\nimport os\nimport errno\nimport re\nimport shutil\nimport subprocess\nimport tempfile\nimport time\nimport uuid\n\nimport requests\nimport yaml\nfrom googleapiclient import discovery, errors\nfrom kubernetes import client as k8s_client\nfrom kubernetes.client import rest\nfrom kubernetes.config import kube_config\nfrom oauth2client.client import GoogleCredentials\n\nfrom kubeflow.testing import test_util, util  # pylint: disable=no-name-in-module\nfrom testing import vm_util\n\n# The ksonnet binary\nks = ""ks-13""\n\ndef _setup_test(api_client, run_label):\n  """"""Create the namespace for the test.\n\n  Returns:\n    test_dir: The local test directory.\n  """"""\n\n  api = k8s_client.CoreV1Api(api_client)\n  namespace = k8s_client.V1Namespace()\n  namespace.api_version = ""v1""\n  namespace.kind = ""Namespace""\n  namespace.metadata = k8s_client.V1ObjectMeta(\n      name=run_label, labels={\n          ""app"": ""kubeflow-e2e-test"",\n      })\n\n  try:\n    logging.info(""Creating namespace %s"", namespace.metadata.name)\n    namespace = api.create_namespace(namespace)\n    logging.info(""Namespace %s created."", namespace.metadata.name)\n  except rest.ApiException as e:\n    if e.status == 409:\n      logging.info(""Namespace %s already exists."", namespace.metadata.name)\n    else:\n      raise\n\n  return namespace\n\n\ndef create_k8s_client(_):\n  # We need to load the kube config so that we can have credentials to\n  # talk to the APIServer.\n  util.load_kube_config(persist_config=False)\n\n  # Create an API client object to talk to the K8s master.\n  api_client = k8s_client.ApiClient()\n\n  return api_client\n\n\n# TODO(jlewi): We should make this a reusable function in kubeflow/testing\n# because we will probably want to use it in other places as well.\ndef setup_kubeflow_ks_app(args, api_client):\n  """"""Create a ksonnet app for Kubeflow""""""\n  try:\n    os.makedirs(args.test_dir)\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(args.test_dir):\n      pass\n    else:\n      raise\n\n  logging.info(""Using test directory: %s"", args.test_dir)\n\n  namespace_name = args.namespace\n\n  namespace = _setup_test(api_client, namespace_name)\n  logging.info(""Using namespace: %s"", namespace)\n  if args.github_token:\n    logging.info(""Setting GITHUB_TOKEN to %s."", args.github_token)\n    # Set a GITHUB_TOKEN so that we don\'t rate limited by GitHub;\n    # see: https://github.com/ksonnet/ksonnet/issues/233\n    os.environ[""GITHUB_TOKEN""] = args.github_token\n\n  if not os.getenv(""GITHUB_TOKEN""):\n    logging.warning(""GITHUB_TOKEN not set; you will probably hit Github API ""\n                    ""limits."")\n  # Initialize a ksonnet app.\n  app_name = ""kubeflow-test-"" + uuid.uuid4().hex[0:4]\n  util.run([\n      ks,\n      ""init"",\n      app_name,\n  ], cwd=args.test_dir)\n\n  app_dir = os.path.join(args.test_dir, app_name)\n\n  kubeflow_registry = ""github.com/kubeflow/kubeflow/tree/master/kubeflow""\n  util.run([ks, ""registry"", ""add"", ""kubeflow"", kubeflow_registry],\n           cwd=app_dir)\n\n  # Install required packages\n  packages = [\n      ""kubeflow/common"", ""kubeflow/tf-serving"", ""kubeflow/tf-training"",\n      ""kubeflow/pytorch-job"", ""kubeflow/argo""\n  ]\n\n  # Instead of installing packages we edit the app.yaml file directly\n  #for p in packages:\n  # util.run([ks, ""pkg"", ""install"", p], cwd=app_dir)\n  app_file = os.path.join(app_dir, ""app.yaml"")\n  with open(app_file) as f:\n    app_yaml = yaml.load(f)\n\n  libraries = {}\n  for pkg in packages:\n    pkg = pkg.split(""/"")[1]\n    libraries[pkg] = {\n        \'gitVersion\': {\n            \'commitSha\': \'fake\',\n            \'refSpec\': \'fake\'\n        },\n        \'name\': pkg,\n        \'registry\': ""kubeflow""\n    }\n  app_yaml[\'libraries\'] = libraries\n\n  with open(app_file, ""w"") as f:\n    yaml.dump(app_yaml, f)\n\n  # Create vendor directory with a symlink to the src\n  # so that we use the code at the desired commit.\n  target_dir = os.path.join(app_dir, ""vendor"", ""kubeflow"")\n\n  REPO_ORG = ""kubeflow""\n  REPO_NAME = ""kubeflow""\n  REGISTRY_PATH = ""kubeflow""\n  source = os.path.join(args.test_dir, ""src"", REPO_ORG, REPO_NAME,\n                        REGISTRY_PATH)\n  logging.info(""Creating link %s -> %s"", target_dir, source)\n  os.symlink(source, target_dir)\n\n  return app_dir\n\n\ndef deploy_model(args):\n  """"""Deploy a TF model using the TF serving component.""""""\n  api_client = create_k8s_client(args)\n  app_dir = setup_kubeflow_ks_app(args, api_client)\n\n  logging.info(""Deploying tf-serving."")\n  params = {}\n  for pair in args.params.split("",""):\n    k, v = pair.split(""="", 1)\n    if k != ""namespace"":\n      params[k] = v\n    else:\n      namespace = v\n\n  if namespace == None:\n    raise ValueError(""namespace must be supplied in args."")\n\n  # deployment component\n  deployComponent = ""modelServer""\n  generate_command = [\n      ks, ""generate"", ""tf-serving-deployment-gcp"", deployComponent\n  ]\n  util.run(generate_command, cwd=app_dir)\n  ks_deploy(\n      app_dir,\n      deployComponent,\n      params,\n      env=None,\n      account=None,\n      namespace=namespace)\n\n  # service component\n  serviceComponent = ""modelServer-service""\n  generate_command = [ks, ""generate"", ""tf-serving-service"", serviceComponent]\n  util.run(generate_command, cwd=app_dir)\n  ks_deploy(\n      app_dir,\n      serviceComponent,\n      params,\n      env=None,\n      account=None,\n      namespace=namespace)\n\n  core_api = k8s_client.CoreV1Api(api_client)\n  deploy = core_api.read_namespaced_service(args.deploy_name, args.namespace)\n  cluster_ip = deploy.spec.cluster_ip\n\n  if not cluster_ip:\n    raise ValueError(""inception service wasn\'t assigned a cluster ip."")\n  util.wait_for_deployment(\n      api_client, namespace, args.deploy_name, timeout_minutes=10)\n  logging.info(""Verified TF serving started."")\n\n\ndef test_successful_deployment(deployment_name):\n  """""" Tests if deployment_name is successfully running using kubectl """"""\n  # TODO use the python kubernetes library to get deployment status\n  # This is using kubectl right now\n  retries = 20\n  i = 0\n  while True:\n    if i == retries:\n      raise Exception(\'Deployment failed: \' + deployment_name)\n    try:\n      output = util.run([""kubectl"", ""get"", ""deployment"", deployment_name])\n      logging.info(""output = \\n"" + output)\n      if output.count(\'\\n\') == 1:\n        output = output.split(\'\\n\')[1]\n        output = re.split(\' +\', output)\n        desired_pods = output[1]\n        current_pods = output[2]\n        uptodate_pods = output[3]\n        available_pods = output[4]\n        logging.info(""desired_pods "" + desired_pods)\n        logging.info(""current_pods "" + current_pods)\n        logging.info(""uptodate_pods "" + uptodate_pods)\n        logging.info(""available_pods "" + available_pods)\n        if desired_pods == current_pods and \\\n           desired_pods == uptodate_pods and \\\n           desired_pods == available_pods:\n          return True\n    except subprocess.CalledProcessError as e:\n      logging.error(e)\n    logging.info(""Sleeping 5 seconds and retrying.."")\n    time.sleep(5)\n    i += 1\n\n\ndef test_katib(args):\n  test_successful_deployment(\'katib-manager\')\n  test_successful_deployment(\'katib-manager-rest\')\n  test_successful_deployment(\'katib-ui\')\n  test_successful_deployment(\'katib-db\')\n  test_successful_deployment(\'katib-suggestion-grid\')\n  test_successful_deployment(\'katib-suggestion-random\')\n  test_successful_deployment(\'katib-suggestion-bayesianoptimization\')\n  test_successful_deployment(\'katib-suggestion-hyperband\')\n  test_successful_deployment(\'katib-controller\')\n\n\ndef deploy_argo(args):\n  api_client = create_k8s_client(args)\n  app_dir = setup_kubeflow_ks_app(args, api_client)\n\n  component = ""argo""\n  logging.info(""Deploying argo"")\n  generate_command = [ks, ""generate"", ""argo"", component, ""--name"", ""argo""]\n  util.run(generate_command, cwd=app_dir)\n\n  ks_deploy(app_dir, component, {}, env=None, account=None, namespace=None)\n\n  # Create a hello world workflow\n  util.run([\n      ""kubectl"", ""create"", ""-n"", ""default"", ""-f"",\n      ""https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml""\n  ],\n           cwd=app_dir)\n\n  # Wait for 200 seconds to check if the hello-world pod was created\n  retries = 20\n  i = 0\n  while True:\n    if i == retries:\n      raise Exception(\'Failed to run argo workflow\')\n    output = util.run([\n        ""kubectl"", ""get"", ""pods"", ""-n"", ""default"",\n        ""-lworkflows.argoproj.io/workflow""\n    ])\n    if ""hello-world-"" in output:\n      return True\n    time.sleep(10)\n    i += 1\n\n\ndef deploy_pytorchjob(args):\n  """"""Deploy Pytorchjob using the pytorch-job component""""""\n  api_client = create_k8s_client(args)\n  app_dir = setup_kubeflow_ks_app(args, api_client)\n\n  component = ""example-job""\n  logging.info(""Deploying pytorch."")\n  generate_command = [ks, ""generate"", ""pytorch-job"", component]\n\n  util.run(generate_command, cwd=app_dir)\n\n  params = {}\n  for pair in args.params.split("",""):\n    k, v = pair.split(""="", 1)\n    params[k] = v\n\n  ks_deploy(app_dir, component, params, env=None, account=None, namespace=None)\n\n\ndef teardown(args):\n  # Delete the namespace\n  logging.info(""Deleting namespace %s"", args.namespace)\n  api_client = create_k8s_client(args)\n  core_api = k8s_client.CoreV1Api(api_client)\n  core_api.delete_namespace(args.namespace, {})\n\n\ndef determine_test_name(args):\n  if args.deploy_name:\n    return args.func.__name__ + ""-"" + args.deploy_name\n  return args.func.__name__\n\n\n# TODO(jlewi): We should probably make this a generic function in\n# kubeflow.testing.`\ndef wrap_test(args):\n  """"""Run the tests given by args.func and output artifacts as necessary.\n  """"""\n  test_name = determine_test_name(args)\n  test_case = test_util.TestCase()\n  test_case.class_name = ""KubeFlow""\n  test_case.name = args.workflow_name + ""-"" + test_name\n  try:\n\n    def run():\n      args.func(args)\n\n    test_util.wrap_test(run, test_case)\n  finally:\n    # Test grid has problems with underscores in the name.\n    # https://github.com/kubeflow/kubeflow/issues/631\n    # TestGrid currently uses the regex junit_(^_)*.xml so we only\n    # want one underscore after junit.\n    junit_name = test_case.name.replace(""_"", ""-"")\n    junit_path = os.path.join(args.artifacts_dir,\n                              ""junit_{0}.xml"".format(junit_name))\n    logging.info(""Writing test results to %s"", junit_path)\n    test_util.create_junit_xml_file([test_case], junit_path)\n\n\n# TODO(jlewi): We should probably make this a reusable function since a\n# lot of test code code use it.\ndef ks_deploy(app_dir,\n              component,\n              params,\n              env=None,\n              account=None,\n              namespace=None):\n  """"""Deploy the specified ksonnet component.\n  Args:\n    app_dir: The ksonnet directory\n    component: Name of the component to deployed\n    params: A dictionary of parameters to set; can be empty but should not be\n      None.\n    env: (Optional) The environment to use, if none is specified a new one\n      is created.\n    account: (Optional) The account to use.\n    namespace: (Optional) The namespace to use when adding the environment\n  Raises:\n    ValueError: If input arguments aren\'t valid.\n  """"""\n  if not component:\n    raise ValueError(""component can\'t be None."")\n\n  # TODO(jlewi): It might be better if the test creates the app and uses\n  # the latest stable release of the ksonnet configs. That however will cause\n  # problems when we make changes to the TFJob operator that require changes\n  # to the ksonnet configs. One advantage of checking in the app is that\n  # we can modify the files in vendor if needed so that changes to the code\n  # and config can be submitted in the same pr.\n  now = datetime.datetime.now()\n  if not env:\n    env = ""e2e-"" + now.strftime(""%m%d-%H%M-"") + uuid.uuid4().hex[0:4]\n\n  logging.info(""Using app directory: %s"", app_dir)\n\n  if not namespace:\n    util.run([ks, ""env"", ""add"", env], cwd=app_dir)\n  else:\n    util.run([ks, ""env"", ""add"", env, ""--namespace="" + namespace], cwd=app_dir)\n\n  for k, v in params.iteritems():\n    util.run([ks, ""param"", ""set"", ""--env="" + env, component, k, v],\n             cwd=app_dir)\n\n  apply_command = [ks, ""apply"", env, ""-c"", component]\n  if account:\n    apply_command.append(""--as="" + account)\n  util.run(apply_command, cwd=app_dir)\n\n\ndef modify_minikube_config(config_path, certs_dir):\n  """"""Modify the kube config file used with minikube.\n\n  This function changes the location of the certificates to certs_dir.\n\n  Args:\n    config_path: The path of the Kubernetes config file.\n    certs_dir: The directory where the certs to use with minikube are stored.\n  """"""\n  with open(config_path, ""r"") as hf:\n    config = yaml.load(hf)\n\n  for cluster in config[""clusters""]:\n    authority = cluster[""cluster""][""certificate-authority""]\n    authority = os.path.join(certs_dir, os.path.basename(authority))\n    cluster[""cluster""][""certificate-authority""] = authority\n\n    for user in config[""users""]:\n      for k in [""client-certificate"", ""client-key""]:\n        user[""user""][k] = os.path.join(certs_dir,\n                                       os.path.basename(user[""user""][k]))\n\n  logging.info(""Updating path of certificates in %s"", config_path)\n  with open(config_path, ""w"") as hf:\n    yaml.dump(config, hf)\n\n\ndef deploy_minikube(args):\n  """"""Create a VM and setup minikube.""""""\n\n  credentials = GoogleCredentials.get_application_default()\n  gce = discovery.build(\n      ""compute"", ""v1"", credentials=credentials, cache_discovery=False)\n  instances = gce.instances()\n  body = {\n      ""name"":\n      args.vm_name,\n      ""machineType"":\n      ""zones/{0}/machineTypes/n1-standard-16"".format(args.zone),\n      ""disks"": [\n          {\n              ""boot"": True,\n              ""initializeParams"": {\n                  ""sourceImage"":\n                  ""projects/ubuntu-os-cloud/global/images/family/ubuntu-1604-lts"",\n                  ""diskSizeGb"": 100,\n                  ""autoDelete"": True,\n              },\n          },\n      ],\n      ""networkInterfaces"": [\n          {\n              ""accessConfigs"": [\n                  {\n                      ""name"": ""external-nat"",\n                      ""type"": ""ONE_TO_ONE_NAT"",\n                  },\n              ],\n              ""network"":\n              ""global/networks/default"",\n          },\n      ],\n  }\n  request = instances.insert(project=args.project, zone=args.zone, body=body)\n  response = None\n  try:\n    response = request.execute()\n    print(""done"")\n  except errors.HttpError as e:\n    if not e.content:\n      raise\n    content = json.loads(e.content)\n    if content.get(""error"", {}).get(""code"") == requests.codes.CONFLICT:\n      # We don\'t want to keep going so we reraise the error after logging\n      # a helpful error message.\n      logging.error(\n          ""Either the VM or the disk %s already exists in zone ""\n          ""%s in project %s "", args.vm_name, args.zone, args.project)\n      raise\n    else:\n      raise\n\n  op_id = response.get(""name"")\n  final_op = vm_util.wait_for_operation(gce, args.project, args.zone, op_id)\n\n  logging.info(""Final result for insert operation: %s"", final_op)\n  if final_op.get(""status"") != ""DONE"":\n    raise ValueError(""Insert operation has status %s"", final_op.get(""status""))\n\n  if final_op.get(""error""):\n    message = ""Insert operation resulted in error %s"".format(\n        final_op.get(""error""))\n    logging.error(message)\n    raise ValueError(message)\n\n  # Locate the install minikube script.\n  install_script = os.path.join(\n      os.path.dirname(__file__), ""install_minikube.sh"")\n\n  if not os.path.exists(install_script):\n    logging.error(""Could not find minikube install script: %s"", install_script)\n\n  vm_util.wait_for_vm(args.project, args.zone, args.vm_name)\n  vm_util.execute_script(args.project, args.zone, args.vm_name, install_script)\n\n  # Copy the .kube and .minikube files to test_dir\n  target = ""~/.kube""\n  full_target = ""{0}:{1}"".format(args.vm_name, target)\n  logging.info(""Copying %s to %s"", target, args.test_dir)\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + args.project, ""scp"", ""--recurse"",\n      full_target, args.test_dir, ""--zone="" + args.zone\n  ])\n\n  # The .minikube directory contains some really large ISO and other files that we don\'t need; so we\n  # only copy the files we need.\n  minikube_dir = os.path.join(args.test_dir, "".minikube"")\n  try:\n    os.makedirs(minikube_dir)\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(minikube_dir):\n      pass\n    else:\n      raise\n\n  for target in [""~/.minikube/*.crt"", ""~/.minikube/client.key""]:\n    full_target = ""{0}:{1}"".format(args.vm_name, target)\n    logging.info(""Copying %s to %s"", target, minikube_dir)\n    util.run([\n        ""gcloud"", ""compute"", ""--project="" + args.project, ""scp"", ""--recurse"",\n        full_target, minikube_dir, ""--zone="" + args.zone\n    ])\n\n  config_path = os.path.join(args.test_dir, "".kube"", ""config"")\n  modify_minikube_config(config_path, minikube_dir)\n\n\ndef teardown_minikube(args):\n  """"""Delete the VM used for minikube.""""""\n\n  credentials = GoogleCredentials.get_application_default()\n  gce = discovery.build(""compute"", ""v1"", credentials=credentials)\n  instances = gce.instances()\n\n  request = instances.delete(\n      project=args.project, zone=args.zone, instance=args.vm_name)\n\n  response = request.execute()\n\n  op_id = response.get(""name"")\n  final_op = vm_util.wait_for_operation(gce, args.project, args.zone, op_id)\n\n  logging.info(""Final result for delete operation: %s"", final_op)\n  if final_op.get(""status"") != ""DONE"":\n    raise ValueError(""Delete operation has status %s"", final_op.get(""status""))\n\n  if final_op.get(""error""):\n    message = ""Delete operation resulted in error %s"".format(\n        final_op.get(""error""))\n    logging.error(message)\n    raise ValueError(message)\n\n  # Ensure the disk is deleted. The disk should be auto-deleted with\n  # the VM but just in case we issue a delete request anyway.\n  disks = gce.disks()\n  request = disks.delete(\n      project=args.project, zone=args.zone, disk=args.vm_name)\n\n  response = None\n  try:\n    response = request.execute()\n  except errors.HttpError as e:\n    if not e.content:\n      raise\n    content = json.loads(e.content)\n    if content.get(""error"", {}).get(""code"") == requests.codes.NOT_FOUND:\n      logging.info(""Disk %s in zone %s in project %s already deleted."",\n                   args.vm_name, args.zone, args.project)\n    else:\n      raise\n\n  if response:\n    logging.info(""Waiting for disk to be deleted."")\n    op_id = response.get(""name"")\n    final_op = vm_util.wait_for_operation(gce, args.project, args.zone, op_id)\n\n    logging.info(""Final result for disk delete operation: %s"", final_op)\n    if final_op.get(""status"") != ""DONE"":\n      raise ValueError(""Disk delete operation has status %s"",\n                       final_op.get(""status""))\n\n    if final_op.get(""error""):\n      message = ""Delete disk operation resulted in error %s"".format(\n          final_op.get(""error""))\n      logging.error(message)\n      raise ValueError(message)\n\n\ndef get_gcp_identity():\n  identity = util.run_and_output([""gcloud"", ""config"", ""get-value"", ""account""])\n  logging.info(""Current GCP account: %s"", identity)\n  return identity\n\n\ndef main():  # pylint: disable=too-many-locals,too-many-statements\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  # create the top-level parser\n  parser = argparse.ArgumentParser(description=""Test Kubeflow E2E."")\n\n  parser.add_argument(\n      ""--test_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for all the test files. If not set a temporary ""\n      ""directory is created."")\n\n  parser.add_argument(\n      ""--artifacts_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for artifacts that should be preserved after ""\n      ""the test runs. Defaults to test_dir if not set."")\n\n  parser.add_argument(\n      ""--as_gcloud_user"",\n      dest=""as_gcloud_user"",\n      action=""store_true"",\n      help=(""Impersonate the user corresponding to the gcloud ""\n            ""command with kubectl and ks.""))\n  parser.add_argument(\n      ""--no-as_gcloud_user"", dest=""as_gcloud_user"", action=""store_false"")\n  parser.set_defaults(as_gcloud_user=False)\n\n  # TODO(jlewi): This should not be a global flag.\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=""The project to use."")\n\n  # TODO(jlewi): This should not be a global flag.\n  parser.add_argument(\n      ""--namespace"", default=None, type=str, help=(""The namespace to use.""))\n\n  parser.add_argument(\n      ""--github_token"",\n      default=None,\n      type=str,\n      help=(\n          ""The GitHub API token to use. This is needed since ksonnet uses the ""\n          ""GitHub API and without it we get rate limited. For more info see: ""\n          ""https://github.com/ksonnet/ksonnet/blob/master/docs""\n          ""/troubleshooting.md. Can also be set using environment variable ""\n          ""GITHUB_TOKEN.""))\n\n  parser.add_argument(\n      ""--deploy_name"", default="""", type=str, help=""The name of the deployment."")\n\n  parser.add_argument(\n      ""--workflow_name"", default="""", type=str, help=""The name of the workflow."")\n\n  subparsers = parser.add_subparsers()\n\n  parser_teardown = subparsers.add_parser(\n      ""teardown"", help=""teardown the test infrastructure."")\n\n  parser_teardown.set_defaults(func=teardown)\n\n  parser_tf_serving = subparsers.add_parser(\n      ""deploy_model"", help=""Deploy a TF serving model."")\n\n  parser_tf_serving.set_defaults(func=deploy_model)\n\n  parser_tf_serving.add_argument(\n      ""--params"",\n      default="""",\n      type=str,\n      help=(""Comma separated list of parameters to set on the model.""))\n\n  parser_pytorch_job = subparsers.add_parser(\n      ""deploy_pytorchjob"", help=""Deploy a pytorch-job"")\n\n  parser_pytorch_job.set_defaults(func=deploy_pytorchjob)\n\n  parser_pytorch_job.add_argument(\n      ""--params"",\n      default="""",\n      type=str,\n      help=(""Comma separated list of parameters to set on the model.""))\n\n  parser_argo_job = subparsers.add_parser(""deploy_argo"", help=""Deploy argo"")\n\n  parser_argo_job.set_defaults(func=deploy_argo)\n\n  parser_katib_test = subparsers.add_parser(""test_katib"", help=""Test Katib"")\n\n  parser_katib_test.set_defaults(func=test_katib)\n\n  parser_minikube = subparsers.add_parser(\n      ""deploy_minikube"", help=""Setup a K8s cluster on minikube."")\n\n  parser_minikube.set_defaults(func=deploy_minikube)\n\n  parser_minikube.add_argument(\n      ""--vm_name"", required=True, type=str, help=""The name of the VM to use."")\n\n  parser_minikube.add_argument(\n      ""--zone"",\n      default=""us-east1-d"",\n      type=str,\n      help=""The zone for the cluster."")\n\n  parser_teardown_minikube = subparsers.add_parser(\n      ""teardown_minikube"", help=""Delete the VM running minikube."")\n\n  parser_teardown_minikube.set_defaults(func=teardown_minikube)\n\n  parser_teardown_minikube.add_argument(\n      ""--zone"",\n      default=""us-east1-d"",\n      type=str,\n      help=""The zone for the cluster."")\n\n  parser_teardown_minikube.add_argument(\n      ""--vm_name"", required=True, type=str, help=""The name of the VM to use."")\n\n  args = parser.parse_args()\n\n  if not args.test_dir:\n    logging.info(""--test_dir not set; using a temporary directory."")\n\n    now = datetime.datetime.now()\n    label = ""test_deploy-"" + now.strftime(""%m%d-%H%M-"") + uuid.uuid4().hex[0:4]\n\n    # Create a temporary directory for this test run\n    args.test_dir = os.path.join(tempfile.gettempdir(), label)\n\n  if not args.artifacts_dir:\n    args.artifacts_dir = args.test_dir\n\n  test_log = os.path.join(\n      args.artifacts_dir, ""logs"",\n      ""test_deploy."" + args.func.__name__ + args.deploy_name + "".log.txt"")\n\n  try:\n    os.makedirs(os.path.dirname(test_log))\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(os.path.dirname(test_log)):\n      pass\n    else:\n      raise\n\n  # TODO(jlewi): We should make this a util routine in kubeflow.testing.util\n  # Setup a logging file handler. This way we can upload the log outputs\n  # to gubernator.\n  root_logger = logging.getLogger()\n\n  file_handler = logging.FileHandler(test_log)\n  root_logger.addHandler(file_handler)\n  # We need to explicitly set the formatter because it will not pick up\n  # the BasicConfig.\n  formatter = logging.Formatter(\n      fmt=(""%(levelname)s|%(asctime)s""\n           ""|%(pathname)s|%(lineno)d| %(message)s""),\n      datefmt=""%Y-%m-%dT%H:%M:%S"")\n  file_handler.setFormatter(formatter)\n  logging.info(""Logging to %s"", test_log)\n  util.run([ks, ""version""])\n\n  util.maybe_activate_service_account()\n  config_file = os.path.expanduser(kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n\n  # Print out the config to help debugging.\n  output = util.run_and_output([""gcloud"", ""config"", ""config-helper""])\n  logging.info(""gcloud config: \\n%s"", output)\n  wrap_test(args)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/test_deploy_app.py,0,"b'# -*- coding: utf-8 -*-\n# Script to start deployment api and make request to it.\nimport argparse\nimport base64\nimport datetime\nimport logging\nimport os\nimport errno\nimport shutil\nimport subprocess\nimport tempfile\nimport threading\nfrom functools import partial\nfrom multiprocessing import Process\nfrom time import sleep\nfrom google.auth.transport.requests import Request\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\nfrom prometheus_client import start_http_server, Gauge, Counter\n\nimport requests\nimport yaml\nimport google.auth\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nfrom retrying import retry\n\nfrom kubeflow.testing import test_util\n\nFILE_PATH = os.path.dirname(os.path.abspath(__file__))\nSSL_DIR = os.path.join(FILE_PATH, ""sslcert"")\nSSL_BUCKET = \'kubeflow-ci-deploy-cert\'\nIAM_SCOPE = \'https://www.googleapis.com/auth/iam\'\nOAUTH_TOKEN_URI = \'https://www.googleapis.com/oauth2/v4/token\'\nMETHOD = \'GET\'\nSERVICE_HEALTH = Gauge(\n    \'deployment_service_status\',\n    \'0: normal; 1: deployment not successful; 2: service down\')\nPROBER_HEALTH = Gauge(\'prober_health\', \'0: normal; 1: not working\')\nLOADTEST_HEALTH = Gauge(\'loadtest_health\', \'0: normal; 1: not working\')\nLOADTEST_SUCCESS = Gauge(\'loadtest_success\',\n                         \'number of successful requests in current load test\')\nSUCCESS_COUNT = Counter(\'deployment_success_count\',\n                        \'accumulative count of successful deployment\')\nFAILURE_COUNT = Counter(\'deployment_failure_count\',\n                        \'accumulative count of failed deployment\')\nLOADTEST_ZONE = [\n    \'us-central1-a\', \'us-central1-c\', \'us-east1-c\', \'us-east1-d\', \'us-west1-b\'\n]\n\n\nclass requestThread(threading.Thread):\n\n  def __init__(self, target_url, req_data, google_open_id_connect_token):\n    threading.Thread.__init__(self)\n    self.target_url = target_url\n    self.req_data = req_data\n    self.google_open_id_connect_token = google_open_id_connect_token\n\n  def run(self):\n    try:\n      resp = requests.post(\n          ""https://%s/kfctl/e2eDeploy"" % self.target_url,\n          json=self.req_data,\n          headers={\n              \'Authorization\':\n              \'Bearer {}\'.format(self.google_open_id_connect_token)\n          })\n      if resp.status_code != 200:\n        logging.error(""request failed:%s\\n request data:%s""\n                      % (resp, self.req_data))\n        # Mark service down if return code abnormal\n        SERVICE_HEALTH.set(2)\n    except Exception as e:\n      logging.error(e)\n      SERVICE_HEALTH.set(2)\n\n\ndef may_get_env_var(name):\n  env_val = os.getenv(name)\n  if env_val:\n    logging.info(""%s is set"" % name)\n    return env_val\n  else:\n    raise Exception(""%s not set"" % name)\n\n\ndef getZone(args, deployment):\n  if args.mode == ""loadtest"":\n    return LOADTEST_ZONE[int(deployment[-1]) % len(LOADTEST_ZONE)]\n  return args.zone\n\n\ndef get_target_url(args):\n  if args.mode == ""loadtest"":\n    return ""deploy-staging.kubeflow.cloud""\n  if args.mode == ""prober"":\n    return ""deploy.kubeflow.cloud""\n  raise RuntimeError(""No default target url for test mode %s !"" % args.mode)\n\n\ndef prepare_request_data(args, deployment):\n  logging.info(""prepare deploy call data"")\n  with open(\n      os.path.join(FILE_PATH, ""../bootstrap/config/gcp_prototype.yaml""),\n      \'r\') as conf_input:\n    defaultApp = yaml.load(conf_input)[""defaultApp""]\n\n  for param in defaultApp[""parameters""]:\n    if param[""name""] == ""acmeEmail"":\n      param[""value""] = args.email\n    if param[""name""] == ""ipName"":\n      param[""value""] = deployment + ""-ip""\n    if param[""name""] == ""hostname"":\n      param[""value""] = ""%s.endpoints.%s.cloud.goog"" % (deployment, args.project)\n  defaultApp[\'registries\'][0][\'version\'] = args.kfversion\n\n  access_token = util_run(\n      \'gcloud auth application-default print-access-token\'.split(\' \'),\n      cwd=FILE_PATH)\n\n  client_id = may_get_env_var(""CLIENT_ID"")\n  client_secret = may_get_env_var(""CLIENT_SECRET"")\n  credentials = GoogleCredentials.get_application_default()\n  crm = discovery.build(\'cloudresourcemanager\', \'v1\', credentials=credentials)\n  project = crm.projects().get(projectId=args.project).execute()\n  logging.info(""project info: %s"", project)\n  request_data = {\n      ""AppConfig"": defaultApp,\n      ""Apply"": True,\n      ""AutoConfigure"": True,\n      ""ClientId"": base64.b64encode(client_id.encode()).decode(""utf-8""),\n      ""ClientSecret"": base64.b64encode(client_secret.encode()).decode(""utf-8""),\n      ""Cluster"": deployment,\n      ""Email"": args.email,\n      ""IpName"": deployment + \'-ip\',\n      ""Name"": deployment,\n      ""Namespace"": \'kubeflow\',\n      ""Project"": args.project,\n      ""ProjectNumber"": project[""projectNumber""],\n      # service account client id of account: kubeflow-testing@kubeflow-ci.iam.gserviceaccount.com\n      ""SAClientId"": args.sa_client_id,\n      ""Token"": access_token,\n      ""Zone"": getZone(args, deployment)\n  }\n  return request_data\n\n\ndef make_e2e_call(args):\n  if not clean_up_resource(args, set([args.deployment])):\n    raise RuntimeError(""Failed to cleanup resource"")\n  req_data = prepare_request_data(args, args.deployment)\n  resp = requests.post(\n      ""http://kubeflow-controller.%s.svc.cluster.local:8080/kfctl/e2eDeploy"" %\n      args.namespace,\n      json=req_data)\n  if resp.status_code != 200:\n    raise RuntimeError(""deploy request received status code: %s, message: %s"" %\n                       (resp.status_code, resp.text))\n  logging.info(""deploy call done"")\n\n\n# Make 1 deployment request to service url, return if request call successful.\ndef make_prober_call(args, service_account_credentials):\n  logging.info(""start new prober call"")\n  req_data = prepare_request_data(args, args.deployment)\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  try:\n    resp = requests.post(\n        ""https://%s/kfctl/e2eDeploy"" % get_target_url(args),\n        json=req_data,\n        headers={\n            \'Authorization\': \'Bearer {}\'.format(google_open_id_connect_token)\n        })\n    if resp.status_code != 200:\n      # Mark service down if return code abnormal\n      SERVICE_HEALTH.set(2)\n      return False\n  except Exception as e:\n    logging.error(e)\n    SERVICE_HEALTH.set(2)\n    return False\n  logging.info(""prober call done"")\n  return True\n\n\n# For each deployment, make a request to service url, return if all requests call successful.\ndef make_loadtest_call(args, service_account_credentials, projects, deployments):\n  logging.info(""start new load test call"")\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  threads = []\n  for project in projects:\n    args.project = project\n    for deployment in deployments:\n      req_data = prepare_request_data(args, deployment)\n      threads.append(\n          requestThread(\n              get_target_url(args), req_data, google_open_id_connect_token))\n  for t in threads:\n    t.start()\n  for t in threads:\n    t.join()\n  if SERVICE_HEALTH._value.get() == 2:\n    return False\n  logging.info(""load test call done"")\n  return True\n\n\ndef get_gcs_path(mode, project, deployment):\n  return os.path.join(SSL_BUCKET, mode, project, deployment)\n\n\n# Insert ssl cert into GKE cluster\ndef insert_ssl_cert(args, deployment):\n  logging.info(""Wait till deployment is done and GKE cluster is up"")\n  credentials = GoogleCredentials.get_application_default()\n\n  service = discovery.build(\'deploymentmanager\', \'v2\', credentials=credentials)\n  # Wait up to 10 minutes till GKE cluster up and available.\n  end_time = datetime.datetime.now() + datetime.timedelta(minutes=10)\n  while datetime.datetime.now() < end_time:\n    sleep(5)\n    try:\n      request = service.deployments().get(\n          project=args.project, deployment=deployment)\n      response = request.execute()\n      if response[\'operation\'][\'status\'] != \'DONE\':\n        logging.info(""Deployment running"")\n        continue\n    except Exception as e:\n      logging.info(""Deployment hasn\'t started"")\n      continue\n    break\n\n  ssl_local_dir = os.path.join(SSL_DIR, args.project, deployment)\n  if os.path.exists(ssl_local_dir):\n    shutil.rmtree(ssl_local_dir)\n  os.makedirs(ssl_local_dir)\n  logging.info(""donwload ssl cert and insert to GKE cluster"")\n  try:\n    # TODO: switch to client lib\n    gcs_path = get_gcs_path(args.mode, args.project, deployment)\n    util_run((""gsutil cp gs://%s/* %s"" % (gcs_path, ssl_local_dir)).split(\' \'))\n  except Exception:\n    logging.warning(""ssl cert for %s doesn\'t exist in gcs"" % args.mode)\n    # clean up local dir\n    shutil.rmtree(ssl_local_dir)\n    return True\n  try:\n    create_secret(args, deployment, ssl_local_dir)\n  except Exception as e:\n    logging.error(e)\n    return False\n  return True\n\n\n@retry(wait_fixed=2000, stop_max_delay=15000)\ndef create_secret(args, deployment, ssl_local_dir):\n  util_run(\n      (""gcloud container clusters get-credentials %s --zone %s --project %s"" %\n       (deployment, getZone(args, deployment), args.project)).split(\' \'))\n  util_run((""kubectl create -f %s"" % ssl_local_dir).split(\' \'))\n\n\n# deployments: set(string) which contains all deployment names in current test round.\ndef check_deploy_status(args, deployments):\n  num_deployments = len(deployments)\n  logging.info(""check deployment status"")\n  service_account_credentials = get_service_account_credentials(""CLIENT_ID"")\n\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  # Wait up to 30 minutes for IAP access test.\n  num_req = 0\n  end_time = datetime.datetime.now() + datetime.timedelta(\n      minutes=args.iap_wait_min)\n  success_deploy = set()\n  while datetime.datetime.now() < end_time and len(deployments) > 0:\n    sleep(10)\n    num_req += 1\n\n    for deployment in deployments:\n      url = ""https://%s.endpoints.%s.cloud.goog"" % (deployment, args.project)\n      logging.info(""Trying url: %s"", url)\n      try:\n        resp = requests.request(\n            METHOD,\n            url,\n            headers={\n                \'Authorization\':\n                \'Bearer {}\'.format(google_open_id_connect_token)\n            },\n            verify=False)\n        if resp.status_code == 200:\n          success_deploy.add(deployment)\n          logging.info(""IAP is ready for %s!"", url)\n        else:\n          logging.info(\n              ""%s: IAP not ready, request number: %s"" % (deployment, num_req))\n      except Exception:\n        logging.info(""%s: IAP not ready, exception caught, request number: %s"" %\n                     (deployment, num_req))\n    deployments = deployments.difference(success_deploy)\n\n  for deployment in success_deploy:\n    try:\n      ssl_local_dir = os.path.join(SSL_DIR, args.project, deployment)\n      try:\n        os.makedirs(ssl_local_dir)\n      except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(ssl_local_dir):\n          pass\n        else:\n          raise\n      util_run((\n          ""gcloud container clusters get-credentials %s --zone %s --project %s""\n          % (deployment, getZone(args, deployment), args.project)).split(\' \'))\n      for sec in [""envoy-ingress-tls"", ""letsencrypt-prod-secret""]:\n        sec_data = util_run(\n            (""kubectl get secret %s -n kubeflow -o yaml"" % sec).split(\' \'))\n        with open(os.path.join(ssl_local_dir, sec + "".yaml""),\n                  \'w+\') as sec_file:\n          sec_file.write(sec_data)\n          sec_file.close()\n      # TODO: switch to client lib\n      gcs_path = get_gcs_path(args.mode, args.project, deployment)\n      util_run(\n          (""gsutil cp %s/* gs://%s/"" % (ssl_local_dir, gcs_path)).split(\' \'))\n    except Exception:\n      logging.error(""%s: failed uploading ssl cert"" % deployment)\n\n  # return number of successful deployments\n  return num_deployments - len(deployments)\n\n\ndef get_service_account_credentials(client_id_key):\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(credentials, google.oauth2.credentials.Credentials):\n    raise Exception(\'make_iap_request is only supported for service \'\n                    \'accounts.\')\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  credentials.refresh(Request())\n\n  signer_email = credentials.service_account_email\n  if isinstance(credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    signer = google.auth.iam.Signer(Request(), credentials, signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer\n  # and email acquired from the bootstrap credentials.\n  return google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={\'target_audience\': may_get_env_var(client_id_key)})\n\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      \'assertion\': service_account_jwt,\n      \'grant_type\': google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[\'id_token\']\n\n\ndef delete_gcloud_resource(args, keyword, filter=\'\', dlt_params=[]):\n  # TODO: switch to client lib\n  get_cmd = \'gcloud compute %s list --project=%s --format=""value(name)""\' % (\n      keyword, args.project)\n  elements = util_run(get_cmd + filter, shell=True)\n  for element in elements.split(\'\\n\'):\n    dlt_cmd = \'gcloud compute %s delete -q --project=%s %s\' % (\n        keyword, args.project, element)\n    try:\n      util_run(dlt_cmd.split(\' \') + dlt_params)\n    except Exception as e:\n      logging.warning(\'Cannot remove %s %s\' % (keyword, element))\n      logging.warning(e)\n\n\ndef clean_up_resource(args, deployments):\n  """"""Clean up deployment / app config from previous test\n\n  Args:\n    args: The args from ArgParse.\n    deployments set(string): which contains all deployment names in current test round.\n  Returns:\n    bool: True if cleanup is done\n  """"""\n  logging.info(\n      ""Clean up project resource (backend service and deployment)"")\n\n  # Will reuse source repo for continuous tests\n  # Within 7 days after repo deleted, source repo won\'t allow recreation with same name\n\n  # Delete deployment\n  credentials = GoogleCredentials.get_application_default()\n  service = discovery.build(\'deploymentmanager\', \'v2\', credentials=credentials)\n  delete_done = False\n  for deployment in deployments:\n    try:\n      request = service.deployments().delete(\n          project=args.project, deployment=deployment)\n      request.execute()\n    except Exception as e:\n      logging.info(""Deployment doesn\'t exist, continue"")\n  # wait up to 10 minutes till delete finish.\n  end_time = datetime.datetime.now() + datetime.timedelta(minutes=10)\n  while datetime.datetime.now() < end_time:\n    sleep(10)\n    try:\n      request = service.deployments().list(project=args.project)\n      response = request.execute()\n      if (\'deployments\' not in response) or (len(deployments & set(\n          d[\'name\'] for d in response[\'deployments\'])) == 0):\n        delete_done = True\n        break\n    except Exception:\n      logging.info(""Failed listing current deployments, retry in 10 seconds"")\n\n  # Delete forwarding-rules\n  delete_gcloud_resource(args, \'forwarding-rules\', dlt_params=[\'--global\'])\n  # Delete target-http-proxies\n  delete_gcloud_resource(args, \'target-http-proxies\')\n  # Delete target-http-proxies\n  delete_gcloud_resource(args, \'target-https-proxies\')\n  # Delete url-maps\n  delete_gcloud_resource(args, \'url-maps\')\n  # Delete backend-services\n  delete_gcloud_resource(args, \'backend-services\', dlt_params=[\'--global\'])\n  # Delete instance-groups\n  for zone in LOADTEST_ZONE:\n    delete_gcloud_resource(\n        args,\n        \'instance-groups unmanaged\',\n        filter=\' --filter=INSTANCES:0\',\n        dlt_params=[\'--zone=\' + zone])\n  # Delete ssl-certificates\n  delete_gcloud_resource(args, \'ssl-certificates\')\n  # Delete health-checks\n  delete_gcloud_resource(args, \'health-checks\')\n\n  if not delete_done:\n    logging.error(""failed to clean up resources for project %s deployments %s"",\n                  args.project, deployments)\n  return delete_done\n\n\ndef util_run(command,\n             cwd=None,\n             env=None,\n             shell=False,\n             polling_interval=datetime.timedelta(seconds=1)):\n  """"""Run a subprocess.\n\n  Any subprocess output is emitted through the logging modules.\n\n  Returns:\n    output: A string containing the output.\n  """"""\n  logging.info(""Running: %s \\ncwd=%s"", "" "".join(command), cwd)\n\n  if not env:\n    env = os.environ\n  else:\n    keys = sorted(env.keys())\n\n    lines = []\n    for k in keys:\n      lines.append(""{0}={1}"".format(k, env[k]))\n    logging.info(""Running: Environment:\\n%s"", ""\\n"".join(lines))\n\n  process = subprocess.Popen(\n      command,\n      cwd=cwd,\n      env=env,\n      shell=shell,\n      stdout=subprocess.PIPE,\n      stderr=subprocess.STDOUT)\n\n  # logging.info(""Subprocess output:\\n"")\n  output = []\n  while process.poll() is None:\n    process.stdout.flush()\n    for line in iter(process.stdout.readline, \'\'):\n      output.append(line.strip(\'\\n\'))\n      # logging.info(line.strip())\n\n    sleep(polling_interval.total_seconds())\n\n  process.stdout.flush()\n  for line in iter(process.stdout.readline, b\'\'):\n    output.append(line.strip(\'\\n\'))\n    # logging.info(line.strip())\n\n  if process.returncode != 0:\n    raise subprocess.CalledProcessError(\n        process.returncode, ""cmd: {0} exited with code {1}"".format(\n            "" "".join(command), process.returncode), ""\\n"".join(output))\n\n  return ""\\n"".join(output)\n\ndef clean_up_project_resource(args, projects, deployments):\n  proc = []\n  for project in projects:\n    args.project = project\n    p = Process(target = partial(clean_up_resource, args, deployments))\n    p.start()\n    proc.append(p)\n\n  for p in proc:\n    p.join()\n\ndef upload_load_test_ssl_cert(args, projects, deployments):\n  for project in projects:\n    args.project = project\n    for deployment in deployments:\n      insert_ssl_cert(args, deployment)\n\ndef check_load_test_results(args, projects, deployments):\n  num_deployments = len(deployments)\n  total_success = 0\n  # deadline for checking all the results.\n  end_time = datetime.datetime.now() + datetime.timedelta(\n      minutes=args.iap_wait_min)\n  for project in projects:\n    args.project = project\n    # set the deadline for each check.\n    now = datetime.datetime.now()\n    if end_time < now:\n      args.iap_wait_min = 1\n    else:\n      delta = end_time - now\n      args.iap_wait_min = delta.seconds / 60 + 1\n    num_success = check_deploy_status(args, deployments)\n    total_success += num_success\n    logging.info(""%s out of %s deployments succeed for project %s"",\n                 num_success, num_deployments, project)\n    # We only wait 1 minute for subsequent checks because we already waited forIAP since we already\n    args.iap_wait_min = 1\n    LOADTEST_SUCCESS.set(num_success)\n    if num_success == num_deployments:\n      SUCCESS_COUNT.inc()\n    else:\n      FAILURE_COUNT.inc()\n  logging.info(""%s out of %s deployments succeed in total"",\n                total_success, num_deployments * len(projects))\n\n\ndef run_load_test(args):\n  num_deployments = args.number_deployments_per_project\n  num_projects = args.number_projects\n  start_http_server(8000)\n  LOADTEST_SUCCESS.set(num_deployments)\n  LOADTEST_HEALTH.set(0)\n  service_account_credentials = get_service_account_credentials(\n      ""SERVICE_CLIENT_ID"")\n  deployments = set(\n      [\'kubeflow\' + str(i) for i in range(1, num_deployments + 1)])\n  projects = [args.project_prefix + str(i)\n             for i in range(1, num_projects + 1)]\n  logging.info(""deployments: %s"" % deployments)\n  logging.info(""projects: %s"" % projects)\n\n  clean_up_project_resource(args, projects, deployments)\n\n  if not make_loadtest_call(\n    args, service_account_credentials, projects, deployments):\n    LOADTEST_SUCCESS.set(0)\n    FAILURE_COUNT.inc()\n    logging.error(""load test request failed"")\n    return\n\n  upload_load_test_ssl_cert(args, projects, deployments)\n\n  check_load_test_results(args, projects, deployments)\n\n  clean_up_project_resource(args, projects, deployments)\n\n\n\n\ndef run_e2e_test(args):\n  sleep(args.wait_sec)\n  make_e2e_call(args)\n  insert_ssl_cert(args, args.deployment)\n  if not check_deploy_status(args, set([args.deployment])):\n    raise RuntimeError(""IAP endpoint not ready after 30 minutes, time out..."")\n  logging.info(""Test finished."")\n\n\ndef wrap_test(args):\n  """"""Run the tests given by args.func and output artifacts as necessary.\n  """"""\n  test_name = ""bootstrapper""\n  test_case = test_util.TestCase()\n  test_case.class_name = ""KubeFlow""\n  test_case.name = args.workflow_name + ""-"" + test_name\n  try:\n\n    def run():\n      args.func(args)\n\n    test_util.wrap_test(run, test_case)\n  finally:\n    # Test grid has problems with underscores in the name.\n    # https://github.com/kubeflow/kubeflow/issues/631\n    # TestGrid currently uses the regex junit_(^_)*.xml so we only\n    # want one underscore after junit.\n    junit_name = test_case.name.replace(""_"", ""-"")\n    junit_path = os.path.join(args.artifacts_dir,\n                              ""junit_{0}.xml"".format(junit_name))\n    logging.info(""Writing test results to %s"", junit_path)\n    test_util.create_junit_xml_file([test_case], junit_path)\n\n\n# Clone repos to tmp folder and build docker images\ndef main(unparsed_args=None):\n  parser = argparse.ArgumentParser(\n      description=""Start deployment api and make request to it."")\n\n  parser.add_argument(\n      ""--deployment"",\n      default=""periodic-test"",\n      type=str,\n      help=""Deployment name."")\n  parser.add_argument(\n      ""--email"",\n      default=""google-kubeflow-support@google.com"",\n      type=str,\n      help=""Email used during e2e test"")\n  parser.add_argument(\n      ""--project"",\n      default=""kubeflow-ci-deployment"",\n      type=str,\n      help=""e2e test project id"")\n  parser.add_argument(\n      ""--project_prefix"",\n      default=""kf-gcp-deploy-test"",\n      type=str,\n      help=""project prefix for load test"")\n  parser.add_argument(\n      ""--number_projects"",\n      default=""2"",\n      type=int,\n      help=""number of projects used in load test"")\n  parser.add_argument(\n      ""--number_deployments_per_project"",\n      default=""5"",\n      type=int,\n      help=""number of deployments per project used in load test"")\n  parser.add_argument(\n      ""--namespace"",\n      default="""",\n      type=str,\n      help=""namespace where deployment service is running"")\n  parser.add_argument(\n      ""--wait_sec"", default=120, type=int, help=""oauth client secret"")\n  parser.add_argument(\n      ""--iap_wait_min"", default=30, type=int, help=""minutes to wait for IAP"")\n  parser.add_argument(\n      ""--zone"", default=""us-east1-d"", type=str, help=""GKE cluster zone"")\n  parser.add_argument(\n      ""--sa_client_id"",\n      default=""111670663612681935351"",\n      type=str,\n      help=""Service account client id"")\n  parser.add_argument(\n      ""--kfversion"",\n      default=""v0.4.1"",\n      type=str,\n      help=""Service account client id"")\n  parser.add_argument(\n      ""--mode"",\n      default=""e2e"",\n      type=str,\n      help=""offer three test mode: e2e, prober, and loadtest"")\n  # args for e2e test\n  parser.set_defaults(func=run_e2e_test)\n  parser.add_argument(\n      ""--artifacts_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for artifacts that should be preserved after ""\n      ""the test runs. Defaults to test_dir if not set."")\n  parser.add_argument(\n      ""--workflow_name"",\n      default=""deployapp"",\n      type=str,\n      help=""The name of the workflow."")\n\n  args = parser.parse_args(args=unparsed_args)\n\n  if not args.artifacts_dir:\n    args.artifacts_dir = tempfile.gettempdir()\n\n  util_run(\n      (\'gcloud auth activate-service-account --key-file=\' +\n       may_get_env_var(""GOOGLE_APPLICATION_CREDENTIALS"")).split(\' \'),\n      cwd=FILE_PATH)\n  if args.mode == ""e2e"":\n    wrap_test(args)\n\n  if args.mode == ""prober"":\n    start_http_server(8000)\n    SERVICE_HEALTH.set(0)\n    PROBER_HEALTH.set(0)\n    service_account_credentials = get_service_account_credentials(\n        ""SERVICE_CLIENT_ID"")\n    while True:\n      sleep(args.wait_sec)\n      if not clean_up_resource(args, set([args.deployment])):\n        PROBER_HEALTH.set(1)\n        FAILURE_COUNT.inc()\n        logging.error(\n            ""request cleanup failed, retry in %s seconds"" % args.wait_sec)\n        continue\n      PROBER_HEALTH.set(0)\n      if make_prober_call(args, service_account_credentials):\n        if insert_ssl_cert(args, args.deployment):\n          PROBER_HEALTH.set(0)\n        else:\n          PROBER_HEALTH.set(1)\n          FAILURE_COUNT.inc()\n          logging.error(""request insert_ssl_cert failed, retry in %s seconds"" %\n                        args.wait_sec)\n          continue\n        if check_deploy_status(args, set([args.deployment])):\n          SERVICE_HEALTH.set(0)\n          SUCCESS_COUNT.inc()\n        else:\n          SERVICE_HEALTH.set(1)\n          FAILURE_COUNT.inc()\n      else:\n        SERVICE_HEALTH.set(2)\n        FAILURE_COUNT.inc()\n        logging.error(\n            ""prober request failed, retry in %s seconds"" % args.wait_sec)\n\n  if args.mode == ""loadtest"":\n    run_load_test(args)\n\n\nif __name__ == \'__main__\':\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger(\'googleapiclient.discovery_cache\').setLevel(logging.ERROR)\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/test_deploy_test.py,0,"b'# -*- coding: utf-8 -*-\nimport tempfile\nimport unittest\nimport yaml\n\nfrom testing import test_deploy\n\n\nclass TestDeploy(unittest.TestCase):\n\n  def testModifyMinikubeConfig(self):\n    """"""Test modeify_minikube_config""""""\n\n    config_path = None\n    with tempfile.NamedTemporaryFile(delete=False) as hf:\n      config_path = hf.name\n      hf.write(""""""apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: /home/jlewi/.minikube/ca.crt\n    server: https://10.240.0.18:8443\n  name: minikube\ncontexts:\n- context:\n    cluster: minikube\n    user: minikube\n  name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n- name: minikube\n  user:\n    as-user-extra: {}\n    client-certificate: /home/jlewi/.minikube/client.crt\n    client-key: /home/jlewi/.minikube/client.key\n"""""")\n\n    test_deploy.modify_minikube_config(config_path, ""/test/.minikube"")\n\n    # Load the output.\n    with open(config_path) as hf:\n      config = yaml.load(hf)\n\n    expected = {\n        ""apiVersion"":\n        ""v1"",\n        ""clusters"": [{\n            ""cluster"": {\n                ""certificate-authority"": ""/test/.minikube/ca.crt"",\n                ""server"": ""https://10.240.0.18:8443""\n            },\n            ""name"": ""minikube""\n        }],\n        ""contexts"": [{\n            ""context"": {\n                ""cluster"": ""minikube"",\n                ""user"": ""minikube""\n            },\n            ""name"": ""minikube""\n        }],\n        ""current-context"":\n        ""minikube"",\n        ""kind"":\n        ""Config"",\n        ""preferences"": {},\n        ""users"": [{\n            ""name"": ""minikube"",\n            ""user"": {\n                ""as-user-extra"": {},\n                ""client-certificate"": ""/test/.minikube/client.crt"",\n                ""client-key"": ""/test/.minikube/client.key""\n            }\n        }]\n    }\n\n    self.assertDictEqual(expected, config)\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/test_flake8.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Run flake8 tests\n\nThis test goes through all Python files in the specified test_files_dirs\ndirectories and runs flake8 <filename> and reports the results\n\nExample invocation\n\npython -m testing.test_flake8 --test_files_dirs=/kubeflow/application/tests,/kubeflow/common/tests,/kubeflow/jupyter/tests,/kubeflow/iap/tests,/kubeflow/gcp/tests,/kubeflow/tensorboard/tests,/kubeflow/examples/tests,/kubeflow/metacontroller/tests,/kubeflow/profiles/tests,/kubeflow/tf-training/tests  # noqa: E501\n\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nimport os\n\nfrom kubeflow.testing import test_helper, util\n\nFLAKE8_OPTS = """"""--count --select=E901,E999,F821,F822,F823 --show-source\n                 --statistics"""""".split()\n\n# Test only files which end in \'.py\' or have no suffix\n\n\ndef should_test(file_path):\n  _, ext = os.path.splitext(file_path.lower())\n  return ext in (\'.py\', \'\')\n\n\ndef run(test_files_dirs, test_case):\n  # Go through each Python file in test_files_dirs and run flake8\n  for test_files_dir in test_files_dirs:\n    for root, _, files in os.walk(test_files_dir):\n      for test_file in files:\n        full_path = os.path.join(root, test_file)\n        assert root == os.path.dirname(full_path)\n        if should_test(full_path):\n          logging.info(""Testing: %s"", test_file)\n          try:\n            output = util.run([\'flake8\', full_path] + FLAKE8_OPTS, cwd=root)\n            try:\n              parsed = json.loads(output)\n            except AttributeError:\n              logging.error(\n                  ""Output of flake8 could not be parsed as json; ""\n                  ""output: %s"", output)\n              parsed = {}\n\n            if not hasattr(parsed, ""get""):\n              # Legacy style tests emit true rather than a json object.\n              # Parsing the string as json converts it to a bool so we\n              # just use parsed as test_passed\n              # Old style tests actually use std.assert so flake8 will\n              # actually return an error in the case the test did\n              # not pass.\n              logging.warn(\n                  ""flake8 is using old style and not emitting an object. ""\n                  ""Result was: %s. Output will be treated as a boolean"", output)\n              test_passed = parsed\n            else:\n              test_passed = parsed.get(""pass"", False)\n\n            if not test_passed:\n              msg = \'{} test failed\'.format(test_file)\n              test_case.add_failure_info(msg)\n              logging.error(\n                  \'{}. See Subprocess output for details.\'.format(msg))\n          except Exception as e:\n            msg = \'{} test failed\'.format(test_file)\n            test_case.add_failure_info(msg)\n            logging.error(\'{} with exception {}. See Subprocess output for \'\n                          \'details.\'.format(msg, e))\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--test_files_dirs"",\n      default=""."",\n      type=str,\n      help=""Comma separated directories containing Python files"")\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef test_flake8(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n  if not args.test_files_dirs:\n    raise ValueError(\'--test_files_dirs needs to be set\')\n  run(args.test_files_dirs.split(\',\'), test_case)\n\n\nif __name__ == ""__main__"":\n  test_case = test_helper.TestCase(name=\'test_flake8\', test_func=test_flake8)\n  test_suite = test_helper.init(\n      name=\'flake8_test_suite\', test_cases=[test_case])\n  test_suite.run()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/test_jsonnet.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Run jsonnet tests\n\nThis test goes through all jsonnet files specified by the test_files_dirs\ndirectory and runs jsonnet eval <filename> and reports the results\n\nExample invocation\n\npython -m testing.test_jsonnet --test_files_dirs=/kubeflow/application/tests,/kubeflow/common/tests,/kubeflow/jupyter/tests,/kubeflow/iap/tests,/kubeflow/gcp/tests,/kubeflow/tensorboard/tests,/kubeflow/examples/tests,/kubeflow/metacontroller/tests,/kubeflow/profiles/tests,/kubeflow/tf-training/tests,/kubeflow/kubebench/tests --artifacts_dir=/tmp/artifacts  # noqa: E501\n\nTODO(jlewi): Should we use pytest to create a parameterized test with respect\nto directory? See https://docs.pytest.org/en/latest/example/parametrize.html\n""""""\n\nfrom __future__ import print_function\n\nimport logging\nimport json\nimport os\n\nimport argparse\n\nfrom kubeflow.testing import test_helper, util\n\n\n# We should test all files which end in .jsonnet or .libsonnet\n# except ksonnet prototype definitions - they require additional\n# dependencies\ndef should_test(file_path):\n  _, ext = os.path.splitext(file_path)\n  if ext not in (\'.jsonnet\', \'.libsonnet\'):\n    return False\n  parts = file_path.split(\'/\')\n  if len(parts) < 2:\n    raise ValueError(\'Invalid file : {}\'.format(file_path))\n  return parts[-2] != \'prototypes\'\n\n\ndef is_excluded(file_name, exclude_dirs):\n  for exclude_dir in exclude_dirs:\n    if file_name.startswith(exclude_dir):\n      return True\n  return False\n\n\ndef run(test_files_dirs, jsonnet_path_args, exclude_dirs, test_case):\n  # Go through each jsonnet file in test_files_dirs and run jsonnet eval\n  for test_files_dir in test_files_dirs:\n    for root, _, files in os.walk(test_files_dir):\n      if is_excluded(root, exclude_dirs):\n        logging.info(""Skipping %s"", root)\n        continue\n\n      for test_file in files:\n        full_path = os.path.join(root, test_file)\n        if should_test(full_path):\n          logging.info(""Testing: %s"", test_file)\n          try:\n            output = util.run(\n                [\'jsonnet\', \'eval\', full_path] + jsonnet_path_args,\n                cwd=os.path.dirname(full_path))\n            try:\n              parsed = json.loads(output)\n            except AttributeError:\n              logging.error(\n                  ""Output of jsonnet eval could not be parsed as json; ""\n                  ""output: %s"", output)\n              parsed = {}\n\n            if not hasattr(parsed, ""get""):\n              # Legacy style tests emit true rather than a json object.\n              # Parsing the string as json converts it to a bool so we\n              # just use parsed as test_passed\n              # Old style tests actually use std.assert so jsonnet eval\n              # will actually return an error in the case the test didn\'t\n              # pass.\n              logging.warn(\n                  ""jsonnet test is using old style and not emitting an object. ""\n                  ""Result was: %s. Output will be treated as a boolean"", output)\n              test_passed = parsed\n            else:\n              test_passed = parsed.get(""pass"", False)\n\n            if not test_passed:\n              test_case.add_failure_info(\'{} test failed\'.format(test_file))\n              logging.error(\n                  \'%s test failed. See Subprocess output for details.\',\n                  test_file)\n          except Exception as e:\n            test_case.add_failure_info(\'{} test failed\'.format(test_file))\n            logging.error(\n                \'%s test failed with exception %s. \'\n                \'See Subprocess output for details.\', e, test_file)\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--test_files_dirs"",\n      default="""",\n      type=str,\n      help=""Comma separated directories where test jsonnet test files are ""\n      ""stored"")\n  parser.add_argument(\n      ""--jsonnet_path_dirs"",\n      default="""",\n      type=str,\n      help=""Comma separated directories used by jsonnet to find additional ""\n      ""libraries"")\n\n  parser.add_argument(\n      ""--exclude_dirs"",\n      default="""",\n      type=str,\n      help=""Comma separated directories which should be excluded from the test"")\n\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef test_jsonnet(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n\n  if not args.test_files_dirs:\n    raise ValueError(\'--test_files_dirs needs to be set\')\n\n  test_files_dirs = args.test_files_dirs.split(\',\')\n\n  jsonnet_path_args = []\n  if len(args.jsonnet_path_dirs) > 0:\n    for jsonnet_path_dir in args.jsonnet_path_dirs.split(\',\'):\n      jsonnet_path_args.append(\'--jpath\')\n      jsonnet_path_args.append(jsonnet_path_dir)\n\n  exclude_dirs = []\n  if args.exclude_dirs:\n    exclude_dirs = args.exclude_dirs.split(\',\')\n\n  run(test_files_dirs, jsonnet_path_args, exclude_dirs, test_case)\n\n\nif __name__ == ""__main__"":\n  # TODO(https://github.com/kubeflow/kubeflow/issues/4159):\n  # quick hack to disable the failing test.\n  print(""Error: skipping test_jsonnet because it is failing ""\n        ""https://github.com/kubeflow/kubeflow/issues/4159"")\n  # test_case = test_helper.TestCase(name=\'test_jsonnet\', test_func=test_jsonnet)\n  # test_suite = test_helper.init(\n  #    name=\'jsonnet_test_suite\', test_cases=[test_case])\n  # test_suite.run()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/test_tf_serving.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nimport numbers\nimport os\nimport time\n\nfrom six.moves import xrange\n\nfrom grpc.beta import implementations\nfrom kubernetes import client as k8s_client\nimport requests\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\nfrom kubeflow.testing import test_util\nfrom kubeflow.testing import util\n\n\ndef almost_equal(a, b, tol=0.001):\n  """"""\n  Compares two json objects (assuming same structure) with tolerance on numbers\n  """"""\n  if isinstance(a, dict):\n    for key in a.keys():\n      if not almost_equal(a[key], b[key]):\n        return False\n    return True\n  elif isinstance(a, list):\n    for i in xrange(len(a)):\n      if not almost_equal(a[i], b[i]):\n        return False\n    return True\n  elif isinstance(a, numbers.Number):\n    return abs(a - b) < tol\n  else:\n    return a == b\n\n\ndef main():\n  parser = argparse.ArgumentParser(\'Label an image using Inception\')\n  parser.add_argument(\n      \'-p\',\n      \'--port\',\n      type=int,\n      default=9000,\n      help=\'Port at which Inception model is being served\')\n  parser.add_argument(\n      ""--namespace"", required=True, type=str, help=(""The namespace to use.""))\n  parser.add_argument(\n      ""--service_name"",\n      required=True,\n      type=str,\n      help=(""The TF serving service to use.""))\n  parser.add_argument(\n      ""--artifacts_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for artifacts that should be preserved after ""\n      ""the test runs. Defaults to test_dir if not set."")\n  parser.add_argument(\n      ""--input_path"", required=True, type=str, help=(""The input file to use.""))\n  parser.add_argument(""--result_path"", type=str, help=(""The expected result.""))\n  parser.add_argument(\n      ""--workflow_name"",\n      default=""tfserving"",\n      type=str,\n      help=""The name of the workflow."")\n\n  args = parser.parse_args()\n\n  t = test_util.TestCase()\n  t.class_name = ""Kubeflow""\n  t.name = args.workflow_name + ""-"" + args.service_name\n\n  start = time.time()\n\n  util.load_kube_config(persist_config=False)\n  api_client = k8s_client.ApiClient()\n  core_api = k8s_client.CoreV1Api(api_client)\n  try:\n    with open(args.input_path) as f:\n      instances = json.loads(f.read())\n\n    service = core_api.read_namespaced_service(args.service_name,\n                                               args.namespace)\n    service_ip = service.spec.cluster_ip\n    model_urls = [\n        ""http://"" + service_ip +\n        "":8500/v1/models/mnist:predict"",  # tf serving\'s http server\n    ]\n    for model_url in model_urls:\n      logging.info(""Try predicting with endpoint {}"".format(model_url))\n      num_try = 1\n      result = None\n      while True:\n        try:\n          result = requests.post(model_url, json=instances)\n          assert (result.status_code == 200)\n        except Exception as e:\n          num_try += 1\n          if num_try > 10:\n            raise\n          logging.info(\'prediction failed: {}. Retrying...\'.format(e))\n          time.sleep(5)\n        else:\n          break\n      logging.info(\'Got result: {}\'.format(result.text))\n      if args.result_path:\n        with open(args.result_path) as f:\n          expected_result = json.loads(f.read())\n          logging.info(\'Expected result: {}\'.format(expected_result))\n          assert (almost_equal(expected_result, json.loads(result.text)))\n  except Exception as e:\n    t.failure = ""Test failed; "" + e.message\n    raise\n  finally:\n    t.time = time.time() - start\n    junit_path = os.path.join(\n        args.artifacts_dir,\n        ""junit_kubeflow-tf-serving-image-{}.xml"".format(args.service_name))\n    logging.info(""Writing test results to %s"", junit_path)\n    test_util.create_junit_xml_file([t], junit_path)\n    # Pause to collect Stackdriver logs.\n    time.sleep(60)\n\n\nif __name__ == \'__main__\':\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/tf_job_simple_test.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTest tf_job_simple prototype. It creates a component from the tf_job_simple\nprototype and applies it to the k8s cluster. It then verifies that two pods\nand services with the appropriate label get created.\n""""""\n\nimport argparse\nimport logging\nimport os\nimport re\nimport subprocess\nfrom kubeflow.testing import test_helper, util\nfrom kubernetes import client as k8s_client\nfrom kubeflow.tf_operator import tf_job_client\nfrom retrying import retry\n\nNAMESPACE = ""default""\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--src_dir"", default="""", type=str, help=""The kubeflow src directory"")\n  parser.add_argument(\n      ""--tf_job_version"",\n      default=""v1"",\n      type=str,\n      help=""Which TFJob version to use"")\n  args, _ = parser.parse_known_args()\n  return args\n\n\n@retry(stop_max_attempt_number=3)\ndef create_app_and_job(args, namespace, name):\n  # The ksonnet binary\n  ks = ""ks-13""\n  try:\n    util.run([\n        ks, ""init"", ""tf-job-simple-app"", ""--skip-default-registries"",\n        ""--namespace="" + namespace\n    ])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the app already exists. This is a sign the a previous\n    # attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  os.chdir(""tf-job-simple-app"")\n  try:\n    util.run([ks, ""registry"", ""add"", ""kubeflow"", args.src_dir + ""/kubeflow""])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the registry has already been added.\n    # This is a sign the a previous attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  try:\n    util.run([ks, ""pkg"", ""install"", ""kubeflow/examples""])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the package has already been added.\n    # This is a sign the a previous attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  if args.tf_job_version == ""v1"":\n    prototype_name = ""tf-job-simple-v1""\n  elif args.tf_job_version == ""v1beta2"":\n    prototype_name = ""tf-job-simple-v1beta2""\n  else:\n    raise ValueError(\n        ""Unrecognized value for tf_job_version: %s"" % args.tf_job_version)\n\n  util.run([ks, ""generate"", prototype_name, name])\n  util.run([ks, ""apply"", ""default"", ""-c"", ""tf-job-simple""])\n\n\ndef test_tf_job_simple(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n  namespace = ""default""\n  name = ""tf-job-simple""\n\n  util.load_kube_config()\n  api_client = k8s_client.ApiClient()\n  create_app_and_job(args, namespace, name)\n  try:\n    tf_job_client.wait_for_condition(\n        api_client,\n        namespace,\n        name, [""Running""],\n        status_callback=tf_job_client.log_status)\n    logging.info(""TFJob launched successfully"")\n  except Exception as e:\n    logging.error(""Test failed waiting for job; %s"", e)\n    test_case.add_failure_info(e.message)\n\n\nif __name__ == ""__main__"":\n  test_case = test_helper.TestCase(\n      name=""test_tf_job_simple"", test_func=test_tf_job_simple)\n  test_suite = test_helper.init(\n      name=""test_tf_job_simple"", test_cases=[test_case])\n  test_suite.run()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/vm_util.py,0,"b'# -*- coding: utf-8 -*-\n""""""Utilities for working with VMs as part of our tests.""""""\n\nimport datetime\nimport logging\nimport os\nimport socket\nimport ssl\nimport subprocess\nimport time\nimport uuid\n\nfrom kubeflow.testing import util\n\n# TODO(jlewi): Should we move this to kubeflow/testing\n\n\ndef wait_for_operation(client,\n                       project,\n                       zone,\n                       op_id,\n                       timeout=datetime.timedelta(hours=1),\n                       polling_interval=datetime.timedelta(seconds=5)):\n  """"""\n  Wait for the specified operation to complete.\n\n  Args:\n    client: Client for the API that owns the operation.\n    project: project\n    zone: Zone. Set to none if its a global operation\n    op_id: Operation id/name.\n    timeout: A datetime.timedelta expressing the amount of time to wait before\n        giving up.\n    polling_interval: A datetime.timedelta to represent the amount of time to\n        wait between requests polling for the operation status.\n\n  Returns:\n    op: The final operation.\n\n  Raises:\n    TimeoutError: if we timeout waiting for the operation to complete.\n  """"""\n  endtime = datetime.datetime.now() + timeout\n  while True:\n    try:\n      if zone:\n        op = client.zoneOperations().get(\n            project=project, zone=zone, operation=op_id).execute()\n      else:\n        op = client.globalOperations().get(\n            project=project, operation=op_id).execute()\n    except socket.error as e:\n      logging.error(""Ignoring error %s"", e)\n      continue\n    except ssl.SSLError as e:\n      logging.error(""Ignoring error %s"", e)\n      continue\n    status = op.get(""status"", """")\n    # Need to handle other status\'s\n    if status == ""DONE"":\n      return op\n    if datetime.datetime.now() > endtime:\n      raise TimeoutError(\n          ""Timed out waiting for op: {0} to complete."".format(op_id))\n    time.sleep(polling_interval.total_seconds())\n\n\ndef wait_for_vm(project,\n                zone,\n                vm,\n                timeout=datetime.timedelta(minutes=5),\n                polling_interval=datetime.timedelta(seconds=10)):\n  """"""\n  Wait for the VM to be ready. This is measured by trying to ssh into the VM\n\n  timeout: A datetime.timedelta expressing the amount of time to wait\n      before giving up.\n  polling_interval: A datetime.timedelta to represent the amount of time\n        to wait between requests polling for the operation status.\n  Raises:\n    TimeoutError: if we timeout waiting for the operation to complete.\n  """"""\n  endtime = datetime.datetime.now() + timeout\n  while True:\n    try:\n      util.run([\n          ""gcloud"", ""compute"", ""--project="" + project, ""ssh"", ""--zone="" + zone,\n          vm, ""--"", ""echo hello world""\n      ])\n      logging.info(""VM is ready"")\n      return\n    except subprocess.CalledProcessError:\n      pass\n\n    if datetime.datetime.now() > endtime:\n      raise util.TimeoutError(\n          (""Timed out waiting for VM to {0} be sshable. Check firewall""\n           ""rules aren\'t blocking ssh."").format(vm))\n\n    time.sleep(polling_interval.total_seconds())\n\n\ndef execute(project, zone, vm, commands):\n  """"""Execute the supplied commands on the VM.""""""\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + project, ""ssh"", ""--zone="" + zone, vm,\n      ""--"", "" && "".join(commands)\n  ])\n\n\ndef execute_script(project, zone, vm, script):\n  """"""Execute the specified script on the VM.""""""\n\n  target_path = os.path.join(\n      ""/tmp"",\n      os.path.basename(script) + ""."" + uuid.uuid4().hex[0:4])\n\n  target = ""{0}:{1}"".format(vm, target_path)\n  logging.info(""Copying %s to %s"", script, target)\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + project, ""scp"", script, target,\n      ""--zone="" + zone\n  ])\n\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + project, ""ssh"", ""--zone="" + zone, vm,\n      ""--"", ""chmod a+rx "" + target_path + "" && "" + target_path\n  ])\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/wait_for_deployment.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Wait for kubeflow deployment.\n\nRight now, it only checks for the presence of tfjobs and pytorchjobs crd. More\nthings can be added incrementally.\npython -m testing.wait_for_deployment --cluster=kubeflow-testing \\\n    --project=kubeflow-ci --zone=us-east1-d --timeout=3\n\nTODO(jlewi): Waiting for the CRD\'s to be created probably isn\'t that useful.\nI think that will be nearly instantaneous. If we\'re going to wait for something\nit should probably be waiting for the controllers to actually be deployed.\nWe can probably get rid of this and just use wait_for_kubeflow.py.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport datetime\nimport logging\nimport subprocess\nimport time\n\nfrom kubeflow.testing import test_helper, util\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--timeout"", default=5, type=int, help=""Timeout in minutes"")\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef wait_for_resource(resource, end_time):\n  while True:\n    if datetime.datetime.now() > end_time:\n      raise RuntimeError(""Timed out waiting for "" + resource)\n    try:\n      if \'error\' not in util.run([""kubectl"", ""get"", resource]).lower():\n        logging.info(""Found "" + resource)\n        break\n    except subprocess.CalledProcessError as e:\n      logging.info(\n          ""Could not find {}. Sleeping for 10 seconds.."".format(resource))\n      time.sleep(10)\n\n\ndef test_wait_for_deployment(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n  util.maybe_activate_service_account()\n  util.load_kube_config()\n  end_time = datetime.datetime.now() + datetime.timedelta(0, args.timeout * 60)\n  wait_for_resource(""crd/tfjobs.kubeflow.org"", end_time)\n  wait_for_resource(""crd/pytorchjobs.kubeflow.org"", end_time)\n  wait_for_resource(""crd/studyjobs.kubeflow.org"", end_time)\n  logging.info(""Found all resources successfully"")\n\n\nif __name__ == ""__main__"":\n  test_case = test_helper.TestCase(\n      name=""test_wait_for_deployment"", test_func=test_wait_for_deployment)\n  test_suite = test_helper.init(name="""", test_cases=[test_case])\n  test_suite.run()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/wait_for_kubeflow.py,0,"b'""""""Wait for Kubeflow to be deployed.\n\n\nTODO(jlewi): With 0.5 and kfctl go binary this test is replaced by\nkf_is_ready_test.py.\n""""""\nimport argparse\nimport logging\n\nfrom testing import deploy_utils\nfrom kubeflow.testing import test_helper\nfrom kubeflow.testing import util  # pylint: disable=no-name-in-module\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--namespace"", default=None, type=str, help=(""The namespace to use.""))\n\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef deploy_kubeflow(_):\n  """"""Deploy Kubeflow.""""""\n  args = parse_args()\n  namespace = args.namespace\n  api_client = deploy_utils.create_k8s_client()\n\n  util.load_kube_config()\n\n  # Verify that Jupyter is actually deployed.\n  jupyter_name = ""jupyter""\n  logging.info(""Verifying TfHub started."")\n  util.wait_for_statefulset(api_client, namespace, jupyter_name)\n\n  # Verify that core components are actually deployed.\n  deployment_names = [\n      ""tf-job-operator"", ""pytorch-operator"", ""studyjob-controller""\n  ]\n  for deployment_name in deployment_names:\n    logging.info(""Verifying that %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, namespace, deployment_name)\n\n\ndef main():\n  test_case = test_helper.TestCase(\n      name=\'deploy_kubeflow\', test_func=deploy_kubeflow)\n  test_suite = test_helper.init(name=\'deploy_kubeflow\', test_cases=[test_case])\n  test_suite.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
kubeflow/install-kubeflow/ks_app/vendor/kubeflow/jupyter/jupyter_config.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nConfiguration file for JupyterHub.\n\nKubeflow uses this file as the configuration file for JupyterHub. It contains\nall glue code necessary to integrate JupyterHub with the remaining Kubeflow\ncomponents.\n\nNote that this file is also responsible for importing the UI-specific Spawner\nclass from <ui-dir>/spawner.py, and setting the `spawner_class` configuration\noption.\n""""""\n\nimport os\nfrom importlib.util import spec_from_file_location, module_from_spec\nfrom jhub_remote_user_authenticator.remote_user_auth import \\\n    RemoteUserAuthenticator\n\nSERVICE_ACCOUNT_SECRET_MOUNT = \'/var/run/secrets/sa\'\n\n# Import the UI-specific Spawner\nspec = spec_from_file_location(\'spawner\', \'/etc/config/spawner.py\')\nspawner = module_from_spec(spec)\nspec.loader.exec_module(spawner)\n\n###################################################\n# JupyterHub Options\n###################################################\nc.JupyterHub.ip = \'0.0.0.0\'\nc.JupyterHub.hub_ip = \'0.0.0.0\'\n# Don\'t try to cleanup servers on exit - since in general for k8s, we want\n# the hub to be able to restart without losing user containers\nc.JupyterHub.cleanup_servers = False\n###################################################\n\n###################################################\n# Spawner Options\n###################################################\nc.JupyterHub.spawner_class = spawner.KubeFormSpawner\n\nc.KubeSpawner.cmd = \'start-singleuser.sh\'\nc.KubeSpawner.args = [\'--allow-root\']\n# gpu images are very large ~15GB. need a large timeout.\nc.KubeSpawner.start_timeout = 60 * 30\n# Increase timeout to 5 minutes to avoid HTTP 500 errors on JupyterHub\nc.KubeSpawner.http_timeout = 60 * 5\n\n# Volume setup\nc.KubeSpawner.singleuser_uid = 1000\nc.KubeSpawner.singleuser_fs_gid = 100\nc.KubeSpawner.singleuser_working_dir = \'/home/jovyan\'\n\n# Allow environment vars to override uid and gid.\n# This allows local host path mounts to be read/writable\nenv_uid = os.environ.get(\'NOTEBOOK_UID\')\nif env_uid:\n  c.KubeSpawner.singleuser_uid = int(env_uid)\nenv_gid = os.environ.get(\'NOTEBOOK_GID\')\nif env_gid:\n  c.KubeSpawner.singleuser_fs_gid = int(env_gid)\naccess_local_fs = os.environ.get(\'ACCESS_LOCAL_FS\')\nif access_local_fs == \'true\':\n\n  def modify_pod_hook(spawner, pod):\n    pod.spec.containers[0].lifecycle = {\n        \'postStart\': {\n            \'exec\': {\n                \'command\': [\n                    \'ln\', \'-s\', \'/mnt/local-notebooks\',\n                    \'/home/jovyan/local-notebooks\'\n                ]\n            }\n        }\n    }\n    return pod\n\n  c.KubeSpawner.modify_pod_hook = modify_pod_hook\n\n###################################################\n# Persistent volume options\n###################################################\n\n# Set user_storage_pvc_ensure to False to prevent KubeSpawner from handling PVCs\n# We natively handle PVCs via KubeFormSpawner and its dedicated methods\n\n# NOTE: user_storage_pvc_ensure has been deprecated in a future release\nc.KubeSpawner.storage_pvc_ensure = False\nc.KubeSpawner.user_storage_pvc_ensure = False\n\nvolumes = []\nvolume_mounts = []\n\ngcp_secret_name = os.environ.get(\'GCP_SECRET_NAME\')\nif gcp_secret_name:\n  volumes.append({\n      \'name\': gcp_secret_name,\n      \'secret\': {\n          \'secretName\': gcp_secret_name,\n      }\n  })\n  volume_mounts.append({\n      \'name\': gcp_secret_name,\n      \'mountPath\': SERVICE_ACCOUNT_SECRET_MOUNT\n  })\n\nc.KubeSpawner.volumes = volumes\nc.KubeSpawner.volume_mounts = volume_mounts\n\nstorage_class = None\nif os.environ.get(\'STORAGE_CLASS\') != \'null\':\n  storage_class = os.environ.get(\'STORAGE_CLASS\')\n\nrok_secret_name = \'\'\nif os.environ.get(\'ROK_SECRET_NAME\') != \'null\':\n  rok_secret_name = os.environ.get(\'ROK_SECRET_NAME\')\n\n# Set both service_account and singleuser_service_account because\n# singleuser_service_account has been deprecated in a future release\nc.KubeSpawner.service_account = \'jupyter-notebook\'\nc.KubeSpawner.singleuser_service_account = \'jupyter-notebook\'\n# Authenticator\nif os.environ.get(\'KF_AUTHENTICATOR\') == \'iap\':\n  c.JupyterHub.authenticator_class = RemoteUserAuthenticator\n  c.RemoteUserAuthenticator.header_name = \'x-goog-authenticated-user-email\'\nelse:\n  c.JupyterHub.authenticator_class = \'dummyauthenticator.DummyAuthenticator\'\n\nif os.environ.get(\'DEFAULT_JUPYTERLAB\').lower() == \'true\':\n  c.KubeSpawner.default_url = \'/lab\'\n\n# Set extra spawner configuration variables\nc.KubeSpawner.extra_spawner_config = {\n    \'gcp_secret_name\': gcp_secret_name,\n    \'storage_class\': storage_class,\n    \'rok_secret_name\': rok_secret_name,\n}\n'"
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/clean_text/Transformer.py,0,"b'import re \nfrom html.parser import HTMLParser\nimport numpy as np\nimport logging\n\nclass Transformer():\n    __html_parser = HTMLParser()\n    __uplus_pattern = \\\n        re.compile(""\\<[uU]\\+(?P<digit>[a-zA-Z0-9]+)\\>"")\n    __markup_link_pattern = \\\n        re.compile(""\\[(.*)\\]\\((.*)\\)"")\n\n    def predict(self, X, feature_names=[]):\n        logging.warning(X)\n        f = np.vectorize(Transformer.transform_clean_text)\n        X_clean = f(X)\n        logging.warning(X_clean)\n        return X_clean\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    @staticmethod\n    def transform_clean_text(raw_text):\n        try:\n            decoded = raw_text.encode(""ISO-8859-1"").decode(""utf-8"")\n        except:\n            decoded = raw_text.encode(""ISO-8859-1"").decode(""cp1252"")\n        html_unescaped =Transformer.\\\n            __html_parser.unescape(decoded) \n        html_unescaped = re.sub(r""\\r\\n"", "" "", html_unescaped)\n        html_unescaped = re.sub(r""\\r\\r\\n"", "" "", html_unescaped)\n        html_unescaped = re.sub(r""\\r"", "" "", html_unescaped)\n        html_unescaped = html_unescaped.replace(""&gt;"", "" > "")\n        html_unescaped = html_unescaped.replace(""&lt;"", "" < "")\n        html_unescaped = html_unescaped.replace(""--"", "" - "")\n        html_unescaped = Transformer.__uplus_pattern.sub(\n            "" U\\g<digit> "", html_unescaped)\n        html_unescaped = Transformer.__markup_link_pattern.sub(\n            "" \\1 \\2 "", html_unescaped)\n        html_unescaped = html_unescaped.replace(""\\\\"", """")\n        return html_unescaped\n\n'"
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/clean_text/__init__.py,0,b''
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/clean_text/pipeline_step.py,0,"b'import dill\nimport click\nimport dill\ntry:\n    # Running for tests\n    from .Transformer import Transformer\nexcept:\n    # Running from CLI\n    from Transformer import Transformer\n\n@click.command()\n@click.option(\'--in-path\', default=""/mnt/raw_text.data"")\n@click.option(\'--out-path\', default=""/mnt/clean_text.data"")\ndef run_pipeline(in_path, out_path):\n    clean_text_transformer = Transformer()\n    with open(in_path, \'rb\') as in_f:\n        x = dill.load(in_f)\n    y = clean_text_transformer.predict(x)\n    with open(out_path, ""wb"") as out_f:\n        dill.dump(y, out_f)\n\nif __name__ == ""__main__"":\n    run_pipeline()\n\n'"
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/data_downloader/__init__.py,0,b''
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/data_downloader/pipeline_step.py,0,"b'import click\nimport numpy as np\nimport dill\nimport pandas as pd\n\n@click.command()\n@click.option(\'--labels-path\', default=""/mnt/labels.data"")\n@click.option(\'--features-path\', default=""/mnt/features.data"")\n@click.option(\'--csv-url\', default=""https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv"")\n@click.option(\'--csv-encoding\', default=""ISO-8859-1"")\n@click.option(\'--features-column\', default=""BODY"")\n@click.option(\'--labels-column\', default=""REMOVED"")\ndef run_pipeline(\n        labels_path, \n        features_path,\n        csv_url, \n        csv_encoding,\n        features_column,\n        labels_column):\n\n    df = pd.read_csv(csv_url, encoding=csv_encoding)\n\n    x = df[features_column].values\n\n    with open(features_path, ""wb"") as out_f:\n        dill.dump(x, out_f)\n\n    y = df[labels_column].values\n\n    with open(labels_path, ""wb"") as out_f:\n        dill.dump(y, out_f)\n\nif __name__ == ""__main__"":\n    run_pipeline()\n\n'"
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/lr_text_classifier/Transformer.py,0,"b""\nimport dill\nimport logging\n\nclass Transformer(object):\n    def __init__(self):\n\n        with open('/mnt/lr.model', 'rb') as model_file:\n            self._lr_model = dill.load(model_file)\n\n    def predict(self, X, feature_names):\n        logging.warning(X)\n        prediction = self._lr_model.predict_proba(X)\n        logging.warning(prediction)\n        return prediction\n\n\n"""
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/lr_text_classifier/__init__.py,0,b''
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/lr_text_classifier/pipeline_step.py,0,"b'import click\nimport numpy as np\nimport dill\nfrom sklearn.linear_model import LogisticRegression\n\n@click.command()\n@click.option(\'--in-path\', default=""/mnt/tfidf_vectors.data"")\n@click.option(\'--labels-path\', default=""/mnt/labels.data"")\n@click.option(\'--out-path\', default=""/mnt/lr_prediction.data"")\n@click.option(\'--c-param\', default=0.1)\n@click.option(\'--action\', default=""predict"", \n        type=click.Choice([\'predict\', \'train\']))\n@click.option(\'--model-path\', default=""/mnt/lr_text.model"")\ndef run_pipeline(\n        in_path, \n        labels_path,\n        out_path, \n        c_param,\n        action,\n        model_path):\n\n    with open(in_path, \'rb\') as in_f:\n        x = dill.load(in_f)\n\n    if action == ""train"":\n        lr_model = LogisticRegression(\n                C=0.1, \n                solver=\'sag\')\n\n        with open(labels_path, ""rb"") as f:\n            labels = dill.load(f)\n\n        lr_model.fit(x, labels)\n\n        with open(model_path, ""wb"") as model_f:\n            dill.dump(lr_model, model_f)\n\n    elif action == ""predict"":\n        with open(model_path, ""rb"") as model_f:\n            lr_model = dill.load(model_f)\n\n    y = lr_model.predict_proba(x)\n\n    with open(out_path, ""wb"") as out_f:\n        dill.dump(y, out_f)\n\nif __name__ == ""__main__"":\n    run_pipeline()\n\n'"
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/spacy_tokenize/Transformer.py,0,"b'import spacy\nimport numpy as np\nimport logging\n\n# from spacy.cli import download\n# import importlib\n# download(""en_core_web_sm"")\n# importlib.reload(spacy)\n\nnlp = spacy.load(\'en_core_web_sm\', parser=False, entity=False)\n    \nclass Transformer():\n    __symbols = set(""!$%^&*()_+|~-=`{}[]:\\"";\'<>?,./-"")\n\n    def predict(self, X, feature_names=[]):\n        logging.warning(X)\n        f = np.vectorize(Transformer.transform_to_token, otypes=[object])\n        X_tokenized = f(X)\n        logging.warning(X_tokenized)\n        return X_tokenized\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    @staticmethod\n    def transform_to_token(text):\n        str_text = str(text)\n        doc = nlp(str_text, disable=[\'parser\', \'tagger\', \'ner\'])\n        tokens = []\n        for token in doc:\n            if token.like_url:\n                clean_token = ""URL""\n            else:\n                clean_token = token.lemma_.lower().strip()\n                if len(clean_token) < 1 or clean_token in \\\n                        Transformer.__symbols: \n                    continue\n            tokens.append(clean_token)\n        return tokens\n\n'"
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/spacy_tokenize/__init__.py,0,b''
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/spacy_tokenize/pipeline_step.py,0,"b'import click\nimport dill\ntry:\n    # Running for tests\n    from .Transformer import Transformer\nexcept:\n    # Running from CLI\n    from Transformer import Transformer\n\n@click.command()\n@click.option(\'--in-path\', default=""/mnt/clean_text.data"")\n@click.option(\'--out-path\', default=""/mnt/tokenized_text.data"")\ndef run_pipeline(in_path, out_path):\n    spacy_transformer = Transformer()\n    with open(in_path, \'rb\') as in_f:\n        x = dill.load(in_f)\n    y = spacy_transformer.predict(x)\n    with open(out_path, ""wb"") as out_f:\n        dill.dump(y, out_f)\n\nif __name__ == ""__main__"":\n    run_pipeline()\n\n'"
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/tfidf_vectorizer/Transformer.py,0,"b""\nimport dill\nimport logging\n\nclass Transformer(object):\n    def __init__(self):\n\n        with open('/mnt/tfidf.model', 'rb') as model_file:\n            self._tfidf_vectorizer = dill.load(model_file)\n\n    def predict(self, X, feature_names):\n        logging.warning(X)\n        tfidf_sparse = self._tfidf_vectorizer.transform(X)\n        tfidf_array = tfidf_sparse.toarray()\n        logging.warning(tfidf_array)\n        return tfidf_array\n\n\n"""
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/tfidf_vectorizer/__init__.py,0,b''
kubeflow/kubeflow-pipelines/nlp/pipeline/pipeline_steps/tfidf_vectorizer/pipeline_step.py,0,"b'import click\nimport numpy as np\nimport dill\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n@click.command()\n@click.option(\'--in-path\', default=""/mnt/tokenized_text.data"")\n@click.option(\'--out-path\', default=""/mnt/tfidf_vectors.data"")\n@click.option(\'--max-features\', default=1000)\n@click.option(\'--ngram-range\', default=3)\n@click.option(\'--action\', default=""predict"", \n        type=click.Choice([\'predict\', \'train\']))\n@click.option(\'--model-path\', default=""/mnt/tfidf.model"")\ndef run_pipeline(\n        in_path, \n        out_path, \n        max_features,\n        ngram_range,\n        action,\n        model_path):\n\n    with open(in_path, \'rb\') as in_f:\n        x = dill.load(in_f)\n\n    if action == ""train"":\n        tfidf_vectorizer = TfidfVectorizer(\n            max_features=max_features,\n            preprocessor=lambda x: x, # We\'re using cleantext\n            tokenizer=lambda x: x, # We\'re using spacy\n            token_pattern=None,\n            ngram_range=(1, ngram_range))\n\n        tfidf_vectorizer.fit(x)\n        with open(model_path, ""wb"") as model_f:\n            dill.dump(tfidf_vectorizer, model_f)\n\n    elif action == ""predict"":\n        with open(model_path, ""rb"") as model_f:\n            tfidf_vectorizer = dill.load(model_f)\n\n    y = tfidf_vectorizer.transform(x)\n\n    with open(out_path, ""wb"") as out_f:\n        dill.dump(y, out_f)\n\nif __name__ == ""__main__"":\n    run_pipeline()\n\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/bootstrap/hack/send_ks_request.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\nA script for manual testing and experimenting with the ks server.\n\nTODO(jlewi): Should we use this as the basis for doing E2E integration testing?\nWe can run the server in a subprocess. Send requests to it and then run various\nchecks on the results.\n""""""\nimport argparse\nimport datetime\nimport logging\nimport requests\n\n\ndef main():\n  parser = argparse.ArgumentParser(\n      description=""Script to test sending requests to the ksonnet server."")\n\n  parser.add_argument(\n      ""--endpoint"",\n      default=""http://localhost:8080"",\n      type=str,\n      help=""The endpoint of the server"")\n\n  args = parser.parse_args()\n\n  create_endpoint = args.endpoint + ""/apps/create""\n\n  now = datetime.datetime.now()\n\n  data = {\n      ""Name"": ""test-app-"" + now.strftime(""%Y%m%d-%H%M%S""),\n      ""AppConfig"": {\n          ""Registries"": [\n              {\n                  ""Name"": ""kubeflow"",\n                  ""RegUri"": ""/home/jlewi/git_kubeflow/kubeflow"",\n              },\n          ],\n          ""Packages"": [{\n              ""Name"": ""core"",\n              ""Registry"": ""kubeflow"",\n          }],\n      },\n      ""Namespace"": ""kubeflow"",\n      ""AutoConfigure"": False,\n  }\n  r = requests.post(create_endpoint, json=data)\n  if r.status_code != requests.codes.OK:\n    logging.error(""Request failed: status_code: %s"", r.status_code)\n\n  logging.info(""Result Body: %s"", r.content)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/build/boilerplate/boilerplate.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2016 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport copy\nimport os\nimport re\nimport sys\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""filenames"",\n    help=""list of files to check, all files if unspecified"",\n    nargs=\'*\')\nargs = parser.parse_args()\n\nrootdir = os.path.dirname(__file__) + ""/../../""\nrootdir = os.path.abspath(rootdir)\n\n\ndef get_refs():\n  ref_file = open(os.path.join(rootdir, ""build/boilerplate/boilerplate.txt""))\n  ref = ref_file.read().splitlines()\n  ref_file.close()\n  refs = {}\n  for extension in [""sh"", ""go"", ""py""]:\n    refs[extension] = copy.copy(ref)\n    prefix = """"\n    if extension == ""go"":\n      prefix = ""//""\n    else:\n      prefix = ""#""\n    for i in range(len(refs[extension])):\n      if len(refs[extension][i]) != 0:\n        p = prefix + "" ""\n      else:\n        p = prefix\n      refs[extension][i] = p + refs[extension][i]\n  return refs\n\n\ndef file_passes(filename, refs, regexs):\n  try:\n    f = open(filename, \'r\')\n  except:  # noqa: E722\n    return False\n\n  data = f.read()\n  f.close()\n\n  extension = file_extension(filename)\n  ref = refs[extension]\n\n  # remove build tags from the top of Go files\n  if extension == ""go"":\n    p = regexs[""go_build_constraints""]\n    (data, found) = p.subn("""", data, 1)\n\n  # remove shebang from the top of shell files\n  if extension == ""sh"" or extension == ""py"":\n    p = regexs[""shebang""]\n    (data, found) = p.subn("""", data, 1)\n\n  data = data.splitlines()\n\n  # if our test file is smaller than the reference it surely fails!\n  if len(ref) > len(data):\n    return False\n\n  # trim our file to the same number of lines as the reference file\n  data = data[:len(ref)]\n\n  p = regexs[""year""]\n  for d in data:\n    if p.search(d):\n      return False\n\n  # Replace all occurrences of the regex ""2016|2015|2014"" with ""YEAR""\n  p = regexs[""date""]\n  for i, d in enumerate(data):\n    (data[i], found) = p.subn(\'YEAR\', d)\n    if found != 0:\n      break\n\n  # if we don\'t match the reference at this point, fail\n  if ref != data:\n    return False\n\n  return True\n\n\ndef file_extension(filename):\n  return os.path.splitext(filename)[1].split(""."")[-1].lower()\n\n\nskipped_dirs = [\'Godeps\', \'vendor\', \'third_party\', \'_gopath\', \'_output\', \'.git\']\n\n\ndef normalize_files(files):\n  newfiles = []\n  for pathname in files:\n    if any(x in pathname for x in skipped_dirs):\n      continue\n    newfiles.append(pathname)\n  for i, pathname in enumerate(newfiles):\n    if not os.path.isabs(pathname):\n      newfiles[i] = os.path.join(rootdir, pathname)\n  return newfiles\n\n\ndef get_files(extensions):\n  files = []\n  if len(args.filenames) > 0:\n    files = args.filenames\n  else:\n    for root, dirs, walkfiles in os.walk(rootdir):\n      # don\'t visit certain dirs. This is just a performance improvement as we\n      # would prune these later in normalize_files(). But doing it cuts down the\n      # amount of filesystem walking we do and cuts down the size of the file\n      # list\n      for d in skipped_dirs:\n        if d in dirs:\n          dirs.remove(d)\n\n      for name in walkfiles:\n        pathname = os.path.join(root, name)\n        files.append(pathname)\n\n  files = normalize_files(files)\n  outfiles = []\n  for pathname in files:\n    extension = file_extension(pathname)\n    if extension in extensions:\n      outfiles.append(pathname)\n  return outfiles\n\n\ndef get_regexs():\n  regexs = {}\n  # Search for ""YEAR"" which exists in the boilerplate, but shouldn\'t in the\n  # real thing\n  regexs[""year""] = re.compile(\'YEAR\')\n  # dates can be 2014, 2015 or 2016, company holder names can be anything\n  regexs[""date""] = re.compile(\'(2014|2015|2016|2017|2018|2019|2020)\')\n  # strip // +build \\n\\n build constraints\n  regexs[""go_build_constraints""] = re.compile(r""^(// \\+build.*\\n)+\\n"",\n                                              re.MULTILINE)\n  # strip #!.* from shell scripts\n  regexs[""shebang""] = re.compile(r""^(#!.*\\n)\\n*"", re.MULTILINE)\n  return regexs\n\n\ndef main():\n  regexs = get_regexs()\n  refs = get_refs()\n  filenames = get_files(refs.keys())\n\n  for filename in filenames:\n    if not file_passes(filename, refs, regexs):\n      print(filename, file=sys.stdout)\n\n\nif __name__ == ""__main__"":\n  sys.exit(main())\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/echo-server/main.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGoogle Cloud Endpoints sample application.\n\nDemonstrates how to create a simple echo API as well as how to deal with\nvarious authentication methods.\n""""""\n\nimport base64\nimport json\nimport logging\n\nfrom flask import Flask, jsonify, request\nfrom flask_cors import cross_origin\nfrom six.moves import http_client\n\napp = Flask(__name__)\n\n\ndef _base64_decode(encoded_str):\n  # Add paddings manually if necessary.\n  num_missed_paddings = 4 - len(encoded_str) % 4\n  if num_missed_paddings != 4:\n    encoded_str += b\'=\' * num_missed_paddings\n  return base64.b64decode(encoded_str).decode(\'utf-8\')\n\n\n@app.route(\'/echo\', methods=[\'POST\'])\ndef echo():\n  """"""Simple echo service.""""""\n  message = request.get_json().get(\'message\', \'\')\n  return jsonify({\'message\': message})\n\n\n@app.route(\'/\')\n@app.route(\'/headers\')\ndef headers():\n  return jsonify({\'headers\': request.headers.to_list()})\n\n\ndef auth_info():\n  """"""Retrieves the authenication information from Google Cloud Endpoints.""""""\n  encoded_info = request.headers.get(\'X-Endpoint-API-UserInfo\', None)\n\n  if encoded_info:\n    info_json = _base64_decode(encoded_info)\n    user_info = json.loads(info_json)\n  else:\n    user_info = {\'id\': \'anonymous\'}\n\n  return jsonify(user_info)\n\n\n@app.route(\'/auth/info/googlejwt\', methods=[\'GET\'])\ndef auth_info_google_jwt():\n  """"""Auth info with Google signed JWT.""""""\n  return auth_info()\n\n\n@app.route(\'/auth/info/googleidtoken\', methods=[\'GET\'])\ndef auth_info_google_id_token():\n  """"""Auth info with Google ID token.""""""\n  return auth_info()\n\n\n@app.route(\'/auth/info/firebase\', methods=[\'GET\'])\n@cross_origin(send_wildcard=True)\ndef auth_info_firebase():\n  """"""Auth info with Firebase auth.""""""\n  return auth_info()\n\n\n@app.errorhandler(http_client.INTERNAL_SERVER_ERROR)\ndef unexpected_error(e):\n  """"""Handle exceptions by returning swagger-compliant json.""""""\n  logging.exception(\'An error occurred while processing the request.\')\n  response = jsonify({\n      \'code\': http_client.INTERNAL_SERVER_ERROR,\n      \'message\': \'Exception: {}\'.format(e)\n  })\n  response.status_code = http_client.INTERNAL_SERVER_ERROR\n  return response\n\n\nif __name__ == \'__main__\':\n  # This is used when running locally. Gunicorn is used to run the application\n  # on Google App Engine. See entrypoint in app.yaml.\n  app.run(host=\'127.0.0.1\', port=8080, debug=True)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/echo-server/main_test.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport json\nimport os\nimport pytest\n\nimport main\n\n\n@pytest.fixture\ndef client(monkeypatch):\n  monkeypatch.chdir(os.path.dirname(main.__file__))\n  main.app.testing = True\n  client = main.app.test_client()\n  return client\n\n\ndef test_echo(client):\n  r = client.post(\n      \'/echo\',\n      data=\'{""message"": ""Hello""}\',\n      headers={\'Content-Type\': \'application/json\'})\n\n  assert r.status_code == 200\n  data = json.loads(r.data.decode(\'utf-8\'))\n  assert data[\'message\'] == \'Hello\'\n\n\ndef test_auth_info(client):\n  endpoints = [\n      \'/auth/info/googlejwt\', \'/auth/info/googleidtoken\', \'/auth/info/firebase\'\n  ]\n\n  encoded_info = base64.b64encode(json.dumps({\'id\': \'123\'}).encode(\'utf-8\'))\n\n  for endpoint in endpoints:\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    assert r.status_code == 200\n    data = json.loads(r.data.decode(\'utf-8\'))\n    assert data[\'id\'] == \'anonymous\'\n\n    r = client.get(\n        endpoint,\n        headers={\n            \'Content-Type\': \'application/json\',\n            \'X-Endpoint-API-UserInfo\': encoded_info\n        })\n\n    assert r.status_code == 200\n    data = json.loads(r.data.decode(\'utf-8\'))\n    assert data[\'id\'] == \'123\'\n\n\ndef test_cors(client):\n  r = client.options(\'/auth/info/firebase\', headers={\'Origin\': \'example.com\'})\n  assert r.status_code == 200\n  assert r.headers[\'Access-Control-Allow-Origin\'] == \'*\'\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/https-redirect/main.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple flask app to redirect all requests to https.""""""\n\nimport logging\n\nfrom flask import Flask, jsonify, redirect, request\n\napp = Flask(__name__)\n\n\n@app.route(\'/healthz\')\ndef health_check():\n  return jsonify({\'isHealthy\': True})\n\n\n@app.route(\'/\')\n@app.route(\'/<path:path>\')\ndef all_handler(path=None):\n  new_url = request.url\n  if request.scheme == ""http"":\n    prefix = ""http""\n    new_url = ""https"" + new_url[len(prefix):]\n  logging.info(""Redirecting to: %s"", new_url)\n\n  response = redirect(new_url)\n\n  # For ""/"" we return a 200 (ok) and not a 302 (redirect) because on GKE we want\n  # to be able to use this to redirect http://mydomain.com/ to\n  # https://mydomain.com/. However, the Ingress sets up the GCP loadbalancer\n  # health check requires that a 200 be served on ""/"". So if we return a 302\n  # the backend will be considered unhealthy.\n  if not path:\n    response.status_code = 200\n  return response\n\n\nif __name__ == \'__main__\':\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  app.run(host=\'127.0.0.1\', port=8080, debug=False)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/https-redirect/main_test.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport main\n\n\nclass TestRedirect(unittest.TestCase):\n\n  def test_non_empty_path(self):\n    main.app.testing = True\n    client = main.app.test_client()\n\n    endpoint = \'/hello/world\'\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    self.assertEqual(302, r.status_code)\n    self.assertEqual(\n        r.data,\n        \'<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">\\n<title>Redirecting...</title>\\n<h1>Redirecting...</h1>\\n<p>You should be redirected automatically to target URL: <a href=""https://localhost/hello/world"">https://localhost/hello/world</a>.  If not click the link.\'  # noqa: E501\n    )\n\n  def test_empty_path(self):\n    main.app.testing = True\n    client = main.app.test_client()\n\n    endpoint = \'/\'\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    self.assertEqual(200, r.status_code)\n    self.assertEqual(\n        r.data,\n        \'<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">\\n<title>Redirecting...</title>\\n<h1>Redirecting...</h1>\\n<p>You should be redirected automatically to target URL: <a href=""https://localhost/"">https://localhost/</a>.  If not click the link.\'  # noqa: E501\n    )\n\n  def test_health_check(self):\n    main.app.testing = True\n    client = main.app.test_client()\n\n    endpoint = \'/healthz\'\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    self.assertEqual(200, r.status_code)\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/tensorflow-notebook-image/jupyter_notebook_config.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2016 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom jupyter_core.paths import jupyter_data_dir\nimport subprocess\nimport os\nimport errno\nimport stat\n\nc = get_config()\nc.NotebookApp.ip = \'*\'\nc.NotebookApp.port = 8888\nc.NotebookApp.open_browser = False\nc.NotebookApp.allow_origin_pat = "".*-dot-devshell.appspot.com$""\n\n# Generate a self-signed certificate\nif \'GEN_CERT\' in os.environ:\n  dir_name = jupyter_data_dir()\n  pem_file = os.path.join(dir_name, \'notebook.pem\')\n  try:\n    os.makedirs(dir_name)\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(dir_name):\n      pass\n    else:\n      raise\n  # Generate a certificate if one doesn\'t exist on disk\n  subprocess.check_call([\n      \'openssl\', \'req\', \'-new\', \'-newkey\', \'rsa:2048\', \'-days\', \'365\', \'-nodes\',\n      \'-x509\', \'-subj\', \'/C=XX/ST=XX/L=XX/O=generated/CN=generated\', \'-keyout\',\n      pem_file, \'-out\', pem_file\n  ])\n  # Restrict access to the file\n  os.chmod(pem_file, stat.S_IRUSR | stat.S_IWUSR)\n  c.NotebookApp.certfile = pem_file\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/docs/gke/iap_request.py,0,"b'# -*- coding: utf-8 -*-\n# Adapted from\n# https://cloud.google.com/iap/docs/authentication-howto#iap-make-request-python\nimport argparse\nimport google.auth\nimport google.auth.app_engine\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nfrom google.auth.transport.requests import Request\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nimport requests\nimport requests_toolbelt.adapters.appengine\n\nIAM_SCOPE = \'https://www.googleapis.com/auth/iam\'\nOAUTH_TOKEN_URI = \'https://www.googleapis.com/oauth2/v4/token\'\n\n\ndef get_service_account_token(client_id):\n  """"""\n  Get open id connect token for default service account.\n\n  Returns:\n    The open id connect token for default service account.\n  """"""\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  bootstrap_credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(bootstrap_credentials, google.oauth2.credentials.Credentials):\n    raise Exception(\'make_iap_request is only supported for service \'\n                    \'accounts.\')\n  elif isinstance(bootstrap_credentials, google.auth.app_engine.Credentials):\n    requests_toolbelt.adapters.appengine.monkeypatch()\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  bootstrap_credentials.refresh(Request())\n\n  signer_email = bootstrap_credentials.service_account_email\n  if isinstance(bootstrap_credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    # Since the Compute Engine metadata service doesn\'t expose the service\n    # account key, we use the IAM signBlob API to sign instead.\n    # In order for this to work:\n    #\n    # 1. Your VM needs the https://www.googleapis.com/auth/iam scope.\n    #    You can specify this specific scope when creating a VM  through the API\n    #    or gcloud. When using Cloud Console, you\'ll need to specify the\n    #    ""full access to all Cloud APIs"" scope. A VM\'s scopes can only be\n    #    specified at creation time.\n    #\n    # 2. The VM\'s default service account needs the ""Service Account Actor""\n    #    role. This can be found under the ""Project"" category in Cloud Console,\n    #    or roles/iam.serviceAccountActor in gcloud.\n    signer = google.auth.iam.Signer(Request(), bootstrap_credentials,\n                                    signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = bootstrap_credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer and email\n  # acquired from the bootstrap credentials.\n  service_account_credentials = google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={\'target_audience\': client_id})\n\n  # service_account_credentials gives us a JWT signed by the service account.\n  # Next, we use that to obtain an OpenID Connect token, which is a JWT signed\n  # by Google.\n  return get_google_open_id_connect_token(\n      service_account_credentials), signer_email\n\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  """"""\n  Get an OpenID Connect token issued by Google for the service account.\n\n  This function:\n    1. Generates a JWT signed with the service account\'s private key containing\n       a special ""target_audience"" claim.\n\n    2. Sends it to the OAUTH_TOKEN_URI endpoint. Because the JWT in #1 has a\n       target_audience claim, that endpoint will respond with an OpenID Connect\n       token for the service account -- in other words, a JWT signed by\n       *Google*. The aud claim in this JWT will be set to the value from the\n       target_audience claim in #1.\n\n  For more information, see\n  https://developers.google.com/identity/protocols/OAuth2ServiceAccount .\n  The HTTP/REST example on that page describes the JWT structure and\n  demonstrates how to call the token endpoint. (The example on that page shows\n  how to get an OAuth2 access token; this code is using a modified version of it\n  to get an OpenID Connect token.)\n  """"""\n\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      \'assertion\': service_account_jwt,\n      \'grant_type\': google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[\'id_token\']\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'url\', help=\'URL of the host model\')\n  parser.add_argument(\'client_id\', help=\'The client id used to setup IAP\')\n  parser.add_argument(\'--input\', help=\'The input file.\')\n  args = parser.parse_args()\n\n  token, signer_email = get_service_account_token(args.client_id)\n  if args.input:\n    with open(args.input) as f:\n      data = f.read()\n    resp = requests.post(\n        args.url,\n        verify=False,\n        data=data,\n        headers={\'Authorization\': \'Bearer {}\'.format(token)})\n  else:\n    resp = requests.get(\n        args.url,\n        verify=False,\n        headers={\'Authorization\': \'Bearer {}\'.format(token)})\n  if resp.status_code == 403:\n    raise Exception(\n        \'Service account {} does not have permission to \'\n        \'access the IAP-protected application.\'.format(signer_email))\n  elif resp.status_code != 200:\n    raise Exception(\'Bad response from application: {!r} / {!r} / {!r}\'.format(\n        resp.status_code, resp.headers, resp.text))\n  else:\n    print(resp.text)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/kubeflow/jupyter/jupyter_config.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nConfiguration file for JupyterHub.\n\nKubeflow uses this file as the configuration file for JupyterHub. It contains\nall glue code necessary to integrate JupyterHub with the remaining Kubeflow\ncomponents.\n\nNote that this file is also responsible for importing the UI-specific Spawner\nclass from <ui-dir>/spawner.py, and setting the `spawner_class` configuration\noption.\n""""""\n\nimport os\nfrom importlib.util import spec_from_file_location, module_from_spec\nfrom jhub_remote_user_authenticator.remote_user_auth import \\\n    RemoteUserAuthenticator\n\nSERVICE_ACCOUNT_SECRET_MOUNT = \'/var/run/secrets/sa\'\n\n# Import the UI-specific Spawner\nspec = spec_from_file_location(\'spawner\', \'/etc/config/spawner.py\')\nspawner = module_from_spec(spec)\nspec.loader.exec_module(spawner)\n\n###################################################\n# JupyterHub Options\n###################################################\nc.JupyterHub.ip = \'0.0.0.0\'\nc.JupyterHub.hub_ip = \'0.0.0.0\'\n# Don\'t try to cleanup servers on exit - since in general for k8s, we want\n# the hub to be able to restart without losing user containers\nc.JupyterHub.cleanup_servers = False\n###################################################\n\n###################################################\n# Spawner Options\n###################################################\nc.JupyterHub.spawner_class = spawner.KubeFormSpawner\n\nc.KubeSpawner.cmd = \'start-singleuser.sh\'\nc.KubeSpawner.args = [\'--allow-root\']\n# gpu images are very large ~15GB. need a large timeout.\nc.KubeSpawner.start_timeout = 60 * 30\n# Increase timeout to 5 minutes to avoid HTTP 500 errors on JupyterHub\nc.KubeSpawner.http_timeout = 60 * 5\n\n# Volume setup\nc.KubeSpawner.singleuser_uid = 1000\nc.KubeSpawner.singleuser_fs_gid = 100\nc.KubeSpawner.singleuser_working_dir = \'/home/jovyan\'\n\n# Allow environment vars to override uid and gid.\n# This allows local host path mounts to be read/writable\nenv_uid = os.environ.get(\'NOTEBOOK_UID\')\nif env_uid:\n  c.KubeSpawner.singleuser_uid = int(env_uid)\nenv_gid = os.environ.get(\'NOTEBOOK_GID\')\nif env_gid:\n  c.KubeSpawner.singleuser_fs_gid = int(env_gid)\naccess_local_fs = os.environ.get(\'ACCESS_LOCAL_FS\')\nif access_local_fs == \'true\':\n\n  def modify_pod_hook(spawner, pod):\n    pod.spec.containers[0].lifecycle = {\n        \'postStart\': {\n            \'exec\': {\n                \'command\': [\n                    \'ln\', \'-s\', \'/mnt/local-notebooks\',\n                    \'/home/jovyan/local-notebooks\'\n                ]\n            }\n        }\n    }\n    return pod\n\n  c.KubeSpawner.modify_pod_hook = modify_pod_hook\n\n###################################################\n# Persistent volume options\n###################################################\n\n# Set user_storage_pvc_ensure to False to prevent KubeSpawner from handling PVCs\n# We natively handle PVCs via KubeFormSpawner and its dedicated methods\n\n# NOTE: user_storage_pvc_ensure has been deprecated in a future release\nc.KubeSpawner.storage_pvc_ensure = False\nc.KubeSpawner.user_storage_pvc_ensure = False\n\nvolumes = []\nvolume_mounts = []\n\ngcp_secret_name = os.environ.get(\'GCP_SECRET_NAME\')\nif gcp_secret_name:\n  volumes.append({\n      \'name\': gcp_secret_name,\n      \'secret\': {\n          \'secretName\': gcp_secret_name,\n      }\n  })\n  volume_mounts.append({\n      \'name\': gcp_secret_name,\n      \'mountPath\': SERVICE_ACCOUNT_SECRET_MOUNT\n  })\n\nc.KubeSpawner.volumes = volumes\nc.KubeSpawner.volume_mounts = volume_mounts\n\nstorage_class = None\nif os.environ.get(\'STORAGE_CLASS\') != \'null\':\n  storage_class = os.environ.get(\'STORAGE_CLASS\')\n\nrok_secret_name = \'\'\nif os.environ.get(\'ROK_SECRET_NAME\') != \'null\':\n  rok_secret_name = os.environ.get(\'ROK_SECRET_NAME\')\n\n# Set both service_account and singleuser_service_account because\n# singleuser_service_account has been deprecated in a future release\nc.KubeSpawner.service_account = \'jupyter-notebook\'\nc.KubeSpawner.singleuser_service_account = \'jupyter-notebook\'\n# Authenticator\nif os.environ.get(\'KF_AUTHENTICATOR\') == \'iap\':\n  c.JupyterHub.authenticator_class = RemoteUserAuthenticator\n  c.RemoteUserAuthenticator.header_name = \'x-goog-authenticated-user-email\'\nelse:\n  c.JupyterHub.authenticator_class = \'dummyauthenticator.DummyAuthenticator\'\n\nif os.environ.get(\'DEFAULT_JUPYTERLAB\').lower() == \'true\':\n  c.KubeSpawner.default_url = \'/lab\'\n\n# Set extra spawner configuration variables\nc.KubeSpawner.extra_spawner_config = {\n    \'gcp_secret_name\': gcp_secret_name,\n    \'storage_class\': storage_class,\n    \'rok_secret_name\': rok_secret_name,\n}\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/metric-collector/service-readiness/kubeflow-readiness.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport google.auth\nimport google.auth.app_engine\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nimport logging\nimport requests\nfrom google.auth.transport.requests import Request\nfrom kubernetes import client, config\nfrom kubernetes.client import V1Event, V1ObjectMeta\nfrom prometheus_client import start_http_server, Gauge\nfrom time import sleep, time\n\nIAM_SCOPE = \'https://www.googleapis.com/auth/iam\'\nOAUTH_TOKEN_URI = \'https://www.googleapis.com/oauth2/v4/token\'\nMETHOD = \'GET\'\nKUBEFLOW_AVAILABILITY = \\\n    Gauge(\'kubeflow_availability\',\n          \'Signal of whether IAP protected kubeflow is available\')\n\n\ndef metric_update(args, google_open_id_connect_token):\n  resp = requests.request(\n      METHOD,\n      args.url,\n      headers={\n          \'Authorization\': \'Bearer {}\'.format(google_open_id_connect_token)\n      })\n  if resp.status_code == 200:\n    KUBEFLOW_AVAILABILITY.set(1)\n    return 1\n  else:\n    KUBEFLOW_AVAILABILITY.set(0)\n    return 0\n\n\ndef main(unparsed_args=None):\n  parser = argparse.ArgumentParser(\n      description=""Output signal of kubeflow service readiness."")\n\n  parser.add_argument(\n      ""--url"", default="""", type=str, help=""kubeflow IAP-protected url"")\n  parser.add_argument(\n      ""--client_id"",\n      default="""",\n      type=str,\n      help=""Service account json credential file"")\n\n  args = parser.parse_args(args=unparsed_args)\n\n  if args.url == """" or args.client_id == """":\n    logging.info(""Url or client_id is empty, exit"")\n    return\n\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(credentials, google.oauth2.credentials.Credentials):\n    raise Exception(\'make_iap_request is only supported for service \'\n                    \'accounts.\')\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  credentials.refresh(Request())\n\n  signer_email = credentials.service_account_email\n  if isinstance(credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    # Since the Compute Engine metadata service doesn\'t expose the service\n    # account key, we use the IAM signBlob API to sign instead.\n    # In order for this to work:\n    #\n    # 1. Your VM needs the https://www.googleapis.com/auth/iam scope. You\n    #    can specify this specific scope when creating a VM through the API\n    #    or gcloud. When using Cloud Console, you\'ll need to specify the\n    #    full access to all Cloud APIs"" scope. A VM\'s scopes can only be\n    #    specified at creation time.\n    #\n    # 2. The VM\'s default service account needs the ""Service Account Actor""\n    #    role. This can be found under the ""Project"" category in Cloud\n    #    Console, or roles/iam.serviceAccountActor in gcloud.\n    signer = google.auth.iam.Signer(Request(), credentials, signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer and\n  # email acquired from the bootstrap credentials.\n  service_account_credentials = google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={\'target_audience\': args.client_id})\n\n  token_refresh_time = 0\n  last_status = -1\n  config.load_incluster_config()\n  coreApi = client.CoreV1Api()\n  while True:\n    if time() > token_refresh_time:\n      # service_account_credentials gives us a JWT signed by the service\n      # account. Next, we use that to obtain an OpenID Connect token,\n      # which is a JWT signed by Google.\n      google_open_id_connect_token = get_google_open_id_connect_token(\n          service_account_credentials)\n      token_refresh_time = time() + 1800\n    url_status = metric_update(args, google_open_id_connect_token)\n    if url_status != last_status:\n      last_status = url_status\n      # get service centraldashboard, attach event to it.\n      svcs = coreApi.list_namespaced_service(\n          \'kubeflow\', label_selector=""app=centraldashboard"")\n      while len(svcs.to_dict()[\'items\']) == 0:\n        logging.info(""Service centraldashboard not ready..."")\n        sleep(10)\n        svcs = coreApi.list_namespaced_service(\n            \'kubeflow\', label_selector=""app=centraldashboard"")\n      uid = svcs.to_dict()[\'items\'][0][\'metadata\'][\'uid\']\n      kf_status = ""up"" if url_status == 1 else ""down""\n      new_event = V1Event(\n          action=""Kubeflow service status update: "" + kf_status,\n          api_version=""v1"",\n          kind=""Event"",\n          message=""Service "" + kf_status + ""; service url: "" + args.url,\n          reason=""Kubeflow Service is "" + kf_status,\n          involved_object=client.V1ObjectReference(\n              api_version=""v1"",\n              kind=""Service"",\n              name=""centraldashboard"",\n              namespace=""kubeflow"",\n              uid=uid),\n          metadata=V1ObjectMeta(generate_name=\'kubeflow-service.\',),\n          type=""Normal"")\n      event = coreApi.create_namespaced_event(""kubeflow"", new_event)\n      print(""New status event created. action=\'%s\'"" % str(event.action))\n\n    # Update status every 10 sec\n    sleep(10)\n\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  """"""\n  Get an OpenID Connect token issued by Google for the service account.\n\n  This function:\n\n    1. Generates a JWT signed with the service account\'s private key containing\n       a special ""target_audience"" claim.\n\n    2. Sends it to the OAUTH_TOKEN_URI endpoint. Because the JWT in #1 has a\n       target_audience claim, that endpoint will respond with an OpenID Connect\n       token for the service account -- in other words, a JWT signed by\n       *Google*. The aud claim in this JWT will be set to the value from the\n       target_audience claim in #1.\n\n  For more information, see\n  https://developers.google.com/identity/protocols/OAuth2ServiceAccount\n  The HTTP/REST example on that page describes the JWT structure and\n  demonstrates how to call the token endpoint. (The example on that paga shows\n  how to get an OAuth2 access token; this code is using a modified version of it\n  to get an OpenID Connect token.)\n  """"""\n\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      \'assertion\': service_account_jwt,\n      \'grant_type\': google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[\'id_token\']\n\n\nif __name__ == \'__main__\':\n  start_http_server(8000)\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/py/kubeflow/__init__.py,0,"b""__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n"""
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/releasing/hubsync/hubsync.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START app]\n\n# import modules\n\nimport json\nimport subprocess\nimport yaml\n\n# declaring some variables\nimages = []\ntimeout = \'7200s\'\nfilename = \'cloudbuild.yaml\'\nbuilder = \'gcr.io/cloud-builders/docker\'\nkfRepo = \'gcr.io/kubeflow-images-public/\'\nmyRepo = \'gcr.io/<my_repo>\'\n\n# Get Auth\nwith open(\'keys.yaml\', \'r\') as keyfile:\n  kcfg = yaml.load(keyfile)\n\nlogin = kcfg[\'username\']\npswd = kcfg[\'password\']\n\n# build a json file with all files.\nrepos = subprocess.run([\n    ""gcloud"", ""--project=kubeflow-images-public"", ""container"", ""images"", ""list"",\n    ""--format=json""\n],\n                       stdout=subprocess.PIPE,\n                       stderr=subprocess.STDOUT)\nmy_json = json.loads(repos.stdout.decode(\'utf8\').strip().replace(""\'"", \'""\'))\nfor data in my_json:\n  for name, image in data.items():\n    # get Tags and Repos\n    raw_images = subprocess.run([\n        ""gcloud"", ""container"", ""images"", ""list"", ""--repository="" + image + """",\n        ""--format=json""\n    ],\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.STDOUT)\n    imgData = raw_images.stdout.decode(""utf-8"")\n    if ""[]"" not in imgData:\n      imgJson = json.loads(\n          raw_images.stdout.decode(""utf-8"").strip().replace(""\'"", \'""\'))\n      for stuff in imgJson:\n        for a, b in stuff.items():\n          images.append(b)\n      images.append(image)\n\nfor item in images:\n  getTags = subprocess.run([\n      ""gcloud"", ""--project=kubeflow-images-public"", ""container"", ""images"",\n      ""list-tags"", item, ""--format=json"", ""--limit=1""\n  ],\n                           stdout=subprocess.PIPE,\n                           stderr=subprocess.STDOUT)\n  preTags = json.loads(getTags.stdout.decode(\'utf8\').replace(""\'"", \'""\'))\n  for datum in preTags:\n    t = datum[\'digest\']\n    s = item[30:]\n    myTag = item + ""@"" + t\n    theyaml = {\n        \'timeout\':\n        timeout,\n        \'steps\': [\n            {\n                \'name\': builder,\n                \'args\': [\'login\', \'-u\', login, \'-p\', pswd],\n                \'timeout\': timeout,\n            },\n            {\n                \'name\': builder,\n                \'args\': [\'pull\', myTag],\n                \'timeout\': timeout,\n            },\n            {\n                \'name\': builder,\n                \'args\': [\'tag\', myTag, myRepo + s],\n                \'timeout\': timeout,\n            },\n            {\n                \'name\': builder,\n                \'args\': [\'push\', myRepo + s],\n                \'timeout\': timeout,\n            },\n        ]\n    }\n    with open(filename, \'a\') as outfile:\n      yaml.dump(theyaml, outfile, default_flow_style=False)\n\n    subprocess.run([""gcloud"", ""builds"", ""submit"", ""--config"", filename],\n                   stdout=subprocess.PIPE,\n                   stderr=subprocess.STDOUT)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/scripts/gke/delete_role_bindings.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# A simple script to delete all role bindings for the service accounts created\n# as part of a Kubeflow deployment. This is an effort to deal with:\n# https://github.com/kubeflow/kubeflow/issues/953\nimport argparse\nimport logging\nimport json\nimport subprocess\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=(""The project.""))\n  parser.add_argument(\n      ""--service_account"", type=str, help=(""The service account.""))\n\n  args = parser.parse_args()\n  output = subprocess.check_output([\n      ""gcloud"",\n      ""projects"",\n      ""get-iam-policy"",\n      ""--format=json"",\n      args.project,\n  ])\n\n  bindings = json.loads(output)\n  roles = []\n  entry = ""serviceAccount:"" + args.service_account\n  for b in bindings[""bindings""]:\n    if entry in b[""members""]:\n      roles.append(b[""role""])\n  # TODO(jlewi): Can we issue a single gcloud command.\n  for r in roles:\n    command = [\n        ""gcloud"",\n        ""projects"",\n        ""remove-iam-policy-binding"",\n        args.project,\n        ""--member"",\n        entry,\n        ""--role"",\n        r,\n    ]\n    print("" "".join(command))\n    subprocess.call(command)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/scripts/gke/iam_patch.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# A python script which manages IAM binding patches declaratively IAM policy\n# patch can be defined in a separate file declaratively and it can either be\n# added or removed from a projects iam policy\n#\n# Usage\n#   python iam_patch.py --action=add --project=agwliamtest \\\n#     --iam_bindings_file=iam_bindings.yaml\n#   python iam_patch.py --action=remove --project=agwliamtest \\\n#     --iam_bindings_file=iam_bindings.yaml\nimport argparse\nimport logging\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport yaml\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--action"",\n      default=""add"",\n      type=str,\n      help=(""The action to take. Valid values: add, remove""))\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=(""The project.""))\n  parser.add_argument(\n      ""--iam_bindings_file"",\n      default=None,\n      type=str,\n      help=(""The IAM bindings file.""))\n  parser.set_defaults(dry_run=False)\n  parser.add_argument(\n      \'--dry_run\',\n      dest=\'dry_run\',\n      action=\'store_true\',\n      help=(""Don\'t patch the final IAM policy, only print it""))  # noqa: E501\n  return parser.parse_args()\n\n\ndef get_current_iam_policy(project):\n  """"""Fetches and returns the current iam policy as a yaml object""""""\n  return yaml.load(\n      subprocess.check_output([""gcloud"", ""projects"", ""get-iam-policy"",\n                               project]))\n\n\ndef iam_policy_to_dict(bindings):\n  """"""\n  iam_policy_to_dict takes an iam policy binding in the GCP API format and\n  converts it into a python dict so that it can be easily updated\n  """"""\n  bindings_dict = dict()\n  for binding in bindings:\n    role = binding[\'role\']\n    bindings_dict[role] = set(binding[\'members\'])\n  return bindings_dict\n\n\ndef iam_dict_to_policy(bindings_dict):\n  """"""\n  iam_dict_to_policy takes an iam policy binding in the dict format and\n  converts it into GCP API format so that it can be sent to GCP IAM API for\n  an update\n  """"""\n  bindings = []\n  for k, v in bindings_dict.items():\n    bindings.append({""role"": k, ""members"": list(v)})\n  return bindings\n\n\ndef apply_iam_bindings_patch(current_policy, bindings_patch, action):\n  """"""\n  Patches the current policy with the supplied patch.\n  action can be add or remove.\n  """"""\n  for item in bindings_patch[\'bindings\']:\n    members = item[\'members\']\n    roles = item[\'roles\']\n    for role in roles:\n      if role not in current_policy.keys():\n        current_policy[role] = set()\n      if action == ""add"":\n        current_policy[role].update(members)\n      else:\n        current_policy[role].difference_update(members)\n  return current_policy\n\n\ndef patch_iam_policy(args):\n  """"""\n  Fetches the current IAM policy, patches it with the bindings supplied in\n  --iam_bindings_file and updates the new iam policy\n  """"""\n  current_policy = get_current_iam_policy(args.project)\n  logging.info(""Current IAM Policy"")\n  logging.info(\n      yaml.dump(current_policy, default_flow_style=False, default_style=\'\'))\n  current_policy_bindings_dict = iam_policy_to_dict(current_policy[\'bindings\'])\n  with open(args.iam_bindings_file) as iam_bindings_file:\n    bindings_patch = yaml.load(iam_bindings_file.read())\n  current_policy_bindings_dict = apply_iam_bindings_patch(\n      current_policy_bindings_dict, bindings_patch, args.action)\n  current_policy[\'bindings\'] = iam_dict_to_policy(current_policy_bindings_dict)\n  logging.info(""Updated Policy"")\n  logging.info(""\\n"" + yaml.dump(\n      current_policy, default_flow_style=False, default_style=\'\'))\n  updated_policy_file = tempfile.NamedTemporaryFile(delete=False)\n  with open(updated_policy_file.name, \'w\') as f:\n    yaml.dump(current_policy, f, default_flow_style=False)\n  logging.debug(""Temp file %s"", updated_policy_file.name)\n  if not args.dry_run:\n    subprocess.check_call([\n        ""gcloud"", ""projects"", ""set-iam-policy"", args.project,\n        updated_policy_file.name\n    ])\n  else:\n    logging.info(""Skipping patching the IAM policy because --dry_run was set"")\n\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)\n  args = parse_args()\n\n  if args.action not in [""add"", ""remove""]:\n    raise ValueError(""invalid --action. Valid values are add, remove"")\n\n  for i in range(5):\n    try:\n      patch_iam_policy(args)\n      logging.info(""Successfully patched IAM policy"")\n      break\n    except Exception as e:\n      logging.error(e)\n      if i < 4:\n        logging.info(""Retrying in 15 seconds.."")\n        time.sleep(15)\n      else:\n        logging.error(""Patching IAM policy failed"")\n        sys.exit(1)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/scripts/gke/iam_patch_test.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Unit tests for iam_patch.py\nimport unittest\n\nimport iam_patch\n\n\nclass Test(unittest.TestCase):\n\n  def test_apply_iam_bindings_patch_add(self):\n    current_policy = {}\n    bindings_patch = {\n        ""bindings"": [{\n            ""members"": [\'admin-sa\'],\n            ""roles"": [""roles/source.admin""]\n        }]\n    }\n    expected = {\'roles/source.admin\': set([\'admin-sa\'])}\n    result = iam_patch.apply_iam_bindings_patch(current_policy, bindings_patch,\n                                                ""add"")\n    self.assertEqual(result, expected)\n\n  def test_apply_iam_bindings_patch_remove(self):\n    current_policy = {\'roles/source.admin\': set([\'admin-sa\'])}\n    bindings_patch = {\n        ""bindings"": [{\n            ""members"": [\'admin-sa\'],\n            ""roles"": [""roles/source.admin""]\n        }]\n    }\n    expected = {\'roles/source.admin\': set()}\n    result = iam_patch.apply_iam_bindings_patch(current_policy, bindings_patch,\n                                                ""remove"")\n    self.assertEqual(result, expected)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/kfctl/conftest.py,0,"b'import pytest\n\ndef pytest_addoption(parser):\n  parser.addoption(\n      ""--app_path"", action=""store"", default="""",\n      help=""Path where the KF application should be stored"")\n  \n  parser.addoption(\n      ""--app_name"", action=""store"", default="""",\n      help=""Name of the KF application"")\n\n  parser.addoption(\n      ""--kfctl_path"", action=""store"", default="""",\n      help=""Path to kfctl."")\n\n  parser.addoption(\n      ""--namespace"", action=""store"", default=""kubeflow"",\n      help=""Namespace to use."")\n\n  parser.addoption(\n      ""--project"", action=""store"", default=""kubeflow-ci-deployment"",\n      help=""GCP project to deploy Kubeflow to"")\n  \n  parser.addoption(\n      ""--config_path"", action=""store"", default="""",\n      help=""The config to use for kfctl init"")\n\n  parser.addoption(\n      ""--use_basic_auth"", action=""store"", default=""False"",\n      help=""Use basic auth."")\n\n  parser.addoption(\n      ""--use_istio"", action=""store"", default=""False"",\n      help=""Use istio."")\n\n@pytest.fixture\ndef app_path(request):\n  return request.config.getoption(""--app_path"")\n\n@pytest.fixture\ndef app_name(request):\n  return request.config.getoption(""--app_name"")\n\n@pytest.fixture\ndef kfctl_path(request):\n  return request.config.getoption(""--kfctl_path"")\n\n@pytest.fixture\ndef namespace(request):\n  return request.config.getoption(""--namespace"")\n\n@pytest.fixture\ndef project(request):\n  return request.config.getoption(""--project"")\n\n@pytest.fixture\ndef config_path(request):\n  return request.config.getoption(""--config_path"")\n\n@pytest.fixture\ndef use_basic_auth(request):\n  value = request.config.getoption(""--use_basic_auth"").lower()\n\n  if value in [""t"", ""true""]:\n    return True\n  else:\n    return False\n\n@pytest.fixture\ndef use_istio(request):\n  value = request.config.getoption(""--use_istio"").lower()\n\n  if value in [""t"", ""true""]:\n    return True\n  else:\n    return False'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/kfctl/endpoint_ready_test.py,0,"b'import datetime\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom retrying import retry\n\nimport pytest\n\nfrom kubeflow.testing import util\nfrom testing import deploy_utils\nfrom testing import gcp_util\n\ndef test_endpoint_is_ready(project, app_name):\n  """"""Test that Kubeflow was successfully deployed.\n\n  Args:\n    project: The gcp project that we deployed kubeflow\n    app_name: The name of the kubeflow deployment\n  """"""\n  # Owned by project kubeflow-ci-deployment.\n  os.environ[""CLIENT_ID""] = ""29647740582-7meo6c7a9a76jvg54j0g2lv8lrsb4l8g.apps.googleusercontent.com""\n  if not gcp_util.endpoint_is_ready(\n      ""https://{}.endpoints.{}.cloud.goog"".format(app_name, project),\n      wait_min=25):\n    raise Exception(""Endpoint not ready"")\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/kfctl/kf_is_ready_test.py,0,"b'import datetime\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom retrying import retry\n\nimport pytest\n\nfrom kubeflow.testing import util\nfrom testing import deploy_utils\n\ndef test_kf_is_ready(namespace, use_basic_auth, use_istio):\n  """"""Test that Kubeflow was successfully deployed.\n\n  Args:\n    namespace: The namespace Kubeflow is deployed to.\n  """"""\n\n  logging.info(""Using namespace %s"", namespace)\n\n  # Need to activate account for scopes.\n  if os.getenv(""GOOGLE_APPLICATION_CREDENTIALS""):\n    util.run([""gcloud"", ""auth"", ""activate-service-account"",\n              ""--key-file="" + os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]])\n\n  api_client = deploy_utils.create_k8s_client()\n\n  util.load_kube_config()\n\n  # Verify that components are actually deployed.\n  # TODO(jlewi): We need to parameterize this list based on whether\n  # we are using IAP or basic auth.\n  deployment_names = [\n      ""argo-ui"",\n      ""centraldashboard"",\n      ""cloud-endpoints-controller"",\n      ""jupyter-web-app-deployment"",\n      ""metadata-db"",\n      ""metadata-deployment"",\n      ""metadata-ui"",\n      ""ml-pipeline"",\n      ""ml-pipeline-scheduledworkflow"",\n      ""ml-pipeline-ui"",\n      ""notebook-controller-deployment"",\n      ""tf-job-operator"",\n      ""pytorch-operator"",\n      ""katib-controller"",\n      ""workflow-controller"",\n  ]\n\n  stateful_set_names = [\n    ""kfserving-controller-manager"",\n  ]\n\n  ingress_related_deployments = []\n  ingress_related_stateful_sets = []\n\n  if use_basic_auth:\n    deployment_names.extend([""basic-auth-login""])\n    ingress_related_stateful_sets.extend([""backend-updater""])\n  else:\n    ingress_related_deployments.extend([""iap-enabler""])\n    ingress_related_stateful_sets.extend([""backend-updater""])\n\n  # TODO(jlewi): Might want to parallelize this.\n  for deployment_name in deployment_names:\n    logging.info(""Verifying that deployment %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, namespace, deployment_name, 10)\n\n  for stateful_set_name in stateful_set_names:\n    logging.info(""Verifying that stateful set %s started..."", stateful_set_name)\n    util.wait_for_statefulset(api_client, namespace, stateful_set_name)\n\n  ingress_namespace = ""istio-system"" if use_istio else namespace\n  for deployment_name in ingress_related_deployments:\n    logging.info(""Verifying that deployment %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, ingress_namespace, deployment_name, 10)\n\n  for name in ingress_related_stateful_sets:\n    logging.info(""Verifying that statefulset %s started..."", name)\n    util.wait_for_statefulset(api_client, ingress_namespace, name)\n\n  # TODO(jlewi): We should verify that the ingress is created and healthy.\n\n  knative_namespace = ""knative-serving""\n  knative_related_deployments = [\n          ""activator"",\n          ""autoscaler"",\n          ""controller"",\n  ]\n  for deployment_name in knative_related_deployments:\n      logging.info(""Verifying that deployment %s started..."", deployment_name)\n      util.wait_for_deployment(api_client, knative_namespace, deployment_name, 10)\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/kfctl/kfctl_delete_test.py,0,"b'""""""Run kfctl delete as a pytest.\n\nWe use this in order to generate a junit_xml file.\n""""""\nimport datetime\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom retrying import retry\n\nimport pytest\n\nfrom kubeflow.testing import util\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\n\n# TODO(gabrielwen): Move this to a separate test ""kfctl_go_check_post_delete""\ndef get_endpoints_list(project):\n  cred = GoogleCredentials.get_application_default()\n  services_mgt = discovery.build(\'servicemanagement\', \'v1\', credentials=cred)\n  services = services_mgt.services()\n  next_page_token = None\n  endpoints = []\n\n  while True:\n    results = services.list(producerProjectId=project,\n                            pageToken=next_page_token).execute()\n\n    for s in results.get(""services"", {}):\n      name = s.get(""serviceName"", """")\n      endpoints.append(name)\n    if not ""nextPageToken"" in results:\n      break\n    next_page_token = results[""nextPageToken""]\n\n  return endpoints\n\ndef test_kfctl_delete(kfctl_path, app_path, project):\n  if not kfctl_path:\n    raise ValueError(""kfctl_path is required"")\n\n  if not app_path:\n    raise ValueError(""app_path is required"")\n\n  logging.info(""Using kfctl path %s"", kfctl_path)\n  logging.info(""Using app path %s"", app_path)\n\n  util.run([kfctl_path, ""delete"", ""all"", ""--delete_storage"", ""-V""],\n           cwd=app_path)\n\n  # Use services.list instead of services.get because error returned is not\n  # 404, it\'s 403 which is confusing.\n  name = os.path.basename(app_path)\n  endpoint_name = ""{deployment}.endpoints.{project}.cloud.goog"".format(\n      deployment=name,\n      project=project)\n  logging.info(""Verify endpoint service is deleted: "" + endpoint_name)\n  if endpoint_name in get_endpoints_list(project):\n    msg = ""Endpoint is not deleted: "" + endpoint_name\n    logging.error(msg)\n    raise AssertionError(msg)\n  else:\n    logging.info(""Verified endpoint service is deleted."")\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/testing/kfctl/kfctl_go_test.py,0,"b'import datetime\nimport logging\nimport os\nimport requests\nimport subprocess\nimport tempfile\nimport uuid\nimport urllib\nfrom retrying import retry\nimport yaml\n\nimport pytest\n\nfrom kubeflow.testing import util\n\n\n# retry 4 times, waiting 3 minutes between retries\n@retry(stop_max_attempt_number=4, wait_fixed=180000)\ndef run_with_retries(*args, **kwargs):\n  util.run(*args, **kwargs)\n\n\ndef load_config(config_path):\n  """"""Load specified KFDEf.\n\n  Args:\n    config_path: Path to a YAML file containing a KFDef object.\n      Can be a local path or a URI like\n      https://raw.githubusercontent.com/kubeflow/manifests/master/kfdef/kfctl_gcp_iap.yaml\n  """"""\n  u = urllib.parse.urlparse(config_path)\n\n  if u.scheme in [""http"", ""https""]:\n    data = requests.get(config_path)\n    return yaml.load(data.content)\n  else:\n    with open(config_path, \'r\') as f:\n      config_spec = yaml.load(f)\n      return config_spec\n\ndef verify_kubeconfig(project, zone, app_path):\n  name = os.path.basename(app_path)\n  context = util.run([""kubectl"", ""config"", ""current-context""]).strip()\n  if name == context:\n    logging.info(""KUBECONFIG current context name matches app name: "" + name)\n  else:\n    msg = ""KUBECONFIG not having expected context: {expected} v.s. {actual}"".format(\n        expected=name, actual=context)\n    logging.error(msg)\n    raise RuntimeError(msg)\n\n\ndef test_build_kfctl_go(app_path, project, use_basic_auth, use_istio, config_path):\n  """"""Test building and deploying Kubeflow.\n\n  Args:\n    app_path: The path to the Kubeflow app.\n    project: The GCP project to use.\n  """"""\n  if not app_path:\n    logging.info(""--app_path not specified"")\n    stamp = datetime.datetime.now().strftime(""%H%M"")\n    parent_dir = tempfile.gettempdir()\n    app_path = os.path.join(\n        parent_dir, ""kfctl-{0}-{1}"".format(stamp,\n                                           uuid.uuid4().hex[0:4]))\n  else:\n    parent_dir = os.path.dirname(app_path)\n\n  logging.info(""Using app path %s"", app_path)\n  this_dir = os.path.dirname(__file__)\n  root = os.path.abspath(os.path.join(this_dir, "".."", ""..""))\n  build_dir = os.path.join(root, ""bootstrap"")\n  zone = \'us-central1-a\'\n\n  # Need to activate account for scopes.\n  if os.getenv(""GOOGLE_APPLICATION_CREDENTIALS""):\n    util.run([\n        ""gcloud"", ""auth"", ""activate-service-account"",\n        ""--key-file="" + os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]\n    ])\n\n  # We need to use retry builds because when building in the test cluster\n  # we see intermittent failures pulling dependencies\n  run_with_retries([""make"", ""build-kfctl""], cwd=build_dir)\n  kfctl_path = os.path.join(build_dir, ""bin"", ""kfctl"")\n\n  # Set ENV for basic auth username/password.\n  init_args = []\n  if use_basic_auth:\n    os.environ[""KUBEFLOW_USERNAME""] = ""kf-test-user""\n    os.environ[""KUBEFLOW_PASSWORD""] = str(uuid.uuid4().hex)\n    init_args = [""--use_basic_auth""]\n  else:\n    # Owned by project kubeflow-ci-deployment.\n    os.environ[""CLIENT_SECRET""] = ""CJ4qVPLTi0j0GJMkONj7Quwt""\n    os.environ[""CLIENT_ID""] = (\n      ""29647740582-7meo6c7a9a76jvg54j0g2lv8lrsb4l8g""\n      "".apps.googleusercontent.com"")\n\n  if use_istio:\n    init_args.append(""--use_istio"")\n  else:\n    init_args.append(""--use_istio=false"")\n\n  version = ""master""\n  if os.getenv(""REPO_NAME"") != ""manifests"":\n    if os.getenv(""PULL_NUMBER""):\n      version = ""pull/{0}"".format(os.getenv(""PULL_NUMBER""))\n  pull_manifests = ""@master""\n  if os.getenv(""REPO_NAME"") == ""manifests"":\n    if os.getenv(""PULL_PULL_SHA""):\n      pull_manifests = ""@"" + os.getenv(""PULL_PULL_SHA"")\n\n  # We need to specify a valid email because\n  #  1. We need to create appropriate RBAC rules to allow the current user\n  #     to create the required K8s resources.\n  #  2. Setting the IAM policy will fail if the email is invalid.\n  email = util.run([""gcloud"", ""config"", ""get-value"", ""account""])\n\n  if not email:\n    raise ValueError(""Could not determine GCP account being used."")\n\n  # username and password are passed as env vars and won\'t appear in the logs\n  # TODO(https://github.com/kubeflow/kubeflow/issues/2831): Once kfctl\n  # supports loading version from a URI we should use that so that we\n  # pull the configs from the repo we checked out.\n  #\n  # We don\'t run with retries because if kfctl init exits with an error\n  # but creates app.yaml then rerunning init will fail because app.yaml\n  # already exists. So retrying ends up masking the original error message\n  config_spec = load_config(config_path)\n  config_spec[""spec""][""project""] = project\n  config_spec[""spec""][""email""] = email\n  config_spec[""spec""] = filterSpartakus(config_spec[""spec""])\n  repos = config_spec[""spec""][""repos""]\n\n  if os.getenv(""REPO_NAME"") == ""manifests"":\n    # kfctl_go_test.py was triggered on presubmit from the kubeflow/manifests\n    # repository. In this case we want to use the specified PR of the\n    # kubeflow/manifests repository; so we need to change the repo specification\n    # in the KFDef spec.\n    # TODO(jlewi): We should also point to a specific commit when triggering\n    # postsubmits from the kubeflow/manifests repo\n    for repo in repos:\n      for key, value in repo.items():\n        if value == ""https://github.com/kubeflow/manifests/archive/master.tar.gz"":\n          repo[""uri""] = str(""https://github.com/kubeflow/manifests/archive/pull/""+str(os.getenv(""PULL_NUMBER""))+""/head.tar.gz"")\n    logging.info(str(config_spec))\n  with open(os.path.join(parent_dir, ""tmp.yaml""), ""w"") as f:\n    yaml.dump(config_spec, f)\n  util.run([\n      kfctl_path, ""init"", app_path, ""-V"",\n      ""--config="" + os.path.join(parent_dir, ""tmp.yaml"")], cwd=parent_dir)\n  util.run([""cat"", ""app.yaml""], cwd=app_path)\n\n  run_with_retries([\n      kfctl_path, ""generate"", ""-V"", ""all"", ""--email="" + email, ""--zone="" + zone\n  ],\n                   cwd=app_path)\n\n  # We need to use retries because if we don\'t we see random failures\n  # where kfctl just appears to die.\n  #\n  # Do not run with retries since it masks errors\n  util.run([kfctl_path, ""apply"", ""-V"", ""all""], cwd=app_path)\n\n  verify_kubeconfig(project, zone, app_path)\n\ndef filterSpartakus(spec):\n  for i, app in enumerate(spec[""applications""]):\n    if app[""name""] == ""spartakus"":\n      spec[""applications""].pop(i)\n      break\n  return spec\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/tf-controller-examples/tf-cnn/create_job_specs.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2017 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple script to generate TfJob templates based on various parameters.""""""\n\nimport argparse\nimport datetime\nimport logging\nimport yaml\n\nTF_JOB_GROUP = ""tensorflow.org""\nTF_JOB_VERSION = ""v1alpha1""\nTF_JOB_PLURAL = ""tfjobs""\nTF_JOB_KIND = ""TfJob""\n\n\n# See\n# https://stackoverflow.com/questions/21016220/is-it-possible-to-emit-valid-yaml-with-anchors-references-disabled-using-ruby  # noqa: E501\nclass ExplicitDumper(yaml.SafeDumper):\n  """"""A dumper that will never emit aliases.""""""\n\n  def ignore_aliases(self, data):\n    return True\n\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  parser = argparse.ArgumentParser(description=""Create TfJob specs."")\n\n  parser.add_argument(\n      ""--cpu_image"",\n      type=str,\n      required=True,\n      help=""The docker image for CPU jobs."")\n\n  parser.add_argument(\n      ""--gpu_image"",\n      type=str,\n      required=True,\n      help=""The docker image for GPU jobs."")\n\n  parser.add_argument(\n      ""--num_workers"",\n      type=int,\n      default=1,\n      help=""The number of workers to use."")\n\n  parser.add_argument(\n      ""--output"",\n      type=str,\n      help=""(Optional) the file to write the template to."")\n\n  parser.add_argument(\n      ""--gpu"", dest=""use_gpu"", action=""store_true"", help=""Use gpus."")\n  parser.add_argument(\n      ""--no-gpu"", dest=""use_gpu"", action=""store_false"", help=""Do not use gpus."")\n\n  parser.set_defaults(use_gpu=True)\n\n  args = parser.parse_args()\n\n  namespace = ""default""\n  job_name = ""inception-"" + datetime.datetime.now().strftime(""%y%m%d-%H%M%S"")\n  if args.use_gpu:\n    job_name += ""-gpu""\n  else:\n    job_name += ""-cpu""\n\n  job_name += ""-{0}"".format(args.num_workers)\n\n  body = {}\n  body[\'apiVersion\'] = TF_JOB_GROUP + ""/"" + TF_JOB_VERSION\n  body[\'kind\'] = TF_JOB_KIND\n  body[\'metadata\'] = {}\n  body[\'metadata\'][\'name\'] = job_name\n  body[\'metadata\'][\'namespace\'] = namespace\n\n  clone_on_cpu = not args.use_gpu\n\n  body[""spec""] = {}\n  body[""spec""][""replicaSpecs""] = []\n\n  working_dir = ""/opt/tf-benchmarks/scripts/tf_cnn_benchmarks""\n\n  num_workers = args.num_workers\n  num_ps = 1\n\n  command = [\n      ""python"",\n      ""tf_cnn_benchmarks.py"",\n      ""--batch_size=32"",\n      ""--model=resnet50"",\n      ""--variable_update=parameter_server"",\n      # tf_cnn_benchmarks uses print for logging and if we don\'t set\n      # flush_stdout the buffer isn\'t outputted until the program ends..\n      ""--flush_stdout=true"",\n  ]\n\n  if args.use_gpu:\n    command.append(""--num_gpus=1"")\n  else:\n    # We need to set num_gpus=1 even if not using GPUs because otherwise\n    # the devie list is empty because of this code\n    # https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L775  # noqa: E501\n    command.append(""--num_gpus=1"")\n    command.append(""--local_parameter_device=cpu"")\n    command.append(""--device=cpu"")\n    command.append(""--data_format=NHWC"")\n\n  # Add the master spec. The master only acts as the chief and doesn\'t do\n  # any training so it can always use the CPU image.\n  master_spec = {\n      ""replicas"": 1,\n      ""tfReplicaType"": ""MASTER"",\n      ""template"": {\n          ""spec"": {\n              ""containers"": [{\n                  ""image"": args.cpu_image,\n                  ""name"": ""tensorflow"",\n                  ""workingDir"": working_dir,\n                  ""args"": command,\n              }],\n              ""restartPolicy"":\n              ""OnFailure"",\n          }\n      }\n  }\n\n  body[""spec""][""replicaSpecs""].append(master_spec)\n\n  worker_image = args.cpu_image\n  if args.use_gpu:\n    worker_image = args.gpu_image\n\n  worker_spec = {\n      ""replicas"": num_workers,\n      ""tfReplicaType"": ""WORKER"",\n      ""template"": {\n          ""spec"": {\n              ""containers"": [{\n                  ""image"": worker_image,\n                  ""name"": ""tensorflow"",\n                  ""workingDir"": working_dir,\n                  ""args"": command,\n              }],\n              ""restartPolicy"":\n              ""OnFailure"",\n          }\n      }\n  }\n\n  if args.use_gpu:\n    worker_spec[""template""][""spec""][""containers""][0][""resources""] = {\n        ""limits"": {\n            ""nvidia.com/gpu"": 1,\n        }\n    }\n\n  body[""spec""][""replicaSpecs""].append(worker_spec)\n\n  ps_spec = {\n      ""replicas"": num_ps,\n      ""tfReplicaType"": ""PS"",\n      ""template"": {\n          ""spec"": {\n              ""containers"": [{\n                  ""image"": args.cpu_image,\n                  ""name"": ""tensorflow"",\n                  ""workingDir"": working_dir,\n                  ""args"": command,\n              }],\n              ""restartPolicy"":\n              ""OnFailure"",\n          }\n      }\n  }\n\n  body[""spec""][""replicaSpecs""].append(ps_spec)\n\n  body[""spec""][""tfImage""] = args.cpu_image\n\n  # Tensorboard is crashing with TF 1.5\n  # body[""spec""][""tensorBoard""] = {\n  #   ""logDir"": job_dir\n  # }\n\n  spec = yaml.dump(body, Dumper=ExplicitDumper, default_flow_style=False)\n\n  if args.output:\n    logging.info(""Writing to %s"", args.output)\n    with open(args.output, ""w"") as hf:\n      hf.write(spec)\n  else:\n    print(spec)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/tf-controller-examples/tf-cnn/launcher.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2017 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA launcher suitable for invoking tf_cnn_benchmarks using TfJob.\n\nAll the launcher does is turn TF_CONFIG environment variable into extra\narguments to append to the command line.\n""""""\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\n\n\ndef run_and_stream(cmd):\n  logging.info(""Running %s"", "" "".join(cmd))\n  process = subprocess.Popen(\n      cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n  while process.poll() is None:\n    process.stdout.flush()\n    if process.stderr:\n      process.stderr.flush()\n    sys.stderr.flush()\n    sys.stdout.flush()\n    for line in iter(process.stdout.readline, b\'\'):\n      process.stdout.flush()\n      logging.info(line.strip())\n\n  sys.stderr.flush()\n  sys.stdout.flush()\n  process.stdout.flush()\n  if process.stderr:\n    process.stderr.flush()\n  for line in iter(process.stdout.readline, b\'\'):\n    logging.info(line.strip())\n\n  if process.returncode != 0:\n    raise ValueError(""cmd: {0} exited with code {1}"".format(\n        "" "".join(cmd), process.returncode))\n\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.info(""Launcher started."")\n  tf_config = os.environ.get(\'TF_CONFIG\', \'{}\')\n  tf_config_json = json.loads(tf_config)\n  cluster = tf_config_json.get(\'cluster\', {})\n  job_name = tf_config_json.get(\'task\', {}).get(\'type\', """")\n  task_index = tf_config_json.get(\'task\', {}).get(\'index\', """")\n\n  command = sys.argv[1:]\n  ps_hosts = "","".join(cluster.get(""ps"", []))\n  worker_hosts = "","".join(cluster.get(""worker"", []))\n  command.append(""--job_name="" + job_name)\n  command.append(""--ps_hosts="" + ps_hosts)\n  command.append(""--worker_hosts="" + worker_hosts)\n  command.append(""--task_index={0}"".format(task_index))\n\n  logging.info(""Command to run: %s"", "" "".join(command))\n  with open(""/opt/run_benchmarks.sh"", ""w"") as hf:\n    hf.write(""#!/bin/bash\\n"")\n    hf.write("" "".join(command))\n    hf.write(""\\n"")\n\n  run_and_stream(command)\n  logging.info(""Finished: %s"", "" "".join(command))\n  # We don\'t want to terminate because TfJob will just restart the job.\n  while True:\n    logging.info(""Command ran successfully sleep for ever."")\n    time.sleep(600)\n'"
kubeflow/install-kubeflow/ks_app/vendor/kubeflow/jupyter/tests/conftest.py,0,"b'# -*- coding: utf-8 -*-\nimport pytest\n\n\ndef pytest_addoption(parser):\n  parser.addoption(\n      ""--namespace"", action=""store"", default="""", help=""namespace  to use"")\n\n  parser.addoption(\n      ""--env"",\n      action=""store"",\n      default=""jupytertest"",\n      help=""ksonnet environment"")\n\n\n@pytest.fixture\ndef namespace(request):\n  return request.config.getoption(""--namespace"")\n\n\n@pytest.fixture\ndef env(request):\n  return request.config.getoption(""--env"")\n'"
kubeflow/install-kubeflow/ks_app/vendor/kubeflow/jupyter/tests/jupyter_test.py,0,"b'""""""Test jupyter custom resource.\n\nThis file tests that we can create notebooks using the Jupyter custom resource.\n\nIt is an integration test as it depends on having access to\na Kubeflow cluster with the custom resource test installed.\n\nWe use the pytest framework because\n  1. It can output results in junit format for prow/gubernator\n  2. It has good support for configuring tests using command line arguments\n    (https://docs.pytest.org/en/latest/example/simple.html)\nPython Path Requirements:\n  kubeflow/testing/py - https://github.com/kubeflow/testing/tree/master/py\n    * Provides utilities for testing\n\nManually running the test\n  1. Configure your KUBECONFIG file to point to the desired cluster\n""""""\n\nimport logging\nimport os\nimport subprocess\nimport re\nimport requests\nfrom retrying import retry\nimport six\n\nimport pytest\n\nfrom kubernetes.config import kube_config\nfrom kubernetes import client as k8s_client\nfrom kubeflow.testing import ks_util\nfrom kubeflow.testing import util\n\nGROUP = ""kubeflow.org""\nPLURAL = ""notebooks""\nKIND = ""Notebook""\nVERSION = ""v1alpha1""\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=(\'%(levelname)s|%(asctime)s\'\n            \'|%(pathname)s|%(lineno)d| %(message)s\'),\n    datefmt=\'%Y-%m-%dT%H:%M:%S\',\n)\nlogging.getLogger().setLevel(logging.INFO)\n\n\ndef is_retryable_result(r):\n  if r.status_code in [requests.codes.NOT_FOUND, requests.codes.UNAVAILABLE]:\n    message = ""Request to {0} returned {1}"".format(r.url, r.status_code)\n    logging.error(message)\n    return True\n\n  return False\n\n\n@retry(\n    wait_exponential_multiplier=1000,\n    wait_exponential_max=10000,\n    stop_max_delay=5 * 60 * 1000,\n    retry_on_result=is_retryable_result)\ndef send_request(*args, **kwargs):\n  """"""Send a request to the Jupyter server.\n\n  Sends a request to verify we can fetch the main page for the Jupyter\n  notebook.\n  """"""\n  # We don\'t use util.run because that ends up including the access token\n  # in the logs\n  token = subprocess.check_output([""gcloud"", ""auth"", ""print-access-token""])\n  if six.PY3 and hasattr(token, ""decode""):\n    token = token.decode()\n  token = token.strip()\n\n  headers = {\n      ""Authorization"": ""Bearer "" + token,\n  }\n\n  if ""headers"" not in kwargs:\n    kwargs[""headers""] = {}\n\n  kwargs[""headers""].update(headers)\n\n  r = requests.get(*args, **kwargs)\n\n  # TODO(https://github.com/kubeflow/testing/issues/288): Use selenium\n  # to create a proper test. Jupyter returns a 404 because the page is\n  # using javascript. If we use selenium we can properly fetch the page.\n  pattern = re.compile("".*Jupyter Notebook.*"")\n\n  content = r.content\n  if six.PY3 and hasattr(content, ""decode""):\n    content = content.decode()\n  if r.status_code == requests.codes.NOT_FOUND and pattern.findall(content):\n    r.status_code = 200\n  return r\n\n\ndef test_jupyter(env, namespace):\n  app_credentials = os.getenv(""GOOGLE_APPLICATION_CREDENTIALS"")\n  if app_credentials:\n    logging.info(""Activate service account"")\n    util.run([\n        ""gcloud"", ""auth"", ""activate-service-account"",\n        ""--key-file="" + app_credentials\n    ])\n\n  # util.load_kube_config appears to hang on python3\n  kube_config.load_kube_config()\n  api_client = k8s_client.ApiClient()\n  host = api_client.configuration.host\n  logging.info(""Kubernetes master: %s"", host)\n  master = host.rsplit(""/"", 1)[-1]\n\n  this_dir = os.path.dirname(__file__)\n  app_dir = os.path.join(this_dir, ""test_app"")\n\n  ks_cmd = ks_util.get_ksonnet_cmd(app_dir)\n\n  name = ""jupyter-test""\n  service = ""jupyter-test""\n  component = ""jupyter""\n  params = """"\n  ks_util.setup_ks_app(app_dir, env, namespace, component, params)\n\n  util.run([ks_cmd, ""apply"", env, ""-c"", component], cwd=app_dir)\n  conditions = [""Ready""]\n  results = util.wait_for_cr_condition(api_client, GROUP, PLURAL, VERSION,\n                                       namespace, name, conditions)\n\n  logging.info(""Result of CRD:\\n%s"", results)\n\n  # We proxy the request through the APIServer so that we can connect\n  # from outside the cluster.\n  url = (""https://{master}/api/v1/namespaces/{namespace}/services/{service}:80""\n         ""/proxy/default/jupyter/lab?"").format(\n             master=master, namespace=namespace, service=service)\n  logging.info(""Request: %s"", url)\n  r = send_request(url, verify=False)\n\n  if r.status_code != requests.codes.OK:\n    msg = ""Request to {0} exited with status code: {1} and content: {2}"".format(\n        url, r.status_code, r.content)\n    logging.error(msg)\n    raise RuntimeError(msg)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/main.py,0,"b'import os\nimport sys\nimport logging\nfrom flask_cors import CORS\nfrom kubeflow_jupyter.default.app import app as default\nfrom kubeflow_jupyter.rok.app import app as rok\n\nlogger = logging.getLogger(""entrypoint"")\n\n# Get the UIs\nui = os.environ.get(""UI"", ""default"")\napps = {\n    ""default"": default,\n    ""rok"": rok\n}\n\ntry:\n    app = apps[ui]\n\n    # Enable CORS for dev\n    if ""--enable-cors"" in sys.argv:\n        logger.warning(""Enabling CORS"")\n        CORS(app)\n\n    app.run(host=""0.0.0.0"")\nexcept KeyError:\n    logger.warning(""There is no "" + ui + "" UI to load."")\n    exit(1)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/k8s-model-server/http-proxy/server.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nfrom itertools import repeat\nimport base64\nimport json\nimport logging\nimport random\n\nfrom google.protobuf.json_format import MessageToDict\nimport grpc\nfrom grpc.beta import implementations\nimport numpy as np\nfrom tensorflow_serving.apis import classification_pb2\nfrom tensorflow_serving.apis import input_pb2\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\nfrom tensorflow_serving.apis import get_model_metadata_pb2\nfrom tornado import gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.options import define, options, parse_command_line\nimport tensorflow as tf\nfrom tensorflow.python.saved_model import signature_constants\nimport tornado.web\n\ndefine(""port"", default=8888, help=""run on the given port"", type=int)\ndefine(\n    ""rpc_timeout"",\n    default=1.0,\n    help=""seconds for time out rpc request"",\n    type=float)\ndefine(""rpc_port"", default=9000, help=""tf serving on the given port"", type=int)\ndefine(\n    ""rpc_address"",\n    default=\'localhost\',\n    help=""tf serving on the given address"",\n    type=str)\ndefine(\n    ""instances_key"",\n    default=\'instances\',\n    help=""requested instances json object key"")\ndefine(""debug"", default=False, help=""run in debug mode"")\ndefine(""log_request"", default=False, help=""whether to log requests"")\ndefine(""request_log_file"", default=""/tmp/logs/request.log"")\ndefine(""request_log_pos_file"", default=""/tmp/logs/request.log.pos"")\ndefine(\n    ""request_log_prob"",\n    default=0.01,\n    help=""probability to log the request (will be sampled uniformly)"")\nB64_KEY = \'b64\'\nWELCOME = ""Hello World""\nMODEL_SERVER_METADATA_TIMEOUT_SEC = 20\n\nDATA_TYPE = {\n    np.string_: lambda r: {\n        \'bytes_list\': tf.train.BytesList(value=r)\n    },\n    np.float64: lambda r: {\n        \'float_list\': tf.train.FloatList(value=r)\n    },\n    np.int64: lambda r: {\n        \'int64_list\': tf.train.Int64List(value=r)\n    }\n}\n\n\ndef from_data_to_feature(data):\n  return tf.train.Feature(**DATA_TYPE[data.dtype.type](data))\n\n\ndef prepare_classify_requests(instances, model_name, model_version):\n  request = classification_pb2.ClassificationRequest()\n  request.model_spec.name = model_name\n\n  if model_version is not None:\n    request.model_spec.version = model_version\n\n  instance_examples = []\n  for instance in instances:\n    feature_dict = {}\n    for key, value in instance.items():\n      if not isinstance(value, list):\n        value = [value]\n      feature_dict[key] = from_data_to_feature(np.array(value).ravel())\n    instance_examples.append(\n        tf.train.Example(features=tf.train.Features(feature=feature_dict)))\n\n  request.input.CopyFrom(\n      input_pb2.Input(\n          example_list=input_pb2.ExampleList(examples=instance_examples)))\n  return request\n\n\n# START code took from\n# https://github.com/grpc/grpc/wiki/Integration-with-tornado-(python)\n\n\ndef _fwrap(f, gf):\n  try:\n    f.set_result(gf.result())\n  except Exception as e:\n    f.set_exception(e)\n\n\ndef fwrap(gf, ioloop=None):\n  """"""\n  Wraps a GRPC result in a future that can be yielded by tornado\n  Usage::\n    @coroutine\n    def my_fn(param):\n      result = yield fwrap(stub.function_name.future(param, timeout))\n  """"""\n  f = gen.Future()\n\n  if ioloop is None:\n    ioloop = IOLoop.current()\n\n  gf.add_done_callback(lambda _: ioloop.add_callback(_fwrap, f, gf))\n  return f\n\n\n# END code took from\n# https://github.com/grpc/grpc/wiki/Integration-with-tornado-(python)\n\n\ndef decode_b64_if_needed(data):\n  if isinstance(data, list):\n    return [decode_b64_if_needed(val) for val in data]\n  elif isinstance(data, dict):\n    if data.viewkeys() == {""b64""}:\n      return base64.b64decode(data[""b64""])\n    else:\n      return {k: decode_b64_if_needed(v) for k, v in data.iteritems()}\n  else:\n    return data\n\n\ndef get_signature_map(model_server_stub, model_name):\n  """"""\n  Gets tensorflow signature map from the model server stub.\n\n  Args:\n    model_server_stub: The grpc stub to call GetModelMetadata.\n    model_name: The model name.\n\n  Returns:\n    The signature map of the model.\n  """"""\n  request = get_model_metadata_pb2.GetModelMetadataRequest()\n  request.model_spec.name = model_name\n  request.metadata_field.append(""signature_def"")\n  try:\n    response = model_server_stub.GetModelMetadata(\n        request, MODEL_SERVER_METADATA_TIMEOUT_SEC)\n  except grpc.RpcError as rpc_error:\n    logging.exception(\n        ""GetModelMetadata call to model server failed with code ""\n        ""%s and message %s"", rpc_error.code(), rpc_error.details())\n    return None\n\n  signature_def_map_proto = get_model_metadata_pb2.SignatureDefMap()\n  response.metadata[""signature_def""].Unpack(signature_def_map_proto)\n  signature_def_map = signature_def_map_proto.signature_def\n  if not signature_def_map:\n    logging.error(""Graph has no signatures."")\n\n  # Delete incomplete signatures without input dtypes.\n  invalid_signatures = []\n  for signature_name in signature_def_map:\n    for tensor in signature_def_map[signature_name].inputs.itervalues():\n      if not tensor.dtype:\n        logging.warn(\n            ""Signature %s has incomplete dtypes, removing from ""\n            ""usable signatures"", signature_name)\n        invalid_signatures.append(signature_name)\n        break\n  for signature_name in invalid_signatures:\n    del signature_def_map[signature_name]\n\n  return signature_def_map\n\n\ndef get_signature(signature_map, signature_name=None):\n  """"""\n  Gets tensorflow signature for the given signature_name.\n\n  Args:\n    signature_name: string The signature name to use to choose the signature\n        from the signature map.\n\n  Returns:\n    a pair of signature_name and signature. The first element is the\n    signature name in string that is actually used. The second one is the\n    signature.\n\n  Raises:\n    KeyError: when the signature is not found with the given signature name or\n        when there are more than one signatures in the signature map.\n  """"""\n  # The way to find signature is:\n  # 1) if signature_name is specified, try to find it in the signature_map. If\n  # not found, raise an exception.\n  # 2) if signature_name is not specified, check if signature_map only\n  # contains one entry. If so, return the only signature.\n  # 3) Otherwise, use the default signature_name and do 1).\n  if not signature_name and len(signature_map) == 1:\n    return signature_map.keys()[0], signature_map.values()[0]\n\n  key = (signature_name or\n         signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n  if key in signature_map:\n    return key, signature_map[key]\n  else:\n    raise KeyError(""No signature found for signature key %s."" % signature_name)\n\n\nclass MetadataHandler(tornado.web.RequestHandler):\n  """"""\n  Metadata Handler proxy return Model metadata (Currently it only supports\n  signature map with latest version). Defined here:\n  https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service.proto#L29  # noqa: E501\n  """"""\n\n  @gen.coroutine\n  def get(self, model_name):\n    if not self.settings[\'signature_map\'].get(model_name):\n      self.settings[\'signature_map\'][model_name] = get_signature_map(\n          self.settings[\'stub\'], model_name)\n    signature_map = self.settings[\'signature_map\'][model_name]\n    self.write(\n        dict((key, MessageToDict(value))\n             for key, value in signature_map.items()))\n\n\nclass PredictHandler(tornado.web.RequestHandler):\n  """"""\n  Predict Handler proxy predict method, the input of tf savedModel is\n  expected to be a `Map<strinbg, tf.Tensor>` protobuf. Defined here:\n      https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service.proto#L23  # noqa: E501\n  """"""\n\n  @gen.coroutine\n  def post(self, model_name, version_name=None):\n    if not self.settings[\'signature_map\'].get(model_name):\n      self.settings[\'signature_map\'][model_name] = get_signature_map(\n          self.settings[\'stub\'], model_name)\n\n    request_key = self.settings[\'request_key\']\n    request_data = tornado.escape.json_decode(self.request.body)\n    instances = request_data.get(request_key)\n    if not instances:\n      self.send_error(\n          \'Request json object have to use the key: %s\' % request_key)\n    if len(instances) < 1 or not isinstance(instances, (list, tuple)):\n      self.send_error(\'Request instances object have to use be a list\')\n    instances = decode_b64_if_needed(instances)\n\n    signature_name = request_data.get(""signature_name"")\n    signature_name_used, signature = get_signature(\n        self.settings[\'signature_map\'][model_name], signature_name)\n    input_columns = signature.inputs.keys()\n\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = model_name\n    request.model_spec.signature_name = signature_name_used\n\n    if version_name is not None:\n      request.model_spec.version = version_name\n\n    inputs_type_map = signature.inputs\n    for input_column in input_columns:\n      values = [instance[input_column] for instance in instances]\n      request.inputs[input_column].CopyFrom(\n          tf.make_tensor_proto(values, inputs_type_map[input_column].dtype))\n\n    stub = self.settings[\'stub\']\n    result = yield fwrap(\n        stub.Predict.future(request, self.settings[\'rpc_timeout\']))\n    output_keys = result.outputs.keys()\n    predictions = zip(*[\n        tf.make_ndarray(result.outputs[output_key]).tolist()\n        for output_key in output_keys\n    ])\n    predictions = [dict(zip(*t)) for t in zip(repeat(output_keys), predictions)]\n    self.write(dict(predictions=predictions))\n\n    if self.settings[\'request_logger\'] is not None:\n      for instance in instances:\n        if random.random() < self.settings[\'request_log_prob\']:\n          self.settings[\'request_logger\'].info(json.dumps(instance))\n\n\nclass ClassifyHandler(tornado.web.RequestHandler):\n  """"""\n  Classify Handler proxy classify method, the input of tf savedModel is\n  expected to be a `tf.Examples` protobuf Defined here:\n      https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service.proto#L17  # noqa: E501\n  """"""\n\n  @gen.coroutine\n  def post(self, model, version=None):\n    request_key = self.settings[\'request_key\']\n    request_data = tornado.escape.json_decode(self.request.body)\n    instances = request_data.get(request_key)\n    if not instances:\n      self.send_error(\n          \'Request json object have to use the key: %s\' % request_key)\n\n    if len(instances) < 1 or not isinstance(instances, (list, tuple)):\n      self.send_error(\'Request instances object have to use be a list\')\n\n    instances = decode_b64_if_needed(instances)\n\n    request = prepare_classify_requests(instances, model, version)\n\n    stub = self.settings[\'stub\']\n    result = yield fwrap(\n        stub.Classify.future(request, self.settings[\'rpc_timeout\']))\n\n    self.write(MessageToDict(result))\n\n\nclass IndexHanlder(tornado.web.RequestHandler):\n\n  def get(self):\n    self.write(WELCOME)\n\n\ndef get_application(**settings):\n  return tornado.web.Application([\n      (r""/model/(.*):metadata"", MetadataHandler),\n      (r""/model/(.*):predict"", PredictHandler),\n      (r""/model/(.*):classify"", ClassifyHandler),\n      (r""/model/(.*)/version/(.*):predict"", PredictHandler),\n      (r""/model/(.*)/version/(.*):classify"", ClassifyHandler),\n      (r""/"", IndexHanlder),\n  ],\n                                 xsrf_cookies=False,\n                                 debug=options.debug,\n                                 rpc_timeout=options.rpc_timeout,\n                                 request_key=options.instances_key,\n                                 **settings)\n\n\ndef main():\n  parse_command_line()\n\n  channel = implementations.insecure_channel(options.rpc_address,\n                                             options.rpc_port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n  if options.log_request:\n    request_logger = logging.getLogger(""RequestLogger"")\n    request_logger.setLevel(logging.INFO)\n    rorate_handler = logging.handlers.RotatingFileHandler(\n        options.request_log_file, maxBytes=1000000, backupCount=1)\n    request_logger.addHandler(rorate_handler)\n    # touch the pos file.\n    open(options.request_log_pos_file, ""a"").close()\n  else:\n    request_logger = None\n\n  extra_settings = dict(\n      stub=stub,\n      signature_map={},\n      request_logger=request_logger,\n      request_log_prob=options.request_log_prob,\n  )\n  app = get_application(**extra_settings)\n  app.listen(options.port)\n  logging.info(\'running at http://localhost:%s\' % options.port)\n  tornado.ioloop.IOLoop.current().start()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/k8s-model-server/http-proxy/server_test.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport json\nimport pytest\nfrom tornado import gen\n\nfrom server import decode_b64_if_needed, get_application, WELCOME\n\n\n@pytest.fixture\ndef app():\n  return get_application(stub=None)\n\n\n@pytest.fixture(params=[\'xa\', u\'sada\'])\ndef mock_data(request):\n  return request.param\n\n\ndef predict_template(data):\n  return {\'instances\': [{\'images\': data}]}\n\n\n@pytest.fixture\ndef raw_predict_payload(mock_data):\n  return predict_template({\'b64\': base64.b64encode(mock_data)})\n\n\n@pytest.fixture\ndef predict_payload(raw_predict_payload):\n  return json.dumps(raw_predict_payload)\n\n\ndef test_base64_decodes(raw_predict_payload, mock_data):\n  actual = decode_b64_if_needed(raw_predict_payload)\n  expected = predict_template(mock_data)\n  assert actual == expected\n\n\ndef test_base64_not_doing_extra_1(mock_data):\n  actual = decode_b64_if_needed(mock_data)\n  assert actual == mock_data\n\n\ndef test_base64_not_doing_extra_2(mock_data):\n  actual = decode_b64_if_needed(predict_template(mock_data))\n  assert actual == predict_template(mock_data)\n\n\ndef test_base64_not_doing_extra_3(mock_data):\n  actual = decode_b64_if_needed(predict_template(predict_template(mock_data)))\n  assert actual == predict_template(predict_template(mock_data))\n\n\n@pytest.mark.gen_test\ndef test_index_success(app, http_client, base_url):\n  response = yield http_client.fetch(base_url)\n  assert response.code == 200\n  assert WELCOME in response.body\n\n\n@pytest.mark.gen_test\ndef test_index_get_only(app, http_client, base_url, predict_payload):\n  with pytest.raises(Exception) as e:\n    yield http_client.fetch(base_url, method=\'POST\', body=predict_payload)\n  assert \'Method Not Allowed\' in e.value.message\n\n\ndef test_predict_with_get(app, http_client, base_url, io_loop):\n\n  @gen.coroutine\n  def test_gen():\n    with pytest.raises(Exception) as e:\n      yield http_client.fetch(\'%s/model/name:predict\' % base_url)\n    assert \'Method Not Allowed\' in e.value.message\n\n  io_loop.run_sync(test_gen)\n\n\ndef test_predict_with_post_without_payload(http_client, base_url, io_loop):\n\n  @gen.coroutine\n  def test_gen():\n    with pytest.raises(Exception) as e:\n      yield http_client.fetch(\'%s/model/name:predict\' % base_url, method=\'POST\')\n    assert \'Body must not be None for method POST\' in e.value.message\n\n  io_loop.run_sync(test_gen)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/k8s-model-server/inception-client/label.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nRuns the Inception model being served on the kubeflow model server on an image\nthat you specify.\n\nNote: This file is a modification of the inception client available on the\nTensorFlow Serving GitHub repository:\n  https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/inception_client.py  # noqa: E501\n""""""\n\nfrom __future__ import print_function\n\n# This is a placeholder for a Google-internal import.\n\nimport argparse\nimport tensorflow as tf\nfrom grpc.beta import implementations\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\n\ndef main(image_paths, server, port):\n  channel = implementations.insecure_channel(server, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n  raw_images = []\n  for path in image_paths:\n    with tf.gfile.Open(path) as img:\n      raw_images.append(img.read())\n\n  # Send request\n  # See prediction_service.proto for gRPC request/response details.\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = \'inception\'\n  request.model_spec.signature_name = \'predict_images\'\n  request.inputs[\'images\'].CopyFrom(\n      tf.make_tensor_proto(raw_images, shape=[len(raw_images)]))\n  result = stub.Predict(request, 10.0)  # 10 secs timeout\n  print(result)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(\'Label an image using Inception\')\n  parser.add_argument(\n      \'-s\', \'--server\', help=\'URL of host serving the Inception model\')\n  parser.add_argument(\n      \'-p\',\n      \'--port\',\n      type=int,\n      default=9000,\n      help=\'Port at which Inception model is being served\')\n  parser.add_argument(\n      \'images\',\n      nargs=\'+\',\n      help=\'Paths (local or GCS) to images you would like to label\')\n\n  args = parser.parse_args()\n\n  main(args.images, args.server, args.port)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/openmpi-controller/controller/__init__.py,0,b''
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/openmpi-controller/controller/controller.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nfrom kubernetes import client, config\nfrom kubernetes.config.config_exception import ConfigException\nfrom pathlib import Path\n\nfrom util import log, api_retry, long_poll, s3_copy\n\nSIG_DIR = \'.openmpi-controller\'\nSIGCONT = f\'{SIG_DIR}/SIGCONT\'\nSIGTERM = f\'{SIG_DIR}/SIGTERM\'\nPHASE_SUCCEEDED = \'Succeeded\'\nPHASE_FAILED = \'Failed\'\nNVIDIA_VERSION_PATH = \'/proc/driver/nvidia/version\'\n\n\nclass Controller:\n  """"""\n  Controller is a sidecar container that extends the ""main"" container\n  (openmpi-job). It communicates with the main container using a shared volume\n  mounted at the working directory. It communicates with the master pod using\n  kubernetes API.\n  """"""\n\n  def __init__(self, namespace, master, num_gpus, timeout_secs,\n               download_data_from, download_data_to, upload_data_from,\n               upload_data_to):\n    self.namespace = namespace\n    self.master = master\n    self.num_gpus = num_gpus\n    self.timeout_secs = timeout_secs\n    self.download_data_from = download_data_from\n    self.download_data_to = download_data_to\n    self.upload_data_from = upload_data_from\n    self.upload_data_to = upload_data_to\n    self._validate_args()\n    Path(SIG_DIR).mkdir()\n\n  def __enter__(self):\n    log(\'controller entered\')\n    try:\n      config.load_incluster_config()\n    except ConfigException:\n      config.load_kube_config()\n\n    self.api = client.CoreV1Api()\n    return self\n\n  def __exit__(self, exc_type, exc_val, exc_tb):\n    log(\'controller exited\')\n    Path(SIGTERM).touch()\n\n  def wait_ready(self):\n    if self.num_gpus > 0:\n      self._wait_nvidia_driver_present()\n    self._download_data()\n    Path(SIGCONT).touch()\n\n  def wait_done(self):\n    self._wait_master_terminated()\n    self._upload_data()\n\n  def _validate_args(self):\n    if (self.download_data_from and\n        self.download_data_to) or (self.upload_data_from and\n                                   self.upload_data_to):\n      if not os.environ.get(\'AWS_ACCESS_KEY_ID\'):\n        raise ValueError(\'AWS_ACCESS_KEY_ID not set\')\n\n      if not os.environ.get(\'AWS_SECRET_ACCESS_KEY\'):\n        raise ValueError(\'AWS_SECRET_ACCESS_KEY not set\')\n\n  def _wait_nvidia_driver_present(self):\n    log(\'waiting for nvidia driver to be installed\')\n    long_poll(self._poll_nvidia_driver_version, timeout_secs=self.timeout_secs)\n\n  def _wait_master_terminated(self):\n    log(\'waiting for master to terminate\')\n    long_poll(self._poll_master_phase)\n\n  def _poll_nvidia_driver_version(self):\n    # Driver installer is expected to be installed externally.\n    # See https://cloud.google.com/kubernetes-engine/docs/concepts/gpus#installing_drivers for GCP instructions.  # noqa: E501\n    version_path = Path(NVIDIA_VERSION_PATH)\n    if not version_path.exists():\n      log(f\'nvidia driver not found\')\n      return None\n    version = version_path.read_text()\n    log(f\'nvidia version: {version}\')\n    return version\n\n  def _poll_master_phase(self):\n    phase = self._query_master_phase()\n    log(f\'{self.master} is in ""{phase}"" phase\')\n    if phase not in (PHASE_SUCCEEDED, PHASE_FAILED):\n      return None\n    return phase\n\n  @api_retry\n  def _query_master_phase(self):\n    pod = self.api.read_namespaced_pod(self.master, self.namespace)\n    return pod.status.phase\n\n  def _download_data(self):\n    if self.download_data_from and self.download_data_to:\n      Path(self.download_data_to).mkdir(exist_ok=True)\n      log(f\'downloading data from {self.download_data_from} to \'\n          \'{self.download_data_to}\')\n      s3_copy(self.download_data_from, self.download_data_to)\n\n  def _upload_data(self):\n    if self.upload_data_from and self.upload_data_to:\n      if Path(self.upload_data_from).exists():\n        log(f\'uploading data from {self.upload_data_from} to \'\n            \'{self.upload_data_to}\')\n        s3_copy(self.upload_data_from, self.upload_data_to)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/openmpi-controller/controller/main.py,0,"b""# -*- coding: utf-8 -*-\nfrom argparse import ArgumentParser\n\nfrom controller import Controller\n\n\ndef main():\n  parser = ArgumentParser()\n  parser.add_argument('--namespace', type=str, required=True)\n  parser.add_argument('--master', type=str, required=True)\n  parser.add_argument('--num-gpus', type=int, default=0)\n  parser.add_argument('--timeout-secs', type=int, default=300)\n  parser.add_argument('--download-data-from', type=str)\n  parser.add_argument('--download-data-to', type=str)\n  parser.add_argument('--upload-data-from', type=str)\n  parser.add_argument('--upload-data-to', type=str)\n  args = parser.parse_args()\n\n  with Controller(\n      namespace=args.namespace,\n      master=args.master,\n      num_gpus=args.num_gpus,\n      timeout_secs=args.timeout_secs,\n      download_data_from=args.download_data_from,\n      download_data_to=args.download_data_to,\n      upload_data_from=args.upload_data_from,\n      upload_data_to=args.upload_data_to) as ctl:\n    ctl.wait_ready()\n    ctl.wait_done()\n\n\nif __name__ == '__main__':\n  main()\n"""
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/openmpi-controller/controller/util.py,0,"b'# -*- coding: utf-8 -*-\nfrom kubernetes.client.rest import ApiException\nfrom retrying import retry\nfrom subprocess import Popen, PIPE\n\nRETRY_MAX_ATTEMPTS = 5\nRETRY_BACKOFF_MS = 1000\nPOLL_BACKOFF_MS = 10000\n\napi_retry = retry(\n    stop_max_attempt_number=RETRY_MAX_ATTEMPTS,\n    wait_exponential_multiplier=RETRY_BACKOFF_MS,\n    retry_on_exception=lambda e: isinstance(e, ApiException))\n\n\nclass S3Exception(Exception):\n  pass\n\n\ndef log(msg):\n  print(msg, flush=True)\n\n\ndef long_poll(poll_fn, timeout_secs=None):\n\n  @retry(\n      stop_max_delay=timeout_secs * 1000 if timeout_secs else None,\n      wait_fixed=POLL_BACKOFF_MS,\n      retry_on_exception=lambda _: False,\n      retry_on_result=lambda result: not result)\n  def poll_wrapper():\n    return poll_fn()\n\n  return poll_wrapper()\n\n\ndef exec_command(command):\n  process = Popen(\n      command, stdin=None, stdout=PIPE, stderr=PIPE, shell=True, close_fds=True)\n  stdout, stderr = process.communicate()\n  return process.returncode, stdout, stderr\n\n\n@retry(\n    stop_max_attempt_number=RETRY_MAX_ATTEMPTS,\n    wait_exponential_multiplier=RETRY_BACKOFF_MS,\n    retry_on_exception=lambda e: isinstance(e, S3Exception))\ndef s3_copy(copy_from, copy_to):\n  exit_code, stdout, stderr = exec_command(\n      f\'aws s3 cp --recursive ""{copy_from}"" ""{copy_to}""\')\n  if exit_code != 0:\n    raise S3Exception(f\'s3 copy failed with exit code \'\n                      \'{exit_code}:\\nstdout:{stdout}\\nstderr:{stderr}\')\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/kubeflow/jupyter/tests/conftest.py,0,"b'# -*- coding: utf-8 -*-\nimport pytest\n\n\ndef pytest_addoption(parser):\n  parser.addoption(\n      ""--namespace"", action=""store"", default="""", help=""namespace  to use"")\n\n  parser.addoption(\n      ""--env"",\n      action=""store"",\n      default=""jupytertest"",\n      help=""ksonnet environment"")\n\n\n@pytest.fixture\ndef namespace(request):\n  return request.config.getoption(""--namespace"")\n\n\n@pytest.fixture\ndef env(request):\n  return request.config.getoption(""--env"")\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/kubeflow/jupyter/tests/jupyter_test.py,0,"b'""""""Test jupyter custom resource.\n\nThis file tests that we can create notebooks using the Jupyter custom resource.\n\nIt is an integration test as it depends on having access to\na Kubeflow cluster with the custom resource test installed.\n\nWe use the pytest framework because\n  1. It can output results in junit format for prow/gubernator\n  2. It has good support for configuring tests using command line arguments\n    (https://docs.pytest.org/en/latest/example/simple.html)\nPython Path Requirements:\n  kubeflow/testing/py - https://github.com/kubeflow/testing/tree/master/py\n    * Provides utilities for testing\n\nManually running the test\n  1. Configure your KUBECONFIG file to point to the desired cluster\n""""""\n\nimport logging\nimport os\nimport subprocess\nimport re\nimport requests\nfrom retrying import retry\nimport six\n\nimport pytest\n\nfrom kubernetes.config import kube_config\nfrom kubernetes import client as k8s_client\nfrom kubeflow.testing import ks_util\nfrom kubeflow.testing import util\n\nGROUP = ""kubeflow.org""\nPLURAL = ""notebooks""\nKIND = ""Notebook""\nVERSION = ""v1alpha1""\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=(\'%(levelname)s|%(asctime)s\'\n            \'|%(pathname)s|%(lineno)d| %(message)s\'),\n    datefmt=\'%Y-%m-%dT%H:%M:%S\',\n)\nlogging.getLogger().setLevel(logging.INFO)\n\n\ndef is_retryable_result(r):\n  if r.status_code in [requests.codes.NOT_FOUND, requests.codes.UNAVAILABLE]:\n    message = ""Request to {0} returned {1}"".format(r.url, r.status_code)\n    logging.error(message)\n    return True\n\n  return False\n\n\n@retry(\n    wait_exponential_multiplier=1000,\n    wait_exponential_max=10000,\n    stop_max_delay=5 * 60 * 1000,\n    retry_on_result=is_retryable_result)\ndef send_request(*args, **kwargs):\n  """"""Send a request to the Jupyter server.\n\n  Sends a request to verify we can fetch the main page for the Jupyter\n  notebook.\n  """"""\n  # We don\'t use util.run because that ends up including the access token\n  # in the logs\n  token = subprocess.check_output([""gcloud"", ""auth"", ""print-access-token""])\n  if six.PY3 and hasattr(token, ""decode""):\n    token = token.decode()\n  token = token.strip()\n\n  headers = {\n      ""Authorization"": ""Bearer "" + token,\n  }\n\n  if ""headers"" not in kwargs:\n    kwargs[""headers""] = {}\n\n  kwargs[""headers""].update(headers)\n\n  r = requests.get(*args, **kwargs)\n\n  # TODO(https://github.com/kubeflow/testing/issues/288): Use selenium\n  # to create a proper test. Jupyter returns a 404 because the page is\n  # using javascript. If we use selenium we can properly fetch the page.\n  pattern = re.compile("".*Jupyter Notebook.*"")\n\n  content = r.content\n  if six.PY3 and hasattr(content, ""decode""):\n    content = content.decode()\n  if r.status_code == requests.codes.NOT_FOUND and pattern.findall(content):\n    r.status_code = 200\n  return r\n\n\ndef test_jupyter(env, namespace):\n  app_credentials = os.getenv(""GOOGLE_APPLICATION_CREDENTIALS"")\n  if app_credentials:\n    logging.info(""Activate service account"")\n    util.run([\n        ""gcloud"", ""auth"", ""activate-service-account"",\n        ""--key-file="" + app_credentials\n    ])\n\n  # util.load_kube_config appears to hang on python3\n  kube_config.load_kube_config()\n  api_client = k8s_client.ApiClient()\n  host = api_client.configuration.host\n  logging.info(""Kubernetes master: %s"", host)\n  master = host.rsplit(""/"", 1)[-1]\n\n  this_dir = os.path.dirname(__file__)\n  app_dir = os.path.join(this_dir, ""test_app"")\n\n  ks_cmd = ks_util.get_ksonnet_cmd(app_dir)\n\n  name = ""jupyter-test""\n  service = ""jupyter-test""\n  component = ""jupyter""\n  params = """"\n  ks_util.setup_ks_app(app_dir, env, namespace, component, params)\n\n  util.run([ks_cmd, ""apply"", env, ""-c"", component], cwd=app_dir)\n  conditions = [""Running""]\n  results = util.wait_for_cr_condition(api_client, GROUP, PLURAL, VERSION,\n                                       namespace, name, conditions)\n\n  logging.info(""Result of CRD:\\n%s"", results)\n\n  # We proxy the request through the APIServer so that we can connect\n  # from outside the cluster.\n  url = (""https://{master}/api/v1/namespaces/{namespace}/services/{service}:80""\n         ""/proxy/default/jupyter/lab?"").format(\n             master=master, namespace=namespace, service=service)\n  logging.info(""Request: %s"", url)\n  r = send_request(url, verify=False)\n\n  if r.status_code != requests.codes.OK:\n    msg = ""Request to {0} exited with status code: {1} and content: {2}"".format(\n        url, r.status_code, r.content)\n    logging.error(msg)\n    raise RuntimeError(msg)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/kubeflow/profiles/tests/profiles_test.py,0,"b'""""""Test Profiles custom resource.\n\ntodo\n\n""""""\n\nimport logging\nimport os\nimport subprocess\nimport re\nimport requests\nfrom retrying import retry\nimport six\nimport time\n\nimport pytest\n\nfrom kubernetes.config import kube_config\nfrom kubernetes import client as k8s_client\nfrom kubernetes.client.rest import ApiException\nfrom kubeflow.testing import util\n\nGROUP = ""kubeflow.org""\nPLURAL = ""profiles""\nKIND = ""Profile""\nVERSION = ""v1alpha1""\n\nlogging.basicConfig(level=logging.INFO,\n                    format=(\'%(levelname)s|%(asctime)s\'\n                            \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                    datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                    )\nlogging.getLogger().setLevel(logging.INFO)\n\ndef test_profiles():\n  app_credentials = os.getenv(""GOOGLE_APPLICATION_CREDENTIALS"")\n  if app_credentials:\n    logging.info(""Activate service account"")\n    util.run([""gcloud"", ""auth"", ""activate-service-account"",\n              ""--key-file="" + app_credentials])\n\n  # util.load_kube_config appears to hang on python3\n  kube_config.load_kube_config()\n  api_client = k8s_client.ApiClient()\n\n  this_dir = os.path.dirname(__file__)\n  util.run([""kubectl"", ""apply"", ""-f"", ""sample_profile.yaml""], cwd=this_dir)\n\n  # TODO: check CR status/condition instead of sleep\n  # conditions = [""Ready""]\n  # namespace = ""kubeflow""\n  # name = ""john""\n  # results = util.wait_for_cr_condition(api_client, GROUP, PLURAL, VERSION,\n  #                                      namespace, name, conditions)\n  # logging.info(""Result of CRD:\\n%s"", results)\n  time.sleep(10)\n\n  # Verifies the namespace is created.\n  name = ""john""  # The name of the profile, also the new namespace\'s name.\n  coreV1 = k8s_client.CoreV1Api(api_client)\n  retry_read_namespace = retry(\n    wait_exponential_multiplier=1000,  # wait 2^i * 1000 ms, on the i-th retry\n    wait_exponential_max=60000,  # 60 sec max\n  )(coreV1.read_namespace)\n  resp = retry_read_namespace(name)\n  logging.info(""found namespace: %s"", resp)\n\n  rbacV1 = k8s_client.RbacAuthorizationV1Api(api_client)\n  resp = rbacV1.read_namespaced_role(""edit"", name)\n  logging.info(""role: %s"", resp)\n  resp = rbacV1.read_namespaced_role_binding(""default"", name)\n  logging.info(""role binding: %s"", resp)\n\n  # delete the profile and make sure namespace is deleted\n  util.run([""kubectl"", ""delete"", ""-f"", ""sample_profile.yaml""], cwd=this_dir)\n  time.sleep(15)\n\n  with pytest.raises(ApiException) as e:\n    resp = coreV1.read_namespace(name)\n  logging.info(""exception info: %s"", e)\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/py/kubeflow/kubeflow/__init__.py,0,b''
kubeflow/install-kubeflow/ks_app/vendor/kubeflow/jupyter/ui/default/spawner.py,0,"b'# -*- coding: utf-8 -*-\nimport json\nimport yaml\nimport string\nimport escapism\nfrom tornado import gen\nfrom traitlets import Dict\nfrom jinja2 import FileSystemLoader, Environment\n\nfrom kubespawner.objects import make_pvc\nfrom kubespawner.spawner import KubeSpawner\nfrom kubernetes.client.rest import ApiException\n\nSERVICE_ACCOUNT_SECRET_MOUNT = \'/var/run/secrets/sa\'\n\n\nclass KubeFormSpawner(KubeSpawner):\n  """"""Implement a custom Spawner to spawn pods in a Kubernetes Cluster.""""""\n\n  def __init__(self, *args, **kwargs):\n    """"""Call init() of parent class and initialize volume lists.""""""\n    super(KubeFormSpawner, self).__init__(*args, **kwargs)\n    self.initial_volumes = list(self.volumes)\n    self.initial_volume_mounts = list(self.volume_mounts)\n\n  @property\n  def spawner_ui_config(self):\n    # Read raw YAML file, format it and parse it as dict\n    if not hasattr(self, ""_spawner_ui_config""):\n      c = None\n      try:\n        with open(\'/etc/config/spawner_ui_config.yaml\', \'r\') as f:\n          c = self._expand_user_properties(f.read())\n      except IOError:\n        self.log.warning(\'Error opening Spawner UI config file\')\n\n      try:\n        if yaml.safe_load(c) is None:\n          # YAML exists but is empty\n          self._spawner_ui_config = {}\n        else:\n          # YAML exists and is not empty\n          self._spawner_ui_config = yaml.safe_load(c)\n      except yaml.YAMLError as e:\n        self.log.warning(\n            \'Spawner UI config file contains\'\n            \'invalid YAML syntax: {}\', e)\n        return None\n\n    return self._spawner_ui_config\n\n  extra_spawner_config = Dict({},\n                              config=True,\n                              help=""""""\n        A dictionary with extra configuration parameters for KubeFormSpawner.\n        """""")\n\n  def options_form(self, form):\n    # Create Jinja environment to dynamically load templates\n    j2_env = Environment(loader=FileSystemLoader(\'/etc/config\'))\n\n    # Get available PVCs in a given namespace\n    # This is a blocking K8s API call\n    existing_pvcs = self._list_pvcs_in_namespace(self.namespace)\n\n    form_defaults = None\n    if self.spawner_ui_config is not None:\n      # YAML exists and was parsed successfully\n      if self.spawner_ui_config[\'spawnerFormDefaults\'] is not None:\n        form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n      else:\n        form_defaults = {}\n\n    # Return the rendered template as a unicode string\n    return j2_env.get_template(\'template.html\').render(\n        form_defaults=form_defaults,\n        existing_pvcs=existing_pvcs,\n        username=self._expand_user_properties(\'{username}\'))\n\n  def options_from_form(self, formdata):\n    options = {}\n    if self.spawner_ui_config is not None:\n      form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n\n    # Manage Image\n    image_readonly = False\n    if self._default_config_contains(\'image\'):\n      options[\'image\'] = form_defaults[\'image\'][\'value\']\n      image_readonly = form_defaults[\'image\'].get(\'readOnly\', False)\n    if (\'image\' in formdata and formdata[\'image\'][0]):\n      image_from_form = formdata[\'image\'][0].strip()\n      if image_readonly:\n        # Provided image must be standard\n        if image_from_form in form_defaults[\'image\'][\'options\']:\n          options[\'image\'] = image_from_form\n      else:\n        # Provided image can be standard or custom\n        options[\'image\'] = image_from_form\n\n    # Manage CPU\n    cpu_readonly = False\n    if self._default_config_contains(\'cpu\'):\n      options[\'cpu\'] = form_defaults[\'cpu\'][\'value\']\n      cpu_readonly = form_defaults[\'cpu\'].get(\'readOnly\', False)\n    if (not cpu_readonly and \'cpu\' in formdata and formdata[\'cpu\'][0]):\n      options[\'cpu\'] = formdata[\'cpu\'][0].strip()\n\n    # Manage Memory\n    memory_readonly = False\n    if self._default_config_contains(\'memory\'):\n      options[\'memory\'] = form_defaults[\'memory\'][\'value\']\n      memory_readonly = form_defaults[\'memory\'].get(\'readOnly\', False)\n    if (not memory_readonly and \'memory\' in formdata and formdata[\'memory\'][0]):\n      options[\'memory\'] = formdata[\'memory\'][0].strip()\n\n    # Manage Workspace Volume\n    options[\'workspaceVolume\'] = {}\n    ws_volume = {}\n\n    ws_volume_readonly = False\n    if self._default_config_contains(\'workspaceVolume\'):\n      ws_volume_readonly = \\\n          form_defaults[\'workspaceVolume\'].get(\'readOnly\', False)\n\n      # The Workspace Volume is specified in `config.yaml`\n      default_ws_volume = form_defaults[\'workspaceVolume\'][\'value\']\n\n      # Get and set the default values from the YAML configuration file,\n      # if present and not marked as readonly\n      ws_type_readonly = False\n      if (\'type\' in default_ws_volume and \'value\' in default_ws_volume[\'type\']):\n        ws_volume[\'type\'] = default_ws_volume[\'type\'][\'value\']\n        ws_type_readonly = \\\n            default_ws_volume[\'type\'].get(\'readOnly\', False)\n\n      ws_name_readonly = False\n      if (\'name\' in default_ws_volume and \'value\' in default_ws_volume[\'name\']):\n        ws_volume[\'name\'] = default_ws_volume[\'name\'][\'value\']\n        ws_name_readonly = \\\n            default_ws_volume[\'name\'].get(\'readOnly\', False)\n\n      ws_size_readonly = False\n      if (\'size\' in default_ws_volume and \'value\' in default_ws_volume[\'size\']):\n        ws_volume[\'size\'] = \\\n            \'%sGi\' % default_ws_volume[\'size\'][\'value\']\n        ws_size_readonly = \\\n            default_ws_volume[\'size\'].get(\'readOnly\', False)\n\n      ws_mount_path_readonly = False\n      if (\'mountPath\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'mountPath\']):\n        ws_volume[\'mountPath\'] = \\\n            default_ws_volume[\'mountPath\'][\'value\']\n        ws_mount_path_readonly = \\\n            default_ws_volume[\'mountPath\'].get(\'readOnly\', False)\n\n      ws_access_modes_readonly = False\n      if (\'accessModes\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'accessModes\']):\n        ws_volume[\'accessModes\'] = \\\n            default_ws_volume[\'accessModes\'][\'value\']\n        ws_access_modes_readonly = \\\n            default_ws_volume[\'accessModes\'].get(\'readOnly\', False)\n\n    # Get and set the Workspace Volume values from the form, if present\n    # and not marked as readonly\n    if not ws_volume_readonly:\n      if (not ws_type_readonly and \'ws_type\' in formdata and\n          formdata[\'ws_type\'][0]):\n        ws_volume[\'type\'] = formdata[\'ws_type\'][0].strip()\n\n      if (not ws_name_readonly and \'ws_name\' in formdata and\n          formdata[\'ws_name\'][0]):\n        ws_volume[\'name\'] = formdata[\'ws_name\'][0].strip()\n\n      if (not ws_size_readonly and \'ws_size\' in formdata and\n          formdata[\'ws_size\'][0]):\n        ws_volume[\'size\'] = \'%sGi\' % formdata[\'ws_size\'][0].strip()\n\n      if (not ws_mount_path_readonly and \'ws_mount_path\' in formdata and\n          formdata[\'ws_mount_path\'][0]):\n        ws_volume[\'mountPath\'] = \\\n            formdata[\'ws_mount_path\'][0].strip()\n\n      if (not ws_access_modes_readonly and \'ws_access_modes\' in formdata and\n          formdata[\'ws_access_modes\'][0]):\n        ws_volume[\'accessModes\'] = \\\n            formdata[\'ws_access_modes\'][0].strip()\n\n    options[\'workspaceVolume\'] = ws_volume\n\n    # Manage Data Volumes\n    options[\'dataVolumes\'] = []\n    data_volumes_readonly = False\n    if self._default_config_contains(\'dataVolumes\'):\n      data_volumes_readonly = \\\n          form_defaults[\'dataVolumes\'].get(\'readOnly\', False)\n\n    if data_volumes_readonly:\n      # Set Data Volumes as specified in the Spawner configuration file\n      for volume in form_defaults[\'dataVolumes\'][\'value\']:\n        data_volume = {}\n        for f in [\'type\', \'name\', \'size\', \'mountPath\', \'accessModes\']:\n          data_volume[f] = volume[\'value\'][f][\'value\']\n        data_volume[\'size\'] += \'Gi\'\n        options[\'dataVolumes\'].append(data_volume)\n    else:\n      # Deduce the total number of Data Volumes\n      data_volumes_cnt = 0\n      for k, v in formdata.items():\n        if k.startswith(\'vol_type\'):\n          data_volumes_cnt += 1\n\n      # Set Data Volumes as specified in the Spawner form\n      for i in range(1, data_volumes_cnt + 1):\n        data_volume = {}\n\n        # Get all Data Volume fields from the form\n        id = \'vol_type\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'type\'] = formdata[id][0].strip()\n\n        id = \'vol_name\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'name\'] = formdata[id][0].strip()\n\n        id = \'vol_size\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'size\'] = \'%sGi\' % formdata[id][0].strip()\n\n        id = \'vol_mount_path\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'mountPath\'] = formdata[id][0].strip()\n\n        id = \'vol_access_modes\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'accessModes\'] = formdata[id][0].strip()\n\n        options[\'dataVolumes\'].append(data_volume)\n\n    # Manage Extra Resources\n    extra_resources_readonly = False\n    if self._default_config_contains(\'extraResources\'):\n      options[\'extraResources\'] = (form_defaults[\'extraResources\'][\'value\'])\n      extra_resources_readonly = \\\n          form_defaults[\'extraResources\'].get(\'readOnly\', False)\n    if (not extra_resources_readonly and \'extraResources\' in formdata and\n        formdata[\'extraResources\'][0]):\n      options[\'extraResources\'] = \\\n          formdata[\'extraResources\'][0].strip()\n\n    return options\n\n  @property\n  def singleuser_image_spec(self):\n    return self.user_options[\'image\']\n\n  image_spec = singleuser_image_spec\n\n  @property\n  def cpu_guarantee(self):\n    return self.user_options[\'cpu\']\n\n  @property\n  def mem_guarantee(self):\n    return self.user_options[\'memory\']\n\n  @property\n  def workspace_volume(self):\n    return self.user_options[""workspaceVolume""]\n\n  @property\n  def data_volumes(self):\n    return self.user_options[""dataVolumes""]\n\n  @property\n  def extra_resource_limits(self):\n    extra = \'\'\n    if self.user_options[\'extraResources\']:\n      extra = json.loads(self.user_options[\'extraResources\'])\n    return extra\n\n  def get_env(self):\n    env = super(KubeFormSpawner, self).get_env()\n    gcp_secret_name = self.extra_spawner_config[\'gcp_secret_name\']\n    if gcp_secret_name:\n      env[\'GOOGLE_APPLICATION_CREDENTIALS\'] = \'{}/{}.json\'.format(\n          SERVICE_ACCOUNT_SECRET_MOUNT, gcp_secret_name)\n    return env\n\n  # TODO(kkasravi): add unit test\n  def _parse_user_name(self, username):\n    safe_chars = set(string.ascii_lowercase + string.digits)\n    name = username.split(\':\')[-1]\n    legacy = \'\'.join([s if s in safe_chars else \'-\' for s in name.lower()])\n    safe = escapism.escape(name, safe=safe_chars, escape_char=\'-\').lower()\n    return legacy, safe, name\n\n  def _expand_user_properties(self, template):\n    # Override KubeSpawner method to remove prefix accounts.google: for iap\n    legacy, safe, name = self._parse_user_name(self.user.name)\n\n    # Set servername based on whether named-server initialised\n    if self.name:\n      servername = \'-{}\'.format(self.name)\n    else:\n      servername = \'\'\n\n    rname = template.format(\n        userid=self.user.id,\n        username=safe,\n        unescaped_username=name,\n        legacy_escape_username=legacy,\n        servername=servername,\n    )\n    return rname\n\n  def _default_config_contains(self, option):\n    """"""Check if config.yaml contains a value for a Spawner option.""""""\n    if self.spawner_ui_config is not None:\n      form_defaults = None\n      if \'spawnerFormDefaults\' in self.spawner_ui_config:\n        form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n\n      if form_defaults is not None and option in form_defaults:\n        if \'value\' in form_defaults[option]:\n          return True\n    return False\n\n  def _get_pvc_manifest(self, name, storage_class, access_modes, storage,\n                        labels, annotations):\n    """"""\n    Return a PVC spec based on the given parameters.\n    This manifest will be used to create PVCs in the K8s cluster.\n    """"""\n    return make_pvc(\n        name=name,\n        storage_class=storage_class,\n        access_modes=access_modes,\n        storage=storage,\n        labels=labels,\n        annotations=annotations)\n\n  def _list_pvcs_in_namespace(self, namespace):\n    """"""\n    Return a list with all non-failed PVCs in a K8s namespace.\n    Each list entry is a dict with `name`, `size` and `access_modes` keys.\n    """"""\n    existing_pvcs = []\n\n    try:\n      resp = self.api.list_namespaced_persistent_volume_claim(\n          namespace=namespace, watch=False)\n\n    except ApiException as e:\n      self.log.warn(\'Could not list PVCs in %s: %s\', namespace, e)\n      raise\n\n    # Iterate over all existing PVCs and return all non-failed ones\n    for pvc in [pvc for pvc in resp.items if pvc.status.phase != \'Failed\']:\n      existing_pvcs.append({\n          ""name"":\n          pvc.metadata.name,\n          ""size"":\n          pvc.spec.resources.requests.get(\'storage\')[:-2],\n          ""access_modes"":\n          pvc.spec.access_modes\n      })\n\n    return existing_pvcs\n\n  @gen.coroutine\n  def _prepare_volumes(self):\n    """"""Create PVC manifests and attach as volumes to the Notebook.""""""\n    # Reset Volumes and VolumeMounts to initial KubeSpawner values\n    self.volumes = list(self.initial_volumes)\n    self.volume_mounts = list(self.initial_volume_mounts)\n\n    # Workspace and Data Volumes are managed as PVCs\n    persistent_volumes = [self.workspace_volume] + self.data_volumes\n\n    for (idx, volume) in enumerate(persistent_volumes):\n      if volume[\'type\'] == \'New\':\n        yield self._provision_new_pvc(volume, self.namespace)\n      elif volume[\'type\'] == \'Existing\':\n        yield self._get_existing_pvc(volume[\'name\'], self.namespace)\n\n      # Upon success, mount PVC as a volume\n      self.volumes.append({\n          \'name\': \'volume-%d-{username}\' % idx,\n          \'persistentVolumeClaim\': {\n              \'claimName\': volume[\'name\']\n          }\n      })\n\n      self.volume_mounts.append({\n          \'mountPath\': volume[\'mountPath\'],\n          \'name\': \'volume-%d-{username}\' % idx\n      })\n\n  @gen.coroutine\n  def _provision_new_pvc(self, volume, namespace):\n    """"""Issue a K8s API request to create a new, namespaced PVC.""""""\n    labels = self._build_common_labels(\n        self._expand_all(self.user_storage_extra_labels))\n    labels.update({\'component\': \'singleuser-storage\'})\n    annotations = self._build_common_annotations({})\n\n    # Create a V1PersistentVolumeClaim for the API call\n    pvc_manifest = self._get_pvc_manifest(\n        name=volume[\'name\'],\n        storage_class=self.extra_spawner_config[\'storage_class\'],\n        access_modes=[volume[\'accessModes\']],\n        storage=volume[\'size\'],\n        labels=labels,\n        annotations=annotations)\n\n    pvc = None\n    try:\n      pvc = yield self.asynchronize(\n          self.api.create_namespaced_persistent_volume_claim,\n          namespace=namespace,\n          body=pvc_manifest)\n\n    except ApiException as e:\n      if e.status == 409:\n        self.log.warning(\'PVC %s already exists. New PVC not created.\',\n                         volume[\'name\'])\n      self.log.info(e.reason)\n      raise\n\n    self.log.info(\'PVC %s was successfully created\', volume[\'name\'])\n    return pvc\n\n  @gen.coroutine\n  def _get_existing_pvc(self, pvc_name, namespace):\n    """"""Issue a K8s API request to retrieve a namespaced PVC.""""""\n    pvc = None\n\n    try:\n      pvc = yield self.asynchronize(\n          self.api.read_namespaced_persistent_volume_claim,\n          name=pvc_name,\n          namespace=namespace)\n\n    except ApiException as e:\n      self.log.warning(\'PVC %s could not be retrieved: %s\', pvc_name, e)\n      raise\n\n    self.log.info(\'PVC %s was successfully retrieved\', pvc_name)\n    return pvc\n\n  @gen.coroutine\n  def start(self):\n    """"""Override KubeSpawner class start method.""""""\n    yield self._prepare_volumes()\n    _start = yield super(KubeFormSpawner, self).start()\n    return _start\n'"
kubeflow/install-kubeflow/ks_app/vendor/kubeflow/jupyter/ui/rok/spawner.py,0,"b'# -*- coding: utf-8 -*-\nimport base64\nfrom tornado import gen\nfrom jinja2 import FileSystemLoader, Environment\nfrom kubernetes.client.rest import ApiException\nfrom kubernetes.client.models import V1DeleteOptions\nfrom importlib.util import spec_from_file_location, module_from_spec\n\n# Import the default KubeFormSpawner as a Python module\n# Our custom spawner extends the default one, but shares the same class name\nspec = spec_from_file_location(\'spawner\', \'/etc/config/default_spawner.py\')\nspawner = module_from_spec(spec)\nspec.loader.exec_module(spawner)\n\nROK_SECRET_MOUNT = \'/var/run/secrets/rok\'\n\n\nclass KubeFormSpawner(spawner.KubeFormSpawner):\n  """"""Implement a custom Spawner to spawn pods in a Kubernetes Cluster.""""""\n\n  def options_form(self, form):\n    # Create Jinja environment to dynamically load templates\n    j2_env = Environment(loader=FileSystemLoader(\'/etc/config\'))\n\n    form_defaults = None\n    if self.spawner_ui_config is not None:\n      # YAML exists and was parsed successfully\n      if self.spawner_ui_config[\'spawnerFormDefaults\'] is not None:\n        form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n      else:\n        form_defaults = {}\n\n    secret_name = self._expand_user_properties(\n        self.extra_spawner_config[\'rok_secret_name\'])\n\n    rok_token = self._get_rok_token(name=secret_name, namespace=self.namespace)\n\n    # Return the rendered template as a unicode string\n    return j2_env.get_template(\'template.html\').render(\n        form_defaults=form_defaults,\n        rok_token=rok_token,\n        username=self._expand_user_properties(\'{username}\'),\n        namespace=self.namespace)\n\n  def options_from_form(self, formdata):\n    options = {}\n    if self.spawner_ui_config is not None:\n      form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n\n    # Manage Image\n    image_readonly = False\n    if self._default_config_contains(\'image\'):\n      options[\'image\'] = form_defaults[\'image\'][\'value\']\n      image_readonly = form_defaults[\'image\'].get(\'readOnly\', False)\n    if (\'image\' in formdata and formdata[\'image\'][0]):\n      image_from_form = formdata[\'image\'][0].strip()\n      if image_readonly:\n        # Provided image must be standard\n        if image_from_form in form_defaults[\'image\'][\'options\']:\n          options[\'image\'] = image_from_form\n      else:\n        # Provided image can be standard or custom\n        options[\'image\'] = image_from_form\n\n    # Manage CPU\n    cpu_readonly = False\n    if self._default_config_contains(\'cpu\'):\n      options[\'cpu\'] = form_defaults[\'cpu\'][\'value\']\n      cpu_readonly = form_defaults[\'cpu\'].get(\'readOnly\', False)\n    if (not cpu_readonly and \'cpu\' in formdata and formdata[\'cpu\'][0]):\n      options[\'cpu\'] = formdata[\'cpu\'][0].strip()\n\n    # Manage Memory\n    memory_readonly = False\n    if self._default_config_contains(\'memory\'):\n      options[\'memory\'] = form_defaults[\'memory\'][\'value\']\n      memory_readonly = form_defaults[\'memory\'].get(\'readOnly\', False)\n    if (not memory_readonly and \'memory\' in formdata and formdata[\'memory\'][0]):\n      options[\'memory\'] = formdata[\'memory\'][0].strip()\n\n    # Manage Workspace Volume\n    options[\'workspaceVolume\'] = {}\n    ws_volume = {}\n\n    ws_volume_readonly = False\n    if self._default_config_contains(\'workspaceVolume\'):\n      ws_volume_readonly = \\\n          form_defaults[\'workspaceVolume\'].get(\'readOnly\', False)\n\n      # The Workspace Volume is specified in `config.yaml`\n      default_ws_volume = form_defaults[\'workspaceVolume\'][\'value\']\n\n      # Get and set the default values from the YAML configuration file,\n      # if present and not marked as readonly\n      ws_type_readonly = False\n      if (\'type\' in default_ws_volume and \'value\' in default_ws_volume[\'type\']):\n        ws_volume[\'type\'] = default_ws_volume[\'type\'][\'value\']\n        ws_type_readonly = \\\n            default_ws_volume[\'type\'].get(\'readOnly\', False)\n\n      ws_rok_url_readonly = False\n      if (\'rokURL\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'rokURL\']):\n        ws_volume[\'rokURL\'] = \\\n            default_ws_volume[\'rokURL\'][\'value\']\n        ws_rok_url_readonly = \\\n            default_ws_volume[\'rokURL\'].get(\'readOnly\', False)\n\n      ws_name_readonly = False\n      if (\'name\' in default_ws_volume and \'value\' in default_ws_volume[\'name\']):\n        ws_volume[\'name\'] = default_ws_volume[\'name\'][\'value\']\n        ws_name_readonly = \\\n            default_ws_volume[\'name\'].get(\'readOnly\', False)\n\n      ws_size_readonly = False\n      if (\'size\' in default_ws_volume and \'value\' in default_ws_volume[\'size\']):\n        ws_volume[\'size\'] = \\\n            \'%sGi\' % default_ws_volume[\'size\'][\'value\']\n        ws_size_readonly = \\\n            default_ws_volume[\'size\'].get(\'readOnly\', False)\n\n      ws_mount_path_readonly = False\n      if (\'mountPath\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'mountPath\']):\n        ws_volume[\'mountPath\'] = \\\n            default_ws_volume[\'mountPath\'][\'value\']\n        ws_mount_path_readonly = \\\n            default_ws_volume[\'mountPath\'].get(\'readOnly\', False)\n\n    # Get and set the Workspace Volume values from the form, if present\n    # and not marked as readonly\n    if not ws_volume_readonly:\n      if (not ws_type_readonly and \'ws_type\' in formdata and\n          formdata[\'ws_type\'][0]):\n        ws_volume[\'type\'] = formdata[\'ws_type\'][0].strip()\n\n      if (not ws_rok_url_readonly and \'ws_rok_url\' in formdata and\n          formdata[\'ws_rok_url\'][0]):\n        ws_volume[\'rokURL\'] = \\\n            formdata[\'ws_rok_url\'][0].strip()\n\n      if (not ws_name_readonly and \'ws_name\' in formdata and\n          formdata[\'ws_name\'][0]):\n        ws_volume[\'name\'] = formdata[\'ws_name\'][0].strip()\n\n      if (not ws_size_readonly and \'ws_size\' in formdata and\n          formdata[\'ws_size\'][0]):\n        ws_volume[\'size\'] = \'%sGi\' % formdata[\'ws_size\'][0].strip()\n\n      if (not ws_mount_path_readonly and \'ws_mount_path\' in formdata and\n          formdata[\'ws_mount_path\'][0]):\n        ws_volume[\'mountPath\'] = \\\n            formdata[\'ws_mount_path\'][0].strip()\n\n    options[\'workspaceVolume\'] = ws_volume\n\n    # Manage Data Volumes\n    options[\'dataVolumes\'] = []\n    data_volumes_readonly = False\n    if self._default_config_contains(\'dataVolumes\'):\n      data_volumes_readonly = \\\n          form_defaults[\'dataVolumes\'].get(\'readOnly\', False)\n\n    if data_volumes_readonly:\n      # Set Data Volumes as specified in the Spawner configuration file\n      for volume in form_defaults[\'dataVolumes\'][\'value\']:\n        data_volume = {}\n        for f in [\'type\', \'rokURL\', \'name\', \'size\', \'mountPath\']:\n          data_volume[f] = volume[\'value\'][f][\'value\']\n        data_volume[\'size\'] += \'Gi\'\n        options[\'dataVolumes\'].append(data_volume)\n    else:\n      # Deduce the total number of Data Volumes\n      data_volumes_cnt = 0\n      for k, v in formdata.items():\n        if k.startswith(\'vol_type\'):\n          data_volumes_cnt += 1\n\n      # Set Data Volumes as specified in the Spawner form\n      for i in range(1, data_volumes_cnt + 1):\n        data_volume = {}\n\n        # Get all Data Volume fields from the form\n        id = \'vol_type\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'type\'] = formdata[id][0].strip()\n\n        id = \'vol_name\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'name\'] = formdata[id][0].strip()\n\n        id = \'vol_rok_url\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'rokURL\'] = formdata[id][0].strip()\n\n        id = \'vol_size\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'size\'] = \'%sGi\' % formdata[id][0].strip()\n\n        id = \'vol_mount_path\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'mountPath\'] = formdata[id][0].strip()\n\n        options[\'dataVolumes\'].append(data_volume)\n\n    # Manage Extra Resources\n    extra_resources_readonly = False\n    if self._default_config_contains(\'extraResources\'):\n      options[\'extraResources\'] = (form_defaults[\'extraResources\'][\'value\'])\n      extra_resources_readonly = \\\n          form_defaults[\'extraResources\'].get(\'readOnly\', False)\n    if (not extra_resources_readonly and \'extraResources\' in formdata and\n        formdata[\'extraResources\'][0]):\n      options[\'extraResources\'] = \\\n          formdata[\'extraResources\'][0].strip()\n\n    return options\n\n  @gen.coroutine\n  def _prepare_volumes(self):\n    """"""Create PVC manifests and attach as volumes to the Notebook.""""""\n    # Reset Volumes and VolumeMounts to initial KubeSpawner values\n    self.volumes = list(self.initial_volumes)\n    self.volume_mounts = list(self.initial_volume_mounts)\n\n    # Set PVC labels\n    labels = self._expand_all(self.user_storage_extra_labels)\n    labels = self._build_common_labels(labels)\n    labels.update({\'component\': \'singleuser-storage\'})\n\n    # Attach the existing Rok GW Token to the Notebook as Volume\n    self._attach_rok_token_secret()\n\n    # Set Rok annotations to PVC\n    secret_name = self._expand_user_properties(\n        self.extra_spawner_config[\'rok_secret_name\'])\n\n    rok_annotations = {\'rok/creds-secret-name\': secret_name}\n\n    # Workspace and Data Volumes are managed as PVCs\n    persistent_volumes = [self.workspace_volume] + self.data_volumes\n\n    for (idx, volume) in enumerate(persistent_volumes):\n      annotations = self._build_common_annotations(rok_annotations)\n      if idx == 0:\n        # This is the Workspace Volume\n        # Rok GW needs an annotation to present this resource\n        annotations.update({\'jupyter-workspace\': volume[\'name\']})\n      else:\n        # This is a Dataset Volume\n        # Rok GW needs an annotation to present this resource\n        annotations.update({\'jupyter-dataset\': volume[\'name\']})\n\n      if volume[\'type\'] == \'Existing\':\n        # Rok CSI needs an extra annotation to work with Rok URLs\n        annotations.update({\'rok/origin\': volume[\'rokURL\']})\n\n      yield self._delete_existing_pvc(volume[\'name\'], self.namespace)\n\n      yield self._provision_new_pvc(volume, self.namespace, labels, annotations)\n\n      # Upon success, mount PVC as a volume\n      self.volumes.append({\n          \'name\': \'volume-%d-{username}\' % idx,\n          \'persistentVolumeClaim\': {\n              \'claimName\': volume[\'name\']\n          }\n      })\n\n      self.volume_mounts.append({\n          \'mountPath\': volume[\'mountPath\'],\n          \'name\': \'volume-%d-{username}\' % idx\n      })\n\n  def _get_rok_token(self, name, namespace):\n    """"""Retrieve the token to authenticate with Rok.""""""\n    secret = None\n\n    try:\n      secret = self.api.read_namespaced_secret(name=name, namespace=namespace)\n    except ApiException as e:\n      self.log.warning(\'Could not retrieve Rok Secret: %s\' % e.reason)\n      return \'\'\n\n    token = secret.data.get(\'token\', \'\')\n\n    return base64.b64decode(token).decode(\'utf-8\')\n\n  @gen.coroutine\n  def _attach_rok_token_secret(self):\n    """"""Attach the existing Rok Secret as Notebook Volume.""""""\n    secret_name = self._expand_user_properties(\n        self.extra_spawner_config[\'rok_secret_name\'])\n    secret_volume_name = \'volume-%s\' % secret_name\n\n    secret_volume = {\n        \'name\': secret_volume_name,\n        \'secret\': {\n            \'secretName\': secret_name\n        }\n    }\n    self.volumes.append(secret_volume)\n    self.volume_mounts.append({\n        \'name\': secret_volume_name,\n        \'mountPath\': ROK_SECRET_MOUNT\n    })\n\n    rok_env = {\n        \'ROK_GW_TOKEN\': \'file:%s/token\' % ROK_SECRET_MOUNT,\n        \'ROK_GW_URL\': \'file:%s/url\' % ROK_SECRET_MOUNT,\n        \'ROK_GW_PARAM_REGISTER_JUPYTER_LAB\': self.pod_name\n    }\n    self.environment.update(rok_env)\n\n  @gen.coroutine\n  def _delete_existing_pvc(self, pvc_name, namespace):\n    """"""Issue a K8s API request to delete a namespaced PVC, if exists.""""""\n    delete_options = V1DeleteOptions()\n    del_status = None\n    try:\n      del_status = yield self.asynchronize(\n          self.api.delete_namespaced_persistent_volume_claim,\n          name=pvc_name,\n          namespace=namespace,\n          body=delete_options)\n    except ApiException as e:\n      if e.status == 404:\n        # The PVC does not exist\n        return del_status\n      else:\n        self.log.warning(\'Could not delete PVC %s\' % pvc_name)\n        raise\n\n    while True:\n      try:\n        yield self.asynchronize(\n            self.api.read_namespaced_persistent_volume_claim,\n            name=pvc_name,\n            namespace=namespace)\n      except ApiException as e:\n        if e.status == 404:\n          self.log.info(\'PVC %s was successfully deleted\', pvc_name)\n          break\n\n    return del_status\n\n  @gen.coroutine\n  def _provision_new_pvc(self, volume, namespace, labels, annotations):\n    """"""Issue a K8s API request to create a new, namespaced PVC.""""""\n    # Create a V1PersistentVolumeClaim for the API call\n    pvc_manifest = self._get_pvc_manifest(\n        name=volume[\'name\'],\n        storage_class=self.extra_spawner_config[\'storage_class\'],\n        access_modes=[\'ReadWriteOnce\'],\n        storage=volume[\'size\'],\n        labels=labels,\n        annotations=annotations)\n    pvc = None\n    try:\n      pvc = yield self.asynchronize(\n          self.api.create_namespaced_persistent_volume_claim,\n          namespace=namespace,\n          body=pvc_manifest)\n\n    except ApiException as e:\n      if e.status == 409:\n        self.log.warning(\'PVC %s already exists. New PVC not created.\',\n                         volume[\'name\'])\n      self.log.info(e.reason)\n      raise\n\n    self.log.info(\'PVC %s was successfully created\', volume[\'name\'])\n    return pvc\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/__init__.py,0,b''
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/kubeflow/jupyter/ui/default/spawner.py,0,"b'# -*- coding: utf-8 -*-\nimport json\nimport yaml\nimport string\nimport escapism\nfrom tornado import gen\nfrom traitlets import Dict\nfrom jinja2 import FileSystemLoader, Environment\n\nfrom kubespawner.objects import make_pvc\nfrom kubespawner.spawner import KubeSpawner\nfrom kubernetes.client.rest import ApiException\n\nSERVICE_ACCOUNT_SECRET_MOUNT = \'/var/run/secrets/sa\'\n\n\nclass KubeFormSpawner(KubeSpawner):\n  """"""Implement a custom Spawner to spawn pods in a Kubernetes Cluster.""""""\n\n  def __init__(self, *args, **kwargs):\n    """"""Call init() of parent class and initialize volume lists.""""""\n    super(KubeFormSpawner, self).__init__(*args, **kwargs)\n    self.initial_volumes = list(self.volumes)\n    self.initial_volume_mounts = list(self.volume_mounts)\n\n  @property\n  def spawner_ui_config(self):\n    # Read raw YAML file, format it and parse it as dict\n    if not hasattr(self, ""_spawner_ui_config""):\n      c = None\n      try:\n        with open(\'/etc/config/spawner_ui_config.yaml\', \'r\') as f:\n          c = self._expand_user_properties(f.read())\n      except IOError:\n        self.log.warning(\'Error opening Spawner UI config file\')\n\n      try:\n        if yaml.safe_load(c) is None:\n          # YAML exists but is empty\n          self._spawner_ui_config = {}\n        else:\n          # YAML exists and is not empty\n          self._spawner_ui_config = yaml.safe_load(c)\n      except yaml.YAMLError as e:\n        self.log.warning(\n            \'Spawner UI config file contains\'\n            \'invalid YAML syntax: {}\', e)\n        return None\n\n    return self._spawner_ui_config\n\n  extra_spawner_config = Dict({},\n                              config=True,\n                              help=""""""\n        A dictionary with extra configuration parameters for KubeFormSpawner.\n        """""")\n\n  def options_form(self, form):\n    # Create Jinja environment to dynamically load templates\n    j2_env = Environment(loader=FileSystemLoader(\'/etc/config\'))\n\n    # Get available PVCs in a given namespace\n    # This is a blocking K8s API call\n    existing_pvcs = self._list_pvcs_in_namespace(self.namespace)\n\n    form_defaults = None\n    if self.spawner_ui_config is not None:\n      # YAML exists and was parsed successfully\n      if self.spawner_ui_config[\'spawnerFormDefaults\'] is not None:\n        form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n      else:\n        form_defaults = {}\n\n    # Return the rendered template as a unicode string\n    return j2_env.get_template(\'template.html\').render(\n        form_defaults=form_defaults,\n        existing_pvcs=existing_pvcs,\n        username=self._expand_user_properties(\'{username}\'))\n\n  def options_from_form(self, formdata):\n    options = {}\n    if self.spawner_ui_config is not None:\n      form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n\n    # Manage Image\n    image_readonly = False\n    if self._default_config_contains(\'image\'):\n      options[\'image\'] = form_defaults[\'image\'][\'value\']\n      image_readonly = form_defaults[\'image\'].get(\'readOnly\', False)\n    if (\'image\' in formdata and formdata[\'image\'][0]):\n      image_from_form = formdata[\'image\'][0].strip()\n      if image_readonly:\n        # Provided image must be standard\n        if image_from_form in form_defaults[\'image\'][\'options\']:\n          options[\'image\'] = image_from_form\n      else:\n        # Provided image can be standard or custom\n        options[\'image\'] = image_from_form\n\n    # Manage CPU\n    cpu_readonly = False\n    if self._default_config_contains(\'cpu\'):\n      options[\'cpu\'] = form_defaults[\'cpu\'][\'value\']\n      cpu_readonly = form_defaults[\'cpu\'].get(\'readOnly\', False)\n    if (not cpu_readonly and \'cpu\' in formdata and formdata[\'cpu\'][0]):\n      options[\'cpu\'] = formdata[\'cpu\'][0].strip()\n\n    # Manage Memory\n    memory_readonly = False\n    if self._default_config_contains(\'memory\'):\n      options[\'memory\'] = form_defaults[\'memory\'][\'value\']\n      memory_readonly = form_defaults[\'memory\'].get(\'readOnly\', False)\n    if (not memory_readonly and \'memory\' in formdata and formdata[\'memory\'][0]):\n      options[\'memory\'] = formdata[\'memory\'][0].strip()\n\n    # Manage Workspace Volume\n    options[\'workspaceVolume\'] = {}\n    ws_volume = {}\n\n    ws_volume_readonly = False\n    if self._default_config_contains(\'workspaceVolume\'):\n      ws_volume_readonly = \\\n          form_defaults[\'workspaceVolume\'].get(\'readOnly\', False)\n\n      # The Workspace Volume is specified in `config.yaml`\n      default_ws_volume = form_defaults[\'workspaceVolume\'][\'value\']\n\n      # Get and set the default values from the YAML configuration file,\n      # if present and not marked as readonly\n      ws_type_readonly = False\n      if (\'type\' in default_ws_volume and \'value\' in default_ws_volume[\'type\']):\n        ws_volume[\'type\'] = default_ws_volume[\'type\'][\'value\']\n        ws_type_readonly = \\\n            default_ws_volume[\'type\'].get(\'readOnly\', False)\n\n      ws_name_readonly = False\n      if (\'name\' in default_ws_volume and \'value\' in default_ws_volume[\'name\']):\n        ws_volume[\'name\'] = default_ws_volume[\'name\'][\'value\']\n        ws_name_readonly = \\\n            default_ws_volume[\'name\'].get(\'readOnly\', False)\n\n      ws_size_readonly = False\n      if (\'size\' in default_ws_volume and \'value\' in default_ws_volume[\'size\']):\n        ws_volume[\'size\'] = \\\n            \'%sGi\' % default_ws_volume[\'size\'][\'value\']\n        ws_size_readonly = \\\n            default_ws_volume[\'size\'].get(\'readOnly\', False)\n\n      ws_mount_path_readonly = False\n      if (\'mountPath\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'mountPath\']):\n        ws_volume[\'mountPath\'] = \\\n            default_ws_volume[\'mountPath\'][\'value\']\n        ws_mount_path_readonly = \\\n            default_ws_volume[\'mountPath\'].get(\'readOnly\', False)\n\n      ws_access_modes_readonly = False\n      if (\'accessModes\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'accessModes\']):\n        ws_volume[\'accessModes\'] = \\\n            default_ws_volume[\'accessModes\'][\'value\']\n        ws_access_modes_readonly = \\\n            default_ws_volume[\'accessModes\'].get(\'readOnly\', False)\n\n    # Get and set the Workspace Volume values from the form, if present\n    # and not marked as readonly\n    if not ws_volume_readonly:\n      if (not ws_type_readonly and \'ws_type\' in formdata and\n          formdata[\'ws_type\'][0]):\n        ws_volume[\'type\'] = formdata[\'ws_type\'][0].strip()\n\n      if (not ws_name_readonly and \'ws_name\' in formdata and\n          formdata[\'ws_name\'][0]):\n        ws_volume[\'name\'] = formdata[\'ws_name\'][0].strip()\n\n      if (not ws_size_readonly and \'ws_size\' in formdata and\n          formdata[\'ws_size\'][0]):\n        ws_volume[\'size\'] = \'%sGi\' % formdata[\'ws_size\'][0].strip()\n\n      if (not ws_mount_path_readonly and \'ws_mount_path\' in formdata and\n          formdata[\'ws_mount_path\'][0]):\n        ws_volume[\'mountPath\'] = \\\n            formdata[\'ws_mount_path\'][0].strip()\n\n      if (not ws_access_modes_readonly and \'ws_access_modes\' in formdata and\n          formdata[\'ws_access_modes\'][0]):\n        ws_volume[\'accessModes\'] = \\\n            formdata[\'ws_access_modes\'][0].strip()\n\n    options[\'workspaceVolume\'] = ws_volume\n\n    # Manage Data Volumes\n    options[\'dataVolumes\'] = []\n    data_volumes_readonly = False\n    if self._default_config_contains(\'dataVolumes\'):\n      data_volumes_readonly = \\\n          form_defaults[\'dataVolumes\'].get(\'readOnly\', False)\n\n    if data_volumes_readonly:\n      # Set Data Volumes as specified in the Spawner configuration file\n      for volume in form_defaults[\'dataVolumes\'][\'value\']:\n        data_volume = {}\n        for f in [\'type\', \'name\', \'size\', \'mountPath\', \'accessModes\']:\n          data_volume[f] = volume[\'value\'][f][\'value\']\n        data_volume[\'size\'] += \'Gi\'\n        options[\'dataVolumes\'].append(data_volume)\n    else:\n      # Deduce the total number of Data Volumes\n      data_volumes_cnt = 0\n      for k, v in formdata.items():\n        if k.startswith(\'vol_type\'):\n          data_volumes_cnt += 1\n\n      # Set Data Volumes as specified in the Spawner form\n      for i in range(1, data_volumes_cnt + 1):\n        data_volume = {}\n\n        # Get all Data Volume fields from the form\n        id = \'vol_type\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'type\'] = formdata[id][0].strip()\n\n        id = \'vol_name\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'name\'] = formdata[id][0].strip()\n\n        id = \'vol_size\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'size\'] = \'%sGi\' % formdata[id][0].strip()\n\n        id = \'vol_mount_path\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'mountPath\'] = formdata[id][0].strip()\n\n        id = \'vol_access_modes\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'accessModes\'] = formdata[id][0].strip()\n\n        options[\'dataVolumes\'].append(data_volume)\n\n    # Manage Extra Resources\n    extra_resources_readonly = False\n    if self._default_config_contains(\'extraResources\'):\n      options[\'extraResources\'] = (form_defaults[\'extraResources\'][\'value\'])\n      extra_resources_readonly = \\\n          form_defaults[\'extraResources\'].get(\'readOnly\', False)\n    if (not extra_resources_readonly and \'extraResources\' in formdata and\n        formdata[\'extraResources\'][0]):\n      options[\'extraResources\'] = \\\n          formdata[\'extraResources\'][0].strip()\n\n    return options\n\n  @property\n  def singleuser_image_spec(self):\n    return self.user_options[\'image\']\n\n  image_spec = singleuser_image_spec\n\n  @property\n  def cpu_guarantee(self):\n    return self.user_options[\'cpu\']\n\n  @property\n  def mem_guarantee(self):\n    return self.user_options[\'memory\']\n\n  @property\n  def workspace_volume(self):\n    return self.user_options[""workspaceVolume""]\n\n  @property\n  def data_volumes(self):\n    return self.user_options[""dataVolumes""]\n\n  @property\n  def extra_resource_limits(self):\n    extra = \'\'\n    if self.user_options[\'extraResources\']:\n      extra = json.loads(self.user_options[\'extraResources\'])\n    return extra\n\n  def get_env(self):\n    env = super(KubeFormSpawner, self).get_env()\n    gcp_secret_name = self.extra_spawner_config[\'gcp_secret_name\']\n    if gcp_secret_name:\n      env[\'GOOGLE_APPLICATION_CREDENTIALS\'] = \'{}/{}.json\'.format(\n          SERVICE_ACCOUNT_SECRET_MOUNT, gcp_secret_name)\n    return env\n\n  # TODO(kkasravi): add unit test\n  def _parse_user_name(self, username):\n    safe_chars = set(string.ascii_lowercase + string.digits)\n    name = username.split(\':\')[-1]\n    legacy = \'\'.join([s if s in safe_chars else \'-\' for s in name.lower()])\n    safe = escapism.escape(name, safe=safe_chars, escape_char=\'-\').lower()\n    return legacy, safe, name\n\n  def _expand_user_properties(self, template):\n    # Override KubeSpawner method to remove prefix accounts.google: for iap\n    legacy, safe, name = self._parse_user_name(self.user.name)\n\n    # Set servername based on whether named-server initialised\n    if self.name:\n      servername = \'-{}\'.format(self.name)\n    else:\n      servername = \'\'\n\n    rname = template.format(\n        userid=self.user.id,\n        username=safe,\n        unescaped_username=name,\n        legacy_escape_username=legacy,\n        servername=servername,\n    )\n    return rname\n\n  def _default_config_contains(self, option):\n    """"""Check if config.yaml contains a value for a Spawner option.""""""\n    if self.spawner_ui_config is not None:\n      form_defaults = None\n      if \'spawnerFormDefaults\' in self.spawner_ui_config:\n        form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n\n      if form_defaults is not None and option in form_defaults:\n        if \'value\' in form_defaults[option]:\n          return True\n    return False\n\n  def _get_pvc_manifest(self, name, storage_class, access_modes, storage,\n                        labels, annotations):\n    """"""\n    Return a PVC spec based on the given parameters.\n    This manifest will be used to create PVCs in the K8s cluster.\n    """"""\n    return make_pvc(\n        name=name,\n        storage_class=storage_class,\n        access_modes=access_modes,\n        storage=storage,\n        labels=labels,\n        annotations=annotations)\n\n  def _list_pvcs_in_namespace(self, namespace):\n    """"""\n    Return a list with all non-failed PVCs in a K8s namespace.\n    Each list entry is a dict with `name`, `size` and `access_modes` keys.\n    """"""\n    existing_pvcs = []\n\n    try:\n      resp = self.api.list_namespaced_persistent_volume_claim(\n          namespace=namespace, watch=False)\n\n    except ApiException as e:\n      self.log.warn(\'Could not list PVCs in %s: %s\', namespace, e)\n      raise\n\n    # Iterate over all existing PVCs and return all non-failed ones\n    for pvc in [pvc for pvc in resp.items if pvc.status.phase != \'Failed\']:\n      existing_pvcs.append({\n          ""name"":\n          pvc.metadata.name,\n          ""size"":\n          pvc.spec.resources.requests.get(\'storage\')[:-2],\n          ""access_modes"":\n          pvc.spec.access_modes\n      })\n\n    return existing_pvcs\n\n  @gen.coroutine\n  def _prepare_volumes(self):\n    """"""Create PVC manifests and attach as volumes to the Notebook.""""""\n    # Reset Volumes and VolumeMounts to initial KubeSpawner values\n    self.volumes = list(self.initial_volumes)\n    self.volume_mounts = list(self.initial_volume_mounts)\n\n    # Workspace and Data Volumes are managed as PVCs\n    persistent_volumes = [self.workspace_volume] + self.data_volumes\n\n    for (idx, volume) in enumerate(persistent_volumes):\n      if volume[\'type\'] == \'New\':\n        yield self._provision_new_pvc(volume, self.namespace)\n      elif volume[\'type\'] == \'Existing\':\n        yield self._get_existing_pvc(volume[\'name\'], self.namespace)\n\n      # Upon success, mount PVC as a volume\n      self.volumes.append({\n          \'name\': \'volume-%d-{username}\' % idx,\n          \'persistentVolumeClaim\': {\n              \'claimName\': volume[\'name\']\n          }\n      })\n\n      self.volume_mounts.append({\n          \'mountPath\': volume[\'mountPath\'],\n          \'name\': \'volume-%d-{username}\' % idx\n      })\n\n  @gen.coroutine\n  def _provision_new_pvc(self, volume, namespace):\n    """"""Issue a K8s API request to create a new, namespaced PVC.""""""\n    labels = self._build_common_labels(\n        self._expand_all(self.user_storage_extra_labels))\n    labels.update({\'component\': \'singleuser-storage\'})\n    annotations = self._build_common_annotations({})\n\n    # Create a V1PersistentVolumeClaim for the API call\n    pvc_manifest = self._get_pvc_manifest(\n        name=volume[\'name\'],\n        storage_class=self.extra_spawner_config[\'storage_class\'],\n        access_modes=[volume[\'accessModes\']],\n        storage=volume[\'size\'],\n        labels=labels,\n        annotations=annotations)\n\n    pvc = None\n    try:\n      pvc = yield self.asynchronize(\n          self.api.create_namespaced_persistent_volume_claim,\n          namespace=namespace,\n          body=pvc_manifest)\n\n    except ApiException as e:\n      if e.status == 409:\n        self.log.warning(\'PVC %s already exists. New PVC not created.\',\n                         volume[\'name\'])\n      self.log.info(e.reason)\n      raise\n\n    self.log.info(\'PVC %s was successfully created\', volume[\'name\'])\n    return pvc\n\n  @gen.coroutine\n  def _get_existing_pvc(self, pvc_name, namespace):\n    """"""Issue a K8s API request to retrieve a namespaced PVC.""""""\n    pvc = None\n\n    try:\n      pvc = yield self.asynchronize(\n          self.api.read_namespaced_persistent_volume_claim,\n          name=pvc_name,\n          namespace=namespace)\n\n    except ApiException as e:\n      self.log.warning(\'PVC %s could not be retrieved: %s\', pvc_name, e)\n      raise\n\n    self.log.info(\'PVC %s was successfully retrieved\', pvc_name)\n    return pvc\n\n  @gen.coroutine\n  def start(self):\n    """"""Override KubeSpawner class start method.""""""\n    yield self._prepare_volumes()\n    _start = yield super(KubeFormSpawner, self).start()\n    return _start\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/kubeflow/jupyter/ui/rok/spawner.py,0,"b'# -*- coding: utf-8 -*-\nimport base64\nfrom tornado import gen\nfrom jinja2 import FileSystemLoader, Environment\nfrom kubernetes.client.rest import ApiException\nfrom kubernetes.client.models import V1DeleteOptions\nfrom importlib.util import spec_from_file_location, module_from_spec\n\n# Import the default KubeFormSpawner as a Python module\n# Our custom spawner extends the default one, but shares the same class name\nspec = spec_from_file_location(\'spawner\', \'/etc/config/default_spawner.py\')\nspawner = module_from_spec(spec)\nspec.loader.exec_module(spawner)\n\nROK_SECRET_MOUNT = \'/var/run/secrets/rok\'\n\n\nclass KubeFormSpawner(spawner.KubeFormSpawner):\n  """"""Implement a custom Spawner to spawn pods in a Kubernetes Cluster.""""""\n\n  def options_form(self, form):\n    # Create Jinja environment to dynamically load templates\n    j2_env = Environment(loader=FileSystemLoader(\'/etc/config\'))\n\n    form_defaults = None\n    if self.spawner_ui_config is not None:\n      # YAML exists and was parsed successfully\n      if self.spawner_ui_config[\'spawnerFormDefaults\'] is not None:\n        form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n      else:\n        form_defaults = {}\n\n    secret_name = self._expand_user_properties(\n        self.extra_spawner_config[\'rok_secret_name\'])\n\n    rok_token = self._get_rok_token(name=secret_name, namespace=self.namespace)\n\n    # Return the rendered template as a unicode string\n    return j2_env.get_template(\'template.html\').render(\n        form_defaults=form_defaults,\n        rok_token=rok_token,\n        username=self._expand_user_properties(\'{username}\'),\n        namespace=self.namespace)\n\n  def options_from_form(self, formdata):\n    options = {}\n    if self.spawner_ui_config is not None:\n      form_defaults = self.spawner_ui_config[\'spawnerFormDefaults\']\n\n    # Manage Image\n    image_readonly = False\n    if self._default_config_contains(\'image\'):\n      options[\'image\'] = form_defaults[\'image\'][\'value\']\n      image_readonly = form_defaults[\'image\'].get(\'readOnly\', False)\n    if (\'image\' in formdata and formdata[\'image\'][0]):\n      image_from_form = formdata[\'image\'][0].strip()\n      if image_readonly:\n        # Provided image must be standard\n        if image_from_form in form_defaults[\'image\'][\'options\']:\n          options[\'image\'] = image_from_form\n      else:\n        # Provided image can be standard or custom\n        options[\'image\'] = image_from_form\n\n    # Manage CPU\n    cpu_readonly = False\n    if self._default_config_contains(\'cpu\'):\n      options[\'cpu\'] = form_defaults[\'cpu\'][\'value\']\n      cpu_readonly = form_defaults[\'cpu\'].get(\'readOnly\', False)\n    if (not cpu_readonly and \'cpu\' in formdata and formdata[\'cpu\'][0]):\n      options[\'cpu\'] = formdata[\'cpu\'][0].strip()\n\n    # Manage Memory\n    memory_readonly = False\n    if self._default_config_contains(\'memory\'):\n      options[\'memory\'] = form_defaults[\'memory\'][\'value\']\n      memory_readonly = form_defaults[\'memory\'].get(\'readOnly\', False)\n    if (not memory_readonly and \'memory\' in formdata and formdata[\'memory\'][0]):\n      options[\'memory\'] = formdata[\'memory\'][0].strip()\n\n    # Manage Workspace Volume\n    options[\'workspaceVolume\'] = {}\n    ws_volume = {}\n\n    ws_volume_readonly = False\n    if self._default_config_contains(\'workspaceVolume\'):\n      ws_volume_readonly = \\\n          form_defaults[\'workspaceVolume\'].get(\'readOnly\', False)\n\n      # The Workspace Volume is specified in `config.yaml`\n      default_ws_volume = form_defaults[\'workspaceVolume\'][\'value\']\n\n      # Get and set the default values from the YAML configuration file,\n      # if present and not marked as readonly\n      ws_type_readonly = False\n      if (\'type\' in default_ws_volume and \'value\' in default_ws_volume[\'type\']):\n        ws_volume[\'type\'] = default_ws_volume[\'type\'][\'value\']\n        ws_type_readonly = \\\n            default_ws_volume[\'type\'].get(\'readOnly\', False)\n\n      ws_rok_url_readonly = False\n      if (\'rokURL\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'rokURL\']):\n        ws_volume[\'rokURL\'] = \\\n            default_ws_volume[\'rokURL\'][\'value\']\n        ws_rok_url_readonly = \\\n            default_ws_volume[\'rokURL\'].get(\'readOnly\', False)\n\n      ws_name_readonly = False\n      if (\'name\' in default_ws_volume and \'value\' in default_ws_volume[\'name\']):\n        ws_volume[\'name\'] = default_ws_volume[\'name\'][\'value\']\n        ws_name_readonly = \\\n            default_ws_volume[\'name\'].get(\'readOnly\', False)\n\n      ws_size_readonly = False\n      if (\'size\' in default_ws_volume and \'value\' in default_ws_volume[\'size\']):\n        ws_volume[\'size\'] = \\\n            \'%sGi\' % default_ws_volume[\'size\'][\'value\']\n        ws_size_readonly = \\\n            default_ws_volume[\'size\'].get(\'readOnly\', False)\n\n      ws_mount_path_readonly = False\n      if (\'mountPath\' in default_ws_volume and\n          \'value\' in default_ws_volume[\'mountPath\']):\n        ws_volume[\'mountPath\'] = \\\n            default_ws_volume[\'mountPath\'][\'value\']\n        ws_mount_path_readonly = \\\n            default_ws_volume[\'mountPath\'].get(\'readOnly\', False)\n\n    # Get and set the Workspace Volume values from the form, if present\n    # and not marked as readonly\n    if not ws_volume_readonly:\n      if (not ws_type_readonly and \'ws_type\' in formdata and\n          formdata[\'ws_type\'][0]):\n        ws_volume[\'type\'] = formdata[\'ws_type\'][0].strip()\n\n      if (not ws_rok_url_readonly and \'ws_rok_url\' in formdata and\n          formdata[\'ws_rok_url\'][0]):\n        ws_volume[\'rokURL\'] = \\\n            formdata[\'ws_rok_url\'][0].strip()\n\n      if (not ws_name_readonly and \'ws_name\' in formdata and\n          formdata[\'ws_name\'][0]):\n        ws_volume[\'name\'] = formdata[\'ws_name\'][0].strip()\n\n      if (not ws_size_readonly and \'ws_size\' in formdata and\n          formdata[\'ws_size\'][0]):\n        ws_volume[\'size\'] = \'%sGi\' % formdata[\'ws_size\'][0].strip()\n\n      if (not ws_mount_path_readonly and \'ws_mount_path\' in formdata and\n          formdata[\'ws_mount_path\'][0]):\n        ws_volume[\'mountPath\'] = \\\n            formdata[\'ws_mount_path\'][0].strip()\n\n    options[\'workspaceVolume\'] = ws_volume\n\n    # Manage Data Volumes\n    options[\'dataVolumes\'] = []\n    data_volumes_readonly = False\n    if self._default_config_contains(\'dataVolumes\'):\n      data_volumes_readonly = \\\n          form_defaults[\'dataVolumes\'].get(\'readOnly\', False)\n\n    if data_volumes_readonly:\n      # Set Data Volumes as specified in the Spawner configuration file\n      for volume in form_defaults[\'dataVolumes\'][\'value\']:\n        data_volume = {}\n        for f in [\'type\', \'rokURL\', \'name\', \'size\', \'mountPath\']:\n          data_volume[f] = volume[\'value\'][f][\'value\']\n        data_volume[\'size\'] += \'Gi\'\n        options[\'dataVolumes\'].append(data_volume)\n    else:\n      # Deduce the total number of Data Volumes\n      data_volumes_cnt = 0\n      for k, v in formdata.items():\n        if k.startswith(\'vol_type\'):\n          data_volumes_cnt += 1\n\n      # Set Data Volumes as specified in the Spawner form\n      for i in range(1, data_volumes_cnt + 1):\n        data_volume = {}\n\n        # Get all Data Volume fields from the form\n        id = \'vol_type\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'type\'] = formdata[id][0].strip()\n\n        id = \'vol_name\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'name\'] = formdata[id][0].strip()\n\n        id = \'vol_rok_url\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'rokURL\'] = formdata[id][0].strip()\n\n        id = \'vol_size\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'size\'] = \'%sGi\' % formdata[id][0].strip()\n\n        id = \'vol_mount_path\' + str(i)\n        if id in formdata and formdata[id][0]:\n          data_volume[\'mountPath\'] = formdata[id][0].strip()\n\n        options[\'dataVolumes\'].append(data_volume)\n\n    # Manage Extra Resources\n    extra_resources_readonly = False\n    if self._default_config_contains(\'extraResources\'):\n      options[\'extraResources\'] = (form_defaults[\'extraResources\'][\'value\'])\n      extra_resources_readonly = \\\n          form_defaults[\'extraResources\'].get(\'readOnly\', False)\n    if (not extra_resources_readonly and \'extraResources\' in formdata and\n        formdata[\'extraResources\'][0]):\n      options[\'extraResources\'] = \\\n          formdata[\'extraResources\'][0].strip()\n\n    return options\n\n  @gen.coroutine\n  def _prepare_volumes(self):\n    """"""Create PVC manifests and attach as volumes to the Notebook.""""""\n    # Reset Volumes and VolumeMounts to initial KubeSpawner values\n    self.volumes = list(self.initial_volumes)\n    self.volume_mounts = list(self.initial_volume_mounts)\n\n    # Set PVC labels\n    labels = self._expand_all(self.user_storage_extra_labels)\n    labels = self._build_common_labels(labels)\n    labels.update({\'component\': \'singleuser-storage\'})\n\n    # Attach the existing Rok GW Token to the Notebook as Volume\n    self._attach_rok_token_secret()\n\n    # Set Rok annotations to PVC\n    secret_name = self._expand_user_properties(\n        self.extra_spawner_config[\'rok_secret_name\'])\n\n    rok_annotations = {\'rok/creds-secret-name\': secret_name}\n\n    # Workspace and Data Volumes are managed as PVCs\n    persistent_volumes = [self.workspace_volume] + self.data_volumes\n\n    for (idx, volume) in enumerate(persistent_volumes):\n      annotations = self._build_common_annotations(rok_annotations)\n      if idx == 0:\n        # This is the Workspace Volume\n        # Rok GW needs an annotation to present this resource\n        annotations.update({\'jupyter-workspace\': volume[\'name\']})\n      else:\n        # This is a Dataset Volume\n        # Rok GW needs an annotation to present this resource\n        annotations.update({\'jupyter-dataset\': volume[\'name\']})\n\n      if volume[\'type\'] == \'Existing\':\n        # Rok CSI needs an extra annotation to work with Rok URLs\n        annotations.update({\'rok/origin\': volume[\'rokURL\']})\n\n      yield self._delete_existing_pvc(volume[\'name\'], self.namespace)\n\n      yield self._provision_new_pvc(volume, self.namespace, labels, annotations)\n\n      # Upon success, mount PVC as a volume\n      self.volumes.append({\n          \'name\': \'volume-%d-{username}\' % idx,\n          \'persistentVolumeClaim\': {\n              \'claimName\': volume[\'name\']\n          }\n      })\n\n      self.volume_mounts.append({\n          \'mountPath\': volume[\'mountPath\'],\n          \'name\': \'volume-%d-{username}\' % idx\n      })\n\n  def _get_rok_token(self, name, namespace):\n    """"""Retrieve the token to authenticate with Rok.""""""\n    secret = None\n\n    try:\n      secret = self.api.read_namespaced_secret(name=name, namespace=namespace)\n    except ApiException as e:\n      self.log.warning(\'Could not retrieve Rok Secret: %s\' % e.reason)\n      return \'\'\n\n    token = secret.data.get(\'token\', \'\')\n\n    return base64.b64decode(token).decode(\'utf-8\')\n\n  @gen.coroutine\n  def _attach_rok_token_secret(self):\n    """"""Attach the existing Rok Secret as Notebook Volume.""""""\n    secret_name = self._expand_user_properties(\n        self.extra_spawner_config[\'rok_secret_name\'])\n    secret_volume_name = \'volume-%s\' % secret_name\n\n    secret_volume = {\n        \'name\': secret_volume_name,\n        \'secret\': {\n            \'secretName\': secret_name\n        }\n    }\n    self.volumes.append(secret_volume)\n    self.volume_mounts.append({\n        \'name\': secret_volume_name,\n        \'mountPath\': ROK_SECRET_MOUNT\n    })\n\n    rok_env = {\n        \'ROK_GW_TOKEN\': \'file:%s/token\' % ROK_SECRET_MOUNT,\n        \'ROK_GW_URL\': \'file:%s/url\' % ROK_SECRET_MOUNT,\n        \'ROK_GW_PARAM_REGISTER_JUPYTER_LAB\': self.pod_name\n    }\n    self.environment.update(rok_env)\n\n  @gen.coroutine\n  def _delete_existing_pvc(self, pvc_name, namespace):\n    """"""Issue a K8s API request to delete a namespaced PVC, if exists.""""""\n    delete_options = V1DeleteOptions()\n    del_status = None\n    try:\n      del_status = yield self.asynchronize(\n          self.api.delete_namespaced_persistent_volume_claim,\n          name=pvc_name,\n          namespace=namespace,\n          body=delete_options)\n    except ApiException as e:\n      if e.status == 404:\n        # The PVC does not exist\n        return del_status\n      else:\n        self.log.warning(\'Could not delete PVC %s\' % pvc_name)\n        raise\n\n    while True:\n      try:\n        yield self.asynchronize(\n            self.api.read_namespaced_persistent_volume_claim,\n            name=pvc_name,\n            namespace=namespace)\n      except ApiException as e:\n        if e.status == 404:\n          self.log.info(\'PVC %s was successfully deleted\', pvc_name)\n          break\n\n    return del_status\n\n  @gen.coroutine\n  def _provision_new_pvc(self, volume, namespace, labels, annotations):\n    """"""Issue a K8s API request to create a new, namespaced PVC.""""""\n    # Create a V1PersistentVolumeClaim for the API call\n    pvc_manifest = self._get_pvc_manifest(\n        name=volume[\'name\'],\n        storage_class=self.extra_spawner_config[\'storage_class\'],\n        access_modes=[\'ReadWriteOnce\'],\n        storage=volume[\'size\'],\n        labels=labels,\n        annotations=annotations)\n    pvc = None\n    try:\n      pvc = yield self.asynchronize(\n          self.api.create_namespaced_persistent_volume_claim,\n          namespace=namespace,\n          body=pvc_manifest)\n\n    except ApiException as e:\n      if e.status == 409:\n        self.log.warning(\'PVC %s already exists. New PVC not created.\',\n                         volume[\'name\'])\n      self.log.info(e.reason)\n      raise\n\n    self.log.info(\'PVC %s was successfully created\', volume[\'name\'])\n    return pvc\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/py/kubeflow/kubeflow/ci/__init__.py,0,b''
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/py/kubeflow/kubeflow/ci/application_util.py,0,"b'""""""Utilities for updating various Kubeflow applications.""""""\n\nimport logging\nimport os\nimport pathlib\nimport tempfile\n\nfrom kubeflow.testing import util # pylint: disable=no-name-in-module\n\nimport yaml\n\ndef set_kustomize_image(kustomize_file, image_name, image):\n  """"""Set the image using kustomize.\n\n  Args:\n    kustomize_file: Path to the kustomize file\n    image_name: The name for the image as defined in the images section\n      of the kustomization file\n    image: New image to set\n\n  Returns:\n    True if the image was updated and false other wise\n  """"""\n  kustomize_dir = os.path.dirname(kustomize_file)\n\n  with open(kustomize_file) as hf:\n    config = yaml.load(hf)\n\n  old_image = """"\n  for i in config.get(""images""):\n    if i[""name""] == image_name:\n      old_image = i.get(""newName"", image_name) + "":"" + i.get(""newTag"", """")\n      break\n\n  if old_image == image:\n    logging.info(""Not updating %s; image is already %s"", kustomize_file,\n                     image)\n\n    return False\n\n  util.run([""kustomize"", ""edit"", ""set"", ""image"",\n            ""{0}={1}"".format(image_name, image)],\n           cwd=kustomize_dir)\n\n  return True\n\ndef regenerate_manifest_tests(manifests_dir):\n  """"""Regenerate manifest tests\n\n  Args:\n    manifests_dir: Directory where kubeflow/manifests is\n      checked out\n  """"""\n  # See https://github.com/kubeflow/manifests/issues/317\n  # We can only run make generate under our GOPATH\n  # So first we have to ensure the source code is linked\n  # from our gopath.\n  go_path = os.getenv(""GOPATH"")\n\n  if not go_path:\n    raise ValueError(""GOPATH not set"")\n\n  parent_dir = os.path.join(go_path, ""src"",\n                            ""github.com"", ""kubeflow"")\n\n  if not os.path.exists(parent_dir):\n    logging.info(""Creating directory %s"", parent_dir)\n    os.makedirs(parent_dir)\n  else:\n    logging.info(""Directory %s already exists"", parent_dir)\n\n  target = os.path.join(parent_dir, ""manifests"")\n\n  if os.path.exists(target):\n    logging.info(""%s already exists"", target)\n    p = pathlib.Path(target)\n    if p.resolve() != pathlib.Path(manifests_dir):\n      raise ValueError(""%s exists but doesn\'t point to %s"",\n                       target, manifests_dir)\n  else:\n    logging.info(""Creating symlink %s -> %s"", target, manifests_dir)\n    os.symlink(manifests_dir, target)\n\n  test_dir = os.path.join(target, ""tests"")\n  with tempfile.NamedTemporaryFile(delete=False, mode=""w"") as hf:\n    hf.write(""#!/bin/bash\\n"")\n    hf.write(""set -ex\\n"")\n    hf.write(""cd {0}\\n"".format(test_dir))\n    hf.write(""make generate \\n"")\n    script = hf.name\n\n  # TODO(jlewi): This is a weird hack to run make generate for the tests.\n  # make generate needs to be run from ${GOPATH}/src/kubeflow/manifests.\n  # Simply setting cwd doesn\'t appear to impact the script; probably something\n  # to do with symlinks? So we write a simply script that executes a CD\n  # and then runs make generate.\n  util.run([""bash"", script], cwd=os.path.join(target, ""tests""))\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/py/kubeflow/kubeflow/ci/application_util_test.py,0,"b'import logging\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nfrom kubeflow.kubeflow.ci import application_util\nfrom kubeflow.kubeflow.ci import update_jupyter_web_app\n\nimport yaml\n\n\nclass ApplicationUttilTest(unittest.TestCase):\n  def test_set_image(self):\n    """"""Verify that we can set the image""""""\n    temp_dir = tempfile.mkdtemp()\n    this_dir = os.path.dirname(__file__)\n    test_app = os.path.join(this_dir, ""test_data"", ""test_app"")\n    logging.info(""Copying %s to %s"", test_app, temp_dir)\n    app_dir = os.path.join(temp_dir, ""test_app"")\n    shutil.copytree(test_app, app_dir)\n\n    kustomize_file = os.path.join(app_dir, ""kustomization.yaml"")\n    image_name = update_jupyter_web_app.JUPYTER_WEB_APP_IMAGE_NAME\n    new_image = ""gcr.io/newrepo/newwebapp:1.0""\n    application_util.set_kustomize_image(kustomize_file, image_name, new_image)\n\n    with open(os.path.join(app_dir, ""kustomization.yaml"")) as hf:\n      new_config = yaml.load(hf)\n\n    self.assertEqual(new_config[""images""][0][""newName""],\n                     ""gcr.io/newrepo/newwebapp"")\n    self.assertEqual(new_config[""images""][0][""newTag""], ""1.0"")\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)\n  unittest.main()'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/py/kubeflow/kubeflow/ci/update_jupyter_web_app.py,0,"b'""""""Script to build and update the Jupyter WebApp image.\n\nRequires python3\n\nhub CLI depends on an OAuth token with repo permissions:\nhttps://hub.github.com/hub.1.html\n  * It will look for environment variable GITHUB_TOKEN\n""""""\n\nimport logging\nimport os\nimport tempfile\nimport yaml\n\nimport fire\nimport git\nimport httplib2\n\nfrom kubeflow.kubeflow.ci import application_util\nfrom kubeflow.testing import util # pylint: disable=no-name-in-module\n\nfrom containerregistry.client import docker_creds\nfrom containerregistry.client import docker_name\nfrom containerregistry.client.v2_2 import docker_http\nfrom containerregistry.client.v2_2 import docker_image as v2_2_image\nfrom containerregistry.transport import transport_pool\n\n# The image name as defined in the kustomization file\nJUPYTER_WEB_APP_IMAGE_NAME = ""gcr.io/kubeflow-images-public/jupyter-web-app""\n\nclass WebAppUpdater(object): # pylint: disable=useless-object-inheritance\n  def __init__(self):\n    self._last_commit = None\n    self.manifests_repo_dir = None\n\n  def build_image(self, build_project, registry_project):\n    """"""Build the image.\n\n    Args:\n      build_project: GCP project used to build the image.\n      registry_project: GCP project used to host the image.\n    """"""\n    env = dict()\n    env.update(os.environ)\n    env[""PROJECT""] = build_project\n    env[""REGISTRY_PROJECT""] = registry_project\n    env[""GIT_TAG""] = self._last_commit\n\n    with tempfile.NamedTemporaryFile() as hf:\n      name = hf.name\n    env[""OUTPUT""] = name\n    web_dir = self._component_dir()\n    util.run([""make"", ""build-gcb""], env=env, cwd=web_dir)\n\n    # TODO(jlewi): We want to get the actual image produced by GCB. Right\n    # now this is a bit brittle because we have multiple layers of substitution\n    # e.g. in the Makefile and then the GCB YAML.\n    # It might be better to parse the stdout of make-build-gcb to get the\n    # GCB job name and then fetch the GCB info specifying the images.\n    with open(name) as hf:\n      data = yaml.load(hf)\n\n    return data[""image""]\n\n  @property\n  def last_commit(self):\n    """"""Get the last commit of a change to the source for the jupyter-web-app.""""""\n    if not self._last_commit:\n      # Get the hash of the last commit to modify the source for the Jupyter web\n      # app image\n      self._last_commit = util.run([""git"", ""log"", ""-n"", ""1"",\n                                    ""--pretty=format:\\""%h\\"""",\n                                    ""components/jupyter-web-app""],\n                                   cwd=self._root_dir()).strip(""\\"""")\n\n    return self._last_commit\n\n  def _find_remote_repo(self, repo, remote_url): # pylint: disable=no-self-use\n    """"""Find the remote repo if it has already been added.\n\n    Args:\n      repo: The git python repo object.\n      remote_url: The URL of the remote repo e.g.\n        git@github.com:jlewi/kubeflow.git\n\n    Returns:\n      remote: git-python object representing the remote repo or none if it\n        isn\'t present.\n    """"""\n    for r in repo.remotes:\n      for u in r.urls:\n        if remote_url == u:\n          return r\n\n    return None\n\n  def all(self, build_project, registry_project, remote_fork, # pylint: disable=too-many-statements,too-many-branches\n          kustomize_file, add_github_host=False):\n    """"""Build the latest image and update the prototype.\n\n    Args:\n      build_project: GCP project used to build the image.\n      registry_project: GCP project used to host the image.\n      remote_fork: Url of the remote fork.\n        The remote fork used to create the PR;\n         e.g. git@github.com:jlewi/kubeflow.git. currently only ssh is\n         supported.\n      kustomize_file: Path to the kustomize file\n      add_github_host: If true will add the github ssh host to known ssh hosts.\n    """"""\n    # TODO(jlewi): How can we automatically determine the root of the git\n    # repo containing the kustomize_file?\n    self.manifests_repo_dir = util.run([""git"", ""rev-parse"", ""--show-toplevel""],\n                                       cwd=os.path.dirname(kustomize_file))\n    repo = git.Repo(self.manifests_repo_dir)\n    util.maybe_activate_service_account()\n    last_commit = self.last_commit\n\n    # Ensure github.com is in the known hosts\n    if add_github_host:\n      output = util.run([""ssh-keyscan"", ""github.com""])\n      with open(os.path.join(os.getenv(""HOME""), "".ssh"", ""known_hosts""),\n                mode=\'a\') as hf:\n        hf.write(output)\n\n    if not remote_fork.startswith(""git@github.com""):\n      raise ValueError(""Remote fork currently only supports ssh"")\n\n    remote_repo = self._find_remote_repo(repo, remote_fork)\n\n    if not remote_repo:\n      fork_name = remote_fork.split("":"", 1)[-1].split(""/"", 1)[0]\n      logging.info(""Adding remote %s=%s"", fork_name, remote_fork)\n      remote_repo = repo.create_remote(fork_name, remote_fork)\n\n    logging.info(""Last change to components-jupyter-web-app was %s"", last_commit)\n\n    base = ""gcr.io/{0}/jupyter-web-app"".format(registry_project)\n\n    # Check if there is already an image tagged with this commit.\n    image = base + "":"" + self.last_commit\n    transport = transport_pool.Http(httplib2.Http)\n    src = docker_name.from_string(image)\n    creds = docker_creds.DefaultKeychain.Resolve(src)\n\n    image_exists = False\n    try:\n      with v2_2_image.FromRegistry(src, creds, transport) as src_image:\n        logging.info(""Image %s exists; digest: %s"", image,\n                     src_image.digest())\n        image_exists = True\n    except docker_http.V2DiagnosticException as e:\n      if e.status == 404:\n        logging.info(""%s doesn\'t exist"", image)\n      else:\n        raise\n\n    if not image_exists:\n      logging.info(""Building the image"")\n      image = self.build_image(build_project, registry_project)\n      logging.info(""Created image: %s"", image)\n    else:\n      logging.info(""Image %s already exists"", image)\n\n    # TODO(jlewi): What if the file was already modified so we didn\'t\n    # modify it in this run but we still need to commit it?\n    image_updated = application_util.set_kustomize_image(\n      kustomize_file, JUPYTER_WEB_APP_IMAGE_NAME, image)\n\n    if not image_updated:\n      logging.info(""kustomization not updated so not creating a PR."")\n      return\n\n    application_util.regenerate_manifest_tests(self.manifests_repo_dir)\n\n    branch_name = ""update_jupyter_{0}"".format(last_commit)\n\n    if repo.active_branch.name != branch_name:\n      logging.info(""Creating branch %s"", branch_name)\n\n      branch_names = [b.name for b in repo.branches]\n      if branch_name in branch_names:\n        logging.info(""Branch %s exists"", branch_name)\n        util.run([""git"", ""checkout"", branch_name], cwd=self.manifests_repo_dir)\n      else:\n        util.run([""git"", ""checkout"", ""-b"", branch_name],\n                 cwd=self.manifests_repo_dir)\n\n    if self._check_if_pr_exists(commit=last_commit):\n      # Since a PR already exists updating to the specified commit\n      # don\'t create a new one.\n      # We don\'t want to just push -f because if the PR already exists\n      # git push -f will retrigger the tests.\n      # To force a recreate of the PR someone could close the existing\n      # PR and a new PR will be created on the next cron run.\n      return\n\n    logging.info(""Add file %s to repo"", kustomize_file)\n    repo.index.add([kustomize_file])\n    repo.index.add([os.path.join(self.manifests_repo_dir, ""tests/*"")])\n    repo.index.commit(""Update the jupyter web app image to {0}"".format(image))\n\n    util.run([""git"", ""push"", ""-f"", remote_repo.name,\n              ""{0}:{0}"".format(branch_name)],\n             cwd=self.manifests_repo_dir)\n\n    self.create_pull_request(commit=last_commit)\n\n  def _pr_title(self, commit):\n    pr_title = ""[auto PR] Update the jupyter-web-app image to {0}"".format(\n      commit)\n    return pr_title\n\n  def _check_if_pr_exists(self, commit=None):\n    """"""Check if a PR is already open.\n\n    Returns:\n      exists: True if a PR updating the image to the specified commit already\n       exists and false otherwise.\n    """"""\n    # TODO(jlewi): Modeled on\n    # https://github.com/kubeflow/examples/blob/master/code_search/docker/ks/update_index.sh\n    # TODO(jlewi): We should use the GitHub API and check if there is an\n    # existing open pull request. Or potentially just use the hub CLI.\n\n    if not commit:\n      commit = self.last_commit\n      logging.info(""No commit specified defaulting to %s"", commit)\n\n    pr_title = self._pr_title(commit)\n\n    # See hub conventions:\n    # https://hub.github.com/hub.1.html\n    # The GitHub repository is determined automatically based on the name\n    # of remote repositories\n    output = util.run([""hub"", ""pr"", ""list"", ""--format=%U;%t\\n""],\n                      cwd=self.manifests_repo_dir)\n\n\n    lines = output.splitlines()\n\n    prs = {}\n    for l in lines:\n      n, t = l.split("";"", 1)\n      prs[t] = n\n\n    if pr_title in prs:\n      logging.info(""PR %s already exists to update the Jupyter web app image ""\n                   ""to %s"", prs[pr_title], commit)\n      return True\n\n    return False\n\n  def create_pull_request(self, base=""kubeflow:master"", commit=None):\n    """"""Create a pull request.\n\n    Args:\n      base: The base to use. Defaults to ""kubeflow:master"". This should be\n        in the form <GitHub OWNER>:<branch>\n    """"""\n    pr_title = self._pr_title(commit)\n\n    with tempfile.NamedTemporaryFile(delete=False, mode=""w"") as hf:\n      hf.write(pr_title)\n      hf.write(""\\n"")\n      hf.write(""\\n"")\n      hf.write(\n        ""Image built from commit https://github.com/kubeflow/kubeflow/""\n        ""commit/{0}"".format(self._last_commit))\n      message_file = hf.name\n\n    # TODO(jlewi): -f creates the pull requests even if there are local changes\n    # this was useful during development but we may want to drop it.\n    util.run([""hub"", ""pull-request"", ""-f"", ""--base="" + base, ""-F"",\n              message_file],\n              cwd=self.manifests_repo_dir)\n\n  def _root_dir(self):\n    this_dir = os.path.dirname(__file__)\n    return os.path.abspath(os.path.join(this_dir, "".."", "".."", "".."", ""..""))\n\n  def _component_dir(self):\n    return os.path.join(self._root_dir(), ""components"", ""jupyter-web-app"")\n\nif __name__ == \'__main__\':\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  fire.Fire(WebAppUpdater)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/common/__init__.py,0,b''
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/common/api.py,0,"b'import json\nimport os\nimport requests\nfrom kubernetes import client, config\nfrom kubernetes.config import ConfigException\nfrom kubernetes.client.rest import ApiException\nfrom . import utils\n\nlogger = utils.create_logger(__name__)\n\nKFAM = os.getenv(""KFAM"", ""profiles-kfam.kubeflow.svc.cluster.local:8081"")\n\n\ntry:\n    # Load configuration inside the Pod\n    config.load_incluster_config()\nexcept ConfigException:\n    # Load configuration for testing\n    config.load_kube_config()\n\n# Create the Apis\nv1_core = client.CoreV1Api()\ncustom_api = client.CustomObjectsApi()\nstorage_api = client.StorageV1Api()\n\n\ndef parse_error(e):\n    try:\n        err = json.loads(e.body)[""message""]\n    except (json.JSONDecodeError, KeyError, AttributeError):\n        err = str(e)\n\n    return err\n\n\ndef is_authorized(user, namespace):\n    \'\'\'\n    Queries KFAM for whether the provided user has access\n    to the specific namespace\n    \'\'\'\n    if user is None:\n        # In case a user is not present, preserve the behavior from 0.5\n        # Pass the authorization check and make the calls with the webapp\'s SA\n        return True\n\n    try:\n        resp = requests.get(""http://{}/kfam/v1/bindings?namespace={}"".format(\n            KFAM, namespace)\n        )\n    except Exception as e:\n        logger.warning(""Error talking to KFAM: {}"".format(parse_error(e)))\n        return False\n\n    if resp.status_code == 200:\n        # Iterate through the namespace\'s bindings and check for the user\n        for binding in resp.json().get(""bindings"", []):\n            if binding[""user""][""name""] == user:\n                return True\n\n        return False\n    else:\n        logger.warning(""{}: Error talking to KFAM!"".format(resp.status_code))\n        return False\n\n\n# Wrapper Functions for error handling\ndef wrap_resp(rsrc, fn, *args, **kwargs):\n    \'\'\'\n    rsrc: Name of the resource, used as the dict key\n    fn: function to get the resource\n    \'\'\'\n    data = {\n        ""success"": True,\n        ""log"": """"\n    }\n\n    try:\n        data[rsrc] = fn(*args, **kwargs)\n    except ApiException as e:\n        data[rsrc] = {}\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n    except Exception as e:\n        data[rsrc] = {}\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n\n    return data\n\n\ndef wrap(fn, *args, **kwargs):\n    \'\'\'\n    fn: function to get the resource\n    \'\'\'\n    data = {\n        ""success"": True,\n        ""log"": """"\n    }\n\n    try:\n        fn(*args, **kwargs)\n    except ApiException as e:\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n    except Exception as e:\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n\n    return data\n\n\n# API Functions\n# GETers\ndef get_pvcs(ns, user=None):\n    if not is_authorized(user, ns):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, ns\n            )\n        }\n\n    return wrap_resp(\n        ""pvcs"",\n        v1_core.list_namespaced_persistent_volume_claim, namespace=ns\n    )\n\n\ndef get_pods(ns, user=None):\n    if not is_authorized(user, ns):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, ns\n            )\n        }\n\n    return wrap_resp(\n        ""pods"",\n        v1_core.list_namespaced_pod, namespace=ns\n    )\n\n\ndef get_notebooks(ns, user=None):\n    if not is_authorized(user, ns):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, ns\n            )\n        }\n\n    return wrap_resp(\n        ""notebooks"",\n        custom_api.list_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1alpha1"",\n        ns,\n        ""notebooks""\n    )\n\n\ndef get_poddefaults(ns, user=None):\n    if not is_authorized(user, ns):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, ns\n            )\n        }\n\n    return wrap_resp(\n        ""poddefaults"",\n        custom_api.list_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1alpha1"",\n        ns,\n        ""poddefaults""\n    )\n\n\ndef get_namespaces(user=None):\n    return wrap_resp(\n        ""namespaces"",\n        v1_core.list_namespace\n    )\n\n\ndef get_storageclasses(user=None):\n    return wrap_resp(\n        ""storageclasses"",\n        storage_api.list_storage_class\n    )\n\n\ndef get_secret(ns, nm, user=None):\n    if not is_authorized(user, ns):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, ns\n            )\n        }\n\n    return wrap_resp(\n        ""secret"",\n        v1_core.read_namespaced_secret, nm, ns\n    )\n\n\n# POSTers\ndef post_notebook(notebook, user=None):\n    if not is_authorized(user, notebook[""metadata""][""namespace""]):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, notebook[""metadata""][""namespace""]\n            )\n        }\n\n    return wrap(\n        custom_api.create_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1alpha1"",\n        notebook[""metadata""][""namespace""],\n        ""notebooks"",\n        notebook\n    )\n\n\ndef post_pvc(pvc, user=None):\n    if not is_authorized(user, pvc.metadata.namespace):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, pvc.metadata.namespace\n            )\n        }\n\n    return wrap_resp(\n        ""pvc"",\n        v1_core.create_namespaced_persistent_volume_claim,\n        pvc.metadata.namespace, pvc\n    )\n\n\n# DELETEers\ndef delete_notebook(namespace, notebook_name, user=None):\n    if not is_authorized(user, namespace):\n        return {\n            ""success"": False,\n            ""log"": ""User \'{}\' is not authorized for namespace \'{}\'"".format(\n                user, namespace\n            )\n        }\n\n    return wrap(\n        custom_api.delete_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1alpha1"",\n        namespace,\n        ""notebooks"",\n        notebook_name,\n        client.V1DeleteOptions(propagation_policy=""Foreground"")\n    )\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/common/base_app.py,0,"b'from flask import jsonify, request, Blueprint\nfrom kubernetes import client\nfrom . import api\nfrom . import utils\n\n# The BaseApp is a Blueprint that other UIs will use\napp = Blueprint(""base_app"", __name__)\nlogger = utils.create_logger(__name__)\n\n\n# Helper function for getting the prefix of the webapp\ndef prefix():\n    if request.headers.get(""x-forwarded-prefix""):\n        return request.headers.get(""x-forwarded-prefix"")\n    else:\n        return """"\n\n\n# REST Routes\n@app.route(""/api/namespaces/<namespace>/notebooks"")\ndef get_notebooks(namespace):\n    user = utils.get_username_from_request()\n    data = api.get_notebooks(namespace, user)\n\n    if not data[""success""]:\n        return jsonify(data)\n\n    data[""notebooks""] = [utils.process_resource(nb)\n                         for nb in data[""notebooks""][""items""]]\n    return jsonify(data)\n\n\n@app.route(""/api/namespaces/<namespace>/poddefaults"")\ndef get_poddefaults(namespace):\n    user = utils.get_username_from_request()\n    data = api.get_poddefaults(namespace, user)\n\n    if not data[""success""]:\n        return jsonify(data)\n\n    # Return a list of (label, desc) with the pod defaults\n    pdefaults = []\n    for pd in data[""poddefaults""][""items""]:\n        label = list(pd[""spec""][""selector""][""matchLabels""].keys())[0]\n        if ""desc"" in pd[""spec""]:\n            desc = pd[""spec""][""desc""]\n        else:\n            desc = pd[""metadata""][""name""]\n\n        pdefaults.append({""label"": label, ""desc"": desc})\n\n    logger.info(""Found poddefaults: {}"".format(pdefaults))\n    data[""poddefaults""] = pdefaults\n    return jsonify(data)\n\n\n@app.route(""/api/namespaces/<namespace>/pvcs"")\ndef get_pvcs(namespace):\n    user = utils.get_username_from_request()\n    data = api.get_pvcs(namespace, user)\n    if not data[""success""]:\n        return jsonify(data)\n\n    data[""pvcs""] = [utils.process_pvc(pvc) for pvc in data[""pvcs""].items]\n\n    return jsonify(data)\n\n\n@app.route(""/api/namespaces"")\ndef get_namespaces():\n    user = utils.get_username_from_request()\n    data = api.get_namespaces(user)\n\n    # Result must be jsonify-able\n    if data[""success""]:\n        nmsps = data[""namespaces""]\n        data[""namespaces""] = [ns.metadata.name for ns in nmsps.items]\n\n    return jsonify(data)\n\n\n@app.route(""/api/storageclasses"")\ndef get_storageclasses():\n    user = utils.get_username_from_request()\n    data = api.get_storageclasses(user)\n\n    # Result must be jsonify-able\n    if data[""success""]:\n        scs = data[""storageclasses""]\n        data[""storageclasses""] = [sc.metadata.name for sc in scs.items]\n\n    return jsonify(data)\n\n\n@app.route(""/api/storageclasses/default"")\ndef get_default_storageclass():\n    user = utils.get_username_from_request()\n    data = api.get_storageclasses(user)\n    if not data[""success""]:\n        return jsonify({\n            ""success"": False,\n            ""log"": data[""log""]\n        })\n\n    strg_classes = data[""storageclasses""].items\n    for strgclss in strg_classes:\n        annotations = strgclss.metadata.annotations\n        if annotations is None:\n            continue\n\n        # List of possible annotations\n        keys = [\n            ""storageclass.kubernetes.io/is-default-class"",\n            ""storageclass.beta.kubernetes.io/is-default-class""  # GKE\n        ]\n\n        for key in keys:\n            is_default = annotations.get(key, ""false"")\n            \n            if is_default == ""true"":\n                return jsonify({\n                    ""success"": True,\n                    ""defaultStorageClass"": strgclss.metadata.name\n                })\n\n    # No StorageClass is default\n    return jsonify({\n        ""success"": True,\n        ""defaultStorageClass"": """"\n    })\n\n\n@app.route(""/api/config"")\ndef get_config():\n    data = {""success"": True}\n\n    data[""config""] = utils.spawner_ui_config()\n    return jsonify(data)\n\n\n# POSTers\n@app.route(""/api/namespaces/<namespace>/pvcs"", methods=[""POST""])\ndef post_pvc(namespace):\n    user = utils.get_username_from_request()\n    body = request.get_json()\n\n    pvc = client.V1PersistentVolumeClaim(\n        metadata=client.V1ObjectMeta(\n            name=body[""name""],\n            namespace=namespace\n        ),\n        spec=client.V1PersistentVolumeClaimSpec(\n            access_modes=[body[""mode""]],\n            resources=client.V1ResourceRequirements(\n                requests={\n                    ""storage"": body[""size""] + ""Gi""\n                }\n            )\n        )\n    )\n\n    return jsonify(api.post_pvc(pvc, user))\n\n\n# DELETErs\n@app.route(""/api/namespaces/<namespace>/notebooks/<notebook>"",\n           methods=[""DELETE""])\ndef delete_notebook(namespace, notebook):\n    user = utils.get_username_from_request()\n    return jsonify(api.delete_notebook(namespace, notebook, user))\n\n\nif __name__ == ""__main__"":\n    app.run(host=""0.0.0.0"")\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/common/utils.py,0,"b'import logging\nimport os\nimport sys\nimport yaml\nimport json\nfrom flask import request\nfrom kubernetes import client\nimport datetime as dt\nfrom . import api\n\n# The backend will send the first config it will successfully load\nCONFIGS = [\n    ""/etc/config/spawner_ui_config.yaml"",\n    ""./kubeflow_jupyter/common/yaml/spawner_ui_config.yaml""\n]\n\n# The values of the headers to look for the User info\nUSER_HEADER = os.getenv(""USERID_HEADER"", ""X-Goog-Authenticated-User-Email"")\nUSER_PREFIX = os.getenv(""USERID_PREFIX"", ""accounts.google.com:"")\n\n\n# Logging\ndef create_logger(name):\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(logging.Formatter(\n        ""%(asctime)s | %(name)s | %(levelname)s | %(message)s""))\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    logger.addHandler(handler)\n    return logger\n\n\nlogger = create_logger(__name__)\n\n\n# Utils\ndef get_username_from_request():\n    if USER_HEADER not in request.headers:\n        logger.warning(""User header not present!"")\n        username = None\n    else:\n        user = request.headers[USER_HEADER]\n        username = user.replace(USER_PREFIX, """")\n        logger.info(""User: \'{}\' | Headers: \'{}\' \'{}\'"".format(\n            username, USER_HEADER, USER_PREFIX\n        ))\n\n    return username\n\n\ndef load_param_yaml(f, **kwargs):\n    c = None\n    try:\n        with open(f, ""r"") as f:\n            c = f.read().format(**kwargs)\n    except IOError:\n        logger.info(""Error opening: {}"".format(f))\n        return None\n\n    try:\n        if yaml.safe_load(c) is None:\n            # YAML exists but is empty\n            return {}\n        else:\n            # YAML exists and is not empty\n            return yaml.safe_load(c)\n    except yaml.YAMLError as e:\n        logger.warning(""Couldn\'t load yaml: {}"".format(e))\n        return None\n\n\ndef spawner_ui_config():\n    for config in CONFIGS:\n        c = None\n        try:\n            with open(config, ""r"") as f:\n                c = f.read()\n        except IOError:\n            logger.warning(""Config file \'{}\' is not found"".format(\n                config\n            ))\n            continue\n\n        try:\n            if yaml.safe_load(c) is None:\n                # YAML exists but is empty\n                return {}\n            else:\n                # YAML exists and is not empty\n                logger.info(""Sending config file \'{}\'"".format(config))\n                return yaml.safe_load(c)[""spawnerFormDefaults""]\n        except yaml.YAMLError:\n            logger.error(""Notebook config is not a valid yaml"")\n            return {}\n        except AttributeError as e:\n            logger.error(""Can\'t load the config at {}: {}"".format(\n                config, str(e)\n            ))\n\n    logger.warning(""Couldn\'t load any config"")\n    return {}\n\n\ndef get_uptime(then):\n    now = dt.datetime.now()\n    then = dt.datetime.strptime(then, ""%Y-%m-%dT%H:%M:%SZ"")\n    diff = now - then.replace(tzinfo=None)\n\n    days = diff.days\n    hours = int(diff.seconds / 3600)\n    mins = int((diff.seconds % 3600) / 60)\n\n    age = """"\n    if days > 0:\n        if days == 1:\n            age = str(days) + "" day""\n        else:\n            age = str(days) + "" days""\n    else:\n        if hours > 0:\n            if hours == 1:\n                age = str(hours) + "" hour""\n            else:\n                age = str(hours) + "" hours""\n        else:\n            if mins == 0:\n                return ""just now""\n            if mins == 1:\n                age = str(mins) + "" min""\n            else:\n                age = str(mins) + "" mins""\n\n    return age + "" ago""\n\n\ndef handle_storage_class(vol):\n    # handle the StorageClass\n    if ""class"" not in vol:\n        return None\n    if vol[""class""] == ""{empty}"":\n        return """"\n    if vol[""class""] == ""{none}"":\n        return None\n    else:\n        return vol[""class""]\n\n\n# Volume handling functions\ndef volume_from_config(config_vol, notebook):\n    \'\'\'\n    Create a Volume Dict from the config.yaml. This dict has the same fields as\n    a Volume returned from the frontend\n    \'\'\'\n    vol_name = config_vol[""name""][""value""].replace(\n        ""{notebook-name}"",\n        notebook[""name""]\n    )\n    vol_class = handle_storage_class(config_vol[""class""][""value""])\n\n    return {\n        ""name"": vol_name,\n        ""type"": config_vol[""type""][""value""],\n        ""size"": config_vol[""size""][""value""],\n        ""mode"": config_vol[""accessModes""][""value""],\n        ""path"": config_vol[""mountPath""][""value""],\n        ""class"": vol_class,\n        ""extraFields"": config_vol.get(""extra"", {}),\n    }\n\n\ndef pvc_from_dict(vol, namespace):\n    if vol is None:\n        return None\n\n    return client.V1PersistentVolumeClaim(\n        metadata=client.V1ObjectMeta(\n            name=vol[""name""],\n            namespace=namespace,\n        ),\n        spec=client.V1PersistentVolumeClaimSpec(\n            access_modes=[vol[""mode""]],\n            storage_class_name=handle_storage_class(vol),\n            resources=client.V1ResourceRequirements(\n                requests={\n                    ""storage"": vol[""size""]\n                }\n            )\n        )\n    )\n\n\ndef get_workspace_vol(body, defaults):\n    \'\'\'\n    Checks the config and the form values and returns a Volume Dict for the\n    workspace. If the workspace is readOnly, then the value from the config\n    will be used instead. The Volume Dict has the same format as the Volume\n    interface of the frontend.\n    \'\'\'\n    default_ws = volume_from_config(defaults[""workspaceVolume""][""value""], body)\n    form_ws = body.get(""workspace"", None)\n\n    if defaults[""workspaceVolume""].get(""readOnly"", False):\n        ws = default_ws\n        logger.info(""Using the default Workspace Volume: {}"".format(ws))\n    elif form_ws is not None:\n        ws = form_ws\n        logger.info(""Using form\'s Workspace Volume: {}"".format(ws))\n    else:\n        ws = default_ws\n        logger.info(""Using the default Workspace Volume: {}"".format(ws))\n\n    return ws\n\n\ndef get_data_vols(body, defaults):\n    \'\'\'\n    Checks the config and the form values and returns a list of Volume\n    Dictionaries for the Notebook\'s Data Volumes. If the Data Volumes are\n    readOnly, then the value from the config will be used instead. The Volume\n    Dict has the same format as the Volume interface of the frontend.\n    \'\'\'\n    default_vols = [volume_from_config(vol[""value""], body)\n                    for vol in defaults[""dataVolumes""][""value""]]\n    form_vols = body.get(""datavols"", [])\n\n    if defaults[""dataVolumes""].get(""readOnly"", False):\n        vols = default_vols\n        logger.info(""Using the default Data Volumes: {}"".format(vols))\n    elif ""datavols"" in body:\n        vols = form_vols\n        logger.info(""Using the form\'s Data Volumes: {}"".format(vols))\n    else:\n        vols = default_vols\n        logger.info(""Using the default Data Volumes: {}"".format(vols))\n\n    return vols\n\n\n# Functions for transforming the data from k8s api\ndef process_pvc(rsrc):\n    # VAR: change this function according to the main resource\n    res = {\n        ""name"": rsrc.metadata.name,\n        ""namespace"": rsrc.metadata.namespace,\n        ""size"": rsrc.spec.resources.requests[""storage""],\n        ""mode"": rsrc.spec.access_modes[0],\n        ""class"": rsrc.spec.storage_class_name,\n    }\n    return res\n\n\ndef process_resource(rsrc):\n    # VAR: change this function according to the main resource\n    cntr = rsrc[""spec""][""template""][""spec""][""containers""][0]\n    status, reason = process_status(rsrc)\n\n    res = {\n        ""name"": rsrc[""metadata""][""name""],\n        ""namespace"": rsrc[""metadata""][""namespace""],\n        ""age"": get_uptime(rsrc[""metadata""][""creationTimestamp""]),\n        ""image"": cntr[""image""],\n        ""shortImage"": cntr[""image""].split(""/"")[-1],\n        ""cpu"": cntr[""resources""][""requests""][""cpu""],\n        ""memory"": cntr[""resources""][""requests""][""memory""],\n        ""volumes"": [v[""name""] for v in cntr[""volumeMounts""]],\n        ""status"": status,\n        ""reason"": reason,\n    }\n    return res\n\n\ndef process_status(rsrc):\n    \'\'\'\n    Return status and reason. Status may be [running|waiting|warning|error]\n    \'\'\'\n    # If the Notebook is being deleted, the status will be waiting\n    if ""deletionTimestamp"" in rsrc[""metadata""]:\n        return ""waiting"", ""Deleting Notebook Server""\n\n    # Check the status\n    try:\n        s = rsrc[""status""][""containerState""]\n    except KeyError:\n        return ""waiting"", ""No Status available""\n\n    if ""running"" in s:\n        return ""running"", ""Running""\n    elif ""waiting"" in s:\n        reason = s[""waiting""][""reason""]\n\n        if reason == ""ImagePullBackOff"":\n            return ""error"", reason\n        elif reason == ""ContainerCreating"":\n            return ""waiting"", reason\n        elif reason == ""PodInitializing"":\n            return ""waiting"", reason\n        else:\n            return ""warning"", reason\n    elif ""terminated"" in s:\n        return ""error"", ""The Pod has Terminated""\n    else:\n        return ""waiting"", ""Scheduling the Pod""\n\n\n# Notebook YAML processing\ndef set_notebook_image(notebook, body, defaults):\n    \'\'\'\n    If the image is set to readOnly, use only the value from the config\n    \'\'\'\n    if defaults[""image""].get(""readOnly"", False):\n        image = defaults[""image""][""value""]\n        logger.info(""Using default Image: "" + image)\n    elif body.get(""customImageCheck"", False):\n        image = body[""customImage""]\n        logger.info(""Using form\'s custom Image: "" + image)\n    elif ""image"" in body:\n        image = body[""image""]\n        logger.info(""Using form\'s Image: "" + image)\n    else:\n        image = defaults[""image""][""value""]\n        logger.info(""Using default Image: "" + image)\n\n    notebook[""spec""][""template""][""spec""][""containers""][0][""image""] = image\n\n\ndef set_notebook_cpu(notebook, body, defaults):\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    if defaults[""cpu""].get(""readOnly"", False):\n        cpu = defaults[""cpu""][""value""]\n        logger.info(""Using default CPU: "" + cpu)\n    elif body.get(""cpu"", """"):\n        cpu = body[""cpu""]\n        logger.info(""Using form\'s CPU: "" + cpu)\n    else:\n        cpu = defaults[""cpu""][""value""]\n        logger.info(""Using default CPU: "" + cpu)\n\n    container[""resources""][""requests""][""cpu""] = cpu\n\n\ndef set_notebook_memory(notebook, body, defaults):\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    if defaults[""memory""].get(""readOnly"", False):\n        memory = defaults[""memory""][""value""]\n        logger.info(""Using default Memory: "" + memory)\n    elif body.get(""memory"", """"):\n        memory = body[""memory""]\n        logger.info(""Using form\'s Memory: "" + memory)\n    else:\n        memory = defaults[""memory""][""value""]\n        logger.info(""Using default Memory: "" + memory)\n\n    container[""resources""][""requests""][""memory""] = memory\n\n\ndef set_notebook_configurations(notebook, body, defaults):\n    notebook_labels = notebook[""metadata""][""labels""]\n\n    if defaults[""configurations""].get(""readOnly"", False):\n        labels = defaults[""configurations""][""value""]\n        logger.info(""Using default Configurations: {}"".format(labels))\n    elif body.get(""configurations"", None) is not None:\n        labels = body[""configurations""]\n        logger.info(""Using form\'s Configurations: {}"".format(labels))\n    else:\n        labels = defaults[""configurations""][""value""]\n        logger.info(""Using default Configurations: {}"".format(labels))\n\n    if not isinstance(labels, list):\n        logger.warning(""Labels for PodDefaults are not list: {}"".format(\n            labels)\n        )\n        return\n\n    for l in labels:\n        notebook_labels[l] = ""true""\n\n\ndef set_notebook_extra_resources(notebook, body, defaults):\n    r = {""success"": True, ""log"": """"}\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    if defaults[""extraResources""].get(""readOnly"", False):\n        resources_str = defaults[""extraResources""][""value""]\n        logger.info(""Using the default Extra Resources: "" + resources_str)\n    elif body.get(""extra"", """"):\n        resources_str = body[""extra""]\n        logger.info(""Using the form\'s Extra Resources: "" + resources_str)\n    else:\n        resources_str = defaults[""extraResources""][""value""]\n        logger.info(""Using the default Extra Resources: "" + resources_str)\n\n    try:\n        extra = json.loads(resources_str)\n    except Exception as e:\n        r[""success""] = False\n        r[""log""] = api.parse_error(e)\n        return r\n\n    container[""resources""][""limits""] = extra\n    return r\n\n\ndef set_notebook_shm(notebook, body, defaults):\n    if defaults[""shm""].get(""readOnly"", False):\n        if not defaults[""shm""][""value""]:\n            return\n    elif ""shm"" in body:\n        if not body[""shm""]:\n            return\n    else:\n        if not defaults[""shm""][""value""]:\n            return\n\n    notebook_spec = notebook[""spec""][""template""][""spec""]\n    notebook_cont = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    shm_volume = {\n        ""name"": ""dshm"",\n        ""emptyDir"": {\n            ""medium"": ""Memory""\n        }\n    }\n    notebook_spec[""volumes""].append(shm_volume)\n    shm_mnt = {\n        ""mountPath"": ""/dev/shm"",\n        ""name"": ""dshm""\n    }\n    notebook_cont[""volumeMounts""].append(shm_mnt)\n\n\ndef add_notebook_volume(notebook, vol_name, claim, mnt_path):\n    spec = notebook[""spec""][""template""][""spec""]\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    volume = {\n        ""name"": vol_name,\n        ""persistentVolumeClaim"": {\n            ""claimName"": claim\n        }\n    }\n    spec[""volumes""].append(volume)\n\n    # Container Mounts\n    mnt = {\n        ""mountPath"": mnt_path,\n        ""name"": vol_name\n    }\n    container[""volumeMounts""].append(mnt)\n\n\ndef add_notebook_volume_secret(nb, secret, secret_name, mnt_path, mode):\n    # Create the volume in the Pod\n    spec = nb[""spec""][""template""][""spec""]\n    container = nb[""spec""][""template""][""spec""][""containers""][0]\n\n    volume = {\n        ""name"": secret,\n        ""secret"": {\n            ""defaultMode"": mode,\n            ""secretName"": secret_name,\n        }\n    }\n    spec[""volumes""].append(volume)\n\n    # Container volumeMounts\n    mnt = {\n        ""mountPath"": mnt_path,\n        ""name"": secret,\n    }\n    container[""volumeMounts""].append(mnt)\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/default/app.py,0,"b'from flask import Flask, request, jsonify, send_from_directory\nfrom ..common.base_app import app as base\nfrom ..common import utils, api\n\napp = Flask(__name__)\napp.register_blueprint(base)\nlogger = utils.create_logger(__name__)\n\nNOTEBOOK = ""./kubeflow_jupyter/common/yaml/notebook.yaml""\n\n\n# POSTers\n@app.route(""/api/namespaces/<namespace>/notebooks"", methods=[""POST""])\ndef post_notebook(namespace):\n    user = utils.get_username_from_request()\n    body = request.get_json()\n    defaults = utils.spawner_ui_config()\n    logger.info(""Got Notebook: {}"".format(body))\n\n    notebook = utils.load_param_yaml(NOTEBOOK,\n                                     name=body[""name""],\n                                     namespace=namespace,\n                                     serviceAccount=""default-editor"")\n\n    utils.set_notebook_image(notebook, body, defaults)\n    utils.set_notebook_cpu(notebook, body, defaults)\n    utils.set_notebook_memory(notebook, body, defaults)\n    utils.set_notebook_configurations(notebook, body, defaults)\n\n    # Workspace Volume\n    workspace_vol = utils.get_workspace_vol(body, defaults)\n    if not body.get(""noWorkspace"", False) and workspace_vol[""type""] == ""New"":\n        # Create the PVC\n        ws_pvc = utils.pvc_from_dict(workspace_vol, namespace)\n\n        logger.info(""Creating Workspace Volume: {}"".format(ws_pvc.to_dict()))\n        r = api.post_pvc(ws_pvc, user)\n        if not r[""success""]:\n            return jsonify(r)\n\n    if not body.get(""noWorkspace"", False) and workspace_vol[""type""] != ""None"":\n        utils.add_notebook_volume(\n            notebook,\n            workspace_vol[""name""],\n            workspace_vol[""name""],\n            ""/home/jovyan"",\n        )\n\n    # Add the Data Volumes\n    for vol in utils.get_data_vols(body, defaults):\n        if vol[""type""] == ""New"":\n            # Create the PVC\n            dtvol_pvc = utils.pvc_from_dict(vol, namespace)\n\n            logger.info(""Creating Data Volume {}:"".format(dtvol_pvc))\n            r = api.post_pvc(dtvol_pvc, user)\n            if not r[""success""]:\n                return jsonify(r)\n\n        utils.add_notebook_volume(\n            notebook,\n            vol[""name""],\n            vol[""name""],\n            vol[""path""]\n        )\n\n    # Extra Resources\n    r = utils.set_notebook_extra_resources(notebook, body, defaults)\n    if not r[""success""]:\n        return jsonify(r)\n\n    # shm\n    utils.set_notebook_shm(notebook, body, defaults)\n\n    logger.info(""Creating Notebook: {}"".format(notebook))\n    return jsonify(api.post_notebook(notebook, user))\n\n\n# Since Angular is a SPA, we serve index.html every time\n@app.route(""/"")\ndef serve_root():\n    return send_from_directory(""./static/"", ""index.html"")\n\n\n@app.route(""/<path:path>"", methods=[""GET""])\ndef static_proxy(path):\n    logger.info(""Sending file \'/static/{}\' for path: {}"".format(path, path))\n    return send_from_directory(""./static/"", path)\n\n\n@app.errorhandler(404)\ndef page_not_found(e):\n    logger.info(""Sending file \'index.html\'"")\n    return send_from_directory(""./static/"", ""index.html"")\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/rok/app.py,0,"b'import base64\nfrom flask import Flask, request, jsonify, send_from_directory\nfrom ..common.base_app import app as base\nfrom ..common import utils, api\nfrom . import rok\n\n# Use the BaseApp, override the POST Notebook Endpoint\napp = Flask(__name__)\napp.register_blueprint(base)\nlogger = utils.create_logger(__name__)\n\nNOTEBOOK = ""./kubeflow_jupyter/common/yaml/notebook.yaml""\n\n\n# GETers\n@app.route(""/api/rok/namespaces/<namespace>/token"")\ndef get_token(namespace):\n    \'\'\'Retrieve the token to authenticate with Rok.\'\'\'\n    secret = None\n    name = rok.rok_secret_name()\n    token = {\n        ""name"": name,\n        ""value"": """",\n    }\n\n    user = utils.get_username_from_request()\n    data = api.get_secret(namespace, name, user)\n    if not data[""success""]:\n        logger.warning(""Couldn\'t load ROK token in namespace \'{}\': {}"".format(\n            namespace, data[""log""]\n        ))\n        data[""token""] = token\n        return jsonify(data)\n\n    secret = data[""secret""]\n    if secret.data is None:\n        logger.warning(\n            ""ROK Secret doesn\'t exist in namespace \'%s\'"" % namespace\n        )\n        return jsonify({\n            ""success"": False,\n            ""log"": ""ROK Secret doesn\'t exist in namespace \'%s\'"" % namespace,\n            ""token"": token\n        })\n\n    token = secret.data.get(""token"", """")\n    data[""token""] = {\n        ""value"": base64.b64decode(token).decode(""utf-8""),\n        ""name"": name\n    }\n    del data[""secret""]\n\n    return jsonify(data)\n\n\n# POSTers\n@app.route(""/api/namespaces/<namespace>/notebooks"", methods=[""POST""])\ndef post_notebook(namespace):\n    user = utils.get_username_from_request()\n    body = request.get_json()\n    defaults = utils.spawner_ui_config()\n    logger.info(""Got Notebook: {}"".format(body))\n\n    notebook = utils.load_param_yaml(NOTEBOOK,\n                                     name=body[""name""],\n                                     namespace=namespace,\n                                     serviceAccount=""default-editor"")\n\n    rok.attach_rok_token_secret(notebook)\n    utils.set_notebook_image(notebook, body, defaults)\n    utils.set_notebook_cpu(notebook, body, defaults)\n    utils.set_notebook_memory(notebook, body, defaults)\n    utils.set_notebook_configurations(notebook, body, defaults)\n\n    # Workspace Volume\n    workspace_vol = utils.get_workspace_vol(body, defaults)\n    if not body.get(""noWorkspace"", False) and workspace_vol[""type""] != ""None"":\n        # Create the PVC\n        ws_pvc = rok.rok_pvc_from_dict(workspace_vol, namespace)\n\n        if workspace_vol[""type""] == ""Existing"":\n            rok.add_workspace_volume_annotations(ws_pvc, workspace_vol)\n\n        logger.info(""Creating Workspace Volume: {}"".format(ws_pvc.to_dict()))\n        r = api.post_pvc(ws_pvc, user)\n        if not r[""success""]:\n            return jsonify(r)\n\n        utils.add_notebook_volume(\n            notebook,\n            r[""pvc""].metadata.name,\n            r[""pvc""].metadata.name,\n            ""/home/jovyan"",\n        )\n\n    # Add the Data Volumes\n    for vol in utils.get_data_vols(body, defaults):\n        # Create the PVC\n        dtvol_pvc = rok.rok_pvc_from_dict(vol, namespace)\n\n        if vol[""type""] == ""Existing"":\n            rok.add_data_volume_annotations(dtvol_pvc, vol)\n\n        logger.info(""Creating Data Volume {}:"".format(dtvol_pvc))\n        r = api.post_pvc(dtvol_pvc, user)\n        if not r[""success""]:\n            return jsonify(r)\n\n        utils.add_notebook_volume(\n            notebook,\n            r[""pvc""].metadata.name,\n            r[""pvc""].metadata.name,\n            vol[""path""]\n        )\n\n    # Extra Resources\n    r = utils.set_notebook_extra_resources(notebook, body, defaults)\n    if not r[""success""]:\n        return jsonify(r)\n\n    # shm\n    utils.set_notebook_shm(notebook, body, defaults)\n\n    logger.info(""Creating Notebook: {}"".format(notebook))\n    return jsonify(api.post_notebook(notebook, user))\n\n\n# Since Angular is a SPA, we serve index.html every time\n@app.route(""/"")\ndef serve_root():\n    return send_from_directory(""./static/"", ""index.html"")\n\n\n@app.route(""/<path:path>"", methods=[""GET""])\ndef static_proxy(path):\n    return send_from_directory(""./static/"", path)\n\n\n@app.errorhandler(404)\ndef page_not_found(e):\n    logger.info(""Sending file \'index.html\'"")\n    return send_from_directory(""./static/"", ""index.html"")\n\n'"
eks/demo/.cache/kubeflow/kubeflow-9804feb9fc23fc30075632a857087f4b529294e2/components/jupyter-web-app/backend/kubeflow_jupyter/rok/rok.py,0,"b'import os\nfrom ..common import utils\n\nROK_SECRET_MOUNT = ""/var/run/secrets/rok""\nlogger = utils.create_logger(__name__)\n\n\ndef parse_user_template(string):\n    return string.format(username=""user"")\n\n\ndef rok_secret_name():\n    secret = os.environ.get(""ROK_SECRET_NAME"", ""secret-rok-user"")\n    secret = parse_user_template(secret)\n    return secret\n\n\ndef attach_rok_token_secret(notebook):\n    # Mount the Rok token as a Volume\n    secret_name = rok_secret_name()\n    secret_volume_name = ""volume-%s"" % secret_name\n    utils.add_notebook_volume_secret(\n        notebook,\n        secret_volume_name,\n        secret_name,\n        ROK_SECRET_MOUNT,\n        420\n    )\n\n    # Set ENV variables needed for rok cli\n    notebook[""spec""][""template""][""spec""][""containers""][0][""env""] += [\n        {\n            ""name"": ""ROK_GW_TOKEN"",\n            ""value"": ""file:%s/token"" % ROK_SECRET_MOUNT\n        },\n        {\n            ""name"": ""ROK_GW_URL"",\n            ""value"": ""file:%s/url"" % ROK_SECRET_MOUNT\n        },\n        {\n            ""name"": ""ROK_GW_PARAM_REGISTER_JUPYTER_LAB"",\n            ""value"": notebook[""metadata""][""name""] + ""-0""\n        },\n    ]\n\n\ndef get_pvc_name(pvc):\n    name = """"\n    try:\n        name = pvc.metadata.generate_name\n    except AttributeError:\n        name = pvc.metadata.name\n\n    return name\n\n\ndef add_workspace_volume_annotations(pvc, vol):\n    \'\'\'\n    Attach the needed annotation to the PVC k8s object\n    \'\'\'\n    name = get_pvc_name(pvc)\n    \n    annotations = {\n        ""rok/creds-secret-name"": rok_secret_name(),\n        ""jupyter-workspace"": name,\n    }\n\n    if vol[""type""] == ""Existing"":\n        annotations[""rok/origin""] = vol[""extraFields""].get(""rokUrl"", """")\n\n    labels = {""component"": ""singleuser-storage""}\n\n    pvc.metadata.annotations = annotations\n    pvc.metadata.labels = labels\n\n\ndef add_data_volume_annotations(pvc, vol):\n    name = get_pvc_name(pvc)\n\n    annotations = {\n        ""rok/creds-secret-name"": rok_secret_name(),\n        ""jupyter-dataset"": name,\n    }\n\n    if vol[""type""] == ""Existing"":\n        annotations[""rok/origin""] = vol[""extraFields""].get(""rokUrl"", """")\n\n    labels = {""component"": ""singleuser-storage""}\n\n    pvc.metadata.annotations = annotations\n    pvc.metadata.labels = labels\n\n\ndef rok_pvc_from_dict(vol, namespace):\n    pvc = utils.pvc_from_dict(vol, namespace)\n    pvc.metadata.name = None\n    pvc.metadata.generate_name = vol[""name""] + ""-""\n    return pvc\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/base.py,0,"b'""""""\nThis file was scratch work. Check out main.py for refactored version.\n\nWeird phenomenon: This file runs about three times slower than the main.py\nversion, but not sure why. The main parts of the code are similar.\nNeeds more investigation.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nfrom scipy import ndimage\nfrom scipy.misc import imresize\nimport time\nimport os\n\n# GET DATA\nnum_labels = 2\npixel_depth = 255.0\nresized_height = 64\nresized_width = 64\nfolder = \'train/\'\nfiles = [folder + f for f in os.listdir(folder)]\n\n# QUEUE GRAPH\nbatch_size = 32\ncapacity = 2000\nnum_channels = 3\nqueue_graph = tf.Graph()\nwith queue_graph.as_default():\n    with queue_graph.device(\'/cpu:0\'):\n        filename_queue = tf.train.string_input_producer(files)\n        reader = tf.WholeFileReader()\n        key, value = reader.read(filename_queue)\n        image = tf.image.decode_jpeg(value, channels=num_channels)\n        image = (tf.cast(image, tf.float32) - 128.0) / 255.0\n        resized = tf.image.resize_images([image], resized_height, resized_width)\n        image_batch, label_batch = tf.train.batch(\n                                        [resized, key],\n                                        batch_size=batch_size,\n                                        capacity=capacity)\n        image_squeezed = tf.squeeze(image_batch)\n        batch_data = tf.identity(image_squeezed, name=\'batch_data\')\n        batch_labels = tf.identity(label_batch)\n\n# TODO: Replace with reading in a GraphDef or from another file\ntrain_graph = tf.Graph()\nwith train_graph.as_default():\n    input_p = tf.placeholder(tf.float32, [None, resized_height, resized_width,\n                             num_channels], name=\'input\')\n    label_p = tf.placeholder(tf.int32, [None], name=\'label\')\n    one_hot_labels = tf.one_hot(label_p, num_labels)\n    shape = input_p.get_shape().as_list()\n    reshaped = tf.reshape(input_p, [-1, shape[1] * shape[2] * shape[3]])\n    weights = tf.Variable(tf.truncated_normal([shape[1] * shape[2] * shape[3],\n                                              num_labels], stddev=0.1))\n    bias = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n    logits = tf.matmul(reshaped, weights) + bias\n    loss = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_labels,\n                                                name=\'loss\'))\n    train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n    init = tf.initialize_all_variables()\n\n    model_prediction = tf.nn.softmax(logits)\n    label_prediction = tf.argmax(model_prediction, 1, name=\'predicted_label\')\n    correct_prediction = tf.equal(label_prediction, tf.argmax(one_hot_labels,\n                                                              1))\n    model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nqueue_sess = tf.Session(graph=queue_graph)\ntrain_sess = tf.Session(graph=train_graph)\ntrain_sess.run(init)\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=queue_sess, coord=coord)\ntry:\n    time_list = []\n    while not coord.should_stop():\n        t_start = time.clock()\n        data_input, labels = queue_sess.run([image_squeezed, batch_labels])\n        labels = [\n            1\n            if b\'dog\' in s\n            else 0\n            for s in labels\n        ]\n        feed_dict = {input_p: data_input, label_p: labels}\n        loss_val, _, acc = train_sess.run([loss, train, model_accuracy],\n                                          feed_dict=feed_dict)\n        t_end = time.clock()\n        time_list.append(t_end - t_start)\n        if len(time_list) >= 30:\n            print(sum(time_list) / len(time_list))\n            time_list = []\nexcept tf.errors.OutOfRangeError:\n    print(\'Done training -- epoch limit reached\')\nfinally:\n    # When done, ask the threads to stop\n    coord.request_stop()\n\ncoord.join(threads)\nqueue_sess.close()\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/config.py,0,"b""# Size of training batch\nBATCH_SIZE = 8\n\n# Max number of images stored in queue\nCAPACITY = 2000\n\n# Directory to find training images\nDATA_DIRECTORY = '../../../datasets/train/'\n\n# Desired rescaled height of images\nHEIGHT = 256\n\n# Desired rescaled width of images\nWIDTH = 256\n\n# Number of channels in image\n# e.g.\n# grayscale = 1\n# rgb = 3\nCHANNELS = 3\n\n# Maximum value for each channel per pixel\nPIXEL_DEPTH = 255.0\n\n# Number of output labels\nNUM_LABELS = 2\n\n\n\n"""
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/data.py,0,"b'import os\n\n\ndef get_data(data_directory):\n    """"""\n    Given a string directory path, returns a list of file paths to images\n    inside that directory.\n\n    Data must be in jpeg format.\n\n    Args:\n        data_directory (str): String directory location of input data\n\n    Returns:\n        :obj:`list` of :obj:`str`: list of image file paths\n    """"""\n    return [\n        data_directory + f\n        for\n        f\n        in\n        os.listdir(data_directory)\n        if\n        f.endswith(\'.jpeg\') or f.endswith(\'.jpg\')\n    ]\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/main.py,0,"b'import config\nfrom queues import create_image_queue_graph\nfrom models import create_model_graph\nfrom train import train_model\nfrom data import get_data\n\n\ndef main():\n    """"""\n    """"""\n    placeholders = [\'input\', \'label\']\n    train_ops = [\'train\']\n    log_ops = [\'accuracy\']\n    files = get_data(config.DATA_DIRECTORY)\n    queue_graph = create_image_queue_graph(files, config.PIXEL_DEPTH,\n                                           config.HEIGHT, config.WIDTH,\n                                           config.CHANNELS,\n                                           config.BATCH_SIZE, config.CAPACITY)\n    model_graph = create_model_graph(config.HEIGHT, config.WIDTH,\n                                     config.CHANNELS, config.NUM_LABELS)\n    train_model(queue_graph, model_graph, placeholders, train_ops, log_ops)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/models.py,0,"b'import tensorflow as tf\n\n\ndef create_model_graph(height, width, channels, num_labels):\n    """"""\n    """"""\n    graph = tf.Graph()\n    with graph.as_default():\n        input_p = tf.placeholder(tf.float32, [None, height, width, channels],\n                                 name=\'input\')\n        label_p = tf.placeholder(tf.int32, [None], name=\'label\')\n        one_hot_labels = tf.one_hot(label_p, num_labels)\n        shape = input_p.get_shape().as_list()\n        num_pixels = shape[1] * shape[2] * shape[3]\n        reshaped = tf.reshape(input_p, [-1, num_pixels])\n        weights = tf.Variable(tf.truncated_normal([num_pixels, num_labels],\n                                                  stddev=0.1))\n        bias = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n        logits = tf.matmul(reshaped, weights) + bias\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_labels,\n                                                    name=\'loss\'))\n        train = tf.train.GradientDescentOptimizer(0.01).minimize(loss,\n                                                                 name=\'train\')\n        init = tf.initialize_all_variables()\n\n        model_prediction = tf.nn.softmax(logits)\n        label_prediction = tf.argmax(model_prediction, 1,\n                                     name=\'predicted_label\')\n        correct_prediction = tf.equal(label_prediction,\n                                      tf.argmax(one_hot_labels, 1))\n        model_accuracy = tf.reduce_mean(tf.cast(correct_prediction,\n                                                tf.float32), name=\'accuracy\')\n    return graph\n\n\ndef generic_supervised_graph(input_type, input_shape, label_type, label_shape):\n    """"""\n    """"""\n    graph = tf.Graph()\n    with graph.as_default():\n        input_p = tf.placeholder(input_type, input_shape, name=\'input\')\n        label_p = tf.placeholder(label_type, label_shape, name=\'label\')\n\n        loss = tf.identity(input_p, name=\'loss\')\n        train = tf.train.GradientDescentOptimizer(0.01).minimize(loss,\n                                                                 name=\'train\')\n    return graph\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/online-tensorflow-consumer.py,0,"b""from confluent_kafka import Consumer, KafkaError\nimport time\nimport os\n\nclass image:\n    image_name = ''\n    image_bytes = ''\n    partition = ''\n    offset = ''\n\ndef open_consumer(stream_host_and_port_list, topic_name, group_name):\n    consumer = Consumer({'bootstrap.servers': stream_host_and_port_list, # kafka broker\n                           'group.id': group_name, # consumer group\n                           'api.version.request':'true'\n                        })\n    consumer.subscribe([topic_name])\n    return consumer\n\ndef next_batch(consumer, batch_size):\n    msg = consumer.poll(timeout=200)\n\n    # TODO:  return batch_size # of images\n    batch = []\n\n    if msg is not None:\n        img = image()\n        img.image_name = msg.key()\n        img.image_bytes = msg.value()\n        img.partition = msg.partition()\n        img.offset = msg.offset()\n\n        batch.append(img)\n\n    return batch\n\nif __name__ == '__main__':\n    stream_host_and_port_list = os.getenv('STREAM_HOST_AND_PORT_LIST')\n    topic_name  = os.getenv('TOPIC_NAME')\n    group_name = os.getenv('GROUP_NAME')\n\n    consumer = open_consumer(stream_host_and_port_list, topic_name, group_name)\n\n    batch_size = 16\n\n    while True:\n        batch = next_batch(consumer, batch_size)\n        if len(batch) > 0:\n            print(batch[0].image_bytes)\n            print(batch[0].image_name)\n        time.sleep(2)\n"""
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/queues.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport imghdr\n\n\ndef create_queue_reader(files):\n    """"""\n    Given a string Tensor of filenames, creates a filename queue and\n    `WholeFileReader`. Returns the key, value tensors returned from the\n    `Reader.read()` method\n\n    Args:\n        files: String Tensor of file paths\n\n    Returns:\n        key, value tensors from the `tf.Reader.read()` method\n    """"""\n    filename_queue = tf.train.string_input_producer(files)\n    reader = tf.WholeFileReader()\n    return reader.read(filename_queue)\n\n\ndef clean_image_data(image, pixel_depth, height, width):\n    """"""\n    Cleans data from an image `Tensor`, and returns a new `Tensor`\n\n    Args\n    """"""\n    norm = (tf.cast(image, tf.float32) - (pixel_depth / 2)) / pixel_depth\n    resized = tf.image.resize_images([norm], height, width)\n    return resized\n\n\ndef is_jpeg(filename, data):\n    return imghdr.what(filename, data) == \'jpeg\'\n\n\ndef is_png(filename, data):\n    return imghdr.what(filename, data) == \'png\'\n\n\ndef has_jpeg_ext(filename):\n    return np.array(filename.endswith(b\'.jpeg\') or filename.endswith(b\'.jpg\'))\n\n\ndef has_png_ext(filename):\n    return np.array(filename.endswith(b\'.png\'))\n\n\ndef create_image_queue_graph(files, pixel_depth, height, width, channels,\n                             batch_size, capacity):\n    """"""\n    Creates and returns a TensorFlow graph containing a configured queue to\n    pull in data\n    """"""\n    graph = tf.Graph()\n    with graph.as_default():\n        with graph.device(\'/cpu:0\'):\n            key, value = create_queue_reader(files)\n            # image = tf.image.decode_jpeg(value, channels=channels)\n            is_jpeg = tf.py_func(has_jpeg_ext, [key], [tf.bool])\n            image = tf.cond(is_jpeg[0],\n                            lambda: tf.image.decode_jpeg(value,\n                                                         channels=channels),\n                            lambda: tf.image.decode_png(value,\n                                                        channels=channels)\n                            )\n            cleaned = clean_image_data(image, pixel_depth, height, width)\n            image_batch, label_batch = tf.train.batch(\n                [cleaned, key],\n                batch_size=batch_size,\n                capacity=capacity)\n            image_squeezed = tf.squeeze(image_batch)\n            batch_data = tf.identity(image_squeezed, name=\'batch_data\')\n            batch_labels = tf.identity(label_batch, name=\'batch_labels\')\n    return graph\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/train.py,0,"b'import tensorflow as tf\nimport time\n\n\ndef get_op(graph, name):\n    """"""\n    Returns the operation(s) in `graph` with `name`\n    """"""\n    return graph.get_operation_by_name(name)\n\n\ndef get_tensor(graph, name, output_idx=0):\n    """"""\n    Returns the output of the operation in `graph` with `name`\n    """"""\n    return get_op(graph, name).outputs[output_idx]\n\n\ndef op_in_types(op, types):\n    """"""\n\n    """"""\n    if type(op) == tf.Tensor:\n        op_type = op.op.type\n    elif type(op) == tf.Operation:\n        op_type = op.type\n    else:\n        raise TypeError(\'Object being checked must be of type `tf.Tensor` or \'\n                        \'`tf.Operation`\')\n    return op in types, op_type\n\n\ndef check_op_type(op, types):\n    is_valid, actual_type = op_in_types(op, types)\n    if is_valid:\n        return\n    else:\n        raise TypeError(\'Expected node \\\'{}\\\' to be in types {}\'\n                        \', got type {}\'.format([op.name, types, actual_type]))\n\n\ndef create_tensor_dict(graph, names, types=None):\n    tensor_dict = {}\n    for name in names:\n        tensor = get_tensor(graph, name)\n        if types is not None:\n            check_op_type(tensor, types)\n        tensor_dict[name] = tensor\n    return tensor_dict\n\n\ndef create_op_dict(graph, names, types=None):\n    op_dict = {}\n    for name in names:\n        op = get_op(graph, name)\n        if types is not None:\n            check_op_type(op, types)\n        op_dict[name] = op\n    return op_dict\n\n\ndef create_placeholder_dict(graph, placeholder_pairs):\n    """"""\n    Creates a dictionary mapping string names to placeholder operations.\n\n    Args:\n        graph: `tf.Graph` object containing operations\n        placeholder_pairs: list of tuples\n\n    Returns:\n        Dictionary mapping string:`tf.placeholder`\n    """"""\n    names = [\n        p[1]\n        for\n        p\n        in\n        placeholder_pairs\n        ]\n    return create_tensor_dict(graph, names, types=[\'Placeholder\'])\n\n\ndef get_init_op(graph):\n    return get_op(graph, \'init\')\n\n\ndef add_variable_summaries(name, op):\n    """"""\n    TODO: Add mean, standard deviation, min, max, histogram\n    """"""\n    pass\n\n\ndef add_summary_ops(graph, log_ops):\n    """"""\n    """"""\n    with graph.as_default():\n        for name in log_ops:\n            tf.scalar_summary(name, log_ops[name])\n\n\ndef add_merged_summaries(graph):\n    """"""\n    """"""\n    with graph.as_default():\n        return tf.merge_all_summaries()\n\n\ndef train_model(queue_graph, model_graph, placeholder_list, train_list,\n                log_list):\n    """"""\n    TODO: Documentation\n    TODO: Clean up the all of the `get_operation_by_name()` lines\n    TODO: TensorBoard\n    """"""\n    batch_data = get_tensor(queue_graph, \'batch_data\')\n    batch_labels = get_tensor(queue_graph, \'batch_labels\')\n    init = get_init_op(model_graph)\n    placeholder_ops = create_tensor_dict(model_graph, placeholder_list)\n    train_ops = create_op_dict(model_graph, train_list)\n    train = train_ops[\'train\']\n    log_ops = create_tensor_dict(model_graph, log_list)\n    add_summary_ops(model_graph, log_ops)\n    summaries = add_merged_summaries(model_graph)\n\n    queue_sess = tf.Session(graph=queue_graph)\n    model_sess = tf.Session(graph=model_graph)\n    model_sess.run(init)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=queue_sess, coord=coord)\n    try:\n        time_list = []\n        while not coord.should_stop():\n            t_start = time.clock()\n            data_input, labels = queue_sess.run([batch_data, batch_labels])\n            labels = [\n                1\n                if b\'dog\' in s\n                else 0\n                for s in labels\n                ]\n            feed_dict = {placeholder_ops[placeholder_list[0]]: data_input,\n                         placeholder_ops[placeholder_list[1]]: labels}\n            summ, _ = model_sess.run([summaries, train],\n                                     feed_dict=feed_dict)\n            t_end = time.clock()\n            time_list.append(t_end - t_start)\n            if len(time_list) >= 30:\n                print(sum(time_list) / len(time_list))\n                time_list = []\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # When done, ask the threads to stop\n        coord.request_stop()\n    coord.join(threads)\n    queue_sess.close()\n    model_sess.close()'"
stream/jvm/src/main/java/io/pipeline/tensorflow/producer/src/online-tensorflow-producer.py,0,"b""from confluent_kafka import Producer\nimport glob\nimport os\n\ndef open_producer(stream_host_and_port_list):\n    producer = Producer({'bootstrap.servers': stream_host_and_port_list, # kafka broker\n                         'api.version.request':'true'\n                        })\n    return producer\n\ndef produce(producer, topic_name, image_paths_glob_str):\n    image_paths = glob.glob(image_paths_glob_str)\n\n    # TODO:  shuffle\n    for image_path in image_paths:\n        image_file = open(image_path, 'rb')\n        image_bytes = image_file.read()\n        image_file.close()\n        print image_bytes\n\n        image_name = os.path.basename(image_path)\n        print image_name\n\n        producer.produce(topic_name, key=image_name, value=image_bytes)\n        producer.flush()\n\nif __name__ == '__main__':\n    stream_host_and_port_list = os.getenv('STREAM_HOST_AND_PORT_LIST')\n    topic_name  = os.getenv('TOPIC_NAME')\n    image_paths_glob_str = os.getenv('IMAGE_PATHS_GLOB_STRING')\n\n    producer = open_producer(stream_host_and_port_list)\n\n    produce(producer, topic_name, image_paths_glob_str)\n"""
