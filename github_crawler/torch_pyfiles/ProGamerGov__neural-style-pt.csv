file_path,api_count,code
CaffeLoader.py,6,"b'import torch\nimport torch.nn as nn\n\n\nclass VGG(nn.Module):\n    def __init__(self, features, num_classes=1000):\n        super(VGG, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n\n\nclass VGG_SOD(nn.Module):\n    def __init__(self, features, num_classes=100):\n        super(VGG_SOD, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 100),\n        )\n\n\nclass VGG_FCN32S(nn.Module):\n    def __init__(self, features, num_classes=1000):\n        super(VGG_FCN32S, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Conv2d(512,4096,(7, 7)),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Conv2d(4096,4096,(1, 1)),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n        )\n\n\nclass VGG_PRUNED(nn.Module):\n    def __init__(self, features, num_classes=1000):\n        super(VGG_PRUNED, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n        )\n\n\nclass NIN(nn.Module):\n    def __init__(self, pooling):\n        super(NIN, self).__init__()\n        if pooling == \'max\':\n            pool2d = nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True)\n        elif pooling == \'avg\':\n            pool2d = nn.AvgPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True)\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3,96,(11, 11),(4, 4)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(96,96,(1, 1)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(96,96,(1, 1)),\n            nn.ReLU(inplace=True),\n            pool2d,\n            nn.Conv2d(96,256,(5, 5),(1, 1),(2, 2)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256,256,(1, 1)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256,256,(1, 1)),\n            nn.ReLU(inplace=True),\n            pool2d,\n            nn.Conv2d(256,384,(3, 3),(1, 1),(1, 1)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384,384,(1, 1)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384,384,(1, 1)),\n            nn.ReLU(inplace=True),\n            pool2d,\n            nn.Dropout(0.5),\n            nn.Conv2d(384,1024,(3, 3),(1, 1),(1, 1)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(1024,1024,(1, 1)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(1024,1000,(1, 1)),\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d((6, 6),(1, 1),(0, 0),ceil_mode=True),\n            nn.Softmax(),\n        )\n\n\n\nclass ModelParallel(nn.Module):\n    def __init__(self, net, device_ids, device_splits):\n        super(ModelParallel, self).__init__()\n        self.device_list = self.name_devices(device_ids.split(\',\'))\n        self.chunks = self.chunks_to_devices(self.split_net(net, device_splits.split(\',\')))\n\n    def name_devices(self, input_list):\n        device_list = []\n        for i, device in enumerate(input_list):\n            if str(device).lower() != \'c\':\n                device_list.append(""cuda:"" + str(device))\n            else:\n                device_list.append(""cpu"")\n        return device_list\n\n    def split_net(self, net, device_splits):\n        chunks, cur_chunk = [], nn.Sequential()\n        for i, l in enumerate(net):\n            cur_chunk.add_module(str(i), net[i])\n            if str(i) in device_splits and device_splits != \'\':\n                del device_splits[0]\n                chunks.append(cur_chunk)\n                cur_chunk = nn.Sequential()\n        chunks.append(cur_chunk)\n        return chunks\n\n    def chunks_to_devices(self, chunks):\n        for i, chunk in enumerate(chunks):\n            chunk.to(self.device_list[i])\n        return chunks\n\n    def c(self, input, i):\n        if input.type() == \'torch.FloatTensor\' and \'cuda\' in self.device_list[i]:\n            input = input.type(\'torch.cuda.FloatTensor\')\n        elif input.type() == \'torch.cuda.FloatTensor\' and \'cpu\' in self.device_list[i]:\n            input = input.type(\'torch.FloatTensor\')\n        return input\n\n    def forward(self, input):\n        for i, chunk in enumerate(self.chunks):\n            if i < len(self.chunks) -1:\n                input = self.c(chunk(self.c(input, i).to(self.device_list[i])), i+1).to(self.device_list[i+1])\n            else:\n                input = chunk(input)\n        return input\n\n\n\ndef buildSequential(channel_list, pooling):\n    layers = []\n    in_channels = 3\n    if pooling == \'max\':\n        pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n    elif pooling == \'avg\':\n        pool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n    else:\n        raise ValueError(""Unrecognized pooling parameter"")\n    for c in channel_list:\n        if c == \'P\':\n            layers += [pool2d]\n        else:\n            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n            layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = c\n    return nn.Sequential(*layers)\n\n\nchannel_list = {\n\'VGG-16p\': [24, 22, \'P\', 41, 51, \'P\', 108, 89, 111, \'P\', 184, 276, 228, \'P\', 512, 512, 512, \'P\'],\n\'VGG-16\': [64, 64, \'P\', 128, 128, \'P\', 256, 256, 256, \'P\', 512, 512, 512, \'P\', 512, 512, 512, \'P\'],\n\'VGG-19\': [64, 64, \'P\', 128, 128, \'P\', 256, 256, 256, 256, \'P\', 512, 512, 512, 512, \'P\', 512, 512, 512, 512, \'P\'],\n}\n\nnin_dict = {\n\'C\': [\'conv1\', \'cccp1\', \'cccp2\', \'conv2\', \'cccp3\', \'cccp4\', \'conv3\', \'cccp5\', \'cccp6\', \'conv4-1024\', \'cccp7-1024\', \'cccp8-1024\'],\n\'R\': [\'relu0\', \'relu1\', \'relu2\', \'relu3\', \'relu5\', \'relu6\', \'relu7\', \'relu8\', \'relu9\', \'relu10\', \'relu11\', \'relu12\'],\n\'P\': [\'pool1\', \'pool2\', \'pool3\', \'pool4\'],\n\'D\': [\'drop\'],\n}\nvgg16_dict = {\n\'C\': [\'conv1_1\', \'conv1_2\', \'conv2_1\', \'conv2_2\', \'conv3_1\', \'conv3_2\', \'conv3_3\', \'conv4_1\', \'conv4_2\', \'conv4_3\', \'conv5_1\', \'conv5_2\', \'conv5_3\'],\n\'R\': [\'relu1_1\', \'relu1_2\', \'relu2_1\', \'relu2_2\', \'relu3_1\', \'relu3_2\', \'relu3_3\', \'relu4_1\', \'relu4_2\', \'relu4_3\', \'relu5_1\', \'relu5_2\', \'relu5_3\'],\n\'P\': [\'pool1\', \'pool2\', \'pool3\', \'pool4\', \'pool5\'],\n}\nvgg19_dict = {\n\'C\': [\'conv1_1\', \'conv1_2\', \'conv2_1\', \'conv2_2\', \'conv3_1\', \'conv3_2\', \'conv3_3\', \'conv3_4\', \'conv4_1\', \'conv4_2\', \'conv4_3\', \'conv4_4\', \'conv5_1\', \'conv5_2\', \'conv5_3\', \'conv5_4\'],\n\'R\': [\'relu1_1\', \'relu1_2\', \'relu2_1\', \'relu2_2\', \'relu3_1\', \'relu3_2\', \'relu3_3\', \'relu3_4\', \'relu4_1\', \'relu4_2\', \'relu4_3\', \'relu4_4\', \'relu5_1\', \'relu5_2\', \'relu5_3\', \'relu5_4\'],\n\'P\': [\'pool1\', \'pool2\', \'pool3\', \'pool4\', \'pool5\'],\n}\n\n\ndef modelSelector(model_file, pooling):\n    vgg_list = [""fcn32s"", ""pruning"", ""sod"", ""vgg""]\n    if any(name in model_file for name in vgg_list):\n        if ""pruning"" in model_file:\n            print(""VGG-16 Architecture Detected"")\n            print(""Using The Channel Pruning Model"")\n            cnn, layerList = VGG_PRUNED(buildSequential(channel_list[\'VGG-16p\'], pooling)), vgg16_dict\n        elif ""fcn32s"" in model_file:\n            print(""VGG-16 Architecture Detected"")\n            print(""Using the fcn32s-heavy-pascal Model"")\n            cnn, layerList = VGG_FCN32S(buildSequential(channel_list[\'VGG-16\'], pooling)), vgg16_dict\n        elif ""sod"" in model_file:\n            print(""VGG-16 Architecture Detected"")\n            print(""Using The SOD Fintune Model"")\n            cnn, layerList = VGG_SOD(buildSequential(channel_list[\'VGG-16\'], pooling)), vgg16_dict\n        elif ""19"" in model_file:\n            print(""VGG-19 Architecture Detected"")\n            cnn, layerList = VGG(buildSequential(channel_list[\'VGG-19\'], pooling)), vgg19_dict\n        elif ""16"" in model_file:\n            print(""VGG-16 Architecture Detected"")\n            cnn, layerList = VGG(buildSequential(channel_list[\'VGG-16\'], pooling)), vgg16_dict\n        else:\n            raise ValueError(""VGG architecture not recognized."")\n    elif ""nin"" in model_file:\n        print(""NIN Architecture Detected"")\n        cnn, layerList = NIN(pooling), nin_dict\n    else:\n        raise ValueError(""Model architecture not recognized."")\n    return cnn, layerList\n\n\n# Print like Torch7/loadcaffe\ndef print_loadcaffe(cnn, layerList):\n    c = 0\n    for l in list(cnn):\n         if ""Conv2d"" in str(l):\n             in_c, out_c, ks  = str(l.in_channels), str(l.out_channels), str(l.kernel_size)\n             print(layerList[\'C\'][c] +"": "" +  (out_c + "" "" + in_c + "" "" + ks).replace("")"",\'\').replace(""("",\'\').replace("","",\'\') )\n             c+=1\n         if c == len(layerList[\'C\']):\n             break\n\n\n# Load the model, and configure pooling layer type\ndef loadCaffemodel(model_file, pooling, use_gpu, disable_check):\n    cnn, layerList = modelSelector(str(model_file).lower(), pooling)\n\n    cnn.load_state_dict(torch.load(model_file), strict=(not disable_check))\n    print(""Successfully loaded "" + str(model_file))\n\n    # Maybe convert the model to cuda now, to avoid later issues\n    if ""c"" not in str(use_gpu).lower() or ""c"" not in str(use_gpu[0]).lower():\n        cnn = cnn.cuda()\n    cnn = cnn.features\n\n    print_loadcaffe(cnn, layerList)\n\n    return cnn, layerList\n'"
neural_style.py,19,"b'import os\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nfrom CaffeLoader import loadCaffemodel, ModelParallel\n\nimport argparse\nparser = argparse.ArgumentParser()\n# Basic options\nparser.add_argument(""-style_image"", help=""Style target image"", default=\'examples/inputs/seated-nude.jpg\')\nparser.add_argument(""-style_blend_weights"", default=None)\nparser.add_argument(""-content_image"", help=""Content target image"", default=\'examples/inputs/tubingen.jpg\')\nparser.add_argument(""-image_size"", help=""Maximum height / width of generated image"", type=int, default=512)\nparser.add_argument(""-gpu"", help=""Zero-indexed ID of the GPU to use; for CPU mode set -gpu = c"", default=0)\n\n# Optimization options\nparser.add_argument(""-content_weight"", type=float, default=5e0)\nparser.add_argument(""-style_weight"", type=float, default=1e2)\nparser.add_argument(""-normalize_weights"", action=\'store_true\')\nparser.add_argument(""-tv_weight"", type=float, default=1e-3)\nparser.add_argument(""-num_iterations"", type=int, default=1000)\nparser.add_argument(""-init"", choices=[\'random\', \'image\'], default=\'random\')\nparser.add_argument(""-init_image"", default=None)\nparser.add_argument(""-optimizer"", choices=[\'lbfgs\', \'adam\'], default=\'lbfgs\')\nparser.add_argument(""-learning_rate"", type=float, default=1e0)\nparser.add_argument(""-lbfgs_num_correction"", type=int, default=100)\n\n# Output options\nparser.add_argument(""-print_iter"", type=int, default=50)\nparser.add_argument(""-save_iter"", type=int, default=100)\nparser.add_argument(""-output_image"", default=\'out.png\')\n\n# Other options\nparser.add_argument(""-style_scale"", type=float, default=1.0)\nparser.add_argument(""-original_colors"", type=int, choices=[0, 1], default=0)\nparser.add_argument(""-pooling"", choices=[\'avg\', \'max\'], default=\'max\')\nparser.add_argument(""-model_file"", type=str, default=\'models/vgg19-d01eb7cb.pth\')\nparser.add_argument(""-disable_check"", action=\'store_true\')\nparser.add_argument(""-backend"", choices=[\'nn\', \'cudnn\', \'mkl\', \'mkldnn\', \'openmp\', \'mkl,cudnn\', \'cudnn,mkl\'], default=\'nn\')\nparser.add_argument(""-cudnn_autotune"", action=\'store_true\')\nparser.add_argument(""-seed"", type=int, default=-1)\n\nparser.add_argument(""-content_layers"", help=""layers for content"", default=\'relu4_2\')\nparser.add_argument(""-style_layers"", help=""layers for style"", default=\'relu1_1,relu2_1,relu3_1,relu4_1,relu5_1\')\n\nparser.add_argument(""-multidevice_strategy"", default=\'4,7,29\')\nparams = parser.parse_args()\n\n\nImage.MAX_IMAGE_PIXELS = 1000000000 # Support gigapixel images\n\n\ndef main():\n    dtype, multidevice, backward_device = setup_gpu()\n\n    cnn, layerList = loadCaffemodel(params.model_file, params.pooling, params.gpu, params.disable_check)\n\n    content_image = preprocess(params.content_image, params.image_size).type(dtype)\n    style_image_input = params.style_image.split(\',\')\n    style_image_list, ext = [], ["".jpg"", "".jpeg"", "".png"", "".tiff""]\n    for image in style_image_input:\n        if os.path.isdir(image):\n            images = (image + ""/"" + file for file in os.listdir(image)\n            if os.path.splitext(file)[1].lower() in ext)\n            style_image_list.extend(images)\n        else:\n            style_image_list.append(image)\n    style_images_caffe = []\n    for image in style_image_list:\n        style_size = int(params.image_size * params.style_scale)\n        img_caffe = preprocess(image, style_size).type(dtype)\n        style_images_caffe.append(img_caffe)\n\n    if params.init_image != None:\n        image_size = (content_image.size(2), content_image.size(3))\n        init_image = preprocess(params.init_image, image_size).type(dtype)\n\n    # Handle style blending weights for multiple style inputs\n    style_blend_weights = []\n    if params.style_blend_weights == None:\n        # Style blending not specified, so use equal weighting\n        for i in style_image_list:\n            style_blend_weights.append(1.0)\n        for i, blend_weights in enumerate(style_blend_weights):\n            style_blend_weights[i] = int(style_blend_weights[i])\n    else:\n        style_blend_weights = params.style_blend_weights.split(\',\')\n        assert len(style_blend_weights) == len(style_image_list), \\\n          ""-style_blend_weights and -style_images must have the same number of elements!""\n\n    # Normalize the style blending weights so they sum to 1\n    style_blend_sum = 0\n    for i, blend_weights in enumerate(style_blend_weights):\n        style_blend_weights[i] = float(style_blend_weights[i])\n        style_blend_sum = float(style_blend_sum) + style_blend_weights[i]\n    for i, blend_weights in enumerate(style_blend_weights):\n        style_blend_weights[i] = float(style_blend_weights[i]) / float(style_blend_sum)\n\n    content_layers = params.content_layers.split(\',\')\n    style_layers = params.style_layers.split(\',\')\n\n    # Set up the network, inserting style and content loss modules\n    cnn = copy.deepcopy(cnn)\n    content_losses, style_losses, tv_losses = [], [], []\n    next_content_idx, next_style_idx = 1, 1\n    net = nn.Sequential()\n    c, r = 0, 0\n    if params.tv_weight > 0:\n        tv_mod = TVLoss(params.tv_weight).type(dtype)\n        net.add_module(str(len(net)), tv_mod)\n        tv_losses.append(tv_mod)\n\n    for i, layer in enumerate(list(cnn), 1):\n        if next_content_idx <= len(content_layers) or next_style_idx <= len(style_layers):\n            if isinstance(layer, nn.Conv2d):\n                net.add_module(str(len(net)), layer)\n\n                if layerList[\'C\'][c] in content_layers:\n                    print(""Setting up content layer "" + str(i) + "": "" + str(layerList[\'C\'][c]))\n                    loss_module = ContentLoss(params.content_weight)\n                    net.add_module(str(len(net)), loss_module)\n                    content_losses.append(loss_module)\n\n                if layerList[\'C\'][c] in style_layers:\n                    print(""Setting up style layer "" + str(i) + "": "" + str(layerList[\'C\'][c]))\n                    loss_module = StyleLoss(params.style_weight)\n                    net.add_module(str(len(net)), loss_module)\n                    style_losses.append(loss_module)\n                c+=1\n\n            if isinstance(layer, nn.ReLU):\n                net.add_module(str(len(net)), layer)\n\n                if layerList[\'R\'][r] in content_layers:\n                    print(""Setting up content layer "" + str(i) + "": "" + str(layerList[\'R\'][r]))\n                    loss_module = ContentLoss(params.content_weight)\n                    net.add_module(str(len(net)), loss_module)\n                    content_losses.append(loss_module)\n                    next_content_idx += 1\n\n                if layerList[\'R\'][r] in style_layers:\n                    print(""Setting up style layer "" + str(i) + "": "" + str(layerList[\'R\'][r]))\n                    loss_module = StyleLoss(params.style_weight)\n                    net.add_module(str(len(net)), loss_module)\n                    style_losses.append(loss_module)\n                    next_style_idx += 1\n                r+=1\n\n            if isinstance(layer, nn.MaxPool2d) or isinstance(layer, nn.AvgPool2d):\n                net.add_module(str(len(net)), layer)\n\n    if multidevice:\n        net = setup_multi_device(net)\n\n    # Capture content targets\n    for i in content_losses:\n        i.mode = \'capture\'\n    print(""Capturing content targets"")\n    print_torch(net, multidevice)\n    net(content_image)\n\n    # Capture style targets\n    for i in content_losses:\n        i.mode = \'None\'\n\n    for i, image in enumerate(style_images_caffe):\n        print(""Capturing style target "" + str(i+1))\n        for j in style_losses:\n            j.mode = \'capture\'\n            j.blend_weight = style_blend_weights[i]\n        net(style_images_caffe[i])\n\n    # Set all loss modules to loss mode\n    for i in content_losses:\n        i.mode = \'loss\'\n    for i in style_losses:\n        i.mode = \'loss\'\n\n    # Maybe normalize content and style weights\n    if params.normalize_weights:\n        normalize_weights(content_losses, style_losses)\n\n    # Freeze the network in order to prevent\n    # unnecessary gradient calculations\n    for param in net.parameters():\n        param.requires_grad = False\n\n    # Initialize the image\n    if params.seed >= 0:\n        torch.manual_seed(params.seed)\n        torch.cuda.manual_seed_all(params.seed)\n        torch.backends.cudnn.deterministic=True\n    if params.init == \'random\':\n        B, C, H, W = content_image.size()\n        img = torch.randn(C, H, W).mul(0.001).unsqueeze(0).type(dtype)\n    elif params.init == \'image\':\n        if params.init_image != None:\n            img = init_image.clone()\n        else:\n            img = content_image.clone()\n    img = nn.Parameter(img)\n\n    def maybe_print(t, loss):\n        if params.print_iter > 0 and t % params.print_iter == 0:\n            print(""Iteration "" + str(t) + "" / ""+ str(params.num_iterations))\n            for i, loss_module in enumerate(content_losses):\n                print(""  Content "" + str(i+1) + "" loss: "" + str(loss_module.loss.item()))\n            for i, loss_module in enumerate(style_losses):\n                print(""  Style "" + str(i+1) + "" loss: "" + str(loss_module.loss.item()))\n            print(""  Total loss: "" + str(loss.item()))\n\n    def maybe_save(t):\n        should_save = params.save_iter > 0 and t % params.save_iter == 0\n        should_save = should_save or t == params.num_iterations\n        if should_save:\n            output_filename, file_extension = os.path.splitext(params.output_image)\n            if t == params.num_iterations:\n                filename = output_filename + str(file_extension)\n            else:\n                filename = str(output_filename) + ""_"" + str(t) + str(file_extension)\n            disp = deprocess(img.clone())\n\n            # Maybe perform postprocessing for color-independent style transfer\n            if params.original_colors == 1:\n                disp = original_colors(deprocess(content_image.clone()), disp)\n\n            disp.save(str(filename))\n\n    # Function to evaluate loss and gradient. We run the net forward and\n    # backward to get the gradient, and sum up losses from the loss modules.\n    # optim.lbfgs internally handles iteration and calls this function many\n    # times, so we manually count the number of iterations to handle printing\n    # and saving intermediate results.\n    num_calls = [0]\n    def feval():\n        num_calls[0] += 1\n        optimizer.zero_grad()\n        net(img)\n        loss = 0\n\n        for mod in content_losses:\n            loss += mod.loss.to(backward_device)\n        for mod in style_losses:\n            loss += mod.loss.to(backward_device)\n        if params.tv_weight > 0:\n            for mod in tv_losses:\n                loss += mod.loss.to(backward_device)\n\n        loss.backward()\n\n        maybe_save(num_calls[0])\n        maybe_print(num_calls[0], loss)\n\n        return loss\n\n    optimizer, loopVal = setup_optimizer(img)\n    while num_calls[0] <= loopVal:\n         optimizer.step(feval)\n\n\n# Configure the optimizer\ndef setup_optimizer(img):\n    if params.optimizer == \'lbfgs\':\n        print(""Running optimization with L-BFGS"")\n        optim_state = {\n            \'max_iter\': params.num_iterations,\n            \'tolerance_change\': -1,\n            \'tolerance_grad\': -1,\n        }\n        if params.lbfgs_num_correction != 100:\n            optim_state[\'history_size\'] = params.lbfgs_num_correction\n        optimizer = optim.LBFGS([img], **optim_state)\n        loopVal = 1\n    elif params.optimizer == \'adam\':\n        print(""Running optimization with ADAM"")\n        optimizer = optim.Adam([img], lr = params.learning_rate)\n        loopVal = params.num_iterations - 1\n    return optimizer, loopVal\n\n\ndef setup_gpu():\n    def setup_cuda():\n        if \'cudnn\' in params.backend:\n            torch.backends.cudnn.enabled = True\n            if params.cudnn_autotune:\n                torch.backends.cudnn.benchmark = True\n        else:\n            torch.backends.cudnn.enabled = False\n\n    def setup_cpu():\n        if \'mkl\' in params.backend and \'mkldnn\' not in params.backend:\n            torch.backends.mkl.enabled = True\n        elif \'mkldnn\' in params.backend:\n            raise ValueError(""MKL-DNN is not supported yet."")\n        elif \'openmp\' in params.backend:\n            torch.backends.openmp.enabled = True\n\n    multidevice = False\n    if "","" in str(params.gpu):\n        devices = params.gpu.split(\',\')\n        multidevice = True\n\n        if \'c\' in str(devices[0]).lower():\n            backward_device = ""cpu""\n            setup_cuda(), setup_cpu()\n        else:\n            backward_device = ""cuda:"" + devices[0]\n            setup_cuda()\n        dtype = torch.FloatTensor\n\n    elif ""c"" not in str(params.gpu).lower():\n        setup_cuda()\n        dtype, backward_device = torch.cuda.FloatTensor, ""cuda:"" + str(params.gpu)\n    else:\n        setup_cpu()\n        dtype, backward_device = torch.FloatTensor, ""cpu""\n    return dtype, multidevice, backward_device\n\n\ndef setup_multi_device(net):\n    assert len(params.gpu.split(\',\')) - 1 == len(params.multidevice_strategy.split(\',\')), \\\n      ""The number of -multidevice_strategy layer indices minus 1, must be equal to the number of -gpu devices.""\n\n    new_net = ModelParallel(net, params.gpu, params.multidevice_strategy)\n    return new_net\n\n\n# Preprocess an image before passing it to a model.\n# We need to rescale from [0, 1] to [0, 255], convert from RGB to BGR,\n# and subtract the mean pixel.\ndef preprocess(image_name, image_size):\n    image = Image.open(image_name).convert(\'RGB\')\n    if type(image_size) is not tuple:\n        image_size = tuple([int((float(image_size) / max(image.size))*x) for x in (image.height, image.width)])\n    Loader = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n    rgb2bgr = transforms.Compose([transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])])])\n    Normalize = transforms.Compose([transforms.Normalize(mean=[103.939, 116.779, 123.68], std=[1,1,1])])\n    tensor = Normalize(rgb2bgr(Loader(image) * 256)).unsqueeze(0)\n    return tensor\n\n\n#  Undo the above preprocessing.\ndef deprocess(output_tensor):\n    Normalize = transforms.Compose([transforms.Normalize(mean=[-103.939, -116.779, -123.68], std=[1,1,1])])\n    bgr2rgb = transforms.Compose([transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])])])\n    output_tensor = bgr2rgb(Normalize(output_tensor.squeeze(0).cpu())) / 256\n    output_tensor.clamp_(0, 1)\n    Image2PIL = transforms.ToPILImage()\n    image = Image2PIL(output_tensor.cpu())\n    return image\n\n\n# Combine the Y channel of the generated image and the UV/CbCr channels of the\n# content image to perform color-independent style transfer.\ndef original_colors(content, generated):\n    content_channels = list(content.convert(\'YCbCr\').split())\n    generated_channels = list(generated.convert(\'YCbCr\').split())\n    content_channels[0] = generated_channels[0]\n    return Image.merge(\'YCbCr\', content_channels).convert(\'RGB\')\n\n\n# Print like Lua/Torch7\ndef print_torch(net, multidevice):\n    if multidevice:\n        return\n    simplelist = """"\n    for i, layer in enumerate(net, 1):\n        simplelist = simplelist + ""("" + str(i) + "") -> ""\n    print(""nn.Sequential ( \\n  [input -> "" + simplelist + ""output]"")\n\n    def strip(x):\n        return str(x).replace("", "",\',\').replace(""("",\'\').replace("")"",\'\') + "", ""\n    def n():\n        return ""  ("" + str(i) + ""): "" + ""nn."" + str(l).split(""("", 1)[0]\n\n    for i, l in enumerate(net, 1):\n         if ""2d"" in str(l):\n             ks, st, pd = strip(l.kernel_size), strip(l.stride), strip(l.padding)\n             if ""Conv2d"" in str(l):\n                 ch = str(l.in_channels) + "" -> "" + str(l.out_channels)\n                 print(n() + ""("" + ch + "", "" + (ks).replace("","",\'x\', 1) + st + pd.replace("", "",\')\'))\n             elif ""Pool2d"" in str(l):\n                 st = st.replace(""  "",\' \') + st.replace("", "",\')\')\n                 print(n() + ""("" + ((ks).replace("","",\'x\' + ks, 1) + st).replace("", "",\',\'))\n         else:\n             print(n())\n    print("")"")\n\n\n# Divide weights by channel size\ndef normalize_weights(content_losses, style_losses):\n    for n, i in enumerate(content_losses):\n        i.strength = i.strength / max(i.target.size())\n    for n, i in enumerate(style_losses):\n        i.strength = i.strength / max(i.target.size())\n\n\n# Define an nn Module to compute content loss\nclass ContentLoss(nn.Module):\n\n    def __init__(self, strength):\n        super(ContentLoss, self).__init__()\n        self.strength = strength\n        self.crit = nn.MSELoss()\n        self.mode = \'None\'\n\n    def forward(self, input):\n        if self.mode == \'loss\':\n            self.loss = self.crit(input, self.target) * self.strength\n        elif self.mode == \'capture\':\n            self.target = input.detach()\n        return input\n\n\nclass GramMatrix(nn.Module):\n\n    def forward(self, input):\n        B, C, H, W = input.size()\n        x_flat = input.view(C, H * W)\n        return torch.mm(x_flat, x_flat.t())\n\n\n# Define an nn Module to compute style loss\nclass StyleLoss(nn.Module):\n\n    def __init__(self, strength):\n        super(StyleLoss, self).__init__()\n        self.target = torch.Tensor()\n        self.strength = strength\n        self.gram = GramMatrix()\n        self.crit = nn.MSELoss()\n        self.mode = \'None\'\n        self.blend_weight = None\n\n    def forward(self, input):\n        self.G = self.gram(input)\n        self.G = self.G.div(input.nelement())\n        if self.mode == \'capture\':\n            if self.blend_weight == None:\n                self.target = self.G.detach()\n            elif self.target.nelement() == 0:\n                self.target = self.G.detach().mul(self.blend_weight)\n            else:\n                self.target = self.target.add(self.blend_weight, self.G.detach())\n        elif self.mode == \'loss\':\n            self.loss = self.strength * self.crit(self.G, self.target)\n        return input\n\n\nclass TVLoss(nn.Module):\n\n    def __init__(self, strength):\n        super(TVLoss, self).__init__()\n        self.strength = strength\n\n    def forward(self, input):\n        self.x_diff = input[:,:,1:,:] - input[:,:,:-1,:]\n        self.y_diff = input[:,:,:,1:] - input[:,:,:,:-1]\n        self.loss = self.strength * (torch.sum(torch.abs(self.x_diff)) + torch.sum(torch.abs(self.y_diff)))\n        return input\n\n\nif __name__ == ""__main__"":\n    main()\n'"
models/download_models.py,3,"b'import torch\nfrom os import path\nfrom sys import version_info\nfrom collections import OrderedDict\nfrom torch.utils.model_zoo import load_url\n\n\n# Download the VGG-19 model and fix the layer names\nprint(""Downloading the VGG-19 model"")\nsd = load_url(""https://web.eecs.umich.edu/~justincj/models/vgg19-d01eb7cb.pth"")\nmap = {\'classifier.1.weight\':u\'classifier.0.weight\', \'classifier.1.bias\':u\'classifier.0.bias\', \'classifier.4.weight\':u\'classifier.3.weight\', \'classifier.4.bias\':u\'classifier.3.bias\'}\nsd = OrderedDict([(map[k] if k in map else k,v) for k,v in sd.items()])\ntorch.save(sd, path.join(""models"", ""vgg19-d01eb7cb.pth""))\n\n# Download the VGG-16 model and fix the layer names\nprint(""Downloading the VGG-16 model"")\nsd = load_url(""https://web.eecs.umich.edu/~justincj/models/vgg16-00b39a1b.pth"")\nmap = {\'classifier.1.weight\':u\'classifier.0.weight\', \'classifier.1.bias\':u\'classifier.0.bias\', \'classifier.4.weight\':u\'classifier.3.weight\', \'classifier.4.bias\':u\'classifier.3.bias\'}\nsd = OrderedDict([(map[k] if k in map else k,v) for k,v in sd.items()])\ntorch.save(sd, path.join(""models"", ""vgg16-00b39a1b.pth""))\n\n# Download the NIN model\nprint(""Downloading the NIN model"")\nif version_info[0] < 3:\n    import urllib\n    urllib.URLopener().retrieve(""https://raw.githubusercontent.com/ProGamerGov/pytorch-nin/master/nin_imagenet.pth"", path.join(""models"", ""nin_imagenet.pth""))\nelse: \n    import urllib.request\n    urllib.request.urlretrieve(""https://raw.githubusercontent.com/ProGamerGov/pytorch-nin/master/nin_imagenet.pth"", path.join(""models"", ""nin_imagenet.pth""))\n\nprint(""All models have been successfully downloaded"")\n'"
