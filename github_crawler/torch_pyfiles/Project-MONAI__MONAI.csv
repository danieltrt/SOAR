file_path,api_count,code
setup.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom setuptools import find_packages, setup\n\nimport versioneer\n\nif __name__ == ""__main__"":\n    setup(\n        version=versioneer.get_version(),\n        cmdclass=versioneer.get_cmdclass(),\n        packages=find_packages(exclude=(""docs"", ""examples"", ""tests"")),\n        zip_safe=True,\n    )\n'"
versioneer.py,0,"b'# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other languages) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\n\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\n            ""Versioneer was unable to run the project root directory. ""\n            ""Versioneer requires setup.py to be executed from ""\n            ""its immediate directory (like \'python setup.py COMMAND\'), ""\n            ""or in a way that lets it use sys.argv[0] to find the root ""\n            ""(like \'python path/to/setup.py COMMAND\').""\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(""Warning: build in %s is using versioneer.py from %s"" % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None)\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\n    ""git""\n] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r""\\d"", r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": date,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n        ""date"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS, [""describe"", ""--tags"", ""--dirty"", ""--always"", ""--long"", ""--match"", ""%s*"" % tag_prefix], cwd=root\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (full_tag, tag_prefix)\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""], cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                ""version"": dirname[len(parentdir_prefix) :],\n                ""full-revisionid"": None,\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" % (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n            ""date"": None,\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n        ""date"": pieces.get(""date""),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n        ""date"": None,\n    }\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if ""py2exe"" in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(target_versionfile, self._versioneer_generated_versions)\n\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                ""DOLLAR"": ""$"",\n                ""STYLE"": cfg.style,\n                ""TAG_PREFIX"": cfg.tag_prefix,\n                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print("" appending versionfile_source (\'%s\') to MANIFEST.in"" % cfg.versionfile_source)\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
monai/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\nfrom ._version import get_versions\nfrom .utils.module import load_submodules\n\n__version__ = get_versions()[""version""]\ndel get_versions\n\n__copyright__ = ""(c) 2020 MONAI Consortium""\n\n__basedir__ = os.path.dirname(__file__)\n\nload_submodules(sys.modules[__name__], False)  # load directory modules only, skip loading individual files\nload_submodules(sys.modules[__name__], True)  # load all modules, this will trigger all export decorations\n'"
monai/_version.py,0,"b'# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    git_date = ""$Format:%ci$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = """"\n    cfg.parentdir_prefix = """"\n    cfg.versionfile_source = ""monai/_version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):  # lgtm[py/unused-loop-variable]\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n'"
tests/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport unittest\nimport warnings\n\n\ndef _enter_pr_4800(self):\n    """"""\n    code from https://github.com/python/cpython/pull/4800\n    """"""\n    # The __warningregistry__\'s need to be in a pristine state for tests\n    # to work properly.\n    for v in list(sys.modules.values()):\n        if getattr(v, ""__warningregistry__"", None):\n            v.__warningregistry__ = {}\n    self.warnings_manager = warnings.catch_warnings(record=True)\n    self.warnings = self.warnings_manager.__enter__()\n    warnings.simplefilter(""always"", self.expected)\n    return self\n\n\n# workaround for https://bugs.python.org/issue29620\ntry:\n    # Suppression for issue #494:  tests/__init__.py:34: error: Cannot assign to a method\n    unittest.case._AssertWarnsContext.__enter__ = _enter_pr_4800  # type: ignore\nexcept AttributeError:\n    pass\n'"
tests/test_activations.py,8,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom parameterized import parameterized\nfrom monai.transforms import Activations\n\nTEST_CASE_1 = [\n    {""sigmoid"": True, ""softmax"": False, ""other"": None},\n    torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]]),\n    torch.tensor([[[[0.5000, 0.7311], [0.8808, 0.9526]]]]),\n    (1, 1, 2, 2),\n]\n\nTEST_CASE_2 = [\n    {""sigmoid"": False, ""softmax"": True, ""other"": None},\n    torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]]),\n    torch.tensor([[[[0.1192, 0.1192]], [[0.8808, 0.8808]]]]),\n    (1, 2, 1, 2),\n]\n\nTEST_CASE_3 = [\n    {""sigmoid"": False, ""softmax"": False, ""other"": lambda x: torch.tanh(x)},\n    torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]]),\n    torch.tensor([[[[0.0000, 0.7616], [0.9640, 0.9951]]]]),\n    (1, 1, 2, 2),\n]\n\n\nclass TestActivations(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_value_shape(self, input_param, img, out, expected_shape):\n        result = Activations(**input_param)(img)\n        torch.testing.assert_allclose(result, out)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_activationsd.py,16,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom parameterized import parameterized\nfrom monai.transforms import Activationsd\n\nTEST_CASE_1 = [\n    {""keys"": [""pred"", ""label""], ""output_postfix"": ""act"", ""sigmoid"": False, ""softmax"": [True, False], ""other"": None},\n    {""pred"": torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]]), ""label"": torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]])},\n    {\n        ""pred_act"": torch.tensor([[[[0.1192, 0.1192]], [[0.8808, 0.8808]]]]),\n        ""label_act"": torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]]),\n    },\n    (1, 2, 1, 2),\n]\n\nTEST_CASE_2 = [\n    {\n        ""keys"": [""pred"", ""label""],\n        ""output_postfix"": ""act"",\n        ""sigmoid"": False,\n        ""softmax"": False,\n        ""other"": [lambda x: torch.tanh(x), None],\n    },\n    {""pred"": torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]]), ""label"": torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]])},\n    {\n        ""pred_act"": torch.tensor([[[[0.0000, 0.7616], [0.9640, 0.9951]]]]),\n        ""label_act"": torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]]),\n    },\n    (1, 1, 2, 2),\n]\n\nTEST_CASE_3 = [\n    {""keys"": ""pred"", ""output_postfix"": ""act"", ""sigmoid"": False, ""softmax"": False, ""other"": lambda x: torch.tanh(x)},\n    {""pred"": torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]])},\n    {""pred_act"": torch.tensor([[[[0.0000, 0.7616], [0.9640, 0.9951]]]])},\n    (1, 1, 2, 2),\n]\n\nTEST_CASE_4 = [\n    {""keys"": ""pred"", ""output_postfix"": None, ""sigmoid"": False, ""softmax"": False, ""other"": lambda x: torch.tanh(x)},\n    {""pred"": torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]])},\n    {""pred"": torch.tensor([[[[0.0000, 0.7616], [0.9640, 0.9951]]]])},\n    (1, 1, 2, 2),\n]\n\n\nclass TestActivationsd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_value_shape(self, input_param, test_input, output, expected_shape):\n        result = Activationsd(**input_param)(test_input)\n        torch.testing.assert_allclose(result[""pred_act""], output[""pred_act""])\n        self.assertTupleEqual(result[""pred_act""].shape, expected_shape)\n        if ""label_act"" in result:\n            torch.testing.assert_allclose(result[""label_act""], output[""label_act""])\n            self.assertTupleEqual(result[""label_act""].shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_4])\n    def test_none_postfix(self, input_param, test_input, output, expected_shape):\n        result = Activationsd(**input_param)(test_input)\n        torch.testing.assert_allclose(result[""pred""], output[""pred""])\n        self.assertTupleEqual(result[""pred""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_adaptors.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport itertools\n\n\nfrom monai.transforms.adaptors import adaptor, apply_alias, to_kwargs, FunctionSignature\n\n\nclass TestAdaptors(unittest.TestCase):\n    def test_function_signature(self):\n        def foo(image, label=None, *a, **kw):\n            pass\n\n        f = FunctionSignature(foo)\n\n    def test_single_in_single_out(self):\n        def foo(image):\n            return image * 2\n\n        it = itertools.product([""image"", [""image""]], [None, ""image"", [""image""], {""image"": ""image""}])\n        for i in it:\n            d = {""image"": 2}\n            dres = adaptor(foo, i[0], i[1])(d)\n            self.assertEqual(dres[""image""], 4)\n\n        d = {""image"": 2}\n        dres = adaptor(foo, ""image"")(d)\n        self.assertEqual(dres[""image""], 4)\n\n        d = {""image"": 2}\n        dres = adaptor(foo, ""image"", ""image"")(d)\n        self.assertEqual(dres[""image""], 4)\n\n        d = {""image"": 2}\n        dres = adaptor(foo, ""image"", {""image"": ""image""})(d)\n        self.assertEqual(dres[""image""], 4)\n\n        d = {""img"": 2}\n        dres = adaptor(foo, ""img"", {""img"": ""image""})(d)\n        self.assertEqual(dres[""img""], 4)\n\n        d = {""img"": 2}\n        dres = adaptor(foo, [""img""], {""img"": ""image""})(d)\n        self.assertEqual(dres[""img""], 4)\n\n    def test_multi_in_single_out(self):\n        def foo(image, label):\n            return image * label\n\n        it = itertools.product([""image"", [""image""]], [None, [""image"", ""label""], {""image"": ""image"", ""label"": ""label""}])\n\n        for i in it:\n            d = {""image"": 2, ""label"": 3}\n            dres = adaptor(foo, i[0], i[1])(d)\n            self.assertEqual(dres[""image""], 6)\n            self.assertEqual(dres[""label""], 3)\n\n        it = itertools.product(\n            [""newimage"", [""newimage""]], [None, [""image"", ""label""], {""image"": ""image"", ""label"": ""label""}]\n        )\n\n        for i in it:\n            d = {""image"": 2, ""label"": 3}\n            dres = adaptor(foo, i[0], i[1])(d)\n            self.assertEqual(dres[""image""], 2)\n            self.assertEqual(dres[""label""], 3)\n            self.assertEqual(dres[""newimage""], 6)\n\n        it = itertools.product([""img"", [""img""]], [{""img"": ""image"", ""lbl"": ""label""}])\n\n        for i in it:\n            d = {""img"": 2, ""lbl"": 3}\n            dres = adaptor(foo, i[0], i[1])(d)\n            self.assertEqual(dres[""img""], 6)\n            self.assertEqual(dres[""lbl""], 3)\n\n    def test_default_arg_single_out(self):\n        def foo(a, b=2):\n            return a * b\n\n        d = {""a"": 5}\n        dres = adaptor(foo, ""c"")(d)\n        self.assertEqual(dres[""c""], 10)\n\n        d = {""b"": 5}\n        with self.assertRaises(TypeError):\n            dres = adaptor(foo, ""c"")(d)\n\n    def test_multi_out(self):\n        def foo(a, b):\n            return a * b, a / b\n\n        d = {""a"": 3, ""b"": 4}\n        dres = adaptor(foo, [""c"", ""d""])(d)\n        self.assertEqual(dres[""c""], 12)\n        self.assertEqual(dres[""d""], 3 / 4)\n\n    def test_dict_out(self):\n        def foo(a):\n            return {""a"": a * 2}\n\n        d = {""a"": 2}\n        dres = adaptor(foo, {""a"": ""a""})(d)\n        self.assertEqual(dres[""a""], 4)\n\n        d = {""b"": 2}\n        dres = adaptor(foo, {""a"": ""b""}, {""b"": ""a""})(d)\n        self.assertEqual(dres[""b""], 4)\n\n\nclass TestApplyAlias(unittest.TestCase):\n    def test_apply_alias(self):\n        def foo(d):\n            d[""x""] *= 2\n            return d\n\n        d = {""a"": 1, ""b"": 3}\n        result = apply_alias(foo, {""b"": ""x""})(d)\n        self.assertDictEqual({""a"": 1, ""b"": 6}, result)\n\n\nclass TestToKwargs(unittest.TestCase):\n    def test_to_kwargs(self):\n        def foo(**kwargs):\n            results = {k: v * 2 for k, v in kwargs.items()}\n            return results\n\n        def compose_like(fn, data):\n            data = fn(data)\n            return data\n\n        d = {""a"": 1, ""b"": 2}\n\n        actual = compose_like(to_kwargs(foo), d)\n        self.assertDictEqual(actual, {""a"": 2, ""b"": 4})\n\n        with self.assertRaises(TypeError):\n            actual = compose_like(foo, d)\n'"
tests/test_add_channeld.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import AddChanneld\n\nTEST_CASE_1 = [\n    {""keys"": [""img"", ""seg""]},\n    {""img"": np.array([[0, 1], [1, 2]]), ""seg"": np.array([[0, 1], [1, 2]])},\n    (1, 2, 2),\n]\n\n\nclass TestAddChanneld(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = AddChanneld(**input_param)(input_data)\n        self.assertEqual(result[""img""].shape, expected_shape)\n        self.assertEqual(result[""seg""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_adjust_contrast.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import AdjustContrast\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASE_1 = [1.0]\n\nTEST_CASE_2 = [0.5]\n\nTEST_CASE_3 = [4.5]\n\n\nclass TestAdjustContrast(NumpyImageTestCase2D):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_correct_results(self, gamma):\n        adjuster = AdjustContrast(gamma=gamma)\n        result = adjuster(self.imt)\n        if gamma == 1.0:\n            expected = self.imt\n        else:\n            epsilon = 1e-7\n            img_min = self.imt.min()\n            img_range = self.imt.max() - img_min\n            expected = np.power(((self.imt - img_min) / float(img_range + epsilon)), gamma) * img_range + img_min\n        np.testing.assert_allclose(expected, result, rtol=1e-05)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_adjust_contrastd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import AdjustContrastd\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASE_1 = [1.0]\n\nTEST_CASE_2 = [0.5]\n\nTEST_CASE_3 = [4.5]\n\n\nclass TestAdjustContrastd(NumpyImageTestCase2D):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_correct_results(self, gamma):\n        adjuster = AdjustContrastd(""img"", gamma=gamma)\n        result = adjuster({""img"": self.imt})\n        if gamma == 1.0:\n            expected = self.imt\n        else:\n            epsilon = 1e-7\n            img_min = self.imt.min()\n            img_range = self.imt.max() - img_min\n            expected = np.power(((self.imt - img_min) / float(img_range + epsilon)), gamma) * img_range + img_min\n        np.testing.assert_allclose(expected, result[""img""], rtol=1e-05)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_affine.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import Affine\n\nTEST_CASES = [\n    [\n        dict(padding_mode=""zeros"", as_tensor_output=False, device=None),\n        {""img"": np.arange(4).reshape((1, 2, 2)), ""spatial_size"": (4, 4)},\n        np.array([[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.25, 0.0], [0.0, 0.5, 0.75, 0.0], [0.0, 0.0, 0.0, 0.0]]]),\n    ],\n    [\n        dict(rotate_params=[np.pi / 2], padding_mode=""zeros"", as_tensor_output=False, device=None),\n        {""img"": np.arange(4).reshape((1, 2, 2)), ""spatial_size"": (4, 4)},\n        np.array([[[0.0, 0.0, 0.0, 0.0], [0.0, 0.5, 0.0, 0.0], [0.0, 0.75, 0.25, 0.0], [0.0, 0.0, 0.0, 0.0]]]),\n    ],\n    [\n        dict(padding_mode=""zeros"", as_tensor_output=False, device=None),\n        {""img"": np.arange(8).reshape((1, 2, 2, 2)), ""spatial_size"": (4, 4, 4)},\n        np.array(\n            [\n                [\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.125, 0.0], [0.0, 0.25, 0.375, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.5, 0.625, 0.0], [0.0, 0.75, 0.875, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                ]\n            ]\n        ),\n    ],\n    [\n        dict(rotate_params=[np.pi / 2], padding_mode=""zeros"", as_tensor_output=False, device=None),\n        {""img"": np.arange(8).reshape((1, 2, 2, 2)), ""spatial_size"": (4, 4, 4)},\n        np.array(\n            [\n                [\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.25, 0.0, 0.0], [0.0, 0.375, 0.125, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.75, 0.5, 0.0], [0.0, 0.875, 0.625, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                ]\n            ]\n        ),\n    ],\n]\n\n\nclass TestAffine(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_affine(self, input_param, input_data, expected_val):\n        g = Affine(**input_param)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_affine_grid.py,15,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import AffineGrid\n\nTEST_CASES = [\n    [\n        {""as_tensor_output"": False, ""device"": torch.device(""cpu:0"")},\n        {""spatial_size"": (2, 2)},\n        np.array([[[-0.5, -0.5], [0.5, 0.5]], [[-0.5, 0.5], [-0.5, 0.5]], [[1.0, 1.0], [1.0, 1.0]]]),\n    ],\n    [\n        {""as_tensor_output"": True, ""device"": None},\n        {""spatial_size"": (2, 2)},\n        torch.tensor([[[-0.5, -0.5], [0.5, 0.5]], [[-0.5, 0.5], [-0.5, 0.5]], [[1.0, 1.0], [1.0, 1.0]]]),\n    ],\n    [{""as_tensor_output"": False, ""device"": None}, {""grid"": np.ones((3, 3, 3))}, np.ones((3, 3, 3))],\n    [{""as_tensor_output"": True, ""device"": torch.device(""cpu:0"")}, {""grid"": np.ones((3, 3, 3))}, torch.ones((3, 3, 3))],\n    [{""as_tensor_output"": False, ""device"": None}, {""grid"": torch.ones((3, 3, 3))}, np.ones((3, 3, 3))],\n    [\n        {""as_tensor_output"": True, ""device"": torch.device(""cpu:0"")},\n        {""grid"": torch.ones((3, 3, 3))},\n        torch.ones((3, 3, 3)),\n    ],\n    [\n        {\n            ""rotate_params"": (1.0, 1.0),\n            ""scale_params"": (-20, 10),\n            ""as_tensor_output"": True,\n            ""device"": torch.device(""cpu:0""),\n        },\n        {""grid"": torch.ones((3, 3, 3))},\n        torch.tensor(\n            [\n                [[-19.2208, -19.2208, -19.2208], [-19.2208, -19.2208, -19.2208], [-19.2208, -19.2208, -19.2208]],\n                [[-11.4264, -11.4264, -11.4264], [-11.4264, -11.4264, -11.4264], [-11.4264, -11.4264, -11.4264]],\n                [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],\n            ]\n        ),\n    ],\n    [\n        {\n            ""rotate_params"": (1.0, 1.0, 1.0),\n            ""scale_params"": (-20, 10),\n            ""as_tensor_output"": True,\n            ""device"": torch.device(""cpu:0""),\n        },\n        {""grid"": torch.ones((4, 3, 3, 3))},\n        torch.tensor(\n            [\n                [\n                    [[-9.5435, -9.5435, -9.5435], [-9.5435, -9.5435, -9.5435], [-9.5435, -9.5435, -9.5435]],\n                    [[-9.5435, -9.5435, -9.5435], [-9.5435, -9.5435, -9.5435], [-9.5435, -9.5435, -9.5435]],\n                    [[-9.5435, -9.5435, -9.5435], [-9.5435, -9.5435, -9.5435], [-9.5435, -9.5435, -9.5435]],\n                ],\n                [\n                    [[-20.2381, -20.2381, -20.2381], [-20.2381, -20.2381, -20.2381], [-20.2381, -20.2381, -20.2381]],\n                    [[-20.2381, -20.2381, -20.2381], [-20.2381, -20.2381, -20.2381], [-20.2381, -20.2381, -20.2381]],\n                    [[-20.2381, -20.2381, -20.2381], [-20.2381, -20.2381, -20.2381], [-20.2381, -20.2381, -20.2381]],\n                ],\n                [\n                    [[-0.5844, -0.5844, -0.5844], [-0.5844, -0.5844, -0.5844], [-0.5844, -0.5844, -0.5844]],\n                    [[-0.5844, -0.5844, -0.5844], [-0.5844, -0.5844, -0.5844], [-0.5844, -0.5844, -0.5844]],\n                    [[-0.5844, -0.5844, -0.5844], [-0.5844, -0.5844, -0.5844], [-0.5844, -0.5844, -0.5844]],\n                ],\n                [\n                    [[1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000]],\n                    [[1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000]],\n                    [[1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000]],\n                ],\n            ]\n        ),\n    ],\n]\n\n\nclass TestAffineGrid(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_affine_grid(self, input_param, input_data, expected_val):\n        g = AffineGrid(**input_param)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_affine_transform.py,57,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.networks.layers import AffineTransform\nfrom monai.networks.utils import normalize_transform, to_norm_affine\n\nTEST_NORM_CASES = [\n    [(4, 5), True, [[[0.666667, 0, -1], [0, 0.5, -1], [0, 0, 1]]]],\n    [\n        (2, 4, 5),\n        True,\n        [[[2.0, 0.0, 0.0, -1.0], [0.0, 0.6666667, 0.0, -1.0], [0.0, 0.0, 0.5, -1.0], [0.0, 0.0, 0.0, 1.0]]],\n    ],\n    [(4, 5), False, [[[0.5, 0.0, -0.75], [0.0, 0.4, -0.8], [0.0, 0.0, 1.0]]]],\n    [(2, 4, 5), False, [[[1.0, 0.0, 0.0, -0.5], [0.0, 0.5, 0.0, -0.75], [0.0, 0.0, 0.4, -0.8], [0.0, 0.0, 0.0, 1.0]]]],\n]\n\nTEST_TO_NORM_AFFINE_CASES = [\n    [\n        [[[1, 0, 0], [0, 1, 0], [0, 0, 1]]],\n        (4, 6),\n        (5, 3),\n        True,\n        [[[1.3333334, 0.0, 0.33333337], [0.0, 0.4, -0.6], [0.0, 0.0, 1.0]]],\n    ],\n    [\n        [[[1, 0, 0], [0, 1, 0], [0, 0, 1]]],\n        (4, 6),\n        (5, 3),\n        False,\n        [[[1.25, 0.0, 0.25], [0.0, 0.5, -0.5], [0.0, 0.0, 1.0]]],\n    ],\n    [\n        [[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]],\n        (2, 4, 6),\n        (3, 5, 3),\n        True,\n        [[[2.0, 0.0, 0.0, 1.0], [0.0, 1.3333334, 0.0, 0.33333337], [0.0, 0.0, 0.4, -0.6], [0.0, 0.0, 0.0, 1.0]]],\n    ],\n    [\n        [[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]],\n        (2, 4, 6),\n        (3, 5, 3),\n        False,\n        [[[1.5, 0.0, 0.0, 0.5], [0.0, 1.25, 0.0, 0.25], [0.0, 0.0, 0.5, -0.5], [0.0, 0.0, 0.0, 1.0]]],\n    ],\n]\n\nTEST_ILL_TO_NORM_AFFINE_CASES = [\n    [[[[1, 0, 0], [0, 1, 0], [0, 0, 1]]], (3, 4, 6), (3, 5, 3), False],\n    [[[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]], (4, 6), (3, 5, 3), True],\n    [[[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]]], (4, 6), (3, 5, 3), True],\n]\n\n\nclass TestNormTransform(unittest.TestCase):\n    @parameterized.expand(TEST_NORM_CASES)\n    def test_norm_xform(self, input_shape, align_corners, expected):\n        norm = normalize_transform(\n            input_shape, device=torch.device(""cpu:0""), dtype=torch.float32, align_corners=align_corners\n        )\n        norm = norm.detach().cpu().numpy()\n        np.testing.assert_allclose(norm, expected, atol=1e-6)\n        if torch.cuda.is_available():\n            norm = normalize_transform(\n                input_shape, device=torch.device(""cuda:0""), dtype=torch.float32, align_corners=align_corners\n            )\n            norm = norm.detach().cpu().numpy()\n            np.testing.assert_allclose(norm, expected, atol=1e-4)\n\n\nclass TestToNormAffine(unittest.TestCase):\n    @parameterized.expand(TEST_TO_NORM_AFFINE_CASES)\n    def test_to_norm_affine(self, affine, src_size, dst_size, align_corners, expected):\n        affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n        new_affine = to_norm_affine(affine, src_size, dst_size, align_corners)\n        new_affine = new_affine.detach().cpu().numpy()\n        np.testing.assert_allclose(new_affine, expected, atol=1e-6)\n\n        if torch.cuda.is_available():\n            affine = torch.as_tensor(affine, device=torch.device(""cuda:0""), dtype=torch.float32)\n            new_affine = to_norm_affine(affine, src_size, dst_size, align_corners)\n            new_affine = new_affine.detach().cpu().numpy()\n            np.testing.assert_allclose(new_affine, expected, atol=1e-4)\n\n    @parameterized.expand(TEST_ILL_TO_NORM_AFFINE_CASES)\n    def test_to_norm_affine_ill(self, affine, src_size, dst_size, align_corners):\n        with self.assertRaises(ValueError):\n            to_norm_affine(affine, src_size, dst_size, align_corners)\n        with self.assertRaises(ValueError):\n            affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n            to_norm_affine(affine, src_size, dst_size, align_corners)\n\n\nclass TestAffineTransform(unittest.TestCase):\n    def test_affine_shift(self):\n        affine = torch.as_tensor([[1.0, 0.0, 0.0], [0.0, 1.0, -1.0]])\n        image = torch.as_tensor([[[[4.0, 1.0, 3.0, 2.0], [7.0, 6.0, 8.0, 5.0], [3.0, 5.0, 3.0, 6.0]]]])\n        out = AffineTransform()(image, affine)\n        out = out.detach().cpu().numpy()\n        expected = [[[[0, 4, 1, 3], [0, 7, 6, 8], [0, 3, 5, 3]]]]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n    def test_affine_shift_1(self):\n        affine = torch.as_tensor([[1.0, 0.0, -1.0], [0.0, 1.0, -1.0]])\n        image = torch.as_tensor([[[[4.0, 1.0, 3.0, 2.0], [7.0, 6.0, 8.0, 5.0], [3.0, 5.0, 3.0, 6.0]]]])\n        out = AffineTransform()(image, affine)\n        out = out.detach().cpu().numpy()\n        expected = [[[[0, 0, 0, 0], [0, 4, 1, 3], [0, 7, 6, 8]]]]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n    def test_affine_shift_2(self):\n        affine = torch.as_tensor([[1.0, 0.0, -1.0], [0.0, 1.0, 0.0]])\n        image = torch.as_tensor([[[[4.0, 1.0, 3.0, 2.0], [7.0, 6.0, 8.0, 5.0], [3.0, 5.0, 3.0, 6.0]]]])\n        out = AffineTransform()(image, affine)\n        out = out.detach().cpu().numpy()\n        expected = [[[[0, 0, 0, 0], [4, 1, 3, 2], [7, 6, 8, 5]]]]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n    def test_zoom(self):\n        affine = torch.as_tensor([[1.0, 0.0, 0.0], [0.0, 2.0, 0.0]])\n        image = torch.arange(1.0, 13.0).view(1, 1, 3, 4).to(device=torch.device(""cpu:0""))\n        out = AffineTransform((3, 2))(image, affine)\n        expected = [[[[1, 3], [5, 7], [9, 11]]]]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n    def test_zoom_1(self):\n        affine = torch.as_tensor([[2.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n        image = torch.arange(1.0, 13.0).view(1, 1, 3, 4).to(device=torch.device(""cpu:0""))\n        out = AffineTransform()(image, affine, (1, 4))\n        expected = [[[[1, 2, 3, 4]]]]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n    def test_zoom_2(self):\n        affine = torch.as_tensor([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]], dtype=torch.float32)\n        image = torch.arange(1.0, 13.0).view(1, 1, 3, 4).to(device=torch.device(""cpu:0""))\n        out = AffineTransform((1, 2))(image, affine)\n        expected = [[[[1, 3]]]]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n    def test_affine_transform_minimum(self):\n        t = np.pi / 3\n        affine = [[np.cos(t), -np.sin(t), 0], [np.sin(t), np.cos(t), 0], [0, 0, 1]]\n        affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n        image = torch.arange(24.0).view(1, 1, 4, 6).to(device=torch.device(""cpu:0""))\n        out = AffineTransform()(image, affine)\n        out = out.detach().cpu().numpy()\n        expected = [\n            [\n                [\n                    [0.0, 0.06698727, 0.0, 0.0, 0.0, 0.0],\n                    [3.8660254, 0.86602557, 0.0, 0.0, 0.0, 0.0],\n                    [7.732051, 3.035899, 0.73205125, 0.0, 0.0, 0.0],\n                    [11.598076, 6.901923, 2.7631402, 0.0, 0.0, 0.0],\n                ]\n            ]\n        ]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n    def test_affine_transform_2d(self):\n        t = np.pi / 3\n        affine = [[np.cos(t), -np.sin(t), 0], [np.sin(t), np.cos(t), 0], [0, 0, 1]]\n        affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n        image = torch.arange(24.0).view(1, 1, 4, 6).to(device=torch.device(""cpu:0""))\n        xform = AffineTransform((3, 4), padding_mode=""border"", align_corners=True, mode=""bilinear"")\n        out = xform(image, affine)\n        out = out.detach().cpu().numpy()\n        expected = [\n            [\n                [\n                    [7.1525574e-07, 4.9999994e-01, 1.0000000e00, 1.4999999e00],\n                    [3.8660259e00, 1.3660253e00, 1.8660252e00, 2.3660252e00],\n                    [7.7320518e00, 3.0358994e00, 2.7320509e00, 3.2320507e00],\n                ]\n            ]\n        ]\n        np.testing.assert_allclose(out, expected, atol=1e-5)\n\n        if torch.cuda.is_available():\n            affine = torch.as_tensor(affine, device=torch.device(""cuda:0""), dtype=torch.float32)\n            image = torch.arange(24.0).view(1, 1, 4, 6).to(device=torch.device(""cuda:0""))\n            xform = AffineTransform(padding_mode=""border"", align_corners=True, mode=""bilinear"")\n            out = xform(image, affine, (3, 4))\n            out = out.detach().cpu().numpy()\n            expected = [\n                [\n                    [\n                        [7.1525574e-07, 4.9999994e-01, 1.0000000e00, 1.4999999e00],\n                        [3.8660259e00, 1.3660253e00, 1.8660252e00, 2.3660252e00],\n                        [7.7320518e00, 3.0358994e00, 2.7320509e00, 3.2320507e00],\n                    ]\n                ]\n            ]\n            np.testing.assert_allclose(out, expected, atol=1e-4)\n\n    def test_affine_transform_3d(self):\n        t = np.pi / 3\n        affine = [[1, 0, 0, 0], [0.0, np.cos(t), -np.sin(t), 0], [0, np.sin(t), np.cos(t), 0], [0, 0, 0, 1]]\n        affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n        image = torch.arange(48.0).view(2, 1, 4, 2, 3).to(device=torch.device(""cpu:0""))\n        xform = AffineTransform((3, 4, 2), padding_mode=""border"", align_corners=False, mode=""bilinear"")\n        out = xform(image, affine)\n        out = out.detach().cpu().numpy()\n        expected = [\n            [\n                [\n                    [[0.00000006, 0.5000001], [2.3660254, 1.3660254], [4.732051, 2.4019241], [5.0, 3.9019237]],\n                    [[6.0, 6.5], [8.366026, 7.3660254], [10.732051, 8.401924], [11.0, 9.901924]],\n                    [[12.0, 12.5], [14.366026, 13.366025], [16.732052, 14.401924], [17.0, 15.901923]],\n                ]\n            ],\n            [\n                [\n                    [[24.0, 24.5], [26.366024, 25.366024], [28.732052, 26.401924], [29.0, 27.901924]],\n                    [[30.0, 30.5], [32.366028, 31.366026], [34.732048, 32.401924], [35.0, 33.901924]],\n                    [[36.0, 36.5], [38.366024, 37.366024], [40.73205, 38.401924], [41.0, 39.901924]],\n                ]\n            ],\n        ]\n        np.testing.assert_allclose(out, expected, atol=1e-4)\n\n        if torch.cuda.is_available():\n            affine = torch.as_tensor(affine, device=torch.device(""cuda:0""), dtype=torch.float32)\n            image = torch.arange(48.0).view(2, 1, 4, 2, 3).to(device=torch.device(""cuda:0""))\n            xform = AffineTransform(padding_mode=""border"", align_corners=False, mode=""bilinear"")\n            out = xform(image, affine, (3, 4, 2))\n            out = out.detach().cpu().numpy()\n            expected = [\n                [\n                    [\n                        [[0.00000006, 0.5000001], [2.3660254, 1.3660254], [4.732051, 2.4019241], [5.0, 3.9019237]],\n                        [[6.0, 6.5], [8.366026, 7.3660254], [10.732051, 8.401924], [11.0, 9.901924]],\n                        [[12.0, 12.5], [14.366026, 13.366025], [16.732052, 14.401924], [17.0, 15.901923]],\n                    ]\n                ],\n                [\n                    [\n                        [[24.0, 24.5], [26.366024, 25.366024], [28.732052, 26.401924], [29.0, 27.901924]],\n                        [[30.0, 30.5], [32.366028, 31.366026], [34.732048, 32.401924], [35.0, 33.901924]],\n                        [[36.0, 36.5], [38.366024, 37.366024], [40.73205, 38.401924], [41.0, 39.901924]],\n                    ]\n                ],\n            ]\n            np.testing.assert_allclose(out, expected, atol=1e-4)\n\n    def test_ill_affine_transform(self):\n        with self.assertRaises(ValueError):  # image too small\n            t = np.pi / 3\n            affine = [[1, 0, 0, 0], [0.0, np.cos(t), -np.sin(t), 0], [0, np.sin(t), np.cos(t), 0], [0, 0, 0, 1]]\n            affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n            xform = AffineTransform((3, 4, 2), padding_mode=""border"", align_corners=False, mode=""bilinear"")\n            xform(torch.as_tensor([1.0, 2.0, 3.0]), affine)\n\n        with self.assertRaises(ValueError):  # output shape too small\n            t = np.pi / 3\n            affine = [[1, 0, 0, 0], [0.0, np.cos(t), -np.sin(t), 0], [0, np.sin(t), np.cos(t), 0], [0, 0, 0, 1]]\n            affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n            image = torch.arange(48).view(2, 1, 4, 2, 3).to(device=torch.device(""cpu:0""))\n            xform = AffineTransform((3, 4), padding_mode=""border"", align_corners=False, mode=""bilinear"")\n            xform(image, affine)\n\n        with self.assertRaises(ValueError):  # incorrect affine\n            t = np.pi / 3\n            affine = [[1, 0, 0, 0], [0.0, np.cos(t), -np.sin(t), 0], [0, np.sin(t), np.cos(t), 0], [0, 0, 0, 1]]\n            affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n            affine = affine.unsqueeze(0).unsqueeze(0)\n            image = torch.arange(48).view(2, 1, 4, 2, 3).to(device=torch.device(""cpu:0""))\n            xform = AffineTransform((2, 3, 4), padding_mode=""border"", align_corners=False, mode=""bilinear"")\n            xform(image, affine)\n\n        with self.assertRaises(ValueError):  # batch doesn\'t match\n            t = np.pi / 3\n            affine = [[1, 0, 0, 0], [0.0, np.cos(t), -np.sin(t), 0], [0, np.sin(t), np.cos(t), 0], [0, 0, 0, 1]]\n            affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n            affine = affine.unsqueeze(0)\n            affine = affine.repeat(3, 1, 1)\n            image = torch.arange(48).view(2, 1, 4, 2, 3).to(device=torch.device(""cpu:0""))\n            xform = AffineTransform((2, 3, 4), padding_mode=""border"", align_corners=False, mode=""bilinear"")\n            xform(image, affine)\n\n        with self.assertRaises(RuntimeError):  # input grid dtypes different\n            t = np.pi / 3\n            affine = [[1, 0, 0, 0], [0.0, np.cos(t), -np.sin(t), 0], [0, np.sin(t), np.cos(t), 0], [0, 0, 0, 1]]\n            affine = torch.as_tensor(affine, device=torch.device(""cpu:0""), dtype=torch.float32)\n            affine = affine.unsqueeze(0)\n            affine = affine.repeat(2, 1, 1)\n            image = torch.arange(48).view(2, 1, 4, 2, 3).to(device=torch.device(""cpu:0""), dtype=torch.int32)\n            xform = AffineTransform((2, 3, 4), padding_mode=""border"", mode=""bilinear"", normalized=True)\n            xform(image, affine)\n\n        with self.assertRaises(ValueError):  # wrong affine\n            affine = torch.as_tensor([[1, 0, 0, 0], [0, 0, 0, 1]])\n            image = torch.arange(48).view(2, 1, 4, 2, 3).to(device=torch.device(""cpu:0""))\n            xform = AffineTransform((2, 3, 4), padding_mode=""border"", align_corners=False, mode=""bilinear"")\n            xform(image, affine)\n\n        with self.assertRaises(RuntimeError):  # dtype doesn\'t match\n            affine = torch.as_tensor([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]], dtype=torch.float64)\n            image = torch.arange(1.0, 13.0).view(1, 1, 3, 4).to(device=torch.device(""cpu:0""))\n            out = AffineTransform((1, 2))(image, affine)\n\n    def test_forward_2d(self):\n        x = torch.rand(2, 1, 4, 4)\n        theta = torch.Tensor([[[0, -1, 0], [1, 0, 0]]]).repeat(2, 1, 1)\n        grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)\n        expected = torch.nn.functional.grid_sample(x, grid, align_corners=False)\n        expected = expected.detach().cpu().numpy()\n\n        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)\n        actual = actual.detach().cpu().numpy()\n        np.testing.assert_allclose(actual, expected)\n        np.testing.assert_allclose(list(theta.shape), [2, 2, 3])\n\n        theta = torch.Tensor([[0, -1, 0], [1, 0, 0]])\n        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)\n        actual = actual.detach().cpu().numpy()\n        np.testing.assert_allclose(actual, expected)\n        np.testing.assert_allclose(list(theta.shape), [2, 3])\n\n        theta = torch.Tensor([[[0, -1, 0], [1, 0, 0]]])\n        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)\n        actual = actual.detach().cpu().numpy()\n        np.testing.assert_allclose(actual, expected)\n        np.testing.assert_allclose(list(theta.shape), [1, 2, 3])\n\n    def test_forward_3d(self):\n        x = torch.rand(2, 1, 4, 4, 4)\n        theta = torch.Tensor([[[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]]]).repeat(2, 1, 1)\n        grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)\n        expected = torch.nn.functional.grid_sample(x, grid, align_corners=False)\n        expected = expected.detach().cpu().numpy()\n\n        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)\n        actual = actual.detach().cpu().numpy()\n        np.testing.assert_allclose(actual, expected)\n        np.testing.assert_allclose(list(theta.shape), [2, 3, 4])\n\n        theta = torch.Tensor([[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]])\n        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)\n        actual = actual.detach().cpu().numpy()\n        np.testing.assert_allclose(actual, expected)\n        np.testing.assert_allclose(list(theta.shape), [3, 4])\n\n        theta = torch.Tensor([[[0, 0, -1, 0], [1, 0, 0, 0], [0, 0, 1, 0]]])\n        actual = AffineTransform(normalized=True, reverse_indexing=False)(x, theta)\n        actual = actual.detach().cpu().numpy()\n        np.testing.assert_allclose(actual, expected)\n        np.testing.assert_allclose(list(theta.shape), [1, 3, 4])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_arraydataset.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport nibabel as nib\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.data import ArrayDataset\nfrom monai.transforms import AddChannel, Compose, LoadNifti, RandAdjustContrast, Spacing, RandGaussianNoise\n\nTEST_CASE_1 = [\n    Compose([LoadNifti(image_only=True), AddChannel(), RandGaussianNoise(prob=1.0)]),\n    Compose([LoadNifti(image_only=True), AddChannel(), RandGaussianNoise(prob=1.0)]),\n    (0, 1),\n    (1, 128, 128, 128),\n]\n\nTEST_CASE_2 = [\n    Compose([LoadNifti(image_only=True), AddChannel(), RandAdjustContrast(prob=1.0)]),\n    Compose([LoadNifti(image_only=True), AddChannel(), RandAdjustContrast(prob=1.0)]),\n    (0, 1),\n    (1, 128, 128, 128),\n]\n\n\nclass TestCompose(Compose):\n    def __call__(self, input_):\n        img, metadata = self.transforms[0](input_)\n        img = self.transforms[1](img)\n        img, _, _ = self.transforms[2](img, metadata[""affine""])\n        return self.transforms[3](img), metadata\n\n\nTEST_CASE_3 = [\n    TestCompose([LoadNifti(image_only=False), AddChannel(), Spacing(pixdim=(2, 2, 4)), RandAdjustContrast(prob=1.0)]),\n    TestCompose([LoadNifti(image_only=False), AddChannel(), Spacing(pixdim=(2, 2, 4)), RandAdjustContrast(prob=1.0)]),\n    (0, 2),\n    (1, 64, 64, 33),\n]\n\nTEST_CASE_4 = [\n    Compose([LoadNifti(image_only=True), AddChannel(), RandGaussianNoise(prob=1.0)]),\n    (1, 128, 128, 128),\n]\n\n\nclass TestArrayDataset(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_shape(self, img_transform, label_transform, indexes, expected_shape):\n        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=(128, 128, 128)), np.eye(4))\n        tempdir = tempfile.mkdtemp()\n        test_image1 = os.path.join(tempdir, ""test_image1.nii.gz"")\n        test_seg1 = os.path.join(tempdir, ""test_seg1.nii.gz"")\n        test_image2 = os.path.join(tempdir, ""test_image2.nii.gz"")\n        test_seg2 = os.path.join(tempdir, ""test_seg2.nii.gz"")\n        nib.save(test_image, test_image1)\n        nib.save(test_image, test_seg1)\n        nib.save(test_image, test_image2)\n        nib.save(test_image, test_seg2)\n        test_images = [test_image1, test_image2]\n        test_segs = [test_seg1, test_seg2]\n        test_labels = [1, 1]\n        dataset = ArrayDataset(test_images, img_transform, test_segs, label_transform, test_labels, None)\n        dataset.set_random_state(1234)\n        data1 = dataset[0]\n        data2 = dataset[1]\n\n        self.assertTupleEqual(data1[indexes[0]].shape, expected_shape)\n        self.assertTupleEqual(data1[indexes[1]].shape, expected_shape)\n        np.testing.assert_allclose(data1[indexes[0]], data1[indexes[1]])\n        self.assertTupleEqual(data2[indexes[0]].shape, expected_shape)\n        self.assertTupleEqual(data2[indexes[1]].shape, expected_shape)\n        np.testing.assert_allclose(data2[indexes[0]], data2[indexes[0]])\n\n        dataset = ArrayDataset(test_images, img_transform, test_segs, label_transform, test_labels, None)\n        dataset.set_random_state(1234)\n        _ = dataset[0]\n        data2_new = dataset[1]\n        np.testing.assert_allclose(data2[indexes[0]], data2_new[indexes[0]], atol=1e-3)\n        shutil.rmtree(tempdir)\n\n    @parameterized.expand([TEST_CASE_4])\n    def test_default_none(self, img_transform, expected_shape):\n        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=(128, 128, 128)), np.eye(4))\n        tempdir = tempfile.mkdtemp()\n        test_image1 = os.path.join(tempdir, ""test_image1.nii.gz"")\n        test_image2 = os.path.join(tempdir, ""test_image2.nii.gz"")\n        nib.save(test_image, test_image1)\n        nib.save(test_image, test_image2)\n        test_images = [test_image1, test_image2]\n        dataset = ArrayDataset(test_images, img_transform)\n        dataset.set_random_state(1234)\n        data1 = dataset[0]\n        data2 = dataset[1]\n        self.assertTupleEqual(data1.shape, expected_shape)\n        self.assertTupleEqual(data2.shape, expected_shape)\n\n        dataset = ArrayDataset(test_images, img_transform)\n        dataset.set_random_state(1234)\n        _ = dataset[0]\n        data2_new = dataset[1]\n        np.testing.assert_allclose(data2, data2_new, atol=1e-3)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_as_channel_first.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import AsChannelFirst\n\nTEST_CASE_1 = [{""channel_dim"": -1}, (4, 1, 2, 3)]\n\nTEST_CASE_2 = [{""channel_dim"": 3}, (4, 1, 2, 3)]\n\nTEST_CASE_3 = [{""channel_dim"": 2}, (3, 1, 2, 4)]\n\n\nclass TestAsChannelFirst(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_shape(self, input_param, expected_shape):\n        test_data = np.random.randint(0, 2, size=[1, 2, 3, 4])\n        result = AsChannelFirst(**input_param)(test_data)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_as_channel_firstd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import AsChannelFirstd\n\nTEST_CASE_1 = [{""keys"": [""image"", ""label"", ""extra""], ""channel_dim"": -1}, (4, 1, 2, 3)]\n\nTEST_CASE_2 = [{""keys"": [""image"", ""label"", ""extra""], ""channel_dim"": 3}, (4, 1, 2, 3)]\n\nTEST_CASE_3 = [{""keys"": [""image"", ""label"", ""extra""], ""channel_dim"": 2}, (3, 1, 2, 4)]\n\n\nclass TestAsChannelFirstd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_shape(self, input_param, expected_shape):\n        test_data = {\n            ""image"": np.random.randint(0, 2, size=[1, 2, 3, 4]),\n            ""label"": np.random.randint(0, 2, size=[1, 2, 3, 4]),\n            ""extra"": np.random.randint(0, 2, size=[1, 2, 3, 4]),\n        }\n        result = AsChannelFirstd(**input_param)(test_data)\n        self.assertTupleEqual(result[""image""].shape, expected_shape)\n        self.assertTupleEqual(result[""label""].shape, expected_shape)\n        self.assertTupleEqual(result[""extra""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_as_channel_last.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import AsChannelLast\n\nTEST_CASE_1 = [{""channel_dim"": 0}, (2, 3, 4, 1)]\n\nTEST_CASE_2 = [{""channel_dim"": 1}, (1, 3, 4, 2)]\n\nTEST_CASE_3 = [{""channel_dim"": 3}, (1, 2, 3, 4)]\n\n\nclass TestAsChannelLast(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_shape(self, input_param, expected_shape):\n        test_data = np.random.randint(0, 2, size=[1, 2, 3, 4])\n        result = AsChannelLast(**input_param)(test_data)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_as_channel_lastd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import AsChannelLastd\n\nTEST_CASE_1 = [{""keys"": [""image"", ""label"", ""extra""], ""channel_dim"": 0}, (2, 3, 4, 1)]\n\nTEST_CASE_2 = [{""keys"": [""image"", ""label"", ""extra""], ""channel_dim"": 1}, (1, 3, 4, 2)]\n\nTEST_CASE_3 = [{""keys"": [""image"", ""label"", ""extra""], ""channel_dim"": 3}, (1, 2, 3, 4)]\n\n\nclass TestAsChannelLastd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_shape(self, input_param, expected_shape):\n        test_data = {\n            ""image"": np.random.randint(0, 2, size=[1, 2, 3, 4]),\n            ""label"": np.random.randint(0, 2, size=[1, 2, 3, 4]),\n            ""extra"": np.random.randint(0, 2, size=[1, 2, 3, 4]),\n        }\n        result = AsChannelLastd(**input_param)(test_data)\n        self.assertTupleEqual(result[""image""].shape, expected_shape)\n        self.assertTupleEqual(result[""label""].shape, expected_shape)\n        self.assertTupleEqual(result[""extra""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_as_discrete.py,7,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom parameterized import parameterized\nfrom monai.transforms import AsDiscrete\n\nTEST_CASE_1 = [\n    {""argmax"": True, ""to_onehot"": False, ""n_classes"": None, ""threshold_values"": False, ""logit_thresh"": 0.5},\n    torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]]),\n    torch.tensor([[[[1.0, 1.0]]]]),\n    (1, 1, 1, 2),\n]\n\nTEST_CASE_2 = [\n    {""argmax"": True, ""to_onehot"": True, ""n_classes"": 2, ""threshold_values"": False, ""logit_thresh"": 0.5},\n    torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]]),\n    torch.tensor([[[[0.0, 0.0]], [[1.0, 1.0]]]]),\n    (1, 2, 1, 2),\n]\n\nTEST_CASE_3 = [\n    {""argmax"": False, ""to_onehot"": False, ""n_classes"": None, ""threshold_values"": True, ""logit_thresh"": 0.6},\n    torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]]),\n    torch.tensor([[[[0.0, 1.0], [1.0, 1.0]]]]),\n    (1, 1, 2, 2),\n]\n\n\nclass TestAsDiscrete(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_value_shape(self, input_param, img, out, expected_shape):\n        result = AsDiscrete(**input_param)(img)\n        torch.testing.assert_allclose(result, out)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_as_discreted.py,13,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom parameterized import parameterized\nfrom monai.transforms import AsDiscreted\n\nTEST_CASE_1 = [\n    {\n        ""keys"": [""pred"", ""label""],\n        ""output_postfix"": ""discrete"",\n        ""argmax"": [True, False],\n        ""to_onehot"": True,\n        ""n_classes"": 2,\n        ""threshold_values"": False,\n        ""logit_thresh"": 0.5,\n    },\n    {""pred"": torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]]), ""label"": torch.tensor([[[[0, 1]]]])},\n    {\n        ""pred_discrete"": torch.tensor([[[[0.0, 0.0]], [[1.0, 1.0]]]]),\n        ""label_discrete"": torch.tensor([[[[1.0, 0.0]], [[0.0, 1.0]]]]),\n    },\n    (1, 2, 1, 2),\n]\n\nTEST_CASE_2 = [\n    {\n        ""keys"": [""pred"", ""label""],\n        ""output_postfix"": ""discrete"",\n        ""argmax"": False,\n        ""to_onehot"": False,\n        ""n_classes"": None,\n        ""threshold_values"": [True, False],\n        ""logit_thresh"": 0.6,\n    },\n    {""pred"": torch.tensor([[[[0.0, 1.0], [2.0, 3.0]]]]), ""label"": torch.tensor([[[[0, 1], [1, 1]]]])},\n    {\n        ""pred_discrete"": torch.tensor([[[[0.0, 1.0], [1.0, 1.0]]]]),\n        ""label_discrete"": torch.tensor([[[[0.0, 1.0], [1.0, 1.0]]]]),\n    },\n    (1, 1, 2, 2),\n]\n\nTEST_CASE_3 = [\n    {\n        ""keys"": [""pred""],\n        ""output_postfix"": ""discrete"",\n        ""argmax"": True,\n        ""to_onehot"": True,\n        ""n_classes"": 2,\n        ""threshold_values"": False,\n        ""logit_thresh"": 0.5,\n    },\n    {""pred"": torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]])},\n    {""pred_discrete"": torch.tensor([[[[0.0, 0.0]], [[1.0, 1.0]]]])},\n    (1, 2, 1, 2),\n]\n\nTEST_CASE_4 = [\n    {\n        ""keys"": ""pred"",\n        ""output_postfix"": None,\n        ""argmax"": True,\n        ""to_onehot"": True,\n        ""n_classes"": 2,\n        ""threshold_values"": False,\n        ""logit_thresh"": 0.5,\n    },\n    {""pred"": torch.tensor([[[[0.0, 1.0]], [[2.0, 3.0]]]])},\n    {""pred"": torch.tensor([[[[0.0, 0.0]], [[1.0, 1.0]]]])},\n    (1, 2, 1, 2),\n]\n\n\nclass TestAsDiscreted(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_value_shape(self, input_param, test_input, output, expected_shape):\n        result = AsDiscreted(**input_param)(test_input)\n        torch.testing.assert_allclose(result[""pred_discrete""], output[""pred_discrete""])\n        self.assertTupleEqual(result[""pred_discrete""].shape, expected_shape)\n        if ""label_discrete"" in result:\n            torch.testing.assert_allclose(result[""label_discrete""], output[""label_discrete""])\n            self.assertTupleEqual(result[""label_discrete""].shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_4])\n    def test_none_postfix(self, input_param, test_input, output, expected_shape):\n        result = AsDiscreted(**input_param)(test_input)\n        torch.testing.assert_allclose(result[""pred""], output[""pred""])\n        self.assertTupleEqual(result[""pred""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_cachedataset.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nimport nibabel as nib\nfrom parameterized import parameterized\nfrom monai.data import CacheDataset\nfrom monai.transforms import Compose, LoadNiftid\n\nTEST_CASE_1 = [(128, 128, 128)]\n\n\nclass TestCacheDataset(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, expected_shape):\n        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))\n        tempdir = tempfile.mkdtemp()\n        nib.save(test_image, os.path.join(tempdir, ""test_image1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_label1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_extra1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_image2.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_label2.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_extra2.nii.gz""))\n        test_data = [\n            {\n                ""image"": os.path.join(tempdir, ""test_image1.nii.gz""),\n                ""label"": os.path.join(tempdir, ""test_label1.nii.gz""),\n                ""extra"": os.path.join(tempdir, ""test_extra1.nii.gz""),\n            },\n            {\n                ""image"": os.path.join(tempdir, ""test_image2.nii.gz""),\n                ""label"": os.path.join(tempdir, ""test_label2.nii.gz""),\n                ""extra"": os.path.join(tempdir, ""test_extra2.nii.gz""),\n            },\n        ]\n        dataset = CacheDataset(\n            data=test_data, transform=Compose([LoadNiftid(keys=[""image"", ""label"", ""extra""])]), cache_rate=0.5\n        )\n        data1 = dataset[0]\n        data2 = dataset[1]\n        shutil.rmtree(tempdir)\n        self.assertTupleEqual(data1[""image""].shape, expected_shape)\n        self.assertTupleEqual(data1[""label""].shape, expected_shape)\n        self.assertTupleEqual(data1[""extra""].shape, expected_shape)\n        self.assertTupleEqual(data2[""image""].shape, expected_shape)\n        self.assertTupleEqual(data2[""label""].shape, expected_shape)\n        self.assertTupleEqual(data2[""extra""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_cachedataset_parallel.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nimport nibabel as nib\nfrom parameterized import parameterized\nfrom monai.data import CacheDataset\nfrom monai.transforms import Compose, LoadNiftid\n\nTEST_CASE_1 = [0, 100]\n\nTEST_CASE_2 = [4, 100]\n\n\nclass TestCacheDatasetParallel(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_shape(self, num_workers, dataset_size):\n        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))\n        tempdir = tempfile.mkdtemp()\n        nib.save(test_image, os.path.join(tempdir, ""test_image1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_label1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_extra1.nii.gz""))\n        test_data = [\n            {\n                ""image"": os.path.join(tempdir, ""test_image1.nii.gz""),\n                ""label"": os.path.join(tempdir, ""test_label1.nii.gz""),\n                ""extra"": os.path.join(tempdir, ""test_extra1.nii.gz""),\n            }\n        ] * dataset_size\n        dataset = CacheDataset(\n            data=test_data,\n            transform=Compose([LoadNiftid(keys=[""image"", ""label"", ""extra""])]),\n            cache_rate=1,\n            num_workers=num_workers,\n        )\n        shutil.rmtree(tempdir)\n        self.assertEqual(len(dataset._cache), dataset.cache_num)\n        for i in range(dataset.cache_num):\n            self.assertIsNotNone(dataset._cache[i])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_cast_to_type.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import CastToType\n\nTEST_CASE_1 = [{""dtype"": np.float64}, np.array([[0, 1], [1, 2]], dtype=np.float32), np.float64]\n\n\nclass TestCastToType(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_type(self, input_param, input_data, expected_type):\n        result = CastToType(**input_param)(input_data)\n        self.assertEqual(result.dtype, expected_type)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_cast_to_typed.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import CastToTyped\n\nTEST_CASE_1 = [\n    {""keys"": [""img""], ""dtype"": np.float64},\n    {""img"": np.array([[0, 1], [1, 2]], dtype=np.float32), ""seg"": np.array([[0, 1], [1, 2]], dtype=np.int8)},\n    {""img"": np.float64, ""seg"": np.int8},\n]\n\n\nclass TestCastToTyped(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_type(self, input_param, input_data, expected_type):\n        result = CastToTyped(**input_param)(input_data)\n        for k, v in result.items():\n            self.assertEqual(v.dtype, expected_type[k])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_center_spatial_crop.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import CenterSpatialCrop\n\nTEST_CASE_1 = [{""roi_size"": [2, 2, 2]}, np.random.randint(0, 2, size=[3, 3, 3, 3]), (3, 2, 2, 2)]\n\nTEST_CASE_2 = [\n    {""roi_size"": [2, 2]},\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]]),\n    np.array([[[1, 2], [2, 3]]]),\n]\n\n\nclass TestCenterSpatialCrop(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = CenterSpatialCrop(**input_param)(input_data)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_2])\n    def test_value(self, input_param, input_data, expected_value):\n        result = CenterSpatialCrop(**input_param)(input_data)\n        np.testing.assert_allclose(result, expected_value)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_center_spatial_cropd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import CenterSpatialCropd\n\nTEST_CASE_1 = [\n    {""keys"": ""img"", ""roi_size"": [2, 2, 2]},\n    {""img"": np.random.randint(0, 2, size=[3, 3, 3, 3])},\n    (3, 2, 2, 2),\n]\n\nTEST_CASE_2 = [\n    {""keys"": ""img"", ""roi_size"": [2, 2]},\n    {""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]])},\n    np.array([[[1, 2], [2, 3]]]),\n]\n\n\nclass TestCenterSpatialCropd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = CenterSpatialCropd(**input_param)(input_data)\n        self.assertTupleEqual(result[""img""].shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_2])\n    def test_value(self, input_param, input_data, expected_value):\n        result = CenterSpatialCropd(**input_param)(input_data)\n        np.testing.assert_allclose(result[""img""], expected_value)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_compose.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom monai.transforms import Compose, Randomizable, AddChannel\n\n\nclass TestCompose(unittest.TestCase):\n    def test_empty_compose(self):\n        c = Compose()\n        i = 1\n        self.assertEqual(c(i), 1)\n\n    def test_non_dict_compose(self):\n        def a(i):\n            return i + ""a""\n\n        def b(i):\n            return i + ""b""\n\n        c = Compose([a, b, a, b])\n        self.assertEqual(c(""""), ""abab"")\n\n    def test_dict_compose(self):\n        def a(d):\n            d = dict(d)\n            d[""a""] += 1\n            return d\n\n        def b(d):\n            d = dict(d)\n            d[""b""] += 1\n            return d\n\n        c = Compose([a, b, a, b, a])\n        self.assertDictEqual(c({""a"": 0, ""b"": 0}), {""a"": 3, ""b"": 2})\n\n    def test_list_dict_compose(self):\n        def a(d):  # transform to handle dict data\n            d = dict(d)\n            d[""a""] += 1\n            return d\n\n        def b(d):  # transform to generate a batch list of data\n            d = dict(d)\n            d[""b""] += 1\n            d = [d] * 5\n            return d\n\n        def c(d):  # transform to handle dict data\n            d = dict(d)\n            d[""c""] += 1\n            return d\n\n        transforms = Compose([a, a, b, c, c])\n        value = transforms({""a"": 0, ""b"": 0, ""c"": 0})\n        for item in value:\n            self.assertDictEqual(item, {""a"": 2, ""b"": 1, ""c"": 2})\n\n    def test_random_compose(self):\n        class _Acc(Randomizable):\n            self.rand = 0.0\n\n            def randomize(self):\n                self.rand = self.R.rand()\n\n            def __call__(self, data):\n                self.randomize()\n                return self.rand + data\n\n        c = Compose([_Acc(), _Acc()])\n        self.assertNotAlmostEqual(c(0), c(0))\n        c.set_random_state(123)\n        self.assertAlmostEqual(c(1), 2.39293837)\n        c.set_random_state(223)\n        c.randomize()\n        self.assertAlmostEqual(c(1), 2.57673391)\n\n    def test_randomize_warn(self):\n        class _RandomClass(Randomizable):\n            def randomize(self, foo):\n                pass\n\n        c = Compose([_RandomClass(), _RandomClass()])\n        with self.assertWarns(Warning):\n            c.randomize()\n\n    def test_err_msg(self):\n        transforms = Compose([abs, AddChannel(), round])\n        with self.assertRaisesRegexp(Exception, ""AddChannel""):\n            transforms(42.1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_compute_meandice.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.metrics import compute_meandice\n\n# keep background\nTEST_CASE_1 = [  # y (1, 1, 2, 2), y_pred (1, 1, 2, 2), expected out (1, 1)\n    {\n        ""y_pred"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n        ""y"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n        ""include_background"": True,\n        ""to_onehot_y"": False,\n        ""mutually_exclusive"": False,\n        ""logit_thresh"": 0.5,\n        ""sigmoid"": True,\n    },\n    [[0.8]],\n]\n\n# remove background and not One-Hot target\nTEST_CASE_2 = [  # y (2, 1, 2, 2), y_pred (2, 3, 2, 2), expected out (2, 2) (no background)\n    {\n        ""y_pred"": torch.tensor(\n            [\n                [[[-1.0, 3.0], [2.0, -4.0]], [[0.0, -1.0], [3.0, 2.0]], [[0.0, 1.0], [2.0, -1.0]]],\n                [[[-2.0, 0.0], [3.0, 1.0]], [[0.0, 2.0], [1.0, -2.0]], [[-1.0, 2.0], [4.0, 0.0]]],\n            ]\n        ),\n        ""y"": torch.tensor([[[[1.0, 2.0], [1.0, 0.0]]], [[[1.0, 1.0], [2.0, 0.0]]]]),\n        ""include_background"": False,\n        ""to_onehot_y"": True,\n        ""mutually_exclusive"": True,\n    },\n    [[0.5000, 0.0000], [0.6666, 0.6666]],\n]\n\n# should return Nan for all labels=0 case and skip for MeanDice\nTEST_CASE_3 = [\n    {\n        ""y_pred"": torch.zeros(2, 3, 2, 2),\n        ""y"": torch.tensor([[[[0.0, 0.0], [0.0, 0.0]]], [[[1.0, 0.0], [0.0, 1.0]]]]),\n        ""include_background"": True,\n        ""to_onehot_y"": True,\n        ""mutually_exclusive"": True,\n    },\n    [[False, True, True], [False, False, True]],\n]\n\n\nclass TestComputeMeanDice(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_value(self, input_data, expected_value):\n        result = compute_meandice(**input_data)\n        self.assertTrue(np.allclose(result.cpu().numpy(), expected_value, atol=1e-4))\n\n    @parameterized.expand([TEST_CASE_3])\n    def test_nans(self, input_data, expected_value):\n        result = compute_meandice(**input_data)\n        self.assertTrue(np.allclose(np.isnan(result.cpu().numpy()), expected_value))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_compute_roc_auc.py,11,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.metrics import compute_roc_auc\n\nTEST_CASE_1 = [\n    {\n        ""y_pred"": torch.tensor([[0.1, 0.9], [0.3, 1.4], [0.2, 0.1], [0.1, 0.5]]),\n        ""y"": torch.tensor([[0], [1], [0], [1]]),\n        ""to_onehot_y"": True,\n        ""softmax"": True,\n    },\n    0.75,\n]\n\nTEST_CASE_2 = [{""y_pred"": torch.tensor([[0.5], [0.5], [0.2], [8.3]]), ""y"": torch.tensor([[0], [1], [0], [1]])}, 0.875]\n\nTEST_CASE_3 = [{""y_pred"": torch.tensor([[0.5], [0.5], [0.2], [8.3]]), ""y"": torch.tensor([0, 1, 0, 1])}, 0.875]\n\nTEST_CASE_4 = [{""y_pred"": torch.tensor([0.5, 0.5, 0.2, 8.3]), ""y"": torch.tensor([0, 1, 0, 1])}, 0.875]\n\nTEST_CASE_5 = [\n    {\n        ""y_pred"": torch.tensor([[0.1, 0.9], [0.3, 1.4], [0.2, 0.1], [0.1, 0.5]]),\n        ""y"": torch.tensor([[0], [1], [0], [1]]),\n        ""to_onehot_y"": True,\n        ""softmax"": True,\n        ""average"": None,\n    },\n    [0.75, 0.75],\n]\n\nTEST_CASE_6 = [\n    {\n        ""y_pred"": torch.tensor([[0.1, 0.9], [0.3, 1.4], [0.2, 0.1], [0.1, 0.5], [0.1, 0.5]]),\n        ""y"": torch.tensor([[1, 0], [0, 1], [0, 0], [1, 1], [0, 1]]),\n        ""softmax"": True,\n        ""average"": ""weighted"",\n    },\n    0.56667,\n]\n\nTEST_CASE_7 = [\n    {\n        ""y_pred"": torch.tensor([[0.1, 0.9], [0.3, 1.4], [0.2, 0.1], [0.1, 0.5], [0.1, 0.5]]),\n        ""y"": torch.tensor([[1, 0], [0, 1], [0, 0], [1, 1], [0, 1]]),\n        ""softmax"": True,\n        ""average"": ""micro"",\n    },\n    0.62,\n]\n\n\nclass TestComputeROCAUC(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5, TEST_CASE_6, TEST_CASE_7])\n    def test_value(self, input_data, expected_value):\n        result = compute_roc_auc(**input_data)\n        np.testing.assert_allclose(expected_value, result, rtol=1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_convolutions.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom tests.utils import TorchImageTestCase2D\n\nfrom monai.networks.blocks import Convolution, ResidualUnit\n\n\nclass TestConvolution2D(TorchImageTestCase2D):\n    def test_conv1(self):\n        conv = Convolution(2, self.input_channels, self.output_channels)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_conv_only1(self):\n        conv = Convolution(2, self.input_channels, self.output_channels, conv_only=True)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_stride1(self):\n        conv = Convolution(2, self.input_channels, self.output_channels, strides=2)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0] // 2, self.im_shape[1] // 2)\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_dilation1(self):\n        conv = Convolution(2, self.input_channels, self.output_channels, dilation=3)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_dropout1(self):\n        conv = Convolution(2, self.input_channels, self.output_channels, dropout=0.15)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_transpose1(self):\n        conv = Convolution(2, self.input_channels, self.output_channels, is_transposed=True)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_transpose2(self):\n        conv = Convolution(2, self.input_channels, self.output_channels, strides=2, is_transposed=True)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0] * 2, self.im_shape[1] * 2)\n        self.assertEqual(out.shape, expected_shape)\n\n\nclass TestResidualUnit2D(TorchImageTestCase2D):\n    def test_conv_only1(self):\n        conv = ResidualUnit(2, 1, self.output_channels)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_stride1(self):\n        conv = ResidualUnit(2, 1, self.output_channels, strides=2)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0] // 2, self.im_shape[1] // 2)\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_dilation1(self):\n        conv = ResidualUnit(2, 1, self.output_channels, dilation=3)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n\n    def test_dropout1(self):\n        conv = ResidualUnit(2, 1, self.output_channels, dropout=0.15)\n        out = conv(self.imt)\n        expected_shape = (1, self.output_channels, self.im_shape[0], self.im_shape[1])\n        self.assertEqual(out.shape, expected_shape)\n'"
tests/test_create_grid_and_affine.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import (\n    create_control_grid,\n    create_grid,\n    create_rotate,\n    create_scale,\n    create_shear,\n    create_translate,\n)\n\n\nclass TestCreateGrid(unittest.TestCase):\n    def test_create_grid(self):\n        with self.assertRaisesRegex(TypeError, """"):\n            create_grid(None)\n        with self.assertRaisesRegex(TypeError, """"):\n            create_grid((1, 1), spacing=2.0)\n        with self.assertRaisesRegex(TypeError, """"):\n            create_grid((1, 1), spacing=2.0)\n\n        g = create_grid((1, 1))\n        expected = np.array([[[0.0]], [[0.0]], [[1.0]]])\n        np.testing.assert_allclose(g, expected)\n\n        g = create_grid((1, 1), homogeneous=False)\n        expected = np.array([[[0.0]], [[0.0]]])\n        np.testing.assert_allclose(g, expected)\n\n        g = create_grid((1, 1), spacing=(1.2, 1.3))\n        expected = np.array([[[0.0]], [[0.0]], [[1.0]]])\n        np.testing.assert_allclose(g, expected)\n\n        g = create_grid((1, 1, 1), spacing=(1.2, 1.3, 1.0))\n        expected = np.array([[[[0.0]]], [[[0.0]]], [[[0.0]]], [[[1.0]]]])\n        np.testing.assert_allclose(g, expected)\n\n        g = create_grid((1, 1, 1), spacing=(1.2, 1.3, 1.0), homogeneous=False)\n        expected = np.array([[[[0.0]]], [[[0.0]]], [[[0.0]]]])\n        np.testing.assert_allclose(g, expected)\n\n        g = create_grid((1, 1, 1), spacing=(1.2, 1.3, 1.0), dtype=np.int32)\n        np.testing.assert_equal(g.dtype, np.int32)\n\n        g = create_grid((2, 2, 2))\n        expected = np.array(\n            [\n                [[[-0.5, -0.5], [-0.5, -0.5]], [[0.5, 0.5], [0.5, 0.5]]],\n                [[[-0.5, -0.5], [0.5, 0.5]], [[-0.5, -0.5], [0.5, 0.5]]],\n                [[[-0.5, 0.5], [-0.5, 0.5]], [[-0.5, 0.5], [-0.5, 0.5]]],\n                [[[1.0, 1.0], [1.0, 1.0]], [[1.0, 1.0], [1.0, 1.0]]],\n            ]\n        )\n        np.testing.assert_allclose(g, expected)\n\n        g = create_grid((2, 2, 2), spacing=(1.2, 1.3, 1.0))\n        expected = np.array(\n            [\n                [[[-0.6, -0.6], [-0.6, -0.6]], [[0.6, 0.6], [0.6, 0.6]]],\n                [[[-0.65, -0.65], [0.65, 0.65]], [[-0.65, -0.65], [0.65, 0.65]]],\n                [[[-0.5, 0.5], [-0.5, 0.5]], [[-0.5, 0.5], [-0.5, 0.5]]],\n                [[[1.0, 1.0], [1.0, 1.0]], [[1.0, 1.0], [1.0, 1.0]]],\n            ]\n        )\n        np.testing.assert_allclose(g, expected)\n\n    def test_create_control_grid(self):\n        with self.assertRaisesRegex(TypeError, """"):\n            create_control_grid(None, None)\n        with self.assertRaisesRegex(TypeError, """"):\n            create_control_grid((1, 1), 2.0)\n\n        g = create_control_grid((1.0, 1.0), (1.0, 1.0))\n        expected = np.array(\n            [\n                [[-1.0, -1.0, -1.0], [0.0, 0.0, 0.0], [1.0, 1.0, 1.0]],\n                [[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]],\n                [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],\n            ]\n        )\n        np.testing.assert_allclose(g, expected)\n\n        g = create_control_grid((1.0, 1.0), (2.0, 2.0))\n        expected = np.array(\n            [\n                [[-2.0, -2.0, -2.0], [0.0, 0.0, 0.0], [2.0, 2.0, 2.0]],\n                [[-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0]],\n                [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],\n            ]\n        )\n        np.testing.assert_allclose(g, expected)\n\n        g = create_control_grid((2.0, 2.0), (1.0, 1.0))\n        expected = np.array(\n            [\n                [[-1.5, -1.5, -1.5, -1.5], [-0.5, -0.5, -0.5, -0.5], [0.5, 0.5, 0.5, 0.5], [1.5, 1.5, 1.5, 1.5]],\n                [[-1.5, -0.5, 0.5, 1.5], [-1.5, -0.5, 0.5, 1.5], [-1.5, -0.5, 0.5, 1.5], [-1.5, -0.5, 0.5, 1.5]],\n                [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]],\n            ]\n        )\n        np.testing.assert_allclose(g, expected)\n\n        g = create_control_grid((2.0, 2.0), (2.0, 2.0))\n        expected = np.array(\n            [\n                [[-3.0, -3.0, -3.0, -3.0], [-1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0], [3.0, 3.0, 3.0, 3.0]],\n                [[-3.0, -1.0, 1.0, 3.0], [-3.0, -1.0, 1.0, 3.0], [-3.0, -1.0, 1.0, 3.0], [-3.0, -1.0, 1.0, 3.0]],\n                [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]],\n            ]\n        )\n        np.testing.assert_allclose(g, expected)\n\n        g = create_control_grid((1.0, 1.0, 1.0), (2.0, 2.0, 2.0), homogeneous=False)\n        expected = np.array(\n            [\n                [\n                    [[-2.0, -2.0, -2.0], [-2.0, -2.0, -2.0], [-2.0, -2.0, -2.0]],\n                    [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n                    [[2.0, 2.0, 2.0], [2.0, 2.0, 2.0], [2.0, 2.0, 2.0]],\n                ],\n                [\n                    [[-2.0, -2.0, -2.0], [0.0, 0.0, 0.0], [2.0, 2.0, 2.0]],\n                    [[-2.0, -2.0, -2.0], [0.0, 0.0, 0.0], [2.0, 2.0, 2.0]],\n                    [[-2.0, -2.0, -2.0], [0.0, 0.0, 0.0], [2.0, 2.0, 2.0]],\n                ],\n                [\n                    [[-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0]],\n                    [[-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0]],\n                    [[-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0], [-2.0, 0.0, 2.0]],\n                ],\n            ]\n        )\n        np.testing.assert_allclose(g, expected)\n\n\ndef test_assert(func, params, expected):\n    m = func(*params)\n    np.testing.assert_allclose(m, expected, atol=1e-7)\n\n\nclass TestCreateAffine(unittest.TestCase):\n    def test_create_rotate(self):\n        with self.assertRaisesRegex(TypeError, """"):\n            create_rotate(2, None)\n\n        with self.assertRaisesRegex(ValueError, """"):\n            create_rotate(5, 1)\n\n        test_assert(\n            create_rotate,\n            (2, 1.1),\n            np.array([[0.45359612, -0.89120736, 0.0], [0.89120736, 0.45359612, 0.0], [0.0, 0.0, 1.0]]),\n        )\n        test_assert(\n            create_rotate,\n            (3, 1.1),\n            np.array(\n                [\n                    [1.0, 0.0, 0.0, 0.0],\n                    [0.0, 0.45359612, -0.89120736, 0.0],\n                    [0.0, 0.89120736, 0.45359612, 0.0],\n                    [0.0, 0.0, 0.0, 1.0],\n                ]\n            ),\n        )\n        test_assert(\n            create_rotate,\n            (3, (1.1, 1)),\n            np.array(\n                [\n                    [0.54030231, 0.0, 0.84147098, 0.0],\n                    [0.74992513, 0.45359612, -0.48152139, 0.0],\n                    [-0.38168798, 0.89120736, 0.24507903, 0.0],\n                    [0.0, 0.0, 0.0, 1.0],\n                ]\n            ),\n        )\n        test_assert(\n            create_rotate,\n            (3, (1, 1, 1.1)),\n            np.array(\n                [\n                    [0.24507903, -0.48152139, 0.84147098, 0.0],\n                    [0.80270075, -0.38596121, -0.45464871, 0.0],\n                    [0.54369824, 0.78687425, 0.29192658, 0.0],\n                    [0.0, 0.0, 0.0, 1.0],\n                ]\n            ),\n        )\n        test_assert(\n            create_rotate,\n            (3, (0, 0, np.pi / 2)),\n            np.array([[0.0, -1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n\n    def test_create_shear(self):\n        test_assert(create_shear, (2, 1.0), np.array([[1.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))\n        test_assert(create_shear, (2, (2.0, 3.0)), np.array([[1.0, 2.0, 0.0], [3.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))\n        test_assert(\n            create_shear,\n            (3, 1.0),\n            np.array([[1.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n\n    def test_create_scale(self):\n        test_assert(create_scale, (2, 2), np.array([[2.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))\n        test_assert(create_scale, (2, [2, 2, 2]), np.array([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 1.0]]))\n        test_assert(\n            create_scale,\n            (3, [1.5, 2.4]),\n            np.array([[1.5, 0.0, 0.0, 0.0], [0.0, 2.4, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n        test_assert(\n            create_scale,\n            (3, 1.5),\n            np.array([[1.5, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n        test_assert(\n            create_scale,\n            (3, [1, 2, 3, 4, 5]),\n            np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 2.0, 0.0, 0.0], [0.0, 0.0, 3.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n\n    def test_create_translate(self):\n        test_assert(create_translate, (2, 2), np.array([[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]))\n        test_assert(create_translate, (2, [2, 2, 2]), np.array([[1.0, 0.0, 2.0], [0.0, 1.0, 2.0], [0.0, 0.0, 1.0]]))\n        test_assert(\n            create_translate,\n            (3, [1.5, 2.4]),\n            np.array([[1.0, 0.0, 0.0, 1.5], [0.0, 1.0, 0.0, 2.4], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n        test_assert(\n            create_translate,\n            (3, 1.5),\n            np.array([[1.0, 0.0, 0.0, 1.5], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n        test_assert(\n            create_translate,\n            (3, [1, 2, 3, 4, 5]),\n            np.array([[1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 2.0], [0.0, 0.0, 1.0, 3.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_crop_foreground.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import CropForeground\n\nTEST_CASE_1 = [\n    {""select_fn"": lambda x: x > 0, ""channel_indexes"": None, ""margin"": 0},\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]]),\n    np.array([[[1, 2, 1], [2, 3, 2], [1, 2, 1]]]),\n]\n\nTEST_CASE_2 = [\n    {""select_fn"": lambda x: x > 1, ""channel_indexes"": None, ""margin"": 0},\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 1, 1, 0], [0, 1, 3, 1, 0], [0, 1, 1, 1, 0], [0, 0, 0, 0, 0]]]),\n    np.array([[[3]]]),\n]\n\nTEST_CASE_3 = [\n    {""select_fn"": lambda x: x > 0, ""channel_indexes"": 0, ""margin"": 0},\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]]),\n    np.array([[[1, 2, 1], [2, 3, 2], [1, 2, 1]]]),\n]\n\nTEST_CASE_4 = [\n    {""select_fn"": lambda x: x > 0, ""channel_indexes"": None, ""margin"": 1},\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]]),\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 0, 0, 0, 0]]]),\n]\n\n\nclass TestCropForeground(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_value(self, argments, image, expected_data):\n        result = CropForeground(**argments)(image)\n        np.testing.assert_allclose(result, expected_data)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_crop_foregroundd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import CropForegroundd\n\nTEST_CASE_1 = [\n    {\n        ""keys"": [""img"", ""label""],\n        ""source_key"": ""label"",\n        ""select_fn"": lambda x: x > 0,\n        ""channel_indexes"": None,\n        ""margin"": 0,\n    },\n    {\n        ""img"": np.array([[[1, 0, 2, 0, 1], [0, 1, 2, 1, 0], [2, 2, 3, 2, 2], [0, 1, 2, 1, 0], [1, 0, 2, 0, 1]]]),\n        ""label"": np.array([[[0, 0, 0, 0, 0], [0, 1, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 1, 0], [0, 0, 0, 0, 0]]]),\n    },\n    np.array([[[1, 2, 1], [2, 3, 2], [1, 2, 1]]]),\n]\n\nTEST_CASE_2 = [\n    {""keys"": [""img""], ""source_key"": ""img"", ""select_fn"": lambda x: x > 1, ""channel_indexes"": None, ""margin"": 0},\n    {""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 1, 1, 0], [0, 1, 3, 1, 0], [0, 1, 1, 1, 0], [0, 0, 0, 0, 0]]])},\n    np.array([[[3]]]),\n]\n\nTEST_CASE_3 = [\n    {""keys"": [""img""], ""source_key"": ""img"", ""select_fn"": lambda x: x > 0, ""channel_indexes"": 0, ""margin"": 0},\n    {""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]])},\n    np.array([[[1, 2, 1], [2, 3, 2], [1, 2, 1]]]),\n]\n\nTEST_CASE_4 = [\n    {""keys"": [""img""], ""source_key"": ""img"", ""select_fn"": lambda x: x > 0, ""channel_indexes"": None, ""margin"": 1},\n    {""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]])},\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 0, 0, 0, 0]]]),\n]\n\n\nclass TestCropForegroundd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_value(self, argments, image, expected_data):\n        result = CropForegroundd(**argments)(image)\n        np.testing.assert_allclose(result[""img""], expected_data)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_csv_saver.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport csv\nimport shutil\nimport unittest\nimport numpy as np\nimport torch\n\n\nfrom monai.data import CSVSaver\n\n\nclass TestCSVSaver(unittest.TestCase):\n    def test_saved_content(self):\n        default_dir = os.path.join(""."", ""tempdir"")\n        shutil.rmtree(default_dir, ignore_errors=True)\n\n        saver = CSVSaver(output_dir=default_dir, filename=""predictions.csv"")\n\n        meta_data = {""filename_or_obj"": [""testfile"" + str(i) for i in range(8)]}\n        saver.save_batch(torch.zeros(8), meta_data)\n        saver.finalize()\n        filepath = os.path.join(default_dir, ""predictions.csv"")\n        self.assertTrue(os.path.exists(filepath))\n        with open(filepath, ""r"") as f:\n            reader = csv.reader(f)\n            i = 0\n            for row in reader:\n                self.assertEqual(row[0], ""testfile"" + str(i))\n                self.assertEqual(np.array(row[1:]).astype(np.float32), 0.0)\n                i += 1\n            self.assertEqual(i, 8)\n        shutil.rmtree(default_dir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_data_stats.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport logging\nimport tempfile\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import DataStats\n\nTEST_CASE_1 = [\n    {\n        ""prefix"": ""test data"",\n        ""data_shape"": False,\n        ""intensity_range"": False,\n        ""data_value"": False,\n        ""additional_info"": None,\n        ""logger_handler"": None,\n    },\n    np.array([[0, 1], [1, 2]]),\n    ""test data statistics:"",\n]\n\nTEST_CASE_2 = [\n    {\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": False,\n        ""data_value"": False,\n        ""additional_info"": None,\n        ""logger_handler"": None,\n    },\n    np.array([[0, 1], [1, 2]]),\n    ""test data statistics:\\nShape: (2, 2)"",\n]\n\nTEST_CASE_3 = [\n    {\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": True,\n        ""data_value"": False,\n        ""additional_info"": None,\n        ""logger_handler"": None,\n    },\n    np.array([[0, 1], [1, 2]]),\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)"",\n]\n\nTEST_CASE_4 = [\n    {\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": True,\n        ""data_value"": True,\n        ""additional_info"": None,\n        ""logger_handler"": None,\n    },\n    np.array([[0, 1], [1, 2]]),\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)\\nValue: [[0 1]\\n [1 2]]"",\n]\n\nTEST_CASE_5 = [\n    {\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": True,\n        ""data_value"": True,\n        ""additional_info"": lambda x: np.mean(x),\n        ""logger_handler"": None,\n    },\n    np.array([[0, 1], [1, 2]]),\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)\\nValue: [[0 1]\\n [1 2]]\\nAdditional info: 1.0"",\n]\n\nTEST_CASE_6 = [\n    np.array([[0, 1], [1, 2]]),\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)\\nValue: [[0 1]\\n [1 2]]\\nAdditional info: 1.0\\n"",\n]\n\n\nclass TestDataStats(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5])\n    def test_value(self, input_param, input_data, expected_print):\n        transform = DataStats(**input_param)\n        _ = transform(input_data)\n        self.assertEqual(transform.output, expected_print)\n\n    @parameterized.expand([TEST_CASE_6])\n    def test_file(self, input_data, expected_print):\n        tempdir = tempfile.mkdtemp()\n        filename = os.path.join(tempdir, ""test_data_stats.log"")\n        handler = logging.FileHandler(filename, mode=""w"")\n        input_param = {\n            ""prefix"": ""test data"",\n            ""data_shape"": True,\n            ""intensity_range"": True,\n            ""data_value"": True,\n            ""additional_info"": lambda x: np.mean(x),\n            ""logger_handler"": handler,\n        }\n        transform = DataStats(**input_param)\n        _ = transform(input_data)\n        handler.stream.close()\n        transform._logger.removeHandler(handler)\n        with open(filename, ""r"") as f:\n            content = f.read()\n            self.assertEqual(content, expected_print)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_data_statsd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport logging\nimport tempfile\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import DataStatsd\n\nTEST_CASE_1 = [\n    {\n        ""keys"": ""img"",\n        ""prefix"": ""test data"",\n        ""data_shape"": False,\n        ""intensity_range"": False,\n        ""data_value"": False,\n        ""additional_info"": None,\n    },\n    {""img"": np.array([[0, 1], [1, 2]])},\n    ""test data statistics:"",\n]\n\nTEST_CASE_2 = [\n    {\n        ""keys"": ""img"",\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": False,\n        ""data_value"": False,\n        ""additional_info"": None,\n    },\n    {""img"": np.array([[0, 1], [1, 2]])},\n    ""test data statistics:\\nShape: (2, 2)"",\n]\n\nTEST_CASE_3 = [\n    {\n        ""keys"": ""img"",\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": True,\n        ""data_value"": False,\n        ""additional_info"": None,\n    },\n    {""img"": np.array([[0, 1], [1, 2]])},\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)"",\n]\n\nTEST_CASE_4 = [\n    {\n        ""keys"": ""img"",\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": True,\n        ""data_value"": True,\n        ""additional_info"": None,\n    },\n    {""img"": np.array([[0, 1], [1, 2]])},\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)\\nValue: [[0 1]\\n [1 2]]"",\n]\n\nTEST_CASE_5 = [\n    {\n        ""keys"": ""img"",\n        ""prefix"": ""test data"",\n        ""data_shape"": True,\n        ""intensity_range"": True,\n        ""data_value"": True,\n        ""additional_info"": lambda x: np.mean(x),\n    },\n    {""img"": np.array([[0, 1], [1, 2]])},\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)\\nValue: [[0 1]\\n [1 2]]\\nAdditional info: 1.0"",\n]\n\nTEST_CASE_6 = [\n    {\n        ""keys"": (""img"", ""affine""),\n        ""prefix"": (""image"", ""affine""),\n        ""data_shape"": True,\n        ""intensity_range"": (True, False),\n        ""data_value"": (False, True),\n        ""additional_info"": (lambda x: np.mean(x), None),\n    },\n    {""img"": np.array([[0, 1], [1, 2]]), ""affine"": np.eye(2, 2)},\n    ""affine statistics:\\nShape: (2, 2)\\nValue: [[1. 0.]\\n [0. 1.]]"",\n]\n\nTEST_CASE_7 = [\n    {""img"": np.array([[0, 1], [1, 2]])},\n    ""test data statistics:\\nShape: (2, 2)\\nIntensity range: (0, 2)\\nValue: [[0 1]\\n [1 2]]\\nAdditional info: 1.0\\n"",\n]\n\n\nclass TestDataStatsd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5, TEST_CASE_6])\n    def test_value(self, input_param, input_data, expected_print):\n        transform = DataStatsd(**input_param)\n        _ = transform(input_data)\n        self.assertEqual(transform.printer.output, expected_print)\n\n    @parameterized.expand([TEST_CASE_7])\n    def test_file(self, input_data, expected_print):\n        tempdir = tempfile.mkdtemp()\n        filename = os.path.join(tempdir, ""test_stats.log"")\n        handler = logging.FileHandler(filename, mode=""w"")\n        input_param = {\n            ""keys"": ""img"",\n            ""prefix"": ""test data"",\n            ""data_shape"": True,\n            ""intensity_range"": True,\n            ""data_value"": True,\n            ""additional_info"": lambda x: np.mean(x),\n            ""logger_handler"": handler,\n        }\n        transform = DataStatsd(**input_param)\n        _ = transform(input_data)\n        handler.stream.close()\n        transform.printer._logger.removeHandler(handler)\n        with open(filename, ""r"") as f:\n            content = f.read()\n            self.assertEqual(content, expected_print)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_dataloader.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom monai.data import CacheDataset, DataLoader\nfrom monai.transforms import DataStatsd, SimulateDelayd, Compose\n\n\nclass TestDataLoader(unittest.TestCase):\n    def test_values(self):\n        datalist = [\n            {""image"": ""spleen_19.nii.gz"", ""label"": ""spleen_label_19.nii.gz""},\n            {""image"": ""spleen_31.nii.gz"", ""label"": ""spleen_label_31.nii.gz""},\n        ]\n        transform = Compose(\n            [\n                DataStatsd(keys=[""image"", ""label""], data_shape=False, intensity_range=False, data_value=True),\n                SimulateDelayd(keys=[""image"", ""label""], delay_time=0.1),\n            ]\n        )\n        dataset = CacheDataset(data=datalist, transform=transform, cache_rate=0.5, cache_num=1)\n        dataloader = DataLoader(dataset=dataset, batch_size=2, num_workers=2)\n        for d in dataloader:\n            self.assertEqual(d[""image""][0], ""spleen_19.nii.gz"")\n            self.assertEqual(d[""image""][1], ""spleen_31.nii.gz"")\n            self.assertEqual(d[""label""][0], ""spleen_label_19.nii.gz"")\n            self.assertEqual(d[""label""][1], ""spleen_label_31.nii.gz"")\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_dataset.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nimport nibabel as nib\nfrom parameterized import parameterized\nfrom monai.data import Dataset\nfrom monai.transforms import Compose, LoadNiftid, SimulateDelayd\n\nTEST_CASE_1 = [(128, 128, 128)]\n\n\nclass TestDataset(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, expected_shape):\n        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))\n        tempdir = tempfile.mkdtemp()\n        nib.save(test_image, os.path.join(tempdir, ""test_image1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_label1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_extra1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_image2.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_label2.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_extra2.nii.gz""))\n        test_data = [\n            {\n                ""image"": os.path.join(tempdir, ""test_image1.nii.gz""),\n                ""label"": os.path.join(tempdir, ""test_label1.nii.gz""),\n                ""extra"": os.path.join(tempdir, ""test_extra1.nii.gz""),\n            },\n            {\n                ""image"": os.path.join(tempdir, ""test_image2.nii.gz""),\n                ""label"": os.path.join(tempdir, ""test_label2.nii.gz""),\n                ""extra"": os.path.join(tempdir, ""test_extra2.nii.gz""),\n            },\n        ]\n        test_transform = Compose(\n            [\n                LoadNiftid(keys=[""image"", ""label"", ""extra""]),\n                SimulateDelayd(keys=[""image"", ""label"", ""extra""], delay_time=[1e-7, 1e-6, 1e-5]),\n            ]\n        )\n        dataset = Dataset(data=test_data, transform=test_transform)\n        data1 = dataset[0]\n        data2 = dataset[1]\n\n        self.assertTupleEqual(data1[""image""].shape, expected_shape)\n        self.assertTupleEqual(data1[""label""].shape, expected_shape)\n        self.assertTupleEqual(data1[""extra""].shape, expected_shape)\n        self.assertTupleEqual(data2[""image""].shape, expected_shape)\n        self.assertTupleEqual(data2[""label""].shape, expected_shape)\n        self.assertTupleEqual(data2[""extra""].shape, expected_shape)\n\n        dataset = Dataset(data=test_data, transform=LoadNiftid(keys=[""image"", ""label"", ""extra""]))\n        data1_simple = dataset[0]\n        data2_simple = dataset[1]\n\n        self.assertTupleEqual(data1_simple[""image""].shape, expected_shape)\n        self.assertTupleEqual(data1_simple[""label""].shape, expected_shape)\n        self.assertTupleEqual(data1_simple[""extra""].shape, expected_shape)\n        self.assertTupleEqual(data2_simple[""image""].shape, expected_shape)\n        self.assertTupleEqual(data2_simple[""label""].shape, expected_shape)\n        self.assertTupleEqual(data2_simple[""extra""].shape, expected_shape)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_delete_keys.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport time\nimport sys\nfrom parameterized import parameterized\nfrom monai.transforms import DeleteKeysd\n\nTEST_CASE_1 = [\n    {""keys"": [str(i) for i in range(30)]},\n    20,\n]\n\n\nclass TestDeleteKeysd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_memory(self, input_param, expected_key_size):\n        input_data = dict()\n        for i in range(50):\n            input_data[str(i)] = [time.time()] * 100000\n        result = DeleteKeysd(**input_param)(input_data)\n        self.assertEqual(len(result.keys()), expected_key_size)\n        self.assertGreaterEqual(\n            sys.getsizeof(input_data) * float(expected_key_size) / len(input_data), sys.getsizeof(result)\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_densenet.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.networks.nets import densenet121, densenet169, densenet201, densenet264\n\n\nTEST_CASE_1 = [  # 4-channel 3D, batch 16\n    {""spatial_dims"": 3, ""in_channels"": 2, ""out_channels"": 3},\n    torch.randn(16, 2, 32, 64, 48),\n    (16, 3),\n]\n\n\nclass TestDENSENET(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_121_shape(self, input_param, input_data, expected_shape):\n        net = densenet121(**input_param)\n        net.eval()\n        with torch.no_grad():\n            result = net.forward(input_data)\n            self.assertEqual(result.shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_1])\n    def test_169_shape(self, input_param, input_data, expected_shape):\n        net = densenet169(**input_param)\n        net.eval()\n        with torch.no_grad():\n            result = net.forward(input_data)\n            self.assertEqual(result.shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_1])\n    def test_201_shape(self, input_param, input_data, expected_shape):\n        net = densenet201(**input_param)\n        net.eval()\n        with torch.no_grad():\n            result = net.forward(input_data)\n            self.assertEqual(result.shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_1])\n    def test_264_shape(self, input_param, input_data, expected_shape):\n        net = densenet264(**input_param)\n        net.eval()\n        with torch.no_grad():\n            result = net.forward(input_data)\n            self.assertEqual(result.shape, expected_shape)\n'"
tests/test_dice_loss.py,25,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.losses import DiceLoss\n\nTEST_CASES = [\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.307576,\n    ],\n    [  # shape: (2, 1, 2, 2), (2, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]], [[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 1.0], [1.0, 1.0]]], [[[1.0, 0.0], [1.0, 0.0]]]]),\n            ""smooth"": 1e-4,\n        },\n        0.416657,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": False, ""to_onehot_y"": True},\n        {\n            ""input"": torch.tensor([[[1.0, 1.0, 0.0], [0.0, 0.0, 1.0]], [[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]]),\n            ""target"": torch.tensor([[[0.0, 0.0, 1.0]], [[0.0, 1.0, 0.0]]]),\n            ""smooth"": 0.0,\n        },\n        0.0,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.435050,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""sigmoid"": True, ""reduction"": ""none""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        [[0.296529, 0.415136], [0.599976, 0.428559]],\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.383713,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True, ""reduction"": ""sum""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        1.534853,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.307576,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True, ""squared_pred"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-5,\n        },\n        0.178337,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True, ""jaccard"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-5,\n        },\n        -0.059094,\n    ],\n]\n\n\nclass TestDiceLoss(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_shape(self, input_param, input_data, expected_val):\n        result = DiceLoss(**input_param).forward(**input_data)\n        np.testing.assert_allclose(result.detach().cpu().numpy(), expected_val, rtol=1e-5)\n\n    def test_ill_shape(self):\n        loss = DiceLoss()\n        with self.assertRaisesRegex(AssertionError, """"):\n            loss.forward(torch.ones((1, 2, 3)), torch.ones((4, 5, 6)))\n\n    def test_ill_opts(self):\n        with self.assertRaisesRegex(ValueError, """"):\n            DiceLoss(sigmoid=True, softmax=True)\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertRaisesRegex(ValueError, """"):\n            DiceLoss(reduction=""unknown"")(chn_input, chn_target)\n        with self.assertRaisesRegex(ValueError, """"):\n            DiceLoss(reduction=None)(chn_input, chn_target)\n\n    def test_input_warnings(self):\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertWarns(Warning):\n            loss = DiceLoss(include_background=False)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = DiceLoss(softmax=True)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = DiceLoss(to_onehot_y=True)\n            loss.forward(chn_input, chn_target)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_flip.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import Flip\nfrom tests.utils import NumpyImageTestCase2D\n\nINVALID_CASES = [(""wrong_axis"", [""s"", 1], TypeError), (""not_numbers"", ""s"", TypeError)]\n\nVALID_CASES = [(""no_axis"", None), (""one_axis"", 1), (""many_axis"", [0, 1])]\n\n\nclass TestFlip(NumpyImageTestCase2D):\n    @parameterized.expand(INVALID_CASES)\n    def test_invalid_inputs(self, _, spatial_axis, raises):\n        with self.assertRaises(raises):\n            flip = Flip(spatial_axis)\n            flip(self.imt[0])\n\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, _, spatial_axis):\n        flip = Flip(spatial_axis=spatial_axis)\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.flip(channel, spatial_axis))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(expected, flip(self.imt[0])))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_flipd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import Flipd\nfrom tests.utils import NumpyImageTestCase2D\n\nINVALID_CASES = [(""wrong_axis"", [""s"", 1], TypeError), (""not_numbers"", ""s"", TypeError)]\n\nVALID_CASES = [(""no_axis"", None), (""one_axis"", 1), (""many_axis"", [0, 1])]\n\n\nclass TestFlipd(NumpyImageTestCase2D):\n    @parameterized.expand(INVALID_CASES)\n    def test_invalid_cases(self, _, spatial_axis, raises):\n        with self.assertRaises(raises):\n            flip = Flipd(keys=""img"", spatial_axis=spatial_axis)\n            flip({""img"": self.imt[0]})\n\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, _, spatial_axis):\n        flip = Flipd(keys=""img"", spatial_axis=spatial_axis)\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.flip(channel, spatial_axis))\n        expected = np.stack(expected)\n        res = flip({""img"": self.imt[0]})\n        assert np.allclose(expected, res[""img""])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_focal_loss.py,17,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom monai.losses import FocalLoss\n\n\nclass TestFocalLoss(unittest.TestCase):\n    def test_consistency_with_cross_entropy_2d(self):\n        # For gamma=0 the focal loss reduces to the cross entropy loss\n        focal_loss = FocalLoss(gamma=0.0, reduction=""mean"")\n        ce = nn.CrossEntropyLoss(reduction=""mean"")\n        max_error = 0\n        class_num = 10\n        batch_size = 128\n        for _ in range(100):\n            # Create a random tensor of shape (batch_size, class_num, 8, 4)\n            x = torch.rand(batch_size, class_num, 8, 4, requires_grad=True)\n            # Create a random batch of classes\n            l = torch.randint(low=0, high=class_num, size=(batch_size, 1, 8, 4))\n            if torch.cuda.is_available():\n                x = x.cuda()\n                l = l.cuda()\n            output0 = focal_loss.forward(x, l)\n            output1 = ce.forward(x, l[:, 0])\n            a = float(output0.cpu().detach())\n            b = float(output1.cpu().detach())\n            if abs(a - b) > max_error:\n                max_error = abs(a - b)\n        self.assertAlmostEqual(max_error, 0.0, places=3)\n\n    def test_consistency_with_cross_entropy_classification(self):\n        # for gamma=0 the focal loss reduces to the cross entropy loss\n        focal_loss = FocalLoss(gamma=0.0, reduction=""mean"")\n        ce = nn.CrossEntropyLoss(reduction=""mean"")\n        max_error = 0\n        class_num = 10\n        batch_size = 128\n        for _ in range(100):\n            # Create a random scores tensor of shape (batch_size, class_num)\n            x = torch.rand(batch_size, class_num, requires_grad=True)\n            # Create a random batch of classes\n            l = torch.randint(low=0, high=class_num, size=(batch_size, 1))\n            l = l.long()\n            if torch.cuda.is_available():\n                x = x.cuda()\n                l = l.cuda()\n            output0 = focal_loss.forward(x, l)\n            output1 = ce.forward(x, l[:, 0])\n            a = float(output0.cpu().detach())\n            b = float(output1.cpu().detach())\n            if abs(a - b) > max_error:\n                max_error = abs(a - b)\n        self.assertAlmostEqual(max_error, 0.0, places=3)\n\n    def test_bin_seg_2d(self):\n        # define 2d examples\n        target = torch.tensor([[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]])\n        # add another dimension corresponding to the batch (batch size = 1 here)\n        target = target.unsqueeze(0)  # shape (1, H, W)\n        pred_very_good = 1000 * F.one_hot(target, num_classes=2).permute(0, 3, 1, 2).float()\n\n        # initialize the mean dice loss\n        loss = FocalLoss()\n\n        # focal loss for pred_very_good should be close to 0\n        target = target.unsqueeze(1)  # shape (1, 1, H, W)\n        focal_loss_good = float(loss.forward(pred_very_good, target).cpu())\n        self.assertAlmostEqual(focal_loss_good, 0.0, places=3)\n\n    def test_empty_class_2d(self):\n        num_classes = 2\n        # define 2d examples\n        target = torch.tensor([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n        # add another dimension corresponding to the batch (batch size = 1 here)\n        target = target.unsqueeze(0)  # shape (1, H, W)\n        pred_very_good = 1000 * F.one_hot(target, num_classes=num_classes).permute(0, 3, 1, 2).float()\n\n        # initialize the mean dice loss\n        loss = FocalLoss()\n\n        # focal loss for pred_very_good should be close to 0\n        target = target.unsqueeze(1)  # shape (1, 1, H, W)\n        focal_loss_good = float(loss.forward(pred_very_good, target).cpu())\n        self.assertAlmostEqual(focal_loss_good, 0.0, places=3)\n\n    def test_multi_class_seg_2d(self):\n        num_classes = 6  # labels 0 to 5\n        # define 2d examples\n        target = torch.tensor([[0, 0, 0, 0], [0, 1, 2, 0], [0, 3, 4, 0], [0, 0, 0, 0]])\n        # add another dimension corresponding to the batch (batch size = 1 here)\n        target = target.unsqueeze(0)  # shape (1, H, W)\n        pred_very_good = 1000 * F.one_hot(target, num_classes=num_classes).permute(0, 3, 1, 2).float()\n\n        # initialize the mean dice loss\n        loss = FocalLoss()\n\n        # focal loss for pred_very_good should be close to 0\n        target = target.unsqueeze(1)  # shape (1, 1, H, W)\n        focal_loss_good = float(loss.forward(pred_very_good, target).cpu())\n        self.assertAlmostEqual(focal_loss_good, 0.0, places=3)\n\n    def test_bin_seg_3d(self):\n        # define 2d examples\n        target = torch.tensor(\n            [\n                # raw 0\n                [[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n                # raw 1\n                [[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n                # raw 2\n                [[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n            ]\n        )\n        # add another dimension corresponding to the batch (batch size = 1 here)\n        target = target.unsqueeze(0)  # shape (1, H, W, D)\n        pred_very_good = 1000 * F.one_hot(target, num_classes=2).permute(0, 4, 1, 2, 3).float()\n\n        # initialize the mean dice loss\n        loss = FocalLoss()\n\n        # focal loss for pred_very_good should be close to 0\n        target = target.unsqueeze(1)  # shape (1, 1, H, W)\n        focal_loss_good = float(loss.forward(pred_very_good, target).cpu())\n        self.assertAlmostEqual(focal_loss_good, 0.0, places=3)\n\n    def test_ill_opts(self):\n        chn_input = torch.ones((1, 2, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertRaisesRegex(ValueError, """"):\n            FocalLoss(reduction=""unknown"")(chn_input, chn_target)\n        with self.assertRaisesRegex(ValueError, """"):\n            FocalLoss(reduction=None)(chn_input, chn_target)\n\n    def test_ill_shape(self):\n        chn_input = torch.ones((1, 2, 3))\n        chn_target = torch.ones((1, 3))\n        with self.assertRaisesRegex(ValueError, """"):\n            FocalLoss(reduction=""mean"")(chn_input, chn_target)\n        chn_target = torch.ones((1, 2, 3))\n        with self.assertRaisesRegex(ValueError, """"):\n            FocalLoss(reduction=""mean"")(chn_input, chn_target)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_gaussian_filter.py,14,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom monai.networks.layers import GaussianFilter\n\n\nclass GaussianFilterTestCase(unittest.TestCase):\n    def test_1d(self):\n        a = torch.ones(1, 8, 10)\n        g = GaussianFilter(1, 3, 3).to(torch.device(""cpu:0""))\n        expected = np.array(\n            [\n                [\n                    [\n                        0.56658804,\n                        0.69108766,\n                        0.79392236,\n                        0.86594427,\n                        0.90267116,\n                        0.9026711,\n                        0.8659443,\n                        0.7939224,\n                        0.6910876,\n                        0.56658804,\n                    ],\n                ]\n            ]\n        )\n        expected = np.tile(expected, (1, 8, 1))\n        np.testing.assert_allclose(g(a).cpu().numpy(), expected, rtol=1e-5)\n\n    def test_2d(self):\n        a = torch.ones(1, 1, 3, 3)\n        g = GaussianFilter(2, 3, 3).to(torch.device(""cpu:0""))\n        expected = np.array(\n            [\n                [\n                    [\n                        [0.13380532, 0.14087981, 0.13380532],\n                        [0.14087981, 0.14832835, 0.14087981],\n                        [0.13380532, 0.14087981, 0.13380532],\n                    ]\n                ]\n            ]\n        )\n\n        np.testing.assert_allclose(g(a).cpu().numpy(), expected, rtol=1e-5)\n        if torch.cuda.is_available():\n            g = GaussianFilter(2, 3, 3).to(torch.device(""cuda:0""))\n            np.testing.assert_allclose(g(a.cuda()).cpu().numpy(), expected, rtol=1e-2)\n\n    def test_3d(self):\n        a = torch.ones(1, 1, 4, 3, 4)\n        g = GaussianFilter(3, 3, 3).to(torch.device(""cpu:0""))\n        expected = np.array(\n            [\n                [\n                    [\n                        [\n                            [0.07294822, 0.08033235, 0.08033235, 0.07294822],\n                            [0.07680509, 0.08457965, 0.08457965, 0.07680509],\n                            [0.07294822, 0.08033235, 0.08033235, 0.07294822],\n                        ],\n                        [\n                            [0.08033235, 0.08846395, 0.08846395, 0.08033235],\n                            [0.08457965, 0.09314119, 0.09314119, 0.08457966],\n                            [0.08033235, 0.08846396, 0.08846396, 0.08033236],\n                        ],\n                        [\n                            [0.08033235, 0.08846395, 0.08846395, 0.08033235],\n                            [0.08457965, 0.09314119, 0.09314119, 0.08457966],\n                            [0.08033235, 0.08846396, 0.08846396, 0.08033236],\n                        ],\n                        [\n                            [0.07294822, 0.08033235, 0.08033235, 0.07294822],\n                            [0.07680509, 0.08457965, 0.08457965, 0.07680509],\n                            [0.07294822, 0.08033235, 0.08033235, 0.07294822],\n                        ],\n                    ]\n                ]\n            ],\n        )\n        np.testing.assert_allclose(g(a).cpu().numpy(), expected, rtol=1e-5)\n\n    def test_3d_sigmas(self):\n        a = torch.ones(1, 1, 4, 3, 2)\n        g = GaussianFilter(3, [3, 2, 1], 3).to(torch.device(""cpu:0""))\n        expected = np.array(\n            [\n                [\n                    [\n                        [[0.1422854, 0.1422854], [0.15806103, 0.15806103], [0.1422854, 0.1422854]],\n                        [[0.15668818, 0.15668817], [0.17406069, 0.17406069], [0.15668818, 0.15668817]],\n                        [[0.15668818, 0.15668817], [0.17406069, 0.17406069], [0.15668818, 0.15668817]],\n                        [[0.1422854, 0.1422854], [0.15806103, 0.15806103], [0.1422854, 0.1422854]],\n                    ]\n                ]\n            ]\n        )\n        np.testing.assert_allclose(g(a).cpu().numpy(), expected, rtol=1e-5)\n        if torch.cuda.is_available():\n            g = GaussianFilter(3, [3, 2, 1], 3).to(torch.device(""cuda:0""))\n            np.testing.assert_allclose(g(a.cuda()).cpu().numpy(), expected, rtol=1e-2)\n\n    def test_wrong_args(self):\n        with self.assertRaisesRegex(ValueError, """"):\n            GaussianFilter(3, [3, 2], 3).to(torch.device(""cpu:0""))\n        GaussianFilter(3, [3, 2, 1], 3).to(torch.device(""cpu:0""))  # test init\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_generalized_dice_loss.py,25,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.losses import GeneralizedDiceLoss\n\nTEST_CASES = [\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.307576,\n    ],\n    [  # shape: (2, 1, 2, 2), (2, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]], [[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 1.0], [1.0, 1.0]]], [[[1.0, 0.0], [1.0, 0.0]]]]),\n            ""smooth"": 1e-4,\n        },\n        0.416597,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": False, ""to_onehot_y"": True},\n        {\n            ""input"": torch.tensor([[[1.0, 1.0, 0.0], [0.0, 0.0, 1.0]], [[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]]),\n            ""target"": torch.tensor([[[0.0, 0.0, 1.0]], [[0.0, 1.0, 0.0]]]),\n            ""smooth"": 0.0,\n        },\n        0.0,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.469964,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.414507,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True, ""reduction"": ""sum""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.829015,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True, ""reduction"": ""none""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        [0.273476, 0.555539],\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": False, ""to_onehot_y"": True},\n        {\n            ""input"": torch.tensor([[[1.0, 1.0, 0.0], [0.0, 0.0, 1.0]], [[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]]),\n            ""target"": torch.tensor([[[0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0]]]),\n            ""smooth"": 1e-8,\n        },\n        0.0,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.307576,\n    ],\n    [  # shape: (1, 2, 4), (1, 1, 4)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True, ""w_type"": ""simple""},\n        {\n            ""input"": torch.tensor([[[0.0, 10.0, 10.0, 10.0], [10.0, 0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1, 1, 0, 0]]]),\n            ""smooth"": 0.0,\n        },\n        0.250023,\n    ],\n]\n\n\nclass TestGeneralizedDiceLoss(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_shape(self, input_param, input_data, expected_val):\n        result = GeneralizedDiceLoss(**input_param).forward(**input_data)\n        np.testing.assert_allclose(result.detach().cpu().numpy(), expected_val, rtol=1e-5)\n\n    def test_ill_shape(self):\n        loss = GeneralizedDiceLoss()\n        with self.assertRaisesRegex(AssertionError, """"):\n            loss.forward(torch.ones((1, 2, 3)), torch.ones((4, 5, 6)))\n\n    def test_ill_opts(self):\n        with self.assertRaisesRegex(ValueError, """"):\n            GeneralizedDiceLoss(sigmoid=True, softmax=True)\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertRaisesRegex(ValueError, """"):\n            GeneralizedDiceLoss(reduction=""unknown"")(chn_input, chn_target)\n        with self.assertRaisesRegex(ValueError, """"):\n            GeneralizedDiceLoss(reduction=None)(chn_input, chn_target)\n\n    def test_input_warnings(self):\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertWarns(Warning):\n            loss = GeneralizedDiceLoss(include_background=False)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = GeneralizedDiceLoss(softmax=True)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = GeneralizedDiceLoss(to_onehot_y=True)\n            loss.forward(chn_input, chn_target)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_generate_pos_neg_label_crop_centers.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import generate_pos_neg_label_crop_centers\n\nTEST_CASE_1 = [\n    {\n        ""label"": np.random.randint(0, 2, size=[3, 3, 3, 3]),\n        ""size"": [2, 2, 2],\n        ""num_samples"": 2,\n        ""pos_ratio"": 1.0,\n        ""image"": None,\n        ""image_threshold"": 0,\n        ""rand_state"": np.random.RandomState(),\n    },\n    list,\n    2,\n    3,\n]\n\n\nclass TestGeneratePosNegLabelCropCenters(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_type_shape(self, input_data, expected_type, expected_count, expected_shape):\n        result = generate_pos_neg_label_crop_centers(**input_data)\n        self.assertIsInstance(result, expected_type)\n        self.assertEqual(len(result), expected_count)\n        self.assertEqual(len(result[0]), expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_generate_spatial_bounding_box.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import generate_spatial_bounding_box\n\nTEST_CASE_1 = [\n    {\n        ""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]]),\n        ""select_fn"": lambda x: x > 0,\n        ""channel_indexes"": None,\n        ""margin"": 0,\n    },\n    ([1, 1], [4, 4]),\n]\n\nTEST_CASE_2 = [\n    {\n        ""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 1, 1, 0], [0, 1, 3, 1, 0], [0, 1, 1, 1, 0], [0, 0, 0, 0, 0]]]),\n        ""select_fn"": lambda x: x > 1,\n        ""channel_indexes"": None,\n        ""margin"": 0,\n    },\n    ([2, 2], [3, 3]),\n]\n\nTEST_CASE_3 = [\n    {\n        ""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]]),\n        ""select_fn"": lambda x: x > 0,\n        ""channel_indexes"": 0,\n        ""margin"": 0,\n    },\n    ([1, 1], [4, 4]),\n]\n\nTEST_CASE_4 = [\n    {\n        ""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]]),\n        ""select_fn"": lambda x: x > 0,\n        ""channel_indexes"": None,\n        ""margin"": 1,\n    },\n    ([0, 0], [4, 5]),\n]\n\n\nclass TestGenerateSpatialBoundingBox(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_value(self, input_data, expected_box):\n        result = generate_spatial_bounding_box(**input_data)\n        self.assertTupleEqual(result, expected_box)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_checkpoint_loader.py,17,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tempfile\nimport shutil\nimport torch\nimport unittest\nfrom ignite.engine import Engine\nimport torch.optim as optim\nfrom monai.handlers import CheckpointSaver, CheckpointLoader\nimport logging\nimport sys\n\n\nclass TestHandlerCheckpointLoader(unittest.TestCase):\n    def test_one_save_one_load(self):\n        logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n        net1 = torch.nn.PReLU()\n        data1 = net1.state_dict()\n        data1[""weight""] = torch.tensor([0.1])\n        net1.load_state_dict(data1)\n        net2 = torch.nn.PReLU()\n        data2 = net2.state_dict()\n        data2[""weight""] = torch.tensor([0.2])\n        net2.load_state_dict(data2)\n        engine = Engine(lambda e, b: None)\n        tempdir = tempfile.mkdtemp()\n        CheckpointSaver(save_dir=tempdir, save_dict={""net"": net1}, save_final=True).attach(engine)\n        engine.run([0] * 8, max_epochs=5)\n        path = tempdir + ""/net_final_iteration=40.pth""\n        CheckpointLoader(load_path=path, load_dict={""net"": net2}).attach(engine)\n        engine.run([0] * 8, max_epochs=1)\n        torch.testing.assert_allclose(net2.state_dict()[""weight""], 0.1)\n        shutil.rmtree(tempdir)\n\n    def test_two_save_one_load(self):\n        logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n        net1 = torch.nn.PReLU()\n        optimizer = optim.SGD(net1.parameters(), lr=0.02)\n        data1 = net1.state_dict()\n        data1[""weight""] = torch.tensor([0.1])\n        net1.load_state_dict(data1)\n        net2 = torch.nn.PReLU()\n        data2 = net2.state_dict()\n        data2[""weight""] = torch.tensor([0.2])\n        net2.load_state_dict(data2)\n        engine = Engine(lambda e, b: None)\n        tempdir = tempfile.mkdtemp()\n        save_dict = {""net"": net1, ""opt"": optimizer}\n        CheckpointSaver(save_dir=tempdir, save_dict=save_dict, save_final=True).attach(engine)\n        engine.run([0] * 8, max_epochs=5)\n        path = tempdir + ""/checkpoint_final_iteration=40.pth""\n        CheckpointLoader(load_path=path, load_dict={""net"": net2}).attach(engine)\n        engine.run([0] * 8, max_epochs=1)\n        torch.testing.assert_allclose(net2.state_dict()[""weight""], 0.1)\n        shutil.rmtree(tempdir)\n\n    def test_save_single_device_load_multi_devices(self):\n        logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n        net1 = torch.nn.PReLU()\n        data1 = net1.state_dict()\n        data1[""weight""] = torch.tensor([0.1])\n        net1.load_state_dict(data1)\n        net2 = torch.nn.PReLU()\n        data2 = net2.state_dict()\n        data2[""weight""] = torch.tensor([0.2])\n        net2.load_state_dict(data2)\n        net2 = torch.nn.DataParallel(net2)\n        engine = Engine(lambda e, b: None)\n        tempdir = tempfile.mkdtemp()\n        CheckpointSaver(save_dir=tempdir, save_dict={""net"": net1}, save_final=True).attach(engine)\n        engine.run([0] * 8, max_epochs=5)\n        path = tempdir + ""/net_final_iteration=40.pth""\n        CheckpointLoader(load_path=path, load_dict={""net"": net2}).attach(engine)\n        engine.run([0] * 8, max_epochs=1)\n        torch.testing.assert_allclose(net2.state_dict()[""module.weight""], 0.1)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_checkpoint_saver.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport tempfile\nimport shutil\nimport torch\nimport unittest\nfrom ignite.engine import Engine\nfrom monai.handlers import CheckpointSaver\nimport torch.optim as optim\nfrom parameterized import parameterized\nimport logging\nimport sys\n\nTEST_CASE_1 = [True, False, None, 1, True, 0, None, [""test_checkpoint_final_iteration=40.pth""]]\n\nTEST_CASE_2 = [\n    False,\n    True,\n    ""val_loss"",\n    2,\n    True,\n    0,\n    None,\n    [""test_checkpoint_key_metric=32.pth"", ""test_checkpoint_key_metric=40.pth""],\n]\n\nTEST_CASE_3 = [False, False, None, 1, True, 2, 2, [""test_checkpoint_epoch=2.pth"", ""test_checkpoint_epoch=4.pth""]]\n\nTEST_CASE_4 = [\n    False,\n    False,\n    None,\n    1,\n    False,\n    10,\n    2,\n    [""test_checkpoint_iteration=30.pth"", ""test_checkpoint_iteration=40.pth""],\n]\n\nTEST_CASE_5 = [True, False, None, 1, True, 0, None, [""test_checkpoint_final_iteration=40.pth""], True]\n\n\nclass TestHandlerCheckpointSaver(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5])\n    def test_file(\n        self,\n        save_final,\n        save_key_metric,\n        key_metric_name,\n        key_metric_n_saved,\n        epoch_level,\n        save_interval,\n        n_saved,\n        filenames,\n        multi_devices=False,\n    ):\n        logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n        data = [0] * 8\n\n        # set up engine\n        def _train_func(engine, batch):\n            engine.state.metrics[""val_loss""] = engine.state.iteration\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        net = torch.nn.PReLU()\n        if multi_devices:\n            net = torch.nn.DataParallel(net)\n        optimizer = optim.SGD(net.parameters(), lr=0.02)\n        tempdir = tempfile.mkdtemp()\n        handler = CheckpointSaver(\n            tempdir,\n            {""net"": net, ""opt"": optimizer},\n            ""CheckpointSaver"",\n            ""test"",\n            save_final,\n            save_key_metric,\n            key_metric_name,\n            key_metric_n_saved,\n            epoch_level,\n            save_interval,\n            n_saved,\n        )\n        handler.attach(engine)\n        engine.run(data, max_epochs=5)\n        for filename in filenames:\n            self.assertTrue(os.path.exists(os.path.join(tempdir, filename)))\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_classification_saver.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport csv\nimport shutil\nimport unittest\nimport numpy as np\nimport torch\nfrom ignite.engine import Engine\n\nfrom monai.handlers import ClassificationSaver\n\n\nclass TestHandlerClassificationSaver(unittest.TestCase):\n    def test_saved_content(self):\n        default_dir = os.path.join(""."", ""tempdir"")\n        shutil.rmtree(default_dir, ignore_errors=True)\n\n        # set up engine\n        def _train_func(engine, batch):\n            return torch.zeros(8)\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        saver = ClassificationSaver(output_dir=default_dir, filename=""predictions.csv"")\n        saver.attach(engine)\n\n        data = [{""filename_or_obj"": [""testfile"" + str(i) for i in range(8)]}]\n        engine.run(data, max_epochs=1)\n        filepath = os.path.join(default_dir, ""predictions.csv"")\n        self.assertTrue(os.path.exists(filepath))\n        with open(filepath, ""r"") as f:\n            reader = csv.reader(f)\n            i = 0\n            for row in reader:\n                self.assertEqual(row[0], ""testfile"" + str(i))\n                self.assertEqual(np.array(row[1:]).astype(np.float32), 0.0)\n                i += 1\n            self.assertEqual(i, 8)\n        shutil.rmtree(default_dir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_lr_scheduler.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport unittest\nimport numpy as np\nfrom ignite.engine import Engine, Events\nfrom monai.handlers import LrScheduleHandler\nimport logging\nimport sys\n\n\nclass TestHandlerLrSchedule(unittest.TestCase):\n    def test_content(self):\n        logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n        data = [0] * 8\n\n        # set up engine\n        def _train_func(engine, batch):\n            pass\n\n        val_engine = Engine(_train_func)\n        train_engine = Engine(_train_func)\n\n        @train_engine.on(Events.EPOCH_COMPLETED)\n        def run_validation(engine):\n            val_engine.run(data)\n            val_engine.state.metrics[""val_loss""] = 1\n\n        # set up testing handler\n        net = torch.nn.PReLU()\n\n        def _reduce_lr_on_plateau():\n            optimizer = torch.optim.SGD(net.parameters(), 0.1)\n            lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n            handler = LrScheduleHandler(lr_scheduler, step_transform=lambda x: val_engine.state.metrics[""val_loss""])\n            handler.attach(train_engine)\n            return lr_scheduler\n\n        def _reduce_on_step():\n            optimizer = torch.optim.SGD(net.parameters(), 0.1)\n            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n            handler = LrScheduleHandler(lr_scheduler)\n            handler.attach(train_engine)\n            return lr_scheduler\n\n        schedulers = _reduce_lr_on_plateau(), _reduce_on_step()\n\n        train_engine.run(data, max_epochs=5)\n        for scheduler in schedulers:\n            np.testing.assert_allclose(scheduler._last_lr[0], 0.001)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_mean_dice.py,10,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.handlers import MeanDice\n\nTEST_CASE_1 = [{""to_onehot_y"": True, ""mutually_exclusive"": True}, 0.75]\nTEST_CASE_2 = [{""include_background"": False, ""to_onehot_y"": True, ""mutually_exclusive"": False}, 0.66666]\nTEST_CASE_3 = [{""mutually_exclusive"": True, ""sigmoid"": True}]\n\n\nclass TestHandlerMeanDice(unittest.TestCase):\n    # TODO test multi node averaged dice\n\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_compute(self, input_params, expected_avg):\n        dice_metric = MeanDice(**input_params)\n\n        y_pred = torch.Tensor([[[0], [1]], [[1], [0]]])\n        y = torch.ones((2, 1, 1))\n        dice_metric.update([y_pred, y])\n\n        y_pred = torch.Tensor([[[0], [1]], [[1], [0]]])\n        y = torch.Tensor([[[1]], [[0]]])\n        dice_metric.update([y_pred, y])\n\n        avg_dice = dice_metric.compute()\n        self.assertAlmostEqual(avg_dice, expected_avg, places=4)\n\n    @parameterized.expand([TEST_CASE_3])\n    def test_misconfig(self, input_params):\n        with self.assertRaisesRegex(ValueError, ""compatib""):\n            dice_metric = MeanDice(**input_params)\n\n            y_pred = torch.Tensor([[0, 1], [1, 0]])\n            y = torch.ones((2, 1))\n            dice_metric.update([y_pred, y])\n\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_shape_mismatch(self, input_params, _expected):\n        dice_metric = MeanDice(**input_params)\n        with self.assertRaises((AssertionError, ValueError)):\n            y_pred = torch.Tensor([[0, 1], [1, 0]])\n            y = torch.ones((2, 3))\n            dice_metric.update([y_pred, y])\n\n        with self.assertRaises((AssertionError, ValueError)):\n            y_pred = torch.Tensor([[0, 1], [1, 0]])\n            y = torch.ones((3, 2))\n            dice_metric.update([y_pred, y])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_rocauc.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nimport torch\n\nfrom monai.handlers import ROCAUC\n\n\nclass TestHandlerROCAUC(unittest.TestCase):\n    def test_compute(self):\n        auc_metric = ROCAUC(to_onehot_y=True, softmax=True)\n\n        y_pred = torch.Tensor([[0.1, 0.9], [0.3, 1.4]])\n        y = torch.Tensor([[0], [1]])\n        auc_metric.update([y_pred, y])\n\n        y_pred = torch.Tensor([[0.2, 0.1], [0.1, 0.5]])\n        y = torch.Tensor([[0], [1]])\n        auc_metric.update([y_pred, y])\n\n        auc = auc_metric.compute()\n        np.testing.assert_allclose(0.75, auc)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_segmentation_saver.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport unittest\nimport torch\nfrom ignite.engine import Engine\nfrom parameterized import parameterized\nfrom monai.handlers import SegmentationSaver\n\nTEST_CASE_1 = ["".nii.gz""]\n\nTEST_CASE_1 = ["".png""]\n\n\nclass TestHandlerSegmentationSaver(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_1])\n    def test_saved_content(self, output_ext):\n        default_dir = os.path.join(""."", ""tempdir"")\n        shutil.rmtree(default_dir, ignore_errors=True)\n\n        # set up engine\n        def _train_func(engine, batch):\n            return torch.zeros(8, 1, 2, 2)\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        saver = SegmentationSaver(output_dir=default_dir, output_postfix=""seg"", output_ext=output_ext)\n        saver.attach(engine)\n\n        data = [{""filename_or_obj"": [""testfile"" + str(i) for i in range(8)]}]\n        engine.run(data, max_epochs=1)\n        for i in range(8):\n            filepath = os.path.join(""testfile"" + str(i), ""testfile"" + str(i) + ""_seg"" + output_ext)\n            self.assertTrue(os.path.exists(os.path.join(default_dir, filepath)))\n        shutil.rmtree(default_dir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_stats.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport os\nimport shutil\nimport logging\nimport tempfile\nimport re\nimport unittest\nfrom io import StringIO\n\nfrom ignite.engine import Engine, Events\n\nfrom monai.handlers import StatsHandler\n\n\nclass TestHandlerStats(unittest.TestCase):\n    def test_metrics_print(self):\n        log_stream = StringIO()\n        logging.basicConfig(stream=log_stream, level=logging.INFO)\n        key_to_handler = ""test_logging""\n        key_to_print = ""testing_metric""\n\n        # set up engine\n        def _train_func(engine, batch):\n            return torch.tensor(0.0)\n\n        engine = Engine(_train_func)\n\n        # set up dummy metric\n        @engine.on(Events.EPOCH_COMPLETED)\n        def _update_metric(engine):\n            current_metric = engine.state.metrics.get(key_to_print, 0.1)\n            engine.state.metrics[key_to_print] = current_metric + 0.1\n\n        # set up testing handler\n        stats_handler = StatsHandler(name=key_to_handler)\n        stats_handler.attach(engine)\n\n        engine.run(range(3), max_epochs=2)\n\n        # check logging output\n        output_str = log_stream.getvalue()\n        grep = re.compile(f"".*{key_to_handler}.*"")\n        has_key_word = re.compile(f"".*{key_to_print}.*"")\n        for idx, line in enumerate(output_str.split(""\\n"")):\n            if grep.match(line):\n                if idx in [5, 10]:\n                    self.assertTrue(has_key_word.match(line))\n\n    def test_loss_print(self):\n        log_stream = StringIO()\n        logging.basicConfig(stream=log_stream, level=logging.INFO)\n        key_to_handler = ""test_logging""\n        key_to_print = ""myLoss""\n\n        # set up engine\n        def _train_func(engine, batch):\n            return torch.tensor(0.0)\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        stats_handler = StatsHandler(name=key_to_handler, tag_name=key_to_print)\n        stats_handler.attach(engine)\n\n        engine.run(range(3), max_epochs=2)\n\n        # check logging output\n        output_str = log_stream.getvalue()\n        grep = re.compile(f"".*{key_to_handler}.*"")\n        has_key_word = re.compile(f"".*{key_to_print}.*"")\n        for idx, line in enumerate(output_str.split(""\\n"")):\n            if grep.match(line):\n                if idx in [1, 2, 3, 6, 7, 8]:\n                    self.assertTrue(has_key_word.match(line))\n\n    def test_loss_dict(self):\n        log_stream = StringIO()\n        logging.basicConfig(stream=log_stream, level=logging.INFO)\n        key_to_handler = ""test_logging""\n        key_to_print = ""myLoss1""\n\n        # set up engine\n        def _train_func(engine, batch):\n            return torch.tensor(0.0)\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        stats_handler = StatsHandler(name=key_to_handler, output_transform=lambda x: {key_to_print: x})\n        stats_handler.attach(engine)\n\n        engine.run(range(3), max_epochs=2)\n\n        # check logging output\n        output_str = log_stream.getvalue()\n        grep = re.compile(f"".*{key_to_handler}.*"")\n        has_key_word = re.compile(f"".*{key_to_print}.*"")\n        for idx, line in enumerate(output_str.split(""\\n"")):\n            if grep.match(line):\n                if idx in [1, 2, 3, 6, 7, 8]:\n                    self.assertTrue(has_key_word.match(line))\n\n    def test_loss_file(self):\n        logging.basicConfig(level=logging.INFO)\n        key_to_handler = ""test_logging""\n        key_to_print = ""myLoss""\n\n        tempdir = tempfile.mkdtemp()\n        filename = os.path.join(tempdir, ""test_loss_stats.log"")\n        handler = logging.FileHandler(filename, mode=""w"")\n\n        # set up engine\n        def _train_func(engine, batch):\n            return torch.tensor(0.0)\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        stats_handler = StatsHandler(name=key_to_handler, tag_name=key_to_print, logger_handler=handler)\n        stats_handler.attach(engine)\n\n        engine.run(range(3), max_epochs=2)\n        handler.stream.close()\n        stats_handler.logger.removeHandler(handler)\n        with open(filename, ""r"") as f:\n            output_str = f.read()\n            grep = re.compile(f"".*{key_to_handler}.*"")\n            has_key_word = re.compile(f"".*{key_to_print}.*"")\n            for idx, line in enumerate(output_str.split(""\\n"")):\n                if grep.match(line):\n                    if idx in [1, 2, 3, 6, 7, 8]:\n                        self.assertTrue(has_key_word.match(line))\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_tb_image.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport glob\nimport os\nimport shutil\nimport unittest\nimport tempfile\nimport numpy as np\nimport torch\nfrom ignite.engine import Engine, Events\nfrom parameterized import parameterized\n\nfrom monai.handlers import TensorBoardImageHandler\n\nTEST_CASES = [\n    [[20, 20]],\n    [[2, 20, 20]],\n    [[3, 20, 20]],\n    [[20, 20, 20]],\n    [[2, 20, 20, 20]],\n    [[2, 2, 20, 20, 20]],\n]\n\n\nclass TestHandlerTBImage(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_tb_image_shape(self, shape):\n        tempdir = tempfile.mkdtemp()\n        shutil.rmtree(tempdir, ignore_errors=True)\n\n        # set up engine\n        def _train_func(engine, batch):\n            return torch.zeros((1, 1, 10, 10))\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        stats_handler = TensorBoardImageHandler(log_dir=tempdir)\n        engine.add_event_handler(Events.ITERATION_COMPLETED, stats_handler)\n\n        data = zip(np.random.normal(size=(10, 4, *shape)), np.random.normal(size=(10, 4, *shape)))\n        engine.run(data, epoch_length=10, max_epochs=1)\n\n        self.assertTrue(os.path.exists(tempdir))\n        self.assertTrue(len(glob.glob(tempdir)) > 0)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_tb_stats.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\nimport glob\n\nfrom ignite.engine import Engine, Events\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom monai.handlers import TensorBoardStatsHandler\n\n\nclass TestHandlerTBStats(unittest.TestCase):\n    def test_metrics_print(self):\n        tempdir = tempfile.mkdtemp()\n        shutil.rmtree(tempdir, ignore_errors=True)\n\n        # set up engine\n        def _train_func(engine, batch):\n            return batch + 1.0\n\n        engine = Engine(_train_func)\n\n        # set up dummy metric\n        @engine.on(Events.EPOCH_COMPLETED)\n        def _update_metric(engine):\n            current_metric = engine.state.metrics.get(""acc"", 0.1)\n            engine.state.metrics[""acc""] = current_metric + 0.1\n\n        # set up testing handler\n        stats_handler = TensorBoardStatsHandler(log_dir=tempdir)\n        stats_handler.attach(engine)\n        engine.run(range(3), max_epochs=2)\n        # check logging output\n\n        self.assertTrue(os.path.exists(tempdir))\n        shutil.rmtree(tempdir)\n\n    def test_metrics_writer(self):\n        tempdir = tempfile.mkdtemp()\n        shutil.rmtree(tempdir, ignore_errors=True)\n\n        # set up engine\n        def _train_func(engine, batch):\n            return batch + 1.0\n\n        engine = Engine(_train_func)\n\n        # set up dummy metric\n        @engine.on(Events.EPOCH_COMPLETED)\n        def _update_metric(engine):\n            current_metric = engine.state.metrics.get(""acc"", 0.1)\n            engine.state.metrics[""acc""] = current_metric + 0.1\n\n        # set up testing handler\n        writer = SummaryWriter(log_dir=tempdir)\n        stats_handler = TensorBoardStatsHandler(\n            writer, output_transform=lambda x: {""loss"": x * 2.0}, global_epoch_transform=lambda x: x * 3.0\n        )\n        stats_handler.attach(engine)\n        engine.run(range(3), max_epochs=2)\n        # check logging output\n        self.assertTrue(os.path.exists(tempdir))\n        self.assertTrue(len(glob.glob(tempdir)) > 0)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_handler_validation.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom ignite.engine import Engine\nfrom monai.data import Dataset\nfrom monai.handlers import ValidationHandler\nfrom monai.engines import Evaluator\n\n\nclass TestEvaluator(Evaluator):\n    def _iteration(self, engine, batchdata):\n        pass\n\n\nclass TestHandlerValidation(unittest.TestCase):\n    def test_content(self):\n        data = [0] * 8\n\n        # set up engine\n        def _train_func(engine, batch):\n            pass\n\n        engine = Engine(_train_func)\n\n        # set up testing handler\n        val_data_loader = torch.utils.data.DataLoader(Dataset(data))\n        evaluator = TestEvaluator(torch.device(""cpu:0""), val_data_loader)\n        saver = ValidationHandler(evaluator, interval=2)\n        saver.attach(engine)\n\n        engine.run(data, max_epochs=5)\n        self.assertEqual(evaluator.state.max_epochs, 4)\n        self.assertEqual(evaluator.state.epoch_length, 8)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_header_correct.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport nibabel as nib\nimport numpy as np\n\nfrom monai.data import correct_nifti_header_if_necessary\n\n\nclass TestCorrection(unittest.TestCase):\n    def test_correct(self):\n        test_img = nib.Nifti1Image(np.zeros((1, 2, 3)), np.eye(4))\n        test_img.header.set_zooms((100, 100, 100))\n        test_img = correct_nifti_header_if_necessary(test_img)\n        np.testing.assert_allclose(\n            test_img.affine,\n            np.array([[100.0, 0.0, 0.0, 0.0], [0.0, 100.0, 0.0, 0.0], [0.0, 0.0, 100.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n\n    def test_affine(self):\n        test_img = nib.Nifti1Image(np.zeros((1, 2, 3)), np.eye(4) * 20.0)\n        test_img = correct_nifti_header_if_necessary(test_img)\n        np.testing.assert_allclose(\n            test_img.affine,\n            np.array([[20.0, 0.0, 0.0, 0.0], [0.0, 20.0, 0.0, 0.0], [0.0, 0.0, 20.0, 0.0], [0.0, 0.0, 0.0, 20.0]]),\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_highresnet.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.networks.nets import HighResNet\n\nTEST_CASE_1 = [  # single channel 3D, batch 16\n    {""spatial_dims"": 3, ""in_channels"": 1, ""out_channels"": 3, ""norm_type"": ""instance""},\n    torch.randn(16, 1, 32, 24, 48),\n    (16, 3, 32, 24, 48),\n]\n\nTEST_CASE_2 = [  # 4-channel 3D, batch 1\n    {""spatial_dims"": 3, ""in_channels"": 4, ""out_channels"": 3, ""acti_type"": ""relu6""},\n    torch.randn(1, 4, 17, 64, 48),\n    (1, 3, 17, 64, 48),\n]\n\nTEST_CASE_3 = [  # 4-channel 2D, batch 7\n    {""spatial_dims"": 2, ""in_channels"": 4, ""out_channels"": 3},\n    torch.randn(7, 4, 64, 48),\n    (7, 3, 64, 48),\n]\n\nTEST_CASE_4 = [  # 4-channel 1D, batch 16\n    {""spatial_dims"": 1, ""in_channels"": 4, ""out_channels"": 3, ""dropout_prob"": 0.1},\n    torch.randn(16, 4, 63),\n    (16, 3, 63),\n]\n\n\nclass TestHighResNet(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_shape(self, input_param, input_data, expected_shape):\n        net = HighResNet(**input_param)\n        net.eval()\n        with torch.no_grad():\n            result = net.forward(input_data)\n            self.assertEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_img2tensorboard.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tempfile\nimport unittest\n\nfrom monai.visualize import make_animated_gif_summary\nimport numpy as np\nimport tensorboard\nimport torch\n\n\nclass TestImg2Tensorboard(unittest.TestCase):\n    def test_write_gray(self):\n        with tempfile.TemporaryDirectory() as out_dir:\n            nparr = np.ones(shape=(1, 32, 32, 32), dtype=np.float32)\n            summary_object_np = make_animated_gif_summary(\n                tag=""test_summary_nparr.png"",\n                image=nparr,\n                max_out=1,\n                animation_axes=(3,),\n                image_axes=(1, 2),\n                scale_factor=253.0,\n            )\n            assert isinstance(\n                summary_object_np, tensorboard.compat.proto.summary_pb2.Summary\n            ), ""make_animated_gif_summary must return a tensorboard.summary object from numpy array""\n\n            tensorarr = torch.tensor(nparr)\n            summary_object_tensor = make_animated_gif_summary(\n                tag=""test_summary_tensorarr.png"",\n                image=tensorarr,\n                max_out=1,\n                animation_axes=(3,),\n                image_axes=(1, 2),\n                scale_factor=253.0,\n            )\n            assert isinstance(\n                summary_object_tensor, tensorboard.compat.proto.summary_pb2.Summary\n            ), ""make_animated_gif_summary must return a tensorboard.summary object from tensor input""\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_integration_classification_2d.py,17,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport subprocess\nimport tarfile\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.metrics import compute_roc_auc\nfrom monai.networks.nets import densenet121\nfrom monai.transforms import (\n    AddChannel,\n    Compose,\n    LoadPNG,\n    RandFlip,\n    RandRotate,\n    RandZoom,\n    ScaleIntensity,\n    ToTensor,\n)\nfrom monai.utils import set_determinism\nfrom tests.utils import skip_if_quick\n\nTEST_DATA_URL = ""https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz""\n\n\nclass MedNISTDataset(torch.utils.data.Dataset):\n    def __init__(self, image_files, labels, transforms):\n        self.image_files = image_files\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, index):\n        return self.transforms(self.image_files[index]), self.labels[index]\n\n\ndef run_training_test(root_dir, train_x, train_y, val_x, val_y, device=torch.device(""cuda:0"")):\n\n    monai.config.print_config()\n    # define transforms for image and classification\n    train_transforms = Compose(\n        [\n            LoadPNG(image_only=True),\n            AddChannel(),\n            ScaleIntensity(),\n            RandRotate(degrees=15, prob=0.5, reshape=False),\n            RandFlip(spatial_axis=0, prob=0.5),\n            RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n            ToTensor(),\n        ]\n    )\n    train_transforms.set_random_state(1234)\n    val_transforms = Compose([LoadPNG(image_only=True), AddChannel(), ScaleIntensity(), ToTensor()])\n\n    # create train, val data loaders\n    train_ds = MedNISTDataset(train_x, train_y, train_transforms)\n    train_loader = DataLoader(train_ds, batch_size=300, shuffle=True, num_workers=10)\n\n    val_ds = MedNISTDataset(val_x, val_y, val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=300, num_workers=10)\n\n    model = densenet121(spatial_dims=2, in_channels=1, out_channels=len(np.unique(train_y)),).to(device)\n    loss_function = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n    epoch_num = 4\n    val_interval = 1\n\n    # start training validation\n    best_metric = -1\n    best_metric_epoch = -1\n    epoch_loss_values = list()\n    metric_values = list()\n    model_filename = os.path.join(root_dir, ""best_metric_model.pth"")\n    for epoch in range(epoch_num):\n        print(""-"" * 10)\n        print(f""Epoch {epoch + 1}/{epoch_num}"")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        epoch_loss /= step\n        epoch_loss_values.append(epoch_loss)\n        print(f""epoch {epoch + 1} average loss:{epoch_loss:0.4f}"")\n\n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n                y = torch.tensor([], dtype=torch.long, device=device)\n                for val_data in val_loader:\n                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n                    y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n                    y = torch.cat([y, val_labels], dim=0)\n                auc_metric = compute_roc_auc(y_pred, y, to_onehot_y=True, softmax=True)\n                metric_values.append(auc_metric)\n                acc_value = torch.eq(y_pred.argmax(dim=1), y)\n                acc_metric = acc_value.sum().item() / len(acc_value)\n                if auc_metric > best_metric:\n                    best_metric = auc_metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), model_filename)\n                    print(""saved new best metric model"")\n                print(\n                    f""current epoch {epoch +1} current AUC: {auc_metric:0.4f} ""\n                    f""current accuracy: {acc_metric:0.4f} best AUC: {best_metric:0.4f} at epoch {best_metric_epoch}""\n                )\n    print(f""train completed, best_metric: {best_metric:0.4f}  at epoch: {best_metric_epoch}"")\n    return epoch_loss_values, best_metric, best_metric_epoch\n\n\ndef run_inference_test(root_dir, test_x, test_y, device=torch.device(""cuda:0"")):\n    # define transforms for image and classification\n    val_transforms = Compose([LoadPNG(image_only=True), AddChannel(), ScaleIntensity(), ToTensor()])\n    val_ds = MedNISTDataset(test_x, test_y, val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=300, num_workers=10)\n\n    model = densenet121(spatial_dims=2, in_channels=1, out_channels=len(np.unique(test_y)),).to(device)\n\n    model_filename = os.path.join(root_dir, ""best_metric_model.pth"")\n    model.load_state_dict(torch.load(model_filename))\n    model.eval()\n    y_true = list()\n    y_pred = list()\n    with torch.no_grad():\n        for test_data in val_loader:\n            test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n            pred = model(test_images).argmax(dim=1)\n            for i in range(len(pred)):\n                y_true.append(test_labels[i].item())\n                y_pred.append(pred[i].item())\n    tps = [np.sum((np.asarray(y_true) == idx) & (np.asarray(y_pred) == idx)) for idx in np.unique(test_y)]\n    return tps\n\n\nclass IntegrationClassification2D(unittest.TestCase):\n    def setUp(self):\n        set_determinism(seed=0)\n        self.data_dir = tempfile.mkdtemp()\n\n        # download\n        subprocess.call([""wget"", ""-nv"", ""-P"", self.data_dir, TEST_DATA_URL])\n        dataset_file = os.path.join(self.data_dir, ""MedNIST.tar.gz"")\n        assert os.path.exists(dataset_file)\n\n        # extract tarfile\n        datafile = tarfile.open(dataset_file)\n        datafile.extractall(path=self.data_dir)\n        datafile.close()\n\n        # find image files and labels\n        data_dir = os.path.join(self.data_dir, ""MedNIST"")\n        class_names = sorted([x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, x))])\n        image_files = [\n            [os.path.join(data_dir, class_name, x) for x in sorted(os.listdir(os.path.join(data_dir, class_name)))]\n            for class_name in class_names\n        ]\n        image_file_list, image_classes = [], []\n        for i, class_name in enumerate(class_names):\n            image_file_list.extend(image_files[i])\n            image_classes.extend([i] * len(image_files[i]))\n\n        # split train, val, test\n        valid_frac, test_frac = 0.1, 0.1\n        self.train_x, self.train_y = [], []\n        self.val_x, self.val_y = [], []\n        self.test_x, self.test_y = [], []\n        for i in range(len(image_classes)):\n            rann = np.random.random()\n            if rann < valid_frac:\n                self.val_x.append(image_file_list[i])\n                self.val_y.append(image_classes[i])\n            elif rann < test_frac + valid_frac:\n                self.test_x.append(image_file_list[i])\n                self.test_y.append(image_classes[i])\n            else:\n                self.train_x.append(image_file_list[i])\n                self.train_y.append(image_classes[i])\n\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0"")\n\n    def tearDown(self):\n        set_determinism(seed=None)\n        shutil.rmtree(self.data_dir)\n\n    @skip_if_quick\n    def test_training(self):\n        repeated = []\n        for i in range(2):\n            torch.manual_seed(0)\n\n            repeated.append([])\n            losses, best_metric, best_metric_epoch = run_training_test(\n                self.data_dir, self.train_x, self.train_y, self.val_x, self.val_y, device=self.device\n            )\n\n            # check training properties\n            np.testing.assert_allclose(\n                losses, [0.8009556981788319, 0.16794362251356149, 0.07708252014912617, 0.04769148252856959], rtol=1e-3\n            )\n            repeated[i].extend(losses)\n            print(""best metric"", best_metric)\n            np.testing.assert_allclose(best_metric, 0.9999460507942981, rtol=1e-4)\n            repeated[i].append(best_metric)\n            np.testing.assert_allclose(best_metric_epoch, 4)\n            model_file = os.path.join(self.data_dir, ""best_metric_model.pth"")\n            self.assertTrue(os.path.exists(model_file))\n\n            infer_metric = run_inference_test(self.data_dir, self.test_x, self.test_y, device=self.device)\n\n            # check inference properties\n            np.testing.assert_allclose(np.asarray(infer_metric), [1034, 895, 982, 1033, 961, 1047], atol=1)\n            repeated[i].extend(infer_metric)\n\n        np.testing.assert_allclose(repeated[0], repeated[1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_integration_determinism.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom monai.data import create_test_image_2d\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets import UNet\nfrom monai.transforms import Compose, AddChannel, RandRotate90, RandSpatialCrop, ScaleIntensity, ToTensor\nfrom monai.utils import set_determinism\n\n\ndef run_test(batch_size=64, train_steps=200, device=torch.device(""cuda:0"")):\n    class _TestBatch(Dataset):\n        def __init__(self, transforms):\n            self.transforms = transforms\n\n        def __getitem__(self, _unused_id):\n            im, seg = create_test_image_2d(128, 128, noise_max=1, num_objs=4, num_seg_classes=1)\n            seed = np.random.randint(2147483647)\n            self.transforms.set_random_state(seed=seed)\n            im = self.transforms(im)\n            self.transforms.set_random_state(seed=seed)\n            seg = self.transforms(seg)\n            return im, seg\n\n        def __len__(self):\n            return train_steps\n\n    net = UNet(\n        dimensions=2, in_channels=1, out_channels=1, channels=(4, 8, 16, 32), strides=(2, 2, 2), num_res_units=2,\n    ).to(device)\n\n    loss = DiceLoss(sigmoid=True)\n    opt = torch.optim.Adam(net.parameters(), 1e-2)\n    train_transforms = Compose(\n        [AddChannel(), ScaleIntensity(), RandSpatialCrop((96, 96), random_size=False), RandRotate90(), ToTensor()]\n    )\n\n    src = DataLoader(_TestBatch(train_transforms), batch_size=batch_size, shuffle=True)\n\n    net.train()\n    epoch_loss = 0\n    step = 0\n    for img, seg in src:\n        step += 1\n        opt.zero_grad()\n        output = net(img.to(device))\n        step_loss = loss(output, seg.to(device))\n        step_loss.backward()\n        opt.step()\n        epoch_loss += step_loss.item()\n    epoch_loss /= step\n\n    return epoch_loss, step\n\n\nclass TestDeterminism(unittest.TestCase):\n    def setUp(self):\n        set_determinism(seed=0)\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0"")\n\n    def tearDown(self):\n        set_determinism(seed=None)\n\n    def test_training(self):\n        loss, step = run_test(device=self.device)\n        print(f""Deterministic loss {loss} at training step {step}"")\n        np.testing.assert_allclose(step, 4)\n        np.testing.assert_allclose(loss, 0.5346279, rtol=1e-6)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_integration_segmentation_3d.py,14,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\nfrom glob import glob\n\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport monai\nfrom monai.data import create_test_image_3d, NiftiSaver, list_data_collate\nfrom monai.inferers import sliding_window_inference\nfrom monai.metrics import compute_meandice\nfrom monai.networks.nets import UNet\nfrom monai.transforms import (\n    Compose,\n    AsChannelFirstd,\n    LoadNiftid,\n    RandCropByPosNegLabeld,\n    RandRotate90d,\n    ScaleIntensityd,\n    Spacingd,\n    ToTensord,\n)\nfrom monai.visualize import plot_2d_or_3d_image\nfrom monai.utils import set_determinism\nfrom tests.utils import skip_if_quick\n\n\ndef run_training_test(root_dir, device=torch.device(""cuda:0""), cachedataset=False):\n    monai.config.print_config()\n    images = sorted(glob(os.path.join(root_dir, ""img*.nii.gz"")))\n    segs = sorted(glob(os.path.join(root_dir, ""seg*.nii.gz"")))\n    train_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images[:20], segs[:20])]\n    val_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images[-20:], segs[-20:])]\n\n    # define transforms for image and segmentation\n    train_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            Spacingd(keys=[""img"", ""seg""], pixdim=[1.2, 0.8, 0.7], interp_order=[""bilinear"", ""nearest""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            RandCropByPosNegLabeld(\n                keys=[""img"", ""seg""], label_key=""seg"", size=[96, 96, 96], pos=1, neg=1, num_samples=4\n            ),\n            RandRotate90d(keys=[""img"", ""seg""], prob=0.8, spatial_axes=[0, 2]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n    train_transforms.set_random_state(1234)\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            Spacingd(keys=[""img"", ""seg""], pixdim=[1.2, 0.8, 0.7], interp_order=[""bilinear"", ""nearest""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n\n    # create a training data loader\n    if cachedataset:\n        train_ds = monai.data.CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.8)\n    else:\n        train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=2,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=list_data_collate,\n        pin_memory=torch.cuda.is_available(),\n    )\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = DataLoader(\n        val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n\n    # create UNet, DiceLoss and Adam optimizer\n    model = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n    loss_function = monai.losses.DiceLoss(sigmoid=True)\n    optimizer = torch.optim.Adam(model.parameters(), 5e-4)\n\n    # start a typical PyTorch training\n    val_interval = 2\n    best_metric, best_metric_epoch = -1, -1\n    epoch_loss_values = list()\n    metric_values = list()\n    writer = SummaryWriter(log_dir=os.path.join(root_dir, ""runs""))\n    model_filename = os.path.join(root_dir, ""best_metric_model.pth"")\n    for epoch in range(6):\n        print(""-"" * 10)\n        print(f""Epoch {epoch + 1}/{6}"")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = batch_data[""img""].to(device), batch_data[""seg""].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            epoch_len = len(train_ds) // train_loader.batch_size\n            print(f""{step}/{epoch_len}, train_loss:{loss.item():0.4f}"")\n            writer.add_scalar(""train_loss"", loss.item(), epoch_len * epoch + step)\n        epoch_loss /= step\n        epoch_loss_values.append(epoch_loss)\n        print(f""epoch {epoch +1} average loss:{epoch_loss:0.4f}"")\n\n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                metric_sum = 0.0\n                metric_count = 0\n                val_images = None\n                val_labels = None\n                val_outputs = None\n                for val_data in val_loader:\n                    val_images, val_labels = val_data[""img""].to(device), val_data[""seg""].to(device)\n                    sw_batch_size, roi_size = 4, (96, 96, 96)\n                    val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n                    value = compute_meandice(\n                        y_pred=val_outputs, y=val_labels, include_background=True, to_onehot_y=False, sigmoid=True\n                    )\n                    metric_count += len(value)\n                    metric_sum += value.sum().item()\n                metric = metric_sum / metric_count\n                metric_values.append(metric)\n                if metric > best_metric:\n                    best_metric = metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), model_filename)\n                    print(""saved new best metric model"")\n                print(\n                    f""current epoch {epoch +1} current mean dice: {metric:0.4f} ""\n                    f""best mean dice: {best_metric:0.4f} at epoch {best_metric_epoch}""\n                )\n                writer.add_scalar(""val_mean_dice"", metric, epoch + 1)\n                # plot the last model output as GIF image in TensorBoard with the corresponding image and label\n                plot_2d_or_3d_image(val_images, epoch + 1, writer, index=0, tag=""image"")\n                plot_2d_or_3d_image(val_labels, epoch + 1, writer, index=0, tag=""label"")\n                plot_2d_or_3d_image(val_outputs, epoch + 1, writer, index=0, tag=""output"")\n    print(f""train completed, best_metric: {best_metric:0.4f}  at epoch: {best_metric_epoch}"")\n    writer.close()\n    return epoch_loss_values, best_metric, best_metric_epoch\n\n\ndef run_inference_test(root_dir, device=torch.device(""cuda:0"")):\n    images = sorted(glob(os.path.join(root_dir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(root_dir, ""seg*.nii.gz"")))\n    val_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images, segs)]\n\n    # define transforms for image and segmentation\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            Spacingd(keys=[""img"", ""seg""], pixdim=[1.2, 0.8, 0.7], interp_order=[""bilinear"", ""nearest""]),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    # sliding window inferene need to input 1 image in every iteration\n    val_loader = DataLoader(\n        val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n\n    model = UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n\n    model_filename = os.path.join(root_dir, ""best_metric_model.pth"")\n    model.load_state_dict(torch.load(model_filename))\n    model.eval()\n    with torch.no_grad():\n        metric_sum = 0.0\n        metric_count = 0\n        saver = NiftiSaver(output_dir=os.path.join(root_dir, ""output""), dtype=int)\n        for val_data in val_loader:\n            val_images, val_labels = val_data[""img""].to(device), val_data[""seg""].to(device)\n            # define sliding window size and batch size for windows inference\n            sw_batch_size, roi_size = 4, (96, 96, 96)\n            val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n            value = compute_meandice(\n                y_pred=val_outputs, y=val_labels, include_background=True, to_onehot_y=False, sigmoid=True\n            )\n            metric_count += len(value)\n            metric_sum += value.sum().item()\n            val_outputs = (val_outputs.sigmoid() >= 0.5).float()\n            saver.save_batch(\n                val_outputs, {""filename_or_obj"": val_data[""img.filename_or_obj""], ""affine"": val_data[""img.affine""]}\n            )\n        metric = metric_sum / metric_count\n    return metric\n\n\nclass IntegrationSegmentation3D(unittest.TestCase):\n    def setUp(self):\n        set_determinism(seed=0)\n\n        self.data_dir = tempfile.mkdtemp()\n        for i in range(40):\n            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n            n = nib.Nifti1Image(im, np.eye(4))\n            nib.save(n, os.path.join(self.data_dir, f""img{i:d}.nii.gz""))\n            n = nib.Nifti1Image(seg, np.eye(4))\n            nib.save(n, os.path.join(self.data_dir, f""seg{i:d}.nii.gz""))\n\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0"")\n\n    def tearDown(self):\n        set_determinism(seed=None)\n        shutil.rmtree(self.data_dir)\n\n    @skip_if_quick\n    def test_training(self):\n        repeated = []\n        for i in range(3):\n            torch.manual_seed(0)\n\n            repeated.append([])\n            losses, best_metric, best_metric_epoch = run_training_test(\n                self.data_dir, device=self.device, cachedataset=(i == 2)\n            )\n\n            # check training properties\n            np.testing.assert_allclose(\n                losses,\n                [\n                    0.5480509608983993,\n                    0.46773012578487394,\n                    0.4449450016021729,\n                    0.4429117202758789,\n                    0.4266220510005951,\n                    0.4189875662326813,\n                ],\n                rtol=1e-3,\n            )\n            repeated[i].extend(losses)\n            print(""best metric"", best_metric)\n            np.testing.assert_allclose(best_metric, 0.9281478852033616, rtol=1e-3)\n            repeated[i].append(best_metric)\n            np.testing.assert_allclose(best_metric_epoch, 6)\n            self.assertTrue(len(glob(os.path.join(self.data_dir, ""runs""))) > 0)\n            model_file = os.path.join(self.data_dir, ""best_metric_model.pth"")\n            self.assertTrue(os.path.exists(model_file))\n\n            infer_metric = run_inference_test(self.data_dir, device=self.device)\n\n            # check inference properties\n            np.testing.assert_allclose(infer_metric, 0.9276287078857421, rtol=1e-3)\n            repeated[i].append(infer_metric)\n            output_files = sorted(glob(os.path.join(self.data_dir, ""output"", ""img*"", ""*.nii.gz"")))\n            sums = [\n                0.14357540823662318,\n                0.15269879069528602,\n                0.15276683013248435,\n                0.14079445671151278,\n                0.18878697237342096,\n                0.17095550477559823,\n                0.1482579336551299,\n                0.1690032222450447,\n                0.15854729382766766,\n                0.18061742579850057,\n                0.16381168481051658,\n                0.16860525572558283,\n                0.14595678853856425,\n                0.11642670997227073,\n                0.16255648557050426,\n                0.20181780835986443,\n                0.17686017253774264,\n                0.1015966801889699,\n                0.19381932320016432,\n                0.2030797473554483,\n                0.19752939817192153,\n                0.20913782479203039,\n                0.1633029937352367,\n                0.13320356629351957,\n                0.14912896682756496,\n                0.1435551889699086,\n                0.23132670483721884,\n                0.16242907209612817,\n                0.14929296754647223,\n                0.10485583341891754,\n                0.120152831981103,\n                0.13234087758036356,\n                0.11596583906747458,\n                0.15332719266714595,\n                0.16359472886926157,\n                0.19406291722296395,\n                0.22271971603163193,\n                0.18138928828181164,\n                0.19065260090376912,\n                0.08892593971449111,\n            ]\n            for (output, s) in zip(output_files, sums):\n                ave = np.mean(nib.load(output).get_fdata())\n                np.testing.assert_allclose(ave, s, rtol=5e-3)\n                repeated[i].append(ave)\n        np.testing.assert_allclose(repeated[0], repeated[1])\n        np.testing.assert_allclose(repeated[0], repeated[2])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_integration_sliding_window.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.engine import Engine\nfrom torch.utils.data import DataLoader\n\nfrom monai.data import NiftiDataset, create_test_image_3d\nfrom monai.inferers import sliding_window_inference\nfrom monai.handlers import SegmentationSaver\nfrom monai.networks.nets import UNet\nfrom monai.networks.utils import predict_segmentation\nfrom monai.transforms import AddChannel\nfrom monai.utils import set_determinism\nfrom tests.utils import make_nifti_image\n\n\ndef run_test(batch_size, img_name, seg_name, output_dir, device=torch.device(""cuda:0"")):\n    ds = NiftiDataset([img_name], [seg_name], transform=AddChannel(), seg_transform=AddChannel(), image_only=False)\n    loader = DataLoader(ds, batch_size=1, pin_memory=torch.cuda.is_available())\n\n    net = UNet(\n        dimensions=3, in_channels=1, out_channels=1, channels=(4, 8, 16, 32), strides=(2, 2, 2), num_res_units=2,\n    ).to(device)\n    roi_size = (16, 32, 48)\n    sw_batch_size = batch_size\n\n    def _sliding_window_processor(_engine, batch):\n        net.eval()\n        img, seg, meta_data = batch\n        with torch.no_grad():\n            seg_probs = sliding_window_inference(img.to(device), roi_size, sw_batch_size, net)\n            return predict_segmentation(seg_probs)\n\n    infer_engine = Engine(_sliding_window_processor)\n\n    SegmentationSaver(\n        output_dir=output_dir, output_ext="".nii.gz"", output_postfix=""seg"", batch_transform=lambda x: x[2]\n    ).attach(infer_engine)\n\n    infer_engine.run(loader)\n\n    basename = os.path.basename(img_name)[: -len("".nii.gz"")]\n    saved_name = os.path.join(output_dir, basename, f""{basename}_seg.nii.gz"")\n    return saved_name\n\n\nclass TestIntegrationSlidingWindow(unittest.TestCase):\n    def setUp(self):\n        set_determinism(seed=0)\n\n        im, seg = create_test_image_3d(25, 28, 63, rad_max=10, noise_max=1, num_objs=4, num_seg_classes=1)\n        self.img_name = make_nifti_image(im)\n        self.seg_name = make_nifti_image(seg)\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0"")\n\n    def tearDown(self):\n        set_determinism(seed=None)\n        if os.path.exists(self.img_name):\n            os.remove(self.img_name)\n        if os.path.exists(self.seg_name):\n            os.remove(self.seg_name)\n\n    def test_training(self):\n        tempdir = tempfile.mkdtemp()\n        output_file = run_test(\n            batch_size=2, img_name=self.img_name, seg_name=self.seg_name, output_dir=tempdir, device=self.device\n        )\n        output_image = nib.load(output_file).get_fdata()\n        np.testing.assert_allclose(np.sum(output_image), 34070)\n        np.testing.assert_allclose(output_image.shape, (28, 25, 63, 1))\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_integration_stn.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom monai.data import create_test_image_2d\nfrom monai.networks.layers import AffineTransform\nfrom monai.utils import set_determinism\n\n\nclass STNBenchmark(nn.Module):\n    """"""\n    adapted from https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html\n    """"""\n\n    def __init__(self, is_ref=True, reverse_indexing=False):\n        super().__init__()\n        self.is_ref = is_ref\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n        )\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = nn.Sequential(nn.Linear(10 * 3 * 3, 32), nn.ReLU(True), nn.Linear(32, 3 * 2))\n        # Initialize the weights/bias with identity transformation\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n        if not self.is_ref:\n            self.xform = AffineTransform(normalized=True, reverse_indexing=reverse_indexing)\n\n    # Spatial transformer network forward function\n    def stn_ref(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size(), align_corners=False)\n        x = F.grid_sample(x, grid, align_corners=False)\n        return x\n\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n        x = self.xform(x, theta, spatial_size=x.size()[2:])\n        return x\n\n    def forward(self, x):\n        if self.is_ref:\n            return self.stn_ref(x)\n        return self.stn(x)\n\n\ndef compare_2d(is_ref=True, device=None, reverse_indexing=False):\n    batch_size = 32\n    img_a = [create_test_image_2d(28, 28, 5, rad_max=6, noise_max=1)[0][None] for _ in range(batch_size)]\n    img_b = [create_test_image_2d(28, 28, 5, rad_max=6, noise_max=1)[0][None] for _ in range(batch_size)]\n    img_a = np.stack(img_a, axis=0)\n    img_b = np.stack(img_b, axis=0)\n    img_a = torch.as_tensor(img_a, device=device)\n    img_b = torch.as_tensor(img_b, device=device)\n    model = STNBenchmark(is_ref=is_ref, reverse_indexing=reverse_indexing).to(device)\n    optimizer = optim.SGD(model.parameters(), lr=0.001)\n    model.train()\n    init_loss = None\n    for _ in range(20):\n        optimizer.zero_grad()\n        output_a = model(img_a)\n        loss = torch.mean((output_a - img_b) ** 2)\n        if init_loss is None:\n            init_loss = loss.item()\n        loss.backward()\n        optimizer.step()\n    return model(img_a).detach().cpu().numpy(), loss.item(), init_loss\n\n\nclass TestSpatialTransformerCore(unittest.TestCase):\n    def setUp(self):\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0"")\n\n    def tearDown(self):\n        set_determinism(seed=None)\n\n    def test_training(self):\n        """"""\n        check that the quality AffineTransform backpropagation\n        """"""\n        atol = 1e-5\n        set_determinism(seed=0)\n        out_ref, loss_ref, init_loss_ref = compare_2d(True, self.device)\n        print(out_ref.shape, loss_ref, init_loss_ref)\n\n        set_determinism(seed=0)\n        out, loss, init_loss = compare_2d(False, self.device)\n        print(out.shape, loss, init_loss)\n        np.testing.assert_allclose(out_ref, out, atol=atol)\n        np.testing.assert_allclose(init_loss_ref, init_loss, atol=atol)\n        np.testing.assert_allclose(loss_ref, loss, atol=atol)\n\n        set_determinism(seed=0)\n        out, loss, init_loss = compare_2d(False, self.device, True)\n        print(out.shape, loss, init_loss)\n        np.testing.assert_allclose(out_ref, out, atol=atol)\n        np.testing.assert_allclose(init_loss_ref, init_loss, atol=atol)\n        np.testing.assert_allclose(loss_ref, loss, atol=atol)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_integration_unet_2d.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom ignite.engine import create_supervised_trainer\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom monai.data import create_test_image_2d\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets import UNet\n\n\ndef run_test(batch_size=64, train_steps=100, device=torch.device(""cuda:0"")):\n    class _TestBatch(Dataset):\n        def __getitem__(self, _unused_id):\n            im, seg = create_test_image_2d(128, 128, noise_max=1, num_objs=4, num_seg_classes=1)\n            return im[None], seg[None].astype(np.float32)\n\n        def __len__(self):\n            return train_steps\n\n    net = UNet(\n        dimensions=2, in_channels=1, out_channels=1, channels=(4, 8, 16, 32), strides=(2, 2, 2), num_res_units=2,\n    ).to(device)\n\n    loss = DiceLoss(sigmoid=True)\n    opt = torch.optim.Adam(net.parameters(), 1e-4)\n    src = DataLoader(_TestBatch(), batch_size=batch_size)\n\n    trainer = create_supervised_trainer(net, opt, loss, device, False)\n\n    trainer.run(src, 1)\n    loss = trainer.state.output\n    return loss\n\n\nclass TestIntegrationUnet2D(unittest.TestCase):\n    def test_unet_training(self):\n        loss = run_test(device=torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0""))\n        print(loss)\n        self.assertGreaterEqual(0.85, loss)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_integration_workflows.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport shutil\nimport tempfile\nimport unittest\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.metrics import Accuracy\n\nimport monai\nfrom monai.transforms import (\n    Compose,\n    LoadNiftid,\n    AsChannelFirstd,\n    ScaleIntensityd,\n    RandCropByPosNegLabeld,\n    RandRotate90d,\n    ToTensord,\n    Activationsd,\n    AsDiscreted,\n    KeepLargestConnectedComponentd,\n)\nfrom monai.handlers import (\n    StatsHandler,\n    TensorBoardStatsHandler,\n    TensorBoardImageHandler,\n    ValidationHandler,\n    LrScheduleHandler,\n    CheckpointSaver,\n    CheckpointLoader,\n    SegmentationSaver,\n    MeanDice,\n)\nfrom monai.data import create_test_image_3d\nfrom monai.engines import SupervisedTrainer, SupervisedEvaluator\nfrom monai.inferers import SimpleInferer, SlidingWindowInferer\nfrom monai.utils import set_determinism\nfrom tests.utils import skip_if_quick\n\n\ndef run_training_test(root_dir, device=torch.device(""cuda:0"")):\n    images = sorted(glob(os.path.join(root_dir, ""img*.nii.gz"")))\n    segs = sorted(glob(os.path.join(root_dir, ""seg*.nii.gz"")))\n    train_files = [{""image"": img, ""label"": seg} for img, seg in zip(images[:20], segs[:20])]\n    val_files = [{""image"": img, ""label"": seg} for img, seg in zip(images[-20:], segs[-20:])]\n\n    # define transforms for image and segmentation\n    train_transforms = Compose(\n        [\n            LoadNiftid(keys=[""image"", ""label""]),\n            AsChannelFirstd(keys=[""image"", ""label""], channel_dim=-1),\n            ScaleIntensityd(keys=[""image"", ""label""]),\n            RandCropByPosNegLabeld(\n                keys=[""image"", ""label""], label_key=""label"", size=[96, 96, 96], pos=1, neg=1, num_samples=4\n            ),\n            RandRotate90d(keys=[""image"", ""label""], prob=0.5, spatial_axes=[0, 2]),\n            ToTensord(keys=[""image"", ""label""]),\n        ]\n    )\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""image"", ""label""]),\n            AsChannelFirstd(keys=[""image"", ""label""], channel_dim=-1),\n            ScaleIntensityd(keys=[""image"", ""label""]),\n            ToTensord(keys=[""image"", ""label""]),\n        ]\n    )\n\n    # create a training data loader\n    train_ds = monai.data.CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.5)\n    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    train_loader = monai.data.DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n    # create a validation data loader\n    val_ds = monai.data.CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0)\n    val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=4)\n\n    # create UNet, DiceLoss and Adam optimizer\n    net = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n    loss = monai.losses.DiceLoss(sigmoid=True)\n    opt = torch.optim.Adam(net.parameters(), 1e-3)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=2, gamma=0.1)\n\n    val_post_transforms = Compose(\n        [\n            Activationsd(keys=""pred"", output_postfix=""act"", sigmoid=True),\n            AsDiscreted(keys=""pred_act"", output_postfix=""dis"", threshold_values=True),\n            KeepLargestConnectedComponentd(keys=""pred_act_dis"", applied_values=[1], output_postfix=None),\n        ]\n    )\n    val_handlers = [\n        StatsHandler(output_transform=lambda x: None),\n        TensorBoardStatsHandler(log_dir=root_dir, output_transform=lambda x: None),\n        TensorBoardImageHandler(\n            log_dir=root_dir,\n            batch_transform=lambda x: (x[""image""], x[""label""]),\n            output_transform=lambda x: x[""pred_act_dis""],\n        ),\n        CheckpointSaver(save_dir=root_dir, save_dict={""net"": net}, save_key_metric=True),\n    ]\n\n    evaluator = SupervisedEvaluator(\n        device=device,\n        val_data_loader=val_loader,\n        network=net,\n        inferer=SlidingWindowInferer(roi_size=(96, 96, 96), sw_batch_size=4, overlap=0.5),\n        post_transform=val_post_transforms,\n        key_val_metric={\n            ""val_mean_dice"": MeanDice(\n                include_background=True, output_transform=lambda x: (x[""pred_act_dis""], x[""label""])\n            )\n        },\n        additional_metrics={""val_acc"": Accuracy(output_transform=lambda x: (x[""pred_act_dis""], x[""label""]))},\n        val_handlers=val_handlers,\n    )\n\n    train_post_transforms = Compose(\n        [\n            Activationsd(keys=""pred"", output_postfix=""act"", sigmoid=True),\n            AsDiscreted(keys=""pred_act"", output_postfix=""dis"", threshold_values=True),\n            KeepLargestConnectedComponentd(keys=""pred_act_dis"", applied_values=[1], output_postfix=None),\n        ]\n    )\n    train_handlers = [\n        LrScheduleHandler(lr_scheduler=lr_scheduler, print_lr=True),\n        ValidationHandler(validator=evaluator, interval=2, epoch_level=True),\n        StatsHandler(tag_name=""train_loss"", output_transform=lambda x: x[""loss""]),\n        TensorBoardStatsHandler(log_dir=root_dir, tag_name=""train_loss"", output_transform=lambda x: x[""loss""]),\n        CheckpointSaver(save_dir=root_dir, save_dict={""net"": net, ""opt"": opt}, save_interval=2, epoch_level=True),\n    ]\n\n    trainer = SupervisedTrainer(\n        device=device,\n        max_epochs=5,\n        train_data_loader=train_loader,\n        network=net,\n        optimizer=opt,\n        loss_function=loss,\n        inferer=SimpleInferer(),\n        amp=False,\n        post_transform=train_post_transforms,\n        key_train_metric={""train_acc"": Accuracy(output_transform=lambda x: (x[""pred_act_dis""], x[""label""]))},\n        train_handlers=train_handlers,\n    )\n    trainer.run()\n\n    return evaluator.state.best_metric\n\n\ndef run_inference_test(root_dir, model_file, device=torch.device(""cuda:0"")):\n    images = sorted(glob(os.path.join(root_dir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(root_dir, ""seg*.nii.gz"")))\n    val_files = [{""image"": img, ""label"": seg} for img, seg in zip(images, segs)]\n\n    # define transforms for image and segmentation\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""image"", ""label""]),\n            AsChannelFirstd(keys=[""image"", ""label""], channel_dim=-1),\n            ScaleIntensityd(keys=[""image"", ""label""]),\n            ToTensord(keys=[""image"", ""label""]),\n        ]\n    )\n\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=4)\n\n    # create UNet, DiceLoss and Adam optimizer\n    net = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n\n    val_post_transforms = Compose(\n        [\n            Activationsd(keys=""pred"", output_postfix=""act"", sigmoid=True),\n            AsDiscreted(keys=""pred_act"", output_postfix=""dis"", threshold_values=True),\n            KeepLargestConnectedComponentd(keys=""pred_act_dis"", applied_values=[1], output_postfix=None),\n        ]\n    )\n    val_handlers = [\n        StatsHandler(output_transform=lambda x: None),\n        CheckpointLoader(load_path=f""{model_file}"", load_dict={""net"": net}),\n        SegmentationSaver(\n            output_dir=root_dir,\n            batch_transform=lambda x: {\n                ""filename_or_obj"": x[""image.filename_or_obj""],\n                ""affine"": x[""image.affine""],\n                ""original_affine"": x[""image.original_affine""],\n                ""spatial_shape"": x[""image.spatial_shape""],\n            },\n            output_transform=lambda x: x[""pred_act_dis""],\n        ),\n    ]\n\n    evaluator = SupervisedEvaluator(\n        device=device,\n        val_data_loader=val_loader,\n        network=net,\n        inferer=SlidingWindowInferer(roi_size=(96, 96, 96), sw_batch_size=4, overlap=0.5),\n        post_transform=val_post_transforms,\n        key_val_metric={\n            ""val_mean_dice"": MeanDice(\n                include_background=True, output_transform=lambda x: (x[""pred_act_dis""], x[""label""])\n            )\n        },\n        additional_metrics={""val_acc"": Accuracy(output_transform=lambda x: (x[""pred_act_dis""], x[""label""]))},\n        val_handlers=val_handlers,\n    )\n    evaluator.run()\n\n    return evaluator.state.best_metric\n\n\nclass IntegrationWorkflows(unittest.TestCase):\n    def setUp(self):\n        set_determinism(seed=0)\n\n        self.data_dir = tempfile.mkdtemp()\n        for i in range(40):\n            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n            n = nib.Nifti1Image(im, np.eye(4))\n            nib.save(n, os.path.join(self.data_dir, f""img{i:d}.nii.gz""))\n            n = nib.Nifti1Image(seg, np.eye(4))\n            nib.save(n, os.path.join(self.data_dir, f""seg{i:d}.nii.gz""))\n\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0"")\n        monai.config.print_config()\n        logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    def tearDown(self):\n        set_determinism(seed=None)\n        shutil.rmtree(self.data_dir)\n\n    @skip_if_quick\n    def test_training(self):\n        repeated = []\n        for i in range(2):\n            torch.manual_seed(0)\n\n            repeated.append([])\n            best_metric = run_training_test(self.data_dir, device=self.device)\n            print(""best metric"", best_metric)\n            np.testing.assert_allclose(best_metric, 0.9232678800821305, rtol=1e-3)\n            repeated[i].append(best_metric)\n\n            model_file = sorted(glob(os.path.join(self.data_dir, ""net_key_metric*.pth"")))[-1]\n            infer_metric = run_inference_test(self.data_dir, model_file, device=self.device)\n            print(""infer metric"", infer_metric)\n            # check inference properties\n            np.testing.assert_allclose(infer_metric, 0.9224808603525162, rtol=1e-3)\n            repeated[i].append(infer_metric)\n            output_files = sorted(glob(os.path.join(self.data_dir, ""img*"", ""*.nii.gz"")))\n            sums = [\n                0.14212512969970703,\n                0.1506481170654297,\n                0.1368846893310547,\n                0.13330554962158203,\n                0.18573999404907227,\n                0.1647019386291504,\n                0.1408066749572754,\n                0.16658973693847656,\n                0.15639686584472656,\n                0.17746448516845703,\n                0.16197776794433594,\n                0.16469907760620117,\n                0.14304876327514648,\n                0.10998392105102539,\n                0.16064167022705078,\n                0.1962604522705078,\n                0.17453575134277344,\n                0.052756309509277344,\n                0.19060277938842773,\n                0.20035600662231445,\n                0.19619369506835938,\n                0.20325279235839844,\n                0.15996408462524414,\n                0.13104581832885742,\n                0.14955568313598633,\n                0.135528564453125,\n                0.2252669334411621,\n                0.16170835494995117,\n                0.14747190475463867,\n                0.10289239883422852,\n                0.11845922470092773,\n                0.13117074966430664,\n                0.11201333999633789,\n                0.15172672271728516,\n                0.15926742553710938,\n                0.18946075439453125,\n                0.21686124801635742,\n                0.1773381233215332,\n                0.1864323616027832,\n                0.035613059997558594,\n            ]\n            for (output, s) in zip(output_files, sums):\n                ave = np.mean(nib.load(output).get_fdata())\n                np.testing.assert_allclose(ave, s, rtol=1e-2)\n                repeated[i].append(ave)\n        np.testing.assert_allclose(repeated[0], repeated[1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_keep_largest_connected_component.py,18,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import KeepLargestConnectedComponent\n\ngrid_1 = torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]])\ngrid_2 = torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [1, 0, 1, 1, 2], [1, 0, 1, 2, 2], [0, 0, 0, 0, 1]]]])\n\nTEST_CASE_1 = [\n    ""value_1"",\n    {""independent"": False, ""applied_values"": [1]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_2 = [\n    ""value_2"",\n    {""independent"": False, ""applied_values"": [2]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_3 = [\n    ""independent_value_1_2"",\n    {""independent"": True, ""applied_values"": [1, 2]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 1, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_4 = [\n    ""dependent_value_1_2"",\n    {""independent"": False, ""applied_values"": [1, 2]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_5 = [\n    ""value_1"",\n    {""independent"": True, ""applied_values"": [1]},\n    grid_2,\n    torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 2], [0, 0, 0, 0, 0]]]]),\n]\n\nTEST_CASE_6 = [\n    ""independent_value_1_2"",\n    {""independent"": True, ""applied_values"": [1, 2]},\n    grid_2,\n    torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 2], [0, 0, 0, 0, 0]]]]),\n]\n\nTEST_CASE_7 = [\n    ""dependent_value_1_2"",\n    {""independent"": False, ""applied_values"": [1, 2]},\n    grid_2,\n    torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 2], [0, 0, 0, 0, 1]]]]),\n]\n\nTEST_CASE_8 = [\n    ""value_1_connect_1"",\n    {""independent"": False, ""applied_values"": [1], ""connectivity"": 1},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 0, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_9 = [\n    ""independent_value_1_2_connect_1"",\n    {""independent"": True, ""applied_values"": [1, 2], ""connectivity"": 1},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 0, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_10 = [\n    ""dependent_value_1_2_connect_1"",\n    {""independent"": False, ""applied_values"": [1, 2], ""connectivity"": 1},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 0, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_11 = [\n    ""value_0_background_3"",\n    {""independent"": False, ""applied_values"": [0], ""background"": 3},\n    grid_1,\n    torch.tensor([[[[3, 3, 1, 3, 3], [3, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_12 = [\n    ""all_0_batch_2"",\n    {""independent"": False, ""applied_values"": [1], ""background"": 3},\n    torch.tensor(\n        [\n            [[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]],\n            [[[1, 1, 1, 1, 1], [0, 0, 0, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0]]],\n        ]\n    ),\n    torch.tensor(\n        [\n            [[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]],\n            [[[1, 1, 1, 1, 1], [0, 0, 0, 0, 0], [0, 0, 3, 3, 3], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0]]],\n        ]\n    ),\n]\n\nVALID_CASES = [\n    TEST_CASE_1,\n    TEST_CASE_2,\n    TEST_CASE_3,\n    TEST_CASE_4,\n    TEST_CASE_5,\n    TEST_CASE_6,\n    TEST_CASE_7,\n    TEST_CASE_8,\n    TEST_CASE_9,\n    TEST_CASE_10,\n    TEST_CASE_11,\n    TEST_CASE_12,\n]\n\n\nclass TestKeepLargestConnectedComponent(unittest.TestCase):\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, _, args, tensor, expected):\n        converter = KeepLargestConnectedComponent(**args)\n        if torch.cuda.is_available():\n            result = converter(tensor.clone().cuda())\n            assert torch.allclose(result, expected.cuda())\n        else:\n            result = converter(tensor.clone())\n            assert torch.allclose(result, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_keep_largest_connected_componentd.py,20,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import KeepLargestConnectedComponentd\n\ngrid_1 = {\n    ""img"": torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]])\n}\ngrid_2 = {\n    ""img"": torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [1, 0, 1, 1, 2], [1, 0, 1, 2, 2], [0, 0, 0, 0, 1]]]])\n}\n\nTEST_CASE_1 = [\n    ""value_1"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [1]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_2 = [\n    ""value_2"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [2]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_3 = [\n    ""independent_value_1_2"",\n    {""keys"": [""img""], ""independent"": True, ""applied_values"": [1, 2]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 1, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_4 = [\n    ""dependent_value_1_2"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [1, 2]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_5 = [\n    ""value_1"",\n    {""keys"": [""img""], ""independent"": True, ""applied_values"": [1]},\n    grid_2,\n    torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 2], [0, 0, 0, 0, 0]]]]),\n]\n\nTEST_CASE_6 = [\n    ""independent_value_1_2"",\n    {""keys"": [""img""], ""independent"": True, ""applied_values"": [1, 2]},\n    grid_2,\n    torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 2], [0, 0, 0, 0, 0]]]]),\n]\n\nTEST_CASE_7 = [\n    ""dependent_value_1_2"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [1, 2]},\n    grid_2,\n    torch.tensor([[[[0, 0, 0, 0, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 2], [0, 0, 0, 0, 1]]]]),\n]\n\nTEST_CASE_8 = [\n    ""value_1_connect_1"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [1], ""connectivity"": 1},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 0, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_9 = [\n    ""independent_value_1_2_connect_1"",\n    {""keys"": [""img""], ""independent"": True, ""applied_values"": [1, 2], ""connectivity"": 1},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 0, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_10 = [\n    ""dependent_value_1_2_connect_1"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [1, 2], ""connectivity"": 1},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 0, 0], [2, 2, 0, 0, 0]]]]),\n]\n\nTEST_CASE_11 = [\n    ""value_0_background_3"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [0], ""background"": 3},\n    grid_1,\n    torch.tensor([[[[3, 3, 1, 3, 3], [3, 2, 1, 1, 1], [1, 2, 1, 0, 0], [1, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nTEST_CASE_12 = [\n    ""all_0_batch_2"",\n    {""keys"": [""img""], ""independent"": False, ""applied_values"": [1], ""background"": 3},\n    {\n        ""img"": torch.tensor(\n            [\n                [[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]],\n                [[[1, 1, 1, 1, 1], [0, 0, 0, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0]]],\n            ]\n        )\n    },\n    torch.tensor(\n        [\n            [[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]],\n            [[[1, 1, 1, 1, 1], [0, 0, 0, 0, 0], [0, 0, 3, 3, 3], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0]]],\n        ]\n    ),\n]\n\nTEST_CASE_13 = [\n    ""none_postfix"",\n    {""keys"": [""img""], ""output_postfix"": None, ""independent"": False, ""applied_values"": [1]},\n    grid_1,\n    torch.tensor([[[[0, 0, 1, 0, 0], [0, 2, 1, 1, 1], [0, 2, 1, 0, 0], [0, 2, 0, 1, 0], [2, 2, 0, 0, 2]]]]),\n]\n\nVALID_CASES = [\n    TEST_CASE_1,\n    TEST_CASE_2,\n    TEST_CASE_3,\n    TEST_CASE_4,\n    TEST_CASE_5,\n    TEST_CASE_6,\n    TEST_CASE_7,\n    TEST_CASE_8,\n    TEST_CASE_9,\n    TEST_CASE_10,\n    TEST_CASE_11,\n    TEST_CASE_12,\n]\n\n\nclass TestKeepLargestConnectedComponentd(unittest.TestCase):\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, _, args, input_dict, expected):\n        converter = KeepLargestConnectedComponentd(**args)\n        if torch.cuda.is_available():\n            input_dict[""img""] = input_dict[""img""].cuda()\n            result = converter(input_dict)\n            torch.allclose(result[""img_largestcc""], expected.cuda())\n        else:\n            result = converter(input_dict)\n            torch.allclose(result[""img_largestcc""], expected)\n\n    @parameterized.expand([TEST_CASE_13])\n    def test_none_postfix(self, _, args, input_dict, expected):\n        converter = KeepLargestConnectedComponentd(**args)\n        input_dict[""img""] = input_dict[""img""].cpu()\n        result = converter(input_dict)\n        torch.allclose(result[""img""], expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_list_data_collate.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.data import list_data_collate\n\na = {""image"": np.array([1, 2, 3]), ""label"": np.array([4, 5, 6])}\nb = {""image"": np.array([7, 8, 9]), ""label"": np.array([10, 11, 12])}\nc = {""image"": np.array([13, 14, 15]), ""label"": np.array([16, 7, 18])}\nd = {""image"": np.array([19, 20, 21]), ""label"": np.array([22, 23, 24])}\nTEST_CASE_1 = [[[a, b], [c, d]], dict, torch.Size([4, 3])]  # dataset returns a list of dictionary data\n\ne = (np.array([1, 2, 3]), np.array([4, 5, 6]))\nf = (np.array([7, 8, 9]), np.array([10, 11, 12]))\ng = (np.array([13, 14, 15]), np.array([16, 7, 18]))\nh = (np.array([19, 20, 21]), np.array([22, 23, 24]))\nTEST_CASE_2 = [[[e, f], [g, h]], list, torch.Size([4, 3])]  # dataset returns a list of tuple data\n\n\nclass TestListDataCollate(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_type_shape(self, input_data, expected_type, expected_shape):\n        result = list_data_collate(input_data)\n        self.assertIsInstance(result, expected_type)\n        if isinstance(result, dict):\n            data = result[""image""]\n        else:\n            data = result[0]\n        self.assertEqual(data.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_load_decathalon_datalist.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport json\nimport shutil\nimport tempfile\nfrom monai.data import load_decathalon_datalist\n\n\nclass TestLoadDecathalonDatalist(unittest.TestCase):\n    def test_seg_values(self):\n        tempdir = tempfile.mkdtemp()\n        test_data = {\n            ""name"": ""Spleen"",\n            ""description"": ""Spleen Segmentation"",\n            ""labels"": {""0"": ""background"", ""1"": ""spleen""},\n            ""training"": [\n                {""image"": ""spleen_19.nii.gz"", ""label"": ""spleen_19.nii.gz""},\n                {""image"": ""spleen_31.nii.gz"", ""label"": ""spleen_31.nii.gz""},\n            ],\n            ""test"": [""spleen_15.nii.gz"", ""spleen_23.nii.gz""],\n        }\n        json_str = json.dumps(test_data)\n        file_path = os.path.join(tempdir, ""test_data.json"")\n        with open(file_path, ""w"") as json_file:\n            json_file.write(json_str)\n        result = load_decathalon_datalist(file_path, True, ""training"", tempdir)\n        self.assertEqual(result[0][""image""], os.path.join(tempdir, ""spleen_19.nii.gz""))\n        self.assertEqual(result[0][""label""], os.path.join(tempdir, ""spleen_19.nii.gz""))\n        shutil.rmtree(tempdir)\n\n    def test_cls_values(self):\n        tempdir = tempfile.mkdtemp()\n        test_data = {\n            ""name"": ""ChestXRay"",\n            ""description"": ""Chest X-ray classification"",\n            ""labels"": {""0"": ""background"", ""1"": ""chest""},\n            ""training"": [{""image"": ""chest_19.nii.gz"", ""label"": 0}, {""image"": ""chest_31.nii.gz"", ""label"": 1}],\n            ""test"": [""chest_15.nii.gz"", ""chest_23.nii.gz""],\n        }\n        json_str = json.dumps(test_data)\n        file_path = os.path.join(tempdir, ""test_data.json"")\n        with open(file_path, ""w"") as json_file:\n            json_file.write(json_str)\n        result = load_decathalon_datalist(file_path, False, ""training"", tempdir)\n        self.assertEqual(result[0][""image""], os.path.join(tempdir, ""chest_19.nii.gz""))\n        self.assertEqual(result[0][""label""], 0)\n        shutil.rmtree(tempdir)\n\n    def test_seg_no_basedir(self):\n        tempdir = tempfile.mkdtemp()\n        test_data = {\n            ""name"": ""Spleen"",\n            ""description"": ""Spleen Segmentation"",\n            ""labels"": {""0"": ""background"", ""1"": ""spleen""},\n            ""training"": [\n                {\n                    ""image"": os.path.join(tempdir, ""spleen_19.nii.gz""),\n                    ""label"": os.path.join(tempdir, ""spleen_19.nii.gz""),\n                },\n                {\n                    ""image"": os.path.join(tempdir, ""spleen_31.nii.gz""),\n                    ""label"": os.path.join(tempdir, ""spleen_31.nii.gz""),\n                },\n            ],\n            ""test"": [os.path.join(tempdir, ""spleen_15.nii.gz""), os.path.join(tempdir, ""spleen_23.nii.gz"")],\n        }\n        json_str = json.dumps(test_data)\n        file_path = os.path.join(tempdir, ""test_data.json"")\n        with open(file_path, ""w"") as json_file:\n            json_file.write(json_str)\n        result = load_decathalon_datalist(file_path, True, ""training"", None)\n        self.assertEqual(result[0][""image""], os.path.join(tempdir, ""spleen_19.nii.gz""))\n        self.assertEqual(result[0][""label""], os.path.join(tempdir, ""spleen_19.nii.gz""))\n\n    def test_seg_no_labels(self):\n        tempdir = tempfile.mkdtemp()\n        test_data = {\n            ""name"": ""Spleen"",\n            ""description"": ""Spleen Segmentation"",\n            ""labels"": {""0"": ""background"", ""1"": ""spleen""},\n            ""test"": [""spleen_15.nii.gz"", ""spleen_23.nii.gz""],\n        }\n        json_str = json.dumps(test_data)\n        file_path = os.path.join(tempdir, ""test_data.json"")\n        with open(file_path, ""w"") as json_file:\n            json_file.write(json_str)\n        result = load_decathalon_datalist(file_path, True, ""test"", tempdir)\n        self.assertEqual(result[0][""image""], os.path.join(tempdir, ""spleen_15.nii.gz""))\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_load_nifti.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nimport nibabel as nib\nfrom parameterized import parameterized\nfrom monai.transforms import LoadNifti\n\nTEST_CASE_1 = [{""as_closest_canonical"": False, ""image_only"": True}, [""test_image.nii.gz""], (128, 128, 128)]\n\nTEST_CASE_2 = [{""as_closest_canonical"": False, ""image_only"": False}, [""test_image.nii.gz""], (128, 128, 128)]\n\nTEST_CASE_3 = [\n    {""as_closest_canonical"": False, ""image_only"": True},\n    [""test_image1.nii.gz"", ""test_image2.nii.gz"", ""test_image3.nii.gz""],\n    (3, 128, 128, 128),\n]\n\nTEST_CASE_4 = [\n    {""as_closest_canonical"": False, ""image_only"": False},\n    [""test_image1.nii.gz"", ""test_image2.nii.gz"", ""test_image3.nii.gz""],\n    (3, 128, 128, 128),\n]\n\n\nclass TestLoadNifti(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_shape(self, input_param, filenames, expected_shape):\n        test_image = np.random.randint(0, 2, size=[128, 128, 128])\n        tempdir = tempfile.mkdtemp()\n        for i, name in enumerate(filenames):\n            filenames[i] = os.path.join(tempdir, name)\n            nib.save(nib.Nifti1Image(test_image, np.eye(4)), filenames[i])\n        result = LoadNifti(**input_param)(filenames)\n\n        if isinstance(result, tuple):\n            result, header = result\n            self.assertTrue(""affine"" in header)\n            np.testing.assert_allclose(header[""affine""], np.eye(4))\n            if input_param[""as_closest_canonical""]:\n                np.testing.asesrt_allclose(header[""original_affine""], np.eye(4))\n        self.assertTupleEqual(result.shape, expected_shape)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_load_niftid.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nimport nibabel as nib\nfrom parameterized import parameterized\nfrom monai.transforms import LoadNiftid\n\nKEYS = [""image"", ""label"", ""extra""]\n\nTEST_CASE_1 = [{""keys"": KEYS, ""as_closest_canonical"": False}, (128, 128, 128)]\n\n\nclass TestLoadNiftid(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, input_param, expected_shape):\n        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))\n        test_data = dict()\n        tempdir = tempfile.mkdtemp()\n        for key in KEYS:\n            nib.save(test_image, os.path.join(tempdir, key + "".nii.gz""))\n            test_data.update({key: os.path.join(tempdir, key + "".nii.gz"")})\n        result = LoadNiftid(**input_param)(test_data)\n        for key in KEYS:\n            self.assertTupleEqual(result[key].shape, expected_shape)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_load_png.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nfrom PIL import Image\nfrom parameterized import parameterized\nfrom monai.transforms import LoadPNG\n\nTEST_CASE_1 = [(128, 128), [""test_image.png""], (128, 128), (128, 128)]\n\nTEST_CASE_2 = [(128, 128, 3), [""test_image.png""], (128, 128, 3), (128, 128)]\n\nTEST_CASE_3 = [(128, 128), [""test_image1.png"", ""test_image2.png"", ""test_image3.png""], (3, 128, 128), (128, 128)]\n\n\nclass TestLoadPNG(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_shape(self, data_shape, filenames, expected_shape, meta_shape):\n        test_image = np.random.randint(0, 256, size=data_shape)\n        tempdir = tempfile.mkdtemp()\n        for i, name in enumerate(filenames):\n            filenames[i] = os.path.join(tempdir, name)\n            Image.fromarray(test_image.astype(""uint8"")).save(filenames[i])\n        result = LoadPNG()(filenames)\n        self.assertTupleEqual(result[1][""spatial_shape""], meta_shape)\n        self.assertTupleEqual(result[0].shape, expected_shape)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_load_pngd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nfrom PIL import Image\nfrom parameterized import parameterized\nfrom monai.transforms import LoadPNGd\n\nKEYS = [""image"", ""label"", ""extra""]\n\nTEST_CASE_1 = [{""keys"": KEYS}, (128, 128, 3)]\n\n\nclass TestLoadPNGd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, input_param, expected_shape):\n        test_image = np.random.randint(0, 256, size=[128, 128, 3])\n        tempdir = tempfile.mkdtemp()\n        test_data = dict()\n        for key in KEYS:\n            Image.fromarray(test_image.astype(""uint8"")).save(os.path.join(tempdir, key + "".png""))\n            test_data.update({key: os.path.join(tempdir, key + "".png"")})\n        result = LoadPNGd(**input_param)(test_data)\n        for key in KEYS:\n            self.assertTupleEqual(result[key].shape, expected_shape)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_load_spacing_orientation.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport time\nimport unittest\n\nimport nibabel\nimport numpy as np\nfrom nibabel.processing import resample_to_output\nfrom parameterized import parameterized\n\nfrom monai.transforms import AddChanneld, LoadNiftid, Orientationd, Spacingd\n\nFILES = tuple(\n    os.path.join(os.path.dirname(__file__), ""testing_data"", filename)\n    for filename in (""anatomical.nii"", ""reoriented_anat_moved.nii"")\n)\n\n\nclass TestLoadSpacingOrientation(unittest.TestCase):\n    @parameterized.expand(FILES)\n    def test_load_spacingd(self, filename):\n        data = {""image"": filename}\n        data_dict = LoadNiftid(keys=""image"")(data)\n        data_dict = AddChanneld(keys=""image"")(data_dict)\n        t = time.time()\n        res_dict = Spacingd(keys=""image"", pixdim=(1, 0.2, 1), diagonal=True, mode=""zeros"")(data_dict)\n        t1 = time.time()\n        print(f""time monai: {t1 - t}"")\n        anat = nibabel.Nifti1Image(data_dict[""image""][0], data_dict[""image.affine""])\n        ref = resample_to_output(anat, (1, 0.2, 1), order=1)\n        t2 = time.time()\n        print(f""time scipy: {t2 - t1}"")\n        self.assertTrue(t2 >= t1)\n        np.testing.assert_allclose(data_dict[""image.affine""], res_dict[""image.original_affine""])\n        np.testing.assert_allclose(res_dict[""image.affine""], ref.affine)\n        np.testing.assert_allclose(res_dict[""image""].shape[1:], ref.shape)\n        np.testing.assert_allclose(ref.get_fdata(), res_dict[""image""][0], atol=0.05)\n\n    @parameterized.expand(FILES)\n    def test_load_spacingd_rotate(self, filename):\n        data = {""image"": filename}\n        data_dict = LoadNiftid(keys=""image"")(data)\n        data_dict = AddChanneld(keys=""image"")(data_dict)\n        affine = data_dict[""image.affine""]\n        data_dict[""image.original_affine""] = data_dict[""image.affine""] = (\n            np.array([[0, 0, 1, 0], [0, 1, 0, 0], [-1, 0, 0, 0], [0, 0, 0, 1]]) @ affine\n        )\n        t = time.time()\n        res_dict = Spacingd(keys=""image"", pixdim=(1, 2, 3), diagonal=True, mode=""zeros"")(data_dict)\n        t1 = time.time()\n        print(f""time monai: {t1 - t}"")\n        anat = nibabel.Nifti1Image(data_dict[""image""][0], data_dict[""image.affine""])\n        ref = resample_to_output(anat, (1, 2, 3), order=1)\n        t2 = time.time()\n        print(f""time scipy: {t2 - t1}"")\n        self.assertTrue(t2 >= t1)\n        np.testing.assert_allclose(data_dict[""image.affine""], res_dict[""image.original_affine""])\n        np.testing.assert_allclose(res_dict[""image.affine""], ref.affine)\n        if ""anatomical"" not in filename:\n            np.testing.assert_allclose(res_dict[""image""].shape[1:], ref.shape)\n            np.testing.assert_allclose(ref.get_fdata(), res_dict[""image""][0], atol=0.05)\n        else:\n            # different from the ref implementation (shape computed by round\n            # instead of ceil)\n            np.testing.assert_allclose(ref.get_fdata()[..., :-1], res_dict[""image""][0], atol=0.05)\n\n    def test_load_spacingd_non_diag(self):\n        data = {""image"": FILES[1]}\n        data_dict = LoadNiftid(keys=""image"")(data)\n        data_dict = AddChanneld(keys=""image"")(data_dict)\n        affine = data_dict[""image.affine""]\n        data_dict[""image.original_affine""] = data_dict[""image.affine""] = (\n            np.array([[0, 0, 1, 0], [0, 1, 0, 0], [-1, 0, 0, 0], [0, 0, 0, 1]]) @ affine\n        )\n        res_dict = Spacingd(keys=""image"", pixdim=(1, 2, 3), diagonal=False, mode=""zeros"")(data_dict)\n        np.testing.assert_allclose(data_dict[""image.affine""], res_dict[""image.original_affine""])\n        np.testing.assert_allclose(\n            res_dict[""image.affine""],\n            np.array(\n                [\n                    [0.0, 0.0, 3.0, -27.599409],\n                    [0.0, 2.0, 0.0, -47.977585],\n                    [-1.0, 0.0, 0.0, 35.297897],\n                    [0.0, 0.0, 0.0, 1.0],\n                ]\n            ),\n        )\n\n    def test_load_spacingd_rotate_non_diag(self):\n        data = {""image"": FILES[0]}\n        data_dict = LoadNiftid(keys=""image"")(data)\n        data_dict = AddChanneld(keys=""image"")(data_dict)\n        res_dict = Spacingd(keys=""image"", pixdim=(1, 2, 3), diagonal=False, mode=""border"")(data_dict)\n        np.testing.assert_allclose(data_dict[""image.affine""], res_dict[""image.original_affine""])\n        np.testing.assert_allclose(\n            res_dict[""image.affine""],\n            np.array([[-1.0, 0.0, 0.0, 32.0], [0.0, 2.0, 0.0, -40.0], [0.0, 0.0, 3.0, -16.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n\n    def test_load_spacingd_rotate_non_diag_ornt(self):\n        data = {""image"": FILES[0]}\n        data_dict = LoadNiftid(keys=""image"")(data)\n        data_dict = AddChanneld(keys=""image"")(data_dict)\n        res_dict = Spacingd(keys=""image"", pixdim=(1, 2, 3), diagonal=False, mode=""border"")(data_dict)\n        res_dict = Orientationd(keys=""image"", axcodes=""LPI"")(res_dict)\n        np.testing.assert_allclose(data_dict[""image.affine""], res_dict[""image.original_affine""])\n        np.testing.assert_allclose(\n            res_dict[""image.affine""],\n            np.array([[-1.0, 0.0, 0.0, 32.0], [0.0, -2.0, 0.0, 40.0], [0.0, 0.0, -3.0, 32.0], [0.0, 0.0, 0.0, 1.0]]),\n        )\n\n    def test_load_spacingd_non_diag_ornt(self):\n        data = {""image"": FILES[1]}\n        data_dict = LoadNiftid(keys=""image"")(data)\n        data_dict = AddChanneld(keys=""image"")(data_dict)\n        affine = data_dict[""image.affine""]\n        data_dict[""image.original_affine""] = data_dict[""image.affine""] = (\n            np.array([[0, 0, 1, 0], [0, 1, 0, 0], [-1, 0, 0, 0], [0, 0, 0, 1]]) @ affine\n        )\n        res_dict = Spacingd(keys=""image"", pixdim=(1, 2, 3), diagonal=False, mode=""border"")(data_dict)\n        res_dict = Orientationd(keys=""image"", axcodes=""LPI"")(res_dict)\n        np.testing.assert_allclose(data_dict[""image.affine""], res_dict[""image.original_affine""])\n        np.testing.assert_allclose(\n            res_dict[""image.affine""],\n            np.array(\n                [\n                    [-3.0, 0.0, 0.0, 56.4005909],\n                    [0.0, -2.0, 0.0, 52.02241516],\n                    [0.0, 0.0, -1.0, 35.29789734],\n                    [0.0, 0.0, 0.0, 1.0],\n                ]\n            ),\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_map_transform.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom parameterized import parameterized\n\nfrom monai.transforms import MapTransform\n\nTEST_CASES = [\n    [""item"", (""item"",)],\n    [None, (None,)],\n    [[""item1"", ""item2""], (""item1"", ""item2"")],\n]\n\nTEST_ILL_CASES = [\n    [list()],\n    [tuple()],\n    [[list()]],\n]\n\n\nclass MapTest(MapTransform):\n    def __call__(self, data):\n        pass\n\n\nclass TestRandomizable(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_keys(self, keys, expected):\n        transform = MapTest(keys=keys)\n        self.assertEqual(transform.keys, expected)\n\n    @parameterized.expand(TEST_ILL_CASES)\n    def test_wrong_keys(self, keys):\n        with self.assertRaisesRegex(ValueError, """"):\n            MapTest(keys=keys)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_masked_dice_loss.py,29,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.losses import MaskedDiceLoss\n\nTEST_CASES = [\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""mask"": torch.tensor([[[[0.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.500,\n    ],\n    [  # shape: (2, 1, 2, 2), (2, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]], [[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 1.0], [1.0, 1.0]]], [[[1.0, 0.0], [1.0, 0.0]]]]),\n            ""mask"": torch.tensor([[[[1.0, 1.0], [1.0, 1.0]]], [[[1.0, 1.0], [0.0, 0.0]]]]),\n            ""smooth"": 1e-4,\n        },\n        0.422969,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": False, ""to_onehot_y"": True},\n        {\n            ""input"": torch.tensor([[[1.0, 1.0, 0.0], [0.0, 0.0, 1.0]], [[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]]),\n            ""target"": torch.tensor([[[0.0, 0.0, 1.0]], [[0.0, 1.0, 0.0]]]),\n            ""mask"": torch.tensor([[[1.0, 1.0, 1.0]], [[0.0, 1.0, 0.0]]]),\n            ""smooth"": 0.0,\n        },\n        0.0,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""mask"": torch.tensor([[[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.47033,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""sigmoid"": True, ""reduction"": ""none""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        [[0.296529, 0.415136], [0.599976, 0.428559]],\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.383713,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True, ""reduction"": ""sum""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        1.534853,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.307576,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True, ""squared_pred"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-5,\n        },\n        0.178337,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True, ""jaccard"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-5,\n        },\n        -0.059094,\n    ],\n]\n\n\nclass TestDiceLoss(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_shape(self, input_param, input_data, expected_val):\n        result = MaskedDiceLoss(**input_param).forward(**input_data)\n        np.testing.assert_allclose(result.detach().cpu().numpy(), expected_val, rtol=1e-5)\n\n    def test_ill_shape(self):\n        loss = MaskedDiceLoss()\n        with self.assertRaisesRegex(AssertionError, """"):\n            loss.forward(torch.ones((1, 2, 3)), torch.ones((4, 5, 6)))\n\n    def test_ill_opts(self):\n        with self.assertRaisesRegex(ValueError, """"):\n            MaskedDiceLoss(sigmoid=True, softmax=True)\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertRaisesRegex(ValueError, """"):\n            MaskedDiceLoss(reduction=""unknown"")(chn_input, chn_target)\n        with self.assertRaisesRegex(ValueError, """"):\n            MaskedDiceLoss(reduction=None)(chn_input, chn_target)\n\n    def test_input_warnings(self):\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertWarns(Warning):\n            loss = MaskedDiceLoss(include_background=False)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = MaskedDiceLoss(softmax=True)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = MaskedDiceLoss(to_onehot_y=True)\n            loss.forward(chn_input, chn_target)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_nifti_dataset.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport nibabel as nib\nimport numpy as np\n\nfrom monai.data import NiftiDataset\nfrom monai.transforms import Randomizable\n\nFILENAMES = [""test1.nii.gz"", ""test2.nii"", ""test3.nii.gz""]\n\n\nclass RandTest(Randomizable):\n    """"""\n    randomisable transform for testing.\n    """"""\n\n    def randomize(self):\n        self._a = self.R.random()\n\n    def __call__(self, data):\n        self.randomize()\n        return data + self._a\n\n\nclass TestNiftiDataset(unittest.TestCase):\n    def test_dataset(self):\n        tempdir = tempfile.mkdtemp()\n        full_names, ref_data = [], []\n        for filename in FILENAMES:\n            test_image = np.random.randint(0, 2, size=(4, 4, 4))\n            ref_data.append(test_image)\n            save_path = os.path.join(tempdir, filename)\n            full_names.append(save_path)\n            nib.save(nib.Nifti1Image(test_image, np.eye(4)), save_path)\n\n        # default loading no meta\n        dataset = NiftiDataset(full_names)\n        for d, ref in zip(dataset, ref_data):\n            np.testing.assert_allclose(d, ref, atol=1e-3)\n\n        # loading no meta, int\n        dataset = NiftiDataset(full_names, dtype=np.float16)\n        for d, ref in zip(dataset, ref_data):\n            self.assertEqual(d.dtype, np.float16)\n\n        # loading with meta, no transform\n        dataset = NiftiDataset(full_names, image_only=False)\n        for d_tuple, ref in zip(dataset, ref_data):\n            d, meta = d_tuple\n            np.testing.assert_allclose(d, ref, atol=1e-3)\n            np.testing.assert_allclose(meta[""original_affine""], np.eye(4))\n\n        # loading image/label, no meta\n        dataset = NiftiDataset(full_names, seg_files=full_names, image_only=True)\n        for d_tuple, ref in zip(dataset, ref_data):\n            img, seg = d_tuple\n            np.testing.assert_allclose(img, ref, atol=1e-3)\n            np.testing.assert_allclose(seg, ref, atol=1e-3)\n\n        # loading image/label, no meta\n        dataset = NiftiDataset(full_names, transform=lambda x: x + 1, image_only=True)\n        for d, ref in zip(dataset, ref_data):\n            np.testing.assert_allclose(d, ref + 1, atol=1e-3)\n\n        # set seg transform, but no seg_files\n        with self.assertRaises(TypeError):\n            dataset = NiftiDataset(full_names, seg_transform=lambda x: x + 1, image_only=True)\n            _ = dataset[0]\n\n        # set seg transform, but no seg_files\n        with self.assertRaises(TypeError):\n            dataset = NiftiDataset(full_names, seg_transform=lambda x: x + 1, image_only=True)\n            _ = dataset[0]\n\n        # loading image/label, with meta\n        dataset = NiftiDataset(\n            full_names, transform=lambda x: x + 1, seg_files=full_names, seg_transform=lambda x: x + 2, image_only=False\n        )\n        for d_tuple, ref in zip(dataset, ref_data):\n            img, seg, meta = d_tuple\n            np.testing.assert_allclose(img, ref + 1, atol=1e-3)\n            np.testing.assert_allclose(seg, ref + 2, atol=1e-3)\n            np.testing.assert_allclose(meta[""original_affine""], np.eye(4), atol=1e-3)\n\n        # loading image/label, with meta\n        dataset = NiftiDataset(\n            full_names, transform=lambda x: x + 1, seg_files=full_names, labels=[1, 2, 3], image_only=False\n        )\n        for idx, (d_tuple, ref) in enumerate(zip(dataset, ref_data)):\n            img, seg, label, meta = d_tuple\n            np.testing.assert_allclose(img, ref + 1, atol=1e-3)\n            np.testing.assert_allclose(seg, ref, atol=1e-3)\n            np.testing.assert_allclose(idx + 1, label)\n            np.testing.assert_allclose(meta[""original_affine""], np.eye(4), atol=1e-3)\n\n        # loading image/label, with sync. transform\n        dataset = NiftiDataset(\n            full_names, transform=RandTest(), seg_files=full_names, seg_transform=RandTest(), image_only=False\n        )\n        for d_tuple, ref in zip(dataset, ref_data):\n            img, seg, meta = d_tuple\n            np.testing.assert_allclose(img, seg, atol=1e-3)\n            self.assertTrue(not np.allclose(img, ref))\n            np.testing.assert_allclose(meta[""original_affine""], np.eye(4), atol=1e-3)\n        shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_nifti_header_revise.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport nibabel as nib\n\nimport numpy as np\n\nfrom monai.data import rectify_header_sform_qform\n\n\nclass TestRectifyHeaderSformQform(unittest.TestCase):\n    def test_revise_q(self):\n        img = nib.Nifti1Image(np.zeros((10, 10, 10)), np.eye(4))\n        img.header.set_zooms((0.1, 0.2, 0.3))\n        output = rectify_header_sform_qform(img)\n        expected = np.diag([0.1, 0.2, 0.3, 1.0])\n        np.testing.assert_allclose(output.affine, expected)\n\n    def test_revise_both(self):\n        img = nib.Nifti1Image(np.zeros((10, 10, 10)), np.eye(4))\n        img.header.set_sform(np.diag([5, 3, 4, 1]))\n        img.header.set_qform(np.diag([2, 3, 4, 1]))\n        img.header.set_zooms((0.1, 0.2, 0.3))\n        output = rectify_header_sform_qform(img)\n        expected = np.diag([0.1, 0.2, 0.3, 1.0])\n        np.testing.assert_allclose(output.affine, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_nifti_rw.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport tempfile\nimport unittest\n\nimport shutil\nimport nibabel as nib\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.data import write_nifti\nfrom monai.transforms import LoadNifti, Orientation, Spacing\nfrom tests.utils import make_nifti_image\n\nTEST_IMAGE = np.arange(24).reshape((2, 4, 3))\nTEST_AFFINE = np.array(\n    [[-5.3, 0.0, 0.0, 102.01], [0.0, 0.52, 2.17, -7.50], [-0.0, 1.98, -0.26, -23.12], [0.0, 0.0, 0.0, 1.0]]\n)\n\nTEST_CASES = [\n    [TEST_IMAGE, TEST_AFFINE, dict(as_closest_canonical=True, image_only=False), np.arange(24).reshape((2, 4, 3))],\n    [\n        TEST_IMAGE,\n        TEST_AFFINE,\n        dict(as_closest_canonical=True, image_only=True),\n        np.array(\n            [\n                [[12.0, 15.0, 18.0, 21.0], [13.0, 16.0, 19.0, 22.0], [14.0, 17.0, 20.0, 23.0]],\n                [[0.0, 3.0, 6.0, 9.0], [1.0, 4.0, 7.0, 10.0], [2.0, 5.0, 8.0, 11.0]],\n            ]\n        ),\n    ],\n    [TEST_IMAGE, TEST_AFFINE, dict(as_closest_canonical=False, image_only=True), np.arange(24).reshape((2, 4, 3))],\n    [TEST_IMAGE, TEST_AFFINE, dict(as_closest_canonical=False, image_only=False), np.arange(24).reshape((2, 4, 3))],\n    [TEST_IMAGE, None, dict(as_closest_canonical=False, image_only=False), np.arange(24).reshape((2, 4, 3))],\n]\n\n\nclass TestNiftiLoadRead(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_orientation(self, array, affine, reader_param, expected):\n        test_image = make_nifti_image(array, affine)\n\n        # read test cases\n        loader = LoadNifti(**reader_param)\n        load_result = loader(test_image)\n        if isinstance(load_result, tuple):\n            data_array, header = load_result\n        else:\n            data_array = load_result\n            header = None\n        if os.path.exists(test_image):\n            os.remove(test_image)\n\n        # write test cases\n        if header is not None:\n            write_nifti(data_array, test_image, header[""affine""], header.get(""original_affine"", None))\n        elif affine is not None:\n            write_nifti(data_array, test_image, affine)\n        saved = nib.load(test_image)\n        saved_affine = saved.affine\n        saved_data = saved.get_fdata()\n        if os.path.exists(test_image):\n            os.remove(test_image)\n\n        if affine is not None:\n            np.testing.assert_allclose(saved_affine, affine)\n        np.testing.assert_allclose(saved_data, expected)\n\n    def test_consistency(self):\n        np.set_printoptions(suppress=True, precision=3)\n        test_image = make_nifti_image(np.arange(64).reshape(1, 8, 8), np.diag([1.5, 1.5, 1.5, 1]))\n        data, header = LoadNifti(as_closest_canonical=False)(test_image)\n        data, original_affine, new_affine = Spacing([0.8, 0.8, 0.8])(\n            data[None], header[""affine""], interp_order=""nearest""\n        )\n        data, _, new_affine = Orientation(""ILP"")(data, new_affine)\n        if os.path.exists(test_image):\n            os.remove(test_image)\n        write_nifti(data[0], test_image, new_affine, original_affine, interp_order=0, mode=""reflect"")\n        saved = nib.load(test_image)\n        saved_data = saved.get_fdata()\n        np.testing.assert_allclose(saved_data, np.arange(64).reshape(1, 8, 8), atol=1e-7)\n        if os.path.exists(test_image):\n            os.remove(test_image)\n        write_nifti(\n            data[0], test_image, new_affine, original_affine, interp_order=0, mode=""reflect"", output_shape=(1, 8, 8)\n        )\n        saved = nib.load(test_image)\n        saved_data = saved.get_fdata()\n        np.testing.assert_allclose(saved_data, np.arange(64).reshape(1, 8, 8), atol=1e-7)\n        if os.path.exists(test_image):\n            os.remove(test_image)\n\n    def test_write_1d(self):\n        out_dir = tempfile.mkdtemp()\n        image_name = os.path.join(out_dir, ""test.nii.gz"")\n        img = np.arange(5).reshape(-1)\n        write_nifti(img, image_name, affine=np.diag([1, 1, 1]), target_affine=np.diag([1.4, 2.0, 1]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [0, 1, 3, 0])\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 1, 1, 1]))\n\n        image_name = os.path.join(out_dir, ""test1.nii.gz"")\n        img = np.arange(5).reshape(-1)\n        write_nifti(img, image_name, affine=[[1]], target_affine=[[1.4]])\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [0, 1, 3, 0])\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 1, 1, 1]))\n\n        image_name = os.path.join(out_dir, ""test2.nii.gz"")\n        img = np.arange(5).reshape(-1)\n        write_nifti(img, image_name, affine=np.diag([1.5, 1.5, 1.5]), target_affine=np.diag([1.5, 1.5, 1.5]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), np.arange(5).reshape(-1))\n        np.testing.assert_allclose(out.affine, np.diag([1.5, 1, 1, 1]))\n        shutil.rmtree(out_dir)\n\n    def test_write_2d(self):\n        out_dir = tempfile.mkdtemp()\n        image_name = os.path.join(out_dir, ""test.nii.gz"")\n        img = np.arange(6).reshape((2, 3))\n        write_nifti(img, image_name, affine=np.diag([1]), target_affine=np.diag([1.4]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [[0, 1, 2], [0, 0, 0]])\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 1, 1, 1]))\n\n        image_name = os.path.join(out_dir, ""test1.nii.gz"")\n        img = np.arange(5).reshape((1, 5))\n        write_nifti(img, image_name, affine=np.diag([1, 1, 1, 3, 3]), target_affine=np.diag([1.4, 2.0, 1, 3, 5]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [[0, 2, 4]])\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 2, 1, 1]))\n        shutil.rmtree(out_dir)\n\n    def test_write_3d(self):\n        out_dir = tempfile.mkdtemp()\n        image_name = os.path.join(out_dir, ""test.nii.gz"")\n        img = np.arange(6).reshape((1, 2, 3))\n        write_nifti(img, image_name, affine=np.diag([1]), target_affine=np.diag([1.4]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [[[0, 1, 2], [3, 4, 5]]])\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 1, 1, 1]))\n\n        image_name = os.path.join(out_dir, ""test1.nii.gz"")\n        img = np.arange(5).reshape((1, 1, 5))\n        write_nifti(img, image_name, affine=np.diag([1, 1, 1, 3, 3]), target_affine=np.diag([1.4, 2.0, 2, 3, 5]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [[[0, 2, 4]]])\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 2, 2, 1]))\n        shutil.rmtree(out_dir)\n\n    def test_write_4d(self):\n        out_dir = tempfile.mkdtemp()\n        image_name = os.path.join(out_dir, ""test.nii.gz"")\n        img = np.arange(6).reshape((1, 1, 3, 2))\n        write_nifti(img, image_name, affine=np.diag([1.4, 1]), target_affine=np.diag([1, 1.4, 1]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [[[[0, 1], [2, 3], [4, 5]]]])\n        np.testing.assert_allclose(out.affine, np.diag([1, 1.4, 1, 1]))\n\n        image_name = os.path.join(out_dir, ""test1.nii.gz"")\n        img = np.arange(5).reshape((1, 1, 5, 1))\n        write_nifti(img, image_name, affine=np.diag([1, 1, 1, 3, 3]), target_affine=np.diag([1.4, 2.0, 2, 3, 5]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), [[[[0], [2], [4]]]])\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 2, 2, 1]))\n        shutil.rmtree(out_dir)\n\n    def test_write_5d(self):\n        out_dir = tempfile.mkdtemp()\n        image_name = os.path.join(out_dir, ""test.nii.gz"")\n        img = np.arange(12).reshape((1, 1, 3, 2, 2))\n        write_nifti(img, image_name, affine=np.diag([1]), target_affine=np.diag([1.4]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(\n            out.get_fdata(),\n            np.array([[[[[0.0, 1.0], [2.0, 3.0]], [[4.0, 5.0], [6.0, 7.0]], [[8.0, 9.0], [10.0, 11.0]]]]]),\n        )\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 1, 1, 1]))\n\n        image_name = os.path.join(out_dir, ""test1.nii.gz"")\n        img = np.arange(10).reshape((1, 1, 5, 1, 2))\n        write_nifti(img, image_name, affine=np.diag([1, 1, 1, 3, 3]), target_affine=np.diag([1.4, 2.0, 2, 3, 5]))\n        out = nib.load(image_name)\n        np.testing.assert_allclose(out.get_fdata(), np.array([[[[[0.0, 1.0]], [[4.0, 5.0]], [[8.0, 9.0]]]]]))\n        np.testing.assert_allclose(out.affine, np.diag([1.4, 2, 2, 1]))\n        shutil.rmtree(out_dir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_nifti_saver.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport unittest\nimport torch\n\n\nfrom monai.data import NiftiSaver\n\n\nclass TestNiftiSaver(unittest.TestCase):\n    def test_saved_content(self):\n        default_dir = os.path.join(""."", ""tempdir"")\n        shutil.rmtree(default_dir, ignore_errors=True)\n\n        saver = NiftiSaver(output_dir=default_dir, output_postfix=""seg"", output_ext="".nii.gz"")\n\n        meta_data = {""filename_or_obj"": [""testfile"" + str(i) for i in range(8)]}\n        saver.save_batch(torch.zeros(8, 1, 2, 2), meta_data)\n        for i in range(8):\n            filepath = os.path.join(""testfile"" + str(i), ""testfile"" + str(i) + ""_seg.nii.gz"")\n            self.assertTrue(os.path.exists(os.path.join(default_dir, filepath)))\n        shutil.rmtree(default_dir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_normalize_intensity.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import NormalizeIntensity\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASE_1 = [{""nonzero"": True}, np.array([0.0, 3.0, 0.0, 4.0]), np.array([0.0, -1.0, 0.0, 1.0])]\n\nTEST_CASE_2 = [\n    {""subtrahend"": np.array([3.5, 3.5, 3.5, 3.5]), ""divisor"": np.array([0.5, 0.5, 0.5, 0.5]), ""nonzero"": True},\n    np.array([0.0, 3.0, 0.0, 4.0]),\n    np.array([0.0, -1.0, 0.0, 1.0]),\n]\n\nTEST_CASE_3 = [{""nonzero"": True}, np.array([0.0, 0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0, 0.0])]\n\n\nclass TestNormalizeIntensity(NumpyImageTestCase2D):\n    def test_default(self):\n        normalizer = NormalizeIntensity()\n        normalized = normalizer(self.imt)\n        expected = (self.imt - np.mean(self.imt)) / np.std(self.imt)\n        np.testing.assert_allclose(normalized, expected, rtol=1e-6)\n\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_nonzero(self, input_param, input_data, expected_data):\n        normalizer = NormalizeIntensity(**input_param)\n        np.testing.assert_allclose(expected_data, normalizer(input_data))\n\n    def test_channel_wise(self):\n        normalizer = NormalizeIntensity(nonzero=True, channel_wise=True)\n        input_data = np.array([[0.0, 3.0, 0.0, 4.0], [0.0, 4.0, 0.0, 5.0]])\n        expected = np.array([[0.0, -1.0, 0.0, 1.0], [0.0, -1.0, 0.0, 1.0]])\n        np.testing.assert_allclose(expected, normalizer(input_data))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_normalize_intensityd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import NormalizeIntensityd\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASE_1 = [\n    {""keys"": [""img""], ""nonzero"": True},\n    {""img"": np.array([0.0, 3.0, 0.0, 4.0])},\n    np.array([0.0, -1.0, 0.0, 1.0]),\n]\n\nTEST_CASE_2 = [\n    {\n        ""keys"": [""img""],\n        ""subtrahend"": np.array([3.5, 3.5, 3.5, 3.5]),\n        ""divisor"": np.array([0.5, 0.5, 0.5, 0.5]),\n        ""nonzero"": True,\n    },\n    {""img"": np.array([0.0, 3.0, 0.0, 4.0])},\n    np.array([0.0, -1.0, 0.0, 1.0]),\n]\n\nTEST_CASE_3 = [\n    {""keys"": [""img""], ""nonzero"": True},\n    {""img"": np.array([0.0, 0.0, 0.0, 0.0])},\n    np.array([0.0, 0.0, 0.0, 0.0]),\n]\n\n\nclass TestNormalizeIntensityd(NumpyImageTestCase2D):\n    def test_image_normalize_intensityd(self):\n        key = ""img""\n        normalizer = NormalizeIntensityd(keys=[key])\n        normalized = normalizer({key: self.imt})\n        expected = (self.imt - np.mean(self.imt)) / np.std(self.imt)\n        np.testing.assert_allclose(normalized[key], expected, rtol=1e-6)\n\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_nonzero(self, input_param, input_data, expected_data):\n        normalizer = NormalizeIntensityd(**input_param)\n        np.testing.assert_allclose(expected_data, normalizer(input_data)[""img""])\n\n    def test_channel_wise(self):\n        key = ""img""\n        normalizer = NormalizeIntensityd(keys=key, nonzero=True, channel_wise=True)\n        input_data = {key: np.array([[0.0, 3.0, 0.0, 4.0], [0.0, 4.0, 0.0, 5.0]])}\n        expected = np.array([[0.0, -1.0, 0.0, 1.0], [0.0, -1.0, 0.0, 1.0]])\n        np.testing.assert_allclose(expected, normalizer(input_data)[key])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_orientation.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport nibabel as nib\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import Orientation, create_rotate, create_translate\n\nTEST_CASES = [\n    [\n        {""axcodes"": ""RAS""},\n        np.arange(12).reshape((2, 1, 2, 3)),\n        {""affine"": np.eye(4)},\n        np.arange(12).reshape((2, 1, 2, 3)),\n        ""RAS"",\n    ],\n    [\n        {""axcodes"": ""ALS""},\n        np.arange(12).reshape((2, 1, 2, 3)),\n        {""affine"": np.diag([-1, -1, 1, 1])},\n        np.array([[[[3, 4, 5]], [[0, 1, 2]]], [[[9, 10, 11]], [[6, 7, 8]]]]),\n        ""ALS"",\n    ],\n    [\n        {""axcodes"": ""RAS""},\n        np.arange(12).reshape((2, 1, 2, 3)),\n        {""affine"": np.diag([-1, -1, 1, 1])},\n        np.array([[[[3, 4, 5], [0, 1, 2]]], [[[9, 10, 11], [6, 7, 8]]]]),\n        ""RAS"",\n    ],\n    [\n        {""axcodes"": ""AL""},\n        np.arange(6).reshape((2, 1, 3)),\n        {""affine"": np.eye(3)},\n        np.array([[[0], [1], [2]], [[3], [4], [5]]]),\n        ""AL"",\n    ],\n    [{""axcodes"": ""L""}, np.arange(6).reshape((2, 3)), {""affine"": np.eye(2)}, np.array([[2, 1, 0], [5, 4, 3]]), ""L""],\n    [{""axcodes"": ""L""}, np.arange(6).reshape((2, 3)), {""affine"": np.eye(2)}, np.array([[2, 1, 0], [5, 4, 3]]), ""L""],\n    [{""axcodes"": ""L""}, np.arange(6).reshape((2, 3)), {""affine"": np.diag([-1, 1])}, np.arange(6).reshape((2, 3)), ""L""],\n    [\n        {""axcodes"": ""LPS""},\n        np.arange(12).reshape((2, 1, 2, 3)),\n        {\n            ""affine"": create_translate(3, (10, 20, 30))\n            @ create_rotate(3, (np.pi / 2, np.pi / 2, np.pi / 4))\n            @ np.diag([-1, 1, 1, 1])\n        },\n        np.array([[[[2, 5]], [[1, 4]], [[0, 3]]], [[[8, 11]], [[7, 10]], [[6, 9]]]]),\n        ""LPS"",\n    ],\n    [\n        {""as_closest_canonical"": True},\n        np.arange(12).reshape((2, 1, 2, 3)),\n        {\n            ""affine"": create_translate(3, (10, 20, 30))\n            @ create_rotate(3, (np.pi / 2, np.pi / 2, np.pi / 4))\n            @ np.diag([-1, 1, 1, 1])\n        },\n        np.array([[[[0, 3]], [[1, 4]], [[2, 5]]], [[[6, 9]], [[7, 10]], [[8, 11]]]]),\n        ""RAS"",\n    ],\n    [\n        {""as_closest_canonical"": True},\n        np.arange(6).reshape((1, 2, 3)),\n        {""affine"": create_translate(2, (10, 20)) @ create_rotate(2, (np.pi / 3)) @ np.diag([-1, -0.2, 1])},\n        np.array([[[3, 0], [4, 1], [5, 2]]]),\n        ""RA"",\n    ],\n    [\n        {""axcodes"": ""LP""},\n        np.arange(6).reshape((1, 2, 3)),\n        {""affine"": create_translate(2, (10, 20)) @ create_rotate(2, (np.pi / 3)) @ np.diag([-1, -0.2, 1])},\n        np.array([[[2, 5], [1, 4], [0, 3]]]),\n        ""LP"",\n    ],\n    [\n        {""axcodes"": ""LPID"", ""labels"": tuple(zip(""LPIC"", ""RASD""))},\n        np.zeros((1, 2, 3, 4, 5)),\n        {""affine"": np.diag([-1, -0.2, -1, 1, 1])},\n        np.zeros((1, 2, 3, 4, 5)),\n        ""LPID"",\n    ],\n    [\n        {""as_closest_canonical"": True, ""labels"": tuple(zip(""LPIC"", ""RASD""))},\n        np.zeros((1, 2, 3, 4, 5)),\n        {""affine"": np.diag([-1, -0.2, -1, 1, 1])},\n        np.zeros((1, 2, 3, 4, 5)),\n        ""RASD"",\n    ],\n]\n\nILL_CASES = [\n    # no axcodes or as_cloest_canonical\n    [{}, np.arange(6).reshape((2, 3)), ""L""],\n    # too short axcodes\n    [{""axcodes"": ""RA""}, np.arange(12).reshape((2, 1, 2, 3)), {""affine"": np.eye(4)}],\n]\n\n\nclass TestOrientationCase(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_ornt(self, init_param, img, data_param, expected_data, expected_code):\n        ornt = Orientation(**init_param)\n        res = ornt(img, **data_param)\n        np.testing.assert_allclose(res[0], expected_data)\n        original_affine = data_param[""affine""]\n        np.testing.assert_allclose(original_affine, res[1])\n        new_code = nib.orientations.aff2axcodes(res[2], labels=ornt.labels)\n        self.assertEqual("""".join(new_code), expected_code)\n\n    @parameterized.expand(ILL_CASES)\n    def test_bad_params(self, init_param, img, data_param):\n        with self.assertRaises(ValueError):\n            Orientation(**init_param)(img, **data_param)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_orientationd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport nibabel as nib\nimport numpy as np\n\nfrom monai.transforms import Orientationd\n\n\nclass TestOrientationdCase(unittest.TestCase):\n    def test_orntd(self):\n        data = {""seg"": np.ones((2, 1, 2, 3)), ""seg.affine"": np.eye(4)}\n        ornt = Orientationd(keys=""seg"", axcodes=""RAS"")\n        res = ornt(data)\n        np.testing.assert_allclose(res[""seg""].shape, (2, 1, 2, 3))\n        code = nib.aff2axcodes(res[""seg.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""R"", ""A"", ""S""))\n\n    def test_orntd_3d(self):\n        data = {\n            ""seg"": np.ones((2, 1, 2, 3)),\n            ""img"": np.ones((2, 1, 2, 3)),\n            ""seg.affine"": np.eye(4),\n            ""img.affine"": np.eye(4),\n        }\n        ornt = Orientationd(keys=(""img"", ""seg""), axcodes=""PLI"")\n        res = ornt(data)\n        np.testing.assert_allclose(res[""img""].shape, (2, 2, 1, 3))\n        np.testing.assert_allclose(res[""seg""].shape, (2, 2, 1, 3))\n        code = nib.aff2axcodes(res[""seg.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""P"", ""L"", ""I""))\n        code = nib.aff2axcodes(res[""img.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""P"", ""L"", ""I""))\n\n    def test_orntd_2d(self):\n        data = {""seg"": np.ones((2, 1, 3)), ""img"": np.ones((2, 1, 3)), ""seg.affine"": np.eye(4), ""img.affine"": np.eye(4)}\n        ornt = Orientationd(keys=(""img"", ""seg""), axcodes=""PLI"")\n        res = ornt(data)\n        np.testing.assert_allclose(res[""img""].shape, (2, 3, 1))\n        code = nib.aff2axcodes(res[""seg.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""P"", ""L"", ""S""))\n        code = nib.aff2axcodes(res[""img.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""P"", ""L"", ""S""))\n\n    def test_orntd_1d(self):\n        data = {""seg"": np.ones((2, 3)), ""img"": np.ones((2, 3)), ""seg.affine"": np.eye(4), ""img.affine"": np.eye(4)}\n        ornt = Orientationd(keys=(""img"", ""seg""), axcodes=""L"")\n        res = ornt(data)\n        np.testing.assert_allclose(res[""img""].shape, (2, 3))\n        code = nib.aff2axcodes(res[""seg.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""L"", ""A"", ""S""))\n        code = nib.aff2axcodes(res[""img.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""L"", ""A"", ""S""))\n\n    def test_orntd_canonical(self):\n        data = {\n            ""seg"": np.ones((2, 1, 2, 3)),\n            ""img"": np.ones((2, 1, 2, 3)),\n            ""seg.affine"": np.eye(4),\n            ""img.affine"": np.eye(4),\n        }\n        ornt = Orientationd(keys=(""img"", ""seg""), as_closest_canonical=True)\n        res = ornt(data)\n        np.testing.assert_allclose(res[""img""].shape, (2, 1, 2, 3))\n        np.testing.assert_allclose(res[""seg""].shape, (2, 1, 2, 3))\n        code = nib.aff2axcodes(res[""seg.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""R"", ""A"", ""S""))\n        code = nib.aff2axcodes(res[""img.affine""], ornt.ornt_transform.labels)\n        self.assertEqual(code, (""R"", ""A"", ""S""))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_parallel_execution.py,8,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport warnings\n\nimport torch\n\nfrom monai.engines import create_multigpu_supervised_trainer\nfrom tests.utils import expect_failure_if_no_gpu\n\n\ndef fake_loss(y_pred, y):\n    return (y_pred[0] + y).sum()\n\n\ndef fake_data_stream():\n    while True:\n        yield torch.rand((10, 1, 64, 64)), torch.rand((10, 1, 64, 64))\n\n\nclass TestParallelExecution(unittest.TestCase):\n    """"""\n    Tests single GPU, multi GPU, and CPU execution with the Ignite supervised trainer.\n    """"""\n\n    @expect_failure_if_no_gpu\n    def test_single_gpu(self):\n        net = torch.nn.Conv2d(1, 1, 3, padding=1)\n        opt = torch.optim.Adam(net.parameters(), 1e-3)\n        trainer = create_multigpu_supervised_trainer(net, opt, fake_loss, [torch.device(""cuda:0"")])\n        trainer.run(fake_data_stream(), 2, 2)\n\n    @expect_failure_if_no_gpu\n    def test_multi_gpu(self):\n        net = torch.nn.Conv2d(1, 1, 3, padding=1)\n        opt = torch.optim.Adam(net.parameters(), 1e-3)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")  # ignore warnings about imbalanced GPU memory\n\n            trainer = create_multigpu_supervised_trainer(net, opt, fake_loss, None)\n\n        trainer.run(fake_data_stream(), 2, 2)\n\n    def test_cpu(self):\n        net = torch.nn.Conv2d(1, 1, 3, padding=1)\n        opt = torch.optim.Adam(net.parameters(), 1e-3)\n        trainer = create_multigpu_supervised_trainer(net, opt, fake_loss, [])\n        trainer.run(fake_data_stream(), 2, 2)\n'"
tests/test_persistentdataset.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport os\nimport shutil\nimport numpy as np\nimport tempfile\nimport nibabel as nib\nfrom parameterized import parameterized\nfrom monai.data import PersistentDataset\nfrom monai.transforms import Compose, LoadNiftid, SimulateDelayd\n\nTEST_CASE_1 = [(128, 128, 128)]\n\n\nclass TestDataset(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, expected_shape):\n        test_image = nib.Nifti1Image(np.random.randint(0, 2, size=[128, 128, 128]), np.eye(4))\n        tempdir = tempfile.mkdtemp()\n        nib.save(test_image, os.path.join(tempdir, ""test_image1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_label1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_extra1.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_image2.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_label2.nii.gz""))\n        nib.save(test_image, os.path.join(tempdir, ""test_extra2.nii.gz""))\n        test_data = [\n            {\n                ""image"": os.path.join(tempdir, ""test_image1.nii.gz""),\n                ""label"": os.path.join(tempdir, ""test_label1.nii.gz""),\n                ""extra"": os.path.join(tempdir, ""test_extra1.nii.gz""),\n            },\n            {\n                ""image"": os.path.join(tempdir, ""test_image2.nii.gz""),\n                ""label"": os.path.join(tempdir, ""test_label2.nii.gz""),\n                ""extra"": os.path.join(tempdir, ""test_extra2.nii.gz""),\n            },\n        ]\n        test_transform = Compose(\n            [\n                LoadNiftid(keys=[""image"", ""label"", ""extra""]),\n                SimulateDelayd(keys=[""image"", ""label"", ""extra""], delay_time=[1e-7, 1e-6, 1e-5]),\n            ]\n        )\n\n        dataset_precached = PersistentDataset(data=test_data, transform=test_transform, cache_dir=tempdir)\n        data1_precached = dataset_precached[0]\n        data2_precached = dataset_precached[1]\n\n        dataset_postcached = PersistentDataset(data=test_data, transform=test_transform, cache_dir=tempdir)\n        data1_postcached = dataset_postcached[0]\n        data2_postcached = dataset_postcached[1]\n        shutil.rmtree(tempdir)\n\n        self.assertTupleEqual(data1_precached[""image""].shape, expected_shape)\n        self.assertTupleEqual(data1_precached[""label""].shape, expected_shape)\n        self.assertTupleEqual(data1_precached[""extra""].shape, expected_shape)\n        self.assertTupleEqual(data2_precached[""image""].shape, expected_shape)\n        self.assertTupleEqual(data2_precached[""label""].shape, expected_shape)\n        self.assertTupleEqual(data2_precached[""extra""].shape, expected_shape)\n\n        self.assertTupleEqual(data1_postcached[""image""].shape, expected_shape)\n        self.assertTupleEqual(data1_postcached[""label""].shape, expected_shape)\n        self.assertTupleEqual(data1_postcached[""extra""].shape, expected_shape)\n        self.assertTupleEqual(data2_postcached[""image""].shape, expected_shape)\n        self.assertTupleEqual(data2_postcached[""label""].shape, expected_shape)\n        self.assertTupleEqual(data2_postcached[""extra""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_plot_2d_or_3d_image.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport glob\nimport os\nimport tempfile\nimport shutil\nimport unittest\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch\nfrom parameterized import parameterized\nfrom monai.visualize import plot_2d_or_3d_image\n\nTEST_CASE_1 = [(1, 1, 10, 10)]\n\nTEST_CASE_2 = [(1, 3, 10, 10)]\n\nTEST_CASE_3 = [(1, 4, 10, 10)]\n\nTEST_CASE_4 = [(1, 1, 10, 10, 10)]\n\nTEST_CASE_5 = [(1, 3, 10, 10, 10)]\n\n\nclass TestPlot2dOr3dImage(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5])\n    def test_tb_image_shape(self, shape):\n        tempdir = tempfile.mkdtemp()\n        shutil.rmtree(tempdir, ignore_errors=True)\n\n        plot_2d_or_3d_image(torch.zeros(shape), 0, SummaryWriter(log_dir=tempdir))\n\n        self.assertTrue(os.path.exists(tempdir))\n        self.assertTrue(len(glob.glob(tempdir)) > 0)\n        shutil.rmtree(tempdir, ignore_errors=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_png_rw.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport tempfile\nimport unittest\n\nfrom monai.data import write_png, InterpolationCode\nfrom skimage import io, transform\nimport numpy as np\n\n\nclass TestPngWrite(unittest.TestCase):\n    def test_write_gray(self):\n        with tempfile.TemporaryDirectory() as out_dir:\n            image_name = os.path.join(out_dir, ""test.png"")\n            img = np.random.rand(2, 3, 1)\n            img_save_val = 255 * img\n            # saving with io.imsave (h, w, 1) will only give us (h,w) while reading it back.\n            img_save_val = img_save_val[:, :, 0].astype(np.uint8)\n            write_png(img, image_name, scale=True)\n            out = io.imread(image_name)\n            np.testing.assert_allclose(out, img_save_val)\n\n    def test_write_rgb(self):\n        with tempfile.TemporaryDirectory() as out_dir:\n            image_name = os.path.join(out_dir, ""test.png"")\n            img = np.random.rand(2, 3, 3)\n            img_save_val = (255 * img).astype(np.uint8)\n            write_png(img, image_name, scale=True)\n            out = io.imread(image_name)\n            np.testing.assert_allclose(out, img_save_val)\n\n    def test_write_output_shape(self):\n        with tempfile.TemporaryDirectory() as out_dir:\n            image_name = os.path.join(out_dir, ""test.png"")\n            img = np.random.rand(2, 2, 3)\n            write_png(img, image_name, (4, 4), scale=True)\n            img_save_val = transform.resize(\n                img, (4, 4), order=InterpolationCode.SPLINE3, mode=""constant"", cval=0, preserve_range=True\n            )\n            img_save_val = (255 * img_save_val).astype(np.uint8)\n            out = io.imread(image_name)\n            np.testing.assert_allclose(out, img_save_val)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_png_saver.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport shutil\nimport unittest\nimport torch\n\nfrom monai.data import PNGSaver\n\n\nclass TestPNGSaver(unittest.TestCase):\n    def test_saved_content(self):\n        default_dir = os.path.join(""."", ""tempdir"")\n        shutil.rmtree(default_dir, ignore_errors=True)\n\n        saver = PNGSaver(output_dir=default_dir, output_postfix=""seg"", output_ext="".png"")\n\n        meta_data = {""filename_or_obj"": [""testfile"" + str(i) for i in range(8)]}\n        saver.save_batch(torch.randint(1, 200, (8, 1, 2, 2)), meta_data)\n        for i in range(8):\n            filepath = os.path.join(""testfile"" + str(i), ""testfile"" + str(i) + ""_seg.png"")\n            self.assertTrue(os.path.exists(os.path.join(default_dir, filepath)))\n        shutil.rmtree(default_dir)\n\n    def test_saved_content_three_channel(self):\n        default_dir = os.path.join(""."", ""tempdir"")\n        shutil.rmtree(default_dir, ignore_errors=True)\n\n        saver = PNGSaver(output_dir=default_dir, output_postfix=""seg"", output_ext="".png"")\n\n        meta_data = {""filename_or_obj"": [""testfile"" + str(i) for i in range(8)]}\n        saver.save_batch(torch.randint(1, 200, (8, 3, 2, 2)), meta_data)\n        for i in range(8):\n            filepath = os.path.join(""testfile"" + str(i), ""testfile"" + str(i) + ""_seg.png"")\n            self.assertTrue(os.path.exists(os.path.join(default_dir, filepath)))\n        shutil.rmtree(default_dir)\n\n    def test_saved_content_spatial_size(self):\n\n        default_dir = os.path.join(""."", ""tempdir"")\n        shutil.rmtree(default_dir, ignore_errors=True)\n\n        saver = PNGSaver(output_dir=default_dir, output_postfix=""seg"", output_ext="".png"")\n\n        meta_data = {\n            ""filename_or_obj"": [""testfile"" + str(i) for i in range(8)],\n            ""spatial_shape"": [(4, 4) for i in range(8)],\n        }\n        saver.save_batch(torch.randint(1, 200, (8, 1, 2, 2)), meta_data)\n        for i in range(8):\n            filepath = os.path.join(""testfile"" + str(i), ""testfile"" + str(i) + ""_seg.png"")\n            self.assertTrue(os.path.exists(os.path.join(default_dir, filepath)))\n\n        shutil.rmtree(default_dir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_query_memory.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom tests.utils import query_memory\n\n\nclass TestQueryMemory(unittest.TestCase):\n    def test_output_str(self):\n        self.assertTrue(isinstance(query_memory(2), str))\n        all_device = query_memory(-1)\n        self.assertTrue(isinstance(all_device, str))\n        self.assertEqual(query_memory(""test""), """")\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_adjust_contrast.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import RandAdjustContrast\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASE_1 = [(0.5, 4.5)]\n\nTEST_CASE_2 = [1.5]\n\n\nclass TestRandAdjustContrast(NumpyImageTestCase2D):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_correct_results(self, gamma):\n        adjuster = RandAdjustContrast(prob=1.0, gamma=gamma)\n        result = adjuster(self.imt)\n        epsilon = 1e-7\n        img_min = self.imt.min()\n        img_range = self.imt.max() - img_min\n        expected = (\n            np.power(((self.imt - img_min) / float(img_range + epsilon)), adjuster.gamma_value) * img_range + img_min\n        )\n        np.testing.assert_allclose(expected, result, rtol=1e-05)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_adjust_contrastd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import RandAdjustContrastd\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASE_1 = [(0.5, 4.5)]\n\nTEST_CASE_2 = [1.5]\n\n\nclass TestRandAdjustContrastd(NumpyImageTestCase2D):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_correct_results(self, gamma):\n        adjuster = RandAdjustContrastd(""img"", prob=1.0, gamma=gamma)\n        result = adjuster({""img"": self.imt})\n        epsilon = 1e-7\n        img_min = self.imt.min()\n        img_range = self.imt.max() - img_min\n        expected = (\n            np.power(((self.imt - img_min) / float(img_range + epsilon)), adjuster.gamma_value) * img_range + img_min\n        )\n        np.testing.assert_allclose(expected, result[""img""], rtol=1e-05)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_affine.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandAffine\n\nTEST_CASES = [\n    [\n        dict(as_tensor_output=False, device=None),\n        {""img"": torch.ones((3, 3, 3)), ""spatial_size"": (2, 2)},\n        np.ones((3, 2, 2)),\n    ],\n    [\n        dict(as_tensor_output=True, device=None),\n        {""img"": torch.ones((1, 3, 3, 3)), ""spatial_size"": (2, 2, 2)},\n        torch.ones((1, 2, 2, 2)),\n    ],\n    [\n        dict(\n            prob=0.9,\n            rotate_range=(np.pi / 2,),\n            shear_range=[1, 2],\n            translate_range=[2, 1],\n            as_tensor_output=True,\n            spatial_size=(2, 2, 2),\n            device=None,\n        ),\n        {""img"": torch.ones((1, 3, 3, 3)), ""mode"": ""bilinear""},\n        torch.tensor([[[[0.0000, 0.6577], [0.9911, 1.0000]], [[0.7781, 1.0000], [1.0000, 0.4000]]]]),\n    ],\n    [\n        dict(\n            prob=0.9,\n            rotate_range=(np.pi / 2,),\n            shear_range=[1, 2],\n            translate_range=[2, 1],\n            scale_range=[0.1, 0.2],\n            as_tensor_output=True,\n            device=None,\n        ),\n        {""img"": torch.arange(64).reshape((1, 8, 8)), ""spatial_size"": (3, 3)},\n        torch.tensor([[[16.9127, 13.3079, 9.7031], [26.8129, 23.2081, 19.6033], [36.7131, 33.1083, 29.5035]]]),\n    ],\n]\n\n\nclass TestRandAffine(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_affine(self, input_param, input_data, expected_val):\n        g = RandAffine(**input_param)\n        g.set_random_state(123)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_affine_grid.py,8,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandAffineGrid\n\nTEST_CASES = [\n    [{""as_tensor_output"": False, ""device"": None}, {""grid"": torch.ones((3, 3, 3))}, np.ones((3, 3, 3))],\n    [\n        {""rotate_range"": (1, 2), ""translate_range"": (3, 3, 3)},\n        {""grid"": torch.arange(0, 27).reshape((3, 3, 3))},\n        torch.tensor(\n            np.array(\n                [\n                    [\n                        [-32.81998, -33.910976, -35.001972],\n                        [-36.092968, -37.183964, -38.27496],\n                        [-39.36596, -40.456955, -41.54795],\n                    ],\n                    [[2.1380205, 3.1015975, 4.0651755], [5.028752, 5.9923296, 6.955907], [7.919484, 8.883063, 9.84664]],\n                    [[18.0, 19.0, 20.0], [21.0, 22.0, 23.0], [24.0, 25.0, 26.0]],\n                ]\n            )\n        ),\n    ],\n    [\n        {""translate_range"": (3, 3, 3), ""as_tensor_output"": False, ""device"": torch.device(""cpu:0"")},\n        {""spatial_size"": (3, 3, 3)},\n        np.array(\n            [\n                [\n                    [\n                        [0.17881513, 0.17881513, 0.17881513],\n                        [0.17881513, 0.17881513, 0.17881513],\n                        [0.17881513, 0.17881513, 0.17881513],\n                    ],\n                    [\n                        [1.1788151, 1.1788151, 1.1788151],\n                        [1.1788151, 1.1788151, 1.1788151],\n                        [1.1788151, 1.1788151, 1.1788151],\n                    ],\n                    [\n                        [2.1788151, 2.1788151, 2.1788151],\n                        [2.1788151, 2.1788151, 2.1788151],\n                        [2.1788151, 2.1788151, 2.1788151],\n                    ],\n                ],\n                [\n                    [\n                        [-2.283164, -2.283164, -2.283164],\n                        [-1.283164, -1.283164, -1.283164],\n                        [-0.28316402, -0.28316402, -0.28316402],\n                    ],\n                    [\n                        [-2.283164, -2.283164, -2.283164],\n                        [-1.283164, -1.283164, -1.283164],\n                        [-0.28316402, -0.28316402, -0.28316402],\n                    ],\n                    [\n                        [-2.283164, -2.283164, -2.283164],\n                        [-1.283164, -1.283164, -1.283164],\n                        [-0.28316402, -0.28316402, -0.28316402],\n                    ],\n                ],\n                [\n                    [\n                        [-2.6388912, -1.6388912, -0.6388912],\n                        [-2.6388912, -1.6388912, -0.6388912],\n                        [-2.6388912, -1.6388912, -0.6388912],\n                    ],\n                    [\n                        [-2.6388912, -1.6388912, -0.6388912],\n                        [-2.6388912, -1.6388912, -0.6388912],\n                        [-2.6388912, -1.6388912, -0.6388912],\n                    ],\n                    [\n                        [-2.6388912, -1.6388912, -0.6388912],\n                        [-2.6388912, -1.6388912, -0.6388912],\n                        [-2.6388912, -1.6388912, -0.6388912],\n                    ],\n                ],\n                [\n                    [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],\n                    [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],\n                    [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],\n                ],\n            ]\n        ),\n    ],\n    [\n        {""rotate_range"": (1.0, 1.0, 1.0), ""shear_range"": (0.1,), ""scale_range"": (1.2,)},\n        {""grid"": torch.arange(0, 108).reshape((4, 3, 3, 3))},\n        torch.tensor(\n            np.array(\n                [\n                    [\n                        [\n                            [-9.4201e00, -8.1672e00, -6.9143e00],\n                            [-5.6614e00, -4.4085e00, -3.1556e00],\n                            [-1.9027e00, -6.4980e-01, 6.0310e-01],\n                        ],\n                        [\n                            [1.8560e00, 3.1089e00, 4.3618e00],\n                            [5.6147e00, 6.8676e00, 8.1205e00],\n                            [9.3734e00, 1.0626e01, 1.1879e01],\n                        ],\n                        [\n                            [1.3132e01, 1.4385e01, 1.5638e01],\n                            [1.6891e01, 1.8144e01, 1.9397e01],\n                            [2.0650e01, 2.1902e01, 2.3155e01],\n                        ],\n                    ],\n                    [\n                        [\n                            [9.9383e-02, -4.8845e-01, -1.0763e00],\n                            [-1.6641e00, -2.2519e00, -2.8398e00],\n                            [-3.4276e00, -4.0154e00, -4.6032e00],\n                        ],\n                        [\n                            [-5.1911e00, -5.7789e00, -6.3667e00],\n                            [-6.9546e00, -7.5424e00, -8.1302e00],\n                            [-8.7180e00, -9.3059e00, -9.8937e00],\n                        ],\n                        [\n                            [-1.0482e01, -1.1069e01, -1.1657e01],\n                            [-1.2245e01, -1.2833e01, -1.3421e01],\n                            [-1.4009e01, -1.4596e01, -1.5184e01],\n                        ],\n                    ],\n                    [\n                        [\n                            [5.9635e01, 6.1199e01, 6.2764e01],\n                            [6.4328e01, 6.5892e01, 6.7456e01],\n                            [6.9021e01, 7.0585e01, 7.2149e01],\n                        ],\n                        [\n                            [7.3714e01, 7.5278e01, 7.6842e01],\n                            [7.8407e01, 7.9971e01, 8.1535e01],\n                            [8.3099e01, 8.4664e01, 8.6228e01],\n                        ],\n                        [\n                            [8.7792e01, 8.9357e01, 9.0921e01],\n                            [9.2485e01, 9.4049e01, 9.5614e01],\n                            [9.7178e01, 9.8742e01, 1.0031e02],\n                        ],\n                    ],\n                    [\n                        [\n                            [8.1000e01, 8.2000e01, 8.3000e01],\n                            [8.4000e01, 8.5000e01, 8.6000e01],\n                            [8.7000e01, 8.8000e01, 8.9000e01],\n                        ],\n                        [\n                            [9.0000e01, 9.1000e01, 9.2000e01],\n                            [9.3000e01, 9.4000e01, 9.5000e01],\n                            [9.6000e01, 9.7000e01, 9.8000e01],\n                        ],\n                        [\n                            [9.9000e01, 1.0000e02, 1.0100e02],\n                            [1.0200e02, 1.0300e02, 1.0400e02],\n                            [1.0500e02, 1.0600e02, 1.0700e02],\n                        ],\n                    ],\n                ]\n            )\n        ),\n    ],\n]\n\n\nclass TestRandAffineGrid(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_affine_grid(self, input_param, input_data, expected_val):\n        g = RandAffineGrid(**input_param)\n        g.set_random_state(123)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_affined.py,11,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandAffined\n\nTEST_CASES = [\n    [\n        dict(as_tensor_output=False, device=None, spatial_size=(2, 2), keys=(""img"", ""seg"")),\n        {""img"": torch.ones((3, 3, 3)), ""seg"": torch.ones((3, 3, 3))},\n        np.ones((3, 2, 2)),\n    ],\n    [\n        dict(as_tensor_output=True, device=None, spatial_size=(2, 2, 2), keys=(""img"", ""seg"")),\n        {""img"": torch.ones((1, 3, 3, 3)), ""seg"": torch.ones((1, 3, 3, 3))},\n        torch.ones((1, 2, 2, 2)),\n    ],\n    [\n        dict(\n            prob=0.9,\n            rotate_range=(np.pi / 2,),\n            shear_range=[1, 2],\n            translate_range=[2, 1],\n            as_tensor_output=True,\n            spatial_size=(2, 2, 2),\n            device=None,\n            keys=(""img"", ""seg""),\n            mode=""bilinear"",\n        ),\n        {""img"": torch.ones((1, 3, 3, 3)), ""seg"": torch.ones((1, 3, 3, 3))},\n        torch.tensor([[[[0.0000, 0.6577], [0.9911, 1.0000]], [[0.7781, 1.0000], [1.0000, 0.4000]]]]),\n    ],\n    [\n        dict(\n            prob=0.9,\n            rotate_range=(np.pi / 2,),\n            shear_range=[1, 2],\n            translate_range=[2, 1],\n            scale_range=[0.1, 0.2],\n            as_tensor_output=True,\n            spatial_size=(3, 3),\n            keys=(""img"", ""seg""),\n            device=None,\n        ),\n        {""img"": torch.arange(64).reshape((1, 8, 8)), ""seg"": torch.arange(64).reshape((1, 8, 8))},\n        torch.tensor([[[16.9127, 13.3079, 9.7031], [26.8129, 23.2081, 19.6033], [36.7131, 33.1083, 29.5035]]]),\n    ],\n    [\n        dict(\n            prob=0.9,\n            mode=(""bilinear"", ""nearest""),\n            rotate_range=(np.pi / 2,),\n            shear_range=[1, 2],\n            translate_range=[2, 1],\n            scale_range=[0.1, 0.2],\n            as_tensor_output=False,\n            spatial_size=(3, 3),\n            keys=(""img"", ""seg""),\n            device=torch.device(""cpu:0""),\n        ),\n        {""img"": torch.arange(64).reshape((1, 8, 8)), ""seg"": torch.arange(64).reshape((1, 8, 8))},\n        {\n            ""img"": np.array([[[16.9127, 13.3079, 9.7031], [26.8129, 23.2081, 19.6033], [36.7131, 33.1083, 29.5035]]]),\n            ""seg"": np.array([[[19.0, 12.0, 12.0], [27.0, 20.0, 21.0], [35.0, 36.0, 29.0]]]),\n        },\n    ],\n]\n\n\nclass TestRandAffined(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_affined(self, input_param, input_data, expected_val):\n        g = RandAffined(**input_param).set_random_state(123)\n        res = g(input_data)\n        for key in res:\n            result = res[key]\n            expected = expected_val[key] if isinstance(expected_val, dict) else expected_val\n            self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected))\n            if torch.is_tensor(result):\n                np.testing.assert_allclose(result.cpu().numpy(), expected.cpu().numpy(), rtol=1e-4, atol=1e-4)\n            else:\n                np.testing.assert_allclose(result, expected, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_crop_by_pos_neg_labeld.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import RandCropByPosNegLabeld\n\nTEST_CASE_1 = [\n    {\n        ""keys"": [""image"", ""extral"", ""label""],\n        ""label_key"": ""label"",\n        ""size"": [2, 2, 2],\n        ""pos"": 1,\n        ""neg"": 1,\n        ""num_samples"": 2,\n        ""image_key"": None,\n        ""image_threshold"": 0,\n    },\n    {\n        ""image"": np.random.randint(0, 2, size=[3, 3, 3, 3]),\n        ""extral"": np.random.randint(0, 2, size=[3, 3, 3, 3]),\n        ""label"": np.random.randint(0, 2, size=[3, 3, 3, 3]),\n        ""affine"": np.eye(3),\n        ""shape"": ""CHWD"",\n    },\n    list,\n    (3, 2, 2, 2),\n]\n\nTEST_CASE_2 = [\n    {\n        ""keys"": [""image"", ""extral"", ""label""],\n        ""label_key"": ""label"",\n        ""size"": [2, 2, 2],\n        ""pos"": 1,\n        ""neg"": 1,\n        ""num_samples"": 2,\n        ""image_key"": None,\n        ""image_threshold"": 0,\n    },\n    {\n        ""image"": np.zeros([3, 3, 3, 3]) - 1,\n        ""extral"": np.zeros([3, 3, 3, 3]),\n        ""label"": np.ones([3, 3, 3, 3]),\n        ""affine"": np.eye(3),\n        ""shape"": ""CHWD"",\n    },\n    list,\n    (3, 2, 2, 2),\n]\n\n\nclass TestRandCropByPosNegLabeld(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_type_shape(self, input_param, input_data, expected_type, expected_shape):\n        result = RandCropByPosNegLabeld(**input_param)(input_data)\n        self.assertIsInstance(result, expected_type)\n        self.assertTupleEqual(result[0][""image""].shape, expected_shape)\n        self.assertTupleEqual(result[0][""extral""].shape, expected_shape)\n        self.assertTupleEqual(result[0][""label""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_deform_grid.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandDeformGrid\n\nTEST_CASES = [\n    [\n        dict(spacing=(1, 2), magnitude_range=(1.0, 2.0), as_tensor_output=False, device=None),\n        {""spatial_size"": (3, 3)},\n        np.array(\n            [\n                [\n                    [-3.45774551, -0.6608006, -1.62002671, -4.02259806, -2.77692349],\n                    [1.21748926, -4.25845712, -1.57592837, 0.69985342, -2.16382767],\n                    [-0.91158377, -0.12717178, 2.00258405, -0.85789449, -0.59616292],\n                    [0.41676882, 3.96204313, 3.93633727, 2.34820726, 1.51855713],\n                    [2.99011186, 4.00170105, 0.74339613, 3.57886072, 0.31633439],\n                ],\n                [\n                    [-4.85634965, -0.78197195, -1.91838077, 1.81192079, 2.84286669],\n                    [-4.34323645, -5.75784424, -2.37875058, 1.06023016, 5.24536301],\n                    [-4.23315172, -1.99617861, 0.92412057, 0.81899041, 4.38084451],\n                    [-5.08141703, -4.31985211, -0.52488611, 2.77048576, 4.45464513],\n                    [-4.01588556, 1.21238156, 0.55444352, 3.31421131, 7.00529793],\n                ],\n                [\n                    [1.0, 1.0, 1.0, 1.0, 1.0],\n                    [1.0, 1.0, 1.0, 1.0, 1.0],\n                    [1.0, 1.0, 1.0, 1.0, 1.0],\n                    [1.0, 1.0, 1.0, 1.0, 1.0],\n                    [1.0, 1.0, 1.0, 1.0, 1.0],\n                ],\n            ]\n        ),\n    ],\n    [\n        dict(spacing=(1, 2, 2), magnitude_range=(1.0, 3.0), as_tensor_output=False, device=None),\n        {""spatial_size"": (1, 2, 2)},\n        np.array(\n            [\n                [\n                    [\n                        [-2.81748977, 0.66968869, -0.52625642, -3.52173734],\n                        [-1.96865364, 1.76472402, -5.06258324, -1.71805669],\n                        [1.11934537, -2.45103851, -2.13654555, -1.15855539],\n                        [1.49678424, -2.06960677, -1.74328475, -1.7271617],\n                    ],\n                    [\n                        [3.69301983, 3.66097025, 1.68091953, 0.6465273],\n                        [1.23445289, 2.49568333, -1.56671014, 1.96849393],\n                        [-2.09916271, -1.06768069, 1.51861453, -2.39180117],\n                        [-0.23449363, -1.44269211, -0.42794076, -4.68520972],\n                    ],\n                    [\n                        [-1.96578162, -0.17168741, 2.55269525, 0.70931081],\n                        [1.00476444, 2.15217619, -0.47246061, 1.4748298],\n                        [-0.34829048, -1.89234811, 0.34558185, 1.9606272],\n                        [1.56684302, 0.98019418, 5.00513708, 1.69126978],\n                    ],\n                ],\n                [\n                    [\n                        [-1.36146598, 0.7469491, -5.16647064, -4.73906938],\n                        [1.91920577, -2.33606298, -0.95030633, 0.7901769],\n                        [2.49116076, 3.93791246, 3.50390686, 2.79030531],\n                        [1.70638302, 4.33070564, 3.52613304, 0.77965554],\n                    ],\n                    [\n                        [-0.62725323, -1.64857887, -2.92384357, -3.39022706],\n                        [-3.00611521, -0.66597021, -0.21577072, -2.39146379],\n                        [2.94568388, -0.83686357, -2.55435186, 2.74064119],\n                        [2.3247117, 2.78900974, 1.59788581, 0.31140512],\n                    ],\n                    [\n                        [-0.89856598, -4.15325814, -0.21934502, -1.64845891],\n                        [-1.52694693, -2.81794479, -2.22623861, -3.0299247],\n                        [4.49410486, 1.27529645, 2.92559679, -1.12171559],\n                        [3.30307684, 4.97189727, 2.43914751, 4.7262225],\n                    ],\n                ],\n                [\n                    [\n                        [-4.81571068, -3.28263239, 1.635167, 2.36520831],\n                        [-1.92511521, -4.311247, 2.19242556, 7.34990574],\n                        [-3.04122716, -0.94284154, 1.30058968, -0.11719455],\n                        [-2.28657395, -3.68766906, 0.28400757, 5.08072864],\n                    ],\n                    [\n                        [-4.2308508, -0.16084264, 2.69545963, 3.4666492],\n                        [-5.29514976, -1.55660775, 4.28031473, -0.39019547],\n                        [-3.4617024, -1.92430221, 1.20214712, 4.25261228],\n                        [-0.30683774, -1.4524049, 2.35996724, 3.83663135],\n                    ],\n                    [\n                        [-2.20587965, -1.94408353, -0.66964855, 1.15838178],\n                        [-4.26637632, -0.46145396, 2.27393031, 3.5415298],\n                        [-3.91902371, 2.02343374, 3.54278271, 2.40735681],\n                        [-4.3785335, -0.78200288, 3.12162619, 3.55709275],\n                    ],\n                ],\n                [\n                    [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]],\n                    [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]],\n                    [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]],\n                ],\n            ]\n        ),\n    ],\n]\n\n\nclass TestRandDeformGrid(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_deform_grid(self, input_param, input_data, expected_val):\n        g = RandDeformGrid(**input_param)\n        g.set_random_state(123)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_elastic_2d.py,7,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import Rand2DElastic\n\nTEST_CASES = [\n    [\n        {""spacing"": (0.3, 0.3), ""magnitude_range"": (1.0, 2.0), ""prob"": 0.0, ""as_tensor_output"": False, ""device"": None},\n        {""img"": torch.ones((3, 3, 3)), ""spatial_size"": (2, 2)},\n        np.ones((3, 2, 2)),\n    ],\n    [\n        {""spacing"": (0.3, 0.3), ""magnitude_range"": (1.0, 2.0), ""prob"": 0.9, ""as_tensor_output"": False, ""device"": None},\n        {""img"": torch.ones((3, 3, 3)), ""spatial_size"": (2, 2), ""mode"": ""bilinear""},\n        np.array([[[0.0, 0.0], [0.0, 0.04970419]], [[0.0, 0.0], [0.0, 0.04970419]], [[0.0, 0.0], [0.0, 0.04970419]]]),\n    ],\n    [\n        {\n            ""spacing"": (1.0, 1.0),\n            ""magnitude_range"": (1.0, 1.0),\n            ""scale_range"": [1.2, 2.2],\n            ""prob"": 0.9,\n            ""padding_mode"": ""border"",\n            ""as_tensor_output"": True,\n            ""device"": None,\n            ""spatial_size"": (2, 2),\n        },\n        {""img"": torch.arange(27).reshape((3, 3, 3))},\n        torch.tensor(\n            [\n                [[1.6605, 1.0083], [6.0000, 6.2224]],\n                [[10.6605, 10.0084], [15.0000, 15.2224]],\n                [[19.6605, 19.0083], [24.0000, 24.2224]],\n            ]\n        ),\n    ],\n    [\n        {\n            ""spacing"": (0.3, 0.3),\n            ""magnitude_range"": (0.1, 0.2),\n            ""translate_range"": [-0.01, 0.01],\n            ""scale_range"": [0.01, 0.02],\n            ""prob"": 0.9,\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2),\n        },\n        {""img"": torch.arange(27).reshape((3, 3, 3))},\n        np.array(\n            [\n                [[0.2001334, 1.2563337], [5.2274017, 7.90148]],\n                [[8.675412, 6.9098353], [13.019891, 16.850012]],\n                [[17.15069, 12.563337], [20.81238, 25.798544]],\n            ]\n        ),\n    ],\n]\n\n\nclass TestRand2DElastic(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_2d_elastic(self, input_param, input_data, expected_val):\n        g = Rand2DElastic(**input_param)\n        g.set_random_state(123)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_elastic_3d.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import Rand3DElastic\n\nTEST_CASES = [\n    [\n        {\n            ""magnitude_range"": (0.3, 2.3),\n            ""sigma_range"": (1.0, 20.0),\n            ""prob"": 0.0,\n            ""as_tensor_output"": False,\n            ""device"": None,\n        },\n        {""img"": torch.ones((2, 3, 3, 3)), ""spatial_size"": (2, 2, 2)},\n        np.ones((2, 2, 2, 2)),\n    ],\n    [\n        {\n            ""magnitude_range"": (0.3, 0.3),\n            ""sigma_range"": (1.0, 2.0),\n            ""prob"": 0.9,\n            ""as_tensor_output"": False,\n            ""device"": None,\n        },\n        {""img"": torch.arange(27).reshape((1, 3, 3, 3)), ""spatial_size"": (2, 2, 2)},\n        np.array([[[[3.2385552, 4.753422], [7.779232, 9.286472]], [[16.769115, 18.287868], [21.300673, 22.808704]]]]),\n    ],\n    [\n        {\n            ""magnitude_range"": (0.3, 0.3),\n            ""sigma_range"": (1.0, 2.0),\n            ""prob"": 0.9,\n            ""rotate_range"": [1, 1, 1],\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2, 2),\n        },\n        {""img"": torch.arange(27).reshape((1, 3, 3, 3)), ""mode"": ""bilinear""},\n        np.array([[[[1.6566806, 7.695548], [7.4342523, 13.580086]], [[11.776854, 18.669481], [18.396517, 21.551771]]]]),\n    ],\n]\n\n\nclass TestRand3DElastic(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_3d_elastic(self, input_param, input_data, expected_val):\n        g = Rand3DElastic(**input_param)\n        g.set_random_state(123)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_elasticd_2d.py,10,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import Rand2DElasticd\n\nTEST_CASES = [\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""spacing"": (0.3, 0.3),\n            ""magnitude_range"": (1.0, 2.0),\n            ""prob"": 0.0,\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2),\n        },\n        {""img"": torch.ones((3, 3, 3)), ""seg"": torch.ones((3, 3, 3))},\n        np.ones((3, 2, 2)),\n    ],\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""spacing"": (0.3, 0.3),\n            ""magnitude_range"": (1.0, 2.0),\n            ""prob"": 0.9,\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2),\n            ""mode"": ""bilinear"",\n        },\n        {""img"": torch.ones((3, 3, 3)), ""seg"": torch.ones((3, 3, 3))},\n        np.array([[[0.0, 0.0], [0.0, 0.04970419]], [[0.0, 0.0], [0.0, 0.04970419]], [[0.0, 0.0], [0.0, 0.04970419]]]),\n    ],\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""spacing"": (1.0, 1.0),\n            ""magnitude_range"": (1.0, 1.0),\n            ""scale_range"": [1.2, 2.2],\n            ""prob"": 0.9,\n            ""padding_mode"": ""border"",\n            ""as_tensor_output"": True,\n            ""device"": None,\n            ""spatial_size"": (2, 2),\n        },\n        {""img"": torch.arange(27).reshape((3, 3, 3)), ""seg"": torch.arange(27).reshape((3, 3, 3))},\n        torch.tensor(\n            [\n                [[1.6605, 1.0083], [6.0000, 6.2224]],\n                [[10.6605, 10.0084], [15.0000, 15.2224]],\n                [[19.6605, 19.0083], [24.0000, 24.2224]],\n            ]\n        ),\n    ],\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""spacing"": (0.3, 0.3),\n            ""magnitude_range"": (0.1, 0.2),\n            ""translate_range"": [-0.01, 0.01],\n            ""scale_range"": [0.01, 0.02],\n            ""prob"": 0.9,\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2),\n        },\n        {""img"": torch.arange(27).reshape((3, 3, 3)), ""seg"": torch.arange(27).reshape((3, 3, 3))},\n        np.array(\n            [\n                [[0.2001334, 1.2563337], [5.2274017, 7.90148]],\n                [[8.675412, 6.9098353], [13.019891, 16.850012]],\n                [[17.15069, 12.563337], [20.81238, 25.798544]],\n            ]\n        ),\n    ],\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""mode"": (""bilinear"", ""nearest""),\n            ""spacing"": (0.3, 0.3),\n            ""magnitude_range"": (0.1, 0.2),\n            ""translate_range"": [-0.01, 0.01],\n            ""scale_range"": [0.01, 0.02],\n            ""prob"": 0.9,\n            ""as_tensor_output"": True,\n            ""device"": None,\n            ""spatial_size"": (2, 2),\n        },\n        {""img"": torch.arange(27).reshape((3, 3, 3)), ""seg"": torch.arange(27).reshape((3, 3, 3))},\n        {\n            ""img"": torch.tensor(\n                [\n                    [[0.2001334, 1.2563337], [5.2274017, 7.90148]],\n                    [[8.675412, 6.9098353], [13.019891, 16.850012]],\n                    [[17.15069, 12.563337], [20.81238, 25.798544]],\n                ]\n            ),\n            ""seg"": torch.tensor([[[0.0, 2.0], [6.0, 8.0]], [[9.0, 11.0], [15.0, 17.0]], [[18.0, 20.0], [24.0, 26.0]]]),\n        },\n    ],\n]\n\n\nclass TestRand2DElasticd(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_2d_elasticd(self, input_param, input_data, expected_val):\n        g = Rand2DElasticd(**input_param)\n        g.set_random_state(123)\n        res = g(input_data)\n        for key in res:\n            result = res[key]\n            expected = expected_val[key] if isinstance(expected_val, dict) else expected_val\n            self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected))\n            if torch.is_tensor(result):\n                np.testing.assert_allclose(result.cpu().numpy(), expected.cpu().numpy(), rtol=1e-4, atol=1e-4)\n            else:\n                np.testing.assert_allclose(result, expected, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_elasticd_3d.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import Rand3DElasticd\n\nTEST_CASES = [\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""magnitude_range"": (0.3, 2.3),\n            ""sigma_range"": (1.0, 20.0),\n            ""prob"": 0.0,\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2, 2),\n        },\n        {""img"": torch.ones((2, 3, 3, 3)), ""seg"": torch.ones((2, 3, 3, 3))},\n        np.ones((2, 2, 2, 2)),\n    ],\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""magnitude_range"": (0.3, 0.3),\n            ""sigma_range"": (1.0, 2.0),\n            ""prob"": 0.9,\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2, 2),\n        },\n        {""img"": torch.arange(27).reshape((1, 3, 3, 3)), ""seg"": torch.arange(27).reshape((1, 3, 3, 3))},\n        np.array([[[[3.2385552, 4.753422], [7.779232, 9.286472]], [[16.769115, 18.287868], [21.300673, 22.808704]]]]),\n    ],\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""magnitude_range"": (0.3, 0.3),\n            ""sigma_range"": (1.0, 2.0),\n            ""prob"": 0.9,\n            ""rotate_range"": [1, 1, 1],\n            ""as_tensor_output"": False,\n            ""device"": None,\n            ""spatial_size"": (2, 2, 2),\n            ""mode"": ""bilinear"",\n        },\n        {""img"": torch.arange(27).reshape((1, 3, 3, 3)), ""seg"": torch.arange(27).reshape((1, 3, 3, 3))},\n        np.array([[[[1.6566806, 7.695548], [7.4342523, 13.580086]], [[11.776854, 18.669481], [18.396517, 21.551771]]]]),\n    ],\n    [\n        {\n            ""keys"": (""img"", ""seg""),\n            ""mode"": (""bilinear"", ""nearest""),\n            ""magnitude_range"": (0.3, 0.3),\n            ""sigma_range"": (1.0, 2.0),\n            ""prob"": 0.9,\n            ""rotate_range"": [1, 1, 1],\n            ""as_tensor_output"": True,\n            ""device"": torch.device(""cpu:0""),\n            ""spatial_size"": (2, 2, 2),\n        },\n        {""img"": torch.arange(27).reshape((1, 3, 3, 3)), ""seg"": torch.arange(27).reshape((1, 3, 3, 3))},\n        {\n            ""img"": torch.tensor(\n                [[[[1.6566806, 7.695548], [7.4342523, 13.580086]], [[11.776854, 18.669481], [18.396517, 21.551771]]]]\n            ),\n            ""seg"": torch.tensor([[[[1.0, 11.0], [7.0, 17.0]], [[9.0, 19.0], [15.0, 25.0]]]]),\n        },\n    ],\n]\n\n\nclass TestRand3DElasticd(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_rand_3d_elasticd(self, input_param, input_data, expected_val):\n        g = Rand3DElasticd(**input_param)\n        g.set_random_state(123)\n        res = g(input_data)\n        for key in res:\n            result = res[key]\n            expected = expected_val[key] if isinstance(expected_val, dict) else expected_val\n            self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected))\n            if torch.is_tensor(result):\n                np.testing.assert_allclose(result.cpu().numpy(), expected.cpu().numpy(), rtol=1e-4, atol=1e-4)\n            else:\n                np.testing.assert_allclose(result, expected, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_flip.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandFlip\nfrom tests.utils import NumpyImageTestCase2D\n\nINVALID_CASES = [(""wrong_axis"", [""s"", 1], TypeError), (""not_numbers"", ""s"", TypeError)]\n\nVALID_CASES = [(""no_axis"", None), (""one_axis"", 1), (""many_axis"", [0, 1])]\n\n\nclass TestRandFlip(NumpyImageTestCase2D):\n    @parameterized.expand(INVALID_CASES)\n    def test_invalid_inputs(self, _, spatial_axis, raises):\n        with self.assertRaises(raises):\n            flip = RandFlip(prob=1.0, spatial_axis=spatial_axis)\n            flip(self.imt[0])\n\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, _, spatial_axis):\n        flip = RandFlip(prob=1.0, spatial_axis=spatial_axis)\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.flip(channel, spatial_axis))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(expected, flip(self.imt[0])))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_flipd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandFlipd\nfrom tests.utils import NumpyImageTestCase2D\n\nVALID_CASES = [(""no_axis"", None), (""one_axis"", 1), (""many_axis"", [0, 1])]\n\n\nclass TestRandFlipd(NumpyImageTestCase2D):\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, _, spatial_axis):\n        flip = RandFlipd(keys=""img"", prob=1.0, spatial_axis=spatial_axis)\n        res = flip({""img"": self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.flip(channel, spatial_axis))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(expected, res[""img""]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_gaussian_noise.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\n\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandGaussianNoise\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandGaussianNoise(NumpyImageTestCase2D):\n    @parameterized.expand([(""test_zero_mean"", 0, 0.1), (""test_non_zero_mean"", 1, 0.5)])\n    def test_correct_results(self, _, mean, std):\n        seed = 0\n        gaussian_fn = RandGaussianNoise(prob=1.0, mean=mean, std=std)\n        gaussian_fn.set_random_state(seed)\n        noised = gaussian_fn(self.imt)\n        np.random.seed(seed)\n        np.random.random()\n        expected = self.imt + np.random.normal(mean, np.random.uniform(0, std), size=self.imt.shape)\n        np.testing.assert_allclose(expected, noised)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_gaussian_noised.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\n\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandGaussianNoised\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandGaussianNoised(NumpyImageTestCase2D):\n    @parameterized.expand([(""test_zero_mean"", [""img""], 0, 0.1), (""test_non_zero_mean"", [""img""], 1, 0.5)])\n    def test_correct_results(self, _, keys, mean, std):\n        seed = 0\n        gaussian_fn = RandGaussianNoised(keys=keys, prob=1.0, mean=mean, std=std)\n        gaussian_fn.set_random_state(seed)\n        noised = gaussian_fn({""img"": self.imt})\n        np.random.seed(seed)\n        np.random.random()\n        expected = self.imt + np.random.normal(mean, np.random.uniform(0, std), size=self.imt.shape)\n        np.testing.assert_allclose(expected, noised[""img""])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_rotate.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\n\nimport scipy.ndimage\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandRotate\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandRotate(NumpyImageTestCase2D):\n    @parameterized.expand(\n        [\n            (90, (0, 1), True, 1, ""reflect"", 0, True),\n            ((-45, 45), (1, 0), True, 3, ""constant"", 0, True),\n            (180, (1, 0), False, 2, ""constant"", 4, False),\n        ]\n    )\n    def test_correct_results(self, degrees, spatial_axes, reshape, order, mode, cval, prefilter):\n        rotate_fn = RandRotate(\n            degrees,\n            prob=1.0,\n            spatial_axes=spatial_axes,\n            reshape=reshape,\n            interp_order=order,\n            mode=mode,\n            cval=cval,\n            prefilter=prefilter,\n        )\n        rotate_fn.set_random_state(243)\n        rotated = rotate_fn(self.imt[0])\n\n        angle = rotate_fn.angle\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                scipy.ndimage.rotate(\n                    channel, angle, spatial_axes, reshape, order=order, mode=mode, cval=cval, prefilter=prefilter\n                )\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, rotated))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_rotate90.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import RandRotate90\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandRotate90(NumpyImageTestCase2D):\n    def test_default(self):\n        rotate = RandRotate90()\n        rotate.set_random_state(123)\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 0, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n    def test_k(self):\n        rotate = RandRotate90(max_k=2)\n        rotate.set_random_state(234)\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 0, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n    def test_spatial_axes(self):\n        rotate = RandRotate90(spatial_axes=(0, 1))\n        rotate.set_random_state(234)\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 0, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n    def test_prob_k_spatial_axes(self):\n        rotate = RandRotate90(prob=1.0, max_k=2, spatial_axes=(0, 1))\n        rotate.set_random_state(234)\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 1, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_rotate90d.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import RandRotate90d\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandRotate90d(NumpyImageTestCase2D):\n    def test_default(self):\n        key = None\n        rotate = RandRotate90d(keys=key)\n        rotate.set_random_state(123)\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 0, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_k(self):\n        key = ""test""\n        rotate = RandRotate90d(keys=key, max_k=2)\n        rotate.set_random_state(234)\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 0, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_spatial_axes(self):\n        key = ""test""\n        rotate = RandRotate90d(keys=key, spatial_axes=(0, 1))\n        rotate.set_random_state(234)\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 0, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_prob_k_spatial_axes(self):\n        key = ""test""\n        rotate = RandRotate90d(keys=key, prob=1.0, max_k=2, spatial_axes=(0, 1))\n        rotate.set_random_state(234)\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 1, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_no_key(self):\n        key = ""unknown""\n        rotate = RandRotate90d(keys=key, prob=1.0, max_k=2, spatial_axes=(0, 1))\n        with self.assertRaisesRegex(KeyError, """"):\n            rotated = rotate({""test"": self.imt[0]})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_rotated.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\n\nimport scipy.ndimage\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandRotated\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandRotated(NumpyImageTestCase2D):\n    @parameterized.expand(\n        [\n            (90, (0, 1), True, 1, ""reflect"", 0, True),\n            ((-45, 45), (1, 0), True, 3, ""constant"", 0, True),\n            (180, (1, 0), False, 2, ""constant"", 4, False),\n        ]\n    )\n    def test_correct_results(self, degrees, spatial_axes, reshape, order, mode, cval, prefilter):\n        rotate_fn = RandRotated(\n            ""img"",\n            degrees,\n            prob=1.0,\n            spatial_axes=spatial_axes,\n            reshape=reshape,\n            interp_order=order,\n            mode=mode,\n            cval=cval,\n            prefilter=prefilter,\n        )\n        rotate_fn.set_random_state(243)\n        rotated = rotate_fn({""img"": self.imt[0]})\n\n        angle = rotate_fn.angle\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                scipy.ndimage.rotate(\n                    channel, angle, spatial_axes, reshape, order=order, mode=mode, cval=cval, prefilter=prefilter\n                )\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, rotated[""img""]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_scale_intensity.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import RandScaleIntensity\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandScaleIntensity(NumpyImageTestCase2D):\n    def test_value(self):\n        scaler = RandScaleIntensity(factors=0.5, prob=1.0)\n        scaler.set_random_state(seed=0)\n        result = scaler(self.imt)\n        np.random.seed(0)\n        expected = (self.imt * (1 + np.random.uniform(low=-0.5, high=0.5))).astype(np.float32)\n        np.testing.assert_allclose(result, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_scale_intensityd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import RandScaleIntensityd\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandScaleIntensityd(NumpyImageTestCase2D):\n    def test_value(self):\n        key = ""img""\n        scaler = RandScaleIntensityd(keys=[key], factors=0.5, prob=1.0)\n        scaler.set_random_state(seed=0)\n        result = scaler({key: self.imt})\n        np.random.seed(0)\n        expected = (self.imt * (1 + np.random.uniform(low=-0.5, high=0.5))).astype(np.float32)\n        np.testing.assert_allclose(result[key], expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_shift_intensity.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import RandShiftIntensity\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandShiftIntensity(NumpyImageTestCase2D):\n    def test_value(self):\n        shifter = RandShiftIntensity(offsets=1.0, prob=1.0)\n        shifter.set_random_state(seed=0)\n        result = shifter(self.imt)\n        np.random.seed(0)\n        expected = self.imt + np.random.uniform(low=-1.0, high=1.0)\n        np.testing.assert_allclose(result, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_shift_intensityd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import RandShiftIntensityd\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRandShiftIntensityd(NumpyImageTestCase2D):\n    def test_value(self):\n        key = ""img""\n        shifter = RandShiftIntensityd(keys=[key], offsets=1.0, prob=1.0)\n        shifter.set_random_state(seed=0)\n        result = shifter({key: self.imt})\n        np.random.seed(0)\n        expected = self.imt + np.random.uniform(low=-1.0, high=1.0)\n        np.testing.assert_allclose(result[key], expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_spatial_crop.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import RandSpatialCrop\n\nTEST_CASE_1 = [{""roi_size"": [3, 3, 3], ""random_center"": True}, np.random.randint(0, 2, size=[3, 3, 3, 3]), (3, 3, 3, 3)]\n\nTEST_CASE_2 = [\n    {""roi_size"": [3, 3, 3], ""random_center"": False},\n    np.random.randint(0, 2, size=[3, 3, 3, 3]),\n    (3, 3, 3, 3),\n]\n\nTEST_CASE_3 = [\n    {""roi_size"": [3, 3], ""random_center"": False},\n    np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]]),\n]\n\n\nclass TestRandSpatialCrop(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = RandSpatialCrop(**input_param)(input_data)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_3])\n    def test_value(self, input_param, input_data):\n        cropper = RandSpatialCrop(**input_param)\n        result = cropper(input_data)\n        roi = [(2 - i // 2, 2 + i - i // 2) for i in cropper._size]\n        np.testing.assert_allclose(result, input_data[:, roi[0][0] : roi[0][1], roi[1][0] : roi[1][1]])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_spatial_cropd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import RandSpatialCropd\n\nTEST_CASE_1 = [\n    {""keys"": ""img"", ""roi_size"": [3, 3, 3], ""random_center"": True},\n    {""img"": np.random.randint(0, 2, size=[3, 3, 3, 3])},\n    (3, 3, 3, 3),\n]\n\nTEST_CASE_2 = [\n    {""keys"": ""img"", ""roi_size"": [3, 3, 3], ""random_center"": False},\n    {""img"": np.random.randint(0, 2, size=[3, 3, 3, 3])},\n    (3, 3, 3, 3),\n]\n\nTEST_CASE_3 = [\n    {""keys"": ""img"", ""roi_size"": [3, 3], ""random_center"": False},\n    {""img"": np.array([[[0, 0, 0, 0, 0], [0, 1, 2, 1, 0], [0, 2, 3, 2, 0], [0, 1, 2, 1, 0], [0, 0, 0, 0, 0]]])},\n]\n\n\nclass TestRandSpatialCropd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = RandSpatialCropd(**input_param)(input_data)\n        self.assertTupleEqual(result[""img""].shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_3])\n    def test_value(self, input_param, input_data):\n        cropper = RandSpatialCropd(**input_param)\n        result = cropper(input_data)\n        roi = [(2 - i // 2, 2 + i - i // 2) for i in cropper._size]\n        np.testing.assert_allclose(result[""img""], input_data[""img""][:, roi[0][0] : roi[0][1], roi[1][0] : roi[1][1]])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_zoom.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport importlib\n\nfrom scipy.ndimage import zoom as zoom_scipy\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandZoom\nfrom tests.utils import NumpyImageTestCase2D\n\nVALID_CASES = [(0.9, 1.1, 3, ""constant"", 0, True, False, False)]\n\n\nclass TestRandZoom(NumpyImageTestCase2D):\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, min_zoom, max_zoom, order, mode, cval, prefilter, use_gpu, keep_size):\n        random_zoom = RandZoom(\n            prob=1.0,\n            min_zoom=min_zoom,\n            max_zoom=max_zoom,\n            interp_order=order,\n            mode=mode,\n            cval=cval,\n            prefilter=prefilter,\n            use_gpu=use_gpu,\n            keep_size=keep_size,\n        )\n        random_zoom.set_random_state(234)\n        zoomed = random_zoom(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                zoom_scipy(channel, zoom=random_zoom._zoom, mode=mode, order=order, cval=cval, prefilter=prefilter)\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, zoomed))\n\n    @parameterized.expand([(0.8, 1.2, 1, ""constant"", 0, True)])\n    def test_gpu_zoom(self, min_zoom, max_zoom, order, mode, cval, prefilter):\n        if importlib.util.find_spec(""cupy""):\n            random_zoom = RandZoom(\n                prob=1.0,\n                min_zoom=min_zoom,\n                max_zoom=max_zoom,\n                interp_order=order,\n                mode=mode,\n                cval=cval,\n                prefilter=prefilter,\n                use_gpu=True,\n                keep_size=False,\n            )\n            random_zoom.set_random_state(234)\n\n            zoomed = random_zoom(self.imt[0])\n            expected = list()\n            for channel in self.imt[0]:\n                expected.append(\n                    zoom_scipy(channel, zoom=random_zoom._zoom, mode=mode, order=order, cval=cval, prefilter=prefilter)\n                )\n            expected = np.stack(expected).astype(np.float32)\n\n            self.assertTrue(np.allclose(expected, zoomed))\n\n    def test_keep_size(self):\n        random_zoom = RandZoom(prob=1.0, min_zoom=0.6, max_zoom=0.7, keep_size=True)\n        zoomed = random_zoom(self.imt[0])\n        self.assertTrue(np.array_equal(zoomed.shape, self.imt.shape[1:]))\n\n    @parameterized.expand([(""no_min_zoom"", None, 1.1, 1, TypeError), (""invalid_order"", 0.9, 1.1, ""s"", TypeError)])\n    def test_invalid_inputs(self, _, min_zoom, max_zoom, order, raises):\n        with self.assertRaises(raises):\n            random_zoom = RandZoom(prob=1.0, min_zoom=min_zoom, max_zoom=max_zoom, interp_order=order)\n            zoomed = random_zoom(self.imt[0])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rand_zoomd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport importlib\n\nfrom scipy.ndimage import zoom as zoom_scipy\nfrom parameterized import parameterized\n\nfrom monai.transforms import RandZoomd\nfrom tests.utils import NumpyImageTestCase2D\n\nVALID_CASES = [(0.9, 1.1, 3, ""constant"", 0, True, False, False)]\n\n\nclass TestRandZoomd(NumpyImageTestCase2D):\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, min_zoom, max_zoom, order, mode, cval, prefilter, use_gpu, keep_size):\n        key = ""img""\n        random_zoom = RandZoomd(\n            key,\n            prob=1.0,\n            min_zoom=min_zoom,\n            max_zoom=max_zoom,\n            interp_order=order,\n            mode=mode,\n            cval=cval,\n            prefilter=prefilter,\n            use_gpu=use_gpu,\n            keep_size=keep_size,\n        )\n        random_zoom.set_random_state(234)\n\n        zoomed = random_zoom({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                zoom_scipy(channel, zoom=random_zoom._zoom, mode=mode, order=order, cval=cval, prefilter=prefilter)\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, zoomed[key]))\n\n    @parameterized.expand([(0.8, 1.2, 1, ""constant"", 0, True)])\n    def test_gpu_zoom(self, min_zoom, max_zoom, order, mode, cval, prefilter):\n        key = ""img""\n        if importlib.util.find_spec(""cupy""):\n            random_zoom = RandZoomd(\n                key,\n                prob=1.0,\n                min_zoom=min_zoom,\n                max_zoom=max_zoom,\n                interp_order=order,\n                mode=mode,\n                cval=cval,\n                prefilter=prefilter,\n                use_gpu=True,\n                keep_size=False,\n            )\n            random_zoom.set_random_state(234)\n\n            zoomed = random_zoom({key: self.imt[0]})\n            expected = list()\n            for channel in self.imt[0]:\n                expected.append(\n                    zoom_scipy(channel, zoom=random_zoom._zoom, mode=mode, order=order, cval=cval, prefilter=prefilter)\n                )\n            expected = np.stack(expected).astype(np.float32)\n            self.assertTrue(np.allclose(expected, zoomed[key]))\n\n    def test_keep_size(self):\n        key = ""img""\n        random_zoom = RandZoomd(key, prob=1.0, min_zoom=0.6, max_zoom=0.7, keep_size=True)\n        zoomed = random_zoom({key: self.imt[0]})\n        self.assertTrue(np.array_equal(zoomed[key].shape, self.imt.shape[1:]))\n\n    @parameterized.expand([(""no_min_zoom"", None, 1.1, 1, TypeError), (""invalid_order"", 0.9, 1.1, ""s"", TypeError)])\n    def test_invalid_inputs(self, _, min_zoom, max_zoom, order, raises):\n        key = ""img""\n        with self.assertRaises(raises):\n            random_zoom = RandZoomd(key, prob=1.0, min_zoom=min_zoom, max_zoom=max_zoom, interp_order=order)\n            zoomed = random_zoom({key: self.imt[0]})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_randomizable.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import Randomizable\n\n\nclass RandTest(Randomizable):\n    def randomize(self):\n        pass\n\n\nclass TestRandomizable(unittest.TestCase):\n    def test_default(self):\n        inst = RandTest()\n        r1 = inst.R.rand()\n        self.assertTrue(isinstance(inst.R, np.random.RandomState))\n        inst.set_random_state()\n        r2 = inst.R.rand()\n        self.assertNotAlmostEqual(r1, r2)\n\n    def test_seed(self):\n        inst = RandTest()\n        inst.set_random_state(seed=123)\n        self.assertAlmostEqual(inst.R.rand(), 0.69646918)\n        inst.set_random_state(123)\n        self.assertAlmostEqual(inst.R.rand(), 0.69646918)\n\n    def test_state(self):\n        inst = RandTest()\n        inst_r = np.random.RandomState(123)\n        inst.set_random_state(state=inst_r)\n        self.assertAlmostEqual(inst.R.rand(), 0.69646918)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_repeat_channel.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import RepeatChannel\n\nTEST_CASE_1 = [{""repeats"": 3}, np.array([[[0, 1], [1, 2]]]), (3, 2, 2)]\n\n\nclass TestRepeatChannel(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = RepeatChannel(**input_param)(input_data)\n        self.assertEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_repeat_channeld.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import RepeatChanneld\n\nTEST_CASE_1 = [\n    {""keys"": [""img""], ""repeats"": 3},\n    {""img"": np.array([[[0, 1], [1, 2]]]), ""seg"": np.array([[[0, 1], [1, 2]]])},\n    (3, 2, 2),\n]\n\n\nclass TestRepeatChanneld(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = RepeatChanneld(**input_param)(input_data)\n        self.assertEqual(result[""img""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_resampler.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import Resample\nfrom monai.transforms.utils import create_grid\n\nTEST_CASES = [\n    [\n        dict(padding_mode=""zeros"", as_tensor_output=False, device=None),\n        {""grid"": create_grid((2, 2)), ""img"": np.arange(4).reshape((1, 2, 2))},\n        np.array([[[0.0, 0.25], [0.5, 0.75]]]),\n    ],\n    [\n        dict(padding_mode=""zeros"", as_tensor_output=False, device=None),\n        {""grid"": create_grid((4, 4)), ""img"": np.arange(4).reshape((1, 2, 2))},\n        np.array([[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.25, 0.0], [0.0, 0.5, 0.75, 0.0], [0.0, 0.0, 0.0, 0.0]]]),\n    ],\n    [\n        dict(padding_mode=""border"", as_tensor_output=False, device=None),\n        {""grid"": create_grid((4, 4)), ""img"": np.arange(4).reshape((1, 2, 2))},\n        np.array([[[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0], [2.0, 2.0, 3, 3.0], [2.0, 2.0, 3.0, 3.0]]]),\n    ],\n    [\n        dict(padding_mode=""reflection"", as_tensor_output=False, device=None),\n        {""grid"": create_grid((4, 4)), ""img"": np.arange(4).reshape((1, 2, 2)), ""mode"": ""nearest""},\n        np.array([[[3.0, 2.0, 3.0, 2.0], [1.0, 0.0, 1.0, 0.0], [3.0, 2.0, 3.0, 2.0], [1.0, 0.0, 1.0, 0.0]]]),\n    ],\n    [\n        dict(padding_mode=""zeros"", as_tensor_output=False, device=None),\n        {""grid"": create_grid((4, 4, 4)), ""img"": np.arange(8).reshape((1, 2, 2, 2)), ""mode"": ""bilinear""},\n        np.array(\n            [\n                [\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.125, 0.0], [0.0, 0.25, 0.375, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.5, 0.625, 0.0], [0.0, 0.75, 0.875, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],\n                ]\n            ]\n        ),\n    ],\n    [\n        dict(padding_mode=""border"", as_tensor_output=False, device=None),\n        {""grid"": create_grid((4, 4, 4)), ""img"": np.arange(8).reshape((1, 2, 2, 2)), ""mode"": ""bilinear""},\n        np.array(\n            [\n                [\n                    [[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0], [2.0, 2.0, 3.0, 3.0], [2.0, 2.0, 3.0, 3.0]],\n                    [[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0], [2.0, 2.0, 3.0, 3.0], [2.0, 2.0, 3.0, 3.0]],\n                    [[4.0, 4.0, 5.0, 5.0], [4.0, 4.0, 5.0, 5.0], [6.0, 6.0, 7.0, 7.0], [6.0, 6.0, 7.0, 7.0]],\n                    [[4.0, 4.0, 5.0, 5.0], [4.0, 4.0, 5.0, 5.0], [6.0, 6.0, 7.0, 7.0], [6.0, 6.0, 7.0, 7.0]],\n                ]\n            ]\n        ),\n    ],\n]\n\n\nclass TestResample(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_resample(self, input_param, input_data, expected_val):\n        g = Resample(**input_param)\n        result = g(**input_data)\n        self.assertEqual(torch.is_tensor(result), torch.is_tensor(expected_val))\n        if torch.is_tensor(result):\n            np.testing.assert_allclose(result.cpu().numpy(), expected_val.cpu().numpy(), rtol=1e-4, atol=1e-4)\n        else:\n            np.testing.assert_allclose(result, expected_val, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_resize.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport skimage\nfrom parameterized import parameterized\n\nfrom monai.transforms import Resize\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestResize(NumpyImageTestCase2D):\n    def test_invalid_inputs(self):\n        with self.assertRaises(TypeError):\n            resize = Resize(spatial_size=(128, 128, 3), interp_order=""order"")\n            resize(self.imt[0])\n\n    @parameterized.expand(\n        [\n            ((64, 64), 1, ""reflect"", 0, True, True, True),\n            ((32, 32), 2, ""constant"", 3, False, False, False),\n            ((256, 256), 3, ""constant"", 3, False, False, False),\n        ]\n    )\n    def test_correct_results(self, spatial_size, order, mode, cval, clip, preserve_range, anti_aliasing):\n        resize = Resize(\n            spatial_size,\n            interp_order=order,\n            mode=mode,\n            cval=cval,\n            clip=clip,\n            preserve_range=preserve_range,\n            anti_aliasing=anti_aliasing,\n        )\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                skimage.transform.resize(\n                    channel,\n                    spatial_size,\n                    order=order,\n                    mode=mode,\n                    cval=cval,\n                    clip=clip,\n                    preserve_range=preserve_range,\n                    anti_aliasing=anti_aliasing,\n                )\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(resize(self.imt[0]), expected))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_resized.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport skimage\nfrom parameterized import parameterized\n\nfrom monai.transforms import Resized\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestResized(NumpyImageTestCase2D):\n    def test_invalid_inputs(self):\n        with self.assertRaises(TypeError):\n            resize = Resized(keys=""img"", spatial_size=(128, 128, 3), interp_order=""order"")\n            resize({""img"": self.imt[0]})\n\n    @parameterized.expand(\n        [\n            ((64, 64), 1, ""reflect"", 0, True, True, True),\n            ((32, 32), 2, ""constant"", 3, False, False, False),\n            ((256, 256), 3, ""constant"", 3, False, False, False),\n        ]\n    )\n    def test_correct_results(self, spatial_size, order, mode, cval, clip, preserve_range, anti_aliasing):\n        resize = Resized(""img"", spatial_size, order, mode, cval, clip, preserve_range, anti_aliasing)\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                skimage.transform.resize(\n                    channel,\n                    spatial_size,\n                    order=order,\n                    mode=mode,\n                    cval=cval,\n                    clip=clip,\n                    preserve_range=preserve_range,\n                    anti_aliasing=anti_aliasing,\n                )\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(resize({""img"": self.imt[0]})[""img""], expected))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rotate.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\n\nimport scipy.ndimage\nfrom parameterized import parameterized\n\nfrom monai.transforms import Rotate\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASES = [\n    (90, (0, 1), True, 1, ""reflect"", 0, True),\n    (-90, (1, 0), True, 3, ""constant"", 0, True),\n    (180, (1, 0), False, 2, ""constant"", 4, False),\n]\n\n\nclass TestRotate(NumpyImageTestCase2D):\n    @parameterized.expand(TEST_CASES)\n    def test_correct_results(self, angle, spatial_axes, reshape, order, mode, cval, prefilter):\n        rotate_fn = Rotate(angle, spatial_axes, reshape, order, mode, cval, prefilter)\n        rotated = rotate_fn(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                scipy.ndimage.rotate(\n                    channel, angle, spatial_axes, reshape, order=order, mode=mode, cval=cval, prefilter=prefilter\n                )\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, rotated))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rotate90.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import Rotate90\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRotate90(NumpyImageTestCase2D):\n    def test_rotate90_default(self):\n        rotate = Rotate90()\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 1, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n    def test_k(self):\n        rotate = Rotate90(k=2)\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 2, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n    def test_spatial_axes(self):\n        rotate = Rotate90(spatial_axes=(0, 1))\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 1, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n    def test_prob_k_spatial_axes(self):\n        rotate = Rotate90(k=2, spatial_axes=(0, 1))\n        rotated = rotate(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 2, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated, expected))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rotate90d.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import Rotate90d\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestRotate90d(NumpyImageTestCase2D):\n    def test_rotate90_default(self):\n        key = ""test""\n        rotate = Rotate90d(keys=key)\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 1, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_k(self):\n        key = None\n        rotate = Rotate90d(keys=key, k=2)\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 2, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_spatial_axes(self):\n        key = ""test""\n        rotate = Rotate90d(keys=key, spatial_axes=(0, 1))\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 1, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_prob_k_spatial_axes(self):\n        key = ""test""\n        rotate = Rotate90d(keys=key, k=2, spatial_axes=(0, 1))\n        rotated = rotate({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(np.rot90(channel, 2, (0, 1)))\n        expected = np.stack(expected)\n        self.assertTrue(np.allclose(rotated[key], expected))\n\n    def test_no_key(self):\n        key = ""unknown""\n        rotate = Rotate90d(keys=key)\n        with self.assertRaisesRegex(KeyError, """"):\n            rotate({""test"": self.imt[0]})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_rotated.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\n\nimport scipy.ndimage\nfrom parameterized import parameterized\n\nfrom monai.transforms import Rotated\nfrom tests.utils import NumpyImageTestCase2D\n\nTEST_CASES = [\n    (90, (0, 1), True, 1, ""reflect"", 0, True),\n    (-90, (1, 0), True, 3, ""constant"", 0, True),\n    (180, (1, 0), False, 2, ""constant"", 4, False),\n]\n\n\nclass TestRotated(NumpyImageTestCase2D):\n    @parameterized.expand(TEST_CASES)\n    def test_correct_results(self, angle, spatial_axes, reshape, interp_order, mode, cval, prefilter):\n        key = ""img""\n        rotate_fn = Rotated(key, angle, spatial_axes, reshape, interp_order, mode, cval, prefilter)\n        rotated = rotate_fn({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(\n                scipy.ndimage.rotate(\n                    channel, angle, spatial_axes, reshape, order=interp_order, mode=mode, cval=cval, prefilter=prefilter\n                )\n            )\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, rotated[key]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_scale_intensity.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import ScaleIntensity\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestScaleIntensity(NumpyImageTestCase2D):\n    def test_range_scale(self):\n        scaler = ScaleIntensity(minv=1.0, maxv=2.0)\n        result = scaler(self.imt)\n        mina = np.min(self.imt)\n        maxa = np.max(self.imt)\n        norm = (self.imt - mina) / (maxa - mina)\n        expected = (norm * (2.0 - 1.0)) + 1.0\n        np.testing.assert_allclose(result, expected)\n\n    def test_factor_scale(self):\n        scaler = ScaleIntensity(minv=None, maxv=None, factor=0.1)\n        result = scaler(self.imt)\n        expected = (self.imt * (1 + 0.1)).astype(np.float32)\n        np.testing.assert_allclose(result, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_scale_intensity_range.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import ScaleIntensityRange\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass IntensityScaleIntensityRange(NumpyImageTestCase2D):\n    def test_image_scale_intensity_range(self):\n        scaler = ScaleIntensityRange(a_min=20, a_max=108, b_min=50, b_max=80)\n        scaled = scaler(self.imt)\n        expected = (self.imt - 20) / 88\n        expected = expected * 30 + 50\n        self.assertTrue(np.allclose(scaled, expected))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_scale_intensity_ranged.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import ScaleIntensityRanged\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass IntensityScaleIntensityRanged(NumpyImageTestCase2D):\n    def test_image_scale_intensity_ranged(self):\n        key = ""img""\n        scaler = ScaleIntensityRanged(keys=key, a_min=20, a_max=108, b_min=50, b_max=80)\n        scaled = scaler({key: self.imt})\n        expected = (self.imt - 20) / 88\n        expected = expected * 30 + 50\n        self.assertTrue(np.allclose(scaled[key], expected))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_scale_intensityd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import ScaleIntensityd\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestScaleIntensityd(NumpyImageTestCase2D):\n    def test_range_scale(self):\n        key = ""img""\n        scaler = ScaleIntensityd(keys=[key], minv=1.0, maxv=2.0)\n        result = scaler({key: self.imt})\n        mina = np.min(self.imt)\n        maxa = np.max(self.imt)\n        norm = (self.imt - mina) / (maxa - mina)\n        expected = (norm * (2.0 - 1.0)) + 1.0\n        np.testing.assert_allclose(result[key], expected)\n\n    def test_factor_scale(self):\n        key = ""img""\n        scaler = ScaleIntensityd(keys=[key], minv=None, maxv=None, factor=0.1)\n        result = scaler({key: self.imt})\n        expected = (self.imt * (1 + 0.1)).astype(np.float32)\n        np.testing.assert_allclose(result[key], expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_seg_loss_integration.py,14,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom parameterized import parameterized\n\nfrom monai.losses import DiceLoss, FocalLoss, GeneralizedDiceLoss, TverskyLoss\n\nTEST_CASES = [\n    [DiceLoss, {""to_onehot_y"": True, ""squared_pred"": True}, {""smooth"": 1e-4}],\n    [DiceLoss, {""to_onehot_y"": True, ""sigmoid"": True}, {}],\n    [DiceLoss, {""to_onehot_y"": True, ""softmax"": True}, {}],\n    [FocalLoss, {""gamma"": 1.5, ""weight"": torch.tensor([1, 2])}, {}],\n    [FocalLoss, {""gamma"": 1.5}, {}],\n    [GeneralizedDiceLoss, {""to_onehot_y"": True, ""softmax"": True}, {}],\n    [GeneralizedDiceLoss, {""to_onehot_y"": True, ""sigmoid"": True}, {}],\n    [GeneralizedDiceLoss, {""to_onehot_y"": True, ""sigmoid"": True, ""w_type"": ""simple""}, {}],\n    [GeneralizedDiceLoss, {""to_onehot_y"": True, ""sigmoid"": True, ""w_type"": ""uniform""}, {}],\n    [TverskyLoss, {""to_onehot_y"": True, ""softmax"": True, ""alpha"": 0.8, ""beta"": 0.2}, {}],\n    [TverskyLoss, {""to_onehot_y"": True, ""softmax"": True, ""alpha"": 1.0, ""beta"": 0.0}, {}],\n]\n\n\nclass TestSegLossIntegration(unittest.TestCase):\n    def setUp(self):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(0)\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu:0"")\n\n    def tearDown(self):\n        torch.backends.cudnn.deterministic = False\n        torch.backends.cudnn.benchmark = True\n\n    @parameterized.expand(TEST_CASES)\n    def test_convergence(self, loss_type, loss_args, forward_args):\n        """"""\n        The goal of this test is to assess if the gradient of the loss function\n        is correct by testing if we can train a one layer neural network\n        to segment one image.\n        We verify that the loss is decreasing in almost all SGD steps.\n        """"""\n        learning_rate = 0.001\n        max_iter = 40\n\n        # define a simple 3d example\n        target_seg = torch.tensor(\n            [\n                [\n                    # raw 0\n                    [[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n                    # raw 1\n                    [[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n                    # raw 2\n                    [[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n                ]\n            ],\n            device=self.device,\n        )\n        target_seg = torch.unsqueeze(target_seg, dim=0)\n        image = 12 * target_seg + 27\n        image = image.float().to(self.device)\n        num_classes = 2\n        num_voxels = 3 * 4 * 4\n\n        # define a one layer model\n        class OnelayerNet(nn.Module):\n            def __init__(self):\n                super(OnelayerNet, self).__init__()\n                self.layer_1 = nn.Linear(num_voxels, 200)\n                self.acti = nn.ReLU()\n                self.layer_2 = nn.Linear(200, num_voxels * num_classes)\n\n            def forward(self, x):\n                x = x.view(-1, num_voxels)\n                x = self.layer_1(x)\n                x = self.acti(x)\n                x = self.layer_2(x)\n                x = x.view(-1, num_classes, 3, 4, 4)\n                return x\n\n        # initialise the network\n        net = OnelayerNet().to(self.device)\n\n        # initialize the loss\n        loss = loss_type(**loss_args)\n\n        # initialize a SGD optimizer\n        optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n\n        loss_history = []\n        init_output = None\n\n        # train the network\n        for iter_i in range(max_iter):\n            # set the gradient to zero\n            optimizer.zero_grad()\n\n            # forward pass\n            output = net(image)\n            if init_output is None:\n                init_output = torch.argmax(output, 1).detach().cpu().numpy()\n\n            loss_val = loss(output, target_seg, **forward_args)\n\n            if iter_i % 10 == 0:\n                pred = torch.argmax(output, 1).detach().cpu().numpy()\n                gt = target_seg.detach().cpu().numpy()[:, 0]\n                print(f""{loss_type.__name__} iter: {iter_i}, acc: {np.sum(pred == gt) / np.prod(pred.shape)}"")\n\n            # backward pass\n            loss_val.backward()\n            optimizer.step()\n\n            # stats\n            loss_history.append(loss_val.item())\n\n        pred = torch.argmax(output, 1).detach().cpu().numpy()\n        target = target_seg.detach().cpu().numpy()[:, 0]\n        # initial predictions are bad\n        self.assertTrue(not np.allclose(init_output, target))\n        # final predictions are good\n        np.testing.assert_allclose(pred, target)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_set_determinism.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nimport numpy as np\nfrom monai.utils import set_determinism, get_seed\n\n\nclass TestSetDeterminism(unittest.TestCase):\n    def test_values(self):\n        # check system default flags\n        self.assertTrue(not torch.backends.cudnn.deterministic)\n        self.assertTrue(get_seed() is None)\n        # set default seed\n        set_determinism()\n        self.assertTrue(get_seed() is not None)\n        self.assertTrue(torch.backends.cudnn.deterministic)\n        self.assertTrue(not torch.backends.cudnn.benchmark)\n        # resume default\n        set_determinism(None)\n        self.assertTrue(not torch.backends.cudnn.deterministic)\n        self.assertTrue(not torch.backends.cudnn.benchmark)\n        self.assertTrue(get_seed() is None)\n        # test seeds\n        seed = 255\n        set_determinism(seed=seed)\n        self.assertEqual(seed, get_seed())\n        a = np.random.randint(seed)\n        b = torch.randint(seed, (1,))\n        set_determinism(seed=seed)\n        c = np.random.randint(seed)\n        d = torch.randint(seed, (1,))\n        self.assertEqual(a, c)\n        self.assertEqual(b, d)\n        self.assertTrue(torch.backends.cudnn.deterministic)\n        self.assertTrue(not torch.backends.cudnn.benchmark)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_shift_intensity.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import ShiftIntensity\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestShiftIntensity(NumpyImageTestCase2D):\n    def test_value(self):\n        shifter = ShiftIntensity(offset=1.0)\n        result = shifter(self.imt)\n        expected = self.imt + 1.0\n        np.testing.assert_allclose(result, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_shift_intensityd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom monai.transforms import ShiftIntensityd\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestShiftIntensityd(NumpyImageTestCase2D):\n    def test_value(self):\n        key = ""img""\n        shifter = ShiftIntensityd(keys=[key], offset=1.0)\n        result = shifter({key: self.imt})\n        expected = self.imt + 1.0\n        np.testing.assert_allclose(result[key], expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_simulatedelay.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport time\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms.utility.array import SimulateDelay\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestSimulateDelay(NumpyImageTestCase2D):\n    @parameterized.expand([(0.45,), (1,)])\n    def test_value(self, delay_test_time: float):\n        resize = SimulateDelay(delay_time=delay_test_time)\n        start: float = time.time()\n        result = resize(self.imt[0])\n        stop: float = time.time()\n        measured_approximate: float = stop - start\n        np.testing.assert_array_less(delay_test_time, measured_approximate)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_simulatedelayd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport time\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms.utility.dictionary import SimulateDelayd\nfrom tests.utils import NumpyImageTestCase2D\n\n\nclass TestSimulateDelay(NumpyImageTestCase2D):\n    @parameterized.expand([(0.45,), (1,)])\n    def test_value(self, delay_test_time: float):\n        resize = SimulateDelayd(keys=""imgd"", delay_time=delay_test_time)\n        start: float = time.time()\n        _ = resize({""imgd"": self.imt[0]})\n        stop: float = time.time()\n        measured_approximate: float = stop - start\n        np.testing.assert_allclose(delay_test_time, measured_approximate, rtol=0.5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_sliding_window_inference.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.inferers import sliding_window_inference\n\nTEST_CASE_1 = [(1, 3, 16, 15, 7), (4, 10, 7), 3, 0.25, ""constant""]  # 3D small roi\n\nTEST_CASE_2 = [(1, 3, 16, 15, 7), (20, 22, 23), 10, 0.25, ""constant""]  # 3D large roi\n\nTEST_CASE_3 = [(1, 3, 15, 7), (2, 6), 1000, 0.25, ""constant""]  # 2D small roi, large batch\n\nTEST_CASE_4 = [(1, 3, 16, 7), (80, 50), 7, 0.25, ""constant""]  # 2D large roi\n\nTEST_CASE_5 = [(1, 3, 16, 15, 7), (20, 22, 23), 10, 0.5, ""constant""]  # 3D large overlap\n\nTEST_CASE_6 = [(1, 3, 16, 7), (80, 50), 7, 0.5, ""gaussian""]  # 2D large overlap, gaussian\n\nTEST_CASE_7 = [(1, 3, 16, 15, 7), (4, 10, 7), 3, 0.25, ""gaussian""]  # 3D small roi, gaussian\n\n\nclass TestSlidingWindowInference(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5, TEST_CASE_6, TEST_CASE_7])\n    def test_sliding_window_default(self, image_shape, roi_shape, sw_batch_size, overlap, mode):\n        inputs = torch.ones(*image_shape)\n        device = torch.device(""cpu:0"")\n\n        def compute(data):\n            return data + 1\n\n        result = sliding_window_inference(\n            inputs.to(device), roi_shape, sw_batch_size, compute, overlap, blend_mode=mode\n        )\n        expected_val = np.ones(image_shape, dtype=np.float32) + 1\n        self.assertTrue(np.allclose(result.numpy(), expected_val))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_spacing.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.transforms import Spacing\nfrom monai.utils import ensure_tuple\n\nTEST_CASES = [\n    [\n        {""pixdim"": (1.0, 1.5, 1.0), ""mode"": ""zeros"", ""dtype"": np.float},\n        np.arange(4).reshape((1, 2, 2)) + 1.0,  # data\n        {""affine"": np.eye(4)},\n        np.array([[[1.0, 1.0], [3.0, 2.0]]]),\n    ],\n    [\n        {""pixdim"": 1.0, ""mode"": ""zeros"", ""dtype"": np.float},\n        np.ones((1, 2, 1, 2)),  # data\n        {""affine"": np.eye(4)},\n        np.array([[[[1.0, 1.0]], [[1.0, 1.0]]]]),\n    ],\n    [\n        {""pixdim"": (1.0, 1.0, 1.0), ""mode"": ""zeros"", ""dtype"": np.float},\n        np.ones((1, 2, 1, 2)),  # data\n        {""affine"": np.eye(4)},\n        np.array([[[[1.0, 1.0]], [[1.0, 1.0]]]]),\n    ],\n    [\n        {""pixdim"": (1.0, 0.2, 1.5), ""diagonal"": False, ""mode"": ""zeros""},\n        np.ones((1, 2, 1, 2)),  # data\n        {""affine"": np.array([[2, 1, 0, 4], [-1, -3, 0, 5], [0, 0, 2.0, 5], [0, 0, 0, 1]])},\n        np.array([[[[0.95527864, 0.95527864]], [[1.0, 1.0]], [[1.0, 1.0]]]]),\n    ],\n    [\n        {""pixdim"": (3.0, 1.0), ""mode"": ""zeros""},\n        np.arange(24).reshape((2, 3, 4)),  # data\n        {""affine"": np.diag([-3.0, 0.2, 1.5, 1])},\n        np.array([[[0, 0], [4, 0], [8, 0]], [[12, 0], [16, 0], [20, 0]]]),\n    ],\n    [\n        {""pixdim"": (3.0, 1.0), ""mode"": ""zeros""},\n        np.arange(24).reshape((2, 3, 4)),  # data\n        {},\n        np.array([[[0, 1, 2, 3], [0, 0, 0, 0]], [[12, 13, 14, 15], [0, 0, 0, 0]]]),\n    ],\n    [\n        {""pixdim"": (1.0, 1.0)},\n        np.arange(24).reshape((2, 3, 4)),  # data\n        {},\n        np.array(\n            [[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]\n        ),\n    ],\n    [\n        {""pixdim"": (4.0, 5.0, 6.0)},\n        np.arange(24).reshape((1, 2, 3, 4)),  # data\n        {""affine"": np.array([[-4, 0, 0, 4], [0, 5, 0, -5], [0, 0, 6, -6], [0, 0, 0, 1]])},\n        np.arange(24).reshape((1, 2, 3, 4)),  # data\n    ],\n    [\n        {""pixdim"": (4.0, 5.0, 6.0), ""diagonal"": True},\n        np.arange(24).reshape((1, 2, 3, 4)),  # data\n        {""affine"": np.array([[-4, 0, 0, 4], [0, 5, 0, 0], [0, 0, 6, 0], [0, 0, 0, 1]])},\n        np.array(\n            [[[[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]], [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]]]\n        ),\n    ],\n    [\n        {""pixdim"": (4.0, 5.0, 6.0), ""mode"": ""border"", ""diagonal"": True},\n        np.arange(24).reshape((1, 2, 3, 4)),  # data\n        {""affine"": np.array([[-4, 0, 0, -4], [0, 5, 0, 0], [0, 0, 6, 0], [0, 0, 0, 1]])},\n        np.array(\n            [[[[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]], [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]]]\n        ),\n    ],\n    [\n        {""pixdim"": (4.0, 5.0, 6.0), ""mode"": ""border"", ""diagonal"": True},\n        np.arange(24).reshape((1, 2, 3, 4)),  # data\n        {""affine"": np.array([[-4, 0, 0, -4], [0, 5, 0, 0], [0, 0, 6, 0], [0, 0, 0, 1]]), ""interp_order"": ""nearest""},\n        np.array(\n            [[[[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]], [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]]]\n        ),\n    ],\n    [\n        {""pixdim"": (1.9, 4.0, 5.0), ""mode"": ""zeros"", ""diagonal"": True},\n        np.arange(24).reshape((1, 4, 6)),  # data\n        {""affine"": np.array([[-4, 0, 0, -4], [0, 5, 0, 0], [0, 0, 6, 0], [0, 0, 0, 1]]), ""interp_order"": ""nearest""},\n        np.array(\n            [\n                [\n                    [18.0, 19.0, 20.0, 20.0, 21.0, 22.0, 23.0],\n                    [18.0, 19.0, 20.0, 20.0, 21.0, 22.0, 23.0],\n                    [12.0, 13.0, 14.0, 14.0, 15.0, 16.0, 17.0],\n                    [12.0, 13.0, 14.0, 14.0, 15.0, 16.0, 17.0],\n                    [6.0, 7.0, 8.0, 8.0, 9.0, 10.0, 11.0],\n                    [6.0, 7.0, 8.0, 8.0, 9.0, 10.0, 11.0],\n                    [0.0, 1.0, 2.0, 2.0, 3.0, 4.0, 5.0],\n                ]\n            ]\n        ),\n    ],\n    [\n        {""pixdim"": (5.0, 3.0, 6.0), ""mode"": ""border"", ""diagonal"": True, ""dtype"": np.float32},\n        np.arange(24).reshape((1, 4, 6)),  # data\n        {""affine"": np.array([[-4, 0, 0, 0], [0, 5, 0, 0], [0, 0, 6, 0], [0, 0, 0, 1]]), ""interp_order"": ""bilinear""},\n        np.array(\n            [\n                [\n                    [18.0, 18.6, 19.2, 19.8, 20.400002, 21.0, 21.6, 22.2, 22.8],\n                    [10.5, 11.1, 11.700001, 12.299999, 12.900001, 13.5, 14.1, 14.700001, 15.3],\n                    [3.0, 3.6000001, 4.2000003, 4.8, 5.4000006, 6.0, 6.6000004, 7.200001, 7.8],\n                ]\n            ]\n        ),\n    ],\n    [\n        {""pixdim"": (5.0, 3.0, 6.0), ""mode"": ""zeros"", ""diagonal"": True, ""dtype"": np.float32},\n        np.arange(24).reshape((1, 4, 6)),  # data\n        {""affine"": np.array([[-4, 0, 0, 0], [0, 5, 0, 0], [0, 0, 6, 0], [0, 0, 0, 1]]), ""interp_order"": ""bilinear""},\n        np.array(\n            [\n                [\n                    [18.0000, 18.6000, 19.2000, 19.8000, 20.4000, 21.0000, 21.6000, 22.2000, 22.8000],\n                    [10.5000, 11.1000, 11.7000, 12.3000, 12.9000, 13.5000, 14.1000, 14.7000, 15.3000],\n                    [3.0000, 3.6000, 4.2000, 4.8000, 5.4000, 6.0000, 6.6000, 7.2000, 7.8000],\n                ]\n            ]\n        ),\n    ],\n    [\n        {""pixdim"": (5.0, 3.0, 6.0), ""diagonal"": True, ""mode"": ""test"", ""interp_order"": ""test""},\n        np.arange(24).reshape((1, 4, 6)),  # data\n        {\n            ""affine"": np.array([[-4, 0, 0, 0], [0, 5, 0, 0], [0, 0, 6, 0], [0, 0, 0, 1]]),\n            ""interp_order"": ""bilinear"",\n            ""mode"": ""zeros"",\n            ""dtype"": np.float64,\n        },\n        np.array(\n            [\n                [\n                    [18.0000, 18.6000, 19.2000, 19.8000, 20.4000, 21.0000, 21.6000, 22.2000, 22.8000],\n                    [10.5000, 11.1000, 11.7000, 12.3000, 12.9000, 13.5000, 14.1000, 14.7000, 15.3000],\n                    [3.0000, 3.6000, 4.2000, 4.8000, 5.4000, 6.0000, 6.6000, 7.2000, 7.8000],\n                ]\n            ]\n        ),\n    ],\n]\n\n\nclass TestSpacingCase(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_spacing(self, init_param, img, data_param, expected_output):\n        res = Spacing(**init_param)(img, **data_param)\n        np.testing.assert_allclose(res[0], expected_output, atol=1e-6)\n        sr = len(res[0].shape) - 1\n        if isinstance(init_param[""pixdim""], float):\n            init_param[""pixdim""] = [init_param[""pixdim""]] * sr\n        init_pixdim = ensure_tuple(init_param[""pixdim""])\n        init_pixdim = init_param[""pixdim""][:sr]\n        np.testing.assert_allclose(init_pixdim[:sr], np.sqrt(np.sum(np.square(res[2]), axis=0))[:sr])\n\n    def test_ill_pixdim(self):\n        with self.assertRaises(ValueError):\n            Spacing(pixdim=(-1, 2.0))(np.zeros((1, 1)))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_spacingd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom monai.transforms import Spacingd\n\n\nclass TestSpacingDCase(unittest.TestCase):\n    def test_spacingd_3d(self):\n        data = {""image"": np.ones((2, 10, 15, 20)), ""image.affine"": np.eye(4)}\n        spacing = Spacingd(keys=""image"", pixdim=(1, 2, 1.4))\n        res = spacing(data)\n        self.assertEqual((""image"", ""image.affine""), tuple(sorted(res)))\n        np.testing.assert_allclose(res[""image""].shape, (2, 10, 8, 15))\n        np.testing.assert_allclose(res[""image.affine""], np.diag([1, 2, 1.4, 1.0]))\n\n    def test_spacingd_2d(self):\n        data = {""image"": np.ones((2, 10, 20)), ""image.affine"": np.eye(3)}\n        spacing = Spacingd(keys=""image"", pixdim=(1, 2, 1.4))\n        res = spacing(data)\n        self.assertEqual((""image"", ""image.affine""), tuple(sorted(res)))\n        np.testing.assert_allclose(res[""image""].shape, (2, 10, 10))\n        np.testing.assert_allclose(res[""image.affine""], np.diag((1, 2, 1)))\n\n    def test_interp_all(self):\n        data = {\n            ""image"": np.arange(20).reshape((2, 1, 10)),\n            ""seg"": np.ones((2, 1, 10)),\n            ""image.affine"": np.eye(4),\n            ""seg.affine"": np.eye(4),\n        }\n        spacing = Spacingd(keys=(""image"", ""seg""), interp_order=""nearest"", pixdim=(1, 0.2,))\n        res = spacing(data)\n        self.assertEqual((""image"", ""image.affine"", ""seg"", ""seg.affine""), tuple(sorted(res)))\n        np.testing.assert_allclose(res[""image""].shape, (2, 1, 46))\n        np.testing.assert_allclose(res[""image.affine""], np.diag((1, 0.2, 1, 1)))\n\n    def test_interp_sep(self):\n        data = {\n            ""image"": np.ones((2, 1, 10)),\n            ""seg"": np.ones((2, 1, 10)),\n            ""image.affine"": np.eye(4),\n            ""seg.affine"": np.eye(4),\n        }\n        spacing = Spacingd(keys=(""image"", ""seg""), interp_order=(""bilinear"", ""nearest""), pixdim=(1, 0.2,))\n        res = spacing(data)\n        self.assertEqual((""image"", ""image.affine"", ""seg"", ""seg.affine""), tuple(sorted(res)))\n        np.testing.assert_allclose(res[""image""].shape, (2, 1, 46))\n        np.testing.assert_allclose(res[""image.affine""], np.diag((1, 0.2, 1, 1)))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_spatial_crop.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import SpatialCrop\n\nTEST_CASE_1 = [\n    {""roi_center"": [1, 1, 1], ""roi_size"": [2, 2, 2]},\n    np.random.randint(0, 2, size=[3, 3, 3, 3]),\n    (3, 2, 2, 2),\n]\n\nTEST_CASE_2 = [{""roi_start"": [0, 0, 0], ""roi_end"": [2, 2, 2]}, np.random.randint(0, 2, size=[3, 3, 3, 3]), (3, 2, 2, 2)]\n\nTEST_CASE_3 = [\n    {""roi_start"": [0, 0], ""roi_end"": [2, 2]},\n    np.random.randint(0, 2, size=[3, 3, 3, 3]),\n    (3, 2, 2, 3),\n]\n\nTEST_CASE_4 = [\n    {""roi_start"": [0, 0, 0, 0, 0], ""roi_end"": [2, 2, 2, 2, 2]},\n    np.random.randint(0, 2, size=[3, 3, 3, 3]),\n    (3, 2, 2, 2),\n]\n\n\nclass TestSpatialCrop(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = SpatialCrop(**input_param)(input_data)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_spatial_cropd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import SpatialCropd\n\nTEST_CASE_1 = [\n    {""keys"": [""img""], ""roi_center"": [1, 1, 1], ""roi_size"": [2, 2, 2]},\n    {""img"": np.random.randint(0, 2, size=[3, 3, 3, 3])},\n    (3, 2, 2, 2),\n]\n\nTEST_CASE_2 = [\n    {""keys"": [""img""], ""roi_start"": [0, 0, 0], ""roi_end"": [2, 2, 2]},\n    {""img"": np.random.randint(0, 2, size=[3, 3, 3, 3])},\n    (3, 2, 2, 2),\n]\n\nTEST_CASE_3 = [\n    {""keys"": [""img""], ""roi_start"": [0, 0], ""roi_end"": [2, 2]},\n    {""img"": np.random.randint(0, 2, size=[3, 3, 3, 3])},\n    (3, 2, 2, 3),\n]\n\nTEST_CASE_4 = [\n    {""keys"": [""img""], ""roi_start"": [0, 0, 0, 0, 0], ""roi_end"": [2, 2, 2, 2, 2]},\n    {""img"": np.random.randint(0, 2, size=[3, 3, 3, 3])},\n    (3, 2, 2, 2),\n]\n\n\nclass TestSpatialCropd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_shape(self, input_param, input_data, expected_shape):\n        result = SpatialCropd(**input_param)(input_data)\n        self.assertTupleEqual(result[""img""].shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_spatial_pad.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import SpatialPad\n\nTEST_CASE_1 = [\n    {""spatial_size"": [15, 8, 8], ""method"": ""symmetric"", ""mode"": ""constant""},\n    np.zeros((3, 8, 8, 4)),\n    np.zeros((3, 15, 8, 8)),\n]\n\nTEST_CASE_2 = [\n    {""spatial_size"": [15, 8, 8], ""method"": ""end"", ""mode"": ""constant""},\n    np.zeros((3, 8, 8, 4)),\n    np.zeros((3, 15, 8, 8)),\n]\n\n\nclass TestSpatialPad(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_pad_shape(self, input_param, input_data, expected_val):\n        padder = SpatialPad(**input_param)\n        result = padder(input_data)\n        self.assertAlmostEqual(result.shape, expected_val.shape)\n        result = padder(input_data, mode=input_param[""mode""])\n        self.assertAlmostEqual(result.shape, expected_val.shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_spatial_padd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import SpatialPadd\n\nTEST_CASE_1 = [\n    {""keys"": [""img""], ""spatial_size"": [15, 8, 8], ""method"": ""symmetric"", ""mode"": ""constant""},\n    {""img"": np.zeros((3, 8, 8, 4))},\n    np.zeros((3, 15, 8, 8)),\n]\n\nTEST_CASE_2 = [\n    {""keys"": [""img""], ""spatial_size"": [15, 8, 8], ""method"": ""end"", ""mode"": ""constant""},\n    {""img"": np.zeros((3, 8, 8, 4))},\n    np.zeros((3, 15, 8, 8)),\n]\n\nTEST_CASE_3 = [\n    {""keys"": [""img""], ""spatial_size"": [15, 8, 8], ""method"": ""end"", ""mode"": {""constant""}},\n    {""img"": np.zeros((3, 8, 8, 4))},\n    np.zeros((3, 15, 8, 8)),\n]\n\n\nclass TestSpatialPadd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_pad_shape(self, input_param, input_data, expected_val):\n        padder = SpatialPadd(**input_param)\n        result = padder(input_data)\n        self.assertAlmostEqual(result[""img""].shape, expected_val.shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_split_channel.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom parameterized import parameterized\nfrom monai.transforms import SplitChannel\n\nTEST_CASE_1 = [{""to_onehot"": False}, torch.randint(0, 2, size=(4, 3, 3, 4)), (4, 1, 3, 4)]\n\nTEST_CASE_2 = [{""to_onehot"": True, ""num_classes"": 3}, torch.randint(0, 3, size=(4, 1, 3, 4)), (4, 1, 3, 4)]\n\n\nclass TestSplitChannel(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2])\n    def test_shape(self, input_param, test_data, expected_shape):\n        result = SplitChannel(**input_param)(test_data)\n        for data in result:\n            self.assertTupleEqual(data.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_split_channeld.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom parameterized import parameterized\nfrom monai.transforms import SplitChanneld\n\nTEST_CASE_1 = [\n    {""keys"": [""pred""], ""output_postfixes"": [""cls1"", ""cls2"", ""cls3""], ""to_onehot"": False},\n    {""pred"": torch.randint(0, 2, size=(4, 3, 3, 4))},\n    (4, 1, 3, 4),\n]\n\nTEST_CASE_2 = [\n    {""keys"": [""pred""], ""output_postfixes"": [""cls1"", ""cls2"", ""cls3""], ""to_onehot"": True, ""num_classes"": 3},\n    {""pred"": torch.randint(0, 3, size=(4, 1, 3, 4))},\n    (4, 1, 3, 4),\n]\n\nTEST_CASE_3 = [\n    {""keys"": [""pred"", ""label""], ""output_postfixes"": [""cls1"", ""cls2"", ""cls3""], ""to_onehot"": True, ""num_classes"": 3},\n    {""pred"": torch.randint(0, 3, size=(4, 1, 3, 4)), ""label"": torch.randint(0, 3, size=(4, 1, 3, 4))},\n    (4, 1, 3, 4),\n]\n\n\nclass TestSplitChanneld(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_shape(self, input_param, test_data, expected_shape):\n        result = SplitChanneld(**input_param)(test_data)\n        for k, v in result.items():\n            if ""cls"" in k:\n                self.assertTupleEqual(v.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_squeezedim.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import SqueezeDim\n\nTEST_CASE_1 = [{""dim"": None}, np.random.rand(1, 2, 1, 3), (2, 3)]\n\nTEST_CASE_2 = [{""dim"": 2}, np.random.rand(1, 2, 1, 8, 16), (1, 2, 8, 16)]\n\nTEST_CASE_3 = [{""dim"": -1}, np.random.rand(1, 1, 16, 8, 1), (1, 1, 16, 8)]\n\nTEST_CASE_4 = [{}, np.random.rand(1, 2, 1, 3), (2, 1, 3)]\n\nTEST_CASE_4_PT = [{}, torch.rand(1, 2, 1, 3), (2, 1, 3)]\n\nTEST_CASE_5 = [\n    {""dim"": -2},\n    np.random.rand(1, 1, 16, 8, 1),\n]\n\nTEST_CASE_6 = [\n    {""dim"": 0.5},\n    np.random.rand(1, 1, 16, 8, 1),\n]\n\n\nclass TestSqueezeDim(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_4_PT])\n    def test_shape(self, input_param, test_data, expected_shape):\n        result = SqueezeDim(**input_param)(test_data)\n        self.assertTupleEqual(result.shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_5, TEST_CASE_6])\n    def test_invalid_inputs(self, input_param, test_data):\n        with self.assertRaises(ValueError):\n            SqueezeDim(**input_param)(test_data)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_squeezedimd.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.transforms import SqueezeDimd\n\nTEST_CASE_1 = [\n    {""keys"": [""img"", ""seg""], ""dim"": None},\n    {""img"": np.random.rand(1, 2, 1, 3), ""seg"": np.random.randint(0, 2, size=[1, 2, 1, 3])},\n    (2, 3),\n]\n\nTEST_CASE_2 = [\n    {""keys"": [""img"", ""seg""], ""dim"": 2},\n    {""img"": np.random.rand(1, 2, 1, 8, 16), ""seg"": np.random.randint(0, 2, size=[1, 2, 1, 8, 16])},\n    (1, 2, 8, 16),\n]\n\nTEST_CASE_3 = [\n    {""keys"": [""img"", ""seg""], ""dim"": -1},\n    {""img"": np.random.rand(1, 1, 16, 8, 1), ""seg"": np.random.randint(0, 2, size=[1, 1, 16, 8, 1])},\n    (1, 1, 16, 8),\n]\n\nTEST_CASE_4 = [\n    {""keys"": [""img"", ""seg""]},\n    {""img"": np.random.rand(1, 2, 1, 3), ""seg"": np.random.randint(0, 2, size=[1, 2, 1, 3])},\n    (2, 1, 3),\n]\n\nTEST_CASE_4_PT = [\n    {""keys"": [""img"", ""seg""], ""dim"": 0},\n    {""img"": torch.rand(1, 2, 1, 3), ""seg"": torch.randint(0, 2, size=[1, 2, 1, 3])},\n    (2, 1, 3),\n]\n\nTEST_CASE_5 = [\n    {""keys"": [""img"", ""seg""], ""dim"": -2},\n    {""img"": np.random.rand(1, 1, 16, 8, 1), ""seg"": np.random.randint(0, 2, size=[1, 1, 16, 8, 1])},\n]\n\nTEST_CASE_6 = [\n    {""keys"": [""img"", ""seg""], ""dim"": 0.5},\n    {""img"": np.random.rand(1, 1, 16, 8, 1), ""seg"": np.random.randint(0, 2, size=[1, 1, 16, 8, 1])},\n]\n\n\nclass TestSqueezeDim(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_4_PT])\n    def test_shape(self, input_param, test_data, expected_shape):\n        result = SqueezeDimd(**input_param)(test_data)\n        self.assertTupleEqual(result[""img""].shape, expected_shape)\n        self.assertTupleEqual(result[""seg""].shape, expected_shape)\n\n    @parameterized.expand([TEST_CASE_5, TEST_CASE_6])\n    def test_invalid_inputs(self, input_param, test_data):\n        with self.assertRaises(ValueError):\n            SqueezeDimd(**input_param)(test_data)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_threshold_intensity.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import ThresholdIntensity\n\nTEST_CASE_1 = [{""threshold"": 5, ""above"": True, ""cval"": 0}, (0, 0, 0, 0, 0, 0, 6, 7, 8, 9)]\n\nTEST_CASE_2 = [{""threshold"": 5, ""above"": False, ""cval"": 0}, (0, 1, 2, 3, 4, 0, 0, 0, 0, 0)]\n\nTEST_CASE_3 = [{""threshold"": 5, ""above"": True, ""cval"": 5}, (5, 5, 5, 5, 5, 5, 6, 7, 8, 9)]\n\n\nclass TestThresholdIntensity(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_value(self, input_param, expected_value):\n        test_data = np.arange(10)\n        result = ThresholdIntensity(**input_param)(test_data)\n        np.testing.assert_allclose(result, expected_value)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_threshold_intensityd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport numpy as np\nfrom parameterized import parameterized\nfrom monai.transforms import ThresholdIntensityd\n\nTEST_CASE_1 = [\n    {""keys"": [""image"", ""label"", ""extra""], ""threshold"": 5, ""above"": True, ""cval"": 0},\n    (0, 0, 0, 0, 0, 0, 6, 7, 8, 9),\n]\n\nTEST_CASE_2 = [\n    {""keys"": [""image"", ""label"", ""extra""], ""threshold"": 5, ""above"": False, ""cval"": 0},\n    (0, 1, 2, 3, 4, 0, 0, 0, 0, 0),\n]\n\nTEST_CASE_3 = [\n    {""keys"": [""image"", ""label"", ""extra""], ""threshold"": 5, ""above"": True, ""cval"": 5},\n    (5, 5, 5, 5, 5, 5, 6, 7, 8, 9),\n]\n\n\nclass TestThresholdIntensityd(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3])\n    def test_value(self, input_param, expected_value):\n        test_data = {""image"": np.arange(10), ""label"": np.arange(10), ""extra"": np.arange(10)}\n        result = ThresholdIntensityd(**input_param)(test_data)\n        np.testing.assert_allclose(result[""image""], expected_value)\n        np.testing.assert_allclose(result[""label""], expected_value)\n        np.testing.assert_allclose(result[""extra""], expected_value)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_to_onehot.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.networks.utils import one_hot\n\nTEST_CASE_1 = [  # single channel 2D, batch 3, shape (2, 1, 2, 2)\n    {""labels"": torch.tensor([[[[0, 1], [1, 2]]], [[[2, 1], [1, 0]]]]), ""num_classes"": 3},\n    (2, 3, 2, 2),\n]\n\nTEST_CASE_2 = [  # single channel 1D, batch 2, shape (2, 1, 4)\n    {""labels"": torch.tensor([[[1, 2, 2, 0]], [[2, 1, 0, 1]]]), ""num_classes"": 3},\n    (2, 3, 4),\n    np.array([[[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 1, 0]], [[0, 0, 1, 0], [0, 1, 0, 1], [1, 0, 0, 0]]]),\n]\n\nTEST_CASE_3 = [  # single channel 0D, batch 2, shape (2, 1)\n    {""labels"": torch.tensor([[1.0], [2.0]]), ""num_classes"": 3},\n    (2, 3),\n    np.array([[0, 1, 0], [0, 0, 1]]),\n]\n\nTEST_CASE_4 = [  # no channel 0D, batch 3, shape (3)\n    {""labels"": torch.tensor([1, 2, 0]), ""num_classes"": 3, ""dtype"": torch.long},\n    (3, 3),\n    np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]]),\n]\n\n\nclass TestToOneHot(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_shape(self, input_data, expected_shape, expected_result=None):\n        result = one_hot(**input_data)\n        self.assertEqual(result.shape, expected_shape)\n        if expected_result is not None:\n            self.assertTrue(np.allclose(expected_result, result.numpy()))\n\n        if ""dtype"" in input_data:\n            self.assertEqual(result.dtype, input_data[""dtype""])\n        else:\n            # by default, expecting float type\n            self.assertEqual(result.dtype, torch.float)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_tversky_loss.py,23,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.losses import TverskyLoss\n\nTEST_CASES = [\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.307576,\n    ],\n    [  # shape: (2, 1, 2, 2), (2, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]], [[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 1.0], [1.0, 1.0]]], [[[1.0, 0.0], [1.0, 0.0]]]]),\n            ""smooth"": 1e-4,\n        },\n        0.416657,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": False, ""to_onehot_y"": True},\n        {\n            ""input"": torch.tensor([[[1.0, 1.0, 0.0], [0.0, 0.0, 1.0]], [[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]]]),\n            ""target"": torch.tensor([[[0.0, 0.0, 1.0]], [[0.0, 1.0, 0.0]]]),\n            ""smooth"": 0.0,\n        },\n        0.0,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""sigmoid"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.435050,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""sigmoid"": True, ""reduction"": ""sum""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        1.74013,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        0.383713,\n    ],\n    [  # shape: (2, 2, 3), (2, 1, 3)\n        {""include_background"": True, ""to_onehot_y"": True, ""softmax"": True, ""reduction"": ""none""},\n        {\n            ""input"": torch.tensor([[[-1.0, 0.0, 1.0], [1.0, 0.0, -1.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]]),\n            ""target"": torch.tensor([[[1.0, 0.0, 0.0]], [[1.0, 1.0, 0.0]]]),\n            ""smooth"": 1e-4,\n        },\n        [[0.210961, 0.295339], [0.599952, 0.428547]],\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True, ""alpha"": 0.3, ""beta"": 0.7},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.3589,\n    ],\n    [  # shape: (1, 1, 2, 2), (1, 1, 2, 2)\n        {""include_background"": True, ""sigmoid"": True, ""alpha"": 0.7, ""beta"": 0.3},\n        {\n            ""input"": torch.tensor([[[[1.0, -1.0], [-1.0, 1.0]]]]),\n            ""target"": torch.tensor([[[[1.0, 0.0], [1.0, 1.0]]]]),\n            ""smooth"": 1e-6,\n        },\n        0.247366,\n    ],\n]\n\n\nclass TestTverskyLoss(unittest.TestCase):\n    @parameterized.expand(TEST_CASES)\n    def test_shape(self, input_param, input_data, expected_val):\n        result = TverskyLoss(**input_param).forward(**input_data)\n        np.testing.assert_allclose(result.detach().cpu().numpy(), expected_val, rtol=1e-4)\n\n    def test_ill_shape(self):\n        loss = TverskyLoss()\n        with self.assertRaisesRegex(AssertionError, """"):\n            loss.forward(torch.ones((2, 2, 3)), torch.ones((4, 5, 6)))\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertRaisesRegex(ValueError, """"):\n            TverskyLoss(reduction=""unknown"")(chn_input, chn_target)\n        with self.assertRaisesRegex(ValueError, """"):\n            TverskyLoss(reduction=None)(chn_input, chn_target)\n\n    def test_input_warnings(self):\n        chn_input = torch.ones((1, 1, 3))\n        chn_target = torch.ones((1, 1, 3))\n        with self.assertWarns(Warning):\n            loss = TverskyLoss(include_background=False)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = TverskyLoss(softmax=True)\n            loss.forward(chn_input, chn_target)\n        with self.assertWarns(Warning):\n            loss = TverskyLoss(to_onehot_y=True)\n            loss.forward(chn_input, chn_target)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_unet.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport torch\nfrom parameterized import parameterized\n\nfrom monai.networks.layers import Norm, Act\nfrom monai.networks.nets import UNet\n\n\nTEST_CASE_0 = [  # single channel 2D, batch 16, no residual\n    {\n        ""dimensions"": 2,\n        ""in_channels"": 1,\n        ""out_channels"": 3,\n        ""channels"": (16, 32, 64),\n        ""strides"": (2, 2),\n        ""num_res_units"": 0,\n    },\n    torch.randn(16, 1, 32, 32),\n    (16, 3, 32, 32),\n]\n\nTEST_CASE_1 = [  # single channel 2D, batch 16\n    {\n        ""dimensions"": 2,\n        ""in_channels"": 1,\n        ""out_channels"": 3,\n        ""channels"": (16, 32, 64),\n        ""strides"": (2, 2),\n        ""num_res_units"": 1,\n    },\n    torch.randn(16, 1, 32, 32),\n    (16, 3, 32, 32),\n]\n\nTEST_CASE_2 = [  # single channel 3D, batch 16\n    {\n        ""dimensions"": 3,\n        ""in_channels"": 1,\n        ""out_channels"": 3,\n        ""channels"": (16, 32, 64),\n        ""strides"": (2, 2),\n        ""num_res_units"": 1,\n    },\n    torch.randn(16, 1, 32, 24, 48),\n    (16, 3, 32, 24, 48),\n]\n\nTEST_CASE_3 = [  # 4-channel 3D, batch 16\n    {\n        ""dimensions"": 3,\n        ""in_channels"": 4,\n        ""out_channels"": 3,\n        ""channels"": (16, 32, 64),\n        ""strides"": (2, 2),\n        ""num_res_units"": 1,\n    },\n    torch.randn(16, 4, 32, 64, 48),\n    (16, 3, 32, 64, 48),\n]\n\nTEST_CASE_4 = [  # 4-channel 3D, batch 16, batch normalisation\n    {\n        ""dimensions"": 3,\n        ""in_channels"": 4,\n        ""out_channels"": 3,\n        ""channels"": (16, 32, 64),\n        ""strides"": (2, 2),\n        ""num_res_units"": 1,\n        ""norm"": Norm.BATCH,\n    },\n    torch.randn(16, 4, 32, 64, 48),\n    (16, 3, 32, 64, 48),\n]\n\nTEST_CASE_5 = [  # 4-channel 3D, batch 16, LeakyReLU activation\n    {\n        ""dimensions"": 3,\n        ""in_channels"": 4,\n        ""out_channels"": 3,\n        ""channels"": (16, 32, 64),\n        ""strides"": (2, 2),\n        ""num_res_units"": 1,\n        ""act"": (Act.LEAKYRELU, {""negative_slope"": 0.2}),\n    },\n    torch.randn(16, 4, 32, 64, 48),\n    (16, 3, 32, 64, 48),\n]\n\nTEST_CASE_6 = [  # 4-channel 3D, batch 16, LeakyReLU activation explicit\n    {\n        ""dimensions"": 3,\n        ""in_channels"": 4,\n        ""out_channels"": 3,\n        ""channels"": (16, 32, 64),\n        ""strides"": (2, 2),\n        ""num_res_units"": 1,\n        ""act"": (torch.nn.LeakyReLU, {""negative_slope"": 0.2}),\n    },\n    torch.randn(16, 4, 32, 64, 48),\n    (16, 3, 32, 64, 48),\n]\n\nCASES = [TEST_CASE_0, TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4, TEST_CASE_5, TEST_CASE_6]\n\n\nclass TestUNET(unittest.TestCase):\n    @parameterized.expand(CASES)\n    def test_shape(self, input_param, input_data, expected_shape):\n        net = UNet(**input_param)\n        net.eval()\n        with torch.no_grad():\n            result = net.forward(input_data)\n            self.assertEqual(result.shape, expected_shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_zipdataset.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport torch\nfrom parameterized import parameterized\nfrom monai.data import ZipDataset\n\n\nclass Dataset_(torch.utils.data.Dataset):\n    def __init__(self, length, index_only=True):\n        self.len = length\n        self.index_only = index_only\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, index):\n        if self.index_only:\n            return index\n        else:\n            return 1, 2, index\n\n\nTEST_CASE_1 = [[Dataset_(5), Dataset_(5), Dataset_(5)], None, [0, 0, 0], 5]\n\nTEST_CASE_2 = [[Dataset_(3), Dataset_(4), Dataset_(5)], None, [0, 0, 0], 3]\n\nTEST_CASE_3 = [[Dataset_(3), Dataset_(4, index_only=False), Dataset_(5)], None, [0, 1, 2, 0, 0], 3]\n\nTEST_CASE_4 = [\n    [Dataset_(3), Dataset_(4, index_only=False), Dataset_(5)],\n    lambda x: [i + 1 for i in x],\n    [1, 2, 3, 1, 1],\n    3,\n]\n\n\nclass TestZipDataset(unittest.TestCase):\n    @parameterized.expand([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3, TEST_CASE_4])\n    def test_value(self, datasets, transform, expected_output, expected_length):\n        test_dataset = ZipDataset(datasets=datasets, transform=transform)\n        self.assertEqual(test_dataset[0], expected_output)\n        self.assertEqual(len(test_dataset), expected_length)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_zoom.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport importlib\n\nfrom scipy.ndimage import zoom as zoom_scipy\nfrom parameterized import parameterized\n\nfrom monai.transforms import Zoom\nfrom tests.utils import NumpyImageTestCase2D\n\nVALID_CASES = [\n    (1.1, 3, ""constant"", 0, True, False, False),\n    (0.9, 3, ""constant"", 0, True, False, False),\n    (0.8, 1, ""reflect"", 0, False, False, False),\n]\n\nGPU_CASES = [(""gpu_zoom"", 0.6, 1, ""constant"", 0, True)]\n\nINVALID_CASES = [(""no_zoom"", None, 1, TypeError), (""invalid_order"", 0.9, ""s"", TypeError)]\n\n\nclass TestZoom(NumpyImageTestCase2D):\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, zoom, order, mode, cval, prefilter, use_gpu, keep_size):\n        zoom_fn = Zoom(\n            zoom=zoom,\n            interp_order=order,\n            mode=mode,\n            cval=cval,\n            prefilter=prefilter,\n            use_gpu=use_gpu,\n            keep_size=keep_size,\n        )\n        zoomed = zoom_fn(self.imt[0])\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(zoom_scipy(channel, zoom=zoom, mode=mode, order=order, cval=cval, prefilter=prefilter))\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, zoomed))\n\n    @parameterized.expand(GPU_CASES)\n    def test_gpu_zoom(self, _, zoom, order, mode, cval, prefilter):\n        if importlib.util.find_spec(""cupy""):\n            zoom_fn = Zoom(\n                zoom=zoom, interp_order=order, mode=mode, cval=cval, prefilter=prefilter, use_gpu=True, keep_size=False\n            )\n            zoomed = zoom_fn(self.imt[0])\n            expected = list()\n            for channel in self.imt[0]:\n                expected.append(zoom_scipy(channel, zoom=zoom, mode=mode, order=order, cval=cval, prefilter=prefilter))\n            expected = np.stack(expected).astype(np.float32)\n            self.assertTrue(np.allclose(expected, zoomed))\n\n    def test_keep_size(self):\n        zoom_fn = Zoom(zoom=0.6, keep_size=True)\n        zoomed = zoom_fn(self.imt[0])\n        self.assertTrue(np.array_equal(zoomed.shape, self.imt.shape[1:]))\n\n        zoom_fn = Zoom(zoom=1.3, keep_size=True)\n        zoomed = zoom_fn(self.imt[0])\n        self.assertTrue(np.array_equal(zoomed.shape, self.imt.shape[1:]))\n\n    @parameterized.expand(INVALID_CASES)\n    def test_invalid_inputs(self, _, zoom, order, raises):\n        with self.assertRaises(raises):\n            zoom_fn = Zoom(zoom=zoom, interp_order=order)\n            zoomed = zoom_fn(self.imt[0])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_zoom_affine.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport nibabel as nib\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom monai.data.utils import zoom_affine\n\nVALID_CASES = [\n    (\n        np.array([[2, 1, 4], [-1, -3, 5], [0, 0, 1]],),\n        (10, 20, 30),\n        np.array([[8.94427191, -8.94427191, 0], [-4.47213595, -17.88854382, 0], [0.0, 0.0, 1.0]],),\n    ),\n    (\n        np.array([[1, 0, 0, 4], [0, 2, 0, 5], [0, 0, 3, 6], [0, 0, 0, 1]],),\n        (10, 20, 30),\n        np.array([[10, 0, 0, 0], [0, 20, 0, 0], [0, 0, 30, 0], [0, 0, 0, 1]],),\n    ),\n    (\n        np.array([[1, 0, 0, 4], [0, 2, 0, 5], [0, 0, 3, 6], [0, 0, 0, 1]],),\n        (10, 20),\n        np.array([[10, 0, 0, 0], [0, 20, 0, 0], [0, 0, 3, 0], [0, 0, 0, 1]],),\n    ),\n    (\n        np.array([[1, 0, 0, 4], [0, 2, 0, 5], [0, 0, 3, 6], [0, 0, 0, 1]],),\n        (10,),\n        np.array([[10, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 1]],),\n    ),\n    (\n        [[1, 0, 10], [0, 1, 20], [0, 0, 1]]\n        @ ([[0, -1, 0], [1, 0, 0], [0, 0, 1]] @ np.array([[2, 0.3, 0], [0, 3, 0], [0, 0, 1]])),\n        (4, 5, 6),\n        ([[0, -1, 0], [1, 0, 0], [0, 0, 1]] @ np.array([[4, 0, 0], [0, 5, 0], [0, 0, 1]])),\n    ),\n]\n\nDIAGONAL_CASES = [\n    (\n        np.array([[-1, 0, 0, 4], [0, 2, 0, 5], [0, 0, 3, 6], [0, 0, 0, 1]],),\n        (10, 20, 30),\n        np.array([[10, 0, 0, 0], [0, 20, 0, 0], [0, 0, 30, 0], [0, 0, 0, 1]],),\n    ),\n    (\n        np.array([[2, 1, 4], [-1, -3, 5], [0, 0, 1]],),\n        (10, 20, 30),\n        np.array([[10, 0, 0], [0, 20, 0], [0.0, 0.0, 1.0]],),\n    ),\n    (  # test default scale from affine\n        np.array([[2, 1, 4], [-1, -3, 5], [0, 0, 1]],),\n        (10,),\n        np.array([[10, 0, 0], [0, 3.162278, 0], [0.0, 0.0, 1.0]],),\n    ),\n]\n\n\nclass TestZoomAffine(unittest.TestCase):\n    @parameterized.expand(VALID_CASES)\n    def test_correct(self, affine, scale, expected):\n        output = zoom_affine(affine, scale, diagonal=False)\n        ornt_affine = nib.orientations.ornt2axcodes(nib.orientations.io_orientation(output))\n        ornt_output = nib.orientations.ornt2axcodes(nib.orientations.io_orientation(affine))\n        np.testing.assert_array_equal(ornt_affine, ornt_output)\n        np.testing.assert_allclose(output, expected, rtol=1e-6, atol=1e-6)\n\n    @parameterized.expand(DIAGONAL_CASES)\n    def test_diagonal(self, affine, scale, expected):\n        output = zoom_affine(affine, scale, diagonal=True)\n        np.testing.assert_allclose(output, expected, rtol=1e-6, atol=1e-6)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_zoomd.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport importlib\n\nfrom scipy.ndimage import zoom as zoom_scipy\nfrom parameterized import parameterized\n\nfrom monai.transforms import Zoomd\nfrom tests.utils import NumpyImageTestCase2D\n\nVALID_CASES = [\n    (1.1, 3, ""constant"", 0, True, False, False),\n    (0.9, 3, ""constant"", 0, True, False, False),\n    (0.8, 1, ""reflect"", 0, False, False, False),\n]\n\nGPU_CASES = [(""gpu_zoom"", 0.6, 1, ""constant"", 0, True)]\n\nINVALID_CASES = [(""no_zoom"", None, 1, TypeError), (""invalid_order"", 0.9, ""s"", TypeError)]\n\n\nclass TestZoomd(NumpyImageTestCase2D):\n    @parameterized.expand(VALID_CASES)\n    def test_correct_results(self, zoom, order, mode, cval, prefilter, use_gpu, keep_size):\n        key = ""img""\n        zoom_fn = Zoomd(\n            key,\n            zoom=zoom,\n            interp_order=order,\n            mode=mode,\n            cval=cval,\n            prefilter=prefilter,\n            use_gpu=use_gpu,\n            keep_size=keep_size,\n        )\n        zoomed = zoom_fn({key: self.imt[0]})\n        expected = list()\n        for channel in self.imt[0]:\n            expected.append(zoom_scipy(channel, zoom=zoom, mode=mode, order=order, cval=cval, prefilter=prefilter))\n        expected = np.stack(expected).astype(np.float32)\n        self.assertTrue(np.allclose(expected, zoomed[key]))\n\n    @parameterized.expand(GPU_CASES)\n    def test_gpu_zoom(self, _, zoom, order, mode, cval, prefilter):\n        key = ""img""\n        if importlib.util.find_spec(""cupy""):\n            zoom_fn = Zoomd(\n                key,\n                zoom=zoom,\n                interp_order=order,\n                mode=mode,\n                cval=cval,\n                prefilter=prefilter,\n                use_gpu=True,\n                keep_size=False,\n            )\n            zoomed = zoom_fn({key: self.imt[0]})\n            expected = list()\n            for channel in self.imt[0]:\n                expected.append(zoom_scipy(channel, zoom=zoom, mode=mode, order=order, cval=cval, prefilter=prefilter))\n            expected = np.stack(expected).astype(np.float32)\n            self.assertTrue(np.allclose(expected, zoomed[key]))\n\n    def test_keep_size(self):\n        key = ""img""\n        zoom_fn = Zoomd(key, zoom=0.6, keep_size=True)\n        zoomed = zoom_fn({key: self.imt[0]})\n        self.assertTrue(np.array_equal(zoomed[key].shape, self.imt.shape[1:]))\n\n        zoom_fn = Zoomd(key, zoom=1.3, keep_size=True)\n        zoomed = zoom_fn({key: self.imt[0]})\n        self.assertTrue(np.array_equal(zoomed[key].shape, self.imt.shape[1:]))\n\n    @parameterized.expand(INVALID_CASES)\n    def test_invalid_inputs(self, _, zoom, order, raises):\n        key = ""img""\n        with self.assertRaises(raises):\n            zoom_fn = Zoomd(key, zoom=zoom, interp_order=order)\n            zoomed = zoom_fn({key: self.imt[0]})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/utils.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport tempfile\nimport unittest\nfrom subprocess import PIPE, Popen\n\nimport nibabel as nib\nimport numpy as np\nimport torch\n\nfrom monai.data.synthetic import create_test_image_2d\n\nquick_test_var = ""QUICKTEST""\n\n\ndef skip_if_quick(obj):\n    is_quick = os.environ.get(quick_test_var, """").lower() == ""true""\n\n    return unittest.skipIf(is_quick, ""Skipping slow tests"")(obj)\n\n\ndef make_nifti_image(array, affine=None):\n    """"""\n    Create a temporary nifti image on the disk and return the image name.\n    User is responsible for deleting the temporary file when done with it.\n    """"""\n    if affine is None:\n        affine = np.eye(4)\n    test_image = nib.Nifti1Image(array, affine)\n\n    temp_f, image_name = tempfile.mkstemp(suffix="".nii.gz"")\n    nib.save(test_image, image_name)\n    os.close(temp_f)\n    return image_name\n\n\nclass NumpyImageTestCase2D(unittest.TestCase):\n    im_shape = (128, 128)\n    input_channels = 1\n    output_channels = 4\n    num_classes = 3\n\n    def setUp(self):\n        im, msk = create_test_image_2d(self.im_shape[0], self.im_shape[1], 4, 20, 0, self.num_classes)\n\n        self.imt = im[None, None]\n        self.seg1 = (msk[None, None] > 0).astype(np.float32)\n        self.segn = msk[None, None]\n\n\nclass TorchImageTestCase2D(NumpyImageTestCase2D):\n    def setUp(self):\n        NumpyImageTestCase2D.setUp(self)\n        self.imt = torch.tensor(self.imt)\n        self.seg1 = torch.tensor(self.seg1)\n        self.segn = torch.tensor(self.segn)\n\n\ndef expect_failure_if_no_gpu(test):\n    if not torch.cuda.is_available():\n        return unittest.expectedFailure(test)\n    else:\n        return test\n\n\ndef query_memory(n=2):\n    """"""\n    Find best n idle devices and return a string of device ids.\n    """"""\n    bash_string = ""nvidia-smi --query-gpu=utilization.gpu,temperature.gpu,memory.used --format=csv,noheader,nounits""\n\n    try:\n        p1 = Popen(bash_string.split(), stdout=PIPE)\n        output, error = p1.communicate()\n        free_memory = [x.split("","") for x in output.decode(""utf-8"").split(""\\n"")[:-1]]\n        free_memory = np.asarray(free_memory, dtype=np.float).T\n        ids = np.lexsort(free_memory)[:n]\n    except (FileNotFoundError, TypeError, IndexError):\n        ids = range(n) if isinstance(n, int) else []\n    return "","".join([f""{int(x)}"" for x in ids])\n\n\nif __name__ == ""__main__"":\n    print(query_memory())\n'"
docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport subprocess\n\nsys.path.insert(0, os.path.abspath(""..""))\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "".."", "".."")))\nprint(sys.path)\n\nimport monai  # noqa: E402\n\n# -- Project information -----------------------------------------------------\nproject = ""MONAI""\ncopyright = ""2020, MONAI Contributors""\nauthor = ""MONAI Contributors""\n\n# The full version, including alpha/beta/rc tags\nshort_version = monai.__version__.split(""+"")[0]\nrelease = short_version\nversion = short_version\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\n    ""transforms"",\n    ""networks"",\n    ""metrics"",\n    ""engines"",\n    ""data"",\n    ""application"",\n    ""config"",\n    ""handlers"",\n    ""losses"",\n    ""visualize"",\n    ""utils"",\n    ""inferers"",\n]\n\n\ndef generate_apidocs(*args):\n    """"""Generate API docs automatically by trawling the available modules""""""\n    module_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "".."", "".."", ""monai""))\n    output_path = os.path.abspath(os.path.join(os.path.dirname(__file__), ""apidocs""))\n    apidoc_command_path = ""sphinx-apidoc""\n    if hasattr(sys, ""real_prefix""):  # called from a virtualenv\n        apidoc_command_path = os.path.join(sys.prefix, ""bin"", ""sphinx-apidoc"")\n        apidoc_command_path = os.path.abspath(apidoc_command_path)\n    print(f""output_path {output_path}"")\n    print(f""module_path {module_path}"")\n    subprocess.check_call(\n        [apidoc_command_path, ""-e""]\n        + [""-o"", output_path]\n        + [module_path]\n        + [os.path.join(module_path, p) for p in exclude_patterns]\n    )\n\n\ndef setup(app):\n    # Hook to allow for automatic generation of API docs\n    # before doc deployment begins.\n    app.connect(""builder-inited"", generate_apidocs)\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nsource_suffix = {\n    "".rst"": ""restructuredtext"",\n    "".txt"": ""restructuredtext"",\n    "".md"": ""markdown"",\n}\n\nextensions = [\n    ""recommonmark"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.autosectionlabel"",\n]\n\nautoclass_content = ""both""\nadd_module_names = True\nautosectionlabel_prefix_document = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n# html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\nhtml_theme_options = {\n    ""collapse_navigation"": True,\n    ""display_version"": True,\n    ""sticky_navigation"": True,  # Set to False to disable the sticky nav while scrolling.\n    ""logo_only"": True,  # if we have a html_logo below, this shows /only/ the logo with no title text\n    ""style_nav_header_background"": ""#FBFBFB"",\n}\nhtml_context = {\n    ""display_github"": True,\n    ""github_user"": ""Project-MONAI"",\n    ""github_repo"": ""MONAI"",\n    ""github_version"": ""master"",\n    ""conf_py_path"": ""/docs/"",\n}\nhtml_scaled_image_link = False\nhtml_show_sourcelink = True\nhtml_favicon = ""../images/favicon.ico""\nhtml_logo = ""../images/MONAI-logo-color.png""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""../_static""]\nhtml_css_files = [""custom.css""]\n'"
examples/classification_3d/densenet_evaluation_array.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.data import NiftiDataset, CSVSaver\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, Resize, ToTensor\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n\n    # Define transforms for image\n    val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), ToTensor()])\n\n    # Define nifti dataset\n    val_ds = NiftiDataset(image_files=images, labels=labels, transform=val_transforms, image_only=False)\n    # create a validation data loader\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    # Create DenseNet121\n    device = torch.device(""cuda:0"")\n    model = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,).to(device)\n\n    model.load_state_dict(torch.load(""best_metric_model.pth""))\n    model.eval()\n    with torch.no_grad():\n        num_correct = 0.0\n        metric_count = 0\n        saver = CSVSaver(output_dir=""./output"")\n        for val_data in val_loader:\n            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n            val_outputs = model(val_images).argmax(dim=1)\n            value = torch.eq(val_outputs, val_labels)\n            metric_count += len(value)\n            num_correct += value.sum().item()\n            saver.save_batch(val_outputs, val_data[2])\n        metric = num_correct / metric_count\n        print(""evaluation metric:"", metric)\n        saver.finalize()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/classification_3d/densenet_evaluation_dict.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.transforms import Compose, LoadNiftid, AddChanneld, ScaleIntensityd, Resized, ToTensord\nfrom monai.data import CSVSaver\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n    val_files = [{""img"": img, ""label"": label} for img, label in zip(images, labels)]\n\n    # Define transforms for image\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img""]),\n            AddChanneld(keys=[""img""]),\n            ScaleIntensityd(keys=[""img""]),\n            Resized(keys=[""img""], spatial_size=(96, 96, 96)),\n            ToTensord(keys=[""img""]),\n        ]\n    )\n\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    # Create DenseNet121\n    device = torch.device(""cuda:0"")\n    model = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,).to(device)\n\n    model.load_state_dict(torch.load(""best_metric_model.pth""))\n    model.eval()\n    with torch.no_grad():\n        num_correct = 0.0\n        metric_count = 0\n        saver = CSVSaver(output_dir=""./output"")\n        for val_data in val_loader:\n            val_images, val_labels = val_data[""img""].to(device), val_data[""label""].to(device)\n            val_outputs = model(val_images).argmax(dim=1)\n            value = torch.eq(val_outputs, val_labels)\n            metric_count += len(value)\n            num_correct += value.sum().item()\n            saver.save_batch(val_outputs, {""filename_or_obj"": val_data[""img.filename_or_obj""]})\n        metric = num_correct / metric_count\n        print(""evaluation metric:"", metric)\n        saver.finalize()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/classification_3d/densenet_training_array.py,11,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport monai\nfrom monai.data import NiftiDataset\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, Resize, RandRotate90, ToTensor\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI314-IOP-0889-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI249-Guys-1072-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI609-HH-2600-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI173-HH-1590-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI020-Guys-0700-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI342-Guys-0909-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI134-Guys-0780-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI577-HH-2661-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI066-Guys-0731-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI130-HH-1528-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n\n    # Define transforms\n    train_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), RandRotate90(), ToTensor()])\n    val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), ToTensor()])\n\n    # Define nifti dataset, data loader\n    check_ds = NiftiDataset(image_files=images, labels=labels, transform=train_transforms)\n    check_loader = DataLoader(check_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n    im, label = monai.utils.misc.first(check_loader)\n    print(type(im), im.shape, label)\n\n    # create a training data loader\n    train_ds = NiftiDataset(image_files=images[:10], labels=labels[:10], transform=train_transforms)\n    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n\n    # create a validation data loader\n    val_ds = NiftiDataset(image_files=images[-10:], labels=labels[-10:], transform=val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n\n    # Create DenseNet121, CrossEntropyLoss and Adam optimizer\n    device = torch.device(""cuda:0"")\n    model = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,).to(device)\n    loss_function = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n\n    # start a typical PyTorch training\n    val_interval = 2\n    best_metric = -1\n    best_metric_epoch = -1\n    epoch_loss_values = list()\n    metric_values = list()\n    writer = SummaryWriter()\n    for epoch in range(5):\n        print(""-"" * 10)\n        print(f""epoch {epoch + 1}/{5}"")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            epoch_len = len(train_ds) // train_loader.batch_size\n            print(f""{step}/{epoch_len}, train_loss: {loss.item():.4f}"")\n            writer.add_scalar(""train_loss"", loss.item(), epoch_len * epoch + step)\n        epoch_loss /= step\n        epoch_loss_values.append(epoch_loss)\n        print(f""epoch {epoch + 1} average loss: {epoch_loss:.4f}"")\n\n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                num_correct = 0.0\n                metric_count = 0\n                for val_data in val_loader:\n                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n                    val_outputs = model(val_images)\n                    value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n                    metric_count += len(value)\n                    num_correct += value.sum().item()\n                metric = num_correct / metric_count\n                metric_values.append(metric)\n                if metric > best_metric:\n                    best_metric = metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), ""best_metric_model.pth"")\n                    print(""saved new best metric model"")\n                print(\n                    ""current epoch: {} current accuracy: {:.4f} best accuracy: {:.4f} at epoch {}"".format(\n                        epoch + 1, metric, best_metric, best_metric_epoch\n                    )\n                )\n                writer.add_scalar(""val_accuracy"", metric, epoch + 1)\n    print(f""train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}"")\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/classification_3d/densenet_training_dict.py,15,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport monai\nfrom monai.transforms import Compose, LoadNiftid, AddChanneld, ScaleIntensityd, Resized, RandRotate90d, ToTensord\nfrom monai.metrics import compute_roc_auc\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI314-IOP-0889-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI249-Guys-1072-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI609-HH-2600-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI173-HH-1590-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI020-Guys-0700-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI342-Guys-0909-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI134-Guys-0780-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI577-HH-2661-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI066-Guys-0731-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI130-HH-1528-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n    train_files = [{""img"": img, ""label"": label} for img, label in zip(images[:10], labels[:10])]\n    val_files = [{""img"": img, ""label"": label} for img, label in zip(images[-10:], labels[-10:])]\n\n    # Define transforms for image\n    train_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img""]),\n            AddChanneld(keys=[""img""]),\n            ScaleIntensityd(keys=[""img""]),\n            Resized(keys=[""img""], spatial_size=(96, 96, 96)),\n            RandRotate90d(keys=[""img""], prob=0.8, spatial_axes=[0, 2]),\n            ToTensord(keys=[""img""]),\n        ]\n    )\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img""]),\n            AddChanneld(keys=[""img""]),\n            ScaleIntensityd(keys=[""img""]),\n            Resized(keys=[""img""], spatial_size=(96, 96, 96)),\n            ToTensord(keys=[""img""]),\n        ]\n    )\n\n    # Define dataset, data loader\n    check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n    check_data = monai.utils.misc.first(check_loader)\n    print(check_data[""img""].shape, check_data[""label""])\n\n    # create a training data loader\n    train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    # Create DenseNet121, CrossEntropyLoss and Adam optimizer\n    device = torch.device(""cuda:0"")\n    model = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,).to(device)\n    loss_function = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n\n    # start a typical PyTorch training\n    val_interval = 2\n    best_metric = -1\n    best_metric_epoch = -1\n    writer = SummaryWriter()\n    for epoch in range(5):\n        print(""-"" * 10)\n        print(f""epoch {epoch + 1}/{5}"")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = batch_data[""img""].to(device), batch_data[""label""].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            epoch_len = len(train_ds) // train_loader.batch_size\n            print(f""{step}/{epoch_len}, train_loss: {loss.item():.4f}"")\n            writer.add_scalar(""train_loss"", loss.item(), epoch_len * epoch + step)\n        epoch_loss /= step\n        print(f""epoch {epoch + 1} average loss: {epoch_loss:.4f}"")\n\n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n                y = torch.tensor([], dtype=torch.long, device=device)\n                for val_data in val_loader:\n                    val_images, val_labels = val_data[""img""].to(device), val_data[""label""].to(device)\n                    y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n                    y = torch.cat([y, val_labels], dim=0)\n\n                acc_value = torch.eq(y_pred.argmax(dim=1), y)\n                acc_metric = acc_value.sum().item() / len(acc_value)\n                auc_metric = compute_roc_auc(y_pred, y, to_onehot_y=True, softmax=True)\n                if acc_metric > best_metric:\n                    best_metric = acc_metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), ""best_metric_model.pth"")\n                    print(""saved new best metric model"")\n                print(\n                    ""current epoch: {} current accuracy: {:.4f} current AUC: {:.4f} best accuracy: {:.4f} at epoch {}"".format(\n                        epoch + 1, acc_metric, auc_metric, best_metric, best_metric_epoch\n                    )\n                )\n                writer.add_scalar(""val_accuracy"", acc_metric, epoch + 1)\n    print(f""train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}"")\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/classification_3d_ignite/densenet_evaluation_array.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom ignite.engine import create_supervised_evaluator, _prepare_batch\nfrom ignite.metrics import Accuracy\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.data import NiftiDataset\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, Resize, ToTensor\nfrom monai.handlers import StatsHandler, ClassificationSaver, CheckpointLoader\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n\n    # define transforms for image\n    val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), ToTensor()])\n    # define nifti dataset\n    val_ds = NiftiDataset(image_files=images, labels=labels, transform=val_transforms, image_only=False)\n    # create DenseNet121\n    net = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,)\n    device = torch.device(""cuda:0"")\n\n    metric_name = ""Accuracy""\n    # add evaluation metric to the evaluator engine\n    val_metrics = {metric_name: Accuracy()}\n\n    def prepare_batch(batch, device=None, non_blocking=False):\n        return _prepare_batch((batch[0], batch[1]), device, non_blocking)\n\n    # Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration,\n    # user can add output_transform to return other values\n    evaluator = create_supervised_evaluator(net, val_metrics, device, True, prepare_batch=prepare_batch)\n\n    # add stats event handler to print validation stats via evaluator\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n    )\n    val_stats_handler.attach(evaluator)\n\n    # for the array data format, assume the 3rd item of batch data is the meta_data\n    prediction_saver = ClassificationSaver(\n        output_dir=""tempdir"",\n        batch_transform=lambda batch: batch[2],\n        output_transform=lambda output: output[0].argmax(1),\n    )\n    prediction_saver.attach(evaluator)\n\n    # the model was trained by ""densenet_training_array"" example\n    CheckpointLoader(load_path=""./runs/net_checkpoint_20.pth"", load_dict={""net"": net}).attach(evaluator)\n\n    # create a validation data loader\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    state = evaluator.run(val_loader)\n    print(state)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/classification_3d_ignite/densenet_evaluation_dict.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom ignite.metrics import Accuracy\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom ignite.engine import create_supervised_evaluator, _prepare_batch\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.handlers import StatsHandler, CheckpointLoader, ClassificationSaver\nfrom monai.transforms import Compose, LoadNiftid, AddChanneld, ScaleIntensityd, Resized, ToTensord\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n    val_files = [{""img"": img, ""label"": label} for img, label in zip(images, labels)]\n\n    # define transforms for image\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img""]),\n            AddChanneld(keys=[""img""]),\n            ScaleIntensityd(keys=[""img""]),\n            Resized(keys=[""img""], spatial_size=(96, 96, 96)),\n            ToTensord(keys=[""img""]),\n        ]\n    )\n\n    # create DenseNet121\n    net = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,)\n    device = torch.device(""cuda:0"")\n\n    def prepare_batch(batch, device=None, non_blocking=False):\n        return _prepare_batch((batch[""img""], batch[""label""]), device, non_blocking)\n\n    metric_name = ""Accuracy""\n    # add evaluation metric to the evaluator engine\n    val_metrics = {metric_name: Accuracy()}\n    # Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration,\n    # user can add output_transform to return other values\n    evaluator = create_supervised_evaluator(net, val_metrics, device, True, prepare_batch=prepare_batch)\n\n    # add stats event handler to print validation stats via evaluator\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n    )\n    val_stats_handler.attach(evaluator)\n\n    # for the array data format, assume the 3rd item of batch data is the meta_data\n    prediction_saver = ClassificationSaver(\n        output_dir=""tempdir"",\n        name=""evaluator"",\n        batch_transform=lambda batch: {""filename_or_obj"": batch[""img.filename_or_obj""]},\n        output_transform=lambda output: output[0].argmax(1),\n    )\n    prediction_saver.attach(evaluator)\n\n    # the model was trained by ""densenet_training_dict"" example\n    CheckpointLoader(load_path=""./runs/net_checkpoint_20.pth"", load_dict={""net"": net}).attach(evaluator)\n\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    state = evaluator.run(val_loader)\n    print(state)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/classification_3d_ignite/densenet_training_array.py,7,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.handlers import ModelCheckpoint, EarlyStopping\nfrom ignite.metrics import Accuracy\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.data import NiftiDataset\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, Resize, RandRotate90, ToTensor\nfrom monai.handlers import StatsHandler, TensorBoardStatsHandler, stopping_fn_from_metric\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI314-IOP-0889-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI249-Guys-1072-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI609-HH-2600-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI173-HH-1590-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI020-Guys-0700-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI342-Guys-0909-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI134-Guys-0780-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI577-HH-2661-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI066-Guys-0731-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI130-HH-1528-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n\n    # define transforms\n    train_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), RandRotate90(), ToTensor()])\n    val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), ToTensor()])\n\n    # define nifti dataset, data loader\n    check_ds = NiftiDataset(image_files=images, labels=labels, transform=train_transforms)\n    check_loader = DataLoader(check_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n    im, label = monai.utils.misc.first(check_loader)\n    print(type(im), im.shape, label)\n\n    # create DenseNet121, CrossEntropyLoss and Adam optimizer\n    net = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,)\n    loss = torch.nn.CrossEntropyLoss()\n    lr = 1e-5\n    opt = torch.optim.Adam(net.parameters(), lr)\n    device = torch.device(""cuda:0"")\n\n    # Ignite trainer expects batch=(img, label) and returns output=loss at every iteration,\n    # user can add output_transform to return other values, like: y_pred, y, etc.\n    trainer = create_supervised_trainer(net, opt, loss, device, False)\n\n    # adding checkpoint handler to save models (network params and optimizer stats) during training\n    checkpoint_handler = ModelCheckpoint(""./runs/"", ""net"", n_saved=10, require_empty=False)\n    trainer.add_event_handler(\n        event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={""net"": net, ""opt"": opt}\n    )\n\n    # StatsHandler prints loss at every iteration and print metrics at every epoch,\n    # we don\'t set metrics for trainer here, so just print loss, user can also customize print functions\n    # and can use output_transform to convert engine.state.output if it\'s not loss value\n    train_stats_handler = StatsHandler(name=""trainer"")\n    train_stats_handler.attach(trainer)\n\n    # TensorBoardStatsHandler plots loss at every iteration and plots metrics at every epoch, same as StatsHandler\n    train_tensorboard_stats_handler = TensorBoardStatsHandler()\n    train_tensorboard_stats_handler.attach(trainer)\n\n    # set parameters for validation\n    validation_every_n_epochs = 1\n\n    metric_name = ""Accuracy""\n    # add evaluation metric to the evaluator engine\n    val_metrics = {metric_name: Accuracy()}\n    # Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration,\n    # user can add output_transform to return other values\n    evaluator = create_supervised_evaluator(net, val_metrics, device, True)\n\n    # add stats event handler to print validation stats via evaluator\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )  # fetch global epoch number from trainer\n    val_stats_handler.attach(evaluator)\n\n    # add handler to record metrics to TensorBoard at every epoch\n    val_tensorboard_stats_handler = TensorBoardStatsHandler(\n        output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )  # fetch global epoch number from trainer\n    val_tensorboard_stats_handler.attach(evaluator)\n\n    # add early stopping handler to evaluator\n    early_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer)\n    evaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper)\n\n    # create a validation data loader\n    val_ds = NiftiDataset(image_files=images[-10:], labels=labels[-10:], transform=val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n\n    @trainer.on(Events.EPOCH_COMPLETED(every=validation_every_n_epochs))\n    def run_validation(engine):\n        evaluator.run(val_loader)\n\n    # create a training data loader\n    train_ds = NiftiDataset(image_files=images[:10], labels=labels[:10], transform=train_transforms)\n    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n\n    train_epochs = 30\n    state = trainer.run(train_loader, train_epochs)\n    print(state)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/classification_3d_ignite/densenet_training_dict.py,7,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport logging\nimport numpy as np\nimport torch\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator, _prepare_batch\nfrom ignite.handlers import ModelCheckpoint, EarlyStopping\nfrom ignite.metrics import Accuracy\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.transforms import Compose, LoadNiftid, AddChanneld, ScaleIntensityd, Resized, RandRotate90d, ToTensord\nfrom monai.handlers import StatsHandler, TensorBoardStatsHandler, stopping_fn_from_metric, ROCAUC\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n    images = [\n        ""/workspace/data/medical/ixi/IXI-T1/IXI314-IOP-0889-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI249-Guys-1072-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI609-HH-2600-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI173-HH-1590-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI020-Guys-0700-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI342-Guys-0909-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI134-Guys-0780-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI577-HH-2661-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI066-Guys-0731-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI130-HH-1528-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI607-Guys-1097-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI175-HH-1570-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI385-HH-2078-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI344-Guys-0905-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI409-Guys-0960-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI584-Guys-1129-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI253-HH-1694-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI092-HH-1436-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI574-IOP-1156-T1.nii.gz"",\n        ""/workspace/data/medical/ixi/IXI-T1/IXI585-Guys-1130-T1.nii.gz"",\n    ]\n    # 2 binary labels for gender classification: man and woman\n    labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n    train_files = [{""img"": img, ""label"": label} for img, label in zip(images[:10], labels[:10])]\n    val_files = [{""img"": img, ""label"": label} for img, label in zip(images[-10:], labels[-10:])]\n\n    # define transforms for image\n    train_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img""]),\n            AddChanneld(keys=[""img""]),\n            ScaleIntensityd(keys=[""img""]),\n            Resized(keys=[""img""], spatial_size=(96, 96, 96)),\n            RandRotate90d(keys=[""img""], prob=0.8, spatial_axes=[0, 2]),\n            ToTensord(keys=[""img""]),\n        ]\n    )\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img""]),\n            AddChanneld(keys=[""img""]),\n            ScaleIntensityd(keys=[""img""]),\n            Resized(keys=[""img""], spatial_size=(96, 96, 96)),\n            ToTensord(keys=[""img""]),\n        ]\n    )\n\n    # define dataset, data loader\n    check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n    check_data = monai.utils.misc.first(check_loader)\n    print(check_data[""img""].shape, check_data[""label""])\n\n    # create DenseNet121, CrossEntropyLoss and Adam optimizer\n    net = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=1, out_channels=2,)\n    loss = torch.nn.CrossEntropyLoss()\n    lr = 1e-5\n    opt = torch.optim.Adam(net.parameters(), lr)\n    device = torch.device(""cuda:0"")\n\n    # Ignite trainer expects batch=(img, label) and returns output=loss at every iteration,\n    # user can add output_transform to return other values, like: y_pred, y, etc.\n    def prepare_batch(batch, device=None, non_blocking=False):\n\n        return _prepare_batch((batch[""img""], batch[""label""]), device, non_blocking)\n\n    trainer = create_supervised_trainer(net, opt, loss, device, False, prepare_batch=prepare_batch)\n\n    # adding checkpoint handler to save models (network params and optimizer stats) during training\n    checkpoint_handler = ModelCheckpoint(""./runs/"", ""net"", n_saved=10, require_empty=False)\n    trainer.add_event_handler(\n        event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={""net"": net, ""opt"": opt}\n    )\n\n    # StatsHandler prints loss at every iteration and print metrics at every epoch,\n    # we don\'t set metrics for trainer here, so just print loss, user can also customize print functions\n    # and can use output_transform to convert engine.state.output if it\'s not loss value\n    train_stats_handler = StatsHandler(name=""trainer"")\n    train_stats_handler.attach(trainer)\n\n    # TensorBoardStatsHandler plots loss at every iteration and plots metrics at every epoch, same as StatsHandler\n    train_tensorboard_stats_handler = TensorBoardStatsHandler()\n    train_tensorboard_stats_handler.attach(trainer)\n\n    # set parameters for validation\n    validation_every_n_epochs = 1\n\n    metric_name = ""Accuracy""\n    # add evaluation metric to the evaluator engine\n    val_metrics = {metric_name: Accuracy(), ""AUC"": ROCAUC(to_onehot_y=True, softmax=True)}\n    # Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration,\n    # user can add output_transform to return other values\n    evaluator = create_supervised_evaluator(net, val_metrics, device, True, prepare_batch=prepare_batch)\n\n    # add stats event handler to print validation stats via evaluator\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )  # fetch global epoch number from trainer\n    val_stats_handler.attach(evaluator)\n\n    # add handler to record metrics to TensorBoard at every epoch\n    val_tensorboard_stats_handler = TensorBoardStatsHandler(\n        output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )  # fetch global epoch number from trainer\n    val_tensorboard_stats_handler.attach(evaluator)\n\n    # add early stopping handler to evaluator\n    early_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer)\n    evaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper)\n\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    @trainer.on(Events.EPOCH_COMPLETED(every=validation_every_n_epochs))\n    def run_validation(engine):\n        evaluator.run(val_loader)\n\n    # create a training data loader\n    train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    train_epochs = 30\n    state = trainer.run(train_loader, train_epochs)\n    print(state)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d/unet_evaluation_array.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom monai import config\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, ToTensor\nfrom monai.networks.nets import UNet\nfrom monai.data import create_test_image_3d, NiftiSaver, NiftiDataset\nfrom monai.inferers import sliding_window_inference\nfrom monai.metrics import compute_meandice\n\n\ndef main():\n    config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(5):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""im{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n\n    # define transforms for image and segmentation\n    imtrans = Compose([ScaleIntensity(), AddChannel(), ToTensor()])\n    segtrans = Compose([AddChannel(), ToTensor()])\n    val_ds = NiftiDataset(images, segs, transform=imtrans, seg_transform=segtrans, image_only=False)\n    # sliding window inference for one image at every iteration\n    val_loader = DataLoader(val_ds, batch_size=1, num_workers=1, pin_memory=torch.cuda.is_available())\n\n    device = torch.device(""cuda:0"")\n    model = UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n\n    model.load_state_dict(torch.load(""best_metric_model.pth""))\n    model.eval()\n    with torch.no_grad():\n        metric_sum = 0.0\n        metric_count = 0\n        saver = NiftiSaver(output_dir=""./output"")\n        for val_data in val_loader:\n            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n            # define sliding window size and batch size for windows inference\n            roi_size = (96, 96, 96)\n            sw_batch_size = 4\n            val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n            value = compute_meandice(\n                y_pred=val_outputs, y=val_labels, include_background=True, to_onehot_y=False, sigmoid=True\n            )\n            metric_count += len(value)\n            metric_sum += value.sum().item()\n            val_outputs = (val_outputs.sigmoid() >= 0.5).float()\n            saver.save_batch(val_outputs, val_data[2])\n        metric = metric_sum / metric_count\n        print(""evaluation metric:"", metric)\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d/unet_evaluation_dict.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.data import list_data_collate, create_test_image_3d, NiftiSaver\nfrom monai.inferers import sliding_window_inference\nfrom monai.metrics import compute_meandice\nfrom monai.networks.nets import UNet\nfrom monai.transforms import Compose, LoadNiftid, AsChannelFirstd, ScaleIntensityd, ToTensord\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(5):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""im{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n    val_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images, segs)]\n\n    # define transforms for image and segmentation\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    # sliding window inference need to input 1 image in every iteration\n    val_loader = DataLoader(\n        val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n\n    device = torch.device(""cuda:0"")\n    model = UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n\n    model.load_state_dict(torch.load(""best_metric_model.pth""))\n    model.eval()\n    with torch.no_grad():\n        metric_sum = 0.0\n        metric_count = 0\n        saver = NiftiSaver(output_dir=""./output"")\n        for val_data in val_loader:\n            val_images, val_labels = val_data[""img""].to(device), val_data[""seg""].to(device)\n            # define sliding window size and batch size for windows inference\n            roi_size = (96, 96, 96)\n            sw_batch_size = 4\n            val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n            value = compute_meandice(\n                y_pred=val_outputs, y=val_labels, include_background=True, to_onehot_y=False, sigmoid=True\n            )\n            metric_count += len(value)\n            metric_sum += value.sum().item()\n            val_outputs = (val_outputs.sigmoid() >= 0.5).float()\n            saver.save_batch(\n                val_outputs, {""filename_or_obj"": val_data[""img.filename_or_obj""], ""affine"": val_data[""img.affine""]}\n            )\n        metric = metric_sum / metric_count\n        print(""evaluation metric:"", metric)\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d/unet_training_array.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport monai\nfrom monai.data import NiftiDataset, create_test_image_3d\nfrom monai.inferers import sliding_window_inference\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, RandSpatialCrop, RandRotate90, ToTensor\nfrom monai.metrics import compute_meandice\nfrom monai.visualize.img2tensorboard import plot_2d_or_3d_image\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # create a temporary directory and 40 random image, mask paris\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(40):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""im{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n\n    # define transforms for image and segmentation\n    train_imtrans = Compose(\n        [\n            ScaleIntensity(),\n            AddChannel(),\n            RandSpatialCrop((96, 96, 96), random_size=False),\n            RandRotate90(prob=0.5, spatial_axes=(0, 2)),\n            ToTensor(),\n        ]\n    )\n    train_segtrans = Compose(\n        [\n            AddChannel(),\n            RandSpatialCrop((96, 96, 96), random_size=False),\n            RandRotate90(prob=0.5, spatial_axes=(0, 2)),\n            ToTensor(),\n        ]\n    )\n    val_imtrans = Compose([ScaleIntensity(), AddChannel(), ToTensor()])\n    val_segtrans = Compose([AddChannel(), ToTensor()])\n\n    # define nifti dataset, data loader\n    check_ds = NiftiDataset(images, segs, transform=train_imtrans, seg_transform=train_segtrans)\n    check_loader = DataLoader(check_ds, batch_size=10, num_workers=2, pin_memory=torch.cuda.is_available())\n    im, seg = monai.utils.misc.first(check_loader)\n    print(im.shape, seg.shape)\n\n    # create a training data loader\n    train_ds = NiftiDataset(images[:20], segs[:20], transform=train_imtrans, seg_transform=train_segtrans)\n    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=8, pin_memory=torch.cuda.is_available())\n    # create a validation data loader\n    val_ds = NiftiDataset(images[-20:], segs[-20:], transform=val_imtrans, seg_transform=val_segtrans)\n    val_loader = DataLoader(val_ds, batch_size=1, num_workers=4, pin_memory=torch.cuda.is_available())\n\n    # create UNet, DiceLoss and Adam optimizer\n    device = torch.device(""cuda:0"")\n    model = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n    loss_function = monai.losses.DiceLoss(sigmoid=True)\n    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n\n    # start a typical PyTorch training\n    val_interval = 2\n    best_metric = -1\n    best_metric_epoch = -1\n    epoch_loss_values = list()\n    metric_values = list()\n    writer = SummaryWriter()\n    for epoch in range(5):\n        print(""-"" * 10)\n        print(f""epoch {epoch + 1}/{5}"")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            epoch_len = len(train_ds) // train_loader.batch_size\n            print(f""{step}/{epoch_len}, train_loss: {loss.item():.4f}"")\n            writer.add_scalar(""train_loss"", loss.item(), epoch_len * epoch + step)\n        epoch_loss /= step\n        epoch_loss_values.append(epoch_loss)\n        print(f""epoch {epoch + 1} average loss: {epoch_loss:.4f}"")\n\n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                metric_sum = 0.0\n                metric_count = 0\n                val_images = None\n                val_labels = None\n                val_outputs = None\n                for val_data in val_loader:\n                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n                    roi_size = (96, 96, 96)\n                    sw_batch_size = 4\n                    val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n                    value = compute_meandice(\n                        y_pred=val_outputs, y=val_labels, include_background=True, to_onehot_y=False, sigmoid=True\n                    )\n                    metric_count += len(value)\n                    metric_sum += value.sum().item()\n                metric = metric_sum / metric_count\n                metric_values.append(metric)\n                if metric > best_metric:\n                    best_metric = metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), ""best_metric_model.pth"")\n                    print(""saved new best metric model"")\n                print(\n                    ""current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}"".format(\n                        epoch + 1, metric, best_metric, best_metric_epoch\n                    )\n                )\n                writer.add_scalar(""val_mean_dice"", metric, epoch + 1)\n                # plot the last model output as GIF image in TensorBoard with the corresponding image and label\n                plot_2d_or_3d_image(val_images, epoch + 1, writer, index=0, tag=""image"")\n                plot_2d_or_3d_image(val_labels, epoch + 1, writer, index=0, tag=""label"")\n                plot_2d_or_3d_image(val_outputs, epoch + 1, writer, index=0, tag=""output"")\n    shutil.rmtree(tempdir)\n    print(f""train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}"")\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d/unet_training_dict.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport monai\nfrom monai.transforms import (\n    Compose,\n    LoadNiftid,\n    AsChannelFirstd,\n    ScaleIntensityd,\n    RandCropByPosNegLabeld,\n    RandRotate90d,\n    ToTensord,\n)\nfrom monai.data import create_test_image_3d, list_data_collate\nfrom monai.inferers import sliding_window_inference\nfrom monai.metrics import compute_meandice\nfrom monai.visualize import plot_2d_or_3d_image\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # create a temporary directory and 40 random image, mask paris\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(40):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""img{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""img*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n    train_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images[:20], segs[:20])]\n    val_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images[-20:], segs[-20:])]\n\n    # define transforms for image and segmentation\n    train_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            RandCropByPosNegLabeld(\n                keys=[""img"", ""seg""], label_key=""seg"", size=[96, 96, 96], pos=1, neg=1, num_samples=4\n            ),\n            RandRotate90d(keys=[""img"", ""seg""], prob=0.5, spatial_axes=[0, 2]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n\n    # define dataset, data loader\n    check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    check_loader = DataLoader(\n        check_ds, batch_size=2, num_workers=4, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n    check_data = monai.utils.misc.first(check_loader)\n    print(check_data[""img""].shape, check_data[""seg""].shape)\n\n    # create a training data loader\n    train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=2,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=list_data_collate,\n        pin_memory=torch.cuda.is_available(),\n    )\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = DataLoader(\n        val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n\n    # create UNet, DiceLoss and Adam optimizer\n    device = torch.device(""cuda:0"")\n    model = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n    loss_function = monai.losses.DiceLoss(sigmoid=True)\n    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n\n    # start a typical PyTorch training\n    val_interval = 2\n    best_metric = -1\n    best_metric_epoch = -1\n    epoch_loss_values = list()\n    metric_values = list()\n    writer = SummaryWriter()\n    for epoch in range(5):\n        print(""-"" * 10)\n        print(f""epoch {epoch + 1}/{5}"")\n        model.train()\n        epoch_loss = 0\n        step = 0\n        for batch_data in train_loader:\n            step += 1\n            inputs, labels = batch_data[""img""].to(device), batch_data[""seg""].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            epoch_len = len(train_ds) // train_loader.batch_size\n            print(f""{step}/{epoch_len}, train_loss: {loss.item():.4f}"")\n            writer.add_scalar(""train_loss"", loss.item(), epoch_len * epoch + step)\n        epoch_loss /= step\n        epoch_loss_values.append(epoch_loss)\n        print(f""epoch {epoch + 1} average loss: {epoch_loss:.4f}"")\n\n        if (epoch + 1) % val_interval == 0:\n            model.eval()\n            with torch.no_grad():\n                metric_sum = 0.0\n                metric_count = 0\n                val_images = None\n                val_labels = None\n                val_outputs = None\n                for val_data in val_loader:\n                    val_images, val_labels = val_data[""img""].to(device), val_data[""seg""].to(device)\n                    roi_size = (96, 96, 96)\n                    sw_batch_size = 4\n                    val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n                    value = compute_meandice(\n                        y_pred=val_outputs, y=val_labels, include_background=True, to_onehot_y=False, sigmoid=True\n                    )\n                    metric_count += len(value)\n                    metric_sum += value.sum().item()\n                metric = metric_sum / metric_count\n                metric_values.append(metric)\n                if metric > best_metric:\n                    best_metric = metric\n                    best_metric_epoch = epoch + 1\n                    torch.save(model.state_dict(), ""best_metric_model.pth"")\n                    print(""saved new best metric model"")\n                print(\n                    ""current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}"".format(\n                        epoch + 1, metric, best_metric, best_metric_epoch\n                    )\n                )\n                writer.add_scalar(""val_mean_dice"", metric, epoch + 1)\n                # plot the last model output as GIF image in TensorBoard with the corresponding image and label\n                plot_2d_or_3d_image(val_images, epoch + 1, writer, index=0, tag=""image"")\n                plot_2d_or_3d_image(val_labels, epoch + 1, writer, index=0, tag=""label"")\n                plot_2d_or_3d_image(val_outputs, epoch + 1, writer, index=0, tag=""output"")\n    shutil.rmtree(tempdir)\n    print(f""train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}"")\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d_ignite/unet_evaluation_array.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.engine import Engine\nfrom torch.utils.data import DataLoader\n\nfrom monai import config\nfrom monai.handlers import CheckpointLoader, SegmentationSaver, StatsHandler, MeanDice\nfrom monai.data import NiftiDataset, create_test_image_3d\nfrom monai.inferers import sliding_window_inference\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, ToTensor\nfrom monai.networks.nets import UNet\nfrom monai.networks.utils import predict_segmentation\n\n\ndef main():\n    config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(5):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""im{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n\n    # define transforms for image and segmentation\n    imtrans = Compose([ScaleIntensity(), AddChannel(), ToTensor()])\n    segtrans = Compose([AddChannel(), ToTensor()])\n    ds = NiftiDataset(images, segs, transform=imtrans, seg_transform=segtrans, image_only=False)\n\n    device = torch.device(""cuda:0"")\n    net = UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    )\n    net.to(device)\n\n    # define sliding window size and batch size for windows inference\n    roi_size = (96, 96, 96)\n    sw_batch_size = 4\n\n    def _sliding_window_processor(engine, batch):\n        net.eval()\n        with torch.no_grad():\n            val_images, val_labels = batch[0].to(device), batch[1].to(device)\n            seg_probs = sliding_window_inference(val_images, roi_size, sw_batch_size, net)\n            return seg_probs, val_labels\n\n    evaluator = Engine(_sliding_window_processor)\n\n    # add evaluation metric to the evaluator engine\n    MeanDice(sigmoid=True, to_onehot_y=False).attach(evaluator, ""Mean_Dice"")\n\n    # StatsHandler prints loss at every iteration and print metrics at every epoch,\n    # we don\'t need to print loss for evaluator, so just print metrics, user can also customize print functions\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n    )\n    val_stats_handler.attach(evaluator)\n\n    # for the array data format, assume the 3rd item of batch data is the meta_data\n    file_saver = SegmentationSaver(\n        output_dir=""tempdir"",\n        output_ext="".nii.gz"",\n        output_postfix=""seg"",\n        name=""evaluator"",\n        batch_transform=lambda x: x[2],\n        output_transform=lambda output: predict_segmentation(output[0]),\n    )\n    file_saver.attach(evaluator)\n\n    # the model was trained by ""unet_training_array"" example\n    ckpt_saver = CheckpointLoader(load_path=""./runs/net_checkpoint_100.pth"", load_dict={""net"": net})\n    ckpt_saver.attach(evaluator)\n\n    # sliding window inference for one image at every iteration\n    loader = DataLoader(ds, batch_size=1, num_workers=1, pin_memory=torch.cuda.is_available())\n    state = evaluator.run(loader)\n    print(state)\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d_ignite/unet_evaluation_dict.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.engine import Engine\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.data import list_data_collate, create_test_image_3d\nfrom monai.inferers import sliding_window_inference\nfrom monai.networks.utils import predict_segmentation\nfrom monai.networks.nets import UNet\nfrom monai.transforms import Compose, LoadNiftid, AsChannelFirstd, ScaleIntensityd, ToTensord\nfrom monai.handlers import SegmentationSaver, CheckpointLoader, StatsHandler, MeanDice\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(5):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""im{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n    val_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images, segs)]\n\n    # define transforms for image and segmentation\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n\n    device = torch.device(""cuda:0"")\n    net = UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    )\n    net.to(device)\n\n    # define sliding window size and batch size for windows inference\n    roi_size = (96, 96, 96)\n    sw_batch_size = 4\n\n    def _sliding_window_processor(engine, batch):\n        net.eval()\n        with torch.no_grad():\n            val_images, val_labels = batch[""img""].to(device), batch[""seg""].to(device)\n            seg_probs = sliding_window_inference(val_images, roi_size, sw_batch_size, net)\n            return seg_probs, val_labels\n\n    evaluator = Engine(_sliding_window_processor)\n\n    # add evaluation metric to the evaluator engine\n    MeanDice(sigmoid=True, to_onehot_y=False).attach(evaluator, ""Mean_Dice"")\n\n    # StatsHandler prints loss at every iteration and print metrics at every epoch,\n    # we don\'t need to print loss for evaluator, so just print metrics, user can also customize print functions\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n    )\n    val_stats_handler.attach(evaluator)\n\n    # convert the necessary metadata from batch data\n    SegmentationSaver(\n        output_dir=""tempdir"",\n        output_ext="".nii.gz"",\n        output_postfix=""seg"",\n        name=""evaluator"",\n        batch_transform=lambda batch: {""filename_or_obj"": batch[""img.filename_or_obj""], ""affine"": batch[""img.affine""]},\n        output_transform=lambda output: predict_segmentation(output[0]),\n    ).attach(evaluator)\n    # the model was trained by ""unet_training_dict"" example\n    CheckpointLoader(load_path=""./runs/net_checkpoint_50.pth"", load_dict={""net"": net}).attach(evaluator)\n\n    # sliding window inference for one image at every iteration\n    val_loader = DataLoader(\n        val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n    state = evaluator.run(val_loader)\n    print(state)\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d_ignite/unet_training_array.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.handlers import ModelCheckpoint, EarlyStopping\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.data import NiftiDataset, create_test_image_3d\nfrom monai.transforms import Compose, AddChannel, ScaleIntensity, RandSpatialCrop, Resize, ToTensor\nfrom monai.handlers import (\n    StatsHandler,\n    TensorBoardStatsHandler,\n    TensorBoardImageHandler,\n    MeanDice,\n    stopping_fn_from_metric,\n)\nfrom monai.networks.utils import predict_segmentation\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # create a temporary directory and 40 random image, mask paris\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(40):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""im{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n\n    # define transforms for image and segmentation\n    train_imtrans = Compose(\n        [ScaleIntensity(), AddChannel(), RandSpatialCrop((96, 96, 96), random_size=False), ToTensor()]\n    )\n    train_segtrans = Compose([AddChannel(), RandSpatialCrop((96, 96, 96), random_size=False), ToTensor()])\n    val_imtrans = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), ToTensor()])\n    val_segtrans = Compose([AddChannel(), Resize((96, 96, 96)), ToTensor()])\n\n    # define nifti dataset, data loader\n    check_ds = NiftiDataset(images, segs, transform=train_imtrans, seg_transform=train_segtrans)\n    check_loader = DataLoader(check_ds, batch_size=10, num_workers=2, pin_memory=torch.cuda.is_available())\n    im, seg = monai.utils.misc.first(check_loader)\n    print(im.shape, seg.shape)\n\n    # create a training data loader\n    train_ds = NiftiDataset(images[:20], segs[:20], transform=train_imtrans, seg_transform=train_segtrans)\n    train_loader = DataLoader(train_ds, batch_size=5, shuffle=True, num_workers=8, pin_memory=torch.cuda.is_available())\n    # create a validation data loader\n    val_ds = NiftiDataset(images[-20:], segs[-20:], transform=val_imtrans, seg_transform=val_segtrans)\n    val_loader = DataLoader(val_ds, batch_size=5, num_workers=8, pin_memory=torch.cuda.is_available())\n\n    # create UNet, DiceLoss and Adam optimizer\n    net = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    )\n    loss = monai.losses.DiceLoss(sigmoid=True)\n    lr = 1e-3\n    opt = torch.optim.Adam(net.parameters(), lr)\n    device = torch.device(""cuda:0"")\n\n    # Ignite trainer expects batch=(img, seg) and returns output=loss at every iteration,\n    # user can add output_transform to return other values, like: y_pred, y, etc.\n    trainer = create_supervised_trainer(net, opt, loss, device, False)\n\n    # adding checkpoint handler to save models (network params and optimizer stats) during training\n    checkpoint_handler = ModelCheckpoint(""./runs/"", ""net"", n_saved=10, require_empty=False)\n    trainer.add_event_handler(\n        event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={""net"": net, ""opt"": opt}\n    )\n\n    # StatsHandler prints loss at every iteration and print metrics at every epoch,\n    # we don\'t set metrics for trainer here, so just print loss, user can also customize print functions\n    # and can use output_transform to convert engine.state.output if it\'s not a loss value\n    train_stats_handler = StatsHandler(name=""trainer"")\n    train_stats_handler.attach(trainer)\n\n    # TensorBoardStatsHandler plots loss at every iteration and plots metrics at every epoch, same as StatsHandler\n    train_tensorboard_stats_handler = TensorBoardStatsHandler()\n    train_tensorboard_stats_handler.attach(trainer)\n\n    validation_every_n_epochs = 1\n    # Set parameters for validation\n    metric_name = ""Mean_Dice""\n    # add evaluation metric to the evaluator engine\n    val_metrics = {metric_name: MeanDice(sigmoid=True, to_onehot_y=False)}\n\n    # Ignite evaluator expects batch=(img, seg) and returns output=(y_pred, y) at every iteration,\n    # user can add output_transform to return other values\n    evaluator = create_supervised_evaluator(net, val_metrics, device, True)\n\n    @trainer.on(Events.EPOCH_COMPLETED(every=validation_every_n_epochs))\n    def run_validation(engine):\n        evaluator.run(val_loader)\n\n    # add early stopping handler to evaluator\n    early_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer)\n    evaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper)\n\n    # add stats event handler to print validation stats via evaluator\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )  # fetch global epoch number from trainer\n    val_stats_handler.attach(evaluator)\n\n    # add handler to record metrics to TensorBoard at every validation epoch\n    val_tensorboard_stats_handler = TensorBoardStatsHandler(\n        output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )  # fetch global epoch number from trainer\n    val_tensorboard_stats_handler.attach(evaluator)\n\n    # add handler to draw the first image and the corresponding label and model output in the last batch\n    # here we draw the 3D output as GIF format along Depth axis, at every validation epoch\n    val_tensorboard_image_handler = TensorBoardImageHandler(\n        batch_transform=lambda batch: (batch[0], batch[1]),\n        output_transform=lambda output: predict_segmentation(output[0]),\n        global_iter_transform=lambda x: trainer.state.epoch,\n    )\n    evaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=val_tensorboard_image_handler)\n\n    train_epochs = 30\n    state = trainer.run(train_loader, train_epochs)\n    print(state)\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/segmentation_3d_ignite/unet_training_dict.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator, _prepare_batch\nfrom ignite.handlers import ModelCheckpoint, EarlyStopping\nfrom torch.utils.data import DataLoader\n\nimport monai\nfrom monai.transforms import (\n    Compose,\n    LoadNiftid,\n    AsChannelFirstd,\n    ScaleIntensityd,\n    RandCropByPosNegLabeld,\n    RandRotate90d,\n    ToTensord,\n)\nfrom monai.handlers import (\n    StatsHandler,\n    TensorBoardStatsHandler,\n    TensorBoardImageHandler,\n    MeanDice,\n    stopping_fn_from_metric,\n)\nfrom monai.data import create_test_image_3d, list_data_collate\nfrom monai.networks.utils import predict_segmentation\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # create a temporary directory and 40 random image, mask paris\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(40):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""img{i:d}.nii.gz""))\n\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""img*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n    train_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images[:20], segs[:20])]\n    val_files = [{""img"": img, ""seg"": seg} for img, seg in zip(images[-20:], segs[-20:])]\n\n    # define transforms for image and segmentation\n    train_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            RandCropByPosNegLabeld(\n                keys=[""img"", ""seg""], label_key=""seg"", size=[96, 96, 96], pos=1, neg=1, num_samples=4\n            ),\n            RandRotate90d(keys=[""img"", ""seg""], prob=0.5, spatial_axes=[0, 2]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""img"", ""seg""]),\n            AsChannelFirstd(keys=[""img"", ""seg""], channel_dim=-1),\n            ScaleIntensityd(keys=[""img"", ""seg""]),\n            ToTensord(keys=[""img"", ""seg""]),\n        ]\n    )\n\n    # define dataset, data loader\n    check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    check_loader = DataLoader(\n        check_ds, batch_size=2, num_workers=4, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n    check_data = monai.utils.misc.first(check_loader)\n    print(check_data[""img""].shape, check_data[""seg""].shape)\n\n    # create a training data loader\n    train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=2,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=list_data_collate,\n        pin_memory=torch.cuda.is_available(),\n    )\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = DataLoader(\n        val_ds, batch_size=5, num_workers=8, collate_fn=list_data_collate, pin_memory=torch.cuda.is_available()\n    )\n\n    # create UNet, DiceLoss and Adam optimizer\n    net = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    )\n    loss = monai.losses.DiceLoss(sigmoid=True)\n    lr = 1e-3\n    opt = torch.optim.Adam(net.parameters(), lr)\n    device = torch.device(""cuda:0"")\n\n    # Ignite trainer expects batch=(img, seg) and returns output=loss at every iteration,\n    # user can add output_transform to return other values, like: y_pred, y, etc.\n    def prepare_batch(batch, device=None, non_blocking=False):\n        return _prepare_batch((batch[""img""], batch[""seg""]), device, non_blocking)\n\n    trainer = create_supervised_trainer(net, opt, loss, device, False, prepare_batch=prepare_batch)\n\n    # adding checkpoint handler to save models (network params and optimizer stats) during training\n    checkpoint_handler = ModelCheckpoint(""./runs/"", ""net"", n_saved=10, require_empty=False)\n    trainer.add_event_handler(\n        event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={""net"": net, ""opt"": opt}\n    )\n\n    # StatsHandler prints loss at every iteration and print metrics at every epoch,\n    # we don\'t set metrics for trainer here, so just print loss, user can also customize print functions\n    # and can use output_transform to convert engine.state.output if it\'s not loss value\n    train_stats_handler = StatsHandler(name=""trainer"")\n    train_stats_handler.attach(trainer)\n\n    # TensorBoardStatsHandler plots loss at every iteration and plots metrics at every epoch, same as StatsHandler\n    train_tensorboard_stats_handler = TensorBoardStatsHandler()\n    train_tensorboard_stats_handler.attach(trainer)\n\n    validation_every_n_iters = 5\n    # set parameters for validation\n    metric_name = ""Mean_Dice""\n    # add evaluation metric to the evaluator engine\n    val_metrics = {metric_name: MeanDice(sigmoid=True, to_onehot_y=False)}\n\n    # Ignite evaluator expects batch=(img, seg) and returns output=(y_pred, y) at every iteration,\n    # user can add output_transform to return other values\n    evaluator = create_supervised_evaluator(net, val_metrics, device, True, prepare_batch=prepare_batch)\n\n    @trainer.on(Events.ITERATION_COMPLETED(every=validation_every_n_iters))\n    def run_validation(engine):\n        evaluator.run(val_loader)\n\n    # add early stopping handler to evaluator\n    early_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer)\n    evaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper)\n\n    # add stats event handler to print validation stats via evaluator\n    val_stats_handler = StatsHandler(\n        name=""evaluator"",\n        output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )  # fetch global epoch number from trainer\n    val_stats_handler.attach(evaluator)\n\n    # add handler to record metrics to TensorBoard at every validation epoch\n    val_tensorboard_stats_handler = TensorBoardStatsHandler(\n        output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output\n        global_epoch_transform=lambda x: trainer.state.iteration,\n    )  # fetch global iteration number from trainer\n    val_tensorboard_stats_handler.attach(evaluator)\n\n    # add handler to draw the first image and the corresponding label and model output in the last batch\n    # here we draw the 3D output as GIF format along the depth axis, every 2 validation iterations.\n    val_tensorboard_image_handler = TensorBoardImageHandler(\n        batch_transform=lambda batch: (batch[""img""], batch[""seg""]),\n        output_transform=lambda output: predict_segmentation(output[0]),\n        global_iter_transform=lambda x: trainer.state.epoch,\n    )\n    evaluator.add_event_handler(event_name=Events.ITERATION_COMPLETED(every=2), handler=val_tensorboard_image_handler)\n\n    train_epochs = 5\n    state = trainer.run(train_loader, train_epochs)\n    print(state)\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/workflows/unet_evaluation_dict.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.metrics import Accuracy\n\nimport monai\nfrom monai.transforms import (\n    Compose,\n    LoadNiftid,\n    AsChannelFirstd,\n    ScaleIntensityd,\n    ToTensord,\n    Activationsd,\n    AsDiscreted,\n    KeepLargestConnectedComponentd,\n)\nfrom monai.handlers import StatsHandler, CheckpointLoader, SegmentationSaver, MeanDice\nfrom monai.data import create_test_image_3d\nfrom monai.engines import SupervisedEvaluator\nfrom monai.inferers import SlidingWindowInferer\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # create a temporary directory and 40 random image, mask paris\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(5):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""im{i:d}.nii.gz""))\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""im*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n    val_files = [{""image"": img, ""label"": seg} for img, seg in zip(images, segs)]\n\n    # define transforms for image and segmentation\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""image"", ""label""]),\n            AsChannelFirstd(keys=[""image"", ""label""], channel_dim=-1),\n            ScaleIntensityd(keys=[""image"", ""label""]),\n            ToTensord(keys=[""image"", ""label""]),\n        ]\n    )\n\n    # create a validation data loader\n    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n    val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=4)\n\n    # create UNet, DiceLoss and Adam optimizer\n    device = torch.device(""cuda:0"")\n    net = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n\n    val_post_transforms = Compose(\n        [\n            Activationsd(keys=""pred"", output_postfix=""act"", sigmoid=True),\n            AsDiscreted(keys=""pred_act"", output_postfix=""dis"", threshold_values=True),\n            KeepLargestConnectedComponentd(keys=""pred_act_dis"", applied_values=[1], output_postfix=None),\n        ]\n    )\n    val_handlers = [\n        StatsHandler(output_transform=lambda x: None),\n        CheckpointLoader(load_path=""./runs/net_key_metric=0.9101.pth"", load_dict={""net"": net}),\n        SegmentationSaver(\n            output_dir=""./runs/"",\n            batch_transform=lambda x: {\n                ""filename_or_obj"": x[""image.filename_or_obj""],\n                ""affine"": x[""image.affine""],\n                ""original_affine"": x[""image.original_affine""],\n                ""spatial_shape"": x[""image.spatial_shape""],\n            },\n            output_transform=lambda x: x[""pred_act_dis""],\n        ),\n    ]\n\n    evaluator = SupervisedEvaluator(\n        device=device,\n        val_data_loader=val_loader,\n        network=net,\n        inferer=SlidingWindowInferer(roi_size=(96, 96, 96), sw_batch_size=4, overlap=0.5),\n        post_transform=val_post_transforms,\n        key_val_metric={\n            ""val_mean_dice"": MeanDice(\n                include_background=True, output_transform=lambda x: (x[""pred_act_dis""], x[""label""])\n            )\n        },\n        additional_metrics={""val_acc"": Accuracy(output_transform=lambda x: (x[""pred_act_dis""], x[""label""]))},\n        val_handlers=val_handlers,\n    )\n    evaluator.run()\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/workflows/unet_training_dict.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tempfile\nimport shutil\nfrom glob import glob\nimport logging\nimport nibabel as nib\nimport numpy as np\nimport torch\nfrom ignite.metrics import Accuracy\n\nimport monai\nfrom monai.transforms import (\n    Compose,\n    LoadNiftid,\n    AsChannelFirstd,\n    ScaleIntensityd,\n    RandCropByPosNegLabeld,\n    RandRotate90d,\n    ToTensord,\n    Activationsd,\n    AsDiscreted,\n    KeepLargestConnectedComponentd,\n)\nfrom monai.handlers import (\n    StatsHandler,\n    TensorBoardStatsHandler,\n    TensorBoardImageHandler,\n    ValidationHandler,\n    LrScheduleHandler,\n    CheckpointSaver,\n    MeanDice,\n)\nfrom monai.data import create_test_image_3d\nfrom monai.engines import SupervisedTrainer, SupervisedEvaluator\nfrom monai.inferers import SimpleInferer, SlidingWindowInferer\n\n\ndef main():\n    monai.config.print_config()\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    # create a temporary directory and 40 random image, mask paris\n    tempdir = tempfile.mkdtemp()\n    print(f""generating synthetic data to {tempdir} (this may take a while)"")\n    for i in range(40):\n        im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n        n = nib.Nifti1Image(im, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""img{i:d}.nii.gz""))\n        n = nib.Nifti1Image(seg, np.eye(4))\n        nib.save(n, os.path.join(tempdir, f""seg{i:d}.nii.gz""))\n\n    images = sorted(glob(os.path.join(tempdir, ""img*.nii.gz"")))\n    segs = sorted(glob(os.path.join(tempdir, ""seg*.nii.gz"")))\n    train_files = [{""image"": img, ""label"": seg} for img, seg in zip(images[:20], segs[:20])]\n    val_files = [{""image"": img, ""label"": seg} for img, seg in zip(images[-20:], segs[-20:])]\n\n    # define transforms for image and segmentation\n    train_transforms = Compose(\n        [\n            LoadNiftid(keys=[""image"", ""label""]),\n            AsChannelFirstd(keys=[""image"", ""label""], channel_dim=-1),\n            ScaleIntensityd(keys=[""image"", ""label""]),\n            RandCropByPosNegLabeld(\n                keys=[""image"", ""label""], label_key=""label"", size=[96, 96, 96], pos=1, neg=1, num_samples=4\n            ),\n            RandRotate90d(keys=[""image"", ""label""], prob=0.5, spatial_axes=[0, 2]),\n            ToTensord(keys=[""image"", ""label""]),\n        ]\n    )\n    val_transforms = Compose(\n        [\n            LoadNiftid(keys=[""image"", ""label""]),\n            AsChannelFirstd(keys=[""image"", ""label""], channel_dim=-1),\n            ScaleIntensityd(keys=[""image"", ""label""]),\n            ToTensord(keys=[""image"", ""label""]),\n        ]\n    )\n\n    # create a training data loader\n    train_ds = monai.data.CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.5)\n    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n    train_loader = monai.data.DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n    # create a validation data loader\n    val_ds = monai.data.CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0)\n    val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=4)\n\n    # create UNet, DiceLoss and Adam optimizer\n    device = torch.device(""cuda:0"")\n    net = monai.networks.nets.UNet(\n        dimensions=3,\n        in_channels=1,\n        out_channels=1,\n        channels=(16, 32, 64, 128, 256),\n        strides=(2, 2, 2, 2),\n        num_res_units=2,\n    ).to(device)\n    loss = monai.losses.DiceLoss(sigmoid=True)\n    opt = torch.optim.Adam(net.parameters(), 1e-3)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=2, gamma=0.1)\n\n    val_post_transforms = Compose(\n        [\n            Activationsd(keys=""pred"", output_postfix=""act"", sigmoid=True),\n            AsDiscreted(keys=""pred_act"", output_postfix=""dis"", threshold_values=True),\n            KeepLargestConnectedComponentd(keys=""pred_act_dis"", applied_values=[1], output_postfix=None),\n        ]\n    )\n    val_handlers = [\n        StatsHandler(output_transform=lambda x: None),\n        TensorBoardStatsHandler(log_dir=""./runs/"", output_transform=lambda x: None),\n        TensorBoardImageHandler(\n            log_dir=""./runs/"",\n            batch_transform=lambda x: (x[""image""], x[""label""]),\n            output_transform=lambda x: x[""pred_act_dis""],\n        ),\n        CheckpointSaver(save_dir=""./runs/"", save_dict={""net"": net}, save_key_metric=True),\n    ]\n\n    evaluator = SupervisedEvaluator(\n        device=device,\n        val_data_loader=val_loader,\n        network=net,\n        inferer=SlidingWindowInferer(roi_size=(96, 96, 96), sw_batch_size=4, overlap=0.5),\n        post_transform=val_post_transforms,\n        key_val_metric={\n            ""val_mean_dice"": MeanDice(\n                include_background=True, output_transform=lambda x: (x[""pred_act_dis""], x[""label""])\n            )\n        },\n        additional_metrics={""val_acc"": Accuracy(output_transform=lambda x: (x[""pred_act_dis""], x[""label""]))},\n        val_handlers=val_handlers,\n    )\n\n    train_post_transforms = Compose(\n        [\n            Activationsd(keys=""pred"", output_postfix=""act"", sigmoid=True),\n            AsDiscreted(keys=""pred_act"", output_postfix=""dis"", threshold_values=True),\n            KeepLargestConnectedComponentd(keys=""pred_act_dis"", applied_values=[1], output_postfix=None),\n        ]\n    )\n    train_handlers = [\n        LrScheduleHandler(lr_scheduler=lr_scheduler, print_lr=True),\n        ValidationHandler(validator=evaluator, interval=2, epoch_level=True),\n        StatsHandler(tag_name=""train_loss"", output_transform=lambda x: x[""loss""]),\n        TensorBoardStatsHandler(log_dir=""./runs/"", tag_name=""train_loss"", output_transform=lambda x: x[""loss""]),\n        CheckpointSaver(save_dir=""./runs/"", save_dict={""net"": net, ""opt"": opt}, save_interval=2, epoch_level=True),\n    ]\n\n    trainer = SupervisedTrainer(\n        device=device,\n        max_epochs=5,\n        train_data_loader=train_loader,\n        network=net,\n        optimizer=opt,\n        loss_function=loss,\n        inferer=SimpleInferer(),\n        amp=False,\n        post_transform=train_post_transforms,\n        key_train_metric={""train_acc"": Accuracy(output_transform=lambda x: (x[""pred_act_dis""], x[""label""]))},\n        train_handlers=train_handlers,\n    )\n    trainer.run()\n\n    shutil.rmtree(tempdir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
monai/application/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/config/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .deviceconfig import *\n'"
monai/config/deviceconfig.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\n\nimport monai\n\ntry:\n    import ignite\n\n    ignite_version = ignite.__version__\nexcept ImportError:\n    ignite_version = ""NOT INSTALLED""\n\n\ndef get_config_values():\n    """"""\n    Read the package versions into a dictionary.\n    """"""\n    output = OrderedDict()\n\n    output[""MONAI version""] = monai.__version__\n    output[""Python version""] = sys.version.replace(""\\n"", "" "")\n    output[""Numpy version""] = np.version.full_version\n    output[""Pytorch version""] = torch.__version__\n    output[""Ignite version""] = ignite_version\n\n    return output\n\n\ndef print_config(file=sys.stdout):\n    """"""\n    Print the package versions to `file`.\n    Defaults to `sys.stdout`.\n    """"""\n    for k, v in get_config_values().items():\n        print(f""{k}: {v}"", file=file, flush=True)\n\n\ndef set_visible_devices(*dev_inds):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join(map(str, dev_inds))\n'"
monai/data/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .csv_saver import CSVSaver\nfrom .dataset import Dataset, PersistentDataset, CacheDataset, ZipDataset, ArrayDataset\nfrom .grid_dataset import GridPatchDataset\nfrom .nifti_reader import NiftiDataset\nfrom .nifti_saver import NiftiSaver\nfrom .nifti_writer import write_nifti\nfrom .synthetic import *\nfrom .utils import *\nfrom .png_saver import PNGSaver\nfrom .png_writer import write_png\nfrom .decathalon_datalist import load_decathalon_datalist\nfrom .dataloader import *\n'"
monai/data/csv_saver.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import OrderedDict\nfrom typing import Union\n\nimport os\nimport csv\nimport numpy as np\nimport torch\n\n\nclass CSVSaver:\n    """"""\n    Save the data in a dictionary format cache, and write to a CSV file finally.\n    Typically, the data can be classification predictions, call `save` for single data\n    or call `save_batch` to save a batch of data together, and call `finalize` to write\n    the cached data into CSV file. If no meta data provided, use index from 0 to save data.\n    """"""\n\n    def __init__(self, output_dir: str = ""./"", filename: str = ""predictions.csv"", overwrite: bool = True):\n        """"""\n        Args:\n            output_dir (str): output CSV file directory.\n            filename (str): name of the saved CSV file name.\n            overwrite (bool): whether to overwriting existing CSV file content. If we are not overwriting,\n                then we check if the results have been previously saved, and load them to the prediction_dict.\n\n        """"""\n        self.output_dir = output_dir\n        self._cache_dict: OrderedDict = OrderedDict()\n        assert isinstance(filename, str) and filename[-4:] == "".csv"", ""filename must be a string with CSV format.""\n        self._filepath = os.path.join(output_dir, filename)\n        self.overwrite = overwrite\n        self._data_index = 0\n\n    def finalize(self):\n        """"""\n        Writes the cached dict to a csv\n\n        """"""\n        if not self.overwrite and os.path.exists(self._filepath):\n            with open(self._filepath, ""r"") as f:\n                reader = csv.reader(f)\n                for row in reader:\n                    self._cache_dict[row[0]] = np.array(row[1:]).astype(np.float32)\n\n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n        with open(self._filepath, ""w"") as f:\n            for k, v in self._cache_dict.items():\n                f.write(k)\n                for result in v.flatten():\n                    f.write("","" + str(result))\n                f.write(""\\n"")\n\n    def save(self, data: np.ndarray, meta_data=None):\n        """"""Save data into the cache dictionary. The metadata should have the following key:\n            - ``\'filename_or_obj\'`` -- save the data corresponding to file name or object.\n        If meta_data is None, use the default index from 0 to save data instead.\n\n        args:\n            data (Tensor or ndarray): target data content that save into cache.\n            meta_data (dict): the meta data information corresponding to the data.\n\n        """"""\n        save_key = meta_data[""filename_or_obj""] if meta_data else str(self._data_index)\n        self._data_index += 1\n        if torch.is_tensor(data):\n            data = data.detach().cpu().numpy()\n        self._cache_dict[save_key] = data.astype(np.float32)\n\n    def save_batch(self, batch_data: Union[torch.Tensor, np.ndarray], meta_data=None):\n        """"""Save a batch of data into the cache dictionary.\n\n        args:\n            batch_data (Tensor or ndarray): target batch data content that save into cache.\n            meta_data (dict): every key-value in the meta_data is corresponding to 1 batch of data.\n\n        """"""\n        for i, data in enumerate(batch_data):  # save a batch of files\n            self.save(data, {k: meta_data[k][i] for k in meta_data} if meta_data else None)\n'"
monai/data/dataloader.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport torch\nfrom monai.data import list_data_collate, worker_init_fn\n\n__all__ = [""DataLoader""]\n\n\nclass DataLoader(torch.utils.data.DataLoader):\n    """"""Generates images/labels for train/validation/testing from dataset.\n    It inherits from PyTorch DataLoader and adds callbacks for `collate` and `worker_fn`.\n\n    Args:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size: how many samples per batch to load\n            (default: ``1``).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: ``False``).\n        sampler (Sampler, optional): defines the strategy to draw samples from\n            the dataset. If specified, :attr:`shuffle` must be ``False``.\n        batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n            indices at a time. Mutually exclusive with :attr:`batch_size`,\n            :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n        num_workers: how many subprocesses to use for data\n            loading. ``0`` means that the data will be loaded in the main process.\n            (default: ``0``)\n        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n            into CUDA pinned memory before returning them.  If your data elements\n            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n            see the example below.\n        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n            if the dataset size is not divisible by the batch size. If ``False`` and\n            the size of dataset is not divisible by the batch size, then the last batch\n            will be smaller. (default: ``False``)\n        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative. (default: ``0``)\n        multiprocessing_context (callable, optional): specify a valid start method for multi-processing.\n\n    """"""\n\n    def __init__(\n        self,\n        dataset,\n        batch_size: Optional[int] = 1,\n        shuffle=False,\n        sampler=None,\n        batch_sampler=None,\n        num_workers: Optional[int] = 0,\n        pin_memory=False,\n        drop_last=False,\n        timeout=0,\n        multiprocessing_context=None,\n    ):\n        super().__init__(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            sampler=sampler,\n            batch_sampler=batch_sampler,\n            num_workers=num_workers,\n            collate_fn=list_data_collate,\n            pin_memory=pin_memory,\n            drop_last=drop_last,\n            timeout=timeout,\n            worker_init_fn=worker_init_fn,\n            multiprocessing_context=multiprocessing_context,\n        )\n'"
monai/data/dataset.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hashlib\nimport json\nimport sys\nimport threading\nfrom multiprocessing.pool import ThreadPool\nfrom pathlib import Path\nfrom typing import Callable, Optional\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset as _TorchDataset\n\nfrom monai.transforms import Compose, Randomizable, Transform\nfrom monai.transforms.utils import apply_transform\nfrom monai.utils import get_seed, process_bar\n\n\nclass Dataset(_TorchDataset):\n    """"""\n    A generic dataset with a length property and an optional callable data transform\n    when fetching a data sample.\n    For example, typical input data can be a list of dictionaries::\n\n        [{                            {                            {\n             \'img\': \'image1.nii.gz\',      \'img\': \'image2.nii.gz\',      \'img\': \'image3.nii.gz\',\n             \'seg\': \'label1.nii.gz\',      \'seg\': \'label2.nii.gz\',      \'seg\': \'label3.nii.gz\',\n             \'extra\': 123                 \'extra\': 456                 \'extra\': 789\n         },                           },                           }]\n    """"""\n\n    def __init__(self, data, transform: Optional[Callable] = None):\n        """"""\n        Args:\n            data (Iterable): input data to load and transform to generate dataset for model.\n            transform (Callable, optional): a callable data transform on input data.\n        """"""\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index: int):\n        data = self.data[index]\n        if self.transform is not None:\n            data = apply_transform(self.transform, data)\n\n        return data\n\n\nclass PersistentDataset(Dataset):\n    """"""\n    Persistent storage of pre-computed values to efficiently manage larger than memory dictionary format data,\n    it can operate transforms for specific fields.  Results from the non-random transform components are computed\n    when first used, and stored in the `cache_dir` for rapid retrieval on subsequent uses.\n\n    For example, typical input data can be a list of dictionaries::\n\n        [{                            {                            {\n             \'img\': \'image1.nii.gz\',      \'img\': \'image2.nii.gz\',      \'img\': \'image3.nii.gz\',\n             \'seg\': \'label1.nii.gz\',      \'seg\': \'label2.nii.gz\',      \'seg\': \'label3.nii.gz\',\n             \'extra\': 123                 \'extra\': 456                 \'extra\': 789\n         },                           },                           }]\n\n    For a composite transform like\n\n        .. code-block:: python\n\n        [ LoadNiftid(keys=[\'image\', \'label\']),\n          Orientationd(keys=[\'image\', \'label\'], axcodes=\'RAS\'),\n          ScaleIntensityRanged(keys=[\'image\'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n          RandCropByPosNegLabeld(keys=[\'image\', \'label\'], label_key=\'label\', size=(96, 96, 96),\n                                 pos=1, neg=1, num_samples=4, image_key=\'image\', image_threshold=0),\n          ToTensord(keys=[\'image\', \'label\'])]\n\n    Upon first use a filename based dataset will be processed by the transform for the\n    [LoadNiftid, Orientationd, ScaleIntensityRanged] and the resulting tensor written to\n    the `cache_dir` before applying the remaining random dependant transforms\n    [RandCropByPosNegLabeld, ToTensord] elements for use in the analysis.\n\n    Subsequent uses of a dataset directly read pre-processed results from `cache_dir`\n    followed by applying the random dependant parts of transform processing.\n    """"""\n\n    def __init__(self, data, transform: Optional[Callable] = None, cache_dir=None):\n        """"""\n        Args:\n            data (Iterable): input data to load and transform to generate dataset for model.\n            transform (Callable, optional): transforms to execute operations on input data.\n            cache_dir (Path or str or None): If specified, this is the location for persistent storage\n                of pre-computed transformed data tensors. The cache_dir is computed once, and\n                persists on disk until explicitly removed.  Different runs, programs, experiments\n                may share a common cache dir provided that the transforms pre-processing is\n                consistent.\n        """"""\n        if not isinstance(transform, Compose):\n            transform = Compose(transform)\n        super().__init__(data=data, transform=transform)\n        self.cache_dir = Path(cache_dir) if cache_dir is not None else None\n\n    def _pre_first_random_transform(self, item_transformed):\n        """"""\n        Process the data from original state up to the first random element.\n\n        Args:\n            item_transformed: The data to be transformed\n        Returns:\n            the transformed element up to the first identified\n            random transform object\n        """"""\n        for _transform in self.transform.transforms:  # pytype: disable=attribute-error\n            # execute all the deterministic transforms\n            if isinstance(_transform, Randomizable) or not isinstance(_transform, Transform):\n                break\n            item_transformed = apply_transform(_transform, item_transformed)\n        return item_transformed\n\n    def _first_random_and_beyond_transform(self, item_transformed):\n        """"""\n        Process the data from before the first random transform to the final state ready for evaluation.\n        Args:\n            item_transformed: The data to be transformed (already processed up to the first random transform)\n        Returns:\n            the transformed element through the random transforms\n        """"""\n        start_post_randomize_run = False\n        for _transform in self.transform.transforms:  # pytype: disable=attribute-error\n            if (\n                start_post_randomize_run\n                or isinstance(_transform, Randomizable)\n                or not isinstance(_transform, Transform)\n            ):\n                start_post_randomize_run = True\n                item_transformed = apply_transform(_transform, item_transformed)\n        return item_transformed\n\n    def _pre_first_random_cachecheck(self, item_transformed):\n        """"""\n            A function to cache the expensive input data transform operations\n            so that huge data sets (larger than computer memory) can be processed\n            on the fly as needed, and intermediate results written to disk for\n            future use.\n        Args:\n            item_transformed: The current data element to be mutated into transformed representation\n\n        Returns:\n            The transformed data_element, either from cache, or explicitly computing it.\n\n        Warning:\n            The current implementation does not encode transform information as part of the\n            hashing mechanism used for generating cache names.  If the transforms applied are\n            changed in any way, the objects in the cache dir will be invalid.  The hash for the\n            cache is ONLY dependant on the input filename paths.\n        """"""\n        if item_transformed.get(""cached"", False) is False:\n            hashfile = None\n            if self.cache_dir is not None:\n                cache_dir_path: Path = Path(self.cache_dir)\n                if cache_dir_path.is_dir():\n                    # TODO: Find way to hash transforms content as part of the cache\n                    data_item_md5 = hashlib.md5(\n                        json.dumps(item_transformed, sort_keys=True).encode(""utf-8"")\n                    ).hexdigest()\n                    hashfile: Path = Path(cache_dir_path) / f""{data_item_md5}.pt""\n\n            if hashfile is not None and hashfile.is_file():\n                item_transformed = torch.load(hashfile)\n            else:\n                item_transformed = self._pre_first_random_transform(item_transformed)\n                if hashfile is not None:\n                    # add sentinel flag to indicate that the transforms have already been computed.\n                    item_transformed[""cache""] = True\n                    # NOTE: Writing to "".temp_write_cache"" and then using a nearly atomic rename operation\n                    #       to make the cache more robust to manual killing of parent process\n                    #       which may leave partially written cache files in an incomplete state\n                    temp_hash_file: Path = hashfile.with_suffix("".temp_write_cache"")\n                    torch.save(item_transformed, temp_hash_file)\n                    temp_hash_file.rename(hashfile)\n\n        return item_transformed\n\n    def __getitem__(self, index):\n        pre_random_item = self._pre_first_random_cachecheck(self.data[index])\n        post_random_item = self._first_random_and_beyond_transform(pre_random_item)\n        return post_random_item\n\n\nclass CacheDataset(Dataset):\n    """"""\n    Dataset with cache mechanism that can load data and cache deterministic transforms\' result during training.\n\n    By caching the results of non-random preprocessing transforms, it accelerates the training data pipeline.\n    If the requested data is not in the cache, all transforms will run normally\n    (see also :py:class:`monai.data.dataset.Dataset`).\n\n    Users can set the cache rate or number of items to cache.\n    It is recommended to experiment with different `cache_num` or `cache_rate` to identify the best training speed.\n\n    To improve the caching efficiency, please always put as many as possible non-random transforms\n    before the randomized ones when composing the chain of transforms.\n\n    For example, if the transform is a `Compose` of::\n\n        transforms = Compose([\n            LoadNiftid(),\n            AddChanneld(),\n            Spacingd(),\n            Orientationd(),\n            ScaleIntensityRanged(),\n            RandCropByPosNegLabeld(),\n            ToTensord()\n        ])\n\n    when `transforms` is used in a multi-epoch training pipeline, before the first training epoch,\n    this dataset will cache the results up to ``ScaleIntensityRanged``, as\n    all non-random transforms `LoadNiftid`, `AddChanneld`, `Spacingd`, `Orientationd`, `ScaleIntensityRanged`\n    can be cached. During training, the dataset will load the cached results and run\n    ``RandCropByPosNegLabeld`` and ``ToTensord``, as ``RandCropByPosNegLabeld`` is a randomized transform\n    and the outcome not cached.\n    """"""\n\n    def __init__(\n        self, data, transform: Callable, cache_num: int = sys.maxsize, cache_rate: float = 1.0, num_workers: int = 0\n    ):\n        """"""\n        Args:\n            data (Iterable): input data to load and transform to generate dataset for model.\n            transform (Callable): transforms to execute operations on input data.\n            cache_num: number of items to be cached. Default is `sys.maxsize`.\n                will take the minimum of (cache_num, data_length x cache_rate, data_length).\n            cache_rate (float): percentage of cached data in total, default is 1.0 (cache all).\n                will take the minimum of (cache_num, data_length x cache_rate, data_length).\n            num_workers: the number of worker threads to use.\n                If 0 a single thread will be used. Default is 0.\n        """"""\n        if not isinstance(transform, Compose):\n            transform = Compose(transform)\n        super().__init__(data, transform)\n        self.cache_num = min(cache_num, int(len(self) * cache_rate), len(self))\n        if self.cache_num > 0:\n            self._cache = [None] * self.cache_num\n            print(""Load and cache transformed data..."")\n            if num_workers > 0:\n                self._item_processed = 0\n                self._thread_lock = threading.Lock()\n                with ThreadPool(num_workers) as p:\n                    p.map(\n                        self._load_cache_item_thread,\n                        [(i, data[i], transform.transforms) for i in range(self.cache_num)],\n                    )\n            else:\n                for i in range(self.cache_num):\n                    self._cache[i] = self._load_cache_item(data[i], transform.transforms)\n                    process_bar(i + 1, self.cache_num)\n\n    def _load_cache_item(self, item, transforms):\n        for _transform in transforms:\n            # execute all the deterministic transforms\n            if isinstance(_transform, Randomizable) or not isinstance(_transform, Transform):\n                break\n            item = apply_transform(_transform, item)\n        return item\n\n    def _load_cache_item_thread(self, args):\n        i, item, transforms = args\n        self._cache[i] = self._load_cache_item(item, transforms)\n        with self._thread_lock:\n            self._item_processed += 1\n            process_bar(self._item_processed, self.cache_num)\n\n    def __getitem__(self, index):\n        if index < self.cache_num:\n            # load data from cache and execute from the first random transform\n            start_run = False\n            data = self._cache[index]\n            for _transform in self.transform.transforms:  # pytype: disable=attribute-error\n                if not start_run and not isinstance(_transform, Randomizable) and isinstance(_transform, Transform):\n                    continue\n                else:\n                    start_run = True\n                data = apply_transform(_transform, data)\n        else:\n            # no cache for this data, execute all the transforms directly\n            data = super(CacheDataset, self).__getitem__(index)\n        return data\n\n\nclass ZipDataset(Dataset):\n    """"""\n    Zip several PyTorch datasets and output data(with the same index) together in a tuple.\n    If the output of single dataset is already a tuple, flatten it and extend to the result.\n    For example: if datasetA returns (img, imgmeta), datasetB returns (seg, segmeta),\n    finally return (img, imgmeta, seg, segmeta).\n    And if the datasets don\'t have same length, use the minimum length of them as the length\n    of ZipDataset.\n\n    Examples::\n\n        >>> zip_data = ZipDataset([[1, 2, 3], [4, 5]])\n        >>> print(len(zip_data))\n        2\n        >>> for item in zip_data:\n        >>>    print(item)\n        [1, 4]\n        [2, 5]\n\n    """"""\n\n    def __init__(self, datasets, transform=None):\n        """"""\n        Args:\n            datasets (list or tuple): list of datasets to zip together.\n            transform (Callable): a callable data transform operates on the zipped item from `datasets`.\n        """"""\n        super().__init__(list(datasets), transform=transform)\n\n    def __len__(self):\n        return min([len(dataset) for dataset in self.data])\n\n    def __getitem__(self, index: int):\n        def to_list(x):\n            return list(x) if isinstance(x, (tuple, list)) else [x]\n\n        data = list()\n        for dataset in self.data:\n            data.extend(to_list(dataset[index]))\n        if self.transform is not None:\n            data = apply_transform(self.transform, data, map_items=False)  # transform the list data\n        return data\n\n\nclass ArrayDataset(Randomizable):\n    """"""\n    Dataset for segmentation and classification tasks based on array format input data and transforms.\n    It ensures the same random seeds in the randomized transforms defined for image, segmentation and label.\n    The `transform` can be :py:class:`monai.transforms.Compose` or any other callable object.\n    For example:\n    If train based on Nifti format images without metadata, all transforms can be composed::\n\n        img_transform = Compose(\n            [\n                LoadNifti(image_only=True),\n                AddChannel(),\n                RandAdjustContrast()\n            ]\n        )\n        ArrayDataset(img_file_list, img_transform=img_transform)\n\n    If training based on images and the metadata, the array transforms can not be composed\n    because several transforms receives multiple parameters or return multiple values. Then Users need\n    to define their own callable method to parse metadata from `LoadNifti` or set `affine` matrix\n    to `Spacing` transform::\n\n        class TestCompose(Compose):\n            def __call__(self, input_):\n                img, metadata = self.transforms[0](input_)\n                img = self.transforms[1](img)\n                img, _, _ = self.transforms[2](img, metadata[""affine""])\n                return self.transforms[3](img), metadata\n        img_transform = TestCompose(\n            [\n                LoadNifti(image_only=False),\n                AddChannel(),\n                Spacing(pixdim=(1.5, 1.5, 3.0)),\n                RandAdjustContrast()\n            ]\n        )\n        ArrayDataset(img_file_list, img_transform=img_transform)\n\n    Examples::\n\n        >>> ds = ArrayDataset([1, 2, 3, 4], lambda x: x + 0.1)\n        >>> print(ds[0])\n        1.1\n\n        >>> ds = ArrayDataset(img=[1, 2, 3, 4], seg=[5, 6, 7, 8])\n        >>> print(ds[0])\n        [1, 5]\n\n    """"""\n\n    def __init__(\n        self,\n        img,\n        img_transform: Optional[Callable] = None,\n        seg=None,\n        seg_transform: Optional[Callable] = None,\n        labels=None,\n        label_transform: Optional[Callable] = None,\n    ):\n        """"""\n        Initializes the dataset with the filename lists. The transform `img_transform` is applied\n        to the images and `seg_transform` to the segmentations.\n\n        Args:\n            img (Sequence): sequence of images.\n            img_transform (Callable, optional): transform to apply to each element in `img`.\n            seg (Sequence, optional): sequence of segmentations.\n            seg_transform (Callable, optional): transform to apply to each element in `seg`.\n            labels (Sequence, optional): sequence of labels.\n            label_transform (Callable, optional): transform to apply to each element in `labels`.\n\n        """"""\n        items = [(img, img_transform), (seg, seg_transform), (labels, label_transform)]\n        self.set_random_state(seed=get_seed())\n        datasets = [Dataset(x[0], x[1]) for x in items if x[0] is not None]\n        self.dataset = datasets[0] if len(datasets) == 1 else ZipDataset(datasets)\n\n        self._seed = 0  # transform synchronization seed\n\n    def randomize(self):\n        self._seed = self.R.randint(np.iinfo(np.int32).max)\n\n    def __getitem__(self, index: int):\n        self.randomize()\n        if isinstance(self.dataset, ZipDataset):\n            # set transforms of each zip component\n            for dataset in self.dataset.data:\n                transform = getattr(dataset, ""transform"", None)\n                if isinstance(transform, Randomizable):\n                    transform.set_random_state(seed=self._seed)\n        transform = getattr(self.dataset, ""transform"", None)\n        if isinstance(transform, Randomizable):\n            transform.set_random_state(seed=self._seed)\n        return self.dataset[index]\n'"
monai/data/decathalon_datalist.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport json\n\n\ndef _compute_path(base_dir, element):\n    if isinstance(element, str):\n        return os.path.normpath(os.path.join(base_dir, element))\n    elif isinstance(element, list):\n        for e in element:\n            if not isinstance(e, str):\n                raise ValueError(""file path must be a string."")\n        return [os.path.normpath(os.path.join(base_dir, e)) for e in element]\n    else:\n        raise ValueError(""file path must be a string or a list of string."")\n\n\ndef _append_paths(base_dir, is_segmentation, items):\n    for item in items:\n        if not isinstance(item, dict):\n            raise ValueError(""data item must be dict."")\n        for k, v in item.items():\n            if k == ""image"":\n                item[k] = _compute_path(base_dir, v)\n            elif is_segmentation and k == ""label"":\n                item[k] = _compute_path(base_dir, v)\n    return items\n\n\ndef load_decathalon_datalist(data_list_file_path, is_segmentation=True, data_list_key=""training"", base_dir=None):\n    """"""Load image/label paths of decathalon challenge from JSON file\n\n    Json file is similar to what you get from http://medicaldecathlon.com/\n    Those dataset.json files\n\n    Args:\n        data_list_file_path (str): the path to the json file of datalist.\n        is_segmentation (bool): whether the datalist is for segmentation task, default is True.\n        data_list_key (str): the key to get a list of dictionary to be used, default is ""training"".\n        base_dir (str): the base directory of the dataset, if None, use the datalist directory.\n\n    Returns a list of data items, each of which is a dict keyed by element names, for example:\n\n    .. code-block::\n\n        [\n            {\'image\': \'/workspace/data/chest_19.nii.gz\',  \'label\': 0}, \n            {\'image\': \'/workspace/data/chest_31.nii.gz\',  \'label\': 1}\n        ]\n\n    """"""\n    if not os.path.isfile(data_list_file_path):\n        raise ValueError(f""data list file {data_list_file_path} does not exist."")\n    with open(data_list_file_path) as json_file:\n        json_data = json.load(json_file)\n    if data_list_key not in json_data:\n        raise ValueError(f""data list {data_list_key} not specified in \'{data_list_file_path}\'."")\n    expected_data = json_data[data_list_key]\n    if data_list_key == ""test"":\n        expected_data = [{""image"": i} for i in expected_data]\n\n    if base_dir is None:\n        base_dir = os.path.dirname(data_list_file_path)\n\n    return _append_paths(base_dir, is_segmentation, expected_data)\n'"
monai/data/grid_dataset.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch\nfrom torch.utils.data import IterableDataset\n\nfrom monai.data.utils import iter_patch\n\n\nclass GridPatchDataset(IterableDataset):\n    """"""\n    Yields patches from arrays read from an input dataset. The patches are chosen in a contiguous grid sampling scheme.\n    """"""\n\n    def __init__(self, dataset, patch_size, start_pos=(), pad_mode: str = ""wrap"", **pad_opts):\n        """"""\n        Initializes this dataset in terms of the input dataset and patch size. The `patch_size` is the size of the \n        patch to sample from the input arrays. It is assumed the arrays first dimension is the channel dimension which\n        will be yielded in its entirety so this should not be specified in `patch_size`. For example, for an input 3D\n        array with 1 channel of size (1, 20, 20, 20) a regular grid sampling of eight patches (1, 10, 10, 10) would be \n        specified by a `patch_size` of (10, 10, 10).\n\n        Args:\n            dataset (Dataset): the dataset to read array data from\n            patch_size (tuple of int or None): size of patches to generate slices for, 0/None selects whole dimension\n            start_pos (tuple of it, optional): starting position in the array, default is 0 for each dimension\n            pad_mode (str, optional): padding mode, see numpy.pad\n            pad_opts (dict, optional): padding options, see numpy.pad\n        """"""\n\n        self.dataset = dataset\n        self.patch_size = (None,) + tuple(patch_size)\n        self.start_pos = start_pos\n        self.pad_mode = pad_mode\n        self.pad_opts = pad_opts\n\n    def __iter__(self):\n        worker_info = torch.utils.data.get_worker_info()\n        iter_start = 0\n        iter_end = len(self.dataset)\n\n        if worker_info is not None:\n            # split workload\n            per_worker = int(math.ceil((iter_end - iter_start) / float(worker_info.num_workers)))\n            worker_id = worker_info.id\n            iter_start = iter_start + worker_id * per_worker\n            iter_end = min(iter_start + per_worker, iter_end)\n\n        for index in range(iter_start, iter_end):\n            arrays = self.dataset[index]\n\n            iters = [\n                iter_patch(a, self.patch_size, self.start_pos, False, self.pad_mode, **self.pad_opts) for a in arrays\n            ]\n\n            yield from zip(*iters)\n'"
monai/data/nifti_reader.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Callable\n\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom monai.transforms import LoadNifti, Randomizable, apply_transform\nfrom monai.utils.misc import get_seed\n\n\nclass NiftiDataset(Dataset, Randomizable):\n    """"""\n    Loads image/segmentation pairs of Nifti files from the given filename lists. Transformations can be specified\n    for the image and segmentation arrays separately.\n    """"""\n\n    def __init__(\n        self,\n        image_files,\n        seg_files=None,\n        labels=None,\n        as_closest_canonical: bool = False,\n        transform: Optional[Callable] = None,\n        seg_transform: Optional[Callable] = None,\n        image_only: bool = True,\n        dtype: Optional[np.dtype] = np.float32,\n    ):\n        """"""\n        Initializes the dataset with the image and segmentation filename lists. The transform `transform` is applied\n        to the images and `seg_transform` to the segmentations.\n\n        Args:\n            image_files (list of str): list of image filenames\n            seg_files (list of str): if in segmentation task, list of segmentation filenames\n            labels (list or array): if in classification task, list of classification labels\n            as_closest_canonical (bool): if True, load the image as closest to canonical orientation\n            transform (Callable, optional): transform to apply to image arrays\n            seg_transform (Callable, optional): transform to apply to segmentation arrays\n            image_only (bool): if True return only the image volume, other return image volume and header dict\n            dtype (np.dtype, optional): if not None convert the loaded image to this data type\n        """"""\n\n        if seg_files is not None and len(image_files) != len(seg_files):\n            raise ValueError(""Must have same number of image and segmentation files"")\n\n        self.image_files = image_files\n        self.seg_files = seg_files\n        self.labels = labels\n        self.as_closest_canonical = as_closest_canonical\n        self.transform = transform\n        self.seg_transform = seg_transform\n        self.image_only = image_only\n        self.dtype = dtype\n        self.set_random_state(seed=get_seed())\n\n        self._seed = 0  # transform synchronization seed\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def randomize(self):\n        self._seed = self.R.randint(np.iinfo(np.int32).max)\n\n    def __getitem__(self, index: int):\n        self.randomize()\n        meta_data = None\n        img_loader = LoadNifti(\n            as_closest_canonical=self.as_closest_canonical, image_only=self.image_only, dtype=self.dtype\n        )\n        if self.image_only:\n            img = img_loader(self.image_files[index])\n        else:\n            img, meta_data = img_loader(self.image_files[index])\n        seg = None\n        if self.seg_files is not None:\n            seg_loader = LoadNifti(image_only=True)\n            seg = seg_loader(self.seg_files[index])\n        label = None\n        if self.labels is not None:\n            label = self.labels[index]\n\n        if self.transform is not None:\n            if isinstance(self.transform, Randomizable):\n                self.transform.set_random_state(seed=self._seed)\n            img = apply_transform(self.transform, img)\n\n        data = [img]\n\n        if self.seg_transform is not None:\n            if isinstance(self.seg_transform, Randomizable):\n                self.seg_transform.set_random_state(seed=self._seed)\n            seg = apply_transform(self.seg_transform, seg)\n\n        if seg is not None:\n            data.append(seg)\n        if label is not None:\n            data.append(label)\n        if not self.image_only and meta_data is not None:\n            data.append(meta_data)\n        if len(data) == 1:\n            return data[0]\n        return data\n'"
monai/data/nifti_saver.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Union, Optional\n\nimport numpy as np\nimport torch\nfrom monai.data.nifti_writer import write_nifti\nfrom .utils import create_file_basename\n\n\nclass NiftiSaver:\n    """"""\n    Save the data as NIfTI file, it can support single data content or a batch of data.\n    Typically, the data can be segmentation predictions, call `save` for single data\n    or call `save_batch` to save a batch of data together. If no meta data provided,\n    use index from 0 as the filename prefix.\n    """"""\n\n    def __init__(\n        self,\n        output_dir: str = ""./"",\n        output_postfix: str = ""seg"",\n        output_ext: str = "".nii.gz"",\n        resample: bool = True,\n        interp_order: int = 3,\n        mode: str = ""constant"",\n        cval: Union[int, float] = 0,\n        dtype: Optional[np.dtype] = None,\n    ):\n        """"""\n        Args:\n            output_dir (str): output image directory.\n            output_postfix (str): a string appended to all output file names.\n            output_ext (str): output file extension name.\n            resample (bool): whether to resample before saving the data array.\n            interp_order: the order of the spline interpolation, default is InterpolationCode.SPLINE3.\n                The order has to be in the range 0 - 5.\n                https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.affine_transform.html\n                this option is used when `resample = True`.\n            mode (`reflect|constant|nearest|mirror|wrap`):\n                The mode parameter determines how the input array is extended beyond its boundaries.\n                this option is used when `resample = True`.\n            cval (scalar): Value to fill past edges of input if mode is ""constant"". Default is 0.0.\n                this option is used when `resample = True`.\n            dtype (np.dtype, optional): convert the image data to save to this data type.\n                If None, keep the original type of data.\n\n        """"""\n        self.output_dir = output_dir\n        self.output_postfix = output_postfix\n        self.output_ext = output_ext\n        self.resample = resample\n        self.interp_order = interp_order\n        self.mode = mode\n        self.cval = cval\n        self.dtype = dtype\n        self._data_index = 0\n\n    def save(self, data: Union[torch.Tensor, np.ndarray], meta_data=None):\n        """"""\n        Save data into a Nifti file.\n        The metadata could optionally have the following keys:\n\n            - ``\'filename_or_obj\'`` -- for output file name creation, corresponding to filename or object.\n            - ``\'original_affine\'`` -- for data orientation handling, defaulting to an identity matrix.\n            - ``\'affine\'`` -- for data output affine, defaulting to an identity matrix.\n            - ``\'spatial_shape\'`` -- for data output shape.\n\n        If meta_data is None, use the default index from 0 to save data instead.\n\n        args:\n            data (Tensor or ndarray): target data content that to be saved as a NIfTI format file.\n                Assuming the data shape starts with a channel dimension and followed by spatial dimensions.\n            meta_data (dict): the meta data information corresponding to the data.\n\n        See Also\n            :py:meth:`monai.data.nifti_writer.write_nifti`\n        """"""\n        filename = meta_data[""filename_or_obj""] if meta_data else str(self._data_index)\n        self._data_index += 1\n        original_affine = meta_data.get(""original_affine"", None) if meta_data else None\n        affine = meta_data.get(""affine"", None) if meta_data else None\n        spatial_shape = meta_data.get(""spatial_shape"", None) if meta_data else None\n\n        if torch.is_tensor(data):\n            data = data.detach().cpu().numpy()\n        filename = create_file_basename(self.output_postfix, filename, self.output_dir)\n        filename = f""{filename}{self.output_ext}""\n        # change data to ""channel last"" format and write to nifti format file\n        data = np.moveaxis(data, 0, -1)\n        write_nifti(\n            data,\n            file_name=filename,\n            affine=affine,\n            target_affine=original_affine,\n            resample=self.resample,\n            output_shape=spatial_shape,\n            interp_order=self.interp_order,\n            mode=self.mode,\n            cval=self.cval,\n            dtype=self.dtype or data.dtype,\n        )\n\n    def save_batch(self, batch_data: Union[torch.Tensor, np.ndarray], meta_data=None):\n        """"""Save a batch of data into Nifti format files.\n\n        args:\n            batch_data (Tensor or ndarray): target batch data content that save into NIfTI format.\n            meta_data (dict): every key-value in the meta_data is corresponding to a batch of data.\n        """"""\n        for i, data in enumerate(batch_data):  # save a batch of files\n            self.save(data, {k: meta_data[k][i] for k in meta_data} if meta_data else None)\n'"
monai/data/nifti_writer.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport nibabel as nib\nimport numpy as np\nimport scipy.ndimage\n\nfrom monai.data.utils import compute_shape_offset, to_affine_nd, InterpolationCode\n\n\ndef write_nifti(\n    data,\n    file_name,\n    affine=None,\n    target_affine=None,\n    resample=True,\n    output_shape=None,\n    interp_order=InterpolationCode.SPLINE3,\n    mode=""constant"",\n    cval=0,\n    dtype=None,\n):\n    """"""\n    Write numpy data into NIfTI files to disk.  This function converts data\n    into the coordinate system defined by `target_affine` when `target_affine`\n    is specified.\n\n    If the coordinate transform between `affine` and `target_affine` could be\n    achieved by simply transposing and flipping `data`, no resampling will\n    happen.  otherwise this function will resample `data` using the coordinate\n    transform computed from `affine` and `target_affine`.  Note that the shape\n    of the resampled `data` may subject to some rounding errors. For example,\n    resampling a 20x20 pixel image from pixel size (1.5, 1.5)-mm to (3.0,\n    3.0)-mm space will return a 10x10-pixel image.  However, resampling a\n    20x20-pixel image from pixel size (2.0, 2.0)-mm to (3.0, 3.0)-mma space\n    will output a 14x14-pixel image, where the image shape is rounded from\n    13.333x13.333 pixels. In this case `output_shape` could be specified so\n    that this function writes image data to a designated shape.\n\n    When `affine` and `target_affine` are None, the data will be saved with an\n    identity matrix as the image affine.\n\n    This function assumes the NIfTI dimension notations.\n    Spatially it supports up to three dimensions, that is, H, HW, HWD for\n    1D, 2D, 3D respectively.\n    When saving multiple time steps or multiple channels `data`, time and/or\n    modality axes should be appended after the first three dimensions.  For\n    example, shape of 2D eight-class segmentation probabilities to be saved\n    could be `(64, 64, 1, 8)`,\n\n    Args:\n        data (numpy.ndarray): input data to write to file.\n        file_name (string): expected file name that saved on disk.\n        affine (numpy.ndarray): the current affine of `data`. Defaults to `np.eye(4)`\n        target_affine (numpy.ndarray, optional): before saving\n            the (`data`, `affine`) as a Nifti1Image,\n            transform the data into the coordinates defined by `target_affine`.\n        resample (bool): whether to run resampling when the target affine\n            could not be achieved by swapping/flipping data axes.\n        output_shape (None or tuple of ints): output image shape.\n            this option is used when resample = True.\n        interp_order: the order of the spline interpolation, default is InterpolationCode.SPLINE3.\n            The order has to be in the range 0 - 5.\n            https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.affine_transform.html\n            this option is used when `resample = True`.\n        mode (`reflect|constant|nearest|mirror|wrap`):\n            The mode parameter determines how the input array is extended beyond its boundaries.\n            this option is used when `resample = True`.\n        cval (scalar): Value to fill past edges of input if mode is ""constant"". Default is 0.0.\n            this option is used when `resample = True`.\n        dtype (np.dtype, optional): convert the image to save to this data type.\n    """"""\n    assert isinstance(data, np.ndarray), ""input data must be numpy array.""\n    sr = min(data.ndim, 3)\n    if affine is None:\n        affine = np.eye(4, dtype=np.float64)\n    affine = to_affine_nd(sr, affine)\n\n    if target_affine is None:\n        target_affine = affine\n    target_affine = to_affine_nd(sr, target_affine)\n\n    if np.allclose(affine, target_affine):\n        # no affine changes, save (data, affine)\n        results_img = nib.Nifti1Image(data.astype(dtype), to_affine_nd(3, target_affine))\n        nib.save(results_img, file_name)\n        return\n\n    # resolve orientation\n    start_ornt = nib.orientations.io_orientation(affine)\n    target_ornt = nib.orientations.io_orientation(target_affine)\n    ornt_transform = nib.orientations.ornt_transform(start_ornt, target_ornt)\n    data_shape = data.shape\n    data = nib.orientations.apply_orientation(data, ornt_transform)\n    _affine = affine @ nib.orientations.inv_ornt_aff(ornt_transform, data_shape)\n    if np.allclose(_affine, target_affine) or not resample:\n        results_img = nib.Nifti1Image(data.astype(dtype), to_affine_nd(3, target_affine))\n        nib.save(results_img, file_name)\n        return\n\n    # need resampling\n    transform = np.linalg.inv(_affine) @ target_affine\n    if output_shape is None:\n        output_shape, _ = compute_shape_offset(data.shape, _affine, target_affine)\n    dtype = dtype or data.dtype\n    if data.ndim > 3:  # multi channel, resampling each channel\n        spatial_shape, channel_shape = data.shape[:3], data.shape[3:]\n        data_ = data.astype(dtype).reshape(list(spatial_shape) + [-1])\n        data_chns = []\n        for chn in range(data_.shape[-1]):\n            data_chns.append(\n                scipy.ndimage.affine_transform(\n                    data_[..., chn],\n                    matrix=transform,\n                    output_shape=output_shape[:3],\n                    order=interp_order,\n                    mode=mode,\n                    cval=cval,\n                )\n            )\n        data_chns = np.stack(data_chns, axis=-1)\n        data_ = data_chns.reshape(list(data_chns.shape[:3]) + list(channel_shape))\n    else:\n        data_ = data.astype(dtype)\n        data_ = scipy.ndimage.affine_transform(\n            data_, matrix=transform, output_shape=output_shape[: data_.ndim], order=interp_order, mode=mode, cval=cval\n        )\n    results_img = nib.Nifti1Image(data_, to_affine_nd(3, target_affine))\n    nib.save(results_img, file_name)\n    return\n'"
monai/data/png_saver.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Union\n\nimport torch\nimport numpy as np\nfrom monai.data.png_writer import write_png\nfrom .utils import create_file_basename\n\n\nclass PNGSaver:\n    """"""\n    Save the data as png file, it can support single data content or a batch of data.\n    Typically, the data can be segmentation predictions, call `save` for single data\n    or call `save_batch` to save a batch of data together. If no meta data provided,\n    use index from 0 as the filename prefix.\n    """"""\n\n    def __init__(\n        self,\n        output_dir: str = ""./"",\n        output_postfix: str = ""seg"",\n        output_ext: str = "".png"",\n        resample: bool = True,\n        interp_order: int = 3,\n        mode: str = ""constant"",\n        cval: Union[int, float] = 0,\n        scale: bool = False,\n    ):\n        """"""\n        Args:\n            output_dir (str): output image directory.\n            output_postfix (str): a string appended to all output file names.\n            output_ext (str): output file extension name.\n            resample (bool): whether to resample and resize if providing spatial_shape in the metadata.\n            interp_order: the order of the spline interpolation, default is InterpolationCode.SPLINE3.\n                This option is used when spatial_shape is specified and different from the data shape.\n                The order has to be in the range 0 - 5.\n            mode (`reflect|constant|nearest|mirror|wrap`):\n                The mode parameter determines how the input array is extended beyond its boundaries.\n                This option is used when spatial_shape is specified and different from the data shape.\n            cval (scalar): Value to fill past edges of input if mode is ""constant"". Default is 0.0.\n                This option is used when spatial_shape is specified and different from the data shape.\n            scale (bool): whether to scale data with 255 and convert to uint8 for data in range [0, 1].\n\n        """"""\n        self.output_dir = output_dir\n        self.output_postfix = output_postfix\n        self.output_ext = output_ext\n        self.resample = resample\n        self.interp_order = interp_order\n        self.mode = mode\n        self.cval = cval\n        self.scale = scale\n        self._data_index = 0\n\n    def save(self, data: Union[torch.Tensor, np.ndarray], meta_data=None):\n        """"""\n        Save data into a png file.\n        The metadata could optionally have the following keys:\n\n            - ``\'filename_or_obj\'`` -- for output file name creation, corresponding to filename or object.\n            - ``\'spatial_shape\'`` -- for data output shape.\n\n        If meta_data is None, use the default index from 0 to save data instead.\n\n        args:\n            data (Tensor or ndarray): target data content that to be saved as a png format file.\n                Assuming the data shape are spatial dimensions.\n                Shape of the spatial dimensions (C,H,W).\n                C should be 1, 3 or 4\n            meta_data (dict): the meta data information corresponding to the data.\n\n        See Also\n            :py:meth:`monai.data.png_writer.write_png`\n        """"""\n        filename = meta_data[""filename_or_obj""] if meta_data else str(self._data_index)\n        self._data_index += 1\n        spatial_shape = meta_data.get(""spatial_shape"", None) if meta_data and self.resample else None\n\n        if torch.is_tensor(data):\n            data = data.detach().cpu().numpy()\n\n        filename = create_file_basename(self.output_postfix, filename, self.output_dir)\n        filename = f""{filename}{self.output_ext}""\n\n        if data.shape[0] == 1:\n            data = data.squeeze(0)\n        elif 2 < data.shape[0] < 5:\n            data = np.moveaxis(data, 0, -1)\n        else:\n            raise ValueError(""PNG image should only have 1, 3 or 4 channels."")\n\n        write_png(\n            data,\n            file_name=filename,\n            output_shape=spatial_shape,\n            interp_order=self.interp_order,\n            mode=self.mode,\n            cval=self.cval,\n            scale=self.scale,\n        )\n\n    def save_batch(self, batch_data: Union[torch.Tensor, np.ndarray], meta_data=None):\n        """"""Save a batch of data into png format files.\n\n        args:\n            batch_data (Tensor or ndarray): target batch data content that save into png format.\n            meta_data (dict): every key-value in the meta_data is corresponding to a batch of data.\n        """"""\n        for i, data in enumerate(batch_data):  # save a batch of files\n            self.save(data, {k: meta_data[k][i] for k in meta_data} if meta_data else None)\n'"
monai/data/png_writer.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom skimage import io, transform\n\n\ndef write_png(\n    data,\n    file_name,\n    output_shape=None,\n    interp_order: int = 3,\n    mode=""constant"",\n    cval=0,\n    scale=False,\n    plugin=None,\n    **plugin_args,\n):\n    """"""\n    Write numpy data into png files to disk.  \n    Spatially it supports HW for 2D.(H,W) or (H,W,3) or (H,W,4)\n    It\'s based on skimage library: https://scikit-image.org/docs/dev/api/skimage\n\n    Args:\n        data (numpy.ndarray): input data to write to file.\n        file_name (string): expected file name that saved on disk.\n        output_shape (None or tuple of ints): output image shape.\n        interp_order: the order of the spline interpolation, default is InterpolationCode.SPLINE3.\n            The order has to be in the range 0 - 5.\n            this option is used when `output_shape != None`.\n        mode (`reflect|constant|nearest|mirror|wrap`):\n            The mode parameter determines how the input array is extended beyond its boundaries.\n            this option is used when `output_shape != None`.\n        cval (scalar): Value to fill past edges of input if mode is ""constant"". Default is 0.0.\n            this option is used when `output_shape != None`.\n        scale (bool): whether to scale data with 255 and convert to uint8 for data in range [0, 1].\n        plugin (string): name of plugin to use in `imsave`. By default, the different plugins\n            are tried(starting with imageio) until a suitable candidate is found.\n        plugin_args (keywords): arguments passed to the given plugin.\n\n    """"""\n    assert isinstance(data, np.ndarray), ""input data must be numpy array.""\n\n    if output_shape is not None:\n        assert (\n            isinstance(output_shape, (list, tuple)) and len(output_shape) == 2\n        ), ""output_shape must be a list of 2 values (H, W).""\n\n        if len(data.shape) == 3:\n            output_shape = tuple(output_shape) + (data.shape[2],)\n\n        data = transform.resize(data, output_shape, order=interp_order, mode=mode, cval=cval, preserve_range=True)\n\n    if scale:\n        assert np.min(data) >= 0 and np.max(data) <= 1, ""png writer only can scale data in range [0, 1].""\n        data = 255 * data\n    data = data.astype(np.uint8)\n    io.imsave(file_name, data, plugin=plugin, **plugin_args)\n    return\n'"
monai/data/synthetic.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\nfrom monai.transforms.utils import rescale_array\n\n__all__ = [""create_test_image_2d"", ""create_test_image_3d""]\n\n\ndef create_test_image_2d(\n    width, height, num_objs=12, rad_max=30, noise_max=0.0, num_seg_classes=5, channel_dim=None, random_state=None\n):\n    """"""\n    Return a noisy 2D image with `num_objs` circles and a 2D mask image. The maximum radius of the circles is given as\n    `rad_max`. The mask will have `num_seg_classes` number of classes for segmentations labeled sequentially from 1, plus a\n    background class represented as 0. If `noise_max` is greater than 0 then noise will be added to the image taken from\n    the uniform distribution on range `[0,noise_max)`. If `channel_dim` is None, will create an image without channel\n    dimension, otherwise create an image with channel dimension as first dim or last dim.\n    """"""\n    image = np.zeros((width, height))\n    rs = np.random if random_state is None else random_state\n\n    for i in range(num_objs):\n        x = rs.randint(rad_max, width - rad_max)\n        y = rs.randint(rad_max, height - rad_max)\n        rad = rs.randint(5, rad_max)\n        spy, spx = np.ogrid[-x : width - x, -y : height - y]\n        circle = (spx * spx + spy * spy) <= rad * rad\n\n        if num_seg_classes > 1:\n            image[circle] = np.ceil(rs.random() * num_seg_classes)\n        else:\n            image[circle] = rs.random() * 0.5 + 0.5\n\n    labels = np.ceil(image).astype(np.int32)\n\n    norm = rs.uniform(0, num_seg_classes * noise_max, size=image.shape)\n    noisyimage = rescale_array(np.maximum(image, norm))\n\n    if channel_dim is not None:\n        assert isinstance(channel_dim, int) and channel_dim in (-1, 0, 2), ""invalid channel dim.""\n        noisyimage, labels = (\n            noisyimage[None],\n            labels[None] if channel_dim == 0 else (noisyimage[..., None], labels[..., None]),\n        )\n\n    return noisyimage, labels\n\n\ndef create_test_image_3d(\n    height, width, depth, num_objs=12, rad_max=30, noise_max=0.0, num_seg_classes=5, channel_dim=None, random_state=None\n):\n    """"""\n    Return a noisy 3D image and segmentation.\n\n    See also:\n        :py:meth:`~create_test_image_2d`\n    """"""\n    image = np.zeros((width, height, depth))\n    rs = np.random if random_state is None else random_state\n\n    for i in range(num_objs):\n        x = rs.randint(rad_max, width - rad_max)\n        y = rs.randint(rad_max, height - rad_max)\n        z = rs.randint(rad_max, depth - rad_max)\n        rad = rs.randint(5, rad_max)\n        spy, spx, spz = np.ogrid[-x : width - x, -y : height - y, -z : depth - z]\n        circle = (spx * spx + spy * spy + spz * spz) <= rad * rad\n\n        if num_seg_classes > 1:\n            image[circle] = np.ceil(rs.random() * num_seg_classes)\n        else:\n            image[circle] = rs.random() * 0.5 + 0.5\n\n    labels = np.ceil(image).astype(np.int32)\n\n    norm = rs.uniform(0, num_seg_classes * noise_max, size=image.shape)\n    noisyimage = rescale_array(np.maximum(image, norm))\n\n    if channel_dim is not None:\n        assert isinstance(channel_dim, int) and channel_dim in (-1, 0, 3), ""invalid channel dim.""\n        noisyimage, labels = (\n            (noisyimage[None], labels[None]) if channel_dim == 0 else (noisyimage[..., None], labels[..., None])\n        )\n\n    return noisyimage, labels\n'"
monai/data/utils.py,7,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport os\nimport warnings\nimport math\nimport nibabel as nib\nfrom itertools import starmap, product\nimport torch\nfrom torch.utils.data._utils.collate import default_collate\nimport numpy as np\nfrom monai.utils import ensure_tuple_size\nfrom monai.networks.layers.simplelayers import GaussianFilter\n\nimport enum\n\n\nclass InterpolationCode(enum.IntEnum):\n    """"""\n    A convenience enumeration to make code uses more expressive\n    """"""\n\n    SPLINE0 = 0\n    NEARESTNEIGHBOR = 0\n    SPLINE1 = 1\n    LINEAR = 1\n    SPLINE2 = 2\n    CUBIC = 2\n    SPLINE3 = 3\n    SPLINE4 = 4\n    SPLINE5 = 5\n\n\ndef get_random_patch(dims, patch_size, rand_state: Optional[np.random.RandomState] = None):\n    """"""\n    Returns a tuple of slices to define a random patch in an array of shape `dims` with size `patch_size` or the as\n    close to it as possible within the given dimension. It is expected that `patch_size` is a valid patch for a source\n    of shape `dims` as returned by `get_valid_patch_size`.\n\n    Args:\n        dims (tuple of int): shape of source array\n        patch_size (tuple of int): shape of patch size to generate\n        rand_state (np.random.RandomState): a random state object to generate random numbers from\n\n    Returns:\n        (tuple of slice): a tuple of slice objects defining the patch\n    """"""\n\n    # choose the minimal corner of the patch\n    rand_int = np.random.randint if rand_state is None else rand_state.randint\n    min_corner = tuple(rand_int(0, ms - ps) if ms > ps else 0 for ms, ps in zip(dims, patch_size))\n\n    # create the slices for each dimension which define the patch in the source array\n    return tuple(slice(mc, mc + ps) for mc, ps in zip(min_corner, patch_size))\n\n\ndef iter_patch_slices(dims, patch_size, start_pos=()):\n    """"""\n    Yield successive tuples of slices defining patches of size `patch_size` from an array of dimensions `dims`. The\n    iteration starts from position `start_pos` in the array, or starting at the origin if this isn\'t provided. Each\n    patch is chosen in a contiguous grid using a first dimension as least significant ordering.\n\n    Args:\n        dims (tuple of int): dimensions of array to iterate over\n        patch_size (tuple of int or None): size of patches to generate slices for, 0 or None selects whole dimension\n        start_pos (tuple of it, optional): starting position in the array, default is 0 for each dimension\n\n    Yields:\n        Tuples of slice objects defining each patch\n    """"""\n\n    # ensure patchSize and startPos are the right length\n    ndim = len(dims)\n    patch_size = get_valid_patch_size(dims, patch_size)\n    start_pos = ensure_tuple_size(start_pos, ndim)\n\n    # collect the ranges to step over each dimension\n    ranges = tuple(starmap(range, zip(start_pos, dims, patch_size)))\n\n    # choose patches by applying product to the ranges\n    for position in product(*ranges[::-1]):  # reverse ranges order to iterate in index order\n        yield tuple(slice(s, s + p) for s, p in zip(position[::-1], patch_size))\n\n\ndef dense_patch_slices(image_size, patch_size, scan_interval):\n    """"""\n    Enumerate all slices defining 2D/3D patches of size `patch_size` from an `image_size` input image.\n\n    Args:\n        image_size (tuple of int): dimensions of image to iterate over\n        patch_size (tuple of int): size of patches to generate slices\n        scan_interval (tuple of int): dense patch sampling interval\n\n    Returns:\n        a list of slice objects defining each patch\n    """"""\n    num_spatial_dims = len(image_size)\n    if num_spatial_dims not in (2, 3):\n        raise ValueError(""image_size should has 2 or 3 elements"")\n    patch_size = get_valid_patch_size(image_size, patch_size)\n    scan_interval = ensure_tuple_size(scan_interval, num_spatial_dims)\n\n    scan_num = [\n        int(math.ceil(float(image_size[i]) / scan_interval[i])) if scan_interval[i] != 0 else 1\n        for i in range(num_spatial_dims)\n    ]\n    slices = []\n    if num_spatial_dims == 3:\n        for i in range(scan_num[0]):\n            start_i = i * scan_interval[0]\n            start_i -= max(start_i + patch_size[0] - image_size[0], 0)\n            slice_i = slice(start_i, start_i + patch_size[0])\n\n            for j in range(scan_num[1]):\n                start_j = j * scan_interval[1]\n                start_j -= max(start_j + patch_size[1] - image_size[1], 0)\n                slice_j = slice(start_j, start_j + patch_size[1])\n\n                for k in range(0, scan_num[2]):\n                    start_k = k * scan_interval[2]\n                    start_k -= max(start_k + patch_size[2] - image_size[2], 0)\n                    slice_k = slice(start_k, start_k + patch_size[2])\n                    slices.append((slice_i, slice_j, slice_k))\n    else:\n        for i in range(scan_num[0]):\n            start_i = i * scan_interval[0]\n            start_i -= max(start_i + patch_size[0] - image_size[0], 0)\n            slice_i = slice(start_i, start_i + patch_size[0])\n\n            for j in range(scan_num[1]):\n                start_j = j * scan_interval[1]\n                start_j -= max(start_j + patch_size[1] - image_size[1], 0)\n                slice_j = slice(start_j, start_j + patch_size[1])\n                slices.append((slice_i, slice_j))\n    return slices\n\n\ndef iter_patch(\n    arr: np.ndarray, patch_size, start_pos=(), copy_back: bool = True, pad_mode: Optional[str] = ""wrap"", **pad_opts\n):\n    """"""\n    Yield successive patches from `arr` of size `patch_size`. The iteration can start from position `start_pos` in `arr`\n    but drawing from a padded array extended by the `patch_size` in each dimension (so these coordinates can be negative\n    to start in the padded region). If `copy_back` is True the values from each patch are written back to `arr`.\n\n    Args:\n        arr (np.ndarray): array to iterate over\n        patch_size (tuple of int or None): size of patches to generate slices for, 0 or None selects whole dimension\n        start_pos (tuple of it, optional): starting position in the array, default is 0 for each dimension\n        copy_back (bool): if True data from the yielded patches is copied back to `arr` once the generator completes\n        pad_mode (str, optional): padding mode, see `numpy.pad`\n        pad_opts (dict, optional): padding options, see `numpy.pad`\n\n    Yields:\n        Patches of array data from `arr` which are views into a padded array which can be modified, if `copy_back` is\n        True these changes will be reflected in `arr` once the iteration completes.\n    """"""\n    # ensure patchSize and startPos are the right length\n    patch_size = get_valid_patch_size(arr.shape, patch_size)\n    start_pos = ensure_tuple_size(start_pos, arr.ndim)\n\n    # pad image by maximum values needed to ensure patches are taken from inside an image\n    arrpad = np.pad(arr, tuple((p, p) for p in patch_size), pad_mode, **pad_opts)\n\n    # choose a start position in the padded image\n    start_pos_padded = tuple(s + p for s, p in zip(start_pos, patch_size))\n\n    # choose a size to iterate over which is smaller than the actual padded image to prevent producing\n    # patches which are only in the padded regions\n    iter_size = tuple(s + p for s, p in zip(arr.shape, patch_size))\n\n    for slices in iter_patch_slices(iter_size, patch_size, start_pos_padded):\n        yield arrpad[slices]\n\n    # copy back data from the padded image if required\n    if copy_back:\n        slices = tuple(slice(p, p + s) for p, s in zip(patch_size, arr.shape))\n        arr[...] = arrpad[slices]\n\n\ndef get_valid_patch_size(dims, patch_size):\n    """"""\n    Given an image of dimensions `dims`, return a patch size tuple taking the dimension from `patch_size` if this is\n    not 0/None. Otherwise, or if `patch_size` is shorter than `dims`, the dimension from `dims` is taken. This ensures\n    the returned patch size is within the bounds of `dims`. If `patch_size` is a single number this is interpreted as a\n    patch of the same dimensionality of `dims` with that size in each dimension.\n    """"""\n    ndim = len(dims)\n\n    try:\n        # if a single value was given as patch size, treat this as the size of the patch over all dimensions\n        single_patch_size = int(patch_size)\n        patch_size = (single_patch_size,) * ndim\n    except TypeError:  # raised if the patch size is multiple values\n        # ensure patch size is at least as long as number of dimensions\n        patch_size = ensure_tuple_size(patch_size, ndim)\n\n    # ensure patch size dimensions are not larger than image dimension, if a dimension is None or 0 use whole dimension\n    return tuple(min(ms, ps or ms) for ms, ps in zip(dims, patch_size))\n\n\ndef list_data_collate(batch):\n    """"""\n    Enhancement for PyTorch DataLoader default collate.\n    If dataset already returns a list of batch data that generated in transforms, need to merge all data to 1 list.\n    Then it\'s same as the default collate behavior.\n\n    Note:\n        Need to use this collate if apply some transforms that can generate batch data.\n\n    """"""\n    elem = batch[0]\n    data = [i for k in batch for i in k] if isinstance(elem, list) else batch\n    return default_collate(data)\n\n\ndef worker_init_fn(worker_id):\n    """"""\n    Callback function for PyTorch DataLoader `worker_init_fn`.\n    It can set different random seed for the transforms in different workers.\n\n    """"""\n    worker_info = torch.utils.data.get_worker_info()  # type: ignore\n    if hasattr(worker_info.dataset, ""transform"") and hasattr(worker_info.dataset.transform, ""set_random_state""):\n        worker_info.dataset.transform.set_random_state(worker_info.seed % (2 ** 32))\n\n\ndef correct_nifti_header_if_necessary(img_nii):\n    """"""\n    Check nifti object header\'s format, update the header if needed.\n    In the updated image pixdim matches the affine.\n\n    Args:\n        img_nii (nifti image object)\n    """"""\n    dim = img_nii.header[""dim""][0]\n    if dim >= 5:\n        return img_nii  # do nothing for high-dimensional array\n    # check that affine matches zooms\n    pixdim = np.asarray(img_nii.header.get_zooms())[:dim]\n    norm_affine = np.sqrt(np.sum(np.square(img_nii.affine[:dim, :dim]), 0))\n    if np.allclose(pixdim, norm_affine):\n        return img_nii\n    if hasattr(img_nii, ""get_sform""):\n        return rectify_header_sform_qform(img_nii)\n    return img_nii\n\n\ndef rectify_header_sform_qform(img_nii):\n    """"""\n    Look at the sform and qform of the nifti object and correct it if any\n    incompatibilities with pixel dimensions\n\n    Adapted from https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/io/misc_io.py\n    """"""\n    d = img_nii.header[""dim""][0]\n    pixdim = np.asarray(img_nii.header.get_zooms())[:d]\n    sform, qform = img_nii.get_sform(), img_nii.get_qform()\n    norm_sform = np.sqrt(np.sum(np.square(sform[:d, :d]), 0))\n    norm_qform = np.sqrt(np.sum(np.square(qform[:d, :d]), 0))\n    sform_mismatch = not np.allclose(norm_sform, pixdim)\n    qform_mismatch = not np.allclose(norm_qform, pixdim)\n\n    if img_nii.header[""sform_code""] != 0:\n        if not sform_mismatch:\n            return img_nii\n        if not qform_mismatch:\n            img_nii.set_sform(img_nii.get_qform())\n            return img_nii\n    if img_nii.header[""qform_code""] != 0:\n        if not qform_mismatch:\n            return img_nii\n        if not sform_mismatch:\n            img_nii.set_qform(img_nii.get_sform())\n            return img_nii\n\n    norm = np.sqrt(np.sum(np.square(img_nii.affine[:d, :d]), 0))\n    warnings.warn(f""Modifying image pixdim from {pixdim} to {norm}"")\n\n    img_nii.header.set_zooms(norm)\n    return img_nii\n\n\ndef zoom_affine(affine, scale, diagonal: bool = True):\n    """"""\n    To make column norm of `affine` the same as `scale`.  If diagonal is False,\n    returns an affine that combines orthogonal rotation and the new scale.\n    This is done by first decomposing `affine`, then setting the zoom factors to\n    `scale`, and composing a new affine; the shearing factors are removed.  If\n    diagonal is True, returns a diagonal matrix, the scaling factors are set\n    to the diagonal elements.  This function always return an affine with zero\n    translations.\n\n    Args:\n        affine (nxn matrix): a square matrix.\n        scale (sequence of floats): new scaling factor along each dimension.\n        diagonal (bool): whether to return a diagonal scaling matrix.\n            Defaults to True.\n\n    returns:\n        the updated `n x n` affine.\n    """"""\n    affine = np.array(affine, dtype=float, copy=True)\n    if len(affine) != len(affine[0]):\n        raise ValueError(""affine should be a square matrix"")\n    scale = np.array(scale, dtype=float, copy=True)\n    if np.any(scale <= 0):\n        raise ValueError(""scale must be a sequence of positive numbers."")\n    d = len(affine) - 1\n    if len(scale) < d:  # defaults based on affine\n        norm = np.sqrt(np.sum(np.square(affine), 0))[:-1]\n        scale = np.append(scale, norm[len(scale) :])\n    scale = scale[:d]\n    scale[scale == 0] = 1.0\n    if diagonal:\n        return np.diag(np.append(scale, [1.0]))\n    rzs = affine[:-1, :-1]  # rotation zoom scale\n    zs = np.linalg.cholesky(rzs.T @ rzs).T\n    rotation = rzs @ np.linalg.inv(zs)\n    s = np.sign(np.diag(zs)) * np.abs(scale)\n    # construct new affine with rotation and zoom\n    new_affine = np.eye(len(affine))\n    new_affine[:-1, :-1] = rotation @ np.diag(s)\n    return new_affine\n\n\ndef compute_shape_offset(spatial_shape, in_affine, out_affine):\n    """"""\n    Given input and output affine, compute appropriate shapes\n    in the output space based on the input array\'s shape.\n    This function also returns the offset to put the shape\n    in a good position with respect to the world coordinate system.\n    """"""\n    shape = np.array(spatial_shape, copy=True, dtype=float)\n    sr = len(shape)\n    in_affine = to_affine_nd(sr, in_affine)\n    out_affine = to_affine_nd(sr, out_affine)\n    in_coords = [(0.0, dim - 1.0) for dim in shape]\n    corners = np.asarray(np.meshgrid(*in_coords, indexing=""ij"")).reshape((len(shape), -1))\n    corners = np.concatenate((corners, np.ones_like(corners[:1])))\n    corners = in_affine @ corners\n    corners_out = np.linalg.inv(out_affine) @ corners\n    corners_out = corners_out[:-1] / corners_out[-1]\n    out_shape = np.round(np.max(corners_out, 1) - np.min(corners_out, 1) + 1.0)\n    if np.allclose(nib.io_orientation(in_affine), nib.io_orientation(out_affine)):\n        # same orientation, get translate from the origin\n        offset = in_affine @ ([0] * sr + [1])\n        offset = offset[:-1] / offset[-1]\n    else:\n        # different orientation, the min is the origin\n        corners = corners[:-1] / corners[-1]\n        offset = np.min(corners, 1)\n    return out_shape.astype(int), offset\n\n\ndef to_affine_nd(r, affine):\n    """"""\n    Using elements from affine, to create a new affine matrix by\n    assigning the rotation/zoom/scaling matrix and the translation vector.\n\n    when ``r`` is an integer, output is an (r+1)x(r+1) matrix,\n    where the top left kxk elements are copied from ``affine``,\n    the last column of the output affine is copied from ``affine``\'s last column.\n    `k` is determined by `min(r, len(affine) - 1)`.\n\n    when ``r`` is an affine matrix, the output has the same as ``r``,\n    the top left kxk elements are  copied from ``affine``,\n    the last column of the output affine is copied from ``affine``\'s last column.\n    `k` is determined by `min(len(r) - 1, len(affine) - 1)`.\n\n\n    Args:\n        r (int or matrix): number of spatial dimensions or an output affine to be filled.\n        affine (matrix): 2D affine matrix\n    Returns:\n        an (r+1) x (r+1) matrix\n    """"""\n    affine_ = np.array(affine, dtype=np.float64)\n    if affine_.ndim != 2:\n        raise ValueError(""input affine must have two dimensions"")\n    new_affine = np.array(r, dtype=np.float64, copy=True)\n    if new_affine.ndim == 0:\n        sr = new_affine.astype(int)\n        if not np.isfinite(sr) or sr < 0:\n            raise ValueError(""r must be positive."")\n        new_affine = np.eye(sr + 1, dtype=np.float64)\n    d = max(min(len(new_affine) - 1, len(affine_) - 1), 1)\n    new_affine[:d, :d] = affine_[:d, :d]\n    if d > 1:\n        new_affine[:d, -1] = affine_[:d, -1]\n    return new_affine\n\n\ndef create_file_basename(postfix: str, input_file_name: str, folder_path: str, data_root_dir: str = """"):\n    """"""\n    Utility function to create the path to the output file based on the input\n    filename (extension is added by lib level writer before writing the file)\n\n    Args:\n        postfix (str): output name\'s postfix\n        input_file_name (str): path to the input image file\n        folder_path (str): path for the output file\n        data_root_dir (str): if not empty, it specifies the beginning parts of the input file\'s\n            absolute path. This is used to compute `input_file_rel_path`, the relative path to the file from\n            `data_root_dir` to preserve folder structure when saving in case there are files in different\n            folders with the same file names.\n    """"""\n\n    # get the filename and directory\n    filedir, filename = os.path.split(input_file_name)\n\n    # jettison the extension to have just filename\n    filename, ext = os.path.splitext(filename)\n    while ext != """":\n        filename, ext = os.path.splitext(filename)\n\n    # use data_root_dir to find relative path to file\n    filedir_rel_path = """"\n    if data_root_dir:\n        filedir_rel_path = os.path.relpath(filedir, data_root_dir)\n\n    # sub-folder path will be original name without the extension\n    subfolder_path = os.path.join(folder_path, filedir_rel_path, filename)\n    if not os.path.exists(subfolder_path):\n        os.makedirs(subfolder_path)\n\n    # add the sub-folder plus the postfix name to become the file basename in the output path\n    return os.path.join(subfolder_path, filename + ""_"" + postfix)\n\n\ndef compute_importance_map(patch_size, mode=""constant"", sigma_scale=0.125, device=None):\n    """"""Get importance map for different weight modes.\n\n    Args:\n        patch_size (tuple): Size of the required importance map. This should be either H, W [,D].\n        mode (str): Importance map type. Options are \'constant\' (Each weight has value 1.0)\n            or \'gaussian\' (Importance becomes lower away from center).\n        sigma_scale (float): Sigma_scale to calculate sigma for each dimension\n            (sigma = sigma_scale * dim_size). Used for gaussian mode only.\n        device (str of pytorch device): Device to put importance map on.\n\n    Returns:\n        Tensor of size patch_size.\n    """"""\n    importance_map = None\n    if mode == ""constant"":\n        importance_map = torch.ones(patch_size, device=device).float()\n    elif mode == ""gaussian"":\n        center_coords = [i // 2 for i in patch_size]\n        sigmas = [i * sigma_scale for i in patch_size]\n\n        importance_map = torch.zeros(patch_size, device=device)\n        importance_map[tuple(center_coords)] = 1\n        pt_gaussian = GaussianFilter(len(patch_size), sigmas).to(device=device, dtype=torch.float)\n        importance_map = pt_gaussian(importance_map.unsqueeze(0).unsqueeze(0))\n        importance_map = importance_map.squeeze(0).squeeze(0)\n        importance_map = importance_map / torch.max(importance_map)\n        importance_map = importance_map.float()\n\n        # importance_map cannot be 0, otherwise we may end up with nans!\n        importance_map[importance_map == 0] = torch.min(importance_map[importance_map != 0])\n    else:\n        raise ValueError(\'mode must be ""constant"" or ""gaussian"".\')\n\n    return importance_map\n'"
monai/engines/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .multi_gpu_supervised_trainer import *\nfrom .trainer import *\nfrom .evaluator import *\n'"
monai/engines/evaluator.py,7,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Callable\n\nimport torch\nfrom monai.inferers.inferer import SimpleInferer\nfrom ignite.metrics import Metric\nfrom ignite.engine import Engine\nfrom .workflow import Workflow\nfrom .utils import default_prepare_batch\nfrom .utils import CommonKeys as Keys\n\n\nclass Evaluator(Workflow):\n    """"""\n    Base class for all kinds of evaluators, inherits from Workflow.\n\n    Args:\n        device (torch.device): an object representing the device on which to run.\n        val_data_loader (torch.DataLoader): Ignite engine use data_loader to run, must be torch.DataLoader.\n        prepare_batch (Callable): function to parse image and label for current iteration.\n        iteration_update (Callable): the callable function for every iteration, expect to accept `engine`\n            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n        post_transform (Transform): execute additional transformation for the model output data.\n            Typically, several Tensor based transforms composed by `Compose`.\n        key_val_metric (ignite.metric): compute metric when every iteration completed, and save average value to\n            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n            checkpoint into files.\n        additional_metrics (dict): more Ignite metrics that also attach to Ignite Engine.\n        val_handlers (list): every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n\n    """"""\n\n    def __init__(\n        self,\n        device: torch.device,\n        val_data_loader,\n        prepare_batch: Callable = default_prepare_batch,\n        iteration_update: Optional[Callable] = None,\n        post_transform=None,\n        key_val_metric: Optional[Metric] = None,\n        additional_metrics=None,\n        val_handlers=None,\n    ):\n        super().__init__(\n            device=device,\n            max_epochs=1,\n            amp=False,\n            data_loader=val_data_loader,\n            prepare_batch=prepare_batch,\n            iteration_update=iteration_update,\n            post_transform=post_transform,\n            key_metric=key_val_metric,\n            additional_metrics=additional_metrics,\n            handlers=val_handlers,\n        )\n\n    def run(self, global_epoch: int = 1):\n        """"""\n        Execute validation/evaluation based on Ignite Engine.\n\n        Args:\n            global_epoch: the overall epoch if during a training. evaluator engine can get it from trainer.\n\n        """"""\n        # init env value for current validation process\n        self.state.max_epochs = global_epoch\n        self.state.epoch = global_epoch - 1\n        self.state.iteration = 0\n        super().run()\n\n    def get_validation_stats(self):\n        return {\n            ""best_validation_metric"": self.state.best_metric,\n            ""best_validation_epoch"": self.state.best_metric_epoch,\n        }\n\n\nclass SupervisedEvaluator(Evaluator):\n    """"""\n    Standard supervised evaluation method with image and label(optional), inherits from evaluator and Workflow.\n\n    Args:\n        device (torch.device): an object representing the device on which to run.\n        val_data_loader (torch.DataLoader): Ignite engine use data_loader to run, must be torch.DataLoader.\n        network (Network): use the network to run model forward.\n        prepare_batch (Callable): function to parse image and label for current iteration.\n        iteration_update (Callable): the callable function for every iteration, expect to accept `engine`\n            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n        inferer (Inferer): inference method that execute model forward on input data, like: SlidingWindow, etc.\n        post_transform (Transform): execute additional transformation for the model output data.\n            Typically, several Tensor based transforms composed by `Compose`.\n        key_val_metric (ignite.metric): compute metric when every iteration completed, and save average value to\n            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n            checkpoint into files.\n        additional_metrics (dict): more Ignite metrics that also attach to Ignite Engine.\n        val_handlers (list): every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n\n    """"""\n\n    def __init__(\n        self,\n        device: torch.device,\n        val_data_loader,\n        network,\n        prepare_batch: Callable = default_prepare_batch,\n        iteration_update: Optional[Callable] = None,\n        inferer=SimpleInferer(),\n        post_transform=None,\n        key_val_metric=None,\n        additional_metrics=None,\n        val_handlers=None,\n    ):\n        super().__init__(\n            device=device,\n            val_data_loader=val_data_loader,\n            prepare_batch=prepare_batch,\n            iteration_update=iteration_update,\n            post_transform=post_transform,\n            key_val_metric=key_val_metric,\n            additional_metrics=additional_metrics,\n            val_handlers=val_handlers,\n        )\n\n        self.network = network\n        self.inferer = inferer\n\n    def _iteration(self, engine: Engine, batchdata):\n        """"""\n        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n            batchdata (TransformContext, ndarray): input data for this iteration.\n\n        """"""\n        if batchdata is None:\n            raise ValueError(""must provide batch data for current iteration."")\n        inputs, targets = self.prepare_batch(batchdata)\n        inputs = inputs.to(engine.state.device)\n        if targets is not None:\n            targets = targets.to(engine.state.device)\n\n        # execute forward computation\n        self.network.eval()\n        with torch.no_grad():\n            predictions = self.inferer(inputs, self.network)\n\n        return {Keys.PRED: predictions, Keys.LABEL: targets}\n'"
monai/engines/multi_gpu_supervised_trainer.py,8,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Callable\n\nimport torch\nfrom ignite.engine import create_supervised_trainer, create_supervised_evaluator, _prepare_batch\nfrom .utils import get_devices_spec\n\n\ndef _default_transform(x, y, y_pred, loss):\n    return loss.item()\n\n\ndef _default_eval_transform(x, y, y_pred):\n    return y_pred, y\n\n\ndef create_multigpu_supervised_trainer(\n    net: torch.nn.Module,\n    optimizer,\n    loss_fn,\n    devices=None,\n    non_blocking: bool = False,\n    prepare_batch: Callable = _prepare_batch,\n    output_transform: Callable = _default_transform,\n):\n    """"""\n    Derived from `create_supervised_trainer` in Ignite.\n\n    Factory function for creating a trainer for supervised models.\n\n    Args:\n        net (`torch.nn.Module`): the network to train.\n        optimizer (`torch.optim.Optimizer`): the optimizer to use.\n        loss_fn (`torch.nn` loss function): the loss function to use.\n        devices (list, optional): device(s) type specification (default: None).\n            Applies to both model and batches. None is all devices used, empty list is CPU only.\n        non_blocking (bool, optional): if True and this copy is between CPU and GPU, the copy may occur asynchronously\n            with respect to the host. For other cases, this argument has no effect.\n        prepare_batch (callable, optional): function that receives `batch`, `device`, `non_blocking` and outputs\n            tuple of tensors `(batch_x, batch_y)`.\n        output_transform (callable, optional): function that receives \'x\', \'y\', \'y_pred\', \'loss\' and returns value\n            to be assigned to engine\'s state.output after each iteration. Default is returning `loss.item()`.\n\n    Returns:\n        Engine: a trainer engine with supervised update function.\n\n    Note:\n        `engine.state.output` for this engine is defined by `output_transform` parameter and is the loss\n        of the processed batch by default.\n    """"""\n\n    devices = get_devices_spec(devices)\n\n    if len(devices) > 1:\n        net = torch.nn.parallel.DataParallel(net)\n\n    return create_supervised_trainer(net, optimizer, loss_fn, devices[0], non_blocking, prepare_batch, output_transform)\n\n\ndef create_multigpu_supervised_evaluator(\n    net: torch.nn.Module,\n    metrics=None,\n    devices=None,\n    non_blocking: bool = False,\n    prepare_batch: Callable = _prepare_batch,\n    output_transform: Callable = _default_eval_transform,\n):\n    """"""\n    Derived from `create_supervised_evaluator` in Ignite.\n\n    Factory function for creating an evaluator for supervised models.\n\n    Args:\n        net (`torch.nn.Module`): the model to train.\n        metrics (dict of str - :class:`~ignite.metrics.Metric`): a map of metric names to Metrics.\n        devices (list, optional): device(s) type specification (default: None).\n            Applies to both model and batches. None is all devices used, empty list is CPU only.\n        non_blocking (bool, optional): if True and this copy is between CPU and GPU, the copy may occur asynchronously\n            with respect to the host. For other cases, this argument has no effect.\n        prepare_batch (callable, optional): function that receives `batch`, `device`, `non_blocking` and outputs\n            tuple of tensors `(batch_x, batch_y)`.\n        output_transform (callable, optional): function that receives \'x\', \'y\', \'y_pred\' and returns value\n            to be assigned to engine\'s state.output after each iteration. Default is returning `(y_pred, y,)` which fits\n            output expected by metrics. If you change it you should use `output_transform` in metrics.\n\n    Note:\n        `engine.state.output` for this engine is defined by `output_transform` parameter and is\n        a tuple of `(batch_pred, batch_y)` by default.\n\n    Returns:\n        Engine: an evaluator engine with supervised inference function.\n    """"""\n\n    devices = get_devices_spec(devices)\n\n    if len(devices) > 1:\n        net = torch.nn.parallel.DataParallel(net)\n\n    return create_supervised_evaluator(net, metrics, devices[0], non_blocking, prepare_batch, output_transform)\n'"
monai/engines/trainer.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Callable, Optional\n\nimport torch\nfrom ignite.metrics import Metric\nfrom ignite.engine import Engine\nfrom monai.inferers.inferer import SimpleInferer\nfrom .workflow import Workflow\nfrom .utils import default_prepare_batch\nfrom .utils import CommonKeys as Keys\n\n\nclass Trainer(Workflow):\n    """"""\n    Base class for all kinds of trainers, inherits from Workflow.\n\n    """"""\n\n    def run(self):\n        """"""\n        Execute training based on Ignite Engine.\n        If call this function multiple times, it will continuously run from the previous state.\n\n        """"""\n        if self._is_done(self.state):\n            self.state.iteration = 0  # to avoid creating new State instance in ignite Engine.run\n        super().run()\n\n    def get_train_stats(self):\n        return {""total_epochs"": self.state.max_epochs, ""total_iterations"": self.state.epoch_length}\n\n\nclass SupervisedTrainer(Trainer):\n    """"""\n    Standard supervised training method with image and label, inherits from trainer and Workflow.\n\n    Args:\n        device (torch.device): an object representing the device on which to run.\n        max_epochs: the total epoch number for engine to run, validator and evaluator have only 1 epoch.\n        train_data_loader (torch.DataLoader): Ignite engine use data_loader to run, must be torch.DataLoader.\n        network (Network): to train with this network.\n        optimizer (Optimizer): the optimizer associated to the network.\n        loss_function (Loss): the loss function associated to the optimizer.\n        prepare_batch (Callable): function to parse image and label for current iteration.\n        iteration_update (Callable): the callable function for every iteration, expect to accept `engine`\n            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n        lr_scheduler (LR Scheduler): the lr scheduler associated to the optimizer.\n        inferer (Inferer): inference method that execute model forward on input data, like: SlidingWindow, etc.\n        amp (bool): whether to enable auto-mixed-precision training, reserved.\n        post_transform (Transform): execute additional transformation for the model output data.\n            Typically, several Tensor based transforms composed by `Compose`.\n        key_train_metric (ignite.metric): compute metric when every iteration completed, and save average value to\n            engine.state.metrics when epoch completed. key_train_metric is the main metric to compare and save the\n            checkpoint into files.\n        additional_metrics (dict): more Ignite metrics that also attach to Ignite Engine.\n        train_handlers (list): every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n\n    """"""\n\n    def __init__(\n        self,\n        device: torch.device,\n        max_epochs: int,\n        train_data_loader,\n        network,\n        optimizer,\n        loss_function,\n        prepare_batch: Callable = default_prepare_batch,\n        iteration_update: Optional[Callable] = None,\n        lr_scheduler=None,\n        inferer=SimpleInferer(),\n        amp: bool = True,\n        post_transform=None,\n        key_train_metric: Optional[Metric] = None,\n        additional_metrics=None,\n        train_handlers=None,\n    ):\n        # set up Ignite engine and environments\n        super().__init__(\n            device=device,\n            max_epochs=max_epochs,\n            amp=amp,\n            data_loader=train_data_loader,\n            prepare_batch=prepare_batch,\n            iteration_update=iteration_update,\n            key_metric=key_train_metric,\n            additional_metrics=additional_metrics,\n            handlers=train_handlers,\n            post_transform=post_transform,\n        )\n\n        self.network = network\n        self.optimizer = optimizer\n        self.loss_function = loss_function\n        self.inferer = inferer\n\n    def _iteration(self, engine: Engine, batchdata):\n        """"""\n        Callback function for the Supervised Training processing logic of 1 iteration in Ignite Engine.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n            batchdata (dict or array of tensor): input data for this iteration.\n\n        """"""\n        if batchdata is None:\n            raise ValueError(""must provide batch data for current iteration."")\n        inputs, targets = self.prepare_batch(batchdata)\n        inputs, targets = inputs.to(engine.state.device), targets.to(engine.state.device)\n\n        self.network.train()\n        self.optimizer.zero_grad()\n        # execute forward computation\n        predictions = self.inferer(inputs, self.network)\n        # compute loss\n        loss = self.loss_function(predictions, targets).mean()\n        loss.backward()\n        self.optimizer.step()\n\n        return {Keys.PRED: predictions, Keys.LABEL: targets, Keys.LOSS: loss.item()}\n'"
monai/engines/utils.py,3,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\n\nclass CommonKeys:\n    """"""\n    A set of common keys for dictionary based supervised training process.\n    `IMAGE` is the input image data.\n    `LABEL` is the training or evaluation label of segmentation or classification task.\n    `PRED` is the prediction data of model output.\n    `LOSS` is the loss value of current iteration.\n    `INFO` is some useful information during training or evaluation, like loss value, etc.\n\n    """"""\n\n    IMAGE = ""image""\n    LABEL = ""label""\n    PRED = ""pred""\n    LOSS = ""loss""\n\n\ndef get_devices_spec(devices=None):\n    """"""\n    Get a valid specification for one or more devices. If `devices` is None get devices for all CUDA devices available.\n    If `devices` is and zero-length structure a single CPU compute device is returned. In any other cases `devices` is\n    returned unchanged.\n\n    Args:\n        devices (list, optional): list of devices to request, None for all GPU devices, [] for CPU.\n\n    Returns:\n        list of torch.device: list of devices.\n    """"""\n    if devices is None:\n        devices = [torch.device(f""cuda:{d:d}"") for d in range(torch.cuda.device_count())]\n\n        if len(devices) == 0:\n            raise ValueError(""No GPU devices available"")\n\n    elif len(devices) == 0:\n        devices = [torch.device(""cpu"")]\n\n    return devices\n\n\ndef default_prepare_batch(batchdata):\n    assert isinstance(batchdata, dict), ""default prepare_batch expects dictionary input data.""\n    return (\n        (batchdata[CommonKeys.IMAGE], batchdata[CommonKeys.LABEL])\n        if CommonKeys.LABEL in batchdata\n        else (batchdata[CommonKeys.IMAGE], None)\n    )\n'"
monai/engines/workflow.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABC, abstractmethod\nimport torch\nfrom ignite.engine import Engine, State, Events\nfrom .utils import default_prepare_batch\nfrom monai.transforms import apply_transform\n\n\nclass Workflow(ABC, Engine):\n    """"""\n    Workflow defines the core work process inheriting from Ignite engine.\n    All trainer, validator and evaluator share this same workflow as base class,\n    because they all can be treated as same Ignite engine loops.\n    It initializes all the sharable data in Ignite engine.state.\n    And attach additional processing logics to Ignite engine based on Event-Handler mechanism.\n\n    Users should consider to inherit from `trainer` or `evaluator` to develop more trainers or evaluators.\n\n    Args:\n        device (torch.device): an object representing the device on which to run.\n        max_epochs: the total epoch number for engine to run, validator and evaluator have only 1 epoch.\n        amp (bool): whether to enable auto-mixed-precision training, reserved.\n        data_loader (torch.DataLoader): Ignite engine use data_loader to run, must be torch.DataLoader.\n        prepare_batch (Callable): function to parse image and label for every iteration.\n        iteration_update (Callable): the callable function for every iteration, expect to accept `engine`\n            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n        post_transform (Transform): execute additional transformation for the model output data.\n            Typically, several Tensor based transforms composed by `Compose`.\n        key_metric (ignite.metric): compute metric when every iteration completed, and save average value to\n            engine.state.metrics when epoch completed. key_metric is the main metric to compare and save the\n            checkpoint into files.\n        additional_metrics (dict): more Ignite metrics that also attach to Ignite Engine.\n        handlers (list): every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n\n    """"""\n\n    def __init__(\n        self,\n        device,\n        max_epochs: int,\n        amp,\n        data_loader,\n        prepare_batch=default_prepare_batch,\n        iteration_update=None,\n        post_transform=None,\n        key_metric=None,\n        additional_metrics=None,\n        handlers=None,\n    ):\n        # pytype: disable=invalid-directive\n        # pytype: disable=wrong-arg-count\n        super().__init__(iteration_update if iteration_update is not None else self._iteration)\n        # pytype: enable=invalid-directive\n        # pytype: enable=wrong-arg-count\n        # FIXME:\n        if amp:\n            self.logger.info(""Will add AMP support when PyTorch v1.6 released."")\n        if not isinstance(device, torch.device):\n            raise ValueError(""device must be PyTorch device object."")\n        if not isinstance(data_loader, torch.utils.data.DataLoader):  # type: ignore\n            raise ValueError(""data_loader must be PyTorch DataLoader."")\n\n        # set all sharable data for the workflow based on Ignite engine.state\n        self.state = State(\n            seed=0,\n            iteration=0,\n            epoch=0,\n            max_epochs=max_epochs,\n            epoch_length=-1,\n            output=None,\n            batch=None,\n            metrics={},\n            dataloader=None,\n            device=device,\n            amp=amp,\n            key_metric_name=None,  # we can set many metrics, only use key_metric to compare and save the best model\n            best_metric=-1,\n            best_metric_epoch=-1,\n        )\n        self.data_loader = data_loader\n        self.prepare_batch = prepare_batch\n\n        if post_transform is not None:\n\n            @self.on(Events.ITERATION_COMPLETED)\n            def run_post_transform(engine):\n                engine.state.output = apply_transform(post_transform, engine.state.output)\n\n        metrics = None\n        if key_metric is not None:\n\n            if not isinstance(key_metric, dict):\n                raise ValueError(""key_metric must be a dict object."")\n            self.state.key_metric_name = list(key_metric.keys())[0]\n            metrics = key_metric\n            if additional_metrics is not None and len(additional_metrics) > 0:\n                if not isinstance(additional_metrics, dict):\n                    raise ValueError(""additional_metrics must be a dict object."")\n                metrics.update(additional_metrics)\n            for name, metric in metrics.items():\n                metric.attach(self, name)\n\n            @self.on(Events.EPOCH_COMPLETED)\n            def _compare_metrics(engine):\n                if engine.state.key_metric_name is not None:\n                    current_val_metric = engine.state.metrics[engine.state.key_metric_name]\n                    if current_val_metric > engine.state.best_metric:\n                        self.logger.info(f""Got new best metric of {engine.state.key_metric_name}: {current_val_metric}"")\n                        engine.state.best_metric = current_val_metric\n                        engine.state.best_metric_epoch = engine.state.epoch\n\n        if handlers is not None and len(handlers) > 0:\n            for handler in handlers:\n                handler.attach(self)\n\n    def run(self):\n        """"""\n        Execute training, validation or evaluation based on Ignite Engine.\n\n        """"""\n        super().run(data=self.data_loader, epoch_length=len(self.data_loader))\n\n    @abstractmethod\n    def _iteration(self, engine: Engine, batchdata):\n        """"""\n        Abstract callback function for the processing logic of 1 iteration in Ignite Engine.\n        Need subclass to implement different logics, like SupervisedTrainer/Evaluator, GANTrainer, etc.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n            batchdata (TransformContext, ndarray): input data for this iteration.\n\n        """"""\n        raise NotImplementedError(f""Subclass {self.__class__.__name__} must implement the compute method"")\n'"
monai/handlers/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .checkpoint_loader import CheckpointLoader\nfrom .checkpoint_saver import CheckpointSaver\nfrom .classification_saver import ClassificationSaver\nfrom .mean_dice import MeanDice\nfrom .roc_auc import ROCAUC\nfrom .metric_logger import *\nfrom .segmentation_saver import SegmentationSaver\nfrom .stats_handler import StatsHandler\nfrom .validation_handler import ValidationHandler\nfrom .tensorboard_handlers import TensorBoardImageHandler, TensorBoardStatsHandler\nfrom .lr_schedule_handler import LrScheduleHandler\nfrom .utils import *\n'"
monai/handlers/checkpoint_loader.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport logging\nimport torch\nfrom ignite.engine import Events, Engine\nfrom ignite.handlers import Checkpoint\n\n\nclass CheckpointLoader:\n    """"""\n    CheckpointLoader acts as an Ignite handler to load checkpoint data from file.\n    It can load variables for network, optimizer, lr_scheduler, etc.\n    If saving checkpoint after `torch.nn.DataParallel`, need to save `model.module` instead\n    as PyTorch recommended and then use this loader to load the model.\n\n    Args:\n        load_path (str): the file path of checkpoint, it should be a PyTorch `pth` file.\n        load_dict (dict): target objects that load checkpoint to. examples::\n\n            {\'network\': net, \'optimizer\': optimizer, \'lr_scheduler\': lr_scheduler}\n\n        name (str): identifier of logging.logger to use, if None, defaulting to ``engine.logger``.\n\n    """"""\n\n    def __init__(self, load_path: str, load_dict, name: Optional[str] = None):\n        assert load_path is not None, ""must provide clear path to load checkpoint.""\n        self.load_path = load_path\n        assert load_dict is not None and len(load_dict) > 0, ""must provide target objects to load.""\n        self.logger = None if name is None else logging.getLogger(name)\n        for k, v in load_dict.items():\n            if hasattr(v, ""module""):\n                load_dict[k] = v.module\n        self.load_dict = load_dict\n\n    def attach(self, engine: Engine):\n        if self.logger is None:\n            self.logger = engine.logger\n        return engine.add_event_handler(Events.STARTED, self)\n\n    def __call__(self, engine):\n        checkpoint = torch.load(self.load_path)\n        if len(self.load_dict) == 1:\n            key = list(self.load_dict.keys())[0]\n            if not (key in checkpoint):\n                checkpoint = {key: checkpoint}\n\n        Checkpoint.load_objects(to_load=self.load_dict, checkpoint=checkpoint)\n        self.logger.info(f""Restored all variables from {self.load_path}"")\n'"
monai/handlers/checkpoint_saver.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport logging\nfrom ignite.engine import Events, Engine\nfrom ignite.handlers import ModelCheckpoint\n\n\nclass CheckpointSaver:\n    """"""\n    CheckpointSaver acts as an Ignite handler to save checkpoint data into files.\n    It supports to save according to metrics result, epoch number, iteration number\n    and last model or exception.\n\n    Args:\n        save_dir (str): the target directory to save the checkpoints.\n        save_dict (dict): source objects that save to the checkpoint. examples::\n\n            {\'network\': net, \'optimizer\': optimizer, \'lr_scheduler\': lr_scheduler}\n\n        name (str): identifier of logging.logger to use, if None, defaulting to ``engine.logger``.\n        file_prefix (str): prefix for the filenames to which objects will be saved.\n        save_final (bool): whether to save checkpoint or session at final iteration or exception.\n        save_key_metric (bool): whether to save checkpoint or session when the value of key_metric is\n            higher than all the previous values during training.keep 4 decimal places of metric,\n            checkpoint name is: {file_prefix}_key_metric=0.XXXX.pth.\n        key_metric_name (str): the name of key_metric in ignite metrics dictionary.\n            if None, use `engine.state.key_metric` instead.\n        key_metric_n_saved: save top N checkpoints or sessions, sorted by the value of key\n            metric in descending order.\n        epoch_level (bool): save checkpoint during training for every N epochs or every N iterations.\n            `True` is epoch level, `False` is iteration level.\n        save_interval: save checkpoint every N epochs, default is 0 to save no checkpoint.\n        n_saved: save latest N checkpoints of epoch level or iteration level, \'None\' is to save all.\n\n    Note:\n        CheckpointHandler can be used during training, validation or evaluation.\n        example of saved files:\n\n            - checkpoint_iteration=400.pth\n            - checkpoint_iteration=800.pth\n            - checkpoint_epoch=1.pth\n            - checkpoint_final_iteration=1000.pth\n            - checkpoint_key_metric=0.9387.pth\n\n    """"""\n\n    def __init__(\n        self,\n        save_dir: str,\n        save_dict,\n        name: Optional[str] = None,\n        file_prefix: str = """",\n        save_final: bool = False,\n        save_key_metric: bool = False,\n        key_metric_name: Optional[str] = None,\n        key_metric_n_saved: int = 1,\n        epoch_level: bool = True,\n        save_interval: int = 0,\n        n_saved: Optional[int] = None,\n    ):\n        assert save_dir is not None, ""must provide directory to save the checkpoints.""\n        self.save_dir = save_dir\n        assert save_dict is not None and len(save_dict) > 0, ""must provide source objects to save.""\n        for k, v in save_dict.items():\n            if hasattr(v, ""module""):\n                save_dict[k] = v.module\n        self.save_dict = save_dict\n        self.logger = None if name is None else logging.getLogger(name)\n        self.epoch_level = epoch_level\n        self.save_interval = save_interval\n        self._final_checkpoint = self._key_metric_checkpoint = self._interval_checkpoint = None\n\n        if save_final:\n\n            def _final_func(engine):\n                return engine.state.iteration\n\n            self._final_checkpoint = ModelCheckpoint(\n                self.save_dir,\n                file_prefix,\n                score_function=_final_func,\n                score_name=""final_iteration"",\n                require_empty=False,\n            )\n        if save_key_metric:\n\n            def _score_func(engine):\n                if isinstance(key_metric_name, str):\n                    metric_name = key_metric_name\n                elif hasattr(engine.state, ""key_metric_name"") and isinstance(engine.state.key_metric_name, str):\n                    metric_name = engine.state.key_metric_name\n                else:\n                    raise ValueError(""must provde key_metric_name to save best validation model."")\n                return round(engine.state.metrics[metric_name], 4)\n\n            self._key_metric_checkpoint = ModelCheckpoint(\n                self.save_dir,\n                file_prefix,\n                score_function=_score_func,\n                score_name=""key_metric"",\n                n_saved=key_metric_n_saved,\n                require_empty=False,\n            )\n        if save_interval > 0:\n\n            def _interval_func(engine):\n                return engine.state.epoch if self.epoch_level else engine.state.iteration\n\n            self._interval_checkpoint = ModelCheckpoint(\n                self.save_dir,\n                file_prefix,\n                score_function=_interval_func,\n                score_name=""epoch"" if self.epoch_level else ""iteration"",\n                n_saved=n_saved,\n                require_empty=False,\n            )\n\n    def attach(self, engine: Engine):\n        if self.logger is None:\n            self.logger = engine.logger\n        if self._final_checkpoint is not None:\n            engine.add_event_handler(Events.COMPLETED, self.completed)\n            engine.add_event_handler(Events.EXCEPTION_RAISED, self.exception_raised)\n        if self._key_metric_checkpoint is not None:\n            engine.add_event_handler(Events.EPOCH_COMPLETED, self.metrics_completed)\n        if self._interval_checkpoint is not None:\n            if self.epoch_level:\n                engine.add_event_handler(Events.EPOCH_COMPLETED(every=self.save_interval), self.interval_completed)\n            else:\n                engine.add_event_handler(Events.ITERATION_COMPLETED(every=self.save_interval), self.interval_completed)\n\n    def completed(self, engine):\n        """"""Callback for train or validation/evaluation completed Event.\n        Save final checkpoint if configure save_final is True.\n\n        """"""\n        self._final_checkpoint(engine, self.save_dict)\n        self.logger.info(f""Train completed, saved final checkpoint: {self._final_checkpoint.last_checkpoint}"")\n\n    def exception_raised(self, engine, e):\n        """"""Callback for train or validation/evaluation exception raised Event.\n        Save current data as final checkpoint if configure save_final is True.\n\n        """"""\n        self._final_checkpoint(engine, self.save_dict)\n        self.logger.info(f""Exception_raised, saved exception checkpoint: {self._final_checkpoint.last_checkpoint}"")\n\n    def metrics_completed(self, engine):\n        """"""Callback to compare metrics and save models in train or validation when epoch completed.\n\n        """"""\n        self._key_metric_checkpoint(engine, self.save_dict)\n\n    def interval_completed(self, engine):\n        """"""Callback for train epoch/iteration completed Event.\n        Save checkpoint if configure save_interval = N\n\n        """"""\n        self._interval_checkpoint(engine, self.save_dict)\n        if self.epoch_level:\n            self.logger.info(f""Saved checkpoint at epoch: {engine.state.epoch}"")\n        else:\n            self.logger.info(f""Saved checkpoint at iteration: {engine.state.iteration}"")\n'"
monai/handlers/classification_saver.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Callable\n\nfrom ignite.engine import Events, Engine\nimport logging\nfrom monai.data import CSVSaver\n\n\nclass ClassificationSaver:\n    """"""\n    Event handler triggered on completing every iteration to save the classification predictions as CSV file.\n    """"""\n\n    def __init__(\n        self,\n        output_dir: str = ""./"",\n        filename: str = ""predictions.csv"",\n        overwrite: bool = True,\n        batch_transform: Callable = lambda x: x,\n        output_transform: Callable = lambda x: x,\n        name: Optional[str] = None,\n    ):\n        """"""\n        Args:\n            output_dir (str): output CSV file directory.\n            filename (str): name of the saved CSV file name.\n            overwrite (bool): whether to overwriting existing CSV file content. If we are not overwriting,\n                then we check if the results have been previously saved, and load them to the prediction_dict.\n            batch_transform (Callable): a callable that is used to transform the\n                ignite.engine.batch into expected format to extract the meta_data dictionary.\n            output_transform (Callable): a callable that is used to transform the\n                ignite.engine.output into the form expected model prediction data.\n                The first dimension of this transform\'s output will be treated as the\n                batch dimension. Each item in the batch will be saved individually.\n            name (str): identifier of logging.logger to use, defaulting to `engine.logger`.\n\n        """"""\n        self.saver = CSVSaver(output_dir, filename, overwrite)\n        self.batch_transform = batch_transform\n        self.output_transform = output_transform\n\n        self.logger = None if name is None else logging.getLogger(name)\n\n    def attach(self, engine: Engine):\n        if self.logger is None:\n            self.logger = engine.logger\n        if not engine.has_event_handler(self, Events.ITERATION_COMPLETED):\n            engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n        if not engine.has_event_handler(self.saver.finalize, Events.COMPLETED):\n            engine.add_event_handler(Events.COMPLETED, lambda engine: self.saver.finalize())\n\n    def __call__(self, engine: Engine):\n        """"""\n        This method assumes self.batch_transform will extract metadata from the input batch.\n\n        """"""\n        meta_data = self.batch_transform(engine.state.batch)\n        engine_output = self.output_transform(engine.state.output)\n        self.saver.save_batch(engine_output, meta_data)\n'"
monai/handlers/lr_schedule_handler.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Callable\n\nimport logging\nfrom ignite.engine import Events, Engine\nfrom monai.utils import ensure_tuple\n\n\nclass LrScheduleHandler:\n    """"""\n    Ignite handler to update the Learning Rate based on PyTorch LR scheduler.\n    """"""\n\n    def __init__(\n        self,\n        lr_scheduler,\n        print_lr: bool = True,\n        name: Optional[str] = None,\n        epoch_level: bool = True,\n        step_transform: Callable = lambda engine: (),\n    ):\n        """"""\n        Args:\n            lr_scheduler (torch.optim.lr_scheduler): typically, lr_scheduler should be PyTorch\n                lr_scheduler object. If customized version, must have `step` and `get_last_lr` methods.\n            print_lr (bool): whether to print out the latest learning rate with logging.\n            name (str): identifier of logging.logger to use, if None, defaulting to ``engine.logger``.\n            epoch_level (bool): execute lr_scheduler.step() after every epoch or every iteration.\n                `True` is epoch level, `False` is iteration level.\n            step_transform (Callable): a callable that is used to transform the information from `engine`\n                to expected input data of lr_scheduler.step() function if necessary.\n\n        """"""\n        self.lr_scheduler = lr_scheduler\n        self.print_lr = print_lr\n        self.logger = None if name is None else logging.getLogger(name)\n        self.epoch_level = epoch_level\n        if not callable(step_transform):\n            raise ValueError(""argument `step_transform` must be a callable."")\n        self.step_transform = step_transform\n\n    def attach(self, engine: Engine):\n        if self.logger is None:\n            self.logger = engine.logger\n        if self.epoch_level:\n            engine.add_event_handler(Events.EPOCH_COMPLETED, self)\n        else:\n            engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n\n    def __call__(self, engine):\n        args = ensure_tuple(self.step_transform(engine))\n        self.lr_scheduler.step(*args)\n        if self.print_lr:\n            self.logger.info(f""Current learning rate: {self.lr_scheduler._last_lr[0]}"")\n'"
monai/handlers/mean_dice.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Callable, Optional, Sequence, Union\n\nimport torch\nfrom ignite.exceptions import NotComputableError\nfrom ignite.metrics import Metric\nfrom ignite.metrics.metric import reinit__is_reduced, sync_all_reduce\n\nfrom monai.metrics import compute_meandice\n\n\nclass MeanDice(Metric):\n    """"""\n    Computes Dice score metric from full size Tensor and collects average over batch, class-channels, iterations.\n    """"""\n\n    def __init__(\n        self,\n        include_background: bool = True,\n        to_onehot_y: bool = False,\n        mutually_exclusive: bool = False,\n        sigmoid: bool = False,\n        logit_thresh: float = 0.5,\n        output_transform: Callable = lambda x: x,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n\n        Args:\n            include_background (Bool): whether to include dice computation on the first channel of the predicted output.\n                Defaults to True.\n            to_onehot_y (Bool): whether to convert the output prediction into the one-hot format. Defaults to False.\n            mutually_exclusive (Bool): if True, the output prediction will be converted into a binary matrix using\n                a combination of argmax and to_onehot. Defaults to False.\n            sigmoid (Bool): whether to add sigmoid function to the output prediction before computing Dice.\n                Defaults to False.\n            logit_thresh (Float): the threshold value to round value to 0.0 and 1.0. Defaults to None (no thresholding).\n            output_transform (Callable): transform the ignite.engine.state.output into [y_pred, y] pair.\n            device (torch.device): device specification in case of distributed computation usage.\n\n        See also:\n            :py:meth:`monai.metrics.meandice.compute_meandice`\n        """"""\n        super().__init__(output_transform, device=device)\n        self.include_background = include_background\n        self.to_onehot_y = to_onehot_y\n        self.mutually_exclusive = mutually_exclusive\n        self.sigmoid = sigmoid\n        self.logit_thresh = logit_thresh\n\n        self._sum = 0\n        self._num_examples = 0\n\n    @reinit__is_reduced\n    def reset(self):\n        self._sum = 0\n        self._num_examples = 0\n\n    @reinit__is_reduced\n    def update(self, output: Sequence[Union[torch.Tensor, dict]]):\n        if not len(output) == 2:\n            raise ValueError(""MeanDice metric can only support y_pred and y."")\n        y_pred, y = output\n        scores = compute_meandice(\n            y_pred,\n            y,\n            self.include_background,\n            self.to_onehot_y,\n            self.mutually_exclusive,\n            self.sigmoid,\n            self.logit_thresh,\n        )\n\n        # add all items in current batch\n        for batch in scores:\n            not_nan = ~torch.isnan(batch)\n            if not_nan.sum() == 0:\n                continue\n            class_avg = batch[not_nan].mean().item()\n            self._sum += class_avg\n            self._num_examples += 1\n\n    @sync_all_reduce(""_sum"", ""_num_examples"")\n    def compute(self):\n        if self._num_examples == 0:\n            raise NotComputableError(""MeanDice must have at least one example before it can be computed."")\n        return self._sum / self._num_examples\n'"
monai/handlers/metric_logger.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import defaultdict\nfrom typing import Callable\n\nfrom ignite.engine import Events, Engine\n\n\nclass MetricLogger:\n    def __init__(self, loss_transform: Callable = lambda x: x, metric_transform: Callable = lambda x: x):\n        self.loss_transform = loss_transform\n        self.metric_transform = metric_transform\n        self.loss: list = []\n        self.metrics: defaultdict = defaultdict(list)\n\n    def attach(self, engine: Engine):\n        return engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n\n    def __call__(self, engine: Engine):\n        self.loss.append(self.loss_transform(engine.state.output))\n\n        for m, v in engine.state.metrics.items():\n            v = self.metric_transform(v)\n            #             # metrics may not be added on the first timestep, pad the list if this is the case\n            #             # so that each metric list is the same length as self.loss\n            #             if len(self.metrics[m])==0:\n            #                 self.metrics[m].append([v[0]]*len(self.loss))\n\n            self.metrics[m].append(v)\n\n\nmetriclogger = MetricLogger\n'"
monai/handlers/roc_auc.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Sequence, Union, Callable\n\nimport torch\nfrom ignite.metrics import Metric\n\nfrom monai.metrics import compute_roc_auc\n\n\nclass ROCAUC(Metric):\n    """"""\n    Computes Area Under the Receiver Operating Characteristic Curve (ROC AUC).\n    accumulating predictions and the ground-truth during an epoch and applying `compute_roc_auc`.\n\n    Args:\n        to_onehot_y: whether to convert `y` into the one-hot format. Defaults to False.\n        softmax: whether to add softmax function to `y_pred` before computation. Defaults to False.\n        average (`macro|weighted|micro|None`): type of averaging performed if not binary classification.\n            Default is \'macro\'.\n\n            - \'macro\': calculate metrics for each label, and find their unweighted mean.\n              This does not take label imbalance into account.\n            - \'weighted\': calculate metrics for each label, and find their average,\n              weighted by support (the number of true instances for each label).\n            - \'micro\': calculate metrics globally by considering each element of the label\n              indicator matrix as a label.\n            - None: the scores for each class are returned.\n\n        output_transform (callable, optional): a callable that is used to transform the\n            :class:`~ignite.engine.Engine` `process_function` output into the\n            form expected by the metric. This can be useful if, for example, you have a multi-output model and\n            you want to compute the metric with respect to one of the outputs.\n        device: device specification in case of distributed computation usage.\n\n    Note:\n        ROCAUC expects y to be comprised of 0\'s and 1\'s.  y_pred must either be probability estimates or confidence values.\n\n    """"""\n\n    def __init__(\n        self,\n        to_onehot_y: bool = False,\n        softmax: bool = False,\n        average: str = ""macro"",\n        output_transform: Callable = lambda x: x,\n        device: Optional[Union[str, torch.device]] = None,\n    ):\n        super().__init__(output_transform, device=device)\n        self.to_onehot_y = to_onehot_y\n        self.softmax = softmax\n        self.average = average\n\n    def reset(self):\n        self._predictions = []\n        self._targets = []\n\n    def update(self, output: Sequence[torch.Tensor]):\n        y_pred, y = output\n        if y_pred.ndimension() not in (1, 2):\n            raise ValueError(""predictions should be of shape (batch_size, n_classes) or (batch_size, )."")\n        if y.ndimension() not in (1, 2):\n            raise ValueError(""targets should be of shape (batch_size, n_classes) or (batch_size, )."")\n\n        self._predictions.append(y_pred.clone())\n        self._targets.append(y.clone())\n\n    def compute(self):\n        _prediction_tensor = torch.cat(self._predictions, dim=0)\n        _target_tensor = torch.cat(self._targets, dim=0)\n        return compute_roc_auc(_prediction_tensor, _target_tensor, self.to_onehot_y, self.softmax, self.average)\n'"
monai/handlers/segmentation_saver.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Union, Callable\n\nimport numpy as np\nfrom ignite.engine import Events, Engine\nimport logging\nfrom monai.data import NiftiSaver, PNGSaver\n\n\nclass SegmentationSaver:\n    """"""\n    Event handler triggered on completing every iteration to save the segmentation predictions into files.\n    """"""\n\n    def __init__(\n        self,\n        output_dir: str = ""./"",\n        output_postfix: str = ""seg"",\n        output_ext: str = "".nii.gz"",\n        resample: bool = True,\n        interp_order: int = 0,\n        mode: str = ""constant"",\n        cval: Union[int, float] = 0,\n        scale: bool = False,\n        dtype: Optional[np.dtype] = None,\n        batch_transform: Callable = lambda x: x,\n        output_transform: Callable = lambda x: x,\n        name: Optional[str] = None,\n    ):\n        """"""\n        Args:\n            output_dir (str): output image directory.\n            output_postfix (str): a string appended to all output file names.\n            output_ext (str): output file extension name.\n            resample (bool): whether to resample before saving the data array.\n            interp_order: the order of the spline interpolation, default is 0.\n                The order has to be in the range 0 - 5.\n                https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.affine_transform.html\n                This option is used when `resample = True`.\n            mode (`reflect|constant|nearest|mirror|wrap`):\n                The mode parameter determines how the input array is extended beyond its boundaries.\n                This option is used when `resample = True`.\n            cval (scalar): Value to fill past edges of input if mode is ""constant"". Default is 0.0.\n                This option is used when `resample = True`.\n            scale (bool): whether to scale data with 255 and convert to uint8 for data in range [0, 1].\n                It\'s used for PNG format only.\n            dtype (np.dtype, optional): convert the image data to save to this data type.\n                If None, keep the original type of data. It\'s used for Nifti format only.\n            batch_transform (Callable): a callable that is used to transform the\n                ignite.engine.batch into expected format to extract the meta_data dictionary.\n            output_transform (Callable): a callable that is used to transform the\n                ignite.engine.output into the form expected image data.\n                The first dimension of this transform\'s output will be treated as the\n                batch dimension. Each item in the batch will be saved individually.\n            name (str): identifier of logging.logger to use, defaulting to `engine.logger`.\n\n        """"""\n        self.saver: Union[NiftiSaver, PNGSaver]\n        if output_ext in ("".nii.gz"", "".nii""):\n            self.saver = NiftiSaver(output_dir, output_postfix, output_ext, resample, interp_order, mode, cval, dtype)\n        elif output_ext == "".png"":\n            self.saver = PNGSaver(output_dir, output_postfix, output_ext, resample, interp_order, mode, cval, scale)\n        self.batch_transform = batch_transform\n        self.output_transform = output_transform\n\n        self.logger = None if name is None else logging.getLogger(name)\n\n    def attach(self, engine: Engine):\n        if self.logger is None:\n            self.logger = engine.logger\n        if not engine.has_event_handler(self, Events.ITERATION_COMPLETED):\n            engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n\n    def __call__(self, engine):\n        """"""\n        This method assumes self.batch_transform will extract metadata from the input batch.\n        Output file datatype is determined from ``engine.state.output.dtype``.\n\n        """"""\n        meta_data = self.batch_transform(engine.state.batch)\n        engine_output = self.output_transform(engine.state.output)\n        self.saver.save_batch(engine_output, meta_data)\n        self.logger.info(""saved all the model outputs into files."")\n'"
monai/handlers/stats_handler.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport warnings\nimport logging\nimport torch\nfrom ignite.engine import Engine, Events\nfrom monai.utils import is_scalar\n\nDEFAULT_KEY_VAL_FORMAT = ""{}: {:.4f} ""\nDEFAULT_TAG = ""Loss""\n\n\nclass StatsHandler(object):\n    """"""\n    StatsHandler defines a set of Ignite Event-handlers for all the log printing logics.\n    It\'s can be used for any Ignite Engine(trainer, validator and evaluator).\n    And it can support logging for epoch level and iteration level with pre-defined loggers.\n\n    Default behaviors:\n        - When EPOCH_COMPLETED, logs ``engine.state.metrics`` using ``self.logger``.\n        - When ITERATION_COMPLETED, logs\n          ``self.output_transform(engine.state.output)`` using ``self.logger``.\n\n    """"""\n\n    def __init__(\n        self,\n        epoch_print_logger=None,\n        iteration_print_logger=None,\n        output_transform=lambda x: x,\n        global_epoch_transform=lambda x: x,\n        name=None,\n        tag_name=DEFAULT_TAG,\n        key_var_format=DEFAULT_KEY_VAL_FORMAT,\n        logger_handler=None,\n    ):\n        """"""\n\n        Args:\n            epoch_print_logger (Callable): customized callable printer for epoch level logging.\n                Must accept parameter ""engine"", use default printer if None.\n            iteration_print_logger (Callable): customized callable printer for iteration level logging.\n                Must accept parameter ""engine"", use default printer if None.\n            output_transform (Callable): a callable that is used to transform the\n                ``ignite.engine.output`` into a scalar to print, or a dictionary of {key: scalar}.\n                In the latter case, the output string will be formatted as key: value.\n                By default this value logging happens when every iteration completed.\n            global_epoch_transform (Callable): a callable that is used to customize global epoch number.\n                For example, in evaluation, the evaluator engine might want to print synced epoch number\n                with the trainer engine.\n            name (str): identifier of logging.logger to use, defaulting to ``engine.logger``.\n            tag_name (string): when iteration output is a scalar, tag_name is used to print\n                tag_name: scalar_value to logger. Defaults to ``\'Loss\'``.\n            key_var_format (string): a formatting string to control the output string format of key: value.\n            logger_handler (logging.handler): add additional handler to handle the stats data: save to file, etc.\n                add existing python logging handlers: https://docs.python.org/3/library/logging.handlers.html\n        """"""\n\n        self.epoch_print_logger = epoch_print_logger\n        self.iteration_print_logger = iteration_print_logger\n        self.output_transform = output_transform\n        self.global_epoch_transform = global_epoch_transform\n        self.logger = None if name is None else logging.getLogger(name)\n\n        self.tag_name = tag_name\n        self.key_var_format = key_var_format\n        if logger_handler is not None:\n            self.logger.addHandler(logger_handler)  # pytype: disable=attribute-error\n\n    def attach(self, engine: Engine):\n        """"""\n        Register a set of Ignite Event-Handlers to a specified Ignite engine.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        if self.logger is None:\n            self.logger = engine.logger\n        if not engine.has_event_handler(self.iteration_completed, Events.ITERATION_COMPLETED):\n            engine.add_event_handler(Events.ITERATION_COMPLETED, self.iteration_completed)\n        if not engine.has_event_handler(self.epoch_completed, Events.EPOCH_COMPLETED):\n            engine.add_event_handler(Events.EPOCH_COMPLETED, self.epoch_completed)\n        if not engine.has_event_handler(self.exception_raised, Events.EXCEPTION_RAISED):\n            engine.add_event_handler(Events.EXCEPTION_RAISED, self.exception_raised)\n\n    def epoch_completed(self, engine: Engine):\n        """"""\n        Handler for train or validation/evaluation epoch completed Event.\n        Print epoch level log, default values are from Ignite state.metrics dict.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        if self.epoch_print_logger is not None:\n            self.epoch_print_logger(engine)\n        else:\n            self._default_epoch_print(engine)\n\n    def iteration_completed(self, engine: Engine):\n        """"""\n        Handler for train or validation/evaluation iteration completed Event.\n        Print iteration level log, default values are from Ignite state.logs dict.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        if self.iteration_print_logger is not None:\n            self.iteration_print_logger(engine)\n        else:\n            self._default_iteration_print(engine)\n\n    def exception_raised(self, engine: Engine, e):\n        """"""\n        Handler for train or validation/evaluation exception raised Event.\n        Print the exception information and traceback.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n            e (Exception): the exception caught in Ignite during engine.run().\n\n        """"""\n        self.logger.exception(f""Exception: {e}"")\n        # import traceback\n        # traceback.print_exc()\n\n    def _default_epoch_print(self, engine: Engine):\n        """"""\n        Execute epoch level log operation based on Ignite engine.state data.\n        print the values from Ignite state.metrics dict.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        prints_dict = engine.state.metrics\n        if not prints_dict:\n            return\n        current_epoch = self.global_epoch_transform(engine.state.epoch)\n\n        out_str = f""Epoch[{current_epoch}] Metrics -- ""\n        for name in sorted(prints_dict):\n            value = prints_dict[name]\n            out_str += self.key_var_format.format(name, value)\n        self.logger.info(out_str)\n\n        if hasattr(engine.state, ""key_metric_name""):\n            if hasattr(engine.state, ""best_metric"") and hasattr(engine.state, ""best_metric_epoch""):\n                out_str = f""Key metric: {engine.state.key_metric_name} ""\n                out_str += f""best value: {engine.state.best_metric} at epoch: {engine.state.best_metric_epoch}""\n        self.logger.info(out_str)\n\n    def _default_iteration_print(self, engine: Engine):\n        """"""\n        Execute iteration log operation based on Ignite engine.state data.\n        Print the values from Ignite state.logs dict.\n        Default behavior is to print loss from output[1], skip if output[1] is not loss.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        loss = self.output_transform(engine.state.output)\n        if loss is None:\n            return  # no printing if the output is empty\n\n        out_str = """"\n        if isinstance(loss, dict):  # print dictionary items\n            for name in sorted(loss):\n                value = loss[name]\n                if not is_scalar(value):\n                    warnings.warn(\n                        ""ignoring non-scalar output in StatsHandler,""\n                        "" make sure `output_transform(engine.state.output)` returns""\n                        "" a scalar or dictionary of key and scalar pairs to avoid this warning.""\n                        "" {}:{}"".format(name, type(value))\n                    )\n                    continue  # not printing multi dimensional output\n                out_str += self.key_var_format.format(name, value.item() if torch.is_tensor(value) else value)\n        else:\n            if is_scalar(loss):  # not printing multi dimensional output\n                out_str += self.key_var_format.format(self.tag_name, loss.item() if torch.is_tensor(loss) else loss)\n            else:\n                warnings.warn(\n                    ""ignoring non-scalar output in StatsHandler,""\n                    "" make sure `output_transform(engine.state.output)` returns""\n                    "" a scalar or a dictionary of key and scalar pairs to avoid this warning.""\n                    "" {}"".format(type(loss))\n                )\n\n        if not out_str:\n            return  # no value to print\n\n        num_iterations = engine.state.epoch_length\n        current_iteration = (engine.state.iteration - 1) % num_iterations + 1\n        current_epoch = engine.state.epoch\n        num_epochs = engine.state.max_epochs\n\n        base_str = f""Epoch: {current_epoch}/{num_epochs}, Iter: {current_iteration}/{num_iterations} --""\n\n        self.logger.info("" "".join([base_str, out_str]))\n'"
monai/handlers/tensorboard_handlers.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Callable\n\nimport numpy as np\nimport warnings\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom ignite.engine import Engine, Events\nfrom monai.visualize import plot_2d_or_3d_image\nfrom monai.utils.misc import is_scalar\n\nDEFAULT_TAG = ""Loss""\n\n\nclass TensorBoardStatsHandler(object):\n    """"""\n    TensorBoardStatsHandler defines a set of Ignite Event-handlers for all the TensorBoard logics.\n    It\'s can be used for any Ignite Engine(trainer, validator and evaluator).\n    And it can support both epoch level and iteration level with pre-defined TensorBoard event writer.\n    The expected data source is Ignite ``engine.state.output`` and ``engine.state.metrics``.\n\n    Default behaviors:\n        - When EPOCH_COMPLETED, write each dictionary item in\n          ``engine.state.metrics`` to TensorBoard.\n        - When ITERATION_COMPLETED, write each dictionary item in\n          ``self.output_transform(engine.state.output)`` to TensorBoard.\n    """"""\n\n    def __init__(\n        self,\n        summary_writer: Optional[SummaryWriter] = None,\n        log_dir: str = ""./runs"",\n        epoch_event_writer: Optional[Callable] = None,\n        iteration_event_writer: Optional[Callable] = None,\n        output_transform: Callable = lambda x: x,\n        global_epoch_transform: Callable = lambda x: x,\n        tag_name: str = DEFAULT_TAG,\n    ):\n        """"""\n        Args:\n            summary_writer (SummaryWriter): user can specify TensorBoard SummaryWriter,\n                default to create a new writer.\n            log_dir (str): if using default SummaryWriter, write logs to this directory, default is `./runs`.\n            epoch_event_writer (Callable): customized callable TensorBoard writer for epoch level.\n                Must accept parameter ""engine"" and ""summary_writer"", use default event writer if None.\n            iteration_event_writer (Callable): customized callable TensorBoard writer for iteration level.\n                Must accept parameter ""engine"" and ""summary_writer"", use default event writer if None.\n            output_transform (Callable): a callable that is used to transform the\n                ``ignite.engine.output`` into a scalar to plot, or a dictionary of {key: scalar}.\n                In the latter case, the output string will be formatted as key: value.\n                By default this value plotting happens when every iteration completed.\n            global_epoch_transform (Callable): a callable that is used to customize global epoch number.\n                For example, in evaluation, the evaluator engine might want to use trainer engines epoch number\n                when plotting epoch vs metric curves.\n            tag_name (string): when iteration output is a scalar, tag_name is used to plot, defaults to ``\'Loss\'``.\n        """"""\n        self._writer = SummaryWriter(log_dir=log_dir) if summary_writer is None else summary_writer\n        self.epoch_event_writer = epoch_event_writer\n        self.iteration_event_writer = iteration_event_writer\n        self.output_transform = output_transform\n        self.global_epoch_transform = global_epoch_transform\n        self.tag_name = tag_name\n\n    def attach(self, engine: Engine):\n        """"""\n        Register a set of Ignite Event-Handlers to a specified Ignite engine.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        if not engine.has_event_handler(self.iteration_completed, Events.ITERATION_COMPLETED):\n            engine.add_event_handler(Events.ITERATION_COMPLETED, self.iteration_completed)\n        if not engine.has_event_handler(self.epoch_completed, Events.EPOCH_COMPLETED):\n            engine.add_event_handler(Events.EPOCH_COMPLETED, self.epoch_completed)\n\n    def epoch_completed(self, engine: Engine):\n        """"""\n        Handler for train or validation/evaluation epoch completed Event.\n        Write epoch level events, default values are from Ignite state.metrics dict.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        if self.epoch_event_writer is not None:\n            self.epoch_event_writer(engine, self._writer)\n        else:\n            self._default_epoch_writer(engine, self._writer)\n\n    def iteration_completed(self, engine: Engine):\n        """"""\n        Handler for train or validation/evaluation iteration completed Event.\n        Write iteration level events, default values are from Ignite state.logs dict.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n\n        """"""\n        if self.iteration_event_writer is not None:\n            self.iteration_event_writer(engine, self._writer)\n        else:\n            self._default_iteration_writer(engine, self._writer)\n\n    def _default_epoch_writer(self, engine: Engine, writer: SummaryWriter):\n        """"""\n        Execute epoch level event write operation based on Ignite engine.state data.\n        Default is to write the values from Ignite state.metrics dict.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n            writer (SummaryWriter): TensorBoard writer, created in TensorBoardHandler.\n\n        """"""\n        current_epoch = self.global_epoch_transform(engine.state.epoch)\n        summary_dict = engine.state.metrics\n        for name, value in summary_dict.items():\n            writer.add_scalar(name, value, current_epoch)\n        writer.flush()\n\n    def _default_iteration_writer(self, engine: Engine, writer: SummaryWriter):\n        """"""\n        Execute iteration level event write operation based on Ignite engine.state data.\n        Default is to write the loss value of current iteration.\n\n        Args:\n            engine (ignite.engine): Ignite Engine, it can be a trainer, validator or evaluator.\n            writer (SummaryWriter): TensorBoard writer, created in TensorBoardHandler.\n\n        """"""\n        loss = self.output_transform(engine.state.output)\n        if loss is None:\n            return  # do nothing if output is empty\n        if isinstance(loss, dict):\n            for name in sorted(loss):\n                value = loss[name]\n                if not is_scalar(value):\n                    warnings.warn(\n                        ""ignoring non-scalar output in TensorBoardStatsHandler,""\n                        "" make sure `output_transform(engine.state.output)` returns""\n                        "" a scalar or dictionary of key and scalar pairs to avoid this warning.""\n                        "" {}:{}"".format(name, type(value))\n                    )\n                    continue  # not plot multi dimensional output\n                writer.add_scalar(name, value.item() if torch.is_tensor(value) else value, engine.state.iteration)\n        elif is_scalar(loss):  # not printing multi dimensional output\n            writer.add_scalar(self.tag_name, loss.item() if torch.is_tensor(loss) else loss, engine.state.iteration)\n        else:\n            warnings.warn(\n                ""ignoring non-scalar output in TensorBoardStatsHandler,""\n                "" make sure `output_transform(engine.state.output)` returns""\n                "" a scalar or a dictionary of key and scalar pairs to avoid this warning.""\n                "" {}"".format(type(loss))\n            )\n        writer.flush()\n\n\nclass TensorBoardImageHandler(object):\n    """"""\n    TensorBoardImageHandler is an Ignite Event handler that can visualise images, labels and outputs as 2D/3D images.\n    2D output (shape in Batch, channel, H, W) will be shown as simple image using the first element in the batch,\n    for 3D to ND output (shape in Batch, channel, H, W, D) input, each of ``self.max_channels`` number of images\'\n    last three dimensions will be shown as animated GIF along the last axis (typically Depth).\n\n    It can be used for any Ignite Engine (trainer, validator and evaluator).\n    User can easily add it to engine for any expected Event, for example: ``EPOCH_COMPLETED``,\n    ``ITERATION_COMPLETED``. The expected data source is ignite\'s ``engine.state.batch`` and ``engine.state.output``.\n\n    Default behavior:\n        - Show y_pred as images (GIF for 3D) on TensorBoard when Event triggered,\n        - Need to use ``batch_transform`` and ``output_transform`` to specify\n          how many images to show and show which channel.\n        - Expects ``batch_transform(engine.state.batch)`` to return data\n          format: (image[N, channel, ...], label[N, channel, ...]).\n        - Expects ``output_transform(engine.state.output)`` to return a torch\n          tensor in format (y_pred[N, channel, ...], loss).\n\n     """"""\n\n    def __init__(\n        self,\n        summary_writer: Optional[SummaryWriter] = None,\n        log_dir: str = ""./runs"",\n        interval: int = 1,\n        epoch_level=True,\n        batch_transform: Callable = lambda x: x,\n        output_transform: Callable = lambda x: x,\n        global_iter_transform: Callable = lambda x: x,\n        index: int = 0,\n        max_channels: int = 1,\n        max_frames: int = 64,\n    ):\n        """"""\n        Args:\n            summary_writer (SummaryWriter): user can specify TensorBoard SummaryWriter,\n                default to create a new writer.\n            log_dir (str): if using default SummaryWriter, write logs to this directory, default is `./runs`.\n            interval: plot content from engine.state every N epochs or every N iterations, default is 1.\n            epoch_level (bool): plot content from engine.state every N epochs or N iterations. `True` is epoch level,\n                `False` is iteration level.\n            batch_transform (Callable): a callable that is used to transform the\n                ``ignite.engine.batch`` into expected format to extract several label data.\n            output_transform (Callable): a callable that is used to transform the\n                ``ignite.engine.output`` into expected format to extract several output data.\n            global_iter_transform (Callable): a callable that is used to customize global step number for TensorBoard.\n                For example, in evaluation, the evaluator engine needs to know current epoch from trainer.\n            index: plot which element in a data batch, default is the first element.\n            max_channels: number of channels to plot.\n            max_frames: number of frames for 2D-t plot.\n        """"""\n        self._writer = SummaryWriter(log_dir=log_dir) if summary_writer is None else summary_writer\n        self.interval = interval\n        self.epoch_level = epoch_level\n        self.batch_transform = batch_transform\n        self.output_transform = output_transform\n        self.global_iter_transform = global_iter_transform\n        self.index = index\n        self.max_frames = max_frames\n        self.max_channels = max_channels\n\n    def attach(self, engine):\n        if self.epoch_level:\n            engine.add_event_handler(Events.EPOCH_COMPLETED(every=self.interval), self)\n        else:\n            engine.add_event_handler(Events.ITERATION_COMPLETED(every=self.interval), self)\n\n    def __call__(self, engine: Engine):\n        step = self.global_iter_transform(engine.state.epoch if self.epoch_level else engine.state.iteration)\n        show_images = self.batch_transform(engine.state.batch)[0]\n        if torch.is_tensor(show_images):\n            show_images = show_images.detach().cpu().numpy()\n        if show_images is not None:\n            if not isinstance(show_images, np.ndarray):\n                raise ValueError(""output_transform(engine.state.output)[0] must be an ndarray or tensor."")\n            plot_2d_or_3d_image(\n                show_images, step, self._writer, self.index, self.max_channels, self.max_frames, ""input_0""\n            )\n\n        show_labels = self.batch_transform(engine.state.batch)[1]\n        if torch.is_tensor(show_labels):\n            show_labels = show_labels.detach().cpu().numpy()\n        if show_labels is not None:\n            if not isinstance(show_labels, np.ndarray):\n                raise ValueError(""batch_transform(engine.state.batch)[1] must be an ndarray or tensor."")\n            plot_2d_or_3d_image(\n                show_labels, step, self._writer, self.index, self.max_channels, self.max_frames, ""input_1""\n            )\n\n        show_outputs = self.output_transform(engine.state.output)\n        if torch.is_tensor(show_outputs):\n            show_outputs = show_outputs.detach().cpu().numpy()\n        if show_outputs is not None:\n            if not isinstance(show_outputs, np.ndarray):\n                raise ValueError(""output_transform(engine.state.output) must be an ndarray or tensor."")\n            plot_2d_or_3d_image(\n                show_outputs, step, self._writer, self.index, self.max_channels, self.max_frames, ""output""\n            )\n\n        self._writer.flush()\n'"
monai/handlers/utils.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef stopping_fn_from_metric(metric_name):\n    """"""Returns a stopping function for ignite.handlers.EarlyStopping using the given metric name.""""""\n\n    def stopping_fn(engine):\n        return engine.state.metrics[metric_name]\n\n    return stopping_fn\n\n\ndef stopping_fn_from_loss():\n    """"""Returns a stopping function for ignite.handlers.EarlyStopping using the loss value.""""""\n\n    def stopping_fn(engine):\n        return -engine.state.output\n\n    return stopping_fn\n'"
monai/handlers/validation_handler.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom ignite.engine import Events, Engine\nfrom monai.engines import Evaluator\n\n\nclass ValidationHandler:\n    """"""\n    Attach validator to the trainer engine in Ignite.\n    It can support to execute validation every N epochs or every N iterations.\n\n    """"""\n\n    def __init__(self, validator: Evaluator, interval: int, epoch_level: bool = True) -> None:  # type: ignore\n        """"""\n        Args:\n            validator (Evaluator): run the validator when trigger validation, suppose to be Evaluator.\n            interval: do validation every N epochs or every N iterations during training.\n            epoch_level (bool): execute validation every N epochs or N iterations.\n                `True` is epoch level, `False` is iteration level.\n\n        """"""\n        if not isinstance(validator, Evaluator):  # type: ignore\n            raise ValueError(""validator must be Evaluator ignite engine."")\n        self.validator = validator\n        self.interval = interval\n        self.epoch_level = epoch_level\n\n    def attach(self, engine: Engine):\n        if self.epoch_level:\n            engine.add_event_handler(Events.EPOCH_COMPLETED(every=self.interval), self)\n        else:\n            engine.add_event_handler(Events.ITERATION_COMPLETED(every=self.interval), self)\n\n    def __call__(self, engine: Engine):\n        self.validator.run(engine.state.epoch)\n'"
monai/inferers/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .utils import sliding_window_inference\nfrom .inferer import *\n'"
monai/inferers/inferer.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABC, abstractmethod\nfrom .utils import sliding_window_inference\nimport torch\n\n\nclass Inferer(ABC):\n    """"""\n    A base class for model inference.\n    Extend this class to support operations during inference, e.g. a sliding window method.\n    """"""\n\n    @abstractmethod\n    def __call__(self, inputs: torch.Tensor, network):\n        """"""\n        Run inference on `inputs` with the `network` model.\n\n        Args:\n            inputs (torch.tensor): input of the model inference.\n            network (Network): model for inference.\n        """"""\n        raise NotImplementedError(""subclass will implement the operations."")\n\n\nclass SimpleInferer(Inferer):\n    """"""\n    SimpleInferer is the normal inference method that run model forward() directly.\n\n    """"""\n\n    def __init__(self):\n        Inferer.__init__(self)\n\n    def __call__(self, inputs: torch.Tensor, network):\n        """"""Unified callable function API of Inferers.\n\n        Args:\n            inputs (torch.tensor): model input data for inference.\n            network (Network): target model to execute inference.\n\n        """"""\n        return network(inputs)\n\n\nclass SlidingWindowInferer(Inferer):\n    """"""\n    Sliding window method for model inference,\n    with `sw_batch_size` windows for every model.forward().\n\n    Args:\n        roi_size (list, tuple): the window size to execute SlidingWindow evaluation.\n        sw_batch_size: the batch size to run window slices.\n        overlap (float): Amount of overlap between scans.\n        blend_mode (str): How to blend output of overlapping windows. Options are \'constant\', \'gaussian\'. \'constant\'\n            gives equal weight to all predictions while gaussian gives less weight to predictions on edges of windows.\n\n    Note:\n        the ""sw_batch_size"" here is to run a batch of window slices of 1 input image,\n        not batch size of input images.\n\n    """"""\n\n    def __init__(self, roi_size, sw_batch_size: int = 1, overlap: float = 0.25, blend_mode: str = ""constant""):\n        Inferer.__init__(self)\n        if not isinstance(roi_size, (list, tuple)):\n            raise ValueError(""must specify the roi size in a list or tuple for SlidingWindow."")\n        self.roi_size = roi_size\n        self.sw_batch_size = sw_batch_size\n        self.overlap = overlap\n        self.blend_mode = blend_mode\n\n    def __call__(self, inputs: torch.Tensor, network):\n        """"""\n        Unified callable function API of Inferers.\n\n        Args:\n            inputs (torch.tensor): model input data for inference.\n            network (Network): target model to execute inference.\n\n        """"""\n        return sliding_window_inference(\n            inputs, self.roi_size, self.sw_batch_size, network, self.overlap, self.blend_mode\n        )\n'"
monai/inferers/utils.py,4,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn.functional as F\nfrom monai.data.utils import dense_patch_slices, compute_importance_map\n\n\ndef sliding_window_inference(\n    inputs, roi_size, sw_batch_size: int, predictor, overlap=0.25, blend_mode=""constant"",\n):\n    """"""\n    Use SlidingWindow method to execute inference.\n\n    Args:\n        inputs (torch Tensor): input image to be processed (assuming NCHW[D])\n        roi_size (list, tuple): the window size to execute SlidingWindow inference.\n        sw_batch_size: the batch size to run window slices.\n        predictor (Callable): given input tensor `patch_data` in shape NCHW[D], `predictor(patch_data)`\n            should return a prediction with the same spatial shape and batch_size, i.e. NMHW[D];\n            where HW[D] represents the patch spatial size, M is the number of output channels, N is `sw_batch_size`.\n        overlap (float): Amount of overlap between scans.\n        blend_mode (str): How to blend output of overlapping windows. Options are \'constant\', \'gaussian\'. \'constant\'\n            gives equal weight to all predictions while gaussian gives less weight to predictions on edges of windows.\n\n    Note:\n        must be channel first, support both 2D and 3D.\n        input data must have batch dim.\n        execute on 1 image/per inference, run a batch of window slices of 1 input image.\n    """"""\n    num_spatial_dims = len(inputs.shape) - 2\n    assert len(roi_size) == num_spatial_dims, f""roi_size {roi_size} does not match input dims.""\n    assert overlap >= 0 and overlap < 1, ""overlap must be >= 0 and < 1.""\n\n    # determine image spatial size and batch size\n    # Note: all input images must have the same image size and batch size\n    image_size_ = list(inputs.shape[2:])\n    batch_size = inputs.shape[0]\n\n    # TODO: Enable batch sizes > 1 in future\n    if batch_size > 1:\n        raise NotImplementedError\n\n    original_image_size = [image_size_[i] for i in range(num_spatial_dims)]\n    # in case that image size is smaller than roi size\n    image_size = tuple(max(image_size_[i], roi_size[i]) for i in range(num_spatial_dims))\n    pad_size = [i for k in range(len(inputs.shape) - 1, 1, -1) for i in (0, max(roi_size[k - 2] - inputs.shape[k], 0))]\n    inputs = F.pad(inputs, pad=pad_size, mode=""constant"", value=0)\n\n    scan_interval = _get_scan_interval(image_size, roi_size, num_spatial_dims, overlap)\n\n    # Store all slices in list\n    slices = dense_patch_slices(image_size, roi_size, scan_interval)\n\n    slice_batches = []\n    for slice_index in range(0, len(slices), sw_batch_size):\n        slice_index_range = range(slice_index, min(slice_index + sw_batch_size, len(slices)))\n        input_slices = []\n        for curr_index in slice_index_range:\n            curr_slice = slices[curr_index]\n            if len(curr_slice) == 3:\n                input_slices.append(inputs[0, :, curr_slice[0], curr_slice[1], curr_slice[2]])\n            else:\n                input_slices.append(inputs[0, :, curr_slice[0], curr_slice[1]])\n        slice_batches.append(torch.stack(input_slices))\n\n    # Perform predictions\n    output_rois = list()\n    for data in slice_batches:\n        seg_prob = predictor(data)  # batched patch segmentation\n        output_rois.append(seg_prob)\n\n    # stitching output image\n    output_classes = output_rois[0].shape[1]\n    output_shape = [batch_size, output_classes] + list(image_size)\n\n    # Create importance map\n    importance_map = compute_importance_map(roi_size, mode=blend_mode, device=inputs.device)\n\n    # allocate memory to store the full output and the count for overlapping parts\n    output_image = torch.zeros(output_shape, dtype=torch.float32, device=inputs.device)\n    count_map = torch.zeros(output_shape, dtype=torch.float32, device=inputs.device)\n\n    for window_id, slice_index in enumerate(range(0, len(slices), sw_batch_size)):\n        slice_index_range = range(slice_index, min(slice_index + sw_batch_size, len(slices)))\n\n        # store the result in the proper location of the full output. Apply weights from importance map.\n        for curr_index in slice_index_range:\n            curr_slice = slices[curr_index]\n            if len(curr_slice) == 3:\n                output_image[0, :, curr_slice[0], curr_slice[1], curr_slice[2]] += (\n                    importance_map * output_rois[window_id][curr_index - slice_index, :]\n                )\n                count_map[0, :, curr_slice[0], curr_slice[1], curr_slice[2]] += importance_map\n            else:\n                output_image[0, :, curr_slice[0], curr_slice[1]] += (\n                    importance_map * output_rois[window_id][curr_index - slice_index, :]\n                )\n                count_map[0, :, curr_slice[0], curr_slice[1]] += importance_map\n\n    # account for any overlapping sections\n    output_image /= count_map\n\n    if num_spatial_dims == 3:\n        return output_image[..., : original_image_size[0], : original_image_size[1], : original_image_size[2]]\n    return output_image[..., : original_image_size[0], : original_image_size[1]]  # 2D\n\n\ndef _get_scan_interval(image_size, roi_size, num_spatial_dims: int, overlap: float):\n    assert len(image_size) == num_spatial_dims, ""image coord different from spatial dims.""\n    assert len(roi_size) == num_spatial_dims, ""roi coord different from spatial dims.""\n\n    scan_interval = []\n    for i in range(num_spatial_dims):\n        if roi_size[i] == image_size[i]:\n            scan_interval.append(int(roi_size[i]))\n        else:\n            # scan interval is (1-overlap)*roi_size\n            scan_interval.append(int(roi_size[i] * (1 - overlap)))\n    return tuple(scan_interval)\n'"
monai/losses/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .dice import Dice, DiceLoss, GeneralizedDiceLoss, dice, generalized_dice, MaskedDiceLoss\nfrom .focal_loss import FocalLoss\nfrom .tversky import TverskyLoss\n'"
monai/losses/dice.py,25,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport warnings\nfrom typing import Callable\n\nimport torch\nfrom torch.nn.modules.loss import _Loss\n\nfrom monai.networks.utils import one_hot\n\n\nclass DiceLoss(_Loss):\n    """"""\n    Compute average Dice loss between two tensors. It can support both multi-classes and multi-labels tasks.\n    Input logits `input` (BNHW[D] where N is number of classes) is compared with ground truth `target` (BNHW[D]).\n    Axis N of `input` is expected to have logit predictions for each class rather than being image channels,\n    while the same axis of `target` can be 1 or N (one-hot format). The `smooth` parameter is a value added to the\n    intersection and union components of the inter-over-union calculation to smooth results and prevent divide by 0,\n    this value should be small. The `include_background` class attribute can be set to False for an instance of\n    DiceLoss to exclude the first category (channel index 0) which is by convention assumed to be background.\n    If the non-background segmentations are small compared to the total image size they can get overwhelmed by\n    the signal from the background so excluding it in such cases helps convergence.\n\n    Milletari, F. et. al. (2016) V-Net: Fully Convolutional Neural Networks forVolumetric Medical Image Segmentation, 3DV, 2016.\n\n    """"""\n\n    def __init__(\n        self,\n        include_background: bool = True,\n        to_onehot_y: bool = False,\n        sigmoid: bool = False,\n        softmax: bool = False,\n        squared_pred: bool = False,\n        jaccard: bool = False,\n        reduction: str = ""mean"",\n    ):\n        """"""\n        Args:\n            include_background (bool): If False channel index 0 (background category) is excluded from the calculation.\n            to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.\n            sigmoid (bool): If True, apply a sigmoid function to the prediction.\n            softmax (bool): If True, apply a softmax function to the prediction.\n            squared_pred (bool): use squared versions of targets and predictions in the denominator or not.\n            jaccard (bool): compute Jaccard Index (soft IoU) instead of dice or not.\n            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:\n                ``\'none\'``: no reduction will be applied,\n                ``\'mean\'``: the sum of the output will be divided by the number of elements in the output,\n                ``\'sum\'``: the output will be summed.\n                Default: ``\'mean\'``.\n        """"""\n        super().__init__(reduction=reduction)\n\n        if reduction not in [""none"", ""mean"", ""sum""]:\n            raise ValueError(f""reduction={reduction} is invalid. Valid options are: none, mean or sum."")\n\n        if sigmoid and softmax:\n            raise ValueError(""do_sigmoid=True and do_softmax=True are not compatible."")\n\n        self.include_background = include_background\n        self.to_onehot_y = to_onehot_y\n        self.sigmoid = sigmoid\n        self.softmax = softmax\n        self.squared_pred = squared_pred\n        self.jaccard = jaccard\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor, smooth: float = 1e-5):\n        """"""\n        Args:\n            input (tensor): the shape should be BNH[WD].\n            target (tensor): the shape should be BNH[WD].\n            smooth (float): a small constant to avoid nan.\n        """"""\n        if self.sigmoid:\n            input = torch.sigmoid(input)\n\n        n_pred_ch = input.shape[1]\n        if n_pred_ch == 1:\n            if self.softmax:\n                warnings.warn(""single channel prediction, `softmax=True` ignored."")\n            if self.to_onehot_y:\n                warnings.warn(""single channel prediction, `to_onehot_y=True` ignored."")\n            if not self.include_background:\n                warnings.warn(""single channel prediction, `include_background=False` ignored."")\n        else:\n            if self.softmax:\n                input = torch.softmax(input, 1)\n\n            if self.to_onehot_y:\n                target = one_hot(target, num_classes=n_pred_ch)\n            if not self.include_background:\n                # if skipping background, removing first channel\n                target = target[:, 1:]\n                input = input[:, 1:]\n\n        assert (\n            target.shape == input.shape\n        ), f""ground truth has differing shape ({target.shape}) from input ({input.shape})""\n\n        # reducing only spatial dimensions (not batch nor channels)\n        reduce_axis = list(range(2, len(input.shape)))\n        intersection = torch.sum(target * input, dim=reduce_axis)\n\n        if self.squared_pred:\n            target = torch.pow(target, 2)\n            input = torch.pow(input, 2)\n\n        ground_o = torch.sum(target, dim=reduce_axis)\n        pred_o = torch.sum(input, dim=reduce_axis)\n\n        denominator = ground_o + pred_o\n\n        if self.jaccard:\n            denominator -= intersection\n\n        f = 1.0 - (2.0 * intersection + smooth) / (denominator + smooth)\n\n        if self.reduction == ""mean"":\n            f = torch.mean(f)  # the batch and channel average\n        elif self.reduction == ""sum"":\n            f = torch.sum(f)  # sum over the batch and channel dims\n        elif self.reduction == ""none"":\n            pass  # returns [N, n_classes] losses\n        else:\n            raise ValueError(f""reduction={self.reduction} is invalid."")\n\n        return f\n\n\nclass MaskedDiceLoss(DiceLoss):\n    """"""\n    Same as DiceLoss, but accepts a binary mask ([0,1]) indicating a region over which to compute the dice.\n    """"""\n\n    def forward(  # type: ignore # see issue #495\n        self, input: torch.Tensor, target: torch.Tensor, mask: torch.Tensor = None, smooth: float = 1e-5\n    ):\n        """"""\n        Args:\n            input (tensor): the shape should be BNH[WD].\n            target (tensor): the shape should be BNH[WD].\n            mask (tensor): (optional) the shape should B1H[WD] or 11H[WD]\n            smooth (float): a small constant to avoid nan.\n        """"""\n        if mask is not None:\n            # checking if mask is of proper shape\n            assert input.dim() == mask.dim(), f""dim of input ({input.shape}) is different from mask ({mask.shape})""\n            assert (\n                input.shape[0] == mask.shape[0] or mask.shape[0] == 1\n            ), f"" batch size of mask ({mask.shape}) must be 1 or equal to input ({input.shape})""\n\n            if target.dim() > 1:\n                assert mask.shape[1] == 1, f""mask ({mask.shape}) must have only 1 channel""\n                assert (\n                    input.shape[2:] == mask.shape[2:]\n                ), f""spatial size of input ({input.shape}) is different from mask ({mask.shape})""\n\n            input = input * mask\n            target = target * mask\n\n        return super().forward(input=input, target=target, smooth=smooth)\n\n\nclass GeneralizedDiceLoss(_Loss):\n    """"""\n    Compute the generalised Dice loss defined in:\n\n        Sudre, C. et. al. (2017) Generalised Dice overlap as a deep learning\n        loss function for highly unbalanced segmentations. DLMIA 2017.\n\n    Adapted from:\n        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L279\n    """"""\n\n    def __init__(\n        self,\n        include_background: bool = True,\n        to_onehot_y: bool = False,\n        sigmoid: bool = False,\n        softmax: bool = False,\n        w_type: str = ""square"",\n        reduction: str = ""mean"",\n    ):\n        """"""\n        Args:\n            include_background (bool): If False channel index 0 (background category) is excluded from the calculation.\n            to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.\n            sigmoid (bool): If True, apply a sigmoid function to the prediction.\n            softmax (bool): If True, apply a softmax function to the prediction.\n            w_type (\'square\'|\'simple\'|\'uniform\'): type of function to transform ground truth volume to a weight factor.\n                Default: `\'square\'`\n            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:\n                ``\'none\'``: no reduction will be applied,\n                ``\'mean\'``: the sum of the output will be divided by the batch size in the output,\n                ``\'sum\'``: the output will be summed over the batch dim.\n                Default: ``\'mean\'``.\n        """"""\n        super().__init__(reduction=reduction)\n\n        if reduction not in [""none"", ""mean"", ""sum""]:\n            raise ValueError(f""reduction={reduction} is invalid. Valid options are: none, mean or sum."")\n\n        self.include_background = include_background\n        self.to_onehot_y = to_onehot_y\n        if sigmoid and softmax:\n            raise ValueError(""sigmoid=True and softmax=True are not compatible."")\n        self.sigmoid = sigmoid\n        self.softmax = softmax\n\n        self.w_func: Callable = torch.ones_like\n        if w_type == ""simple"":\n            self.w_func = torch.reciprocal\n        elif w_type == ""square"":\n            self.w_func = lambda x: torch.reciprocal(x * x)\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor, smooth: float = 1e-5):\n        """"""\n        Args:\n            input (tensor): the shape should be BNH[WD].\n            target (tensor): the shape should be BNH[WD].\n            smooth (float): a small constant to avoid nan.\n        """"""\n        if self.sigmoid:\n            input = torch.sigmoid(input)\n        n_pred_ch = input.shape[1]\n        if n_pred_ch == 1:\n            if self.softmax:\n                warnings.warn(""single channel prediction, `softmax=True` ignored."")\n            if self.to_onehot_y:\n                warnings.warn(""single channel prediction, `to_onehot_y=True` ignored."")\n            if not self.include_background:\n                warnings.warn(""single channel prediction, `include_background=False` ignored."")\n        else:\n            if self.softmax:\n                input = torch.softmax(input, 1)\n            if self.to_onehot_y:\n                target = one_hot(target, n_pred_ch)\n            if not self.include_background:\n                # if skipping background, removing first channel\n                target = target[:, 1:]\n                input = input[:, 1:]\n        assert (\n            target.shape == input.shape\n        ), f""ground truth has differing shape ({target.shape}) from input ({input.shape})""\n\n        # reducing only spatial dimensions (not batch nor channels)\n        reduce_axis = list(range(2, len(input.shape)))\n        intersection = torch.sum(target * input, reduce_axis)\n\n        ground_o = torch.sum(target, reduce_axis)\n        pred_o = torch.sum(input, reduce_axis)\n\n        denominator = ground_o + pred_o\n\n        w = self.w_func(ground_o.float())\n        for b in w:\n            infs = torch.isinf(b)\n            b[infs] = 0.0\n            b[infs] = torch.max(b)\n\n        f = 1.0 - (2.0 * (intersection * w).sum(1) + smooth) / ((denominator * w).sum(1) + smooth)\n\n        if self.reduction == ""mean"":\n            f = torch.mean(f)  # the batch and channel average\n        elif self.reduction == ""sum"":\n            f = torch.sum(f)  # sum over the batch and channel dims\n        elif self.reduction == ""none"":\n            pass  # returns [N, n_classes] losses\n        else:\n            raise ValueError(f""reduction={self.reduction} is invalid."")\n\n        return f\n\n\ndice = Dice = DiceLoss\ngeneralized_dice = GeneralizedDiceLoss\n'"
monai/losses/focal_loss.py,10,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\nclass FocalLoss(_WeightedLoss):\n    """"""\n    PyTorch implementation of the Focal Loss.\n    [1] ""Focal Loss for Dense Object Detection"", T. Lin et al., ICCV 2017\n    """"""\n\n    def __init__(self, gamma: float = 2.0, weight: Optional[torch.Tensor] = None, reduction: str = ""mean""):\n        """"""\n        Args:\n            gamma (float): value of the exponent gamma in the definition of the Focal loss.\n            weight (tensor): weights to apply to the voxels of each class. If None no weights are applied.\n                This corresponds to the weights `\\alpha` in [1].\n            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:\n                ``\'none\'``: no reduction will be applied,\n                ``\'mean\'``: the sum of the output will be divided by the batch size in the output,\n                ``\'sum\'``: the output will be summed over the batch dim.\n                Default: ``\'mean\'``.\n\n        Example:\n            .. code-block:: python\n\n                import torch\n                from monai.losses import FocalLoss\n\n                pred = torch.tensor([[1, 0], [0, 1], [1, 0]], dtype=torch.float32)\n                grnd = torch.tensor([[0], [1], [0]], dtype=torch.int64)\n                fl = FocalLoss()\n                fl(pred, grnd)\n\n        """"""\n        super(FocalLoss, self).__init__(weight=weight, reduction=reduction)\n        self.gamma = gamma\n\n    def forward(self, input, target):\n        """"""\n        Args:\n            input: (tensor): the shape should be BCH[WD].\n                where C is the number of classes.\n            target: (tensor): the shape should be B1H[WD].\n                The target that this loss expects should be a class index in the range\n                [0, C-1] where C is the number of classes.\n        """"""\n        i = input\n        t = target\n\n        if i.ndim != t.ndim:\n            raise ValueError(f""input and target must have the same number of dimensions, got {i.ndim} and {t.ndim}"")\n\n        if target.shape[1] != 1:\n            raise ValueError(\n                ""target must have one channel, and should be a class index in the range [0, C-1] ""\n                + f""where C is the number of classes inferred from \'input\': C={i.shape[1]}.""\n            )\n        # Change the shape of input and target to\n        # num_batch x num_class x num_voxels.\n        if input.dim() > 2:\n            i = i.view(i.size(0), i.size(1), -1)  # N,C,H,W => N,C,H*W\n            t = t.view(t.size(0), t.size(1), -1)  # N,1,H,W => N,1,H*W\n        else:  # Compatibility with classification.\n            i = i.unsqueeze(2)  # N,C => N,C,1\n            t = t.unsqueeze(2)  # N,1 => N,1,1\n\n        # Compute the log proba (more stable numerically than softmax).\n        logpt = F.log_softmax(i, dim=1)  # N,C,H*W\n        # Keep only log proba values of the ground truth class for each voxel.\n        logpt = logpt.gather(1, t.long())  # N,C,H*W => N,1,H*W\n        logpt = torch.squeeze(logpt, dim=1)  # N,1,H*W => N,H*W\n\n        # Get the proba\n        pt = torch.exp(logpt)  # N,H*W\n\n        if self.weight is not None:\n            self.weight = self.weight.to(i)\n            # Convert the weight to a map in which each voxel\n            # has the weight associated with the ground-truth label\n            # associated with this voxel in target.\n            at = self.weight[None, :, None]  # C => 1,C,1\n            at = at.expand((t.size(0), -1, t.size(2)))  # 1,C,1 => N,C,H*W\n            at = at.gather(1, t.long())  # selection of the weights  => N,1,H*W\n            at = torch.squeeze(at, dim=1)  # N,1,H*W => N,H*W\n            # Multiply the log proba by their weights.\n            logpt = logpt * at\n\n        # Compute the loss mini-batch.\n        weight = torch.pow(-pt + 1.0, self.gamma)\n        loss = torch.mean(-weight * logpt, dim=1)  # N\n\n        if self.reduction == ""sum"":\n            return loss.sum()\n        if self.reduction == ""none"":\n            return loss\n        if self.reduction == ""mean"":\n            return loss.mean()\n        raise ValueError(f""reduction={self.reduction} is invalid."")\n'"
monai/losses/tversky.py,7,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport warnings\n\nimport torch\nfrom torch.nn.modules.loss import _Loss\n\nfrom monai.networks.utils import one_hot\n\n\nclass TverskyLoss(_Loss):\n\n    """"""\n    Compute the Tversky loss defined in:\n\n        Sadegh et al. (2017) Tversky loss function for image segmentation\n        using 3D fully convolutional deep networks. (https://arxiv.org/abs/1706.05721)\n\n    Adapted from:\n        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L631\n\n    """"""\n\n    def __init__(\n        self,\n        include_background: bool = True,\n        to_onehot_y: bool = False,\n        sigmoid: bool = False,\n        softmax: bool = False,\n        alpha: float = 0.5,\n        beta: float = 0.5,\n        reduction: str = ""mean"",\n    ):\n\n        """"""\n        Args:\n            include_background (bool): If False channel index 0 (background category) is excluded from the calculation.\n            to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.\n            sigmoid (bool): If True, apply a sigmoid function to the prediction.\n            softmax (bool): If True, apply a softmax function to the prediction.\n            alpha (float): weight of false positives\n            beta  (float): weight of false negatives\n            reduction (`none|mean|sum`): Specifies the reduction to apply to the output:\n                ``\'none\'``: no reduction will be applied,\n                ``\'mean\'``: the sum of the output will be divided by the number of elements in the output,\n                ``\'sum\'``: the output will be summed.\n                Default: ``\'mean\'``.\n\n        """"""\n\n        super().__init__(reduction=reduction)\n        self.include_background = include_background\n        self.to_onehot_y = to_onehot_y\n\n        if sigmoid and softmax:\n            raise ValueError(""sigmoid=True and softmax=True are not compatible."")\n        self.sigmoid = sigmoid\n        self.softmax = softmax\n        self.alpha = alpha\n        self.beta = beta\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor, smooth: float = 1e-5):\n        """"""\n        Args:\n            input (tensor): the shape should be BNH[WD].\n            target (tensor): the shape should be BNH[WD].\n            smooth (float): a small constant to avoid nan.\n        """"""\n        if self.sigmoid:\n            input = torch.sigmoid(input)\n        n_pred_ch = input.shape[1]\n        if n_pred_ch == 1:\n            if self.softmax:\n                warnings.warn(""single channel prediction, `softmax=True` ignored."")\n            if self.to_onehot_y:\n                warnings.warn(""single channel prediction, `to_onehot_y=True` ignored."")\n            if not self.include_background:\n                warnings.warn(""single channel prediction, `include_background=False` ignored."")\n        else:\n            if self.softmax:\n                input = torch.softmax(input, 1)\n            if self.to_onehot_y:\n                target = one_hot(target, n_pred_ch)\n            if not self.include_background:\n                # if skipping background, removing first channel\n                target = target[:, 1:]\n                input = input[:, 1:]\n        assert (\n            target.shape == input.shape\n        ), f""ground truth has differing shape ({target.shape}) from input ({input.shape})""\n\n        p0 = input\n        p1 = 1 - p0\n        g0 = target\n        g1 = 1 - g0\n\n        # reducing only spatial dimensions (not batch nor channels)\n        reduce_axis = list(range(2, len(input.shape)))\n\n        tp = torch.sum(p0 * g0, reduce_axis)\n        fp = self.alpha * torch.sum(p0 * g1, reduce_axis)\n        fn = self.beta * torch.sum(p1 * g0, reduce_axis)\n\n        numerator = tp + smooth\n        denominator = tp + fp + fn + smooth\n\n        score = 1.0 - numerator / denominator\n\n        if self.reduction == ""sum"":\n            return score.sum()  # sum over the batch and channel dims\n        if self.reduction == ""none"":\n            return score  # returns [N, n_classes] losses\n        if self.reduction == ""mean"":\n            return score.mean()\n        raise ValueError(f""reduction={self.reduction} is invalid."")\n'"
monai/metrics/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .meandice import compute_meandice\nfrom .rocauc import compute_roc_auc\n'"
monai/metrics/meandice.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport warnings\n\nimport torch\n\nfrom monai.networks.utils import one_hot\n\n\ndef compute_meandice(\n    y_pred: torch.Tensor,\n    y: torch.Tensor,\n    include_background: bool = True,\n    to_onehot_y: bool = False,\n    mutually_exclusive: bool = False,\n    sigmoid: bool = False,\n    logit_thresh: float = 0.5,\n):\n    """"""Computes Dice score metric from full size Tensor and collects average.\n\n    Args:\n        y_pred (torch.Tensor): input data to compute, typical segmentation model output.\n            it must be one-hot format and first dim is batch, example shape: [16, 3, 32, 32].\n        y (torch.Tensor): ground truth to compute mean dice metric, the first dim is batch.\n            example shape: [16, 1, 32, 32] will be converted into [16, 3, 32, 32].\n            alternative shape: [16, 3, 32, 32] and set `to_onehot_y=False` to use 3-class labels directly.\n        include_background (bool): whether to skip Dice computation on the first channel of\n            the predicted output. Defaults to True.\n        to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.\n        mutually_exclusive (bool): if True, `y_pred` will be converted into a binary matrix using\n            a combination of argmax and to_onehot.  Defaults to False.\n        sigmoid (bool): whether to add sigmoid function to y_pred before computation. Defaults to False.\n        logit_thresh (Float): the threshold value used to convert (after sigmoid if `sigmoid=True`)\n            `y_pred` into a binary matrix. Defaults to 0.5.\n\n    Returns:\n        Dice scores per batch and per class, (shape [batch_size, n_classes]).\n\n    Note:\n        This method provides two options to convert `y_pred` into a binary matrix\n            (1) when `mutually_exclusive` is True, it uses a combination of ``argmax`` and ``to_onehot``,\n            (2) when `mutually_exclusive` is False, it uses a threshold ``logit_thresh``\n                (optionally with a ``sigmoid`` function before thresholding).\n\n    """"""\n    n_classes = y_pred.shape[1]\n    n_len = len(y_pred.shape)\n\n    if sigmoid:\n        y_pred = y_pred.float().sigmoid()\n\n    if n_classes == 1:\n        if mutually_exclusive:\n            warnings.warn(""y_pred has only one class, mutually_exclusive=True ignored."")\n        if to_onehot_y:\n            warnings.warn(""y_pred has only one channel, to_onehot_y=True ignored."")\n        if not include_background:\n            warnings.warn(""y_pred has only one channel, include_background=False ignored."")\n        # make both y and y_pred binary\n        y_pred = (y_pred >= logit_thresh).float()\n        y = (y > 0).float()\n    else:  # multi-channel y_pred\n        # make both y and y_pred binary\n        if mutually_exclusive:\n            if sigmoid:\n                raise ValueError(""sigmoid=True is incompatible with mutually_exclusive=True."")\n            y_pred = torch.argmax(y_pred, dim=1, keepdim=True)\n            y_pred = one_hot(y_pred, n_classes)\n        else:\n            y_pred = (y_pred >= logit_thresh).float()\n        if to_onehot_y:\n            y = one_hot(y, n_classes)\n\n    if not include_background:\n        y = y[:, 1:] if y.shape[1] > 1 else y\n        y_pred = y_pred[:, 1:] if y_pred.shape[1] > 1 else y_pred\n\n    assert y.shape == y_pred.shape, ""Ground truth one-hot has differing shape (%r) from source (%r)"" % (\n        y.shape,\n        y_pred.shape,\n    )\n\n    # reducing only spatial dimensions (not batch nor channels)\n    reduce_axis = list(range(2, n_len))\n    intersection = torch.sum(y * y_pred, reduce_axis)\n\n    y_o = torch.sum(y, reduce_axis)\n    y_pred_o = torch.sum(y_pred, reduce_axis)\n    denominator = y_o + y_pred_o\n\n    f = torch.where(y_o > 0, (2.0 * intersection) / denominator, torch.tensor(float(""nan"")).to(y_o.float()))\n    return f  # returns array of Dice shape: [Batch, n_classes]\n'"
monai/metrics/rocauc.py,5,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport torch\nimport warnings\nimport numpy as np\nfrom monai.networks.utils import one_hot\n\n\ndef _calculate(y, y_pred):\n    assert y.ndimension() == y_pred.ndimension() == 1 and len(y) == len(\n        y_pred\n    ), ""y and y_pred must be 1 dimension data with same length.""\n    assert y.unique().equal(\n        torch.tensor([0, 1], dtype=y.dtype, device=y.device)\n    ), ""y values must be 0 or 1, can not be all 0 or all 1.""\n    n = len(y)\n    indexes = y_pred.argsort()\n    y = y[indexes].cpu().numpy()\n    y_pred = y_pred[indexes].cpu().numpy()\n    nneg = auc = tmp_pos = tmp_neg = 0\n\n    for i in range(n):\n        y_i = y[i]\n        if i + 1 < n and y_pred[i] == y_pred[i + 1]:\n            tmp_pos += y_i\n            tmp_neg += 1 - y_i\n            continue\n        if tmp_pos + tmp_neg > 0:\n            tmp_pos += y_i\n            tmp_neg += 1 - y_i\n            nneg += tmp_neg\n            auc += tmp_pos * (nneg - tmp_neg / 2)\n            tmp_pos = tmp_neg = 0\n            continue\n        if y_i == 1:\n            auc += nneg\n        else:\n            nneg += 1\n    return auc / (nneg * (n - nneg))\n\n\ndef compute_roc_auc(\n    y_pred: torch.Tensor,\n    y: torch.Tensor,\n    to_onehot_y: bool = False,\n    softmax: bool = False,\n    average: Optional[str] = ""macro"",\n):\n    """"""Computes Area Under the Receiver Operating Characteristic Curve (ROC AUC). Referring to:\n    `sklearn.metrics.roc_auc_score <https://scikit-learn.org/stable/modules/generated/\n    sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score>`_.\n\n    Args:\n        y_pred (torch.Tensor): input data to compute, typical classification model output.\n            it must be One-Hot format and first dim is batch, example shape: [16] or [16, 2].\n        y (torch.Tensor): ground truth to compute ROC AUC metric, the first dim is batch.\n            example shape: [16, 1] will be converted into [16, 2] (where `2` is inferred from `y_pred`).\n        to_onehot_y (bool): whether to convert `y` into the one-hot format. Defaults to False.\n        softmax (bool): whether to add softmax function to `y_pred` before computation. Defaults to False.\n        average (`macro|weighted|micro|None`): type of averaging performed if not binary\n            classification. Default is \'macro\'.\n\n            - \'macro\': calculate metrics for each label, and find their unweighted mean.\n              this does not take label imbalance into account.\n            - \'weighted\': calculate metrics for each label, and find their average,\n              weighted by support (the number of true instances for each label).\n            - \'micro\': calculate metrics globally by considering each element of the label\n              indicator matrix as a label.\n            - None: the scores for each class are returned.\n\n    Note:\n        ROCAUC expects y to be comprised of 0\'s and 1\'s. `y_pred` must be either prob. estimates or confidence values.\n\n    """"""\n    y_pred_ndim = y_pred.ndimension()\n    y_ndim = y.ndimension()\n    if y_pred_ndim not in (1, 2):\n        raise ValueError(""predictions should be of shape (batch_size, n_classes) or (batch_size, )."")\n    if y_ndim not in (1, 2):\n        raise ValueError(""targets should be of shape (batch_size, n_classes) or (batch_size, )."")\n    if y_pred_ndim == 2 and y_pred.shape[1] == 1:\n        y_pred = y_pred.squeeze(dim=-1)\n        y_pred_ndim = 1\n    if y_ndim == 2 and y.shape[1] == 1:\n        y = y.squeeze(dim=-1)\n\n    if y_pred_ndim == 1:\n        if to_onehot_y:\n            warnings.warn(""y_pred has only one channel, to_onehot_y=True ignored."")\n        if softmax:\n            warnings.warn(""y_pred has only one channel, softmax=True ignored."")\n        return _calculate(y, y_pred)\n    else:\n        n_classes = y_pred.shape[1]\n        if to_onehot_y:\n            y = one_hot(y, n_classes)\n        if softmax:\n            y_pred = y_pred.float().softmax(dim=1)\n\n        assert y.shape == y_pred.shape, ""data shapes of y_pred and y do not match.""\n\n        if average == ""micro"":\n            return _calculate(y.flatten(), y_pred.flatten())\n        else:\n            y, y_pred = y.transpose(0, 1), y_pred.transpose(0, 1)\n            auc_values = [_calculate(y_, y_pred_) for y_, y_pred_ in zip(y, y_pred)]\n            if average is None:\n                return auc_values\n            if average == ""macro"":\n                return np.mean(auc_values)\n            if average == ""weighted"":\n                weights = [sum(y_) for y_ in y]\n                return np.average(auc_values, weights=weights)\n            raise ValueError(""unsupported average method."")\n'"
monai/networks/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/networks/utils.py,13,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtilities and types for defining networks, these depend on PyTorch.\n""""""\n\nimport warnings\nimport torch\nimport torch.nn.functional as f\n\n\ndef one_hot(labels, num_classes: int, dtype: torch.dtype = torch.float):\n    """"""\n    For a tensor `labels` of dimensions B1[spatial_dims], return a tensor of dimensions `BN[spatial_dims]`\n    for `num_classes` N number of classes.\n\n    Example:\n\n        For every value v = labels[b,1,h,w], the value in the result at [b,v,h,w] will be 1 and all others 0.\n        Note that this will include the background label, thus a binary mask should be treated as having 2 classes.\n    """"""\n    assert labels.dim() > 0, ""labels should have dim of 1 or more.""\n\n    # if 1D, add singelton dim at the end\n    if labels.dim() == 1:\n        labels = labels.view(-1, 1)\n\n    sh = list(labels.shape)\n\n    assert sh[1] == 1, ""labels should have a channel with length equals to one.""\n    sh[1] = num_classes\n\n    o = torch.zeros(size=sh, dtype=dtype, device=labels.device)\n    labels = o.scatter_(dim=1, index=labels.long(), value=1)\n\n    return labels\n\n\ndef slice_channels(tensor: torch.Tensor, *slicevals):\n    slices = [slice(None)] * len(tensor.shape)\n    slices[1] = slice(*slicevals)\n\n    return tensor[slices]\n\n\ndef predict_segmentation(logits: torch.Tensor, mutually_exclusive: bool = False, threshold: float = 0):\n    """"""\n    Given the logits from a network, computing the segmentation by thresholding all values above 0\n    if multi-labels task, computing the `argmax` along the channel axis if multi-classes task,\n    logits has shape `BCHW[D]`.\n\n    Args:\n        logits (Tensor): raw data of model output.\n        mutually_exclusive (bool): if True, `logits` will be converted into a binary matrix using\n            a combination of argmax, which is suitable for multi-classes task. Defaults to False.\n        threshold (float): thresholding the prediction values if multi-labels task.\n    """"""\n    if not mutually_exclusive:\n        return (logits >= threshold).int()\n    else:\n        if logits.shape[1] == 1:\n            warnings.warn(""single channel prediction, `mutually_exclusive=True` ignored, use threshold instead."")\n            return (logits >= threshold).int()\n        return logits.argmax(1, keepdim=True)\n\n\ndef normalize_transform(shape, device=None, dtype=None, align_corners=False):\n    """"""\n    Compute an affine matrix according to the input shape.\n    The transform normalizes the homogeneous image coordinates to the\n    range of `[-1, 1]`.\n\n    Args:\n        shape (sequence of int): input spatial shape\n        device (torch device): device on which the returned affine will be allocated.\n        dtype (torch dtype): data type of the returned affine\n        align_corners (bool): if True, consider -1 and 1 to refer to the centers of the\n            corner pixels rather than the image corners.\n            See also: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.grid_sample\n    """"""\n    norm = torch.tensor(shape, dtype=torch.float64, device=device)  # no in-place change\n    if align_corners:\n        norm[norm <= 1.0] = 2.0\n        norm = 2.0 / (norm - 1.0)\n        norm = torch.diag(torch.cat((norm, torch.ones((1,), dtype=torch.float64, device=device))))\n        norm[:-1, -1] = -1.0\n    else:\n        norm[norm <= 0.0] = 2.0\n        norm = 2.0 / norm\n        norm = torch.diag(torch.cat((norm, torch.ones((1,), dtype=torch.float64, device=device))))\n        norm[:-1, -1] = 1.0 / torch.tensor(shape, dtype=torch.float32, device=device) - 1.0\n    norm = norm.unsqueeze(0).to(dtype=dtype)\n    norm.requires_grad = False\n    return norm\n\n\ndef to_norm_affine(affine, src_size, dst_size, align_corners=False):\n    """"""\n    Given ``affine`` defined for coordinates in the pixel space, compute the corresponding affine\n    for the normalized coordinates.\n\n    Args:\n        affine (torch Tensor): Nxdxd batched square matrix\n        src_size (sequence of int): source image spatial shape\n        dst_size (sequence of int): target image spatial shape\n        align_corners (bool): if True, consider -1 and 1 to refer to the centers of the\n            corner pixels rather than the image corners.\n            See also: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.grid_sample\n    """"""\n    if not torch.is_tensor(affine):\n        raise ValueError(""affine must be a tensor"")\n    if affine.ndim != 3 or affine.shape[1] != affine.shape[2]:\n        raise ValueError(f""affine must be Nxdxd, got {tuple(affine.shape)}"")\n    sr = affine.shape[1] - 1\n    if sr != len(src_size) or sr != len(dst_size):\n        raise ValueError(\n            f""affine suggests a {sr}-D transform, but the sizes are src_size={src_size}, dst_size={dst_size}""\n        )\n\n    src_xform = normalize_transform(src_size, affine.device, affine.dtype, align_corners)\n    dst_xform = normalize_transform(dst_size, affine.device, affine.dtype, align_corners)\n    new_affine = src_xform @ affine @ torch.inverse(dst_xform)\n    return new_affine\n'"
monai/transforms/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .compose import *\nfrom .croppad.array import *\nfrom .croppad.dictionary import *\nfrom .intensity.array import *\nfrom .intensity.dictionary import *\nfrom .io.array import *\nfrom .io.dictionary import *\nfrom .spatial.array import *\nfrom .spatial.dictionary import *\nfrom .utility.array import *\nfrom .utility.dictionary import *\nfrom .post.array import *\nfrom .post.dictionary import *\n'"
monai/transforms/adaptors.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nHow to use the adaptor function\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe key to using \'adaptor\' lies in understanding the function that want to\nadapt. The \'inputs\' and \'outputs\' parameters take either strings, lists/tuples\nof strings or a dictionary mapping strings, depending on call signature of the\nfunction being called.\n\nThe adaptor function is written to minimise the cognitive load on the caller.\nThere should be a minimal number of cases where the caller has to set anything\non the input parameter, and for functions that return a single value, it is\nonly necessary to name the dictionary keyword to which that value is assigned.\n\nUse of `outputs`\n----------------\n\n`outputs` can take either a string, a list/tuple of string or a dict of string\nto string, depending on what the transform being adapted returns:\n\n    - If the transform returns a single argument, then outputs can be supplied a\n      string that indicates what key to assign the return value to in the\n      dictionary\n    - If the transform returns a list/tuple of values, then outputs can be supplied\n      a list/tuple of the same length. The strings in outputs map the return value\n      at the corresponding position to a key in the dictionary\n    - If the transform returns a dictionary of values, then outputs must be supplied\n      a dictionary that maps keys in the function\'s return dictionary to the\n      dictionary being passed between functions\n\nNote, the caller is free to use a more complex way of specifying the outputs\nparameter than is required. The following are synonymous and will be treated\nidentically:\n\n.. code-block:: python\n\n   # single argument\n   adaptor(MyTransform(), \'image\')\n   adaptor(MyTransform(), [\'image\'])\n   adaptor(MyTransform(), {\'image\': \'image\'})\n\n   # multiple arguments\n   adaptor(MyTransform(), [\'image\', \'label\'])\n   adaptor(MyTransform(), {\'image\': \'image\', \'label\': \'label\'})\n\nUse of `inputs`\n---------------\n\n`inputs` can usually be omitted when using `adaptor`. It is only required when a\nthe function\'s parameter names do not match the names in the dictionary that is\nused to chain transform calls.\n\n.. code-block:: python\n\n    class MyTransform1:\n        def __call__(self, image):\n            # do stuff to image\n            return image + 1\n\n\n    class MyTransform2:\n        def __call__(self, img_dict):\n            # do stuff to image\n            img_dict[""image""] += 1\n            return img_dict\n\n\n    xform = Compose([adaptor(MyTransform1(), ""image""), MyTransform2()])\n    d = {""image"": 1}\n    print(xform(d))\n\n    >>> {\'image\': 3}\n\n.. code-block:: python\n\n    class MyTransform3:\n        def __call__(self, img_dict):\n            # do stuff to image\n            img_dict[""image""] -= 1\n            img_dict[""segment""] = img_dict[""image""]\n            return img_dict\n\n\n    class MyTransform4:\n        def __call__(self, img, seg):\n            # do stuff to image\n            img -= 1\n            seg -= 1\n            return img, seg\n\n\n    xform = Compose([MyTransform3(), adaptor(MyTransform4(), [""img"", ""seg""], {""image"": ""img"", ""segment"": ""seg""})])\n    d = {""image"": 1}\n    print(xform(d))\n\n    >>> {\'image\': 0, \'segment\': 0, \'img\': -1, \'seg\': -1}\n\nInputs:\n\n- dictionary in: None | Name maps\n- params in (match): None | Name list | Name maps\n- params in (mismatch): Name maps\n- params & `**kwargs` (match) : None | Name maps\n- params & `**kwargs` (mismatch) : Name maps\n\nOutputs:\n\n- dictionary out: None | Name maps\n- list/tuple out: list/tuple\n- variable out: string\n\n""""""\n\nfrom monai.utils import export as _monai_export\n\n\n@_monai_export(""monai.transforms"")\ndef adaptor(function, outputs, inputs=None):\n    def must_be_types_or_none(variable_name, variable, types):\n        if variable is not None:\n            if not isinstance(variable, types):\n                raise ValueError(f""\'{variable_name}\' must be None or {types} but is {type(variable)}"")\n\n    def must_be_types(variable_name, variable, types):\n        if not isinstance(variable, types):\n            raise ValueError(f""\'{variable_name}\' must be one of {types} but is {type(variable)}"")\n\n    def map_names(ditems, input_map):\n        return {input_map(k, k): v for k, v in ditems.items()}\n\n    def map_only_names(ditems, input_map):\n        return {v: ditems[k] for k, v in input_map.items()}\n\n    def _inner(ditems):\n\n        sig = FunctionSignature(function)\n\n        if sig.found_kwargs:\n            must_be_types_or_none(""inputs"", inputs, (dict,))\n            # we just forward all arguments unless we have been provided an input map\n            if inputs is None:\n                dinputs = dict(ditems)\n            else:\n                # dict\n                dinputs = map_names(ditems, inputs)\n\n        else:\n            # no **kwargs\n            # select only items from the method signature\n            dinputs = dict((k, v) for k, v in ditems.items() if k in sig.non_var_parameters)\n            must_be_types_or_none(""inputs"", inputs, (str, list, tuple, dict))\n            if inputs is None:\n                pass\n            elif isinstance(inputs, str):\n                if len(sig.non_var_parameters) != 1:\n                    raise ValueError(""if \'inputs\' is a string, function may only have a single non-variadic parameter"")\n                dinputs = {inputs: ditems[inputs]}\n            elif isinstance(inputs, (list, tuple)):\n                dinputs = dict((k, dinputs[k]) for k in inputs)\n            else:\n                # dict\n                dinputs = map_only_names(ditems, inputs)\n\n        ret = function(**dinputs)\n\n        # now the mapping back to the output dictionary depends on outputs and what was returned from the function\n        op = outputs\n        if isinstance(ret, dict):\n            must_be_types_or_none(""outputs"", op, (dict,))\n            if op is not None:\n                ret = {v: ret[k] for k, v in op.items()}\n        elif isinstance(ret, (list, tuple)):\n            if len(ret) == 1:\n                must_be_types(""outputs"", op, (str, list, tuple))\n            else:\n                must_be_types(""outputs"", op, (list, tuple))\n\n            if isinstance(op, str):\n                op = [op]\n\n            if len(ret) != len(outputs):\n                raise ValueError(""\'outputs\' must have the same length as the number of elements that were returned"")\n\n            ret = dict((k, v) for k, v in zip(op, ret))\n        else:\n            must_be_types(""outputs"", op, (str, list, tuple))\n            if isinstance(op, (list, tuple)):\n                if len(op) != 1:\n                    raise ValueError(""\'outputs\' must be of length one if it is a list or tuple"")\n                op = op[0]\n            ret = {op: ret}\n\n        ditems = dict(ditems)\n        for k, v in ret.items():\n            ditems[k] = v\n\n        return ditems\n\n    return _inner\n\n\n@_monai_export(""monai.transforms"")\ndef apply_alias(fn, name_map):\n    def _inner(data):\n\n        # map names\n        pre_call = dict(data)\n        for _from, _to in name_map.items():\n            pre_call[_to] = pre_call.pop(_from)\n\n        # execute\n        post_call = fn(pre_call)\n\n        # map names back\n        for _from, _to in name_map.items():\n            post_call[_from] = post_call.pop(_to)\n\n        return post_call\n\n    return _inner\n\n\n@_monai_export(""monai.transforms"")\ndef to_kwargs(fn):\n    def _inner(data):\n        return fn(**data)\n\n    return _inner\n\n\nclass FunctionSignature:\n    def __init__(self, function):\n        import inspect\n\n        sfn = inspect.signature(function)\n        self.found_args = False\n        self.found_kwargs = False\n        self.defaults = {}\n        self.non_var_parameters = set()\n        for p in sfn.parameters.values():\n            if p.kind is inspect.Parameter.VAR_POSITIONAL:\n                self.found_args = True\n            if p.kind is inspect.Parameter.VAR_KEYWORD:\n                self.found_kwargs = True\n            else:\n                self.non_var_parameters.add(p.name)\n                self.defaults[p.name] = p.default is not p.empty\n\n    def __repr__(self):\n        s = ""<class \'FunctionSignature\': found_args={}, found_kwargs={}, defaults={}""\n        return s.format(self.found_args, self.found_kwargs, self.defaults)\n\n    def __str__(self):\n        return self.__repr__()\n'"
monai/transforms/compose.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of generic interfaces for MONAI transforms.\n""""""\n\nimport warnings\nfrom typing import Hashable, Optional\nfrom abc import ABC, abstractmethod\nimport numpy as np\n\nfrom monai.utils.misc import ensure_tuple, get_seed\nfrom .utils import apply_transform\n\n\nclass Transform(ABC):\n    """"""\n    An abstract class of a ``Transform``.\n    A transform is callable that processes ``data``.\n\n    It could be stateful and may modify ``data`` in place,\n    the implementation should be aware of:\n\n        #. thread safety when mutating its own states.\n           When used from a multi-process context, transform\'s instance variables are read-only.\n        #. ``data`` content unused by this transform may still be used in the\n           subsequent transforms in a composed transform.\n        #. storing too much information in ``data`` may not scale.\n\n    See Also\n\n        :py:class:`monai.transforms.Compose`\n    """"""\n\n    @abstractmethod\n    def __call__(self, data, *args, **kwargs):\n        """"""\n        ``data`` is an element which often comes from an iteration over an\n        iterable, such as :py:class:`torch.utils.data.Dataset`. This method should\n        return an updated version of ``data``.\n        To simplify the input validations, most of the transforms assume that\n\n        - ``data`` component is a ""channel-first"" array,\n        - the channel dimension is not omitted even if number of channels is one.\n\n        This method can optionally take additional arguments to help execute transformation operation.\n        """"""\n        raise NotImplementedError\n\n\nclass Randomizable(ABC):\n    """"""\n    An interface for handling local numpy random state.\n    this is mainly for randomized data augmentation transforms.\n    """"""\n\n    R = np.random.RandomState()\n\n    def set_random_state(self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None):\n        """"""\n        Set the random state locally, to control the randomness, the derived\n        classes should use :py:attr:`self.R` instead of `np.random` to introduce random\n        factors.\n\n        Args:\n            seed: set the random state with an integer seed.\n            state (np.random.RandomState): set the random state with a `np.random.RandomState` object.\n\n        Returns:\n            a Randomizable instance.\n        """"""\n        if seed is not None:\n            _seed = id(seed) if not isinstance(seed, int) else seed\n            self.R = np.random.RandomState(_seed)\n            return self\n\n        if state is not None:\n            if not isinstance(state, np.random.RandomState):\n                raise ValueError(f""`state` must be a `np.random.RandomState`, got {type(state)}"")\n            self.R = state\n            return self\n\n        self.R = np.random.RandomState()\n        return self\n\n    @abstractmethod\n    def randomize(self, *args, **kwargs):\n        """"""\n        Within this method, :py:attr:`self.R` should be used, instead of `np.random`, to introduce random factors.\n\n        all :py:attr:`self.R` calls happen here so that we have a better chance to\n        identify errors of sync the random state.\n\n        This method can optionally take additional arguments so that the random factors are generated based on\n        properties of the input data.\n        """"""\n        raise NotImplementedError\n\n\nclass Compose(Randomizable):\n    """"""\n    ``Compose`` provides the ability to chain a series of calls together in a\n    sequence. Each transform in the sequence must take a single argument and\n    return a single value, so that the transforms can be called in a chain.\n\n    ``Compose`` can be used in two ways:\n\n    #. With a series of transforms that accept and return a single\n       ndarray / tensor / tensor-like parameter.\n    #. With a series of transforms that accept and return a dictionary that\n       contains one or more parameters. Such transforms must have pass-through\n       semantics; unused values in the dictionary must be copied to the return\n       dictionary. It is required that the dictionary is copied between input\n       and output of each transform.\n\n    If some transform generates a list batch of data in the transform chain,\n    every item in the list is still a dictionary, and all the following\n    transforms will apply to every item of the list, for example:\n\n    #. transformA normalizes the intensity of \'img\' field in the dict data.\n    #. transformB crops out a list batch of images on \'img\' and \'seg\' field.\n       And constructs a list of dict data, other fields are copied::\n\n            {                          [{                   {\n                \'img\': [1, 2],              \'img\': [1],         \'img\': [2],\n                \'seg\': [1, 2],              \'seg\': [1],         \'seg\': [2],\n                \'extra\': 123,    -->        \'extra\': 123,       \'extra\': 123,\n                \'shape\': \'CHWD\'             \'shape\': \'CHWD\'     \'shape\': \'CHWD\'\n            }                           },                  }]\n\n    #. transformC then randomly rotates or flips \'img\' and \'seg\' fields of\n       every dictionary item in the list.\n\n    The composed transforms will be set the same global random seed if user called\n    `set_determinism()`.\n\n    When using the pass-through dictionary operation, you can make use of\n    :class:`monai.transforms.adaptors.adaptor` to wrap transforms that don\'t conform\n    to the requirements. This approach allows you to use transforms from\n    otherwise incompatible libraries with minimal additional work.\n\n    Note:\n\n        In many cases, Compose is not the best way to create pre-processing\n        pipelines. Pre-processing is often not a strictly sequential series of\n        operations, and much of the complexity arises when a not-sequential\n        set of functions must be called as if it were a sequence.\n\n        Example: images and labels\n        Images typically require some kind of normalisation that labels do not.\n        Both are then typically augmented through the use of random rotations,\n        flips, and deformations.\n        Compose can be used with a series of transforms that take a dictionary\n        that contains \'image\' and \'label\' entries. This might require wrapping\n        `torchvision` transforms before passing them to compose.\n        Alternatively, one can create a class with a `__call__` function that\n        calls your pre-processing functions taking into account that not all of\n        them are called on the labels.\n    """"""\n\n    def __init__(self, transforms=None):\n        if transforms is None:\n            transforms = []\n        if not isinstance(transforms, (list, tuple)):\n            raise ValueError(""Parameters \'transforms\' must be a list or tuple"")\n        self.transforms = transforms\n        self.set_random_state(seed=get_seed())\n\n    def set_random_state(self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None):\n        for _transform in self.transforms:\n            if not isinstance(_transform, Randomizable):\n                continue\n            _transform.set_random_state(seed, state)\n\n    def randomize(self):\n        for _transform in self.transforms:\n            if not isinstance(_transform, Randomizable):\n                continue\n            try:\n                _transform.randomize()\n            except TypeError as type_error:\n                tfm_name: str = type(_transform).__name__\n                warnings.warn(\n                    f\'Transform ""{tfm_name}"" in Compose not randomized\\n{tfm_name}.{type_error}.\', RuntimeWarning,\n                )\n\n    def __call__(self, input_):\n        for _transform in self.transforms:\n            input_ = apply_transform(_transform, input_)\n        return input_\n\n\nclass MapTransform(Transform):\n    """"""\n    A subclass of :py:class:`monai.transforms.Transform` with an assumption\n    that the ``data`` input of ``self.__call__`` is a MutableMapping such as ``dict``.\n\n    The ``keys`` parameter will be used to get and set the actual data\n    item to transform.  That is, the callable of this transform should\n    follow the pattern:\n\n        .. code-block:: python\n\n            def __call__(self, data):\n                for key in self.keys:\n                    if key in data:\n                        # update output data with some_transform_function(data[key]).\n                    else:\n                        # do nothing or some exceptions handling.\n                return data\n\n    """"""\n\n    def __init__(self, keys: Hashable):\n        self.keys = ensure_tuple(keys)\n        if not self.keys:\n            raise ValueError(""keys unspecified"")\n        for key in self.keys:\n            if not isinstance(key, Hashable):\n                raise ValueError(f""keys should be a hashable or a sequence of hashables, got {type(key)}"")\n\n    @abstractmethod\n    def __call__(self, data):\n        raise NotImplementedError\n'"
monai/transforms/utils.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nimport warnings\nfrom typing import Optional, Union, Callable\n\nimport torch\nimport numpy as np\nfrom skimage import measure\n\nfrom monai.utils.misc import ensure_tuple\n\n\ndef rand_choice(prob=0.5):\n    """"""Returns True if a randomly chosen number is less than or equal to `prob`, by default this is a 50/50 chance.""""""\n    return random.random() <= prob\n\n\ndef img_bounds(img):\n    """"""Returns the minimum and maximum indices of non-zero lines in axis 0 of `img`, followed by that for axis 1.""""""\n    ax0 = np.any(img, axis=0)\n    ax1 = np.any(img, axis=1)\n    return np.concatenate((np.where(ax0)[0][[0, -1]], np.where(ax1)[0][[0, -1]]))\n\n\ndef in_bounds(x, y, margin, maxx, maxy):\n    """"""Returns True if (x,y) is within the rectangle (margin, margin, maxx-margin, maxy-margin).""""""\n    return margin <= x < (maxx - margin) and margin <= y < (maxy - margin)\n\n\ndef is_empty(img):\n    """"""Returns True if `img` is empty, that is its maximum value is not greater than its minimum.""""""\n    return not (img.max() > img.min())  # use > instead of <= so that an image full of NaNs will result in True\n\n\ndef zero_margins(img, margin):\n    """"""Returns True if the values within `margin` indices of the edges of `img` in dimensions 1 and 2 are 0.""""""\n    if np.any(img[:, :, :margin]) or np.any(img[:, :, -margin:]):\n        return False\n\n    if np.any(img[:, :margin, :]) or np.any(img[:, -margin:, :]):\n        return False\n\n    return True\n\n\ndef rescale_array(arr, minv=0.0, maxv=1.0, dtype: Optional[np.dtype] = np.float32):\n    """"""Rescale the values of numpy array `arr` to be from `minv` to `maxv`.""""""\n    if dtype is not None:\n        arr = arr.astype(dtype)\n\n    mina = np.min(arr)\n    maxa = np.max(arr)\n\n    if mina == maxa:\n        return arr * minv\n\n    norm = (arr - mina) / (maxa - mina)  # normalize the array first\n    return (norm * (maxv - minv)) + minv  # rescale by minv and maxv, which is the normalized array by default\n\n\ndef rescale_instance_array(arr: np.ndarray, minv: float = 0.0, maxv: float = 1.0, dtype: np.dtype = np.float32):\n    """"""Rescale each array slice along the first dimension of `arr` independently.""""""\n    out: np.ndarray = np.zeros(arr.shape, dtype)\n    for i in range(arr.shape[0]):\n        out[i] = rescale_array(arr[i], minv, maxv, dtype)\n\n    return out\n\n\ndef rescale_array_int_max(arr: np.ndarray, dtype: np.dtype = np.uint16):\n    """"""Rescale the array `arr` to be between the minimum and maximum values of the type `dtype`.""""""\n    info: np.iinfo = np.iinfo(dtype)\n    return rescale_array(arr, info.min, info.max).astype(dtype)\n\n\ndef copypaste_arrays(src, dest, srccenter, destcenter, dims):\n    """"""\n    Calculate the slices to copy a sliced area of array `src` into array `dest`. The area has dimensions `dims` (use 0\n    or None to copy everything in that dimension), the source area is centered at `srccenter` index in `src` and copied\n    into area centered at `destcenter` in `dest`. The dimensions of the copied area will be clipped to fit within the\n    source and destination arrays so a smaller area may be copied than expected. Return value is the tuples of slice\n    objects indexing the copied area in `src`, and those indexing the copy area in `dest`.\n\n    Example\n\n    .. code-block:: python\n\n        src = np.random.randint(0,10,(6,6))\n        dest = np.zeros_like(src)\n        srcslices, destslices = copypaste_arrays(src, dest, (3, 2),(2, 1),(3, 4))\n        dest[destslices] = src[srcslices]\n        print(src)\n        print(dest)\n\n        >>> [[9 5 6 6 9 6]\n             [4 3 5 6 1 2]\n             [0 7 3 2 4 1]\n             [3 0 0 1 5 1]\n             [9 4 7 1 8 2]\n             [6 6 5 8 6 7]]\n            [[0 0 0 0 0 0]\n             [7 3 2 4 0 0]\n             [0 0 1 5 0 0]\n             [4 7 1 8 0 0]\n             [0 0 0 0 0 0]\n             [0 0 0 0 0 0]]\n\n    """"""\n    srcslices = [slice(None)] * src.ndim\n    destslices = [slice(None)] * dest.ndim\n\n    for i, ss, ds, sc, dc, dim in zip(range(src.ndim), src.shape, dest.shape, srccenter, destcenter, dims):\n        if dim:\n            # dimension before midpoint, clip to size fitting in both arrays\n            d1 = np.clip(dim // 2, 0, min(sc, dc))\n            # dimension after midpoint, clip to size fitting in both arrays\n            d2 = np.clip(dim // 2 + 1, 0, min(ss - sc, ds - dc))\n\n            srcslices[i] = slice(sc - d1, sc + d2)\n            destslices[i] = slice(dc - d1, dc + d2)\n\n    return tuple(srcslices), tuple(destslices)\n\n\ndef resize_center(img, *resize_dims, fill_value=0):\n    """"""\n    Resize `img` by cropping or expanding the image from the center. The `resize_dims` values are the output dimensions\n    (or None to use original dimension of `img`). If a dimension is smaller than that of `img` then the result will be\n    cropped and if larger padded with zeros, in both cases this is done relative to the center of `img`. The result is\n    a new image with the specified dimensions and values from `img` copied into its center.\n    """"""\n    resize_dims = tuple(resize_dims[i] or img.shape[i] for i in range(len(resize_dims)))\n\n    dest = np.full(resize_dims, fill_value, img.dtype)\n    half_img_shape = np.asarray(img.shape) // 2\n    half_dest_shape = np.asarray(dest.shape) // 2\n\n    srcslices, destslices = copypaste_arrays(img, dest, half_img_shape, half_dest_shape, resize_dims)\n    dest[destslices] = img[srcslices]\n\n    return dest\n\n\ndef one_hot(labels, num_classes):\n    """"""\n    Converts label image `labels` to a one-hot vector with `num_classes` number of channels as last dimension.\n    """"""\n    labels = labels % num_classes\n    y = np.eye(num_classes)\n    onehot = y[labels.flatten()]\n\n    return onehot.reshape(tuple(labels.shape) + (num_classes,)).astype(labels.dtype)\n\n\ndef generate_pos_neg_label_crop_centers(\n    label: np.ndarray,\n    size,\n    num_samples: int,\n    pos_ratio: float,\n    image: Optional[np.ndarray] = None,\n    image_threshold: Union[int, float] = 0,\n    rand_state: np.random.RandomState = np.random,\n):\n    """"""Generate valid sample locations based on image with option for specifying foreground ratio\n    Valid: samples sitting entirely within image, expected input shape: [C, H, W, D] or [C, H, W]\n\n    Args:\n        label (numpy.ndarray): use the label data to get the foreground/background information.\n        size (list or tuple): size of the ROIs to be sampled.\n        num_samples: total sample centers to be generated.\n        pos_ratio (float): ratio of total locations generated that have center being foreground.\n        image (numpy.ndarray): if image is not None, use ``label = 0 & image > image_threshold``\n            to select background. so the crop center will only exist on valid image area.\n        image_threshold (int or float): if enabled image_key, use ``image > image_threshold`` to\n            determine the valid image content area.\n        rand_state (random.RandomState): numpy randomState object to align with other modules.\n    """"""\n    max_size = label.shape[1:]\n    assert len(max_size) == len(size), f""expected size ({len(max_size)}) does not match label dim ({len(size)}).""\n    assert (np.subtract(max_size, size) >= 0).all(), ""proposed roi is larger than image itself.""\n\n    # Select subregion to assure valid roi\n    valid_start = np.floor_divide(size, 2)\n    valid_end = np.subtract(max_size + np.array(1), size / np.array(2)).astype(np.uint16)  # add 1 for random\n    # int generation to have full range on upper side, but subtract unfloored size/2 to prevent rounded range\n    # from being too high\n    for i in range(len(valid_start)):  # need this because np.random.randint does not work with same start and end\n        if valid_start[i] == valid_end[i]:\n            valid_end[i] += 1\n\n    # Prepare fg/bg indices\n    label_flat = np.any(label, axis=0).ravel()  # in case label has multiple dimensions\n    fg_indices = np.nonzero(label_flat)[0]\n    if image is not None:\n        img_flat = np.any(image > image_threshold, axis=0).ravel()\n        bg_indices = np.nonzero(np.logical_and(img_flat, ~label_flat))[0]\n    else:\n        bg_indices = np.nonzero(~label_flat)[0]\n\n    if not len(fg_indices) or not len(bg_indices):\n        if not len(fg_indices) and not len(bg_indices):\n            raise ValueError(""no sampling location available."")\n        warnings.warn(\n            ""N foreground {}, N  background {}, unable to generate class balanced samples."".format(\n                len(fg_indices), len(bg_indices)\n            )\n        )\n        pos_ratio = 0 if not len(fg_indices) else 1\n\n    centers = []\n    for _ in range(num_samples):\n        indices_to_use = fg_indices if rand_state.rand() < pos_ratio else bg_indices\n        random_int = rand_state.randint(len(indices_to_use))\n        center = np.unravel_index(indices_to_use[random_int], label.shape)\n        center = center[1:]\n        # shift center to range of valid centers\n        center_ori = [c for c in center]\n        for i, c in enumerate(center):\n            center_i = c\n            if c < valid_start[i]:\n                center_i = valid_start[i]\n            if c >= valid_end[i]:\n                center_i = valid_end[i] - 1\n            center_ori[i] = center_i\n        centers.append(center_ori)\n\n    return centers\n\n\ndef apply_transform(transform: Callable, data, map_items: bool = True):\n    """"""\n    Transform `data` with `transform`.\n    If `data` is a list or tuple and `map_data` is True, each item of `data` will be transformed\n    and this method returns a list of outcomes.\n    otherwise transform will be applied once with `data` as the argument.\n\n    Args:\n        transform (callable): a callable to be used to transform `data`\n        data (object): an object to be transformed.\n        map_item (bool): whether to apply transform to each item in `data`,\n            if `data` is a list or tuple. Defaults to True.\n    """"""\n    try:\n        if isinstance(data, (list, tuple)) and map_items:\n            return [transform(item) for item in data]\n        return transform(data)\n    except Exception as e:\n        raise type(e)(f""applying transform {transform}."").with_traceback(e.__traceback__)\n\n\ndef create_grid(spatial_size, spacing=None, homogeneous: bool = True, dtype: np.dtype = float):\n    """"""\n    compute a `spatial_size` mesh.\n\n    Args:\n        spatial_size (sequence of ints): spatial size of the grid.\n        spacing (sequence of ints): same len as ``spatial_size``, defaults to 1.0 (dense grid).\n        homogeneous (bool): whether to make homogeneous coordinates.\n        dtype (type): output grid data type.\n    """"""\n    spacing = spacing or tuple(1.0 for _ in spatial_size)\n    ranges = [np.linspace(-(d - 1.0) / 2.0 * s, (d - 1.0) / 2.0 * s, int(d)) for d, s in zip(spatial_size, spacing)]\n    coords = np.asarray(np.meshgrid(*ranges, indexing=""ij""), dtype=dtype)\n    if not homogeneous:\n        return coords\n    return np.concatenate([coords, np.ones_like(coords[:1])])\n\n\ndef create_control_grid(spatial_shape, spacing, homogeneous: bool = True, dtype: Optional[np.dtype] = float):\n    """"""\n    control grid with two additional point in each direction\n    """"""\n    grid_shape = []\n    for d, s in zip(spatial_shape, spacing):\n        d = int(d)\n        if d % 2 == 0:\n            grid_shape.append(np.ceil((d - 1.0) / (2.0 * s) + 0.5) * 2.0 + 2.0)\n        else:\n            grid_shape.append(np.ceil((d - 1.0) / (2.0 * s)) * 2.0 + 3.0)\n    return create_grid(grid_shape, spacing, homogeneous, dtype)\n\n\ndef create_rotate(spatial_dims: int, radians):\n    """"""\n    create a 2D or 3D rotation matrix\n\n    Args:\n        spatial_dims (2|3): spatial rank\n        radians (float or a sequence of floats): rotation radians\n        when spatial_dims == 3, the `radians` sequence corresponds to\n        rotation in the 1st, 2nd, and 3rd dim respectively.\n    """"""\n    radians = ensure_tuple(radians)\n    if spatial_dims == 2:\n        if len(radians) >= 1:\n            sin_, cos_ = np.sin(radians[0]), np.cos(radians[0])\n            return np.array([[cos_, -sin_, 0.0], [sin_, cos_, 0.0], [0.0, 0.0, 1.0]])\n\n    if spatial_dims == 3:\n        affine = None\n        if len(radians) >= 1:\n            sin_, cos_ = np.sin(radians[0]), np.cos(radians[0])\n            affine = np.array(\n                [[1.0, 0.0, 0.0, 0.0], [0.0, cos_, -sin_, 0.0], [0.0, sin_, cos_, 0.0], [0.0, 0.0, 0.0, 1.0]]\n            )\n        if len(radians) >= 2:\n            sin_, cos_ = np.sin(radians[1]), np.cos(radians[1])\n            affine = affine @ np.array(\n                [[cos_, 0.0, sin_, 0.0], [0.0, 1.0, 0.0, 0.0], [-sin_, 0.0, cos_, 0.0], [0.0, 0.0, 0.0, 1.0]]\n            )\n        if len(radians) >= 3:\n            sin_, cos_ = np.sin(radians[2]), np.cos(radians[2])\n            affine = affine @ np.array(\n                [[cos_, -sin_, 0.0, 0.0], [sin_, cos_, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\n            )\n        return affine\n\n    raise ValueError(f""create_rotate got spatial_dims={spatial_dims}, radians={radians}."")\n\n\ndef create_shear(spatial_dims: int, coefs):\n    """"""\n    create a shearing matrix\n    Args:\n        spatial_dims: spatial rank\n        coefs (floats): shearing factors, defaults to 0.\n    """"""\n    coefs = list(ensure_tuple(coefs))\n    if spatial_dims == 2:\n        while len(coefs) < 2:\n            coefs.append(0.0)\n        return np.array([[1, coefs[0], 0.0], [coefs[1], 1.0, 0.0], [0.0, 0.0, 1.0]])\n    if spatial_dims == 3:\n        while len(coefs) < 6:\n            coefs.append(0.0)\n        return np.array(\n            [\n                [1.0, coefs[0], coefs[1], 0.0],\n                [coefs[2], 1.0, coefs[3], 0.0],\n                [coefs[4], coefs[5], 1.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n            ]\n        )\n    raise NotImplementedError\n\n\ndef create_scale(spatial_dims: int, scaling_factor):\n    """"""\n    create a scaling matrix\n    Args:\n        spatial_dims: spatial rank\n        scaling_factor (floats): scaling factors, defaults to 1.\n    """"""\n    scaling_factor = list(ensure_tuple(scaling_factor))\n    while len(scaling_factor) < spatial_dims:\n        scaling_factor.append(1.0)\n    return np.diag(scaling_factor[:spatial_dims] + [1.0])\n\n\ndef create_translate(spatial_dims: int, shift):\n    """"""\n    create a translation matrix\n    Args:\n        spatial_dims: spatial rank\n        shift (floats): translate factors, defaults to 0.\n    """"""\n    shift = ensure_tuple(shift)\n    affine = np.eye(spatial_dims + 1)\n    for i, a in enumerate(shift[:spatial_dims]):\n        affine[i, spatial_dims] = a\n    return affine\n\n\ndef generate_spatial_bounding_box(\n    img: np.ndarray, select_fn: Callable = lambda x: x > 0, channel_indexes=None, margin: int = 0\n):\n    """"""\n    generate the spatial bounding box of foreground in the image with start-end positions.\n    Users can define arbitrary function to select expected foreground from the whole image or specified channels.\n    And it can also add margin to every dim of the bounding box.\n\n    Args:\n        img (ndarrary): source image to generate bounding box from.\n        select_fn (Callable): function to select expected foreground, default is to select values > 0.\n        channel_indexes (int, tuple or list): if defined, select foreground only on the specified channels\n            of image. if None, select foreground on the whole image.\n        margin: add margin to all dims of the bounding box.\n    """"""\n    assert isinstance(margin, int), ""margin must be int type.""\n    data = img[[*(ensure_tuple(channel_indexes))]] if channel_indexes is not None else img\n    data = np.any(select_fn(data), axis=0)\n    nonzero_idx = np.nonzero(data)\n\n    box_start = list()\n    box_end = list()\n    for i in range(data.ndim):\n        assert len(nonzero_idx[i]) > 0, f""did not find nonzero index at spatial dim {i}""\n        box_start.append(max(0, np.min(nonzero_idx[i]) - margin))\n        box_end.append(min(data.shape[i], np.max(nonzero_idx[i]) + margin + 1))\n    return box_start, box_end\n\n\ndef get_largest_connected_component_mask(img, connectivity: Optional[int] = None):\n    """"""\n    Gets the largest connected component mask of an image.\n\n    Args:\n        img: Image to get largest connected component from. Shape is (batch_size, spatial_dim1 [, spatial_dim2, ...])\n        connectivity: Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\n            Accepted values are ranging from  1 to input.ndim. If ``None``, a full\n            connectivity of ``input.ndim`` is used.\n    """"""\n    img_arr = img.detach().cpu().numpy()\n    largest_cc = np.zeros(shape=img_arr.shape, dtype=img_arr.dtype)\n    for i, item in enumerate(img_arr):\n        item = measure.label(item, connectivity=connectivity)\n        if item.max() != 0:\n            largest_cc[i, ...] = item == (np.argmax(np.bincount(item.flat)[1:]) + 1)\n    return torch.as_tensor(largest_cc, device=img.device)\n'"
monai/utils/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# have to explicitly bring these in here to resolve circular import issues\nfrom .module import export\nfrom .decorators import *\nfrom .misc import *\n'"
monai/utils/aliases.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nThis module is written for configurable workflow, not currently in use.\n""""""\n\nimport threading\nimport sys\nimport inspect\nimport importlib\n\n\nalias_lock = threading.RLock()\nGlobalAliases = {}\n\n\ndef alias(*names):\n    """"""\n    Stores the decorated function or class in the global aliases table under the given names and as the `__aliases__`\n    member of the decorated object. This new member will contain all alias names declared for that object.\n    """"""\n\n    def _outer(obj):\n        for n in names:\n            with alias_lock:\n                GlobalAliases[n] = obj\n\n        # set the member list __aliases__ to contain the alias names defined by the decorator for `obj`\n        obj.__aliases__ = getattr(obj, ""__aliases__"", ()) + tuple(names)\n\n        return obj\n\n    return _outer\n\n\ndef resolve_name(name):\n    """"""\n    Search for the declaration (function or class) with the given name. This will first search the list of aliases to\n    see if it was declared with this aliased name, then search treating `name` as a fully qualified name, then search\n    the loaded modules for one having a declaration with the given name. If no declaration is found, raise ValueError.\n    """"""\n    # attempt to resolve an alias\n    with alias_lock:\n        obj = GlobalAliases.get(name, None)\n\n    assert name not in GlobalAliases or obj is not None\n\n    # attempt to resolve a qualified name\n    if obj is None and ""."" in name:\n        modname, declname = name.rsplit(""."", 1)\n\n        try:\n            mod = importlib.import_module(modname)\n            obj = getattr(mod, declname, None)\n        except ModuleNotFoundError:\n            raise ValueError(f""Module {modname!r} not found"")\n\n        if obj is None:\n            raise ValueError(f""Module {modname!r} does not have member {declname!r}"")\n\n    # attempt to resolve a simple name\n    if obj is None:\n        # Get all modules having the declaration/import, need to check here that getattr returns something which doesn\'t\n        # equate to False since in places __getattr__ returns 0 incorrectly:\n        # https://github.com/tensorflow/tensorboard/blob/a22566561d2b4fea408755a951ac9eaf3a156f8e/tensorboard/compat/tensorflow_stub/pywrap_tensorflow.py#L35  # noqa: B950\n        mods = [m for m in list(sys.modules.values()) if getattr(m, name, None)]\n\n        if len(mods) > 0:  # found modules with this declaration or import\n            if len(mods) > 1:  # found multiple modules, need to determine if ambiguous or just multiple imports\n                foundmods = {inspect.getmodule(getattr(m, name)) for m in mods}  # resolve imports\n                foundmods = {m for m in foundmods if m is not None}\n\n                if len(foundmods) > 1:  # found multiple declarations with the same name\n                    modnames = [m.__name__ for m in foundmods]\n                    msg = ""Multiple modules (%r) with declaration name %r found, resolution is ambiguous"" % (\n                        modnames,\n                        name,\n                    )\n                    raise ValueError(msg)\n                else:\n                    mods = list(foundmods)\n\n            obj = getattr(mods[0], name)\n\n        if obj is None:\n            raise ValueError(f""No module with member {name!r} found"")\n\n    return obj\n'"
monai/utils/decorators.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nfrom functools import wraps\n\n\ndef timing(func):\n    """"""\n    This simple timing function decorator prints to stdout/logfile (it uses printFlush) how many seconds a call to the\n    original function took to execute, as well as the name before and after the call.\n    """"""\n\n    @wraps(func)\n    def timingwrap(*args, **kwargs):\n        print(func.__name__, flush=True)\n        start = time.time()\n        res = func(*args, **kwargs)\n        end = time.time()\n        print(func.__name__, ""dT (s) ="", (end - start), flush=True)\n        return res\n\n    return timingwrap\n\n\nclass RestartGenerator:\n    """"""\n    Wraps a generator callable which will be called whenever this class is iterated and its result returned. This is\n    used to create an iterator which can start iteration over the given generator multiple times.\n    """"""\n\n    def __init__(self, create_gen):\n        self.create_gen = create_gen\n\n    def __iter__(self):\n        return self.create_gen()\n\n\nclass MethodReplacer(object):\n    """"""\n    Base class for method decorators which can be used to replace methods pass to replace_method() with wrapped versions.\n    """"""\n\n    replace_list_name = ""__replacemethods__""\n\n    def __init__(self, meth):\n        self.meth = meth\n\n    def replace_method(self, meth):\n        """"""Return a new method to replace `meth` in the instantiated object, or `meth` to do nothing.""""""\n        return meth\n\n    def __set_name__(self, owner, name):\n        """"""\n        Add the (name,self.replace_method) pair to the list named by replace_list_name in `owner`, creating the list and\n        replacing the constructor of `owner` if necessary. The replaced constructor will call the old one then do the\n        replacing operation of substituting, for each (name,self.replace_method) pair, the named method with the returned\n        value from self.replace_method.\n        """"""\n        entry = (name, owner, self.replace_method)\n\n        if not hasattr(owner, self.replace_list_name):\n            oldinit = owner.__init__\n\n            # replace the constructor with a new one which calls the old then replaces methods\n            @wraps(oldinit)\n            def newinit(_self, *args, **kwargs):\n                oldinit(_self, *args, **kwargs)\n\n                # replace each listed method of this newly constructed object\n                for m, owner, replacer in getattr(_self, self.replace_list_name):\n                    if isinstance(_self, owner):\n                        meth = getattr(_self, m)\n                        newmeth = replacer(meth)\n                        setattr(_self, m, newmeth)\n\n            owner.__init__ = newinit\n            setattr(owner, self.replace_list_name, [entry])\n        else:\n            namelist = getattr(owner, self.replace_list_name)\n\n            if not any(nl[0] == name for nl in namelist):\n                namelist.append(entry)\n\n        setattr(owner, name, self.meth)\n'"
monai/utils/misc.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nfrom collections.abc import Iterable\n\nimport numpy as np\nimport torch\nimport random\n\n_seed = None\n\n\ndef zip_with(op, *vals, mapfunc=map):\n    """"""\n    Map `op`, using `mapfunc`, to each tuple derived from zipping the iterables in `vals`.\n    """"""\n    return mapfunc(op, zip(*vals))\n\n\ndef star_zip_with(op, *vals):\n    """"""\n    Use starmap as the mapping function in zipWith.\n    """"""\n    return zip_with(op, *vals, mapfunc=itertools.starmap)\n\n\ndef first(iterable, default=None):\n    """"""\n    Returns the first item in the given iterable or `default` if empty, meaningful mostly with \'for\' expressions.\n    """"""\n    for i in iterable:\n        return i\n    return default\n\n\ndef issequenceiterable(obj):\n    """"""\n    Determine if the object is an iterable sequence and is not a string\n    """"""\n    return isinstance(obj, Iterable) and not isinstance(obj, str)\n\n\ndef ensure_tuple(vals):\n    """"""Returns a tuple of `vals`""""""\n    if not issequenceiterable(vals):\n        vals = (vals,)\n\n    return tuple(vals)\n\n\ndef ensure_tuple_size(tup, dim):\n    """"""Returns a copy of `tup` with `dim` values by either shortened or padded with zeros as necessary.""""""\n    tup = tuple(tup) + (0,) * dim\n    return tup[:dim]\n\n\ndef ensure_tuple_rep(tup, dim):\n    """"""\n    Returns a copy of `tup` with `dim` values by either shortened or duplicated input.\n    """"""\n    if not issequenceiterable(tup):\n        return (tup,) * dim\n    elif len(tup) == dim:\n        return tuple(tup)\n\n    raise ValueError(f""sequence must have length {dim}, got length {len(tup)}."")\n\n\ndef is_scalar_tensor(val):\n    if torch.is_tensor(val) and val.ndim == 0:\n        return True\n    return False\n\n\ndef is_scalar(val):\n    if torch.is_tensor(val) and val.ndim == 0:\n        return True\n    return np.isscalar(val)\n\n\ndef process_bar(index: int, count: int, bar_len: int = 30, newline: bool = False):\n    """"""print a process bar to track some time consuming task.\n\n    Args:\n        index: current satus in process.\n        count: total steps of the process.\n        bar_len: the total length of the bar on screen, default is 30 char.\n        newline (bool): whether to print in a new line for every index.\n    """"""\n    end = ""\\r"" if newline is False else ""\\r\\n""\n    filled_len = int(bar_len * index // count)\n    bar = ""["" + ""="" * filled_len + "" "" * (bar_len - filled_len) + ""]""\n    print(f""{index}/{count} {bar:s}  "", end=end)\n    if index == count:\n        print("""")\n\n\ndef get_seed():\n    return _seed\n\n\ndef set_determinism(seed=np.iinfo(np.int32).max, additional_settings=None):\n    """"""\n    Set random seed for modules to enable or disable deterministic training.\n\n    Args:\n        seed (None, int): the random seed to use, default is np.iinfo(np.int32).max.\n            It is recommended to set a large seed, i.e. a number that has a good balance\n            of 0 and 1 bits. Avoid having many 0 bits in the seed.\n            if set to None, will disable deterministic training.\n        additional_settings (Callable, list or tuple of Callables): additional settings\n            that need to set random seed.\n\n    """"""\n    if seed is None:\n        # cast to 32 bit seed for CUDA\n        seed_ = torch.default_generator.seed() % (np.iinfo(np.int32).max + 1)\n        if not torch.cuda._is_in_bad_fork():\n            torch.cuda.manual_seed_all(seed_)\n    else:\n        torch.manual_seed(seed)\n\n    global _seed\n    _seed = seed\n    random.seed(seed)\n    np.random.seed(seed)\n\n    if additional_settings is not None:\n        additional_settings = ensure_tuple(additional_settings)\n        for func in additional_settings:\n            func(seed)\n\n    if seed is not None:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    else:\n        torch.backends.cudnn.deterministic = False\n'"
monai/utils/module.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom importlib import import_module\nfrom pkgutil import walk_packages\nfrom re import match\n\n\ndef export(modname):\n    """"""\n    Make the decorated object a member of the named module. This will also add the object under its aliases if it has\n    a `__aliases__` member, thus this decorator should be before the `alias` decorator to pick up those names. Alias\n    names which conflict with package names or existing members will be ignored.\n    """"""\n\n    def _inner(obj):\n        mod = import_module(modname)\n        if not hasattr(mod, obj.__name__):\n            setattr(mod, obj.__name__, obj)\n\n            # add the aliases for `obj` to the target module\n            for alias in getattr(obj, ""__aliases__"", ()):\n                if not hasattr(mod, alias):\n                    setattr(mod, alias, obj)\n\n        return obj\n\n    return _inner\n\n\ndef load_submodules(basemod, load_all: bool = True, exclude_pattern: str = ""(.*[tT]est.*)|(_.*)""):\n    """"""\n    Traverse the source of the module structure starting with module `basemod`, loading all packages plus all files if\n    `loadAll` is True, excluding anything whose name matches `excludePattern`.\n    """"""\n    submodules = []\n\n    for importer, name, is_pkg in walk_packages(basemod.__path__):\n        if (is_pkg or load_all) and match(exclude_pattern, name) is None:\n            mod = import_module(basemod.__name__ + ""."" + name)  # why do I need to do this first?\n            importer.find_module(name).load_module(name)\n            submodules.append(mod)\n\n    return submodules\n\n\n@export(""monai.utils"")\ndef get_full_type_name(typeobj):\n    module = typeobj.__module__\n    if module is None or module == str.__class__.__module__:\n        return typeobj.__name__  # Avoid reporting __builtin__\n    else:\n        return module + ""."" + typeobj.__name__\n'"
monai/visualize/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .img2tensorboard import *\n'"
monai/visualize/img2tensorboard.py,8,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport numpy as np\nimport PIL\nfrom PIL.GifImagePlugin import Image as GifImage\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tensorboard.compat.proto import summary_pb2\nfrom monai.transforms.utils import rescale_array\n\nfrom typing import Sequence, Union, Optional\n\n\ndef _image3_animated_gif(tag: str, image: Union[np.ndarray, torch.Tensor], scale_factor: float = 1):\n    """"""Function to actually create the animated gif.\n    Args:\n        tag: Data identifier\n        image: 3D image tensors expected to be in `HWD` format\n        scale_factor: amount to multiply values by. if the image data is between 0 and 1, using 255 for this value will\n        scale it to displayable range\n    """"""\n    assert len(image.shape) == 3, ""3D image tensors expected to be in `HWD` format, len(image.shape) != 3""\n\n    ims = [(np.asarray((image[:, :, i])) * scale_factor).astype(np.uint8) for i in range(image.shape[2])]\n    ims = [GifImage.fromarray(im) for im in ims]\n    img_str = b""""\n    for b_data in PIL.GifImagePlugin.getheader(ims[0])[0]:\n        img_str += b_data\n    img_str += b""\\x21\\xFF\\x0B\\x4E\\x45\\x54\\x53\\x43\\x41\\x50"" b""\\x45\\x32\\x2E\\x30\\x03\\x01\\x00\\x00\\x00""\n    for i in ims:\n        for b_data in PIL.GifImagePlugin.getdata(i):\n            img_str += b_data\n    img_str += b""\\x3B""\n    summary_image_str = summary_pb2.Summary.Image(height=10, width=10, colorspace=1, encoded_image_string=img_str)\n    image_summary = summary_pb2.Summary.Value(tag=tag, image=summary_image_str)\n    return summary_pb2.Summary(value=[image_summary])\n\n\ndef make_animated_gif_summary(\n    tag: str,\n    image: Union[np.ndarray, torch.Tensor],\n    max_out: int = 3,\n    animation_axes: Sequence[int] = (3,),\n    image_axes: Sequence[int] = (1, 2),\n    other_indices=None,\n    scale_factor: float = 1,\n):\n    """"""Creates an animated gif out of an image tensor in \'CHWD\' format and returns Summary.\n\n    Args:\n        tag: Data identifier\n        image: The image, expected to be in CHWD format\n        max_out: maximum number of slices to animate through\n        animation_axes: axis to animate on (not currently used)\n        image_axes: axes of image (not currently used)\n        other_indices: (not currently used)\n        scale_factor: amount to multiply values by.\n            if the image data is between 0 and 1, using 255 for this value will scale it to displayable range\n    """"""\n\n    if max_out == 1:\n        suffix = ""/image""\n    else:\n        suffix = ""/image/{}""\n    if other_indices is None:\n        other_indices = {}\n    axis_order = [0] + list(animation_axes) + list(image_axes)\n\n    slicing = []\n    for i in range(len(image.shape)):\n        if i in axis_order:\n            slicing.append(slice(None))\n        else:\n            other_ind = other_indices.get(i, 0)\n            slicing.append(slice(other_ind, other_ind + 1))\n    image = image[tuple(slicing)]\n\n    for it_i in range(min(max_out, list(image.shape)[0])):\n        one_channel_img: Union[torch.Tensor, np.ndarray] = image[it_i, :, :, :].squeeze(dim=0) if torch.is_tensor(\n            image\n        ) else image[it_i, :, :, :]\n        summary_op = _image3_animated_gif(tag + suffix.format(it_i), one_channel_img, scale_factor)\n    return summary_op\n\n\ndef add_animated_gif(\n    writer: SummaryWriter,\n    tag: str,\n    image_tensor: Union[np.ndarray, torch.Tensor],\n    max_out: int,\n    scale_factor: float,\n    global_step: Optional[int] = None,\n):\n    """"""Creates an animated gif out of an image tensor in \'CHWD\' format and writes it with SummaryWriter.\n\n    Args:\n        writer: Tensorboard SummaryWriter to write to\n        tag: Data identifier\n        image_tensor: tensor for the image to add, expected to be in CHWD format\n        max_out: maximum number of slices to animate through\n        scale_factor: amount to multiply values by. If the image data is between 0 and 1, using 255 for this value will\n            scale it to displayable range\n        global_step: Global step value to record\n    """"""\n    writer._get_file_writer().add_summary(\n        make_animated_gif_summary(\n            tag, image_tensor, max_out=max_out, animation_axes=[1], image_axes=[2, 3], scale_factor=scale_factor\n        ),\n        global_step,\n    )\n\n\ndef add_animated_gif_no_channels(\n    writer: SummaryWriter,\n    tag: str,\n    image_tensor: Union[np.ndarray, torch.Tensor],\n    max_out: int,\n    scale_factor: float,\n    global_step: Optional[int] = None,\n):\n    """"""Creates an animated gif out of an image tensor in \'HWD\' format that does not have\n    a channel dimension and writes it with SummaryWriter. This is similar to the ""add_animated_gif""\n    after inserting a channel dimension of 1.\n\n    Args:\n        writer: Tensorboard SummaryWriter to write to\n        tag: Data identifier\n        image_tensor: tensor for the image to add, expected to be in CHWD format\n        max_out: maximum number of slices to animate through\n        scale_factor: amount to multiply values by. If the image data is between 0 and 1,\n                              using 255 for this value will scale it to displayable range\n        global_step: Global step value to record\n    """"""\n    writer._get_file_writer().add_summary(\n        make_animated_gif_summary(\n            tag, image_tensor, max_out=max_out, animation_axes=[1], image_axes=[1, 2], scale_factor=scale_factor,\n        ),\n        global_step,\n    )\n\n\ndef plot_2d_or_3d_image(\n    data: Union[torch.Tensor, np.ndarray],\n    step: int,\n    writer: SummaryWriter,\n    index: int = 0,\n    max_channels: int = 1,\n    max_frames: int = 64,\n    tag: str = ""output"",\n):\n    """"""Plot 2D or 3D image on the TensorBoard, 3D image will be converted to GIF image.\n\n    Note:\n        Plot 3D or 2D image(with more than 3 channels) as separate images.\n\n    Args:\n        data (Tensor or np.array): target data to be plotted as image on the TensorBoard.\n            The data is expected to have \'NCHW[D]\' dimensions, and only plot the first in the batch.\n        step: current step to plot in a chart.\n        writer (SummaryWriter): specify TensorBoard SummaryWriter to plot the image.\n        index: plot which element in the input data batch, default is the first element.\n        max_channels: number of channels to plot.\n        max_frames: number of frames for 2D-t plot.\n        tag (str): tag of the plotted image on TensorBoard.\n    """"""\n    assert isinstance(writer, SummaryWriter) is True, ""must provide a TensorBoard SummaryWriter.""\n    d = data[index]\n    if torch.is_tensor(d):\n        d = d.detach().cpu().numpy()\n\n    if d.ndim == 2:\n        d = rescale_array(d, 0, 1)\n        dataformats = ""HW""\n        writer.add_image(f""{tag}_{dataformats}"", d, step, dataformats=dataformats)\n        return\n\n    if d.ndim == 3:\n        if d.shape[0] == 3 and max_channels == 3:  # RGB\n            dataformats = ""CHW""\n            writer.add_image(f""{tag}_{dataformats}"", d, step, dataformats=dataformats)\n            return\n        for j, d2 in enumerate(d[:max_channels]):\n            d2 = rescale_array(d2, 0, 1)\n            dataformats = ""HW""\n            writer.add_image(f""{tag}_{dataformats}_{j}"", d2, step, dataformats=dataformats)\n        return\n\n    if d.ndim >= 4:\n        spatial = d.shape[-3:]\n        for j, d3 in enumerate(d.reshape([-1] + list(spatial))[:max_channels]):\n            d3 = rescale_array(d3, 0, 255)\n            add_animated_gif(writer, f""{tag}_HWD_{j}"", d3[None], max_frames, 1.0, step)\n        return\n'"
monai/networks/blocks/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .convolutions import *\n'"
monai/networks/blocks/convolutions.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch.nn as nn\n\nfrom monai.networks.layers.factories import Dropout, Norm, Act, Conv, split_args\nfrom monai.networks.layers.convutils import same_padding\n\n\nclass Convolution(nn.Sequential):\n    """"""\n    Constructs a convolution with optional dropout, normalization, and activation layers.\n    """"""\n\n    def __init__(\n        self,\n        dimensions,\n        in_channels,\n        out_channels,\n        strides=1,\n        kernel_size=3,\n        act=Act.PRELU,\n        norm=Norm.INSTANCE,\n        dropout=None,\n        dilation=1,\n        bias=True,\n        conv_only=False,\n        is_transposed=False,\n    ):\n        super().__init__()\n        self.dimensions = dimensions\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.is_transposed = is_transposed\n\n        padding = same_padding(kernel_size, dilation)\n        conv_type = Conv[Conv.CONVTRANS if is_transposed else Conv.CONV, dimensions]\n\n        # define the normalisation type and the arguments to the constructor\n        norm_name, norm_args = split_args(norm)\n        norm_type = Norm[norm_name, dimensions]\n\n        # define the activation type and the arguments to the constructor\n        act_name, act_args = split_args(act)\n        act_type = Act[act_name]\n\n        if dropout:\n            # if dropout was specified simply as a p value, use default name and make a keyword map with the value\n            if isinstance(dropout, (int, float)):\n                drop_name = Dropout.DROPOUT\n                drop_args = {""p"": dropout}\n            else:\n                drop_name, drop_args = split_args(dropout)\n\n            drop_type = Dropout[drop_name, dimensions]\n\n        if is_transposed:\n            conv = conv_type(in_channels, out_channels, kernel_size, strides, padding, strides - 1, 1, bias, dilation)\n        else:\n            conv = conv_type(in_channels, out_channels, kernel_size, strides, padding, dilation, bias=bias)\n\n        self.add_module(""conv"", conv)\n\n        if not conv_only:\n            self.add_module(""norm"", norm_type(out_channels, **norm_args))\n            if dropout:\n                self.add_module(""dropout"", drop_type(**drop_args))\n\n            self.add_module(""act"", act_type(**act_args))\n\n\nclass ResidualUnit(nn.Module):\n    def __init__(\n        self,\n        dimensions,\n        in_channels,\n        out_channels,\n        strides=1,\n        kernel_size=3,\n        subunits=2,\n        act=Act.PRELU,\n        norm=Norm.INSTANCE,\n        dropout=None,\n        dilation=1,\n        bias=True,\n        last_conv_only=False,\n    ):\n        super().__init__()\n        self.dimensions = dimensions\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.conv = nn.Sequential()\n        self.residual = nn.Identity()\n\n        padding = same_padding(kernel_size, dilation)\n        schannels = in_channels\n        sstrides = strides\n        subunits = max(1, subunits)\n\n        for su in range(subunits):\n            conv_only = last_conv_only and su == (subunits - 1)\n            unit = Convolution(\n                dimensions,\n                schannels,\n                out_channels,\n                sstrides,\n                kernel_size,\n                act,\n                norm,\n                dropout,\n                dilation,\n                bias,\n                conv_only,\n            )\n\n            self.conv.add_module(f""unit{su:d}"", unit)\n\n            # after first loop set channels and strides to what they should be for subsequent units\n            schannels = out_channels\n            sstrides = 1\n\n        # apply convolution to input to change number of output channels and size to match that coming from self.conv\n        if np.prod(strides) != 1 or in_channels != out_channels:\n            rkernel_size = kernel_size\n            rpadding = padding\n\n            if np.prod(strides) == 1:  # if only adapting number of channels a 1x1 kernel is used with no padding\n                rkernel_size = 1\n                rpadding = 0\n\n            conv_type = Conv[Conv.CONV, dimensions]\n            self.residual = conv_type(in_channels, out_channels, rkernel_size, strides, rpadding, bias=bias)\n\n    def forward(self, x):\n        res = self.residual(x)  # create the additive residual from x\n        cx = self.conv(x)  # apply x to sequence of operations\n        return cx + res  # add the residual to the output\n'"
monai/networks/layers/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .convutils import *\nfrom .factories import *\nfrom .simplelayers import *\nfrom .spatial_transforms import *\n'"
monai/networks/layers/convutils.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\n__all__ = [""same_padding"", ""calculate_out_shape"", ""gaussian_1d""]\n\n\ndef same_padding(kernel_size, dilation=1):\n    """"""\n    Return the padding value needed to ensure a convolution using the given kernel size produces an output of the same\n    shape as the input for a stride of 1, otherwise ensure a shape of the input divided by the stride rounded down.\n    """"""\n    kernel_size = np.atleast_1d(kernel_size)\n    padding = ((kernel_size - 1) // 2) + (dilation - 1)\n    padding = tuple(int(p) for p in padding)\n\n    return tuple(padding) if len(padding) > 1 else padding[0]\n\n\ndef calculate_out_shape(in_shape, kernel_size, stride, padding):\n    """"""\n    Calculate the output tensor shape when applying a convolution to a tensor of shape `inShape` with kernel size\n    `kernel_size`, stride value `stride`, and input padding value `padding`. All arguments can be scalars or multiple\n    values, return value is a scalar if all inputs are scalars.\n    """"""\n    in_shape = np.atleast_1d(in_shape)\n    out_shape = ((in_shape - kernel_size + padding + padding) // stride) + 1\n    out_shape = tuple(int(s) for s in out_shape)\n\n    return tuple(out_shape) if len(out_shape) > 1 else out_shape[0]\n\n\ndef gaussian_1d(sigma, truncated=4.0):\n    """"""\n    one dimensional gaussian kernel.\n\n    Args:\n        sigma: std of the kernel\n        truncated: tail length\n\n    Returns:\n        1D numpy array\n    """"""\n    if sigma <= 0:\n        raise ValueError(""sigma must be positive"")\n\n    tail = int(sigma * truncated + 0.5)\n    sigma2 = sigma * sigma\n    x = np.arange(-tail, tail + 1)\n    out = np.exp(-0.5 / sigma2 * x ** 2)\n    out /= out.sum()\n    return out\n'"
monai/networks/layers/factories.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nDefines factories for creating layers in generic, extensible, and dimensionally independent ways. A separate factory\nobject is created for each type of layer, and factory functions keyed to names are added to these objects. Whenever\na layer is requested the factory name and any necessary arguments are passed to the factory object. The return value\nis typically a type but can be any callable producing a layer object.\n\nThe factory objects contain functions keyed to names converted to upper case, these names can be referred to as members\nof the factory so that they can function as constant identifiers. eg. instance normalisation is named `Norm.INSTANCE`.\n\nFor example, to get a transpose convolution layer the name is needed and then a dimension argument is provided which is\npassed to the factory function:\n\n.. code-block:: python\n\n    dimension = 3\n    name = Conv.CONVTRANS\n    conv = Conv[name, dimension]\n\nThis allows the `dimension` value to be set in the constructor, for example so that the dimensionality of a network is\nparameterizable. Not all factories require arguments after the name, the caller must be aware which are required.\n\nDefining new factories involves creating the object then associating it with factory functions:\n\n.. code-block:: python\n\n    fact = LayerFactory()\n\n    @fact.factory_function(\'test\')\n    def make_something(x, y):\n        # do something with x and y to choose which layer type to return\n        return SomeLayerType\n    ...\n\n    # request object from factory TEST with 1 and 2 as values for x and y\n    layer = fact[fact.TEST, 1, 2]\n\nTypically the caller of a factory would know what arguments to pass (ie. the dimensionality of the requested type) but\ncan be parameterized with the factory name and the arguments to pass to the created type at instantiation time:\n\n.. code-block:: python\n\n    def use_factory(fact_args):\n        fact_name, type_args = split_args\n        layer_type = fact[fact_name, 1, 2]\n        return layer_type(**type_args)\n    ...\n\n    kw_args = {\'arg0\':0, \'arg1\':True}\n    layer = use_factory( (fact.TEST, kwargs) )\n""""""\n\nfrom typing import Callable, Any\n\nimport torch.nn as nn\n\n__all__ = [""LayerFactory"", ""Dropout"", ""Norm"", ""Act"", ""Conv"", ""Pool""]\n\n\nclass LayerFactory:\n    """"""\n    Factory object for creating layers, this uses given factory functions to actually produce the types or constructing\n    callables. These functions are referred to by name and can be added at any time.\n    """"""\n\n    def __init__(self):\n        self.factories = {}\n\n    @property\n    def names(self):\n        """"""\n        Produces all factory names.\n        """"""\n\n        return tuple(self.factories)\n\n    def add_factory_callable(self, name, func):\n        """"""\n        Add the factory function to this object under the given name.\n        """"""\n\n        self.factories[name.upper()] = func\n\n    def factory_function(self, name):\n        """"""\n        Decorator for adding a factory function with the given name.\n        """"""\n\n        def _add(func):\n            self.add_factory_callable(name, func)\n            return func\n\n        return _add\n\n    def get_constructor(self, factory_name, *args) -> Any:\n        """"""\n        Get the constructor for the given factory name and arguments.\n        """"""\n\n        if not isinstance(factory_name, str):\n            raise ValueError(""Factories must be selected by name"")\n\n        fact = self.factories[factory_name.upper()]\n        return fact(*args)\n\n    def __getitem__(self, args) -> Any:\n        """"""\n        Get the given name or name/arguments pair. If `args` is a callable it is assumed to be the constructor\n        itself and is returned, otherwise it should be the factory name or a pair containing the name and arguments.\n        """"""\n\n        # `args[0]` is actually a type or constructor\n        if callable(args):\n            return args\n\n        # `args` is a factory name or a name with arguments\n        if isinstance(args, str):\n            name_obj, args = args, ()\n        else:\n            name_obj, *args = args\n\n        return self.get_constructor(name_obj, *args)\n\n    def __getattr__(self, key):\n        """"""\n        If `key` is a factory name, return it, otherwise behave as inherited. This allows referring to factory names\n        as if they were constants, eg. `Fact.FOO` for a factory Fact with factory function foo.\n        """"""\n\n        if key in self.factories:\n            return key\n\n        return super().__getattribute__(key)\n\n\ndef split_args(args):\n    """"""\n    Split arguments in a way to be suitable for using with the factory types. If `args` is a name it\'s interpreted\n    """"""\n\n    if isinstance(args, str):\n        return args, {}\n    else:\n        name_obj, args = args\n\n        if not isinstance(name_obj, (str, Callable)) or not isinstance(args, dict):\n            msg = ""Layer specifiers must be single strings or pairs of the form (name/object-types, argument dict)""\n            raise ValueError(msg)\n\n        return name_obj, args\n\n\n# Define factories for these layer types\n\nDropout = LayerFactory()\nNorm = LayerFactory()\nAct = LayerFactory()\nConv = LayerFactory()\nPool = LayerFactory()\n\n\n@Dropout.factory_function(""dropout"")\ndef dropout_factory(dim):\n    types = [nn.Dropout, nn.Dropout2d, nn.Dropout3d]\n    return types[dim - 1]\n\n\n@Norm.factory_function(""instance"")\ndef instance_factory(dim):\n    types = [nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d]\n    return types[dim - 1]\n\n\n@Norm.factory_function(""batch"")\ndef batch_factory(dim):\n    types = [nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]\n    return types[dim - 1]\n\n\nAct.add_factory_callable(""relu"", lambda: nn.modules.ReLU)\nAct.add_factory_callable(""leakyrelu"", lambda: nn.modules.LeakyReLU)\nAct.add_factory_callable(""prelu"", lambda: nn.modules.PReLU)\n\n\n@Conv.factory_function(""conv"")\ndef conv_factory(dim):\n    types = [nn.Conv1d, nn.Conv2d, nn.Conv3d]\n    return types[dim - 1]\n\n\n@Conv.factory_function(""convtrans"")\ndef convtrans_factory(dim):\n    types = [nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d]\n    return types[dim - 1]\n\n\n@Pool.factory_function(""max"")\ndef maxpooling_factory(dim):\n    types = [nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]\n    return types[dim - 1]\n\n\n@Pool.factory_function(""adaptivemax"")\ndef adaptive_maxpooling_factory(dim):\n    types = [nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d]\n    return types[dim - 1]\n\n\n@Pool.factory_function(""avg"")\ndef avgpooling_factory(dim):\n    types = [nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d]\n    return types[dim - 1]\n\n\n@Pool.factory_function(""adaptiveavg"")\ndef adaptive_avgpooling_factory(dim):\n    types = [nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d]\n    return types[dim - 1]\n'"
monai/networks/layers/simplelayers.py,6,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom monai.networks.layers.convutils import gaussian_1d, same_padding\nfrom monai.utils.misc import ensure_tuple_rep\n\n__all__ = [""SkipConnection"", ""Flatten"", ""GaussianFilter""]\n\n\nclass SkipConnection(nn.Module):\n    """"""Concats the forward pass input with the result from the given submodule.""""""\n\n    def __init__(self, submodule, cat_dim=1):\n        super().__init__()\n        self.submodule = submodule\n        self.cat_dim = cat_dim\n\n    def forward(self, x):\n        return torch.cat([x, self.submodule(x)], self.cat_dim)\n\n\nclass Flatten(nn.Module):\n    """"""Flattens the given input in the forward pass to be [B,-1] in shape.""""""\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass GaussianFilter(nn.Module):\n    def __init__(self, spatial_dims: int, sigma, truncated: float = 4.0):\n        """"""\n        Args:\n            spatial_dims: number of spatial dimensions of the input image.\n                must have shape (Batch, channels, H[, W, ...]).\n            sigma (float or sequence of floats): std.\n            truncated (float): spreads how many stds.\n        """"""\n        super().__init__()\n        self.spatial_dims = int(spatial_dims)\n        _sigma = ensure_tuple_rep(sigma, self.spatial_dims)\n        self.kernel = [\n            torch.nn.Parameter(torch.as_tensor(gaussian_1d(s, truncated), dtype=torch.float), False) for s in _sigma\n        ]\n        self.padding = [same_padding(k.size()[0]) for k in self.kernel]\n        self.conv_n = [F.conv1d, F.conv2d, F.conv3d][spatial_dims - 1]\n        for idx, param in enumerate(self.kernel):\n            self.register_parameter(f""kernel_{idx}"", param)\n\n    def forward(self, x: torch.Tensor):\n        """"""\n        Args:\n            x (tensor): in shape [Batch, chns, H, W, D].\n        """"""\n        if not torch.is_tensor(x):\n            raise TypeError(f""x must be a Tensor, got {type(x).__name__}."")\n        chns = x.shape[1]\n        sp_dim = self.spatial_dims\n        x = x.clone()  # no inplace change of x\n\n        def _conv(input_, d):\n            if d < 0:\n                return input_\n            s = [1] * (sp_dim + 2)\n            s[d + 2] = -1\n            kernel = self.kernel[d].reshape(s)\n            kernel = kernel.repeat([chns, 1] + [1] * sp_dim)\n            padding = [0] * sp_dim\n            padding[d] = self.padding[d]\n            return self.conv_n(input=_conv(input_, d - 1), weight=kernel, padding=padding, groups=chns)\n\n        return _conv(x, sp_dim - 1)\n'"
monai/networks/layers/spatial_transforms.py,8,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\n\nfrom monai.networks.utils import to_norm_affine\nfrom monai.utils import ensure_tuple\n\n__all__ = [""AffineTransform""]\n\n\nclass AffineTransform(nn.Module):\n    def __init__(\n        self,\n        spatial_size=None,\n        normalized=False,\n        mode=""bilinear"",\n        padding_mode=""zeros"",\n        align_corners=False,\n        reverse_indexing=True,\n    ):\n        """"""\n        Apply affine transformations with a batch of affine matrices.\n\n        When `normalized=False` and `reverse_indexing=True`,\n        it does the commonly used resampling in the \'pull\' direction\n        following the ``scipy.ndimage.affine_transform`` convention.\n        In this case `theta` is equivalent to (ndim+1, ndim+1) input ``matrix`` of ``scipy.ndimage.affine_transform``,\n        operates on homogeneous coordinates.\n        See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.affine_transform.html\n\n        When `normalized=True` and `reverse_indexing=False`,\n        it applies `theta` to the normalized coordinates (coords. in the range of [-1, 1]) directly.\n        This is often used with `align_corners=False` to achieve resolution-agnostic resampling,\n        thus useful as a part of trainable modules such as the spatial transformer networks.\n        See also: https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html\n\n        Args:\n            spatial_size (list or tuple of int): output spatial shape, the full output shape will be\n                `[N, C, *spatial_size]` where N and C are inferred from the `src` input of `self.forward`.\n            normalized (bool): indicating whether the provided affine matrix `theta` is defined\n                for the normalized coordinates. If `normalized=False`, `theta` will be converted\n                to operate on normalized coordinates as pytorch affine_grid works with the normalized\n                coordinates.\n            mode (`nearest|bilinear`): interpolation mode.\n                See also: https://pytorch.org/docs/stable/nn.functional.html#grid-sample.\n            padding_mode (`zeros|border|reflection`): padding mode for outside grid values.\n            align_corners (bool): see also https://pytorch.org/docs/stable/nn.functional.html#grid-sample.\n            reverse_indexing (bool): whether to reverse the spatial indexing of image and coordinates.\n                set to `False` if `theta` follows pytorch\'s default ""D, H, W"" convention.\n                set to `True` if `theta` follows `scipy.ndimage` default ""i, j, k"" convention.\n        """"""\n        super().__init__()\n        self.spatial_size = ensure_tuple(spatial_size) if spatial_size is not None else None\n        self.normalized = normalized\n        self.mode = mode\n        self.padding_mode = padding_mode\n        self.align_corners = align_corners\n        self.reverse_indexing = reverse_indexing\n\n    def forward(self, src, theta, spatial_size=None):\n        """"""\n        ``theta`` must be an affine transformation matrix with shape\n        3x3 or Nx3x3 or Nx2x3 or 2x3 for spatial 2D transforms,\n        4x4 or Nx4x4 or Nx3x4 or 3x4 for spatial 3D transforms,\n        where `N` is the batch size. `theta` will be converted into float Tensor for the computation.\n\n        Args:\n            src (array_like): image in spatial 2D or 3D (N, C, spatial_dims),\n                where N is the batch dim, C is the number of channels.\n            theta (array_like): Nx3x3, Nx2x3, 3x3, 2x3 for spatial 2D inputs,\n                Nx4x4, Nx3x4, 3x4, 4x4 for spatial 3D inputs. When the batch dimension is omitted,\n                `theta` will be repeated N times, N is the batch dim of `src`.\n            spatial_size (list or tuple of int): output spatial shape, the full output shape will be\n                `[N, C, *spatial_size]` where N and C are inferred from the `src`.\n        """"""\n        # validate `theta`\n        if not torch.is_tensor(theta) or not torch.is_tensor(src):\n            raise TypeError(\n                f""both src and theta must be torch Tensor, got {type(src).__name__}, {type(theta).__name__}.""\n            )\n        if theta.ndim not in (2, 3):\n            raise ValueError(""affine must be Nxdxd or dxd."")\n        if theta.ndim == 2:\n            theta = theta[None]  # adds a batch dim.\n        theta = theta.clone()  # no in-place change of theta\n        theta_shape = tuple(theta.shape[1:])\n        if theta_shape in ((2, 3), (3, 4)):  # needs padding to dxd\n            pad_affine = torch.tensor([0, 0, 1] if theta_shape[0] == 2 else [0, 0, 0, 1])\n            pad_affine = pad_affine.repeat(theta.shape[0], 1, 1).to(theta)\n            pad_affine.requires_grad = False\n            theta = torch.cat([theta, pad_affine], dim=1)\n        if tuple(theta.shape[1:]) not in ((3, 3), (4, 4)):\n            raise ValueError(f""affine must be Nx3x3 or Nx4x4, got: {theta.shape}."")\n\n        # validate `src`\n        sr = src.ndim - 2  # input spatial rank\n        if sr not in (2, 3):\n            raise ValueError(""src must be spatially 2D or 3D."")\n\n        # set output shape\n        src_size = tuple(src.shape)\n        dst_size = src_size  # default to the src shape\n        if self.spatial_size is not None:\n            dst_size = src_size[:2] + self.spatial_size\n        if spatial_size is not None:\n            dst_size = src_size[:2] + ensure_tuple(spatial_size)\n\n        # reverse and normalise theta if needed\n        if not self.normalized:\n            theta = to_norm_affine(\n                affine=theta, src_size=src_size[2:], dst_size=dst_size[2:], align_corners=self.align_corners\n            )\n        if self.reverse_indexing:\n            rev_idx = torch.as_tensor(range(sr - 1, -1, -1), device=src.device)\n            theta[:, :sr] = theta[:, rev_idx]\n            theta[:, :, :sr] = theta[:, :, rev_idx]\n        if (theta.shape[0] == 1) and src_size[0] > 1:\n            # adds a batch dim to `theta` in order to match `src`\n            theta = theta.repeat(src_size[0], 1, 1)\n        if theta.shape[0] != src_size[0]:\n            raise ValueError(\n                ""batch dimension of affine and image does not match, got affine: {} and image: {}."".format(\n                    theta.shape[0], src_size[0]\n                )\n            )\n\n        grid = nn.functional.affine_grid(theta=theta[:, :sr], size=dst_size, align_corners=self.align_corners)\n        dst = nn.functional.grid_sample(\n            input=src.contiguous(),\n            grid=grid,\n            mode=self.mode,\n            padding_mode=self.padding_mode,\n            align_corners=self.align_corners,\n        )\n        return dst\n'"
monai/networks/nets/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .densenet import DenseNet, densenet121, densenet169, densenet201, densenet264\nfrom .highresnet import HighResNet, HighResBlock\nfrom .unet import *\n'"
monai/networks/nets/densenet.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import OrderedDict\nfrom typing import Callable\n\nimport torch\nimport torch.nn as nn\n\nfrom monai.networks.layers.factories import Conv, Dropout, Pool, Norm\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, spatial_dims, in_channels, growth_rate, bn_size, dropout_prob):\n        super(_DenseLayer, self).__init__()\n\n        out_channels = bn_size * growth_rate\n        conv_type: Callable = Conv[Conv.CONV, spatial_dims]\n        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]\n        dropout_type: Callable = Dropout[Dropout.DROPOUT, spatial_dims]\n\n        self.add_module(""norm1"", norm_type(in_channels))\n        self.add_module(""relu1"", nn.ReLU(inplace=True))\n        self.add_module(""conv1"", conv_type(in_channels, out_channels, kernel_size=1, bias=False))\n\n        self.add_module(""norm2"", norm_type(out_channels))\n        self.add_module(""relu2"", nn.ReLU(inplace=True))\n        self.add_module(""conv2"", conv_type(out_channels, growth_rate, kernel_size=3, padding=1, bias=False))\n\n        if dropout_prob > 0:\n            self.add_module(""dropout"", dropout_type(dropout_prob))\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, spatial_dims, layers, in_channels, bn_size, growth_rate, dropout_prob):\n        super(_DenseBlock, self).__init__()\n        for i in range(layers):\n            layer = _DenseLayer(spatial_dims, in_channels, growth_rate, bn_size, dropout_prob)\n            in_channels += growth_rate\n            self.add_module(""denselayer%d"" % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, spatial_dims, in_channels, out_channels):\n        super(_Transition, self).__init__()\n\n        conv_type: Callable = Conv[Conv.CONV, spatial_dims]\n        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]\n        pool_type: Callable = Pool[Pool.AVG, spatial_dims]\n\n        self.add_module(""norm"", norm_type(in_channels))\n        self.add_module(""relu"", nn.ReLU(inplace=True))\n        self.add_module(""conv"", conv_type(in_channels, out_channels, kernel_size=1, bias=False))\n        self.add_module(""pool"", pool_type(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    """"""\n    Densenet based on: ""Densely Connected Convolutional Networks"" https://arxiv.org/pdf/1608.06993.pdf\n    Adapted from PyTorch Hub 2D version:\n    https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n\n    Args:\n        spatial_dims: number of spatial dimensions of the input image.\n        in_channels: number of the input channel.\n        out_channels: number of the output classes.\n        init_features: number of filters in the first convolution layer.\n        growth_rate: how many filters to add each layer (k in paper).\n        block_config (tuple): how many layers in each pooling block.\n        bn_size: multiplicative factor for number of bottle neck layers.\n                      (i.e. bn_size * k features in the bottleneck layer)\n        dropout_prob (Float): dropout rate after each dense layer.\n    """"""\n\n    def __init__(\n        self,\n        spatial_dims: int,\n        in_channels: int,\n        out_channels: int,\n        init_features: int = 64,\n        growth_rate: int = 32,\n        block_config=(6, 12, 24, 16),\n        bn_size: int = 4,\n        dropout_prob=0,\n    ):\n\n        super(DenseNet, self).__init__()\n\n        conv_type: Callable = Conv[Conv.CONV, spatial_dims]\n        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]\n        pool_type: Callable = Pool[Pool.MAX, spatial_dims]\n        avg_pool_type: Callable = Pool[Pool.ADAPTIVEAVG, spatial_dims]\n\n        self.features = nn.Sequential(\n            OrderedDict(\n                [\n                    (""conv0"", conv_type(in_channels, init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n                    (""norm0"", norm_type(init_features)),\n                    (""relu0"", nn.ReLU(inplace=True)),\n                    (""pool0"", pool_type(kernel_size=3, stride=2, padding=1)),\n                ]\n            )\n        )\n\n        in_channels = init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                spatial_dims=spatial_dims,\n                layers=num_layers,\n                in_channels=in_channels,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                dropout_prob=dropout_prob,\n            )\n            self.features.add_module(""denseblock%d"" % (i + 1), block)\n            in_channels += num_layers * growth_rate\n            if i == len(block_config) - 1:\n                self.features.add_module(""norm5"", norm_type(in_channels))\n            else:\n                _out_channels = in_channels // 2\n                trans = _Transition(spatial_dims, in_channels=in_channels, out_channels=_out_channels)\n                self.features.add_module(""transition%d"" % (i + 1), trans)\n                in_channels = _out_channels\n\n        # pooling and classification\n        self.class_layers = nn.Sequential(\n            OrderedDict(\n                [\n                    (""relu"", nn.ReLU(inplace=True)),\n                    (""norm"", avg_pool_type(1)),\n                    (""flatten"", nn.Flatten(1)),\n                    (""class"", nn.Linear(in_channels, out_channels)),\n                ]\n            )\n        )\n\n        # Avoid Built-in function isinstance was called with the wrong arguments warning\n        # pytype: disable=wrong-arg-types\n        for m in self.modules():\n            if isinstance(m, conv_type):  # type: ignore\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, norm_type):  # type: ignore\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):  # type: ignore\n                nn.init.constant_(m.bias, 0)\n        # pytype: enable=wrong-arg-types\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.class_layers(x)\n        return x\n\n\ndef densenet121(**kwargs) -> DenseNet:\n    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)\n    return model\n\n\ndef densenet169(**kwargs) -> DenseNet:\n    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 32, 32), **kwargs)\n    return model\n\n\ndef densenet201(**kwargs) -> DenseNet:\n    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 48, 32), **kwargs)\n    return model\n\n\ndef densenet264(**kwargs) -> DenseNet:\n    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 64, 48), **kwargs)\n    return model\n'"
monai/networks/nets/highresnet.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom monai.networks.layers.convutils import same_padding\nfrom monai.networks.layers.factories import Conv, Dropout, Norm\n\nSUPPORTED_NORM = {\n    ""batch"": lambda spatial_dims: Norm[Norm.BATCH, spatial_dims],\n    ""instance"": lambda spatial_dims: Norm[Norm.INSTANCE, spatial_dims],\n}\nSUPPORTED_ACTI = {""relu"": nn.ReLU, ""prelu"": nn.PReLU, ""relu6"": nn.ReLU6}\nDEFAULT_LAYER_PARAMS_3D = (\n    # initial conv layer\n    {""name"": ""conv_0"", ""n_features"": 16, ""kernel_size"": 3},\n    # residual blocks\n    {""name"": ""res_1"", ""n_features"": 16, ""kernels"": (3, 3), ""repeat"": 3},\n    {""name"": ""res_2"", ""n_features"": 32, ""kernels"": (3, 3), ""repeat"": 3},\n    {""name"": ""res_3"", ""n_features"": 64, ""kernels"": (3, 3), ""repeat"": 3},\n    # final conv layers\n    {""name"": ""conv_1"", ""n_features"": 80, ""kernel_size"": 1},\n    {""name"": ""conv_2"", ""kernel_size"": 1},\n)\n\n\nclass ConvNormActi(nn.Module):\n    def __init__(\n        self,\n        spatial_dims: int,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        norm_type: Optional[str] = None,\n        acti_type: Optional[str] = None,\n        dropout_prob: Optional[float] = None,\n    ):\n\n        super(ConvNormActi, self).__init__()\n\n        layers = nn.ModuleList()\n\n        conv_type = Conv[Conv.CONV, spatial_dims]\n        padding_size = same_padding(kernel_size)\n        conv = conv_type(in_channels, out_channels, kernel_size, padding=padding_size)\n        layers.append(conv)\n\n        if norm_type is not None:\n            layers.append(SUPPORTED_NORM[norm_type](spatial_dims)(out_channels))\n        if acti_type is not None:\n            layers.append(SUPPORTED_ACTI[acti_type](inplace=True))\n        if dropout_prob is not None:\n            dropout_type = Dropout[Dropout.DROPOUT, spatial_dims]\n            layers.append(dropout_type(p=dropout_prob))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass HighResBlock(nn.Module):\n    def __init__(\n        self,\n        spatial_dims: int,\n        in_channels: int,\n        out_channels: int,\n        kernels=(3, 3),\n        dilation=1,\n        norm_type: str = ""instance"",\n        acti_type: str = ""relu"",\n        channel_matching: str = ""pad"",\n    ):\n        """"""\n        Args:\n            kernels (list of int): each integer k in `kernels` corresponds to a convolution layer with kernel size k.\n            channel_matching (\'pad\'|\'project\'): handling residual branch and conv branch channel mismatches\n                with either zero padding (\'pad\') or a trainable conv with kernel size 1 (\'project\').\n        """"""\n        super(HighResBlock, self).__init__()\n        conv_type = Conv[Conv.CONV, spatial_dims]\n\n        self.project, self.pad = None, None\n        if in_channels != out_channels:\n            if channel_matching not in (""pad"", ""project""):\n                raise ValueError(f""channel matching must be pad or project, got {channel_matching}."")\n            if channel_matching == ""project"":\n                self.project = conv_type(in_channels, out_channels, kernel_size=1)\n            if channel_matching == ""pad"":\n                if in_channels > out_channels:\n                    raise ValueError(""in_channels > out_channels is incompatible with `channel_matching=pad`."")\n                pad_1 = (out_channels - in_channels) // 2\n                pad_2 = out_channels - in_channels - pad_1\n                pad = [0, 0] * spatial_dims + [pad_1, pad_2] + [0, 0]\n                self.pad = lambda input: F.pad(input, pad)\n\n        layers = nn.ModuleList()\n        _in_chns, _out_chns = in_channels, out_channels\n        for kernel_size in kernels:\n            layers.append(SUPPORTED_NORM[norm_type](spatial_dims)(_in_chns))\n            layers.append(SUPPORTED_ACTI[acti_type](inplace=True))\n            layers.append(\n                conv_type(\n                    _in_chns, _out_chns, kernel_size, padding=same_padding(kernel_size, dilation), dilation=dilation\n                )\n            )\n            _in_chns = _out_chns\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_conv = self.layers(x)\n        if self.project is not None:\n            return x_conv + self.project(x)\n        if self.pad is not None:\n            return x_conv + self.pad(x)\n        return x_conv + x\n\n\nclass HighResNet(nn.Module):\n    """"""\n    Reimplementation of highres3dnet based on\n    Li et al., ""On the compactness, efficiency, and representation of 3D\n    convolutional networks: Brain parcellation as a pretext task"", IPMI \'17\n\n    Adapted from:\n    https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/network/highres3dnet.py\n    https://github.com/fepegar/highresnet\n\n    Args:\n        spatial_dims: number of spatial dimensions of the input image.\n        in_channels: number of input channels.\n        out_channels: number of output channels.\n        norm_type (\'batch\'|\'instance\'): feature normalisation with batchnorm or instancenorm.\n        acti_type (\'relu\'|\'prelu\'|\'relu6\'): non-linear activation using ReLU or PReLU.\n        dropout_prob (float): probability of the feature map to be zeroed\n            (only applies to the penultimate conv layer).\n        layer_params (a list of dictionaries): specifying key parameters of each layer/block.\n    """"""\n\n    def __init__(\n        self,\n        spatial_dims: int = 3,\n        in_channels: int = 1,\n        out_channels: int = 1,\n        norm_type: str = ""batch"",\n        acti_type: str = ""relu"",\n        dropout_prob: Optional[float] = None,\n        layer_params=DEFAULT_LAYER_PARAMS_3D,\n    ):\n\n        super(HighResNet, self).__init__()\n        blocks = nn.ModuleList()\n\n        # intial conv layer\n        params = layer_params[0]\n        _in_chns, _out_chns = in_channels, params[""n_features""]\n        blocks.append(\n            ConvNormActi(\n                spatial_dims,\n                _in_chns,\n                _out_chns,\n                kernel_size=params[""kernel_size""],\n                norm_type=norm_type,\n                acti_type=acti_type,\n                dropout_prob=None,\n            )\n        )\n\n        # residual blocks\n        for (idx, params) in enumerate(layer_params[1:-2]):  # res blocks except the 1st and last two conv layers.\n            _in_chns, _out_chns = _out_chns, params[""n_features""]\n            _dilation = 2 ** idx\n            for _ in range(params[""repeat""]):\n                blocks.append(\n                    HighResBlock(\n                        spatial_dims,\n                        _in_chns,\n                        _out_chns,\n                        params[""kernels""],\n                        dilation=_dilation,\n                        norm_type=norm_type,\n                        acti_type=acti_type,\n                    )\n                )\n                _in_chns = _out_chns\n\n        # final conv layers\n        params = layer_params[-2]\n        _in_chns, _out_chns = _out_chns, params[""n_features""]\n        blocks.append(\n            ConvNormActi(\n                spatial_dims,\n                _in_chns,\n                _out_chns,\n                kernel_size=params[""kernel_size""],\n                norm_type=norm_type,\n                acti_type=acti_type,\n                dropout_prob=dropout_prob,\n            )\n        )\n\n        params = layer_params[-1]\n        _in_chns = _out_chns\n        blocks.append(\n            ConvNormActi(\n                spatial_dims,\n                _in_chns,\n                out_channels,\n                kernel_size=params[""kernel_size""],\n                norm_type=norm_type,\n                acti_type=None,\n                dropout_prob=None,\n            )\n        )\n\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.blocks(x)\n'"
monai/networks/nets/unet.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch.nn as nn\n\nfrom monai.networks.blocks.convolutions import Convolution, ResidualUnit\nfrom monai.networks.layers.factories import Norm, Act\nfrom monai.networks.layers.simplelayers import SkipConnection\nfrom monai.utils import export\nfrom monai.utils.aliases import alias\n\n\n@export(""monai.networks.nets"")\n@alias(""Unet"")\nclass UNet(nn.Module):\n    def __init__(\n        self,\n        dimensions,\n        in_channels,\n        out_channels,\n        channels,\n        strides,\n        kernel_size=3,\n        up_kernel_size=3,\n        num_res_units=0,\n        act=Act.PRELU,\n        norm=Norm.INSTANCE,\n        dropout=0,\n    ):\n        super().__init__()\n        assert len(channels) == (len(strides) + 1)\n        self.dimensions = dimensions\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.channels = channels\n        self.strides = strides\n        self.kernel_size = kernel_size\n        self.up_kernel_size = up_kernel_size\n        self.num_res_units = num_res_units\n        self.act = act\n        self.norm = norm\n        self.dropout = dropout\n\n        def _create_block(inc, outc, channels, strides, is_top):\n            """"""\n            Builds the UNet structure from the bottom up by recursing down to the bottom block, then creating sequential\n            blocks containing the downsample path, a skip connection around the previous block, and the upsample path.\n            """"""\n            c = channels[0]\n            s = strides[0]\n\n            if len(channels) > 2:\n                subblock = _create_block(c, c, channels[1:], strides[1:], False)  # continue recursion down\n                upc = c * 2\n            else:\n                # the next layer is the bottom so stop recursion, create the bottom layer as the sublock for this layer\n                subblock = self._get_bottom_layer(c, channels[1])\n                upc = c + channels[1]\n\n            down = self._get_down_layer(inc, c, s, is_top)  # create layer in downsampling path\n            up = self._get_up_layer(upc, outc, s, is_top)  # create layer in upsampling path\n\n            return nn.Sequential(down, SkipConnection(subblock), up)\n\n        self.model = _create_block(in_channels, out_channels, self.channels, self.strides, True)\n\n    def _get_down_layer(self, in_channels, out_channels, strides, is_top):\n        if self.num_res_units > 0:\n            return ResidualUnit(\n                self.dimensions,\n                in_channels,\n                out_channels,\n                strides,\n                self.kernel_size,\n                self.num_res_units,\n                self.act,\n                self.norm,\n                self.dropout,\n            )\n        else:\n            return Convolution(\n                self.dimensions, in_channels, out_channels, strides, self.kernel_size, self.act, self.norm, self.dropout\n            )\n\n    def _get_bottom_layer(self, in_channels, out_channels):\n        return self._get_down_layer(in_channels, out_channels, 1, False)\n\n    def _get_up_layer(self, in_channels, out_channels, strides, is_top):\n        conv = Convolution(\n            self.dimensions,\n            in_channels,\n            out_channels,\n            strides,\n            self.up_kernel_size,\n            self.act,\n            self.norm,\n            self.dropout,\n            conv_only=is_top and self.num_res_units == 0,\n            is_transposed=True,\n        )\n\n        if self.num_res_units > 0:\n            ru = ResidualUnit(\n                self.dimensions,\n                out_channels,\n                out_channels,\n                1,\n                self.kernel_size,\n                1,\n                self.act,\n                self.norm,\n                self.dropout,\n                last_conv_only=is_top,\n            )\n            return nn.Sequential(conv, ru)\n        else:\n            return conv\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nUnet = unet = UNet\n'"
monai/transforms/croppad/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/transforms/croppad/array.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of ""vanilla"" transforms for crop and pad operations\nhttps://github.com/Project-MONAI/MONAI/wiki/MONAI_Design\n""""""\n\nfrom typing import Optional, Callable\n\nimport numpy as np\n\nfrom monai.data.utils import get_random_patch, get_valid_patch_size\nfrom monai.transforms.compose import Transform, Randomizable\nfrom monai.transforms.utils import generate_spatial_bounding_box\nfrom monai.utils.misc import ensure_tuple, ensure_tuple_rep\n\n\nclass SpatialPad(Transform):\n    """"""Performs padding to the data, symmetric for all sides or all on one side for each dimension.\n     Uses np.pad so in practice, a mode needs to be provided. See numpy.lib.arraypad.pad\n     for additional details.\n\n    Args:\n        spatial_size (list): the spatial size of output data after padding.\n        method (str): pad image symmetric on every side or only pad at the end sides. default is \'symmetric\'.\n        mode (str): one of the following string values or a user supplied function: {\'constant\', \'edge\', \'linear_ramp\',\n            \'maximum\', \'mean\', \'median\', \'minimum\', \'reflect\', \'symmetric\', \'wrap\', \'empty\', <function>}\n            for more details, please check: https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n    """"""\n\n    def __init__(self, spatial_size, method: str = ""symmetric"", mode: str = ""constant""):\n        self.spatial_size = ensure_tuple(spatial_size)\n        assert method in (""symmetric"", ""end""), ""unsupported padding type.""\n        self.method = method\n        assert isinstance(mode, str), ""mode must be str.""\n        self.mode = mode\n\n    def _determine_data_pad_width(self, data_shape):\n        if self.method == ""symmetric"":\n            pad_width = list()\n            for i in range(len(self.spatial_size)):\n                width = max(self.spatial_size[i] - data_shape[i], 0)\n                pad_width.append((width // 2, width - (width // 2)))\n            return pad_width\n        else:\n            return [(0, max(self.spatial_size[i] - data_shape[i], 0)) for i in range(len(self.spatial_size))]\n\n    def __call__(self, img, mode: Optional[str] = None):  # type: ignore # see issue #495\n        data_pad_width = self._determine_data_pad_width(img.shape[1:])\n        all_pad_width = [(0, 0)] + data_pad_width\n        img = np.pad(img, all_pad_width, mode=mode or self.mode)\n        return img\n\n\nclass SpatialCrop(Transform):\n    """"""General purpose cropper to produce sub-volume region of interest (ROI).\n    It can support to crop ND spatial (channel-first) data.\n    Either a spatial center and size must be provided, or alternatively if center and size\n    are not provided, the start and end coordinates of the ROI must be provided.\n    The sub-volume must sit the within original image.\n    Note: This transform will not work if the crop region is larger than the image itself.\n    """"""\n\n    def __init__(self, roi_center=None, roi_size=None, roi_start=None, roi_end=None):\n        """"""\n        Args:\n            roi_center (list or tuple): voxel coordinates for center of the crop ROI.\n            roi_size (list or tuple): size of the crop ROI.\n            roi_start (list or tuple): voxel coordinates for start of the crop ROI.\n            roi_end (list or tuple): voxel coordinates for end of the crop ROI.\n        """"""\n        if roi_center is not None and roi_size is not None:\n            roi_center = np.asarray(roi_center, dtype=np.uint16)\n            roi_size = np.asarray(roi_size, dtype=np.uint16)\n            self.roi_start = np.subtract(roi_center, np.floor_divide(roi_size, 2))\n            self.roi_end = np.add(self.roi_start, roi_size)\n        else:\n            assert roi_start is not None and roi_end is not None, ""roi_start and roi_end must be provided.""\n            self.roi_start = np.asarray(roi_start, dtype=np.uint16)\n            self.roi_end = np.asarray(roi_end, dtype=np.uint16)\n\n        assert np.all(self.roi_start >= 0), ""all elements of roi_start must be greater than or equal to 0.""\n        assert np.all(self.roi_end > 0), ""all elements of roi_end must be positive.""\n        assert np.all(self.roi_end >= self.roi_start), ""invalid roi range.""\n\n    def __call__(self, img):\n        max_end = img.shape[1:]\n        sd = min(len(self.roi_start), len(max_end))\n        assert np.all(max_end[:sd] >= self.roi_start[:sd]), ""roi start out of image space.""\n        assert np.all(max_end[:sd] >= self.roi_end[:sd]), ""roi end out of image space.""\n\n        slices = [slice(None)] + [slice(s, e) for s, e in zip(self.roi_start[:sd], self.roi_end[:sd])]\n        return img[tuple(slices)]\n\n\nclass CenterSpatialCrop(Transform):\n    """"""\n    Crop at the center of image with specified ROI size.\n\n    Args:\n        roi_size (list, tuple): the spatial size of the crop region e.g. [224,224,128]\n    """"""\n\n    def __init__(self, roi_size):\n        self.roi_size = roi_size\n\n    def __call__(self, img):\n        center = [i // 2 for i in img.shape[1:]]\n        cropper = SpatialCrop(roi_center=center, roi_size=self.roi_size)\n        return cropper(img)\n\n\nclass RandSpatialCrop(Randomizable, Transform):\n    """"""\n    Crop image with random size or specific size ROI. It can crop at a random position as center\n    or at the image center. And allows to set the minimum size to limit the randomly generated ROI.\n    This transform assumes all the expected fields specified by `keys` have same shape.\n\n    Args:\n        roi_size (list, tuple): if `random_size` is True, the spatial size of the minimum crop region.\n            if `random_size` is False, specify the expected ROI size to crop. e.g. [224, 224, 128]\n        random_center (bool): crop at random position as center or the image center.\n        random_size (bool): crop with random size or specific size ROI.\n            The actual size is sampled from `randint(roi_size, img_size)`.\n    """"""\n\n    def __init__(self, roi_size, random_center: bool = True, random_size: bool = True):\n        self.roi_size = roi_size\n        self.random_center = random_center\n        self.random_size = random_size\n\n    def randomize(self, img_size):\n        self._size = ensure_tuple_rep(self.roi_size, len(img_size))\n        if self.random_size:\n            self._size = [self.R.randint(low=self._size[i], high=img_size[i] + 1) for i in range(len(img_size))]\n        if self.random_center:\n            valid_size = get_valid_patch_size(img_size, self._size)\n            self._slices = ensure_tuple(slice(None)) + get_random_patch(img_size, valid_size, self.R)\n\n    def __call__(self, img):\n        self.randomize(img.shape[1:])\n        if self.random_center:\n            return img[self._slices]\n        else:\n            cropper = CenterSpatialCrop(self._size)\n            return cropper(img)\n\n\nclass CropForeground(Transform):\n    """"""\n    Crop an image using a bounding box. The bounding box is generated by selecting foreground using select_fn\n    at channels channel_indexes. margin is added in each spatial dimension of the bounding box.\n    The typical usage is to help training and evaluation if the valid part is small in the whole medical image.\n    Users can define arbitrary function to select expected foreground from the whole image or specified channels.\n    And it can also add margin to every dim of the bounding box of foreground object.\n    For example:\n\n    .. code-block:: python\n\n        image = np.array(\n            [[[0, 0, 0, 0, 0],\n              [0, 1, 2, 1, 0],\n              [0, 1, 3, 2, 0],\n              [0, 1, 2, 1, 0],\n              [0, 0, 0, 0, 0]]])  # 1x5x5, single channel 5x5 image\n        cropper = CropForeground(select_fn=lambda x: x > 1, margin=0)\n        print(cropper(image))\n        [[[2, 1],\n          [3, 2],\n          [2, 1]]]\n\n    """"""\n\n    def __init__(self, select_fn: Callable = lambda x: x > 0, channel_indexes=None, margin: int = 0):\n        """"""\n        Args:\n            select_fn (Callable): function to select expected foreground, default is to select values > 0.\n            channel_indexes (int, tuple or list): if defined, select foreground only on the specified channels\n                of image. if None, select foreground on the whole image.\n            margin: add margin to all dims of the bounding box.\n        """"""\n        self.select_fn = select_fn\n        self.channel_indexes = ensure_tuple(channel_indexes) if channel_indexes is not None else None\n        self.margin = margin\n\n    def __call__(self, img):\n        box_start, box_end = generate_spatial_bounding_box(img, self.select_fn, self.channel_indexes, self.margin)\n        cropper = SpatialCrop(roi_start=box_start, roi_end=box_end)\n        return cropper(img)\n'"
monai/transforms/croppad/dictionary.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of dictionary-based wrappers around the ""vanilla"" transforms for crop and pad operations\ndefined in :py:class:`monai.transforms.croppad.array`.\n\nClass names are ended with \'d\' to denote dictionary-based transforms.\n""""""\n\nfrom typing import Union, Hashable, Optional, Callable\n\nfrom monai.data.utils import get_random_patch, get_valid_patch_size\nfrom monai.transforms.compose import MapTransform, Randomizable\nfrom monai.transforms.croppad.array import SpatialCrop, CenterSpatialCrop, SpatialPad\nfrom monai.transforms.utils import generate_pos_neg_label_crop_centers, generate_spatial_bounding_box\nfrom monai.utils.misc import ensure_tuple, ensure_tuple_rep\n\n\nclass SpatialPadd(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:`monai.transforms.SpatialPad`.\n    Performs padding to the data, symmetric for all sides or all on one side for each dimension.\n    """"""\n\n    def __init__(self, keys: Hashable, spatial_size, method: str = ""symmetric"", mode=""constant""):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            spatial_size (list): the spatial size of output data after padding.\n            method (str): pad image symmetric on every side or only pad at the end sides. default is \'symmetric\'.\n            mode (str or sequence of str): one of the following string values or a user supplied function:\n                {\'constant\', \'edge\', \'linear_ramp\', \'maximum\', \'mean\', \'median\', \'minimum\', \'reflect\', \'symmetric\',\n                \'wrap\', \'empty\', <function>}\n                for more details, please check: https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n        """"""\n        super().__init__(keys)\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n        self.padder = SpatialPad(spatial_size, method)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key, m in zip(self.keys, self.mode):\n            d[key] = self.padder(d[key], mode=m)\n        return d\n\n\nclass SpatialCropd(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:`monai.transforms.SpatialCrop`.\n    Either a spatial center and size must be provided, or alternatively if center and size\n    are not provided, the start and end coordinates of the ROI must be provided.\n    """"""\n\n    def __init__(self, keys: Hashable, roi_center=None, roi_size=None, roi_start=None, roi_end=None):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            roi_center (list or tuple): voxel coordinates for center of the crop ROI.\n            roi_size (list or tuple): size of the crop ROI.\n            roi_start (list or tuple): voxel coordinates for start of the crop ROI.\n            roi_end (list or tuple): voxel coordinates for end of the crop ROI.\n        """"""\n        super().__init__(keys)\n        self.cropper = SpatialCrop(roi_center, roi_size, roi_start, roi_end)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.cropper(d[key])\n        return d\n\n\nclass CenterSpatialCropd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.CenterSpatialCrop`.\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: monai.transforms.MapTransform\n        roi_size (list, tuple): the size of the crop region e.g. [224,224,128]\n    """"""\n\n    def __init__(self, keys: Hashable, roi_size):\n        super().__init__(keys)\n        self.cropper = CenterSpatialCrop(roi_size)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.cropper(d[key])\n        return d\n\n\nclass RandSpatialCropd(Randomizable, MapTransform):\n    """"""\n    Dictionary-based version :py:class:`monai.transforms.RandSpatialCrop`.\n    Crop image with random size or specific size ROI. It can crop at a random position as\n    center or at the image center. And allows to set the minimum size to limit the randomly\n    generated ROI. Suppose all the expected fields specified by `keys` have same shape.\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: monai.transforms.MapTransform\n        roi_size (list, tuple): if `random_size` is True, the spatial size of the minimum crop region.\n            if `random_size` is False, specify the expected ROI size to crop. e.g. [224, 224, 128]\n        random_center (bool): crop at random position as center or the image center.\n        random_size (bool): crop with random size or specific size ROI.\n            The actual size is sampled from `randint(roi_size, img_size)`.\n    """"""\n\n    def __init__(self, keys: Hashable, roi_size, random_center: bool = True, random_size: bool = True):\n        super().__init__(keys)\n        self.roi_size = roi_size\n        self.random_center = random_center\n        self.random_size = random_size\n\n    def randomize(self, img_size):\n        self._size = ensure_tuple_rep(self.roi_size, len(img_size))\n        if self.random_size:\n            self._size = [self.R.randint(low=self._size[i], high=img_size[i] + 1) for i in range(len(img_size))]\n        if self.random_center:\n            valid_size = get_valid_patch_size(img_size, self._size)\n            self._slices = ensure_tuple(slice(None)) + get_random_patch(img_size, valid_size, self.R)\n\n    def __call__(self, data):\n        d = dict(data)\n        self.randomize(d[self.keys[0]].shape[1:])  # image shape from the first data key\n        for key in self.keys:\n            if self.random_center:\n                d[key] = d[key][self._slices]\n            else:\n                cropper = CenterSpatialCrop(self._size)\n                d[key] = cropper(d[key])\n        return d\n\n\nclass CropForegroundd(MapTransform):\n    """"""\n    dictionary-based version :py:class:`monai.transforms.CropForeground`.\n    Crop only the foreground object of the expected images.\n    The typical usage is to help training and evaluation if the valid part is small in the whole medical image.\n    The valid part can be determined by any field in the data with `source_key`, for example:\n    - Select values > 0 in image field as the foreground and crop on all fields specified by `keys`.\n    - Select label = 3 in label field as the foreground to crop on all fields specified by `keys`.\n    - Select label > 0 in the third channel of a One-Hot label field as the foreground to crop all `keys` fields.\n    Users can define arbitrary function to select expected foreground from the whole source image or specified\n    channels. And it can also add margin to every dim of the bounding box of foreground object.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        source_key: str,\n        select_fn: Callable = lambda x: x > 0,\n        channel_indexes: Callable = None,\n        margin: int = 0,\n    ):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            source_key (str): data source to generate the bounding box of foreground, can be image or label, etc.\n            select_fn (Callable): function to select expected foreground, default is to select values > 0.\n            channel_indexes (int, tuple or list): if defined, select foreground only on the specified channels\n                of image. if None, select foreground on the whole image.\n            margin: add margin to all dims of the bounding box.\n        """"""\n        super().__init__(keys)\n        self.source_key = source_key\n        self.select_fn = select_fn\n        self.channel_indexes = ensure_tuple(channel_indexes) if channel_indexes is not None else None\n        self.margin = margin\n\n    def __call__(self, data):\n        d = dict(data)\n        box_start, box_end = generate_spatial_bounding_box(\n            d[self.source_key], self.select_fn, self.channel_indexes, self.margin\n        )\n        cropper = SpatialCrop(roi_start=box_start, roi_end=box_end)\n        for key in self.keys:\n            d[key] = cropper(d[key])\n        return d\n\n\nclass RandCropByPosNegLabeld(Randomizable, MapTransform):\n    """"""\n    Crop random fixed sized regions with the center being a foreground or background voxel\n    based on the Pos Neg Ratio.\n    And will return a list of dictionaries for all the cropped images.\n\n    Args:\n        keys (list): parameter will be used to get and set the actual data item to transform.\n        label_key (str): name of key for label image, this will be used for finding foreground/background.\n        size (list, tuple): the size of the crop region e.g. [224,224,128]\n        pos (int, float): used to calculate the ratio ``pos / (pos + neg)`` for the probability to pick a\n          foreground voxel as a center rather than a background voxel.\n        neg (int, float): used to calculate the ratio ``pos / (pos + neg)`` for the probability to pick a\n          foreground voxel as a center rather than a background voxel.\n        num_samples: number of samples (crop regions) to take in each list.\n        image_key (str): if image_key is not None, use ``label == 0 & image > image_threshold`` to select\n            the negative sample(background) center. so the crop center will only exist on valid image area.\n        image_threshold (int or float): if enabled image_key, use ``image > image_threshold`` to determine\n            the valid image content area.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        label_key: str,\n        size,\n        pos: Union[int, float] = 1,\n        neg: Union[int, float] = 1,\n        num_samples: int = 1,\n        image_key: Optional[str] = None,\n        image_threshold: Union[int, float] = 0,\n    ):\n        super().__init__(keys)\n        assert isinstance(label_key, str), ""label_key must be a string.""\n        assert isinstance(size, (list, tuple)), ""size must be list or tuple.""\n        assert all(isinstance(x, int) and x > 0 for x in size), ""all elements of size must be positive integers.""\n        assert float(pos) >= 0 and float(neg) >= 0, ""pos and neg must be greater than or equal to 0.""\n        assert float(pos) + float(neg) > 0, ""pos and neg cannot both be 0.""\n        assert isinstance(num_samples, int), ""invalid samples number: {}. num_samples must be an integer."".format(\n            num_samples\n        )\n        assert num_samples >= 0, ""num_samples must be greater than or equal to 0.""\n        self.label_key = label_key\n        self.size = size\n        self.pos_ratio = float(pos) / (float(pos) + float(neg))\n        self.num_samples = num_samples\n        self.image_key = image_key\n        self.image_threshold = image_threshold\n        self.centers = None\n\n    def randomize(self, label, image):\n        self.centers = generate_pos_neg_label_crop_centers(\n            label, self.size, self.num_samples, self.pos_ratio, image, self.image_threshold, self.R\n        )\n\n    def __call__(self, data):\n        d = dict(data)\n        label = d[self.label_key]\n        image = d[self.image_key] if self.image_key else None\n        self.randomize(label, image)\n        results = [dict() for _ in range(self.num_samples)]\n        for key in data.keys():\n            if key in self.keys:\n                img = d[key]\n                for i, center in enumerate(self.centers):\n                    cropper = SpatialCrop(roi_center=tuple(center), roi_size=self.size)\n                    results[i][key] = cropper(img)\n            else:\n                for i in range(self.num_samples):\n                    results[i][key] = data[key]\n\n        return results\n\n\nSpatialPadD = SpatialPadDict = SpatialPadd\nSpatialCropD = SpatialCropDict = SpatialCropd\nCenterSpatialCropD = CenterSpatialCropDict = CenterSpatialCropd\nRandSpatialCropD = RandSpatialCropDict = RandSpatialCropd\nCropForegroundD = CropForegroundDict = CropForegroundd\nRandCropByPosNegLabelD = RandCropByPosNegLabelDict = RandCropByPosNegLabeld\n'"
monai/transforms/intensity/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/transforms/intensity/array.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of ""vanilla"" transforms for intensity adjustment\nhttps://github.com/Project-MONAI/MONAI/wiki/MONAI_Design\n""""""\n\nfrom typing import Union, Optional\n\nimport numpy as np\n\nfrom monai.transforms.compose import Transform, Randomizable\nfrom monai.transforms.utils import rescale_array\n\n\nclass RandGaussianNoise(Randomizable, Transform):\n    """"""Add Gaussian noise to image.\n\n    Args:\n        prob (float): Probability to add Gaussian noise.\n        mean (float or array of floats): Mean or \xe2\x80\x9ccentre\xe2\x80\x9d of the distribution.\n        std (float): Standard deviation (spread) of distribution.\n    """"""\n\n    def __init__(self, prob: float = 0.1, mean: float = 0.0, std: float = 0.1):\n        self.prob = prob\n        self.mean = mean\n        self.std = std\n        self._do_transform = False\n        self._noise = None\n\n    def randomize(self, im_shape):\n        self._do_transform = self.R.random() < self.prob\n        self._noise = self.R.normal(self.mean, self.R.uniform(0, self.std), size=im_shape)\n\n    def __call__(self, img):\n        self.randomize(img.shape)\n        return img + self._noise.astype(img.dtype) if self._do_transform else img\n\n\nclass ShiftIntensity(Transform):\n    """"""Shift intensity uniformly for the entire image with specified `offset`.\n\n    Args:\n        offset (int or float): offset value to shift the intensity of image.\n    """"""\n\n    def __init__(self, offset: Union[int, float]):\n        self.offset = offset\n\n    def __call__(self, img):\n        return (img + self.offset).astype(img.dtype)\n\n\nclass RandShiftIntensity(Randomizable, Transform):\n    """"""Randomly shift intensity with randomly picked offset.\n    """"""\n\n    def __init__(self, offsets, prob: float = 0.1):\n        """"""\n        Args:\n            offsets(int, float, tuple or list): offset range to randomly shift.\n                if single number, offset value is picked from (-offsets, offsets).\n            prob (float): probability of shift.\n        """"""\n        self.offsets = (-offsets, offsets) if not isinstance(offsets, (list, tuple)) else offsets\n        assert len(self.offsets) == 2, ""offsets should be a number or pair of numbers.""\n        self.prob = prob\n        self._do_transform = False\n\n    def randomize(self):\n        self._offset = self.R.uniform(low=self.offsets[0], high=self.offsets[1])\n        self._do_transform = self.R.random() < self.prob\n\n    def __call__(self, img):\n        self.randomize()\n        if not self._do_transform:\n            return img\n        shifter = ShiftIntensity(self._offset)\n        return shifter(img)\n\n\nclass ScaleIntensity(Transform):\n    """"""\n    Scale the intensity of input image to the given value range (minv, maxv).\n    If `minv` and `maxv` not provided, use `factor` to scale image by ``v = v * (1 + factor)``.\n    """"""\n\n    def __init__(\n        self,\n        minv: Optional[Union[int, float]] = 0.0,\n        maxv: Optional[Union[int, float]] = 1.0,\n        factor: Optional[float] = None,\n    ):\n        """"""\n        Args:\n            minv (int or float): minimum value of output data.\n            maxv (int or float): maximum value of output data.\n            factor (float): factor scale by ``v = v * (1 + factor)``.\n        """"""\n        self.minv = minv\n        self.maxv = maxv\n        self.factor = factor\n\n    def __call__(self, img):\n        if self.minv is not None and self.maxv is not None:\n            return rescale_array(img, self.minv, self.maxv, img.dtype)\n        else:\n            return (img * (1 + self.factor)).astype(img.dtype)\n\n\nclass RandScaleIntensity(Randomizable, Transform):\n    """"""\n    Randomly scale the intensity of input image by ``v = v * (1 + factor)`` where the `factor`\n    is randomly picked from (factors[0], factors[0]).\n    """"""\n\n    def __init__(self, factors, prob: float = 0.1):\n        """"""\n        Args:\n            factors(float, tuple or list): factor range to randomly scale by ``v = v * (1 + factor)``.\n                if single number, factor value is picked from (-factors, factors).\n            prob (float): probability of scale.\n\n        """"""\n        self.factors = (-factors, factors) if not isinstance(factors, (list, tuple)) else factors\n        assert len(self.factors) == 2, ""factors should be a number or pair of numbers.""\n        self.prob = prob\n        self._do_transform = False\n\n    def randomize(self):\n        self.factor = self.R.uniform(low=self.factors[0], high=self.factors[1])\n        self._do_transform = self.R.random() < self.prob\n\n    def __call__(self, img):\n        self.randomize()\n        if not self._do_transform:\n            return img\n        scaler = ScaleIntensity(minv=None, maxv=None, factor=self.factor)\n        return scaler(img)\n\n\nclass NormalizeIntensity(Transform):\n    """"""Normalize input based on provided args, using calculated mean and std if not provided\n    (shape of subtrahend and divisor must match. if 0, entire volume uses same subtrahend and\n    divisor, otherwise the shape can have dimension 1 for channels).\n    This transform can normalize only non-zero values or entire image, and can also calculate\n    mean and std on each channel separately.\n\n    Args:\n        subtrahend (ndarray): the amount to subtract by (usually the mean).\n        divisor (ndarray): the amount to divide by (usually the standard deviation).\n        nonzero (bool): whether only normalize non-zero values.\n        channel_wise (bool): if using calculated mean and std, calculate on each channel separately\n            or calculate on the entire image directly.\n    """"""\n\n    def __init__(\n        self,\n        subtrahend: Optional[np.ndarray] = None,\n        divisor: Optional[np.ndarray] = None,\n        nonzero: bool = False,\n        channel_wise: bool = False,\n    ):\n        if subtrahend is not None or divisor is not None:\n            assert isinstance(subtrahend, np.ndarray) and isinstance(\n                divisor, np.ndarray\n            ), ""subtrahend and divisor must be set in pair and in numpy array.""\n        self.subtrahend = subtrahend\n        self.divisor = divisor\n        self.nonzero = nonzero\n        self.channel_wise = channel_wise\n\n    def _normalize(self, img):\n        slices = (img != 0) if self.nonzero else np.ones(img.shape, dtype=np.bool_)\n        if np.any(slices):\n            if self.subtrahend is not None and self.divisor is not None:\n                img[slices] = (img[slices] - self.subtrahend[slices]) / self.divisor[slices]\n            else:\n                img[slices] = (img[slices] - np.mean(img[slices])) / np.std(img[slices])\n        return img\n\n    def __call__(self, img):\n        if self.channel_wise:\n            for i, d in enumerate(img):\n                img[i] = self._normalize(d)\n        else:\n            img = self._normalize(img)\n\n        return img\n\n\nclass ThresholdIntensity(Transform):\n    """"""Filter the intensity values of whole image to below threshold or above threshold.\n    And fill the remaining parts of the image to the `cval` value.\n\n    Args:\n        threshold (float or int): the threshold to filter intensity values.\n        above (bool): filter values above the threshold or below the threshold, default is True.\n        cval (float or int): value to fill the remaining parts of the image, default is 0.\n    """"""\n\n    def __init__(self, threshold: Union[int, float], above: bool = True, cval: Union[int, float] = 0):\n        assert isinstance(threshold, (float, int)), ""must set the threshold to filter intensity.""\n        self.threshold = threshold\n        self.above = above\n        self.cval = cval\n\n    def __call__(self, img):\n        return np.where(img > self.threshold if self.above else img < self.threshold, img, self.cval).astype(img.dtype)\n\n\nclass ScaleIntensityRange(Transform):\n    """"""Apply specific intensity scaling to the whole numpy array.\n    Scaling from [a_min, a_max] to [b_min, b_max] with clip option.\n\n    Args:\n        a_min (int or float): intensity original range min.\n        a_max (int or float): intensity original range max.\n        b_min (int or float): intensity target range min.\n        b_max (int or float): intensity target range max.\n        clip (bool): whether to perform clip after scaling.\n    """"""\n\n    def __init__(\n        self,\n        a_min: Union[int, float],\n        a_max: Union[int, float],\n        b_min: Union[int, float],\n        b_max: Union[int, float],\n        clip: bool = False,\n    ):\n        self.a_min = a_min\n        self.a_max = a_max\n        self.b_min = b_min\n        self.b_max = b_max\n        self.clip = clip\n\n    def __call__(self, img):\n        img = (img - self.a_min) / (self.a_max - self.a_min)\n        img = img * (self.b_max - self.b_min) + self.b_min\n        if self.clip:\n            img = np.clip(img, self.b_min, self.b_max)\n\n        return img\n\n\nclass AdjustContrast(Transform):\n    """"""Changes image intensity by gamma. Each pixel/voxel intensity is updated as:\n        `x = ((x - min) / intensity_range) ^ gamma * intensity_range + min`\n\n    Args:\n        gamma (float): gamma value to adjust the contrast as function.\n    """"""\n\n    def __init__(self, gamma: Union[int, float]):\n        assert isinstance(gamma, (float, int)), ""gamma must be a float or int number.""\n        self.gamma = gamma\n\n    def __call__(self, img):\n        epsilon = 1e-7\n        img_min = img.min()\n        img_range = img.max() - img_min\n        return np.power(((img - img_min) / float(img_range + epsilon)), self.gamma) * img_range + img_min\n\n\nclass RandAdjustContrast(Randomizable, Transform):\n    """"""Randomly changes image intensity by gamma. Each pixel/voxel intensity is updated as:\n        `x = ((x - min) / intensity_range) ^ gamma * intensity_range + min`\n\n    Args:\n        prob (float): Probability of adjustment.\n        gamma (tuple of float or float): Range of gamma values.\n            If single number, value is picked from (0.5, gamma), default is (0.5, 4.5).\n    """"""\n\n    def __init__(self, prob=0.1, gamma=(0.5, 4.5)):\n        self.prob = prob\n        if not isinstance(gamma, (tuple, list)):\n            assert gamma > 0.5, ""if gamma is single number, must greater than 0.5 and value is picked from (0.5, gamma)""\n            self.gamma = (0.5, gamma)\n        else:\n            self.gamma = gamma\n        assert len(self.gamma) == 2, ""gamma should be a number or pair of numbers.""\n\n        self._do_transform = False\n        self.gamma_value = None\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n        self.gamma_value = self.R.uniform(low=self.gamma[0], high=self.gamma[1])\n\n    def __call__(self, img):\n        self.randomize()\n        if not self._do_transform:\n            return img\n        adjuster = AdjustContrast(self.gamma_value)\n        return adjuster(img)\n'"
monai/transforms/intensity/dictionary.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of dictionary-based wrappers around the ""vanilla"" transforms for intensity adjustment\ndefined in :py:class:`monai.transforms.intensity.array`.\n\nClass names are ended with \'d\' to denote dictionary-based transforms.\n""""""\n\nfrom typing import Hashable, Union, Optional\n\nimport numpy as np\nfrom monai.transforms.compose import MapTransform, Randomizable\nfrom monai.transforms.intensity.array import (\n    NormalizeIntensity,\n    ScaleIntensityRange,\n    ThresholdIntensity,\n    AdjustContrast,\n    ShiftIntensity,\n    ScaleIntensity,\n)\n\n\nclass RandGaussianNoised(Randomizable, MapTransform):\n    """"""Dictionary-based version :py:class:`monai.transforms.RandGaussianNoise`.\n    Add Gaussian noise to image. This transform assumes all the expected fields have same shape.\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: :py:class:`monai.transforms.compose.MapTransform`\n        prob (float): Probability to add Gaussian noise.\n        mean (float or array of floats): Mean or \xe2\x80\x9ccentre\xe2\x80\x9d of the distribution.\n        std (float): Standard deviation (spread) of distribution.\n    """"""\n\n    def __init__(self, keys: Hashable, prob: float = 0.1, mean=0.0, std: float = 0.1):\n        super().__init__(keys)\n        self.prob = prob\n        self.mean = mean\n        self.std = std\n        self._do_transform = False\n        self._noise = None\n\n    def randomize(self, im_shape):\n        self._do_transform = self.R.random() < self.prob\n        self._noise = self.R.normal(self.mean, self.R.uniform(0, self.std), size=im_shape)\n\n    def __call__(self, data):\n        d = dict(data)\n\n        image_shape = d[self.keys[0]].shape  # image shape from the first data key\n        self.randomize(image_shape)\n        if not self._do_transform:\n            return d\n        for key in self.keys:\n            d[key] = d[key] + self._noise.astype(d[key].dtype)\n        return d\n\n\nclass ShiftIntensityd(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:`monai.transforms.ShiftIntensity`.\n    """"""\n\n    def __init__(self, keys: Hashable, offset: Union[int, float]):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            offset (int or float): offset value to shift the intensity of image.\n        """"""\n        super().__init__(keys)\n        self.shifter = ShiftIntensity(offset)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.shifter(d[key])\n        return d\n\n\nclass RandShiftIntensityd(Randomizable, MapTransform):\n    """"""\n    dictionary-based version :py:class:`monai.transforms.RandShiftIntensity`.\n    """"""\n\n    def __init__(self, keys: Hashable, offsets, prob: float = 0.1):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            offsets(int, float, tuple or list): offset range to randomly shift.\n                if single number, offset value is picked from (-offsets, offsets).\n            prob (float): probability of rotating.\n                (Default 0.1, with 10% probability it returns a rotated array.)\n        """"""\n        super().__init__(keys)\n        self.offsets = (-offsets, offsets) if not isinstance(offsets, (list, tuple)) else offsets\n        assert len(self.offsets) == 2, ""offsets should be a number or pair of numbers.""\n        self.prob = prob\n        self._do_transform = False\n\n    def randomize(self):\n        self._offset = self.R.uniform(low=self.offsets[0], high=self.offsets[1])\n        self._do_transform = self.R.random() < self.prob\n\n    def __call__(self, data):\n        d = dict(data)\n        self.randomize()\n        if not self._do_transform:\n            return d\n        shifter = ShiftIntensity(self._offset)\n        for key in self.keys:\n            d[key] = shifter(d[key])\n        return d\n\n\nclass ScaleIntensityd(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensity`.\n    Scale the intensity of input image to the given value range (minv, maxv).\n    If `minv` and `maxv` not provided, use `factor` to scale image by ``v = v * (1 + factor)``.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        minv: Union[int, float] = 0.0,\n        maxv: Union[int, float] = 1.0,\n        factor: Optional[float] = None,\n    ):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            minv (int or float): minimum value of output data.\n            maxv (int or float): maximum value of output data.\n            factor (float): factor scale by ``v = v * (1 + factor)``.\n\n        """"""\n        super().__init__(keys)\n        self.scaler = ScaleIntensity(minv, maxv, factor)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.scaler(d[key])\n        return d\n\n\nclass RandScaleIntensityd(Randomizable, MapTransform):\n    """"""\n    dictionary-based version :py:class:`monai.transforms.RandScaleIntensity`.\n    """"""\n\n    def __init__(self, keys: Hashable, factors, prob: float = 0.1):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            factors(float, tuple or list): factor range to randomly scale by ``v = v * (1 + factor)``.\n                if single number, factor value is picked from (-factors, factors).\n            prob (float): probability of rotating.\n                (Default 0.1, with 10% probability it returns a rotated array.)\n\n        """"""\n        super().__init__(keys)\n        self.factors = (-factors, factors) if not isinstance(factors, (list, tuple)) else factors\n        assert len(self.factors) == 2, ""factors should be a number or pair of numbers.""\n        self.prob = prob\n        self._do_transform = False\n\n    def randomize(self):\n        self.factor = self.R.uniform(low=self.factors[0], high=self.factors[1])\n        self._do_transform = self.R.random() < self.prob\n\n    def __call__(self, data):\n        d = dict(data)\n        self.randomize()\n        if not self._do_transform:\n            return d\n        scaler = ScaleIntensity(minv=None, maxv=None, factor=self.factor)\n        for key in self.keys:\n            d[key] = scaler(d[key])\n        return d\n\n\nclass NormalizeIntensityd(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:`monai.transforms.NormalizeIntensity`.\n    This transform can normalize only non-zero values or entire image, and can also calculate\n    mean and std on each channel separately.\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: monai.transforms.MapTransform\n        subtrahend (ndarray): the amount to subtract by (usually the mean)\n        divisor (ndarray): the amount to divide by (usually the standard deviation)\n        nonzero (bool): whether only normalize non-zero values.\n        channel_wise (bool): if using calculated mean and std, calculate on each channel separately\n            or calculate on the entire image directly.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        subtrahend: Optional[np.ndarray] = None,\n        divisor: Optional[np.ndarray] = None,\n        nonzero: bool = False,\n        channel_wise: bool = False,\n    ):\n        super().__init__(keys)\n        self.normalizer = NormalizeIntensity(subtrahend, divisor, nonzero, channel_wise)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.normalizer(d[key])\n        return d\n\n\nclass ThresholdIntensityd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.ThresholdIntensity`.\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: monai.transforms.MapTransform\n        threshold (float or int): the threshold to filter intensity values.\n        above (bool): filter values above the threshold or below the threshold, default is True.\n        cval (float or int): value to fill the remaining parts of the image, default is 0.\n    """"""\n\n    def __init__(self, keys: Hashable, threshold: Union[int, float], above: bool = True, cval: Union[int, float] = 0):\n        super().__init__(keys)\n        self.filter = ThresholdIntensity(threshold, above, cval)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.filter(d[key])\n        return d\n\n\nclass ScaleIntensityRanged(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.ScaleIntensityRange`.\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: monai.transforms.MapTransform\n        a_min (int or float): intensity original range min.\n        a_max (int or float): intensity original range max.\n        b_min (int or float): intensity target range min.\n        b_max (int or float): intensity target range max.\n        clip (bool): whether to perform clip after scaling.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        a_min: Union[int, float],\n        a_max: Union[int, float],\n        b_min: Union[int, float],\n        b_max: Union[int, float],\n        clip: bool = False,\n    ):\n        super().__init__(keys)\n        self.scaler = ScaleIntensityRange(a_min, a_max, b_min, b_max, clip)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.scaler(d[key])\n        return d\n\n\nclass AdjustContrastd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.AdjustContrast`.\n    Changes image intensity by gamma. Each pixel/voxel intensity is updated as:\n\n        `x = ((x - min) / intensity_range) ^ gamma * intensity_range + min`\n\n    Args:\n        gamma (float): gamma value to adjust the contrast as function.\n    """"""\n\n    def __init__(self, keys: Hashable, gamma: Union[int, float]):\n        super().__init__(keys)\n        self.adjuster = AdjustContrast(gamma)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.adjuster(d[key])\n        return d\n\n\nclass RandAdjustContrastd(Randomizable, MapTransform):\n    """"""\n    Dictionary-based version :py:class:`monai.transforms.RandAdjustContrast`.\n    Randomly changes image intensity by gamma. Each pixel/voxel intensity is updated as:\n\n        `x = ((x - min) / intensity_range) ^ gamma * intensity_range + min`\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: monai.transforms.MapTransform\n        prob (float): Probability of adjustment.\n        gamma (tuple of float or float): Range of gamma values.\n            If single number, value is picked from (0.5, gamma), default is (0.5, 4.5).\n    """"""\n\n    def __init__(self, keys, prob=0.1, gamma=(0.5, 4.5)):\n        super().__init__(keys)\n        self.prob = prob\n        if not isinstance(gamma, (tuple, list)):\n            assert gamma > 0.5, ""if gamma is single number, must greater than 0.5 and value is picked from (0.5, gamma)""\n            self.gamma = (0.5, gamma)\n        else:\n            self.gamma = gamma\n        assert len(self.gamma) == 2, ""gamma should be a number or pair of numbers.""\n\n        self._do_transform = False\n        self.gamma_value = None\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n        self.gamma_value = self.R.uniform(low=self.gamma[0], high=self.gamma[1])\n\n    def __call__(self, data):\n        d = dict(data)\n        self.randomize()\n        if not self._do_transform:\n            return d\n        adjuster = AdjustContrast(self.gamma_value)\n        for key in self.keys:\n            d[key] = adjuster(d[key])\n        return d\n\n\nRandGaussianNoiseD = RandGaussianNoiseDict = RandGaussianNoised\nShiftIntensityD = ShiftIntensityDict = ShiftIntensityd\nRandShiftIntensityD = RandShiftIntensityDict = RandShiftIntensityd\nScaleIntensityD = ScaleIntensityDict = ScaleIntensityd\nRandScaleIntensityD = RandScaleIntensityDict = RandScaleIntensityd\nNormalizeIntensityD = NormalizeIntensityDict = NormalizeIntensityd\nThresholdIntensityD = ThresholdIntensityDict = ThresholdIntensityd\nScaleIntensityRangeD = ScaleIntensityRangeDict = ScaleIntensityRanged\nAdjustContrastD = AdjustContrastDict = AdjustContrastd\nRandAdjustContrastD = RandAdjustContrastDict = RandAdjustContrastd\n'"
monai/transforms/io/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/transforms/io/array.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of ""vanilla"" transforms for IO functions\nhttps://github.com/Project-MONAI/MONAI/wiki/MONAI_Design\n""""""\n\nfrom typing import Optional\n\nimport numpy as np\nimport nibabel as nib\nfrom PIL import Image\nfrom torch.utils.data._utils.collate import np_str_obj_array_pattern\n\nfrom monai.data.utils import correct_nifti_header_if_necessary\nfrom monai.transforms.compose import Transform\nfrom monai.utils.misc import ensure_tuple\n\n\nclass LoadNifti(Transform):\n    """"""\n    Load Nifti format file or files from provided path. If loading a list of\n    files, stack them together and add a new dimension as first dimension, and\n    use the meta data of the first image to represent the stacked result. Note\n    that the affine transform of all the images should be same if ``image_only=False``.\n    """"""\n\n    def __init__(\n        self, as_closest_canonical: bool = False, image_only: bool = False, dtype: Optional[np.dtype] = np.float32\n    ):\n        """"""\n        Args:\n            as_closest_canonical (bool): if True, load the image as closest to canonical axis format.\n            image_only (bool): if True return only the image volume, otherwise return image data array and header dict.\n            dtype (np.dtype, optional): if not None convert the loaded image to this data type.\n\n        Note:\n            The transform returns image data array if `image_only` is True,\n            or a tuple of two elements containing the data array, and the Nifti\n            header in a dict format otherwise.\n            if a dictionary header is returned:\n\n            - header[\'affine\'] stores the affine of the image.\n            - header[\'original_affine\'] will be additionally created to store the original affine.\n        """"""\n        self.as_closest_canonical = as_closest_canonical\n        self.image_only = image_only\n        self.dtype = dtype\n\n    def __call__(self, filename):\n        """"""\n        Args:\n            filename (str, list, tuple, file): path file or file-like object or a list of files.\n        """"""\n        filename = ensure_tuple(filename)\n        img_array = list()\n        compatible_meta = dict()\n        for name in filename:\n            img = nib.load(name)\n            img = correct_nifti_header_if_necessary(img)\n            header = dict(img.header)\n            header[""filename_or_obj""] = name\n            header[""affine""] = img.affine\n            header[""original_affine""] = img.affine.copy()\n            header[""as_closest_canonical""] = self.as_closest_canonical\n            ndim = img.header[""dim""][0]\n            spatial_rank = min(ndim, 3)\n            header[""spatial_shape""] = img.header[""dim""][1 : spatial_rank + 1]\n\n            if self.as_closest_canonical:\n                img = nib.as_closest_canonical(img)\n                header[""affine""] = img.affine\n\n            img_array.append(np.array(img.get_fdata(dtype=self.dtype)))\n            img.uncache()\n\n            if self.image_only:\n                continue\n\n            if not compatible_meta:\n                for meta_key in header:\n                    meta_datum = header[meta_key]\n                    # pytype: disable=attribute-error\n                    if (\n                        type(meta_datum).__name__ == ""ndarray""\n                        and np_str_obj_array_pattern.search(meta_datum.dtype.str) is not None\n                    ):\n                        continue\n                    # pytype: enable=attribute-error\n                    compatible_meta[meta_key] = meta_datum\n            else:\n                assert np.allclose(\n                    header[""affine""], compatible_meta[""affine""]\n                ), ""affine data of all images should be same.""\n\n        img_array = np.stack(img_array, axis=0) if len(img_array) > 1 else img_array[0]\n        if self.image_only:\n            return img_array\n        return img_array, compatible_meta\n\n\nclass LoadPNG(Transform):\n    """"""\n    Load common 2D image format (PNG, JPG, etc. using PIL) file or files from provided path.\n    It\'s based on the Image module in PIL library:\n    https://pillow.readthedocs.io/en/stable/reference/Image.html\n    """"""\n\n    def __init__(self, image_only: bool = False, dtype: Optional[np.dtype] = np.float32):\n        """"""Args:\n            image_only (bool): if True return only the image volume, otherwise return image data array and metadata.\n            dtype (np.dtype, optional): if not None convert the loaded image to this data type.\n        """"""\n        self.image_only = image_only\n        self.dtype = dtype\n\n    def __call__(self, filename):\n        """"""\n        Args:\n            filename (str, list, tuple, file): path file or file-like object or a list of files.\n        """"""\n        filename = ensure_tuple(filename)\n        img_array = list()\n        compatible_meta = None\n        for name in filename:\n            img = Image.open(name)\n            data = np.asarray(img)\n            if self.dtype:\n                data = data.astype(self.dtype)\n            img_array.append(data)\n            meta = dict()\n            meta[""filename_or_obj""] = name\n            meta[""spatial_shape""] = data.shape[:2]\n            meta[""format""] = img.format\n            meta[""mode""] = img.mode\n            meta[""width""] = img.width\n            meta[""height""] = img.height\n            meta[""info""] = img.info\n\n            if self.image_only:\n                continue\n\n            if not compatible_meta:\n                compatible_meta = meta\n            else:\n                assert np.allclose(\n                    meta[""spatial_shape""], compatible_meta[""spatial_shape""]\n                ), ""all the images in the list should have same spatial shape.""\n\n        img_array = np.stack(img_array, axis=0) if len(img_array) > 1 else img_array[0]\n        return img_array if self.image_only else (img_array, compatible_meta)\n'"
monai/transforms/io/dictionary.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of dictionary-based wrappers around the ""vanilla"" transforms for IO functions\ndefined in :py:class:`monai.transforms.io.array`.\n\nClass names are ended with \'d\' to denote dictionary-based transforms.\n""""""\n\nfrom typing import Hashable, Optional\n\nimport numpy as np\n\nfrom monai.transforms.compose import MapTransform\nfrom monai.transforms.io.array import LoadNifti, LoadPNG\n\n\nclass LoadNiftid(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.LoadNifti`,\n    must load image and metadata together. If loading a list of files in one key,\n    stack them together and add a new dimension as the first dimension, and use the\n    meta data of the first image to represent the stacked result. Note that the affine\n    transform of all the stacked images should be same. The output metadata field will\n    be created as ``self.meta_key_format(key, metadata_key)``.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        as_closest_canonical: bool = False,\n        dtype: Optional[np.dtype] = np.float32,\n        meta_key_format: str = ""{}.{}"",\n        overwriting_keys: bool = False,\n    ):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            as_closest_canonical (bool): if True, load the image as closest to canonical axis format.\n            dtype (np.dtype, optional): if not None convert the loaded image to this data type.\n            meta_key_format (str): key format to store meta data of the nifti image.\n                it must contain 2 fields for the key of this image and the key of every meta data item.\n            overwriting_keys (bool): whether allow to overwrite existing keys of meta data.\n                default is False, which will raise exception if encountering existing key.\n        """"""\n        super().__init__(keys)\n        self.loader = LoadNifti(as_closest_canonical, False, dtype)\n        self.meta_key_format = meta_key_format\n        self.overwriting_keys = overwriting_keys\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            data = self.loader(d[key])\n            assert isinstance(data, (tuple, list)), ""loader must return a tuple or list.""\n            d[key] = data[0]\n            assert isinstance(data[1], dict), ""metadata must be a dict.""\n            for k in sorted(data[1]):\n                key_to_add = self.meta_key_format.format(key, k)\n                if key_to_add in d and not self.overwriting_keys:\n                    raise KeyError(f""meta data key {key_to_add} already exists."")\n                d[key_to_add] = data[1][k]\n        return d\n\n\nclass LoadPNGd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.LoadPNG`.\n    """"""\n\n    def __init__(self, keys: Hashable, dtype: Optional[np.dtype] = np.float32, meta_key_format: str = ""{}.{}""):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            dtype (np.dtype, optional): if not None convert the loaded image to this data type.\n            meta_key_format (str): key format to store meta data of the loaded image.\n                it must contain 2 fields for the key of this image and the key of every meta data item.\n        """"""\n        super().__init__(keys)\n        self.loader = LoadPNG(False, dtype)\n        self.meta_key_format = meta_key_format\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            data = self.loader(d[key])\n            assert isinstance(data, (tuple, list)), ""loader must return a tuple or list.""\n            d[key] = data[0]\n            assert isinstance(data[1], dict), ""metadata must be a dict.""\n            for k in sorted(data[1]):\n                key_to_add = self.meta_key_format.format(key, k)\n                d[key_to_add] = data[1][k]\n        return d\n\n\nLoadNiftiD = LoadNiftiDict = LoadNiftid\nLoadPNGD = LoadPNGDict = LoadPNGd\n'"
monai/transforms/post/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/transforms/post/array.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of ""vanilla"" transforms for the model output tensors\nhttps://github.com/Project-MONAI/MONAI/wiki/MONAI_Design\n""""""\n\nfrom typing import Optional, Callable\n\nimport torch\nfrom monai.transforms.compose import Transform\nfrom monai.networks.utils import one_hot\nfrom monai.transforms.utils import get_largest_connected_component_mask\n\n\nclass SplitChannel(Transform):\n    """"""\n    Split PyTorch Tensor data according to the channel dim, if only 1 channel, convert to One-Hot\n    format first based on the class number. Users can use this transform to compute metrics on every\n    single class to get more details of validation/evaluation. Expected input shape:\n    (batch_size, num_channels, spatial_dim_1[, spatial_dim_2, ...])\n\n    Args:\n        to_onehot (bool): whether to convert the data to One-Hot format first, default is False.\n        num_classes: the class number used to convert to One-Hot format if `to_onehot` is True.\n\n    """"""\n\n    def __init__(self, to_onehot: bool = False, num_classes: Optional[int] = None):\n        self.to_onehot = to_onehot\n        self.num_classes = num_classes\n\n    def __call__(self, img, to_onehot: Optional[bool] = None, num_classes: Optional[int] = None):  # type: ignore # see issue #495\n        if to_onehot or self.to_onehot:\n            if num_classes is None:\n                num_classes = self.num_classes\n            assert isinstance(num_classes, int), ""must specify class number for One-Hot.""\n            img = one_hot(img, num_classes)\n        n_classes = img.shape[1]\n        outputs = list()\n        for i in range(n_classes):\n            outputs.append(img[:, i : i + 1])\n\n        return outputs\n\n\nclass Activations(Transform):\n    """"""\n    Add activation operations to the model output, typically `Sigmoid` or `Softmax`.\n\n    Args:\n        sigmoid (bool): whether to execute sigmoid function on model output before transform.\n        softmax (bool): whether to execute softmax function on model output before transform.\n        other (Callable): callable function to execute other activation layers, for example:\n            `other = lambda x: torch.tanh(x)`\n\n    """"""\n\n    def __init__(self, sigmoid: bool = False, softmax: bool = False, other: Optional[Callable] = None):\n        self.sigmoid = sigmoid\n        self.softmax = softmax\n        self.other = other\n\n    def __call__(  # type: ignore # see issue #495\n        self, img, sigmoid: Optional[bool] = None, softmax: Optional[bool] = None, other: Optional[Callable] = None\n    ):\n        if sigmoid is True and softmax is True:\n            raise ValueError(""sigmoid=True and softmax=True are not compatible."")\n        if sigmoid or self.sigmoid:\n            img = torch.sigmoid(img)\n        if softmax or self.softmax:\n            img = torch.softmax(img, dim=1)\n        act_func = self.other if other is None else other\n        if act_func is not None:\n            if not callable(act_func):\n                raise ValueError(""act_func must be a Callable function."")\n            img = act_func(img)\n\n        return img\n\n\nclass AsDiscrete(Transform):\n    """"""Execute after model forward to transform model output to discrete values.\n    It can complete below operations:\n\n        -  execute `argmax` for input logits values.\n        -  threshold input value to 0.0 or 1.0.\n        -  convert input value to One-Hot format\n\n    Args:\n        argmax (bool): whether to execute argmax function on input data before transform.\n        to_onehot (bool): whether to convert input data into the one-hot format. Defaults to False.\n        n_classes (bool): the number of classes to convert to One-Hot format.\n        threshold_values (bool): whether threshold the float value to int number 0 or 1, default is False.\n        logit_thresh (float): the threshold value for thresholding operation, default is 0.5.\n\n    """"""\n\n    def __init__(\n        self,\n        argmax: bool = False,\n        to_onehot: bool = False,\n        n_classes: Optional[int] = None,\n        threshold_values: bool = False,\n        logit_thresh: float = 0.5,\n    ):\n        self.argmax = argmax\n        self.to_onehot = to_onehot\n        self.n_classes = n_classes\n        self.threshold_values = threshold_values\n        self.logit_thresh = logit_thresh\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img,\n        argmax: Optional[bool] = None,\n        to_onehot: Optional[bool] = None,\n        n_classes: Optional[int] = None,\n        threshold_values: Optional[bool] = None,\n        logit_thresh: Optional[float] = None,\n    ):\n        if argmax or self.argmax:\n            img = torch.argmax(img, dim=1, keepdim=True)\n\n        if to_onehot or self.to_onehot:\n            _nclasses = self.n_classes if n_classes is None else n_classes\n            assert isinstance(_nclasses, int), ""One of self.n_classes or n_classes must be an integer""\n            img = one_hot(img, _nclasses)\n\n        if threshold_values or self.threshold_values:\n            img = img >= (self.logit_thresh if logit_thresh is None else logit_thresh)\n\n        return img.float()\n\n\nclass KeepLargestConnectedComponent(Transform):\n    """"""\n    Keeps only the largest connected component in the image.\n    This transform can be used as a post-processing step to clean up over-segment areas in model output.\n    The input is assumed to be a PyTorch Tensor with shape (batch_size, 1, spatial_dim1[, spatial_dim2, ...])\n\n    Expected input data should have only 1 channel and the values correspond to expected labels.\n\n    For example:\n    Use KeepLargestConnectedComponent with applied_values=[1], connectivity=1\n\n       [1, 0, 0]         [0, 0, 0]\n       [0, 1, 1]    =>   [0, 1 ,1]\n       [0, 1, 1]         [0, 1, 1]\n\n    Use KeepLargestConnectedComponent with applied_values[1, 2], independent=False, connectivity=1\n\n      [0, 0, 1, 0 ,0]           [0, 0, 1, 0 ,0]\n      [0, 2, 1, 1 ,1]           [0, 2, 1, 1 ,1]\n      [1, 2, 1, 0 ,0]    =>     [1, 2, 1, 0 ,0]\n      [1, 2, 0, 1 ,0]           [1, 2, 0, 0 ,0]\n      [2, 2, 0, 0 ,2]           [2, 2, 0, 0 ,0]\n\n    Use KeepLargestConnectedComponent with applied_values[1, 2], independent=True, connectivity=1\n\n      [0, 0, 1, 0 ,0]           [0, 0, 1, 0 ,0]\n      [0, 2, 1, 1 ,1]           [0, 2, 1, 1 ,1]\n      [1, 2, 1, 0 ,0]    =>     [0, 2, 1, 0 ,0]\n      [1, 2, 0, 1 ,0]           [0, 2, 0, 0 ,0]\n      [2, 2, 0, 0 ,2]           [2, 2, 0, 0 ,0]\n\n    Use KeepLargestConnectedComponent with applied_values[1, 2], independent=False, connectivity=2\n\n      [0, 0, 1, 0 ,0]           [0, 0, 1, 0 ,0]\n      [0, 2, 1, 1 ,1]           [0, 2, 1, 1 ,1]\n      [1, 2, 1, 0 ,0]    =>     [1, 2, 1, 0 ,0]\n      [1, 2, 0, 1 ,0]           [1, 2, 0, 1 ,0]\n      [2, 2, 0, 0 ,2]           [2, 2, 0, 0 ,2]\n\n    """"""\n\n    def __init__(\n        self, applied_values, independent: bool = True, background: int = 0, connectivity: Optional[int] = None\n    ):\n        """"""\n        Args:\n            applied_values (list or tuple of int): number list for applying the connected component on.\n                The pixel whose value is not in this list will remain unchanged.\n            independent (bool): consider several labels as a whole or independent, default is `True`.\n                Example use case would be segment label 1 is liver and label 2 is liver tumor, in that case\n                you want this ""independent"" to be specified as False.\n            background: Background pixel value. The over-segmented pixels will be set as this value.\n            connectivity: Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\n                Accepted values are ranging from  1 to input.ndim. If ``None``, a full\n                connectivity of ``input.ndim`` is used.\n        """"""\n        super().__init__()\n        self.applied_values = applied_values\n        self.independent = independent\n        self.background = background\n        self.connectivity = connectivity\n        if background in applied_values:\n            raise ValueError(""Background pixel can\'t be in applied_values."")\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img: shape must be (batch_size, 1, spatial_dim1[, spatial_dim2, ...]).\n\n        Returns:\n            A PyTorch Tensor with shape (batch_size, 1, spatial_dim1[, spatial_dim2, ...]).\n        """"""\n        channel_dim = 1\n        if img.shape[channel_dim] == 1:\n            img = torch.squeeze(img, dim=channel_dim)\n        else:\n            raise ValueError(""Input data have more than 1 channel."")\n\n        if self.independent:\n            for i in self.applied_values:\n                foreground = (img == i).type(torch.uint8)\n                mask = get_largest_connected_component_mask(foreground, self.connectivity)\n                img[foreground != mask] = self.background\n        else:\n            foreground = torch.zeros_like(img)\n            for i in self.applied_values:\n                foreground += (img == i).type(torch.uint8)\n            mask = get_largest_connected_component_mask(foreground, self.connectivity)\n            img[foreground != mask] = self.background\n\n        return torch.unsqueeze(img, dim=channel_dim)\n'"
monai/transforms/post/dictionary.py,1,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of dictionary-based wrappers around the ""vanilla"" transforms for model output tensors\ndefined in :py:class:`monai.transforms.utility.array`.\n\nClass names are ended with \'d\' to denote dictionary-based transforms.\n""""""\n\nfrom typing import Optional, Hashable\n\nfrom monai.utils.misc import ensure_tuple_rep\nfrom monai.transforms.compose import MapTransform\nfrom monai.transforms.post.array import SplitChannel, Activations, AsDiscrete, KeepLargestConnectedComponent\n\n\nclass SplitChanneld(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.SplitChannel`.\n    All the input specified by `keys` should be splitted into same count of data.\n\n    """"""\n\n    def __init__(self, keys: Hashable, output_postfixes, to_onehot=False, num_classes=None):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            output_postfixes (list, tuple): the postfixes to construct keys to store splitted data.\n                for example: if the key of input data is `pred` and split 2 classes, the output\n                data keys will be: pred_(output_postfixes[0]), pred_(output_postfixes[1])\n            to_onehot (bool or list of bool): whether to convert the data to One-Hot format, default is False.\n            num_classes (int or list of int): the class number used to convert to One-Hot format\n                if `to_onehot` is True.\n        """"""\n        super().__init__(keys)\n        if not isinstance(output_postfixes, (list, tuple)):\n            raise ValueError(""must specify key postfixes to store splitted data."")\n        self.output_postfixes = output_postfixes\n        self.to_onehot = ensure_tuple_rep(to_onehot, len(self.keys))\n        self.num_classes = ensure_tuple_rep(num_classes, len(self.keys))\n        self.splitter = SplitChannel()\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            rets = self.splitter(d[key], self.to_onehot[idx], self.num_classes[idx])\n            assert len(self.output_postfixes) == len(rets), ""count of splitted results must match output_postfixes.""\n            for i, r in enumerate(rets):\n                d[f""{key}_{self.output_postfixes[i]}""] = r\n        return d\n\n\nclass Activationsd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.AddActivations`.\n    Add activation layers to the input data specified by `keys`.\n    """"""\n\n    def __init__(self, keys: Hashable, output_postfix: str = ""act"", sigmoid=False, softmax=False, other=None):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to model output and label.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            output_postfix (str): the postfix string to construct keys to store converted data.\n                for example: if the keys of input data is `pred` and `label`, output_postfix is `act`,\n                the output data keys will be: `pred_act`, `label_act`.\n                if set to None, will replace the original data with the same key.\n            sigmoid (bool, tuple or list of bool): whether to execute sigmoid function on model\n                output before transform.\n            softmax (bool, tuple or list of bool): whether to execute softmax function on model\n                output before transform.\n            other (Callable, tuple or list of Callables): callable function to execute other activation layers,\n                for example: `other = lambda x: torch.tanh(x)`\n        """"""\n        super().__init__(keys)\n        if output_postfix is not None and not isinstance(output_postfix, str):\n            raise ValueError(""output_postfix must be a string."")\n        self.output_postfix = output_postfix\n        self.sigmoid = ensure_tuple_rep(sigmoid, len(self.keys))\n        self.softmax = ensure_tuple_rep(softmax, len(self.keys))\n        self.other = ensure_tuple_rep(other, len(self.keys))\n        self.converter = Activations()\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            ret = self.converter(d[key], self.sigmoid[idx], self.softmax[idx], self.other[idx])\n            output_key = key if self.output_postfix is None else f""{key}_{self.output_postfix}""\n            d[output_key] = ret\n        return d\n\n\nclass AsDiscreted(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.AsDiscrete`.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        output_postfix: str = ""discreted"",\n        argmax=False,\n        to_onehot=False,\n        n_classes=None,\n        threshold_values=False,\n        logit_thresh=0.5,\n    ):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to model output and label.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            output_postfix (str): the postfix string to construct keys to store converted data.\n                for example: if the keys of input data is `pred` and `label`, output_postfix is `discreted`,\n                the output data keys will be: `pred_discreted`, `label_discreted`.\n                if set to None, will replace the original data with the same key.\n            argmax (bool): whether to execute argmax function on input data before transform.\n            to_onehot (bool): whether to convert input data into the one-hot format. Defaults to False.\n            n_classes (bool): the number of classes to convert to One-Hot format.\n            threshold_values (bool): whether threshold the float value to int number 0 or 1, default is False.\n            logit_thresh (float): the threshold value for thresholding operation, default is 0.5.\n        """"""\n        super().__init__(keys)\n        if output_postfix is not None and not isinstance(output_postfix, str):\n            raise ValueError(""output_postfix must be a string."")\n        self.output_postfix = output_postfix\n        self.argmax = ensure_tuple_rep(argmax, len(self.keys))\n        self.to_onehot = ensure_tuple_rep(to_onehot, len(self.keys))\n        self.n_classes = ensure_tuple_rep(n_classes, len(self.keys))\n        self.threshold_values = ensure_tuple_rep(threshold_values, len(self.keys))\n        self.logit_thresh = ensure_tuple_rep(logit_thresh, len(self.keys))\n        self.converter = AsDiscrete()\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            output_key = key if self.output_postfix is None else f""{key}_{self.output_postfix}""\n            d[output_key] = self.converter(\n                d[key],\n                self.argmax[idx],\n                self.to_onehot[idx],\n                self.n_classes[idx],\n                self.threshold_values[idx],\n                self.logit_thresh[idx],\n            )\n        return d\n\n\nclass KeepLargestConnectedComponentd(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:monai.transforms.utility.array.KeepLargestConnectedComponent.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        applied_values,\n        independent: bool = True,\n        background: int = 0,\n        connectivity: Optional[int] = None,\n        output_postfix: str = ""largestcc"",\n    ):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            applied_values (list or tuple of int): number list for applying the connected component on.\n                The pixel whose value is not in this list will remain unchanged.\n            independent (bool): consider several labels as a whole or independent, default is `True`.\n                Example use case would be segment label 1 is liver and label 2 is liver tumor, in that case\n                you want this ""independent"" to be specified as False.\n            background: Background pixel value. The over-segmented pixels will be set as this value.\n            connectivity: Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\n                Accepted values are ranging from  1 to input.ndim. If ``None``, a full\n                connectivity of ``input.ndim`` is used.\n            output_postfix (str): the postfix string to construct keys to store converted data.\n                for example: if the keys of input data is `label`, output_postfix is `largestcc`,\n                the output data keys will be: `label_largestcc`.\n                if set to None, will replace the original data with the same key.\n        """"""\n        super().__init__(keys)\n        if output_postfix is not None and not isinstance(output_postfix, str):\n            raise ValueError(""output_postfix must be a string."")\n        self.output_postfix = output_postfix\n        self.converter = KeepLargestConnectedComponent(applied_values, independent, background, connectivity)\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            output_key = key if self.output_postfix is None else f""{key}_{self.output_postfix}""\n            d[output_key] = self.converter(d[key])\n        return d\n\n\nSplitChannelD = SplitChannelDict = SplitChanneld\nActivationsD = ActivationsDict = Activationsd\nAsDiscreteD = AsDiscreteDict = AsDiscreted\nKeepLargestConnectedComponentD = KeepLargestConnectedComponentDict = KeepLargestConnectedComponentd\n'"
monai/transforms/spatial/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/transforms/spatial/array.py,32,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of ""vanilla"" transforms for spatial operations\nhttps://github.com/Project-MONAI/MONAI/wiki/MONAI_Design\n""""""\n\nimport warnings\nfrom typing import List, Optional, Union\n\nimport nibabel as nib\nimport numpy as np\nimport scipy.ndimage\nimport torch\nfrom skimage.transform import resize\n\nfrom monai.data.utils import InterpolationCode, compute_shape_offset, to_affine_nd, zoom_affine\nfrom monai.networks.layers import AffineTransform, GaussianFilter\nfrom monai.transforms.compose import Randomizable, Transform\nfrom monai.transforms.utils import (\n    create_control_grid,\n    create_grid,\n    create_rotate,\n    create_scale,\n    create_shear,\n    create_translate,\n)\nfrom monai.utils.misc import ensure_tuple\n\n\nclass Spacing(Transform):\n    """"""\n    Resample input image into the specified `pixdim`.\n    """"""\n\n    def __init__(\n        self,\n        pixdim,\n        diagonal: bool = False,\n        interp_order: str = ""bilinear"",\n        mode: str = ""border"",\n        dtype: Optional[np.dtype] = None,\n    ):\n        """"""\n        Args:\n            pixdim (sequence of floats): output voxel spacing.\n            diagonal (bool): whether to resample the input to have a diagonal affine matrix.\n                If True, the input data is resampled to the following affine::\n\n                    np.diag((pixdim_0, pixdim_1, ..., pixdim_n, 1))\n\n                This effectively resets the volume to the world coordinate system (RAS+ in nibabel).\n                The original orientation, rotation, shearing are not preserved.\n\n                If False, this transform preserves the axes orientation, orthogonal rotation and\n                translation components from the original affine. This option will not flip/swap axes\n                of the original data.\n            interp_order (`nearest|bilinear`): The interpolation mode, default is `bilinear`.\n                See also: https://pytorch.org/docs/stable/nn.functional.html#grid-sample.\n            mode (`zeros|border|reflection`):\n                The mode parameter determines how the input array is extended beyond its boundaries.\n                Defaults to `border`.\n            dtype (None or np.dtype): output array data type, defaults to np.float32.\n        """"""\n        self.pixdim = np.array(ensure_tuple(pixdim), dtype=np.float64)\n        self.diagonal = diagonal\n        self.interp_order = interp_order\n        self.mode = mode\n        self.dtype = dtype\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        data_array: np.ndarray,\n        affine=None,\n        interp_order: Optional[str] = None,\n        mode: Optional[str] = None,\n        dtype: Optional[np.dtype] = None,\n    ):\n        """"""\n        Args:\n            data_array (ndarray): in shape (num_channels, H[, W, ...]).\n            affine (matrix): (N+1)x(N+1) original affine matrix for spatially ND `data_array`. Defaults to identity.\n        Returns:\n            data_array (resampled into `self.pixdim`), original pixdim, current pixdim.\n        """"""\n        sr = data_array.ndim - 1\n        if sr <= 0:\n            raise ValueError(""the array should have at least one spatial dimension."")\n        if affine is None:\n            # default to identity\n            affine = np.eye(sr + 1, dtype=np.float64)\n            affine_ = np.eye(sr + 1, dtype=np.float64)\n        else:\n            affine_ = to_affine_nd(sr, affine)\n        out_d = self.pixdim[:sr]\n        if out_d.size < sr:\n            out_d = np.append(out_d, [1.0] * (out_d.size - sr))\n        if np.any(out_d <= 0):\n            raise ValueError(f""pixdim must be positive, got {out_d}"")\n        # compute output affine, shape and offset\n        new_affine = zoom_affine(affine_, out_d, diagonal=self.diagonal)\n        output_shape, offset = compute_shape_offset(data_array.shape[1:], affine_, new_affine)\n        new_affine[:sr, -1] = offset[:sr]\n        transform = np.linalg.inv(affine_) @ new_affine\n        # adapt to the actual rank\n        transform_ = to_affine_nd(sr, transform)\n        _dtype = dtype or self.dtype or np.float32\n\n        # no resampling if it\'s identity transform\n        if np.allclose(transform_, np.diag(np.ones(len(transform_))), atol=1e-3):\n            output_data = data_array.copy().astype(_dtype)\n            new_affine = to_affine_nd(affine, new_affine)\n            return output_data, affine, new_affine\n\n        # resample\n        affine_xform = AffineTransform(\n            normalized=False,\n            mode=interp_order or self.interp_order,\n            padding_mode=mode or self.mode,\n            align_corners=True,\n            reverse_indexing=True,\n        )\n        output_data = affine_xform(\n            torch.from_numpy((data_array.astype(np.float64))[None]),  # AffineTransform requires a batch dim\n            torch.from_numpy(transform_.astype(np.float64)),\n            spatial_size=output_shape,\n        )\n        output_data = output_data.squeeze(0).detach().cpu().numpy().astype(_dtype)\n        new_affine = to_affine_nd(affine, new_affine)\n        return output_data, affine, new_affine\n\n\nclass Orientation(Transform):\n    """"""\n    Change the input image\'s orientation into the specified based on `axcodes`.\n    """"""\n\n    def __init__(self, axcodes=None, as_closest_canonical: bool = False, labels=tuple(zip(""LPI"", ""RAS""))):\n        """"""\n        Args:\n            axcodes (N elements sequence): for spatial ND input\'s orientation.\n                e.g. axcodes=\'RAS\' represents 3D orientation:\n                (Left, Right), (Posterior, Anterior), (Inferior, Superior).\n                default orientation labels options are: \'L\' and \'R\' for the first dimension,\n                \'P\' and \'A\' for the second, \'I\' and \'S\' for the third.\n            as_closest_canonical (boo): if True, load the image as closest to canonical axis format.\n            labels : optional, None or sequence of (2,) sequences\n                (2,) sequences are labels for (beginning, end) of output axis.\n                Defaults to ``((\'L\', \'R\'), (\'P\', \'A\'), (\'I\', \'S\'))``.\n\n        See Also: `nibabel.orientations.ornt2axcodes`.\n        """"""\n        if axcodes is None and not as_closest_canonical:\n            raise ValueError(""provide either `axcodes` or `as_closest_canonical=True`."")\n        if axcodes is not None and as_closest_canonical:\n            warnings.warn(""using as_closest_canonical=True, axcodes ignored."")\n        self.axcodes = axcodes\n        self.as_closest_canonical = as_closest_canonical\n        self.labels = labels\n\n    def __call__(self, data_array: np.ndarray, affine=None):  # type: ignore # see issue #495\n        """"""\n        original orientation of `data_array` is defined by `affine`.\n\n        Args:\n            data_array (ndarray): in shape (num_channels, H[, W, ...]).\n            affine (matrix): (N+1)x(N+1) original affine matrix for spatially ND `data_array`. Defaults to identity.\n        Returns:\n            data_array (reoriented in `self.axcodes`), original axcodes, current axcodes.\n        """"""\n        sr = data_array.ndim - 1\n        if sr <= 0:\n            raise ValueError(""the array should have at least one spatial dimension."")\n        if affine is None:\n            affine = np.eye(sr + 1, dtype=np.float64)\n            affine_ = np.eye(sr + 1, dtype=np.float64)\n        else:\n            affine_ = to_affine_nd(sr, affine)\n        src = nib.io_orientation(affine_)\n        if self.as_closest_canonical:\n            spatial_ornt = src\n        else:\n            dst = nib.orientations.axcodes2ornt(self.axcodes[:sr], labels=self.labels)\n            if len(dst) < sr:\n                raise ValueError(\n                    f""`self.axcodes` should have at least {sr} elements""\n                    f\' given the data array is in spatial {sr}D, got ""{self.axcodes}""\'\n                )\n            spatial_ornt = nib.orientations.ornt_transform(src, dst)\n        ornt = spatial_ornt.copy()\n        ornt[:, 0] += 1  # skip channel dim\n        ornt = np.concatenate([np.array([[0, 1]]), ornt])\n        shape = data_array.shape[1:]\n        data_array = nib.orientations.apply_orientation(data_array, ornt)\n        new_affine = affine_ @ nib.orientations.inv_ornt_aff(spatial_ornt, shape)\n        new_affine = to_affine_nd(affine, new_affine)\n        return data_array, affine, new_affine\n\n\nclass Flip(Transform):\n    """"""Reverses the order of elements along the given spatial axis. Preserves shape.\n    Uses ``np.flip`` in practice. See numpy.flip for additional details.\n    https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html\n\n    Args:\n        spatial_axis (None, int or tuple of ints): spatial axes along which to flip over. Default is None.\n    """"""\n\n    def __init__(self, spatial_axis=None):\n        self.spatial_axis = spatial_axis\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (ndarray): channel first array, must have shape: (num_channels, H[, W, ..., ]),\n        """"""\n        flipped = list()\n        for channel in img:\n            flipped.append(np.flip(channel, self.spatial_axis))\n        return np.stack(flipped).astype(img.dtype)\n\n\nclass Resize(Transform):\n    """"""\n    Resize the input image to given resolution. Uses skimage.transform.resize underneath.\n    For additional details, see https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize.\n\n    Args:\n        spatial_size (tuple or list): expected shape of spatial dimensions after resize operation.\n        interp_order: Order of spline interpolation. Default=InterpolationCode.LINEAR.\n        mode (str): Points outside boundaries are filled according to given mode.\n            Options are \'constant\', \'edge\', \'symmetric\', \'reflect\', \'wrap\'.\n        cval (float): Used with mode \'constant\', the value outside image boundaries.\n        clip (bool): Whether to clip range of output values after interpolation. Default: True.\n        preserve_range (bool): Whether to keep original range of values. Default is True.\n            If False, input is converted according to conventions of img_as_float. See\n            https://scikit-image.org/docs/dev/user_guide/data_types.html.\n        anti_aliasing (bool): Whether to apply a gaussian filter to image before down-scaling.\n            Default is True.\n    """"""\n\n    def __init__(\n        self,\n        spatial_size,\n        interp_order=InterpolationCode.LINEAR,\n        mode: str = ""reflect"",\n        cval: Union[int, float] = 0,\n        clip: bool = True,\n        preserve_range: bool = True,\n        anti_aliasing: bool = True,\n    ):\n        self.spatial_size = spatial_size\n        self.interp_order = interp_order\n        self.mode = mode\n        self.cval = cval\n        self.clip = clip\n        self.preserve_range = preserve_range\n        self.anti_aliasing = anti_aliasing\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img,\n        order=None,\n        mode: Optional[str] = None,\n        cval: Optional[Union[int, float]] = None,\n        clip: Optional[bool] = None,\n        preserve_range: Optional[bool] = None,\n        anti_aliasing: Optional[bool] = None,\n    ):\n        """"""\n        Args:\n            img (ndarray): channel first array, must have shape: (num_channels, H[, W, ..., ]),\n        """"""\n        resized = list()\n        for channel in img:\n            resized.append(\n                resize(\n                    image=channel,\n                    output_shape=self.spatial_size,\n                    order=self.interp_order if order is None else order,\n                    mode=mode or self.mode,\n                    cval=self.cval if cval is None else cval,\n                    clip=self.clip if clip is None else clip,\n                    preserve_range=self.preserve_range if preserve_range is None else preserve_range,\n                    anti_aliasing=self.anti_aliasing if anti_aliasing is None else anti_aliasing,\n                )\n            )\n        return np.stack(resized).astype(img.dtype)\n\n\nclass Rotate(Transform):\n    """"""\n    Rotates an input image by given angle. Uses scipy.ndimage.rotate. For more details, see\n    https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.rotate.html\n\n    Args:\n        angle (float): Rotation angle in degrees.\n        spatial_axes (tuple of 2 ints): Spatial axes of rotation. Default: (0, 1).\n            This is the first two axis in spatial dimensions.\n        reshape (bool): If reshape is true, the output shape is adapted so that the\n            input array is contained completely in the output. Default is True.\n        interp_order: Order of spline interpolation. Range 0-5. Default: InterpolationCode.LINEAR. This is\n            different from scipy where default interpolation is InterpolationCode.SPLINE3.\n        mode (str): Points outside boundary filled according to this mode. Options are\n            \'constant\', \'nearest\', \'reflect\', \'wrap\'. Default: \'constant\'.\n        cval (scalar): Values to fill outside boundary. Default: 0.\n        prefilter (bool): Apply spline_filter before interpolation. Default: True.\n    """"""\n\n    def __init__(\n        self,\n        angle,\n        spatial_axes=(0, 1),\n        reshape: bool = True,\n        interp_order=1,\n        mode: str = ""constant"",\n        cval: Union[int, float] = 0,\n        prefilter: bool = True,\n    ):\n        self.angle = angle\n        self.spatial_axes = spatial_axes\n        self.reshape = reshape\n        self.interp_order = interp_order\n        self.mode = mode\n        self.cval = cval\n        self.prefilter = prefilter\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img,\n        order=None,\n        mode: Optional[str] = None,\n        cval: Optional[Union[int, float]] = None,\n        prefilter: Optional[bool] = None,\n    ):\n        """"""\n        Args:\n            img (ndarray): channel first array, must have shape: (num_channels, H[, W, ..., ]),\n        """"""\n        rotated = list()\n        for channel in img:\n            rotated.append(\n                scipy.ndimage.rotate(\n                    input=channel,\n                    angle=self.angle,\n                    axes=self.spatial_axes,\n                    reshape=self.reshape,\n                    order=self.interp_order if order is None else order,\n                    mode=mode or self.mode,\n                    cval=self.cval if cval is None else cval,\n                    prefilter=self.prefilter if prefilter is None else prefilter,\n                )\n            )\n        return np.stack(rotated).astype(img.dtype)\n\n\nclass Zoom(Transform):\n    """""" Zooms a nd image. Uses scipy.ndimage.zoom or cupyx.scipy.ndimage.zoom in case of gpu.\n    For details, please see https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.zoom.html.\n\n    Args:\n        zoom (float or sequence): The zoom factor along the spatial axes.\n            If a float, zoom is the same for each spatial axis.\n            If a sequence, zoom should contain one value for each spatial axis.\n        interp_order: order of interpolation. Default=InterpolationCode.SPLINE3.\n        mode (str): Determines how input is extended beyond boundaries. Default is \'constant\'.\n        cval (scalar, optional): Value to fill past edges. Default is 0.\n        prefilter (bool): Apply spline_filter before interpolation. Default: True.\n        use_gpu (bool): Should use cpu or gpu. Uses cupyx which doesn\'t support order > 1 and modes\n            \'wrap\' and \'reflect\'. Defaults to cpu for these cases or if cupyx not found.\n        keep_size (bool): Should keep original size (pad if needed), default is True.\n    """"""\n\n    def __init__(\n        self,\n        zoom,\n        interp_order=InterpolationCode.SPLINE3,\n        mode: str = ""constant"",\n        cval: Union[int, float] = 0,\n        prefilter: bool = True,\n        use_gpu: bool = False,\n        keep_size: bool = True,\n    ):\n        self.zoom = zoom\n        self.interp_order = interp_order\n        self.mode = mode\n        self.cval = cval\n        self.prefilter = prefilter\n        self.use_gpu = use_gpu\n        self.keep_size = keep_size\n\n        if self.use_gpu:\n            try:\n                from cupyx.scipy.ndimage import zoom as zoom_gpu  # type: ignore\n\n                self._zoom = zoom_gpu\n            except ImportError:\n                print(""For GPU zoom, please install cupy. Defaulting to cpu."")\n                self._zoom = scipy.ndimage.zoom\n                self.use_gpu = False\n        else:\n            self._zoom = scipy.ndimage.zoom\n\n    def __call__(\n        self, img, order=None, mode=None, cval=None, prefilter=None,\n    ):\n        """"""\n        Args:\n            img (ndarray): channel first array, must have shape: (num_channels, H[, W, ..., ]),\n        """"""\n        zoomed = list()\n        if self.use_gpu:\n            import cupy  # type: ignore\n\n            for channel in cupy.array(img):\n                zoom_channel = self._zoom(\n                    channel,\n                    zoom=self.zoom,\n                    order=self.interp_order if order is None else order,\n                    mode=self.mode if mode is None else mode,\n                    cval=self.cval if cval is None else cval,\n                    prefilter=self.prefilter if prefilter is None else prefilter,\n                )\n                zoomed.append(cupy.asnumpy(zoom_channel))\n        else:\n            for channel in img:\n                zoomed.append(\n                    self._zoom(\n                        channel,\n                        zoom=self.zoom,\n                        order=self.interp_order if order is None else order,\n                        mode=mode or self.mode,\n                        cval=self.cval if cval is None else cval,\n                        prefilter=self.prefilter if prefilter is None else prefilter,\n                    )\n                )\n        zoomed = np.stack(zoomed).astype(img.dtype)\n\n        if not self.keep_size or np.allclose(img.shape, zoomed.shape):\n            return zoomed\n\n        pad_vec = [[0, 0]] * len(img.shape)\n        slice_vec = [slice(None)] * len(img.shape)\n        for idx, (od, zd) in enumerate(zip(img.shape, zoomed.shape)):\n            diff = od - zd\n            half = abs(diff) // 2\n            if diff > 0:  # need padding\n                pad_vec[idx] = [half, diff - half]\n            elif diff < 0:  # need slicing\n                slice_vec[idx] = slice(half, half + od)\n        zoomed = np.pad(zoomed, pad_vec, mode=""edge"")\n        return zoomed[tuple(slice_vec)]\n\n\nclass Rotate90(Transform):\n    """"""\n    Rotate an array by 90 degrees in the plane specified by `axes`.\n    """"""\n\n    def __init__(self, k: int = 1, spatial_axes=(0, 1)):\n        """"""\n        Args:\n            k: number of times to rotate by 90 degrees.\n            spatial_axes (2 ints): defines the plane to rotate with 2 spatial axes.\n                Default: (0, 1), this is the first two axis in spatial dimensions.\n        """"""\n        self.k = k\n        self.spatial_axes = spatial_axes\n\n    def __call__(self, img: np.ndarray):  # type: ignore # see issue #495\n        """"""\n        Args:\n            img (ndarray): channel first array, must have shape: (num_channels, H[, W, ..., ]),\n        """"""\n        rotated = list()\n        for channel in img:\n            rotated.append(np.rot90(channel, self.k, self.spatial_axes))\n        return np.stack(rotated).astype(img.dtype)\n\n\nclass RandRotate90(Randomizable, Transform):\n    """"""\n    With probability `prob`, input arrays are rotated by 90 degrees\n    in the plane specified by `spatial_axes`.\n    """"""\n\n    def __init__(self, prob: float = 0.1, max_k: int = 3, spatial_axes=(0, 1)):\n        """"""\n        Args:\n            prob (float): probability of rotating.\n                (Default 0.1, with 10% probability it returns a rotated array)\n            max_k: number of rotations will be sampled from `np.random.randint(max_k) + 1`.\n                (Default 3)\n            spatial_axes (2 ints): defines the plane to rotate with 2 spatial axes.\n                Default: (0, 1), this is the first two axis in spatial dimensions.\n        """"""\n        self.prob = min(max(prob, 0.0), 1.0)\n        self.max_k = max_k\n        self.spatial_axes = spatial_axes\n\n        self._do_transform = False\n        self._rand_k = 0\n\n    def randomize(self):\n        self._rand_k = self.R.randint(self.max_k) + 1\n        self._do_transform = self.R.random() < self.prob\n\n    def __call__(self, img):\n        self.randomize()\n        if not self._do_transform:\n            return img\n        rotator = Rotate90(self._rand_k, self.spatial_axes)\n        return rotator(img)\n\n\nclass RandRotate(Randomizable, Transform):\n    """"""Randomly rotates the input arrays.\n\n    Args:\n        degrees (tuple of float or float): Range of rotation in degrees. If single number,\n            angle is picked from (-degrees, degrees).\n        prob (float): Probability of rotation.\n        spatial_axes (tuple of 2 ints): Spatial axes of rotation. Default: (0, 1).\n            This is the first two axis in spatial dimensions.\n        reshape (bool): If reshape is true, the output shape is adapted so that the\n            input array is contained completely in the output. Default is True.\n        interp_order: Order of spline interpolation. Range 0-5. Default: InterpolationCode.LINEAR. This is\n            different from scipy where default interpolation is InterpolationCode.SPLINE3.\n        mode (str): Points outside boundary filled according to this mode. Options are\n            \'constant\', \'nearest\', \'reflect\', \'wrap\'. Default: \'constant\'.\n        cval (scalar): Value to fill outside boundary. Default: 0.\n        prefilter (bool): Apply spline_filter before interpolation. Default: True.\n    """"""\n\n    def __init__(\n        self,\n        degrees,\n        prob: float = 0.1,\n        spatial_axes=(0, 1),\n        reshape: bool = True,\n        interp_order=InterpolationCode.LINEAR,\n        mode: str = ""constant"",\n        cval: Union[int, float] = 0,\n        prefilter: bool = True,\n    ):\n        self.degrees = degrees\n        self.prob = prob\n        self.spatial_axes = spatial_axes\n        self.reshape = reshape\n        self.interp_order = interp_order\n        self.mode = mode\n        self.cval = cval\n        self.prefilter = prefilter\n\n        if not hasattr(self.degrees, ""__iter__""):\n            self.degrees = (-self.degrees, self.degrees)\n        assert len(self.degrees) == 2, ""degrees should be a number or pair of numbers.""\n\n        self._do_transform = False\n        self.angle = None\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n        self.angle = self.R.uniform(low=self.degrees[0], high=self.degrees[1])\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img,\n        order=None,\n        mode: Optional[str] = None,\n        cval: Optional[Union[int, float]] = None,\n        prefilter: Optional[bool] = None,\n    ):\n        self.randomize()\n        if not self._do_transform:\n            return img\n        rotator = Rotate(\n            angle=self.angle,\n            spatial_axes=self.spatial_axes,\n            reshape=self.reshape,\n            interp_order=self.interp_order if order is None else order,\n            mode=mode or self.mode,\n            cval=self.cval if cval is None else cval,\n            prefilter=self.prefilter if prefilter is None else prefilter,\n        )\n        return rotator(img)\n\n\nclass RandFlip(Randomizable, Transform):\n    """"""Randomly flips the image along axes. Preserves shape.\n    See numpy.flip for additional details.\n    https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html\n\n    Args:\n        prob (float): Probability of flipping.\n        spatial_axis (None, int or tuple of ints): Spatial axes along which to flip over. Default is None.\n    """"""\n\n    def __init__(self, prob: float = 0.1, spatial_axis=None):\n        self.prob = prob\n        self.flipper = Flip(spatial_axis=spatial_axis)\n        self._do_transform = False\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n\n    def __call__(self, img):\n        self.randomize()\n        if not self._do_transform:\n            return img\n        return self.flipper(img)\n\n\nclass RandZoom(Randomizable, Transform):\n    """"""Randomly zooms input arrays with given probability within given zoom range.\n\n    Args:\n        prob (float): Probability of zooming.\n        min_zoom (float or sequence): Min zoom factor. Can be float or sequence same size as image.\n            If a float, min_zoom is the same for each spatial axis.\n            If a sequence, min_zoom should contain one value for each spatial axis.\n        max_zoom (float or sequence): Max zoom factor. Can be float or sequence same size as image.\n            If a float, max_zoom is the same for each spatial axis.\n            If a sequence, max_zoom should contain one value for each spatial axis.\n        interp_order: order of interpolation. Default=InterpolationCode.SPLINE3.\n        mode (\'reflect\', \'constant\', \'nearest\', \'mirror\', \'wrap\'): Determines how input is\n            extended beyond boundaries. Default: \'constant\'.\n        cval (scalar, optional): Value to fill past edges. Default is 0.\n        prefilter (bool): Apply spline_filter before interpolation. Default: True.\n        use_gpu (bool): Should use cpu or gpu. Uses cupyx which doesn\'t support order > 1 and modes\n            \'wrap\' and \'reflect\'. Defaults to cpu for these cases or if cupyx not found.\n        keep_size (bool): Should keep original size (pad if needed), default is True.\n    """"""\n\n    def __init__(\n        self,\n        prob: float = 0.1,\n        min_zoom=0.9,\n        max_zoom=1.1,\n        interp_order=InterpolationCode.SPLINE3,\n        mode: str = ""constant"",\n        cval: Union[int, float] = 0,\n        prefilter: bool = True,\n        use_gpu: bool = False,\n        keep_size: bool = True,\n    ):\n        if hasattr(min_zoom, ""__iter__"") and hasattr(max_zoom, ""__iter__""):\n            assert len(min_zoom) == len(max_zoom), ""min_zoom and max_zoom must have same length.""\n        self.min_zoom = min_zoom\n        self.max_zoom = max_zoom\n        self.prob = prob\n        self.use_gpu = use_gpu\n        self.keep_size = keep_size\n\n        self.interp_order = interp_order\n        self.mode = mode\n        self.cval = cval\n        self.prefilter = prefilter\n\n        self._do_transform = False\n        self._zoom = None\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n        if hasattr(self.min_zoom, ""__iter__""):\n            self._zoom = (self.R.uniform(l, h) for l, h in zip(self.min_zoom, self.max_zoom))\n        else:\n            self._zoom = self.R.uniform(self.min_zoom, self.max_zoom)\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img,\n        order=None,\n        mode: Optional[str] = None,\n        cval: Union[int, float] = None,\n        prefilter: Optional[bool] = None,\n    ):\n        self.randomize()\n        if not self._do_transform:\n            return img\n        zoomer = Zoom(self._zoom, use_gpu=self.use_gpu, keep_size=self.keep_size)\n        return zoomer(\n            img,\n            order=self.interp_order if order is None else order,\n            mode=mode or self.mode,\n            cval=self.cval if cval is None else cval,\n            prefilter=self.prefilter if prefilter is None else prefilter,\n        )\n\n\nclass AffineGrid(Transform):\n    """"""\n    Affine transforms on the coordinates.\n    """"""\n\n    def __init__(\n        self,\n        rotate_params=None,\n        shear_params=None,\n        translate_params=None,\n        scale_params=None,\n        as_tensor_output: bool = True,\n        device: Optional[torch.device] = None,\n    ):\n        self.rotate_params = rotate_params\n        self.shear_params = shear_params\n        self.translate_params = translate_params\n        self.scale_params = scale_params\n\n        self.as_tensor_output = as_tensor_output\n        self.device = device\n\n    def __call__(self, spatial_size=None, grid=None):\n        """"""\n        Args:\n            spatial_size (list or tuple of int): output grid size.\n            grid (ndarray): grid to be transformed. Shape must be (3, H, W) for 2D or (4, H, W, D) for 3D.\n        """"""\n        if grid is None:\n            if spatial_size is not None:\n                grid = create_grid(spatial_size)\n            else:\n                raise ValueError(""Either specify a grid or a spatial size to create a grid from."")\n\n        spatial_dims = len(grid.shape) - 1\n        affine = np.eye(spatial_dims + 1)\n        if self.rotate_params:\n            affine = affine @ create_rotate(spatial_dims, self.rotate_params)\n        if self.shear_params:\n            affine = affine @ create_shear(spatial_dims, self.shear_params)\n        if self.translate_params:\n            affine = affine @ create_translate(spatial_dims, self.translate_params)\n        if self.scale_params:\n            affine = affine @ create_scale(spatial_dims, self.scale_params)\n        affine = torch.as_tensor(np.ascontiguousarray(affine), device=self.device)\n\n        grid = torch.tensor(grid) if not torch.is_tensor(grid) else grid.detach().clone()\n        if self.device:\n            grid = grid.to(self.device)\n        grid = (affine.float() @ grid.reshape((grid.shape[0], -1)).float()).reshape([-1] + list(grid.shape[1:]))\n        if self.as_tensor_output:\n            return grid\n        return grid.cpu().numpy()\n\n\nclass RandAffineGrid(Randomizable, Transform):\n    """"""\n    generate randomised affine grid\n    """"""\n\n    def __init__(\n        self,\n        rotate_range=None,\n        shear_range=None,\n        translate_range=None,\n        scale_range=None,\n        as_tensor_output: bool = True,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        Args:\n            rotate_range (a sequence of positive floats): rotate_range[0] with be used to generate the 1st rotation\n                parameter from `uniform[-rotate_range[0], rotate_range[0])`. Similarly, `rotate_range[2]` and\n                `rotate_range[3]` are used in 3D affine for the range of 2nd and 3rd axes.\n            shear_range (a sequence of positive floats): shear_range[0] with be used to generate the 1st shearing\n                parameter from `uniform[-shear_range[0], shear_range[0])`. Similarly, `shear_range[1]` to\n                `shear_range[N]` controls the range of the uniform distribution used to generate the 2nd to\n                N-th parameter.\n            translate_range (a sequence of positive floats): translate_range[0] with be used to generate the 1st\n                shift parameter from `uniform[-translate_range[0], translate_range[0])`. Similarly, `translate_range[1]`\n                to `translate_range[N]` controls the range of the uniform distribution used to generate\n                the 2nd to N-th parameter.\n            scale_range (a sequence of positive floats): scaling_range[0] with be used to generate the 1st scaling\n                factor from `uniform[-scale_range[0], scale_range[0]) + 1.0`. Similarly, `scale_range[1]` to\n                `scale_range[N]` controls the range of the uniform distribution used to generate the 2nd to\n                N-th parameter.\n\n        See also:\n            - :py:meth:`monai.transforms.utils.create_rotate`\n            - :py:meth:`monai.transforms.utils.create_shear`\n            - :py:meth:`monai.transforms.utils.create_translate`\n            - :py:meth:`monai.transforms.utils.create_scale`\n        """"""\n        self.rotate_range = ensure_tuple(rotate_range)\n        self.shear_range = ensure_tuple(shear_range)\n        self.translate_range = ensure_tuple(translate_range)\n        self.scale_range = ensure_tuple(scale_range)\n\n        self.rotate_params = None\n        self.shear_params = None\n        self.translate_params = None\n        self.scale_params = None\n\n        self.as_tensor_output = as_tensor_output\n        self.device = device\n\n    def randomize(self):\n        if self.rotate_range:\n            self.rotate_params = [self.R.uniform(-f, f) for f in self.rotate_range if f is not None]\n        if self.shear_range:\n            self.shear_params = [self.R.uniform(-f, f) for f in self.shear_range if f is not None]\n        if self.translate_range:\n            self.translate_params = [self.R.uniform(-f, f) for f in self.translate_range if f is not None]\n        if self.scale_range:\n            self.scale_params = [self.R.uniform(-f, f) + 1.0 for f in self.scale_range if f is not None]\n\n    def __call__(self, spatial_size=None, grid=None):\n        """"""\n        Returns:\n            a 2D (3xHxW) or 3D (4xHxWxD) grid.\n        """"""\n        self.randomize()\n        affine_grid = AffineGrid(\n            rotate_params=self.rotate_params,\n            shear_params=self.shear_params,\n            translate_params=self.translate_params,\n            scale_params=self.scale_params,\n            as_tensor_output=self.as_tensor_output,\n            device=self.device,\n        )\n        return affine_grid(spatial_size, grid)\n\n\nclass RandDeformGrid(Randomizable, Transform):\n    """"""\n    generate random deformation grid\n    """"""\n\n    def __init__(self, spacing, magnitude_range, as_tensor_output: bool = True, device: Optional[torch.device] = None):\n        """"""\n        Args:\n            spacing (2 or 3 ints): spacing of the grid in 2D or 3D.\n                e.g., spacing=(1, 1) indicates pixel-wise deformation in 2D,\n                spacing=(1, 1, 1) indicates voxel-wise deformation in 3D,\n                spacing=(2, 2) indicates deformation field defined on every other pixel in 2D.\n            magnitude_range (2 ints): the random offsets will be generated from\n                `uniform[magnitude[0], magnitude[1])`.\n            as_tensor_output (bool): whether to output tensor instead of numpy array.\n                defaults to True.\n            device (torch device): device to store the output grid data.\n        """"""\n        self.spacing = spacing\n        self.magnitude = magnitude_range\n\n        self.rand_mag = 1.0\n        self.as_tensor_output = as_tensor_output\n        self.random_offset = 0.0\n        self.device = device\n\n    def randomize(self, grid_size):\n        self.random_offset = self.R.normal(size=([len(grid_size)] + list(grid_size))).astype(np.float32)\n        self.rand_mag = self.R.uniform(self.magnitude[0], self.magnitude[1])\n\n    def __call__(self, spatial_size):\n        control_grid = create_control_grid(spatial_size, self.spacing)\n        self.randomize(control_grid.shape[1:])\n        control_grid[: len(spatial_size)] += self.rand_mag * self.random_offset\n        if self.as_tensor_output:\n            control_grid = torch.as_tensor(np.ascontiguousarray(control_grid), device=self.device)\n        return control_grid\n\n\nclass Resample(Transform):\n    def __init__(\n        self,\n        padding_mode: str = ""zeros"",\n        mode: str = ""bilinear"",\n        as_tensor_output: bool = False,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        computes output image using values from `img`, locations from `grid` using pytorch.\n        supports spatially 2D or 3D (num_channels, H, W[, D]).\n\n        Args:\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices. Defaults to \'zeros\'.\n            as_tensor_output(bool): whether to return a torch tensor. Defaults to False.\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to \'bilinear\'.\n            device (torch.device): device on which the tensor will be allocated.\n        """"""\n        self.padding_mode = padding_mode\n        self.mode = mode\n        self.as_tensor_output = as_tensor_output\n        self.device = device\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img: Union[np.ndarray, torch.Tensor],\n        grid: Union[np.ndarray, torch.Tensor],\n        padding_mode: Optional[str] = None,\n        mode: Optional[str] = None,\n    ):\n        """"""\n        Args:\n            img (ndarray or tensor): shape must be (num_channels, H, W[, D]).\n            grid (ndarray or tensor): shape must be (3, H, W) for 2D or (4, H, W, D) for 3D.\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to \'bilinear\'.\n        """"""\n        if not torch.is_tensor(img):\n            img = torch.as_tensor(np.ascontiguousarray(img))\n        grid = torch.tensor(grid) if not torch.is_tensor(grid) else grid.detach().clone()\n        if self.device:\n            img = img.to(self.device)\n            grid = grid.to(self.device)\n\n        for i, dim in enumerate(img.shape[1:]):\n            grid[i] = 2.0 * grid[i] / (dim - 1.0)\n        grid = grid[:-1] / grid[-1:]\n        index_ordering: List[int] = [x for x in range(img.ndim - 2, -1, -1)]\n        grid = grid[index_ordering]\n        grid = grid.permute(list(range(grid.ndim))[1:] + [0])\n        out = torch.nn.functional.grid_sample(\n            img[None].float(),\n            grid[None].float(),\n            mode=mode or self.mode,\n            padding_mode=padding_mode or self.padding_mode,\n            align_corners=False,\n        )[0]\n        if self.as_tensor_output:\n            return out\n        return out.cpu().numpy()\n\n\nclass Affine(Transform):\n    """"""\n    transform ``img`` given the affine parameters.\n    """"""\n\n    def __init__(\n        self,\n        rotate_params=None,\n        shear_params=None,\n        translate_params=None,\n        scale_params=None,\n        spatial_size=None,\n        mode: str = ""bilinear"",\n        padding_mode: str = ""zeros"",\n        as_tensor_output: bool = False,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        The affine transformations are applied in rotate, shear, translate, scale order.\n\n        Args:\n            rotate_params (float, list of floats): a rotation angle in radians,\n                a scalar for 2D image, a tuple of 3 floats for 3D. Defaults to no rotation.\n            shear_params (list of floats):\n                a tuple of 2 floats for 2D, a tuple of 6 floats for 3D. Defaults to no shearing.\n            translate_params (list of floats):\n                a tuple of 2 floats for 2D, a tuple of 3 floats for 3D. Translation is in pixel/voxel\n                relative to the center of the input image. Defaults to no translation.\n            scale_params (list of floats):\n                a tuple of 2 floats for 2D, a tuple of 3 floats for 3D. Defaults to no scaling.\n            spatial_size (list or tuple of int): output image spatial size.\n                if `img` has two spatial dimensions, `spatial_size` should have 2 elements [h, w].\n                if `img` has three spatial dimensions, `spatial_size` should have 3 elements [h, w, d].\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to \'bilinear\'.\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices. Defaults to \'zeros\'.\n            as_tensor_output (bool): the computation is implemented using pytorch tensors, this option specifies\n                whether to convert it back to numpy arrays.\n            device (torch.device): device on which the tensor will be allocated.\n        """"""\n        self.affine_grid = AffineGrid(\n            rotate_params=rotate_params,\n            shear_params=shear_params,\n            translate_params=translate_params,\n            scale_params=scale_params,\n            as_tensor_output=True,\n            device=device,\n        )\n        self.resampler = Resample(as_tensor_output=as_tensor_output, device=device)\n        self.spatial_size = spatial_size\n        self.padding_mode = padding_mode\n        self.mode = mode\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img: Union[np.ndarray, torch.Tensor],\n        spatial_size=None,\n        padding_mode: Optional[str] = None,\n        mode: Optional[str] = None,\n    ):\n        """"""\n        Args:\n            img (ndarray or tensor): shape must be (num_channels, H, W[, D]),\n            spatial_size (list or tuple of int): output image spatial size.\n                if `img` has two spatial dimensions, `spatial_size` should have 2 elements [h, w].\n                if `img` has three spatial dimensions, `spatial_size` should have 3 elements [h, w, d].\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices. Defaults to \'zeros\'.\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to \'bilinear\'.\n        """"""\n        grid = self.affine_grid(spatial_size=spatial_size or self.spatial_size)\n        return self.resampler(\n            img=img, grid=grid, padding_mode=padding_mode or self.padding_mode, mode=mode or self.mode\n        )\n\n\nclass RandAffine(Randomizable, Transform):\n    """"""\n    Random affine transform.\n    """"""\n\n    def __init__(\n        self,\n        prob: float = 0.1,\n        rotate_range=None,\n        shear_range=None,\n        translate_range=None,\n        scale_range=None,\n        spatial_size=None,\n        mode: str = ""bilinear"",\n        padding_mode: str = ""zeros"",\n        as_tensor_output: bool = True,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        Args:\n            prob (float): probability of returning a randomized affine grid.\n                defaults to 0.1, with 10% chance returns a randomized grid.\n            spatial_size (list or tuple of int): output image spatial size.\n                if `img` has two spatial dimensions, `spatial_size` should have 2 elements [h, w].\n                if `img` has three spatial dimensions, `spatial_size` should have 3 elements [h, w, d].\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to \'bilinear\'.\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices. Defaults to \'zeros\'.\n            as_tensor_output (bool): the computation is implemented using pytorch tensors, this option specifies\n                whether to convert it back to numpy arrays.\n            device (torch.device): device on which the tensor will be allocated.\n\n        See also:\n            - :py:class:`RandAffineGrid` for the random affine parameters configurations.\n            - :py:class:`Affine` for the affine transformation parameters configurations.\n        """"""\n\n        self.rand_affine_grid = RandAffineGrid(\n            rotate_range=rotate_range,\n            shear_range=shear_range,\n            translate_range=translate_range,\n            scale_range=scale_range,\n            as_tensor_output=True,\n            device=device,\n        )\n        self.resampler = Resample(as_tensor_output=as_tensor_output, device=device)\n\n        self.spatial_size = spatial_size\n        self.padding_mode = padding_mode\n        self.mode = mode\n\n        self.do_transform = False\n        self.prob = prob\n\n    def set_random_state(self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None):\n        self.rand_affine_grid.set_random_state(seed, state)\n        super().set_random_state(seed, state)\n        return self\n\n    def randomize(self):\n        self.do_transform = self.R.rand() < self.prob\n        self.rand_affine_grid.randomize()\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img: Union[np.ndarray, torch.Tensor],\n        spatial_size=None,\n        padding_mode: Optional[str] = None,\n        mode: Optional[str] = None,\n    ):\n        """"""\n        Args:\n            img (ndarray or tensor): shape must be (num_channels, H, W[, D]),\n            spatial_size (list or tuple of int): output image spatial size.\n                if `img` has two spatial dimensions, `spatial_size` should have 2 elements [h, w].\n                if `img` has three spatial dimensions, `spatial_size` should have 3 elements [h, w, d].\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices. Defaults to \'zeros\'.\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to \'bilinear\'.\n        """"""\n        self.randomize()\n        _spatial_size = spatial_size or self.spatial_size\n        if self.do_transform:\n            grid = self.rand_affine_grid(spatial_size=_spatial_size)\n        else:\n            grid = create_grid(_spatial_size)\n        return self.resampler(\n            img=img, grid=grid, padding_mode=padding_mode or self.padding_mode, mode=mode or self.mode\n        )\n\n\nclass Rand2DElastic(Randomizable, Transform):\n    """"""\n    Random elastic deformation and affine in 2D\n    """"""\n\n    def __init__(\n        self,\n        spacing,\n        magnitude_range,\n        prob: float = 0.1,\n        rotate_range=None,\n        shear_range=None,\n        translate_range=None,\n        scale_range=None,\n        spatial_size=None,\n        mode: str = ""bilinear"",\n        padding_mode: str = ""zeros"",\n        as_tensor_output: bool = False,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        Args:\n            spacing (2 ints): distance in between the control points.\n            magnitude_range (2 ints): the random offsets will be generated from\n                ``uniform[magnitude[0], magnitude[1])``.\n            prob (float): probability of returning a randomized affine grid.\n                defaults to 0.1, with 10% chance returns a randomized grid,\n                otherwise returns a ``spatial_size`` centered area extracted from the input image.\n            spatial_size (2 ints): specifying output image spatial size [h, w].\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to ``\'bilinear\'``.\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices.\n                Defaults to ``\'zeros\'``.\n            as_tensor_output (bool): the computation is implemented using pytorch tensors, this option specifies\n                whether to convert it back to numpy arrays.\n            device (torch.device): device on which the tensor will be allocated.\n\n        See also:\n            - :py:class:`RandAffineGrid` for the random affine parameters configurations.\n            - :py:class:`Affine` for the affine transformation parameters configurations.\n        """"""\n        self.deform_grid = RandDeformGrid(\n            spacing=spacing, magnitude_range=magnitude_range, as_tensor_output=True, device=device\n        )\n        self.rand_affine_grid = RandAffineGrid(\n            rotate_range=rotate_range,\n            shear_range=shear_range,\n            translate_range=translate_range,\n            scale_range=scale_range,\n            as_tensor_output=True,\n            device=device,\n        )\n        self.resampler = Resample(as_tensor_output=as_tensor_output, device=device)\n\n        self.spatial_size = spatial_size\n        self.padding_mode = padding_mode\n        self.mode = mode\n        self.prob = prob\n        self.do_transform = False\n\n    def set_random_state(self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None):\n        self.deform_grid.set_random_state(seed, state)\n        self.rand_affine_grid.set_random_state(seed, state)\n        super().set_random_state(seed, state)\n        return self\n\n    def randomize(self, spatial_size):\n        self.do_transform = self.R.rand() < self.prob\n        self.deform_grid.randomize(spatial_size)\n        self.rand_affine_grid.randomize()\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img: Union[np.ndarray, torch.Tensor],\n        spatial_size=None,\n        padding_mode: Optional[str] = None,\n        mode: Optional[str] = None,\n    ):\n        """"""\n        Args:\n            img (ndarray or tensor): shape must be (num_channels, H, W),\n            spatial_size (2 ints): specifying output image spatial size [h, w].\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices.\n                Defaults to ``\'zeros\'``.\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to ``self.mode``.\n        """"""\n        spatial_size = spatial_size or self.spatial_size\n        self.randomize(spatial_size)\n        if self.do_transform:\n            grid = self.deform_grid(spatial_size=spatial_size)\n            grid = self.rand_affine_grid(grid=grid)\n            grid = torch.nn.functional.interpolate(grid[None], spatial_size, mode=""bicubic"", align_corners=False)[0]\n        else:\n            grid = create_grid(spatial_size)\n        return self.resampler(img, grid, padding_mode=padding_mode or self.padding_mode, mode=mode or self.mode)\n\n\nclass Rand3DElastic(Randomizable, Transform):\n    """"""\n    Random elastic deformation and affine in 3D\n    """"""\n\n    def __init__(\n        self,\n        sigma_range,\n        magnitude_range,\n        prob: float = 0.1,\n        rotate_range=None,\n        shear_range=None,\n        translate_range=None,\n        scale_range=None,\n        spatial_size=None,\n        mode: str = ""bilinear"",\n        padding_mode: str = ""zeros"",\n        as_tensor_output: bool = False,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        Args:\n            sigma_range (2 ints): a Gaussian kernel with standard deviation sampled\n                 from ``uniform[sigma_range[0], sigma_range[1])`` will be used to smooth the random offset grid.\n            magnitude_range (2 ints): the random offsets on the grid will be generated from\n                ``uniform[magnitude[0], magnitude[1])``.\n            prob (float): probability of returning a randomized affine grid.\n                defaults to 0.1, with 10% chance returns a randomized grid,\n                otherwise returns a ``spatial_size`` centered area extracted from the input image.\n            spatial_size (3 ints): specifying output image spatial size [h, w, d].\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to ``\'bilinear\'``.\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices.\n                Defaults to ``\'zeros\'``.\n            as_tensor_output (bool): the computation is implemented using pytorch tensors, this option specifies\n                whether to convert it back to numpy arrays.\n            device (torch.device): device on which the tensor will be allocated.\n\n        See also:\n            - :py:class:`RandAffineGrid` for the random affine parameters configurations.\n            - :py:class:`Affine` for the affine transformation parameters configurations.\n        """"""\n        self.rand_affine_grid = RandAffineGrid(rotate_range, shear_range, translate_range, scale_range, True, device)\n        self.resampler = Resample(as_tensor_output=as_tensor_output, device=device)\n\n        self.sigma_range = sigma_range\n        self.magnitude_range = magnitude_range\n        self.spatial_size = spatial_size\n        self.padding_mode = padding_mode\n        self.mode = mode\n        self.device = device\n\n        self.prob = prob\n        self.do_transform = False\n        self.rand_offset = None\n        self.magnitude = 1.0\n        self.sigma = 1.0\n\n    def set_random_state(self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None):\n        self.rand_affine_grid.set_random_state(seed, state)\n        super().set_random_state(seed, state)\n        return self\n\n    def randomize(self, grid_size):\n        self.do_transform = self.R.rand() < self.prob\n        if self.do_transform:\n            self.rand_offset = self.R.uniform(-1.0, 1.0, [3] + list(grid_size)).astype(np.float32)\n        self.magnitude = self.R.uniform(self.magnitude_range[0], self.magnitude_range[1])\n        self.sigma = self.R.uniform(self.sigma_range[0], self.sigma_range[1])\n        self.rand_affine_grid.randomize()\n\n    def __call__(\n        self, img, spatial_size=None, padding_mode=None, mode=None,\n    ):\n        """"""\n        Args:\n            img (ndarray or tensor): shape must be (num_channels, H, W, D),\n            spatial_size (3 ints): specifying spatial 3D output image spatial size [h, w, d].\n            padding_mode (\'zeros\'|\'border\'|\'reflection\'): mode of handling out of range indices.\n                Defaults to ``\'zeros\'``.\n            mode (\'nearest\'|\'bilinear\'): interpolation order. Defaults to \'self.mode\'.\n        """"""\n        spatial_size = spatial_size or self.spatial_size\n        self.randomize(spatial_size)\n        grid = create_grid(spatial_size)\n        if self.do_transform:\n            grid = torch.as_tensor(np.ascontiguousarray(grid), device=self.device)\n            gaussian = GaussianFilter(3, self.sigma, 3.0).to(device=self.device)\n            offset = torch.as_tensor(self.rand_offset[None], device=self.device)\n            grid[:3] += gaussian(offset)[0] * self.magnitude\n            grid = self.rand_affine_grid(grid=grid)\n        return self.resampler(img, grid, padding_mode=self.padding_mode, mode=mode or self.mode)\n'"
monai/transforms/spatial/dictionary.py,9,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of dictionary-based wrappers around the ""vanilla"" transforms for spatial operations\ndefined in :py:class:`monai.transforms.spatial.array`.\n\nClass names are ended with \'d\' to denote dictionary-based transforms.\n""""""\n\nfrom typing import Hashable, Optional\n\nimport numpy as np\nimport torch\nfrom monai.data.utils import InterpolationCode\n\nfrom monai.networks.layers.simplelayers import GaussianFilter\nfrom monai.transforms.compose import MapTransform, Randomizable\nfrom monai.transforms.spatial.array import (\n    Flip,\n    Orientation,\n    Rand2DElastic,\n    Rand3DElastic,\n    RandAffine,\n    Resize,\n    Rotate,\n    Rotate90,\n    Spacing,\n    Zoom,\n)\nfrom monai.transforms.utils import create_grid\nfrom monai.utils.misc import ensure_tuple_rep\n\n\nclass Spacingd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.Spacing`.\n\n    This transform assumes the ``data`` dictionary has a key for the input\n    data\'s affine.  The key is formed by ``meta_key_format.format(key, \'affine\')``.\n\n    After resampling the input array, this transform will write the new affine\n     to the key formed by ``meta_key_format.format(key, \'affine\')``.\n\n    see also:\n        :py:class:`monai.transforms.Spacing`\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        pixdim,\n        diagonal: bool = False,\n        interp_order: str = ""bilinear"",\n        mode: str = ""border"",\n        dtype: Optional[np.dtype] = None,\n        meta_key_format: str = ""{}.{}"",\n    ):\n        """"""\n        Args:\n            pixdim (sequence of floats): output voxel spacing.\n            diagonal (bool): whether to resample the input to have a diagonal affine matrix.\n                If True, the input data is resampled to the following affine::\n\n                    np.diag((pixdim_0, pixdim_1, pixdim_2, 1))\n\n                This effectively resets the volume to the world coordinate system (RAS+ in nibabel).\n                The original orientation, rotation, shearing are not preserved.\n\n                If False, the axes orientation, orthogonal rotation and\n                translations components from the original affine will be\n                preserved in the target affine. This option will not flip/swap\n                axes against the original ones.\n            interp_order (`nearest|bilinear` or a squence of str): str: the same interpolation order\n                for all data indexed by `self.keys`; sequence of str, should\n                correspond to an interpolation order for each data item indexed\n                by `self.keys` respectively. Defaults to `bilinear`.\n            mode (str or sequence of str):\n                Available options are `zeros|border|reflection`.\n                The mode parameter determines how the input array is extended beyond its boundaries.\n                Default is \'border\'.\n            dtype (None or np.dtype or sequence of np.dtype): output array data type.\n                Defaults to None to use input data\'s dtype.\n            meta_key_format (str): key format to read/write affine matrices to the data dictionary.\n        """"""\n        super().__init__(keys)\n        self.spacing_transform = Spacing(pixdim, diagonal=diagonal)\n        self.interp_order = ensure_tuple_rep(interp_order, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n        self.dtype = ensure_tuple_rep(dtype, len(self.keys))\n        self.meta_key_format = meta_key_format\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            affine_key = self.meta_key_format.format(key, ""affine"")\n            # resample array of each corresponding key\n            # using affine fetched from d[affine_key]\n            d[key], _, new_affine = self.spacing_transform(\n                data_array=d[key],\n                affine=d[affine_key],\n                interp_order=self.interp_order[idx],\n                mode=self.mode[idx],\n                dtype=self.dtype[idx],\n            )\n            # set the \'affine\' key\n            d[affine_key] = new_affine\n        return d\n\n\nclass Orientationd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.Orientation`.\n\n    This transform assumes the ``data`` dictionary has a key for the input\n    data\'s affine.  The key is formed by ``meta_key_format.format(key, \'affine\')``.\n\n    After reorienting the input array, this transform will write the new affine\n     to the key formed by ``meta_key_format.format(key, \'affine\')``.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        axcodes=None,\n        as_closest_canonical: bool = False,\n        labels=tuple(zip(""LPI"", ""RAS"")),\n        meta_key_format: str = ""{}.{}"",\n    ):\n        """"""\n        Args:\n            axcodes (N elements sequence): for spatial ND input\'s orientation.\n                e.g. axcodes=\'RAS\' represents 3D orientation:\n                (Left, Right), (Posterior, Anterior), (Inferior, Superior).\n                default orientation labels options are: \'L\' and \'R\' for the first dimension,\n                \'P\' and \'A\' for the second, \'I\' and \'S\' for the third.\n            as_closest_canonical (boo): if True, load the image as closest to canonical axis format.\n            labels : optional, None or sequence of (2,) sequences\n                (2,) sequences are labels for (beginning, end) of output axis.\n                Defaults to ``((\'L\', \'R\'), (\'P\', \'A\'), (\'I\', \'S\'))``.\n            meta_key_format (str): key format to read/write affine matrices to the data dictionary.\n\n        See Also:\n            `nibabel.orientations.ornt2axcodes`.\n        """"""\n        super().__init__(keys)\n        self.ornt_transform = Orientation(axcodes=axcodes, as_closest_canonical=as_closest_canonical, labels=labels)\n        self.meta_key_format = meta_key_format\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            affine_key = self.meta_key_format.format(key, ""affine"")\n            d[key], _, new_affine = self.ornt_transform(d[key], affine=d[affine_key])\n            d[affine_key] = new_affine\n        return d\n\n\nclass Rotate90d(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.Rotate90`.\n    """"""\n\n    def __init__(self, keys: Hashable, k: int = 1, spatial_axes=(0, 1)):\n        """"""\n        Args:\n            k: number of times to rotate by 90 degrees.\n            spatial_axes (2 ints): defines the plane to rotate with 2 spatial axes.\n                Default: (0, 1), this is the first two axis in spatial dimensions.\n        """"""\n        super().__init__(keys)\n        self.rotator = Rotate90(k, spatial_axes)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.rotator(d[key])\n        return d\n\n\nclass RandRotate90d(Randomizable, MapTransform):\n    """"""Dictionary-based version :py:class:`monai.transforms.RandRotate90`.\n    With probability `prob`, input arrays are rotated by 90 degrees\n    in the plane specified by `spatial_axes`.\n    """"""\n\n    def __init__(self, keys: Hashable, prob: float = 0.1, max_k: int = 3, spatial_axes=(0, 1)):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            prob (float): probability of rotating.\n                (Default 0.1, with 10% probability it returns a rotated array.)\n            max_k: number of rotations will be sampled from `np.random.randint(max_k) + 1`.\n                (Default 3)\n            spatial_axes (2 ints): defines the plane to rotate with 2 spatial axes.\n                Default: (0, 1), this is the first two axis in spatial dimensions.\n        """"""\n        super().__init__(keys)\n\n        self.prob = min(max(prob, 0.0), 1.0)\n        self.max_k = max_k\n        self.spatial_axes = spatial_axes\n\n        self._do_transform = False\n        self._rand_k = 0\n\n    def randomize(self):\n        self._rand_k = self.R.randint(self.max_k) + 1\n        self._do_transform = self.R.random() < self.prob\n\n    def __call__(self, data):\n        self.randomize()\n        if not self._do_transform:\n            return data\n\n        rotator = Rotate90(self._rand_k, self.spatial_axes)\n        d = dict(data)\n        for key in self.keys:\n            d[key] = rotator(d[key])\n        return d\n\n\nclass Resized(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.Resize`.\n\n    Args:\n        keys (hashable items): keys of the corresponding items to be transformed.\n            See also: :py:class:`monai.transforms.compose.MapTransform`\n        spatial_size (tuple or list): expected shape of spatial dimensions after resize operation.\n        interp_order (int or sequence of int): Order of spline interpolation. Default=InterpolationCode.LINEAR.\n        mode (str or sequence of str): Points outside boundaries are filled according to given mode.\n            Options are \'constant\', \'edge\', \'symmetric\', \'reflect\', \'wrap\'.\n        cval (float or sequence of float): Used with mode \'constant\', the value outside image boundaries.\n        clip (bool or sequence of bool): Whether to clip range of output values after interpolation. Default: True.\n        preserve_range (bool or sequence of bool): Whether to keep original range of values. Default is True.\n            If False, input is converted according to conventions of img_as_float. See\n            https://scikit-image.org/docs/dev/user_guide/data_types.html.\n        anti_aliasing (bool or sequence of bool): Whether to apply a gaussian filter to image before down-scaling.\n            Default is True.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        spatial_size,\n        interp_order=1,\n        mode=""reflect"",\n        cval=0,\n        clip=True,\n        preserve_range=True,\n        anti_aliasing=True,\n    ):\n        super().__init__(keys)\n        self.interp_order = ensure_tuple_rep(interp_order, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n        self.cval = ensure_tuple_rep(cval, len(self.keys))\n        self.clip = ensure_tuple_rep(clip, len(self.keys))\n        self.preserve_range = ensure_tuple_rep(preserve_range, len(self.keys))\n        self.anti_aliasing = ensure_tuple_rep(anti_aliasing, len(self.keys))\n\n        self.resizer = Resize(spatial_size=spatial_size)\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            d[key] = self.resizer(\n                d[key],\n                order=self.interp_order[idx],\n                mode=self.mode[idx],\n                cval=self.cval[idx],\n                clip=self.clip[idx],\n                preserve_range=self.preserve_range[idx],\n                anti_aliasing=self.anti_aliasing[idx],\n            )\n        return d\n\n\nclass RandAffined(Randomizable, MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.RandAffine`.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        spatial_size,\n        prob: float = 0.1,\n        rotate_range=None,\n        shear_range=None,\n        translate_range=None,\n        scale_range=None,\n        mode=""bilinear"",\n        padding_mode=""zeros"",\n        as_tensor_output: bool = True,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        Args:\n            keys (Hashable items): keys of the corresponding items to be transformed.\n            spatial_size (list or tuple of int): output image spatial size.\n                if ``data`` component has two spatial dimensions, ``spatial_size`` should have 2 elements [h, w].\n                if ``data`` component has three spatial dimensions, ``spatial_size`` should have 3 elements [h, w, d].\n            prob (float): probability of returning a randomized affine grid.\n                defaults to 0.1, with 10% chance returns a randomized grid.\n            mode (str or sequence of str): interpolation order.\n                Available options are \'nearest\', \'bilinear\'. Defaults to ``\'bilinear\'``.\n                if mode is a tuple of interpolation mode strings, each string corresponds to a key in ``keys``.\n                this is useful to set different modes for different data items.\n            padding_mode (str or sequence of str): mode of handling out of range indices.\n                Available options are \'zeros\', \'border\', \'reflection\'.  Defaults to ``\'zeros\'``.\n            as_tensor_output (bool): the computation is implemented using pytorch tensors, this option specifies\n                whether to convert it back to numpy arrays.\n            device (torch.device): device on which the tensor will be allocated.\n\n        See also:\n            - :py:class:`monai.transforms.compose.MapTransform`\n            - :py:class:`RandAffineGrid` for the random affine parameters configurations.\n        """"""\n        super().__init__(keys)\n        self.rand_affine = RandAffine(\n            prob=prob,\n            rotate_range=rotate_range,\n            shear_range=shear_range,\n            translate_range=translate_range,\n            scale_range=scale_range,\n            spatial_size=spatial_size,\n            as_tensor_output=as_tensor_output,\n            device=device,\n        )\n        self.padding_mode = ensure_tuple_rep(padding_mode, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n\n    def set_random_state(self, seed=None, state=None):\n        self.rand_affine.set_random_state(seed, state)\n        super().set_random_state(seed, state)\n        return self\n\n    def randomize(self):\n        self.rand_affine.randomize()\n\n    def __call__(self, data):\n        d = dict(data)\n        self.randomize()\n\n        spatial_size = self.rand_affine.spatial_size\n        if self.rand_affine.do_transform:\n            grid = self.rand_affine.rand_affine_grid(spatial_size=spatial_size)\n        else:\n            grid = create_grid(spatial_size=spatial_size)\n\n        for idx, key in enumerate(self.keys):\n            d[key] = self.rand_affine.resampler(d[key], grid, padding_mode=self.padding_mode[idx], mode=self.mode[idx])\n        return d\n\n\nclass Rand2DElasticd(Randomizable, MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.Rand2DElastic`.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        spatial_size,\n        spacing,\n        magnitude_range,\n        prob: float = 0.1,\n        rotate_range=None,\n        shear_range=None,\n        translate_range=None,\n        scale_range=None,\n        mode=""bilinear"",\n        padding_mode=""zeros"",\n        as_tensor_output: bool = False,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        Args:\n            keys (Hashable items): keys of the corresponding items to be transformed.\n            spatial_size (2 ints): specifying output image spatial size [h, w].\n            spacing (2 ints): distance in between the control points.\n            magnitude_range (2 ints): the random offsets will be generated from\n                ``uniform[magnitude[0], magnitude[1])``.\n            prob (float): probability of returning a randomized affine grid.\n                defaults to 0.1, with 10% chance returns a randomized grid,\n                otherwise returns a ``spatial_size`` centered area extracted from the input image.\n            mode (str or sequence of str): interpolation order.\n                Available options are \'nearest\', \'bilinear\'. Defaults to ``\'bilinear\'``.\n                if mode is a tuple of interpolation mode strings, each string corresponds to a key in ``keys``.\n                this is useful to set different modes for different data items.\n            padding_mode (str or sequence of str): mode of handling out of range indices.\n                Available options are \'zeros\', \'border\', \'reflection\'.  Defaults to ``\'zeros\'``.\n            as_tensor_output (bool): the computation is implemented using pytorch tensors, this option specifies\n                whether to convert it back to numpy arrays.\n            device (torch.device): device on which the tensor will be allocated.\n        See also:\n            - :py:class:`RandAffineGrid` for the random affine parameters configurations.\n            - :py:class:`Affine` for the affine transformation parameters configurations.\n        """"""\n        super().__init__(keys)\n        self.rand_2d_elastic = Rand2DElastic(\n            spacing=spacing,\n            magnitude_range=magnitude_range,\n            prob=prob,\n            rotate_range=rotate_range,\n            shear_range=shear_range,\n            translate_range=translate_range,\n            scale_range=scale_range,\n            spatial_size=spatial_size,\n            as_tensor_output=as_tensor_output,\n            device=device,\n        )\n        self.padding_mode = ensure_tuple_rep(padding_mode, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n\n    def set_random_state(self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None):\n        self.rand_2d_elastic.set_random_state(seed, state)\n        super().set_random_state(seed, state)\n        return self\n\n    def randomize(self, spatial_size):\n        self.rand_2d_elastic.randomize(spatial_size)\n\n    def __call__(self, data):\n        d = dict(data)\n        spatial_size = self.rand_2d_elastic.spatial_size\n        self.randomize(spatial_size)\n\n        if self.rand_2d_elastic.do_transform:\n            grid = self.rand_2d_elastic.deform_grid(spatial_size)\n            grid = self.rand_2d_elastic.rand_affine_grid(grid=grid)\n            grid = torch.nn.functional.interpolate(grid[None], spatial_size, mode=""bicubic"", align_corners=False)[0]\n        else:\n            grid = create_grid(spatial_size)\n\n        for idx, key in enumerate(self.keys):\n            d[key] = self.rand_2d_elastic.resampler(\n                d[key], grid, padding_mode=self.padding_mode[idx], mode=self.mode[idx]\n            )\n        return d\n\n\nclass Rand3DElasticd(Randomizable, MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.Rand3DElastic`.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        spatial_size,\n        sigma_range,\n        magnitude_range,\n        prob: float = 0.1,\n        rotate_range=None,\n        shear_range=None,\n        translate_range=None,\n        scale_range=None,\n        mode=""bilinear"",\n        padding_mode=""zeros"",\n        as_tensor_output: bool = False,\n        device: Optional[torch.device] = None,\n    ):\n        """"""\n        Args:\n            keys (Hashable items): keys of the corresponding items to be transformed.\n            spatial_size (3 ints): specifying output image spatial size [h, w, d].\n            sigma_range (2 ints): a Gaussian kernel with standard deviation sampled\n                 from ``uniform[sigma_range[0], sigma_range[1])`` will be used to smooth the random offset grid.\n            magnitude_range (2 ints): the random offsets on the grid will be generated from\n                ``uniform[magnitude[0], magnitude[1])``.\n            prob (float): probability of returning a randomized affine grid.\n                defaults to 0.1, with 10% chance returns a randomized grid,\n                otherwise returns a ``spatial_size`` centered area extracted from the input image.\n            mode (str or sequence of str): interpolation order.\n                Available options are \'nearest\', \'bilinear\'. Defaults to ``\'bilinear\'``.\n                if mode is a tuple of interpolation mode strings, each string corresponds to a key in ``keys``.\n                this is useful to set different modes for different data items.\n            padding_mode (str or sequence of str): mode of handling out of range indices.\n                Available options are \'zeros\', \'border\', \'reflection\'.  Defaults to ``\'zeros\'``.\n            as_tensor_output (bool): the computation is implemented using pytorch tensors, this option specifies\n                whether to convert it back to numpy arrays.\n            device (torch.device): device on which the tensor will be allocated.\n        See also:\n            - :py:class:`RandAffineGrid` for the random affine parameters configurations.\n            - :py:class:`Affine` for the affine transformation parameters configurations.\n        """"""\n        super().__init__(keys)\n        self.rand_3d_elastic = Rand3DElastic(\n            sigma_range=sigma_range,\n            magnitude_range=magnitude_range,\n            prob=prob,\n            rotate_range=rotate_range,\n            shear_range=shear_range,\n            translate_range=translate_range,\n            scale_range=scale_range,\n            spatial_size=spatial_size,\n            as_tensor_output=as_tensor_output,\n            device=device,\n        )\n        self.padding_mode = ensure_tuple_rep(padding_mode, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n\n    def set_random_state(self, seed: Optional[int] = None, state: Optional[np.random.RandomState] = None):\n        self.rand_3d_elastic.set_random_state(seed, state)\n        super().set_random_state(seed, state)\n        return self\n\n    def randomize(self, grid_size):\n        self.rand_3d_elastic.randomize(grid_size)\n\n    def __call__(self, data):\n        d = dict(data)\n        spatial_size = self.rand_3d_elastic.spatial_size\n        self.randomize(spatial_size)\n        grid = create_grid(spatial_size)\n        if self.rand_3d_elastic.do_transform:\n            device = self.rand_3d_elastic.device\n            grid = torch.tensor(grid).to(device)\n            gaussian = GaussianFilter(spatial_dims=3, sigma=self.rand_3d_elastic.sigma, truncated=3.0).to(device)\n            offset = torch.tensor(self.rand_3d_elastic.rand_offset[None], device=device)\n            grid[:3] += gaussian(offset)[0] * self.rand_3d_elastic.magnitude\n            grid = self.rand_3d_elastic.rand_affine_grid(grid=grid)\n\n        for idx, key in enumerate(self.keys):\n            d[key] = self.rand_3d_elastic.resampler(\n                d[key], grid, padding_mode=self.padding_mode[idx], mode=self.mode[idx]\n            )\n        return d\n\n\nclass Flipd(MapTransform):\n    """"""Dictionary-based wrapper of :py:class:`monai.transforms.Flip`.\n\n    See `numpy.flip` for additional details.\n    https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html\n\n    Args:\n        keys (dict): Keys to pick data for transformation.\n        spatial_axis (None, int or tuple of ints): Spatial axes along which to flip over. Default is None.\n    """"""\n\n    def __init__(self, keys: Hashable, spatial_axis=None):\n        super().__init__(keys)\n        self.flipper = Flip(spatial_axis=spatial_axis)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.flipper(d[key])\n        return d\n\n\nclass RandFlipd(Randomizable, MapTransform):\n    """"""Dictionary-based version :py:class:`monai.transforms.RandFlip`.\n\n    See `numpy.flip` for additional details.\n    https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html\n\n    Args:\n        prob (float): Probability of flipping.\n        spatial_axis (None, int or tuple of ints): Spatial axes along which to flip over. Default is None.\n    """"""\n\n    def __init__(self, keys: Hashable, prob: float = 0.1, spatial_axis=None):\n        super().__init__(keys)\n        self.spatial_axis = spatial_axis\n        self.prob = prob\n\n        self._do_transform = False\n        self.flipper = Flip(spatial_axis=spatial_axis)\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n\n    def __call__(self, data):\n        self.randomize()\n        d = dict(data)\n        if not self._do_transform:\n            return d\n        for key in self.keys:\n            d[key] = self.flipper(d[key])\n        return d\n\n\nclass Rotated(MapTransform):\n    """"""Dictionary-based wrapper of :py:class:`monai.transforms.Rotate`.\n\n    Args:\n        keys (dict): Keys to pick data for transformation.\n        angle (float): Rotation angle in degrees.\n        spatial_axes (tuple of 2 ints): Spatial axes of rotation. Default: (0, 1).\n            This is the first two axis in spatial dimensions.\n        reshape (bool): If reshape is true, the output shape is adapted so that the\n            input array is contained completely in the output. Default is True.\n        interp_order (int or sequence of int): Order of spline interpolation. Range 0-5.\n            Default: InterpolationCode.LINEAR. This is different from scipy where default interpolation\n            is InterpolationCode.SPLINE3.\n        mode (str or sequence of str): Points outside boundary filled according to this mode. Options are\n            \'constant\', \'nearest\', \'reflect\', \'wrap\'. Default: \'constant\'.\n        cval (scalar or sequence of scalar): Values to fill outside boundary. Default: 0.\n        prefilter (bool or sequence of bool): Apply spline_filter before interpolation. Default: True.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        angle: float,\n        spatial_axes=(0, 1),\n        reshape: bool = True,\n        interp_order=InterpolationCode.LINEAR,\n        mode=""constant"",\n        cval=0,\n        prefilter=True,\n    ):\n        super().__init__(keys)\n        self.rotator = Rotate(angle=angle, spatial_axes=spatial_axes, reshape=reshape)\n\n        self.interp_order = ensure_tuple_rep(interp_order, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n        self.cval = ensure_tuple_rep(cval, len(self.keys))\n        self.prefilter = ensure_tuple_rep(prefilter, len(self.keys))\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            d[key] = self.rotator(\n                d[key],\n                order=self.interp_order[idx],\n                mode=self.mode[idx],\n                cval=self.cval[idx],\n                prefilter=self.prefilter[idx],\n            )\n        return d\n\n\nclass RandRotated(Randomizable, MapTransform):\n    """"""Dictionary-based version :py:class:`monai.transforms.RandRotate`\n    Randomly rotates the input arrays.\n\n    Args:\n        prob (float): Probability of rotation.\n        degrees (tuple of float or float): Range of rotation in degrees. If single number,\n            angle is picked from (-degrees, degrees).\n        spatial_axes (tuple of 2 ints): Spatial axes of rotation. Default: (0, 1).\n            This is the first two axis in spatial dimensions.\n        reshape (bool): If reshape is true, the output shape is adapted so that the\n            input array is contained completely in the output. Default is True.\n        interp_order (int or sequence of int): Order of spline interpolation. Range 0-5.\n            Default: InterpolationCode.LINEAR. This is different from scipy where default\n            interpolation is InterpolationCode.SPLINE3.\n        mode (str or sequence of str): Points outside boundary filled according to this mode. Options are\n            \'constant\', \'nearest\', \'reflect\', \'wrap\'. Default: \'constant\'.\n        cval (scalar or sequence of scalar): Value to fill outside boundary. Default: 0.\n        prefilter (bool or sequence of bool): Apply spline_filter before interpolation. Default: True.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        degrees,\n        prob: float = 0.1,\n        spatial_axes=(0, 1),\n        reshape: bool = True,\n        interp_order=InterpolationCode.LINEAR,\n        mode=""constant"",\n        cval=0,\n        prefilter=True,\n    ):\n        super().__init__(keys)\n        self.prob = prob\n        self.degrees = degrees\n        self.reshape = reshape\n        self.spatial_axes = spatial_axes\n\n        self.interp_order = ensure_tuple_rep(interp_order, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n        self.cval = ensure_tuple_rep(cval, len(self.keys))\n        self.prefilter = ensure_tuple_rep(prefilter, len(self.keys))\n\n        if not hasattr(self.degrees, ""__iter__""):\n            self.degrees = (-self.degrees, self.degrees)\n        assert len(self.degrees) == 2, ""degrees should be a number or pair of numbers.""\n\n        self._do_transform = False\n        self.angle = None\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n        self.angle = self.R.uniform(low=self.degrees[0], high=self.degrees[1])\n\n    def __call__(self, data):\n        self.randomize()\n        d = dict(data)\n        if not self._do_transform:\n            return d\n        rotator = Rotate(angle=self.angle, spatial_axes=self.spatial_axes, reshape=self.reshape)\n        for idx, key in enumerate(self.keys):\n            d[key] = rotator(\n                d[key],\n                order=self.interp_order[idx],\n                mode=self.mode[idx],\n                cval=self.cval[idx],\n                prefilter=self.prefilter[idx],\n            )\n        return d\n\n\nclass Zoomd(MapTransform):\n    """"""Dictionary-based wrapper of :py:class:`monai.transforms.Zoom`.\n\n    Args:\n        zoom (float or sequence): The zoom factor along the spatial axes.\n            If a float, zoom is the same for each spatial axis.\n            If a sequence, zoom should contain one value for each spatial axis.\n        interp_order (int or sequence of int): order of interpolation. Default=InterpolationCode.SPLINE3.\n        mode (str or sequence of str): Determines how input is extended beyond boundaries. Default is \'constant\'.\n        cval (scalar or sequence of scalar): Value to fill past edges. Default is 0.\n        prefilter (bool or sequence of bool): Apply spline_filter before interpolation. Default: True.\n        use_gpu (bool): Should use cpu or gpu. Uses cupyx which doesn\'t support order > 1 and modes\n            \'wrap\' and \'reflect\'. Defaults to cpu for these cases or if cupyx not found.\n        keep_size (bool): Should keep original size (pad if needed), default is True.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        zoom,\n        interp_order=InterpolationCode.SPLINE3,\n        mode=""constant"",\n        cval=0,\n        prefilter=True,\n        use_gpu: bool = False,\n        keep_size=True,\n    ):\n        super().__init__(keys)\n        self.zoomer = Zoom(zoom=zoom, use_gpu=use_gpu, keep_size=keep_size)\n\n        self.interp_order = ensure_tuple_rep(interp_order, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n        self.cval = ensure_tuple_rep(cval, len(self.keys))\n        self.prefilter = ensure_tuple_rep(prefilter, len(self.keys))\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            d[key] = self.zoomer(\n                d[key],\n                order=self.interp_order[idx],\n                mode=self.mode[idx],\n                cval=self.cval[idx],\n                prefilter=self.prefilter[idx],\n            )\n        return d\n\n\nclass RandZoomd(Randomizable, MapTransform):\n    """"""Dict-based version :py:class:`monai.transforms.RandZoom`.\n\n    Args:\n        keys (dict): Keys to pick data for transformation.\n        prob (float): Probability of zooming.\n        min_zoom (float or sequence): Min zoom factor. Can be float or sequence same size as image.\n            If a float, min_zoom is the same for each spatial axis.\n            If a sequence, min_zoom should contain one value for each spatial axis.\n        max_zoom (float or sequence): Max zoom factor. Can be float or sequence same size as image.\n            If a float, max_zoom is the same for each spatial axis.\n            If a sequence, max_zoom should contain one value for each spatial axis.\n        interp_order (int or sequence of int): order of interpolation. Default=InterpolationCode.SPLINE3.\n        mode (str or sequence of str): Available options are \'reflect\', \'constant\', \'nearest\', \'mirror\', \'wrap\'.\n            Determines how input is extended beyond boundaries. Default: \'constant\'.\n        cval (scalar or sequence of scalar): Value to fill past edges. Default is 0.\n        prefilter (bool or sequence of bool): Apply spline_filter before interpolation. Default: True.\n        use_gpu (bool): Should use cpu or gpu. Uses cupyx which doesn\'t support order > 1 and modes\n            \'wrap\' and \'reflect\'. Defaults to cpu for these cases or if cupyx not found.\n        keep_size (bool): Should keep original size (pad if needed), default is True.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        prob: float = 0.1,\n        min_zoom=0.9,\n        max_zoom=1.1,\n        interp_order=InterpolationCode.SPLINE3,\n        mode=""constant"",\n        cval=0,\n        prefilter=True,\n        use_gpu: bool = False,\n        keep_size: bool = True,\n    ):\n        super().__init__(keys)\n        if hasattr(min_zoom, ""__iter__"") and hasattr(max_zoom, ""__iter__""):\n            assert len(min_zoom) == len(max_zoom), ""min_zoom and max_zoom must have same length.""\n        self.min_zoom = min_zoom\n        self.max_zoom = max_zoom\n        self.prob = prob\n        self.use_gpu = use_gpu\n        self.keep_size = keep_size\n\n        self.interp_order = ensure_tuple_rep(interp_order, len(self.keys))\n        self.mode = ensure_tuple_rep(mode, len(self.keys))\n        self.cval = ensure_tuple_rep(cval, len(self.keys))\n        self.prefilter = ensure_tuple_rep(prefilter, len(self.keys))\n\n        self._do_transform = False\n        self._zoom = None\n\n    def randomize(self):\n        self._do_transform = self.R.random_sample() < self.prob\n        if hasattr(self.min_zoom, ""__iter__""):\n            self._zoom = (self.R.uniform(l, h) for l, h in zip(self.min_zoom, self.max_zoom))\n        else:\n            self._zoom = self.R.uniform(self.min_zoom, self.max_zoom)\n\n    def __call__(self, data):\n        self.randomize()\n        d = dict(data)\n        if not self._do_transform:\n            return d\n        zoomer = Zoom(self._zoom, use_gpu=self.use_gpu, keep_size=self.keep_size)\n        for idx, key in enumerate(self.keys):\n            d[key] = zoomer(\n                d[key],\n                order=self.interp_order[idx],\n                mode=self.mode[idx],\n                cval=self.cval[idx],\n                prefilter=self.prefilter[idx],\n            )\n        return d\n\n\nSpacingD = SpacingDict = Spacingd\nOrientationD = OrientationDict = Orientationd\nRotate90D = Rotate90Dict = Rotate90d\nRandRotate90D = RandRotate90Dict = RandRotate90d\nResizeD = ResizeDict = Resized\nRandAffineD = RandAffineDict = RandAffined\nRand2DElasticD = Rand2DElasticDict = Rand2DElasticd\nRand3DElasticD = Rand3DElasticDict = Rand3DElasticd\nFlipD = FlipDict = Flipd\nRandFlipD = RandFlipDict = RandFlipd\nRotateD = RotateDict = Rotated\nRandRotateD = RandRotateDict = RandRotated\nZoomD = ZoomDict = Zoomd\nRandZoomD = RandZoomDict = RandZoomd\n'"
monai/transforms/utility/__init__.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
monai/transforms/utility/array.py,2,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of ""vanilla"" transforms for utility functions\nhttps://github.com/Project-MONAI/MONAI/wiki/MONAI_Design\n""""""\n\nimport time\n\nfrom typing import Callable, Optional\nimport logging\nimport numpy as np\nimport torch\n\nfrom monai.transforms.compose import Transform\n\n\nclass AsChannelFirst(Transform):\n    """"""\n    Change the channel dimension of the image to the first dimension.\n\n    Most of the image transformations in ``monai.transforms``\n    assume the input image is in the channel-first format, which has the shape\n    (num_channels, spatial_dim_1[, spatial_dim_2, ...]).\n\n    This transform could be used to convert, for example, a channel-last image array in shape\n    (spatial_dim_1[, spatial_dim_2, ...], num_channels) into the channel-first format,\n    so that the multidimensional image array can be correctly interpreted by the other transforms.\n\n    Args:\n        channel_dim: which dimension of input image is the channel, default is the last dimension.\n    """"""\n\n    def __init__(self, channel_dim: int = -1):\n        assert isinstance(channel_dim, int) and channel_dim >= -1, ""invalid channel dimension.""\n        self.channel_dim = channel_dim\n\n    def __call__(self, img):\n        return np.moveaxis(img, self.channel_dim, 0)\n\n\nclass AsChannelLast(Transform):\n    """"""\n    Change the channel dimension of the image to the last dimension.\n\n    Some of other 3rd party transforms assume the input image is in the channel-last format with shape\n    (spatial_dim_1[, spatial_dim_2, ...], num_channels).\n\n    This transform could be used to convert, for example, a channel-first image array in shape\n    (num_channels, spatial_dim_1[, spatial_dim_2, ...]) into the channel-last format,\n    so that MONAI transforms can construct a chain with other 3rd party transforms together.\n\n    Args:\n        channel_dim: which dimension of input image is the channel, default is the first dimension.\n    """"""\n\n    def __init__(self, channel_dim: int = 0):\n        assert isinstance(channel_dim, int) and channel_dim >= -1, ""invalid channel dimension.""\n        self.channel_dim = channel_dim\n\n    def __call__(self, img):\n        return np.moveaxis(img, self.channel_dim, -1)\n\n\nclass AddChannel(Transform):\n    """"""\n    Adds a 1-length channel dimension to the input image.\n\n    Most of the image transformations in ``monai.transforms``\n    assumes the input image is in the channel-first format, which has the shape\n    (num_channels, spatial_dim_1[, spatial_dim_2, ...]).\n\n    This transform could be used, for example, to convert a (spatial_dim_1[, spatial_dim_2, ...])\n    spatial image into the channel-first format so that the\n    multidimensional image array can be correctly interpreted by the other\n    transforms.\n    """"""\n\n    def __call__(self, img):\n        return img[None]\n\n\nclass RepeatChannel(Transform):\n    """"""\n    Repeat channel data to construct expected input shape for models.\n    The `repeats` count includes the origin data, for example:\n    ``RepeatChannel(repeats=2)([[1, 2], [3, 4]])`` generates: ``[[1, 2], [1, 2], [3, 4], [3, 4]]``\n\n    Args:\n        repeats: the number of repetitions for each element.\n    """"""\n\n    def __init__(self, repeats: int):\n        assert repeats > 0, ""repeats count must be greater than 0.""\n        self.repeats = repeats\n\n    def __call__(self, img):\n        return np.repeat(img, self.repeats, 0)\n\n\nclass CastToType(Transform):\n    """"""\n    Cast the image data to specified numpy data type.\n    """"""\n\n    def __init__(self, dtype: np.dtype = np.float32):\n        """"""\n        Args:\n            dtype (np.dtype): convert image to this data type, default is `np.float32`.\n        """"""\n        self.dtype = dtype\n\n    def __call__(self, img: np.ndarray):  # type: ignore # see issue #495\n        assert isinstance(img, np.ndarray), ""image must be numpy array.""\n        return img.astype(self.dtype)\n\n\nclass ToTensor(Transform):\n    """"""\n    Converts the input image to a tensor without applying any other transformations.\n    """"""\n\n    def __call__(self, img):\n        if torch.is_tensor(img):\n            return img.contiguous()\n        return torch.as_tensor(np.ascontiguousarray(img))\n\n\nclass Transpose(Transform):\n    """"""\n    Transposes the input image based on the given `indices` dimension ordering.\n    """"""\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __call__(self, img):\n        return img.transpose(self.indices)\n\n\nclass SqueezeDim(Transform):\n    """"""\n    Squeeze a unitary dimension.\n    """"""\n\n    def __init__(self, dim: Optional[int] = 0):\n        """"""\n        Args:\n            dim: dimension to be squeezed. Default = 0\n                ""None"" works when the input is numpy array.\n        """"""\n        if dim is not None and not isinstance(dim, int):\n            raise ValueError(f""Invalid channel dimension {dim}"")\n        self.dim = dim\n\n    def __call__(self, img):  # type: ignore # see issue #495\n        """"""\n        Args:\n            img (ndarray): numpy arrays with required dimension `dim` removed\n        """"""\n        return img.squeeze(self.dim)\n\n\nclass DataStats(Transform):\n    """"""\n    Utility transform to show the statistics of data for debug or analysis.\n    It can be inserted into any place of a transform chain and check results of previous transforms.\n    """"""\n\n    def __init__(\n        self,\n        prefix: str = ""Data"",\n        data_shape: bool = True,\n        intensity_range: bool = True,\n        data_value: bool = False,\n        additional_info: Optional[Callable] = None,\n        logger_handler: Optional[logging.Handler] = None,\n    ):\n        """"""\n        Args:\n            prefix (string): will be printed in format: ""{prefix} statistics"".\n            data_shape (bool): whether to show the shape of input data.\n            intensity_range (bool): whether to show the intensity value range of input data.\n            data_value (bool): whether to show the raw value of input data.\n                a typical example is to print some properties of Nifti image: affine, pixdim, etc.\n            additional_info (Callable): user can define callable function to extract additional info from input data.\n            logger_handler (logging.handler): add additional handler to output data: save to file, etc.\n                add existing python logging handlers: https://docs.python.org/3/library/logging.handlers.html\n        """"""\n        assert isinstance(prefix, str), ""prefix must be a string.""\n        self.prefix = prefix\n        self.data_shape = data_shape\n        self.intensity_range = intensity_range\n        self.data_value = data_value\n        if additional_info is not None and not callable(additional_info):\n            raise ValueError(""argument `additional_info` must be a callable."")\n        self.additional_info = additional_info\n        self.output: Optional[str] = None\n        logging.basicConfig(level=logging.NOTSET)\n        self._logger = logging.getLogger(""DataStats"")\n        if logger_handler is not None:\n            self._logger.addHandler(logger_handler)\n\n    def __call__(  # type: ignore # see issue #495\n        self,\n        img,\n        prefix: Optional[str] = None,\n        data_shape: Optional[bool] = None,\n        intensity_range: Optional[bool] = None,\n        data_value: Optional[bool] = None,\n        additional_info=None,\n    ):\n        lines = [f""{prefix or self.prefix} statistics:""]\n\n        if self.data_shape if data_shape is None else data_shape:\n            lines.append(f""Shape: {img.shape}"")\n        if self.intensity_range if intensity_range is None else intensity_range:\n            lines.append(f""Intensity range: ({np.min(img)}, {np.max(img)})"")\n        if self.data_value if data_value is None else data_value:\n            lines.append(f""Value: {img}"")\n        additional_info = self.additional_info if additional_info is None else additional_info\n        if additional_info is not None:\n            lines.append(f""Additional info: {additional_info(img)}"")\n        separator = ""\\n""\n        self.output = f""{separator.join(lines)}""\n        self._logger.debug(self.output)\n\n        return img\n\n\nclass SimulateDelay(Transform):\n    """"""\n    This is a pass through transform to be used for testing purposes. It allows\n    adding fake behaviors that are useful for testing purposes to simulate\n    how large datasets behave without needing to test on large data sets.\n\n    For example, simulating slow NFS data transfers, or slow network transfers\n    in testing by adding explicit timing delays. Testing of small test data\n    can lead to incomplete understanding of real world issues, and may lead\n    to sub-optimal design choices.\n    """"""\n\n    def __init__(self, delay_time: float = 0.0):\n        """"""\n        Args:\n            delay_time(float): The minimum amount of time, in fractions of seconds,\n                to accomplish this delay task.\n        """"""\n        super().__init__()\n        self.delay_time: float = delay_time\n\n    def __call__(self, img, delay_time=None):\n        time.sleep(self.delay_time if delay_time is None else delay_time)\n        return img\n'"
monai/transforms/utility/dictionary.py,0,"b'# Copyright 2020 MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA collection of dictionary-based wrappers around the ""vanilla"" transforms for utility functions\ndefined in :py:class:`monai.transforms.utility.array`.\n\nClass names are ended with \'d\' to denote dictionary-based transforms.\n""""""\n\nfrom logging import Handler\nfrom typing import Optional, Hashable\n\nimport numpy as np\n\nfrom monai.transforms.compose import MapTransform\nfrom monai.utils.misc import ensure_tuple_rep\nfrom monai.transforms.utility.array import (\n    AddChannel,\n    AsChannelFirst,\n    ToTensor,\n    AsChannelLast,\n    CastToType,\n    RepeatChannel,\n    SqueezeDim,\n    DataStats,\n    SimulateDelay,\n)\n\n\nclass AsChannelFirstd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.AsChannelFirst`.\n    """"""\n\n    def __init__(self, keys: Hashable, channel_dim: int = -1):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            channel_dim: which dimension of input image is the channel, default is the last dimension.\n        """"""\n        super().__init__(keys)\n        self.converter = AsChannelFirst(channel_dim=channel_dim)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.converter(d[key])\n        return d\n\n\nclass AsChannelLastd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.AsChannelLast`.\n    """"""\n\n    def __init__(self, keys: Hashable, channel_dim: int = 0):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            channel_dim: which dimension of input image is the channel, default is the first dimension.\n        """"""\n        super().__init__(keys)\n        self.converter = AsChannelLast(channel_dim=channel_dim)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.converter(d[key])\n        return d\n\n\nclass AddChanneld(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.AddChannel`.\n    """"""\n\n    def __init__(self, keys: Hashable):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n        """"""\n        super().__init__(keys)\n        self.adder = AddChannel()\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.adder(d[key])\n        return d\n\n\nclass RepeatChanneld(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:`monai.transforms.RepeatChannel`.\n    """"""\n\n    def __init__(self, keys: Hashable, repeats: int):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            repeats: the number of repetitions for each element.\n        """"""\n        super().__init__(keys)\n        self.repeater = RepeatChannel(repeats)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.repeater(d[key])\n        return d\n\n\nclass CastToTyped(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.CastToType`.\n    """"""\n\n    def __init__(self, keys: Hashable, dtype: np.dtype = np.float32):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            dtype (np.dtype): convert image to this data type, default is `np.float32`.\n        """"""\n        MapTransform.__init__(self, keys)\n        self.converter = CastToType(dtype)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.converter(d[key])\n        return d\n\n\nclass ToTensord(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.ToTensor`.\n    """"""\n\n    def __init__(self, keys: Hashable):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n        """"""\n        super().__init__(keys)\n        self.converter = ToTensor()\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.converter(d[key])\n        return d\n\n\nclass DeleteKeysd(MapTransform):\n    """"""\n    Delete specified keys from data dictionary to release memory.\n    It will remove the key-values and copy the others to construct a new dictionary.\n    """"""\n\n    def __init__(self, keys: Hashable):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n        """"""\n        super().__init__(keys)\n\n    def __call__(self, data):\n        return {key: val for key, val in data.items() if key not in self.keys}\n\n\nclass SqueezeDimd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.SqueezeDim`.\n    """"""\n\n    def __init__(self, keys: Hashable, dim: Optional[int] = 0):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            dim: dimension to be squeezed. Default: 0 (the first dimension)\n        """"""\n        super().__init__(keys)\n        self.converter = SqueezeDim(dim=dim)\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            d[key] = self.converter(d[key])\n        return d\n\n\nclass DataStatsd(MapTransform):\n    """"""\n    Dictionary-based wrapper of :py:class:`monai.transforms.DataStats`.\n    """"""\n\n    def __init__(\n        self,\n        keys: Hashable,\n        prefix=""Data"",\n        data_shape=True,\n        intensity_range=True,\n        data_value=False,\n        additional_info=None,\n        logger_handler: Optional[Handler] = None,\n    ):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            prefix (string or list of string): will be printed in format: ""{prefix} statistics"".\n            data_shape (bool or list of bool): whether to show the shape of input data.\n            intensity_range (bool or list of bool): whether to show the intensity value range of input data.\n            data_value (bool or list of bool): whether to show the raw value of input data.\n                a typical example is to print some properties of Nifti image: affine, pixdim, etc.\n            additional_info (Callable or list of Callable): user can define callable function to extract\n                additional info from input data.\n            logger_handler (logging.handler): add additional handler to output data: save to file, etc.\n                add existing python logging handlers: https://docs.python.org/3/library/logging.handlers.html\n        """"""\n        super().__init__(keys)\n        self.prefix = ensure_tuple_rep(prefix, len(self.keys))\n        self.data_shape = ensure_tuple_rep(data_shape, len(self.keys))\n        self.intensity_range = ensure_tuple_rep(intensity_range, len(self.keys))\n        self.data_value = ensure_tuple_rep(data_value, len(self.keys))\n        self.additional_info = ensure_tuple_rep(additional_info, len(self.keys))\n        self.logger_handler = logger_handler\n        self.printer = DataStats(logger_handler=logger_handler)\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            d[key] = self.printer(\n                d[key],\n                self.prefix[idx],\n                self.data_shape[idx],\n                self.intensity_range[idx],\n                self.data_value[idx],\n                self.additional_info[idx],\n            )\n        return d\n\n\nclass SimulateDelayd(MapTransform):\n    """"""\n    dictionary-based wrapper of :py:class:monai.transforms.utility.array.SimulateDelay.\n    """"""\n\n    def __init__(self, keys: Hashable, delay_time=0.0):\n        """"""\n        Args:\n            keys (hashable items): keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            delay_time(float or list of float): The minimum amount of time, in fractions of seconds,\n                to accomplish this identity task. If a list is provided, it must be of length equal\n                to the keys representing the delay for each key element.\n        """"""\n        super().__init__(keys)\n        self.delay_time = ensure_tuple_rep(delay_time, len(self.keys))\n        self.delayer = SimulateDelay()\n\n    def __call__(self, data):\n        d = dict(data)\n        for idx, key in enumerate(self.keys):\n            d[key] = self.delayer(d[key], delay_time=self.delay_time[idx])\n        return d\n\n\nAsChannelFirstD = AsChannelFirstDict = AsChannelFirstd\nAsChannelLastD = AsChannelLastDict = AsChannelLastd\nAddChannelD = AddChannelDict = AddChanneld\nRepeatChannelD = RepeatChannelDict = RepeatChanneld\nCastToTypeD = CastToTypeDict = CastToTyped\nToTensorD = ToTensorDict = ToTensord\nDeleteKeysD = DeleteKeysDict = DeleteKeysd\nSqueezeDimD = SqueezeDimDict = SqueezeDimd\nDataStatsD = DataStatsDict = DataStatsd\nSimulateDelayD = SimulateDelayDict = SimulateDelayd\n'"
