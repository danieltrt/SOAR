file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport subprocess\nimport time\n\nfrom setuptools import find_packages, setup\n\nMAJOR = 0\nMINOR = 1\nPATCH = 0\nSUFFIX = \'\'\nSHORT_VERSION = \'{}.{}.{}{}\'.format(MAJOR, MINOR, PATCH, SUFFIX)\nVERSION_FILE = \'pyretri/version.py\'\n\n\ndef get_git_hash():\n    """"""Get git hash value.\n    Returns:\n        str, git hash value.\n    """"""\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for key in [\'SYSTEMROOT\', \'PATH\', \'HOME\']:\n            value = os.environ.get(key)\n            if value is not None:\n                env[key] = value\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        sha = out.strip().decode(\'ascii\')\n    except OSError:\n        sha = \'unknown\'\n    return sha\n\n\ndef get_hash():\n    """"""Get hash value.\n    Returns:\n        str, hash value.\n    """"""\n    if os.path.exists(\'.git\'):\n        sha = get_git_hash()[:7]\n    elif os.path.exists(VERSION_FILE):\n        try:\n            from pyretri.version import __version__\n            sha = __version__.split(\'+\')[-1]\n        except ImportError:\n            raise ImportError(\'Unable to get git version\')\n    else:\n        sha = \'unknown\'\n    return sha\n\n\ndef readme():\n    """"""Get readme.\n    Returns:\n        str, readme content string.\n    """"""\n    with open(\'README.md\') as fid:\n        content = fid.read()\n    return content\n\n\ndef write_version_py():\n    """"""Write version.py.\n    """"""\n    content = """"""# GENERATED VERSION FILE\n# TIME: {}\n__version__ = \'{}\'\nshort_version = \'{}\'\n""""""\n    sha = get_hash()\n    version = SHORT_VERSION + \'+\' + sha\n    with open(VERSION_FILE, \'w\') as fid:\n        fid.write(content.format(time.asctime(), version, SHORT_VERSION))\n\n\ndef get_version():\n    """"""Get version.\n    Returns:\n        str, version string.\n    """"""\n    with open(VERSION_FILE, \'r\') as fid:\n        exec (compile(fid.read(), VERSION_FILE, \'exec\'))\n    return locals()[\'__version__\']\n\n\ndef parse_requirements(fname=\'requirements.txt\', with_version=True):\n    """"""\n    Parse the package dependencies listed in a requirements file but strips\n    specific versioning information.\n    Args:\n        fname (str): path to requirements file\n        with_version (bool, default=False): if True include version specs\n    Returns:\n        List[str]: list of requirements items\n    CommandLine:\n        python -c ""import setup; print(setup.parse_requirements())""\n    """"""\n    import sys\n    from os.path import exists\n    import re\n    require_fpath = fname\n\n    def parse_line(line):\n        """"""\n        Parse information from a line in a requirements text file\n        """"""\n        if line.startswith(\'-r \'):\n            # Allow specifying requirements in other files\n            target = line.split(\' \')[1]\n            for info in parse_require_file(target):\n                yield info\n        else:\n            info = {\'line\': line}\n            if line.startswith(\'-e \'):\n                info[\'package\'] = line.split(\'#egg=\')[1]\n            else:\n                # Remove versioning from the package\n                pat = \'(\' + \'|\'.join([\'>=\', \'==\', \'>\']) + \')\'\n                parts = re.split(pat, line, maxsplit=1)\n                parts = [p.strip() for p in parts]\n\n                info[\'package\'] = parts[0]\n                if len(parts) > 1:\n                    op, rest = parts[1:]\n                    if \';\' in rest:\n                        # Handle platform specific dependencies\n                        # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\n                        version, platform_deps = map(str.strip,\n                                                     rest.split(\';\'))\n                        info[\'platform_deps\'] = platform_deps\n                    else:\n                        version = rest  # NOQA\n                    info[\'version\'] = (op, version)\n            yield info\n\n    def parse_require_file(fpath):\n        with open(fpath, \'r\') as f:\n            for line in f.readlines():\n                line = line.strip()\n                if line and not line.startswith(\'#\'):\n                    for info in parse_line(line):\n                        yield info\n\n    def gen_packages_items():\n        if exists(require_fpath):\n            for info in parse_require_file(require_fpath):\n                parts = [info[\'package\']]\n                if with_version and \'version\' in info:\n                    parts.extend(info[\'version\'])\n                if not sys.version.startswith(\'3.4\'):\n                    # apparently package_deps are broken in 3.4\n                    platform_deps = info.get(\'platform_deps\')\n                    if platform_deps is not None:\n                        parts.append(\';\' + platform_deps)\n                item = \'\'.join(parts)\n                yield item\n\n    packages = list(gen_packages_items())\n    return packages\n\n\nif __name__ == \'__main__\':\n    write_version_py()\n    setup(\n        name=\'pyretri\',\n        version=get_version(),\n        description=\'A Toolbox for Deep Learning-based Image Retrieval\',\n        long_description=readme(),\n        author=\'Megvii & XJTU\',\n        author_email=\'https://github.com/???\',\n        keywords=\'computer vision, image retrieval\',\n        url=\'https://github.com/???\',\n        classifiers=[\n            \'Development Status :: 4 - Beta\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n            \'Programming Language :: Python :: 3.7\',\n        ],\n        license=\'Apache License 2.0\',\n        install_requires=[\n            \'numpy\', \'torch>=1.2\', \'torchvision>=0.4\', \'sklearn\', \'yacs\', \'tqdm\',\n        ],\n        packages=find_packages(),\n        zip_safe=False)\n'"
main/extract_feature.py,0,"b""# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\n\nimport torch\n\nfrom pyretri.config import get_defaults_cfg, setup_cfg\nfrom pyretri.datasets import build_folder, build_loader\nfrom pyretri.models import build_model\nfrom pyretri.extract import build_extract_helper\n\nfrom torchvision import models\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='A tool box for deep learning-based image retrieval')\n    parser.add_argument('opts', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument('--data_json', '-dj', default=None, type=str, help='json file for dataset to be extracted')\n    parser.add_argument('--save_path', '-sp', default=None, type=str, help='save path for features')\n    parser.add_argument('--config_file', '-cfg', default=None, metavar='FILE', type=str, help='path to config file')\n    parser.add_argument('--save_interval', '-si', default=5000, type=int, help='number of features saved in one part file')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n    assert args.data_json is not None, 'the dataset json must be provided!'\n    assert args.save_path is not None, 'the save path must be provided!'\n    assert args.config_file is not None, 'a config file must be provided!'\n\n    # init and load retrieval pipeline settings\n    cfg = get_defaults_cfg()\n    cfg = setup_cfg(cfg, args.config_file, args.opts)\n\n    # build dataset and dataloader\n    dataset = build_folder(args.data_json, cfg.datasets)\n    dataloader = build_loader(dataset, cfg.datasets)\n\n    # build model\n    model = build_model(cfg.model)\n\n    # build helper and extract features\n    extract_helper = build_extract_helper(model, cfg.extract)\n    extract_helper.do_extract(dataloader, args.save_path, args.save_interval)\n\n\nif __name__ == '__main__':\n    main()\n"""
main/index.py,0,"b""# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport pickle\n\n\nfrom pyretri.config import get_defaults_cfg, setup_cfg\nfrom pyretri.index import build_index_helper, feature_loader\nfrom pyretri.evaluate import build_evaluate_helper\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='A tool box for deep learning-based image retrieval')\n    parser.add_argument('opts', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument('--config_file', '-cfg', default=None, metavar='FILE', type=str, help='path to config file')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n    assert args.config_file is not None, 'a config file must be provided!'\n\n    # init and load retrieval pipeline settings\n    cfg = get_defaults_cfg()\n    cfg = setup_cfg(cfg, args.config_file, args.opts)\n\n    # load features\n    query_fea, query_info, _ = feature_loader.load(cfg.index.query_fea_dir, cfg.index.feature_names)\n    gallery_fea, gallery_info, _ = feature_loader.load(cfg.index.gallery_fea_dir, cfg.index.feature_names)\n\n    # build helper and index features\n    index_helper = build_index_helper(cfg.index)\n    index_result_info, query_fea, gallery_fea = index_helper.do_index(query_fea, query_info, gallery_fea)\n\n    # build helper and evaluate results\n    evaluate_helper = build_evaluate_helper(cfg.evaluate)\n    mAP, recall_at_k = evaluate_helper.do_eval(index_result_info, gallery_info)\n\n    # show results\n    evaluate_helper.show_results(mAP, recall_at_k)\n\n\nif __name__ == '__main__':\n    main()\n"""
main/make_data_json.py,0,"b'# -*- coding: utf-8 -*-\n\nimport argparse\n\nfrom pyretri.extract import make_data_json\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument(\'--dataset\', \'-d\', default=None, type=str, help=""path for the dataset that make the json file"")\n    parser.add_argument(\'--save_path\', \'-sp\', default=None, type=str, help=""save path for the json file"")\n    parser.add_argument(\'--type\', \'-t\', default=None, type=str, help=""mode of the dataset"")\n    parser.add_argument(\'--ground_truth\', \'-gt\', default=None, type=str, help=""ground truth of the dataset"")\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n    assert args.dataset is not None, \'the data must be provided!\'\n    assert args.save_path is not None, \'the save path must be provided!\'\n    assert args.type is not None, \'the type must be provided!\'\n\n    # make data json\n    make_data_json(args.dataset, args.save_path, args.type, args.ground_truth)\n\n    print(\'make data json have done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main/single_index.py,0,"b'# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nfrom PIL import Image\nimport numpy as np\n\nfrom pyretri.config import get_defaults_cfg, setup_cfg\nfrom pyretri.datasets import build_transformers\nfrom pyretri.models import build_model\nfrom pyretri.extract import build_extract_helper\nfrom pyretri.index import build_index_helper, feature_loader\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument(\'--config_file\', \'-cfg\', default=None, metavar=\'FILE\', type=str, help=\'path to config file\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n    assert args.config_file is not """", \'a config file must be provided!\'\n    assert os.path.exists(args.config_file), \'the config file must be existed!\'\n\n    # init and load retrieval pipeline settings\n    cfg = get_defaults_cfg()\n    cfg = setup_cfg(cfg, args.config_file, args.opts)\n\n    # set path for single image\n    path = \'/data/caltech101/query/airplanes/image_0004.jpg\'\n\n    # build transformers\n    transformers = build_transformers(cfg.datasets.transformers)\n\n    # build model\n    model = build_model(cfg.model)\n\n    # read image and convert it to tensor\n    img = Image.open(path).convert(""RGB"")\n    img_tensor = transformers(img)\n\n    # build helper and extract feature for single image\n    extract_helper = build_extract_helper(model, cfg.extract)\n    img_fea_info = extract_helper.do_single_extract(img_tensor)\n    stacked_feature = list()\n    for name in cfg.index.feature_names:\n        assert name in img_fea_info[0], ""invalid feature name: {} not in {}!"".format(name, img_fea_info[0].keys())\n        stacked_feature.append(img_fea_info[0][name])\n    img_fea = np.concatenate(stacked_feature, axis=1)\n\n    # load gallery features\n    gallery_fea, gallery_info, _ = feature_loader.load(cfg.index.gallery_fea_dir, cfg.index.feature_names)\n\n    # build helper and single index feature\n    index_helper = build_index_helper(cfg.index)\n    index_result_info, query_fea, gallery_fea = index_helper.do_index(img_fea, img_fea_info, gallery_fea)\n\n    index_helper.save_topk_retrieved_images(\'retrieved_images/\', index_result_info[0], 5, gallery_info)\n\n    print(\'single index have done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main/split_dataset.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport os\n\nfrom pyretri.extract.utils import split_dataset\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument(\'--dataset\', \'-d\', default=None, type=str, help=""path for the dataset."")\n    parser.add_argument(\'--split_file\', \'-sf\', default=None, type=str, help=""name for the dataset."")\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n    assert args.dataset is not None, \'the dataset must be provided!\'\n    assert args.split_file is not None, \'the save path must be provided!\'\n\n    # split dataset\n    split_dataset(args.dataset, args.split_file)\n\n    print(\'split dataset have done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pyretri/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n__version__ = ""0.1.0""\n'"
search/__init__.py,0,b''
search/reid_search_extract.py,0,"b'# -*- coding: utf-8 -*-\n\n\nimport os\n\nimport torch\n\nimport argparse\nimport importlib\n\nfrom pyretri.config import get_defaults_cfg\nfrom pyretri.datasets import build_folder, build_loader\nfrom pyretri.models import build_model\nfrom pyretri.extract import build_extract_helper\n\nfrom pyretri.models.backbone import ft_net\n\n\ndef load_datasets():\n    data_json_dir = ""/home/songrenjie/projects/RetrievalToolBox/new_data_jsons/""\n    datasets = {\n        ""market_gallery"": os.path.join(data_json_dir, ""market_gallery.json""),\n        ""market_query"": os.path.join(data_json_dir, ""market_query.json""),\n        ""duke_gallery"": os.path.join(data_json_dir, ""duke_gallery.json""),\n        ""duke_query"": os.path.join(data_json_dir, ""duke_query.json""),\n    }\n    for data_path in datasets.values():\n        assert os.path.exists(data_path), ""non-exist dataset path {}"".format(data_path)\n    return datasets\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument(\'--save_path\', \'-sp\', default=None, type=str, help=""the save path for feature"")\n    parser.add_argument(""--search_modules"", ""-sm"", default="""", type=str, help=""name of search module\'s directory"")\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n\n    # init retrieval pipeline settings\n    cfg = get_defaults_cfg()\n\n    # load search space\n    datasets = load_datasets()\n    pre_processes = importlib.import_module(""{}.pre_process_dict"".format(args.search_modules)).pre_processes\n    models = importlib.import_module(""{}.extract_dict"".format(args.search_modules)).models\n    extracts = importlib.import_module(""{}.extract_dict"".format(args.search_modules)).extracts\n\n    # search in an exhaustive way\n    for data_name, data_args in datasets.items():\n        for pre_proc_name, pre_proc_args in pre_processes.items():\n            if \'market\' in data_name:\n                model_name = \'market_res50\'\n            elif \'duke\' in data_name:\n                model_name = \'duke_res50\'\n\n            feature_full_name = data_name + ""_"" + pre_proc_name + ""_"" + model_name\n            print(feature_full_name)\n\n            if os.path.exists(os.path.join(args.save_path, feature_full_name)):\n                print(""[Search Extract]: config exists..."")\n                continue\n\n            # load retrieval pipeline settings\n            cfg.datasets.merge_from_other_cfg(pre_proc_args)\n            cfg.model.merge_from_other_cfg(models[model_name])\n            cfg.extract.merge_from_other_cfg(extracts[model_name])\n\n            # build dataset and dataloader\n            dataset = build_folder(data_args, cfg.datasets)\n            dataloader = build_loader(dataset, cfg.datasets)\n\n            # build model\n            model = build_model(cfg.model)\n\n            # build helper and extract features\n            extract_helper = build_extract_helper(model, cfg.extract)\n            extract_helper.do_extract(dataloader, save_path=os.path.join(args.save_path, feature_full_name))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
search/reid_search_index.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport importlib\nimport os\nimport argparse\n\nfrom utils.misc import check_result_exist, get_dir, get_default_result_dict\n\nfrom pyretri.config import get_defaults_cfg\nfrom pyretri.index import build_index_helper, feature_loader\nfrom pyretri.evaluate import build_evaluate_helper\n\n\nfea_names = [""output""]\n\n\ndef load_datasets():\n    datasets = {\n        ""market_gallery"": {\n            ""gallery"": ""market_gallery"",\n            ""query"": ""market_query"",\n            ""train"": ""market_gallery""\n        },\n        ""duke_gallery"": {\n            ""gallery"": ""duke_gallery"",\n            ""query"": ""duke_query"",\n            ""train"": ""duke_gallery""\n        },\n    }\n    return datasets\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'--fea_dir\', \'-fd\', default=None, type=str, help=""path of feature dirs"", required=True)\n    parser.add_argument(""--search_modules"", ""-sm"", default=None, type=str, help=""name of search module\'s directory"")\n    parser.add_argument(""--save_path"", ""-sp"", default=None, type=str, help=""path for saving results"")\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    # init args\n    args = parse_args()\n    assert args.fea_dir is not None, \'the feature directory must be provided!\'\n    assert args.search_modules is not None, \'the search modules must be provided!\'\n    assert args.save_path is not None, \'the save path must be provided!\'\n\n    # init retrieval pipeline settings\n    cfg = get_defaults_cfg()\n\n    # load search space\n    datasets = load_datasets()\n    indexes = importlib.import_module(""{}.index_dict"".format(args.search_modules)).indexes\n    evaluates = importlib.import_module(""{}.index_dict"".format(args.search_modules)).evaluates\n\n    if os.path.exists(args.save_path):\n        with open(args.save_path, ""r"") as f:\n            results = json.load(f)\n    else:\n        results = list()\n\n    for dir in os.listdir(args.fea_dir):\n        for data_name, data_args in datasets.items():\n            for index_name, index_args in indexes.items():\n                if data_name in dir:\n                    print(dir)\n\n                    # get dirs\n                    gallery_fea_dir, query_fea_dir, train_fea_dir = get_dir(args.fea_dir, dir, data_args)\n\n                    # get evaluate setting\n                    evaluate_args = evaluates[""reid_overall""]\n\n                    for dim_proc in index_args.dim_processors.names:\n                        if dim_proc in [""PartPCA"", ""PartSVD"", ""PCA"", ""SVD""]:\n                            index_args.dim_processors[dim_proc].train_fea_dir = train_fea_dir\n\n                    for fea_name in fea_names:\n                        result_dict = get_default_result_dict(dir, data_name, index_name, fea_name)\n                        if check_result_exist(result_dict, results):\n                            print(""[Search Query]: config exists..."")\n                            continue\n\n                        # load retrieval pipeline settings\n                        index_args.feature_names = [fea_name]\n                        cfg.index.merge_from_other_cfg(index_args)\n                        cfg.evaluate.merge_from_other_cfg(evaluate_args)\n\n                        # load features\n                        query_fea, query_info, _ = feature_loader.load(query_fea_dir, [fea_name])\n                        gallery_fea, gallery_info, _ = feature_loader.load(gallery_fea_dir, [fea_name])\n\n                        # build helper and index features\n                        index_helper = build_index_helper(cfg.index)\n                        index_result_info, _, _ = index_helper.do_index(query_fea, query_info, gallery_fea)\n\n                        # build helper and evaluate results\n                        evaluate_helper = build_evaluate_helper(cfg.evaluate)\n                        mAP, recall_at_k = evaluate_helper.do_eval(index_result_info, gallery_info)\n\n                        # record results\n                        to_save_recall = dict()\n                        for k in recall_at_k:\n                            to_save_recall[str(k)] = recall_at_k[k]\n                        result_dict[""mAP""] = float(mAP)\n                        result_dict[""recall_at_k""] = to_save_recall\n                        results.append(result_dict)\n\n                        # save results\n                        with open(args.save_path, ""w"") as f:\n                            json.dump(results, f)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
search/search_extract.py,0,"b'# -*- coding: utf-8 -*-\n\n\nimport os\n\nimport argparse\nimport importlib\n\nfrom pyretri.config import get_defaults_cfg\nfrom pyretri.datasets import build_folder, build_loader\nfrom pyretri.models import build_model\nfrom pyretri.extract import build_extract_helper\n\n\ndef load_datasets():\n    data_json_dir = ""/home/songrenjie/projects/RetrievalToolBox/new_data_jsons/""\n    datasets = {\n        ""oxford_gallery"": os.path.join(data_json_dir, ""oxford_gallery.json""),\n        ""oxford_query"": os.path.join(data_json_dir, ""oxford_query.json""),\n        ""cub_gallery"": os.path.join(data_json_dir, ""cub_gallery.json""),\n        ""cub_query"": os.path.join(data_json_dir, ""cub_query.json""),\n        ""indoor_gallery"": os.path.join(data_json_dir, ""indoor_gallery.json""),\n        ""indoor_query"": os.path.join(data_json_dir, ""indoor_query.json""),\n        ""caltech_gallery"": os.path.join(data_json_dir, ""caltech_gallery.json""),\n        ""caltech_query"": os.path.join(data_json_dir, ""caltech_query.json""),\n        ""paris_all"": os.path.join(data_json_dir, ""paris.json""),\n    }\n    for data_path in datasets.values():\n        assert os.path.exists(data_path), ""non-exist dataset path {}"".format(data_path)\n    return datasets\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument(\'--save_path\', \'-sp\', default=None, type=str, help=""save path for feature"")\n    parser.add_argument(""--search_modules"", ""-sm"", default=None, type=str, help=""name of search module\'s directory"")\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n    assert args.save_path is not None, \'the save path must be provided!\'\n    assert args.search_modules is not None, \'the search modules must be provided!\'\n\n    # init retrieval pipeline settings\n    cfg = get_defaults_cfg()\n\n    # load search space\n    datasets = load_datasets()\n    pre_processes = importlib.import_module(""{}.pre_process_dict"".format(args.search_modules)).pre_processes\n    models = importlib.import_module(""{}.extract_dict"".format(args.search_modules)).models\n    extracts = importlib.import_module(""{}.extract_dict"".format(args.search_modules)).extracts\n\n    # search in an exhaustive way\n    for data_name, data_args in datasets.items():\n        for pre_proc_name, pre_proc_args in pre_processes.items():\n            for model_name, model_args in models.items():\n\n                feature_full_name = data_name + ""_"" + pre_proc_name + ""_"" + model_name\n                print(feature_full_name)\n\n                if os.path.exists(os.path.join(args.save_path, feature_full_name)):\n                    print(""[Search Extract]: config exists..."")\n                    continue\n\n                # load retrieval pipeline settings\n                cfg.datasets.merge_from_other_cfg(pre_proc_args)\n                cfg.model.merge_from_other_cfg(model_args)\n                cfg.extract.merge_from_other_cfg(extracts[model_name])\n\n                # build dataset and dataloader\n                dataset = build_folder(data_args, cfg.datasets)\n                dataloader = build_loader(dataset, cfg.datasets)\n\n                # build model\n                model = build_model(cfg.model)\n\n                # build helper and extract features\n                extract_helper = build_extract_helper(model, cfg.extract)\n                extract_helper.do_extract(dataloader, save_path=os.path.join(args.save_path, feature_full_name))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
search/search_index.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport importlib\nimport os\nimport argparse\n\nfrom utils.misc import check_result_exist, get_dir, get_default_result_dict\n\nfrom pyretri.config import get_defaults_cfg\nfrom pyretri.index import build_index_helper, feature_loader\nfrom pyretri.evaluate import build_evaluate_helper\n\n\n# # gap, gmp, gem, spoc, crow\n# vgg_fea = [""pool4_GAP"", ""pool4_GMP"", ""pool4_GeM"", ""pool4_SPoC"", ""pool4_Crow"",\n#            ""pool5_GAP"", ""pool5_GMP"", ""pool5_GeM"", ""pool5_SPoC"", ""pool5_Crow"",\n#            ""fc""]\n# res_fea = [""pool3_GAP"", ""pool3_GMP"", ""pool3_GeM"", ""pool3_SPoC"", ""pool4_Crow"",\n#            ""pool4_GAP"", ""pool4_GMP"", ""pool4_GeM"", ""pool4_SPoC"", ""pool4_Crow"",\n#            ""pool5_GAP"", ""pool5_GMP"", ""pool5_GeM"", ""pool5_SPoC"", ""pool5_Crow""]\n\n# # scda, rmca\n# vgg_fea = [""pool5_SCDA"", ""pool5_RMAC""]\n# res_fea = [""pool5_SCDA"", ""pool5_RMAC""]\n\n# pwa\nvgg_fea = [""pool5_PWA""]\nres_fea = [""pool5_PWA""]\n\n\ndef load_datasets():\n    datasets = {\n        ""oxford_gallery"": {\n            ""gallery"": ""oxford_gallery"",\n            ""query"": ""oxford_query"",\n            ""train"": ""paris_all""\n        },\n        ""cub_gallery"": {\n            ""gallery"": ""cub_gallery"",\n            ""query"": ""cub_query"",\n            ""train"": ""cub_gallery""\n        },\n        ""indoor_gallery"": {\n            ""gallery"": ""indoor_gallery"",\n            ""query"": ""indoor_query"",\n            ""train"": ""indoor_gallery""\n        },\n        ""caltech_gallery"": {\n            ""gallery"": ""caltech_gallery"",\n            ""query"": ""caltech_query"",\n            ""train"": ""caltech_gallery""\n        }\n    }\n    return datasets\n\n\ndef get_evaluate(fea_dir, evaluates):\n    if ""oxford"" in fea_dir:\n        evaluate = evaluates[""oxford_overall""]\n    else:\n        evaluate = evaluates[""overall""]\n    return evaluate\n\n\ndef get_fea_names(fea_dir):\n    if ""vgg"" in fea_dir:\n        fea_names = vgg_fea\n    else:\n        fea_names = res_fea\n    return fea_names\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'--fea_dir\', \'-fd\', default=None, type=str, help=""path of feature dirs"", required=True)\n    parser.add_argument(""--search_modules"", ""-sm"", default=None, type=str, help=""name of search module\'s directory"")\n    parser.add_argument(""--save_path"", ""-sp"", default=None, type=str, help=""path for saving results"")\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    # init args\n    args = parse_args()\n    assert args.fea_dir is not None, \'the feature directory must be provided!\'\n    assert args.search_modules is not None, \'the search modules must be provided!\'\n    assert args.save_path is not None, \'the save path must be provided!\'\n\n    # init retrieval pipeline settings\n    cfg = get_defaults_cfg()\n\n    # load search space\n    datasets = load_datasets()\n    indexes = importlib.import_module(""{}.index_dict"".format(args.search_modules)).indexes\n    evaluates = importlib.import_module(""{}.index_dict"".format(args.search_modules)).evaluates\n\n    if os.path.exists(args.save_path):\n        with open(args.save_path, ""r"") as f:\n            results = json.load(f)\n    else:\n        results = list()\n\n    for dir in os.listdir(args.fea_dir):\n        for data_name, data_args in datasets.items():\n            for index_name, index_args in indexes.items():\n                if data_name in dir:\n                    print(dir)\n\n                    # get dirs\n                    gallery_fea_dir, query_fea_dir, train_fea_dir = get_dir(args.fea_dir, dir, data_args)\n\n                    # get evaluate setting\n                    evaluate_args = get_evaluate(gallery_fea_dir, evaluates)\n\n                    # get feature names\n                    fea_names = get_fea_names(gallery_fea_dir)\n\n                    # set train feature path for dimension reduction processes\n                    for dim_proc in index_args.dim_processors.names:\n                        if dim_proc in [""PartPCA"", ""PartSVD"", ""PCA"", ""SVD""]:\n                            index_args.dim_processors[dim_proc].train_fea_dir = train_fea_dir\n\n                    for fea_name in fea_names:\n                        result_dict = get_default_result_dict(dir, data_name, index_name, fea_name)\n                        if check_result_exist(result_dict, results):\n                            print(""[Search Query]: config exists..."")\n                            continue\n\n                        # load retrieval pipeline settings\n                        index_args.feature_names = [fea_name]\n                        cfg.index.merge_from_other_cfg(index_args)\n                        cfg.evaluate.merge_from_other_cfg(evaluate_args)\n\n                        # load features\n                        query_fea, query_info, _ = feature_loader.load(query_fea_dir, [fea_name])\n                        gallery_fea, gallery_info, _ = feature_loader.load(gallery_fea_dir, [fea_name])\n\n                        # build helper and index features\n                        index_helper = build_index_helper(cfg.index)\n                        index_result_info, _, _ = index_helper.do_index(query_fea, query_info, gallery_fea)\n\n                        # build helper and evaluate results\n                        evaluate_helper = build_evaluate_helper(cfg.evaluate)\n                        mAP, recall_at_k = evaluate_helper.do_eval(index_result_info, gallery_info)\n\n                        # record results\n                        to_save_recall = dict()\n                        for k in recall_at_k:\n                            to_save_recall[str(k)] = recall_at_k[k]\n                        result_dict[""mAP""] = float(mAP)\n                        result_dict[""recall_at_k""] = to_save_recall\n                        results.append(result_dict)\n\n                        # save results\n                        with open(args.save_path, ""w"") as f:\n                            json.dump(results, f)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
search/search_pwa_extract.py,0,"b'# -*- coding: utf-8 -*-\n\n\nimport os\n\nimport argparse\nimport importlib\n\nfrom pyretri.config import get_defaults_cfg\nfrom pyretri.datasets import build_folder, build_loader\nfrom pyretri.models import build_model\nfrom pyretri.extract import build_extract_helper\n\n\ndef load_datasets():\n    data_json_dir = ""/home/songrenjie/projects/RetrievalToolBox/new_data_jsons/""\n    datasets = {\n        ""oxford_gallery"": os.path.join(data_json_dir, ""oxford_gallery.json""),\n        ""oxford_query"": os.path.join(data_json_dir, ""oxford_query.json""),\n        ""cub_gallery"": os.path.join(data_json_dir, ""cub_gallery.json""),\n        ""cub_query"": os.path.join(data_json_dir, ""cub_query.json""),\n        ""indoor_gallery"": os.path.join(data_json_dir, ""indoor_gallery.json""),\n        ""indoor_query"": os.path.join(data_json_dir, ""indoor_query.json""),\n        ""caltech_gallery"": os.path.join(data_json_dir, ""caltech_gallery.json""),\n        ""caltech_query"": os.path.join(data_json_dir, ""caltech_query.json""),\n        ""paris_all"": os.path.join(data_json_dir, ""paris.json""),\n    }\n    for data_path in datasets.values():\n        assert os.path.exists(data_path), ""non-exist dataset path {}"".format(data_path)\n    return datasets\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument(\'--save_path\', \'-sp\', default=None, type=str, help=""the save path for feature"")\n    parser.add_argument(""--search_modules"", ""-sm"", default="""", type=str, help=""name of search module\'s directory"")\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n\n    # init args\n    args = parse_args()\n\n    # init retrieval pipeline settings\n    cfg = get_defaults_cfg()\n\n    # load search space\n    datasets = load_datasets()\n    pre_processes = importlib.import_module(""{}.pre_process_dict"".format(args.search_modules)).pre_processes\n    models = importlib.import_module(""{}.extract_dict"".format(args.search_modules)).models\n    extracts = importlib.import_module(""{}.extract_dict"".format(args.search_modules)).extracts\n\n    for data_name, data_args in datasets.items():\n        for pre_proc_name, pre_proc_args in pre_processes.items():\n            for model_name, model_args in models.items():\n\n                feature_full_name = data_name + ""_"" + pre_proc_name + ""_"" + model_name\n                print(feature_full_name)\n\n                if os.path.exists(os.path.join(args.save_path, feature_full_name)):\n                    print(""[Search Extract]: config exists..."")\n                    continue\n\n                # load retrieval pipeline settings\n                cfg.datasets.merge_from_other_cfg(pre_proc_args)\n                cfg.model.merge_from_other_cfg(model_args)\n                cfg.extract.merge_from_other_cfg(extracts[model_name])\n\n                # set train feature path for pwa\n                pwa_train_fea_dir = os.path.join(""/data/features/test_gap_gmp_gem_crow_spoc"", feature_full_name)\n                if ""query"" in pwa_train_fea_dir:\n                    pwa_train_fea_dir.replace(""query"", ""gallery"")\n                elif ""paris"" in pwa_train_fea_dir:\n                    pwa_train_fea_dir.replace(""paris"", ""oxford_gallery"")\n                print(""[PWA Extractor]: train feature: {}"".format(pwa_train_fea_dir))\n                cfg.extract.aggregators.PWA.train_fea_dir = pwa_train_fea_dir\n\n                # build dataset and dataloader\n                dataset = build_folder(data_args, cfg.datasets)\n                dataloader = build_loader(dataset, cfg.datasets)\n\n                # build model\n                model = build_model(cfg.model)\n\n                # build helper and extract features\n                extract_helper = build_extract_helper(model, cfg.extract)\n                extract_helper.do_extract(dataloader, save_path=os.path.join(args.save_path, feature_full_name))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
search/show_search_results.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\nimport json\n\nimport codecs\n\nfrom utils.misc import save_to_csv, filter_by_keywords\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'A tool box for deep learning-based image retrieval\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER)\n    parser.add_argument(\'--results_json_path\', \'-r\', default=None, type=str, help=""path of the result json"")\n    args = parser.parse_args()\n\n    return args\n\n\ndef show_results(results):\n    for i in range(len(results)):\n        print(results[i])\n\n\ndef main():\n    # init args\n    args = parse_args()\n    assert os.path.exists(args.results_json_path), \'the config file must be existed!\'\n\n    with open(args.results_json_path, ""r"") as f:\n        results = json.load(f)\n\n    # save the search results in a csv format file.\n    csv_path = \'/home/songrenjie/projects/RetrievalToolBox/test.csv\'\n    save_to_csv(results, csv_path)\n\n    # define the keywords to be selected\n    keywords = {\n        \'data_name\': [\'market\'],\n        \'pre_process_name\': list(),\n        \'model_name\': list(),\n        \'feature_map_name\': list(),\n        \'aggregator_name\': list(),\n        \'post_process_name\': [\'no_fea_process\', \'l2_normalize\', \'pca_whiten\', \'pca_wo_whiten\'],\n    }\n\n    # show search results according to the given keywords\n    results = filter_by_keywords(results, keywords)\n    show_results(results)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pyretri/config/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .config import setup_cfg, get_defaults_cfg\n\n\n__all__ = [\n    'get_defaults_cfg',\n    'setup_cfg',\n]\n"""
pyretri/config/config.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom ..datasets import get_datasets_cfg\nfrom ..models import get_model_cfg\nfrom ..extract import get_extract_cfg\nfrom ..index import get_index_cfg\nfrom ..evaluate import get_evaluate_cfg\n\n\ndef get_defaults_cfg() -> CfgNode:\n    """"""\n    Construct the default configuration tree.\n\n    Returns:\n        cfg (CfgNode): the default configuration tree.\n    """"""\n    cfg = CfgNode()\n\n    cfg[""datasets""] = get_datasets_cfg()\n    cfg[""model""] = get_model_cfg()\n    cfg[""extract""] = get_extract_cfg()\n    cfg[""index""] = get_index_cfg()\n    cfg[""evaluate""] = get_evaluate_cfg()\n\n    return cfg\n\n\ndef setup_cfg(cfg: CfgNode, cfg_file: str, cfg_opts: list or None = None) -> CfgNode:\n    """"""\n    Load a yaml config file and merge it this CfgNode.\n\n    Args:\n        cfg (CfgNode): the configuration tree with default structure.\n        cfg_file (str): the path for yaml config file which is matched with the CfgNode.\n        cfg_opts (list, optional): config (keys, values) in a list (e.g., from command line) into this CfgNode.\n\n    Returns:\n        cfg (CfgNode): the configuration tree with settings in the config file.\n    """"""\n    cfg.merge_from_file(cfg_file)\n    cfg.merge_from_list(cfg_opts)\n    cfg.freeze()\n\n    return cfg\n'"
pyretri/datasets/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .builder import build_collate, build_folder, build_transformers, build_loader\n\nfrom .config import get_datasets_cfg\n\n__all__ = [\n    'get_datasets_cfg',\n    'build_collate', 'build_folder', 'build_transformers', 'build_loader',\n]\n"""
pyretri/datasets/builder.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import COLLATEFNS, FOLDERS, TRANSFORMERS\nfrom .collate_fn import CollateFnBase\nfrom .folder import FolderBase\nfrom .transformer import TransformerBase\n\nfrom ..utils import simple_build\n\nfrom torch.utils.data import DataLoader\n\nfrom torchvision.transforms import Compose\n\n\ndef build_collate(cfg: CfgNode) -> CollateFnBase:\n    """"""\n    Instantiate a collate class with the given configuration tree.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        collate (CollateFnBase): a collate class.\n    """"""\n    name = cfg[""name""]\n    collate = simple_build(name, cfg, COLLATEFNS)\n    return collate\n\n\ndef build_transformers(cfg: CfgNode) -> Compose:\n    """"""\n    Instantiate a compose class containing several transforms with the given configuration tree.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        transformers (Compose): a compose class.\n    """"""\n    names = cfg[""names""]\n    transformers = list()\n    for name in names:\n        transformers.append(simple_build(name, cfg, TRANSFORMERS))\n    transformers = Compose(transformers)\n    return transformers\n\n\ndef build_folder(data_json_path: str, cfg: CfgNode) -> FolderBase:\n    """"""\n    Instantiate a folder class with the given configuration tree.\n\n    Args:\n        data_json_path (str): the path of the data json file.\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        folder (FolderBase): a folder class.\n    """"""\n    trans = build_transformers(cfg.transformers)\n    folder = simple_build(cfg.folder[""name""], cfg.folder, FOLDERS, data_json_path=data_json_path, transformer=trans)\n    return folder\n\n\ndef build_loader(folder: FolderBase, cfg: CfgNode) -> DataLoader:\n    """"""\n    Instantiate a data loader class with the given configuration tree.\n\n    Args:\n        folder (FolderBase): the folder function.\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        data_loader (DataLoader): a data loader class.\n    """"""\n    co_fn = build_collate(cfg.collate_fn)\n\n    data_loader = DataLoader(folder, cfg[""batch_size""], collate_fn=co_fn, num_workers=8, pin_memory=True)\n\n    return data_loader\n'"
pyretri/datasets/config.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import COLLATEFNS, FOLDERS, TRANSFORMERS\n\nfrom ..utils import get_config_from_registry\n\ndef get_collate_cfg() -> CfgNode:\n    cfg = get_config_from_registry(COLLATEFNS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\n\ndef get_folder_cfg() -> CfgNode:\n    cfg = get_config_from_registry(FOLDERS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\n\ndef get_tranformers_cfg() -> CfgNode:\n    cfg = get_config_from_registry(TRANSFORMERS)\n    cfg[""names""] = [""unknown""]\n    return cfg\n\n\ndef get_datasets_cfg() -> CfgNode:\n    cfg = CfgNode()\n    cfg[""collate_fn""] = get_collate_cfg()\n    cfg[""folder""] = get_folder_cfg()\n    cfg[""transformers""] = get_tranformers_cfg()\n    cfg[""batch_size""] = 1\n    return cfg\n'"
pyretri/datasets/registry.py,0,b'# -*- coding: utf-8 -*-\n\nfrom ..utils.registry import Registry\n\nCOLLATEFNS = Registry()\nFOLDERS = Registry()\nTRANSFORMERS = Registry()\n'
pyretri/evaluate/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .config import get_evaluate_cfg\nfrom .builder import build_evaluate_helper\n\n\n__all__ = [\n    'get_evaluate_cfg',\n    'build_evaluate_helper',\n]\n"""
pyretri/evaluate/builder.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import EVALUATORS\nfrom .evaluator import EvaluatorBase\nfrom .helper import EvaluateHelper\n\nfrom ..utils import simple_build\n\n\ndef build_evaluator(cfg: CfgNode) -> EvaluatorBase:\n    """"""\n    Instantiate a evaluator class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        evaluator (EvaluatorBase): a evaluator class.\n    """"""\n    name = cfg[""name""]\n    evaluator = simple_build(name, cfg, EVALUATORS)\n    return evaluator\n\n\ndef build_evaluate_helper(cfg: CfgNode) -> EvaluateHelper:\n    """"""\n    Instantiate a evaluate helper class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        helper (EvaluateHelper): a evaluate helper class.\n    """"""\n    evaluator = build_evaluator(cfg.evaluator)\n    helper = EvaluateHelper(evaluator)\n    return helper\n'"
pyretri/evaluate/config.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import EVALUATORS\n\nfrom ..utils import get_config_from_registry\n\n\ndef get_evaluator_cfg() -> CfgNode:\n    cfg = get_config_from_registry(EVALUATORS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\ndef get_evaluate_cfg() -> CfgNode:\n    cfg = CfgNode()\n    cfg[""evaluator""] = get_evaluator_cfg()\n    return cfg\n'"
pyretri/evaluate/registry.py,0,b'# -*- coding: utf-8 -*-\n\nfrom ..utils import Registry\n\nEVALUATORS = Registry()\n'
pyretri/extract/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .config import get_extractor_cfg, get_aggregators_cfg, get_extract_cfg\nfrom .builder import build_aggregators, build_extractor, build_extract_helper\n\nfrom .utils import split_dataset, make_data_json\n\n\n__all__ = [\n    'get_extract_cfg',\n    'build_aggregators', 'build_extractor', 'build_extract_helper',\n    'split_dataset', 'make_data_json',\n]\n"""
pyretri/extract/builder.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import AGGREGATORS, SPLITTERS, EXTRACTORS\nfrom .extractor import ExtractorBase\nfrom .splitter import SplitterBase\nfrom .aggregator import AggregatorBase\nfrom .helper import ExtractHelper\n\nfrom ..utils import simple_build\n\nimport torch.nn as nn\n\nfrom typing import List\n\n\ndef build_aggregators(cfg: CfgNode) -> List[AggregatorBase]:\n    """"""\n    Instantiate a list of aggregator classes.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        aggregators (list): a list of instances of aggregator class.\n    """"""\n    names = cfg[""names""]\n    aggregators = list()\n    for name in names:\n        aggregators.append(simple_build(name, cfg, AGGREGATORS))\n    return aggregators\n\n\ndef build_extractor(model: nn.Module, cfg: CfgNode) -> ExtractorBase:\n    """"""\n    Instantiate a extractor class.\n\n    Args:\n        model (nn.Module): the model for extracting features.\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        extractor (ExtractorBase): an instance of extractor class.\n    """"""\n    name = cfg[""name""]\n    extractor = simple_build(name, cfg, EXTRACTORS, model=model)\n    return extractor\n\n\ndef build_splitter(cfg: CfgNode) -> SplitterBase:\n    """"""\n    Instantiate a splitter class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        splitter (SplitterBase): an instance of splitter class.\n    """"""\n    name = cfg[""name""]\n    splitter = simple_build(name, cfg, SPLITTERS)\n    return splitter\n\n\ndef build_extract_helper(model: nn.Module, cfg: CfgNode) -> ExtractHelper:\n    """"""\n    Instantiate a extract helper class.\n\n    Args:\n        model (nn.Module): the model for extracting features.\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        helper (ExtractHelper): an instance of extract helper class.\n    """"""\n    assemble = cfg.assemble\n    extractor = build_extractor(model, cfg.extractor)\n    splitter = build_splitter(cfg.splitter)\n    aggregators = build_aggregators(cfg.aggregators)\n    helper = ExtractHelper(assemble, extractor, splitter, aggregators)\n    return helper\n\n'"
pyretri/extract/config.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import EXTRACTORS, SPLITTERS, AGGREGATORS\n\nfrom ..utils import get_config_from_registry\n\n\ndef get_aggregators_cfg() -> CfgNode:\n    cfg = get_config_from_registry(AGGREGATORS)\n    cfg[""names""] = list()\n    return cfg\n\n\ndef get_splitter_cfg() -> CfgNode:\n    cfg = get_config_from_registry(SPLITTERS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\n\ndef get_extractor_cfg() -> CfgNode:\n    cfg = get_config_from_registry(EXTRACTORS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\n\ndef get_extract_cfg() -> CfgNode:\n    cfg = CfgNode()\n    cfg[""assemble""] = 0\n    cfg[""extractor""] = get_extractor_cfg()\n    cfg[""splitter""] = get_splitter_cfg()\n    cfg[""aggregators""] = get_aggregators_cfg()\n    return cfg\n\n'"
pyretri/extract/registry.py,0,b'# -*- coding: utf-8 -*-\n\nfrom ..utils.registry import Registry\n\nEXTRACTORS = Registry()\nSPLITTERS = Registry()\nAGGREGATORS = Registry()\n'
pyretri/index/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .config import get_index_cfg\n\nfrom .builder import build_index_helper\n\nfrom .utils import feature_loader\n\n\n__all__ = [\n    'get_index_cfg',\n    'build_index_helper',\n    'feature_loader',\n]\n"""
pyretri/index/builder.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import ENHANCERS, METRICS, DIMPROCESSORS, RERANKERS\nfrom .feature_enhancer import EnhanceBase\nfrom .helper import IndexHelper\nfrom .metric import MetricBase\nfrom .dim_processor import DimProcessorBase\nfrom .re_ranker import ReRankerBase\n\nfrom ..utils import simple_build\n\nfrom typing import List\n\n\ndef build_enhance(cfg: CfgNode) -> EnhanceBase:\n    """"""\n    Instantiate a feature enhancer class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        enhance (EnhanceBase): an instance of feature enhancer class.\n    """"""\n    name = cfg[""name""]\n    enhance = simple_build(name, cfg, ENHANCERS)\n    return enhance\n\n\ndef build_metric(cfg: CfgNode) -> MetricBase:\n    """"""\n    Instantiate a metric class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        metric (MetricBase): an instance of metric class.\n    """"""\n    name = cfg[""name""]\n    metric = simple_build(name, cfg, METRICS)\n    return metric\n\n\ndef build_processors(feature_names: List[str], cfg: CfgNode) -> DimProcessorBase:\n    """"""\n    Instantiate a list of dimension processor classes.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        processors (list): a list of instances of dimension process class.\n    """"""\n    names = cfg[""names""]\n    processors = list()\n    for name in names:\n        processors.append(simple_build(name, cfg, DIMPROCESSORS, feature_names=feature_names))\n    return processors\n\n\ndef build_ranker(cfg: CfgNode) -> ReRankerBase:\n    """"""\n    Instantiate a re-ranker class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        re_rank (list): an instance of re-ranker class.\n    """"""\n    name = cfg[""name""]\n    re_rank = simple_build(name, cfg, RERANKERS)\n    return re_rank\n\n\ndef build_index_helper(cfg: CfgNode) -> IndexHelper:\n    """"""\n    Instantiate a index helper class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        helper (IndexHelper): an instance of index helper class.\n    """"""\n    dim_processors = build_processors(cfg[""feature_names""], cfg.dim_processors)\n    metric = build_metric(cfg.metric)\n    feature_enhancer = build_enhance(cfg.feature_enhancer)\n    re_ranker = build_ranker(cfg.re_ranker)\n    helper = IndexHelper(dim_processors, feature_enhancer, metric, re_ranker)\n    return helper\n'"
pyretri/index/config.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .registry import ENHANCERS, METRICS, DIMPROCESSORS, RERANKERS\n\nfrom ..utils import get_config_from_registry\n\n\ndef get_enhancer_cfg() -> CfgNode:\n    cfg = get_config_from_registry(ENHANCERS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\n\ndef get_metric_cfg() -> CfgNode:\n    cfg = get_config_from_registry(METRICS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\n\ndef get_processors_cfg() -> CfgNode:\n    cfg = get_config_from_registry(DIMPROCESSORS)\n    cfg[""names""] = [""unknown""]\n    return cfg\n\n\ndef get_ranker_cfg() -> CfgNode:\n    cfg = get_config_from_registry(RERANKERS)\n    cfg[""name""] = ""unknown""\n    return cfg\n\n\ndef get_index_cfg() -> CfgNode:\n    cfg = CfgNode()\n    cfg[""query_fea_dir""] = ""unknown""\n    cfg[""gallery_fea_dir""] = ""unknown""\n    cfg[""feature_names""] = [""all""]\n    cfg[""dim_processors""] = get_processors_cfg()\n    cfg[""feature_enhancer""] = get_enhancer_cfg()\n    cfg[""metric""] = get_metric_cfg()\n    cfg[""re_ranker""] = get_ranker_cfg()\n    return cfg\n'"
pyretri/index/registry.py,0,b'# -*- coding: utf-8 -*-\n\nfrom ..utils import Registry\n\nfrom typing import Dict\n\n\nENHANCERS = Registry()\nMETRICS = Registry()\nDIMPROCESSORS = Registry()\nRERANKERS = Registry()\n'
pyretri/models/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .config import get_model_cfg\n\nfrom .builder import build_model\n\n\n__all__ = [\n    'get_model_cfg',\n    'build_model',\n]\n"""
pyretri/models/builder.py,19,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nimport torch\nimport torch.nn as nn\n\nfrom .registry import BACKBONES\n\nfrom ..utils import load_state_dict\nfrom torchvision.models.utils import load_state_dict_from_url\n\n\n# the urls for pre-trained models in torchvision.\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'resnext50_32x4d\': \'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\',\n    \'resnext101_32x8d\': \'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\',\n    \'wide_resnet50_2\': \'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\',\n    \'wide_resnet101_2\': \'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\',\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\ndef build_model(cfg: CfgNode) -> nn.Module:\n    """"""\n    Instantiate a backbone class.\n\n    Args:\n        cfg (CfgNode): the configuration tree.\n\n    Returns:\n        model (nn.Module): the model for extracting features.\n    """"""\n    name = cfg[""name""]\n    model = BACKBONES.get(name)()\n\n    load_checkpoint = cfg[cfg.name][""load_checkpoint""]\n    if \'torchvision\' in load_checkpoint:\n        arch = load_checkpoint.split(\'://\')[-1]\n        state_dict = load_state_dict_from_url(model_urls[arch], progress=True)\n    else:\n        state_dict = torch.load(load_checkpoint)\n\n    try:\n        model.load_state_dict(state_dict, strict=False)\n    except:\n        load_state_dict(model, state_dict)\n\n    return model\n'"
pyretri/models/config.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\n# from .registry import BACKBONES\nfrom .backbone.backbone_base import BACKBONES\n\n\ndef get_model_cfg() -> CfgNode:\n    cfg = CfgNode()\n    for name in BACKBONES:\n        cfg[name] = CfgNode()\n        cfg[name][""load_checkpoint""] = """"\n    cfg[""name""] = ""unknown""\n    return cfg\n'"
pyretri/models/registry.py,0,b'# -*- coding: utf-8 -*-\n\nfrom ..utils import Registry\n\nBACKBONES = Registry()\n'
pyretri/utils/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .builder import get_config_from_registry, simple_build\nfrom .misc import ensure_dir, load_state_dict\nfrom .module_base import ModuleBase\nfrom .registry import Registry\n\n\n__all__ = [\n    'ModuleBase', 'Registry',\n    'get_config_from_registry', 'simple_build', 'ensure_dir', 'load_state_dict',\n]\n"""
pyretri/utils/builder.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .module_base import ModuleBase\nfrom .registry import Registry\n\n\ndef get_config_from_registry(registry: Registry) -> CfgNode:\n    """"""\n    Collect all hyper-parameters from modules in registry.\n\n    Args:\n        registry (Registry): module registry.\n\n    Returns:\n        cfg (CfgNode): configurations for this registry.\n    """"""\n    cfg = CfgNode()\n    for name in registry:\n        cfg[name] = CfgNode()\n        loss = registry[name]\n        hps = loss.default_hyper_params\n        for hp_name in hps:\n            cfg[name][hp_name] = hps[hp_name]\n    return cfg\n\n\ndef simple_build(name: str, cfg: CfgNode, registry: Registry, **kwargs):\n    """"""\n    Simply build a module according to name and hyper-parameters.\n\n    Args:\n        name (str): name for instance to be built.\n        cfg (CfgNode): configurations for this sub-module.\n        registry (Registry): registry for this sub-module.\n        **kwargs: keyword arguments.\n\n    Returns:\n        module: a initialized instance\n    """"""\n    assert name in registry\n    module = registry[name]\n    hps = module.default_hyper_params\n\n    for hp_name in hps:\n        new_value = cfg[name][hp_name]\n        hps[hp_name] = new_value\n\n    return module(hps=hps, **kwargs)\n'"
pyretri/utils/misc.py,3,"b'# -*- coding: utf-8 -*-\n\nimport os\n\nimport torch.nn as nn\nfrom torch.nn import Parameter\n\nfrom torchvision.models.utils import load_state_dict_from_url\n\nfrom typing import Dict\n\ndef ensure_dir(path: str) -> None:\n    """"""\n    Check if a directory exists, if not, create a new one.\n\n    Args:\n        path (str): the path of the directory.\n    """"""\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef load_state_dict(model: nn.Module, state_dict: Dict) -> None:\n    """"""\n    Load parameters regardless the shape of parameters with the same name need to match,\n    which is a slight modification to load_state_dict of pytorch.\n\n    Args:\n        model (nn.Module): the model for extracting features.\n        state_dict (Dict): a dict of model parameters.\n    """"""\n    own_state = model.state_dict()\n    success_keys = list()\n    for name, param in state_dict.items():\n        if name in own_state:\n            if isinstance(param, Parameter):\n                # backwards compatibility for serialized parameters\n                param = param.data\n            try:\n                own_state[name].copy_(param)\n                success_keys.append(name)\n            except Exception:\n                print(""[LoadStateDict]: shape mismatch in parameter {}, {} vs {}"".format(\n                    name, own_state[name].size(), param.size()\n                ))\n        else:\n            print(""[LoadStateDict]: "" + \'unexpected key ""{}"" in state_dict\'.format(name))\n    missing = set(own_state.keys()) - set(success_keys)\n    if len(missing) > 0:\n        print(""[LoadStateDict]: "" + ""missing keys or mismatch param in state_dict: {}"".format(missing))\n'"
pyretri/utils/module_base.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom abc import ABCMeta\nfrom copy import deepcopy\n\nfrom typing import Dict\n\nclass ModuleBase:\n    """"""\n    The base class of all classes. You can access default hyper-parameters by Class. And\n    set hyper-parameters for each instance at the initialization.\n    """"""\n    __metaclass__ = ABCMeta\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        # copy hyper_params from class attribute.\n        self._hyper_params = deepcopy(self.default_hyper_params)\n        if hps is not None:\n            self._set_hps(hps)\n\n    def __setattr__(self, key, value) -> None:\n        assert key != ""hyper_params"", ""default Hyper-Parameters can not be set in each instance""\n        self.__dict__[key] = value\n\n    def get_hps(self) -> Dict:\n        return self._hyper_params\n\n    def _set_hps(self, hps: Dict or None = None):\n        for key in hps:\n            if key not in self._hyper_params:\n                raise KeyError\n            self._hyper_params[key] = hps[key]\n'"
pyretri/utils/registry.py,0,"b'# -*- coding: utf-8 -*-\n\n\ndef _register_generic(module_dict, module_name, module):\n    assert module_name not in module_dict\n    module_dict[module_name] = module\n\n\nclass Registry(dict):\n    """"""\n    A helper class to register class.\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(Registry, self).__init__(*args, **kwargs)\n\n    def register(self, module):\n        _register_generic(self, module.__name__, module)\n        return module\n'"
search/reid_search_modules/__init__.py,0,b''
search/reid_search_modules/extract_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nmodels = SearchModules()\nextracts = SearchModules()\n\n\nmodels.add(\n    ""market_res50"",\n    {\n        ""name"": ""ft_net"",\n        ""ft_net"": {\n            ""load_checkpoint"": ""/data/my_model_zoo/res50_market1501.pth""\n        }\n    }\n)\nextracts.add(\n    ""market_res50"",\n    {\n        ""assemble"": 1,\n        ""extractor"": {\n            ""name"": ""ReIDSeries"",\n            ""ReIDSeries"": {\n                ""extract_features"": [""output""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""GAP""]\n        },\n    }\n)\n\nmodels.add(\n    ""duke_res50"",\n    {\n        ""name"": ""ft_net"",\n        ""ft_net"": {\n            ""load_checkpoint"": ""/home/songrenjie/projects/reID_baseline/model/ft_ResNet50/res50_duke.pth""\n        }\n    }\n)\nextracts.add(\n    ""duke_res50"",\n    {\n        ""assemble"": 1,\n        ""extractor"": {\n            ""name"": ""ReIDSeries"",\n            ""ReIDSeries"": {\n                ""extract_features"": [""output""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""GAP""]\n        },\n    }\n)\n\n\ncfg = get_defaults_cfg()\n\nmodels.check_valid(cfg[""model""])\nextracts.check_valid(cfg[""extract""])\n'"
search/reid_search_modules/index_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nindexes = SearchModules()\nevaluates = SearchModules()\n\nindexes.add(\n    ""no_fea_process"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""Identity""],\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""l2_normalize"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize""],\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""pca_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""pca_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nevaluates.add(\n    ""overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OverAll""\n        }\n    }\n)\n\nevaluates.add(\n    ""oxford_overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OxfordOverAll""\n        }\n    }\n)\n\nevaluates.add(\n    ""reid_overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""ReIDOverAll""\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\nindexes.check_valid(cfg[""index""])\nevaluates.check_valid(cfg[""evaluate""])\n'"
search/reid_search_modules/pre_process_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\npre_processes = SearchModules()\n\npre_processes.add(\n    ""Direct256128"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""DirectResize"", ""TwoFlip"", ""ToTensor"", ""Normalize""],\n            ""DirectResize"": {\n                ""size"": (256, 128),\n                ""interpolation"": 3\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\npre_processes.check_valid(cfg[""datasets""])\n'"
search/search_modules/__init__.py,0,b'# -*- coding: utf-8 -*-\n'
search/search_modules/extract_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nmodels = SearchModules()\nextracts = SearchModules()\n\n\nmodels.add(\n    ""imagenet_vgg16"",\n    {\n        ""name"": ""vgg16"",\n        ""vgg16"": {\n            ""load_checkpoint"": ""torchvision://vgg16""\n        }\n    }\n)\nextracts.add(\n    ""imagenet_vgg16"",\n    {\n        ""extractor"": {\n            ""name"": ""VggSeries"",\n            ""VggSeries"": {\n                ""extract_features"": [""all""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""Crow"", ""GAP"", ""GMP"", ""GeM"", ""SPoC""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""imagenet_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""torchvision://resnet50""\n        }\n    }\n)\nextracts.add(\n    ""imagenet_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""all""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""Crow"", ""GAP"", ""GMP"", ""GeM"", ""SPoC""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""places365_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""/data/places365_model/res50_places365.pt""\n        }\n    }\n)\nextracts.add(\n    ""places365_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""all""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""Crow"", ""GAP"", ""GMP"", ""GeM"", ""SPoC""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""hybrid1365_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""/data/places365_model/res50_hybrid1365.pt""\n        }\n    }\n)\nextracts.add(\n    ""hybrid1365_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""all""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""Crow"", ""GAP"", ""GMP"", ""GeM"", ""SPoC""]\n        },\n    }\n)\n\ncfg = get_defaults_cfg()\n\nmodels.check_valid(cfg[""model""])\nextracts.check_valid(cfg[""extract""])\n'"
search/search_modules/index_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nindexes = SearchModules()\nevaluates = SearchModules()\n\nindexes.add(\n    ""pca_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""pca_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nevaluates.add(\n    ""overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OverAll""\n        }\n    }\n)\n\nevaluates.add(\n    ""oxford_overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OxfordOverAll""\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\nindexes.check_valid(cfg[""index""])\nevaluates.check_valid(cfg[""evaluate""])\n'"
search/search_modules/pre_process_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\npre_processes = SearchModules()\n\npre_processes.add(\n    ""Shorter256Center224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""ShorterResize"", ""CenterCrop"", ""ToTensor"", ""Normalize""],\n            ""ShorterResize"": {\n                ""size"": 256\n            },\n            ""CenterCrop"": {\n                ""size"": 224\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\npre_processes.add(\n    ""Direct224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""DirectResize"", ""ToTensor"", ""Normalize""],\n            ""DirectResize"": {\n                ""size"": (224, 224)\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\npre_processes.add(\n    ""PadResize224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""PadResize"", ""ToTensor"", ""Normalize""],\n            ""PadResize"": {\n                ""size"": 224,\n                ""padding_v"": [124, 116, 104]\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\npre_processes.check_valid(cfg[""datasets""])\n'"
search/search_pwa_modules/__init__.py,0,b''
search/search_pwa_modules/extract_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nmodels = SearchModules()\nextracts = SearchModules()\n\n\nmodels.add(\n    ""imagenet_vgg16"",\n    {\n        ""name"": ""vgg16"",\n        ""vgg16"": {\n            ""load_checkpoint"": ""torchvision://vgg16""\n        }\n    }\n)\nextracts.add(\n    ""imagenet_vgg16"",\n    {\n        ""extractor"": {\n            ""name"": ""VggSeries"",\n            ""VggSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""PWA""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""imagenet_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""torchvision://resnet50""\n        }\n    }\n)\nextracts.add(\n    ""imagenet_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""PWA""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""places365_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""/data/places365_model/res50_places365.pt""\n        }\n    }\n)\nextracts.add(\n    ""places365_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""PWA""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""hybrid1365_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""/data/places365_model/res50_hybrid1365.pt""\n        }\n    }\n)\nextracts.add(\n    ""hybrid1365_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""PWA""]\n        },\n    }\n)\n\ncfg = get_defaults_cfg()\n\nmodels.check_valid(cfg[""model""])\nextracts.check_valid(cfg[""extract""])\n'"
search/search_pwa_modules/index_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nindexes = SearchModules()\nevaluates = SearchModules()\n\nindexes.add(\n    ""pca_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""pca_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nevaluates.add(\n    ""overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OverAll""\n        }\n    }\n)\n\nevaluates.add(\n    ""oxford_overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OxfordOverAll""\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\nindexes.check_valid(cfg[""index""])\nevaluates.check_valid(cfg[""evaluate""])\n'"
search/search_pwa_modules/pre_process_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\npre_processes = SearchModules()\n\npre_processes.add(\n    ""Shorter256Center224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""ShorterResize"", ""CenterCrop"", ""ToTensor"", ""Normalize""],\n            ""ShorterResize"": {\n                ""size"": 256\n            },\n            ""CenterCrop"": {\n                ""size"": 224\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\npre_processes.add(\n    ""Direct224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""DirectResize"", ""ToTensor"", ""Normalize""],\n            ""DirectResize"": {\n                ""size"": (224, 224)\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\npre_processes.add(\n    ""PadResize224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""PadResize"", ""ToTensor"", ""Normalize""],\n            ""PadResize"": {\n                ""size"": 224,\n                ""padding_v"": [124, 116, 104]\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\npre_processes.check_valid(cfg[""datasets""])\n'"
search/search_rmac_modules/__init__.py,0,b''
search/search_rmac_modules/extract_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nmodels = SearchModules()\nextracts = SearchModules()\n\n\nmodels.add(\n    ""imagenet_vgg16"",\n    {\n        ""name"": ""vgg16"",\n        ""vgg16"": {\n            ""load_checkpoint"": ""torchvision://vgg16""\n        }\n    }\n)\nextracts.add(\n    ""imagenet_vgg16"",\n    {\n        ""extractor"": {\n            ""name"": ""VggSeries"",\n            ""VggSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""SCDA"", ""RMAC""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""imagenet_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""torchvision://resnet50""\n        }\n    }\n)\nextracts.add(\n    ""imagenet_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""SCDA"", ""RMAC""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""places365_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""/data/places365_model/res50_places365.pt""\n        }\n    }\n)\nextracts.add(\n    ""places365_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""SCDA"", ""RMAC""]\n        },\n    }\n)\n\n\nmodels.add(\n    ""hybrid1365_res50"",\n    {\n        ""name"": ""resnet50"",\n        ""resnet50"": {\n            ""load_checkpoint"": ""/data/places365_model/res50_hybrid1365.pt""\n        }\n    }\n)\nextracts.add(\n    ""hybrid1365_res50"",\n    {\n        ""extractor"": {\n            ""name"": ""ResSeries"",\n            ""ResSeries"": {\n                ""extract_features"": [""pool5""],\n            }\n        },\n        ""splitter"": {\n            ""name"": ""Identity"",\n        },\n        ""aggregators"": {\n            ""names"": [""SCDA"", ""RMAC""]\n        },\n    }\n)\n\ncfg = get_defaults_cfg()\n\nmodels.check_valid(cfg[""model""])\nextracts.check_valid(cfg[""extract""])\n'"
search/search_rmac_modules/index_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\nindexes = SearchModules()\nevaluates = SearchModules()\n\nindexes.add(\n    ""pca_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""pca_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""PCA"", ""L2Normalize""],\n            ""PCA"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 512,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_wo_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": False,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nindexes.add(\n    ""svd_whiten"",\n    {\n        ""gallery_fea_dir"": """",\n        ""query_fea_dir"": """",\n\n        ""feature_names"": [],\n\n        ""dim_processors"": {\n            ""names"": [""L2Normalize"", ""SVD"", ""L2Normalize""],\n            ""SVD"": {\n                ""whiten"": True,\n                ""train_fea_dir"": """",\n                ""proj_dim"": 511,\n                ""l2"": True,\n            }\n        },\n\n        ""feature_enhancer"": {\n            ""name"": ""Identity""\n        },\n\n        ""metric"": {\n            ""name"": ""KNN""\n        },\n\n        ""re_ranker"": {\n            ""name"": ""Identity""\n        }\n    }\n)\n\nevaluates.add(\n    ""overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OverAll""\n        }\n    }\n)\n\nevaluates.add(\n    ""oxford_overall"",\n    {\n        ""evaluator"": {\n            ""name"": ""OxfordOverAll""\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\nindexes.check_valid(cfg[""index""])\nevaluates.check_valid(cfg[""evaluate""])\n'"
search/search_rmac_modules/pre_process_dict.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom utils.search_modules import SearchModules\nfrom pyretri.config import get_defaults_cfg\n\npre_processes = SearchModules()\n\npre_processes.add(\n    ""Shorter256Center224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""ShorterResize"", ""CenterCrop"", ""ToTensor"", ""Normalize""],\n            ""ShorterResize"": {\n                ""size"": 256\n            },\n            ""CenterCrop"": {\n                ""size"": 224\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\npre_processes.add(\n    ""Direct224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""DirectResize"", ""ToTensor"", ""Normalize""],\n            ""DirectResize"": {\n                ""size"": (224, 224)\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\npre_processes.add(\n    ""PadResize224"",\n    {\n        ""batch_size"": 32,\n        ""folder"": {\n            ""name"": ""Folder""\n        },\n        ""collate_fn"": {\n            ""name"": ""CollateFn""\n        },\n        ""transformers"": {\n            ""names"": [""PadResize"", ""ToTensor"", ""Normalize""],\n            ""PadResize"": {\n                ""size"": 224,\n                ""padding_v"": [124, 116, 104]\n            },\n            ""Normalize"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225]\n            }\n        }\n    }\n)\n\ncfg = get_defaults_cfg()\n\npre_processes.check_valid(cfg[""datasets""])\n'"
search/utils/__init__.py,0,b''
search/utils/misc.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nfrom typing import Dict, List\nimport csv\n\ndef check_result_exist(now_res: Dict, exist_results: List) -> bool:\n    """"""\n    Check if the config exists.\n\n    Args:\n        now_res (Dict): configuration to be checked.\n        exist_results (List): a list of existing configurations.\n\n    Returns:\n        bool: if the config exists.\n    """"""\n    for e_r in exist_results:\n        totoal_equal = True\n        for key in now_res:\n            if now_res[key] != e_r[key]:\n                totoal_equal = False\n                break\n        if totoal_equal:\n            return True\n    return False\n\n\ndef get_dir(root_path: str, dir: str, dataset: Dict) -> (str, str, str):\n    """"""\n    Get the feature directory path of gallery set, query set and feature set for training PCA/SVD.\n\n    Args:\n        root_path (str): the root path of all extracted features.\n        dir (str): the path of one single extracted feature directory.\n        dataset (Dict): a dict containing the information of gallery set, query set and training set.\n\n    Returns:\n        tuple(str, str, str): path of gallery set, query set and feature set for training PCA/SVD.\n    """"""\n    template_dir = os.path.join(root_path, dir)\n    target = dir.split(\'_\')[0] + \'_\' + dir.split(\'_\')[1]\n    gallery_fea_dir = template_dir.replace(target, dataset[""gallery""])\n    query_fea_dir = template_dir.replace(target, dataset[""query""])\n    train_fea_dir = template_dir.replace(target, dataset[""train""])\n    return gallery_fea_dir, query_fea_dir, train_fea_dir\n\n\ndef get_default_result_dict(dir: str, data_name: str, index_name: str, fea_name: str) -> Dict:\n    """"""\n    Get the default result dict based on the experimental factors.\n\n    Args:\n        dir (str): the path of one single extracted feature directory.\n        data_name (str): the name of the dataset.\n        index_name (str): the name of query process.\n        fea_name (str): the name of the features to be loaded.\n\n    Returns:\n        result_dict (Dict): a default configuration dict.\n    """"""\n    result_dict = {\n        ""data_name"": data_name.split(""_"")[0],\n        ""pre_process_name"": dir.split(""_"")[2],\n        ""model_name"": ""_"".join(dir.split(""_"")[-2:]),\n        ""feature_map_name"": fea_name.split(""_"")[0],\n        ""post_process_name"": index_name\n    }\n\n    if len(fea_name.split(""_"")) == 1:\n        result_dict[""aggregator_name""] = ""none""\n    else:\n        result_dict[""aggregator_name""] = fea_name.split(""_"")[1]\n\n    return result_dict\n\n\ndef save_to_csv(results: List[Dict], csv_path: str) -> None:\n    """"""\n    Save the search results in a csv format file.\n\n    Args:\n        results (List): a list of retrieval results.\n        csv_path (str): the path for saving the csv file.\n    """"""\n    start = [""data"", ""pre_process"", ""model"", ""feature_map"", ""aggregator"", ""post_process""]\n    for i in range(len(start)):\n        results = sorted(results, key=lambda result: result[start[len(start) - i - 1] + ""_name""])\n    start.append(\'mAP\')\n    start.append(\'Recall@1\')\n\n    with open(csv_path, \'w\') as f:\n        csv_write = csv.writer(f)\n        if len(start) > 0:\n            csv_write.writerow(start)\n        for i in range(len(results)):\n            data_row = [0 for x in range(len(start))]\n            data_row[0] = results[i][""data_name""]\n            data_row[1] = results[i][""pre_process_name""]\n            data_row[2] = results[i][""model_name""]\n            data_row[3] = results[i][""feature_map_name""]\n            data_row[4] = results[i][""aggregator_name""]\n            data_row[5] = results[i][""post_process_name""]\n            data_row[6] = results[i][""mAP""]\n            data_row[7] = results[i][""recall_at_k""][\'1\']\n            csv_write.writerow(data_row)\n\n\ndef filter_by_keywords(results: List[Dict], keywords: Dict) -> List[Dict]:\n    """"""\n    Filter the search results according to the given keywords\n\n    Args:\n        results (List): a list of retrieval results.\n        keywords (Dict): a dict containing keywords to be selected.\n\n    Returns:\n\n    """"""\n    for key in keywords:\n        no_match = []\n        if len(keywords[key]) == 0:\n            continue\n        else:\n            for i in range(len(results)):\n                if not results[i][key] in keywords[key]:\n                    no_match.append(i)\n        for num in no_match[::-1]:\n            results.pop(num)\n    return results\n'"
search/utils/search_modules.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\nfrom copy import deepcopy\n\n\ndef _convert_dict_to_cfg(d: dict) -> CfgNode:\n    ret = CfgNode()\n    for key in d:\n        if isinstance(d[key], dict):\n            ret[key] = _convert_dict_to_cfg(d[key])\n        else:\n            ret[key] = d[key]\n    return ret\n\n\nclass SearchModules(dict):\n    r""""""\n    This class defines the search args for one module, e.g., data process, feature extraction, feature\n    aggregation, feature process and query.\n    """"""\n    def __init__(self):\n        super(SearchModules, self).__init__()\n\n    def add(self, name: str, value: dict):\n        self[name] = _convert_dict_to_cfg(value)\n\n    def check_valid(self, cfg: CfgNode):\n        cfg = deepcopy(cfg)\n        for module in self.values():\n            cfg.merge_from_other_cfg(module)\n'"
pyretri/datasets/collate_fn/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .collate_fn_impl.collate_fn import CollateFn\nfrom .collate_fn_base import CollateFnBase\n\n__all__ = [\n    'CollateFnBase',\n    'CollateFn',\n]\n"""
pyretri/datasets/collate_fn/collate_fn_base.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict, List\n\n\nclass CollateFnBase(ModuleBase):\n    """"""\n    The base class of collate function.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps: default hyper parameters in a dict (keys, values).\n        """"""\n        super(CollateFnBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, batch: List[Dict]) -> Dict[str, torch.tensor]:\n        pass\n'"
pyretri/datasets/folder/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .folder_impl.folder import Folder\nfrom .folder_base import FolderBase\n\n\n__all__ = [\n    'FolderBase',\n    'Folder',\n]\n"""
pyretri/datasets/folder/folder_base.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nfrom PIL import Image\nimport pickle\nimport os\nfrom abc import abstractmethod\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict, List\n\n\nclass FolderBase(ModuleBase):\n    """"""\n    The base class of folder function.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, data_json_path: str, transformer: callable or None = None, hps: Dict or None = None):\n        """"""\n        Args:\n            data_json_path (str): the path for data json file.\n            transformer (callable): a list of data augmentation operations.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(FolderBase, self).__init__(hps)\n        with open(data_json_path, ""rb"") as f:\n            self.data_info = pickle.load(f)\n        self.data_json_path = data_json_path\n        self.transformer = transformer\n\n    def __len__(self) -> int:\n        pass\n\n    @abstractmethod\n    def __getitem__(self, idx: int) -> Dict:\n        pass\n\n    def find_classes(self, info_dicts: Dict) -> (List, Dict):\n        pass\n\n    def read_img(self, path: str) -> Image:\n        """"""\n        Load image.\n\n        Args:\n            path (str): the path of the image.\n\n        Returns:\n            image (Image): shape (H, W, C).\n        """"""\n        try:\n            img = Image.open(path)\n            img = img.convert(""RGB"")\n            return img\n        except Exception as e:\n            print(\'[DataSet]: WARNING image can not be loaded: {}\'.format(str(e)))\n            return None\n'"
pyretri/datasets/transformer/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .transformers_impl.transformers import (DirectResize, PadResize, ShorterResize, CenterCrop,\n                                             ToTensor, ToCaffeTensor, Normalize, TenCrop, TwoFlip)\nfrom .transformers_base import TransformerBase\n\n\n__all__ = [\n    'TransformerBase',\n    'DirectResize', 'PadResize', 'ShorterResize', 'CenterCrop', 'ToTensor', 'ToCaffeTensor',\n    'Normalize', 'TenCrop', 'TwoFlip',\n]\n"""
pyretri/datasets/transformer/transformers_base.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nfrom PIL import Image\n\nimport torch\n\nfrom ...utils import ModuleBase\nfrom ...utils import Registry\n\nfrom typing import Dict\n\n\nclass TransformerBase(ModuleBase):\n    """"""\n    The base class of data augmentation operations.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(TransformerBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, img: Image) -> Image or torch.tensor:\n        pass\n'"
pyretri/evaluate/evaluator/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .evaluators_impl.overall import OverAll\nfrom .evaluators_impl.oxford_overall import OxfordOverAll\nfrom .evaluators_impl.reid_overall import ReIDOverAll\nfrom .evaluators_base import EvaluatorBase\n\n\n__all__ = [\n    'EvaluatorBase',\n    'OverAll', 'OxfordOverAll',\n    'ReIDOverAll',\n]\n"""
pyretri/evaluate/evaluator/evaluators_base.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict\n\n\nclass EvaluatorBase(ModuleBase):\n    """"""\n    The base class of evaluators which compute mAP and recall.\n    """"""\n    default_hyper_params = {}\n\n    def __init__(self, hps: Dict or None = None):\n        super(EvaluatorBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, query_result: Dict, gallery_info: Dict) -> (float, Dict):\n        pass\n'"
pyretri/evaluate/helper/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .helper import EvaluateHelper\n\n\n__all__ = [\n    'EvaluateHelper',\n]\n"""
pyretri/evaluate/helper/helper.py,0,"b'# -*- coding: utf-8 -*-\n\nimport torch\nfrom ..evaluator import EvaluatorBase\n\nfrom typing import Dict, List\n\nclass EvaluateHelper:\n    """"""\n    A helper class to evaluate query results.\n    """"""\n    def __init__(self, evaluator: EvaluatorBase):\n        """"""\n        Args:\n            evaluator: a evaluator class.\n        """"""\n        self.evaluator = evaluator\n        self.recall_k = evaluator.default_hyper_params[""recall_k""]\n\n    def show_results(self, mAP: float, recall_at_k: Dict) -> None:\n        """"""\n        Show the evaluate results.\n\n        Args:\n            mAP (float): mean average precision.\n            recall_at_k (Dict): recall at the k position.\n        """"""\n        repr_str = ""mAP: {:.1f}\\n"".format(mAP)\n\n        for k in self.recall_k:\n            repr_str += ""R@{}: {:.1f}\\t"".format(k, recall_at_k[k])\n\n        print(\'--------------- Retrieval Evaluation ------------\')\n        print(repr_str)\n\n    def do_eval(self, query_result_info: List, gallery_info: List) -> (float, Dict):\n        """"""\n        Get the evaluate results.\n\n        Args:\n            query_result_info (list): a list of indexing results.\n            gallery_info (list): a list of gallery set information.\n\n        Returns:\n            tuple (float, Dict): mean average precision and recall for each position.\n        """"""\n        mAP, recall_at_k = self.evaluator(query_result_info, gallery_info)\n\n        return mAP, recall_at_k\n'"
pyretri/extract/aggregator/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .aggregators_impl.crow import Crow\nfrom .aggregators_impl.gap import GAP\nfrom .aggregators_impl.gem import GeM\nfrom .aggregators_impl.gmp import GMP\nfrom .aggregators_impl.pwa import PWA\nfrom .aggregators_impl.r_mac import RMAC\nfrom .aggregators_impl.scda import SCDA\nfrom .aggregators_impl.spoc import SPoC\n\nfrom .aggregators_base import AggregatorBase\n\n\n__all__ = [\n    'AggregatorBase',\n    'Crow', 'GAP', 'GeM', 'GMP', 'PWA', 'RMAC', 'SCDA', 'SPoC',\n    'build_aggregators',\n]\n"""
pyretri/extract/aggregator/aggregators_base.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict\n\n\nclass AggregatorBase(ModuleBase):\n    r""""""\n    The base class for feature aggregators.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(AggregatorBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        pass\n'"
pyretri/extract/extractor/__init__.py,1,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nimport torch.nn as nn\n\nfrom .extractors_impl.vgg_series import VggSeries\nfrom .extractors_impl.res_series import ResSeries\nfrom .extractors_impl.reid_series import ReIDSeries\nfrom .extractors_base import ExtractorBase\n\n\n__all__ = [\n    'ExtractorBase',\n    'VggSeries', 'ResSeries',\n    'ReIDSeries',\n]\n"""
pyretri/extract/extractor/extractors_base.py,6,"b'# -*- coding: utf-8 -*-\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict\n\n\nclass ExtractorBase(ModuleBase):\n    """"""\n    The base class feature map extractors.\n\n    Hyper-Parameters\n        extract_features (list): indicates which feature maps to output. See available_feas for available feature maps.\n            If it is [""all""], then all available features will be output.\n    """"""\n    available_feas = list()\n    default_hyper_params = {\n        ""extract_features"": list(),\n    }\n\n    def __init__(self, model: nn.Module, feature_modules: Dict[str, nn.Module], hps: Dict or None = None):\n        """"""\n        Args:\n            model (nn.Module): the model for extracting features.\n            feature_modules (dict): the output layer of the model.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(ExtractorBase, self).__init__(hps)\n        assert len(self._hyper_params[""extract_features""]) > 0\n\n        self.model = model.eval()\n        if torch.cuda.is_available():\n            self.model.cuda()\n            if torch.cuda.device_count() > 1:\n                self.model = nn.DataParallel(self.model)\n        self.feature_modules = feature_modules\n        self.feature_buffer = dict()\n\n        if self._hyper_params[""extract_features""][0] == ""all"":\n            self._hyper_params[""extract_features""] = self.available_feas\n        for fea in self._hyper_params[""extract_features""]:\n            self.feature_buffer[fea] = dict()\n\n        self._register_hook()\n\n    def _register_hook(self) -> None:\n        """"""\n        Register hooks to output inner feature map.\n        """"""\n        def hook(feature_buffer, fea_name, module, input, output):\n            feature_buffer[fea_name][str(output.device)] = output.data\n\n        for fea in self._hyper_params[""extract_features""]:\n            assert fea in self.feature_modules, \'unknown feature {}!\'.format(fea)\n            self.feature_modules[fea].register_forward_hook(partial(hook, self.feature_buffer, fea))\n\n    def __call__(self, x: torch.tensor) -> Dict:\n        with torch.no_grad():\n            self.model(x)\n            ret = dict()\n            for fea in self._hyper_params[""extract_features""]:\n                ret[fea] = list()\n                devices = list(self.feature_buffer[fea].keys())\n                devices = np.sort(devices)\n                for d in devices:\n                    ret[fea].append(self.feature_buffer[fea][d])\n                ret[fea] = torch.cat(ret[fea], dim=0)\n        return ret\n'"
pyretri/extract/helper/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .helper import ExtractHelper\n\n\n__all__ = [\n    'ExtractHelper',\n]\n"""
pyretri/extract/helper/helper.py,4,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport pickle\nfrom tqdm import tqdm\n\nimport torch\n\nfrom ..extractor import ExtractorBase\nfrom ..aggregator import AggregatorBase\nfrom ..splitter import SplitterBase\nfrom ...utils import ensure_dir\nfrom torch.utils.data import DataLoader\nfrom typing import Dict, List\n\nimport time\n\nclass ExtractHelper:\n    """"""\n    A helper class to extract feature maps from model, and then aggregate them.\n    """"""\n    def __init__(self, assemble: int, extractor: ExtractorBase, splitter: SplitterBase, aggregators: List[AggregatorBase]):\n        """"""\n        Args:\n            assemble (int): way to assemble features if transformers produce multiple images (e.g. TwoFlip, TenCrop).\n            extractor (ExtractorBase): a extractor class for extracting features.\n            splitter (SplitterBase): a splitter class for splitting features.\n            aggregators (list): a list of extractor classes for aggregating features.\n        """"""\n        self.assemble = assemble\n        self.extractor = extractor\n        self.splitter = splitter\n        self.aggregators = aggregators\n\n    def _save_part_fea(self, datainfo: Dict, save_fea: List, save_path: str) -> None:\n        """"""\n        Save features in a json file.\n\n        Args:\n            datainfo (dict): the dataset information contained the data json file.\n            save_fea (list): a list of features to be saved.\n            save_path (str): the save path for the extracted features.\n        """"""\n        save_json = dict()\n        for key in datainfo:\n            if key != ""info_dicts"":\n                save_json[key] = datainfo[key]\n        save_json[""info_dicts""] = save_fea\n\n        with open(save_path, ""wb"") as f:\n            pickle.dump(save_json, f)\n\n    def extract_one_batch(self, batch: Dict) -> Dict:\n        """"""\n        Extract features for a batch of images.\n\n        Args:\n            batch (dict): a dict containing several image tensors.\n\n        Returns:\n            all_fea_dict (dict): a dict containing extracted features.\n        """"""\n        img = batch[""img""]\n        if torch.cuda.is_available():\n            img = img.cuda()\n        # img is in the shape (N, IMG_AUG, C, H, W)\n        batch_size, aug_size = img.shape[0], img.shape[1]\n        img = img.view(-1, img.shape[2], img.shape[3], img.shape[4])\n\n        features = self.extractor(img)\n\n        features = self.splitter(features)\n\n        all_fea_dict = dict()\n        for aggregator in self.aggregators:\n            fea_dict = aggregator(features)\n            all_fea_dict.update(fea_dict)\n\n        # PyTorch will duplicate inputs if batch_size < n_gpu\n        for key in all_fea_dict.keys():\n            if self.assemble == 0:\n                features = all_fea_dict[key][:img.shape[0], :]\n                features = features.view(batch_size, aug_size, -1)\n                features = features.view(batch_size, -1)\n                all_fea_dict[key] = features\n            elif self.assemble == 1:\n                features = all_fea_dict[key].view(batch_size, aug_size, -1)\n                features = features.sum(dim=1)\n                all_fea_dict[key] = features\n\n        return all_fea_dict\n\n    def do_extract(self, dataloader: DataLoader, save_path: str, save_interval: int = 5000) -> None:\n        """"""\n        Extract features for a whole dataset and save features in json files.\n\n        Args:\n            dataloader (DataLoader): a DataLoader class for loading images for training.\n            save_path (str): the save path for the extracted features.\n            save_interval (int, optional): number of features saved in one part file.\n        """"""\n        datainfo = dataloader.dataset.data_info\n        pbar = tqdm(range(len(dataloader)))\n        save_fea = list()\n        part_cnt = 0\n        ensure_dir(save_path)\n\n        start = time.time()\n        for _, batch in zip(pbar, dataloader):\n            feature_dict = self.extract_one_batch(batch)\n            for i in range(len(batch[""img""])):\n                idx = batch[""idx""][i]\n                save_fea.append(datainfo[""info_dicts""][idx])\n                single_fea_dict = dict()\n                for key in feature_dict:\n                    single_fea_dict[key] = feature_dict[key][i].tolist()\n                save_fea[-1][""feature""] = single_fea_dict\n                save_fea[-1][""idx""] = int(idx)\n\n            if len(save_fea) >= save_interval:\n                self._save_part_fea(datainfo, save_fea, os.path.join(save_path, ""part_{}.json"".format(part_cnt)))\n                part_cnt += 1\n                del save_fea\n                save_fea = list()\n        end = time.time()\n        print(\'time: \', end - start)\n\n        if len(save_fea) >= 1:\n            self._save_part_fea(datainfo, save_fea, os.path.join(save_path, ""part_{}.json"".format(part_cnt)))\n\n    def do_single_extract(self, img: torch.Tensor) -> [Dict]:\n        """"""\n        Extract features for a single image.\n\n        Args:\n            img (torch.Tensor): a single image tensor.\n\n        Returns:\n            [fea_dict] (sequence): the extract features of the image.\n        """"""\n        batch = dict()\n        batch[""img""] = img.view(1, img.shape[0], img.shape[1], img.shape[2], img.shape[3])\n        fea_dict = self.extract_one_batch(batch)\n\n        return [fea_dict]\n'"
pyretri/extract/splitter/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .splitter_impl.identity import Identity\nfrom .splitter_impl.pcb import PCB\nfrom .splitter_base import SplitterBase\n\n\n__all__ = [\n    'SplitterBase',\n    'Identity', 'PCB',\n]\n"""
pyretri/extract/splitter/splitter_base.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict\n\n\nclass SplitterBase(ModuleBase):\n    """"""\n    The base class for splitter function.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(SplitterBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, features: torch.tensor) -> Dict:\n        pass\n'"
pyretri/extract/utils/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .make_data_json import make_data_json\nfrom .split_dataset import split_dataset\n\n__all__ = [\n    'split_dataset', 'make_data_json',\n]\n"""
pyretri/extract/utils/make_data_json.py,0,"b'# -*- coding: utf-8 -*-\n\nimport pickle\nimport os\n\n\ndef make_ds_for_general(dataset_path: str, save_path: str) -> None:\n    """"""\n    Generate data json file for dataset collecting images with the same label one directory. e.g. CUB-200-2011.\n\n    Args:\n        dataset_path (str): the path of the dataset.\n        save_ds_path (str): the path for saving the data json files.\n    """"""\n    info_dicts = list()\n    img_dirs = os.listdir(dataset_path)\n    label_list = list()\n    label_to_idx = dict()\n    for dir in img_dirs:\n        for root, _, files in os.walk(os.path.join(dataset_path, dir)):\n            for file in files:\n                info_dict = dict()\n                info_dict[\'path\'] = os.path.join(root, file)\n                if dir not in label_list:\n                    label_to_idx[dir] = len(label_list)\n                    label_list.append(dir)\n                info_dict[\'label\'] = dir\n                info_dict[\'label_idx\'] = label_to_idx[dir]\n                info_dicts += [info_dict]\n    with open(save_path, \'wb\') as f:\n        pickle.dump({\'nr_class\': len(img_dirs), \'path_type\': \'absolute_path\', \'info_dicts\': info_dicts}, f)\n\n\ndef make_ds_for_oxford(dataset_path, save_path: str or None=None, gt_path: str or None=None) -> None:\n    """"""\n    Generate data json file for oxford dataset.\n\n    Args:\n        dataset_path (str): the path of the dataset.\n        save_ds_path (str): the path for saving the data json files.\n        gt_path (str, optional): the path of the ground truth, necessary for Oxford.\n    """"""\n    label_list = list()\n    info_dicts = list()\n    query_info = dict()\n    if \'query\' in dataset_path:\n        for root, _, files in os.walk(gt_path):\n            for file in files:\n                if \'query\' in file:\n                    with open(os.path.join(root, file), \'r\') as f:\n                        line = f.readlines()[0].strip(\'\\n\').split(\' \')\n                        query_name = file[:-10]\n                        label = line[0][5:]\n                        bbox = [float(line[1]), float(line[2]), float(line[3]), float(line[4])]\n                        query_info[label] = {\'query_name\': query_name, \'bbox\': bbox,}\n\n    for root, _, files in os.walk(dataset_path):\n        for file in files:\n            info_dict = dict()\n            info_dict[\'path\'] = os.path.join(root, file)\n            label = file.split(\'.\')[0]\n            if label not in label_list:\n                label_list.append(label)\n            info_dict[\'label\'] = label\n            if \'query\' in dataset_path:\n                info_dict[\'bbox\'] = query_info[label][\'bbox\']\n                info_dict[\'query_name\'] = query_info[label][\'query_name\']\n            info_dicts += [info_dict]\n\n    with open(save_path, \'wb\') as f:\n        pickle.dump({\'nr_class\': len(label_list), \'path_type\': \'absolute_path\', \'info_dicts\': info_dicts}, f)\n\n\ndef make_ds_for_reid(dataset_path: str, save_path: str) -> None:\n    """"""\n    Generating data json file for Re-ID dataset.\n\n    Args:\n        dataset_path (str): the path of the dataset.\n        save_ds_path (str): the path for saving the data json files.\n    """"""\n    label_list = list()\n    info_dicts = list()\n    for root, _, files in os.walk(dataset_path):\n        for file in files:\n            info_dict = dict()\n            info_dict[\'path\'] = os.path.join(root, file)\n            label = file.split(\'_\')[0]\n            cam = file.split(\'_\')[1][1]\n            if label not in label_list:\n                label_list.append(label)\n            info_dict[\'label\'] = label\n            info_dict[\'cam\'] = cam\n            info_dicts += [info_dict]\n    with open(save_path, \'wb\') as f:\n        pickle.dump({\'nr_class\': len(label_list), \'path_type\': \'absolute_path\', \'info_dicts\': info_dicts}, f)\n\n\ndef make_data_json(dataset_path: str, save_path: str, type: str, gt_path: str or None=None) -> None:\n    """"""\n    Generate data json file for dataset.\n\n    Args:\n        dataset_path (str): the path of the dataset.\n        save_ds_path (str): the path for saving the data json files.\n        type (str): the structure type of the dataset.\n        gt_path (str, optional): the path of the ground truth, necessary for Oxford.\n    """"""\n    assert type in [\'general\', \'oxford\', \'reid\']\n    if type == \'general\':\n        make_ds_for_general(dataset_path, save_path)\n    elif type == \'oxford\':\n        make_ds_for_oxford(dataset_path, save_path, gt_path)\n    elif type == \'reid\':\n        make_ds_for_reid(dataset_path, save_path)\n'"
pyretri/extract/utils/split_dataset.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nfrom shutil import copyfile\n\n\ndef split_dataset(dataset_path: str, split_file: str) -> None:\n    """"""\n    Split the dataset according to the given splitting rules.\n\n    Args:\n        dataset_path (str): the path of the dataset.\n        split_file (str): the path of the file containing the splitting rules.\n    """"""\n\n    with open(split_file, \'r\') as f:\n        lines = f.readlines()\n        for line in lines:\n            path = line.strip(\'\\n\').split(\' \')[0]\n            is_gallery = line.strip(\'\\n\').split(\' \')[1]\n            if is_gallery == \'0\':\n                src = os.path.join(dataset_path, path)\n                dst = src.replace(path.split(\'/\')[0], \'query\')\n                dst_index = len(dst.split(\'/\')[-1])\n                dst_dir = dst[:len(dst) - dst_index]\n                if not os.path.isdir(dst_dir):\n                    os.makedirs(dst_dir)\n                if not os.path.exists(dst):\n                    os.symlink(src, dst)\n            elif is_gallery == \'1\':\n                src = os.path.join(dataset_path, path)\n                dst = src.replace(path.split(\'/\')[0], \'gallery\')\n                dst_index = len(dst.split(\'/\')[-1])\n                dst_dir = dst[:len(dst) - dst_index]\n                if not os.path.isdir(dst_dir):\n                    os.makedirs(dst_dir)\n                if not os.path.exists(dst):\n                    os.symlink(src, dst)\n'"
pyretri/index/dim_processor/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .dim_processors_impl.identity import Identity\nfrom .dim_processors_impl.l2_normalize import L2Normalize\nfrom .dim_processors_impl.part_pca import PartPCA\nfrom .dim_processors_impl.part_svd import PartSVD\nfrom .dim_processors_impl.pca import PCA\nfrom .dim_processors_impl.svd import SVD\nfrom .dim_processors_impl.rmac_pca import RMACPCA\nfrom .dim_processors_base import DimProcessorBase\n\n\n__all__ = [\n    'DimProcessorBase',\n    'Identity', 'L2Normalize', 'PartPCA', 'PartSVD', 'PCA', 'SVD', 'RMACPCA',\n]\n"""
pyretri/index/dim_processor/dim_processors_base.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nimport numpy as np\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict, List\n\n\nclass DimProcessorBase(ModuleBase):\n    """"""\n    The base class of dimension processor.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        ModuleBase.__init__(self, hps)\n        self.feature_names = feature_names\n\n    @abstractmethod\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        pass\n'"
pyretri/index/feature_enhancer/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .feature_enhancer_impl.identity import Identity\nfrom .feature_enhancer_impl.database_augmentation import DBA\nfrom .feature_enhancer_base import EnhanceBase\n\n\n__all__ = [\n    'EnhanceBase',\n    'Identity', 'DBA',\n]\n"""
pyretri/index/feature_enhancer/feature_enhancer_base.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict\n\n\nclass EnhanceBase(ModuleBase):\n    """"""\n    The base class of feature enhancer.\n    """"""\n    default_hyper_params = {}\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(EnhanceBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, feature: torch.tensor) -> torch.tensor:\n        pass\n'"
pyretri/index/helper/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .helper import IndexHelper\n\n\n__all__ = [\n    'IndexHelper',\n]"""
pyretri/index/helper/helper.py,2,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport shutil\n\nimport torch\n\nimport numpy as np\n\nfrom ..dim_processor import DimProcessorBase\nfrom ..feature_enhancer import EnhanceBase\nfrom ..metric import MetricBase\nfrom ..re_ranker import ReRankerBase\nfrom ..utils import feature_loader\n\nimport matplotlib.pyplot as plt\n\nfrom typing import Dict, List\n\n\nclass IndexHelper:\n    """"""\n    A helper class to index features.\n    """"""\n    def __init__(\n            self,\n            dim_processors: List[DimProcessorBase],\n            feature_enhancer: EnhanceBase,\n            metric: MetricBase,\n            re_ranker: ReRankerBase,\n    ):\n        """"""\n        Args:\n            dim_processors (list):\n            feature_enhancer (EnhanceBase):\n            metric (MetricBase):\n            re_ranker (ReRankerBase):\n        """"""\n        self.dim_procs = dim_processors\n        self.feature_enhance = feature_enhancer\n        self.metric = metric\n        self.re_rank = re_ranker\n\n    def show_topk_retrieved_images(self, single_query_info: Dict, topk: int, gallery_info: List[Dict]) -> None:\n        """"""\n        Show the top-k retrieved images of one query.\n\n        Args:\n            single_query_info (dict): a dict of single query information.\n            topk (int): number of the nearest images to be showed.\n            gallery_info (list): a list of gallery set information.\n        """"""\n        query_idx = single_query_info[""ranked_neighbors_idx""]\n        query_topk_idx = query_idx[:topk]\n\n        for idx in query_topk_idx:\n            img_path = gallery_info[idx][""path""]\n            plt.figure()\n            plt.imshow(img_path)\n            plt.show()\n\n    def save_topk_retrieved_images(self, save_path: str, single_query_info: Dict, topk: int, gallery_info: List[Dict]) -> None:\n        """"""\n        Save the top-k retrieved images of one query.\n\n        Args:\n            save_path (str): the path to save the retrieved images.\n            single_query_info (dict): a dict of single query information.\n            topk (int): number of the nearest images to be saved.\n            gallery_info (list): a list of gallery set information.\n        """"""\n        query_idx = single_query_info[""ranked_neighbors_idx""]\n        query_topk_idx = query_idx[:topk]\n\n        for idx in query_topk_idx:\n            img_path = gallery_info[idx][""path""]\n            shutil.copy(img_path, os.path.join(save_path, str(idx)+\'.png\'))\n\n    def do_index(self, query_fea: np.ndarray, query_info: List, gallery_fea: np.ndarray) -> (List, np.ndarray, np.ndarray):\n        """"""\n        Index the query features.\n\n        Args:\n            query_fea (np.ndarray): query set features.\n            query_info (list): a list of gallery set information.\n            gallery_fea (np.ndarray): gallery set features.\n\n        Returns:\n            tuple(List, np.ndarray, np.ndarray): query feature information, query features and gallery features after process.\n        """"""\n        for dim_proc in self.dim_procs:\n            query_fea, gallery_fea = dim_proc(query_fea), dim_proc(gallery_fea)\n\n        query_fea, gallery_fea = torch.Tensor(query_fea), torch.Tensor(gallery_fea)\n        # if torch.cuda.is_available():\n        #     query_fea = query_fea.cuda()\n        #     gallery_fea = gallery_fea.cuda()\n\n        gallery_fea = self.feature_enhance(gallery_fea)\n\n        dis, sorted_index = self.metric(query_fea, gallery_fea)\n\n        sorted_index = self.re_rank(query_fea, gallery_fea, dis=dis, sorted_index=sorted_index)\n        for i, info in enumerate(query_info):\n            info[""ranked_neighbors_idx""] = sorted_index[i].tolist()\n\n        return query_info, query_fea, gallery_fea\n'"
pyretri/index/metric/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .metric_impl.knn import KNN\nfrom .metric_base import MetricBase\n\n\n__all__ = [\n    'MetricBase',\n    'KNN',\n]\n"""
pyretri/index/metric/metric_base.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict\n\n\nclass MetricBase(ModuleBase):\n    """"""\n    The base class for similarity metric.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(MetricBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, query_fea: torch.tensor, gallery_fea: torch.tensor):\n        pass\n'"
pyretri/index/re_ranker/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom yacs.config import CfgNode\n\nfrom .re_ranker_impl.identity import Identity\nfrom .re_ranker_impl.k_reciprocal import KReciprocal\nfrom .re_ranker_impl.query_expansion import QE\nfrom .re_ranker_impl.qe_kr import QEKR\nfrom .re_ranker_base import ReRankerBase\n\n\n__all__ = [\n    'ReRankerBase',\n    'Identity', 'KReciprocal', 'QE', 'QEKR',\n]"""
pyretri/index/re_ranker/re_ranker_base.py,2,"b'# -*- coding: utf-8 -*-\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom ...utils import ModuleBase\n\nfrom typing import Dict\n\n\nclass ReRankerBase(ModuleBase):\n    """"""\n    The base class of re-ranker.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(ReRankerBase, self).__init__(hps)\n\n    @abstractmethod\n    def __call__(self, query_fea: torch.tensor, gallery_fea: torch.tensor, dis: torch.tensor or None = None,\n                 sorted_index: torch.tensor or None = None) -> torch.tensor:\n        pass\n'"
pyretri/index/utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n\nfrom .feature_loader import feature_loader\n'
pyretri/index/utils/feature_loader.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport pickle\n\nimport numpy as np\n\nfrom typing import Dict, List\n\nclass FeatureLoader:\n    """"""\n    A class for load features and information.\n    """"""\n    def __init__(self):\n        self.feature_cache = dict()\n\n    def _load_from_cache(self, fea_dir: str, feature_names: List[str]) -> (np.ndarray, Dict, Dict):\n        """"""\n        Load feature and its information from cache.\n\n        Args:\n            fea_dir (str): the path of features to be loaded.\n            feature_names (list): a list of str indicating which feature will be output.\n\n        Returns:\n            tuple (np.ndarray, Dict, Dict): a stacked feature, a list of dicts which describes the image information of each feature,\n                and a dict map from feature name to its position.\n        """"""\n        assert fea_dir in self.feature_cache, ""feature in {} not cached!"".format(fea_dir)\n\n        feature_dict = self.feature_cache[fea_dir][""feature_dict""]\n        info_dicts = self.feature_cache[fea_dir][""info_dicts""]\n        stacked_feature = list()\n        pos_info = dict()\n\n        if len(feature_names) == 1 and feature_names[0] == ""all"":\n            feature_names = list(feature_dict.keys())\n        feature_names = np.sort(feature_names)\n\n        st_idx = 0\n        for name in feature_names:\n            assert name in feature_dict, ""invalid feature name: {} not in {}!"".format(name, feature_dict.keys())\n            stacked_feature.append(feature_dict[name])\n            pos_info[name] = (st_idx, st_idx + stacked_feature[-1].shape[1])\n            st_idx = st_idx + stacked_feature[-1].shape[1]\n        stacked_feature = np.concatenate(stacked_feature, axis=1)\n\n        print(""[LoadFeature] Success, total {} images, \\n feature names: {}"".format(\n            len(info_dicts),\n            pos_info.keys())\n        )\n        return stacked_feature, info_dicts, pos_info\n\n    def load(self, fea_dir: str, feature_names: List[str]) -> (np.ndarray, Dict, Dict):\n        """"""\n        Load and concat feature from feature directory.\n\n        Args:\n            fea_dir (str): the path of features to be loaded.\n            feature_names (list): a list of str indicating which feature will be output.\n\n        Returns:\n            tuple (np.ndarray, Dict, Dict): a stacked feature, a list of dicts which describes the image information of each feature,\n                and a dict map from feature name to its position.\n\n        """"""\n        assert os.path.exists(fea_dir), ""non-exist feature path: {}"".format(fea_dir)\n\n        if fea_dir in self.feature_cache:\n            return self._load_from_cache(fea_dir, feature_names)\n\n        feature_dict = dict()\n        info_dicts = list()\n\n        for root, dirs, files in os.walk(fea_dir):\n            for file in files:\n                if file.endswith("".json""):\n                    print(""[LoadFeature]: loading feature from {}..."".format(os.path.join(root, file)))\n                    with open(os.path.join(root, file), ""rb"") as f:\n                        part_info = pickle.load(f)\n                        for info in part_info[""info_dicts""]:\n                            for key in info[""feature""].keys():\n                                if key not in feature_dict:\n                                    feature_dict[key] = list()\n                                feature_dict[key].append(info[""feature""][key])\n                            del info[""feature""]\n                            info_dicts.append(info)\n        for key, fea in feature_dict.items():\n            fea = np.array(fea)\n            feature_dict[key] = fea\n\n        self.feature_cache[fea_dir] = {\n            ""feature_dict"": feature_dict,\n            ""info_dicts"": info_dicts\n        }\n\n        return self._load_from_cache(fea_dir, feature_names)\n\n\nfeature_loader = FeatureLoader()\n'"
pyretri/models/backbone/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .backbone_impl.resnet import ResNet\nfrom .backbone_impl.vgg import VGG\nfrom .backbone_impl.reid_baseline import ft_net\nfrom .backbone_base import BackboneBase\n\n\n__all__ = [\n    'BackboneBase',\n    'ResNet', 'VGG',\n    'ft_net',\n]\n"""
pyretri/models/backbone/backbone_base.py,2,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\n\nfrom ..registry import BACKBONES\n\n\nclass BackboneBase(nn.Module):\n    """"""\n    The base class of backbone.\n    """"""\n    def __init__(self):\n        super(BackboneBase, self).__init__()\n\n    def _forward(self, x: torch.tensor) -> torch.tensor:\n        pass\n'"
pyretri/datasets/collate_fn/collate_fn_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/datasets/collate_fn/collate_fn_impl/collate_fn.py,3,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..collate_fn_base import CollateFnBase\nfrom ...registry import COLLATEFNS\nfrom torch.utils.data.dataloader import default_collate\n\nfrom typing import Dict, List\n\n@COLLATEFNS.register\nclass CollateFn(CollateFnBase):\n    """"""\n    A wrapper for torch.utils.data.dataloader.default_collate.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(CollateFn, self).__init__(hps)\n\n    def __call__(self, batch: List[Dict]) -> Dict[str, torch.tensor]:\n        assert isinstance(batch, list)\n        assert isinstance(batch[0], dict)\n        return default_collate(batch)\n'"
pyretri/datasets/folder/folder_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/datasets/folder/folder_impl/folder.py,0,"b'# -*- coding: utf-8 -*-\n\nimport pickle\n\nfrom ..folder_base import FolderBase\nfrom ...registry import FOLDERS\n\nfrom typing import Dict, List\n\n\n@FOLDERS.register\nclass Folder(FolderBase):\n    """"""\n    A folder function for loading images.\n\n    Hyper-Params:\n        use_bbox: bool, whether use bbox to crop image. When set to true,\n            make sure that bbox attribute is provided in your data json and bbox format is [x1, y1, x2, y2].\n    """"""\n    default_hyper_params = {\n        ""use_bbox"": False,\n    }\n\n    def __init__(self, data_json_path: str, transformer: callable or None = None, hps: Dict or None = None):\n        """"""\n        Args:\n            data_json_path (str): the path for data json file.\n            transformer (callable): a list of data augmentation operations.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(Folder, self).__init__(data_json_path, transformer, hps)\n        self.classes, self.class_to_idx = self.find_classes(self.data_info[""info_dicts""])\n\n    def find_classes(self, info_dicts: Dict) -> (List, Dict):\n        """"""\n        Get the class names and the mapping relations.\n\n        Args:\n            info_dicts (dict): the dataset information contained the data json file.\n\n        Returns:\n            tuple (list, dict): a list of class names and a dict for projecting class name into int label.\n        """"""\n        classes = list()\n        for i in range(len(info_dicts)):\n            if info_dicts[i][""label""] not in classes:\n                classes.append(info_dicts[i][""label""])\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    def __len__(self) -> int:\n        """"""\n        Get the number of total training samples.\n\n        Returns:\n            length (int): the number of total training samples.\n        """"""\n        return len(self.data_info[""info_dicts""])\n\n    def __getitem__(self, idx: int) -> Dict:\n        """"""\n        Load the image and convert it to tensor for training.\n\n        Args:\n            idx (int): the serial number of the image.\n\n        Returns:\n            item (dict): the dict containing the image after augmentations, serial number and label.\n        """"""\n        info = self.data_info[""info_dicts""][idx]\n        img = self.read_img(info[""path""])\n        if self._hyper_params[""use_bbox""]:\n            assert info[""bbox""] is not None, \'image {} does not have a bbox\'.format(info[""path""])\n            x1, y1, x2, y2 = info[""bbox""]\n            box = map(int, (x1, y1, x2, y2))\n            img = img.crop(box)\n        img = self.transformer(img)\n        return {""img"": img, ""idx"": idx, ""label"": self.class_to_idx[info[""label""]]}\n'"
pyretri/datasets/transformer/transformers_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/datasets/transformer/transformers_impl/transformers.py,7,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom ..transformers_base import TransformerBase\nfrom ...registry import TRANSFORMERS\nfrom torchvision.transforms import Resize as TResize\nfrom torchvision.transforms import TenCrop as TTenCrop\nfrom torchvision.transforms import CenterCrop as TCenterCrop\nfrom torchvision.transforms import ToTensor as TToTensor\nfrom torchvision.transforms.functional import hflip\n\nfrom typing import Dict\n\n@TRANSFORMERS.register\nclass DirectResize(TransformerBase):\n    """"""\n    Directly resize image to target size, regardless of h: w ratio.\n\n    Hyper-Params\n        size (sequence): desired output size.\n        interpolation (int): desired interpolation.\n    """"""\n    default_hyper_params = {\n        ""size"": (224, 224),\n        ""interpolation"": Image.BILINEAR,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(DirectResize, self).__init__(hps)\n        self.t_transformer = TResize(self._hyper_params[""size""], self._hyper_params[""interpolation""])\n\n    def __call__(self, img: Image) -> Image:\n        return self.t_transformer(img)\n\n\n@TRANSFORMERS.register\nclass PadResize(TransformerBase):\n    """"""\n    Resize image\'s longer edge to target size, and then pad the shorter edge to target size.\n\n    Hyper-Params\n        size (int): desired output size of the longer edge.\n        padding_v (sequence): padding pixel value.\n        interpolation (int): desired interpolation.\n    """"""\n    default_hyper_params = {\n        ""size"": 224,\n        ""padding_v"": [124, 116, 104],\n        ""interpolation"": Image.BILINEAR,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps: default hyper parameters in a dict (keys, values).\n        """"""\n        super(PadResize, self).__init__(hps)\n\n    def __call__(self, img: Image) -> Image:\n        target_size = self._hyper_params[""size""]\n        padding_v = tuple(self._hyper_params[""padding_v""])\n        interpolation = self._hyper_params[""interpolation""]\n\n        w, h = img.size\n        if w > h:\n            img = img.resize((int(target_size), int(h * target_size * 1.0 / w)), interpolation)\n        else:\n            img = img.resize((int(w * target_size * 1.0 / h), int(target_size)), interpolation)\n\n        ret_img = Image.new(""RGB"", (target_size, target_size), padding_v)\n        w, h = img.size\n        st_w = int((ret_img.size[0] - w) / 2.0)\n        st_h = int((ret_img.size[1] - h) / 2.0)\n        ret_img.paste(img, (st_w, st_h))\n        return ret_img\n\n\n@TRANSFORMERS.register\nclass ShorterResize(TransformerBase):\n    """"""\n    Resize image\'s shorter edge to target size, while keep h: w ratio.\n\n    Hyper-Params\n        size (int): desired output size.\n        interpolation (int): desired interpolation.\n    """"""\n    default_hyper_params = {\n        ""size"": 224,\n        ""interpolation"": Image.BILINEAR,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(ShorterResize, self).__init__(hps)\n        self.t_transformer = TResize(self._hyper_params[""size""], self._hyper_params[""interpolation""])\n\n    def __call__(self, img: Image) -> Image:\n        return self.t_transformer(img)\n\n\n@TRANSFORMERS.register\nclass CenterCrop(TransformerBase):\n    """"""\n    A wrapper from CenterCrop in pytorch, see torchvision.transformers.CenterCrop for explanation.\n\n    Hyper-Params\n        size(sequence or int): desired output size.\n    """"""\n    default_hyper_params = {\n        ""size"": 224,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(CenterCrop, self).__init__(hps)\n        self.t_transformer = TCenterCrop(self._hyper_params[""size""])\n\n    def __call__(self, img: Image) -> Image:\n        return self.t_transformer(img)\n\n\n@TRANSFORMERS.register\nclass ToTensor(TransformerBase):\n    """"""\n    A wrapper from ToTensor in pytorch, see torchvision.transformers.ToTensor for explanation.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(ToTensor, self).__init__(hps)\n        self.t_transformer = TToTensor()\n\n    def __call__(self, imgs: Image or tuple) -> torch.Tensor:\n        if not isinstance(imgs, tuple):\n            imgs = [imgs]\n        ret_tensor = list()\n        for img in imgs:\n            ret_tensor.append(self.t_transformer(img))\n        ret_tensor = torch.stack(ret_tensor, dim=0)\n        return ret_tensor\n\n\n@TRANSFORMERS.register\nclass ToCaffeTensor(TransformerBase):\n    """"""\n    Create tensors for models trained in caffe.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(ToCaffeTensor, self).__init__(hps)\n\n    def __call__(self, imgs: Image or tuple) -> torch.tensor:\n        if not isinstance(imgs, tuple):\n            imgs = [imgs]\n\n        ret_tensor = list()\n        for img in imgs:\n            img = np.array(img, np.int32, copy=False)\n            r, g, b = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n            img = np.stack([b, g, r], axis=2)\n            img = torch.from_numpy(img)\n            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n            img = img.float()\n            ret_tensor.append(img)\n        ret_tensor = torch.stack(ret_tensor, dim=0)\n        return ret_tensor\n\n\n@TRANSFORMERS.register\nclass Normalize(TransformerBase):\n    """"""\n    Normalize a tensor image with mean and standard deviation.\n\n    Hyper-Params\n        mean (sequence): sequence of means for each channel.\n        std (sequence): sequence of standard deviations for each channel.\n    """"""\n    default_hyper_params = {\n        ""mean"": [0.485, 0.456, 0.406],\n        ""std"": [0.229, 0.224, 0.225],\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(Normalize, self).__init__(hps)\n        for v in [""mean"", ""std""]:\n            self.__dict__[v] = np.array(self._hyper_params[v])[None, :, None, None]\n            self.__dict__[v] = torch.from_numpy(self.__dict__[v]).float()\n\n    def __call__(self, tensor: torch.tensor) -> torch.tensor:\n        assert tensor.ndimension() == 4\n        tensor.sub_(self.mean).div_(self.std)\n        return tensor\n\n\n@TRANSFORMERS.register\nclass TenCrop(TransformerBase):\n    """"""\n    A wrapper from TenCrop in pytorch\xef\xbc\x8csee torchvision.transformers.TenCrop for explanation.\n\n    Hyper-Params\n        size (sequence or int): desired output size.\n    """"""\n    default_hyper_params = {\n        ""size"": 224,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(TenCrop, self).__init__(hps)\n        self.t_transformer = TTenCrop(self._hyper_params[""size""])\n\n    def __call__(self, img: Image) -> Image:\n        return self.t_transformer(img)\n\n\n@TRANSFORMERS.register\nclass TwoFlip(TransformerBase):\n    """"""\n    Return the image itself and its horizontal flipped one.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(TwoFlip, self).__init__(hps)\n\n    def __call__(self, img: Image) -> (Image, Image):\n        return img, hflip(img)\n'"
pyretri/evaluate/evaluator/evaluators_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/evaluate/evaluator/evaluators_impl/overall.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..evaluators_base import EvaluatorBase\nfrom ...registry import EVALUATORS\n\nfrom sklearn.metrics import average_precision_score\n\nfrom typing import Dict, List\n\n\n@EVALUATORS.register\nclass OverAll(EvaluatorBase):\n    """"""\n    A evaluator for mAP and recall computation.\n\n    Hyper-Params\n        recall_k (sequence): positions of recalls to be calculated.\n    """"""\n    default_hyper_params = {\n        ""recall_k"": [1, 2, 4, 8],\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(OverAll, self).__init__(hps)\n        self._hyper_params[""recall_k""] = np.sort(self._hyper_params[""recall_k""])\n\n    def compute_recall_at_k(self, gt: List[bool], result_dict: Dict) -> None:\n        """"""\n        Calculate the recall at each position.\n\n        Args:\n            gt (sequence): a list of bool indicating if the result is equal to the label.\n            result_dict (dict): a dict of indexing results.\n        """"""\n        ks = self._hyper_params[""recall_k""]\n        gt = gt[:ks[-1]]\n        first_tp = np.where(gt)[0]\n        if len(first_tp) == 0:\n            return\n        for k in ks:\n            if k >= first_tp[0] + 1:\n                result_dict[k] = result_dict[k] + 1\n\n    def __call__(self, query_result: List, gallery_info: List) -> (float, Dict):\n        """"""\n        Calculate the mAP and recall for the indexing results.\n\n        Args:\n            query_result (list): a list of indexing results.\n            gallery_info (list): a list of gallery set information.\n\n        Returns:\n            tuple (float, dict): mean average precision and recall for each position.\n        """"""\n        aps = list()\n\n        # For mAP calculation\n        pseudo_score = np.arange(0, len(gallery_info))[::-1]\n\n        recall_at_k = dict()\n        for k in self._hyper_params[""recall_k""]:\n            recall_at_k[k] = 0\n\n        gallery_label = np.array([gallery_info[idx][""label_idx""] for idx in range(len(gallery_info))])\n        for i in range(len(query_result)):\n            ranked_idx = query_result[i][""ranked_neighbors_idx""]\n            gt = (gallery_label[query_result[i][""ranked_neighbors_idx""]] == query_result[i][""label_idx""])\n            aps.append(average_precision_score(gt, pseudo_score[:len(gt)]))\n\n            # deal with \'gallery as query\' test\n            if gallery_info[ranked_idx[0]][""path""] == query_result[i][""path""]:\n                gt.pop(0)\n            self.compute_recall_at_k(gt, recall_at_k)\n\n        mAP = np.mean(aps) * 100\n\n        for k in recall_at_k:\n            recall_at_k[k] = recall_at_k[k] * 100 / len(query_result)\n\n        return mAP, recall_at_k\n'"
pyretri/evaluate/evaluator/evaluators_impl/oxford_overall.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\n\nimport numpy as np\n\nfrom ..evaluators_base import EvaluatorBase\nfrom ...registry import EVALUATORS\n\nfrom typing import Dict, List\n\n\n@EVALUATORS.register\nclass OxfordOverAll(EvaluatorBase):\n    """"""\n    A evaluator for Oxford mAP and recall computation.\n\n    Hyper-Params\n        gt_dir (str): the path of the oxford ground truth.\n        recall_k (sequence): positions of recalls to be calculated.\n    """"""\n    default_hyper_params = {\n        ""gt_dir"": ""/data/cbir/oxford/gt"",\n        ""recall_k"": [1, 2, 4, 8],\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(OxfordOverAll, self).__init__(hps)\n        assert os.path.exists(self._hyper_params[""gt_dir""]), \'the ground truth files must be existed!\'\n\n    @staticmethod\n    def _load_tag_set(file: str) -> set:\n        """"""\n        Read information from the txt file.\n\n        Args:\n            file (str): the path of the txt file.\n\n        Returns:\n            ret (set): the information.\n        """"""\n        ret = set()\n        with open(file, ""r"") as f:\n            for line in f.readlines():\n                ret.add(line.strip())\n        return ret\n\n    def compute_ap(self, query_tag: str, ranked_tags: List[str], recall_at_k: Dict) -> float:\n        """"""\n        Calculate the ap for one query.\n\n        Args:\n            query_tag (str): name of the query image.\n            ranked_tags (list): a list of label of the indexing results.\n            recall_at_k (dict): positions of recalls to be calculated.\n\n        Returns:\n            ap (float): ap for one query.\n        """"""\n        gt_prefix = os.path.join(self._hyper_params[""gt_dir""], query_tag)\n        good_set = self._load_tag_set(gt_prefix + ""_good.txt"")\n        ok_set = self._load_tag_set(gt_prefix + ""_ok.txt"")\n        junk_set = self._load_tag_set(gt_prefix + ""_junk.txt"")\n        pos_set = set.union(good_set, ok_set)\n\n        old_recall = 0.0\n        old_precision = 1.0\n        ap = 0.0\n        intersect_size = 0.0\n        i = 0\n        first_tp = -1\n\n        for tag in ranked_tags:\n            if tag in junk_set:\n                continue\n            if tag in pos_set:\n                intersect_size += 1\n\n                # Remember that in oxford query mode, the first element in rank_list is the query itself.\n                if first_tp == -1:\n                    first_tp = i\n\n            recall = intersect_size * 1.0 / len(pos_set)\n            precision = intersect_size / (i + 1.0)\n\n            ap += (recall - old_recall) * ((old_precision + precision) / 2.0)\n            old_recall = recall\n            old_precision = precision\n            i += 1\n\n        if first_tp != -1:\n            ks = self._hyper_params[""recall_k""]\n            for k in ks:\n                if k >= first_tp + 1:\n                    recall_at_k[k] = recall_at_k[k] + 1\n\n        return ap\n\n    def __call__(self, query_result: List, gallery_info: List) -> (float, Dict):\n        """"""\n        Calculate the mAP and recall for the indexing results.\n\n        Args:\n            query_result (list): a list of indexing results.\n            gallery_info (list): a list of gallery set information.\n\n        Returns:\n            tuple (float, dict): mean average precision and recall for each position.\n        """"""\n        aps = list()\n\n        recall_at_k = dict()\n        for k in self._hyper_params[""recall_k""]:\n            recall_at_k[k] = 0\n\n        for i in range(len(query_result)):\n            ranked_idx = query_result[i][""ranked_neighbors_idx""]\n            ranked_tags = list()\n            for idx in ranked_idx:\n                ranked_tags.append(gallery_info[idx][""label""])\n            aps.append(self.compute_ap(query_result[i][""query_name""], ranked_tags, recall_at_k))\n\n        mAP = np.mean(aps) * 100\n\n        for k in recall_at_k:\n            recall_at_k[k] = recall_at_k[k] * 100 / len(query_result)\n\n        return mAP, recall_at_k\n'"
pyretri/evaluate/evaluator/evaluators_impl/reid_overall.py,6,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nimport torch\n\nfrom ..evaluators_base import EvaluatorBase\nfrom ...registry import EVALUATORS\nfrom sklearn.metrics import average_precision_score\n\nfrom typing import Dict, List\n\n\n@EVALUATORS.register\nclass ReIDOverAll(EvaluatorBase):\n    """"""\n    A evaluator for Re-ID task mAP and recall computation.\n\n    Hyper-Params\n        recall_k (sequence): positions of recalls to be calculated.\n    """"""\n    default_hyper_params = {\n        ""recall_k"": [1, 2, 4, 8],\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(ReIDOverAll, self).__init__(hps)\n        self._hyper_params[""recall_k""] = np.sort(self._hyper_params[""recall_k""])\n\n    def compute_ap_cmc(self, index: np.ndarray, good_index: np.ndarray, junk_index: np.ndarray) -> (float, torch.tensor):\n        """"""\n        Calculate the ap and cmc for one query.\n\n        Args:\n            index (np.ndarray): the sorted retrieval index for one query.\n            good_index (np.ndarray): the index for good matching.\n            junk_index (np.ndarray): the index for junk matching.\n\n        Returns:\n            tupele (float, torch.tensor): (ap, cmc), ap and cmc for one query.\n        """"""\n        ap = 0\n        cmc = torch.IntTensor(len(index)).zero_()\n        if good_index.size == 0:\n            cmc[0] = -1\n            return ap, cmc\n\n        # remove junk_index\n        mask = np.in1d(index, junk_index, invert=True)\n        index = index[mask]\n\n        # find good_index index\n        ngood = len(good_index)\n        mask = np.in1d(index, good_index)\n        rows_good = np.argwhere(mask == True)\n        rows_good = rows_good.flatten()\n\n        cmc[rows_good[0]:] = 1\n        for i in range(ngood):\n            d_recall = 1.0 / ngood\n            precision = (i + 1) * 1.0 / (rows_good[i] + 1)\n            if rows_good[i] != 0:\n                old_precision = i * 1.0 / rows_good[i]\n            else:\n                old_precision = 1.0\n            ap = ap + d_recall * (old_precision + precision) / 2\n\n        return ap, cmc\n\n    def evaluate_once(self, index: np.ndarray, ql: int, qc: int, gl: np.ndarray, gc: np.ndarray) -> (float, torch.tensor):\n        """"""\n        Generate the indexes and calculate the ap and cmc for one query.\n\n        Args:\n            index (np.ndarray): the sorted retrieval index for one query.\n            ql (int): the person id of the query.\n            qc (int): the camera id of the query.\n            gl (np.ndarray): the person ids of the gallery set.\n            gc (np.ndarray): the camera ids of the gallery set.\n\n        Returns:\n            tuple (float, torch.tensor): ap and cmc for one query.\n        """"""\n        query_index = (ql == gl)\n        query_index = np.argwhere(query_index)\n\n        camera_index = (qc == gc)\n        camera_index = np.argwhere(camera_index)\n\n        # good index\n        good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n\n        # junk index\n        junk_index1 = np.argwhere(gl == -1)\n        junk_index2 = np.intersect1d(query_index, camera_index)\n        junk_index = np.append(junk_index2, junk_index1)\n\n        AP_tmp, CMC_tmp = self.compute_ap_cmc(index, good_index, junk_index)\n\n        return AP_tmp, CMC_tmp\n\n    def __call__(self, query_result: List, gallery_info: List) -> (float, Dict):\n        """"""\n        Calculate the mAP and recall for the indexing results.\n\n        Args:\n            query_result (list): a list of indexing results.\n            gallery_info (list): a list of gallery set information.\n\n        Returns:\n            tuple (float, dict): mean average precision and recall for each position.\n        """"""\n        AP = 0.0\n        CMC = torch.IntTensor(range(len(gallery_info))).zero_()\n        gallery_label = np.array([int(gallery_info[idx][""label""]) for idx in range(len(gallery_info))])\n        gallery_cam = np.array([int(gallery_info[idx][""cam""]) for idx in range(len(gallery_info))])\n\n        recall_at_k = dict()\n        for k in self._hyper_params[""recall_k""]:\n            recall_at_k[k] = 0\n\n        for i in range(len(query_result)):\n            AP_tmp, CMC_tmp = self.evaluate_once(np.array(query_result[i][""ranked_neighbors_idx""]),\n                                                 int(query_result[i][""label""]), int(query_result[i][""cam""]),\n                                                 gallery_label, gallery_cam)\n            if CMC_tmp[0] == -1:\n                continue\n            CMC = CMC + CMC_tmp\n            AP += AP_tmp\n\n        CMC = CMC.float()\n        CMC = CMC / len(query_result)  # average CMC\n\n        for k in recall_at_k:\n            recall_at_k[k] = (CMC[k-1] * 100).item()\n\n        mAP = AP / len(query_result) * 100\n\n        return mAP, recall_at_k\n\n'"
pyretri/extract/aggregator/aggregators_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/extract/aggregator/aggregators_impl/crow.py,2,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\n\nfrom typing import Dict\n\n@AGGREGATORS.register\nclass Crow(AggregatorBase):\n    """"""\n    Cross-dimensional Weighting for Aggregated Deep Convolutional Features.\n    c.f. https://arxiv.org/pdf/1512.04065.pdf\n\n    Hyper-Params\n        spatial_a (float): hyper-parameter for calculating spatial weight.\n        spatial_b (float): hyper-parameter for calculating spatial weight.\n    """"""\n    default_hyper_params = {\n        ""spatial_a"": 2.0,\n        ""spatial_b"": 2.0,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        self.first_show = True\n        super(Crow, self).__init__(hps)\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        spatial_a = self._hyper_params[""spatial_a""]\n        spatial_b = self._hyper_params[""spatial_b""]\n\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                spatial_weight = fea.sum(dim=1, keepdims=True)\n                z = (spatial_weight ** spatial_a).sum(dim=(2, 3), keepdims=True)\n                z = z ** (1.0 / spatial_a)\n                spatial_weight = (spatial_weight / z) ** (1.0 / spatial_b)\n\n                c, w, h = fea.shape[1:]\n                nonzeros = (fea!=0).float().sum(dim=(2, 3)) / 1.0 / (w * h) + 1e-6\n                channel_weight = torch.log(nonzeros.sum(dim=1, keepdims=True) / nonzeros)\n\n                fea = fea * spatial_weight\n                fea = fea.sum(dim=(2, 3))\n                fea = fea * channel_weight\n\n                ret[key + ""_{}"".format(self.__class__.__name__)] = fea\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[Crow Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/aggregator/aggregators_impl/gap.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\n\nfrom typing import Dict\n\n@AGGREGATORS.register\nclass GAP(AggregatorBase):\n    """"""\n    Global average pooling.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        self.first_show = True\n        super(GAP, self).__init__(hps)\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                fea = fea.mean(dim=3).mean(dim=2)\n                ret[key + ""_{}"".format(self.__class__.__name__)] = fea\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[GAP Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/aggregator/aggregators_impl/gem.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\n\nfrom typing import Dict\n\n@AGGREGATORS.register\nclass GeM(AggregatorBase):\n    """"""\n    Generalized-mean pooling.\n    c.f. https://pdfs.semanticscholar.org/a2ca/e0ed91d8a3298b3209fc7ea0a4248b914386.pdf\n\n    Hyper-Params\n        p (float): hyper-parameter for calculating generalized mean. If p = 1, GeM is equal to global average pooling, and\n            if p = +infinity, GeM is equal to global max pooling.\n    """"""\n    default_hyper_params = {\n        ""p"": 3.0,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        self.first_show = True\n        super(GeM, self).__init__(hps)\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        p = self._hyper_params[""p""]\n\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                fea = fea ** p\n                h, w = fea.shape[2:]\n                fea = fea.sum(dim=(2, 3)) * 1.0 / w / h\n                fea = fea ** (1.0 / p)\n                ret[key + ""_{}"".format(self.__class__.__name__)] = fea\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[GeM Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/aggregator/aggregators_impl/gmp.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\n\nfrom typing import Dict\n\n@AGGREGATORS.register\nclass GMP(AggregatorBase):\n    """"""\n    Global maximum pooling\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        self.first_show = True\n        super(GMP, self).__init__(hps)\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                fea = (fea.max(dim=3)[0]).max(dim=2)[0]\n                ret[key + ""_{}"".format(self.__class__.__name__)] = fea\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[GMP Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/aggregator/aggregators_impl/pwa.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\nfrom ....index.utils import feature_loader\n\nfrom typing import Dict\n\n@AGGREGATORS.register\nclass PWA(AggregatorBase):\n    """"""\n    Part-based Weighting Aggregation.\n    c.f. https://arxiv.org/abs/1705.01247\n\n    Hyper-Params\n        train_fea_dir (str): path of feature dir for selecting channels.\n        n_proposal (int): number of proposals to be selected.\n        alpha (float): alpha for calculate spatial weight.\n        beta (float): beta for calculate spatial weight.\n    """"""\n\n    default_hyper_params = {\n        ""train_fea_dir"": """",\n        ""n_proposal"": 25,\n        ""alpha"": 2.0,\n        ""beta"": 2.0,\n        ""train_fea_names"": [""pool5_GAP""],\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(PWA, self).__init__(hps)\n        self.first_show = True\n        assert self._hyper_params[""train_fea_dir""] != """"\n        self.selected_proposals_idx = None\n        self.train()\n\n    def train(self) -> None:\n        n_proposal = self._hyper_params[""n_proposal""]\n        stacked_fea, _, pos_info = feature_loader.load(\n            self._hyper_params[""train_fea_dir""],\n            self._hyper_params[""train_fea_names""]\n        )\n        self.selected_proposals_idx = dict()\n        for fea_name in pos_info:\n            st_idx, ed_idx = pos_info[fea_name]\n            fea = stacked_fea[:, st_idx: ed_idx]\n            assert fea.ndim == 2, ""invalid train feature""\n            channel_variance = np.std(fea, axis=0)\n            selected_idx = channel_variance.argsort()[-n_proposal:]\n            fea_name = ""_"".join(fea_name.split(""_"")[:-1])\n            self.selected_proposals_idx[fea_name] = selected_idx.tolist()\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        alpha, beta = self._hyper_params[""alpha""], self._hyper_params[""beta""]\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                assert (key in self.selected_proposals_idx), \'{} is not in the {}\'.format(key, self.selected_proposals_idx.keys())\n                proposals_idx = np.array(self.selected_proposals_idx[key])\n                proposals = fea[:, proposals_idx, :, :]\n                power_norm = (proposals ** alpha).sum(dim=(2, 3), keepdims=True) ** (1.0 / alpha)\n                normed_proposals = (proposals / (power_norm + 1e-5)) ** (1.0 / beta)\n                fea = (fea[:, None, :, :, :] * normed_proposals[:, :, None, :, :]).sum(dim=(3, 4))\n                fea = fea.view(fea.shape[0], -1)\n                ret[key + ""_{}"".format(self.__class__.__name__)] = fea\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[PWA Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/aggregator/aggregators_impl/r_mac.py,2,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\n\nfrom typing import Dict, List\n\n@AGGREGATORS.register\nclass RMAC(AggregatorBase):\n    """"""\n    Regional Maximum activation of convolutions (R-MAC).\n    c.f. https://arxiv.org/pdf/1511.05879.pdf\n\n    Hyper-Params\n        level_n (int): number of levels for selecting regions.\n    """"""\n\n    default_hyper_params = {\n        ""level_n"": 3,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(RMAC, self).__init__(hps)\n        self.first_show = True\n        self.cached_regions = dict()\n\n    def _get_regions(self, h: int, w: int) -> List:\n        """"""\n        Divide the image into several regions.\n\n        Args:\n            h (int): height for dividing regions.\n            w (int): width for dividing regions.\n\n        Returns:\n            regions (List): a list of region positions.\n        """"""\n        if (h, w) in self.cached_regions:\n            return self.cached_regions[(h, w)]\n\n        m = 1\n        n_h, n_w = 1, 1\n        regions = list()\n        if h != w:\n            min_edge = min(h, w)\n            left_space = max(h, w) - min(h, w)\n            iou_target = 0.4\n            iou_best = 1.0\n            while True:\n                iou_tmp = (min_edge ** 2 - min_edge * (left_space // m)) / (min_edge ** 2)\n\n                # small m maybe result in non-overlap\n                if iou_tmp <= 0:\n                    m += 1\n                    continue\n\n                if abs(iou_tmp - iou_target) <= iou_best:\n                    iou_best = abs(iou_tmp - iou_target)\n                    m += 1\n                else:\n                    break\n            if h < w:\n                n_w = m\n            else:\n                n_h = m\n\n        for i in range(self._hyper_params[""level_n""]):\n            region_width = int(2 * 1.0 / (i + 2) * min(h, w))\n            step_size_h = (h - region_width) // n_h\n            step_size_w = (w - region_width) // n_w\n\n            for x in range(n_h):\n                for y in range(n_w):\n                    st_x = step_size_h * x\n                    ed_x = st_x + region_width - 1\n                    assert ed_x < h\n                    st_y = step_size_w * y\n                    ed_y = st_y + region_width - 1\n                    assert ed_y < w\n                    regions.append((st_x, st_y, ed_x, ed_y))\n\n            n_h += 1\n            n_w += 1\n\n        self.cached_regions[(h, w)] = regions\n        return regions\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                h, w = fea.shape[2:]\n                final_fea = None\n                regions = self._get_regions(h, w)\n                for _, r in enumerate(regions):\n                    st_x, st_y, ed_x, ed_y = r\n                    region_fea = (fea[:, :, st_x: ed_x, st_y: ed_y].max(dim=3)[0]).max(dim=2)[0]\n                    region_fea = region_fea / torch.norm(region_fea, dim=1, keepdim=True)\n                    if final_fea is None:\n                        final_fea = region_fea\n                    else:\n                        final_fea = final_fea + region_fea\n                ret[key + ""_{}"".format(self.__class__.__name__)] = final_fea\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[RMAC Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/aggregator/aggregators_impl/scda.py,7,"b'# -*- coding: utf-8 -*-\n\nimport queue\n\nimport torch\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\n\nfrom typing import Dict\n\n\n@AGGREGATORS.register\nclass SCDA(AggregatorBase):\n    """"""\n    Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval.\n    c.f. http://www.weixiushen.com/publication/tip17SCDA.pdf\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(SCDA, self).__init__(hps)\n        self.first_show = True\n\n    def bfs(self, x: int, y: int, mask: torch.tensor, cc_map: torch.tensor, cc_id: int) -> int:\n        dirs = [[1, 0], [-1, 0], [0, 1], [0, -1]]\n        q = queue.LifoQueue()\n        q.put((x, y))\n\n        ret = 1\n        cc_map[x][y] = cc_id\n\n        while not q.empty():\n            x, y = q.get()\n\n            for (dx, dy) in dirs:\n                new_x = x + dx\n                new_y = y + dy\n                if 0 <= new_x < mask.shape[0] and 0 <= new_y < mask.shape[1]:\n                    if mask[new_x][new_y] == 1 and cc_map[new_x][new_y] == 0:\n                        q.put((new_x, new_y))\n                        ret += 1\n                        cc_map[new_x][new_y] = cc_id\n        return ret\n\n    def find_max_cc(self, mask: torch.tensor) -> torch.tensor:\n        """"""\n        Find the largest connected component of the mask\xe3\x80\x82\n\n        Args:\n            mask (torch.tensor): the original mask.\n\n        Returns:\n            mask (torch.tensor): the mask only containing the maximum connected component.\n        """"""\n        assert mask.ndim == 4\n        assert mask.shape[1] == 1\n        mask = mask[:, 0, :, :]\n        for i in range(mask.shape[0]):\n            m = mask[i]\n            cc_map = torch.zeros(m.shape)\n            cc_num = list()\n\n            for x in range(m.shape[0]):\n                for y in range(m.shape[1]):\n                    if m[x][y] == 1 and cc_map[x][y] == 0:\n                        cc_id = len(cc_num) + 1\n                        cc_num.append(self.bfs(x, y, m, cc_map, cc_id))\n\n            max_cc_id = cc_num.index(max(cc_num)) + 1\n            m[cc_map != max_cc_id] = 0\n        mask = mask[:, None, :, :]\n        return mask\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                mask = fea.sum(dim=1, keepdims=True)\n                thres = mask.mean(dim=(2, 3), keepdims=True)\n                mask[mask <= thres] = 0\n                mask[mask > thres] = 1\n                mask = self.find_max_cc(mask)\n                fea = fea * mask\n\n                gap = fea.mean(dim=(2, 3))\n                gmp, _ = fea.max(dim=3)\n                gmp, _ = gmp.max(dim=2)\n\n                ret[key + ""_{}"".format(self.__class__.__name__)] = torch.cat([gap, gmp], dim=1)\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[SCDA Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/aggregator/aggregators_impl/spoc.py,5,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\n\nfrom ..aggregators_base import AggregatorBase\nfrom ...registry import AGGREGATORS\n\nfrom typing import Dict\n\n@AGGREGATORS.register\nclass SPoC(AggregatorBase):\n    """"""\n    SPoC with center prior.\n    c.f. https://arxiv.org/pdf/1510.07493.pdf\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(SPoC, self).__init__(hps)\n        self.first_show = True\n        self.spatial_weight_cache = dict()\n\n    def __call__(self, features: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            if fea.ndimension() == 4:\n                h, w = fea.shape[2:]\n                if (h, w) in self.spatial_weight_cache:\n                    spatial_weight = self.spatial_weight_cache[(h, w)]\n                else:\n                    sigma = min(h, w) / 2.0 / 3.0\n                    x = torch.Tensor(range(w))\n                    y = torch.Tensor(range(h))[:, None]\n                    spatial_weight = torch.exp(-((x - (w - 1) / 2.0) ** 2 + (y - (h - 1) / 2.0) ** 2) / 2.0 / (sigma ** 2))\n                    if torch.cuda.is_available():\n                        spatial_weight = spatial_weight.cuda()\n                    spatial_weight = spatial_weight[None, None, :, :]\n                    self.spatial_weight_cache[(h, w)] = spatial_weight\n                fea = (fea * spatial_weight).sum(dim=(2, 3))\n                ret[key + ""_{}"".format(self.__class__.__name__)] = fea\n            else:\n                # In case of fc feature.\n                assert fea.ndimension() == 2\n                if self.first_show:\n                    print(""[SPoC Aggregator]: find 2-dimension feature map, skip aggregation"")\n                    self.first_show = False\n                ret[key] = fea\n        return ret\n'"
pyretri/extract/extractor/extractors_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/extract/extractor/extractors_impl/reid_series.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch.nn as nn\n\nfrom ..extractors_base import ExtractorBase\nfrom ...registry import EXTRACTORS\n\nfrom typing import Dict\n\n@EXTRACTORS.register\nclass ReIDSeries(ExtractorBase):\n    """"""\n    The extractors for reid baseline models.\n\n    Hyper-Parameters\n        extract_features (list): indicates which feature maps to output. See available_feas for available feature maps.\n            If it is [""all""], then all available features will be output.\n    """"""\n    default_hyper_params = {\n        ""extract_features"": list(),\n    }\n\n    available_feas = [""output""]\n\n    def __init__(self, model: nn.Module, hps: Dict or None = None):\n        """"""\n        Args:\n            model (nn.Module): the model for extracting features.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        children = list(model.children())\n        feature_modules = {\n            ""output"": children[1].add_block,\n        }\n        super(ReIDSeries, self).__init__(model, feature_modules, hps)\n'"
pyretri/extract/extractor/extractors_impl/res_series.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ..extractors_base import ExtractorBase\nfrom ...registry import EXTRACTORS\n\nfrom typing import Dict\n\n@EXTRACTORS.register\nclass ResSeries(ExtractorBase):\n    """"""\n    The extractors for ResNet.\n\n    Hyper-Parameters\n        extract_features (list): indicates which feature maps to output. See available_feas for available feature maps.\n            If it is [""all""], then all available features will be output.\n    """"""\n    default_hyper_params = {\n        ""extract_features"": list(),\n    }\n\n    available_feas = [""pool5"", ""pool4"", ""pool3""]\n\n    def __init__(self, model, hps: Dict or None = None):\n        """"""\n        Args:\n            model (nn.Module): the model for extracting features.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        children = list(model.children())\n        feature_modules = {\n            ""pool5"": children[-3][-1].relu,\n            ""pool4"": children[-4][-1].relu,\n            ""pool3"": children[-5][-1].relu\n        }\n        super(ResSeries, self).__init__(model, feature_modules, hps)\n'"
pyretri/extract/extractor/extractors_impl/vgg_series.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom ..extractors_base import ExtractorBase\nfrom ...registry import EXTRACTORS\n\nfrom typing import Dict\n\n@EXTRACTORS.register\nclass VggSeries(ExtractorBase):\n    """"""\n    The extractors for VGG Net.\n\n    Hyper-Parameters\n        extract_features (list): indicates which feature maps to output. See available_feas for available feature maps.\n            If it is [""all""], then all available features will be output.\n    """"""\n    default_hyper_params = {\n        ""extract_features"": list(),\n    }\n\n    available_feas = [""fc"", ""pool5"", ""pool4""]\n\n    def __init__(self, model, hps: Dict or None = None):\n        """"""\n        Args:\n            model (nn.Module): the model for extracting features.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        feature_modules = {\n            ""fc"": model.classifier[4],\n            ""pool5"": model.features[-1],\n            ""pool4"": model.features[23]\n        }\n        super(VggSeries, self).__init__(model, feature_modules, hps)\n'"
pyretri/extract/splitter/splitter_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/extract/splitter/splitter_impl/identity.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\n\nfrom ..splitter_base import SplitterBase\nfrom ...registry import SPLITTERS\n\nfrom typing import Dict\n\n\n@SPLITTERS.register\nclass Identity(SplitterBase):\n    """"""\n    Directly return feature maps without any operations.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(Identity, self).__init__(hps)\n\n    def __call__(self, features: torch.tensor) -> torch.tensor:\n        return features\n'"
pyretri/extract/splitter/splitter_impl/pcb.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\n\nfrom ..splitter_base import SplitterBase\nfrom ...registry import SPLITTERS\n\nfrom typing import Dict\n\n\n@SPLITTERS.register\nclass PCB(SplitterBase):\n    """"""\n    PCB function to split feature maps.\n    c.f. http://openaccess.thecvf.com/content_ECCV_2018/papers/Yifan_Sun_Beyond_Part_Models_ECCV_2018_paper.pdf\n\n    Hyper-Params:\n        stripe_num (int): the number of stripes divided.\n    """"""\n    default_hyper_params = {\n        \'stripe_num\': 2,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(PCB, self).__init__(hps)\n\n    def __call__(self, features: torch.tensor) -> Dict:\n        ret = dict()\n        for key in features:\n            fea = features[key]\n            assert fea.ndimension() == 4\n            assert self.default_hyper_params[""stripe_num""] <= fea.shape[2], \\\n                \'stripe num must be less than or equal to the height of fea\'\n\n            stride = fea.shape[2] // self.default_hyper_params[""stripe_num""]\n\n            for i in range(int(self.default_hyper_params[""stripe_num""])):\n                ret[key + ""_part_{}"".format(i)] = fea[:, :, stride * i: stride * (i + 1), :]\n            ret[key + ""_global""] = fea\n        return ret\n'"
pyretri/index/dim_processor/dim_processors_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/index/dim_processor/dim_processors_impl/identity.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..dim_processors_base import DimProcessorBase\nfrom ...registry import DIMPROCESSORS\n\nfrom typing import Dict, List\n\n@DIMPROCESSORS.register\nclass Identity(DimProcessorBase):\n    """"""\n    Directly return feature without any dimension process operations.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(Identity, self).__init__(feature_names, hps)\n\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        return fea\n'"
pyretri/index/dim_processor/dim_processors_impl/l2_normalize.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..dim_processors_base import DimProcessorBase\nfrom ...registry import DIMPROCESSORS\nfrom sklearn.preprocessing import normalize\n\nfrom typing import Dict, List\n\n@DIMPROCESSORS.register\nclass L2Normalize(DimProcessorBase):\n    """"""\n    L2 normalize the features.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(L2Normalize, self).__init__(feature_names, hps)\n\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        return normalize(fea, norm=""l2"")\n'"
pyretri/index/dim_processor/dim_processors_impl/part_pca.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..dim_processors_base import DimProcessorBase\nfrom ...registry import DIMPROCESSORS\nfrom ...utils import feature_loader\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA as SKPCA\n\nfrom typing import Dict, List\n\n@DIMPROCESSORS.register\nclass PartPCA(DimProcessorBase):\n    """"""\n    Part PCA will divided whole feature into several parts. Then apply PCA transformation to each part.\n    It is usually used for features that extracted by several feature maps and concatenated together.\n\n    Hyper-Params:\n        proj_dim (int): the dimension after reduction. If it is 0, then no reduction will be done.\n        whiten (bool): whether do whiten for each part.\n        train_fea_dir (str): the path of features for training PCA.\n        l2 (bool): whether do l2-normalization for the training features.\n    """"""\n    default_hyper_params = {\n        ""proj_dim"": 0,\n        ""whiten"": True,\n        ""train_fea_dir"": ""unknown"",\n        ""l2"": True,\n    }\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(PartPCA, self).__init__(feature_names, hps)\n\n        self.pcas = dict()\n        self._train(self._hyper_params[""train_fea_dir""])\n\n    def _train(self, fea_dir: str) -> None:\n        """"""\n        Train the part PCA.\n\n        Args:\n            fea_dir (str): the path of features for training part PCA.\n        """"""\n        fea, _, pos_info = feature_loader.load(fea_dir, self.feature_names)\n        fea_names = list(pos_info.keys())\n        ori_dim = fea.shape[1]\n\n        already_proj_dim = 0\n        for fea_name in fea_names:\n            st_idx, ed_idx = pos_info[fea_name][0], pos_info[fea_name][1]\n            ori_part_dim = ed_idx - st_idx\n            if self._hyper_params[""proj_dim""] == 0:\n                proj_part_dim = ori_part_dim\n            else:\n                ratio = self._hyper_params[""proj_dim""] * 1.0 / ori_dim\n                if fea_name != fea_names[-1]:\n                    proj_part_dim = int(ori_part_dim * ratio)\n                else:\n                    proj_part_dim = self._hyper_params[""proj_dim""] - already_proj_dim\n                    assert proj_part_dim <= ori_part_dim, ""reduction dimension can not be distributed to each part!""\n                already_proj_dim += proj_part_dim\n\n            pca = SKPCA(n_components=proj_part_dim, whiten=self._hyper_params[""whiten""])\n            train_fea = fea[:, st_idx: ed_idx]\n            if self._hyper_params[""l2""]:\n                train_fea = normalize(train_fea, norm=""l2"")\n            pca.fit(train_fea)\n            self.pcas[fea_name] = {\n                ""pos"": (st_idx, ed_idx),\n                ""pca"": pca\n            }\n\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        fea_names = np.sort(list(self.pcas.keys()))\n        ret = list()\n        for fea_name in fea_names:\n            st_idx, ed_idx = self.pcas[fea_name][""pos""][0], self.pcas[fea_name][""pos""][1]\n            pca = self.pcas[fea_name][""pca""]\n\n            ori_fea = fea[:, st_idx: ed_idx]\n            proj_fea = normalize(ori_fea, norm=\'l2\')\n            proj_fea = pca.transform(proj_fea)\n            proj_fea = normalize(proj_fea, norm=\'l2\')\n\n            ret.append(proj_fea)\n\n        ret = np.concatenate(ret, axis=1)\n        return ret'"
pyretri/index/dim_processor/dim_processors_impl/part_svd.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..dim_processors_base import DimProcessorBase\nfrom ...registry import DIMPROCESSORS\nfrom ...utils import feature_loader\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import TruncatedSVD as SKSVD\n\nfrom typing import Dict, List\n\n@DIMPROCESSORS.register\nclass PartSVD(DimProcessorBase):\n    """"""\n    Part SVD will divided whole feature into several parts. Then apply SVD transformation to each part.\n    It is usually used for features that extracted by several feature maps and concatenated together.\n\n    Hyper-Params:\n        proj_dim (int):  the dimension after reduction. If it is 0, then no reduction will be done\n            (in SVD, we will minus origin dimension by 1).\n        whiten (bool): whether do whiten for each part.\n        train_fea_dir (str): the path of features for training SVD.\n        l2 (bool): whether do l2-normalization for the training features.\n    """"""\n    default_hyper_params = {\n        ""proj_dim"": 0,\n        ""whiten"": True,\n        ""train_fea_dir"": ""unknown"",\n        ""l2"": True,\n    }\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(PartSVD, self).__init__(feature_names, hps)\n\n        self.svds = dict()\n        self._train(self._hyper_params[""train_fea_dir""])\n\n    def _train(self, fea_dir: str) -> None:\n        """"""\n        Train the part SVD.\n\n        Args:\n            fea_dir (str): the path of features for training part SVD.\n        """"""\n        fea, _, pos_info = feature_loader.load(fea_dir, self.feature_names)\n        fea_names = list(pos_info.keys())\n        ori_dim = fea.shape[1]\n\n        already_proj_dim = 0\n        for fea_name in fea_names:\n            st_idx, ed_idx = pos_info[fea_name][0], pos_info[fea_name][1]\n            ori_part_dim = ed_idx - st_idx\n            if self._hyper_params[""proj_dim""] == 0:\n                proj_part_dim = ori_part_dim - 1\n            else:\n                ratio = self._hyper_params[""proj_dim""] * 1.0 / ori_dim\n                if fea_name != fea_names[-1]:\n                    proj_part_dim = int(ori_part_dim * ratio)\n                else:\n                    proj_part_dim = self._hyper_params[""proj_dim""] - already_proj_dim\n                    assert proj_part_dim < ori_part_dim, ""reduction dimension can not be distributed to each part!""\n                already_proj_dim += proj_part_dim\n\n            svd = SKSVD(n_components=proj_part_dim)\n            train_fea = fea[:, st_idx: ed_idx]\n            if self._hyper_params[""l2""]:\n                train_fea = normalize(train_fea, norm=""l2"")\n            train_fea = svd.fit_transform(train_fea)\n            std = train_fea.std(axis=0, keepdims=True)\n            self.svds[fea_name] = {\n                ""pos"": (st_idx, ed_idx),\n                ""svd"": svd,\n                ""std"": std\n            }\n\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        fea_names = np.sort(list(self.svds.keys()))\n        ret = list()\n\n        for fea_name in fea_names:\n            st_idx, ed_idx = self.svds[fea_name][""pos""][0], self.svds[fea_name][""pos""][1]\n            svd = self.svds[fea_name][""svd""]\n\n            proj_fea = fea[:, st_idx: ed_idx]\n            proj_fea = normalize(proj_fea, norm=\'l2\')\n            proj_fea = svd.transform(proj_fea)\n            if self._hyper_params[""whiten""]:\n                proj_fea = proj_fea / (self.svds[fea_name][""std""] + 1e-6)\n            proj_fea = normalize(proj_fea, norm=\'l2\')\n\n            ret.append(proj_fea)\n\n        ret = np.concatenate(ret, axis=1)\n        return ret\n\n\n'"
pyretri/index/dim_processor/dim_processors_impl/pca.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..dim_processors_base import DimProcessorBase\nfrom ...registry import DIMPROCESSORS\nfrom ...utils import feature_loader\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA as SKPCA\n\nfrom typing import Dict, List\n\n@DIMPROCESSORS.register\nclass PCA(DimProcessorBase):\n    """"""\n    Do the PCA transformation for dimension reduction.\n\n    Hyper-Params:\n        proj_dim (int): the dimension after reduction. If it is 0, then no reduction will be done.\n        whiten (bool): whether do whiten.\n        train_fea_dir (str): the path of features for training PCA.\n        l2 (bool): whether do l2-normalization for the training features.\n    """"""\n    default_hyper_params = {\n        ""proj_dim"": 0,\n        ""whiten"": True,\n        ""train_fea_dir"": ""unknown"",\n        ""l2"": True,\n    }\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(PCA, self).__init__(feature_names, hps)\n\n        self.pca = SKPCA(n_components=self._hyper_params[""proj_dim""], whiten=self._hyper_params[""whiten""])\n        self._train(self._hyper_params[""train_fea_dir""])\n\n    def _train(self, fea_dir: str) -> None:\n        """"""\n        Train the PCA.\n\n        Args:\n            fea_dir (str): the path of features for training PCA.\n        """"""\n        train_fea, _, _ = feature_loader.load(fea_dir, self.feature_names)\n        if self._hyper_params[""l2""]:\n            train_fea = normalize(train_fea, norm=""l2"")\n        self.pca.fit(train_fea)\n\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        ori_fea = fea\n        proj_fea = self.pca.transform(ori_fea)\n        return proj_fea\n\n\n'"
pyretri/index/dim_processor/dim_processors_impl/rmac_pca.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..dim_processors_base import DimProcessorBase\nfrom ...registry import DIMPROCESSORS\nfrom ...utils import feature_loader\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\n\nfrom typing import Dict, List\n\n@DIMPROCESSORS.register\nclass RMACPCA(DimProcessorBase):\n    """"""\n    Do the PCA transformation for R-MAC only.\n    When call this transformation, each part feature is processed by l2-normalize, PCA and l2-normalize.\n    Then the global feature is processed by l2-normalize.\n\n    Hyper-Params:\n        proj_dim: int. The dimension after reduction. If it is 0, then no reduction will be done.\n        whiten: bool, whether do whiten for each part.\n        train_fea: str, feature directory for training PCA.\n        l2 (bool): whether do l2-normalization for the training features.\n    """"""\n    default_hyper_params = {\n        ""proj_dim"": 0,\n        ""whiten"": True,\n        ""train_fea_dir"": ""unknown"",\n        ""l2"": True,\n    }\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(RMACPCA, self).__init__(feature_names, hps)\n\n        self.pca = dict()\n        self._train(self._hyper_params[""train_fea_dir""])\n\n    def _train(self, fea_dir: str) -> None:\n        """"""\n        Train the PCA for R-MAC.\n\n        Args:\n            fea_dir (str): the path of features for training PCA.\n        """"""\n        fea, _, pos_info = feature_loader.load(fea_dir, self.feature_names)\n\n        fea_names = np.sort(list(pos_info.keys()))\n        region_feas = list()\n        for fea_name in fea_names:\n            st_idx, ed_idx = pos_info[fea_name][0], pos_info[fea_name][1]\n            region_fea = fea[:, st_idx: ed_idx]\n            region_feas.append(region_fea)\n\n        train_fea = np.concatenate(region_feas, axis=0)\n        if self._hyper_params[""l2""]:\n            train_fea = normalize(train_fea, norm=""l2"")\n        pca = PCA(n_components=self._hyper_params[""proj_dim""], whiten=self._hyper_params[""whiten""])\n        pca.fit(train_fea)\n\n        self.pca = {\n            ""pca"": pca,\n            ""pos_info"": pos_info\n        }\n\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        pca = self.pca[""pca""]\n        pos_info = self.pca[""pos_info""]\n\n        fea_names = np.sort(list(pos_info.keys()))\n\n        final_fea = None\n        for fea_name in fea_names:\n            st_idx, ed_idx = pos_info[fea_name][0], pos_info[fea_name][1]\n            region_fea = fea[:, st_idx: ed_idx]\n            region_fea = normalize(region_fea)\n            region_fea = pca.transform(region_fea)\n            region_fea = normalize(region_fea)\n            if final_fea is None:\n                final_fea = region_fea\n            else:\n                final_fea += region_fea\n        final_fea = normalize(final_fea)\n\n        return final_fea'"
pyretri/index/dim_processor/dim_processors_impl/svd.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom ..dim_processors_base import DimProcessorBase\nfrom ...registry import DIMPROCESSORS\nfrom ...utils import feature_loader\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import TruncatedSVD as SKSVD\n\nfrom typing import Dict, List\n\n@DIMPROCESSORS.register\nclass SVD(DimProcessorBase):\n    """"""\n    Do the SVD transformation for dimension reduction.\n\n    Hyper-Params:\n        proj_dim (int):  the dimension after reduction. If it is 0, then no reduction will be done\n            (in SVD, we will minus origin dimension by 1).\n        whiten (bool): whether do whiten for each part.\n        train_fea_dir (str): the path of features for training SVD.\n        l2 (bool): whether do l2-normalization for the training features.\n    """"""\n    default_hyper_params = {\n        ""proj_dim"": 0,\n        ""whiten"": True,\n        ""train_fea_dir"": ""unknown"",\n        ""l2"": True,\n    }\n\n    def __init__(self, feature_names: List[str], hps: Dict or None = None):\n        """"""\n        Args:\n            feature_names (list): a list of features names to be loaded.\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(SVD, self).__init__(feature_names, hps)\n\n        self.svd = SKSVD(n_components=self._hyper_params[""proj_dim""])\n        self.std = 0.0\n        self._train(self._hyper_params[""train_fea_dir""])\n\n    def _train(self, fea_dir: str) -> None:\n        """"""\n        Train the SVD.\n\n        Args:\n            fea_dir: the path of features for training SVD.\n        """"""\n        train_fea, _, _ = feature_loader.load(fea_dir, self.feature_names)\n        if self._hyper_params[""l2""]:\n            train_fea = normalize(train_fea, norm=""l2"")\n        train_fea = self.svd.fit_transform(train_fea)\n        self.std = train_fea.std(axis=0, keepdims=True)\n\n    def __call__(self, fea: np.ndarray) -> np.ndarray:\n        ori_fea = fea\n        proj_fea = self.svd.transform(ori_fea)\n        if self._hyper_params[""whiten""]:\n            proj_fea = proj_fea / (self.std + 1e-6)\n        return proj_fea\n'"
pyretri/index/feature_enhancer/feature_enhancer_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/index/feature_enhancer/feature_enhancer_impl/database_augmentation.py,2,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..feature_enhancer_base import EnhanceBase\nfrom ...registry import ENHANCERS\nfrom ...metric import KNN\n\nfrom typing import Dict\n\n@ENHANCERS.register\nclass DBA(EnhanceBase):\n    """"""\n    Every feature in the database is replaced with a weighted sum of the point \xe2\x80\x99s own value and those of its top k nearest neighbors (k-NN).\n    c.f. https://www.robots.ox.ac.uk/~vgg/publications/2012/Arandjelovic12/arandjelovic12.pdf\n\n    Hyper-Params:\n        enhance_k (int): number of the nearest points to be calculated.\n    """"""\n    default_hyper_params = {\n        ""enhance_k"": 10,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(DBA, self).__init__(hps)\n        knn_hps = {\n            ""top_k"": self._hyper_params[""enhance_k""] + 1,\n        }\n        self.knn = KNN(knn_hps)\n\n    def __call__(self, feature: torch.tensor) -> torch.tensor:\n        _, sorted_idx = self.knn(feature, feature)\n        sorted_idx = sorted_idx[:, 1:].reshape(-1)\n\n        arg_fea = feature[sorted_idx].view(feature.shape[0], -1, feature.shape[1]).sum(dim=1)\n        feature = feature + arg_fea\n\n        feature = feature / torch.norm(feature, dim=1, keepdim=True)\n\n        return feature\n'"
pyretri/index/feature_enhancer/feature_enhancer_impl/identity.py,1,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..feature_enhancer_base import EnhanceBase\nfrom ...registry import ENHANCERS\n\nfrom typing import Dict\n\n@ENHANCERS.register\nclass Identity(EnhanceBase):\n    """"""\n    Directly return features without any feature enhance operations.\n    """"""\n    default_hyper_params = {}\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(Identity, self).__init__(hps)\n\n    def __call__(self, feature: torch.tensor) -> torch.tensor:\n        return feature\n'"
pyretri/index/metric/metric_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/index/metric/metric_impl/knn.py,6,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..metric_base import MetricBase\nfrom ...registry import METRICS\n\nfrom typing import Dict\n\n@METRICS.register\nclass KNN(MetricBase):\n    """"""\n    Similarity measure based on the euclidean distance.\n\n    Hyper-Params:\n        top_k (int): top_k nearest neighbors will be output in sorted order. If it is 0, all neighbors will be output.\n    """"""\n    default_hyper_params = {\n        ""top_k"": 0,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(KNN, self).__init__(hps)\n\n    def _cal_dis(self, query_fea: torch.tensor, gallery_fea: torch.tensor) -> torch.tensor:\n        """"""\n        Calculate the distance between query set features and gallery set features.\n\n        Args:\n            query_fea (torch.tensor): query set features.\n            gallery_fea (torch.tensor): gallery set features.\n\n        Returns:\n            dis (torch.tensor): the distance between query set features and gallery set features.\n        """"""\n        query_fea = query_fea.transpose(1, 0)\n        inner_dot = gallery_fea.mm(query_fea)\n        dis = (gallery_fea ** 2).sum(dim=1, keepdim=True) + (query_fea ** 2).sum(dim=0, keepdim=True)\n        dis = dis - 2 * inner_dot\n        dis = dis.transpose(1, 0)\n        return dis\n\n    def __call__(self, query_fea: torch.tensor, gallery_fea: torch.tensor) -> (torch.tensor, torch.tensor):\n\n        dis = self._cal_dis(query_fea, gallery_fea)\n        sorted_index = torch.argsort(dis, dim=1)\n        if self._hyper_params[""top_k""] != 0:\n            sorted_index = sorted_index[:, :self._hyper_params[""top_k""]]\n        return dis, sorted_index\n'"
pyretri/index/re_ranker/re_ranker_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/index/re_ranker/re_ranker_impl/identity.py,3,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..re_ranker_base import ReRankerBase\nfrom ...registry import RERANKERS\n\nfrom typing import Dict\n\n@RERANKERS.register\nclass Identity(ReRankerBase):\n    """"""\n    Directly return features without any re-rank operations.\n    """"""\n    default_hyper_params = dict()\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(Identity, self).__init__(hps)\n\n    def __call__(self, query_fea: torch.tensor, gallery_fea: torch.tensor, dis: torch.tensor or None = None,\n                 sorted_index: torch.tensor or None = None) -> torch.tensor:\n        if sorted_index is None:\n            sorted_index = torch.argsort(dis, dim=1)\n        return sorted_index\n'"
pyretri/index/re_ranker/re_ranker_impl/k_reciprocal.py,9,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\n\nfrom ..re_ranker_base import ReRankerBase\nfrom ...registry import RERANKERS\n\nfrom typing import Dict\n\n@RERANKERS.register\nclass KReciprocal(ReRankerBase):\n    """"""\n    Encoding k-reciprocal nearest neighbors to enhance the performance of retrieval.\n    c.f. https://arxiv.org/pdf/1701.08398.pdf\n\n    Hyper-Params:\n        k1 (int): hyper-parameter for calculating jaccard distance.\n        k2 (int): hyper-parameter for calculating local query expansion.\n        lambda_value (float): hyper-parameter for calculating the final distance.\n    """"""\n    default_hyper_params = {\n        ""k1"": 20,\n        ""k2"": 6,\n        ""lambda_value"": 0.3,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(KReciprocal, self).__init__(hps)\n\n    def _cal_dis(self, query_fea: torch.tensor, gallery_fea: torch.tensor) -> torch.tensor:\n        """"""\n        Calculate the distance between query set features and gallery set features.\n\n        Args:\n            query_fea (torch.tensor): query set features.\n            gallery_fea (torch.tensor): gallery set features.\n\n        Returns:\n            dis (torch.tensor): the distance between query set features and gallery set features.\n        """"""\n        query_fea = query_fea.transpose(1, 0)\n        inner_dot = gallery_fea.mm(query_fea)\n        dis = (gallery_fea ** 2).sum(dim=1, keepdim=True) + (query_fea ** 2).sum(dim=0, keepdim=True)\n        dis = dis - 2 * inner_dot\n        dis = dis.transpose(1, 0)\n        return dis\n\n    def __call__(self, query_fea: torch.tensor, gallery_fea: torch.tensor,  dis: torch.tensor or None = None,\n                 sorted_index: torch.tensor or None = None) -> torch.tensor or np.ndarray:\n        # The following naming, e.g. gallery_num, is different from outer scope.\n        # Don\'t care about it.\n        q_g_dist = dis.cpu().numpy()\n        g_g_dist = self._cal_dis(gallery_fea, gallery_fea).cpu().numpy()\n        q_q_dist = self._cal_dis(query_fea, query_fea).cpu().numpy()\n\n        original_dist = np.concatenate(\n            [np.concatenate([q_q_dist, q_g_dist], axis=1),\n             np.concatenate([q_g_dist.T, g_g_dist], axis=1)],\n            axis=0)\n        original_dist = np.power(original_dist, 2).astype(np.float32)\n        original_dist = np.transpose(1. * original_dist / np.max(original_dist, axis=0))\n        V = np.zeros_like(original_dist).astype(np.float32)\n        initial_rank = np.argsort(original_dist).astype(np.int32)\n\n        query_num = q_g_dist.shape[0]\n        gallery_num = q_g_dist.shape[0] + q_g_dist.shape[1]\n        all_num = gallery_num\n\n        for i in range(all_num):\n            # k-reciprocal neighbors\n            forward_k_neigh_index = initial_rank[i, :self._hyper_params[""k1""] + 1]\n            backward_k_neigh_index = initial_rank[forward_k_neigh_index, :self._hyper_params[""k1""] + 1]\n            fi = np.where(backward_k_neigh_index == i)[0]\n            k_reciprocal_index = forward_k_neigh_index[fi]\n            k_reciprocal_expansion_index = k_reciprocal_index\n            for j in range(len(k_reciprocal_index)):\n                candidate = k_reciprocal_index[j]\n                candidate_forward_k_neigh_index = initial_rank[candidate, :int(np.around(self._hyper_params[""k1""] / 2.)) + 1]\n                candidate_backward_k_neigh_index = initial_rank[candidate_forward_k_neigh_index,\n                                                   :int(np.around(self._hyper_params[""k1""] / 2.)) + 1]\n                fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]\n                candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]\n                if len(np.intersect1d(candidate_k_reciprocal_index, k_reciprocal_index)) > 2. / 3 * len(\n                        candidate_k_reciprocal_index):\n                    k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index, candidate_k_reciprocal_index)\n\n            k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\n            weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])\n            V[i, k_reciprocal_expansion_index] = 1. * weight / np.sum(weight)\n        original_dist = original_dist[:query_num, ]\n        if self._hyper_params[""k2""] != 1:\n            V_qe = np.zeros_like(V, dtype=np.float32)\n            for i in range(all_num):\n                V_qe[i, :] = np.mean(V[initial_rank[i, :self._hyper_params[""k2""]], :], axis=0)\n            V = V_qe\n            del V_qe\n        del initial_rank\n        invIndex = []\n        for i in range(gallery_num):\n            invIndex.append(np.where(V[:, i] != 0)[0])\n\n        jaccard_dist = np.zeros_like(original_dist, dtype=np.float32)\n\n        for i in range(query_num):\n            temp_min = np.zeros(shape=[1, gallery_num], dtype=np.float32)\n            indNonZero = np.where(V[i, :] != 0)[0]\n            indImages = [invIndex[ind] for ind in indNonZero]\n            for j in range(len(indNonZero)):\n                temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(V[i, indNonZero[j]],\n                                                                                   V[indImages[j], indNonZero[j]])\n            jaccard_dist[i] = 1 - temp_min / (2. - temp_min)\n\n        final_dist = jaccard_dist * (1 - self._hyper_params[""lambda_value""]) + original_dist * self._hyper_params[\n            ""lambda_value""]\n        del original_dist, V, jaccard_dist\n        final_dist = final_dist[:query_num, query_num:]\n\n        if torch.cuda.is_available():\n            final_dist = torch.Tensor(final_dist).cuda()\n            sorted_idx = torch.argsort(final_dist, dim=1)\n        else:\n            sorted_idx = np.argsort(final_dist, axis=1)\n        return sorted_idx\n'"
pyretri/index/re_ranker/re_ranker_impl/qe_kr.py,2,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom .query_expansion import QE\nfrom .k_reciprocal import KReciprocal\nfrom ..re_ranker_base import ReRankerBase\nfrom ...registry import RERANKERS\n\nfrom typing import Dict\n\n@RERANKERS.register\nclass QEKR(ReRankerBase):\n    """"""\n    Apply query expansion and k-reciprocal.\n\n    Hyper-Params:\n        qe_times (int): number of query expansion times.\n        qe_k (int): number of the neighbors to be combined.\n        k1 (int): hyper-parameter for calculating jaccard distance.\n        k2 (int): hyper-parameter for calculating local query expansion.\n        lambda_value (float): hyper-parameter for calculating the final distance.\n    """"""\n    default_hyper_params = {\n        ""qe_times"": 1,\n        ""qe_k"": 10,\n        ""k1"": 20,\n        ""k2"": 6,\n        ""lambda_value"": 0.3,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(QEKR, self).__init__(hps)\n        qe_hyper_params = {\n            ""qe_times"": self.default_hyper_params[""qe_times""],\n            ""qe_k"": self.default_hyper_params[""qe_k""],\n        }\n        kr_hyper_params = {\n            ""k1"": self.default_hyper_params[""k1""],\n            ""k2"": self.default_hyper_params[""k2""],\n            ""lambda_value"": self.default_hyper_params[""lambda_value""],\n        }\n        self.qe = QE(hps=qe_hyper_params)\n        self.kr = KReciprocal(hps=kr_hyper_params)\n\n    def __call__(self, query_fea: torch.tensor, gallery_fea: torch.tensor, dis: torch.tensor or None = None,\n                 sorted_index: torch.tensor or None = None) -> torch.tensor:\n\n        sorted_index = self.qe(query_fea, gallery_fea, dis, kr=self.kr)\n\n        return sorted_index\n\n'"
pyretri/index/re_ranker/re_ranker_impl/query_expansion.py,8,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\nfrom ..re_ranker_base import ReRankerBase\nfrom ...registry import RERANKERS\n\nfrom typing import Dict\n\n@RERANKERS.register\nclass QE(ReRankerBase):\n    """"""\n    Combining the retrieved topk nearest neighbors with the original query and doing another retrieval.\n    c.f. https://www.robots.ox.ac.uk/~vgg/publications/papers/chum07b.pdf\n\n    Hyper-Params:\n        qe_times (int): number of query expansion times.\n        qe_k (int): number of the neighbors to be combined.\n    """"""\n    default_hyper_params = {\n        ""qe_times"": 1,\n        ""qe_k"": 10,\n    }\n\n    def __init__(self, hps: Dict or None = None):\n        """"""\n        Args:\n            hps (dict): default hyper parameters in a dict (keys, values).\n        """"""\n        super(QE, self).__init__(hps)\n\n    def _cal_dis(self, query_fea: torch.tensor, gallery_fea: torch.tensor) -> torch.tensor:\n        """"""\n        Calculate the distance between query set features and gallery set features.\n\n        Args:\n            query_fea (torch.tensor): query set features.\n            gallery_fea (torch.tensor): gallery set features.\n\n        Returns:\n            dis (torch.tensor): the distance between query set features and gallery set features.\n        """"""\n        query_fea = query_fea.transpose(1, 0)\n        inner_dot = gallery_fea.mm(query_fea)\n        dis = (gallery_fea ** 2).sum(dim=1, keepdim=True) + (query_fea ** 2).sum(dim=0, keepdim=True)\n        dis = dis - 2 * inner_dot\n        dis = dis.transpose(1, 0)\n        return dis\n\n    def __call__(self, query_fea: torch.tensor, gallery_fea: torch.tensor, dis: torch.tensor or None = None,\n                 sorted_index: torch.tensor or None = None, kr=None) -> torch.tensor:\n        if sorted_index is None:\n            sorted_index = torch.argsort(dis, dim=1)\n        for i in range(self._hyper_params[\'qe_times\']):\n            sorted_index = sorted_index[:, :self._hyper_params[\'qe_k\']]\n            sorted_index = sorted_index.reshape(-1)\n            requery_fea = gallery_fea[sorted_index].view(query_fea.shape[0], -1, query_fea.shape[1]).sum(dim=1)\n            requery_fea = requery_fea + query_fea\n            query_fea = requery_fea\n            dis = self._cal_dis(query_fea, gallery_fea)\n\n            if kr is None:\n                sorted_index = torch.argsort(dis, dim=1)\n            else:\n                sorted_index = kr(query_fea, gallery_fea, dis)\n\n        return sorted_index\n'"
pyretri/models/backbone/backbone_impl/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport glob\nfrom os.path import basename, dirname, isfile\n\n\nmodules = glob.glob(dirname(__file__) + ""/*.py"")\n__all__ = [\n    basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(""__init__.py"") and not f.endswith(""utils.py"")\n]\n'"
pyretri/models/backbone/backbone_impl/reid_baseline.py,5,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torchvision import models\nfrom torch.autograd import Variable\n\nfrom ..backbone_base import BackboneBase\nfrom ...registry import BACKBONES\n\n\n######################################################################\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')  # For old pytorch, you may use kaiming_normal.\n    elif classname.find(\'Linear\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_out\')\n        init.constant_(m.bias.data, 0.0)\n    elif classname.find(\'BatchNorm1d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Linear\') != -1:\n        init.normal_(m.weight.data, std=0.001)\n        init.constant_(m.bias.data, 0.0)\n\n\n# Defines the new fc layer and classification layer\n# |--Linear--|--bn--|--relu--|--Linear--|\nclass ClassBlock(nn.Module):\n    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True,\n                 return_f=False):\n        super(ClassBlock, self).__init__()\n        self.return_f = return_f\n        add_block = []\n        if linear:\n            add_block += [nn.Linear(input_dim, num_bottleneck)]\n        else:\n            num_bottleneck = input_dim\n        if bnorm:\n            add_block += [nn.BatchNorm1d(num_bottleneck)]\n        if relu:\n            add_block += [nn.LeakyReLU(0.1)]\n        if droprate > 0:\n            add_block += [nn.Dropout(p=droprate)]\n        add_block = nn.Sequential(*add_block)\n        add_block.apply(weights_init_kaiming)\n\n        classifier = []\n        classifier += [nn.Linear(num_bottleneck, class_num)]\n        classifier = nn.Sequential(*classifier)\n        classifier.apply(weights_init_classifier)\n\n        self.add_block = add_block\n        self.classifier = classifier\n\n    def forward(self, x):\n        x = self.add_block(x)\n        if self.return_f:\n            f = x\n            x = self.classifier(x)\n            return x, f\n        else:\n            x = self.classifier(x)\n            return x\n\n\n# Define the ResNet50-based Model\n@BACKBONES.register\nclass ft_net(BackboneBase):\n    def __init__(self, class_num=751, droprate=0.5, stride=2):\n        super(ft_net, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        if stride == 1:\n            self.model.layer4[0].downsample[0].stride = (1, 1)\n            self.model.layer4[0].conv2.stride = (1, 1)\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.model.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the DenseNet121-based Model\nclass ft_net_dense(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super().__init__()\n        model_ft = models.densenet121(pretrained=True)\n        model_ft.features.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        model_ft.fc = nn.Sequential()\n        self.model = model_ft\n        # For DenseNet, the feature dim is 1024\n        self.classifier = ClassBlock(1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the ResNet50-based Model (Middle-Concat)\n# In the spirit of ""The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching."" Yu, Qian, et al. arXiv:1711.08106 (2017).\nclass ft_net_middle(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super(ft_net_middle, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048 + 1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        # x0  n*1024*1*1\n        x0 = self.model.avgpool(x)\n        x = self.model.layer4(x)\n        # x1  n*2048*1*1\n        x1 = self.model.avgpool(x)\n        x = torch.cat((x0, x1), 1)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n# Part Model proposed in Yifan Sun etal. (2018)\nclass PCB(nn.Module):\n    def __init__(self, class_num):\n        super(PCB, self).__init__()\n\n        self.part = 6  # We cut the pool5 to 6 parts\n        model_ft = models.resnet50(pretrained=True)\n        self.model = model_ft\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part, 1))\n        self.dropout = nn.Dropout(p=0.5)\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1, 1)\n        self.model.layer4[0].conv2.stride = (1, 1)\n        # define 6 classifiers\n        for i in range(self.part):\n            name = \'classifier\' + str(i)\n            setattr(self, name, ClassBlock(2048, class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        part = {}\n        predict = {}\n        # get six part feature batchsize*2048*6\n        for i in range(self.part):\n            part[i] = torch.squeeze(x[:, :, i])\n            name = \'classifier\' + str(i)\n            c = getattr(self, name)\n            predict[i] = c(part[i])\n\n        # sum prediction\n        # y = predict[0]\n        # for i in range(self.part-1):\n        #    y += predict[i+1]\n        y = []\n        for i in range(self.part):\n            y.append(predict[i])\n        return y\n\n\nclass PCB_test(nn.Module):\n    def __init__(self, model):\n        super(PCB_test, self).__init__()\n        self.part = 6\n        self.model = model.model\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part, 1))\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1, 1)\n        self.model.layer4[0].conv2.stride = (1, 1)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        y = x.view(x.size(0), x.size(1), x.size(2))\n        return y\n'"
pyretri/models/backbone/backbone_impl/resnet.py,11,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\n\nfrom ..backbone_base import BackboneBase\nfrom ...registry import BACKBONES\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'resnext50_32x4d\': \'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\',\n    \'resnext101_32x8d\': \'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\',\n    \'wide_resnet50_2\': \'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\',\n    \'wide_resnet101_2\': \'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    __constants__ = [\'downsample\']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    __constants__ = [\'downsample\']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(BackboneBase):\n    def __init__(self, block=Bottleneck, layers=None, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None, hps=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(""replace_stride_with_dilation should be None ""\n                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    # Allow for accessing forward method in a inherited class\n    forward = _forward\n\n\n@BACKBONES.register\ndef resnet18(progress=True, **kwargs):\n    r""""""ResNet-18 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef resnet34(progress=True, **kwargs):\n    r""""""ResNet-34 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef resnet50(progress=True, **kwargs):\n    r""""""ResNet-50 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef resnet101(progress=True, **kwargs):\n    r""""""ResNet-101 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef resnet152(progress=True, **kwargs):\n    r""""""ResNet-152 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n'"
pyretri/models/backbone/backbone_impl/vgg.py,10,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\n\nfrom ..backbone_base import BackboneBase\nfrom ...registry import BACKBONES\n\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfgs = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\n@BACKBONES.register\ndef vgg11(progress=True, **kwargs):\n    r""""""VGG 11-layer model (configuration ""A"") from\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'A\'], batch_norm=False), **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef vgg11_bn(progress=True, **kwargs):\n    r""""""VGG 11-layer model (configuration ""A"") with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'A\'], batch_norm=True), **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef vgg13(progress=True, **kwargs):\n    r""""""VGG 13-layer model (configuration ""B"")\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'B\'], batch_norm=False), **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef vgg13_bn(progress=True, **kwargs):\n    r""""""VGG 13-layer model (configuration ""B"") with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'B\'], batch_norm=True), **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef vgg16(progress=True, **kwargs):\n    r""""""VGG 16-layer model (configuration ""D"")\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'D\'], batch_norm=False), **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef vgg16_bn(progress=True, **kwargs):\n    r""""""VGG 16-layer model (configuration ""D"") with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'D\'], batch_norm=True), **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef vgg19(progress=True, **kwargs):\n    r""""""VGG 19-layer model (configuration ""E"")\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'E\'], batch_norm=False), **kwargs)\n    return model\n\n\n@BACKBONES.register\ndef vgg19_bn(progress=True, **kwargs):\n    r""""""VGG 19-layer model (configuration \'E\') with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    model = VGG(make_layers(cfgs[\'E\'], batch_norm=True), **kwargs)\n    return model\n'"
