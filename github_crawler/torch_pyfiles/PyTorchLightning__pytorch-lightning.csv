file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nimport os\nfrom io import open\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n\ntry:\n    import builtins\nexcept ImportError:\n    import __builtin__ as builtins\n\n# https://packaging.python.org/guides/single-sourcing-package-version/\n# http://blog.ionelmc.ro/2014/05/25/python-packaging/\n\nPATH_ROOT = os.path.dirname(__file__)\nbuiltins.__LIGHTNING_SETUP__ = True\n\nimport pytorch_lightning  # noqa: E402\n\n\ndef load_requirements(path_dir=PATH_ROOT, comment_char=\'#\'):\n    with open(os.path.join(path_dir, \'requirements.txt\'), \'r\') as file:\n        lines = [ln.strip() for ln in file.readlines()]\n    reqs = []\n    for ln in lines:\n        # filer all comments\n        if comment_char in ln:\n            ln = ln[:ln.index(comment_char)]\n        if ln:  # if requirement is not empty\n            reqs.append(ln)\n    return reqs\n\n\ndef load_long_description():\n    # https://github.com/PyTorchLightning/pytorch-lightning/raw/master/docs/source/_images/lightning_module/pt_to_pl.png\n    url = os.path.join(pytorch_lightning.__homepage__, \'raw\', pytorch_lightning.__version__, \'docs\')\n    text = open(\'README.md\', encoding=\'utf-8\').read()\n    # replace relative repository path to absolute link to the release\n    text = text.replace(\'](docs\', f\']({url}\')\n    # SVG images are not readable on PyPI, so replace them  with PNG\n    text = text.replace(\'.svg\', \'.png\')\n    return text\n\n\n# https://packaging.python.org/discussions/install-requires-vs-requirements /\n# keep the meta-data here for simplicity in reading this file... it\'s not obvious\n# what happens and to non-engineers they won\'t know to look in init ...\n# the goal of the project is simplicity for researchers, don\'t want to add too much\n# engineer specific practices\nsetup(\n    name=\'pytorch-lightning\',\n    version=pytorch_lightning.__version__,\n    description=pytorch_lightning.__docs__,\n    author=pytorch_lightning.__author__,\n    author_email=pytorch_lightning.__author_email__,\n    url=pytorch_lightning.__homepage__,\n    download_url=\'https://github.com/PyTorchLightning/pytorch-lightning\',\n    license=pytorch_lightning.__license__,\n    packages=find_packages(exclude=[\'tests\', \'tests/*\', \'benchmarks\']),\n\n    long_description=load_long_description(),\n    long_description_content_type=\'text/markdown\',\n    include_package_data=True,\n    zip_safe=False,\n\n    keywords=[\'deep learning\', \'pytorch\', \'AI\'],\n    python_requires=\'>=3.6\',\n    setup_requires=[],\n    install_requires=load_requirements(PATH_ROOT),\n\n    project_urls={\n        ""Bug Tracker"": ""https://github.com/PyTorchLightning/pytorch-lightning/issues"",\n        ""Documentation"": ""https://pytorch-lightning.rtfd.io/en/latest/"",\n        ""Source Code"": ""https://github.com/PyTorchLightning/pytorch-lightning"",\n    },\n\n    classifiers=[\n        \'Environment :: Console\',\n        \'Natural Language :: English\',\n        # How mature is this project? Common values are\n        #   3 - Alpha, 4 - Beta, 5 - Production/Stable\n        \'Development Status :: 4 - Beta\',\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Image Recognition\',\n        \'Topic :: Scientific/Engineering :: Information Analysis\',\n        # Pick your license as you wish\n        \'License :: OSI Approved :: BSD License\',\n        \'Operating System :: OS Independent\',\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n)\n'"
benchmarks/__init__.py,0,b''
benchmarks/parity_modules.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom pytorch_lightning import LightningModule\nfrom tests.base.datasets import MNIST\n\n\nclass AverageDataset(Dataset):\n    def __init__(self, dataset_len=300, sequence_len=100):\n        self.dataset_len = dataset_len\n        self.sequence_len = sequence_len\n        self.input_seq = torch.randn(dataset_len, sequence_len, 10)\n        top, bottom = self.input_seq.chunk(2, -1)\n        self.output_seq = top + bottom.roll(shifts=1, dims=-1)\n\n    def __len__(self):\n        return self.dataset_len\n\n    def __getitem__(self, item):\n        return self.input_seq[item], self.output_seq[item]\n\n\nclass ParityModuleRNN(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.rnn = nn.LSTM(10, 20, batch_first=True)\n        self.linear_out = nn.Linear(in_features=20, out_features=5)\n\n    def forward(self, x):\n        seq, last = self.rnn(x)\n        return self.linear_out(seq)\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y)\n        return {'loss': loss}\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.02)\n\n    def train_dataloader(self):\n        return DataLoader(AverageDataset(), batch_size=30)\n\n\nclass ParityModuleMNIST(LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        self.c_d1 = nn.Linear(in_features=28 * 28, out_features=128)\n        self.c_d1_bn = nn.BatchNorm1d(128)\n        self.c_d1_drop = nn.Dropout(0.3)\n        self.c_d2 = nn.Linear(in_features=128, out_features=10)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = self.c_d1(x)\n        x = torch.tanh(x)\n        x = self.c_d1_bn(x)\n        x = self.c_d1_drop(x)\n        x = self.c_d2(x)\n        return x\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        return {'loss': loss}\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.02)\n\n    def train_dataloader(self):\n        return DataLoader(MNIST(train=True, download=True,),\n                          batch_size=128)\n"""
benchmarks/test_parity.py,3,"b'import time\n\nimport numpy as np\nimport pytest\nimport torch\n\nimport tests.base.utils as tutils\nfrom benchmarks.parity_modules import ParityModuleRNN, ParityModuleMNIST\nfrom pytorch_lightning import Trainer, seed_everything\n\n\n@pytest.mark.parametrize(\'cls_model,max_diff\', [\n    (ParityModuleRNN, 0.05),\n    (ParityModuleMNIST, 0.5)\n])\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""test requires GPU machine"")\ndef test_pytorch_parity(tmpdir, cls_model, max_diff):\n    """"""\n    Verify that the same  pytorch and lightning models achieve the same results\n    """"""\n    num_epochs = 4\n    num_rums = 3\n    lightning_outs, pl_times = lightning_loop(cls_model, num_rums, num_epochs)\n    manual_outs, pt_times = vanilla_loop(cls_model, num_rums, num_epochs)\n\n    # make sure the losses match exactly  to 5 decimal places\n    for pl_out, pt_out in zip(lightning_outs, manual_outs):\n        np.testing.assert_almost_equal(pl_out, pt_out, 5)\n\n    # the fist run initialize dataset (download & filter)\n    tutils.assert_speed_parity_absolute(pl_times[1:], pt_times[1:],\n                                        nb_epochs=num_epochs, max_diff=max_diff)\n\n\ndef vanilla_loop(cls_model, num_runs=10, num_epochs=10):\n    """"""\n    Returns an array with the last loss from each epoch for each run\n    """"""\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else ""cpu"")\n    errors = []\n    times = []\n\n    torch.backends.cudnn.deterministic = True\n    for i in range(num_runs):\n        time_start = time.perf_counter()\n\n        # set seed\n        seed = i\n        seed_everything(seed)\n\n        # init model parts\n        model = cls_model()\n        dl = model.train_dataloader()\n        optimizer = model.configure_optimizers()\n\n        # model to GPU\n        model = model.to(device)\n\n        epoch_losses = []\n        # as the first run is skipped, no need to run it long\n        for epoch in range(num_epochs if i > 0 else 1):\n\n            # run through full training set\n            for j, batch in enumerate(dl):\n                batch = [x.to(device) for x in batch]\n                loss_dict = model.training_step(batch, j)\n                loss = loss_dict[\'loss\']\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # track last epoch loss\n            epoch_losses.append(loss.item())\n\n        time_end = time.perf_counter()\n        times.append(time_end - time_start)\n\n        errors.append(epoch_losses[-1])\n\n    return errors, times\n\n\ndef lightning_loop(cls_model, num_runs=10, num_epochs=10):\n    errors = []\n    times = []\n\n    for i in range(num_runs):\n        time_start = time.perf_counter()\n\n        # set seed\n        seed = i\n        seed_everything(seed)\n\n        model = cls_model()\n        # init model parts\n        trainer = Trainer(\n            # as the first run is skipped, no need to run it long\n            max_epochs=num_epochs if i > 0 else 1,\n            progress_bar_refresh_rate=0,\n            weights_summary=None,\n            gpus=1,\n            early_stop_callback=False,\n            checkpoint_callback=False,\n            deterministic=True,\n            logger=False,\n            replace_sampler_ddp=False,\n        )\n        trainer.fit(model)\n\n        final_loss = trainer.running_loss.last().item()\n        errors.append(final_loss)\n\n        time_end = time.perf_counter()\n        times.append(time_end - time_start)\n\n    return errors, times\n'"
pl_examples/__init__.py,0,"b'""""""\nTemplate model definition\n-------------------------\n\nIn 99% of cases you want to just copy `one of the examples\n<https://github.com/PyTorchLightning/pytorch-lightning/tree/master/pl_examples>`_\nto start a new lightningModule and change the core of what your model is actually trying to do.\n\n.. code-block:: bash\n\n    # get a copy of the module template\n    wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/pl_examples/new_project_templates/lightning_module_template.py  # noqa: E501\n\n\nTrainer Example\n---------------\n\n**`__main__` function**\n\nNormally, we want to let the `__main__` function start the training.\n Inside the main we parse training arguments with whatever hyperparameters we want.\n Your LightningModule will have a chance to add hyperparameters.\n\n.. code-block:: python\n\n    from test_tube import HyperOptArgumentParser\n\n    if __name__ == \'__main__\':\n\n        # use default args given by lightning\n        root_dir = os.path.split(os.path.dirname(sys.modules[\'__main__\'].__file__))[0]\n        parent_parser = HyperOptArgumentParser(strategy=\'random_search\', add_help=False)\n        add_default_args(parent_parser, root_dir)\n\n        # allow model to overwrite or extend args\n        parser = ExampleModel.add_model_specific_args(parent_parser)\n        hyperparams = parser.parse_args()\n\n        # train model\n        main(hyperparams)\n\n**Main Function**\n\nThe main function is your entry into the program. This is where you init your model, checkpoint directory,\n and launch the training. The main function should have 3 arguments:\n\n- hparams: a configuration of hyperparameters.\n- slurm_manager: Slurm cluster manager object (can be None)\n- dict: for you to return any values you want (useful in meta-learning, otherwise set to)\n\n.. code-block:: python\n\n    def main(hparams, cluster, results_dict):\n        # build model\n        model = MyLightningModule(hparams)\n\n        # configure trainer\n        trainer = Trainer()\n\n        # train model\n        trainer.fit(model)\n\n\nThe `__main__` function will start training on your **main** function.\n If you use the HyperParameterOptimizer in hyper parameter optimization mode,\n this main function will get one set of hyperparameters. If you use it as a simple\n argument parser you get the default arguments in the argument parser.\n\nSo, calling main(hyperparams) runs the model with the default argparse arguments.::\n\n    main(hyperparams)\n\n\nCPU hyperparameter search\n-------------------------\n\n.. code-block:: python\n\n    # run a grid search over 20 hyperparameter combinations.\n    hyperparams.optimize_parallel_cpu(\n        main_local,\n        nb_trials=20,\n        nb_workers=1\n    )\n\n\nHyperparameter search on a single or multiple GPUs\n--------------------------------------------------\n\n.. code-block:: python\n\n    # run a grid search over 20 hyperparameter combinations.\n    hyperparams.optimize_parallel_gpu(\n        main_local,\n        nb_trials=20,\n        nb_workers=1,\n        gpus=[0,1,2,3]\n    )\n\n\nHyperparameter search on a SLURM HPC cluster\n--------------------------------------------\n\n.. code-block:: python\n\n    def optimize_on_cluster(hyperparams):\n        # enable cluster training\n        cluster = SlurmCluster(\n            hyperparam_optimizer=hyperparams,\n            log_path=hyperparams.tt_save_path,\n            test_tube_exp_name=hyperparams.tt_name\n        )\n\n        # email for cluster coms\n        cluster.notify_job_status(email=\'add_email_here\', on_done=True, on_fail=True)\n\n        # configure cluster\n        cluster.per_experiment_nb_gpus = hyperparams.per_experiment_nb_gpus\n        cluster.job_time = \'48:00:00\'\n        cluster.gpu_type = \'1080ti\'\n        cluster.memory_mb_per_node = 48000\n\n        # any modules for code to run in env\n        cluster.add_command(\'source activate pytorch_lightning\')\n\n        # name of exp\n        job_display_name = hyperparams.tt_name.split(\'_\')[0]\n        job_display_name = job_display_name[0:3]\n\n        # run hopt\n        logging.info(\'submitting jobs...\')\n        cluster.optimize_parallel_cluster_gpu(\n            main,\n            nb_trials=hyperparams.nb_hopt_trials,\n            job_name=job_display_name\n        )\n\n    # run cluster hyperparameter search\n    optimize_on_cluster(hyperparams)\n\n""""""\n\nfrom pl_examples.models.lightning_template import LightningTemplateModel\n\n__all__ = [\n    \'LightningTemplateModel\'\n]\n'"
pytorch_lightning/__init__.py,0,"b'""""""Root package info.""""""\n\n__version__ = \'0.8.0rc1\'\n__author__ = \'William Falcon et al.\'\n__author_email__ = \'waf2107@columbia.edu\'\n__license__ = \'Apache-2.0\'\n__copyright__ = \'Copyright (c) 2018-2020, %s.\' % __author__\n__homepage__ = \'https://github.com/PyTorchLightning/pytorch-lightning\'\n# this has to be simple string, see: https://github.com/pypa/twine/issues/522\n__docs__ = ""PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers."" \\\n           "" Scale your models. Write less boilerplate.""\n__long_docs__ = """"""\nLightning is a way to organize your PyTorch code to decouple the science code from the engineering.\n It\'s more of a style-guide than a framework.\n\nIn Lightning, you organize your code into 3 distinct categories:\n\n1. Research code (goes in the LightningModule).\n2. Engineering code (you delete, and is handled by the Trainer).\n3. Non-essential research code (logging, etc. this goes in Callbacks).\n\nAlthough your research/production project might start simple, once you add things like GPU AND TPU training,\n 16-bit precision, etc, you end up spending more time engineering than researching.\n Lightning automates AND rigorously tests those parts for you.\n\nOverall, Lightning guarantees rigorously tested, correct, modern best practices for the automated parts.\n\nDocumentation\n-------------\n- https://pytorch-lightning.readthedocs.io/en/latest\n- https://pytorch-lightning.readthedocs.io/en/stable\n""""""\n\nimport logging as python_logging\n\n_logger = python_logging.getLogger(""lightning"")\n_logger.addHandler(python_logging.StreamHandler())\n_logger.setLevel(python_logging.INFO)\n\ntry:\n    # This variable is injected in the __builtins__ by the build\n    # process. It used to enable importing subpackages of skimage when\n    # the binaries are not built\n    __LIGHTNING_SETUP__\nexcept NameError:\n    __LIGHTNING_SETUP__ = False\n\nif __LIGHTNING_SETUP__:\n    import sys  # pragma: no-cover\n    sys.stdout.write(f\'Partial import of `{__name__}` during the build process.\\n\')  # pragma: no-cover\n    # We are not importing the rest of the lightning during the build process, as it may not be compiled yet\nelse:\n    from pytorch_lightning.core import LightningModule\n    from pytorch_lightning.trainer import Trainer\n    from pytorch_lightning.trainer.seed import seed_everything\n    from pytorch_lightning.callbacks import Callback\n    from pytorch_lightning.core import data_loader\n\n    __all__ = [\n        \'Trainer\',\n        \'LightningModule\',\n        \'Callback\',\n        \'data_loader\',\n        \'seed_everything\',\n    ]\n\n    # necessary for regular bolts imports. Skip exception since bolts is not always installed\n    try:\n        from pytorch_lightning import bolts\n    except ImportError:\n        pass\n    # __call__ = __all__\n\n# for compatibility with namespace packages\n__import__(\'pkg_resources\').declare_namespace(__name__)\n'"
tests/__init__.py,1,"b""import os\n\nimport numpy as np\nimport torch\n\nTEST_ROOT = os.path.dirname(__file__)\nPACKAGE_ROOT = os.path.dirname(TEST_ROOT)\nTEMP_PATH = os.path.join(PACKAGE_ROOT, 'test_temp')\n\n# generate a list of random seeds for each test\nRANDOM_PORTS = list(np.random.randint(12000, 19000, 1000))\nROOT_SEED = 1234\ntorch.manual_seed(ROOT_SEED)\nnp.random.seed(ROOT_SEED)\nRANDOM_SEEDS = list(np.random.randint(0, 10000, 1000))\n\nif not os.path.isdir(TEMP_PATH):\n    os.mkdir(TEMP_PATH)\n"""
tests/collect_env_details.py,5,"b'""""""Diagnose your system and show basic information\n\nThis server mainly to get detail info for better bug reporting.\n\n""""""\n\nimport os\nimport platform\nimport re\nimport sys\n\nimport numpy\nimport tensorboard\nimport torch\nimport tqdm\n\nsys.path += [os.path.abspath(\'..\'), os.path.abspath(\'.\')]\nimport pytorch_lightning  # noqa: E402\n\nLEVEL_OFFSET = \'\\t\'\nKEY_PADDING = 20\n\n\ndef run_and_parse_first_match(run_lambda, command, regex):\n    """"""Runs command using run_lambda, returns the first regex match if it exists""""""\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    match = re.search(regex, out)\n    if match is None:\n        return None\n    return match.group(1)\n\n\ndef get_running_cuda_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, \'nvcc --version\', r\'V(.*)$\')\n\n\ndef info_system():\n    return {\n        \'OS\': platform.system(),\n        \'architecture\': platform.architecture(),\n        \'version\': platform.version(),\n        \'processor\': platform.processor(),\n        \'python\': platform.python_version(),\n    }\n\n\ndef info_cuda():\n    return {\n        \'GPU\': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n        # \'nvidia_driver\': get_nvidia_driver_version(run_lambda),\n        \'available\': torch.cuda.is_available(),\n        \'version\': torch.version.cuda,\n    }\n\n\ndef info_packages():\n    return {\n        \'numpy\': numpy.__version__,\n        ""pyTorch_version"": torch.__version__,\n        \'pyTorch_debug\': torch.version.debug,\n        \'pytorch-lightning\': pytorch_lightning.__version__,\n        \'tensorboard\': tensorboard.__version__,\n        \'tqdm\': tqdm.__version__,\n    }\n\n\ndef nice_print(details, level=0):\n    lines = []\n    for k in sorted(details):\n        key = f\'* {k}:\' if level == 0 else f\'- {k}:\'\n        if isinstance(details[k], dict):\n            lines += [level * LEVEL_OFFSET + key]\n            lines += nice_print(details[k], level + 1)\n        elif isinstance(details[k], (set, list, tuple)):\n            lines += [level * LEVEL_OFFSET + key]\n            lines += [(level + 1) * LEVEL_OFFSET + \'- \' + v for v in details[k]]\n        else:\n            template = \'{:%is} {}\' % KEY_PADDING\n            key_val = template.format(key, details[k])\n            lines += [(level * LEVEL_OFFSET) + key_val]\n    return lines\n\n\ndef main():\n    details = {\n        ""System"": info_system(),\n        \'CUDA\': info_cuda(),\n        \'Packages\': info_packages(),\n    }\n    lines = nice_print(details)\n    text = os.linesep.join(lines)\n    print(text)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/conftest.py,2,"b'from functools import wraps\n\nimport pytest\nimport torch.multiprocessing as mp\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(""markers"", ""spawn: spawn test in a separate process using torch.multiprocessing.spawn"")\n\n\n@pytest.mark.tryfirst\ndef pytest_pyfunc_call(pyfuncitem):\n    if pyfuncitem.get_closest_marker(""spawn""):\n        testfunction = pyfuncitem.obj\n        funcargs = pyfuncitem.funcargs\n        testargs = tuple([funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames])\n\n        mp.spawn(wraps, (testfunction, testargs))\n        return True\n'"
tests/test_deprecated.py,0,"b'""""""Test deprecated functionality which will be removed in vX.Y.Z""""""\nimport sys\n\nimport pytest\n\nfrom pytorch_lightning import Trainer\nfrom tests.base import EvalModelTemplate\n\n\ndef _soft_unimport_module(str_module):\n    # once the module is imported  e.g with parsing with pytest it lives in memory\n    if str_module in sys.modules:\n        del sys.modules[str_module]\n\n\ndef test_tbd_remove_in_v0_8_0_module_imports():\n    _soft_unimport_module(""pytorch_lightning.logging.comet_logger"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.logging.comet_logger import CometLogger  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.logging.mlflow_logger"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.logging.mlflow_logger import MLFlowLogger  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.logging.test_tube_logger"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.logging.test_tube_logger import TestTubeLogger  # noqa: F811\n\n    _soft_unimport_module(""pytorch_lightning.pt_overrides.override_data_parallel"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.pt_overrides.override_data_parallel import (  # noqa: F811\n            LightningDataParallel, LightningDistributedDataParallel)\n    _soft_unimport_module(""pytorch_lightning.overrides.override_data_parallel"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.overrides.override_data_parallel import (  # noqa: F811\n            LightningDataParallel, LightningDistributedDataParallel)\n\n    _soft_unimport_module(""pytorch_lightning.core.model_saving"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.core.model_saving import ModelIO  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.core.root_module"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.core.root_module import LightningModule  # noqa: F811\n\n    _soft_unimport_module(""pytorch_lightning.root_module.decorators"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.root_module.decorators import data_loader  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.root_module.grads"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.root_module.grads import GradInformation  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.root_module.hooks"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.root_module.hooks import ModelHooks  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.root_module.memory"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.root_module.memory import ModelSummary  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.root_module.model_saving"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.root_module.model_saving import ModelIO  # noqa: F811\n    _soft_unimport_module(""pytorch_lightning.root_module.root_module"")\n    with pytest.deprecated_call(match=\'v0.8.0\'):\n        from pytorch_lightning.root_module.root_module import LightningModule  # noqa: F811\n\n\ndef test_tbd_remove_in_v0_8_0_trainer():\n    mapping_old_new = {\n        \'gradient_clip\': \'gradient_clip_val\',\n        \'nb_gpu_nodes\': \'num_nodes\',\n        \'max_nb_epochs\': \'max_epochs\',\n        \'min_nb_epochs\': \'min_epochs\',\n        \'nb_sanity_val_steps\': \'num_sanity_val_steps\',\n        \'default_save_path\': \'default_root_dir\',\n    }\n    # skip 0 since it may be interested as False\n    kwargs = {k: (i + 1) for i, k in enumerate(mapping_old_new)}\n\n    trainer = Trainer(**kwargs)\n\n    for attr_old in mapping_old_new:\n        attr_new = mapping_old_new[attr_old]\n        with pytest.deprecated_call(match=\'v0.8.0\'):\n            _ = getattr(trainer, attr_old)\n        assert kwargs[attr_old] == getattr(trainer, attr_old), \\\n            \'Missing deprecated attribute ""%s""\' % attr_old\n        assert kwargs[attr_old] == getattr(trainer, attr_new), \\\n            \'Wrongly passed deprecated argument ""%s"" to attribute ""%s""\' % (attr_old, attr_new)\n\n\ndef test_tbd_remove_in_v0_9_0_trainer():\n    # test show_progress_bar set by progress_bar_refresh_rate\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        trainer = Trainer(progress_bar_refresh_rate=0, show_progress_bar=True)\n    assert not getattr(trainer, \'show_progress_bar\')\n\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        trainer = Trainer(progress_bar_refresh_rate=50, show_progress_bar=False)\n    assert getattr(trainer, \'show_progress_bar\')\n\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        trainer = Trainer(num_tpu_cores=8)\n        assert trainer.tpu_cores == 8\n\n\ndef test_tbd_remove_in_v0_9_0_module_imports():\n    _soft_unimport_module(""pytorch_lightning.core.decorators"")\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        from pytorch_lightning.core.decorators import data_loader  # noqa: F811\n        data_loader(print)\n\n    _soft_unimport_module(""pytorch_lightning.logging.comet"")\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        from pytorch_lightning.logging.comet import CometLogger  # noqa: F402\n    _soft_unimport_module(""pytorch_lightning.logging.mlflow"")\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        from pytorch_lightning.logging.mlflow import MLFlowLogger  # noqa: F402\n    _soft_unimport_module(""pytorch_lightning.logging.neptune"")\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        from pytorch_lightning.logging.neptune import NeptuneLogger  # noqa: F402\n    _soft_unimport_module(""pytorch_lightning.logging.test_tube"")\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        from pytorch_lightning.logging.test_tube import TestTubeLogger  # noqa: F402\n    _soft_unimport_module(""pytorch_lightning.logging.wandb"")\n    with pytest.deprecated_call(match=\'v0.9.0\'):\n        from pytorch_lightning.logging.wandb import WandbLogger  # noqa: F402\n\n\nclass ModelVer0_6(EvalModelTemplate):\n\n    # todo: this shall not be needed while evaluate asks for dataloader explicitly\n    def val_dataloader(self):\n        return self.dataloader(train=False)\n\n    def validation_step(self, batch, batch_idx, *args, **kwargs):\n        return {\'val_loss\': 0.6}\n\n    def validation_end(self, outputs):\n        return {\'val_loss\': 0.6}\n\n    def test_dataloader(self):\n        return self.dataloader(train=False)\n\n    def test_end(self, outputs):\n        return {\'test_loss\': 0.6}\n\n\nclass ModelVer0_7(EvalModelTemplate):\n\n    # todo: this shall not be needed while evaluate asks for dataloader explicitly\n    def val_dataloader(self):\n        return self.dataloader(train=False)\n\n    def validation_step(self, batch, batch_idx, *args, **kwargs):\n        return {\'val_loss\': 0.7}\n\n    def validation_end(self, outputs):\n        return {\'val_loss\': 0.7}\n\n    def test_dataloader(self):\n        return self.dataloader(train=False)\n\n    def test_end(self, outputs):\n        return {\'test_loss\': 0.7}\n\n\ndef test_tbd_remove_in_v1_0_0_model_hooks():\n    hparams = EvalModelTemplate.get_default_hparams()\n\n    model = ModelVer0_6(hparams)\n\n    with pytest.deprecated_call(match=\'v1.0\'):\n        trainer = Trainer(logger=False)\n        trainer.test(model)\n    assert trainer.callback_metrics == {\'test_loss\': 0.6}\n\n    with pytest.deprecated_call(match=\'v1.0\'):\n        trainer = Trainer(logger=False)\n        # TODO: why `dataloder` is required if it is not used\n        result = trainer._evaluate(model, dataloaders=[[None]], max_batches=1)\n    assert result == {\'val_loss\': 0.6}\n\n    model = ModelVer0_7(hparams)\n\n    with pytest.deprecated_call(match=\'v1.0\'):\n        trainer = Trainer(logger=False)\n        trainer.test(model)\n    assert trainer.callback_metrics == {\'test_loss\': 0.7}\n\n    with pytest.deprecated_call(match=\'v1.0\'):\n        trainer = Trainer(logger=False)\n        # TODO: why `dataloder` is required if it is not used\n        result = trainer._evaluate(model, dataloaders=[[None]], max_batches=1)\n    assert result == {\'val_loss\': 0.7}\n'"
tests/test_profiler.py,0,"b'import os\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\n\nfrom pytorch_lightning.profiler import AdvancedProfiler, SimpleProfiler\n\nPROFILER_OVERHEAD_MAX_TOLERANCE = 0.0005\n\n\ndef _get_python_cprofile_total_duration(profile):\n    return sum([x.inlinetime for x in profile.getstats()])\n\n\ndef _sleep_generator(durations):\n    """"""\n    the profile_iterable method needs an iterable in which we can ensure that we\'re\n    properly timing how long it takes to call __next__\n    """"""\n    for duration in durations:\n        time.sleep(duration)\n        yield duration\n\n\n@pytest.fixture\ndef simple_profiler():\n    profiler = SimpleProfiler()\n    return profiler\n\n\n@pytest.fixture\ndef advanced_profiler(tmpdir):\n    profiler = AdvancedProfiler(output_filename=os.path.join(tmpdir, ""profiler.txt""))\n    return profiler\n\n\n@pytest.mark.parametrize([""action"", ""expected""], [\n    pytest.param(""a"", [3, 1]),\n    pytest.param(""b"", [2]),\n    pytest.param(""c"", [1])\n])\ndef test_simple_profiler_durations(simple_profiler, action, expected):\n    """"""Ensure the reported durations are reasonably accurate.""""""\n\n    for duration in expected:\n        with simple_profiler.profile(action):\n            time.sleep(duration)\n\n    # different environments have different precision when it comes to time.sleep()\n    # see: https://github.com/PyTorchLightning/pytorch-lightning/issues/796\n    np.testing.assert_allclose(\n        simple_profiler.recorded_durations[action], expected, rtol=0.2\n    )\n\n\n@pytest.mark.parametrize([""action"", ""expected""], [\n    pytest.param(""a"", [3, 1]),\n    pytest.param(""b"", [2]),\n    pytest.param(""c"", [1])\n])\ndef test_simple_profiler_iterable_durations(simple_profiler, action, expected):\n    """"""Ensure the reported durations are reasonably accurate.""""""\n    iterable = _sleep_generator(expected)\n\n    for _ in simple_profiler.profile_iterable(iterable, action):\n        pass\n\n    # we exclude the last item in the recorded durations since that\'s when StopIteration is raised\n    np.testing.assert_allclose(\n        simple_profiler.recorded_durations[action][:-1], expected, rtol=0.2\n    )\n\n\ndef test_simple_profiler_overhead(simple_profiler, n_iter=5):\n    """"""Ensure that the profiler doesn\'t introduce too much overhead during training.""""""\n    for _ in range(n_iter):\n        with simple_profiler.profile(""no-op""):\n            pass\n\n    durations = np.array(simple_profiler.recorded_durations[""no-op""])\n    assert all(durations < PROFILER_OVERHEAD_MAX_TOLERANCE)\n\n\ndef test_simple_profiler_describe(caplog, simple_profiler):\n    """"""Ensure the profiler won\'t fail when reporting the summary.""""""\n    simple_profiler.describe()\n\n    assert ""Profiler Report"" in caplog.text\n\n\ndef test_simple_profiler_value_errors(simple_profiler):\n    """"""Ensure errors are raised where expected.""""""\n\n    action = ""test""\n    with pytest.raises(ValueError):\n        simple_profiler.stop(action)\n\n    simple_profiler.start(action)\n\n    with pytest.raises(ValueError):\n        simple_profiler.start(action)\n\n    simple_profiler.stop(action)\n\n\n@pytest.mark.parametrize([""action"", ""expected""], [\n    pytest.param(""a"", [3, 1]),\n    pytest.param(""b"", [2]),\n    pytest.param(""c"", [1])\n])\ndef test_advanced_profiler_durations(advanced_profiler, action, expected):\n\n    for duration in expected:\n        with advanced_profiler.profile(action):\n            time.sleep(duration)\n\n    # different environments have different precision when it comes to time.sleep()\n    # see: https://github.com/PyTorchLightning/pytorch-lightning/issues/796\n    recored_total_duration = _get_python_cprofile_total_duration(\n        advanced_profiler.profiled_actions[action]\n    )\n    expected_total_duration = np.sum(expected)\n    np.testing.assert_allclose(\n        recored_total_duration, expected_total_duration, rtol=0.2\n    )\n\n\n@pytest.mark.parametrize([""action"", ""expected""], [\n    pytest.param(""a"", [3, 1]),\n    pytest.param(""b"", [2]),\n    pytest.param(""c"", [1])\n])\ndef test_advanced_profiler_iterable_durations(advanced_profiler, action, expected):\n    """"""Ensure the reported durations are reasonably accurate.""""""\n    iterable = _sleep_generator(expected)\n\n    for _ in advanced_profiler.profile_iterable(iterable, action):\n        pass\n\n    recored_total_duration = _get_python_cprofile_total_duration(\n        advanced_profiler.profiled_actions[action]\n    )\n    expected_total_duration = np.sum(expected)\n    np.testing.assert_allclose(\n        recored_total_duration, expected_total_duration, rtol=0.2\n    )\n\n\ndef test_advanced_profiler_overhead(advanced_profiler, n_iter=5):\n    """"""\n    ensure that the profiler doesn\'t introduce too much overhead during training\n    """"""\n    for _ in range(n_iter):\n        with advanced_profiler.profile(""no-op""):\n            pass\n\n    action_profile = advanced_profiler.profiled_actions[""no-op""]\n    total_duration = _get_python_cprofile_total_duration(action_profile)\n    average_duration = total_duration / n_iter\n    assert average_duration < PROFILER_OVERHEAD_MAX_TOLERANCE\n\n\ndef test_advanced_profiler_describe(tmpdir, advanced_profiler):\n    """"""\n    ensure the profiler won\'t fail when reporting the summary\n    """"""\n    # record at least one event\n    with advanced_profiler.profile(""test""):\n        pass\n    # log to stdout and print to file\n    advanced_profiler.describe()\n    data = Path(advanced_profiler.output_fname).read_text()\n    assert len(data) > 0\n\n\ndef test_advanced_profiler_value_errors(advanced_profiler):\n    """"""Ensure errors are raised where expected.""""""\n\n    action = ""test""\n    with pytest.raises(ValueError):\n        advanced_profiler.stop(action)\n\n    advanced_profiler.start(action)\n    advanced_profiler.stop(action)\n'"
docs/source/conf.py,1,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\nimport glob\nimport shutil\nimport inspect\n\n# import m2r\nimport builtins\nimport pt_lightning_sphinx_theme\nfrom sphinx.ext import apidoc\n\nPATH_HERE = os.path.abspath(os.path.dirname(__file__))\nPATH_ROOT = os.path.join(PATH_HERE, \'..\', \'..\')\nsys.path.insert(0, os.path.abspath(PATH_ROOT))\n\nbuiltins.__LIGHTNING_SETUP__ = True\n\nSPHINX_MOCK_REQUIREMENTS = int(os.environ.get(\'SPHINX_MOCK_REQUIREMENTS\', True))\n\nimport pytorch_lightning  # noqa: E402\n\n# -- Project documents -------------------------------------------------------\n\n# # export the documentation\n# with open(\'intro.rst\', \'w\') as fp:\n#     intro = pytorch_lightning.__doc__.replace(os.linesep + \' \', \'\')\n#     fp.write(m2r.convert(intro))\n#     # fp.write(pytorch_lightning.__doc__)\n\n# # export the READme\n# with open(os.path.join(PATH_ROOT, \'README.md\'), \'r\') as fp:\n#     readme = fp.read()\n# # replace all paths to relative\n# for ndir in (os.path.basename(p) for p in glob.glob(os.path.join(PATH_ROOT, \'*\'))\n#              if os.path.isdir(p)):\n#     readme = readme.replace(\'](%s/\' % ndir, \'](%s/%s/\' % (PATH_ROOT, ndir))\n# with open(\'readme.md\', \'w\') as fp:\n#     fp.write(readme)\n\nfor md in glob.glob(os.path.join(PATH_ROOT, \'.github\', \'*.md\')):\n    shutil.copy(md, os.path.join(PATH_HERE, os.path.basename(md)))\n\n# -- Project information -----------------------------------------------------\n\nproject = \'PyTorch-Lightning\'\ncopyright = pytorch_lightning.__copyright__\nauthor = pytorch_lightning.__author__\n\n# The short X.Y version\nversion = pytorch_lightning.__version__\n# The full version, including alpha/beta/rc tags\nrelease = pytorch_lightning.__version__\n\n# Options for the linkcode extension\n# ----------------------------------\ngithub_user = \'PyTorchLightning\'\ngithub_repo = project\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n\nneeds_sphinx = \'2.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    # \'sphinxcontrib.mockautodoc\',  # raises error: directive \'automodule\' is already registered ...\n    # \'sphinxcontrib.fulltoc\',  # breaks pytorch-theme with unexpected kw argument \'titles_only\'\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.linkcode\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.napoleon\',\n    \'recommonmark\',\n    \'sphinx.ext.autosectionlabel\',\n    # \'m2r\',\n    \'nbsphinx\',\n    \'sphinx_autodoc_typehints\',\n    \'sphinx_paramlinks\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# https://berkeley-stat159-f17.github.io/stat159-f17/lectures/14-sphinx..html#conf.py-(cont.)\n# https://stackoverflow.com/questions/38526888/embed-ipython-notebook-in-sphinx-document\n# I execute the notebooks manually in advance. If notebooks test the code,\n# they should be run at build time.\nnbsphinx_execute = \'never\'\nnbsphinx_allow_errors = True\nnbsphinx_requirejs_path = \'\'\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\n# source_suffix = [\'.rst\', \'.md\', \'.ipynb\']\nsource_suffix = {\n    \'.rst\': \'restructuredtext\',\n    \'.txt\': \'markdown\',\n    \'.md\': \'markdown\',\n    \'.ipynb\': \'nbsphinx\',\n}\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\n    \'api/pytorch_lightning.rst\',\n    \'api/pl_examples.*\',\n    \'api/modules.rst\',\n\n    # deprecated/renamed:\n    \'api/pytorch_lightning.loggers.comet_logger.rst\',           # TODO: remove in v0.8.0\n    \'api/pytorch_lightning.loggers.mlflow_logger.rst\',          # TODO: remove in v0.8.0\n    \'api/pytorch_lightning.loggers.test_tube_logger.rst\',       # TODO: remove in v0.8.0\n    \'api/pytorch_lightning.callbacks.pt_callbacks.*\',           # TODO: remove in v0.8.0\n    \'api/pytorch_lightning.pt_overrides.*\',                     # TODO: remove in v0.8.0\n    \'api/pytorch_lightning.root_module.*\',                      # TODO: remove in v0.8.0\n    \'api/pytorch_lightning.logging.*\',                          # TODO: remove in v0.8.0\n]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# http://www.sphinx-doc.org/en/master/usage/theming.html#builtin-themes\n# html_theme = \'bizstyle\'\n# https://sphinx-themes.org\nhtml_theme = \'pt_lightning_sphinx_theme\'\nhtml_theme_path = [pt_lightning_sphinx_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n\nhtml_theme_options = {\n    \'pytorch_project\': pytorch_lightning.__homepage__,\n    \'canonical_url\': pytorch_lightning.__homepage__,\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': False,\n}\n\nhtml_logo = \'_images/logos/lightning_logo-name.svg\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_images\', \'_templates\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = project + \'-doc\'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, project + \'.tex\', project + \' Documentation\', author, \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, project, project + \' Documentation\', [author], 1)\n]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, project, project + \' Documentation\', author, project,\n     \'One line description of project.\', \'Miscellaneous\'),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3\', None),\n    \'torch\': (\'https://pytorch.org/docs/stable/\', None),\n    \'numpy\': (\'https://docs.scipy.org/doc/numpy/\', None),\n    \'PIL\': (\'https://pillow.readthedocs.io/en/stable/\', None),\n}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# packages for which sphinx-apidoc should generate the docs (.rst files)\nPACKAGES = [\n    pytorch_lightning.__name__,\n    \'pl_examples\',\n]\n\napidoc_output_folder = os.path.join(PATH_HERE, \'api\')\n\n\ndef run_apidoc(_):\n    sys.path.insert(0, apidoc_output_folder)\n\n    # delete api-doc files before generating them\n    if os.path.exists(apidoc_output_folder):\n        shutil.rmtree(apidoc_output_folder)\n\n    for pkg in PACKAGES:\n        argv = [\'-e\',\n                \'-o\', apidoc_output_folder,\n                os.path.join(PATH_ROOT, pkg),\n                \'**/test_*\',\n                \'--force\',\n                \'--private\',\n                \'--module-first\']\n\n        apidoc.main(argv)\n\n\ndef setup(app):\n    app.connect(\'builder-inited\', run_apidoc)\n\n\n# copy all notebooks to local folder\npath_nbs = os.path.join(PATH_HERE, \'notebooks\')\nif not os.path.isdir(path_nbs):\n    os.mkdir(path_nbs)\nfor path_ipynb in glob.glob(os.path.join(PATH_ROOT, \'notebooks\', \'*.ipynb\')):\n    path_ipynb2 = os.path.join(path_nbs, os.path.basename(path_ipynb))\n    shutil.copy(path_ipynb, path_ipynb2)\n\n\n# Ignoring Third-party packages\n# https://stackoverflow.com/questions/15889621/sphinx-how-to-exclude-imports-in-automodule\ndef package_list_from_file(file):\n    mocked_packages = []\n    with open(file, \'r\') as fp:\n        for ln in fp.readlines():\n            found = [ln.index(ch) for ch in list(\',=<>#\') if ch in ln]\n            pkg = ln[:min(found)] if found else ln\n            if pkg.rstrip():\n                mocked_packages.append(pkg.rstrip())\n    return mocked_packages\n\n\nMOCK_PACKAGES = []\nif SPHINX_MOCK_REQUIREMENTS:\n    # mock also base packages when we are on RTD since we don\'t install them there\n    MOCK_PACKAGES += package_list_from_file(os.path.join(PATH_ROOT, \'requirements.txt\'))\n    MOCK_PACKAGES += package_list_from_file(os.path.join(PATH_ROOT, \'requirements-extra.txt\'))\n\nMOCK_MANUAL_PACKAGES = [\n    \'torchvision\',\n    \'PIL\',\n    # packages with different package name compare to import name\n    \'yaml\',\n    \'comet_ml\',\n    \'neptune\',\n]\nautodoc_mock_imports = MOCK_PACKAGES + MOCK_MANUAL_PACKAGES\n\n\n# Resolve function\n# This function is used to populate the (source) links in the API\ndef linkcode_resolve(domain, info):\n    def find_source():\n        # try to find the file and line number, based on code from numpy:\n        # https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286\n        obj = sys.modules[info[\'module\']]\n        for part in info[\'fullname\'].split(\'.\'):\n            obj = getattr(obj, part)\n        fname = inspect.getsourcefile(obj)\n        # https://github.com/rtfd/readthedocs.org/issues/5735\n        if any([s in fname for s in (\'readthedocs\', \'rtfd\', \'checkouts\')]):\n            # /home/docs/checkouts/readthedocs.org/user_builds/pytorch_lightning/checkouts/\n            #  devel/pytorch_lightning/utilities/cls_experiment.py#L26-L176\n            path_top = os.path.abspath(os.path.join(\'..\', \'..\', \'..\'))\n            fname = os.path.relpath(fname, start=path_top)\n        else:\n            # Local build, imitate master\n            fname = \'master/\' + os.path.relpath(fname, start=os.path.abspath(\'..\'))\n        source, lineno = inspect.getsourcelines(obj)\n        return fname, lineno, lineno + len(source) - 1\n\n    if domain != \'py\' or not info[\'module\']:\n        return None\n    try:\n        filename = \'%s#L%d-L%d\' % find_source()\n    except Exception:\n        filename = info[\'module\'].replace(\'.\', \'/\') + \'.py\'\n    # import subprocess\n    # tag = subprocess.Popen([\'git\', \'rev-parse\', \'HEAD\'], stdout=subprocess.PIPE,\n    #                        universal_newlines=True).communicate()[0][:-1]\n    branch = filename.split(\'/\')[0]\n    # do mapping from latest tags to master\n    branch = {\'latest\': \'master\', \'stable\': \'master\'}.get(branch, branch)\n    filename = \'/\'.join([branch] + filename.split(\'/\')[1:])\n    return ""https://github.com/%s/%s/blob/%s"" \\\n           % (github_user, github_repo, filename)\n\n\nautodoc_member_order = \'groupwise\'\nautoclass_content = \'both\'\n# the options are fixed and will be soon in release,\n#  see https://github.com/sphinx-doc/sphinx/issues/5459\nautodoc_default_options = {\n    \'members\': None,\n    \'methods\': None,\n    # \'attributes\': None,\n    \'special-members\': \'__call__\',\n    \'exclude-members\': \'_abc_impl\',\n    \'show-inheritance\': True,\n    \'private-members\': True,\n    \'noindex\': True,\n}\n\n# Sphinx will add \xe2\x80\x9cpermalinks\xe2\x80\x9d for each heading and description environment as paragraph signs that\n#  become visible when the mouse hovers over them.\n# This value determines the text for the permalink; it defaults to ""\xc2\xb6"". Set it to None or the empty\n#  string to disable permalinks.\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-html_add_permalinks\nhtml_add_permalinks = ""\xc2\xb6""\n\n# True to prefix each section label with the name of the document it is in, followed by a colon.\n#  For example, index:Introduction for a section called Introduction that appears in document index.rst.\n#  Useful for avoiding ambiguity when the same section heading appears in different documents.\n# http://www.sphinx-doc.org/en/master/usage/extensions/autosectionlabel.html\nautosectionlabel_prefix_document = True\n\n# only run doctests marked with a "".. doctest::"" directive\ndoctest_test_doctest_blocks = \'\'\ndoctest_global_setup = """"""\n\nimport importlib\nimport os\nimport torch\n\nTORCHVISION_AVAILABLE = importlib.util.find_spec(\'torchvision\')\n\n""""""\ncoverage_skip_undoc_in_source = True\n'"
pl_examples/basic_examples/__init__.py,0,b''
pl_examples/basic_examples/cpu_template.py,0,"b'""""""\nRuns a model on the CPU on a single node.\n""""""\nimport os\nfrom argparse import ArgumentParser\n\nimport numpy as np\nimport torch\n\nimport pytorch_lightning as pl\nfrom pl_examples.models.lightning_template import LightningTemplateModel\n\npl.seed_everything(234)\n\n\ndef main(args):\n    """"""\n    Main training routine specific for this project\n    :param args:\n    """"""\n    # ------------------------\n    # 1 INIT LIGHTNING MODEL\n    # ------------------------\n    model = LightningTemplateModel(**vars(args))\n\n    # ------------------------\n    # 2 INIT TRAINER\n    # ------------------------\n    trainer = pl.Trainer.from_argparse_args(args)\n\n    # ------------------------\n    # 3 START TRAINING\n    # ------------------------\n    trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    # ------------------------\n    # TRAINING ARGUMENTS\n    # ------------------------\n    # these are project-wide arguments\n    root_dir = os.path.dirname(os.path.realpath(__file__))\n    parent_parser = ArgumentParser(add_help=False)\n\n    # each LightningModule defines arguments relevant to it\n    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\n    parser = pl.Trainer.add_argparse_args(parser)\n    args = parser.parse_args()\n\n    # ---------------------\n    # RUN TRAINING\n    # ---------------------\n    main(args)\n'"
pl_examples/basic_examples/gpu_template.py,1,"b'""""""\nRuns a model on a single node across multiple gpus.\n""""""\nimport os\nfrom argparse import ArgumentParser\n\nimport numpy as np\nimport torch\n\nimport pytorch_lightning as pl\nfrom pl_examples.models.lightning_template import LightningTemplateModel\n\nSEED = 2334\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n\ndef main(hparams):\n    """"""\n    Main training routine specific for this project\n    :param hparams:\n    """"""\n    # ------------------------\n    # 1 INIT LIGHTNING MODEL\n    # ------------------------\n    model = LightningTemplateModel(hparams)\n\n    # ------------------------\n    # 2 INIT TRAINER\n    # ------------------------\n    trainer = pl.Trainer(\n        max_epochs=hparams.epochs,\n        gpus=hparams.gpus,\n        distributed_backend=hparams.distributed_backend,\n        precision=16 if hparams.use_16bit else 32,\n    )\n\n    # ------------------------\n    # 3 START TRAINING\n    # ------------------------\n    trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    # ------------------------\n    # TRAINING ARGUMENTS\n    # ------------------------\n    # these are project-wide arguments\n\n    root_dir = os.path.dirname(os.path.realpath(__file__))\n    parent_parser = ArgumentParser(add_help=False)\n\n    # gpu args\n    parent_parser.add_argument(\n        \'--gpus\',\n        type=int,\n        default=2,\n        help=\'how many gpus\'\n    )\n    parent_parser.add_argument(\n        \'--distributed_backend\',\n        type=str,\n        default=\'dp\',\n        help=\'supports three options dp, ddp, ddp2\'\n    )\n    parent_parser.add_argument(\n        \'--use_16bit\',\n        dest=\'use_16bit\',\n        action=\'store_true\',\n        help=\'if true uses 16 bit precision\'\n    )\n\n    # each LightningModule defines arguments relevant to it\n    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\n    hyperparams = parser.parse_args()\n\n    # ---------------------\n    # RUN TRAINING\n    # ---------------------\n    main(hyperparams)\n'"
pl_examples/basic_examples/multi_node_ddp2_demo.py,1,"b'""""""\nMulti-node example (GPU)\n""""""\nimport os\nfrom argparse import ArgumentParser\n\nimport numpy as np\nimport torch\n\nimport pytorch_lightning as pl\nfrom pl_examples.models.lightning_template import LightningTemplateModel\n\nSEED = 2334\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n\ndef main(hparams):\n    """"""Main training routine specific for this project.""""""\n    # ------------------------\n    # 1 INIT LIGHTNING MODEL\n    # ------------------------\n    model = LightningTemplateModel(hparams)\n\n    # ------------------------\n    # 2 INIT TRAINER\n    # ------------------------\n    trainer = pl.Trainer(\n        gpus=2,\n        num_nodes=2,\n        distributed_backend=\'ddp2\'\n    )\n\n    # ------------------------\n    # 3 START TRAINING\n    # ------------------------\n    trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    root_dir = os.path.dirname(os.path.realpath(__file__))\n    parent_parser = ArgumentParser(add_help=False)\n\n    # each LightningModule defines arguments relevant to it\n    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\n    hyperparams = parser.parse_args()\n\n    # ---------------------\n    # RUN TRAINING\n    # ---------------------\n    main(hyperparams)\n'"
pl_examples/basic_examples/multi_node_ddp_demo.py,1,"b'""""""\nMulti-node example (GPU)\n""""""\nimport os\nfrom argparse import ArgumentParser\n\nimport numpy as np\nimport torch\n\nimport pytorch_lightning as pl\nfrom pl_examples.models.lightning_template import LightningTemplateModel\n\nSEED = 2334\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n\ndef main(hparams):\n    """"""Main training routine specific for this project.""""""\n    # ------------------------\n    # 1 INIT LIGHTNING MODEL\n    # ------------------------\n    model = LightningTemplateModel(hparams)\n\n    # ------------------------\n    # 2 INIT TRAINER\n    # ------------------------\n    trainer = pl.Trainer(\n        gpus=2,\n        num_nodes=2,\n        distributed_backend=\'ddp\'\n    )\n\n    # ------------------------\n    # 3 START TRAINING\n    # ------------------------\n    trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    root_dir = os.path.dirname(os.path.realpath(__file__))\n    parent_parser = ArgumentParser(add_help=False)\n\n    # each LightningModule defines arguments relevant to it\n    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\n    hyperparams = parser.parse_args()\n\n    # ---------------------\n    # RUN TRAINING\n    # ---------------------\n    main(hyperparams)\n'"
pl_examples/domain_templates/__init__.py,0,b''
pl_examples/domain_templates/computer_vision_fine_tuning.py,24,"b'""""""Computer vision example on Transfer Learning.\n\nThis computer vision example illustrates how one could fine-tune a pre-trained\nnetwork (by default, a ResNet50 is used) using pytorch-lightning. For the sake\nof this example, the \'cats and dogs dataset\' (~60MB, see `DATA_URL` below) and\nthe proposed network (denoted by `TransferLearningModel`, see below) is\ntrained for 15 epochs. The training consists in three stages. From epoch 0 to\n4, the feature extractor (the pre-trained network) is frozen except maybe for\nthe BatchNorm layers (depending on whether `train_bn = True`). The BatchNorm\nlayers (if `train_bn = True`) and the parameters of the classifier are trained\nas a single parameters group with lr = 1e-2. From epoch 5 to 9, the last two\nlayer groups of the pre-trained network are unfrozen and added to the\noptimizer as a new parameter group with lr = 1e-4 (while lr = 1e-3 for the\nfirst parameter group in the optimizer). Eventually, from epoch 10, all the\nremaining layer groups of the pre-trained network are unfrozen and added to\nthe optimizer as a third parameter group. From epoch 10, the parameters of the\npre-trained network are trained with lr = 1e-5 while those of the classifier\nare trained with lr = 1e-4.\n\nNote:\n    See: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n""""""\n\nimport argparse\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom typing import Optional, Generator, Union\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_lightning import _logger as log\nfrom torch import optim\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.optim.optimizer import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torchvision import models\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.datasets.utils import download_and_extract_archive\n\nBN_TYPES = (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)\nDATA_URL = \'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\'\n\n\n#  --- Utility functions ---\n\n\ndef _make_trainable(module: torch.nn.Module) -> None:\n    """"""Unfreezes a given module.\n\n    Args:\n        module: The module to unfreeze\n    """"""\n    for param in module.parameters():\n        param.requires_grad = True\n    module.train()\n\n\ndef _recursive_freeze(module: torch.nn.Module,\n                      train_bn: bool = True) -> None:\n    """"""Freezes the layers of a given module.\n\n    Args:\n        module: The module to freeze\n        train_bn: If True, leave the BatchNorm layers in training mode\n    """"""\n    children = list(module.children())\n    if not children:\n        if not (isinstance(module, BN_TYPES) and train_bn):\n            for param in module.parameters():\n                param.requires_grad = False\n            module.eval()\n        else:\n            # Make the BN layers trainable\n            _make_trainable(module)\n    else:\n        for child in children:\n            _recursive_freeze(module=child, train_bn=train_bn)\n\n\ndef freeze(module: torch.nn.Module,\n           n: Optional[int] = None,\n           train_bn: bool = True) -> None:\n    """"""Freezes the layers up to index n (if n is not None).\n\n    Args:\n        module: The module to freeze (at least partially)\n        n: Max depth at which we stop freezing the layers. If None, all\n            the layers of the given module will be frozen.\n        train_bn: If True, leave the BatchNorm layers in training mode\n    """"""\n    children = list(module.children())\n    n_max = len(children) if n is None else int(n)\n\n    for child in children[:n_max]:\n        _recursive_freeze(module=child, train_bn=train_bn)\n\n    for child in children[n_max:]:\n        _make_trainable(module=child)\n\n\ndef filter_params(module: torch.nn.Module,\n                  train_bn: bool = True) -> Generator:\n    """"""Yields the trainable parameters of a given module.\n\n    Args:\n        module: A given module\n        train_bn: If True, leave the BatchNorm layers in training mode\n\n    Returns:\n        Generator\n    """"""\n    children = list(module.children())\n    if not children:\n        if not (isinstance(module, BN_TYPES) and train_bn):\n            for param in module.parameters():\n                if param.requires_grad:\n                    yield param\n    else:\n        for child in children:\n            for param in filter_params(module=child, train_bn=train_bn):\n                yield param\n\n\ndef _unfreeze_and_add_param_group(module: torch.nn.Module,\n                                  optimizer: Optimizer,\n                                  lr: Optional[float] = None,\n                                  train_bn: bool = True):\n    """"""Unfreezes a module and adds its parameters to an optimizer.""""""\n    _make_trainable(module)\n    params_lr = optimizer.param_groups[0][\'lr\'] if lr is None else float(lr)\n    optimizer.add_param_group(\n        {\'params\': filter_params(module=module, train_bn=train_bn),\n         \'lr\': params_lr / 10.,\n         })\n\n\n#  --- Pytorch-lightning module ---\n\n\nclass TransferLearningModel(pl.LightningModule):\n    """"""Transfer Learning with pre-trained ResNet50.\n\n    Args:\n        hparams: Model hyperparameters\n        dl_path: Path where the data will be downloaded\n    """"""\n    def __init__(self,\n                 dl_path: Union[str, Path],\n                 backbone: str = \'resnet50\',\n                 train_bn: bool = True,\n                 milestones: tuple = (5, 10),\n                 batch_size: int = 8,\n                 lr: float = 1e-2,\n                 lr_scheduler_gamma: float = 1e-1,\n                 num_workers: int = 6, **kwargs) -> None:\n        super().__init__()\n        self.dl_path = dl_path\n        self.backbone = backbone\n        self.train_bn = train_bn\n        self.milestones = milestones\n        self.batch_size = batch_size\n        self.lr = lr\n        self.lr_scheduler_gamma = lr_scheduler_gamma\n        self.num_workers = num_workers\n\n        self.dl_path = dl_path\n        self.__build_model()\n\n    def __build_model(self):\n        """"""Define model layers & loss.""""""\n\n        # 1. Load pre-trained network:\n        model_func = getattr(models, self.backbone)\n        backbone = model_func(pretrained=True)\n\n        _layers = list(backbone.children())[:-1]\n        self.feature_extractor = torch.nn.Sequential(*_layers)\n        freeze(module=self.feature_extractor, train_bn=self.train_bn)\n\n        # 2. Classifier:\n        _fc_layers = [torch.nn.Linear(2048, 256),\n                      torch.nn.Linear(256, 32),\n                      torch.nn.Linear(32, 1)]\n        self.fc = torch.nn.Sequential(*_fc_layers)\n\n        # 3. Loss:\n        self.loss_func = F.binary_cross_entropy_with_logits\n\n    def forward(self, x):\n        """"""Forward pass. Returns logits.""""""\n\n        # 1. Feature extraction:\n        x = self.feature_extractor(x)\n        x = x.squeeze(-1).squeeze(-1)\n\n        # 2. Classifier (returns logits):\n        x = self.fc(x)\n\n        return x\n\n    def loss(self, labels, logits):\n        return self.loss_func(input=logits, target=labels)\n\n    def train(self, mode=True):\n        super().train(mode=mode)\n\n        epoch = self.current_epoch\n        if epoch < self.milestones[0] and mode:\n            # feature extractor is frozen (except for BatchNorm layers)\n            freeze(module=self.feature_extractor,\n                   train_bn=self.train_bn)\n\n        elif self.milestones[0] <= epoch < self.milestones[1] and mode:\n            # Unfreeze last two layers of the feature extractor\n            freeze(module=self.feature_extractor,\n                   n=-2,\n                   train_bn=self.train_bn)\n\n    def on_epoch_start(self):\n        """"""Use `on_epoch_start` to unfreeze layers progressively.""""""\n        optimizer = self.trainer.optimizers[0]\n        if self.current_epoch == self.milestones[0]:\n            _unfreeze_and_add_param_group(module=self.feature_extractor[-2:],\n                                          optimizer=optimizer,\n                                          train_bn=self.train_bn)\n\n        elif self.current_epoch == self.milestones[1]:\n            _unfreeze_and_add_param_group(module=self.feature_extractor[:-2],\n                                          optimizer=optimizer,\n                                          train_bn=self.train_bn)\n\n    def training_step(self, batch, batch_idx):\n\n        # 1. Forward pass:\n        x, y = batch\n        y_logits = self.forward(x)\n        y_true = y.view((-1, 1)).type_as(x)\n        y_bin = torch.ge(y_logits, 0)\n\n        # 2. Compute loss & accuracy:\n        train_loss = self.loss(y_true, y_logits)\n        num_correct = torch.eq(y_bin.view(-1), y_true.view(-1)).sum()\n\n        # 3. Outputs:\n        tqdm_dict = {\'train_loss\': train_loss}\n        output = OrderedDict({\'loss\': train_loss,\n                              \'num_correct\': num_correct,\n                              \'log\': tqdm_dict,\n                              \'progress_bar\': tqdm_dict})\n\n        return output\n\n    def training_epoch_end(self, outputs):\n        """"""Compute and log training loss and accuracy at the epoch level.""""""\n\n        train_loss_mean = torch.stack([output[\'loss\']\n                                       for output in outputs]).mean()\n        train_acc_mean = torch.stack([output[\'num_correct\']\n                                      for output in outputs]).sum().float()\n        train_acc_mean /= (len(outputs) * self.batch_size)\n        return {\'log\': {\'train_loss\': train_loss_mean,\n                        \'train_acc\': train_acc_mean,\n                        \'step\': self.current_epoch}}\n\n    def validation_step(self, batch, batch_idx):\n\n        # 1. Forward pass:\n        x, y = batch\n        y_logits = self.forward(x)\n        y_true = y.view((-1, 1)).type_as(x)\n        y_bin = torch.ge(y_logits, 0)\n\n        # 2. Compute loss & accuracy:\n        val_loss = self.loss(y_true, y_logits)\n        num_correct = torch.eq(y_bin.view(-1), y_true.view(-1)).sum()\n\n        return {\'val_loss\': val_loss,\n                \'num_correct\': num_correct}\n\n    def validation_epoch_end(self, outputs):\n        """"""Compute and log validation loss and accuracy at the epoch level.""""""\n\n        val_loss_mean = torch.stack([output[\'val_loss\']\n                                     for output in outputs]).mean()\n        val_acc_mean = torch.stack([output[\'num_correct\']\n                                    for output in outputs]).sum().float()\n        val_acc_mean /= (len(outputs) * self.batch_size)\n        return {\'log\': {\'val_loss\': val_loss_mean,\n                        \'val_acc\': val_acc_mean,\n                        \'step\': self.current_epoch}}\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n                                      self.parameters()),\n                               lr=self.lr)\n\n        scheduler = MultiStepLR(optimizer,\n                                milestones=self.milestones,\n                                gamma=self.lr_scheduler_gamma)\n\n        return [optimizer], [scheduler]\n\n    def prepare_data(self):\n        """"""Download images and prepare images datasets.""""""\n\n        # 1. Download the images\n        download_and_extract_archive(url=DATA_URL,\n                                     download_root=self.dl_path,\n                                     remove_finished=True)\n\n        data_path = Path(self.dl_path).joinpath(\'cats_and_dogs_filtered\')\n\n        # 2. Load the data + preprocessing & data augmentation\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n\n        train_dataset = ImageFolder(root=data_path.joinpath(\'train\'),\n                                    transform=transforms.Compose([\n                                        transforms.Resize((224, 224)),\n                                        transforms.RandomHorizontalFlip(),\n                                        transforms.ToTensor(),\n                                        normalize,\n                                    ]))\n\n        valid_dataset = ImageFolder(root=data_path.joinpath(\'validation\'),\n                                    transform=transforms.Compose([\n                                        transforms.Resize((224, 224)),\n                                        transforms.ToTensor(),\n                                        normalize,\n                                    ]))\n\n        self.train_dataset = train_dataset\n        self.valid_dataset = valid_dataset\n\n    def __dataloader(self, train):\n        """"""Train/validation loaders.""""""\n\n        _dataset = self.train_dataset if train else self.valid_dataset\n        loader = DataLoader(dataset=_dataset,\n                            batch_size=self.batch_size,\n                            num_workers=self.num_workers,\n                            shuffle=True if train else False)\n\n        return loader\n\n    def train_dataloader(self):\n        log.info(\'Training data loaded.\')\n        return self.__dataloader(train=True)\n\n    def val_dataloader(self):\n        log.info(\'Validation data loaded.\')\n        return self.__dataloader(train=False)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser):\n        parser = argparse.ArgumentParser(parents=[parent_parser])\n        parser.add_argument(\'--backbone\',\n                            default=\'resnet50\',\n                            type=str,\n                            metavar=\'BK\',\n                            help=\'Name (as in ``torchvision.models``) of the feature extractor\')\n        parser.add_argument(\'--epochs\',\n                            default=15,\n                            type=int,\n                            metavar=\'N\',\n                            help=\'total number of epochs\',\n                            dest=\'nb_epochs\')\n        parser.add_argument(\'--batch-size\',\n                            default=8,\n                            type=int,\n                            metavar=\'B\',\n                            help=\'batch size\',\n                            dest=\'batch_size\')\n        parser.add_argument(\'--gpus\',\n                            type=int,\n                            default=1,\n                            help=\'number of gpus to use\')\n        parser.add_argument(\'--lr\',\n                            \'--learning-rate\',\n                            default=1e-2,\n                            type=float,\n                            metavar=\'LR\',\n                            help=\'initial learning rate\',\n                            dest=\'lr\')\n        parser.add_argument(\'--lr-scheduler-gamma\',\n                            default=1e-1,\n                            type=float,\n                            metavar=\'LRG\',\n                            help=\'Factor by which the learning rate is reduced at each milestone\',\n                            dest=\'lr_scheduler_gamma\')\n        parser.add_argument(\'--num-workers\',\n                            default=6,\n                            type=int,\n                            metavar=\'W\',\n                            help=\'number of CPU workers\',\n                            dest=\'num_workers\')\n        parser.add_argument(\'--train-bn\',\n                            default=True,\n                            type=bool,\n                            metavar=\'TB\',\n                            help=\'Whether the BatchNorm layers should be trainable\',\n                            dest=\'train_bn\')\n        parser.add_argument(\'--milestones\',\n                            default=[5, 10],\n                            type=list,\n                            metavar=\'M\',\n                            help=\'List of two epochs milestones\')\n        return parser\n\n\ndef main(args: argparse.Namespace) -> None:\n    """"""Train the model.\n\n    Args:\n        args: Model hyper-parameters\n\n    Note:\n        For the sake of the example, the images dataset will be downloaded\n        to a temporary directory.\n    """"""\n\n    with TemporaryDirectory(dir=args.root_data_path) as tmp_dir:\n\n        model = TransferLearningModel(dl_path=tmp_dir, **vars(args))\n\n        trainer = pl.Trainer(\n            weights_summary=None,\n            show_progress_bar=True,\n            num_sanity_val_steps=0,\n            gpus=args.gpus,\n            min_epochs=args.nb_epochs,\n            max_epochs=args.nb_epochs)\n\n        trainer.fit(model)\n\n\ndef get_args() -> argparse.Namespace:\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    parent_parser.add_argument(\'--root-data-path\',\n                               metavar=\'DIR\',\n                               type=str,\n                               default=Path.cwd().as_posix(),\n                               help=\'Root directory where to download the data\',\n                               dest=\'root_data_path\')\n    parser = TransferLearningModel.add_model_specific_args(parent_parser)\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main(get_args())\n'"
pl_examples/domain_templates/generative_adversarial_net.py,11,"b'""""""\nTo run this template just do:\npython generative_adversarial_net.py\n\nAfter a few epochs, launch TensorBoard to see the images being generated at every batch:\n\ntensorboard --logdir default\n""""""\nimport os\nfrom argparse import ArgumentParser, Namespace\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.core import LightningModule\nfrom pytorch_lightning.trainer import Trainer\n\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_shape):\n        super().__init__()\n        self.img_shape = img_shape\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *self.img_shape)\n        return img\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_shape):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n\n        return validity\n\n\nclass GAN(LightningModule):\n\n    def __init__(self,\n                 latent_dim: int = 100,\n                 lr: float = 0.0002,\n                 b1: float = 0.5,\n                 b2: float = 0.999,\n                 batch_size: int = 64, **kwargs):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.lr = lr\n        self.b1 = b1\n        self.b2 = b2\n        self.batch_size = batch_size\n\n        # networks\n        mnist_shape = (1, 28, 28)\n        self.generator = Generator(latent_dim=self.latent_dim, img_shape=mnist_shape)\n        self.discriminator = Discriminator(img_shape=mnist_shape)\n\n        self.validation_z = torch.randn(8, self.latent_dim)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        imgs, _ = batch\n\n        # sample noise\n        z = torch.randn(imgs.shape[0], self.latent_dim)\n        z = z.type_as(imgs)\n\n        # train generator\n        if optimizer_idx == 0:\n\n            # generate images\n            self.generated_imgs = self(z)\n\n            # log sampled images\n            sample_imgs = self.generated_imgs[:6]\n            grid = torchvision.utils.make_grid(sample_imgs)\n            self.logger.experiment.add_image(\'generated_images\', grid, 0)\n\n            # ground truth result (ie: all fake)\n            # put on GPU because we created this tensor inside training_loop\n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n\n            # adversarial loss is binary cross-entropy\n            g_loss = self.adversarial_loss(self.discriminator(self(z)), valid)\n            tqdm_dict = {\'g_loss\': g_loss}\n            output = OrderedDict({\n                \'loss\': g_loss,\n                \'progress_bar\': tqdm_dict,\n                \'log\': tqdm_dict\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator\'s ability to classify real from generated samples\n\n            # how well can it label as real?\n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n\n            real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n\n            # how well can it label as fake?\n            fake = torch.zeros(imgs.size(0), 1)\n            fake = fake.type_as(imgs)\n\n            fake_loss = self.adversarial_loss(\n                self.discriminator(self(z).detach()), fake)\n\n            # discriminator loss is the average of these\n            d_loss = (real_loss + fake_loss) / 2\n            tqdm_dict = {\'d_loss\': d_loss}\n            output = OrderedDict({\n                \'loss\': d_loss,\n                \'progress_bar\': tqdm_dict,\n                \'log\': tqdm_dict\n            })\n            return output\n\n    def configure_optimizers(self):\n        lr = self.lr\n        b1 = self.b1\n        b2 = self.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.batch_size)\n\n    def on_epoch_end(self):\n        z = self.validation_z.type_as(self.generator.model[0].weight)\n\n        # log sampled images\n        sample_imgs = self(z)\n        grid = torchvision.utils.make_grid(sample_imgs)\n        self.logger.experiment.add_image(\'generated_images\', grid, self.current_epoch)\n\n\ndef main(args: Namespace) -> None:\n    # ------------------------\n    # 1 INIT LIGHTNING MODEL\n    # ------------------------\n    model = GAN(**vars(args))\n\n    # ------------------------\n    # 2 INIT TRAINER\n    # ------------------------\n    # If use distubuted training  PyTorch recommends to use DistributedDataParallel.\n    # See: https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel\n    trainer = Trainer()\n\n    # ------------------------\n    # 3 START TRAINING\n    # ------------------------\n    trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n    parser.add_argument(""--batch_size"", type=int, default=64, help=""size of the batches"")\n    parser.add_argument(""--lr"", type=float, default=0.0002, help=""adam: learning rate"")\n    parser.add_argument(""--b1"", type=float, default=0.5,\n                        help=""adam: decay of first order momentum of gradient"")\n    parser.add_argument(""--b2"", type=float, default=0.999,\n                        help=""adam: decay of first order momentum of gradient"")\n    parser.add_argument(""--latent_dim"", type=int, default=100,\n                        help=""dimensionality of the latent space"")\n\n    hparams = parser.parse_args()\n\n    main(hparams)\n'"
pl_examples/domain_templates/imagenet.py,13,"b'""""""\nThis example is largely adapted from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n""""""\nfrom argparse import ArgumentParser, Namespace\nimport os\nimport random\nfrom collections import OrderedDict\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.core import LightningModule\n\n# pull out resnet names from torchvision models\nMODEL_NAMES = sorted(\n    name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"") and callable(models.__dict__[name])\n)\n\n\nclass ImageNetLightningModel(LightningModule):\n    def __init__(self,\n                 arch,\n                 pretrained,\n                 lr: float,\n                 momentum: float,\n                 weight_decay: int,\n                 data_path: str,\n                 batch_size: int, **kwargs):\n        """"""\n        TODO: add docstring here\n        """"""\n        super().__init__()\n        self.arch = arch\n        self.pretrained = pretrained\n        self.lr = lr\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n        self.data_path = data_path\n        self.batch_size = batch_size\n        self.model = models.__dict__[self.arch](pretrained=self.pretrained)\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        images, target = batch\n        output = self(images)\n        loss_val = F.cross_entropy(output, target)\n        acc1, acc5 = self.__accuracy(output, target, topk=(1, 5))\n\n        tqdm_dict = {\'train_loss\': loss_val}\n        output = OrderedDict({\n            \'loss\': loss_val,\n            \'acc1\': acc1,\n            \'acc5\': acc5,\n            \'progress_bar\': tqdm_dict,\n            \'log\': tqdm_dict\n        })\n\n        return output\n\n    def validation_step(self, batch, batch_idx):\n        images, target = batch\n        output = self(images)\n        loss_val = F.cross_entropy(output, target)\n        acc1, acc5 = self.__accuracy(output, target, topk=(1, 5))\n\n        output = OrderedDict({\n            \'val_loss\': loss_val,\n            \'val_acc1\': acc1,\n            \'val_acc5\': acc5,\n        })\n\n        return output\n\n    def validation_epoch_end(self, outputs):\n\n        tqdm_dict = {}\n\n        for metric_name in [""val_loss"", ""val_acc1"", ""val_acc5""]:\n            metric_total = 0\n\n            for output in outputs:\n                metric_value = output[metric_name]\n\n                # reduce manually when using dp\n                if self.trainer.use_dp or self.trainer.use_ddp2:\n                    metric_value = torch.mean(metric_value)\n\n                metric_total += metric_value\n\n            tqdm_dict[metric_name] = metric_total / len(outputs)\n\n        result = {\'progress_bar\': tqdm_dict, \'log\': tqdm_dict, \'val_loss\': tqdm_dict[""val_loss""]}\n        return result\n\n    @classmethod\n    def __accuracy(cls, output, target, topk=(1,)):\n        """"""Computes the accuracy over the k top predictions for the specified values of k""""""\n        with torch.no_grad():\n            maxk = max(topk)\n            batch_size = target.size(0)\n\n            _, pred = output.topk(maxk, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n            res = []\n            for k in topk:\n                correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n                res.append(correct_k.mul_(100.0 / batch_size))\n            return res\n\n    def configure_optimizers(self):\n        optimizer = optim.SGD(\n            self.parameters(),\n            lr=self.lr,\n            momentum=self.momentum,\n            weight_decay=self.weight_decay\n        )\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n        return [optimizer], [scheduler]\n\n    def train_dataloader(self):\n        normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n\n        train_dir = os.path.join(self.data_path, \'train\')\n        train_dataset = datasets.ImageFolder(\n            train_dir,\n            transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n\n        if self.use_ddp:\n            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n        else:\n            train_sampler = None\n\n        train_loader = torch.utils.data.DataLoader(\n            dataset=train_dataset,\n            batch_size=self.batch_size,\n            shuffle=(train_sampler is None),\n            num_workers=0,\n            sampler=train_sampler\n        )\n        return train_loader\n\n    def val_dataloader(self):\n        normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n        val_dir = os.path.join(self.data_path, \'val\')\n        val_loader = torch.utils.data.DataLoader(\n            datasets.ImageFolder(val_dir, transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                normalize,\n            ])),\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=0,\n        )\n        return val_loader\n\n    @staticmethod\n    def add_model_specific_args(parent_parser):  # pragma: no-cover\n        parser = ArgumentParser(parents=[parent_parser])\n        parser.add_argument(\'-a\', \'--arch\', metavar=\'ARCH\', default=\'resnet18\', choices=MODEL_NAMES,\n                            help=\'model architecture: \' +\n                                 \' | \'.join(MODEL_NAMES) +\n                                 \' (default: resnet18)\')\n        parser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                            help=\'number of total epochs to run\')\n        parser.add_argument(\'--seed\', type=int, default=42,\n                            help=\'seed for initializing training. \')\n        parser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                            metavar=\'N\',\n                            help=\'mini-batch size (default: 256), this is the total \'\n                                 \'batch size of all GPUs on the current node when \'\n                                 \'using Data Parallel or Distributed Data Parallel\')\n        parser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                            metavar=\'LR\', help=\'initial learning rate\', dest=\'lr\')\n        parser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                            help=\'momentum\')\n        parser.add_argument(\'--wd\', \'--weight-decay\', default=1e-4, type=float,\n                            metavar=\'W\', help=\'weight decay (default: 1e-4)\',\n                            dest=\'weight_decay\')\n        parser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                            help=\'use pre-trained model\')\n        return parser\n\n\ndef get_args():\n    parent_parser = ArgumentParser(add_help=False)\n    parent_parser.add_argument(\'--data-path\', metavar=\'DIR\', type=str,\n                               help=\'path to dataset\')\n    parent_parser.add_argument(\'--save-path\', metavar=\'DIR\', default=""."", type=str,\n                               help=\'path to save output\')\n    parent_parser.add_argument(\'--gpus\', type=int, default=1,\n                               help=\'how many gpus\')\n    parent_parser.add_argument(\'--distributed-backend\', type=str, default=\'dp\', choices=(\'dp\', \'ddp\', \'ddp2\'),\n                               help=\'supports three options dp, ddp, ddp2\')\n    parent_parser.add_argument(\'--use-16bit\', dest=\'use_16bit\', action=\'store_true\',\n                               help=\'if true uses 16 bit precision\')\n    parent_parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                               help=\'evaluate model on validation set\')\n\n    parser = ImageNetLightningModel.add_model_specific_args(parent_parser)\n    return parser.parse_args()\n\n\ndef main(args: Namespace) -> None:\n    model = ImageNetLightningModel(**vars(args))\n\n    if args.seed is not None:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        cudnn.deterministic = True\n\n    trainer = pl.Trainer(\n        default_root_dir=args.save_path,\n        gpus=args.gpus,\n        max_epochs=args.epochs,\n        distributed_backend=args.distributed_backend,\n        precision=16 if args.use_16bit else 32,\n    )\n\n    if args.evaluate:\n        trainer.run_evaluation()\n    else:\n        trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    main(get_args())\n'"
pl_examples/domain_templates/reinforce_learn_Qnet.py,16,"b'""""""\nDeep Reinforcement Learning: Deep Q-network (DQN)\n\nThis example is based on https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-\nSecond-Edition/blob/master/Chapter06/02_dqn_pong.py\n\nThe template illustrates using Lightning for Reinforcement Learning. The example builds a basic DQN using the\nclassic CartPole environment.\n\nTo run the template just run:\npython reinforce_learn_Qnet.py\n\nAfter ~1500 steps, you will see the total_reward hitting the max score of 200. Open up TensorBoard to\nsee the metrics:\n\ntensorboard --logdir default\n""""""\n\nimport pytorch_lightning as pl\n\nfrom typing import Tuple, List\n\nimport argparse\nfrom collections import OrderedDict, deque, namedtuple\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import IterableDataset\n\n\nclass DQN(nn.Module):\n    """"""\n    Simple MLP network\n\n    Args:\n        obs_size: observation/state size of the environment\n        n_actions: number of discrete actions available in the environment\n        hidden_size: size of hidden layers\n    """"""\n\n    def __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n        super(DQN, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(obs_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_actions)\n        )\n\n    def forward(self, x):\n        return self.net(x.float())\n\n\n# Named tuple for storing experience steps gathered in training\nExperience = namedtuple(\n    \'Experience\', field_names=[\'state\', \'action\', \'reward\',\n                               \'done\', \'new_state\'])\n\n\nclass ReplayBuffer:\n    """"""\n    Replay Buffer for storing past experiences allowing the agent to learn from them\n\n    Args:\n        capacity: size of the buffer\n    """"""\n\n    def __init__(self, capacity: int) -> None:\n        self.buffer = deque(maxlen=capacity)\n\n    def __len__(self) -> int:\n        return len(self.buffer)\n\n    def append(self, experience: Experience) -> None:\n        """"""\n        Add experience to the buffer\n\n        Args:\n            experience: tuple (state, action, reward, done, new_state)\n        """"""\n        self.buffer.append(experience)\n\n    def sample(self, batch_size: int) -> Tuple:\n        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n\n        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n                np.array(dones, dtype=np.bool), np.array(next_states))\n\n\nclass RLDataset(IterableDataset):\n    """"""\n    Iterable Dataset containing the ExperienceBuffer\n    which will be updated with new experiences during training\n\n    Args:\n        buffer: replay buffer\n        sample_size: number of experiences to sample at a time\n    """"""\n\n    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n        self.buffer = buffer\n        self.sample_size = sample_size\n\n    def __iter__(self) -> Tuple:\n        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n        for i in range(len(dones)):\n            yield states[i], actions[i], rewards[i], dones[i], new_states[i]\n\n\nclass Agent:\n    """"""\n    Base Agent class handeling the interaction with the environment\n\n    Args:\n        env: training environment\n        replay_buffer: replay buffer storing experiences\n    """"""\n\n    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n        self.env = env\n        self.replay_buffer = replay_buffer\n        self.reset()\n        self.state = self.env.reset()\n\n    def reset(self) -> None:\n        """"""Resets the environment and updates the state""""""\n        self.state = self.env.reset()\n\n    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n        """"""\n        Using the given network, decide what action to carry out\n        using an epsilon-greedy policy\n\n        Args:\n            net: DQN network\n            epsilon: value to determine likelihood of taking a random action\n            device: current device\n\n        Returns:\n            action\n        """"""\n        if np.random.random() < epsilon:\n            action = self.env.action_space.sample()\n        else:\n            state = torch.tensor([self.state])\n\n            if device not in [\'cpu\']:\n                state = state.cuda(device)\n\n            q_values = net(state)\n            _, action = torch.max(q_values, dim=1)\n            action = int(action.item())\n\n        return action\n\n    @torch.no_grad()\n    def play_step(self, net: nn.Module, epsilon: float = 0.0, device: str = \'cpu\') -> Tuple[float, bool]:\n        """"""\n        Carries out a single interaction step between the agent and the environment\n\n        Args:\n            net: DQN network\n            epsilon: value to determine likelihood of taking a random action\n            device: current device\n\n        Returns:\n            reward, done\n        """"""\n\n        action = self.get_action(net, epsilon, device)\n\n        # do step in the environment\n        new_state, reward, done, _ = self.env.step(action)\n\n        exp = Experience(self.state, action, reward, done, new_state)\n\n        self.replay_buffer.append(exp)\n\n        self.state = new_state\n        if done:\n            self.reset()\n        return reward, done\n\n\nclass DQNLightning(pl.LightningModule):\n    """""" Basic DQN Model """"""\n\n    def __init__(self,\n                 replay_size,\n                 warm_start_steps: int,\n                 gamma: float,\n                 eps_start: int,\n                 eps_end: int,\n                 eps_last_frame: int,\n                 sync_rate,\n                 lr: float,\n                 episode_length,\n                 batch_size, **kwargs) -> None:\n        super().__init__()\n        self.replay_size = replay_size\n        self.warm_start_steps = warm_start_steps\n        self.gamma = gamma\n        self.eps_start = eps_start\n        self.eps_end = eps_end\n        self.eps_last_frame = eps_last_frame\n        self.sync_rate = sync_rate\n        self.lr = lr\n        self.episode_length = episode_length\n        self.batch_size = batch_size\n\n        self.env = gym.make(self.env)\n        obs_size = self.env.observation_space.shape[0]\n        n_actions = self.env.action_space.n\n\n        self.net = DQN(obs_size, n_actions)\n        self.target_net = DQN(obs_size, n_actions)\n\n        self.buffer = ReplayBuffer(self.replay_size)\n        self.agent = Agent(self.env, self.buffer)\n        self.total_reward = 0\n        self.episode_reward = 0\n        self.populate(self.warm_start_steps)\n\n    def populate(self, steps: int = 1000) -> None:\n        """"""\n        Carries out several random steps through the environment to initially fill\n        up the replay buffer with experiences\n\n        Args:\n            steps: number of random steps to populate the buffer with\n        """"""\n        for i in range(steps):\n            self.agent.play_step(self.net, epsilon=1.0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""\n        Passes in a state `x` through the network and gets the `q_values` of each action as an output\n\n        Args:\n            x: environment state\n\n        Returns:\n            q values\n        """"""\n        output = self.net(x)\n        return output\n\n    def dqn_mse_loss(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""\n        Calculates the mse loss using a mini batch from the replay buffer\n\n        Args:\n            batch: current mini batch of replay data\n\n        Returns:\n            loss\n        """"""\n        states, actions, rewards, dones, next_states = batch\n\n        state_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n\n        with torch.no_grad():\n            next_state_values = self.target_net(next_states).max(1)[0]\n            next_state_values[dones] = 0.0\n            next_state_values = next_state_values.detach()\n\n        expected_state_action_values = next_state_values * self.gamma + rewards\n\n        return nn.MSELoss()(state_action_values, expected_state_action_values)\n\n    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], nb_batch) -> OrderedDict:\n        """"""\n        Carries out a single step through the environment to update the replay buffer.\n        Then calculates loss based on the minibatch received\n\n        Args:\n            batch: current mini batch of replay data\n            nb_batch: batch number\n\n        Returns:\n            Training loss and log metrics\n        """"""\n        device = self.get_device(batch)\n        epsilon = max(self.eps_end, self.eps_start -\n                      self.global_step + 1 / self.eps_last_frame)\n\n        # step through environment with agent\n        reward, done = self.agent.play_step(self.net, epsilon, device)\n        self.episode_reward += reward\n\n        # calculates training loss\n        loss = self.dqn_mse_loss(batch)\n\n        if done:\n            self.total_reward = self.episode_reward\n            self.episode_reward = 0\n\n        # Soft update of target network\n        if self.global_step % self.sync_rate == 0:\n            self.target_net.load_state_dict(self.net.state_dict())\n\n        log = {\'total_reward\': torch.tensor(self.total_reward).to(device),\n               \'reward\': torch.tensor(reward).to(device),\n               \'steps\': torch.tensor(self.global_step).to(device)}\n\n        return OrderedDict({\'loss\': loss, \'log\': log, \'progress_bar\': log})\n\n    def configure_optimizers(self) -> List[Optimizer]:\n        """"""Initialize Adam optimizer""""""\n        optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n        return [optimizer]\n\n    def __dataloader(self) -> DataLoader:\n        """"""Initialize the Replay Buffer dataset used for retrieving experiences""""""\n        dataset = RLDataset(self.buffer, self.episode_length)\n        dataloader = DataLoader(\n            dataset=dataset,\n            batch_size=self.batch_size,\n            sampler=None,\n        )\n        return dataloader\n\n    def train_dataloader(self) -> DataLoader:\n        """"""Get train loader""""""\n        return self.__dataloader()\n\n    def get_device(self, batch) -> str:\n        """"""Retrieve device currently being used by minibatch""""""\n        return batch[0].device.index if self.on_gpu else \'cpu\'\n\n\ndef main(args) -> None:\n    model = DQNLightning(**vars(args))\n\n    trainer = pl.Trainer(\n        gpus=1,\n        distributed_backend=\'dp\',\n        early_stop_callback=False,\n        val_check_interval=100\n    )\n\n    trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    torch.manual_seed(0)\n    np.random.seed(0)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--batch_size"", type=int, default=16, help=""size of the batches"")\n    parser.add_argument(""--lr"", type=float, default=1e-2, help=""learning rate"")\n    parser.add_argument(""--env"", type=str, default=""CartPole-v0"", help=""gym environment tag"")\n    parser.add_argument(""--gamma"", type=float, default=0.99, help=""discount factor"")\n    parser.add_argument(""--sync_rate"", type=int, default=10,\n                        help=""how many frames do we update the target network"")\n    parser.add_argument(""--replay_size"", type=int, default=1000,\n                        help=""capacity of the replay buffer"")\n    parser.add_argument(""--warm_start_size"", type=int, default=1000,\n                        help=""how many samples do we use to fill our buffer at the start of training"")\n    parser.add_argument(""--eps_last_frame"", type=int, default=1000,\n                        help=""what frame should epsilon stop decaying"")\n    parser.add_argument(""--eps_start"", type=float, default=1.0, help=""starting value of epsilon"")\n    parser.add_argument(""--eps_end"", type=float, default=0.01, help=""final value of epsilon"")\n    parser.add_argument(""--episode_length"", type=int, default=200, help=""max length of an episode"")\n    parser.add_argument(""--max_episode_reward"", type=int, default=200,\n                        help=""max episode reward in the environment"")\n    parser.add_argument(""--warm_start_steps"", type=int, default=1000,\n                        help=""max episode reward in the environment"")\n\n    args = parser.parse_args()\n\n    main(args)\n'"
pl_examples/domain_templates/semantic_segmentation.py,5,"b'import os\nfrom argparse import ArgumentParser, Namespace\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nimport random\n\nimport pytorch_lightning as pl\nfrom pl_examples.models.unet import UNet\nfrom pytorch_lightning.loggers import WandbLogger\n\nDEFAULT_VOID_LABELS = (0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1)\nDEFAULT_VALID_LABELS = (7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33)\n\n\nclass KITTI(Dataset):\n    """"""\n    Class for KITTI Semantic Segmentation Benchmark dataset\n    Dataset link - http://www.cvlibs.net/datasets/kitti/eval_semseg.php?benchmark=semantics2015\n\n    There are 34 classes in the given labels. However, not all of them are useful for training\n    (like railings on highways, road dividers, etc.).\n    So, these useless classes (the pixel values of these classes) are stored in the `void_labels`.\n    The useful classes are stored in the `valid_labels`.\n\n    The `encode_segmap` function sets all pixels with any of the `void_labels` to `ignore_index`\n    (250 by default). It also sets all of the valid pixels to the appropriate value between 0 and\n    `len(valid_labels)` (since that is the number of valid classes), so it can be used properly by\n    the loss function when comparing with the output.\n\n    The `get_filenames` function retrieves the filenames of all images in the given `path` and\n    saves the absolute path in a list.\n\n    In the `get_item` function, images and masks are resized to the given `img_size`, masks are\n    encoded using `encode_segmap`, and given `transform` (if any) are applied to the image only\n    (mask does not usually require transforms, but they can be implemented in a similar way).\n    """"""\n    IMAGE_PATH = os.path.join(\'training\', \'image_2\')\n    MASK_PATH = os.path.join(\'training\', \'semantic\')\n\n    def __init__(\n        self,\n        data_path: str,\n        split: str,\n        img_size: tuple = (1242, 376),\n        void_labels: list = DEFAULT_VOID_LABELS,\n        valid_labels: list = DEFAULT_VALID_LABELS,\n        transform=None\n    ):\n        self.img_size = img_size\n        self.void_labels = void_labels\n        self.valid_labels = valid_labels\n        self.ignore_index = 250\n        self.class_map = dict(zip(self.valid_labels, range(len(self.valid_labels))))\n        self.transform = transform\n\n        self.split = split\n        self.data_path = data_path\n        self.img_path = os.path.join(self.data_path, self.IMAGE_PATH)\n        self.mask_path = os.path.join(self.data_path, self.MASK_PATH)\n        self.img_list = self.get_filenames(self.img_path)\n        self.mask_list = self.get_filenames(self.mask_path)\n\n        # Split between train and valid set (80/20)\n        random_inst = random.Random(12345)  # for repeatability\n        n_items = len(self.img_list)\n        idxs = random_inst.sample(range(n_items), n_items // 5)\n        if self.split == \'train\':\n            idxs = [idx for idx in range(n_items) if idx not in idxs]\n        self.img_list = [self.img_list[i] for i in idxs]\n        self.mask_list = [self.mask_list[i] for i in idxs]\n\n    def __len__(self):\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.img_list[idx])\n        img = img.resize(self.img_size)\n        img = np.array(img)\n\n        mask = Image.open(self.mask_list[idx]).convert(\'L\')\n        mask = mask.resize(self.img_size)\n        mask = np.array(mask)\n        mask = self.encode_segmap(mask)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, mask\n\n    def encode_segmap(self, mask):\n        """"""\n        Sets void classes to zero so they won\'t be considered for training\n        """"""\n        for voidc in self.void_labels:\n            mask[mask == voidc] = self.ignore_index\n        for validc in self.valid_labels:\n            mask[mask == validc] = self.class_map[validc]\n        # remove extra idxs from updated dataset\n        mask[mask > 18] = self.ignore_index\n        return mask\n\n    def get_filenames(self, path):\n        """"""\n        Returns a list of absolute paths to images inside given `path`\n        """"""\n        files_list = list()\n        for filename in os.listdir(path):\n            files_list.append(os.path.join(path, filename))\n        return files_list\n\n\nclass SegModel(pl.LightningModule):\n    """"""\n    Semantic Segmentation Module\n\n    This is a basic semantic segmentation module implemented with Lightning.\n    It uses CrossEntropyLoss as the default loss function. May be replaced with\n    other loss functions as required.\n    It is specific to KITTI dataset i.e. dataloaders are for KITTI\n    and Normalize transform uses the mean and standard deviation of this dataset.\n    It uses the FCN ResNet50 model as an example.\n\n    Adam optimizer is used along with Cosine Annealing learning rate scheduler.\n    """"""\n\n    def __init__(self,\n                 data_path: str,\n                 batch_size: int,\n                 lr: float,\n                 num_layers: int,\n                 features_start: int,\n                 bilinear: bool, **kwargs):\n        super().__init__()\n        self.data_path = data_path\n        self.batch_size = batch_size\n        self.lr = lr\n        self.num_layers = num_layers\n        self.features_start = features_start\n        self.bilinear = bilinear\n\n        self.net = UNet(num_classes=19, num_layers=self.num_layers,\n                        features_start=self.features_start, bilinear=self.bilinear)\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.35675976, 0.37380189, 0.3764753],\n                                 std=[0.32064945, 0.32098866, 0.32325324])\n        ])\n        self.trainset = KITTI(self.data_path, split=\'train\', transform=self.transform)\n        self.validset = KITTI(self.data_path, split=\'valid\', transform=self.transform)\n\n    def forward(self, x):\n        return self.net(x)\n\n    def training_step(self, batch, batch_nb):\n        img, mask = batch\n        img = img.float()\n        mask = mask.long()\n        out = self(img)\n        loss_val = F.cross_entropy(out, mask, ignore_index=250)\n        log_dict = {\'train_loss\': loss_val}\n        return {\'loss\': loss_val, \'log\': log_dict, \'progress_bar\': log_dict}\n\n    def validation_step(self, batch, batch_idx):\n        img, mask = batch\n        img = img.float()\n        mask = mask.long()\n        out = self(img)\n        loss_val = F.cross_entropy(out, mask, ignore_index=250)\n        return {\'val_loss\': loss_val}\n\n    def validation_epoch_end(self, outputs):\n        loss_val = torch.stack([x[\'val_loss\'] for x in outputs]).mean()\n        log_dict = {\'val_loss\': loss_val}\n        return {\'log\': log_dict, \'val_loss\': log_dict[\'val_loss\'], \'progress_bar\': log_dict}\n\n    def configure_optimizers(self):\n        opt = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n        return [opt], [sch]\n\n    def train_dataloader(self):\n        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=False)\n\n\ndef main(hparams: Namespace):\n    # ------------------------\n    # 1 INIT LIGHTNING MODEL\n    # ------------------------\n    model = SegModel(**vars(hparams))\n\n    # ------------------------\n    # 2 SET LOGGER\n    # ------------------------\n    logger = False\n    if hparams.log_wandb:\n        logger = WandbLogger()\n\n        # optional: log model topology\n        logger.watch(model.net)\n\n    # ------------------------\n    # 3 INIT TRAINER\n    # ------------------------\n    trainer = pl.Trainer(\n        gpus=hparams.gpus,\n        logger=logger,\n        max_epochs=hparams.epochs,\n        accumulate_grad_batches=hparams.grad_batches,\n        distributed_backend=hparams.distributed_backend,\n        precision=16 if hparams.use_amp else 32,\n    )\n\n    # ------------------------\n    # 5 START TRAINING\n    # ------------------------\n    trainer.fit(model)\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n    parser.add_argument(""--data_path"", type=str, help=""path where dataset is stored"")\n    parser.add_argument(""--gpus"", type=int, default=-1, help=""number of available GPUs"")\n    parser.add_argument(\'--distributed-backend\', type=str, default=\'dp\', choices=(\'dp\', \'ddp\', \'ddp2\'),\n                        help=\'supports three options dp, ddp, ddp2\')\n    parser.add_argument(\'--use_amp\', action=\'store_true\', help=\'if true uses 16 bit precision\')\n    parser.add_argument(""--batch_size"", type=int, default=4, help=""size of the batches"")\n    parser.add_argument(""--lr"", type=float, default=0.001, help=""adam: learning rate"")\n    parser.add_argument(""--num_layers"", type=int, default=5, help=""number of layers on u-net"")\n    parser.add_argument(""--features_start"", type=float, default=64, help=""number of features in first layer"")\n    parser.add_argument(""--bilinear"", action=\'store_true\', default=False,\n                        help=""whether to use bilinear interpolation or transposed"")\n    parser.add_argument(""--grad_batches"", type=int, default=1, help=""number of batches to accumulate"")\n    parser.add_argument(""--epochs"", type=int, default=20, help=""number of epochs to train"")\n    parser.add_argument(""--log_wandb"", action=\'store_true\', help=""log training on Weights & Biases"")\n\n    hparams = parser.parse_args()\n\n    main(hparams)\n'"
pl_examples/models/__init__.py,0,b''
pl_examples/models/lightning_template.py,10,"b'""""""\nExample template for defining a system.\n""""""\nimport os\nfrom argparse import ArgumentParser\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.core import LightningModule\n\n\nclass LightningTemplateModel(LightningModule):\n    """"""\n    Sample model to show how to define a template.\n\n    Example:\n\n        >>> # define simple Net for MNIST dataset\n        >>> params = dict(\n        ...     drop_prob=0.2,\n        ...     batch_size=2,\n        ...     in_features=28 * 28,\n        ...     learning_rate=0.001 * 8,\n        ...     optimizer_name=\'adam\',\n        ...     data_root=\'./datasets\',\n        ...     out_features=10,\n        ...     hidden_dim=1000,\n        ... )\n        >>> model = LightningTemplateModel(**params)\n    """"""\n\n    def __init__(self,\n                 drop_prob: float = 0.2,\n                 batch_size: int = 2,\n                 in_features: int = 28 * 28,\n                 learning_rate: float = 0.001 * 8,\n                 optimizer_name: str = \'adam\',\n                 data_root: str = \'./datasets\',\n                 out_features: int = 10,\n                 hidden_dim: int = 1000,\n                 **kwargs\n                 ) -> \'LightningTemplateModel\':\n        # init superclass\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.batch_size = batch_size\n        self.in_features = in_features\n        self.learning_rate = learning_rate\n        self.optimizer_name = optimizer_name\n        self.data_root = data_root\n        self.out_features = out_features\n        self.hidden_dim = hidden_dim\n\n        self.c_d1 = nn.Linear(in_features=self.in_features,\n                              out_features=self.hidden_dim)\n        self.c_d1_bn = nn.BatchNorm1d(self.hidden_dim)\n        self.c_d1_drop = nn.Dropout(self.drop_prob)\n\n        self.c_d2 = nn.Linear(in_features=self.hidden_dim,\n                              out_features=self.out_features)\n\n    def forward(self, x):\n        """"""\n        No special modification required for Lightning, define it as you normally would\n        in the `nn.Module` in vanilla PyTorch.\n        """"""\n        x = self.c_d1(x.view(x.size(0), -1))\n        x = torch.tanh(x)\n        x = self.c_d1_bn(x)\n        x = self.c_d1_drop(x)\n        x = self.c_d2(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        """"""\n        Lightning calls this inside the training loop with the data from the training dataloader\n        passed in as `batch`.\n        """"""\n        # forward pass\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        tensorboard_logs = {\'train_loss\': loss}\n        return {\'loss\': loss, \'log\': tensorboard_logs}\n\n    def validation_step(self, batch, batch_idx):\n        """"""\n        Lightning calls this inside the validation loop with the data from the validation dataloader\n        passed in as `batch`.\n        """"""\n        x, y = batch\n        y_hat = self(x)\n        val_loss = F.cross_entropy(y_hat, y)\n        labels_hat = torch.argmax(y_hat, dim=1)\n        n_correct_pred = torch.sum(y == labels_hat).item()\n        return {\'val_loss\': val_loss, ""n_correct_pred"": n_correct_pred, ""n_pred"": len(x)}\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        test_loss = F.cross_entropy(y_hat, y)\n        labels_hat = torch.argmax(y_hat, dim=1)\n        n_correct_pred = torch.sum(y == labels_hat).item()\n        return {\'test_loss\': test_loss, ""n_correct_pred"": n_correct_pred, ""n_pred"": len(x)}\n\n    def validation_epoch_end(self, outputs):\n        """"""\n        Called at the end of validation to aggregate outputs.\n        :param outputs: list of individual outputs of each validation step.\n        """"""\n        avg_loss = torch.stack([x[\'val_loss\'] for x in outputs]).mean()\n        val_acc = sum([x[\'n_correct_pred\'] for x in outputs]) / sum(x[\'n_pred\'] for x in outputs)\n        tensorboard_logs = {\'val_loss\': avg_loss, \'val_acc\': val_acc}\n        return {\'val_loss\': avg_loss, \'log\': tensorboard_logs}\n\n    def test_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\'test_loss\'] for x in outputs]).mean()\n        test_acc = sum([x[\'n_correct_pred\'] for x in outputs]) / sum(x[\'n_pred\'] for x in outputs)\n        tensorboard_logs = {\'test_loss\': avg_loss, \'test_acc\': test_acc}\n        return {\'test_loss\': avg_loss, \'log\': tensorboard_logs}\n\n    # ---------------------\n    # TRAINING SETUP\n    # ---------------------\n    def configure_optimizers(self):\n        """"""\n        Return whatever optimizers and learning rate schedulers you want here.\n        At least one optimizer is required.\n        """"""\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n        return [optimizer], [scheduler]\n\n    def prepare_data(self):\n        transform = transforms.Compose([transforms.ToTensor(),\n                                        transforms.Normalize((0.5,), (1.0,))])\n        self.mnist_train = MNIST(self.data_root, train=True, download=True, transform=transform)\n        self.mnist_test = MNIST(self.data_root, train=False, download=True, transform=transform)\n\n    def train_dataloader(self):\n        log.info(\'Training data loader called.\')\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n\n    def val_dataloader(self):\n        log.info(\'Validation data loader called.\')\n        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n\n    def test_dataloader(self):\n        log.info(\'Test data loader called.\')\n        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser, root_dir):  # pragma: no-cover\n        """"""\n        Define parameters that only apply to this model\n        """"""\n        parser = ArgumentParser(parents=[parent_parser])\n\n        # param overwrites\n        # parser.set_defaults(gradient_clip_val=5.0)\n\n        # network params\n        parser.add_argument(\'--in_features\', default=28 * 28, type=int)\n        parser.add_argument(\'--out_features\', default=10, type=int)\n        # use 500 for CPU, 50000 for GPU to see speed difference\n        parser.add_argument(\'--hidden_dim\', default=50000, type=int)\n        parser.add_argument(\'--drop_prob\', default=0.2, type=float)\n        parser.add_argument(\'--learning_rate\', default=0.001, type=float)\n\n        # data\n        parser.add_argument(\'--data_root\', default=os.path.join(root_dir, \'mnist\'), type=str)\n\n        # training params (opt)\n        parser.add_argument(\'--epochs\', default=20, type=int)\n        parser.add_argument(\'--optimizer_name\', default=\'adam\', type=str)\n        parser.add_argument(\'--batch_size\', default=64, type=int)\n        return parser\n'"
pl_examples/models/unet.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    """"""\n    Architecture based on U-Net: Convolutional Networks for Biomedical Image Segmentation\n    Link - https://arxiv.org/abs/1505.04597\n\n    Parameters:\n        num_classes: Number of output classes required (default 19 for KITTI dataset)\n        num_layers: Number of layers in each side of U-net\n        features_start: Number of features in first layer\n        bilinear: Whether to use bilinear interpolation or transposed\n            convolutions for upsampling.\n    """"""\n\n    def __init__(\n            self, num_classes: int = 19,\n            num_layers: int = 5,\n            features_start: int = 64,\n            bilinear: bool = False\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n\n        layers = [DoubleConv(3, features_start)]\n\n        feats = features_start\n        for _ in range(num_layers - 1):\n            layers.append(Down(feats, feats * 2))\n            feats *= 2\n\n        for _ in range(num_layers - 1):\n            layers.append(Up(feats, feats // 2, bilinear))\n            feats //= 2\n\n        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\n\n        self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        xi = [self.layers[0](x)]\n        # Down path\n        for layer in self.layers[1:self.num_layers]:\n            xi.append(layer(xi[-1]))\n        # Up path\n        for i, layer in enumerate(self.layers[self.num_layers:-1]):\n            xi[-1] = layer(xi[-1], xi[-2 - i])\n        return self.layers[-1](xi[-1])\n\n\nclass DoubleConv(nn.Module):\n    """"""\n    Double Convolution and BN and ReLU\n    (3x3 conv -> BN -> ReLU) ** 2\n    """"""\n\n    def __init__(self, in_ch: int, out_ch: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Down(nn.Module):\n    """"""\n    Combination of MaxPool2d and DoubleConv in series\n    """"""\n\n    def __init__(self, in_ch: int, out_ch: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            DoubleConv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Up(nn.Module):\n    """"""\n    Upsampling (by either bilinear interpolation or transpose convolutions)\n    followed by concatenation of feature map from contracting path,\n    followed by double 3x3 convolution.\n    """"""\n\n    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False):\n        super().__init__()\n        self.upsample = None\n        if bilinear:\n            self.upsample = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=""bilinear"", align_corners=True),\n                nn.Conv2d(in_ch, in_ch // 2, kernel_size=1),\n            )\n        else:\n            self.upsample = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n\n        self.conv = DoubleConv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.upsample(x1)\n\n        # Pad x1 to the size of x2\n        diff_h = x2.shape[2] - x1.shape[2]\n        diff_w = x2.shape[3] - x1.shape[3]\n\n        x1 = F.pad(x1, [diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2])\n\n        # Concatenate along the channels axis\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n'"
pytorch_lightning/callbacks/__init__.py,0,"b""from pytorch_lightning.callbacks.base import Callback\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks.gradient_accumulation_scheduler import GradientAccumulationScheduler\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom pytorch_lightning.callbacks.lr_logger import LearningRateLogger\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase, ProgressBar\n\n__all__ = [\n    'Callback',\n    'EarlyStopping',\n    'ModelCheckpoint',\n    'GradientAccumulationScheduler',\n    'LearningRateLogger',\n    'ProgressBarBase',\n    'ProgressBar',\n]\n"""
pytorch_lightning/callbacks/base.py,0,"b'r""""""\nCallback Base\n=============\n\nAbstract base class used to build new callbacks.\n\n""""""\n\nimport abc\n\n\nclass Callback(abc.ABC):\n    r""""""\n    Abstract base class used to build new callbacks.\n    """"""\n\n    def on_init_start(self, trainer):\n        """"""Called when the trainer initialization begins, model has not yet been set.""""""\n        pass\n\n    def on_init_end(self, trainer):\n        """"""Called when the trainer initialization ends, model has not yet been set.""""""\n        pass\n\n    def on_sanity_check_start(self, trainer, pl_module):\n        """"""Called when the validation sanity check starts.""""""\n        pass\n\n    def on_sanity_check_end(self, trainer, pl_module):\n        """"""Called when the validation sanity check ends.""""""\n        pass\n\n    def on_epoch_start(self, trainer, pl_module):\n        """"""Called when the epoch begins.""""""\n        pass\n\n    def on_epoch_end(self, trainer, pl_module):\n        """"""Called when the epoch ends.""""""\n        pass\n\n    def on_batch_start(self, trainer, pl_module):\n        """"""Called when the training batch begins.""""""\n        pass\n\n    def on_validation_batch_start(self, trainer, pl_module):\n        """"""Called when the validation batch begins.""""""\n        pass\n\n    def on_validation_batch_end(self, trainer, pl_module):\n        """"""Called when the validation batch ends.""""""\n        pass\n\n    def on_test_batch_start(self, trainer, pl_module):\n        """"""Called when the test batch begins.""""""\n        pass\n\n    def on_test_batch_end(self, trainer, pl_module):\n        """"""Called when the test batch ends.""""""\n        pass\n\n    def on_batch_end(self, trainer, pl_module):\n        """"""Called when the training batch ends.""""""\n        pass\n\n    def on_train_start(self, trainer, pl_module):\n        """"""Called when the train begins.""""""\n        pass\n\n    def on_train_end(self, trainer, pl_module):\n        """"""Called when the train ends.""""""\n        pass\n\n    def on_validation_start(self, trainer, pl_module):\n        """"""Called when the validation loop begins.""""""\n        pass\n\n    def on_validation_end(self, trainer, pl_module):\n        """"""Called when the validation loop ends.""""""\n        pass\n\n    def on_test_start(self, trainer, pl_module):\n        """"""Called when the test begins.""""""\n        pass\n\n    def on_test_end(self, trainer, pl_module):\n        """"""Called when the test ends.""""""\n        pass\n'"
pytorch_lightning/callbacks/early_stopping.py,7,"b'r""""""\nEarly Stopping\n==============\n\nMonitor a validation metric and stop training when it stops improving.\n\n""""""\n\nimport numpy as np\nimport torch\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.callbacks.base import Callback\nfrom pytorch_lightning.utilities import rank_zero_warn\n\ntorch_inf = torch.tensor(np.Inf)\n\n\nclass EarlyStopping(Callback):\n    r""""""\n\n    Args:\n        monitor: quantity to be monitored. Default: ``\'val_loss\'``.\n        min_delta: minimum change in the monitored quantity\n            to qualify as an improvement, i.e. an absolute\n            change of less than `min_delta`, will count as no\n            improvement. Default: ``0``.\n        patience: number of validation epochs with no improvement\n            after which training will be stopped. Default: ``0``.\n        verbose: verbosity mode. Default: ``False``.\n        mode: one of {auto, min, max}. In `min` mode,\n            training will stop when the quantity\n            monitored has stopped decreasing; in `max`\n            mode it will stop when the quantity\n            monitored has stopped increasing; in `auto`\n            mode, the direction is automatically inferred\n            from the name of the monitored quantity. Default: ``\'auto\'``.\n        strict: whether to crash the training if `monitor` is\n            not found in the validation metrics. Default: ``True``.\n\n    Example::\n\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.callbacks import EarlyStopping\n        >>> early_stopping = EarlyStopping(\'val_loss\')\n        >>> trainer = Trainer(early_stop_callback=early_stopping)\n    """"""\n    mode_dict = {\n        \'min\': torch.lt,\n        \'max\': torch.gt,\n    }\n\n    def __init__(self, monitor: str = \'val_loss\', min_delta: float = 0.0, patience: int = 3,\n                 verbose: bool = False, mode: str = \'auto\', strict: bool = True):\n        super().__init__()\n        self.monitor = monitor\n        self.patience = patience\n        self.verbose = verbose\n        self.strict = strict\n        self.min_delta = min_delta\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.mode = mode\n\n        if mode not in self.mode_dict:\n            if self.verbose > 0:\n                log.info(f\'EarlyStopping mode {mode} is unknown, fallback to auto mode.\')\n            self.mode = \'auto\'\n\n        if self.mode == \'auto\':\n            if self.monitor == \'acc\':\n                self.mode = \'max\'\n            else:\n                self.mode = \'min\'\n            if self.verbose > 0:\n                log.info(f\'EarlyStopping mode set to {self.mode} for monitoring {self.monitor}.\')\n\n        self.min_delta *= 1 if self.monitor_op == torch.gt else -1\n\n    def _validate_condition_metric(self, logs):\n        """"""\n        Checks that the condition metric for early stopping is good\n        :param logs:\n        :return:\n        """"""\n        monitor_val = logs.get(self.monitor)\n        error_msg = (f\'Early stopping conditioned on metric `{self.monitor}`\'\n                     f\' which is not available. Either add `{self.monitor}` to the return of \'\n                     f\' validation_epoch end or modify your EarlyStopping callback to use any of the \'\n                     f\'following: `{""`, `"".join(list(logs.keys()))}`\')\n\n        if monitor_val is None:\n            if self.strict:\n                raise RuntimeError(error_msg)\n            if self.verbose > 0:\n                rank_zero_warn(error_msg, RuntimeWarning)\n\n            return False\n\n        return True\n\n    @property\n    def monitor_op(self):\n        return self.mode_dict[self.mode]\n\n    def on_train_start(self, trainer, pl_module):\n        # Allow instances to be re-used\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.best = torch_inf if self.monitor_op == torch.lt else -torch_inf\n\n    def on_validation_end(self, trainer, pl_module):\n        return self._run_early_stopping_check(trainer, pl_module)\n\n    def _run_early_stopping_check(self, trainer, pl_module):\n        logs = trainer.callback_metrics\n        stop_training = False\n        if not self._validate_condition_metric(logs):\n            return stop_training\n\n        current = logs.get(self.monitor)\n        if not isinstance(current, torch.Tensor):\n            current = torch.tensor(current)\n\n        if self.monitor_op(current - self.min_delta, self.best):\n            self.best = current\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stopped_epoch = trainer.current_epoch\n                stop_training = True\n                self.on_train_end(trainer, pl_module)\n\n        return stop_training\n\n    def on_train_end(self, trainer, pl_module):\n        if self.stopped_epoch > 0 and self.verbose > 0:\n            rank_zero_warn(\'Displayed epoch numbers by `EarlyStopping` start from ""1"" until v0.6.x,\'\n                           \' but will start from ""0"" in v0.8.0.\', DeprecationWarning)\n            log.info(f\'Epoch {self.stopped_epoch + 1:05d}: early stopping\')\n'"
pytorch_lightning/callbacks/gradient_accumulation_scheduler.py,0,"b'r""""""\nGradient Accumulator\n====================\n\nChange gradient accumulation factor according to scheduling.\n\n""""""\n\nfrom pytorch_lightning.callbacks.base import Callback\nfrom pytorch_lightning.utilities import rank_zero_warn\n\n\nclass GradientAccumulationScheduler(Callback):\n    r""""""\n    Change gradient accumulation factor according to scheduling.\n\n    Args:\n        scheduling: scheduling in format {epoch: accumulation_factor}\n\n            .. warning::\n                Epochs indexing starts from ""1"" until v0.6.x,\n                but will start from ""0"" in v0.8.0.\n\n    Example::\n\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.callbacks import GradientAccumulationScheduler\n\n        # at epoch 5 start accumulating every 2 batches\n        >>> accumulator = GradientAccumulationScheduler(scheduling={5: 2})\n        >>> trainer = Trainer(callbacks=[accumulator])\n\n        # alternatively, pass the scheduling dict directly to the Trainer\n        >>> trainer = Trainer(accumulate_grad_batches={5: 2})\n    """"""\n\n    def __init__(self, scheduling: dict):\n        super().__init__()\n\n        if not scheduling:  # empty dict error\n            raise TypeError(""Empty dict cannot be interpreted correct"")\n\n        for key in scheduling:\n            if not isinstance(key, int) or not isinstance(scheduling[key], int):\n                raise TypeError(""All epoches and accumulation factor must be integers"")\n\n        minimal_epoch = min(scheduling.keys())\n        # rank_zero_warn(\'Epochs indexing of `scheduling` starts from ""1"" until v0.6.x,\'\n        #                \' but will start from ""0"" in v0.8.0.\', DeprecationWarning)\n        if minimal_epoch < 1:\n            raise IndexError(f""Epochs indexing from 1, epoch {minimal_epoch} cannot be interpreted correct"")\n        if minimal_epoch != 1:  # if user didnt define first epoch accumulation factor\n            scheduling.update({1: 1})\n\n        self.scheduling = scheduling\n        self.epochs = sorted(scheduling.keys())\n\n    def on_epoch_start(self, trainer, pl_module):\n        # indexing epochs from 1 (until v0.6.x)\n        # In v0.8.0, ` + 1` should be removed.\n        epoch = trainer.current_epoch + 1\n        for i in reversed(range(len(self.epochs))):\n            if epoch >= self.epochs[i]:\n                trainer.accumulate_grad_batches = self.scheduling.get(self.epochs[i])\n                break\n'"
pytorch_lightning/callbacks/lr_logger.py,2,"b'r""""""\n\nLogging of learning rates\n=========================\n\nLog learning rate for lr schedulers during training\n\n""""""\n\nfrom pytorch_lightning.callbacks.base import Callback\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\n\nclass LearningRateLogger(Callback):\n    r""""""\n    Automatically logs learning rate for learning rate schedulers during training.\n\n    Example::\n\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.callbacks import LearningRateLogger\n        >>> lr_logger = LearningRateLogger()\n        >>> trainer = Trainer(callbacks=[lr_logger])\n\n    Logging names are automatically determined based on optimizer class name.\n    In case of multiple optimizers of same type, they will be named `Adam`,\n    `Adam-1` etc. If a optimizer has multiple parameter groups they will\n    be named `Adam/pg1`, `Adam/pg2` etc. To control naming, pass in a\n    `name` keyword in the construction of the learning rate schdulers\n\n    Example::\n\n        def configure_optimizer(self):\n            optimizer = torch.optim.Adam(...)\n            lr_scheduler = {\'scheduler\': torch.optim.lr_schedulers.LambdaLR(optimizer, ...)\n                            \'name\': \'my_logging_name\'}\n            return [optimizer], [lr_scheduler]\n    """"""\n    def __init__(self):\n        self.lrs = None\n        self.lr_sch_names = []\n\n    def on_train_start(self, trainer, pl_module):\n        """""" Called before training, determines unique names for all lr\n            schedulers in the case of multiple of the same type or in\n            the case of multiple parameter groups\n        """"""\n        if not trainer.logger:\n            raise MisconfigurationException(\n                \'Cannot use LearningRateLogger callback with Trainer that has no logger.\')\n\n        if not trainer.lr_schedulers:\n            rank_zero_warn(\n                \'You are using LearningRateLogger callback with models that\'\n                \' have no learning rate schedulers. Please see documentation\'\n                \' for `configure_optimizers` method.\', RuntimeWarning\n            )\n\n        # Find names for schedulers\n        names = self._find_names(trainer.lr_schedulers)\n\n        # Initialize for storing values\n        self.lrs = {name: [] for name in names}\n\n    def on_batch_start(self, trainer, pl_module):\n        latest_stat = self._extract_lr(trainer, \'step\')\n        if trainer.logger and latest_stat:\n            trainer.logger.log_metrics(latest_stat, step=trainer.global_step)\n\n    def on_epoch_start(self, trainer, pl_module):\n        latest_stat = self._extract_lr(trainer, \'epoch\')\n        if trainer.logger and latest_stat:\n            trainer.logger.log_metrics(latest_stat, step=trainer.global_step)\n\n    def _extract_lr(self, trainer, interval):\n        """""" Extracts learning rates for lr schedulers and saves information\n            into dict structure. """"""\n        latest_stat = {}\n        for name, scheduler in zip(self.lr_sch_names, trainer.lr_schedulers):\n            if scheduler[\'interval\'] == interval:\n                param_groups = scheduler[\'scheduler\'].optimizer.param_groups\n                if len(param_groups) != 1:\n                    for i, pg in enumerate(param_groups):\n                        lr, key = pg[\'lr\'], f\'{name}/pg{i + 1}\'\n                        self.lrs[key].append(lr)\n                        latest_stat[key] = lr\n                else:\n                    self.lrs[name].append(param_groups[0][\'lr\'])\n                    latest_stat[name] = param_groups[0][\'lr\']\n        return latest_stat\n\n    def _find_names(self, lr_schedulers):\n        # Create uniqe names in the case we have multiple of the same learning\n        # rate schduler + multiple parameter groups\n        names = []\n        for scheduler in lr_schedulers:\n            sch = scheduler[\'scheduler\']\n            if \'name\' in scheduler:\n                name = scheduler[\'name\']\n            else:\n                opt_name = \'lr-\' + sch.optimizer.__class__.__name__\n                i, name = 1, opt_name\n                # Multiple schduler of the same type\n                while True:\n                    if name not in names:\n                        break\n                    i, name = i + 1, f\'{opt_name}-{i}\'\n\n            # Multiple param groups for the same schduler\n            param_groups = sch.optimizer.param_groups\n            if len(param_groups) != 1:\n                for i, pg in enumerate(param_groups):\n                    temp = f\'{name}/pg{i + 1}\'\n                    names.append(temp)\n            else:\n                names.append(name)\n\n            self.lr_sch_names.append(name)\n        return names\n'"
pytorch_lightning/callbacks/model_checkpoint.py,7,"b'""""""\nModel Checkpointing\n===================\n\nAutomatically save model checkpoints during training.\n\n""""""\n\nimport os\nimport re\n\nimport numpy as np\nfrom typing import Optional\n\nimport torch\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.callbacks.base import Callback\nfrom pytorch_lightning.utilities import rank_zero_warn, rank_zero_only\n\n\nclass ModelCheckpoint(Callback):\n    r""""""\n    Save the model after every epoch if it improves.\n\n    After training finishes, use :attr:`best_model_path` to retrieve the path to the\n    best checkpoint file and :attr:`best_model_score` to retrieve its score.\n\n    Args:\n        filepath: path to save the model file.\n            Can contain named formatting options to be auto-filled.\n\n            Example::\n\n                # custom path\n                # saves a file like: my/path/epoch_0.ckpt\n                >>> checkpoint_callback = ModelCheckpoint(\'my/path/\')\n\n                # save any arbitrary metrics like `val_loss`, etc. in name\n                # saves a file like: my/path/epoch=2-val_loss=0.2_other_metric=0.3.ckpt\n                >>> checkpoint_callback = ModelCheckpoint(\n                ...     filepath=\'my/path/{epoch}-{val_loss:.2f}-{other_metric:.2f}\'\n                ... )\n\n            Can also be set to `None`, then it will be set to default location\n            during trainer construction.\n\n        monitor: quantity to monitor.\n        verbose: verbosity mode. Default: ``False``.\n        save_last: always saves the model at the end of the epoch. Default: ``False``.\n        save_top_k: if `save_top_k == k`,\n            the best k models according to\n            the quantity monitored will be saved.\n            if ``save_top_k == 0``, no models are saved.\n            if ``save_top_k == -1``, all models are saved.\n            Please note that the monitors are checked every `period` epochs.\n            if ``save_top_k >= 2`` and the callback is called multiple\n            times inside an epoch, the name of the saved file will be\n            appended with a version count starting with `v0`.\n        mode: one of {auto, min, max}.\n            If ``save_top_k != 0``, the decision\n            to overwrite the current save file is made\n            based on either the maximization or the\n            minimization of the monitored quantity. For `val_acc`,\n            this should be `max`, for `val_loss` this should\n            be `min`, etc. In `auto` mode, the direction is\n            automatically inferred from the name of the monitored quantity.\n        save_weights_only: if ``True``, then only the model\'s weights will be\n            saved (``model.save_weights(filepath)``), else the full model\n            is saved (``model.save(filepath)``).\n        period: Interval (number of epochs) between checkpoints.\n\n    Example::\n\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.callbacks import ModelCheckpoint\n\n        # saves checkpoints to \'my/path/\' whenever \'val_loss\' has a new min\n        >>> checkpoint_callback = ModelCheckpoint(filepath=\'my/path/\')\n        >>> trainer = Trainer(checkpoint_callback=checkpoint_callback)\n\n        # save epoch and val_loss in name\n        # saves a file like: my/path/sample-mnist_epoch=02_val_loss=0.32.ckpt\n        >>> checkpoint_callback = ModelCheckpoint(\n        ...     filepath=\'my/path/sample-mnist_{epoch:02d}-{val_loss:.2f}\'\n        ... )\n\n        # retrieve the best checkpoint after training\n        checkpoint_callback = ModelCheckpoint(filepath=\'my/path/\')\n        trainer = Trainer(checkpoint_callback=checkpoint_callback)\n        model = ...\n        trainer.fit(model)\n        checkpoint_callback.best_model_path\n\n    """"""\n\n    def __init__(self, filepath: Optional[str] = None, monitor: str = \'val_loss\', verbose: bool = False,\n                 save_last: bool = False, save_top_k: int = 1, save_weights_only: bool = False,\n                 mode: str = \'auto\', period: int = 1, prefix: str = \'\'):\n        super().__init__()\n        if save_top_k > 0 and filepath is not None and os.path.isdir(filepath) and len(os.listdir(filepath)) > 0:\n            rank_zero_warn(\n                f""Checkpoint directory {filepath} exists and is not empty with save_top_k != 0.""\n                ""All files in this directory will be deleted when a checkpoint is saved!""\n            )\n        self._rank = 0\n\n        self.monitor = monitor\n        self.verbose = verbose\n        if filepath is None:  # will be determined by trainer at runtime\n            self.dirpath, self.filename = None, None\n        else:\n            if os.path.isdir(filepath):\n                self.dirpath, self.filename = filepath, \'{epoch}\'\n            else:\n                self.dirpath, self.filename = os.path.split(filepath)\n            os.makedirs(self.dirpath, exist_ok=True)\n        self.save_last = save_last\n        self.save_top_k = save_top_k\n        self.save_weights_only = save_weights_only\n        self.period = period\n        self.epoch_last_check = None\n        self.prefix = prefix\n        self.best_k_models = {}\n        # {filename: monitor}\n        self.kth_best_model_path = \'\'\n        self.best_model_score = 0\n        self.best_model_path = \'\'\n        self.save_function = None\n\n        torch_inf = torch.tensor(np.Inf)\n        mode_dict = {\n            \'min\': (torch_inf, \'min\'),\n            \'max\': (-torch_inf, \'max\'),\n            \'auto\': (-torch_inf, \'max\') if \'acc\' in self.monitor or self.monitor.startswith(\'fmeasure\')\n            else (torch_inf, \'min\'),\n        }\n\n        if mode not in mode_dict:\n            rank_zero_warn(f\'ModelCheckpoint mode {mode} is unknown, \'\n                           f\'fallback to auto mode.\', RuntimeWarning)\n            mode = \'auto\'\n\n        self.kth_value, self.mode = mode_dict[mode]\n\n    @property\n    def best(self):\n        rank_zero_warn(""Attribute `best` has been renamed to `best_model_score` since v0.8.0""\n                       "" and will be removed in v0.10.0"", DeprecationWarning)\n        return self.best_model_score\n\n    @property\n    def kth_best_model(self):\n        rank_zero_warn(""Attribute `kth_best_model` has been renamed to `kth_best_model_path` since v0.8.0""\n                       "" and will be removed in v0.10.0"", DeprecationWarning)\n        return self.kth_best_model_path\n\n    def _del_model(self, filepath):\n        if os.path.isfile(filepath):\n            os.remove(filepath)\n\n    def _save_model(self, filepath):\n        # make paths\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n        # delegate the saving to the model\n        if self.save_function is not None:\n            self.save_function(filepath, self.save_weights_only)\n        else:\n            raise ValueError("".save_function() not set"")\n\n    def check_monitor_top_k(self, current):\n        less_than_k_models = len(self.best_k_models) < self.save_top_k\n        if less_than_k_models:\n            return True\n\n        if not isinstance(current, torch.Tensor):\n            rank_zero_warn(\n                f\'{current} is supposed to be a torch.Tensor. Saving checkpoint may not work correctly. \'\n                f\'HINT: check the value of {self.monitor} in your validation loop\', RuntimeWarning\n            )\n            current = torch.tensor(current)\n\n        monitor_op = {\n            ""min"": torch.lt,\n            ""max"": torch.gt,\n        }[self.mode]\n\n        return monitor_op(current, self.best_k_models[self.kth_best_model_path])\n\n    def format_checkpoint_name(self, epoch, metrics, ver=None):\n        """"""Generate a filename according to the defined template.\n\n        Example::\n\n            >>> tmpdir = os.path.dirname(__file__)\n            >>> ckpt = ModelCheckpoint(os.path.join(tmpdir, \'{epoch}\'))\n            >>> os.path.basename(ckpt.format_checkpoint_name(0, {}))\n            \'epoch=0.ckpt\'\n            >>> ckpt = ModelCheckpoint(os.path.join(tmpdir, \'{epoch:03d}\'))\n            >>> os.path.basename(ckpt.format_checkpoint_name(5, {}))\n            \'epoch=005.ckpt\'\n            >>> ckpt = ModelCheckpoint(os.path.join(tmpdir, \'{epoch}-{val_loss:.2f}\'))\n            >>> os.path.basename(ckpt.format_checkpoint_name(2, dict(val_loss=0.123456)))\n            \'epoch=2-val_loss=0.12.ckpt\'\n            >>> ckpt = ModelCheckpoint(os.path.join(tmpdir, \'{missing:d}\'))\n            >>> os.path.basename(ckpt.format_checkpoint_name(0, {}))\n            \'missing=0.ckpt\'\n        """"""\n        # check if user passed in keys to the string\n        groups = re.findall(r\'(\\{.*?)[:\\}]\', self.filename)\n\n        if len(groups) == 0:\n            # default name\n            filename = f\'{self.prefix}_ckpt_epoch_{epoch}\'\n        else:\n            metrics[\'epoch\'] = epoch\n            filename = self.filename\n            for tmp in groups:\n                name = tmp[1:]\n                filename = filename.replace(tmp, name + \'={\' + name)\n                if name not in metrics:\n                    metrics[name] = 0\n            filename = filename.format(**metrics)\n        str_ver = f\'_v{ver}\' if ver is not None else \'\'\n        filepath = os.path.join(self.dirpath, self.prefix + filename + str_ver + \'.ckpt\')\n        return filepath\n\n    @rank_zero_only\n    def on_validation_end(self, trainer, pl_module):\n        # only run on main process\n        if trainer.proc_rank != 0:\n            return\n\n        metrics = trainer.callback_metrics\n        epoch = trainer.current_epoch\n        if self.save_top_k == 0:\n            # no models are saved\n            return\n        if self.epoch_last_check is not None and (epoch - self.epoch_last_check) < self.period:\n            # skipping in this term\n            return\n\n        self.epoch_last_check = epoch\n\n        if self.save_last:\n            filepath = os.path.join(self.dirpath, self.prefix + \'last.ckpt\')\n            self._save_model(filepath)\n\n        filepath = self.format_checkpoint_name(epoch, metrics)\n        version_cnt = 0\n        while os.path.isfile(filepath):\n            filepath = self.format_checkpoint_name(epoch, metrics, ver=version_cnt)\n            # this epoch called before\n            version_cnt += 1\n\n        if self.save_top_k != -1:\n            current = metrics.get(self.monitor)\n\n            if not isinstance(current, torch.Tensor):\n                rank_zero_warn(\n                    f\'The metric you returned {current} must be a Torch.Tensor instance, checkpoint not saved \'\n                    f\'HINT: what is the value of {self.monitor} in validation_end()?\', RuntimeWarning\n                )\n\n            if current is None:\n                rank_zero_warn(\n                    f\'Can save best model only with {self.monitor} available, skipping.\', RuntimeWarning\n                )\n            elif self.check_monitor_top_k(current):\n                self._do_check_save(filepath, current, epoch)\n            elif self.verbose > 0:\n                log.info(f\'\\nEpoch {epoch:05d}: {self.monitor}  was not in top {self.save_top_k}\')\n\n        else:\n            if self.verbose > 0:\n                log.info(f\'\\nEpoch {epoch:05d}: saving model to {filepath}\')\n            self._save_model(filepath)\n\n    def _do_check_save(self, filepath, current, epoch):\n        # remove kth\n\n        del_list = []\n        if len(self.best_k_models) == self.save_top_k and self.save_top_k > 0:\n            delpath = self.kth_best_model_path\n            self.best_k_models.pop(self.kth_best_model_path)\n            del_list.append(delpath)\n\n        self.best_k_models[filepath] = current\n        if len(self.best_k_models) == self.save_top_k:\n            # monitor dict has reached k elements\n            _op = max if self.mode == \'min\' else min\n            self.kth_best_model_path = _op(self.best_k_models,\n                                           key=self.best_k_models.get)\n            self.kth_value = self.best_k_models[self.kth_best_model_path]\n\n        _op = min if self.mode == \'min\' else max\n        self.best_model_path = _op(self.best_k_models, key=self.best_k_models.get)\n        self.best_model_score = self.best_k_models[self.best_model_path]\n\n        if self.verbose > 0:\n            log.info(\n                f\'\\nEpoch {epoch:05d}: {self.monitor} reached\'\n                f\' {current:0.5f} (best {self.best_model_score:0.5f}), saving model to\'\n                f\' {filepath} as top {self.save_top_k}\')\n        self._save_model(filepath)\n\n        for cur_path in del_list:\n            if cur_path != filepath:\n                self._del_model(cur_path)\n'"
pytorch_lightning/callbacks/progress.py,0,"b'""""""\nProgress Bars\n=============\n\nUse or override one of the progress bar callbacks.\n\n""""""\nimport sys\n\nfrom tqdm.auto import tqdm\n\nfrom pytorch_lightning.callbacks import Callback\n\n\nclass ProgressBarBase(Callback):\n    r""""""\n    The base class for progress bars in Lightning. It is a :class:`~pytorch_lightning.callbacks.Callback`\n    that keeps track of the batch progress in the :class:`~pytorch_lightning.trainer.trainer.Trainer`.\n    You should implement your highly custom progress bars with this as the base class.\n\n    Example::\n\n        class LitProgressBar(ProgressBarBase):\n\n            def __init__(self):\n                super().__init__()  # don\'t forget this :)\n                self.enable = True\n\n            def disable(self):\n                self.enable = False\n\n            def on_batch_end(self, trainer, pl_module):\n                super().on_batch_end(trainer, pl_module)  # don\'t forget this :)\n                percent = (self.train_batch_idx / self.total_train_batches) * 100\n                sys.stdout.flush()\n                sys.stdout.write(f\'{percent:.01f} percent complete \\r\')\n\n        bar = LitProgressBar()\n        trainer = Trainer(callbacks=[bar])\n\n    """"""\n    def __init__(self):\n\n        self._trainer = None\n        self._train_batch_idx = 0\n        self._val_batch_idx = 0\n        self._test_batch_idx = 0\n\n    @property\n    def trainer(self):\n        return self._trainer\n\n    @property\n    def train_batch_idx(self) -> int:\n        """"""\n        The current batch index being processed during training.\n        Use this to update your progress bar.\n        """"""\n        return self._train_batch_idx\n\n    @property\n    def val_batch_idx(self) -> int:\n        """"""\n        The current batch index being processed during validation.\n        Use this to update your progress bar.\n        """"""\n        return self._val_batch_idx\n\n    @property\n    def test_batch_idx(self) -> int:\n        """"""\n        The current batch index being processed during testing.\n        Use this to update your progress bar.\n        """"""\n        return self._test_batch_idx\n\n    @property\n    def total_train_batches(self) -> int:\n        """"""\n        The total number of training batches during training, which may change from epoch to epoch.\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the\n        training dataloader is of infinite size.\n        """"""\n        total_train_batches = 1 if self.trainer.fast_dev_run else self.trainer.num_training_batches\n        return total_train_batches\n\n    @property\n    def total_val_batches(self) -> int:\n        """"""\n        The total number of training batches during validation, which may change from epoch to epoch.\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the\n        validation dataloader is of infinite size.\n        """"""\n        trainer = self.trainer\n        total_val_batches = 0\n        if trainer.fast_dev_run and trainer.val_dataloaders is not None:\n            total_val_batches = len(trainer.val_dataloaders)\n        elif not self.trainer.disable_validation:\n            is_val_epoch = (trainer.current_epoch + 1) % trainer.check_val_every_n_epoch == 0\n            total_val_batches = trainer.num_val_batches if is_val_epoch else 0\n        return total_val_batches\n\n    @property\n    def total_test_batches(self) -> int:\n        """"""\n        The total number of training batches during testing, which may change from epoch to epoch.\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the\n        test dataloader is of infinite size.\n        """"""\n        if self.trainer.fast_dev_run:\n            total_test_batches = len(self.trainer.test_dataloaders)\n        else:\n            total_test_batches = self.trainer.num_test_batches\n        return total_test_batches\n\n    def disable(self):\n        """"""\n        You should provide a way to disable the progress bar.\n        The :class:`~pytorch_lightning.trainer.trainer.Trainer` will call this to disable the\n        output on processes that have a rank different from 0, e.g., in multi-node training.\n        """"""\n        raise NotImplementedError\n\n    def enable(self):\n        """"""\n        You should provide a way to enable the progress bar.\n        The :class:`~pytorch_lightning.trainer.trainer.Trainer` will call this in e.g. pre-training\n        routines like the `learning rate finder <lr_finder.rst>`_ to temporarily enable and\n        disable the main progress bar.\n        """"""\n        raise NotImplementedError\n\n    def on_init_end(self, trainer):\n        self._trainer = trainer\n\n    def on_train_start(self, trainer, pl_module):\n        self._train_batch_idx = trainer.batch_idx\n\n    def on_epoch_start(self, trainer, pl_module):\n        self._train_batch_idx = 0\n\n    def on_batch_end(self, trainer, pl_module):\n        self._train_batch_idx += 1\n\n    def on_validation_start(self, trainer, pl_module):\n        self._val_batch_idx = 0\n\n    def on_validation_batch_end(self, trainer, pl_module):\n        self._val_batch_idx += 1\n\n    def on_test_start(self, trainer, pl_module):\n        self._test_batch_idx = 0\n\n    def on_test_batch_end(self, trainer, pl_module):\n        self._test_batch_idx += 1\n\n\nclass ProgressBar(ProgressBarBase):\n    r""""""\n    This is the default progress bar used by Lightning. It prints to `stdout` using the\n    :mod:`tqdm` package and shows up to four different bars:\n\n    - **sanity check progress:** the progress during the sanity check run\n    - **main progress:** shows training + validation progress combined. It also accounts for\n      multiple validation runs during training when\n      :paramref:`~pytorch_lightning.trainer.trainer.Trainer.val_check_interval` is used.\n    - **validation progress:** only visible during validation;\n      shows total progress over all validation datasets.\n    - **test progress:** only active when testing; shows total progress over all test datasets.\n\n    For infinite datasets, the progress bar never ends.\n\n    If you want to customize the default ``tqdm`` progress bars used by Lightning, you can override\n    specific methods of the callback class and pass your custom implementation to the\n    :class:`~pytorch_lightning.trainer.trainer.Trainer`:\n\n    Example::\n\n        class LitProgressBar(ProgressBar):\n\n            def init_validation_tqdm(self):\n                bar = super().init_validation_tqdm()\n                bar.set_description(\'running validation ...\')\n                return bar\n\n        bar = LitProgressBar()\n        trainer = Trainer(callbacks=[bar])\n\n    Args:\n        refresh_rate:\n            Determines at which rate (in number of batches) the progress bars get updated.\n            Set it to ``0`` to disable the display. By default, the\n            :class:`~pytorch_lightning.trainer.trainer.Trainer` uses this implementation of the progress\n            bar and sets the refresh rate to the value provided to the\n            :paramref:`~pytorch_lightning.trainer.trainer.Trainer.progress_bar_refresh_rate` argument in the\n            :class:`~pytorch_lightning.trainer.trainer.Trainer`.\n        process_position:\n            Set this to a value greater than ``0`` to offset the progress bars by this many lines.\n            This is useful when you have progress bars defined elsewhere and want to show all of them\n            together. This corresponds to\n            :paramref:`~pytorch_lightning.trainer.trainer.Trainer.process_position` in the\n            :class:`~pytorch_lightning.trainer.trainer.Trainer`.\n\n    """"""\n    def __init__(self, refresh_rate: int = 1, process_position: int = 0):\n        super().__init__()\n        self._refresh_rate = refresh_rate\n        self._process_position = process_position\n        self._enabled = True\n        self.main_progress_bar = None\n        self.val_progress_bar = None\n        self.test_progress_bar = None\n\n    def __getstate__(self):\n        # can\'t pickle the tqdm objects\n        state = self.__dict__.copy()\n        state[\'main_progress_bar\'] = None\n        state[\'val_progress_bar\'] = None\n        state[\'test_progress_bar\'] = None\n        return state\n\n    @property\n    def refresh_rate(self) -> int:\n        return self._refresh_rate\n\n    @property\n    def process_position(self) -> int:\n        return self._process_position\n\n    @property\n    def is_enabled(self) -> bool:\n        return self._enabled and self.refresh_rate > 0\n\n    @property\n    def is_disabled(self) -> bool:\n        return not self.is_enabled\n\n    def disable(self) -> None:\n        self._enabled = False\n\n    def enable(self) -> None:\n        self._enabled = True\n\n    def init_sanity_tqdm(self) -> tqdm:\n        """""" Override this to customize the tqdm bar for the validation sanity run. """"""\n        bar = tqdm(\n            desc=\'Validation sanity check\',\n            position=(2 * self.process_position),\n            disable=self.is_disabled,\n            leave=False,\n            dynamic_ncols=True,\n            file=sys.stdout,\n        )\n        return bar\n\n    def init_train_tqdm(self) -> tqdm:\n        """""" Override this to customize the tqdm bar for training. """"""\n        bar = tqdm(\n            desc=\'Training\',\n            initial=self.train_batch_idx,\n            position=(2 * self.process_position),\n            disable=self.is_disabled,\n            leave=True,\n            dynamic_ncols=True,\n            file=sys.stdout,\n            smoothing=0,\n        )\n        return bar\n\n    def init_validation_tqdm(self) -> tqdm:\n        """""" Override this to customize the tqdm bar for validation. """"""\n        bar = tqdm(\n            desc=\'Validating\',\n            position=(2 * self.process_position + 1),\n            disable=self.is_disabled,\n            leave=False,\n            dynamic_ncols=True,\n            file=sys.stdout\n        )\n        return bar\n\n    def init_test_tqdm(self) -> tqdm:\n        """""" Override this to customize the tqdm bar for testing. """"""\n        bar = tqdm(\n            desc=\'Testing\',\n            position=(2 * self.process_position),\n            disable=self.is_disabled,\n            leave=True,\n            dynamic_ncols=True,\n            file=sys.stdout\n        )\n        return bar\n\n    def on_sanity_check_start(self, trainer, pl_module):\n        super().on_sanity_check_start(trainer, pl_module)\n        self.val_progress_bar = self.init_sanity_tqdm()\n        self.val_progress_bar.total = trainer.num_sanity_val_steps * len(trainer.val_dataloaders)\n        self.main_progress_bar = tqdm(disable=True)  # dummy progress bar\n\n    def on_sanity_check_end(self, trainer, pl_module):\n        super().on_sanity_check_end(trainer, pl_module)\n        self.main_progress_bar.close()\n        self.val_progress_bar.close()\n\n    def on_train_start(self, trainer, pl_module):\n        super().on_train_start(trainer, pl_module)\n        self.main_progress_bar = self.init_train_tqdm()\n\n    def on_epoch_start(self, trainer, pl_module):\n        super().on_epoch_start(trainer, pl_module)\n        total_train_batches = self.total_train_batches\n        total_val_batches = self.total_val_batches\n        if total_train_batches != float(\'inf\') and not trainer.fast_dev_run:\n            # val can be checked multiple times per epoch\n            val_checks_per_epoch = total_train_batches // trainer.val_check_batch\n            total_val_batches = total_val_batches * val_checks_per_epoch\n        total_batches = total_train_batches + total_val_batches\n        if not self.main_progress_bar.disable:\n            self.main_progress_bar.reset(convert_inf(total_batches))\n        self.main_progress_bar.set_description(f\'Epoch {trainer.current_epoch + 1}\')\n\n    def on_batch_end(self, trainer, pl_module):\n        super().on_batch_end(trainer, pl_module)\n        if self.is_enabled and self.train_batch_idx % self.refresh_rate == 0:\n            self.main_progress_bar.update(self.refresh_rate)\n            self.main_progress_bar.set_postfix(trainer.progress_bar_dict)\n\n    def on_validation_start(self, trainer, pl_module):\n        super().on_validation_start(trainer, pl_module)\n        self.val_progress_bar = self.init_validation_tqdm()\n        self.val_progress_bar.total = convert_inf(self.total_val_batches)\n\n    def on_validation_batch_end(self, trainer, pl_module):\n        super().on_validation_batch_end(trainer, pl_module)\n        if self.is_enabled and self.val_batch_idx % self.refresh_rate == 0:\n            self.val_progress_bar.update(self.refresh_rate)\n            self.main_progress_bar.update(self.refresh_rate)\n\n    def on_validation_end(self, trainer, pl_module):\n        super().on_validation_end(trainer, pl_module)\n        self.main_progress_bar.set_postfix(trainer.progress_bar_dict)\n        self.val_progress_bar.close()\n\n    def on_train_end(self, trainer, pl_module):\n        super().on_train_end(trainer, pl_module)\n        self.main_progress_bar.close()\n\n    def on_test_start(self, trainer, pl_module):\n        super().on_test_start(trainer, pl_module)\n        self.test_progress_bar = self.init_test_tqdm()\n        self.test_progress_bar.total = convert_inf(self.total_test_batches)\n\n    def on_test_batch_end(self, trainer, pl_module):\n        super().on_test_batch_end(trainer, pl_module)\n        if self.is_enabled and self.test_batch_idx % self.refresh_rate == 0:\n            self.test_progress_bar.update(self.refresh_rate)\n\n    def on_test_end(self, trainer, pl_module):\n        super().on_test_end(trainer, pl_module)\n        self.test_progress_bar.close()\n\n\ndef convert_inf(x):\n    """""" The tqdm doesn\'t support inf values. We have to convert it to None. """"""\n    if x == float(\'inf\'):\n        return None\n    return x\n'"
pytorch_lightning/core/__init__.py,11,"b'""""""\nA :class:`~LightningModule` organizes your PyTorch code into the following sections:\n\n.. figure:: /_images/lightning_module/pt_to_pl.png\n   :alt: Convert from PyTorch to Lightning\n\n\nNotice a few things.\n\n1.  It\'s the SAME code.\n2.  The PyTorch code IS NOT abstracted - just organized.\n3.  All the other code that\'s not in the :class:`~LightningModule`\n    has been automated for you by the trainer.\n\n    .. code-block:: python\n\n        net = Net()\n        trainer = Trainer()\n        trainer.fit(net)\n\n4.  There are no .cuda() or .to() calls... Lightning does these for you.\n\n    .. code-block:: python\n\n        # don\'t do in lightning\n        x = torch.Tensor(2, 3)\n        x = x.cuda()\n        x = x.to(device)\n\n        # do this instead\n        x = x  # leave it alone!\n\n        # or to init a new tensor\n        new_x = torch.Tensor(2, 3)\n        new_x = new_x.type_as(x.type())\n\n5.  There are no samplers for distributed, Lightning also does this for you.\n\n    .. code-block:: python\n\n        # Don\'t do in Lightning...\n        data = MNIST(...)\n        sampler = DistributedSampler(data)\n        DataLoader(data, sampler=sampler)\n\n        # do this instead\n        data = MNIST(...)\n        DataLoader(data)\n\n6.  A :class:`~LightningModule` is a :class:`torch.nn.Module` but with added functionality. Use it as such!\n\n    .. code-block:: python\n\n        net = Net.load_from_checkpoint(PATH)\n        net.freeze()\n        out = net(x)\n\nThus, to use Lightning, you just need to organize your code which takes about 30 minutes,\n(and let\'s be real, you probably should do anyhow).\n\n------------\n\nMinimal Example\n---------------\n\nHere are the only required methods.\n\n.. code-block:: python\n\n    >>> import pytorch_lightning as pl\n    >>> class LitModel(pl.LightningModule):\n    ...\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self.l1 = torch.nn.Linear(28 * 28, 10)\n    ...\n    ...     def forward(self, x):\n    ...         return torch.relu(self.l1(x.view(x.size(0), -1)))\n    ...\n    ...     def training_step(self, batch, batch_idx):\n    ...         x, y = batch\n    ...         y_hat = self(x)\n    ...         return {\'loss\': F.cross_entropy(y_hat, y)}\n    ...\n    ...     def train_dataloader(self):\n    ...         return DataLoader(MNIST(os.getcwd(), train=True, download=True,\n    ...                                 transform=transforms.ToTensor()), batch_size=32)\n    ...\n    ...     def configure_optimizers(self):\n    ...         return torch.optim.Adam(self.parameters(), lr=0.02)\n\nWhich you can train by doing:\n\n.. code-block:: python\n\n   trainer = pl.Trainer()\n   model = LitModel()\n\n   trainer.fit(model)\n\n----------\n\nTraining loop structure\n-----------------------\n\nThe general pattern is that each loop (training, validation, test loop)\nhas 3 methods:\n\n- ``___step``\n- ``___step_end``\n- ``___epoch_end``\n\nTo show how Lightning calls these, let\'s use the validation loop as an example:\n\n.. code-block:: python\n\n    val_outs = []\n    for val_batch in val_data:\n        # do something with each batch\n        out = validation_step(val_batch)\n        val_outs.append(out)\n\n    # do something with the outputs for all batches\n    # like calculate validation set accuracy or loss\n    validation_epoch_end(val_outs)\n\nIf we use dp or ddp2 mode, we can also define the ``XXX_step_end`` method to operate\non all parts of the batch::\n\n    val_outs = []\n    for val_batch in val_data:\n        batches = split_batch(val_batch)\n        dp_outs = []\n        for sub_batch in batches:\n            dp_out = validation_step(sub_batch)\n            dp_outs.append(dp_out)\n\n        out = validation_step_end(dp_outs)\n        val_outs.append(out)\n\n    # do something with the outputs for all batches\n    # like calculate validation set accuracy or loss\n    validation_epoch_end(val_outs)\n\n\nAdd validation loop\n^^^^^^^^^^^^^^^^^^^\n\nThus, if we wanted to add a validation loop you would add this to your\n:class:`~LightningModule`:\n\n    >>> class LitModel(pl.LightningModule):\n    ...     def validation_step(self, batch, batch_idx):\n    ...         x, y = batch\n    ...         y_hat = self(x)\n    ...         return {\'val_loss\': F.cross_entropy(y_hat, y)}\n    ...\n    ...     def validation_epoch_end(self, outputs):\n    ...         val_loss_mean = torch.stack([x[\'val_loss\'] for x in outputs]).mean()\n    ...         return {\'val_loss\': val_loss_mean}\n    ...\n    ...     def val_dataloader(self):\n    ...         # can also return a list of val dataloaders\n    ...         return DataLoader(...)\n\nAdd test loop\n^^^^^^^^^^^^^\n\n    >>> class LitModel(pl.LightningModule):\n    ...     def test_step(self, batch, batch_idx):\n    ...         x, y = batch\n    ...         y_hat = self(x)\n    ...         return {\'test_loss\': F.cross_entropy(y_hat, y)}\n    ...\n    ...     def test_epoch_end(self, outputs):\n    ...         test_loss_mean = torch.stack([x[\'test_loss\'] for x in outputs]).mean()\n    ...         return {\'test_loss\': test_loss_mean}\n    ...\n    ...     def test_dataloader(self):\n    ...         # can also return a list of test dataloaders\n    ...         return DataLoader(...)\n\nHowever, the test loop won\'t ever be called automatically to make sure you\ndon\'t run your test data by accident. Instead you have to explicitly call:\n\n.. code-block:: python\n\n    # call after training\n    trainer = Trainer()\n    trainer.fit(model)\n    trainer.test()\n\n    # or call with pretrained model\n    model = MyLightningModule.load_from_checkpoint(PATH)\n    trainer = Trainer()\n    trainer.test(model)\n\n----------\n\nTraining_step_end method\n------------------------\nWhen using :class:`~pytorch_lightning.overrides.data_parallel.LightningDataParallel` or\n:class:`~pytorch_lightning.overrides.data_parallel.LightningDistributedDataParallel`, the\n:meth:`~LightningModule.training_step`\nwill be operating on a portion of the batch. This is normally ok but in special\ncases like calculating NCE loss using negative samples, we might want to\nperform a softmax across all samples in the batch.\n\nFor these types of situations, each loop has an additional ``__step_end`` method\nwhich allows you to operate on the pieces of the batch:\n\n.. code-block:: python\n\n    training_outs = []\n    for train_batch in train_data:\n        # dp, ddp2 splits the batch\n        sub_batches = split_batches_for_dp(batch)\n\n        # run training_step on each piece of the batch\n        batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]\n\n        # do softmax with all pieces\n        out = training_step_end(batch_parts_outputs)\n        training_outs.append(out)\n\n    # do something with the outputs for all batches\n    # like calculate validation set accuracy or loss\n    training_epoch_end(val_outs)\n\n----------\n\nRemove cuda calls\n-----------------\nIn a :class:`~LightningModule`, all calls to ``.cuda()``\nand ``.to(device)`` should be removed. Lightning will do these\nautomatically. This will allow your code to work on CPUs, TPUs and GPUs.\n\nWhen you init a new tensor in your code, just use :meth:`~torch.Tensor.type_as`:\n\n.. code-block:: python\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n\n        # put the z on the appropriate gpu or tpu core\n        z = sample_noise()\n        z = z.type_as(x)\n\n----------\n\nData preparation\n----------------\nData preparation in PyTorch follows 5 steps:\n\n1. Download\n2. Clean and (maybe) save to disk\n3. Load inside :class:`~torch.utils.data.Dataset`\n4. Apply transforms (rotate, tokenize, etc...)\n5. Wrap inside a :class:`~torch.utils.data.DataLoader`\n\nWhen working in distributed settings, steps 1 and 2 have to be done\nfrom a single GPU, otherwise you will overwrite these files from\nevery GPU. The :class:`~LightningModule` has the\n:class:`~LightningModule.prepare_data` method to\nallow for this:\n\n    >>> class LitModel(pl.LightningModule):\n    ...     def prepare_data(self):\n    ...         # download\n    ...         mnist_train = MNIST(os.getcwd(), train=True, download=True,\n    ...                             transform=transforms.ToTensor())\n    ...         mnist_test = MNIST(os.getcwd(), train=False, download=True,\n    ...                            transform=transforms.ToTensor())\n    ...\n    ...         # train/val split\n    ...         mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n    ...\n    ...         # assign to use in dataloaders\n    ...         self.train_dataset = mnist_train\n    ...         self.val_dataset = mnist_val\n    ...         self.test_dataset = mnist_test\n    ...\n    ...     def train_dataloader(self):\n    ...         return DataLoader(self.train_dataset, batch_size=64)\n    ...\n    ...     def val_dataloader(self):\n    ...         return DataLoader(self.mnist_val, batch_size=64)\n    ...\n    ...     def test_dataloader(self):\n    ...         return DataLoader(self.mnist_test, batch_size=64)\n\nNote:\n    :meth:`~LightningModule.prepare_data` is called once.\n\nNote:\n    Do anything with data that needs to happen ONLY once here, like download, tokenize, etc...\n\n\nLifecycle\n---------\nThe methods in the :class:`~LightningModule` are called in this order:\n\n1. :meth:`~LightningModule.__init__`\n2. :meth:`~LightningModule.prepare_data`\n3. :meth:`~LightningModule.configure_optimizers`\n4. :meth:`~LightningModule.train_dataloader`\n\nIf you define a validation loop then\n\n5. :meth:`~LightningModule.val_dataloader`\n\nAnd if you define a test loop:\n\n6. :meth:`~LightningModule.test_dataloader`\n\nNote:\n    :meth:`~LightningModule.test_dataloader` is only called with ``.test()``\n\nIn every epoch, the loop methods are called in this frequency:\n\n1. :meth:`~LightningModule.validation_step` called every batch\n2. :meth:`~LightningModule.validation_epoch_end` called every epoch\n\nLive demo\n---------\nCheck out this\n`COLAB <https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=HOk9c4_35FKg>`_\nfor a live demo.\n\nLightningModule Class\n---------------------\n\n""""""\n\nfrom pytorch_lightning.core.decorators import data_loader\nfrom pytorch_lightning.core.lightning import LightningModule\n\n__all__ = [\'LightningModule\', \'data_loader\']\n# __call__ = __all__\n'"
pytorch_lightning/core/decorators.py,0,"b'from pytorch_lightning.utilities import rank_zero_warn\n\n\ndef data_loader(fn):\n    """"""Decorator to make any fx with this use the lazy property.\n\n    Warnings:\n        This decorator deprecated in v0.7.0 and it will be removed v0.9.0.\n    """"""\n    rank_zero_warn(\'`data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\', DeprecationWarning)\n\n    def inner_fx(self):\n        return fn(self)\n    return inner_fx\n'"
pytorch_lightning/core/grads.py,2,"b'""""""\nModule to describe gradients\n""""""\nfrom typing import Dict, Union\n\nimport torch\n\n\nclass GradInformation(torch.nn.Module):\n\n    def grad_norm(self, norm_type: Union[float, int, str]) -> Dict[str, float]:\n        """"""Compute each parameter\'s gradient\'s norm and their overall norm.\n\n        The overall norm is computed over all gradients together, as if they\n        were concatenated into a single vector.\n\n        Args:\n            norm_type: The type of the used p-norm, cast to float if necessary.\n                Can be ``\'inf\'`` for infinity norm.\n\n        Return:\n            norms: The dictionary of p-norms of each parameter\'s gradient and\n                a special entry for the total p-norm of the gradients viewed\n                as a single vector.\n        """"""\n        norm_type = float(norm_type)\n\n        norms, all_norms = {}, []\n        for name, p in self.named_parameters():\n            if p.grad is None:\n                continue\n\n            param_norm = float(p.grad.data.norm(norm_type))\n            norms[f\'grad_{norm_type}_norm_{name}\'] = round(param_norm, 3)\n\n            all_norms.append(param_norm)\n\n        total_norm = float(torch.tensor(all_norms).norm(norm_type))\n        norms[f\'grad_{norm_type}_norm_total\'] = round(total_norm, 3)\n\n        return norms\n'"
pytorch_lightning/core/hooks.py,5,"b'from typing import Any\n\nimport torch\nfrom torch import Tensor\nfrom torch.optim.optimizer import Optimizer\nfrom pytorch_lightning.utilities import move_data_to_device\n\n\ntry:\n    from apex import amp\nexcept ImportError:\n    APEX_AVAILABLE = False\nelse:\n    APEX_AVAILABLE = True\n\n\nclass ModelHooks(torch.nn.Module):\n\n    # TODO: remove in v0.9.0\n    def on_sanity_check_start(self):\n        """"""\n        Called before starting evaluation.\n\n        Warning:\n            Deprecated. Will be removed in v0.9.0.\n        """"""\n\n    def on_train_start(self) -> None:\n        """"""\n        Called at the beginning of training before sanity check.\n        """"""\n        # do something at the start of training\n\n    def on_train_end(self) -> None:\n        """"""\n        Called at the end of training before logger experiment is closed.\n        """"""\n        # do something at the end of training\n\n    def on_batch_start(self, batch: Any) -> None:\n        """"""\n        Called in the training loop before anything happens for that batch.\n\n        If you return -1 here, you will skip training for the rest of the current epoch.\n\n        Args:\n            batch: The batched data as it is returned by the training DataLoader.\n        """"""\n        # do something when the batch starts\n\n    def on_batch_end(self) -> None:\n        """"""\n        Called in the training loop after the batch.\n        """"""\n        # do something when the batch ends\n\n    def on_epoch_start(self) -> None:\n        """"""\n        Called in the training loop at the very beginning of the epoch.\n        """"""\n        # do something when the epoch starts\n\n    def on_epoch_end(self) -> None:\n        """"""\n        Called in the training loop at the very end of the epoch.\n        """"""\n        # do something when the epoch ends\n\n    def on_pre_performance_check(self) -> None:\n        """"""\n        Called at the very beginning of the validation loop.\n        """"""\n        # do something before validation starts\n\n    def on_post_performance_check(self) -> None:\n        """"""\n        Called at the very end of the validation loop.\n        """"""\n        # do something before validation end\n\n    def on_before_zero_grad(self, optimizer: Optimizer) -> None:\n        """"""\n        Called after optimizer.step() and before optimizer.zero_grad().\n\n        Called in the training loop after taking an optimizer step and before zeroing grads.\n        Good place to inspect weight information with weights updated.\n\n        This is where it is called::\n\n            for optimizer in optimizers:\n                optimizer.step()\n                model.on_before_zero_grad(optimizer) # < ---- called here\n                optimizer.zero_grad\n\n        Args:\n            optimizer: The optimizer for which grads should be zeroed.\n        """"""\n        # do something with the optimizer or inspect it.\n\n    def on_after_backward(self) -> None:\n        """"""\n        Called in the training loop after loss.backward() and before optimizers do anything.\n        This is the ideal place to inspect or log gradient information.\n\n        Example::\n\n            def on_after_backward(self):\n                # example to inspect gradient information in tensorboard\n                if self.trainer.global_step % 25 == 0:  # don\'t make the tf file huge\n                    params = self.state_dict()\n                    for k, v in params.items():\n                        grads = v\n                        name = k\n                        self.logger.experiment.add_histogram(tag=name, values=grads,\n                                                             global_step=self.trainer.global_step)\n\n        """"""\n\n    def backward(self, trainer, loss: Tensor, optimizer: Optimizer, optimizer_idx: int) -> None:\n        """"""\n        Override backward with your own implementation if you need to.\n\n        Args:\n            trainer: Pointer to the trainer\n            loss: Loss is already scaled by accumulated grads\n            optimizer: Current optimizer being used\n            optimizer_idx: Index of the current optimizer being used\n\n        Called to perform backward step.\n        Feel free to override as needed.\n\n        The loss passed in has already been scaled for accumulated gradients if requested.\n\n        Example::\n\n            def backward(self, use_amp, loss, optimizer):\n                if use_amp:\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n\n        """"""\n        if trainer.precision == 16:\n            # .backward is not special on 16-bit with TPUs\n            if trainer.on_tpu:\n                return\n\n            if self.trainer.use_native_amp:\n                self.trainer.scaler.scale(loss).backward()\n\n            # TODO: remove in v0.8.0\n            else:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n        else:\n            loss.backward()\n\n    def transfer_batch_to_device(self, batch: Any, device: torch.device) -> Any:\n        """"""\n        Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors\n        wrapped in a custom data structure.\n\n        The data types listed below (and any arbitrary nesting of them) are supported out of the box:\n\n        - :class:`torch.Tensor`\n        - :class:`list`\n        - :class:`dict`\n        - :class:`tuple`\n        - ``torchtext.data.Batch`` (COMING SOON)\n\n        For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).\n\n        Example::\n\n            def transfer_batch_to_device(self, batch, device)\n                if isinstance(batch, CustomBatch):\n                    # move all tensors in your custom data structure to the device\n                    batch.samples = batch.samples.to(device)\n                    batch.targets = batch.targets.to(device)\n                else:\n                    batch = super().transfer_batch_to_device(data, device)\n                return batch\n\n        Args:\n            batch: A batch of data that needs to be transferred to a new device.\n            device: The target device as defined in PyTorch.\n\n        Returns:\n            A reference to the data on the new device.\n\n        Note:\n            This hook should only transfer the data and not modify it, nor should it move the data to\n            any other device than the one passed in as argument (unless you know what you are doing).\n            The :class:`~pytorch_lightning.trainer.trainer.Trainer` already takes care of splitting the\n            batch and determines the target devices.\n\n        See Also:\n            - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device`\n            - :func:`~pytorch_lightning.utilities.apply_func.apply_to_collection`\n        """"""\n        return move_data_to_device(batch, device)\n'"
pytorch_lightning/core/lightning.py,30,"b'import collections\nimport inspect\nimport os\nfrom abc import ABC, abstractmethod\nfrom argparse import Namespace\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union, Sequence\n\nimport torch\nimport torch.distributed as torch_distrib\nfrom torch import Tensor\nfrom torch.nn import Module\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.optim.optimizer import Optimizer\nfrom torch.utils.data import DataLoader\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.core.grads import GradInformation\nfrom pytorch_lightning.core.hooks import ModelHooks\nfrom pytorch_lightning.core.memory import ModelSummary\nfrom pytorch_lightning.core.saving import ModelIO, PRIMITIVE_TYPES, ALLOWED_CONFIG_TYPES\nfrom pytorch_lightning.utilities.device_dtype_mixin import DeviceDtypeModuleMixin\nfrom pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities import rank_zero_warn\nfrom pytorch_lightning.utilities.parsing import AttributeDict, collect_init_args, get_init_args\n\ntry:\n    import torch_xla.core.xla_model as xm\nexcept ImportError:\n    XLA_AVAILABLE = False\nelse:\n    XLA_AVAILABLE = True\n\n\nclass LightningModule(ABC, DeviceDtypeModuleMixin, GradInformation, ModelIO, ModelHooks, Module):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.exp_save_path = None\n\n        #: The current epoch\n        self.current_epoch = 0\n\n        #: Total training batches seen across all epochs\n        self.global_step = 0\n\n        self.loaded_optimizer_states_dict = {}\n\n        #: Pointer to the trainer object\n        self.trainer = None\n\n        #: Pointer to the logger object\n        self.logger = None\n        self.example_input_array = None\n\n        #: True if using dp\n        self.use_dp = False\n\n        #: True if using ddp\n        self.use_ddp = False\n\n        #: True if using ddp2\n        self.use_ddp2 = False\n\n        # True if on tpu\n        self.use_tpu = False\n\n        #: True if using amp\n        self.use_amp = False\n\n        #: Current dtype\n        self._dtype = torch.float\n\n        #: device reference\n        self._device = torch.device(\'cpu\')\n\n    @property\n    def on_gpu(self):\n        """"""\n        True if your model is currently running on GPUs.\n        Useful to set flags around the LightningModule for different CPU vs GPU behavior.\n        """"""\n        return self.device.type == \'cuda\'\n\n    def print(self, *args, **kwargs) -> None:\n        r""""""\n        Prints only from process 0. Use this in any distributed mode to log only once.\n\n        Args:\n            *args: The thing to print. Will be passed to Python\'s built-in print function.\n            **kwargs: Will be passed to Python\'s built-in print function.\n\n        Example:\n\n            .. code-block:: python\n\n                def forward(self, x):\n                    self.print(x, \'in forward\')\n\n        """"""\n        if self.trainer.proc_rank == 0:\n            print(*args, **kwargs)\n\n    @abstractmethod\n    def forward(self, *args, **kwargs):\n        r""""""\n        Same as :meth:`torch.nn.Module.forward()`, however in Lightning you want this to define\n        the operations you want to use for prediction (i.e.: on a server or as a feature extractor).\n\n        Normally you\'d call ``self()`` from your :meth:`training_step` method.\n        This makes it easy to write a complex system for training with the outputs\n        you\'d want in a prediction setting.\n\n        Args:\n            *args: Whatever you decide to pass into the forward method.\n            **kwargs: Keyword arguments are also possible.\n\n        Return:\n            Predicted output\n\n        Examples:\n            .. code-block:: python\n\n                # example if we were using this model as a feature extractor\n                def forward(self, x):\n                    feature_maps = self.convnet(x)\n                    return feature_maps\n\n                def training_step(self, batch, batch_idx):\n                    x, y = batch\n                    feature_maps = self(x)\n                    logits = self.classifier(feature_maps)\n\n                    # ...\n                    return loss\n\n                # splitting it this way allows model to be used a feature extractor\n                model = MyModelAbove()\n\n                inputs = server.get_request()\n                results = model(inputs)\n                server.write_results(results)\n\n                # -------------\n                # This is in stark contrast to torch.nn.Module where normally you would have this:\n                def forward(self, batch):\n                    x, y = batch\n                    feature_maps = self.convnet(x)\n                    logits = self.classifier(feature_maps)\n                    return logits\n\n        """"""\n\n    def training_step(self, *args, **kwargs) -> Union[\n        int, Dict[str, Union[Tensor, Dict[str, Tensor]]]\n    ]:\n        r""""""\n        Here you compute and return the training loss and some additional metrics for e.g.\n        the progress bar or logger.\n\n        Args:\n            batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):\n                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.\n            batch_idx (int): Integer displaying index of this batch\n            optimizer_idx (int): When using multiple optimizers, this argument will also be present.\n            hiddens(:class:`~torch.Tensor`): Passed in if\n                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps` > 0.\n\n        Return:\n            Dict with loss key and optional log or progress bar keys.\n            When implementing :meth:`training_step`, return whatever you need in that step:\n\n            - loss -> tensor scalar **REQUIRED**\n            - progress_bar -> Dict for progress bar display. Must have only tensors\n            - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc)\n\n        In this step you\'d normally do the forward pass and calculate the loss for a batch.\n        You can also do fancier things like multiple forward passes or something model specific.\n\n        Examples:\n            .. code-block:: python\n\n                def training_step(self, batch, batch_idx):\n                    x, y, z = batch\n\n                    # implement your own\n                    out = self(x)\n                    loss = self.loss(out, x)\n\n                    logger_logs = {\'training_loss\': loss} # optional (MUST ALL BE TENSORS)\n\n                    # if using TestTubeLogger or TensorBoardLogger you can nest scalars\n                    logger_logs = {\'losses\': logger_logs} # optional (MUST ALL BE TENSORS)\n\n                    output = {\n                        \'loss\': loss, # required\n                        \'progress_bar\': {\'training_loss\': loss}, # optional (MUST ALL BE TENSORS)\n                        \'log\': logger_logs\n                    }\n\n                    # return a dict\n                    return output\n\n            If you define multiple optimizers, this step will be called with an additional\n            ``optimizer_idx`` parameter.\n\n            .. code-block:: python\n\n                # Multiple optimizers (e.g.: GANs)\n                def training_step(self, batch, batch_idx, optimizer_idx):\n                    if optimizer_idx == 0:\n                        # do training_step with encoder\n                    if optimizer_idx == 1:\n                        # do training_step with decoder\n\n\n            If you add truncated back propagation through time you will also get an additional\n            argument with the hidden states of the previous step.\n\n            .. code-block:: python\n\n                # Truncated back-propagation through time\n                def training_step(self, batch, batch_idx, hiddens):\n                    # hiddens are the hidden states from the previous truncated backprop step\n                    ...\n                    out, hiddens = self.lstm(data, hiddens)\n                    ...\n\n                    return {\n                        ""loss"": ...,\n                        ""hiddens"": hiddens  # remember to detach() this\n                    }\n\n        Notes:\n            The loss value shown in the progress bar is smoothed (averaged) over the last values,\n            so it differs from the actual loss returned in train/validation step.\n        """"""\n        rank_zero_warn(\'`training_step` must be implemented to be used with the Lightning Trainer\')\n\n    def training_end(self, *args, **kwargs):\n        """"""\n        Warnings:\n            Deprecated in v0.7.0. Use  :meth:`training_step_end` instead.\n        """"""\n\n    def training_epoch_end(\n            self,\n            outputs: Union[List[Dict[str, Tensor]], List[List[Dict[str, Tensor]]]]\n    ) -> Dict[str, Dict[str, Tensor]]:\n        """"""Called at the end of the training epoch with the outputs of all training steps.\n\n        .. code-block:: python\n\n            # the pseudocode for these calls\n            train_outs = []\n            for train_batch in train_data:\n                out = training_step(train_batch)\n                train_outs.append(out)\n            training_epoch_end(train_outs)\n\n        Args:\n            outputs: List of outputs you defined in :meth:`training_step`, or if there are\n                multiple dataloaders, a list containing a list of outputs for each dataloader.\n\n        Return:\n            Dict or OrderedDict.\n            May contain the following optional keys:\n\n            - log (metrics to be added to the logger; only tensors)\n            - progress_bar (dict for progress bar display)\n            - any metric used in a callback (e.g. early stopping).\n\n        Note:\n            If this method is not overridden, this won\'t be called.\n\n        - The outputs here are strictly for logging or progress bar.\n        - If you don\'t need to display anything, don\'t return anything.\n        - If you want to manually set current step, you can specify the \'step\' key in the \'log\' dict.\n\n        Examples:\n            With a single dataloader:\n\n            .. code-block:: python\n\n                def training_epoch_end(self, outputs):\n                    train_acc_mean = 0\n                    for output in outputs:\n                        train_acc_mean += output[\'train_acc\']\n\n                    train_acc_mean /= len(outputs)\n\n                    # log training accuracy at the end of an epoch\n                    results = {\n                        \'log\': {\'train_acc\': train_acc_mean.item()},\n                        \'progress_bar\': {\'train_acc\': train_acc_mean},\n                    }\n                    return results\n\n            With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains\n            one entry per dataloader, while the inner list contains the individual outputs of\n            each training step for that dataloader.\n\n            .. code-block:: python\n\n                def training_epoch_end(self, outputs):\n                    train_acc_mean = 0\n                    i = 0\n                    for dataloader_outputs in outputs:\n                        for output in dataloader_outputs:\n                            train_acc_mean += output[\'train_acc\']\n                            i += 1\n\n                    train_acc_mean /= i\n\n                    # log training accuracy at the end of an epoch\n                    results = {\n                        \'log\': {\'train_acc\': train_acc_mean.item(), \'step\': self.current_epoch}\n                        \'progress_bar\': {\'train_acc\': train_acc_mean},\n                    }\n                    return results\n        """"""\n\n    def training_step_end(self, *args, **kwargs) -> Dict[\n        str, Union[Tensor, Dict[str, Tensor]]\n    ]:\n        """"""\n        Use this when training with dp or ddp2 because :meth:`training_step`\n        will operate on only part of the batch. However, this is still optional\n        and only needed for things like softmax or NCE loss.\n\n        Note:\n            If you later switch to ddp or some other mode, this will still be called\n            so that you don\'t have to change your code\n\n        .. code-block:: python\n\n            # pseudocode\n            sub_batches = split_batches_for_dp(batch)\n            batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]\n            training_step_end(batch_parts_outputs)\n\n        Args:\n            batch_parts_outputs: What you return in `training_step` for each batch part.\n\n        Return:\n            Dict with loss key and optional log or progress bar keys.\n\n            - loss -> tensor scalar **REQUIRED**\n            - progress_bar -> Dict for progress bar display. Must have only tensors\n            - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc)\n\n        Examples:\n            .. code-block:: python\n\n                # WITHOUT training_step_end\n                # if used in DP or DDP2, this batch is 1/num_gpus large\n                def training_step(self, batch, batch_idx):\n                    # batch is 1/num_gpus big\n                    x, y = batch\n\n                    out = self(x)\n                    loss = self.softmax(out)\n                    loss = nce_loss(loss)\n                    return {\'loss\': loss}\n\n                # --------------\n                # with training_step_end to do softmax over the full batch\n                def training_step(self, batch, batch_idx):\n                    # batch is 1/num_gpus big\n                    x, y = batch\n\n                    out = self(x)\n                    return {\'out\': out}\n\n                def training_step_end(self, outputs):\n                    # this out is now the full size of the batch\n                    out = outputs[\'out\']\n\n                    # this softmax now uses the full batch size\n                    loss = nce_loss(loss)\n                    return {\'loss\': loss}\n\n        See Also:\n            See the :ref:`multi-gpu-training` guide for more details.\n        """"""\n\n    def validation_step(self, *args, **kwargs) -> Dict[str, Tensor]:\n        r""""""\n        Operates on a single batch of data from the validation set.\n        In this step you\'d might generate examples or calculate anything of interest like accuracy.\n\n        .. code-block:: python\n\n            # the pseudocode for these calls\n            val_outs = []\n            for val_batch in val_data:\n                out = validation_step(train_batch)\n                val_outs.append(out)\n                validation_epoch_end(val_outs)\n\n        Args:\n            batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):\n                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.\n            batch_idx (int): The index of this batch\n            dataloader_idx (int): The index of the dataloader that produced this batch\n                (only if multiple val datasets used)\n\n        Return:\n            Dict or OrderedDict - passed to :meth:`validation_epoch_end`.\n            If you defined :meth:`validation_step_end` it will go to that first.\n\n        .. code-block:: python\n\n            # pseudocode of order\n            out = validation_step()\n            if defined(\'validation_step_end\'):\n                out = validation_step_end(out)\n            out = validation_epoch_end(out)\n\n\n        .. code-block:: python\n\n            # if you have one val dataloader:\n            def validation_step(self, batch, batch_idx)\n\n            # if you have multiple val dataloaders:\n            def validation_step(self, batch, batch_idx, dataloader_idx)\n\n        Examples:\n            .. code-block:: python\n\n                # CASE 1: A single validation dataset\n                def validation_step(self, batch, batch_idx):\n                    x, y = batch\n\n                    # implement your own\n                    out = self(x)\n                    loss = self.loss(out, y)\n\n                    # log 6 example images\n                    # or generated text... or whatever\n                    sample_imgs = x[:6]\n                    grid = torchvision.utils.make_grid(sample_imgs)\n                    self.logger.experiment.add_image(\'example_images\', grid, 0)\n\n                    # calculate acc\n                    labels_hat = torch.argmax(out, dim=1)\n                    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n\n                    # all optional...\n                    # return whatever you need for the collation function validation_epoch_end\n                    output = OrderedDict({\n                        \'val_loss\': loss_val,\n                        \'val_acc\': torch.tensor(val_acc), # everything must be a tensor\n                    })\n\n                    # return an optional dict\n                    return output\n\n            If you pass in multiple val datasets, validation_step will have an additional argument.\n\n            .. code-block:: python\n\n                # CASE 2: multiple validation datasets\n                def validation_step(self, batch, batch_idx, dataset_idx):\n                    # dataset_idx tells you which dataset this is.\n\n        Note:\n            If you don\'t need to validate you don\'t need to implement this method.\n\n        Note:\n            When the :meth:`validation_step` is called, the model has been put in eval mode\n            and PyTorch gradients have been disabled. At the end of validation,\n            the model goes back to training mode and gradients are enabled.\n        """"""\n\n    def validation_step_end(self, *args, **kwargs) -> Dict[str, Tensor]:\n        """"""\n        Use this when validating with dp or ddp2 because :meth:`validation_step`\n        will operate on only part of the batch. However, this is still optional\n        and only needed for things like softmax or NCE loss.\n\n        Note:\n            If you later switch to ddp or some other mode, this will still be called\n            so that you don\'t have to change your code.\n\n        .. code-block:: python\n\n            # pseudocode\n            sub_batches = split_batches_for_dp(batch)\n            batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches]\n            validation_step_end(batch_parts_outputs)\n\n        Args:\n            batch_parts_outputs: What you return in :meth:`validation_step`\n                for each batch part.\n\n        Return:\n           Dict or OrderedDict - passed to the :meth:`validation_epoch_end` method.\n\n        Examples:\n            .. code-block:: python\n\n                # WITHOUT validation_step_end\n                # if used in DP or DDP2, this batch is 1/num_gpus large\n                def validation_step(self, batch, batch_idx):\n                    # batch is 1/num_gpus big\n                    x, y = batch\n\n                    out = self(x)\n                    loss = self.softmax(out)\n                    loss = nce_loss(loss)\n                    return {\'loss\': loss}\n\n                # --------------\n                # with validation_step_end to do softmax over the full batch\n                def validation_step(self, batch, batch_idx):\n                    # batch is 1/num_gpus big\n                    x, y = batch\n\n                    out = self(x)\n                    return {\'out\': out}\n\n                def validation_epoch_end(self, outputs):\n                    # this out is now the full size of the batch\n                    out = outputs[\'out\']\n\n                    # this softmax now uses the full batch size\n                    loss = nce_loss(loss)\n                    return {\'loss\': loss}\n\n        See Also:\n            See the :ref:`multi-gpu-training` guide for more details.\n        """"""\n\n    def validation_end(self, outputs):\n        """"""\n        Warnings:\n            Deprecated in v0.7.0. Use :meth:`validation_epoch_end` instead.\n            Will be removed in 1.0.0.\n        """"""\n\n    def validation_epoch_end(\n            self,\n            outputs: Union[List[Dict[str, Tensor]], List[List[Dict[str, Tensor]]]]\n    ) -> Dict[str, Dict[str, Tensor]]:\n        """"""\n        Called at the end of the validation epoch with the outputs of all validation steps.\n\n        .. code-block:: python\n\n            # the pseudocode for these calls\n            val_outs = []\n            for val_batch in val_data:\n                out = validation_step(val_batch)\n                val_outs.append(out)\n            validation_epoch_end(val_outs)\n\n        Args:\n            outputs: List of outputs you defined in :meth:`validation_step`, or if there\n                are multiple dataloaders, a list containing a list of outputs for each dataloader.\n\n        Return:\n            Dict or OrderedDict.\n            May have the following optional keys:\n\n            - progress_bar (dict for progress bar display; only tensors)\n            - log (dict of metrics to add to logger; only tensors).\n\n        Note:\n            If you didn\'t define a :meth:`validation_step`, this won\'t be called.\n\n        - The outputs here are strictly for logging or progress bar.\n        - If you don\'t need to display anything, don\'t return anything.\n        - If you want to manually set current step, you can specify the \'step\' key in the \'log\' dict.\n\n        Examples:\n            With a single dataloader:\n\n            .. code-block:: python\n\n                def validation_epoch_end(self, outputs):\n                    val_acc_mean = 0\n                    for output in outputs:\n                        val_acc_mean += output[\'val_acc\']\n\n                    val_acc_mean /= len(outputs)\n                    tqdm_dict = {\'val_acc\': val_acc_mean.item()}\n\n                    # show val_acc in progress bar but only log val_loss\n                    results = {\n                        \'progress_bar\': tqdm_dict,\n                        \'log\': {\'val_acc\': val_acc_mean.item()}\n                    }\n                    return results\n\n            With multiple dataloaders, `outputs` will be a list of lists. The outer list contains\n            one entry per dataloader, while the inner list contains the individual outputs of\n            each validation step for that dataloader.\n\n            .. code-block:: python\n\n                def validation_epoch_end(self, outputs):\n                    val_acc_mean = 0\n                    i = 0\n                    for dataloader_outputs in outputs:\n                        for output in dataloader_outputs:\n                            val_acc_mean += output[\'val_acc\']\n                            i += 1\n\n                    val_acc_mean /= i\n                    tqdm_dict = {\'val_acc\': val_acc_mean.item()}\n\n                    # show val_loss and val_acc in progress bar but only log val_loss\n                    results = {\n                        \'progress_bar\': tqdm_dict,\n                        \'log\': {\'val_acc\': val_acc_mean.item(), \'step\': self.current_epoch}\n                    }\n                    return results\n        """"""\n\n    def test_step(self, *args, **kwargs) -> Dict[str, Tensor]:\n        r""""""\n        Operates on a single batch of data from the test set.\n        In this step you\'d normally generate examples or calculate anything of interest\n        such as accuracy.\n\n        .. code-block:: python\n\n            # the pseudocode for these calls\n            test_outs = []\n            for test_batch in test_data:\n                out = test_step(test_batch)\n                test_outs.append(out)\n            test_epoch_end(test_outs)\n\n        Args:\n            batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):\n                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.\n            batch_idx (int): The index of this batch.\n            dataloader_idx (int): The index of the dataloader that produced this batch\n                (only if multiple test datasets used).\n\n        Return:\n            Dict or OrderedDict - passed to the :meth:`test_epoch_end` method.\n            If you defined :meth:`test_step_end` it will go to that first.\n\n        .. code-block:: python\n\n            # if you have one test dataloader:\n            def test_step(self, batch, batch_idx)\n\n            # if you have multiple test dataloaders:\n            def test_step(self, batch, batch_idx, dataloader_idx)\n\n        Examples:\n            .. code-block:: python\n\n                # CASE 1: A single test dataset\n                def test_step(self, batch, batch_idx):\n                    x, y = batch\n\n                    # implement your own\n                    out = self(x)\n                    loss = self.loss(out, y)\n\n                    # log 6 example images\n                    # or generated text... or whatever\n                    sample_imgs = x[:6]\n                    grid = torchvision.utils.make_grid(sample_imgs)\n                    self.logger.experiment.add_image(\'example_images\', grid, 0)\n\n                    # calculate acc\n                    labels_hat = torch.argmax(out, dim=1)\n                    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n\n                    # all optional...\n                    # return whatever you need for the collation function test_epoch_end\n                    output = OrderedDict({\n                        \'val_loss\': loss_val,\n                        \'val_acc\': torch.tensor(val_acc), # everything must be a tensor\n                    })\n\n                    # return an optional dict\n                    return output\n\n            If you pass in multiple validation datasets, :meth:`test_step` will have an additional\n            argument.\n\n            .. code-block:: python\n\n                # CASE 2: multiple test datasets\n                def test_step(self, batch, batch_idx, dataset_idx):\n                    # dataset_idx tells you which dataset this is.\n\n        Note:\n            If you don\'t need to validate you don\'t need to implement this method.\n\n        Note:\n            When the :meth:`test_step` is called, the model has been put in eval mode and\n            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back\n            to training mode and gradients are enabled.\n        """"""\n\n    def test_step_end(self, *args, **kwargs) -> Dict[str, Tensor]:\n        """"""\n        Use this when testing with dp or ddp2 because :meth:`test_step` will operate\n        on only part of the batch. However, this is still optional\n        and only needed for things like softmax or NCE loss.\n\n        Note:\n            If you later switch to ddp or some other mode, this will still be called\n            so that you don\'t have to change your code.\n\n        .. code-block:: python\n\n            # pseudocode\n            sub_batches = split_batches_for_dp(batch)\n            batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches]\n            test_step_end(batch_parts_outputs)\n\n        Args:\n            batch_parts_outputs: What you return in :meth:`test_step` for each batch part.\n\n        Return:\n             Dict or OrderedDict - passed to the :meth:`test_epoch_end`.\n\n        Examples:\n            .. code-block:: python\n\n                # WITHOUT test_step_end\n                # if used in DP or DDP2, this batch is 1/num_gpus large\n                def test_step(self, batch, batch_idx):\n                    # batch is 1/num_gpus big\n                    x, y = batch\n\n                    out = self(x)\n                    loss = self.softmax(out)\n                    loss = nce_loss(loss)\n                    return {\'loss\': loss}\n\n                # --------------\n                # with test_step_end to do softmax over the full batch\n                def test_step(self, batch, batch_idx):\n                    # batch is 1/num_gpus big\n                    x, y = batch\n\n                    out = self(x)\n                    return {\'out\': out}\n\n                def test_step_end(self, outputs):\n                    # this out is now the full size of the batch\n                    out = outputs[\'out\']\n\n                    # this softmax now uses the full batch size\n                    loss = nce_loss(loss)\n                    return {\'loss\': loss}\n\n        See Also:\n            See the :ref:`multi-gpu-training` guide for more details.\n        """"""\n\n    def test_end(self, outputs):\n        """"""\n        Warnings:\n             Deprecated in v0.7.0. Use :meth:`test_epoch_end` instead.\n             Will be removed in 1.0.0.\n        """"""\n\n    def test_epoch_end(\n            self,\n            outputs: Union[List[Dict[str, Tensor]], List[List[Dict[str, Tensor]]]]\n    ) -> Dict[str, Dict[str, Tensor]]:\n        """"""\n        Called at the end of a test epoch with the output of all test steps.\n\n        .. code-block:: python\n\n            # the pseudocode for these calls\n            test_outs = []\n            for test_batch in test_data:\n                out = test_step(test_batch)\n                test_outs.append(out)\n            test_epoch_end(test_outs)\n\n        Args:\n            outputs: List of outputs you defined in :meth:`test_step_end`, or if there\n                are multiple dataloaders, a list containing a list of outputs for each dataloader\n\n        Return:\n            Dict or OrderedDict: Dict has the following optional keys:\n\n            - progress_bar -> Dict for progress bar display. Must have only tensors.\n            - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc).\n\n        Note:\n            If you didn\'t define a :meth:`test_step`, this won\'t be called.\n\n        - The outputs here are strictly for logging or progress bar.\n        - If you don\'t need to display anything, don\'t return anything.\n        - If you want to manually set current step, specify it with the \'step\' key in the \'log\' Dict\n\n        Examples:\n            With a single dataloader:\n\n            .. code-block:: python\n\n                def test_epoch_end(self, outputs):\n                    test_acc_mean = 0\n                    for output in outputs:\n                        test_acc_mean += output[\'test_acc\']\n\n                    test_acc_mean /= len(outputs)\n                    tqdm_dict = {\'test_acc\': test_acc_mean.item()}\n\n                    # show test_loss and test_acc in progress bar but only log test_loss\n                    results = {\n                        \'progress_bar\': tqdm_dict,\n                        \'log\': {\'test_acc\': test_acc_mean.item()}\n                    }\n                    return results\n\n            With multiple dataloaders, `outputs` will be a list of lists. The outer list contains\n            one entry per dataloader, while the inner list contains the individual outputs of\n            each test step for that dataloader.\n\n            .. code-block:: python\n\n                def test_epoch_end(self, outputs):\n                    test_acc_mean = 0\n                    i = 0\n                    for dataloader_outputs in outputs:\n                        for output in dataloader_outputs:\n                            test_acc_mean += output[\'test_acc\']\n                            i += 1\n\n                    test_acc_mean /= i\n                    tqdm_dict = {\'test_acc\': test_acc_mean.item()}\n\n                    # show test_loss and test_acc in progress bar but only log test_loss\n                    results = {\n                        \'progress_bar\': tqdm_dict,\n                        \'log\': {\'test_acc\': test_acc_mean.item(), \'step\': self.current_epoch}\n                    }\n                    return results\n        """"""\n\n    def configure_ddp(\n            self,\n            model: \'LightningModule\',\n            device_ids: List[int]\n    ) -> DistributedDataParallel:\n        r""""""\n        Override to init DDP in your own way or with your own wrapper.\n        The only requirements are that:\n\n        1. On a validation batch the call goes to ``model.validation_step``.\n        2. On a training batch the call goes to ``model.training_step``.\n        3. On a testing batch, the call goes to ``model.test_step``.+\n\n        Args:\n            model: the :class:`LightningModule` currently being optimized.\n            device_ids: the list of GPU ids.\n\n        Return:\n            DDP wrapped model\n\n        Examples:\n            .. code-block:: python\n\n                # default implementation used in Trainer\n                def configure_ddp(self, model, device_ids):\n                    # Lightning DDP simply routes to test_step, val_step, etc...\n                    model = LightningDistributedDataParallel(\n                        model,\n                        device_ids=device_ids,\n                        find_unused_parameters=True\n                    )\n                    return model\n\n        """"""\n        model = LightningDistributedDataParallel(\n            model,\n            device_ids=device_ids,\n            find_unused_parameters=True\n        )\n        return model\n\n    def _init_slurm_connection(self) -> None:\n        """"""\n        Sets up environment variables necessary for pytorch distributed communications\n        based on slurm environment.\n        """"""\n        # use slurm job id for the port number\n        # guarantees unique ports across jobs from same grid search\n        try:\n            # use the last 4 numbers in the job id as the id\n            default_port = os.environ[\'SLURM_JOB_ID\']\n            default_port = default_port[-4:]\n\n            # all ports should be in the 10k+ range\n            default_port = int(default_port) + 15000\n\n        except Exception:\n            default_port = 12910\n\n        # if user gave a port number, use that one instead\n        try:\n            default_port = os.environ[\'MASTER_PORT\']\n        except Exception:\n            os.environ[\'MASTER_PORT\'] = str(default_port)\n\n        # figure out the root node addr\n        try:\n            root_node = os.environ[\'SLURM_NODELIST\'].split(\' \')[0]\n        except Exception:\n            root_node = \'127.0.0.1\'\n\n        root_node = self.trainer.resolve_root_node_address(root_node)\n        os.environ[\'MASTER_ADDR\'] = root_node\n\n    def init_ddp_connection(\n            self,\n            proc_rank: int,\n            world_size: int,\n            is_slurm_managing_tasks: bool = True\n    ) -> None:\n        """"""\n        Override to define your custom way of setting up a distributed environment.\n\n        Lightning\'s implementation uses env:// init by default and sets the first node as root\n        for SLURM managed cluster.\n\n        Args:\n            proc_rank: The current process rank within the node.\n            world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus).\n            is_slurm_managing_tasks: is cluster managed by SLURM.\n\n        """"""\n        if is_slurm_managing_tasks:\n            self._init_slurm_connection()\n\n        if \'MASTER_ADDR\' not in os.environ:\n            log.warning(""MASTER_ADDR environment variable is not defined. Set as localhost"")\n            os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\'\n        log.debug(f""MASTER_ADDR: {os.environ[\'MASTER_ADDR\']}"")\n\n        if \'MASTER_PORT\' not in os.environ:\n            log.warning(""MASTER_PORT environment variable is not defined. Set as 12910"")\n            os.environ[\'MASTER_PORT\'] = \'12910\'\n        log.debug(f""MASTER_PORT: {os.environ[\'MASTER_PORT\']}"")\n\n        if \'WORLD_SIZE\' in os.environ and int(os.environ[\'WORLD_SIZE\']) != world_size:\n            log.warning(f""WORLD_SIZE environment variable ({os.environ[\'WORLD_SIZE\']}) ""\n                        f""is not equal to the computed world size ({world_size}). Ignored."")\n\n        torch_backend = ""nccl"" if self.trainer.on_gpu else ""gloo""\n        log.info(f""initializing ddp: LOCAL_RANK: {proc_rank}/{world_size - 1} WORLD_SIZE:{world_size}"")\n        torch_distrib.init_process_group(torch_backend, rank=proc_rank, world_size=world_size)\n\n    def configure_apex(\n            self,\n            amp: object,\n            model: \'LightningModule\',\n            optimizers: List[Optimizer],\n            amp_level: str\n    ) -> Tuple[\'LightningModule\', List[Optimizer]]:\n        r""""""\n        Override to init AMP your own way.\n        Must return a model and list of optimizers.\n\n        Args:\n            amp: pointer to amp library object.\n            model: pointer to current :class:`LightningModule`.\n            optimizers: list of optimizers passed in :meth:`configure_optimizers`.\n            amp_level: AMP mode chosen (\'O1\', \'O2\', etc...)\n\n        Return:\n            Apex wrapped model and optimizers\n\n        Examples:\n            .. code-block:: python\n\n                # Default implementation used by Trainer.\n                def configure_apex(self, amp, model, optimizers, amp_level):\n                    model, optimizers = amp.initialize(\n                        model, optimizers, opt_level=amp_level,\n                    )\n\n                    return model, optimizers\n        """"""\n        model, optimizers = amp.initialize(\n            model, optimizers, opt_level=amp_level,\n        )\n\n        return model, optimizers\n\n    def configure_optimizers(self) -> Optional[Union[\n        Optimizer, Sequence[Optimizer], Dict, Sequence[Dict], Tuple[List, List]\n    ]]:\n        r""""""\n        Choose what optimizers and learning-rate schedulers to use in your optimization.\n        Normally you\'d need one. But in the case of GANs or similar you might have multiple.\n\n        Return:\n            Any of these 6 options.\n\n            - Single optimizer.\n            - List or Tuple - List of optimizers.\n            - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).\n            - Dictionary, with an \'optimizer\' key, and (optionally) a \'lr_scheduler\' key which value is a single LR scheduler or lr_dict.\n            - Tuple of dictionaries as described, with an optional \'frequency\' key.\n            - None - Fit will run without any optimizer.\n\n        Note:\n            The \'frequency\' value is an int corresponding to the number of sequential batches\n            optimized with the specific optimizer. It should be given to none or to all of the optimizers.\n            There is a difference between passing multiple optimizers in a list,\n            and passing multiple optimizers in dictionaries with a frequency of 1:\n            In the former case, all optimizers will operate on the given batch in each optimization step.\n            In the latter, only one optimizer will operate on the given batch at every step.\n\n            The lr_dict is a dictionary which contains scheduler and its associated configuration.\n            It has five keys. The default configuration is shown below.\n\n            .. code-block:: python\n\n                {\n                    \'scheduler\': lr_scheduler, # The LR schduler\n                    \'interval\': \'epoch\', # The unit of the scheduler\'s step size\n                    \'frequency\': 1, # The frequency of the scheduler\n                    \'reduce_on_plateau\': False, # For ReduceLROnPlateau scheduler\n                    \'monitor\': \'val_loss\' # Metric to monitor\n                }\n\n            If user only provides LR schedulers, then their configuration will set to default as shown above.\n\n        Examples:\n            .. code-block:: python\n\n                # most cases\n                def configure_optimizers(self):\n                    opt = Adam(self.parameters(), lr=1e-3)\n                    return opt\n\n                # multiple optimizer case (e.g.: GAN)\n                def configure_optimizers(self):\n                    generator_opt = Adam(self.model_gen.parameters(), lr=0.01)\n                    disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)\n                    return generator_opt, disriminator_opt\n\n                # example with learning rate schedulers\n                def configure_optimizers(self):\n                    generator_opt = Adam(self.model_gen.parameters(), lr=0.01)\n                    disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)\n                    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10)\n                    return [generator_opt, disriminator_opt], [discriminator_sched]\n\n                # example with step-based learning rate schedulers\n                def configure_optimizers(self):\n                    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n                    dis_opt = Adam(self.model_disc.parameters(), lr=0.02)\n                    gen_sched = {\'scheduler\': ExponentialLR(gen_opt, 0.99),\n                                 \'interval\': \'step\'}  # called after each training step\n                    dis_sched = CosineAnnealing(discriminator_opt, T_max=10) # called every epoch\n                    return [gen_opt, dis_opt], [gen_sched, dis_sched]\n\n                # example with optimizer frequencies\n                # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1\n                # https://arxiv.org/abs/1704.00028\n                def configure_optimizers(self):\n                    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n                    dis_opt = Adam(self.model_disc.parameters(), lr=0.02)\n                    n_critic = 5\n                    return (\n                        {\'optimizer\': dis_opt, \'frequency\': n_critic},\n                        {\'optimizer\': gen_opt, \'frequency\': 1}\n                    )\n\n        Note:\n\n            Some things to know:\n\n            - Lightning calls ``.backward()`` and ``.step()`` on each optimizer\n              and learning rate scheduler as needed.\n\n            - If you use 16-bit precision (``precision=16``), Lightning will automatically\n              handle the optimizers for you.\n\n            - If you use multiple optimizers, :meth:`training_step` will have an additional\n              ``optimizer_idx`` parameter.\n\n            - If you use LBFGS Lightning handles the closure function automatically for you.\n\n            - If you use multiple optimizers, gradients will be calculated only\n              for the parameters of current optimizer at each training step.\n\n            - If you need to control how often those optimizers step or override the\n              default ``.step()`` schedule, override the :meth:`optimizer_step` hook.\n\n            - If you only want to call a learning rate scheduler every ``x`` step or epoch,\n              or want to monitor a custom metric, you can specify these in a lr_dict:\n\n              .. code-block:: python\n\n                  {\n                      \'scheduler\': lr_scheduler,\n                      \'interval\': \'step\',  # or \'epoch\'\n                      \'monitor\': \'val_f1\',\n                      \'frequency\': x,\n                  }\n\n        """"""\n        rank_zero_warn(\'`configure_optimizers` must be implemented to be used with the Lightning Trainer\')\n\n    def optimizer_step(\n            self,\n            epoch: int,\n            batch_idx: int,\n            optimizer: Optimizer,\n            optimizer_idx: int,\n            second_order_closure: Optional[Callable] = None,\n    ) -> None:\n        r""""""\n        Override this method to adjust the default way the\n        :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.\n        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example\n        once per optimizer.\n\n        Args:\n            epoch: Current epoch\n            batch_idx: Index of current batch\n            optimizer: A PyTorch optimizer\n            optimizer_idx: If you used multiple optimizers this indexes into that list.\n            second_order_closure: closure for second order methods\n\n        Examples:\n            .. code-block:: python\n\n                # DEFAULT\n                def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx,\n                                   second_order_closure=None):\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n                # Alternating schedule for optimizer steps (i.e.: GANs)\n                def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx,\n                                   second_order_closure=None):\n                    # update generator opt every 2 steps\n                    if optimizer_idx == 0:\n                        if batch_idx % 2 == 0 :\n                            optimizer.step()\n                            optimizer.zero_grad()\n\n                    # update discriminator opt every 4 steps\n                    if optimizer_idx == 1:\n                        if batch_idx % 4 == 0 :\n                            optimizer.step()\n                            optimizer.zero_grad()\n\n                    # ...\n                    # add as many optimizers as you want\n\n\n            Here\'s another example showing how to use this for more advanced things such as\n            learning rate warm-up:\n\n            .. code-block:: python\n\n                # learning rate warm-up\n                def optimizer_step(self, current_epoch, batch_idx, optimizer,\n                                    optimizer_idx, second_order_closure=None):\n                    # warm up lr\n                    if self.trainer.global_step < 500:\n                        lr_scale = min(1., float(self.trainer.global_step + 1) / 500.)\n                        for pg in optimizer.param_groups:\n                            pg[\'lr\'] = lr_scale * self.learning_rate\n\n                    # update params\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n        Note:\n            If you also override the :meth:`~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad`\n            model hook don\'t forget to add the call to it before ``optimizer.zero_grad()`` yourself.\n\n        """"""\n        if self.trainer.use_tpu and XLA_AVAILABLE:\n            xm.optimizer_step(optimizer)\n        elif isinstance(optimizer, torch.optim.LBFGS):\n\n            # native amp + lbfgs is a no go right now\n            if self.trainer.use_amp and self.trainer.use_native_amp:\n                raise MisconfigurationException(\n                    \'native PyTorch amp and lbfgs are not compatible.\'\n                    \' To request, please file a Github issue in PyTorch and tag @mcarilli\')\n            optimizer.step(second_order_closure)\n        else:\n            if self.trainer.use_amp and self.trainer.use_native_amp:\n                self.trainer.scaler.step(optimizer)\n            else:\n                optimizer.step()\n\n        # in native 16-bit we need to update scaler after optimizer step\n        if self.trainer.use_amp and self.trainer.use_native_amp:\n            self.trainer.scaler.update()\n\n        # model hook\n        self.on_before_zero_grad(optimizer)\n\n        # clear gradients\n        optimizer.zero_grad()\n\n    def tbptt_split_batch(self, batch: Tensor, split_size: int) -> list:\n        r""""""\n        When using truncated backpropagation through time, each batch must be split along the\n        time dimension. Lightning handles this by default, but for custom behavior override\n        this function.\n\n        Args:\n            batch: Current batch\n            split_size: The size of the split\n\n        Return:\n            List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated\n            back propagation through time. The default implementation splits root level Tensors and\n            Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.\n\n        Examples:\n            .. code-block:: python\n\n                def tbptt_split_batch(self, batch, split_size):\n                  splits = []\n                  for t in range(0, time_dims[0], split_size):\n                      batch_split = []\n                      for i, x in enumerate(batch):\n                          if isinstance(x, torch.Tensor):\n                              split_x = x[:, t:t + split_size]\n                          elif isinstance(x, collections.Sequence):\n                              split_x = [None] * len(x)\n                              for batch_idx in range(len(x)):\n                                  split_x[batch_idx] = x[batch_idx][t:t + split_size]\n\n                          batch_split.append(split_x)\n\n                      splits.append(batch_split)\n\n                  return splits\n\n        Note:\n            Called in the training loop after\n            :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start`\n            if :paramref:`~pytorch_lightning.trainer.Trainer.truncated_bptt_steps` > 0.\n            Each returned batch split is passed separately to :meth:`training_step`.\n\n        """"""\n        time_dims = [len(x[0]) for x in batch if isinstance(x, (torch.Tensor, collections.Sequence))]\n        assert len(time_dims) >= 1, ""Unable to determine batch time dimension""\n        assert all(x == time_dims[0] for x in time_dims), ""Batch time dimension length is ambiguous""\n\n        splits = []\n        for t in range(0, time_dims[0], split_size):\n            batch_split = []\n            for i, x in enumerate(batch):\n                if isinstance(x, torch.Tensor):\n                    split_x = x[:, t:t + split_size]\n                elif isinstance(x, collections.Sequence):\n                    split_x = [None] * len(x)\n                    for batch_idx in range(len(x)):\n                        split_x[batch_idx] = x[batch_idx][t:t + split_size]\n\n                batch_split.append(split_x)\n\n            splits.append(batch_split)\n\n        return splits\n\n    def prepare_data(self) -> None:\n        """"""\n        Use this to download and prepare data.\n        In distributed (GPU, TPU), this will only be called once.\n        This is called before requesting the dataloaders:\n\n        .. code-block:: python\n\n            model.prepare_data()\n            model.train_dataloader()\n            model.val_dataloader()\n            model.test_dataloader()\n\n        Examples:\n            .. code-block:: python\n\n                def prepare_data(self):\n                    download_imagenet()\n                    clean_imagenet()\n                    cache_imagenet()\n        """"""\n\n    def train_dataloader(self) -> DataLoader:\n        """"""\n        Implement a PyTorch DataLoader for training.\n\n        Return:\n            Single PyTorch :class:`~torch.utils.data.DataLoader`.\n\n        The dataloader you return will not be called every epoch unless you set\n        :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch` to ``True``.\n\n        It\'s recommended that all data downloads and preparation happen in :meth:`prepare_data`.\n\n        - :meth:`~pytorch_lightning.trainer.Trainer.fit`\n        - ...\n        - :meth:`prepare_data`\n        - :meth:`train_dataloader`\n\n        Note:\n            Lightning adds the correct sampler for distributed and arbitrary hardware.\n            There is no need to set it yourself.\n\n        Example:\n            .. code-block:: python\n\n                def train_dataloader(self):\n                    transform = transforms.Compose([transforms.ToTensor(),\n                                                    transforms.Normalize((0.5,), (1.0,))])\n                    dataset = MNIST(root=\'/path/to/mnist/\', train=True, transform=transform,\n                                    download=True)\n                    loader = torch.utils.data.DataLoader(\n                        dataset=dataset,\n                        batch_size=self.batch_size,\n                        shuffle=True\n                    )\n                    return loader\n\n        """"""\n        rank_zero_warn(\'`train_dataloader` must be implemented to be used with the Lightning Trainer\')\n\n    def tng_dataloader(self):  # todo: remove in v1.0.0\n        """"""\n        Warnings:\n            Deprecated in v0.5.0. Use :meth:`train_dataloader` instead. Will be removed in 1.0.0.\n        """"""\n        output = self.train_dataloader()\n        rank_zero_warn(""`tng_dataloader` has been renamed to `train_dataloader` since v0.5.0.""\n                       "" and this method will be removed in v1.0.0"", DeprecationWarning)\n        return output\n\n    def test_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n        r""""""\n        Implement one or multiple PyTorch DataLoaders for testing.\n\n        The dataloader you return will not be called every epoch unless you set\n        :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch` to ``True``.\n\n        It\'s recommended that all data downloads and preparation happen in :meth:`prepare_data`.\n\n        - :meth:`~pytorch_lightning.trainer.Trainer.fit`\n        - ...\n        - :meth:`prepare_data`\n        - :meth:`train_dataloader`\n        - :meth:`val_dataloader`\n        - :meth:`test_dataloader`\n\n        Note:\n            Lightning adds the correct sampler for distributed and arbitrary hardware.\n            There is no need to set it yourself.\n\n        Return:\n            Single or multiple PyTorch DataLoaders.\n\n        Example:\n            .. code-block:: python\n\n                def test_dataloader(self):\n                    transform = transforms.Compose([transforms.ToTensor(),\n                                                    transforms.Normalize((0.5,), (1.0,))])\n                    dataset = MNIST(root=\'/path/to/mnist/\', train=False, transform=transform,\n                                    download=True)\n                    loader = torch.utils.data.DataLoader(\n                        dataset=dataset,\n                        batch_size=self.batch_size,\n                        shuffle=False\n                    )\n\n                    return loader\n\n        Note:\n            If you don\'t need a test dataset and a :meth:`test_step`, you don\'t need to implement\n            this method.\n\n        """"""\n\n    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n        r""""""\n        Implement one or multiple PyTorch DataLoaders for validation.\n\n        The dataloader you return will not be called every epoch unless you set\n        :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch` to ``True``.\n\n        It\'s recommended that all data downloads and preparation happen in :meth:`prepare_data`.\n\n        - :meth:`~pytorch_lightning.trainer.Trainer.fit`\n        - ...\n        - :meth:`prepare_data`\n        - :meth:`train_dataloader`\n        - :meth:`val_dataloader`\n        - :meth:`test_dataloader`\n\n        Note:\n            Lightning adds the correct sampler for distributed and arbitrary hardware\n            There is no need to set it yourself.\n\n        Return:\n            Single or multiple PyTorch DataLoaders.\n\n        Examples:\n            .. code-block:: python\n\n                def val_dataloader(self):\n                    transform = transforms.Compose([transforms.ToTensor(),\n                                                    transforms.Normalize((0.5,), (1.0,))])\n                    dataset = MNIST(root=\'/path/to/mnist/\', train=False,\n                                    transform=transform, download=True)\n                    loader = torch.utils.data.DataLoader(\n                        dataset=dataset,\n                        batch_size=self.batch_size,\n                        shuffle=False\n                    )\n\n                    return loader\n\n                # can also return multiple dataloaders\n                def val_dataloader(self):\n                    return [loader_a, loader_b, ..., loader_n]\n\n        Note:\n            If you don\'t need a validation dataset and a :meth:`validation_step`, you don\'t need to\n            implement this method.\n\n        Note:\n            In the case where you return multiple validation dataloaders, the :meth:`validation_step`\n            will have an argument ``dataset_idx`` which matches the order here.\n        """"""\n\n    def summarize(self, mode: str) -> None:\n        model_summary = ModelSummary(self, mode=mode)\n        log.info(\'\\n\' + model_summary.__str__())\n\n    def freeze(self) -> None:\n        r""""""\n        Freeze all params for inference.\n\n        Example:\n            .. code-block:: python\n\n                model = MyLightningModule(...)\n                model.freeze()\n\n        """"""\n        for param in self.parameters():\n            param.requires_grad = False\n\n        self.eval()\n\n    def unfreeze(self) -> None:\n        """"""\n        Unfreeze all parameters for training.\n\n        .. code-block:: python\n\n            model = MyLightningModule(...)\n            model.unfreeze()\n\n        """"""\n        for param in self.parameters():\n            param.requires_grad = True\n\n        self.train()\n\n    def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n        r""""""\n        Called by Lightning to restore your model.\n        If you saved something with :meth:`on_save_checkpoint` this is your chance to restore this.\n\n        Args:\n            checkpoint: Loaded checkpoint\n\n\n        Example:\n            .. code-block:: python\n\n                def on_load_checkpoint(self, checkpoint):\n                    # 99% of the time you don\'t need to implement this method\n                    self.something_cool_i_want_to_save = checkpoint[\'something_cool_i_want_to_save\']\n\n        Note:\n            Lightning auto-restores global step, epoch, and train state including amp scaling.\n            There is no need for you to restore anything regarding training.\n        """"""\n\n    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n        r""""""\n        Called by Lightning when saving a checkpoint to give you a chance to store anything\n        else you might want to save.\n\n        Args:\n            checkpoint: Checkpoint to be saved\n\n        Example:\n            .. code-block:: python\n\n\n                def on_save_checkpoint(self, checkpoint):\n                    # 99% of use cases you don\'t need to implement this method\n                    checkpoint[\'something_cool_i_want_to_save\'] = my_cool_pickable_object\n\n        Note:\n            Lightning saves all aspects of training (epoch, global step, etc...)\n            including amp scaling.\n            There is no need for you to store anything about training.\n\n        """"""\n\n    def get_progress_bar_dict(self) -> Dict[str, Union[int, str]]:\n        r""""""\n        Additional items to be displayed in the progress bar.\n\n        Return:\n            Dictionary with the items to be displayed in the progress bar.\n        """"""\n        # call .item() only once but store elements without graphs\n        running_train_loss = self.trainer.running_loss.mean()\n        avg_training_loss = running_train_loss.cpu().item() if running_train_loss is not None else float(\'NaN\')\n        tqdm_dict = {\n            \'loss\': \'{:.3f}\'.format(avg_training_loss)\n        }\n\n        if self.trainer.truncated_bptt_steps is not None:\n            tqdm_dict[\'split_idx\'] = self.trainer.split_idx\n\n        if self.trainer.logger is not None and self.trainer.logger.version is not None:\n            tqdm_dict[\'v_num\'] = self.trainer.logger.version\n\n        return tqdm_dict\n\n    def get_tqdm_dict(self) -> Dict[str, Union[int, str]]:\n        """"""\n        Additional items to be displayed in the progress bar.\n\n        Return:\n            Dictionary with the items to be displayed in the progress bar.\n\n        Warning:\n            Deprecated since v0.7.3.\n            Use :meth:`get_progress_bar_dict` instead.\n        """"""\n        rank_zero_warn(""`get_tqdm_dict` was renamed to `get_progress_bar_dict` in v0.7.3""\n                       "" and this method will be removed in v1.0.0"", DeprecationWarning)\n        return self.get_progress_bar_dict()\n\n    @classmethod\n    def _auto_collect_arguments(cls, frame=None) -> Tuple[Dict, Dict]:\n        """"""\n        Collect all module arguments in the current constructor and all child constructors.\n        The child constructors are all the ``__init__`` methods that reach the current class through\n        (chained) ``super().__init__()`` calls.\n\n        Args:\n            frame: instance frame\n\n        Returns:\n            self_arguments: arguments dictionary of the first instance\n            parents_arguments: arguments dictionary of the parent\'s instances\n        """"""\n        if not frame:\n            frame = inspect.currentframe()\n\n        frame_args = collect_init_args(frame.f_back, [])\n        self_arguments = frame_args[-1]\n\n        # set module_arguments in child\n        self_arguments = self_arguments\n        parents_arguments = {}\n\n        # add all arguments from parents\n        for args in frame_args[:-1]:\n            parents_arguments.update(args)\n        return self_arguments, parents_arguments\n\n    def save_hyperparameters(self, *args, frame=None) -> None:\n        """"""Save all model arguments.\n\n        Args:\n            args: single object of `dict`, `NameSpace` or `OmegaConf`\n             or string names or argumenst from class `__init__`\n\n        >>> from collections import OrderedDict\n        >>> class ManuallyArgsModel(LightningModule):\n        ...     def __init__(self, arg1, arg2, arg3):\n        ...         super().__init__()\n        ...         # manually assine arguments\n        ...         self.save_hyperparameters(\'arg1\', \'arg3\')\n        ...     def forward(self, *args, **kwargs):\n        ...         ...\n        >>> model = ManuallyArgsModel(1, \'abc\', 3.14)\n        >>> model.hparams\n        ""arg1"": 1\n        ""arg3"": 3.14\n\n        >>> class AutomaticArgsModel(LightningModule):\n        ...     def __init__(self, arg1, arg2, arg3):\n        ...         super().__init__()\n        ...         # equivalent automatic\n        ...         self.save_hyperparameters()\n        ...     def forward(self, *args, **kwargs):\n        ...         ...\n        >>> model = AutomaticArgsModel(1, \'abc\', 3.14)\n        >>> model.hparams\n        ""arg1"": 1\n        ""arg2"": abc\n        ""arg3"": 3.14\n\n        >>> class SingleArgModel(LightningModule):\n        ...     def __init__(self, params):\n        ...         super().__init__()\n        ...         # manually assign single argument\n        ...         self.save_hyperparameters(params)\n        ...     def forward(self, *args, **kwargs):\n        ...         ...\n        >>> model = SingleArgModel(Namespace(p1=1, p2=\'abc\', p3=3.14))\n        >>> model.hparams\n        ""p1"": 1\n        ""p2"": abc\n        ""p3"": 3.14\n        """"""\n        if not frame:\n            frame = inspect.currentframe().f_back\n        init_args = get_init_args(frame)\n        assert init_args, \'failed to inspect the self init\'\n        if not args:\n            hp = init_args\n            self._hparams_name = \'kwargs\' if hp else None\n        else:\n            isx_non_str = [i for i, arg in enumerate(args) if not isinstance(arg, str)]\n            if len(isx_non_str) == 1:\n                hp = args[isx_non_str[0]]\n                cand_names = [k for k, v in init_args.items() if v == hp]\n                self._hparams_name = cand_names[0] if cand_names else None\n            else:\n                hp = {arg: init_args[arg] for arg in args if isinstance(arg, str)}\n                self._hparams_name = \'kwargs\'\n\n        # `hparams` are expected here\n        if hp:\n            self._set_hparams(hp)\n\n    def _set_hparams(self, hp: Union[dict, Namespace, str]) -> None:\n        if isinstance(hp, Namespace):\n            hp = vars(hp)\n        if isinstance(hp, dict):\n            hp = AttributeDict(hp)\n        elif isinstance(hp, PRIMITIVE_TYPES):\n            raise ValueError(f\'Primitives {PRIMITIVE_TYPES} are not allowed.\')\n        elif not isinstance(hp, ALLOWED_CONFIG_TYPES):\n            raise ValueError(f\'Unsupported config type of {type(hp)}.\')\n\n        if isinstance(hp, dict) and isinstance(self.hparams, dict):\n            self.hparams.update(hp)\n        else:\n            self._hparams = hp\n\n    @property\n    def hparams(self) -> Union[AttributeDict, str]:\n        if not hasattr(self, \'_hparams\'):\n            self._hparams = AttributeDict()\n        return self._hparams\n\n    @hparams.setter\n    def hparams(self, hp: Union[dict, Namespace, Any]):\n        self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)\n'"
pytorch_lightning/core/memory.py,6,"b'""""""\nGenerates a summary of a model\'s layers and dimensionality\n""""""\n\nimport gc\nimport os\nimport subprocess\nfrom subprocess import PIPE\nfrom typing import Tuple, Dict, Union, List\n\nimport numpy as np\nimport torch\nfrom torch.nn import Module\n\nimport pytorch_lightning as pl\n\nfrom pytorch_lightning import _logger as log\n\n\nclass ModelSummary(object):\n\n    def __init__(self, model: \'pl.LightningModule\', mode: str = \'full\'):\n        """""" Generates summaries of model layers and dimensions. """"""\n        self.model = model\n        self.mode = mode\n        self.in_sizes = []\n        self.out_sizes = []\n\n        self.summarize()\n\n    def __str__(self):\n        return self.summary.__str__()\n\n    def __repr__(self):\n        return self.summary.__str__()\n\n    def named_modules(self) -> List[Tuple[str, Module]]:\n        if self.mode == \'full\':\n            mods = self.model.named_modules()\n            mods = list(mods)[1:]  # do not include root module (LightningModule)\n        elif self.mode == \'top\':\n            # the children are the top-level modules\n            mods = self.model.named_children()\n        else:\n            mods = []\n        return list(mods)\n\n    def get_variable_sizes(self) -> None:\n        """""" Run sample input through each layer to get output sizes. """"""\n        mods = self.named_modules()\n        in_sizes = []\n        out_sizes = []\n        input_ = self.model.example_input_array\n\n        if self.model.on_gpu:\n            device = next(self.model.parameters()).get_device()\n            # test if input is a list or a tuple\n            if isinstance(input_, (list, tuple)):\n                input_ = [input_i.cuda(device) if torch.is_tensor(input_i) else input_i\n                          for input_i in input_]\n            else:\n                input_ = input_.cuda(device)\n\n        if self.model.trainer.use_amp:\n            # test if it is not a list or a tuple\n            if isinstance(input_, (list, tuple)):\n                input_ = [input_i.half() if torch.is_tensor(input_i) else input_i\n                          for input_i in input_]\n            else:\n                input_ = input_.half()\n\n        with torch.no_grad():\n\n            for _, m in mods:\n                if isinstance(input_, (list, tuple)):  # pragma: no-cover\n                    out = m(*input_)\n                else:\n                    out = m(input_)\n\n                if isinstance(input_, (list, tuple)):  # pragma: no-cover\n                    in_size = []\n                    for x in input_:\n                        if isinstance(x, list):\n                            in_size.append(len(x))\n                        else:\n                            in_size.append(x.size())\n                else:\n                    in_size = np.array(input_.size())\n\n                in_sizes.append(in_size)\n\n                if isinstance(out, (list, tuple)):  # pragma: no-cover\n                    out_size = np.asarray([x.size() for x in out])\n                else:\n                    out_size = np.array(out.size())\n\n                out_sizes.append(out_size)\n                input_ = out\n\n        self.in_sizes = in_sizes\n        self.out_sizes = out_sizes\n        assert len(in_sizes) == len(out_sizes)\n\n    def get_layer_names(self) -> None:\n        """""" Collect Layer Names """"""\n        mods = self.named_modules()\n        names = []\n        layers = []\n        for name, m in mods:\n            names += [name]\n            layers += [str(m.__class__)]\n\n        layer_types = [x.split(\'.\')[-1][:-2] for x in layers]\n\n        self.layer_names = names\n        self.layer_types = layer_types\n\n    def get_parameter_sizes(self) -> None:\n        """""" Get sizes of all parameters in `model`. """"""\n        mods = self.named_modules()\n        sizes = []\n        for _, m in mods:\n            p = list(m.parameters())\n            modsz = [np.array(param.size()) for param in p]\n            sizes.append(modsz)\n\n        self.param_sizes = sizes\n\n    def get_parameter_nums(self) -> None:\n        """""" Get number of parameters in each layer. """"""\n        param_nums = []\n        for mod in self.param_sizes:\n            all_params = 0\n            for p in mod:\n                all_params += np.prod(p)\n            param_nums.append(all_params)\n        self.param_nums = param_nums\n\n    def make_summary(self) -> None:\n        """"""\n        Makes a summary listing with:\n\n        Layer Name, Layer Type, Input Size, Output Size, Number of Parameters\n        """"""\n        arrays = [[\'Name\', self.layer_names],\n                  [\'Type\', self.layer_types],\n                  [\'Params\', list(map(get_human_readable_count, self.param_nums))]]\n        if self.model.example_input_array is not None:\n            arrays.append([\'In sizes\', self.in_sizes])\n            arrays.append([\'Out sizes\', self.out_sizes])\n\n        self.summary = _format_summary_table(*arrays)\n\n    def summarize(self) -> None:\n        self.get_layer_names()\n        self.get_parameter_sizes()\n        self.get_parameter_nums()\n\n        if self.model.example_input_array is not None:\n            self.get_variable_sizes()\n        self.make_summary()\n\n\ndef _format_summary_table(*cols) -> str:\n    """"""\n    Takes in a number of arrays, each specifying a column in\n    the summary table, and combines them all into one big\n    string defining the summary table that are nicely formatted.\n    """"""\n    n_rows = len(cols[0][1])\n    n_cols = 1 + len(cols)\n\n    # Layer counter\n    counter = list(map(str, list(range(n_rows))))\n    counter_len = max([len(c) for c in counter])\n\n    # Get formatting length of each column\n    length = []\n    for c in cols:\n        str_l = len(c[0])  # default length is header length\n        for a in c[1]:\n            if isinstance(a, np.ndarray):\n                array_string = \'[\' + \', \'.join([str(j) for j in a]) + \']\'\n                str_l = max(len(array_string), str_l)\n            else:\n                str_l = max(len(a), str_l)\n        length.append(str_l)\n\n    # Formatting\n    s = \'{:<{}}\'\n    full_length = sum(length) + 3 * n_cols\n    header = [s.format(\' \', counter_len)] + [s.format(c[0], l) for c, l in zip(cols, length)]\n\n    # Summary = header + divider + Rest of table\n    summary = \' | \'.join(header) + \'\\n\' + \'-\' * full_length\n    for i in range(n_rows):\n        line = s.format(counter[i], counter_len)\n        for c, l in zip(cols, length):\n            if isinstance(c[1][i], np.ndarray):\n                array_string = \'[\' + \', \'.join([str(j) for j in c[1][i]]) + \']\'\n                line += \' | \' + array_string + \' \' * (l - len(array_string))\n            else:\n                line += \' | \' + s.format(c[1][i], l)\n        summary += \'\\n\' + line\n\n    return summary\n\n\ndef print_mem_stack() -> None:  # pragma: no-cover\n    for obj in gc.get_objects():\n        try:\n            if torch.is_tensor(obj) or (hasattr(obj, \'data\') and torch.is_tensor(obj.data)):\n                log.info(type(obj), obj.size())\n        except Exception:\n            pass\n\n\ndef count_mem_items() -> Tuple[int, int]:  # pragma: no-cover\n    num_params = 0\n    num_tensors = 0\n    for obj in gc.get_objects():\n        try:\n            if torch.is_tensor(obj) or (hasattr(obj, \'data\') and torch.is_tensor(obj.data)):\n                obj_type = str(type(obj))\n                if \'parameter\' in obj_type:\n                    num_params += 1\n                else:\n                    num_tensors += 1\n        except Exception:\n            pass\n\n    return num_params, num_tensors\n\n\ndef get_memory_profile(mode: str) -> Union[Dict[str, int], Dict[int, int]]:\n    """""" Get a profile of the current memory usage.\n\n    Args:\n        mode: There are two modes:\n\n            - \'all\' means return memory for all gpus\n            - \'min_max\' means return memory for max and min\n\n    Return:\n        A dictionary in which the keys are device ids as integers and\n        values are memory usage as integers in MB.\n        If mode is \'min_max\', the dictionary will also contain two additional keys:\n\n        - \'min_gpu_mem\': the minimum memory usage in MB\n        - \'max_gpu_mem\': the maximum memory usage in MB\n    """"""\n    memory_map = get_gpu_memory_map()\n\n    if mode == \'min_max\':\n        min_index, min_memory = min(memory_map.items(), key=lambda item: item[1])\n        max_index, max_memory = max(memory_map.items(), key=lambda item: item[1])\n\n        memory_map = {\'min_gpu_mem\': min_memory, \'max_gpu_mem\': max_memory}\n\n    return memory_map\n\n\ndef get_gpu_memory_map() -> Dict[str, int]:\n    """"""Get the current gpu usage.\n\n    Return:\n        A dictionary in which the keys are device ids as integers and\n        values are memory usage as integers in MB.\n    """"""\n    result = subprocess.run(\n        [\n            \'nvidia-smi\',\n            \'--query-gpu=memory.used\',\n            \'--format=csv,nounits,noheader\',\n        ],\n        encoding=\'utf-8\',\n        # capture_output=True,          # valid for python version >=3.7\n        stdout=PIPE, stderr=PIPE,       # for backward compatibility with python version 3.6\n        check=True)\n    # Convert lines into a dictionary\n    gpu_memory = [int(x) for x in result.stdout.strip().split(os.linesep)]\n    gpu_memory_map = {f\'gpu_{index}\': memory for index, memory in enumerate(gpu_memory)}\n    return gpu_memory_map\n\n\ndef get_human_readable_count(number: int) -> str:\n    """"""\n    Abbreviates an integer number with K, M, B, T for thousands, millions,\n    billions and trillions, respectively.\n\n    Examples:\n        >>> get_human_readable_count(123)\n        \'123  \'\n        >>> get_human_readable_count(1234)  # (one thousand)\n        \'1 K\'\n        >>> get_human_readable_count(2e6)   # (two million)\n        \'2 M\'\n        >>> get_human_readable_count(3e9)   # (three billion)\n        \'3 B\'\n        >>> get_human_readable_count(4e12)  # (four trillion)\n        \'4 T\'\n        >>> get_human_readable_count(5e15)  # (more than trillion)\n        \'5,000 T\'\n\n    Args:\n        number: a positive integer number\n\n    Return:\n        A string formatted according to the pattern described above.\n\n    """"""\n    assert number >= 0\n    labels = [\' \', \'K\', \'M\', \'B\', \'T\']\n    num_digits = int(np.floor(np.log10(number)) + 1 if number > 0 else 1)\n    num_groups = int(np.ceil(num_digits / 3))\n    num_groups = min(num_groups, len(labels))  # don\'t abbreviate beyond trillions\n    shift = -3 * (num_groups - 1)\n    number = number * (10 ** shift)\n    index = num_groups - 1\n    return f\'{int(number):,d} {labels[index]}\'\n'"
pytorch_lightning/core/model_saving.py,0,"b'""""""\n.. warning:: `model_saving` module has been renamed to `saving` since v0.6.0.\n  The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`model_saving` module has been renamed to `saving` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.saving import *  # noqa: F403 E402\n'"
pytorch_lightning/core/root_module.py,0,"b'""""""\n.. warning:: `root_module` module has been renamed to `lightning` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module` module has been renamed to `lightning` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.lightning import *  # noqa: F403 E402\n'"
pytorch_lightning/core/saving.py,4,"b'import ast\nimport csv\nimport inspect\nimport os\n\nimport torch\nimport yaml\nfrom argparse import Namespace\nfrom typing import Union, Dict, Any, Optional, Callable\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.utilities import rank_zero_warn, AttributeDict\n\nPRIMITIVE_TYPES = (bool, int, float, str)\nALLOWED_CONFIG_TYPES = (AttributeDict, dict, Namespace)\ntry:\n    from omegaconf import Container\nexcept ImportError:\n    pass\nelse:\n    ALLOWED_CONFIG_TYPES = ALLOWED_CONFIG_TYPES + (Container, )\n\n\nclass ModelIO(object):\n    CHECKPOINT_KEY_HYPER_PARAMS = \'hyper_parameters\'\n    CHECKPOINT_NAME_HYPER_PARAMS = \'hparams_name\'\n\n    @classmethod\n    def load_from_metrics(cls, weights_path, tags_csv, map_location=None):\n        r""""""\n        Warning:\n            Deprecated in version 0.7.0. You should use :meth:`load_from_checkpoint` instead.\n            Will be removed in v0.9.0.\n        """"""\n        rank_zero_warn(\n            ""`load_from_metrics` method has been unified with `load_from_checkpoint` in v0.7.0.""\n            "" The deprecated method will be removed in v0.9.0."", DeprecationWarning\n        )\n        return cls.load_from_checkpoint(weights_path, tags_csv=tags_csv, map_location=map_location)\n\n    @classmethod\n    def load_from_checkpoint(\n            cls,\n            checkpoint_path: str,\n            *args,\n            map_location: Optional[Union[Dict[str, str], str, torch.device, int, Callable]] = None,\n            hparams_file: Optional[str] = None,\n            tags_csv: Optional[str] = None,  # backward compatible, todo: remove in v0.9.0\n            **kwargs\n    ):\n        r""""""\n        Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\n        it stores the arguments passed to `__init__`  in the checkpoint under `module_arguments`\n\n        Any arguments specified through \\*args and \\*\\*kwargs will override args stored in `module_arguments`.\n\n        Args:\n            checkpoint_path: Path to checkpoint.\n            args: Any positional args needed to init the model.\n            map_location:\n                If your checkpoint saved a GPU model and you now load on CPUs\n                or a different number of GPUs, use this to map to the new setup.\n                The behaviour is the same as in :func:`torch.load`.\n            hparams_file: Optional path to a .yaml file with hierarchical structure\n                as in this example::\n\n                    drop_prob: 0.2\n                    dataloader:\n                        batch_size: 32\n\n                You most likely won\'t need this since Lightning will always save the hyperparameters\n                to the checkpoint.\n                However, if your checkpoint weights don\'t have the hyperparameters saved,\n                use this method to pass in a .yaml file with the hparams you\'d like to use.\n                These will be converted into a :class:`~dict` and passed into your\n                :class:`LightningModule` for use.\n\n                If your model\'s `hparams` argument is :class:`~argparse.Namespace`\n                and .yaml file has hierarchical structure, you need to refactor your model to treat\n                `hparams` as :class:`~dict`.\n\n                .csv files are acceptable here till v0.9.0, see tags_csv argument for detailed usage.\n            tags_csv:\n                .. warning:: .. deprecated:: 0.7.6\n\n                    `tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0.\n\n                Optional path to a .csv file with two columns (key, value)\n                as in this example::\n\n                    key,value\n                    drop_prob,0.2\n                    batch_size,32\n\n                Use this method to pass in a .csv file with the hparams you\'d like to use.\n            hparam_overrides: A dictionary with keys to override in the hparams\n            kwargs: Any keyword args needed to init the model.\n\n        Return:\n            :class:`LightningModule` with loaded weights and hyperparameters (if available).\n\n        Example:\n            .. code-block:: python\n\n                # load weights without mapping ...\n                MyLightningModule.load_from_checkpoint(\'path/to/checkpoint.ckpt\')\n\n                # or load weights mapping all weights from GPU 1 to GPU 0 ...\n                map_location = {\'cuda:1\':\'cuda:0\'}\n                MyLightningModule.load_from_checkpoint(\n                    \'path/to/checkpoint.ckpt\',\n                    map_location=map_location\n                )\n\n                # or load weights and hyperparameters from separate files.\n                MyLightningModule.load_from_checkpoint(\n                    \'path/to/checkpoint.ckpt\',\n                    hparams_file=\'/path/to/hparams_file.yaml\'\n                )\n\n                # override some of the params with new values\n                MyLightningModule.load_from_checkpoint(\n                    PATH,\n                    num_layers=128,\n                    pretrained_ckpt_path: NEW_PATH,\n                )\n\n                # predict\n                pretrained_model.eval()\n                pretrained_model.freeze()\n                y_hat = pretrained_model(x)\n        """"""\n        if map_location is not None:\n            checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        else:\n            checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n\n        # add the hparams from csv file to checkpoint\n        if tags_csv is not None:\n            hparams_file = tags_csv\n            rank_zero_warn(\'`tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0\', DeprecationWarning)\n\n        if hparams_file is not None:\n            extension = hparams_file.split(\'.\')[-1]\n            if extension.lower() in (\'csv\'):\n                hparams = load_hparams_from_tags_csv(hparams_file)\n            elif extension.lower() in (\'yml\', \'yaml\'):\n                hparams = load_hparams_from_yaml(hparams_file)\n            else:\n                raise ValueError(\'.csv, .yml or .yaml is required for `hparams_file`\')\n\n            hparams[\'on_gpu\'] = False\n\n            # overwrite hparams by the given file\n            checkpoint[cls.CHECKPOINT_KEY_HYPER_PARAMS] = hparams\n\n        # override the module_arguments with values that were passed in\n        checkpoint[cls.CHECKPOINT_KEY_HYPER_PARAMS].update(kwargs)\n\n        model = cls._load_model_state(checkpoint, *args, **kwargs)\n        return model\n\n    @classmethod\n    def _load_model_state(cls, checkpoint: Dict[str, Any], *args, **kwargs):\n\n        # pass in the values we saved automatically\n        if cls.CHECKPOINT_KEY_HYPER_PARAMS in checkpoint:\n            # todo add some back compatibility\n            model_args = checkpoint[cls.CHECKPOINT_KEY_HYPER_PARAMS]\n            args_name = checkpoint.get(cls.CHECKPOINT_NAME_HYPER_PARAMS)\n            init_args_name = inspect.signature(cls).parameters.keys()\n            if args_name == \'kwargs\':\n                cls_kwargs = {k: v for k, v in model_args.items() if k in init_args_name}\n                kwargs.update(**cls_kwargs)\n            elif args_name:\n                if args_name in init_args_name:\n                    kwargs.update({args_name: model_args})\n            else:\n                args = (model_args, ) + args\n\n        # load the state_dict on the model automatically\n        model = cls(*args, **kwargs)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n\n        # give model a chance to load something\n        model.on_load_checkpoint(checkpoint)\n\n        return model\n\n    def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n        """"""\n        Do something with the checkpoint.\n        Gives model a chance to load something before ``state_dict`` is restored.\n\n        Args:\n            checkpoint: A dictionary with variables from the checkpoint.\n        """"""\n\n    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n        """"""\n        Give the model a chance to add something to the checkpoint.\n        ``state_dict`` is already there.\n\n        Args:\n            checkpoint: A dictionary in which you can save variables to save in a checkpoint.\n                Contents need to be pickleable.\n        """"""\n\n    # -------------------------\n    # OPTIONAL HOOKS\n    # -------------------------\n    def on_hpc_save(self, checkpoint: Dict[str, Any]) -> None:\n        """"""\n        Hook to do whatever you need right before Slurm manager saves the model.\n\n        Args:\n            checkpoint: A dictionary in which you can save variables to save in a checkpoint.\n                Contents need to be pickleable.\n        """"""\n\n    def on_hpc_load(self, checkpoint: Dict[str, Any]) -> None:\n        """"""\n        Hook to do whatever you need right before Slurm manager loads the model.\n\n        Args:\n            checkpoint: A dictionary with variables from the checkpoint.\n        """"""\n\n\ndef update_hparams(hparams: dict, updates: dict) -> None:\n    """"""\n    Overrides hparams with new values\n\n    >>> hparams = {\'c\': 4}\n    >>> update_hparams(hparams, {\'a\': {\'b\': 2}, \'c\': 1})\n    >>> hparams[\'a\'][\'b\'], hparams[\'c\']\n    (2, 1)\n    >>> update_hparams(hparams, {\'a\': {\'b\': 4}, \'c\': 7})\n    >>> hparams[\'a\'][\'b\'], hparams[\'c\']\n    (4, 7)\n\n    Args:\n        hparams: the original params and also target object\n        updates: new params to be used as update\n\n    """"""\n    for k, v in updates.items():\n        # if missing, add the key\n        if k not in hparams:\n            hparams[k] = v\n            continue\n\n        # recurse if dictionary\n        if isinstance(v, dict):\n            update_hparams(hparams[k], updates[k])\n        else:\n            # update the value\n            hparams.update({k: v})\n\n\ndef load_hparams_from_tags_csv(tags_csv: str) -> Dict[str, Any]:\n    """"""Load hparams from a file.\n\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root=\'./any/path/here\')\n    >>> path_csv = \'./testing-hparams.csv\'\n    >>> save_hparams_to_tags_csv(path_csv, hparams)\n    >>> hparams_new = load_hparams_from_tags_csv(path_csv)\n    >>> vars(hparams) == hparams_new\n    True\n    >>> os.remove(path_csv)\n    """"""\n    if not os.path.isfile(tags_csv):\n        rank_zero_warn(f\'Missing Tags: {tags_csv}.\', RuntimeWarning)\n        return {}\n\n    with open(tags_csv) as fp:\n        csv_reader = csv.reader(fp, delimiter=\',\')\n        tags = {row[0]: convert(row[1]) for row in list(csv_reader)[1:]}\n\n    return tags\n\n\ndef save_hparams_to_tags_csv(tags_csv: str, hparams: Union[dict, Namespace]) -> None:\n    if not os.path.isdir(os.path.dirname(tags_csv)):\n        raise RuntimeError(f\'Missing folder: {os.path.dirname(tags_csv)}.\')\n\n    if isinstance(hparams, Namespace):\n        hparams = vars(hparams)\n\n    with open(tags_csv, \'w\') as fp:\n        fieldnames = [\'key\', \'value\']\n        writer = csv.DictWriter(fp, fieldnames=fieldnames)\n        writer.writerow({\'key\': \'key\', \'value\': \'value\'})\n        for k, v in hparams.items():\n            writer.writerow({\'key\': k, \'value\': v})\n\n\ndef load_hparams_from_yaml(config_yaml: str) -> Dict[str, Any]:\n    """"""Load hparams from a file.\n\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root=\'./any/path/here\')\n    >>> path_yaml = \'./testing-hparams.yaml\'\n    >>> save_hparams_to_yaml(path_yaml, hparams)\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\n    >>> vars(hparams) == hparams_new\n    True\n    >>> os.remove(path_yaml)\n    """"""\n    if not os.path.isfile(config_yaml):\n        rank_zero_warn(f\'Missing Tags: {config_yaml}.\', RuntimeWarning)\n        return {}\n\n    with open(config_yaml) as fp:\n        tags = yaml.load(fp, Loader=yaml.SafeLoader)\n\n    return tags\n\n\ndef save_hparams_to_yaml(config_yaml, hparams: Union[dict, Namespace]) -> None:\n    if not os.path.isdir(os.path.dirname(config_yaml)):\n        raise RuntimeError(f\'Missing folder: {os.path.dirname(config_yaml)}.\')\n\n    if isinstance(hparams, Namespace):\n        hparams = vars(hparams)\n\n    with open(config_yaml, \'w\', newline=\'\') as fp:\n        yaml.dump(hparams, fp)\n\n\ndef convert(val: str) -> Union[int, float, bool, str]:\n    try:\n        return ast.literal_eval(val)\n    except (ValueError, SyntaxError) as err:\n        log.debug(err)\n        return val\n'"
pytorch_lightning/loggers/__init__.py,0,"b'""""""\nLightning supports the most popular logging frameworks (TensorBoard, Comet, Weights and Biases, etc...).\nTo use a logger, simply pass it into the :class:`~pytorch_lightning.trainer.trainer.Trainer`.\nLightning uses TensorBoard by default.\n\n.. code-block:: python\n\n    from pytorch_lightning import Trainer\n    from pytorch_lightning import loggers\n    tb_logger = loggers.TensorBoardLogger(\'logs/\')\n    trainer = Trainer(logger=tb_logger)\n\nChoose from any of the others such as MLflow, Comet, Neptune, WandB, ...\n\n.. code-block:: python\n\n    comet_logger = loggers.CometLogger(save_dir=\'logs/\')\n    trainer = Trainer(logger=comet_logger)\n\nTo use multiple loggers, simply pass in a ``list`` or ``tuple`` of loggers ...\n\n.. code-block:: python\n\n    tb_logger = loggers.TensorBoardLogger(\'logs/\')\n    comet_logger = loggers.CometLogger(save_dir=\'logs/\')\n    trainer = Trainer(logger=[tb_logger, comet_logger])\n\nNote:\n    All loggers log by default to ``os.getcwd()``. To change the path without creating a logger set\n    ``Trainer(default_root_dir=\'/your/path/to/save/checkpoints\')``\n\nCustom Logger\n-------------\n\nYou can implement your own logger by writing a class that inherits from\n:class:`LightningLoggerBase`. Use the :func:`~pytorch_lightning.loggers.base.rank_zero_only`\ndecorator to make sure that only the first process in DDP training logs data.\n\n.. code-block:: python\n\n    from pytorch_lightning.utilities import rank_zero_only\n    from pytorch_lightning.loggers import LightningLoggerBase\n    class MyLogger(LightningLoggerBase):\n\n        @rank_zero_only\n        def log_hyperparams(self, params):\n            # params is an argparse.Namespace\n            # your code to record hyperparameters goes here\n            pass\n\n        @rank_zero_only\n        def log_metrics(self, metrics, step):\n            # metrics is a dictionary of metric names and values\n            # your code to record metrics goes here\n            pass\n\n        def save(self):\n            # Optional. Any code necessary to save logger data goes here\n            pass\n\n        @rank_zero_only\n        def finalize(self, status):\n            # Optional. Any code that needs to be run after training\n            # finishes goes here\n            pass\n\nIf you write a logger that may be useful to others, please send\na pull request to add it to Lighting!\n\nUsing loggers\n-------------\n\nCall the logger anywhere except ``__init__`` in your\n:class:`~pytorch_lightning.core.lightning.LightningModule` by doing:\n\n.. code-block:: python\n\n    from pytorch_lightning import LightningModule\n    class LitModel(LightningModule):\n        def training_step(self, batch, batch_idx):\n            # example\n            self.logger.experiment.whatever_method_summary_writer_supports(...)\n\n            # example if logger is a tensorboard logger\n            self.logger.experiment.add_image(\'images\', grid, 0)\n            self.logger.experiment.add_graph(model, images)\n\n        def any_lightning_module_function_or_hook(self):\n            self.logger.experiment.add_histogram(...)\n\nRead more in the `Experiment Logging use case <./experiment_logging.html>`_.\n\nSupported Loggers\n-----------------\n""""""\nfrom os import environ\n\nfrom pytorch_lightning.loggers.base import LightningLoggerBase, LoggerCollection\nfrom pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n\n__all__ = [\n    \'LightningLoggerBase\',\n    \'LoggerCollection\',\n    \'TensorBoardLogger\',\n]\n\ntry:\n    # needed to prevent ImportError and duplicated logs.\n    environ[""COMET_DISABLE_AUTO_LOGGING""] = ""1""\n\n    from pytorch_lightning.loggers.comet import CometLogger\nexcept ImportError:  # pragma: no-cover\n    del environ[""COMET_DISABLE_AUTO_LOGGING""]  # pragma: no-cover\nelse:\n    __all__.append(\'CometLogger\')\n\ntry:\n    from pytorch_lightning.loggers.mlflow import MLFlowLogger\nexcept ImportError:  # pragma: no-cover\n    pass  # pragma: no-cover\nelse:\n    __all__.append(\'MLFlowLogger\')\n\ntry:\n    from pytorch_lightning.loggers.neptune import NeptuneLogger\nexcept ImportError:  # pragma: no-cover\n    pass  # pragma: no-cover\nelse:\n    __all__.append(\'NeptuneLogger\')\n\ntry:\n    from pytorch_lightning.loggers.test_tube import TestTubeLogger\nexcept ImportError:  # pragma: no-cover\n    pass  # pragma: no-cover\nelse:\n    __all__.append(\'TestTubeLogger\')\n\ntry:\n    from pytorch_lightning.loggers.wandb import WandbLogger\nexcept ImportError:  # pragma: no-cover\n    pass  # pragma: no-cover\nelse:\n    __all__.append(\'WandbLogger\')\n\ntry:\n    from pytorch_lightning.loggers.trains import TrainsLogger\nexcept ImportError:  # pragma: no-cover\n    pass  # pragma: no-cover\nelse:\n    __all__.append(\'TrainsLogger\')\n'"
pytorch_lightning/loggers/base.py,3,"b'import argparse\nimport functools\nimport operator\nfrom abc import ABC, abstractmethod\nfrom argparse import Namespace\nfrom typing import Union, Optional, Dict, Iterable, Any, Callable, List, Sequence, Mapping, Tuple\n\nimport numpy as np\nimport torch\n\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass LightningLoggerBase(ABC):\n    """"""\n    Base class for experiment loggers.\n\n    Args:\n        agg_key_funcs:\n            Dictionary which maps a metric name to a function, which will\n            aggregate the metric values for the same steps.\n        agg_default_func:\n            Default function to aggregate metric values. If some metric name\n            is not presented in the `agg_key_funcs` dictionary, then the\n            `agg_default_func` will be used for aggregation.\n\n    Note:\n        The `agg_key_funcs` and `agg_default_func` arguments are used only when\n        one logs metrics with the :meth:`~LightningLoggerBase.agg_and_log_metrics` method.\n    """"""\n\n    def __init__(\n            self,\n            agg_key_funcs: Optional[Mapping[str, Callable[[Sequence[float]], float]]] = None,\n            agg_default_func: Callable[[Sequence[float]], float] = np.mean\n    ):\n        self._rank = 0\n        self._prev_step: int = -1\n        self._metrics_to_agg: List[Dict[str, float]] = []\n        self._agg_key_funcs = agg_key_funcs if agg_key_funcs else {}\n        self._agg_default_func = agg_default_func\n\n    def update_agg_funcs(\n            self,\n            agg_key_funcs: Optional[Mapping[str, Callable[[Sequence[float]], float]]] = None,\n            agg_default_func: Callable[[Sequence[float]], float] = np.mean\n    ):\n        """"""\n        Update aggregation methods.\n\n        Args:\n            agg_key_funcs:\n                Dictionary which maps a metric name to a function, which will\n                aggregate the metric values for the same steps.\n            agg_default_func:\n                Default function to aggregate metric values. If some metric name\n                is not presented in the `agg_key_funcs` dictionary, then the\n                `agg_default_func` will be used for aggregation.\n        """"""\n        if agg_key_funcs:\n            self._agg_key_funcs.update(agg_key_funcs)\n        if agg_default_func:\n            self._agg_default_func = agg_default_func\n\n    @property\n    @abstractmethod\n    def experiment(self) -> Any:\n        """"""Return the experiment object associated with this logger.""""""\n\n    def _aggregate_metrics(\n            self, metrics: Dict[str, float], step: Optional[int] = None\n    ) -> Tuple[int, Optional[Dict[str, float]]]:\n        """"""\n        Aggregates metrics.\n\n        Args:\n            metrics: Dictionary with metric names as keys and measured quantities as values\n            step: Step number at which the metrics should be recorded\n\n        Returns:\n            Step and aggregated metrics. The return value could be ``None``. In such case, metrics\n            are added to the aggregation list, but not aggregated yet.\n        """"""\n        # if you still receiving metric from the same step, just accumulate it\n        if step == self._prev_step:\n            self._metrics_to_agg.append(metrics)\n            return step, None\n\n        # compute the metrics\n        agg_step, agg_mets = self._reduce_agg_metrics()\n\n        # as new step received reset accumulator\n        self._metrics_to_agg = [metrics]\n        self._prev_step = step\n        return agg_step, agg_mets\n\n    def _reduce_agg_metrics(self):\n        """"""Aggregate accumulated metrics.""""""\n        # compute the metrics\n        if not self._metrics_to_agg:\n            agg_mets = None\n        elif len(self._metrics_to_agg) == 1:\n            agg_mets = self._metrics_to_agg[0]\n        else:\n            agg_mets = merge_dicts(self._metrics_to_agg, self._agg_key_funcs, self._agg_default_func)\n        return self._prev_step, agg_mets\n\n    def _finalize_agg_metrics(self):\n        """"""This shall be called before save/close.""""""\n        agg_step, metrics_to_log = self._reduce_agg_metrics()\n        self._metrics_to_agg = []\n\n        if metrics_to_log is not None:\n            self.log_metrics(metrics=metrics_to_log, step=agg_step)\n\n    def agg_and_log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None):\n        """"""\n        Aggregates and records metrics.\n        This method doesn\'t log the passed metrics instantaneously, but instead\n        it aggregates them and logs only if metrics are ready to be logged.\n\n        Args:\n            metrics: Dictionary with metric names as keys and measured quantities as values\n            step: Step number at which the metrics should be recorded\n        """"""\n        agg_step, metrics_to_log = self._aggregate_metrics(metrics=metrics, step=step)\n\n        if metrics_to_log:\n            self.log_metrics(metrics=metrics_to_log, step=agg_step)\n\n    @abstractmethod\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None):\n        """"""\n        Records metrics.\n        This method logs metrics as as soon as it received them. If you want to aggregate\n        metrics for one specific `step`, use the\n        :meth:`~pytorch_lightning.loggers.base.LightningLoggerBase.agg_and_log_metrics` method.\n\n        Args:\n            metrics: Dictionary with metric names as keys and measured quantities as values\n            step: Step number at which the metrics should be recorded\n        """"""\n        pass\n\n    @staticmethod\n    def _convert_params(params: Union[Dict[str, Any], Namespace]) -> Dict[str, Any]:\n        # in case converting from namespace\n        if isinstance(params, Namespace):\n            params = vars(params)\n\n        if params is None:\n            params = {}\n\n        return params\n\n    @staticmethod\n    def _flatten_dict(params: Dict[str, Any], delimiter: str = \'/\') -> Dict[str, Any]:\n        """"""\n        Flatten hierarchical dict, e.g. ``{\'a\': {\'b\': \'c\'}} -> {\'a/b\': \'c\'}``.\n\n        Args:\n            params: Dictionary containing the hyperparameters\n            delimiter: Delimiter to express the hierarchy. Defaults to ``\'/\'``.\n\n        Returns:\n            Flattened dict.\n\n        Examples:\n            >>> LightningLoggerBase._flatten_dict({\'a\': {\'b\': \'c\'}})\n            {\'a/b\': \'c\'}\n            >>> LightningLoggerBase._flatten_dict({\'a\': {\'b\': 123}})\n            {\'a/b\': 123}\n        """"""\n\n        def _dict_generator(input_dict, prefixes=None):\n            prefixes = prefixes[:] if prefixes else []\n            if isinstance(input_dict, dict):\n                for key, value in input_dict.items():\n                    if isinstance(value, (dict, Namespace)):\n                        value = vars(value) if isinstance(value, Namespace) else value\n                        for d in _dict_generator(value, prefixes + [key]):\n                            yield d\n                    else:\n                        yield prefixes + [key, value if value is not None else str(None)]\n            else:\n                yield prefixes + [input_dict if input_dict is None else str(input_dict)]\n\n        return {delimiter.join(keys): val for *keys, val in _dict_generator(params)}\n\n    @staticmethod\n    def _sanitize_params(params: Dict[str, Any]) -> Dict[str, Any]:\n        """"""\n        Returns params with non-primitvies converted to strings for logging.\n\n        >>> params = {""float"": 0.3,\n        ...           ""int"": 1,\n        ...           ""string"": ""abc"",\n        ...           ""bool"": True,\n        ...           ""list"": [1, 2, 3],\n        ...           ""namespace"": Namespace(foo=3),\n        ...           ""layer"": torch.nn.BatchNorm1d}\n        >>> import pprint\n        >>> pprint.pprint(LightningLoggerBase._sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE\n        {\'bool\': True,\n         \'float\': 0.3,\n         \'int\': 1,\n         \'layer\': ""<class \'torch.nn.modules.batchnorm.BatchNorm1d\'>"",\n         \'list\': \'[1, 2, 3]\',\n         \'namespace\': \'Namespace(foo=3)\',\n         \'string\': \'abc\'}\n        """"""\n        return {k: v if type(v) in [bool, int, float, str, torch.Tensor] else str(v) for k, v in params.items()}\n\n    @abstractmethod\n    def log_hyperparams(self, params: argparse.Namespace):\n        """"""\n        Record hyperparameters.\n\n        Args:\n            params: :class:`~argparse.Namespace` containing the hyperparameters\n        """"""\n\n    def save(self) -> None:\n        """"""Save log data.""""""\n        self._finalize_agg_metrics()\n\n    def finalize(self, status: str) -> None:\n        """"""\n        Do any processing that is necessary to finalize an experiment.\n\n        Args:\n            status: Status that the experiment finished with (e.g. success, failed, aborted)\n        """"""\n        self.save()\n\n    def close(self) -> None:\n        """"""Do any cleanup that is necessary to close an experiment.""""""\n        self.save()\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        """"""Return the experiment name.""""""\n\n    @property\n    @abstractmethod\n    def version(self) -> Union[int, str]:\n        """"""Return the experiment version.""""""\n\n\nclass LoggerCollection(LightningLoggerBase):\n    """"""\n    The :class:`LoggerCollection` class is used to iterate all logging actions over\n    the given `logger_iterable`.\n\n    Args:\n        logger_iterable: An iterable collection of loggers\n    """"""\n\n    def __init__(self, logger_iterable: Iterable[LightningLoggerBase]):\n        super().__init__()\n        self._logger_iterable = logger_iterable\n\n    def __getitem__(self, index: int) -> LightningLoggerBase:\n        return [logger for logger in self._logger_iterable][index]\n\n    @property\n    def experiment(self) -> List[Any]:\n        return [logger.experiment for logger in self._logger_iterable]\n\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n        [logger.log_metrics(metrics, step) for logger in self._logger_iterable]\n\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace]) -> None:\n        [logger.log_hyperparams(params) for logger in self._logger_iterable]\n\n    def save(self) -> None:\n        [logger.save() for logger in self._logger_iterable]\n\n    def finalize(self, status: str) -> None:\n        [logger.finalize(status) for logger in self._logger_iterable]\n\n    def close(self) -> None:\n        [logger.close() for logger in self._logger_iterable]\n\n    @property\n    def name(self) -> str:\n        return \'_\'.join([str(logger.name) for logger in self._logger_iterable])\n\n    @property\n    def version(self) -> str:\n        return \'_\'.join([str(logger.version) for logger in self._logger_iterable])\n\n\nclass DummyExperiment(object):\n    """""" Dummy experiment """"""\n    def nop(*args, **kw):\n        pass\n\n    def __getattr__(self, _):\n        return self.nop\n\n\nclass DummyLogger(LightningLoggerBase):\n    """""" Dummy logger for internal use. Is usefull if we want to disable users\n        logger for a feature, but still secure that users code can run """"""\n    def __init__(self):\n        super().__init__()\n        self._experiment = DummyExperiment()\n\n    @property\n    def experiment(self):\n        return self._experiment\n\n    def log_metrics(self, metrics, step):\n        pass\n\n    def log_hyperparams(self, params):\n        pass\n\n    @property\n    def name(self):\n        pass\n\n    @property\n    def version(self):\n        pass\n\n\ndef merge_dicts(\n        dicts: Sequence[Mapping],\n        agg_key_funcs: Optional[Mapping[str, Callable[[Sequence[float]], float]]] = None,\n        default_func: Callable[[Sequence[float]], float] = np.mean\n) -> Dict:\n    """"""\n    Merge a sequence with dictionaries into one dictionary by aggregating the\n    same keys with some given function.\n\n    Args:\n        dicts:\n            Sequence of dictionaries to be merged.\n        agg_key_funcs:\n            Mapping from key name to function. This function will aggregate a\n            list of values, obtained from the same key of all dictionaries.\n            If some key has no specified aggregation function, the default one\n            will be used. Default is: ``None`` (all keys will be aggregated by the\n            default function).\n        default_func:\n            Default function to aggregate keys, which are not presented in the\n            `agg_key_funcs` map.\n\n    Returns:\n        Dictionary with merged values.\n\n    Examples:\n        >>> import pprint\n        >>> d1 = {\'a\': 1.7, \'b\': 2.0, \'c\': 1, \'d\': {\'d1\': 1, \'d3\': 3}}\n        >>> d2 = {\'a\': 1.1, \'b\': 2.2, \'v\': 1, \'d\': {\'d1\': 2, \'d2\': 3}}\n        >>> d3 = {\'a\': 1.1, \'v\': 2.3, \'d\': {\'d3\': 3, \'d4\': {\'d5\': 1}}}\n        >>> dflt_func = min\n        >>> agg_funcs = {\'a\': np.mean, \'v\': max, \'d\': {\'d1\': sum}}\n        >>> pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func))\n        {\'a\': 1.3,\n         \'b\': 2.0,\n         \'c\': 1,\n         \'d\': {\'d1\': 3, \'d2\': 3, \'d3\': 3, \'d4\': {\'d5\': 1}},\n         \'v\': 2.3}\n    """"""\n    agg_key_funcs = agg_key_funcs or dict()\n    keys = list(functools.reduce(operator.or_, [set(d.keys()) for d in dicts]))\n    d_out = {}\n    for k in keys:\n        fn = agg_key_funcs.get(k)\n        values_to_agg = [v for v in [d_in.get(k) for d_in in dicts] if v is not None]\n\n        if isinstance(values_to_agg[0], dict):\n            d_out[k] = merge_dicts(values_to_agg, fn, default_func)\n        else:\n            d_out[k] = (fn or default_func)(values_to_agg)\n\n    return d_out\n'"
pytorch_lightning/loggers/comet.py,1,"b'""""""\nComet\n-----\n""""""\n\nfrom argparse import Namespace\nfrom typing import Optional, Dict, Union, Any\n\ntry:\n    from comet_ml import Experiment as CometExperiment\n    from comet_ml import ExistingExperiment as CometExistingExperiment\n    from comet_ml import OfflineExperiment as CometOfflineExperiment\n    from comet_ml import BaseExperiment as CometBaseExperiment\n    try:\n        from comet_ml.api import API\n    except ImportError:  # pragma: no-cover\n        # For more information, see: https://www.comet.ml/docs/python-sdk/releases/#release-300\n        from comet_ml.papi import API  # pragma: no-cover\n\n    _COMET_AVAILABLE = True\nexcept ImportError:  # pragma: no-cover\n    CometExperiment = None\n    CometExistingExperiment = None\n    CometOfflineExperiment = None\n    CometBaseExperiment = None\n    API = None\n    _COMET_AVAILABLE = False\n\n\nimport torch\nfrom torch import is_tensor\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.loggers.base import LightningLoggerBase\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass CometLogger(LightningLoggerBase):\n    r""""""\n    Log using `Comet.ml <https://www.comet.ml>`_. Install it with pip:\n\n    .. code-block:: bash\n\n        pip install comet-ml\n\n    Comet requires either an API Key (online mode) or a local directory path (offline mode).\n\n    **ONLINE MODE**\n\n    Example:\n        >>> import os\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.loggers import CometLogger\n        >>> # arguments made to CometLogger are passed on to the comet_ml.Experiment class\n        >>> comet_logger = CometLogger(\n        ...     api_key=os.environ.get(\'COMET_API_KEY\'),\n        ...     workspace=os.environ.get(\'COMET_WORKSPACE\'),  # Optional\n        ...     save_dir=\'.\',  # Optional\n        ...     project_name=\'default_project\',  # Optional\n        ...     rest_api_key=os.environ.get(\'COMET_REST_API_KEY\'),  # Optional\n        ...     experiment_name=\'default\'  # Optional\n        ... )\n        >>> trainer = Trainer(logger=comet_logger)\n\n    **OFFLINE MODE**\n\n    Example:\n        >>> from pytorch_lightning.loggers import CometLogger\n        >>> # arguments made to CometLogger are passed on to the comet_ml.Experiment class\n        >>> comet_logger = CometLogger(\n        ...     save_dir=\'.\',\n        ...     workspace=os.environ.get(\'COMET_WORKSPACE\'),  # Optional\n        ...     project_name=\'default_project\',  # Optional\n        ...     rest_api_key=os.environ.get(\'COMET_REST_API_KEY\'),  # Optional\n        ...     experiment_name=\'default\'  # Optional\n        ... )\n        >>> trainer = Trainer(logger=comet_logger)\n\n    Args:\n        api_key: Required in online mode. API key, found on Comet.ml\n        save_dir: Required in offline mode. The path for the directory to save local comet logs\n        workspace: Optional. Name of workspace for this user\n        project_name: Optional. Send your experiment to a specific project.\n            Otherwise will be sent to Uncategorized Experiments.\n            If the project name does not already exist, Comet.ml will create a new project.\n        rest_api_key: Optional. Rest API key found in Comet.ml settings.\n            This is used to determine version number\n        experiment_name: Optional. String representing the name for this particular experiment on Comet.ml.\n        experiment_key: Optional. If set, restores from existing experiment.\n    """"""\n\n    def __init__(self,\n                 api_key: Optional[str] = None,\n                 save_dir: Optional[str] = None,\n                 workspace: Optional[str] = None,\n                 project_name: Optional[str] = None,\n                 rest_api_key: Optional[str] = None,\n                 experiment_name: Optional[str] = None,\n                 experiment_key: Optional[str] = None,\n                 **kwargs):\n\n        if not _COMET_AVAILABLE:\n            raise ImportError(\'You want to use `comet_ml` logger which is not installed yet,\'\n                              \' install it with `pip install comet-ml`.\')\n        super().__init__()\n        self._experiment = None\n\n        # Determine online or offline mode based on which arguments were passed to CometLogger\n        if api_key is not None:\n            self.mode = ""online""\n            self.api_key = api_key\n        elif save_dir is not None:\n            self.mode = ""offline""\n            self.save_dir = save_dir\n        else:\n            # If neither api_key nor save_dir are passed as arguments, raise an exception\n            raise MisconfigurationException(""CometLogger requires either api_key or save_dir during initialization."")\n\n        log.info(f""CometLogger will be initialized in {self.mode} mode"")\n\n        self.workspace = workspace\n        self.project_name = project_name\n        self.experiment_key = experiment_key\n        self._kwargs = kwargs\n\n        if rest_api_key is not None:\n            # Comet.ml rest API, used to determine version number\n            self.rest_api_key = rest_api_key\n            self.comet_api = API(self.rest_api_key)\n        else:\n            self.rest_api_key = None\n            self.comet_api = None\n\n        if experiment_name:\n            try:\n                self.name = experiment_name\n            except TypeError:\n                log.exception(""Failed to set experiment name for comet.ml logger"")\n        self._kwargs = kwargs\n\n    @property\n    def experiment(self) -> CometBaseExperiment:\n        r""""""\n        Actual Comet object. To use Comet features in your\n        :class:`~pytorch_lightning.core.lightning.LightningModule` do the following.\n\n        Example::\n\n            self.logger.experiment.some_comet_function()\n\n        """"""\n        if self._experiment is not None:\n            return self._experiment\n\n        if self.mode == ""online"":\n            if self.experiment_key is None:\n                self._experiment = CometExperiment(\n                    api_key=self.api_key,\n                    workspace=self.workspace,\n                    project_name=self.project_name,\n                    **self._kwargs\n                )\n                self.experiment_key = self._experiment.get_key()\n            else:\n                self._experiment = CometExistingExperiment(\n                    api_key=self.api_key,\n                    workspace=self.workspace,\n                    project_name=self.project_name,\n                    previous_experiment=self.experiment_key,\n                    **self._kwargs\n                )\n        else:\n            self._experiment = CometOfflineExperiment(\n                offline_directory=self.save_dir,\n                workspace=self.workspace,\n                project_name=self.project_name,\n                **self._kwargs\n            )\n\n        return self._experiment\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace]) -> None:\n        params = self._convert_params(params)\n        params = self._flatten_dict(params)\n        self.experiment.log_parameters(params)\n\n    @rank_zero_only\n    def log_metrics(\n            self,\n            metrics: Dict[str, Union[torch.Tensor, float]],\n            step: Optional[int] = None\n    ) -> None:\n        # Comet.ml expects metrics to be a dictionary of detached tensors on CPU\n        for key, val in metrics.items():\n            if is_tensor(val):\n                metrics[key] = val.cpu().detach()\n\n        self.experiment.log_metrics(metrics, step=step)\n\n    def reset_experiment(self):\n        self._experiment = None\n\n    @rank_zero_only\n    def finalize(self, status: str) -> None:\n        r""""""\n        When calling ``self.experiment.end()``, that experiment won\'t log any more data to Comet.\n        That\'s why, if you need to log any more data, you need to create an ExistingCometExperiment.\n        For example, to log data when testing your model after training, because when training is\n        finalized :meth:`CometLogger.finalize` is called.\n\n        This happens automatically in the :meth:`~CometLogger.experiment` property, when\n        ``self._experiment`` is set to ``None``, i.e. ``self.reset_experiment()``.\n        """"""\n        self.experiment.end()\n        self.reset_experiment()\n\n    @property\n    def name(self) -> str:\n        return str(self.experiment.project_name)\n\n    @name.setter\n    def name(self, value: str) -> None:\n        self.experiment.set_name(value)\n\n    @property\n    def version(self) -> str:\n        return self.experiment.id\n'"
pytorch_lightning/loggers/mlflow.py,0,"b'""""""\nMLflow\n------\n""""""\nimport os\nfrom argparse import Namespace\nfrom time import time\nfrom typing import Optional, Dict, Any, Union\n\ntry:\n    import mlflow\n    from mlflow.tracking import MlflowClient\n    _MLFLOW_AVAILABLE = True\nexcept ImportError:  # pragma: no-cover\n    mlflow = None\n    MlflowClient = None\n    _MLFLOW_AVAILABLE = False\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.loggers.base import LightningLoggerBase\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass MLFlowLogger(LightningLoggerBase):\n    """"""\n    Log using `MLflow <https://mlflow.org>`_. Install it with pip:\n\n    .. code-block:: bash\n\n        pip install mlflow\n\n    Example:\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.loggers import MLFlowLogger\n        >>> mlf_logger = MLFlowLogger(\n        ...     experiment_name=""default"",\n        ...     tracking_uri=""file:./ml-runs""\n        ... )\n        >>> trainer = Trainer(logger=mlf_logger)\n\n    Use the logger anywhere in you :class:`~pytorch_lightning.core.lightning.LightningModule` as follows:\n\n    >>> from pytorch_lightning import LightningModule\n    >>> class LitModel(LightningModule):\n    ...     def training_step(self, batch, batch_idx):\n    ...         # example\n    ...         self.logger.experiment.whatever_ml_flow_supports(...)\n    ...\n    ...     def any_lightning_module_function_or_hook(self):\n    ...         self.logger.experiment.whatever_ml_flow_supports(...)\n\n    Args:\n        experiment_name: The name of the experiment\n        tracking_uri: Address of local or remote tracking server.\n            If not provided, defaults to the service set by ``mlflow.tracking.set_tracking_uri``.\n        tags: A dictionary tags for the experiment.\n\n    """"""\n\n    def __init__(self,\n                 experiment_name: str = \'default\',\n                 tracking_uri: Optional[str] = None,\n                 tags: Optional[Dict[str, Any]] = None,\n                 save_dir: Optional[str] = None):\n\n        if not _MLFLOW_AVAILABLE:\n            raise ImportError(\'You want to use `mlflow` logger which is not installed yet,\'\n                              \' install it with `pip install mlflow`.\')\n        super().__init__()\n        if not tracking_uri and save_dir:\n            tracking_uri = f\'file:{os.sep * 2}{save_dir}\'\n        self._mlflow_client = MlflowClient(tracking_uri)\n        self.experiment_name = experiment_name\n        self._run_id = None\n        self.tags = tags\n\n    @property\n    def experiment(self) -> MlflowClient:\n        r""""""\n        Actual MLflow object. To use mlflow features in your\n        :class:`~pytorch_lightning.core.lightning.LightningModule` do the following.\n\n        Example::\n\n            self.logger.experiment.some_mlflow_function()\n\n        """"""\n        return self._mlflow_client\n\n    @property\n    def run_id(self):\n        if self._run_id is not None:\n            return self._run_id\n\n        expt = self._mlflow_client.get_experiment_by_name(self.experiment_name)\n\n        if expt:\n            self._expt_id = expt.experiment_id\n        else:\n            log.warning(f\'Experiment with name {self.experiment_name} not found. Creating it.\')\n            self._expt_id = self._mlflow_client.create_experiment(name=self.experiment_name)\n\n        run = self._mlflow_client.create_run(experiment_id=self._expt_id, tags=self.tags)\n        self._run_id = run.info.run_id\n        return self._run_id\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace]) -> None:\n        params = self._convert_params(params)\n        params = self._flatten_dict(params)\n        for k, v in params.items():\n            self.experiment.log_param(self.run_id, k, v)\n\n    @rank_zero_only\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n        timestamp_ms = int(time() * 1000)\n        for k, v in metrics.items():\n            if isinstance(v, str):\n                log.warning(f\'Discarding metric with string value {k}={v}.\')\n                continue\n            self.experiment.log_metric(self.run_id, k, v, timestamp_ms, step)\n\n    @rank_zero_only\n    def finalize(self, status: str = \'FINISHED\') -> None:\n        super().finalize(status)\n        if status == \'success\':\n            status = \'FINISHED\'\n        self.experiment.set_terminated(self.run_id, status)\n\n    @property\n    def name(self) -> str:\n        return self.experiment_name\n\n    @property\n    def version(self) -> str:\n        return self._run_id\n'"
pytorch_lightning/loggers/neptune.py,2,"b'""""""\nNeptune\n-------\n""""""\nfrom argparse import Namespace\nfrom typing import Optional, List, Dict, Any, Union, Iterable\n\nfrom PIL.Image import Image\n\ntry:\n    import neptune\n    from neptune.experiments import Experiment\n    _NEPTUNE_AVAILABLE = True\nexcept ImportError:  # pragma: no-cover\n    neptune = None\n    Experiment = None\n    _NEPTUNE_AVAILABLE = False\n\nimport torch\nfrom torch import is_tensor\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.loggers.base import LightningLoggerBase\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass NeptuneLogger(LightningLoggerBase):\n    r""""""\n    Log using `Neptune <https://neptune.ai>`_. Install it with pip:\n\n    .. code-block:: bash\n\n        pip install neptune-client\n\n    The Neptune logger can be used in the online mode or offline (silent) mode.\n    To log experiment data in online mode, :class:`NeptuneLogger` requires an API key.\n    In offline mode, the logger does not connect to Neptune.\n\n    **ONLINE MODE**\n\n    Example:\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.loggers import NeptuneLogger\n        >>> # arguments made to NeptuneLogger are passed on to the neptune.experiments.Experiment class\n        >>> # We are using an api_key for the anonymous user ""neptuner"" but you can use your own.\n        >>> neptune_logger = NeptuneLogger(\n        ...     api_key=\'ANONYMOUS\',\n        ...     project_name=\'shared/pytorch-lightning-integration\',\n        ...     experiment_name=\'default\',  # Optional,\n        ...     params={\'max_epochs\': 10},  # Optional,\n        ...     tags=[\'pytorch-lightning\', \'mlp\']  # Optional,\n        ... )\n        >>> trainer = Trainer(max_epochs=10, logger=neptune_logger)\n\n    **OFFLINE MODE**\n\n    Example:\n        >>> from pytorch_lightning.loggers import NeptuneLogger\n        >>> # arguments made to NeptuneLogger are passed on to the neptune.experiments.Experiment class\n        >>> neptune_logger = NeptuneLogger(\n        ...     offline_mode=True,\n        ...     project_name=\'USER_NAME/PROJECT_NAME\',\n        ...     experiment_name=\'default\',  # Optional,\n        ...     params={\'max_epochs\': 10},  # Optional,\n        ...     tags=[\'pytorch-lightning\', \'mlp\']  # Optional,\n        ... )\n        >>> trainer = Trainer(max_epochs=10, logger=neptune_logger)\n\n    Use the logger anywhere in you :class:`~pytorch_lightning.core.lightning.LightningModule` as follows:\n\n    >>> from pytorch_lightning import LightningModule\n    >>> class LitModel(LightningModule):\n    ...     def training_step(self, batch, batch_idx):\n    ...         # log metrics\n    ...         self.logger.experiment.log_metric(\'acc_train\', ...)\n    ...         # log images\n    ...         self.logger.experiment.log_image(\'worse_predictions\', ...)\n    ...         # log model checkpoint\n    ...         self.logger.experiment.log_artifact(\'model_checkpoint.pt\', ...)\n    ...         self.logger.experiment.whatever_neptune_supports(...)\n    ...\n    ...     def any_lightning_module_function_or_hook(self):\n    ...         self.logger.experiment.log_metric(\'acc_train\', ...)\n    ...         self.logger.experiment.log_image(\'worse_predictions\', ...)\n    ...         self.logger.experiment.log_artifact(\'model_checkpoint.pt\', ...)\n    ...         self.logger.experiment.whatever_neptune_supports(...)\n\n    If you want to log objects after the training is finished use ``close_after_fit=False``:\n\n    .. code-block:: python\n\n        neptune_logger = NeptuneLogger(\n            ...\n            close_after_fit=False,\n            ...\n        )\n        trainer = Trainer(logger=neptune_logger)\n        trainer.fit()\n\n        # Log test metrics\n        trainer.test(model)\n\n        # Log additional metrics\n        from sklearn.metrics import accuracy_score\n\n        accuracy = accuracy_score(y_true, y_pred)\n        neptune_logger.experiment.log_metric(\'test_accuracy\', accuracy)\n\n        # Log charts\n        from scikitplot.metrics import plot_confusion_matrix\n        import matplotlib.pyplot as plt\n\n        fig, ax = plt.subplots(figsize=(16, 12))\n        plot_confusion_matrix(y_true, y_pred, ax=ax)\n        neptune_logger.experiment.log_image(\'confusion_matrix\', fig)\n\n        # Save checkpoints folder\n        neptune_logger.experiment.log_artifact(\'my/checkpoints\')\n\n        # When you are done, stop the experiment\n        neptune_logger.experiment.stop()\n\n    See Also:\n        - An `Example experiment <https://ui.neptune.ai/o/shared/org/\n          pytorch-lightning-integration/e/PYTOR-66/charts>`_ showing the UI of Neptune.\n        - `Tutorial <https://docs.neptune.ai/integrations/pytorch_lightning.html>`_ on how to use\n          Pytorch Lightning with Neptune.\n\n    Args:\n        api_key: Required in online mode.\n            Neptune API token, found on https://neptune.ai.\n            Read how to get your\n            `API key <https://docs.neptune.ai/python-api/tutorials/get-started.html#copy-api-token>`_.\n            It is recommended to keep it in the `NEPTUNE_API_TOKEN`\n            environment variable and then you can leave ``api_key=None``.\n        project_name: Required in online mode. Qualified name of a project in a form of\n            ""namespace/project_name"" for example ""tom/minst-classification"".\n            If ``None``, the value of `NEPTUNE_PROJECT` environment variable will be taken.\n            You need to create the project in https://neptune.ai first.\n        offline_mode: Optional default ``False``. If ``True`` no logs will be sent\n            to Neptune. Usually used for debug purposes.\n        close_after_fit: Optional default ``True``. If ``False`` the experiment\n            will not be closed after training and additional metrics,\n            images or artifacts can be logged. Also, remember to close the experiment explicitly\n            by running ``neptune_logger.experiment.stop()``.\n        experiment_name: Optional. Editable name of the experiment.\n            Name is displayed in the experiment\xe2\x80\x99s Details (Metadata section) and\n            in experiments view as a column.\n        upload_source_files: Optional. List of source files to be uploaded.\n            Must be list of str or single str. Uploaded sources are displayed\n            in the experiment\xe2\x80\x99s Source code tab.\n            If ``None`` is passed, the Python file from which the experiment was created will be uploaded.\n            Pass an empty list (``[]``) to upload no files.\n            Unix style pathname pattern expansion is supported.\n            For example, you can pass ``\'\\*.py\'``\n            to upload all python source files from the current directory.\n            For recursion lookup use ``\'\\**/\\*.py\'`` (for Python 3.5 and later).\n            For more information see :mod:`glob` library.\n        params: Optional. Parameters of the experiment.\n            After experiment creation params are read-only.\n            Parameters are displayed in the experiment\xe2\x80\x99s Parameters section and\n            each key-value pair can be viewed in the experiments view as a column.\n        properties: Optional. Default is ``{}``. Properties of the experiment.\n            They are editable after the experiment is created.\n            Properties are displayed in the experiment\xe2\x80\x99s Details section and\n            each key-value pair can be viewed in the experiments view as a column.\n        tags: Optional. Default is ``[]``. Must be list of str. Tags of the experiment.\n            They are editable after the experiment is created (see: ``append_tag()`` and ``remove_tag()``).\n            Tags are displayed in the experiment\xe2\x80\x99s Details section and can be viewed\n            in the experiments view as a column.\n    """"""\n\n    def __init__(self,\n                 api_key: Optional[str] = None,\n                 project_name: Optional[str] = None,\n                 close_after_fit: Optional[bool] = True,\n                 offline_mode: bool = False,\n                 experiment_name: Optional[str] = None,\n                 upload_source_files: Optional[List[str]] = None,\n                 params: Optional[Dict[str, Any]] = None,\n                 properties: Optional[Dict[str, Any]] = None,\n                 tags: Optional[List[str]] = None,\n                 **kwargs):\n        if not _NEPTUNE_AVAILABLE:\n            raise ImportError(\'You want to use `neptune` logger which is not installed yet,\'\n                              \' install it with `pip install neptune-client`.\')\n        super().__init__()\n        self.api_key = api_key\n        self.project_name = project_name\n        self.offline_mode = offline_mode\n        self.close_after_fit = close_after_fit\n        self.experiment_name = experiment_name\n        self.upload_source_files = upload_source_files\n        self.params = params\n        self.properties = properties\n        self.tags = tags\n        self._kwargs = kwargs\n        self._experiment_id = None\n        self._experiment = self._create_or_get_experiment()\n\n        log.info(f\'NeptuneLogger will work in {""offline"" if self.offline_mode else ""online""} mode\')\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n\n        # Experiment cannot be pickled, and additionally its ID cannot be pickled in offline mode\n        state[\'_experiment\'] = None\n        if self.offline_mode:\n            state[\'_experiment_id\'] = None\n\n        return state\n\n    @property\n    def experiment(self) -> Experiment:\n        r""""""\n        Actual Neptune object. To use neptune features in your\n        :class:`~pytorch_lightning.core.lightning.LightningModule` do the following.\n\n        Example::\n\n            self.logger.experiment.some_neptune_function()\n\n        """"""\n\n        # Note that even though we initialize self._experiment in __init__,\n        # it may still end up being None after being pickled and un-pickled\n        if self._experiment is None:\n            self._experiment = self._create_or_get_experiment()\n\n        return self._experiment\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace]) -> None:\n        params = self._convert_params(params)\n        params = self._flatten_dict(params)\n        for key, val in params.items():\n            self.experiment.set_property(f\'param__{key}\', val)\n\n    @rank_zero_only\n    def log_metrics(\n            self,\n            metrics: Dict[str, Union[torch.Tensor, float]],\n            step: Optional[int] = None\n    ) -> None:\n        """"""\n        Log metrics (numeric values) in Neptune experiments.\n\n        Args:\n            metrics: Dictionary with metric names as keys and measured quantities as values\n            step: Step number at which the metrics should be recorded, must be strictly increasing\n        """"""\n        for key, val in metrics.items():\n            self.log_metric(key, val, step=step)\n\n    @rank_zero_only\n    def finalize(self, status: str) -> None:\n        super().finalize(status)\n        if self.close_after_fit:\n            self.experiment.stop()\n\n    @property\n    def name(self) -> str:\n        if self.offline_mode:\n            return \'offline-name\'\n        else:\n            return self.experiment.name\n\n    @property\n    def version(self) -> str:\n        if self.offline_mode:\n            return \'offline-id-1234\'\n        else:\n            return self.experiment.id\n\n    @rank_zero_only\n    def log_metric(\n            self,\n            metric_name: str,\n            metric_value: Union[torch.Tensor, float, str],\n            step: Optional[int] = None\n    ) -> None:\n        """"""\n        Log metrics (numeric values) in Neptune experiments.\n\n        Args:\n            metric_name: The name of log, i.e. mse, loss, accuracy.\n            metric_value: The value of the log (data-point).\n            step: Step number at which the metrics should be recorded, must be strictly increasing\n        """"""\n        if is_tensor(metric_value):\n            metric_value = metric_value.cpu().detach()\n\n        if step is None:\n            self.experiment.log_metric(metric_name, metric_value)\n        else:\n            self.experiment.log_metric(metric_name, x=step, y=metric_value)\n\n    @rank_zero_only\n    def log_text(self, log_name: str, text: str, step: Optional[int] = None) -> None:\n        """"""\n        Log text data in Neptune experiments.\n\n        Args:\n            log_name: The name of log, i.e. mse, my_text_data, timing_info.\n            text: The value of the log (data-point).\n            step: Step number at which the metrics should be recorded, must be strictly increasing\n        """"""\n        self.log_metric(log_name, text, step=step)\n\n    @rank_zero_only\n    def log_image(self,\n                  log_name: str,\n                  image: Union[str, Image, Any],\n                  step: Optional[int] = None) -> None:\n        """"""\n        Log image data in Neptune experiment\n\n        Args:\n            log_name: The name of log, i.e. bboxes, visualisations, sample_images.\n            image: The value of the log (data-point).\n                Can be one of the following types: PIL image, `matplotlib.figure.Figure`,\n                path to image file (str)\n            step: Step number at which the metrics should be recorded, must be strictly increasing\n        """"""\n        if step is None:\n            self.experiment.log_image(log_name, image)\n        else:\n            self.experiment.log_image(log_name, x=step, y=image)\n\n    @rank_zero_only\n    def log_artifact(self, artifact: str, destination: Optional[str] = None) -> None:\n        """"""Save an artifact (file) in Neptune experiment storage.\n\n        Args:\n            artifact: A path to the file in local filesystem.\n            destination: Optional. Default is ``None``. A destination path.\n                If ``None`` is passed, an artifact file name will be used.\n        """"""\n        self.experiment.log_artifact(artifact, destination)\n\n    @rank_zero_only\n    def set_property(self, key: str, value: Any) -> None:\n        """"""\n        Set key-value pair as Neptune experiment property.\n\n        Args:\n            key: Property key.\n            value: New value of a property.\n        """"""\n        self.experiment.set_property(key, value)\n\n    @rank_zero_only\n    def append_tags(self, tags: Union[str, Iterable[str]]) -> None:\n        """"""\n        Appends tags to the neptune experiment.\n\n        Args:\n            tags: Tags to add to the current experiment. If str is passed, a single tag is added.\n                If multiple - comma separated - str are passed, all of them are added as tags.\n                If list of str is passed, all elements of the list are added as tags.\n        """"""\n        if str(tags) == tags:\n            tags = [tags]  # make it as an iterable is if it is not yet\n        self.experiment.append_tags(*tags)\n\n    def _create_or_get_experiment(self):\n        if self.offline_mode:\n            project = neptune.Session(backend=neptune.OfflineBackend()).get_project(\'dry-run/project\')\n        else:\n            session = neptune.Session.with_default_backend(api_token=self.api_key)\n            project = session.get_project(self.project_name)\n\n        if self._experiment_id is None:\n            exp = project.create_experiment(\n                name=self.experiment_name,\n                params=self.params,\n                properties=self.properties,\n                tags=self.tags,\n                upload_source_files=self.upload_source_files,\n                **self._kwargs)\n        else:\n            exp = project.get_experiments(id=self._experiment_id)[0]\n\n        self._experiment_id = exp.id\n        return exp\n'"
pytorch_lightning/loggers/tensorboard.py,6,"b'""""""\nTensorBoard\n-----------\n""""""\n\nimport os\nimport yaml\nfrom argparse import Namespace\nfrom typing import Optional, Dict, Union, Any\nfrom warnings import warn\n\nimport torch\nfrom pkg_resources import parse_version\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.core.saving import save_hparams_to_yaml\nfrom pytorch_lightning.loggers.base import LightningLoggerBase\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass TensorBoardLogger(LightningLoggerBase):\n    r""""""\n    Log to local file system in `TensorBoard <https://www.tensorflow.org/tensorboard>`_ format.\n    Implemented using :class:`~torch.utils.tensorboard.SummaryWriter`. Logs are saved to\n    ``os.path.join(save_dir, name, version)``. This is the default logger in Lightning, it comes\n    preinstalled.\n\n    Example:\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.loggers import TensorBoardLogger\n        >>> logger = TensorBoardLogger(""tb_logs"", name=""my_model"")\n        >>> trainer = Trainer(logger=logger)\n\n    Args:\n        save_dir: Save directory\n        name: Experiment name. Defaults to ``\'default\'``. If it is the empty string then no per-experiment\n            subdirectory is used.\n        version: Experiment version. If version is not specified the logger inspects the save\n            directory for existing versions, then automatically assigns the next available version.\n            If it is a string then it is used as the run-specific subdirectory name,\n            otherwise ``\'version_${version}\'`` is used.\n        \\**kwargs: Other arguments are passed directly to the :class:`SummaryWriter` constructor.\n\n    """"""\n    NAME_HPARAMS_FILE = \'hparams.yaml\'\n\n    def __init__(self,\n                 save_dir: str,\n                 name: Optional[str] = ""default"",\n                 version: Optional[Union[int, str]] = None,\n                 **kwargs):\n        super().__init__()\n        self.save_dir = save_dir\n        self._name = name\n        self._version = version\n\n        self._experiment = None\n        self.hparams = {}\n        self._kwargs = kwargs\n\n    @property\n    def root_dir(self) -> str:\n        """"""\n        Parent directory for all tensorboard checkpoint subdirectories.\n        If the experiment name parameter is ``None`` or the empty string, no experiment subdirectory is used\n        and the checkpoint will be saved in ""save_dir/version_dir""\n        """"""\n        if self.name is None or len(self.name) == 0:\n            return self.save_dir\n        else:\n            return os.path.join(self.save_dir, self.name)\n\n    @property\n    def log_dir(self) -> str:\n        """"""\n        The directory for this run\'s tensorboard checkpoint. By default, it is named\n        ``\'version_${self.version}\'`` but it can be overridden by passing a string value\n        for the constructor\'s version parameter instead of ``None`` or an int.\n        """"""\n        # create a pseudo standard path ala test-tube\n        version = self.version if isinstance(self.version, str) else f""version_{self.version}""\n        log_dir = os.path.join(self.root_dir, version)\n        return log_dir\n\n    @property\n    def experiment(self) -> SummaryWriter:\n        r""""""\n        Actual tensorboard object. To use TensorBoard features in your\n        :class:`~pytorch_lightning.core.lightning.LightningModule` do the following.\n\n        Example::\n\n            self.logger.experiment.some_tensorboard_function()\n\n        """"""\n        if self._experiment is not None:\n            return self._experiment\n\n        os.makedirs(self.root_dir, exist_ok=True)\n        self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)\n        return self._experiment\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace],\n                        metrics: Optional[Dict[str, Any]] = None) -> None:\n        params = self._convert_params(params)\n\n        # store params to output\n        self.hparams.update(params)\n\n        # format params into the suitable for tensorboard\n        params = self._flatten_dict(params)\n        params = self._sanitize_params(params)\n\n        if parse_version(torch.__version__) < parse_version(""1.3.0""):\n            warn(\n                f""Hyperparameter logging is not available for Torch version {torch.__version__}.""\n                "" Skipping log_hyperparams. Upgrade to Torch 1.3.0 or above to enable""\n                "" hyperparameter logging.""\n            )\n        else:\n            from torch.utils.tensorboard.summary import hparams\n\n            if metrics is None:\n                metrics = {}\n            exp, ssi, sei = hparams(params, metrics)\n            writer = self.experiment._get_file_writer()\n            writer.add_summary(exp)\n            writer.add_summary(ssi)\n            writer.add_summary(sei)\n\n            if metrics:\n                # necessary for hparam comparison with metrics\n                self.log_metrics(metrics)\n\n    @rank_zero_only\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n        for k, v in metrics.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            self.experiment.add_scalar(k, v, step)\n\n    @rank_zero_only\n    def save(self) -> None:\n        super().save()\n        dir_path = self.log_dir\n        if not os.path.isdir(dir_path):\n            dir_path = self.save_dir\n\n        # prepare the file path\n        hparams_file = os.path.join(dir_path, self.NAME_HPARAMS_FILE)\n\n        # save the metatags file\n        save_hparams_to_yaml(hparams_file, self.hparams)\n\n    @rank_zero_only\n    def finalize(self, status: str) -> None:\n        self.save()\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    @property\n    def version(self) -> int:\n        if self._version is None:\n            self._version = self._get_next_version()\n        return self._version\n\n    def _get_next_version(self):\n        root_dir = os.path.join(self.save_dir, self.name)\n\n        if not os.path.isdir(root_dir):\n            log.warning(\'Missing logger folder: %s\', root_dir)\n            return 0\n\n        existing_versions = []\n        for d in os.listdir(root_dir):\n            if os.path.isdir(os.path.join(root_dir, d)) and d.startswith(""version_""):\n                existing_versions.append(int(d.split(""_"")[1]))\n\n        if len(existing_versions) == 0:\n            return 0\n\n        return max(existing_versions) + 1\n'"
pytorch_lightning/loggers/test_tube.py,0,"b'""""""\nTest Tube\n---------\n""""""\nfrom argparse import Namespace\nfrom typing import Optional, Dict, Any, Union\n\ntry:\n    from test_tube import Experiment\n    _TEST_TUBE_AVAILABLE = True\nexcept ImportError:  # pragma: no-cover\n    Experiment = None\n    _TEST_TUBE_AVAILABLE = False\n\nfrom pytorch_lightning.loggers.base import LightningLoggerBase\nfrom pytorch_lightning.utilities.distributed import rank_zero_only\n\n\nclass TestTubeLogger(LightningLoggerBase):\n    r""""""\n    Log to local file system in `TensorBoard <https://www.tensorflow.org/tensorboard>`_ format\n    but using a nicer folder structure (see `full docs <https://williamfalcon.github.io/test-tube>`_).\n    Install it with pip:\n\n    .. code-block:: bash\n\n        pip install test_tube\n\n    Example:\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.loggers import TestTubeLogger\n        >>> logger = TestTubeLogger(""tt_logs"", name=""my_exp_name"")\n        >>> trainer = Trainer(logger=logger)\n\n    Use the logger anywhere in your :class:`~pytorch_lightning.core.lightning.LightningModule` as follows:\n\n    >>> from pytorch_lightning import LightningModule\n    >>> class LitModel(LightningModule):\n    ...     def training_step(self, batch, batch_idx):\n    ...         # example\n    ...         self.logger.experiment.whatever_method_summary_writer_supports(...)\n    ...\n    ...     def any_lightning_module_function_or_hook(self):\n    ...         self.logger.experiment.add_histogram(...)\n\n    Args:\n        save_dir: Save directory\n        name: Experiment name. Defaults to ``\'default\'``.\n        description: A short snippet about this experiment\n        debug: If ``True``, it doesn\'t log anything.\n        version: Experiment version. If version is not specified the logger inspects the save\n            directory for existing versions, then automatically assigns the next available version.\n        create_git_tag: If ``True`` creates a git tag to save the code used in this experiment.\n\n    """"""\n\n    __test__ = False\n\n    def __init__(self,\n                 save_dir: str,\n                 name: str = ""default"",\n                 description: Optional[str] = None,\n                 debug: bool = False,\n                 version: Optional[int] = None,\n                 create_git_tag: bool = False):\n\n        if not _TEST_TUBE_AVAILABLE:\n            raise ImportError(\'You want to use `test_tube` logger which is not installed yet,\'\n                              \' install it with `pip install test-tube`.\')\n        super().__init__()\n        self.save_dir = save_dir\n        self._name = name\n        self.description = description\n        self.debug = debug\n        self._version = version\n        self.create_git_tag = create_git_tag\n        self._experiment = None\n\n    @property\n    def experiment(self) -> Experiment:\n        r""""""\n\n        Actual TestTube object. To use TestTube features in your\n        :class:`~pytorch_lightning.core.lightning.LightningModule` do the following.\n\n        Example::\n\n            self.logger.experiment.some_test_tube_function()\n\n        """"""\n        if self._experiment is not None:\n            return self._experiment\n\n        self._experiment = Experiment(\n            save_dir=self.save_dir,\n            name=self._name,\n            debug=self.debug,\n            version=self.version,\n            description=self.description,\n            create_git_tag=self.create_git_tag,\n            rank=rank_zero_only.rank,\n        )\n        return self._experiment\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace]) -> None:\n        # TODO: HACK figure out where this is being set to true\n        self.experiment.debug = self.debug\n        params = self._convert_params(params)\n        params = self._flatten_dict(params)\n        self.experiment.argparse(Namespace(**params))\n\n    @rank_zero_only\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n        # TODO: HACK figure out where this is being set to true\n        self.experiment.debug = self.debug\n        self.experiment.log(metrics, global_step=step)\n\n    @rank_zero_only\n    def save(self) -> None:\n        super().save()\n        # TODO: HACK figure out where this is being set to true\n        self.experiment.debug = self.debug\n        self.experiment.save()\n\n    @rank_zero_only\n    def finalize(self, status: str) -> None:\n        super().finalize(status)\n        # TODO: HACK figure out where this is being set to true\n        self.experiment.debug = self.debug\n        self.save()\n        self.close()\n\n    @rank_zero_only\n    def close(self) -> None:\n        super().save()\n        # TODO: HACK figure out where this is being set to true\n        self.experiment.debug = self.debug\n        if not self.debug:\n            exp = self.experiment\n            exp.close()\n\n    @property\n    def name(self) -> str:\n        if self._experiment is None:\n            return self._name\n        else:\n            return self.experiment.name\n\n    @property\n    def version(self) -> int:\n        if self._experiment is None:\n            return self._version\n        else:\n            return self.experiment.version\n\n    # Test tube experiments are not pickleable, so we need to override a few\n    # methods to get DDP working. See\n    # https://docs.python.org/3/library/pickle.html#handling-stateful-objects\n    # for more info.\n    def __getstate__(self) -> Dict[Any, Any]:\n        state = self.__dict__.copy()\n        state[""_experiment""] = self.experiment.get_meta_copy()\n        return state\n\n    def __setstate__(self, state: Dict[Any, Any]):\n        self._experiment = state[""_experiment""].get_non_ddp_exp()\n        del state[""_experiment""]\n        self.__dict__.update(state)\n'"
pytorch_lightning/loggers/trains.py,5,"b'""""""\nTRAINS\n------\n""""""\nfrom argparse import Namespace\nfrom os import environ\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\nimport numpy as np\nimport torch\nfrom PIL.Image import Image\n\ntry:\n    import trains\n    from trains import Task\n    _TRAINS_AVAILABLE = True\nexcept ImportError:  # pragma: no-cover\n    trains = None\n    Task = None\n    _TRAINS_AVAILABLE = False\n    raise ImportError(\'You want to use `TRAINS` logger which is not installed yet,\'  # pragma: no-cover\n                      \' install it with `pip install trains`.\')\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.loggers.base import LightningLoggerBase\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass TrainsLogger(LightningLoggerBase):\n    """"""\n    Log using `allegro.ai TRAINS <https://github.com/allegroai/trains>`_. Install it with pip:\n\n    .. code-block:: bash\n\n        pip install trains\n\n    Example:\n        >>> from pytorch_lightning import Trainer\n        >>> from pytorch_lightning.loggers import TrainsLogger\n        >>> trains_logger = TrainsLogger(\n        ...     project_name=\'pytorch lightning\',\n        ...     task_name=\'default\',\n        ...     output_uri=\'.\',\n        ... ) # doctest: +ELLIPSIS\n        TRAINS Task: ...\n        TRAINS results page: ...\n        >>> trainer = Trainer(logger=trains_logger)\n\n    Use the logger anywhere in your :class:`~pytorch_lightning.core.lightning.LightningModule` as follows:\n\n    >>> from pytorch_lightning import LightningModule\n    >>> class LitModel(LightningModule):\n    ...     def training_step(self, batch, batch_idx):\n    ...         # example\n    ...         self.logger.experiment.whatever_trains_supports(...)\n    ...\n    ...     def any_lightning_module_function_or_hook(self):\n    ...         self.logger.experiment.whatever_trains_supports(...)\n\n    Args:\n        project_name: The name of the experiment\'s project. Defaults to ``None``.\n        task_name: The name of the experiment. Defaults to ``None``.\n        task_type: The name of the experiment. Defaults to ``\'training\'``.\n        reuse_last_task_id: Start with the previously used task id. Defaults to ``True``.\n        output_uri: Default location for output models. Defaults to ``None``.\n        auto_connect_arg_parser: Automatically grab the :class:`~argparse.ArgumentParser`\n            and connect it with the task. Defaults to ``True``.\n        auto_connect_frameworks: If ``True``, automatically patch to trains backend. Defaults to ``True``.\n        auto_resource_monitoring: If ``True``, machine vitals will be\n            sent along side the task scalars. Defaults to ``True``.\n\n    Examples:\n        >>> logger = TrainsLogger(""pytorch lightning"", ""default"", output_uri=""."")  # doctest: +ELLIPSIS\n        TRAINS Task: ...\n        TRAINS results page: ...\n        >>> logger.log_metrics({""val_loss"": 1.23}, step=0)\n        >>> logger.log_text(""sample test"")\n        sample test\n        >>> import numpy as np\n        >>> logger.log_artifact(""confusion matrix"", np.ones((2, 3)))\n        >>> logger.log_image(""passed"", ""Image 1"", np.random.randint(0, 255, (200, 150, 3), dtype=np.uint8))\n    """"""\n\n    _bypass = None\n\n    def __init__(\n            self,\n            project_name: Optional[str] = None,\n            task_name: Optional[str] = None,\n            task_type: str = \'training\',\n            reuse_last_task_id: bool = True,\n            output_uri: Optional[str] = None,\n            auto_connect_arg_parser: bool = True,\n            auto_connect_frameworks: bool = True,\n            auto_resource_monitoring: bool = True\n    ) -> None:\n        if not _TRAINS_AVAILABLE:\n            raise ImportError(\'You want to use `test_tube` logger which is not installed yet,\'\n                              \' install it with `pip install test-tube`.\')\n        super().__init__()\n        if self.bypass_mode():\n            self._trains = None\n            print(\'TRAINS Task: running in bypass mode\')\n            print(\'TRAINS results page: disabled\')\n\n            class _TaskStub(object):\n                def __call__(self, *args, **kwargs):\n                    return self\n\n                def __getattr__(self, attr):\n                    if attr in (\'name\', \'id\'):\n                        return \'\'\n                    return self\n\n                def __setattr__(self, attr, val):\n                    pass\n\n            self._trains = _TaskStub()\n        else:\n            self._trains = Task.init(\n                project_name=project_name,\n                task_name=task_name,\n                task_type=task_type,\n                reuse_last_task_id=reuse_last_task_id,\n                output_uri=output_uri,\n                auto_connect_arg_parser=auto_connect_arg_parser,\n                auto_connect_frameworks=auto_connect_frameworks,\n                auto_resource_monitoring=auto_resource_monitoring\n            )\n\n    @property\n    def experiment(self) -> Task:\n        r""""""\n        Actual TRAINS object. To use TRAINS features in your\n        :class:`~pytorch_lightning.core.lightning.LightningModule` do the following.\n\n        Example::\n\n            self.logger.experiment.some_trains_function()\n\n        """"""\n        return self._trains\n\n    @property\n    def id(self) -> Union[str, None]:\n        """"""\n        ID is a uuid (string) representing this specific experiment in the entire system.\n        """"""\n        if not self._trains:\n            return None\n\n        return self._trains.id\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace]) -> None:\n        """"""\n        Log hyperparameters (numeric values) in TRAINS experiments.\n\n        Args:\n            params: The hyperparameters that passed through the model.\n        """"""\n        if not self._trains:\n            return\n        if not params:\n            return\n\n        params = self._convert_params(params)\n        params = self._flatten_dict(params)\n        self._trains.connect(params)\n\n    @rank_zero_only\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n        """"""\n        Log metrics (numeric values) in TRAINS experiments.\n        This method will be called by Trainer.\n\n        Args:\n            metrics: The dictionary of the metrics.\n                If the key contains ""/"", it will be split by the delimiter,\n                then the elements will be logged as ""title"" and ""series"" respectively.\n            step: Step number at which the metrics should be recorded. Defaults to ``None``.\n        """"""\n        if not self._trains:\n            return\n\n        if not step:\n            step = self._trains.get_last_iteration()\n\n        for k, v in metrics.items():\n            if isinstance(v, str):\n                log.warning(""Discarding metric with string value {}={}"".format(k, v))\n                continue\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            parts = k.split(\'/\')\n            if len(parts) <= 1:\n                series = title = k\n            else:\n                title = parts[0]\n                series = \'/\'.join(parts[1:])\n            self._trains.get_logger().report_scalar(\n                title=title, series=series, value=v, iteration=step)\n\n    @rank_zero_only\n    def log_metric(self, title: str, series: str, value: float, step: Optional[int] = None) -> None:\n        """"""\n        Log metrics (numeric values) in TRAINS experiments.\n        This method will be called by the users.\n\n        Args:\n            title: The title of the graph to log, e.g. loss, accuracy.\n            series: The series name in the graph, e.g. classification, localization.\n            value: The value to log.\n            step: Step number at which the metrics should be recorded. Defaults to ``None``.\n        """"""\n        if not self._trains:\n            return\n\n        if not step:\n            step = self._trains.get_last_iteration()\n\n        if isinstance(value, torch.Tensor):\n            value = value.item()\n        self._trains.get_logger().report_scalar(\n            title=title, series=series, value=value, iteration=step)\n\n    @rank_zero_only\n    def log_text(self, text: str) -> None:\n        """"""Log console text data in TRAINS experiment.\n\n        Args:\n            text: The value of the log (data-point).\n        """"""\n        if self.bypass_mode():\n            print(text)\n            return\n\n        if not self._trains:\n            return\n\n        self._trains.get_logger().report_text(text)\n\n    @rank_zero_only\n    def log_image(\n            self, title: str, series: str,\n            image: Union[str, np.ndarray, Image, torch.Tensor],\n            step: Optional[int] = None) -> None:\n        """"""\n        Log Debug image in TRAINS experiment\n\n        Args:\n            title: The title of the debug image, i.e. ""failed"", ""passed"".\n            series: The series name of the debug image, i.e. ""Image 0"", ""Image 1"".\n            image: Debug image to log. If :class:`numpy.ndarray` or :class:`torch.Tensor`,\n                the image is assumed to be the following:\n\n                - shape: CHW\n                - color space: RGB\n                - value range: [0., 1.] (float) or [0, 255] (uint8)\n\n            step: Step number at which the metrics should be recorded. Defaults to None.\n        """"""\n        if not self._trains:\n            return\n\n        if not step:\n            step = self._trains.get_last_iteration()\n\n        if isinstance(image, str):\n            self._trains.get_logger().report_image(\n                title=title, series=series, local_path=image, iteration=step)\n        else:\n            if isinstance(image, torch.Tensor):\n                image = image.cpu().numpy()\n            if isinstance(image, np.ndarray):\n                image = image.transpose(1, 2, 0)\n            self._trains.get_logger().report_image(\n                title=title, series=series, image=image, iteration=step)\n\n    @rank_zero_only\n    def log_artifact(\n            self, name: str,\n            artifact: Union[str, Path, Dict[str, Any], np.ndarray, Image],\n            metadata: Optional[Dict[str, Any]] = None, delete_after_upload: bool = False) -> None:\n        """"""\n        Save an artifact (file/object) in TRAINS experiment storage.\n\n        Args:\n            name: Artifact name. Notice! it will override the previous artifact\n                if the name already exists.\n            artifact: Artifact object to upload. Currently supports:\n\n                - string / :class:`pathlib.Path` are treated as path to artifact file to upload\n                  If a wildcard or a folder is passed, a zip file containing the\n                  local files will be created and uploaded.\n                - dict will be stored as .json file and uploaded\n                - :class:`pandas.DataFrame` will be stored as .csv.gz (compressed CSV file) and uploaded\n                - :class:`numpy.ndarray` will be stored as .npz and uploaded\n                - :class:`PIL.Image.Image` will be stored to .png file and uploaded\n\n            metadata:\n                Simple key/value dictionary to store on the artifact. Defaults to ``None``.\n            delete_after_upload:\n                If ``True``, the local artifact will be deleted (only applies if ``artifact`` is a\n                local file). Defaults to ``False``.\n        """"""\n        if not self._trains:\n            return\n\n        self._trains.upload_artifact(\n            name=name, artifact_object=artifact, metadata=metadata,\n            delete_after_upload=delete_after_upload\n        )\n\n    @rank_zero_only\n    def finalize(self, status: str = None) -> None:\n        # super().finalize(status)\n        if self.bypass_mode() or not self._trains:\n            return\n\n        self._trains.close()\n        self._trains = None\n\n    @property\n    def name(self) -> Union[str, None]:\n        """"""\n        Name is a human readable non-unique name (str) of the experiment.\n        """"""\n        if not self._trains:\n            return \'\'\n\n        return self._trains.name\n\n    @property\n    def version(self) -> Union[str, None]:\n        if not self._trains:\n            return None\n\n        return self._trains.id\n\n    @classmethod\n    def set_credentials(cls, api_host: str = None, web_host: str = None, files_host: str = None,\n                        key: str = None, secret: str = None) -> None:\n        """"""\n        Set new default TRAINS-server host and credentials.\n        These configurations could be overridden by either OS environment variables\n        or trains.conf configuration file.\n\n        Note:\n            Credentials need to be set *prior* to Logger initialization.\n\n        Args:\n            api_host: Trains API server url, example: ``host=\'http://localhost:8008\'``\n            web_host: Trains WEB server url, example: ``host=\'http://localhost:8080\'``\n            files_host: Trains Files server url, example: ``host=\'http://localhost:8081\'``\n            key: user key/secret pair, example: ``key=\'thisisakey123\'``\n            secret: user key/secret pair, example: ``secret=\'thisisseceret123\'``\n        """"""\n        Task.set_credentials(api_host=api_host, web_host=web_host, files_host=files_host,\n                             key=key, secret=secret)\n\n    @classmethod\n    def set_bypass_mode(cls, bypass: bool) -> None:\n        """"""\n        Will bypass all outside communication, and will drop all logs.\n        Should only be used in ""standalone mode"", when there is no access to the *trains-server*.\n\n        Args:\n            bypass: If ``True``, all outside communication is skipped.\n        """"""\n        cls._bypass = bypass\n\n    @classmethod\n    def bypass_mode(cls) -> bool:\n        """"""\n        Returns the bypass mode state.\n\n        Note:\n            `GITHUB_ACTIONS` env will automatically set bypass_mode to ``True``\n            unless overridden specifically with ``TrainsLogger.set_bypass_mode(False)``.\n\n        Return:\n            If True, all outside communication is skipped.\n        """"""\n        return cls._bypass if cls._bypass is not None else bool(environ.get(\'CI\'))\n\n    def __getstate__(self) -> Union[str, None]:\n        if self.bypass_mode() or not self._trains:\n            return \'\'\n\n        return self._trains.id\n\n    def __setstate__(self, state: str) -> None:\n        self._rank = 0\n        self._trains = None\n        if state:\n            self._trains = Task.get_task(task_id=state)\n'"
pytorch_lightning/loggers/wandb.py,1,"b'""""""\nWeights and Biases\n------------------\n""""""\nimport os\nfrom argparse import Namespace\nfrom typing import Optional, List, Dict, Union, Any\n\nimport torch.nn as nn\n\ntry:\n    import wandb\n    from wandb.wandb_run import Run\n    _WANDB_AVAILABLE = True\nexcept ImportError:  # pragma: no-cover\n    wandb = None\n    Run = None\n    _WANDB_AVAILABLE = False\n\nfrom pytorch_lightning.loggers.base import LightningLoggerBase\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass WandbLogger(LightningLoggerBase):\n    """"""\n    Log using `Weights and Biases <https://www.wandb.com/>`_. Install it with pip:\n\n    .. code-block:: bash\n\n        pip install wandb\n\n    Args:\n        name: Display name for the run.\n        save_dir: Path where data is saved.\n        offline: Run offline (data can be streamed later to wandb servers).\n        id: Sets the version, mainly used to resume a previous run.\n        anonymous: Enables or explicitly disables anonymous logging.\n        version: Sets the version, mainly used to resume a previous run.\n        project: The name of the project to which this run will belong.\n        tags: Tags associated with this run.\n        log_model: Save checkpoints in wandb dir to upload on W&B servers.\n        experiment: WandB experiment object\n        entity: The team posting this run (default: your username or your default team)\n        group: A unique string shared by all runs in a given group\n\n    Example:\n        >>> from pytorch_lightning.loggers import WandbLogger\n        >>> from pytorch_lightning import Trainer\n        >>> wandb_logger = WandbLogger()\n        >>> trainer = Trainer(logger=wandb_logger)\n\n    See Also:\n        - `Tutorial <https://app.wandb.ai/cayush/pytorchlightning/reports/\n          Use-Pytorch-Lightning-with-Weights-%26-Biases--Vmlldzo2NjQ1Mw>`__\n          on how to use W&B with Pytorch Lightning.\n\n    """"""\n\n    def __init__(self,\n                 name: Optional[str] = None,\n                 save_dir: Optional[str] = None,\n                 offline: bool = False,\n                 id: Optional[str] = None,\n                 anonymous: bool = False,\n                 version: Optional[str] = None,\n                 project: Optional[str] = None,\n                 tags: Optional[List[str]] = None,\n                 log_model: bool = False,\n                 experiment=None,\n                 entity=None,\n                 group: Optional[str] = None):\n        if not _WANDB_AVAILABLE:\n            raise ImportError(\'You want to use `wandb` logger which is not installed yet,\'  # pragma: no-cover\n                              \' install it with `pip install wandb`.\')\n        super().__init__()\n        self._name = name\n        self._save_dir = save_dir\n        self._anonymous = \'allow\' if anonymous else None\n        self._id = version or id\n        self._tags = tags\n        self._project = project\n        self._experiment = experiment\n        self._offline = offline\n        self._entity = entity\n        self._log_model = log_model\n        self._group = group\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        # args needed to reload correct experiment\n        state[\'_id\'] = self._experiment.id if self._experiment is not None else None\n\n        # cannot be pickled\n        state[\'_experiment\'] = None\n        return state\n\n    @property\n    def experiment(self) -> Run:\n        r""""""\n\n        Actual wandb object. To use wandb features in your\n        :class:`~pytorch_lightning.core.lightning.LightningModule` do the following.\n\n        Example::\n\n            self.logger.experiment.some_wandb_function()\n\n        """"""\n        if self._experiment is None:\n            if self._offline:\n                os.environ[\'WANDB_MODE\'] = \'dryrun\'\n            self._experiment = wandb.init(\n                name=self._name, dir=self._save_dir, project=self._project, anonymous=self._anonymous,\n                reinit=True, id=self._id, resume=\'allow\', tags=self._tags, entity=self._entity,\n                group=self._group)\n            # save checkpoints in wandb dir to upload on W&B servers\n            if self._log_model:\n                self.save_dir = self._experiment.dir\n        return self._experiment\n\n    def watch(self, model: nn.Module, log: str = \'gradients\', log_freq: int = 100):\n        self.experiment.watch(model, log=log, log_freq=log_freq)\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Namespace]) -> None:\n        params = self._convert_params(params)\n        self.experiment.config.update(params, allow_val_change=True)\n\n    @rank_zero_only\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:\n        self.experiment.log({\'global_step\': step, **metrics} if step is not None else metrics)\n\n    @property\n    def name(self) -> str:\n        # don\'t create an experiment if we don\'t have one\n        name = self._experiment.project_name() if self._experiment else None\n        return name\n\n    @property\n    def version(self) -> str:\n        # don\'t create an experiment if we don\'t have one\n        return self._experiment.id if self._experiment else None\n'"
pytorch_lightning/logging/__init__.py,0,"b'""""""\n.. warning:: `logging` package has been renamed to `loggers` since v0.7.0.\n The deprecated package name will be removed in v0.9.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`logging` package has been renamed to `loggers` since v0.7.0""\n               "" The deprecated package name will be removed in v0.9.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers import *  # noqa: F403 E402\n'"
pytorch_lightning/logging/comet.py,0,"b'""""""\n.. warning:: `logging` package has been renamed to `loggers` since v0.7.0 and will be removed in v0.9.0\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`logging.comet` module has been renamed to `loggers.comet` since v0.7.0.""\n               "" The deprecated module name will be removed in v0.9.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.comet import CometLogger  # noqa: F403 E402\n'"
pytorch_lightning/logging/comet_logger.py,0,"b'""""""\n.. warning:: `comet_logger` module has been renamed to `comet` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`comet_logger` module has been renamed to `comet` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.comet import CometLogger  # noqa: E402\n'"
pytorch_lightning/logging/mlflow.py,0,"b'""""""\n.. warning:: `logging` package has been renamed to `loggers` since v0.7.0 and will be removed in v0.9.0\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`logging.mlflow` module has been renamed to `loggers.mlflow` since v0.7.0.""\n               "" The deprecated module name will be removed in v0.9.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.mlflow import MLFlowLogger  # noqa: F403 E402\n'"
pytorch_lightning/logging/mlflow_logger.py,0,"b'""""""\n.. warning:: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`mlflow_logger` module has been renamed to `mlflow` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.mlflow import MLFlowLogger  # noqa: E402\n'"
pytorch_lightning/logging/neptune.py,0,"b'""""""\n.. warning:: `logging` package has been renamed to `loggers` since v0.7.0 and will be removed in v0.9.0\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`logging.neptune` module has been renamed to `loggers.neptune` since v0.7.0.""\n               "" The deprecated module name will be removed in v0.9.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.neptune import NeptuneLogger  # noqa: F403 E402\n'"
pytorch_lightning/logging/test_tube.py,0,"b'""""""\n.. warning:: `logging` package has been renamed to `loggers` since v0.7.0 and will be removed in v0.9.0\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`logging.test_tube` module has been renamed to `loggers.test_tube` since v0.7.0.""\n               "" The deprecated module name will be removed in v0.9.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.test_tube import TestTubeLogger  # noqa: F403 E402\n'"
pytorch_lightning/logging/test_tube_logger.py,0,"b'""""""\n.. warning:: `test_tube_logger` module has been renamed to `test_tube` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`test_tube_logger` module has been renamed to `test_tube` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.test_tube import TestTubeLogger  # noqa: E402\n'"
pytorch_lightning/logging/wandb.py,0,"b'""""""\n.. warning:: `logging` package has been renamed to `loggers` since v0.7.0 and will be removed in v0.9.0\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`logging.wandb` module has been renamed to `loggers.wandb` since v0.7.0.""\n               "" The deprecated module name will be removed in v0.9.0."", DeprecationWarning)\n\nfrom pytorch_lightning.loggers.wandb import WandbLogger  # noqa: F403 E402\n'"
pytorch_lightning/metrics/__init__.py,0,"b'""""""\nMetrics\n=======\n\nMetrics are generally used to monitor model performance.\n\nThe following package aims to provide the most convenient ones as well\nas a structure to implement your custom metrics for all the fancy research\nyou want to do.\n\nFor native PyTorch implementations of metrics, it is recommended to use\nthe :class:`TensorMetric` which handles automated DDP syncing and conversions\nto tensors for all inputs and outputs.\n\nIf your metrics implementation works on numpy, just use the\n:class:`NumpyMetric`, which handles the automated conversion of\ninputs to and outputs from numpy as well as automated ddp syncing.\n\n.. warning:: Employing numpy in your metric calculation might slow\n    down your training substantially, since every metric computation\n    requires a GPU sync to convert tensors to numpy.\n\n\n""""""\n'"
pytorch_lightning/metrics/converters.py,21,"b'""""""\nThis file provides functions and decorators for automated input and output\nconversion to/from :class:`numpy.ndarray` and :class:`torch.Tensor` as well as utilities to\nsync tensors between different processes in a DDP scenario, when needed.\n""""""\n\nimport sys\nimport numbers\nfrom typing import Union, Any, Callable, Optional\n\nimport numpy as np\nimport torch\nfrom torch.utils.data._utils.collate import np_str_obj_array_pattern\n\nfrom pytorch_lightning.utilities.apply_func import apply_to_collection\n\n\ndef _apply_to_inputs(func_to_apply: Callable, *dec_args, **dec_kwargs) -> Callable:\n    """"""\n    Decorator function to apply a function to all inputs of a function.\n    Args:\n        func_to_apply: the function to apply to the inputs\n        *dec_args: positional arguments for the function to be applied\n        **dec_kwargs: keyword arguments for the function to be applied\n\n    Returns:\n        the decorated function\n    """"""\n\n    def decorator_fn(func_to_decorate):\n        # actual function applying the give function to inputs\n        def new_func(*args, **kwargs):\n            args = func_to_apply(args, *dec_args, **dec_kwargs)\n            kwargs = func_to_apply(kwargs, *dec_args, **dec_kwargs)\n            return func_to_decorate(*args, **kwargs)\n\n        return new_func\n\n    return decorator_fn\n\n\ndef _apply_to_outputs(func_to_apply: Callable, *dec_args, **dec_kwargs) -> Callable:\n    """"""\n    Decorator function to apply a function to all outputs of a function.\n    Args:\n        func_to_apply: the function to apply to the outputs\n        *dec_args: positional arguments for the function to be applied\n        **dec_kwargs: keyword arguments for the function to be applied\n\n    Returns:\n        the decorated function\n    """"""\n\n    def decorator_fn(function_to_decorate):\n        # actual function applying the give function to outputs\n        def new_func(*args, **kwargs):\n            result = function_to_decorate(*args, **kwargs)\n            return func_to_apply(result, *dec_args, **dec_kwargs)\n\n        return new_func\n\n    return decorator_fn\n\n\ndef _convert_to_tensor(data: Any) -> Any:\n    """"""\n    Maps all kind of collections and numbers to tensors.\n\n    Args:\n        data: the data to convert to tensor\n\n    Returns:\n        the converted data\n\n    """"""\n    if isinstance(data, numbers.Number):\n        return torch.tensor([data])\n    # is not array of object\n    elif isinstance(data, np.ndarray) and np_str_obj_array_pattern.search(data.dtype.str) is None:\n        return torch.from_numpy(data)\n    elif isinstance(data, torch.Tensor):\n        return data\n\n    raise TypeError(f""The given type (\'{type(data).__name__}\') cannot be converted to a tensor!"")\n\n\ndef _convert_to_numpy(data: Union[torch.Tensor, np.ndarray, numbers.Number]) -> np.ndarray:\n    """"""Convert all tensors and numpy arrays to numpy arrays.\n    Args:\n        data: the tensor or array to convert to numpy\n\n    Returns:\n        the resulting numpy array\n\n    """"""\n    if isinstance(data, torch.Tensor):\n        return data.cpu().detach().numpy()\n    elif isinstance(data, numbers.Number):\n        return np.array([data])\n    elif isinstance(data, np.ndarray):\n        return data\n\n    raise TypeError(""The given type (\'%s\') cannot be converted to a numpy array!"" % type(data).__name__)\n\n\ndef _numpy_metric_conversion(func_to_decorate: Callable) -> Callable:\n    """"""\n    Decorator handling the argument conversion for metrics working on numpy.\n    All inputs of the decorated function will be converted to numpy and all\n    outputs will be converted to tensors.\n\n    Args:\n        func_to_decorate: the function whose inputs and outputs shall be converted\n\n    Returns:\n        the decorated function\n\n    """"""\n    # applies collection conversion from tensor to numpy to all inputs\n    # we need to include numpy arrays here, since otherwise they will also be treated as sequences\n    func_convert_inputs = _apply_to_inputs(\n        apply_to_collection, (torch.Tensor, np.ndarray, numbers.Number), _convert_to_numpy)(func_to_decorate)\n    # converts all inputs back to tensors (device doesn\'t matter here, since this is handled by BaseMetric)\n    func_convert_in_out = _apply_to_outputs(_convert_to_tensor)(func_convert_inputs)\n    return func_convert_in_out\n\n\ndef _tensor_metric_conversion(func_to_decorate: Callable) -> Callable:\n    """"""\n    Decorator Handling the argument conversion for metrics working on tensors.\n    All inputs and outputs of the decorated function will be converted to tensors\n\n    Args:\n        func_to_decorate: the function whose inputs and outputs shall be converted\n\n    Returns:\n        the decorated function\n\n    """"""\n    # converts all inputs to tensor if possible\n    # we need to include tensors here, since otherwise they will also be treated as sequences\n    func_convert_inputs = _apply_to_inputs(\n        apply_to_collection, (torch.Tensor, np.ndarray, numbers.Number), _convert_to_tensor)(func_to_decorate)\n    # convert all outputs to tensor if possible\n    return _apply_to_outputs(_convert_to_tensor)(func_convert_inputs)\n\n\ndef _sync_ddp_if_available(result: Union[torch.Tensor],\n                           group: Optional[Any] = None,\n                           reduce_op: Optional[torch.distributed.ReduceOp] = None,\n                           ) -> torch.Tensor:\n    """"""\n    Function to reduce the tensors from several ddp processes to one master process\n\n    Args:\n        result: the value to sync and reduce (typically tensor or number)\n        group: the process group to gather results from. Defaults to all processes (world)\n        reduce_op: the reduction operation. Defaults to sum.\n\n    Returns:\n        reduced value\n\n    """"""\n\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        if group is None:\n            group = torch.distributed.group.WORLD\n\n        if reduce_op is None:\n            reduce_op = torch.distributed.ReduceOp.SUM\n\n        # sync all processes before reduction\n        torch.distributed.barrier(group=group)\n        torch.distributed.all_reduce(result, op=reduce_op, group=group,\n                                     async_op=False)\n\n    return result\n\n\ndef numpy_metric(group: Optional[Any] = None,\n                 reduce_op: Optional[torch.distributed.ReduceOp] = None) -> Callable:\n    """"""\n    This decorator shall be used on all function metrics working on numpy arrays.\n\n    It handles the argument conversion and DDP reduction for metrics working on numpy.\n    All inputs of the decorated function will be converted to numpy and all\n    outputs will be converted to tensors.\n    In DDP Training all output tensors will be reduced according to the given rules.\n\n    Args:\n        group: the process group to gather results from. Defaults to all processes (world)\n        reduce_op: the reduction operation. Defaults to sum\n\n    Returns:\n        the decorated function\n\n    """"""\n\n    def decorator_fn(func_to_decorate):\n        return _apply_to_outputs(apply_to_collection, torch.Tensor, _sync_ddp_if_available,\n                                 group=group,\n                                 reduce_op=reduce_op)(_numpy_metric_conversion(func_to_decorate))\n\n    return decorator_fn\n\n\ndef tensor_metric(group: Optional[Any] = None,\n                  reduce_op: Optional[torch.distributed.ReduceOp] = None) -> Callable:\n    """"""\n    This decorator shall be used on all function metrics working on tensors.\n\n    It handles the argument conversion and DDP reduction for metrics working on tensors.\n    All inputs and outputs of the decorated function will be converted to tensors.\n    In DDP Training all output tensors will be reduced according to the given rules.\n\n    Args:\n       group: the process group to gather results from. Defaults to all processes (world)\n       reduce_op: the reduction operation. Defaults to sum\n\n    Returns:\n       the decorated function\n\n    """"""\n\n    def decorator_fn(func_to_decorate):\n        return _apply_to_outputs(apply_to_collection, torch.Tensor, _sync_ddp_if_available,\n                                 group=group,\n                                 reduce_op=reduce_op)(_tensor_metric_conversion(func_to_decorate))\n\n    return decorator_fn\n'"
pytorch_lightning/metrics/metric.py,11,"b'from abc import ABC, abstractmethod\nfrom typing import Any, Optional, Union\n\nimport torch\nimport torch.distributed\n\nfrom pytorch_lightning.metrics.converters import tensor_metric, numpy_metric\nfrom pytorch_lightning.utilities.apply_func import apply_to_collection\nfrom pytorch_lightning.utilities.device_dtype_mixin import DeviceDtypeModuleMixin\n\n__all__ = [\'Metric\', \'TensorMetric\', \'NumpyMetric\']\n\n\nclass Metric(DeviceDtypeModuleMixin, torch.nn.Module, ABC):\n    """"""\n    Abstract base class for metric implementation.\n\n    Should be used to implement metrics that\n    1. Return multiple Outputs\n    2. Handle their own DDP sync\n    """"""\n    def __init__(self, name: str):\n        """"""\n        Args:\n            name: the metric\'s name\n\n        """"""\n        super().__init__()\n        self.name = name\n        self._dtype = torch.get_default_dtype()\n        self._device = torch.device(\'cpu\')\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -> torch.Tensor:\n        """"""\n        Implements the actual metric computation.\n\n        Returns:\n            metric value\n\n        """"""\n        raise NotImplementedError\n\n\nclass TensorMetric(Metric):\n    """"""\n    Base class for metric implementation operating directly on tensors.\n    All inputs and outputs will be casted to tensors if necessary.\n    Already handles DDP sync and input/output conversions.\n    """"""\n    def __init__(self, name: str,\n                 reduce_group: Optional[Any] = None,\n                 reduce_op: Optional[Any] = None):\n        """"""\n\n        Args:\n            name: the metric\'s name\n            reduce_group: the process group for DDP reduces (only needed for DDP training).\n                Defaults to all processes (world)\n            reduce_op: the operation to perform during reduction within DDP (only needed for DDP training).\n                Defaults to sum.\n        """"""\n        super().__init__(name)\n        self._orig_call = tensor_metric(group=reduce_group,\n                                        reduce_op=reduce_op)(super().__call__)\n\n    def __call__(self, *args, **kwargs) -> torch.Tensor:\n        def _to_device_dtype(x: torch.Tensor) -> torch.Tensor:\n            return x.to(device=self.device, dtype=self.dtype, non_blocking=True)\n\n        return apply_to_collection(self._orig_call(*args, **kwargs), torch.Tensor,\n                                   _to_device_dtype)\n\n\nclass NumpyMetric(Metric):\n    """"""\n    Base class for metric implementation operating on numpy arrays.\n    All inputs will be casted to numpy if necessary and all outputs will\n    be casted to tensors if necessary.\n    Already handles DDP sync and input/output conversions.\n    """"""\n    def __init__(self, name: str,\n                 reduce_group: Optional[Any] = None,\n                 reduce_op: Optional[Any] = None):\n        """"""\n\n        Args:\n            name: the metric\'s name\n            reduce_group: the process group for DDP reduces (only needed for DDP training).\n                Defaults to all processes (world)\n            reduce_op: the operation to perform during reduction within DDP (only needed for DDP training).\n                Defaults to sum.\n        """"""\n        super().__init__(name)\n        self._orig_call = numpy_metric(group=reduce_group,\n                                       reduce_op=reduce_op)(super().__call__)\n\n    def __call__(self, *args, **kwargs) -> torch.Tensor:\n        def _to_device_dtype(x: torch.Tensor) -> torch.Tensor:\n            return x.to(device=self.device, dtype=self.dtype, non_blocking=True)\n\n        return apply_to_collection(self._orig_call(*args, **kwargs), torch.Tensor,\n                                   _to_device_dtype)\n'"
pytorch_lightning/overrides/__init__.py,0,b''
pytorch_lightning/overrides/data_parallel.py,13,"b'import itertools\nimport threading\nfrom itertools import chain\n\nimport torch\nfrom torch.cuda._utils import _get_device_index\nfrom torch.nn import DataParallel\nfrom torch.nn.parallel import DistributedDataParallel\n\n\ndef _find_tensors(obj):  # pragma: no-cover\n    r""""""\n    Recursively find all tensors contained in the specified object.\n    """"""\n    if isinstance(obj, torch.Tensor):\n        return [obj]\n    if isinstance(obj, (list, tuple)):\n        return itertools.chain(*map(_find_tensors, obj))\n    if isinstance(obj, dict):\n        return itertools.chain(*map(_find_tensors, obj.values()))\n    return []\n\n\ndef get_a_var(obj):  # pragma: no-cover\n    if isinstance(obj, torch.Tensor):\n        return obj\n\n    if isinstance(obj, (list, tuple)):\n        for result in map(get_a_var, obj):\n            if isinstance(result, torch.Tensor):\n                return result\n    if isinstance(obj, dict):\n        for result in map(get_a_var, obj.items()):\n            if isinstance(result, torch.Tensor):\n                return result\n    return None\n\n\nclass LightningDataParallel(DataParallel):\n    """"""\n    Override the forward call in lightning so it goes to training and validation step respectively\n    """"""\n\n    def forward(self, *inputs, **kwargs):\n        if not self.device_ids:\n            return self.module(*inputs, **kwargs)\n\n        for t in chain(self.module.parameters(), self.module.buffers()):\n            if t.device != self.src_device_obj:\n                raise RuntimeError(""module must have its parameters and buffers ""\n                                   ""on device {} (device_ids[0]) but found one of ""\n                                   ""them on device: {}"".format(self.src_device_obj, t.device))\n\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            # lightning\n            if self.module.training:\n                return self.module.training_step(*inputs[0], **kwargs[0])\n            if self.module.testing:\n                return self.module.test_step(*inputs[0], **kwargs[0])\n\n            return self.module.validation_step(*inputs[0], **kwargs[0])\n\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n\n    def parallel_apply(self, replicas, inputs, kwargs):\n        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n\n\nclass LightningDistributedDataParallel(DistributedDataParallel):\n    """"""\n    Override the forward call in lightning so it goes to training and validation step respectively\n    """"""\n\n    def parallel_apply(self, replicas, inputs, kwargs):\n        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n\n    def forward(self, *inputs, **kwargs):  # pragma: no-cover\n        self._sync_params()\n        if self.device_ids:\n            inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n            if len(self.device_ids) == 1:\n                # --------------\n                # LIGHTNING MOD\n                # --------------\n                # normal\n                # output = self.module(*inputs[0], **kwargs[0])\n                # lightning\n                if self.module.training:\n                    output = self.module.training_step(*inputs[0], **kwargs[0])\n                elif self.module.testing:\n                    output = self.module.test_step(*inputs[0], **kwargs[0])\n                else:\n                    output = self.module.validation_step(*inputs[0], **kwargs[0])\n            else:\n                outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)\n                output = self.gather(outputs, self.output_device)\n        else:\n            # normal\n            # output = self.module(*inputs, **kwargs)\n            # lightning (ddp_cpu)\n            if self.module.training:\n                output = self.module.training_step(*inputs, **kwargs)\n            elif self.module.testing:\n                output = self.module.test_step(*inputs, **kwargs)\n            else:\n                output = self.module.validation_step(*inputs, **kwargs)\n\n        if torch.is_grad_enabled():\n            # We\'ll return the output object verbatim since it is a freeform\n            # object. We need to find any tensors in this object, though,\n            # because we need to figure out which parameters were used during\n            # this forward pass, to ensure we short circuit reduction for any\n            # unused parameters. Only if `find_unused_parameters` is set.\n            if self.find_unused_parameters:\n                self.reducer.prepare_for_backward(list(_find_tensors(output)))\n            else:\n                self.reducer.prepare_for_backward([])\n        return output\n\n\ndef parallel_apply(modules, inputs, kwargs_tup=None, devices=None):  # pragma: no-cover\n    r""""""Applies each `module` in :attr:`modules` in parallel on arguments\n    contained in :attr:`inputs` (positional) and :attr:`kwargs_tup` (keyword)\n    on each of :attr:`devices`.\n\n    Args:\n        modules (Module): modules to be parallelized\n        inputs (tensor): inputs to the modules\n        devices (list of int or torch.device): CUDA devices\n\n    :attr:`modules`, :attr:`inputs`, :attr:`kwargs_tup` (if given), and\n    :attr:`devices` (if given) should all have same length. Moreover, each\n    element of :attr:`inputs` can either be a single object as the only argument\n    to a module, or a collection of positional arguments.\n    """"""\n    assert len(modules) == len(inputs)\n    if kwargs_tup is not None:\n        assert len(modules) == len(kwargs_tup)\n    else:\n        kwargs_tup = ({},) * len(modules)\n    if devices is not None:\n        assert len(modules) == len(devices)\n    else:\n        devices = [None] * len(modules)\n    devices = list(map(lambda x: _get_device_index(x, True), devices))\n    lock = threading.Lock()\n    results = {}\n    grad_enabled = torch.is_grad_enabled()\n\n    def _worker(i, module, input, kwargs, device=None):\n        torch.set_grad_enabled(grad_enabled)\n        if device is None:\n            device = get_a_var(input).get_device()\n        try:\n            with torch.cuda.device(device):\n                # this also avoids accidental slicing of `input` if it is a Tensor\n                if not isinstance(input, (list, tuple)):\n                    input = (input,)\n\n                # ---------------\n                # CHANGE\n                if module.training:\n                    output = module.training_step(*input, **kwargs)\n\n                elif module.testing:\n                    output = module.test_step(*input, **kwargs)\n\n                else:\n                    output = module.validation_step(*input, **kwargs)\n\n                if module.use_dp or module.use_ddp2:\n                    auto_squeeze_dim_zeros(output)\n                # ---------------\n\n            with lock:\n                results[i] = output\n        except Exception as ex:\n            with lock:\n                results[i] = ex\n\n    # TODO: fix hack (maybe not a hack)\n    # make sure each module knows what training state it\'s in...\n    # fixes weird bug where copies are out of sync\n    root_m = modules[0]\n    for m in modules[1:]:\n        m.training = root_m.training\n        m.testing = root_m.testing\n\n    if len(modules) > 1:\n        threads = [threading.Thread(target=_worker,\n                                    args=(i, module, input, kwargs, device))\n                   for i, (module, input, kwargs, device) in\n                   enumerate(zip(modules, inputs, kwargs_tup, devices))]\n\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n    else:\n        _worker(0, modules[0], inputs[0], kwargs_tup[0], devices[0])\n\n    outputs = []\n    for i in range(len(inputs)):\n        output = results[i]\n        if isinstance(output, Exception):\n            raise output\n        outputs.append(output)\n    return outputs\n\n\ndef auto_squeeze_dim_zeros(output):\n    """"""\n    In DP or DDP2 we need to unsqueeze dim 0\n    :param output:\n    :return:\n    """"""\n    for k, v in output.items():\n        if not isinstance(v, torch.Tensor):\n            continue\n\n        is_scalar = v.dim() == 0\n        if is_scalar:\n            output[k] = output[k].unsqueeze(0)\n'"
pytorch_lightning/overrides/override_data_parallel.py,0,"b'""""""\n.. warning:: `override_data_parallel` module has been renamed to `data_parallel` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`override_data_parallel` module has been renamed to `data_parallel` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.overrides.data_parallel import (  # noqa: E402\n    get_a_var, parallel_apply, LightningDataParallel, LightningDistributedDataParallel)\n'"
pytorch_lightning/profiler/__init__.py,0,"b'""""""\nProfiling your training run can help you understand if there are any bottlenecks in your code.\n\n\nBuilt-in checks\n---------------\n\nPyTorch Lightning supports profiling standard actions in the training loop out of the box, including:\n\n- on_epoch_start\n- on_epoch_end\n- on_batch_start\n- tbptt_split_batch\n- model_forward\n- model_backward\n- on_after_backward\n- optimizer_step\n- on_batch_end\n- training_step_end\n- on_training_end\n\nEnable simple profiling\n-----------------------\n\nIf you only wish to profile the standard actions, you can set `profiler=True` when constructing\nyour `Trainer` object.\n\n.. code-block:: python\n\n    trainer = Trainer(..., profiler=True)\n\nThe profiler\'s results will be printed at the completion of a training `fit()`.\n\n.. code-block:: python\n\n    Profiler Report\n\n    Action                  |  Mean duration (s)    |  Total time (s)\n    -----------------------------------------------------------------\n    on_epoch_start          |  5.993e-06            |  5.993e-06\n    get_train_batch         |  0.0087412            |  16.398\n    on_batch_start          |  5.0865e-06           |  0.0095372\n    model_forward           |  0.0017818            |  3.3408\n    model_backward          |  0.0018283            |  3.4282\n    on_after_backward       |  4.2862e-06           |  0.0080366\n    optimizer_step          |  0.0011072            |  2.0759\n    on_batch_end            |  4.5202e-06           |  0.0084753\n    on_epoch_end            |  3.919e-06            |  3.919e-06\n    on_train_end            |  5.449e-06            |  5.449e-06\n\n\nAdvanced Profiling\n--------------------\n\nIf you want more information on the functions called during each event, you can use the `AdvancedProfiler`.\nThis option uses Python\'s cProfiler_ to provide a report of time spent on *each* function called within your code.\n\n.. _cProfiler: https://docs.python.org/3/library/profile.html#module-cProfile\n\n.. code-block:: python\n\n    profiler = AdvancedProfiler()\n    trainer = Trainer(..., profiler=profiler)\n\nThe profiler\'s results will be printed at the completion of a training `fit()`. This profiler\nreport can be quite long, so you can also specify an `output_filename` to save the report instead\nof logging it to the output in your terminal. The output below shows the profiling for the action\n`get_train_batch`.\n\n.. code-block:: python\n\n    Profiler Report\n\n    Profile stats for: get_train_batch\n            4869394 function calls (4863767 primitive calls) in 18.893 seconds\n    Ordered by: cumulative time\n    List reduced from 76 to 10 due to restriction <10>\n    ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    3752/1876    0.011    0.000   18.887    0.010 {built-in method builtins.next}\n        1876     0.008    0.000   18.877    0.010 dataloader.py:344(__next__)\n        1876     0.074    0.000   18.869    0.010 dataloader.py:383(_next_data)\n        1875     0.012    0.000   18.721    0.010 fetch.py:42(fetch)\n        1875     0.084    0.000   18.290    0.010 fetch.py:44(<listcomp>)\n        60000    1.759    0.000   18.206    0.000 mnist.py:80(__getitem__)\n        60000    0.267    0.000   13.022    0.000 transforms.py:68(__call__)\n        60000    0.182    0.000    7.020    0.000 transforms.py:93(__call__)\n        60000    1.651    0.000    6.839    0.000 functional.py:42(to_tensor)\n        60000    0.260    0.000    5.734    0.000 transforms.py:167(__call__)\n\nYou can also reference this profiler in your LightningModule to profile specific actions of interest.\nIf you don\'t want to always have the profiler turned on, you can optionally pass a `PassThroughProfiler`\nwhich will allow you to skip profiling without having to make any code changes. Each profiler has a\nmethod `profile()` which returns a context handler. Simply pass in the name of your action that you want\nto track and the profiler will record performance for code executed within this context.\n\n.. code-block:: python\n\n    from pytorch_lightning.profiler import Profiler, PassThroughProfiler\n\n    class MyModel(LightningModule):\n        def __init__(self, profiler=None):\n            self.profiler = profiler or PassThroughProfiler()\n\n        def custom_processing_step(self, data):\n            with profiler.profile(\'my_custom_action\'):\n                # custom processing step\n            return data\n\n    profiler = Profiler()\n    model = MyModel(profiler)\n    trainer = Trainer(profiler=profiler, max_epochs=1)\n\n""""""\n\nfrom pytorch_lightning.profiler.profilers import SimpleProfiler, AdvancedProfiler, PassThroughProfiler, BaseProfiler\n\n__all__ = [\n    \'BaseProfiler\',\n    \'SimpleProfiler\',\n    \'AdvancedProfiler\',\n    \'PassThroughProfiler\',\n]\n'"
pytorch_lightning/profiler/profilers.py,0,"b'import cProfile\nimport io\nimport os\nimport pstats\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\nimport numpy as np\n\nfrom pytorch_lightning import _logger as log\n\n\nclass BaseProfiler(ABC):\n    """"""\n    If you wish to write a custom profiler, you should inhereit from this class.\n    """"""\n\n    def __init__(self, output_streams: list = None):\n        """"""\n        Params:\n            stream_out: callable\n        """"""\n        if output_streams:\n            if not isinstance(output_streams, (list, tuple)):\n                output_streams = [output_streams]\n        else:\n            output_streams = []\n        self.write_streams = output_streams\n\n    @abstractmethod\n    def start(self, action_name: str) -> None:\n        """"""Defines how to start recording an action.""""""\n\n    @abstractmethod\n    def stop(self, action_name: str) -> None:\n        """"""Defines how to record the duration once an action is complete.""""""\n\n    @contextmanager\n    def profile(self, action_name: str) -> None:\n        """"""\n        Yields a context manager to encapsulate the scope of a profiled action.\n\n        Example::\n\n            with self.profile(\'load training data\'):\n                # load training data code\n\n        The profiler will start once you\'ve entered the context and will automatically\n        stop once you exit the code block.\n        """"""\n        try:\n            self.start(action_name)\n            yield action_name\n        finally:\n            self.stop(action_name)\n\n    def profile_iterable(self, iterable, action_name: str) -> None:\n        iterator = iter(iterable)\n        while True:\n            try:\n                self.start(action_name)\n                value = next(iterator)\n                self.stop(action_name)\n                yield value\n            except StopIteration:\n                self.stop(action_name)\n                break\n\n    def describe(self) -> None:\n        """"""Logs a profile report after the conclusion of the training run.""""""\n        for write in self.write_streams:\n            write(self.summary())\n\n    @abstractmethod\n    def summary(self) -> str:\n        """"""Create profiler summary in text format.""""""\n\n\nclass PassThroughProfiler(BaseProfiler):\n    """"""\n    This class should be used when you don\'t want the (small) overhead of profiling.\n    The Trainer uses this class by default.\n    """"""\n\n    def __init__(self):\n        super().__init__(output_streams=None)\n\n    def start(self, action_name: str) -> None:\n        pass\n\n    def stop(self, action_name: str) -> None:\n        pass\n\n    def summary(self) -> str:\n        return """"\n\n\nclass SimpleProfiler(BaseProfiler):\n    """"""\n    This profiler simply records the duration of actions (in seconds) and reports\n    the mean duration of each action and the total time spent over the entire training run.\n    """"""\n\n    def __init__(self, output_filename: str = None):\n        """"""\n        Params:\n            output_filename (str): optionally save profile results to file instead of printing\n                to std out when training is finished.\n        """"""\n        self.current_actions = {}\n        self.recorded_durations = defaultdict(list)\n\n        self.output_fname = output_filename\n        self.output_file = open(self.output_fname, \'w\') if self.output_fname else None\n\n        streaming_out = [self.output_file.write] if self.output_file else [log.info]\n        super().__init__(output_streams=streaming_out)\n\n    def start(self, action_name: str) -> None:\n        if action_name in self.current_actions:\n            raise ValueError(\n                f""Attempted to start {action_name} which has already started.""\n            )\n        self.current_actions[action_name] = time.monotonic()\n\n    def stop(self, action_name: str) -> None:\n        end_time = time.monotonic()\n        if action_name not in self.current_actions:\n            raise ValueError(\n                f""Attempting to stop recording an action ({action_name}) which was never started.""\n            )\n        start_time = self.current_actions.pop(action_name)\n        duration = end_time - start_time\n        self.recorded_durations[action_name].append(duration)\n\n    def summary(self) -> str:\n        output_string = ""\\n\\nProfiler Report\\n""\n\n        def log_row(action, mean, total):\n            return f""{os.linesep}{action:<20s}\\t|  {mean:<15}\\t|  {total:<15}""\n\n        output_string += log_row(""Action"", ""Mean duration (s)"", ""Total time (s)"")\n        output_string += f""{os.linesep}{\'-\' * 65}""\n        for action, durations in self.recorded_durations.items():\n            output_string += log_row(\n                action, f""{np.mean(durations):.5}"", f""{np.sum(durations):.5}"",\n            )\n        output_string += os.linesep\n        return output_string\n\n    def describe(self):\n        """"""Logs a profile report after the conclusion of the training run.""""""\n        super().describe()\n        if self.output_file:\n            self.output_file.flush()\n\n    def __del__(self):\n        """"""Close profiler\'s stream.""""""\n        if self.output_file:\n            self.output_file.close()\n\n\nclass AdvancedProfiler(BaseProfiler):\n    """"""\n    This profiler uses Python\'s cProfiler to record more detailed information about\n    time spent in each function call recorded during a given action. The output is quite\n    verbose and you should only use this if you want very detailed reports.\n    """"""\n\n    def __init__(self, output_filename: str = None, line_count_restriction: float = 1.0):\n        """"""\n        Args:\n            output_filename: optionally save profile results to file instead of printing\n                to std out when training is finished.\n            line_count_restriction: this can be used to limit the number of functions\n                reported for each action. either an integer (to select a count of lines),\n                or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines)\n        """"""\n        self.profiled_actions = {}\n        self.line_count_restriction = line_count_restriction\n\n        self.output_fname = output_filename\n        self.output_file = open(self.output_fname, \'w\') if self.output_fname else None\n\n        streaming_out = [self.output_file.write] if self.output_file else [log.info]\n        super().__init__(output_streams=streaming_out)\n\n    def start(self, action_name: str) -> None:\n        if action_name not in self.profiled_actions:\n            self.profiled_actions[action_name] = cProfile.Profile()\n        self.profiled_actions[action_name].enable()\n\n    def stop(self, action_name: str) -> None:\n        pr = self.profiled_actions.get(action_name)\n        if pr is None:\n            raise ValueError(  # pragma: no-cover\n                f""Attempting to stop recording an action ({action_name}) which was never started.""\n            )\n        pr.disable()\n\n    def summary(self) -> str:\n        recorded_stats = {}\n        for action_name, pr in self.profiled_actions.items():\n            s = io.StringIO()\n            ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(\'cumulative\')\n            ps.print_stats(self.line_count_restriction)\n            recorded_stats[action_name] = s.getvalue()\n\n        # log to standard out\n        output_string = f""{os.linesep}Profiler Report{os.linesep}""\n        for action, stats in recorded_stats.items():\n            output_string += f""{os.linesep}Profile stats for: {action}{os.linesep}{stats}""\n\n        return output_string\n\n    def describe(self):\n        """"""Logs a profile report after the conclusion of the training run.""""""\n        super().describe()\n        if self.output_file:\n            self.output_file.flush()\n\n    def __del__(self):\n        """"""Close profiler\'s stream.""""""\n        if self.output_file:\n            self.output_file.close()\n'"
pytorch_lightning/pt_overrides/__init__.py,0,"b'""""""\n.. warning:: `pt_overrides` package has been renamed to `overrides` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`pt_overrides` package has been renamed to `overrides` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n'"
pytorch_lightning/pt_overrides/override_data_parallel.py,0,"b'""""""\n.. warning:: `override_data_parallel` module has been renamed to `data_parallel` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`override_data_parallel` module has been renamed to `data_parallel` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.overrides.data_parallel import (  # noqa: F402 E402\n    get_a_var, parallel_apply, LightningDataParallel, LightningDistributedDataParallel)\n'"
pytorch_lightning/root_module/__init__.py,0,"b'""""""\n.. warning:: `root_module` package has been renamed to `core` since v0.6.0.\n The deprecated package name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module` package has been renamed to `core` since v0.6.0.""\n               "" The deprecated package name will be removed in v0.8.0."", DeprecationWarning)\n'"
pytorch_lightning/root_module/decorators.py,0,"b'""""""\n.. warning:: `root_module.decorators` module has been renamed to `core.decorators` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module.decorators` module has been renamed to `core.decorators` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.decorators import *  # noqa: F403 E402\n'"
pytorch_lightning/root_module/grads.py,0,"b'""""""\n.. warning:: `root_module.grads` module has been renamed to `core.grads` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module.grads` module has been renamed to `core.grads` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.grads import *  # noqa: F403 E402\n'"
pytorch_lightning/root_module/hooks.py,0,"b'""""""\n.. warning:: `root_module.hooks` module has been renamed to `core.hooks` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module.hooks` module has been renamed to `core.hooks` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.hooks import *  # noqa: F403 E402\n'"
pytorch_lightning/root_module/memory.py,0,"b'""""""\n.. warning:: `root_module.memory` module has been renamed to `core.memory` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module.memory` module has been renamed to `core.memory` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.memory import *  # noqa: F403 E402\n'"
pytorch_lightning/root_module/model_saving.py,0,"b'""""""\n.. warning:: `root_module.model_saving` module has been renamed to `core.saving` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module.model_saving` module has been renamed to `core.saving` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.saving import *  # noqa: F403 E402\n'"
pytorch_lightning/root_module/root_module.py,0,"b'""""""\n.. warning:: `root_module.root_module` module has been renamed to `core.lightning` since v0.6.0.\n The deprecated module name will be removed in v0.8.0.\n""""""\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\nrank_zero_warn(""`root_module.root_module` module has been renamed to `core.lightning` since v0.6.0.""\n               "" The deprecated module name will be removed in v0.8.0."", DeprecationWarning)\n\nfrom pytorch_lightning.core.lightning import *  # noqa: F403 E402\n'"
pytorch_lightning/trainer/__init__.py,6,"b'""""""\nOnce you\'ve organized your PyTorch code into a LightningModule,\nthe Trainer automates everything else.\n\n.. figure:: /_images/lightning_module/pt_trainer.png\n   :alt: Convert from PyTorch to Lightning\n\nThis abstraction achieves the following:\n\n    1. You maintain control over all aspects via PyTorch code without an added abstraction.\n\n    2. The trainer uses best practices embedded by contributors and users\n       from top AI labs such as Facebook AI Research, NYU, MIT, Stanford, etc...\n\n    3. The trainer allows overriding any key part that you don\'t want automated.\n\n-----------\n\nBasic use\n---------\n\nThis is the basic use of the trainer:\n\n.. code-block:: python\n\n    from pytorch_lightning import Trainer\n\n    model = MyLightningModule()\n\n    trainer = Trainer()\n    trainer.fit(model)\n\n\n--------\n\nBest Practices\n--------------\nFor cluster computing, it\'s recommended you structure your\nmain.py file this way\n\n.. code-block:: python\n\n    from argparse import ArgumentParser\n\n    def main(hparams):\n        model = LightningModule()\n        trainer = Trainer(gpus=hparams.gpus)\n        trainer.fit(model)\n\n    if __name__ == \'__main__\':\n        parser = ArgumentParser()\n        parser.add_argument(\'--gpus\', default=None)\n        args = parser.parse_args()\n\n        main(args)\n\nSo you can run it like so:\n\n.. code-block:: bash\n\n    python main.py --gpus 2\n\n\n.. note::\n    If you want to stop a training run early, you can press ""Ctrl + C"" on your keyboard.\n    The trainer will catch the `KeyboardInterrupt` and attempt a graceful shutdown, including\n    running callbacks such as `on_train_end`. The trainer object will also set an attribute\n    `interrupted` to `True` in such cases. If you have a callback which shuts down compute\n    resources, for example, you can conditionally run the shutdown logic for only uninterrupted runs.\n\n------------\n\nTesting\n-------\nOnce you\'re done training, feel free to run the test set!\n(Only right before publishing your paper or pushing to production)\n\n.. code-block:: python\n\n    trainer.test()\n\n------------\n\nDeployment / prediction\n-----------------------\nYou just trained a LightningModule which is also just a torch.nn.Module.\nUse it to do whatever!\n\n.. code-block:: python\n\n    # load model\n    pretrained_model = LightningModule.load_from_checkpoint(PATH)\n    pretrained_model.freeze()\n\n    # use it for finetuning\n    def forward(self, x):\n        features = pretrained_model(x)\n        classes = classifier(features)\n\n    # or for prediction\n    out = pretrained_model(x)\n    api_write({\'response\': out}\n\n------------\n\nReproducibility\n---------------\n\nTo ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,\nand set ``deterministic``` flag in ``Trainer``.\n\n.. code-block:: python\n\n    from pytorch_lightning import Trainer, seed_everything\n\n    seed_everything(42)\n    # sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n    model = Model()\n    trainer = Trainer(deterministic=True)\n\n\n-------\n\nTrainer flags\n-------------\n\naccumulate_grad_batches\n^^^^^^^^^^^^^^^^^^^^^^^\nAccumulates grads every k batches or as set up in the dict.\n\n.. code-block:: python\n\n    # default used by the Trainer (no accumulation)\n    trainer = Trainer(accumulate_grad_batches=1)\n\nExample::\n\n    # accumulate every 4 batches (effective batch size is batch*4)\n    trainer = Trainer(accumulate_grad_batches=4)\n\n    # no accumulation for epochs 1-4. accumulate 3 for epochs 5-10. accumulate 20 after that\n    trainer = Trainer(accumulate_grad_batches={5: 3, 10: 20})\n\namp_level\n^^^^^^^^^\nThe optimization level to use (O1, O2, etc...)\nfor 16-bit GPU precision (using NVIDIA apex under the hood).\n\nCheck `NVIDIA apex docs <https://nvidia.github.io/apex/amp.html#opt-levels>`_ for level\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(amp_level=\'O1\')\n\nauto_scale_batch_size\n^^^^^^^^^^^^^^^^^^^^^\nAutomatically tries to find the largest batch size that fits into memory,\nbefore any training.\n\n.. code-block:: python\n\n    # default used by the Trainer (no scaling of batch size)\n    trainer = Trainer(auto_scale_batch_size=None)\n\n    # run batch size scaling, result overrides hparams.batch_size\n    trainer = Trainer(auto_scale_batch_size=\'binsearch\')\n\nauto_lr_find\n^^^^^^^^^^^^\nRuns a learning rate finder algorithm (see this `paper <https://arxiv.org/abs/1506.01186>`_)\nbefore any training, to find optimal initial learning rate.\n\n.. code-block:: python\n\n    # default used by the Trainer (no learning rate finder)\n    trainer = Trainer(auto_lr_find=False)\n\nExample::\n\n    # run learning rate finder, results override hparams.learning_rate\n    trainer = Trainer(auto_lr_find=True)\n\n    # run learning rate finder, results override hparams.my_lr_arg\n    trainer = Trainer(auto_lr_find=\'my_lr_arg\')\n\n.. note::\n    See the `learning rate finder guide <lr_finder.rst>`_\n\nbenchmark\n^^^^^^^^^\n\nIf true enables cudnn.benchmark.\nThis flag is likely to increase the speed of your system if your\ninput sizes don\'t change. However, if it does, then it will likely\nmake your system slower.\n\nThe speedup comes from allowing the cudnn auto-tuner to find the best\nalgorithm for the hardware `[see discussion here]\n<https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936>`_.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(benchmark=False)\n\ndeterministic\n^^^^^^^^^^^^^\n\nIf true enables cudnn.deterministic.\nMight make your system slower, but ensures reproducibility.\nAlso sets ``$HOROVOD_FUSION_THRESHOLD=0``.\n\nFor more info check `[pytorch docs]\n<https://pytorch.org/docs/stable/notes/randomness.html>`_.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(deterministic=False)\n\ncallbacks\n^^^^^^^^^\n\nAdd a list of user defined callbacks. These callbacks DO NOT replace the explicit callbacks\n(loggers, EarlyStopping or ModelCheckpoint).\n\n.. note:: Only user defined callbacks (ie: Not EarlyStopping or ModelCheckpoint)\n\n.. code-block:: python\n\n    # a list of callbacks\n    callbacks = [PrintCallback()]\n    trainer = Trainer(callbacks=callbacks)\n\nExample::\n\n    from pytorch_lightning.callbacks import Callback\n\n    class PrintCallback(Callback):\n        def on_train_start(self, trainer, pl_module):\n            print(""Training is started!"")\n        def on_train_end(self, trainer, pl_module):\n            print(""Training is done."")\n\ncheck_val_every_n_epoch\n^^^^^^^^^^^^^^^^^^^^^^^\n\nCheck val every n train epochs.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(check_val_every_n_epoch=1)\n\n    # run val loop every 10 training epochs\n    trainer = Trainer(check_val_every_n_epoch=10)\n\ncheckpoint_callback\n^^^^^^^^^^^^^^^^^^^\nCallback for checkpointing.\n\n.. code-block:: python\n\n    trainer = Trainer(checkpoint_callback=checkpoint_callback)\n\nExample::\n\n    from pytorch_lightning.callbacks import ModelCheckpoint\n\n    # default used by the Trainer\n    checkpoint_callback = ModelCheckpoint(\n        filepath=os.getcwd(),\n        save_top_k=True,\n        verbose=True,\n        monitor=\'val_loss\',\n        mode=\'min\',\n        prefix=\'\'\n    )\n\ndefault_root_dir\n^^^^^^^^^^^^^^^^^\n\nDefault path for logs and weights when no logger\nor :class:`pytorch_lightning.callbacks.ModelCheckpoint` callback passed.\nOn certain clusters you might want to separate where logs and checkpoints\nare stored. If you don\'t then use this method for convenience.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(default_root_path=os.getcwd())\n\ndistributed_backend\n^^^^^^^^^^^^^^^^^^^\nThe distributed backend to use.\n\n- (```dp```) is DataParallel (split batch among GPUs of same machine)\n- (```ddp```) is DistributedDataParallel (each gpu on each node trains, and syncs grads)\n- (```ddp_cpu```) is DistributedDataParallel on CPU (same as `ddp`, but does not use GPUs.\n  Useful for multi-node CPU training or single-node debugging. Note that this will **not** give\n  a speedup on a single node, since Torch already makes effient use of multiple CPUs on a single\n  machine.)\n- (```ddp2```) dp on node, ddp across nodes. Useful for things like increasing\n    the number of negative samples\n\n.. code-block:: python\n\n    # default used by the Trainer\n    trainer = Trainer(distributed_backend=None)\n\nExample::\n\n    # dp = DataParallel\n    trainer = Trainer(gpus=2, distributed_backend=\'dp\')\n\n    # ddp = DistributedDataParallel\n    trainer = Trainer(gpus=2, num_nodes=2, distributed_backend=\'ddp\')\n\n    # ddp2 = DistributedDataParallel + dp\n    trainer = Trainer(gpus=2, num_nodes=2, distributed_backend=\'ddp2\')\n\n.. note:: this option does not apply to TPU. TPUs use ```ddp``` by default (over each core)\n\nSee Also:\n    - `Multi-GPU training guide <multi_gpu.rst>`_\n    - `Multi-node (SLURM) guide <slurm.rst>`_\n\nearly_stop_callback\n^^^^^^^^^^^^^^^^^^^\n\nCallback for early stopping.\nearly_stop_callback (:class:`pytorch_lightning.callbacks.EarlyStopping`)\n\n- ``True``: A default callback monitoring ``\'val_loss\'`` is created.\n   Will raise an error if ``\'val_loss\'`` is not found.\n- ``False``: Early stopping will be disabled.\n- ``None``: The default callback monitoring ``\'val_loss\'`` is created.\n- Default: ``None``.\n\n.. code-block:: python\n\n    trainer = Trainer(early_stop_callback=early_stop_callback)\n\nExample::\n\n    from pytorch_lightning.callbacks import EarlyStopping\n\n    # default used by the Trainer\n    early_stop_callback = EarlyStopping(\n        monitor=\'val_loss\',\n        patience=3,\n        strict=False,\n        verbose=False,\n        mode=\'min\'\n    )\n\n.. note:: If ``\'val_loss\'`` is not found will work as if early stopping is disabled.\n\nfast_dev_run\n^^^^^^^^^^^^\n\nRuns 1 batch of train, test  and val to find any bugs (ie: a sort of unit test).\n\nUnder the hood the pseudocode looks like this:\n\n.. code-block:: python\n\n    # loading\n    __init__()\n    prepare_data\n\n    # test training step\n    training_batch = next(train_dataloader)\n    training_step(training_batch)\n\n    # test val step\n    val_batch = next(val_dataloader)\n    out = validation_step(val_batch)\n    validation_epoch_end([out])\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(fast_dev_run=False)\n\n    # runs 1 train, val, test  batch and program ends\n    trainer = Trainer(fast_dev_run=True)\n\ngpus\n^^^^\n\n- Number of GPUs to train on\n- or Which GPUs to train on\n- can handle strings\n\nExample::\n\n    # default used by the Trainer (ie: train on CPU)\n    trainer = Trainer(gpus=None)\n\n    # int: train on 2 gpus\n    trainer = Trainer(gpus=2)\n\n    # list: train on GPUs 1, 4 (by bus ordering)\n    trainer = Trainer(gpus=[1, 4])\n    trainer = Trainer(gpus=\'1, 4\') # equivalent\n\n    # -1: train on all gpus\n    trainer = Trainer(gpus=-1)\n    trainer = Trainer(gpus=\'-1\') # equivalent\n\n    # combine with num_nodes to train on multiple GPUs across nodes\n    # uses 8 gpus in total\n    trainer = Trainer(gpus=2, num_nodes=4)\n\nSee Also:\n    - `Multi-GPU training guide <multi_gpu.rst>`_\n\ngradient_clip_val\n^^^^^^^^^^^^^^^^^\nGradient clipping value\n\n- 0 means don\'t clip.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(gradient_clip_val=0.0)\n\n\ngradient_clip:\n\n.. warning:: .. deprecated:: 0.5.0\n\n    Use `gradient_clip_val` instead. Will remove 0.8.0.\n\nlog_gpu_memory\n^^^^^^^^^^^^^^\nOptions:\n\n- None\n- \'min_max\'\n- \'all\'\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(log_gpu_memory=None)\n\n    # log all the GPUs (on master node only)\n    trainer = Trainer(log_gpu_memory=\'all\')\n\n    # log only the min and max memory on the master node\n    trainer = Trainer(log_gpu_memory=\'min_max\')\n\n.. note:: Might slow performance because it uses the output of nvidia-smi.\n\nlog_save_interval\n^^^^^^^^^^^^^^^^^\n\nWrites logs to disk this often.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(log_save_interval=100)\n\nlogger\n^^^^^^\n\n`Logger <loggers.rst>`_ (or iterable collection of loggers) for experiment tracking.\n\n.. code-block:: python\n\n    Trainer(logger=logger)\n\nExample::\n\n    from pytorch_lightning.loggers import TensorBoardLogger\n\n    # default logger used by trainer\n    logger = TensorBoardLogger(\n        save_dir=os.getcwd(),\n        version=self.slurm_job_id,\n        name=\'lightning_logs\'\n    )\n\nmax_epochs\n^^^^^^^^^^\nStop training once this number of epochs is reached\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(max_epochs=1000)\n\nmax_nb_epochs:\n\n.. warning:: .. deprecated:: 0.5.0\n\n    Use `max_epochs` instead. Will remove 0.8.0.\n\nmin_epochs\n^^^^^^^^^^\nForce training for at least these many epochs\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(min_epochs=1)\n\nmin_nb_epochs:\n\n.. warning:: deprecated:: 0.5.0\n    Use `min_epochs` instead. Will remove 0.8.0.\n\nmax_steps\n^^^^^^^^^\nStop training after this number of steps\nTraining will stop if max_steps or max_epochs have reached (earliest).\n\n.. code-block:: python\n\n    # Default (disabled)\n    trainer = Trainer(max_steps=None)\n\nExample::\n\n    # Stop after 100 steps\n    trainer = Trainer(max_steps=100)\n\nmin_steps\n^^^^^^^^^\n\nForce training for at least these number of steps.\nTrainer will train model for at least min_steps or min_epochs (latest).\n\n.. code-block:: python\n\n    # Default (disabled)\n    trainer = Trainer(min_steps=None)\n\nExample::\n\n    # Run at least for 100 steps (disable min_epochs)\n    trainer = Trainer(min_steps=100, min_epochs=0)\n\nnum_nodes\n^^^^^^^^^\n\nNumber of GPU nodes for distributed training.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(num_nodes=1)\n\n    # to train on 8 nodes\n    trainer = Trainer(num_nodes=8)\n\nnb_gpu_nodes:\n\n.. warning:: .. deprecated:: 0.5.0\n\n    Use `num_nodes` instead. Will remove 0.8.0.\n\nnum_processes\n^^^^^^^^^^^^^\n\nNumber of processes to train with. Automatically set to the number of GPUs\nwhen using ``distrbuted_backend=""ddp""``. Set to a number greater than 1 when\nusing ``distributed_backend=""ddp_cpu""`` to mimic distributed training on a\nmachine without GPUs. This is useful for debugging, but **will not** provide\nany speedup, since single-process Torch already makes effient use of multiple\nCPUs.\n\nExample::\n\n    # Simulate DDP for debugging on your GPU-less laptop\n    trainer = Trainer(distributed_backend=""ddp_cpu"", num_processes=2)\n\nnum_sanity_val_steps\n^^^^^^^^^^^^^^^^^^^^\n\nSanity check runs n batches of val before starting the training routine.\nThis catches any bugs in your validation without having to wait for the first validation check.\nThe Trainer uses 5 steps by default. Turn it off or modify it here.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(num_sanity_val_steps=5)\n\n    # turn it off\n    trainer = Trainer(num_sanity_val_steps=0)\n\nnb_sanity_val_steps:\n\n.. warning:: .. deprecated:: 0.5.0\n\n    Use `num_sanity_val_steps` instead. Will remove 0.8.0.\n\nnum_tpu_cores\n^^^^^^^^^^^^^\n.. warning:: .. deprecated:: 0.7.6\n\n    Use `tpu_cores` instead. Will remove 0.9.0.\n\nExample::\n\n    python -m torch_xla.distributed.xla_dist\n    --tpu=$TPU_POD_NAME\n    --conda-env=torch-xla-nightly\n    --env=XLA_USE_BF16=1\n    -- python your_trainer_file.py\n\ntpu_cores\n^^^^^^^^^\n- How many TPU cores to train on (1 or 8).\n- Which TPU core to train on [1-8]\n\nA single TPU v2 or v3 has 8 cores. A TPU pod has\nup to 2048 cores. A slice of a POD means you get as many cores\nas you request.\n\nYour effective batch size is batch_size * total tpu cores.\n\n.. note:: No need to add a DistributedDataSampler, Lightning automatically does it for you.\n\nThis parameter can be either 1 or 8.\n\nExample::\n\n    # your_trainer_file.py\n\n    # default used by the Trainer (ie: train on CPU)\n    trainer = Trainer(tpu_cores=None)\n\n    # int: train on a single core\n    trainer = Trainer(tpu_cores=1)\n\n    # list: train on a single selected core\n    trainer = Trainer(tpu_cores=[2])\n\n    # int: train on all cores few cores\n    trainer = Trainer(tpu_cores=8)\n\n    # for 8+ cores must submit via xla script with\n    # a max of 8 cores specified. The XLA script\n    # will duplicate script onto each TPU in the POD\n    trainer = Trainer(tpu_cores=8)\n\nTo train on more than 8 cores (ie: a POD),\nsubmit this script using the xla_dist script.\n\nExample::\n\n    python -m torch_xla.distributed.xla_dist\n    --tpu=$TPU_POD_NAME\n    --conda-env=torch-xla-nightly\n    --env=XLA_USE_BF16=1\n    -- python your_trainer_file.py\n\noverfit_pct\n^^^^^^^^^^^\nUses this much data of all datasets (training, validation, test).\nUseful for quickly debugging or trying to overfit on purpose.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(overfit_pct=0.0)\n\n    # use only 1% of the train, test, val datasets\n    trainer = Trainer(overfit_pct=0.01)\n\n    # equivalent:\n    trainer = Trainer(\n        train_percent_check=0.01,\n        val_percent_check=0.01,\n        test_percent_check=0.01\n    )\n\nSee Also:\n    - `train_percent_check`_\n    - `val_percent_check`_\n    - `test_percent_check`_\n\n\nprecision\n^^^^^^^^^\nFull precision (32), half precision (16).\nCan be used on CPU, GPU or TPUs.\n\nIf used on TPU will use torch.bfloat16 but tensor printing\nwill still show torch.float32.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(precision=32)\n\n    # 16-bit precision\n    trainer = Trainer(precision=16)\n\n    # one day\n    trainer = Trainer(precision=8|4|2)\n\nprint_nan_grads\n^^^^^^^^^^^^^^^\n\n.. warning:: .. deprecated:: 0.7.2.\n\n    Has no effect. When detected, NaN grads will be printed automatically.\n    Will remove 0.9.0.\n\n\nprocess_position\n^^^^^^^^^^^^^^^^\nOrders the progress bar. Useful when running multiple trainers on the same node.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(process_position=0)\n\nNote:\n    This argument is ignored if a custom callback is passed to :paramref:`~Trainer.callbacks`.\n\nprofiler\n^^^^^^^^\nTo profile individual steps during training and assist in identifying bottlenecks.\n\nSee the `profiler documentation <profiler.rst>`_. for more details.\n\nExample::\n\n    from pytorch_lightning.profiler import Profiler, AdvancedProfiler\n\n    # default used by the Trainer\n    trainer = Trainer(profiler=None)\n\n    # to profile standard training events\n    trainer = Trainer(profiler=True)\n\n    # equivalent to profiler=True\n    profiler = Profiler()\n    trainer = Trainer(profiler=profiler)\n\n    # advanced profiler for function-level stats\n    profiler = AdvancedProfiler()\n    trainer = Trainer(profiler=profiler)\n\nprogress_bar_refresh_rate\n^^^^^^^^^^^^^^^^^^^^^^^^^\nHow often to refresh progress bar (in steps).\nIn notebooks, faster refresh rates (lower number) is known to crash them\nbecause of their screen refresh rates, so raise it to 50 or more.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(progress_bar_refresh_rate=1)\n\n    # disable progress bar\n    trainer = Trainer(progress_bar_refresh_rate=0)\n\nNote:\n    This argument is ignored if a custom callback is passed to :paramref:`~Trainer.callbacks`.\n\nreload_dataloaders_every_epoch\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nSet to True to reload dataloaders every epoch.\n\n.. code-block:: python\n\n    # if False (default)\n    train_loader = model.train_dataloader()\n    for epoch in epochs:\n        for batch in train_loader:\n            ...\n\n    # if True\n    for epoch in epochs:\n        train_loader = model.train_dataloader()\n        for batch in train_loader:\n\nreplace_sampler_ddp\n^^^^^^^^^^^^^^^^^^^\nEnables auto adding of distributed sampler.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(replace_sampler_ddp=True)\n\nBy setting to False, you have to add your own distributed sampler:\n\nExample::\n\n    # default used by the Trainer\n    sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=True)\n    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n\nresume_from_checkpoint\n^^^^^^^^^^^^^^^^^^^^^^\nTo resume training from a specific checkpoint pass in the path here.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(resume_from_checkpoint=None)\n\n    # resume from a specific checkpoint\n    trainer = Trainer(resume_from_checkpoint=\'some/path/to/my_checkpoint.ckpt\')\n\nrow_log_interval\n^^^^^^^^^^^^^^^^\n\nHow often to add logging rows (does not write to disk)\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(row_log_interval=10)\n\n\nadd_row_log_interval:\n\n.. warning:: .. deprecated:: 0.5.0\n\n    Use `row_log_interval` instead. Will remove 0.8.0.\n\nuse_amp:\n\n.. warning:: .. deprecated:: 0.7.0\n\n    Use `precision` instead. Will remove 0.9.0.\n\nshow_progress_bar\n^^^^^^^^^^^^^^^^^\n\n.. warning:: .. deprecated:: 0.7.2\n\n    Set `progress_bar_refresh_rate` to 0 instead. Will remove 0.9.0.\n\ntest_percent_check\n^^^^^^^^^^^^^^^^^^\n\nHow much of test dataset to check.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(test_percent_check=1.0)\n\n    # run through only 25% of the test set each epoch\n    trainer = Trainer(test_percent_check=0.25)\n\nval_check_interval\n^^^^^^^^^^^^^^^^^^\n\nHow often within one training epoch to check the validation set.\nCan specify as float or int.\n\n- use (float) to check within a training epoch\n- use (int) to check every n steps (batches)\n\n.. code-block:: python\n\n    # default used by the Trainer\n    trainer = Trainer(val_check_interval=1.0)\n\nExample::\n\n    # check validation set 4 times during a training epoch\n    trainer = Trainer(val_check_interval=0.25)\n\n    # check validation set every 1000 training batches\n    # use this when using iterableDataset and your dataset has no length\n    # (ie: production cases with streaming data)\n    trainer = Trainer(val_check_interval=1000)\n\ntrack_grad_norm\n^^^^^^^^^^^^^^^\n\n- no tracking (-1)\n- Otherwise tracks that norm (2 for 2-norm)\n\n.. code-block:: python\n\n    # default used by the Trainer\n    trainer = Trainer(track_grad_norm=-1)\n\nExample::\n\n    # track the 2-norm\n    trainer = Trainer(track_grad_norm=2)\n\ntrain_percent_check\n^^^^^^^^^^^^^^^^^^^\n\nHow much of training dataset to check.\nUseful when debugging or testing something that happens at the end of an epoch.\n\n.. code-block::python\n\n    # default used by the Trainer\n    trainer = Trainer(train_percent_check=1.0)\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(train_percent_check=1.0)\n\n    # run through only 25% of the training set each epoch\n    trainer = Trainer(train_percent_check=0.25)\n\ntruncated_bptt_steps\n^^^^^^^^^^^^^^^^^^^^\n\nTruncated back prop breaks performs backprop every k steps of\na much longer sequence.\n\nIf this is enabled, your batches will automatically get truncated\nand the trainer will apply Truncated Backprop to it.\n\n(`Williams et al. ""An efficient gradient-based algorithm for on-line training of\nrecurrent network trajectories.""\n<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7941&rep=rep1&type=pdf>`_)\n\nExample::\n\n    # default used by the Trainer (ie: disabled)\n    trainer = Trainer(truncated_bptt_steps=None)\n\n    # backprop every 5 steps in a batch\n    trainer = Trainer(truncated_bptt_steps=5)\n\n.. note::  Make sure your batches have a sequence dimension.\n\nLightning takes care to split your batch along the time-dimension.\n\n.. code-block:: python\n\n    # we use the second as the time dimension\n    # (batch, time, ...)\n    sub_batch = batch[0, 0:t, ...]\n\nUsing this feature requires updating your LightningModule\'s\n:meth:`pytorch_lightning.core.LightningModule.training_step` to include a `hiddens` arg\nwith the hidden\n\n.. code-block:: python\n\n        # Truncated back-propagation through time\n        def training_step(self, batch, batch_idx, hiddens):\n            # hiddens are the hiddens from the previous truncated backprop step\n            out, hiddens = self.lstm(data, hiddens)\n\n            return {\n                ""loss"": ...,\n                ""hiddens"": hiddens  # remember to detach() this\n            }\n\nTo modify how the batch is split,\noverride :meth:`pytorch_lightning.core.LightningModule.tbptt_split_batch`:\n\n.. code-block:: python\n\n        class LitMNIST(pl.LightningModule):\n            def tbptt_split_batch(self, batch, split_size):\n                # do your own splitting on the batch\n                return splits\n\n\nval_percent_check\n^^^^^^^^^^^^^^^^^\n\nHow much of validation dataset to check.\nUseful when debugging or testing something that happens at the end of an epoch.\n\nExample::\n\n    # default used by the Trainer\n    trainer = Trainer(val_percent_check=1.0)\n\n    # run through only 25% of the validation set each epoch\n    trainer = Trainer(val_percent_check=0.25)\n\nweights_save_path\n^^^^^^^^^^^^^^^^^\nDirectory of where to save weights if specified.\n\n.. code-block:: python\n\n    # default used by the Trainer\n    trainer = Trainer(weights_save_path=os.getcwd())\n\nExample::\n\n    # save to your custom path\n    trainer = Trainer(weights_save_path=\'my/path\')\n\n    # if checkpoint callback used, then overrides the weights path\n    # **NOTE: this saves weights to some/path NOT my/path\n    checkpoint_callback = ModelCheckpoint(filepath=\'some/path\')\n    trainer = Trainer(\n        checkpoint_callback=checkpoint_callback,\n        weights_save_path=\'my/path\'\n    )\n\nweights_summary\n^^^^^^^^^^^^^^^\nPrints a summary of the weights when training begins.\nOptions: \'full\', \'top\', None.\n\nExample::\n\n    # default used by the Trainer (ie: print summary of top level modules)\n    trainer = Trainer(weights_summary=\'top\')\n\n    # print full summary of all modules and submodules\n    trainer = Trainer(weights_summary=\'full\')\n\n    # don\'t print a summary\n    trainer = Trainer(weights_summary=None)\n\nTrainer class\n-------------\n\n""""""\n\nfrom pytorch_lightning.trainer.trainer import Trainer\nfrom pytorch_lightning.trainer.seed import seed_everything\n\n__all__ = [\'Trainer\', \'seed_everything\']\n'"
pytorch_lightning/trainer/auto_mix_precision.py,0,"b'from abc import ABC\nimport torch\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.utilities import rank_zero_warn\n\ntry:\n    from apex import amp\nexcept ImportError:\n    APEX_AVAILABLE = False\nelse:\n    APEX_AVAILABLE = True\n\n\nclass TrainerAMPMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    precision: int\n    use_native_amp: bool\n\n    def init_amp(self, use_amp):\n        # TODO: remove in v 0.8.0\n        if self.use_native_amp:\n            rank_zero_warn(""`amp_level` has been deprecated since v0.7.4 ""\n                           ""(native amp does not require it)""\n                           "" and this argument will be removed in v0.8.0"", DeprecationWarning)\n\n        # Backward compatibility, TODO: remove in v0.9.0\n        if use_amp is not None:\n            rank_zero_warn(""`use_amp` has been replaced by `precision` since v0.7.0""\n                           "" and this argument will be removed in v0.9.0"", DeprecationWarning)\n            self.precision = 16 if use_amp else 32\n\n        assert self.precision in (16, 32), \'only 32 or 16 bit precision supported\'\n\n        if use_amp and self.use_native_amp:\n            log.info(\'Using 16bit precision.\')\n            return\n\n        # TODO: remove all below for v0.8.0\n        if use_amp and not APEX_AVAILABLE:  # pragma: no-cover\n            raise ModuleNotFoundError(""""""\n            You set `use_amp=True` but do not have apex installed.\n            Install apex first using this guide and rerun with use_amp=True:\n            https://github.com/NVIDIA/apex#linux\n\n            this run will NOT use 16 bit precision\n            """""")\n\n        if self.use_amp:\n            log.info(\'Using 16bit precision.\')\n\n    @property\n    def use_amp(self) -> bool:\n        return self.precision == 16\n'"
pytorch_lightning/trainer/callback_config.py,0,"b'import os\nfrom abc import ABC, abstractmethod\nfrom typing import Union, List\n\n\nfrom pytorch_lightning.callbacks import Callback, ModelCheckpoint, EarlyStopping, ProgressBarBase, ProgressBar\nfrom pytorch_lightning.loggers import LightningLoggerBase\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\n\n\nclass TrainerCallbackConfigMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    callbacks: List[Callback]\n    default_root_dir: str\n    logger: Union[LightningLoggerBase, bool]\n    weights_save_path: str\n    ckpt_path: str\n    checkpoint_callback: ModelCheckpoint\n\n    @property\n    @abstractmethod\n    def slurm_job_id(self) -> int:\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def save_checkpoint(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def configure_checkpoint_callback(self):\n        """"""\n        Weight path set in this priority:\n        Checkpoint_callback\'s path (if passed in).\n        User provided weights_saved_path\n        Otherwise use os.getcwd()\n        """"""\n        ckpt_path = self.default_root_dir\n        if self.checkpoint_callback:\n            # init a default one\n            if self.logger is not None:\n                save_dir = (getattr(self.logger, \'save_dir\', None) or\n                            getattr(self.logger, \'_save_dir\', None) or\n                            self.default_root_dir)\n\n                # weights_save_path overrides anything\n                if self.weights_save_path is not None:\n                    save_dir = self.weights_save_path\n\n                version = self.logger.version if isinstance(\n                    self.logger.version, str) else f\'version_{self.logger.version}\'\n                ckpt_path = os.path.join(\n                    save_dir,\n                    self.logger.name,\n                    version,\n                    ""checkpoints""\n                )\n            else:\n                ckpt_path = os.path.join(self.default_root_dir, ""checkpoints"")\n\n            # when no val step is defined, use \'loss\' otherwise \'val_loss\'\n            train_step_only = not self.is_overridden(\'validation_step\')\n            monitor_key = \'loss\' if train_step_only else \'val_loss\'\n\n            if self.checkpoint_callback is True:\n                os.makedirs(ckpt_path, exist_ok=True)\n                self.checkpoint_callback = ModelCheckpoint(\n                    filepath=ckpt_path,\n                    monitor=monitor_key\n                )\n            # If user specified None in filepath, override with runtime default\n            elif isinstance(self.checkpoint_callback, ModelCheckpoint) \\\n                    and self.checkpoint_callback.dirpath is None:\n                self.checkpoint_callback.dirpath = ckpt_path\n                self.checkpoint_callback.filename = \'{epoch}\'\n                os.makedirs(self.checkpoint_callback.dirpath, exist_ok=True)\n        elif self.checkpoint_callback is False:\n            self.checkpoint_callback = None\n\n        self.ckpt_path = ckpt_path\n\n        if self.checkpoint_callback:\n            # set the path for the callbacks\n            self.checkpoint_callback.save_function = self.save_checkpoint\n\n            # if checkpoint callback used, then override the weights path\n            self.weights_save_path = self.checkpoint_callback.dirpath\n\n        # if weights_save_path is still none here, set to current working dir\n        if self.weights_save_path is None:\n            self.weights_save_path = self.default_root_dir\n\n    def configure_early_stopping(self, early_stop_callback):\n        if early_stop_callback is True or None:\n            self.early_stop_callback = EarlyStopping(\n                monitor=\'val_loss\',\n                patience=3,\n                strict=True,\n                verbose=True,\n                mode=\'min\'\n            )\n            self.enable_early_stop = True\n        elif not early_stop_callback:\n            self.early_stop_callback = None\n            self.enable_early_stop = False\n        else:\n            self.early_stop_callback = early_stop_callback\n            self.enable_early_stop = True\n\n    def configure_progress_bar(self, refresh_rate=1, process_position=0):\n        progress_bars = [c for c in self.callbacks if isinstance(c, ProgressBarBase)]\n        if len(progress_bars) > 1:\n            raise MisconfigurationException(\n                \'You added multiple progress bar callbacks to the Trainer, but currently only one\'\n                \' progress bar is supported.\'\n            )\n        elif len(progress_bars) == 1:\n            progress_bar_callback = progress_bars[0]\n        elif refresh_rate > 0:\n            progress_bar_callback = ProgressBar(\n                refresh_rate=refresh_rate,\n                process_position=process_position,\n            )\n            self.callbacks.append(progress_bar_callback)\n        else:\n            progress_bar_callback = None\n\n        return progress_bar_callback\n'"
pytorch_lightning/trainer/callback_hook.py,0,"b'from abc import ABC\nfrom typing import Callable, List\n\nfrom pytorch_lightning.callbacks import Callback\n\n\nclass TrainerCallbackHookMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    # the proper values/initialisation should be done in child class\n    callbacks: List[Callback] = []\n    get_model: Callable = ...\n\n    def on_init_start(self):\n        """"""Called when the trainer initialization begins, model has not yet been set.""""""\n        for callback in self.callbacks:\n            callback.on_init_start(self)\n\n    def on_init_end(self):\n        """"""Called when the trainer initialization ends, model has not yet been set.""""""\n        for callback in self.callbacks:\n            callback.on_init_end(self)\n\n    def on_sanity_check_start(self):\n        """"""Called when the validation sanity check starts.""""""\n        for callback in self.callbacks:\n            callback.on_sanity_check_start(self, self.get_model())\n\n    def on_sanity_check_end(self):\n        """"""Called when the validation sanity check ends.""""""\n        for callback in self.callbacks:\n            callback.on_sanity_check_end(self, self.get_model())\n\n    def on_epoch_start(self):\n        """"""Called when the epoch begins.""""""\n        for callback in self.callbacks:\n            callback.on_epoch_start(self, self.get_model())\n\n    def on_epoch_end(self):\n        """"""Called when the epoch ends.""""""\n        for callback in self.callbacks:\n            callback.on_epoch_end(self, self.get_model())\n\n    def on_train_start(self):\n        """"""Called when the train begins.""""""\n        for callback in self.callbacks:\n            callback.on_train_start(self, self.get_model())\n\n    def on_train_end(self):\n        """"""Called when the train ends.""""""\n        for callback in self.callbacks:\n            callback.on_train_end(self, self.get_model())\n\n    def on_batch_start(self):\n        """"""Called when the training batch begins.""""""\n        for callback in self.callbacks:\n            callback.on_batch_start(self, self.get_model())\n\n    def on_batch_end(self):\n        """"""Called when the training batch ends.""""""\n        for callback in self.callbacks:\n            callback.on_batch_end(self, self.get_model())\n\n    def on_validation_batch_start(self):\n        """"""Called when the validation batch begins.""""""\n        for callback in self.callbacks:\n            callback.on_validation_batch_start(self, self.get_model())\n\n    def on_validation_batch_end(self):\n        """"""Called when the validation batch ends.""""""\n        for callback in self.callbacks:\n            callback.on_validation_batch_end(self, self.get_model())\n\n    def on_test_batch_start(self):\n        """"""Called when the test batch begins.""""""\n        for callback in self.callbacks:\n            callback.on_test_batch_start(self, self.get_model())\n\n    def on_test_batch_end(self):\n        """"""Called when the test batch ends.""""""\n        for callback in self.callbacks:\n            callback.on_test_batch_end(self, self.get_model())\n\n    def on_validation_start(self):\n        """"""Called when the validation loop begins.""""""\n        for callback in self.callbacks:\n            callback.on_validation_start(self, self.get_model())\n\n    def on_validation_end(self):\n        """"""Called when the validation loop ends.""""""\n        for callback in self.callbacks:\n            callback.on_validation_end(self, self.get_model())\n\n    def on_test_start(self):\n        """"""Called when the test begins.""""""\n        for callback in self.callbacks:\n            callback.on_test_start(self, self.get_model())\n\n    def on_test_end(self):\n        """"""Called when the test ends.""""""\n        for callback in self.callbacks:\n            callback.on_test_end(self, self.get_model())\n'"
pytorch_lightning/trainer/data_loading.py,4,"b'import platform\nfrom abc import ABC, abstractmethod\nfrom typing import Union, List, Tuple, Callable\n\nimport torch.distributed as torch_distrib\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom pytorch_lightning.core import LightningModule\nfrom pytorch_lightning.utilities import rank_zero_warn\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\n\ntry:\n    from torch.utils.data import IterableDataset\n    ITERABLE_DATASET_EXISTS = True\nexcept ImportError:\n    ITERABLE_DATASET_EXISTS = False\n\ntry:\n    from apex import amp\nexcept ImportError:\n    APEX_AVAILABLE = False\nelse:\n    APEX_AVAILABLE = True\n\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.xla_multiprocessing as xmp\nexcept ImportError:\n    XLA_AVAILABLE = False\nelse:\n    XLA_AVAILABLE = True\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n\ndef _has_len(dataloader: DataLoader) -> bool:\n    """""" Checks if a given Dataloader has __len__ method implemented i.e. if\n    it is a finite dataloader or infinite dataloader """"""\n    try:\n        # try getting the length\n        if len(dataloader) == 0:\n            raise ValueError(\'`Dataloader` returned 0 length.\'\n                             \' Please make sure that your Dataloader at least returns 1 batch\')\n        return True\n    except TypeError:\n        return False\n\n\nclass TrainerDataLoadingMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    proc_rank: int\n    use_ddp: bool\n    use_ddp2: bool\n    use_horovod: bool\n    shown_warnings: ...\n    val_check_interval: float\n    use_tpu: bool\n    tpu_local_core_rank: int\n    train_dataloader: DataLoader\n    num_training_batches: Union[int, float]\n    val_check_batch: ...\n    val_dataloaders: List[DataLoader]\n    num_val_batches: Union[int, float]\n    test_dataloaders: List[DataLoader]\n    num_test_batches: Union[int, float]\n    train_percent_check: float\n    val_percent_check: float\n    test_percent_check: float\n    replace_sampler_ddp: bool\n\n    @abstractmethod\n    def is_overridden(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def _percent_range_check(self, name: str) -> None:\n        value = getattr(self, name)\n        msg = f\'`{name}` must lie in the range [0.0, 1.0], but got {value:.3f}.\'\n        if name == \'val_check_interval\':\n            msg += \' If you want to disable validation set `val_percent_check` to 0.0 instead.\'\n\n        if not 0. <= value <= 1.:\n            raise ValueError(msg)\n\n    def _worker_check(self, dataloader: DataLoader, name: str) -> None:\n        on_windows = platform.system() == \'Windows\'\n\n        if isinstance(dataloader, DataLoader) and dataloader.num_workers <= 2 and not on_windows:\n            rank_zero_warn(f\'The dataloader, {name}, does not have many workers which may be a bottleneck.\'\n                           \' Consider increasing the value of the `num_workers` argument`\'\n                           \' in the `DataLoader` init to improve performance.\')\n\n    def auto_add_sampler(self, dataloader: DataLoader, train: bool) -> DataLoader:\n\n        # don\'t do anything if it\'s not a dataloader\n        # don\'t manipulate iterable datasets\n        is_dataloader = isinstance(dataloader, DataLoader)\n\n        is_iterable_ds = False\n        if ITERABLE_DATASET_EXISTS and hasattr(dataloader, \'dataset\'):\n            is_iterable_ds = isinstance(dataloader.dataset, IterableDataset)\n\n        if not is_dataloader or is_iterable_ds:\n            return dataloader\n        need_dist_sampler = (self.use_ddp or self.use_ddp2 or self.use_horovod or self.use_tpu)\n\n        if self.replace_sampler_ddp and need_dist_sampler:\n            if not isinstance(dataloader.sampler, (SequentialSampler, RandomSampler)):\n                raise MisconfigurationException(\n                    \'You seem to have configured a sampler in your DataLoader. This will be replaced \'\n                    \' by `DistributedSampler` since `replace_sampler_ddp` is True and you are using\'\n                    \' distributed training. Either remove the sampler from your DataLoader or set\'\n                    \' `replace_sampler_ddp`=False if you want to use your custom sampler.\')\n\n            skip_keys = [\'sampler\', \'batch_sampler\', \'dataset_kind\']\n\n            dl_args = {\n                k: v for k, v in dataloader.__dict__.items() if not k.startswith(\'_\') and k not in skip_keys\n            }\n\n            dl_args[\'sampler\'] = self._get_distributed_sampler(dataloader)\n            dataloader = type(dataloader)(**dl_args)\n\n        return dataloader\n\n    def _get_distributed_sampler(self, dataloader):\n        if self.use_tpu:\n            kwargs = dict(num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal())\n        elif self.use_horovod:\n            kwargs = dict(num_replicas=hvd.size(), rank=hvd.rank())\n        else:\n            world_size = {\n                \'ddp\': self.num_nodes * self.num_processes,\n                \'ddp_spawn\': self.num_nodes * self.num_processes,\n                \'ddp2\': self.num_nodes,\n                \'ddp_cpu\': self.num_processes * self.num_nodes\n            }\n            kwargs = dict(num_replicas=world_size[self.distributed_backend], rank=self.proc_rank)\n        sampler = DistributedSampler(dataloader.dataset, **kwargs)\n        return sampler\n\n    def reset_train_dataloader(self, model: LightningModule) -> None:\n        """"""Resets the train dataloader and initialises required variables\n        (number of batches, when to validate, etc.).\n\n        Args:\n            model: The current `LightningModule`\n        """"""\n        self.train_dataloader = self.request_dataloader(model.train_dataloader)\n\n        self.num_training_batches = 0\n\n        # automatically add samplers\n        self.train_dataloader = self.auto_add_sampler(self.train_dataloader, train=True)\n\n        self._worker_check(self.train_dataloader, \'train dataloader\')\n        self._percent_range_check(\'train_percent_check\')\n\n        if not _has_len(self.train_dataloader):\n            self.num_training_batches = float(\'inf\')\n        else:\n            # try getting the length\n            self.num_training_batches = len(self.train_dataloader)\n            self.num_training_batches = int(self.num_training_batches * self.train_percent_check)\n\n        # determine when to check validation\n        # if int passed in, val checks that often\n        # otherwise, it checks in [0, 1.0] % range of a training epoch\n        if isinstance(self.val_check_interval, int):\n            self.val_check_batch = self.val_check_interval\n            if self.val_check_batch > self.num_training_batches:\n                raise ValueError(\n                    f\'`val_check_interval` ({self.val_check_interval}) must be less than or equal \'\n                    f\'to the number of the training batches ({self.num_training_batches}). \'\n                    \'If you want to disable validation set `val_percent_check` to 0.0 instead.\')\n        else:\n            if not _has_len(self.train_dataloader):\n                if self.val_check_interval == 1.0:\n                    self.val_check_batch = float(\'inf\')\n                else:\n                    raise MisconfigurationException(\n                        \'When using an infinite DataLoader (e.g. with an IterableDataset\'\n                        \' or when DataLoader does not implement `__len__`) for `train_dataloader`,\'\n                        \' `Trainer(val_check_interval)` must be `1.0` or an int. An int k specifies\'\n                        \' checking validation every k training batches.\')\n            else:\n                self._percent_range_check(\'val_check_interval\')\n\n                self.val_check_batch = int(self.num_training_batches * self.val_check_interval)\n                self.val_check_batch = max(1, self.val_check_batch)\n\n    def _reset_eval_dataloader(self, model: LightningModule, mode: str) -> Tuple[int, List[DataLoader]]:\n        """"""Generic method to reset a dataloader for evaluation.\n\n        Args:\n            model: The current `LightningModule`\n            mode: Either `\'val\'` or `\'test\'`\n\n        Returns:\n            Tuple (num_batches, dataloaders)\n        """"""\n        dataloaders = self.request_dataloader(getattr(model, f\'{mode}_dataloader\'))\n\n        if not isinstance(dataloaders, list):\n            dataloaders = [dataloaders]\n\n        # shuffling in val and test set is bad practice\n        for loader in dataloaders:\n            if mode in (\'val\', \'test\') and hasattr(loader, \'sampler\') and isinstance(loader.sampler, RandomSampler):\n                rank_zero_warn(\n                    f\'Your {mode}_dataloader has shuffle=True, it is best practice to turn\'\n                    \' this off for validation and test dataloaders.\')\n\n        if any([dl is None for dl in dataloaders]):\n            rank_zero_warn(""One of given dataloaders is None and it will be skipped."")\n\n        # add samplers\n        dataloaders = [self.auto_add_sampler(dl, train=False) for dl in dataloaders if dl is not None]\n\n        num_batches = 0\n\n        # determine number of batches\n        # datasets could be none, 1 or 2+\n        if len(dataloaders) != 0:\n            for i, dataloader in enumerate(dataloaders):\n                self._worker_check(dataloader, f\'{mode} dataloader {i}\')\n                if not _has_len(dataloader):\n                    num_batches = float(\'inf\')\n\n            percent_check = getattr(self, f\'{mode}_percent_check\')\n\n            if num_batches != float(\'inf\'):\n                self._percent_range_check(f\'{mode}_percent_check\')\n\n                num_batches = sum(len(dataloader) for dataloader in dataloaders)\n                num_batches = int(num_batches * percent_check)\n            elif percent_check not in (0.0, 1.0):\n                raise MisconfigurationException(\n                    \'When using an infinite DataLoader (e.g. with an IterableDataset\'\n                    f\' or when DataLoader does not implement `__len__`) for `{mode}_dataloader`,\'\n                    f\' `Trainer({mode}_percent_check)` must be `0.0` or `1.0`.\')\n        return num_batches, dataloaders\n\n    def reset_val_dataloader(self, model: LightningModule) -> None:\n        """"""Resets the validation dataloader and determines the number of batches.\n\n        Args:\n            model: The current `LightningModule`\n        """"""\n        if self.is_overridden(\'validation_step\'):\n            self.num_val_batches, self.val_dataloaders = \\\n                self._reset_eval_dataloader(model, \'val\')\n\n    def reset_test_dataloader(self, model) -> None:\n        """"""Resets the validation dataloader and determines the number of batches.\n\n        Args:\n            model: The current `LightningModule`\n        """"""\n        if self.is_overridden(\'test_step\'):\n            self.num_test_batches, self.test_dataloaders =\\\n                self._reset_eval_dataloader(model, \'test\')\n\n    def request_dataloader(self, dataloader_fx: Callable) -> DataLoader:\n        """"""Handles downloading data in the GPU or TPU case.\n\n        Args:\n            dataloader_fx: The bound dataloader getter\n\n        Returns:\n            The dataloader\n        """"""\n        dataloader = dataloader_fx()\n\n        # get the function we\'ll use to get data\n        if self.use_ddp or self.use_ddp2:\n            # all processes wait until data download has happened\n            torch_distrib.barrier()\n\n        # data download/load on TPU\n        elif self.use_tpu and XLA_AVAILABLE:\n            # all processes wait until data download has happened\n            torch_xla.core.xla_model.rendezvous(\'pl.TrainerDataLoadingMixin.get_dataloaders\')\n\n        elif self.use_horovod:\n            # all processes wait until data download has happened\n            hvd.join()\n\n        return dataloader\n\n    def determine_data_use_amount(self, train_percent_check: float, val_percent_check: float,\n                                  test_percent_check: float, overfit_pct: float) -> None:\n        """"""Use less data for debugging purposes\n        """"""\n        self.train_percent_check = train_percent_check\n        self.val_percent_check = val_percent_check\n        self.test_percent_check = test_percent_check\n        if overfit_pct > 0:\n            if overfit_pct > 1:\n                raise ValueError(\n                    f\'`overfit_pct` must be not greater than 1.0, but got {overfit_pct:.3f}.\')\n\n            self.train_percent_check = overfit_pct\n            self.val_percent_check = overfit_pct\n            self.test_percent_check = overfit_pct\n'"
pytorch_lightning/trainer/deprecated_api.py,0,"b'""""""Mirroring deprecated API""""""\n\nfrom abc import ABC\n\nfrom pytorch_lightning.utilities import rank_zero_warn\n\n\nclass TrainerDeprecatedAPITillVer0_8(ABC):\n\n    def __init__(self):\n        super().__init__()  # mixin calls super too\n\n    @property\n    def nb_gpu_nodes(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `nb_gpu_nodes` has renamed to `num_nodes` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.num_nodes\n\n    @property\n    def num_gpu_nodes(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `num_gpu_nodes` has renamed to `num_nodes` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.num_nodes\n\n    @num_gpu_nodes.setter\n    def num_gpu_nodes(self, num_nodes):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `num_gpu_nodes` has renamed to `num_nodes` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        self.num_nodes = num_nodes\n\n    @property\n    def gradient_clip(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `gradient_clip` has renamed to `gradient_clip_val` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.gradient_clip_val\n\n    @gradient_clip.setter\n    def gradient_clip(self, gradient_clip):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `gradient_clip` has renamed to `gradient_clip_val` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        self.gradient_clip_val = gradient_clip\n\n    @property\n    def max_nb_epochs(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `max_nb_epochs` has renamed to `max_epochs` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.max_epochs\n\n    @max_nb_epochs.setter\n    def max_nb_epochs(self, max_epochs):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `max_nb_epochs` has renamed to `max_epochs` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        self.max_epochs = max_epochs\n\n    @property\n    def min_nb_epochs(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `min_nb_epochs` has renamed to `min_epochs` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.min_epochs\n\n    @min_nb_epochs.setter\n    def min_nb_epochs(self, min_epochs):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `min_nb_epochs` has renamed to `min_epochs` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        self.min_epochs = min_epochs\n\n    @property\n    def nb_sanity_val_steps(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `nb_sanity_val_steps` has renamed to ""\n                       ""`num_sanity_val_steps` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.num_sanity_val_steps\n\n    @nb_sanity_val_steps.setter\n    def nb_sanity_val_steps(self, nb):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `nb_sanity_val_steps` has renamed to ""\n                       ""`num_sanity_val_steps` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        self.num_sanity_val_steps = nb\n\n    @property\n    def default_save_path(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `default_save_path` has renamed to `default_root_dir` since v0.5.x""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.default_root_dir\n\n    @default_save_path.setter\n    def default_save_path(self, path):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""Attribute `default_save_path` has renamed to `default_root_dir` since v0.5.x""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        self.default_root_dir = path\n\n    @property\n    def tng_tqdm_dic(self):\n        """"""Back compatibility, will be removed in v0.8.0""""""\n        rank_zero_warn(""`tng_tqdm_dic` has renamed to `training_tqdm_dict` since v0.5.0""\n                       "" and this method will be removed in v0.8.0"", DeprecationWarning)\n        return self.progress_bar_dict\n\n\nclass TrainerDeprecatedAPITillVer0_9(ABC):\n\n    def __init__(self):\n        super().__init__()  # mixin calls super too\n\n    @property\n    def show_progress_bar(self):\n        """"""Back compatibility, will be removed in v0.9.0""""""\n        rank_zero_warn(""Argument `show_progress_bar` is now set by `progress_bar_refresh_rate` since v0.7.2""\n                       "" and this method will be removed in v0.9.0"", DeprecationWarning)\n        return self.progress_bar_callback and self.progress_bar_callback.refresh_rate >= 1\n\n    @show_progress_bar.setter\n    def show_progress_bar(self, tf):\n        """"""Back compatibility, will be removed in v0.9.0""""""\n        rank_zero_warn(""Argument `show_progress_bar` is now set by `progress_bar_refresh_rate` since v0.7.2""\n                       "" and this method will be removed in v0.9.0"", DeprecationWarning)\n\n    @property\n    def training_tqdm_dict(self):\n        """"""Back compatibility, will be removed in v0.9.0""""""\n        rank_zero_warn(""`training_tqdm_dict` was renamed to `progress_bar_dict` in v0.7.3""\n                       "" and this method will be removed in v0.9.0"", DeprecationWarning)\n        return self.progress_bar_dict\n\n    @property\n    def num_tpu_cores(self):\n        """"""Back compatibility, will be removed in v0.9.0""""""\n        rank_zero_warn(""Argument `num_tpu_cores` is now set by `tpu_cores` since v0.7.6""\n                       "" and this argument will be removed in v0.9.0"", DeprecationWarning)\n'"
pytorch_lightning/trainer/distrib_data_parallel.py,2,"b'""""""\nLightning supports model training on a cluster managed by SLURM in the following cases:\n\n1. Training on a single cpu or single GPU.\n2. Train on multiple GPUs on the same node using DataParallel or DistributedDataParallel\n3. Training across multiple GPUs on multiple different nodes via DistributedDataParallel.\n\n.. note:: A node means a machine with multiple GPUs\n\nRunning grid search on a cluster\n--------------------------------\n\nTo use lightning to run a hyperparameter search (grid-search or random-search) on a cluster do 4 things:\n\n(1). Define the parameters for the grid search\n\n.. code-block:: python\n\n    from test_tube import HyperOptArgumentParser\n\n    # subclass of argparse\n    parser = HyperOptArgumentParser(strategy=\'random_search\')\n    parser.add_argument(\'--learning_rate\', default=0.002, type=float, help=\'the learning rate\')\n\n    # let\'s enable optimizing over the number of layers in the network\n    parser.opt_list(\'--nb_layers\', default=2, type=int, tunable=True, options=[2, 4, 8])\n\n    hparams = parser.parse_args()\n\n.. note:: You must set `Tunable=True` for that argument to be considered in the permutation set.\n Otherwise test-tube will use the default value. This flag is useful when you don\'t want\n to search over an argument and want to use the default instead.\n\n(2). Define the cluster options in the\n `SlurmCluster object <https://williamfalcon.github.io/test-tube/hpc/SlurmCluster>`_ (over 5 nodes and 8 gpus)\n\n.. code-block:: python\n\n    from test_tube.hpc import SlurmCluster\n\n    # hyperparameters is a test-tube hyper params object\n    # see https://williamfalcon.github.io/test-tube/hyperparameter_optimization/HyperOptArgumentParser/\n    hyperparams = args.parse()\n\n    # init cluster\n    cluster = SlurmCluster(\n        hyperparam_optimizer=hyperparams,\n        log_path=\'/path/to/log/results/to\',\n        python_cmd=\'python3\'\n    )\n\n    # let the cluster know where to email for a change in job status (ie: complete, fail, etc...)\n    cluster.notify_job_status(email=\'some@email.com\', on_done=True, on_fail=True)\n\n    # set the job options. In this instance, we\'ll run 20 different models\n    # each with its own set of hyperparameters giving each one 1 GPU (ie: taking up 20 GPUs)\n    cluster.per_experiment_nb_gpus = 8\n    cluster.per_experiment_nb_nodes = 5\n\n    # we\'ll request 10GB of memory per node\n    cluster.memory_mb_per_node = 10000\n\n    # set a walltime of 10 minues\n    cluster.job_time = \'10:00\'\n\n\n(3). Make a main function with your model and trainer. Each job will call this function with a particular\nhparams configuration.::\n\n    from pytorch_lightning import Trainer\n\n    def train_fx(trial_hparams, cluster_manager, _):\n        # hparams has a specific set of hyperparams\n\n        my_model = MyLightningModel()\n\n        # give the trainer the cluster object\n        trainer = Trainer()\n        trainer.fit(my_model)\n\n    `\n\n(4). Start the grid/random search::\n\n    # run the models on the cluster\n    cluster.optimize_parallel_cluster_gpu(\n        train_fx,\n        nb_trials=20,\n        job_name=\'my_grid_search_exp_name\',\n        job_display_name=\'my_exp\')\n\n.. note:: `nb_trials` specifies how many of the possible permutations to use. If using `grid_search` it will use\n the depth first ordering. If using `random_search` it will use the first k shuffled options. FYI, random search\n has been shown to be just as good as any Bayesian optimization method when using a reasonable number of samples (60),\n see this `paper <http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf>`_  for more information.\n\nWalltime auto-resubmit\n----------------------\n\nLightning automatically resubmits jobs when they reach the walltime. Make sure to set the SIGUSR1 signal in\nyour SLURM script.::\n\n    # 90 seconds before training ends\n    #SBATCH --signal=SIGUSR1@90\n\nWhen lightning receives the SIGUSR1 signal it will:\n1. save a checkpoint with \'hpc_ckpt\' in the name.\n2. resubmit the job using the SLURM_JOB_ID\n\nWhen the script starts again, Lightning will:\n1. search for a \'hpc_ckpt\' checkpoint.\n2. restore the model, optimizers, schedulers, epoch, etc...\n\n""""""\n\nimport os\nimport re\nfrom abc import ABC, abstractmethod\nfrom typing import Union\nimport subprocess\nimport sys\nfrom time import sleep\nimport numpy as np\nfrom os.path import abspath\n\nimport torch\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import LightningLoggerBase\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities.distributed import rank_zero_only, rank_zero_warn\n\ntry:\n    from apex import amp\nexcept ImportError:\n    APEX_AVAILABLE = False\nelse:\n    APEX_AVAILABLE = True\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n\nclass TrainerDDPMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    on_gpu: bool\n    num_gpu_nodes: int\n    logger: Union[LightningLoggerBase, bool]\n    checkpoint_callback: Union[ModelCheckpoint, bool]\n    data_parallel_device_ids: ...\n    distributed_backend: str\n    amp_level: str\n    use_tpu: bool\n    default_root_dir: str\n    use_native_amp: bool\n    progress_bar_callback: ...\n    num_processes: int\n\n    @property\n    @abstractmethod\n    def num_gpus(self) -> int:\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @property\n    @abstractmethod\n    def use_amp(self) -> bool:\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def copy_trainer_model_properties(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def run_pretrain_routine(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def init_optimizers(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def init_tpu(self):\n        # turn off all the GPU stuff\n        self.distributed_backend = None\n\n        # enable tpu\n        self.use_tpu = True\n\n    def set_distributed_mode(self, distributed_backend):\n        self.use_dp = False\n        self.use_ddp = False\n        self.use_ddp2 = False\n        self.use_horovod = False\n        self.single_gpu = False\n\n        if distributed_backend is None:\n            if self.has_horovodrun():\n                self._set_horovod_backend()\n            elif self.num_gpus == 0:\n                if self.num_nodes > 1 or self.num_processes > 1:\n                    self.use_ddp = True  # ddp_cpu\n            elif self.num_gpus == 1:\n                self.single_gpu = True\n            elif self.num_gpus > 1:\n                rank_zero_warn(\'You requested multiple GPUs but did not specify a backend, e.g.\'\n                               \' Trainer(distributed_backend=dp) (or ddp, ddp2).\'\n                               \' Setting distributed_backend=ddp for you.\')\n                self.distributed_backend = \'ddp\'\n                distributed_backend = \'ddp\'\n\n        if distributed_backend == ""dp"":\n            # do nothing if num_gpus == 0\n            if self.num_gpus == 1:\n                self.single_gpu = True\n                self.use_dp = True\n            elif self.num_gpus > 1:\n                self.use_dp = True\n\n        elif distributed_backend in [\'ddp\', \'ddp_spawn\']:\n            if self.num_gpus == 0:\n                if self.num_nodes > 1 or self.num_processes > 1:\n                    self.use_ddp = True  # ddp_cpu\n            elif self.num_gpus == 1:\n                self.single_gpu = True\n                self.use_ddp = True\n            elif self.num_gpus > 1:\n                self.use_ddp = True\n                self.num_processes = self.num_gpus\n\n        elif distributed_backend == ""ddp2"":\n            # do nothing if num_gpus == 0\n            if self.num_gpus >= 1:\n                self.use_ddp2 = True\n        elif distributed_backend == ""ddp_cpu"":\n            if self.num_gpus > 0:\n                rank_zero_warn(\'You requested one or more GPUs, but set the backend to `ddp_cpu`.\'\n                               \' Training will not use GPUs.\')\n            self.use_ddp = True\n            self.data_parallel_device_ids = None\n            self.on_gpu = False\n        elif distributed_backend == \'horovod\':\n            self._set_horovod_backend()\n\n        # throw error to force user ddp or ddp2 choice\n        if self.num_nodes > 1 and not (self.use_ddp2 or self.use_ddp):\n            raise MisconfigurationException(\n                \'DataParallel does not support num_nodes > 1. Switching to DistributedDataParallel for you. \'\n                \'To silence this warning set distributed_backend=ddp or distributed_backend=ddp2\'\n            )\n\n        log.info(f\'GPU available: {torch.cuda.is_available()}, used: {self.on_gpu}\')\n\n    def configure_slurm_ddp(self, num_gpu_nodes):\n        self.is_slurm_managing_tasks = False\n\n        # extract SLURM flag vars\n        # whenever we have the correct number of tasks, we let slurm manage processes\n        # otherwise we launch the required number of processes\n        if self.use_ddp:\n            self.num_requested_gpus = self.num_gpus * num_gpu_nodes\n            self.num_slurm_tasks = 0\n            try:\n                self.num_slurm_tasks = int(os.environ[\'SLURM_NTASKS\'])\n                self.is_slurm_managing_tasks = self.num_slurm_tasks == self.num_requested_gpus\n\n                # in interactive mode we don\'t manage tasks\n                job_name = os.environ[\'SLURM_JOB_NAME\']\n                if job_name == \'bash\':\n                    self.is_slurm_managing_tasks = False\n\n            except Exception:\n                # likely not on slurm, so set the slurm managed flag to false\n                self.is_slurm_managing_tasks = False\n\n        # used for tests only, set this flag to simulate slurm managing a task\n        try:\n            should_fake = int(os.environ[\'FAKE_SLURM_MANAGING_TASKS\'])\n            if should_fake:\n                self.is_slurm_managing_tasks = True\n        except Exception:\n            pass\n\n        # notify user the that slurm is managing tasks\n        if self.is_slurm_managing_tasks:\n            log.info(\'Multi-processing is handled by Slurm.\')\n\n    def determine_ddp_node_rank(self):\n        if self.is_slurm_managing_tasks:\n            return int(os.environ[\'SLURM_NODEID\'])\n\n        # torchelastic uses the envvar GROUP_RANK, whereas other systems(?) use NODE_RANK.\n        # otherwise use given node rank or default to node rank 0\n        env_vars = [\'NODE_RANK\', \'GROUP_RANK\']\n        node_ids = [(k, os.environ.get(k, None)) for k in env_vars]\n        node_ids = [(k, v) for k, v in node_ids if v is not None]\n        if len(node_ids) == 0:\n            log.warning(""No environment variable for node rank defined. Set as 0."")\n            return 0\n        if len(node_ids) > 1:\n            log.warning(f""Multiple environment variables ({node_ids}) defined for node rank. ""\n                        f""Using the first one."")\n        k, rank = node_ids.pop()\n        log.info(f""Using environment variable {k} for node rank ({rank})."")\n        return int(rank)\n\n    def set_nvidia_flags(self, is_slurm_managing_tasks, data_parallel_device_ids):\n        if data_parallel_device_ids is None:\n            return\n\n        # set the correct cuda visible devices (using pci order)\n        os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n\n        # when slurm is managing the task it sets the visible devices\n        if not is_slurm_managing_tasks and \'CUDA_VISIBLE_DEVICES\' not in os.environ:\n            if isinstance(data_parallel_device_ids, int):\n                id_str = \',\'.join(str(x) for x in list(range(data_parallel_device_ids)))\n                os.environ[""CUDA_VISIBLE_DEVICES""] = id_str\n            else:\n                gpu_str = \',\'.join([str(x) for x in data_parallel_device_ids])\n                os.environ[""CUDA_VISIBLE_DEVICES""] = gpu_str\n\n        # don\'t make this debug... this is good UX\n        log.info(f\'CUDA_VISIBLE_DEVICES: [{os.environ[""CUDA_VISIBLE_DEVICES""]}]\')\n\n    def __set_random_port(self):\n        """"""\n        When running DDP NOT managed by SLURM, the ports might collide\n        :return:\n        """"""\n        try:\n            default_port = os.environ[\'MASTER_PORT\']\n        except Exception:\n            import random\n            default_port = random.randint(10000, 19000)\n            os.environ[\'MASTER_PORT\'] = str(default_port)\n\n    def spawn_ddp_children(self, model):\n        self.__set_random_port()\n        port = os.environ[\'MASTER_PORT\']\n\n        master_address = \'127.0.0.1\' if \'MASTER_ADDR\' not in os.environ else os.environ[\'MASTER_ADDR\']\n        os.environ[\'MASTER_PORT\'] = f\'{port}\'\n        os.environ[\'MASTER_ADDR\'] = f\'{master_address}\'\n\n        # allow the user to pass the node rank\n        node_rank = \'0\'\n        if \'NODE_RANK\' in os.environ:\n            node_rank = os.environ[\'NODE_RANK\']\n        if \'GROUP_RANK\' in os.environ:\n            node_rank = os.environ[\'GROUP_RANK\']\n\n        os.environ[\'NODE_RANK\'] = node_rank\n        os.environ[\'LOCAL_RANK\'] = \'0\'\n\n        # pull out the commands used to run the script and resolve the abs file path\n        command = sys.argv\n        full_path = abspath(command[0])\n        command[0] = full_path\n        command = [\'python\'] + command\n\n        # since this script sets the visible devices we replace the gpus flag with a number\n        num_gpus = os.environ[\'CUDA_VISIBLE_DEVICES\'].split(\',\').__len__()\n\n        # if script called without a flag, pass in a flag anyhow\n        if \'--gpus\' not in command:\n            arg_gpus = len(self.gpus) if isinstance(self.gpus, list) else self.gpus\n            command += [\'--gpus\', arg_gpus]\n\n        gpu_flag_idx = command.index(\'--gpus\')\n        command[gpu_flag_idx + 1] = f\'{num_gpus}\'\n\n        os.environ[\'WORLD_SIZE\'] = f\'{num_gpus * self.num_nodes}\'\n\n        self.interactive_ddp_procs = []\n        for local_rank in range(1, self.num_processes):\n            print(\'launching local_rank\', local_rank)\n            env_copy = os.environ.copy()\n            env_copy[\'LOCAL_RANK\'] = f\'{local_rank}\'\n\n            # import pdb; pdb.set_trace()\n            # start process\n            proc = subprocess.Popen(command, env=env_copy)\n            self.interactive_ddp_procs.append(proc)\n\n            # starting all processes at once can cause issues\n            # with dataloaders delay between 1-10 seconds\n            delay = np.random.uniform(1, 5, 1)[0]\n            sleep(delay)\n\n        local_rank = 0\n        self.ddp_train(local_rank, model, is_master=True)\n\n    def ddp_train(self, process_idx, model, is_master=False, proc_offset=0):\n        """"""\n        Entry point into a DP thread\n        :param gpu_idx:\n        :param model:\n        :param cluster_obj:\n        :return:\n        """"""\n        # offset the process id if requested\n        process_idx = process_idx + proc_offset\n\n        # show progressbar only on progress_rank 0\n        if (self.node_rank != 0 or process_idx != 0) and self.progress_bar_callback is not None:\n            self.progress_bar_callback.disable()\n\n        # determine which process we are and world size\n        if self.use_ddp:\n            self.proc_rank = self.node_rank * self.num_processes + process_idx\n            self.world_size = self.num_nodes * self.num_processes\n\n        elif self.use_ddp2:\n            self.proc_rank = self.node_rank\n            self.world_size = self.num_nodes\n\n        # set warning rank\n        rank_zero_only.rank = self.proc_rank\n\n        # set up server using proc 0\'s ip address\n        # try to init for 20 times at max in case ports are taken\n        # where to store ip_table\n        model.trainer = self\n        model.init_ddp_connection(self.proc_rank, self.world_size, self.is_slurm_managing_tasks)\n\n        # CHOOSE OPTIMIZER\n        # allow for lr schedulers as well\n        self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n\n        # MODEL\n        # copy model to each gpu\n        if self.on_gpu:\n            gpu_idx = process_idx\n            if is_master:\n                # source of truth is cuda for gpu idx\n                gpus = os.environ[\'CUDA_VISIBLE_DEVICES\'].split(\',\')\n                local_rank = int(os.environ[\'LOCAL_RANK\'])\n                gpu_idx = int(gpus[local_rank])\n\n            self.root_gpu = gpu_idx\n            torch.cuda.set_device(self.root_gpu)\n            model.cuda(self.root_gpu)\n\n        # set model properties before going into wrapper\n        self.copy_trainer_model_properties(model)\n\n        # AMP\n        # run through amp wrapper before going to distributed DP\n        # TODO: remove in v0.8.0\n        if self.use_amp and not self.use_native_amp:\n            model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)\n            self.optimizers = optimizers\n            self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)\n\n        # DDP2 uses all GPUs on the machine\n        if self.distributed_backend == \'ddp\' or self.distributed_backend == \'ddp_spawn\':\n            device_ids = [self.root_gpu]\n        elif self.use_ddp2:\n            device_ids = self.data_parallel_device_ids\n        else:  # includes ddp_cpu\n            device_ids = None\n\n        # allow user to configure ddp\n        model = model.configure_ddp(model, device_ids)\n\n        # continue training routine\n        self.run_pretrain_routine(model)\n\n    def save_spawn_weights(self, model):\n        """"""\n        Dump a temporary checkpoint after ddp ends to get weights out of the process\n        :param model:\n        :return:\n        """"""\n        if self.proc_rank == 0:\n            path = os.path.join(self.default_root_dir, \'__temp_weight_ddp_end.ckpt\')\n            self.save_checkpoint(path)\n\n    def load_spawn_weights(self, original_model):\n        """"""\n        Load the temp weights saved in the process\n        To recover the trained model from the ddp process we load the saved weights\n        :param model:\n        :return:\n        """"""\n\n        loaded_model = original_model\n\n        if self.proc_rank == 0:\n            # load weights saved in ddp\n            path = os.path.join(self.default_root_dir, \'__temp_weight_ddp_end.ckpt\')\n            loaded_model = original_model.__class__.load_from_checkpoint(path)\n\n            # copy loaded weights to old model\n            original_model.load_state_dict(loaded_model.state_dict())\n\n            # remove ddp weights\n            os.remove(path)\n\n        return loaded_model\n\n    def resolve_root_node_address(self, root_node):\n        if \'[\' in root_node:\n            name, numbers = root_node.split(\'[\', maxsplit=1)\n            number = numbers.split(\',\', maxsplit=1)[0]\n            if \'-\' in number:\n                number = number.split(\'-\')[0]\n\n            number = re.sub(\'[^0-9]\', \'\', number)\n            root_node = name + number\n\n        return root_node\n\n    def _set_horovod_backend(self):\n        self.check_horovod()\n        self.use_horovod = True\n\n        # Initialize Horovod to get rank / size info\n        hvd.init()\n        if self.on_gpu:\n            # Horovod assigns one local GPU per process\n            self.root_gpu = hvd.local_rank()\n\n    def check_horovod(self):\n        """"""Raises a `MisconfigurationException` if the Trainer is not configured correctly for Horovod.""""""\n        if not HOROVOD_AVAILABLE:\n            raise MisconfigurationException(\n                \'Requested `distributed_backend=""horovod""`, but Horovod is not installed.\'\n                \'Install with \\n $HOROVOD_WITH_PYTORCH=1 pip install horovod[pytorch]\'\n            )\n\n        if self.num_gpus > 1 or self.num_nodes > 1:\n            raise MisconfigurationException(\n                \'Horovod does not support setting num_nodes / num_gpus explicitly. Use \'\n                \'horovodrun / mpirun to configure the number of processes.\'\n            )\n\n    @staticmethod\n    def has_horovodrun():\n        """"""Returns True if running with `horovodrun` using Gloo or OpenMPI.""""""\n        return \'OMPI_COMM_WORLD_RANK\' in os.environ or \'HOROVOD_RANK\' in os.environ\n'"
pytorch_lightning/trainer/distrib_parts.py,10,"b'""""""\nRoot module for all distributed operations in Lightning.\nCurrently supports training on CPU, GPU (dp, ddp, ddp2, horovod) and TPU.\n\n""""""\n\nfrom contextlib import ExitStack\nimport os\nfrom abc import ABC, abstractmethod\nimport time\nimport random\nimport torch\nfrom typing import Union, Callable, Any, List, Optional\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.loggers import LightningLoggerBase\nfrom pytorch_lightning.overrides.data_parallel import (\n    LightningDistributedDataParallel,\n    LightningDataParallel,\n)\nfrom pytorch_lightning.utilities import move_data_to_device\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities.distributed import rank_zero_only\n\ntry:\n    from apex import amp\nexcept ImportError:\n    APEX_AVAILABLE = False\nelse:\n    APEX_AVAILABLE = True\n\ntry:\n    import torch_xla.core.xla_model as xm\nexcept ImportError:\n    XLA_AVAILABLE = False\nelse:\n    XLA_AVAILABLE = True\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n\nclass TrainerDPMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    on_gpu: bool\n    use_dp: bool\n    use_ddp2: bool\n    use_ddp: bool\n    testing: bool\n    single_gpu: bool\n    root_gpu: ...\n    amp_level: str\n    precision: ...\n    proc_rank: int\n    tpu_local_core_rank: int\n    tpu_global_core_rank: int\n    use_tpu: bool\n    use_native_amp: bool\n    data_parallel_device_ids: ...\n    logger: Union[LightningLoggerBase, bool]\n    progress_bar_callback: ...\n    tpu_id: int\n\n    @property\n    @abstractmethod\n    def use_amp(self) -> bool:\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def run_pretrain_routine(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def init_optimizers(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def copy_trainer_model_properties(self, model):\n        if isinstance(model, LightningDataParallel):\n            ref_model = model.module\n        elif isinstance(model, LightningDistributedDataParallel):\n            ref_model = model.module\n        else:\n            ref_model = model\n\n        for m in [model, ref_model]:\n            m.trainer = self\n            m.use_dp = self.use_dp\n            m.use_ddp2 = self.use_ddp2\n            m.use_ddp = self.use_ddp\n            m.use_amp = self.use_amp\n            m.testing = self.testing\n            m.single_gpu = self.single_gpu\n            m.use_tpu = self.use_tpu\n            m.tpu_local_core_rank = self.tpu_local_core_rank\n            m.tpu_global_core_rank = self.tpu_global_core_rank\n\n    def transfer_batch_to_tpu(self, batch: Any, tpu_id: Optional[int] = None):\n        """"""\n        Transfers the data to the TPU.\n\n        Args:\n            batch: A tensor or collection of tensors.\n            tpu_id: The id of the TPU core. If omitted, the first available core is chosen.\n\n        Return:\n            the tensor on the TPU device.\n\n        See Also:\n            - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device`\n        """"""\n        if not XLA_AVAILABLE:\n            raise MisconfigurationException(\n                \'Requested to transfer batch to TPU but XLA is not available.\'\n                \' Are you sure this machine has TPUs?\'\n            )\n        device = xm.xla_device(tpu_id)\n        return self.__transfer_batch_to_device(batch, device)\n\n    def transfer_batch_to_gpu(self, batch: Any, gpu_id: Optional[int] = None):\n        """"""\n        Transfers the data to the GPU.\n\n        Args:\n            batch: A tensor or collection of tensors.\n            gpu_id: The id of the GPU device. If omitted, the first available GPU is chosen.\n\n        Return:\n            the tensor on the GPU device.\n\n        See Also:\n            - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device`\n        """"""\n        device = torch.device(\'cuda\', gpu_id)\n        return self.__transfer_batch_to_device(batch, device)\n\n    def __transfer_batch_to_device(self, batch: Any, device: torch.device):\n        model = self.get_model()\n        if model is not None:\n            return model.transfer_batch_to_device(batch, device)\n        return move_data_to_device(batch, device)\n\n    def single_gpu_train(self, model):\n        model.cuda(self.root_gpu)\n\n        # CHOOSE OPTIMIZER\n        # allow for lr schedulers as well\n        self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n\n        # TODO: update for 0.8.0\n        if self.use_amp and not self.use_native_amp:\n            # An example\n            model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)\n            self.optimizers = optimizers\n            self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)\n\n        self.run_pretrain_routine(model)\n\n    def tpu_train(self, tpu_core_idx, model):\n        # put model on tpu\n        self._device = xm.xla_device(self.tpu_id) if self.tpu_id is not None else xm.xla_device()\n        model.to(self._device)\n\n        # get the appropriate tpu ranks\n        self.tpu_local_core_rank = xm.get_local_ordinal()\n        self.tpu_global_core_rank = xm.get_ordinal()\n\n        # avoid duplicating progress bar\n        if self.tpu_global_core_rank != 0 and self.progress_bar_callback is not None:\n            self.progress_bar_callback.disable()\n\n        self.proc_rank = self.tpu_local_core_rank\n        rank_zero_only.rank = self.proc_rank\n\n        # CHOOSE OPTIMIZER\n        # allow for lr schedulers as well\n        self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n\n        # init 16 bit for TPU\n        if self.precision == 16:\n            os.environ[\'XLA_USE_BF16\'] = str(1)\n\n        log.info(f\'INIT TPU local core: {self.tpu_local_core_rank},\'\n                 f\' global rank: {self.tpu_global_core_rank}\')\n\n        # continue training routine\n        self.run_pretrain_routine(model)\n\n        # when training ends on these platforms dump weights to get out of the main process\n        if self.on_colab_kaggle:\n            self.save_spawn_weights(model)\n\n    def dp_train(self, model):\n\n        # CHOOSE OPTIMIZER\n        # allow for lr schedulers as well\n        self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n\n        model.cuda(self.root_gpu)\n\n        # hack forward to do autocast for the user\n        model_autocast_original_forward = model.forward\n        if self.use_amp and self.use_native_amp:\n            # wrap the user\'s forward in autocast and give it back at the end\n            model.forward = torch.cuda.amp.autocast()(model.forward)\n\n        # TODO: remove in v0.8.0\n        # check for this bug (amp + dp + !01 doesn\'t work)\n        # https://github.com/NVIDIA/apex/issues/227\n        if self.use_dp and self.use_amp and not self.use_native_amp:\n            if self.amp_level == \'O2\':\n                raise MisconfigurationException(\n                    f\'Amp level {self.amp_level} with DataParallel is not supported.\'\n                    f\' See this note from NVIDIA for more info: https://github.com/NVIDIA/apex/issues/227.\'\n                    f\' We recommend you switch to ddp if you want to use amp\')\n            else:\n                model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)\n                self.reinit_scheduler_properties(optimizers, self.lr_schedulers)\n\n        # create list of device ids\n        device_ids = self.data_parallel_device_ids\n        if isinstance(device_ids, int):\n            device_ids = list(range(device_ids))\n\n        # set dp device\n        torch.cuda.set_device(self.root_gpu)\n\n        model = LightningDataParallel(model, device_ids=device_ids)\n\n        self.run_pretrain_routine(model)\n\n        model.forward = model_autocast_original_forward\n\n    def horovod_train(self, model):\n        if torch.cuda.is_available() and self.on_gpu:\n            # Horovod: pin GPU to local rank\n            assert self.root_gpu == hvd.local_rank()\n            torch.cuda.set_device(self.root_gpu)\n            model.cuda(self.root_gpu)\n\n        # avoid duplicating progress bar\n        if hvd.rank() != 0 and self.progress_bar_callback is not None:\n            self.progress_bar_callback.disable()\n\n        # CHOOSE OPTIMIZER\n        # allow for lr schedulers as well\n        self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n\n        # Horovod: scale the learning rate by the number of workers to account for\n        # increased total batch size\n        for optimizer in self.optimizers:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] *= hvd.size()\n\n        if self.use_amp:\n            # An example\n            model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)\n            self.optimizers = optimizers\n            self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)\n\n        # Horovod: broadcast parameters & optimizer state to ensure consistent initialization\n        hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n        for optimizer in self.optimizers:\n            hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n\n        def filter_named_parameters(model, optimizer):\n            opt_params = set([p for group in optimizer.param_groups for p in group.get(\'params\', [])])\n            return [(name, p) for name, p in model.named_parameters() if p in opt_params]\n\n        # Horovod: wrap optimizers to perform gradient aggregation via allreduce\n        self.optimizers = [\n            hvd.DistributedOptimizer(optimizer, named_parameters=filter_named_parameters(model, optimizer))\n            for optimizer in self.optimizers\n        ]\n\n        # Update logger rank info from Horovod to avoid race conditions from  different ranks\n        # creating directories / writing files in the same locations.\n        self.proc_rank = hvd.rank()\n        rank_zero_only.rank = self.proc_rank\n\n        with ExitStack() as stack:\n            for optimizer in self.optimizers:\n                # Synchronization will be performed explicitly following backward()\n                stack.enter_context(optimizer.skip_synchronize())\n\n            self.run_pretrain_routine(model)\n\n        # Make sure all workers have finished training before returning to the user\n        hvd.join()\n\n\ndef normalize_parse_gpu_string_input(s):\n    if isinstance(s, str):\n        if s == \'-1\':\n            return -1\n        else:\n            return [int(x.strip()) for x in s.split(\',\') if len(x) > 0]\n    else:\n        return s\n\n\ndef get_all_available_gpus() -> List[int]:\n    """"""\n    Returns:\n         a list of all available gpus\n    """"""\n    return list(range(torch.cuda.device_count()))\n\n\ndef check_gpus_data_type(gpus: Any) -> None:\n    """"""\n    Checks that the gpus argument is one of: None, Int, String or List.\n    Raises a MisconfigurationException otherwise.\n\n    Args:\n        gpus: parameter as passed to the Trainer\n    """"""\n    if gpus is not None and (not isinstance(gpus, (int, str, list)) or isinstance(gpus, bool)):\n        raise MisconfigurationException(""GPUs must be int, string or list of ints or None."")\n\n\ndef normalize_parse_gpu_input_to_list(gpus: Union[int, List[int]]) -> Optional[List[int]]:\n    assert gpus is not None\n    if isinstance(gpus, list):\n        return gpus\n\n    # must be an int\n    if not gpus:  # gpus==0\n        return None\n    if gpus == -1:\n        return get_all_available_gpus()\n\n    return list(range(gpus))\n\n\ndef sanitize_gpu_ids(gpus: List[int]) -> List[int]:\n    """"""\n    Checks that each of the GPUs in the list is actually available.\n    Raises a MisconfigurationException if any of the GPUs is not available.\n\n    Args:\n        gpus: list of ints corresponding to GPU indices\n\n    Returns:\n        unmodified gpus variable\n    """"""\n    all_available_gpus = get_all_available_gpus()\n    misconfig = False\n    for gpu in gpus:\n        if gpu not in all_available_gpus:\n            misconfig = True\n\n    if misconfig:\n        # sometimes auto ddp might have different flags\n        # but this is not what the user intended\n        # correct for the user\n        if len(gpus) == len(all_available_gpus):\n            gpus = all_available_gpus\n        else:\n            raise MisconfigurationException(f""""""\n                You requested GPUs: {gpus}\n                But your machine only has: {all_available_gpus}\n            """""")\n    return gpus\n\n\ndef parse_gpu_ids(gpus: Union[int, str, List]) -> Optional[List[int]]:\n    """"""\n    Parses the GPU ids given in the format as accepted by the\n    :class:`~pytorch_lightning.trainer.Trainer`.\n\n    Args:\n        gpus: An int -1 or string \'-1\' indicate that all available GPUs should be used.\n            A list of ints or a string containing list of comma separated integers\n            indicates specific GPUs to use.\n            An int 0 means that no GPUs should be used.\n            Any int N > 0 indicates that GPUs [0..N) should be used.\n\n    Returns:\n        a list of gpus to be used or ``None`` if no GPUs were requested\n\n    If no GPUs are available but the value of gpus variable indicates request for GPUs\n    then a MisconfigurationException is raised.\n    """"""\n\n    # nothing was passed into the GPUs argument\n    if callable(gpus):\n        return None\n\n    # Check that gpus param is None, Int, String or List\n    check_gpus_data_type(gpus)\n\n    # Handle the case when no gpus are requested\n    if gpus is None or isinstance(gpus, int) and gpus == 0:\n        return None\n\n    # We know user requested GPUs therefore if some of the\n    # requested GPUs are not available an exception is thrown.\n\n    gpus = normalize_parse_gpu_string_input(gpus)\n    gpus = normalize_parse_gpu_input_to_list(gpus)\n    gpus = sanitize_gpu_ids(gpus)\n\n    if not gpus:\n        raise MisconfigurationException(""GPUs requested but none are available."")\n    return gpus\n\n\ndef determine_root_gpu_device(gpus: List[int]) -> Optional[int]:\n    """"""\n    Args:\n        gpus: non-empty list of ints representing which gpus to use\n\n    Returns:\n        designated root GPU device id\n    """"""\n    if gpus is None:\n        return None\n\n    assert isinstance(gpus, list), ""gpus should be a list""\n    assert len(gpus) > 0, ""gpus should be a non empty list""\n\n    # set root gpu\n    root_gpu = gpus[0]\n\n    return root_gpu\n\n\ndef retry_jittered_backoff(func: Callable, num_retries: int = 5, cap_delay: float = 1.0, base_delay: float = 0.01):\n    """"""Retry jittered backoff.\n\n    Based on:\n    https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/\n\n    Args:\n        func: tested function\n        num_retries: number of tries\n        cap_delay: max sleep time\n        base_delay: initial sleep time is 10ms\n    """"""\n    sleep_delay = base_delay         # initial sleep time is 10ms\n\n    for i in range(num_retries):\n        try:\n            return func()\n        except RuntimeError as err:\n            if i == num_retries - 1:\n                raise err\n            else:\n                continue\n        time.sleep(sleep_delay)\n        sleep_delay = min(cap_delay, random.uniform(base_delay, sleep_delay * 3))\n\n\ndef pick_single_gpu(exclude_gpus: list):\n    for i in range(torch.cuda.device_count()):\n        if i in exclude_gpus:\n            continue\n        # Try to allocate on device:\n        device = torch.device(f""cuda:{i}"")\n        try:\n            torch.ones(1).to(device)\n        except RuntimeError:\n            continue\n        return i\n    raise RuntimeError(""No GPUs available."")\n\n\ndef pick_multiple_gpus(nb):\n    picked = []\n    for _ in range(nb):\n        picked.append(pick_single_gpu(exclude_gpus=picked))\n\n    return picked\n'"
pytorch_lightning/trainer/evaluation_loop.py,4,"b'""""""\nValidation loop\n===============\n\nThe lightning validation loop handles everything except the actual computations of your model.\nTo decide what will happen in your validation loop, define the `validation_step` function.\nBelow are all the things lightning automates for you in the validation loop.\n\n.. note:: Lightning will run 5 steps of validation in the beginning of training as a sanity\n check so you don\'t have to wait until a full epoch to catch possible validation issues.\n\nCheck validation every n epochs\n-------------------------------\n\nIf you have a small dataset you might want to check validation every n epochs\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(check_val_every_n_epoch=1)\n\nSet how much of the validation set to check\n-------------------------------------------\n\nIf you don\'t want to check 100% of the validation set (for debugging or if it\'s huge), set this flag\n\nval_percent_check will be overwritten by overfit_pct if `overfit_pct > 0`\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(val_percent_check=1.0)\n\n    # check 10% only\n    trainer = Trainer(val_percent_check=0.1)\n\nSet how much of the test set to check\n-------------------------------------\n\nIf you don\'t want to check 100% of the test set (for debugging or if it\'s huge), set this flag\n\ntest_percent_check will be overwritten by overfit_pct if `overfit_pct > 0`\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(test_percent_check=1.0)\n\n    # check 10% only\n    trainer = Trainer(test_percent_check=0.1)\n\nSet validation check frequency within 1 training epoch\n------------------------------------------------------\n\nFor large datasets it\'s often desirable to check validation multiple times within a training loop.\n Pass in a float to check that often within 1 training epoch.\n Pass in an int k to check every k training batches. Must use an int if using an IterableDataset.\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(val_check_interval=0.95)\n\n    # check every .25 of an epoch\n    trainer = Trainer(val_check_interval=0.25)\n\n    # check every 100 train batches (ie: for IterableDatasets or fixed frequency)\n    trainer = Trainer(val_check_interval=100)\n\n\nSet the number of validation sanity steps\n-----------------------------------------\n\nLightning runs a few steps of validation in the beginning of training.\n This avoids crashing in the validation loop sometime deep into a lengthy training loop.\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(num_sanity_val_steps=5)\n\n\nYou can use `Trainer(num_sanity_val_steps=0)` to skip the sanity check.\n\n# Testing loop\n\nTo ensure you don\'t accidentally use test data to guide training decisions Lightning\n makes running the test set deliberate.\n\n**test**\n\nYou have two options to run the test set.\nFirst case is where you test right after a full training routine.\n\n.. code-block:: python\n\n    # run full training\n    trainer.fit(model)\n\n    # run test set\n    trainer.test()\n\n\nSecond case is where you load a model and run the test set\n\n.. code-block:: python\n\n    model = MyLightningModule.load_from_checkpoint(\n        checkpoint_path=\'/path/to/pytorch_checkpoint.ckpt\',\n        hparams_file=\'/path/to/test_tube/experiment/version/hparams.yaml\',\n        map_location=None\n    )\n\n    # init trainer with whatever options\n    trainer = Trainer(...)\n\n    # test (pass in the model)\n    trainer.test(model)\n\nIn this second case, the options you pass to trainer will be used when running\n the test set (ie: 16-bit, dp, ddp, etc...)\n\n""""""\n\nfrom abc import ABC, abstractmethod\nfrom pprint import pprint\nfrom typing import Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel, LightningDataParallel\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities import rank_zero_warn\n\ntry:\n    import torch_xla.distributed.parallel_loader as xla_pl\n    import torch_xla.core.xla_model as xm\nexcept ImportError:\n    XLA_AVAILABLE = False\nelse:\n    XLA_AVAILABLE = True\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n\nclass TrainerEvaluationLoopMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    on_gpu: bool\n    use_ddp: bool\n    use_dp: bool\n    use_ddp2: bool\n    use_horovod: bool\n    single_gpu: bool\n    data_parallel_device_ids: ...\n    model: LightningModule\n    num_test_batches: int\n    num_val_batches: int\n    fast_dev_run: ...\n    process_output: ...\n    progress_bar_dict: ...\n    proc_rank: int\n    current_epoch: int\n    callback_metrics: ...\n    test_dataloaders: DataLoader\n    val_dataloaders: DataLoader\n    use_tpu: bool\n    reload_dataloaders_every_epoch: ...\n    tpu_id: int\n\n    # Callback system\n    on_validation_batch_start: Callable\n    on_validation_batch_end: Callable\n    on_test_batch_start: Callable\n    on_test_batch_end: Callable\n    on_validation_start: Callable\n    on_validation_end: Callable\n    on_test_start: Callable\n    on_test_end: Callable\n\n    @abstractmethod\n    def copy_trainer_model_properties(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def get_model(self):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def is_overridden(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def transfer_batch_to_tpu(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def transfer_batch_to_gpu(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def add_progress_bar_metrics(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def log_metrics(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def reset_test_dataloader(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def reset_val_dataloader(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def _evaluate(self, model: LightningModule, dataloaders, max_batches: int, test_mode: bool = False):\n        """"""Run evaluation code.\n\n        Args:\n            model: PT model\n            dataloaders: list of PT dataloaders\n            max_batches: Scalar\n            test_mode:\n        """"""\n        # enable eval mode\n        model.zero_grad()\n        model.eval()\n\n        # copy properties for forward overrides\n        self.copy_trainer_model_properties(model)\n\n        # disable gradients to save memory\n        torch.set_grad_enabled(False)\n\n        # bookkeeping\n        outputs = []\n\n        # run validation\n        for dataloader_idx, dataloader in enumerate(dataloaders):\n            dl_outputs = []\n\n            # on TPU we have to wrap it under the ParallelLoader\n            if self.use_tpu:\n                device = xm.xla_device(self.tpu_id)\n                dataloader = xla_pl.ParallelLoader(dataloader, [device])\n                dataloader = dataloader.per_device_loader(device)\n\n            for batch_idx, batch in enumerate(dataloader):\n                if batch is None:\n                    continue\n\n                # stop short when on fast_dev_run (sets max_batch=1)\n                if batch_idx >= max_batches:\n                    break\n\n                # callbacks\n                if test_mode:\n                    self.on_test_batch_start()\n                else:\n                    self.on_validation_batch_start()\n\n                # -----------------\n                # RUN EVALUATION STEP\n                # -----------------\n                if self.use_amp and self.use_native_amp:\n                    with torch.cuda.amp.autocast():\n                        output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\n                else:\n                    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\n\n                # on dp / ddp2 might still want to do something with the batch parts\n                if test_mode:\n                    if self.is_overridden(\'test_step_end\'):\n                        model_ref = self.get_model()\n                        with self.profiler.profile(\'test_step_end\'):\n                            output = model_ref.test_step_end(output)\n                    self.on_test_batch_end()\n                else:\n                    if self.is_overridden(\'validation_step_end\'):\n                        model_ref = self.get_model()\n                        with self.profiler.profile(\'validation_step_end\'):\n                            output = model_ref.validation_step_end(output)\n                    self.on_validation_batch_end()\n\n                # track outputs for collation\n                dl_outputs.append(output)\n\n            outputs.append(dl_outputs)\n\n        eval_results = {}\n\n        # with a single dataloader don\'t pass an array\n        if len(dataloaders) == 1:\n            outputs = outputs[0]\n\n        # give model a chance to do something with the outputs (and method defined)\n        if isinstance(model, (LightningDistributedDataParallel, LightningDataParallel)):\n            model = model.module\n\n        if test_mode:\n            if self.is_overridden(\'test_end\', model=model):\n                # TODO: remove in v1.0.0\n                eval_results = model.test_end(outputs)\n                rank_zero_warn(\'Method `test_end` was deprecated in v0.7 and will be removed v1.0.\'\n                               \' Use `test_epoch_end` instead.\', DeprecationWarning)\n\n            elif self.is_overridden(\'test_epoch_end\', model=model):\n                eval_results = model.test_epoch_end(outputs)\n\n        else:\n            if self.is_overridden(\'validation_end\', model=model):\n                # TODO: remove in v1.0.0\n                eval_results = model.validation_end(outputs)\n                rank_zero_warn(\'Method `validation_end` was deprecated in v0.7 and will be removed v1.0.\'\n                               \' Use `validation_epoch_end` instead.\', DeprecationWarning)\n\n            elif self.is_overridden(\'validation_epoch_end\', model=model):\n                eval_results = model.validation_epoch_end(outputs)\n\n        # enable train mode again\n        model.train()\n\n        # enable gradients to save memory\n        torch.set_grad_enabled(True)\n\n        return eval_results\n\n    def run_evaluation(self, test_mode: bool = False):\n        # hook\n        model = self.get_model()\n        model.on_pre_performance_check()\n\n        # select dataloaders\n        if test_mode:\n            self.reset_test_dataloader(model)\n\n            dataloaders = self.test_dataloaders\n            max_batches = self.num_test_batches\n        else:\n            # val\n            if self.val_dataloaders is None:\n                self.reset_val_dataloader(model)\n\n            dataloaders = self.val_dataloaders\n            max_batches = self.num_val_batches\n\n        # enable fast_dev_run without val loop\n        if dataloaders is None:\n            return\n\n        # cap max batches to 1 when using fast_dev_run\n        if self.fast_dev_run:\n            max_batches = 1\n\n        # Validation/Test begin callbacks\n        if test_mode:\n            self.on_test_start()\n        else:\n            self.on_validation_start()\n\n        # run evaluation\n        eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n        _, prog_bar_metrics, log_metrics, callback_metrics, _ = self.process_output(eval_results)\n\n        # add metrics to prog bar\n        self.add_progress_bar_metrics(prog_bar_metrics)\n\n        # log results of test\n        if test_mode and self.proc_rank == 0:\n            print(\'-\' * 80)\n            print(\'TEST RESULTS\')\n            pprint(callback_metrics)\n            print(\'-\' * 80)\n\n        # log metrics\n        self.log_metrics(log_metrics, {})\n\n        # track metrics for callbacks\n        self.callback_metrics.update(callback_metrics)\n\n        # hook\n        model.on_post_performance_check()\n\n        # eventual dataset reloading\n        if test_mode:\n            if self.reload_dataloaders_every_epoch:\n                self.reset_test_dataloader(model)\n        else:\n            # val\n            if self.reload_dataloaders_every_epoch:\n                self.reset_val_dataloader(model)\n\n        # Validation/Test end callbacks\n        if test_mode:\n            self.on_test_end()\n        else:\n            self.on_validation_end()\n\n    def evaluation_forward(self, model, batch, batch_idx, dataloader_idx, test_mode: bool = False):\n        # make dataloader_idx arg in validation_step optional\n        args = [batch, batch_idx]\n\n        if (test_mode and len(self.test_dataloaders) > 1) \\\n                or (not test_mode and len(self.val_dataloaders) > 1):\n            args.append(dataloader_idx)\n\n        # handle DP, DDP forward\n        if self.use_ddp or self.use_dp or self.use_ddp2:\n            output = model(*args)\n            return output\n\n        # Horovod\n        if self.use_horovod and self.on_gpu:\n            batch = self.transfer_batch_to_gpu(batch, hvd.local_rank())\n            args[0] = batch\n\n        # single GPU data transfer\n        if self.single_gpu:\n            # for single GPU put inputs on gpu manually\n            root_gpu = 0\n            if isinstance(self.data_parallel_device_ids, list):\n                root_gpu = self.data_parallel_device_ids[0]\n            batch = self.transfer_batch_to_gpu(batch, root_gpu)\n            args[0] = batch\n\n        # TPU data  transfer\n        if self.use_tpu:\n            batch = self.transfer_batch_to_tpu(batch, self.tpu_id)\n            args[0] = batch\n\n        # CPU, TPU or gpu step\n        if test_mode:\n            output = model.test_step(*args)\n        else:\n            output = model.validation_step(*args)\n\n        return output\n'"
pytorch_lightning/trainer/ignored_warnings.py,0,"b'import warnings\n\n\ndef ignore_scalar_return_in_dp():\n    # Users get confused by this warning so we silence it\n    m_1 = """"""\n    Was asked to gather along dimension 0, but all\n    input tensors were scalars; will instead unsqueeze\n    and return a vector.\n    """"""\n    warnings.filterwarnings(\'ignore\', message=m_1)\n\n\nignore_scalar_return_in_dp()\n'"
pytorch_lightning/trainer/logging.py,6,"b'from abc import ABC\nfrom typing import Union, Iterable\n\nimport torch\n\nfrom pytorch_lightning.core import memory\nfrom pytorch_lightning.loggers import TensorBoardLogger, LightningLoggerBase, LoggerCollection\nfrom pytorch_lightning.utilities.memory import recursive_detach\n\n\nclass TrainerLoggingMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    current_epoch: int\n    on_gpu: bool\n    log_gpu_memory: ...\n    logger: Union[LightningLoggerBase, bool]\n    progress_bar_metrics: ...\n    global_step: int\n    proc_rank: int\n    use_dp: bool\n    use_ddp2: bool\n    default_root_dir: str\n    slurm_job_id: int\n    num_gpus: int\n\n    def configure_logger(self, logger):\n        if logger is True:\n            # default logger\n            self.logger = TensorBoardLogger(\n                save_dir=self.default_root_dir,\n                version=self.slurm_job_id,\n                name=\'lightning_logs\'\n            )\n        elif logger is False:\n            self.logger = None\n        else:\n            if isinstance(logger, Iterable):\n                self.logger = LoggerCollection(logger)\n            else:\n                self.logger = logger\n\n    def log_metrics(self, metrics, grad_norm_dic, step=None):\n        """"""Logs the metric dict passed in.\n        If `step` parameter is None and `step` key is presented is metrics,\n        uses metrics[""step""] as a step\n\n        Args:\n            metrics (dict): Metric values\n            grad_norm_dic (dict): Gradient norms\n            step (int): Step for which metrics should be logged. Default value corresponds to `self.global_step`\n        """"""\n        # add gpu memory\n        if self.on_gpu and self.log_gpu_memory:\n            mem_map = memory.get_memory_profile(self.log_gpu_memory)\n            metrics.update(mem_map)\n\n        # add norms\n        metrics.update(grad_norm_dic)\n\n        # turn all tensors to scalars\n        scalar_metrics = self.metrics_to_scalars(metrics)\n\n        if ""step"" in scalar_metrics and step is None:\n            step = scalar_metrics.pop(""step"")\n        else:\n            # added metrics by Lightning for convenience\n            scalar_metrics[\'epoch\'] = self.current_epoch\n            step = step if step is not None else self.global_step\n        # log actual metrics\n        if self.proc_rank == 0 and self.logger is not None:\n            self.logger.agg_and_log_metrics(scalar_metrics, step=step)\n            self.logger.save()\n\n    def add_progress_bar_metrics(self, metrics):\n        for k, v in metrics.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n\n            self.progress_bar_metrics[k] = v\n\n    def metrics_to_scalars(self, metrics):\n        new_metrics = {}\n        for k, v in metrics.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n\n            if isinstance(v, dict):\n                v = self.metrics_to_scalars(v)\n\n            new_metrics[k] = v\n\n        return new_metrics\n\n    def process_output(self, output, train=False):\n        """"""Reduces output according to the training mode.\n\n        Separates loss from logging and progress bar metrics\n        """"""\n        # ---------------\n        # EXTRACT CALLBACK KEYS\n        # ---------------\n        # all keys not progress_bar or log are candidates for callbacks\n        callback_metrics = {}\n        for k, v in output.items():\n            if k not in [\'progress_bar\', \'log\', \'hiddens\']:\n                callback_metrics[k] = v\n\n        if train and (self.use_dp or self.use_ddp2):\n            num_gpus = self.num_gpus\n            callback_metrics = self.reduce_distributed_output(callback_metrics, num_gpus)\n\n        # ---------------\n        # EXTRACT PROGRESS BAR KEYS\n        # ---------------\n        try:\n            progress_output = output[\'progress_bar\']\n\n            # reduce progress metrics for progress bar when using dp\n            if train and (self.use_dp or self.use_ddp2):\n                num_gpus = self.num_gpus\n                progress_output = self.reduce_distributed_output(progress_output, num_gpus)\n\n            progress_bar_metrics = progress_output\n        except Exception:\n            progress_bar_metrics = {}\n\n        # ---------------\n        # EXTRACT LOGGING KEYS\n        # ---------------\n        # extract metrics to log to experiment\n        try:\n            log_output = output[\'log\']\n\n            # reduce progress metrics for progress bar when using dp\n            if train and (self.use_dp or self.use_ddp2):\n                num_gpus = self.num_gpus\n                log_output = self.reduce_distributed_output(log_output, num_gpus)\n\n            log_metrics = log_output\n        except Exception:\n            log_metrics = {}\n\n        # ---------------\n        # EXTRACT LOSS\n        # ---------------\n        # if output dict doesn\'t have the keyword loss\n        # then assume the output=loss if scalar\n        loss = None\n        if train:\n            try:\n                loss = output[\'loss\']\n            except Exception:\n                if isinstance(output, torch.Tensor):\n                    loss = output\n                else:\n                    raise RuntimeError(\n                        \'No `loss` value in the dictionary returned from `model.training_step()`.\'\n                    )\n\n            # when using dp need to reduce the loss\n            if self.use_dp or self.use_ddp2:\n                loss = self.reduce_distributed_output(loss, self.num_gpus)\n\n        # ---------------\n        # EXTRACT HIDDEN\n        # ---------------\n        hiddens = output.get(\'hiddens\')\n\n        # use every metric passed in as a candidate for callback\n        callback_metrics.update(progress_bar_metrics)\n        callback_metrics.update(log_metrics)\n\n        # detach all metrics for callbacks to prevent memory leaks\n        # no .item() because it will slow things down\n        callback_metrics = recursive_detach(callback_metrics)\n\n        return loss, progress_bar_metrics, log_metrics, callback_metrics, hiddens\n\n    def reduce_distributed_output(self, output, num_gpus):\n        if num_gpus <= 1:\n            return output\n\n        # when using DP, we get one output per gpu\n        # average outputs and return\n        if isinstance(output, torch.Tensor):\n            return output.mean()\n\n        for k, v in output.items():\n            # recurse on nested dics\n            if isinstance(output[k], dict):\n                output[k] = self.reduce_distributed_output(output[k], num_gpus)\n\n            # do nothing when there\'s a scalar\n            elif isinstance(output[k], torch.Tensor) and output[k].dim() == 0:\n                pass\n\n            # do not reduce metrics that have batch size > num gpus\n            elif output[k].size(0) <= num_gpus:\n                output[k] = torch.mean(output[k])\n\n        return output\n'"
pytorch_lightning/trainer/lr_finder.py,6,"b'""""""\nTrainer Learning Rate Finder\n""""""\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport os\n\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.callbacks import Callback\nfrom pytorch_lightning.loggers.base import DummyLogger\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities import rank_zero_warn\n\n\nclass TrainerLRFinderMixin(ABC):\n    default_root_dir: str\n\n    @abstractmethod\n    def save_checkpoint(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def restore(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def _run_lr_finder_internally(self, model: LightningModule):\n        """""" Call lr finder internally during Trainer.fit() """"""\n        lr_finder = self.lr_find(model)\n        lr = lr_finder.suggestion()\n        # TODO: log lr.results to self.logger\n        if isinstance(self.auto_lr_find, str):\n            # Try to find requested field, may be nested\n            if _nested_hasattr(model, self.auto_lr_find):\n                _nested_setattr(model, self.auto_lr_find, lr)\n            else:\n                raise MisconfigurationException(\n                    f\'`auto_lr_find` was set to {self.auto_lr_find}, however\'\n                    \' could not find this as a field in `model.hparams`.\')\n        else:\n            if hasattr(model, \'lr\'):\n                model.lr = lr\n            elif hasattr(model, \'learning_rate\'):\n                model.learning_rate = lr\n            else:\n                raise MisconfigurationException(\n                    \'When auto_lr_find is set to True, expects that hparams\'\n                    \' either has field `lr` or `learning_rate` that can overridden\')\n        log.info(f\'Learning rate set to {lr}\')\n\n    def lr_find(self,\n                model: LightningModule,\n                train_dataloader: Optional[DataLoader] = None,\n                val_dataloaders: Optional[DataLoader] = None,\n                min_lr: float = 1e-8,\n                max_lr: float = 1,\n                num_training: int = 100,\n                mode: str = \'exponential\',\n                early_stop_threshold: float = 4.0,\n                num_accumulation_steps=None):\n        r""""""\n        lr_find enables the user to do a range test of good initial learning rates,\n        to reduce the amount of guesswork in picking a good starting learning rate.\n\n        Args:\n            model: Model to do range testing for\n\n            train_dataloader: A PyTorch\n                DataLoader with training samples. If the model has\n                a predefined train_dataloader method this will be skipped.\n\n            min_lr: minimum learning rate to investigate\n\n            max_lr: maximum learning rate to investigate\n\n            num_training: number of learning rates to test\n\n            mode: search strategy, either \'linear\' or \'exponential\'. If set to\n                \'linear\' the learning rate will be searched by linearly increasing\n                after each batch. If set to \'exponential\', will increase learning\n                rate exponentially.\n\n            early_stop_threshold: threshold for stopping the search. If the\n                loss at any point is larger than early_stop_threshold*best_loss\n                then the search is stopped. To disable, set to None.\n\n            num_accumulation_steps: deprepecated, number of batches to calculate loss over.\n                Set trainer argument ``accumulate_grad_batches`` instead.\n\n        Example::\n\n            # Setup model and trainer\n            model = MyModelClass(hparams)\n            trainer = pl.Trainer()\n\n            # Run lr finder\n            lr_finder = trainer.lr_find(model, ...)\n\n            # Inspect results\n            fig = lr_finder.plot(); fig.show()\n            suggested_lr = lr_finder.suggestion()\n\n            # Overwrite lr and create new model\n            hparams.lr = suggested_lr\n            model = MyModelClass(hparams)\n\n            # Ready to train with new learning rate\n            trainer.fit(model)\n\n        """"""\n        if num_accumulation_steps is not None:\n            rank_zero_warn(""Argument `num_accumulation_steps` has been deprepecated""\n                           "" since v0.7.6 and will be removed in 0.9. Please""\n                           "" set trainer argument `accumulate_grad_batches` instead."",\n                           DeprecationWarning)\n\n        save_path = os.path.join(self.default_root_dir, \'lr_find_temp.ckpt\')\n\n        self.__lr_finder_dump_params(model)\n\n        # Prevent going into infinite loop\n        self.auto_lr_find = False\n\n        # Initialize lr finder object (stores results)\n        lr_finder = _LRFinder(mode, min_lr, max_lr, num_training)\n\n        # Use special lr logger callback\n        self.callbacks = [_LRCallback(num_training,\n                                      early_stop_threshold,\n                                      progress_bar_refresh_rate=1)]\n\n        # No logging\n        self.logger = DummyLogger()\n\n        # Max step set to number of iterations\n        self.max_steps = num_training\n\n        # Disable standard progress bar for fit\n        if self.progress_bar_callback:\n            self.progress_bar_callback.disable()\n\n        # Disable standard checkpoint & early stopping\n        self.checkpoint_callback = False\n        self.early_stop_callback = None\n        self.enable_early_stop = False\n\n        # Required for saving the model\n        self.optimizers, self.schedulers = [], [],\n        self.model = model\n\n        # Dump model checkpoint\n        self.save_checkpoint(str(save_path))\n\n        # Configure optimizer and scheduler\n        optimizers, _, _ = self.init_optimizers(model)\n\n        if len(optimizers) != 1:\n            raise MisconfigurationException(\n                f\'`model.configure_optimizers()` returned {len(optimizers)}, but\'\n                \' learning rate finder only works with single optimizer\')\n        model.configure_optimizers = lr_finder._get_new_optimizer(optimizers[0])\n\n        # Fit, lr & loss logged in callback\n        self.fit(model,\n                 train_dataloader=train_dataloader,\n                 val_dataloaders=val_dataloaders)\n\n        # Prompt if we stopped early\n        if self.global_step != num_training:\n            log.info(\'LR finder stopped early due to diverging loss.\')\n\n        # Transfer results from callback to lr finder object\n        lr_finder.results.update({\'lr\': self.callbacks[0].lrs,\n                                  \'loss\': self.callbacks[0].losses})\n        lr_finder._total_batch_idx = self.total_batch_idx  # for debug purpose\n\n        # Reset model state\n        self.restore(str(save_path), on_gpu=self.on_gpu)\n        os.remove(save_path)\n\n        # Finish by resetting variables so trainer is ready to fit model\n        self.__lr_finder_restore_params(model)\n        if self.progress_bar_callback:\n            self.progress_bar_callback.enable()\n\n        return lr_finder\n\n    def __lr_finder_dump_params(self, model):\n        # Prevent going into infinite loop\n        self.__dumped_params = {\n            \'auto_lr_find\': self.auto_lr_find,\n            \'callbacks\': self.callbacks,\n            \'logger\': self.logger,\n            \'max_steps\': self.max_steps,\n            \'checkpoint_callback\': self.checkpoint_callback,\n            \'early_stop_callback\': self.early_stop_callback,\n            \'enable_early_stop\': self.enable_early_stop,\n            \'configure_optimizers\': model.configure_optimizers,\n        }\n\n    def __lr_finder_restore_params(self, model):\n        self.auto_lr_find = self.__dumped_params[\'auto_lr_find\']\n        self.logger = self.__dumped_params[\'logger\']\n        self.callbacks = self.__dumped_params[\'callbacks\']\n        self.max_steps = self.__dumped_params[\'max_steps\']\n        self.checkpoint_callback = self.__dumped_params[\'checkpoint_callback\']\n        self.early_stop_callback = self.__dumped_params[\'early_stop_callback\']\n        self.enable_early_stop = self.__dumped_params[\'enable_early_stop\']\n        model.configure_optimizers = self.__dumped_params[\'configure_optimizers\']\n        del self.__dumped_params\n\n\nclass _LRFinder(object):\n    """""" LR finder object. This object stores the results of Trainer.lr_find().\n\n    Args:\n        mode: either `linear` or `exponential`, how to increase lr after each step\n\n        lr_min: lr to start search from\n\n        lr_max: lr to stop search\n\n        num_training: number of steps to take between lr_min and lr_max\n\n    Example::\n        # Run lr finder\n        lr_finder = trainer.lr_find(model)\n\n        # Results stored in\n        lr_finder.results\n\n        # Plot using\n        lr_finder.plot()\n\n        # Get suggestion\n        lr = lr_finder.suggestion()\n    """"""\n    def __init__(self, mode: str, lr_min: float, lr_max: float, num_training: int):\n        assert mode in (\'linear\', \'exponential\'), \\\n            \'mode should be either `linear` or `exponential`\'\n\n        self.mode = mode\n        self.lr_min = lr_min\n        self.lr_max = lr_max\n        self.num_training = num_training\n\n        self.results = {}\n        self._total_batch_idx = 0  # for debug purpose\n\n    def _get_new_optimizer(self, optimizer: torch.optim.Optimizer):\n        """""" Construct a new `configure_optimizers()` method, that has a optimizer\n            with initial lr set to lr_min and a scheduler that will either\n            linearly or exponentially increase the lr to lr_max in num_training steps.\n\n        Args:\n            optimizer: instance of `torch.optim.Optimizer`\n\n        """"""\n        new_lrs = [self.lr_min] * len(optimizer.param_groups)\n        for param_group, new_lr in zip(optimizer.param_groups, new_lrs):\n            param_group[""lr""] = new_lr\n            param_group[""initial_lr""] = new_lr\n\n        args = (optimizer, self.lr_max, self.num_training)\n        scheduler = _LinearLR(*args) if self.mode == \'linear\' else _ExponentialLR(*args)\n\n        def configure_optimizers():\n            return [optimizer], [{\'scheduler\': scheduler,\n                                  \'interval\': \'step\'}]\n\n        return configure_optimizers\n\n    def plot(self, suggest: bool = False, show: bool = False):\n        """""" Plot results from lr_find run\n        Args:\n            suggest: if True, will mark suggested lr to use with a red point\n\n            show: if True, will show figure\n        """"""\n        import matplotlib.pyplot as plt\n\n        lrs = self.results[""lr""]\n        losses = self.results[""loss""]\n\n        fig, ax = plt.subplots()\n\n        # Plot loss as a function of the learning rate\n        ax.plot(lrs, losses)\n        if self.mode == \'exponential\':\n            ax.set_xscale(""log"")\n        ax.set_xlabel(""Learning rate"")\n        ax.set_ylabel(""Loss"")\n\n        if suggest:\n            _ = self.suggestion()\n            if self._optimal_idx:\n                ax.plot(lrs[self._optimal_idx], losses[self._optimal_idx],\n                        markersize=10, marker=\'o\', color=\'red\')\n\n        if show:\n            plt.show()\n\n        return fig\n\n    def suggestion(self, skip_begin: int = 10, skip_end: int = 1):\n        """""" This will propose a suggestion for choice of initial learning rate\n        as the point with the steepest negative gradient.\n\n        Returns:\n            lr: suggested initial learning rate to use\n            skip_begin: how many samples to skip in the beginning. Prevent too naive estimates\n            skip_end: how many samples to skip in the end. Prevent too optimistic estimates\n\n        """"""\n        try:\n            loss = np.array(self.results[""loss""][skip_begin:-skip_end])\n            loss = loss[np.isfinite(loss)]\n            min_grad = np.gradient(loss).argmin()\n            self._optimal_idx = min_grad + skip_begin\n            return self.results[""lr""][self._optimal_idx]\n        except Exception:\n            log.exception(\'Failed to compute suggesting for `lr`. There might not be enough points.\')\n            self._optimal_idx = None\n\n\nclass _LRCallback(Callback):\n    """""" Special callback used by the learning rate finder. This callbacks log\n    the learning rate before each batch and log the corresponding loss after\n    each batch.\n\n    Args:\n        num_training: number of iterations done by the learning rate finder\n        early_stop_threshold: threshold for stopping the search. If the\n            loss at any point is larger than ``early_stop_threshold*best_loss``\n            then the search is stopped. To disable, set to ``None``.\n        progress_bar_refresh_rate: rate to refresh the progress bar for\n            the learning rate finder\n        beta: smoothing value, the loss being logged is a running average of\n            loss values logged until now. ``beta`` controls the forget rate i.e.\n            if ``beta=0`` all past information is ignored.\n\n    """"""\n    def __init__(self, num_training: int,\n                 early_stop_threshold: float = 4.0,\n                 progress_bar_refresh_rate: int = 0,\n                 beta: float = 0.98):\n        self.num_training = num_training\n        self.early_stop_threshold = early_stop_threshold\n        self.beta = beta\n        self.losses = []\n        self.lrs = []\n        self.avg_loss = 0.0\n        self.best_loss = 0.0\n        self.progress_bar_refresh_rate = progress_bar_refresh_rate\n        self.progress_bar = None\n\n    def on_batch_start(self, trainer, pl_module):\n        """""" Called before each training batch, logs the lr that will be used """"""\n        if (trainer.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\n            return\n\n        if self.progress_bar_refresh_rate and self.progress_bar is None:\n            self.progress_bar = tqdm(desc=\'Finding best initial lr\', total=self.num_training)\n\n        self.lrs.append(trainer.lr_schedulers[0][\'scheduler\'].lr[0])\n\n    def on_batch_end(self, trainer, pl_module):\n        """""" Called when the training batch ends, logs the calculated loss """"""\n        if (trainer.batch_idx + 1) % trainer.accumulate_grad_batches != 0:\n            return\n\n        if self.progress_bar:\n            self.progress_bar.update()\n\n        current_loss = trainer.running_loss.last().item()\n        current_step = trainer.global_step + 1  # remove the +1 in 1.0\n\n        # Avg loss (loss with momentum) + smoothing\n        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * current_loss\n        smoothed_loss = self.avg_loss / (1 - self.beta**current_step)\n\n        # Check if we diverging\n        if self.early_stop_threshold is not None:\n            if current_step > 1 and smoothed_loss > self.early_stop_threshold * self.best_loss:\n                trainer.max_steps = current_step  # stop signal\n                if self.progress_bar:\n                    self.progress_bar.close()\n\n        # Save best loss for diverging checking\n        if smoothed_loss < self.best_loss or current_step == 1:\n            self.best_loss = smoothed_loss\n\n        self.losses.append(smoothed_loss)\n\n\nclass _LinearLR(_LRScheduler):\n    """"""Linearly increases the learning rate between two boundaries\n    over a number of iterations.\n    Arguments:\n\n        optimizer: wrapped optimizer.\n\n        end_lr: the final learning rate.\n\n        num_iter: the number of iterations over which the test occurs.\n\n        last_epoch: the index of last epoch. Default: -1.\n    """"""\n    last_epoch: int\n    base_lrs: Sequence\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 end_lr: float,\n                 num_iter: int,\n                 last_epoch: int = -1):\n        self.end_lr = end_lr\n        self.num_iter = num_iter\n        super(_LinearLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        curr_iter = self.last_epoch + 1\n        r = curr_iter / self.num_iter\n\n        if self.last_epoch > 0:\n            val = [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n        else:\n            val = [base_lr for base_lr in self.base_lrs]\n        self._lr = val\n        return val\n\n    @property\n    def lr(self):\n        return self._lr\n\n\nclass _ExponentialLR(_LRScheduler):\n    """"""Exponentially increases the learning rate between two boundaries\n    over a number of iterations.\n\n    Arguments:\n\n        optimizer: wrapped optimizer.\n\n        end_lr: the final learning rate.\n\n        num_iter: the number of iterations over which the test occurs.\n\n        last_epoch: the index of last epoch. Default: -1.\n    """"""\n    last_epoch: int\n    base_lrs: Sequence\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 end_lr: float,\n                 num_iter: int,\n                 last_epoch: int = -1):\n        self.end_lr = end_lr\n        self.num_iter = num_iter\n        super(_ExponentialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        curr_iter = self.last_epoch + 1\n        r = curr_iter / self.num_iter\n\n        if self.last_epoch > 0:\n            val = [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n        else:\n            val = [base_lr for base_lr in self.base_lrs]\n        self._lr = val\n        return val\n\n    @property\n    def lr(self):\n        return self._lr\n\n\ndef _nested_hasattr(obj, path):\n    parts = path.split(""."")\n    for part in parts:\n        if hasattr(obj, part):\n            obj = getattr(obj, part)\n        else:\n            return False\n    else:\n        return True\n\n\ndef _nested_setattr(obj, path, val):\n    parts = path.split(""."")\n    for part in parts[:-1]:\n        if hasattr(obj, part):\n            obj = getattr(obj, part)\n    setattr(obj, parts[-1], val)\n'"
pytorch_lightning/trainer/model_hooks.py,0,"b'import inspect\nfrom abc import ABC, abstractmethod\n\nfrom pytorch_lightning.core.lightning import LightningModule\n\n\nclass TrainerModelHooksMixin(ABC):\n\n    def is_function_implemented(self, f_name):\n        model = self.get_model()\n        f_op = getattr(model, f_name, None)\n        return callable(f_op)\n\n    def is_overridden(self, method_name: str, model: LightningModule = None) -> bool:\n        if model is None:\n            model = self.get_model()\n        super_object = LightningModule\n\n        if not hasattr(model, method_name):\n            # in case of calling deprecated method\n            return False\n\n        instance_attr = getattr(model, method_name)\n        if not instance_attr:\n            return False\n        super_attr = getattr(super_object, method_name)\n\n        # when code pointers are different, it was implemented\n        if hasattr(instance_attr, \'patch_loader_code\'):\n            # cannot pickle __code__ so cannot verify if PatchDataloader\n            # exists which shows dataloader methods have been overwritten.\n            # so, we hack it by using the string representation\n            is_overridden = instance_attr.patch_loader_code != str(super_attr.__code__)\n        else:\n            is_overridden = instance_attr.__code__ is not super_attr.__code__\n        return is_overridden\n\n    def has_arg(self, f_name, arg_name):\n        model = self.get_model()\n        f_op = getattr(model, f_name, None)\n        return arg_name in inspect.signature(f_op).parameters\n\n    @abstractmethod\n    def get_model(self):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n'"
pytorch_lightning/trainer/optimizers.py,8,"b'from abc import ABC\nfrom typing import List, Tuple\n\nimport torch\nfrom torch import optim\nfrom torch.optim.optimizer import Optimizer\n\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.utilities import rank_zero_warn\n\n\nclass TrainerOptimizersMixin(ABC):\n\n    def init_optimizers(\n            self,\n            model: LightningModule\n    ) -> Tuple[List, List, List]:\n        optim_conf = model.configure_optimizers()\n\n        if optim_conf is None:\n            rank_zero_warn(\'`LightningModule.configure_optimizers` returned `None`, \'\n                           \'this fit will run with no optimizer\', UserWarning)\n            optim_conf = _MockOptimizer()\n\n        # single output, single optimizer\n        if isinstance(optim_conf, Optimizer):\n            return [optim_conf], [], []\n\n        # two lists, optimizer + lr schedulers\n        elif isinstance(optim_conf, (list, tuple)) and len(optim_conf) == 2 \\\n                and isinstance(optim_conf[0], list):\n            optimizers, lr_schedulers = optim_conf\n            lr_schedulers = self.configure_schedulers(lr_schedulers)\n            return optimizers, lr_schedulers, []\n\n        # single dictionary\n        elif isinstance(optim_conf, dict):\n            optimizer = optim_conf[""optimizer""]\n            lr_scheduler = optim_conf.get(""lr_scheduler"", [])\n            if lr_scheduler:\n                lr_schedulers = self.configure_schedulers([lr_scheduler])\n            else:\n                lr_schedulers = []\n            return [optimizer], lr_schedulers, []\n\n        # multiple dictionaries\n        elif isinstance(optim_conf, (list, tuple)) and isinstance(optim_conf[0], dict):\n            optimizers = [opt_dict[""optimizer""] for opt_dict in optim_conf]\n            # take only lr wif exists and ot they are defined - not None\n            lr_schedulers = [\n                opt_dict[""lr_scheduler""] for opt_dict in optim_conf if opt_dict.get(""lr_scheduler"")\n            ]\n            # take only freq wif exists and ot they are defined - not None\n            optimizer_frequencies = [\n                opt_dict[""frequency""] for opt_dict in optim_conf if opt_dict.get(""frequency"") is not None\n            ]\n\n            # clean scheduler list\n            if lr_schedulers:\n                lr_schedulers = self.configure_schedulers(lr_schedulers)\n            # assert that if frequencies are present, they are given for all optimizers\n            if optimizer_frequencies and len(optimizer_frequencies) != len(optimizers):\n                raise ValueError(""A frequency must be given to each optimizer."")\n            return optimizers, lr_schedulers, optimizer_frequencies\n\n        # single list or tuple, multiple optimizer\n        elif isinstance(optim_conf, (list, tuple)):\n            return list(optim_conf), [], []\n\n        # unknown configuration\n        else:\n            raise ValueError(\n                \'Unknown configuration for model optimizers.\'\n                \' Output from `model.configure_optimizers()` should either be:\'\n                \' * single output, single `torch.optim.Optimizer`\'\n                \' * single output, list of `torch.optim.Optimizer`\'\n                \' * single output, a dictionary with `optimizer` key (`torch.optim.Optimizer`)\'\n                \'    and an optional `lr_scheduler` key (`torch.optim.lr_scheduler`)\'\n                \' * two outputs, first being a list of `torch.optim.Optimizer` second being\'\n                \'    a list of `torch.optim.lr_scheduler`\'\n                \' * multiple outputs, dictionaries as described with an optional `frequency` key (int)\')\n\n    def configure_schedulers(self, schedulers: list):\n        # Convert each scheduler into dict structure with relevant information\n        lr_schedulers = []\n        default_config = {\'interval\': \'epoch\',  # default every epoch\n                          \'frequency\': 1,  # default every epoch/batch\n                          \'reduce_on_plateau\': False,  # most often not ReduceLROnPlateau scheduler\n                          \'monitor\': \'val_loss\'}  # default value to monitor for ReduceLROnPlateau\n        for scheduler in schedulers:\n            if isinstance(scheduler, dict):\n                if \'scheduler\' not in scheduler:\n                    raise ValueError(\'Lr scheduler should have key `scheduler`\',\n                                     \' with item being a lr scheduler\')\n                scheduler[\'reduce_on_plateau\'] = isinstance(\n                    scheduler[\'scheduler\'], optim.lr_scheduler.ReduceLROnPlateau)\n\n                lr_schedulers.append({**default_config, **scheduler})\n\n            elif isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n                lr_schedulers.append({**default_config, \'scheduler\': scheduler,\n                                      \'reduce_on_plateau\': True})\n\n            elif isinstance(scheduler, optim.lr_scheduler._LRScheduler):\n                lr_schedulers.append({**default_config, \'scheduler\': scheduler})\n            else:\n                raise ValueError(f\'Input {scheduler} to lr schedulers \'\n                                 \'is a invalid input.\')\n        return lr_schedulers\n\n    def reinit_scheduler_properties(self, optimizers: list, schedulers: list):\n        # Reinitialize optimizer.step properties added by schedulers\n        for scheduler in schedulers:\n            for optimizer in optimizers:\n                scheduler = scheduler[\'scheduler\']\n                # check that we dont mix users optimizers and schedulers\n                if scheduler.optimizer == optimizer:\n                    # Find the mro belonging to the base lr scheduler class\n                    for i, mro in enumerate(scheduler.__class__.__mro__):\n                        if mro == optim.lr_scheduler._LRScheduler:\n                            idx = i\n                    scheduler.__class__.__mro__[idx].__init__(scheduler, optimizer)\n\n\nclass _MockOptimizer(Optimizer):\n    """"""The `_MockOptimizer` will be used inplace of an optimizer in the event that `None`\n    is returned from `configure_optimizers`.\n    """"""\n\n    def __init__(self):\n        super().__init__([torch.zeros(1)], {})\n\n    def add_param_group(self, param_group):\n        pass  # Do Nothing\n\n    def load_state_dict(self, state_dict):\n        pass  # Do Nothing\n\n    def state_dict(self):\n        return {}  # Return Empty\n\n    def step(self, closure=None):\n        if closure is not None:\n            closure()\n\n    def zero_grad(self):\n        pass  # Do Nothing\n\n    def __repr__(self):\n        return \'No Optimizer\'\n'"
pytorch_lightning/trainer/seed.py,1,"b'""""""Helper functions to help with reproducibility of models. """"""\n\nimport os\nfrom typing import Optional\n\nimport numpy as np\nimport random\nimport torch\n\nfrom pytorch_lightning import _logger as log\n\n\ndef seed_everything(seed: Optional[int] = None) -> int:\n    """"""Function that sets seed for pseudo-random number generators  in:\n        pytorch, numpy, python.random and sets PYTHONHASHSEED environment variable.\n    """"""\n    max_seed_value = np.iinfo(np.uint32).max\n    min_seed_value = np.iinfo(np.uint32).min\n\n    try:\n        seed = int(seed)\n    except (TypeError, ValueError):\n        seed = _select_seed_randomly(min_seed_value, max_seed_value)\n\n    if (seed > max_seed_value) or (seed < min_seed_value):\n        log.warning(\n            f""{seed} is not in bounds, \\\n            numpy accepts from {min_seed_value} to {max_seed_value}""\n        )\n        seed = _select_seed_randomly(min_seed_value, max_seed_value)\n\n    os.environ[""PYTHONHASHSEED""] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    return seed\n\n\ndef _select_seed_randomly(min_seed_value: int = 0, max_seed_value: int = 255) -> int:\n    seed = random.randint(min_seed_value, max_seed_value)\n    log.warning(f""No correct seed found, seed set to {seed}"")\n    return seed\n'"
pytorch_lightning/trainer/supporters.py,5,"b'import torch\n\n\nclass TensorRunningAccum(object):\n    """"""Tracks a running accumulation values (min, max, mean) without graph\n    references.\n\n    Examples:\n        >>> accum = TensorRunningAccum(5)\n        >>> accum.last(), accum.mean()\n        (None, None)\n        >>> accum.append(torch.tensor(1.5))\n        >>> accum.last(), accum.mean()\n        (tensor(1.5000), tensor(1.5000))\n        >>> accum.append(torch.tensor(2.5))\n        >>> accum.last(), accum.mean()\n        (tensor(2.5000), tensor(2.))\n        >>> accum.reset()\n        >>> _= [accum.append(torch.tensor(i)) for i in range(13)]\n        >>> accum.last(), accum.mean(), accum.min(), accum.max()\n        (tensor(12.), tensor(10.), tensor(8.), tensor(12.))\n    """"""\n\n    def __init__(self, window_length: int):\n        self.window_length = window_length\n        self.memory = torch.Tensor(self.window_length)\n        self.current_idx: int = 0\n        self.last_idx: int = None\n        self.rotated: bool = False\n\n    def reset(self) -> None:\n        """"""Empty the accumulator.""""""\n        self = TensorRunningAccum(self.window_length)\n\n    def last(self):\n        """"""Get the last added element.""""""\n        if self.last_idx is not None:\n            return self.memory[self.last_idx]\n\n    def append(self, x):\n        """"""Add an element to the accumulator.""""""\n        # ensure same device and type\n        if self.memory.device != x.device or self.memory.type() != x.type():\n            x = x.to(self.memory)\n\n        # store without grads\n        with torch.no_grad():\n            self.memory[self.current_idx] = x\n            self.last_idx = self.current_idx\n\n        # increase index\n        self.current_idx += 1\n\n        # reset index when hit limit of tensor\n        self.current_idx = self.current_idx % self.window_length\n        if self.current_idx == 0:\n            self.rotated = True\n\n    def mean(self):\n        """"""Get mean value from stored elements.""""""\n        return self._agg_memory(\'mean\')\n\n    def max(self):\n        """"""Get maximal value from stored elements.""""""\n        return self._agg_memory(\'max\')\n\n    def min(self):\n        """"""Get minimal value from stored elements.""""""\n        return self._agg_memory(\'min\')\n\n    def _agg_memory(self, how: str):\n        if self.last_idx is not None:\n            if self.rotated:\n                return getattr(self.memory, how)()\n            else:\n                return getattr(self.memory[:self.current_idx], how)()\n'"
pytorch_lightning/trainer/trainer.py,12,"b'import inspect\nimport os\nfrom argparse import ArgumentParser, Namespace\nfrom typing import Union, Optional, List, Dict, Tuple, Iterable, Any\n\nimport torch\nimport torch.distributed as torch_distrib\nimport torch.multiprocessing as mp\nfrom torch.utils.data import DataLoader\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback, ProgressBarBase\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.loggers import LightningLoggerBase\nfrom pytorch_lightning.profiler import SimpleProfiler, PassThroughProfiler, BaseProfiler\nfrom pytorch_lightning.trainer.seed import seed_everything\nfrom pytorch_lightning.trainer.auto_mix_precision import TrainerAMPMixin\nfrom pytorch_lightning.trainer.callback_config import TrainerCallbackConfigMixin\nfrom pytorch_lightning.trainer.callback_hook import TrainerCallbackHookMixin\nfrom pytorch_lightning.trainer.data_loading import TrainerDataLoadingMixin\nfrom pytorch_lightning.trainer.deprecated_api import TrainerDeprecatedAPITillVer0_8, TrainerDeprecatedAPITillVer0_9\nfrom pytorch_lightning.trainer.distrib_data_parallel import TrainerDDPMixin\nfrom pytorch_lightning.trainer.distrib_parts import (\n    TrainerDPMixin, parse_gpu_ids, determine_root_gpu_device, pick_multiple_gpus)\nfrom pytorch_lightning.trainer.evaluation_loop import TrainerEvaluationLoopMixin\nfrom pytorch_lightning.trainer.logging import TrainerLoggingMixin\nfrom pytorch_lightning.trainer.model_hooks import TrainerModelHooksMixin\nfrom pytorch_lightning.trainer.optimizers import TrainerOptimizersMixin\nfrom pytorch_lightning.trainer.supporters import TensorRunningAccum\nfrom pytorch_lightning.trainer.training_io import TrainerIOMixin\nfrom pytorch_lightning.trainer.training_loop import TrainerTrainLoopMixin\nfrom pytorch_lightning.trainer.training_tricks import TrainerTrainingTricksMixin\nfrom pytorch_lightning.trainer.lr_finder import TrainerLRFinderMixin\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities import rank_zero_warn, parsing\n\ntry:\n    from apex import amp\nexcept ImportError:\n    APEX_AVAILABLE = False\nelse:\n    APEX_AVAILABLE = True\n\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.xla_multiprocessing as xmp\nexcept ImportError:\n    XLA_AVAILABLE = False\nelse:\n    XLA_AVAILABLE = True\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n\nclass Trainer(\n    TrainerIOMixin,\n    TrainerOptimizersMixin,\n    TrainerAMPMixin,\n    TrainerDPMixin,\n    TrainerDDPMixin,\n    TrainerLoggingMixin,\n    TrainerModelHooksMixin,\n    TrainerTrainingTricksMixin,\n    TrainerDataLoadingMixin,\n    TrainerEvaluationLoopMixin,\n    TrainerTrainLoopMixin,\n    TrainerCallbackConfigMixin,\n    TrainerCallbackHookMixin,\n    TrainerLRFinderMixin,\n    TrainerDeprecatedAPITillVer0_8,\n    TrainerDeprecatedAPITillVer0_9,\n):\n    DEPRECATED_IN_0_8 = (\n        \'gradient_clip\', \'nb_gpu_nodes\', \'max_nb_epochs\', \'min_nb_epochs\',\n        \'add_row_log_interval\', \'nb_sanity_val_steps\', \'tng_tqdm_dic\',\n    )\n    DEPRECATED_IN_0_9 = (\'use_amp\', \'show_progress_bar\', \'training_tqdm_dict\', \'num_tpu_cores\')\n\n    def __init__(\n            self,\n            logger: Union[LightningLoggerBase, Iterable[LightningLoggerBase], bool] = True,\n            checkpoint_callback: Union[ModelCheckpoint, bool] = True,\n            early_stop_callback: Optional[Union[EarlyStopping, bool]] = False,\n            callbacks: Optional[List[Callback]] = None,\n            default_root_dir: Optional[str] = None,\n            gradient_clip_val: float = 0,\n            process_position: int = 0,\n            num_nodes: int = 1,\n            num_processes: int = 1,\n            gpus: Optional[Union[List[int], str, int]] = None,\n            auto_select_gpus: bool = False,\n            tpu_cores: Optional[Union[List[int], int]] = None,\n            log_gpu_memory: Optional[str] = None,\n            progress_bar_refresh_rate: int = 1,\n            overfit_pct: float = 0.0,\n            track_grad_norm: Union[int, float, str] = -1,\n            check_val_every_n_epoch: int = 1,\n            fast_dev_run: bool = False,\n            accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1,\n            max_epochs: int = 1000,\n            min_epochs: int = 1,\n            max_steps: Optional[int] = None,\n            min_steps: Optional[int] = None,\n            train_percent_check: float = 1.0,\n            val_percent_check: float = 1.0,\n            test_percent_check: float = 1.0,\n            val_check_interval: float = 1.0,\n            log_save_interval: int = 100,\n            row_log_interval: int = 10,\n            add_row_log_interval=None,  # backward compatible, todo: remove in v0.8.0\n            distributed_backend: Optional[str] = None,\n            precision: int = 32,\n            print_nan_grads: bool = False,  # backward compatible, todo: remove in v0.9.0\n            weights_summary: Optional[str] = \'top\',\n            weights_save_path: Optional[str] = None,\n            num_sanity_val_steps: int = 2,\n            truncated_bptt_steps: Optional[int] = None,\n            resume_from_checkpoint: Optional[str] = None,\n            profiler: Optional[Union[BaseProfiler, bool]] = None,\n            benchmark: bool = False,\n            deterministic: bool = False,\n            reload_dataloaders_every_epoch: bool = False,\n            auto_lr_find: Union[bool, str] = False,\n            replace_sampler_ddp: bool = True,\n            terminate_on_nan: bool = False,\n            auto_scale_batch_size: Union[str, bool] = False,\n            num_tpu_cores: Optional[int] = None,  # backward compatible, todo: remove in v0.9.0\n            amp_level: str = \'O1\',  # backward compatible, todo: remove in v0.8.0\n            default_save_path=None,  # backward compatible, todo: remove in v0.8.0\n            gradient_clip=None,  # backward compatible, todo: remove in v0.8.0\n            nb_gpu_nodes=None,  # backward compatible, todo: remove in v0.8.0\n            max_nb_epochs=None,  # backward compatible, todo: remove in v0.8.0\n            min_nb_epochs=None,  # backward compatible, todo: remove in v0.8.0\n            use_amp=None,  # backward compatible, todo: remove in v0.9.0\n            show_progress_bar=None,  # backward compatible, todo: remove in v0.9.0\n            nb_sanity_val_steps=None,  # backward compatible, todo: remove in v0.8.0\n    ):\n        r""""""\n\n        Customize every aspect of training via flags\n\n        Args:\n            logger: Logger (or iterable collection of loggers) for experiment tracking.\n\n            checkpoint_callback: Callback for checkpointing.\n\n            early_stop_callback (:class:`pytorch_lightning.callbacks.EarlyStopping`):\n\n            callbacks: Add a list of callbacks.\n\n            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed\n\n            default_save_path:\n                .. warning:: .. deprecated:: 0.7.3\n\n                    Use `default_root_dir` instead. Will remove 0.9.0.\n\n            gradient_clip_val: 0 means don\'t clip.\n\n            gradient_clip:\n                .. warning:: .. deprecated:: 0.7.0\n\n                    Use `gradient_clip_val` instead. Will remove 0.9.0.\n\n            process_position: orders the progress bar when running multiple models on same machine.\n\n            num_nodes: number of GPU nodes for distributed training.\n\n            nb_gpu_nodes:\n                .. warning:: .. deprecated:: 0.7.0\n\n                    Use `num_nodes` instead. Will remove 0.9.0.\n\n            gpus: Which GPUs to train on.\n\n            auto_select_gpus:\n\n                If enabled and `gpus` is an integer, pick available\n                gpus automatically. This is especially useful when\n                GPUs are configured to be in ""exclusive mode"", such\n                that only one process at a time can access them.\n\n            tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on [1]\n\n            num_tpu_cores: How many TPU cores to train on (1 or 8)\n                .. warning:: .. deprecated:: 0.7.6. Will remove 0.9.0.\n\n            log_gpu_memory: None, \'min_max\', \'all\'. Might slow performance\n\n            show_progress_bar:\n                .. warning:: .. deprecated:: 0.7.2\n\n                        Set `progress_bar_refresh_rate` to positive integer to enable. Will remove 0.9.0.\n\n            progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n                Ignored when a custom callback is passed to :paramref:`~Trainer.callbacks`.\n\n            overfit_pct: How much of training-, validation-, and test dataset to check.\n\n            track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to \'inf\' infinity-norm.\n\n            check_val_every_n_epoch: Check val every n train epochs.\n\n            fast_dev_run: runs 1 batch of train, test  and val to find any bugs (ie: a sort of unit test).\n\n            accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n\n            max_epochs: Stop training once this number of epochs is reached.\n\n            max_nb_epochs:\n                .. warning:: .. deprecated:: 0.7.0\n\n                    Use `max_epochs` instead. Will remove 0.9.0.\n\n            min_epochs: Force training for at least these many epochs\n\n            min_nb_epochs:\n                .. warning:: .. deprecated:: 0.7.0\n\n                    Use `min_epochs` instead. Will remove 0.9.0.\n\n            max_steps: Stop training after this number of steps. Disabled by default (None).\n\n            min_steps: Force training for at least these number of steps. Disabled by default (None).\n\n            train_percent_check: How much of training dataset to check.\n\n            val_percent_check: How much of validation dataset to check.\n\n            test_percent_check: How much of test dataset to check.\n\n            val_check_interval: How often within one training epoch to check the validation set\n\n            log_save_interval: Writes logs to disk this often\n\n            row_log_interval: How often to add logging rows (does not write to disk)\n\n            add_row_log_interval:\n                .. warning:: .. deprecated:: 0.7.0\n\n                    Use `row_log_interval` instead. Will remove 0.9.0.\n\n            distributed_backend: The distributed backend to use (dp, ddp, ddp2, ddp_spawn)\n\n            use_amp:\n                .. warning:: .. deprecated:: 0.7.0\n\n                    Use `precision` instead. Will remove 0.9.0.\n\n            precision: Full precision (32), half precision (16).\n\n            print_nan_grads:\n                .. warning:: .. deprecated:: 0.7.2\n\n                    Has no effect. When detected, NaN grads will be printed automatically.\n                    Will remove 0.9.0.\n\n            weights_summary: Prints a summary of the weights when training begins.\n\n            weights_save_path: Where to save weights if specified. Will override default_root_dir\n                    for checkpoints only. Use this if for whatever reason you need the checkpoints\n                    stored in a different place than the logs written in `default_root_dir`.\n\n            amp_level: The optimization level to use (O1, O2, etc...).\n\n            num_sanity_val_steps: Sanity check runs n batches of val before starting the training routine.\n\n            nb_sanity_val_steps:\n                .. warning:: .. deprecated:: 0.7.0\n\n                    Use `num_sanity_val_steps` instead. Will remove 0.8.0.\n\n            truncated_bptt_steps: Truncated back prop breaks performs backprop every k steps of\n\n            resume_from_checkpoint: To resume training from a specific checkpoint pass in the path here.\n\n            profiler:  To profile individual steps during training and assist in\n\n            reload_dataloaders_every_epoch: Set to True to reload dataloaders every epoch\n\n            auto_lr_find: If set to True, will `initially` run a learning rate finder,\n                trying to optimize initial learning for faster convergence. Sets learning\n                rate in self.lr or self.learning_rate in the LightningModule.\n                To use a different key, set a string instead of True with the key name.\n\n            replace_sampler_ddp: Explicitly enables or disables sampler replacement.\n                If not specified this will toggled automatically ddp is used\n\n            benchmark: If true enables cudnn.benchmark.\n\n            deterministic: If true enables cudnn.deterministic\n\n            terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n                end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n\n            auto_scale_batch_size: If set to True, will `initially` run a batch size\n                finder trying to find the largest batch size that fits into memory.\n                The result will be stored in self.batch_size in the LightningModule.\n                Additionally, can be set to either `power` that estimates the batch size through\n                a power search or `binsearch` that estimates the batch size through a binary search.\n        """"""\n        super().__init__()\n\n        self.deterministic = deterministic\n        torch.backends.cudnn.deterministic = self.deterministic\n        if self.deterministic:\n            # fixing non-deterministic part of horovod\n            # https://github.com/PyTorchLightning/pytorch-lightning/pull/1572/files#r420279383\n            os.environ[""HOROVOD_FUSION_THRESHOLD""] = str(0)\n\n        # Init callbacks\n        self.callbacks = callbacks or []\n        self.on_init_start()\n\n        # benchmarking\n        self.benchmark = benchmark\n        torch.backends.cudnn.benchmark = self.benchmark\n\n        # Transfer params\n        self.num_nodes = num_nodes\n        # Backward compatibility, TODO: remove in v0.8.0\n        if nb_gpu_nodes is not None:\n            rank_zero_warn(""Argument `nb_gpu_nodes` has renamed to `num_nodes` since v0.5.0""\n                           "" and this method will be removed in v0.8.0"", DeprecationWarning)\n            self.num_gpu_nodes = nb_gpu_nodes\n        self.log_gpu_memory = log_gpu_memory\n\n        self.gradient_clip_val = gradient_clip_val\n        # Backward compatibility, TODO: remove in v0.8.0\n        if gradient_clip is not None:\n            rank_zero_warn(""Argument `gradient_clip` has renamed to `gradient_clip_val` since v0.5.0""\n                           "" and this method will be removed in v0.8.0"", DeprecationWarning)\n            self.gradient_clip = gradient_clip\n\n        self.check_val_every_n_epoch = check_val_every_n_epoch\n\n        if not isinstance(track_grad_norm, (int, float)) and track_grad_norm != \'inf\':\n            raise MisconfigurationException(\n                ""track_grad_norm can be an int, a float or \'inf\' (infinity norm)."")\n        self.track_grad_norm = float(track_grad_norm)\n\n        self.on_gpu = True if (gpus and torch.cuda.is_available()) else False\n\n        # tpu config\n        if num_tpu_cores is not None:\n            rank_zero_warn(""Argument `num_tpu_cores` is now set by `tpu_cores` since v0.7.6""\n                           "" and this argument will be removed in v0.9.0"", DeprecationWarning)\n\n        if tpu_cores is None:\n            tpu_cores = num_tpu_cores\n        self.on_tpu = tpu_cores is not None\n        self.tpu_cores = tpu_cores\n        assert self.tpu_cores in (1, 8, None) or (\n            isinstance(self.tpu_cores, (list, tuple, set)) and len(self.tpu_cores) == 1\n        ), \'`tpu_cores` can only be 1, 8 or [<1-8>]\'\n\n        self.tpu_id = tpu_cores[0] if isinstance(tpu_cores, list) else None\n\n        if num_processes != 1 and distributed_backend != ""ddp_cpu"":\n            rank_zero_warn(""num_processes is only used for distributed_backend=\\""ddp_cpu\\"". Ignoring it."")\n        self.num_processes = num_processes\n\n        self.weights_summary = weights_summary\n\n        self.max_epochs = max_epochs\n        # Backward compatibility, TODO: remove in v0.8.0\n        if max_nb_epochs is not None:\n            rank_zero_warn(""Argument `max_nb_epochs` has renamed to `max_epochs` since v0.5.0""\n                           "" and this method will be removed in v0.8.0"", DeprecationWarning)\n            self.max_nb_epochs = max_nb_epochs\n\n        self.min_epochs = min_epochs\n        # Backward compatibility, TODO: remove in v0.8.0\n        if min_nb_epochs is not None:\n            rank_zero_warn(""Argument `min_nb_epochs` has renamed to `min_epochs` since v0.5.0""\n                           "" and this method will be removed in v0.8.0"", DeprecationWarning)\n            self.min_nb_epochs = min_nb_epochs\n\n        self.max_steps = max_steps\n        self.min_steps = min_steps\n\n        self.num_sanity_val_steps = num_sanity_val_steps\n        # Backward compatibility, TODO: remove in v0.8.0\n        if nb_sanity_val_steps is not None:\n            rank_zero_warn(""Argument `nb_sanity_val_steps` has renamed to ""\n                           ""`num_sanity_val_steps` since v0.5.0""\n                           "" and this method will be removed in v0.8.0"", DeprecationWarning)\n            self.nb_sanity_val_steps = nb_sanity_val_steps\n\n        # Backward compatibility, TODO: remove in v0.9.0\n        if print_nan_grads:\n            rank_zero_warn(""Argument `print_nan_grads` has no effect and will be removed in v0.9.0.""\n                           "" NaN grads will be printed automatically when detected."", DeprecationWarning)\n\n        self.reload_dataloaders_every_epoch = reload_dataloaders_every_epoch\n\n        self.auto_lr_find = auto_lr_find\n        self.auto_scale_batch_size = auto_scale_batch_size\n        self._is_data_prepared = False\n        self.replace_sampler_ddp = replace_sampler_ddp\n\n        self.truncated_bptt_steps = truncated_bptt_steps\n        self.resume_from_checkpoint = resume_from_checkpoint\n        self.terminate_on_nan = terminate_on_nan\n        self.shown_warnings = set()\n\n        self.fast_dev_run = fast_dev_run\n        if self.fast_dev_run:\n            self.num_sanity_val_steps = 0\n            self.max_epochs = 1\n            log.info(\'Running in fast_dev_run mode: will run a full train,\'\n                     \' val and test loop using a single batch\')\n\n        # set default save path if user didn\'t provide one\n        self.default_root_dir = default_root_dir\n\n        # Backward compatibility, TODO: remove in v0.8.0\n        if default_save_path is not None:\n            self.default_root_dir = default_save_path\n\n        if self.default_root_dir is None:\n            self.default_root_dir = os.getcwd()\n\n        # training bookeeping\n        self.total_batch_idx = 0\n        self.running_loss = TensorRunningAccum(window_length=20)\n        self.batch_idx = 0\n        self.progress_bar_metrics = {}\n        self.callback_metrics = {}\n        self.num_val_batches = 0\n        self.num_training_batches = 0\n        self.num_test_batches = 0\n        self.train_dataloader = None\n        self.test_dataloaders = None\n        self.val_dataloaders = None\n\n        # training state\n        self.model = None\n        self.testing = False\n        self.disable_validation = False\n        self.lr_schedulers = []\n        self.optimizers = None\n        self.optimizer_frequencies = []\n        self.global_step = 0\n        self.current_epoch = 0\n        self.interrupted = False\n\n        # configure logger\n        self.configure_logger(logger)\n\n        # configure profiler\n        if profiler is True:\n            profiler = SimpleProfiler()\n        self.profiler = profiler or PassThroughProfiler()\n\n        # configure early stop callback\n        # creates a default one if none passed in\n        self.configure_early_stopping(early_stop_callback)\n\n        # configure checkpoint callback\n        self.checkpoint_callback = checkpoint_callback\n        self.weights_save_path = weights_save_path\n\n        # accumulated grads\n        self.accumulate_grad_batches = accumulate_grad_batches\n        self.configure_accumulated_gradients(accumulate_grad_batches)\n\n        # for gpus allow int, string and gpu list\n        if auto_select_gpus and isinstance(gpus, int):\n            self.gpus = pick_multiple_gpus(gpus)\n        else:\n            self.gpus = gpus\n\n        self.data_parallel_device_ids = parse_gpu_ids(self.gpus)\n        self.root_gpu = determine_root_gpu_device(self.data_parallel_device_ids)\n        self.root_device = torch.device(""cpu"")\n\n        # tpu state flags\n        self.use_tpu = False\n        self.tpu_local_core_rank = None\n        self.tpu_global_core_rank = None\n\n        # distributed backend choice\n        self.distributed_backend = distributed_backend\n        self.set_distributed_mode(distributed_backend)\n\n        # override dist backend when using tpus\n        if self.on_tpu:\n            self.init_tpu()\n\n        # init flags for SLURM+ddp to work\n        self.proc_rank = 0\n        self.world_size = 1\n        self.interactive_ddp_procs = []\n        self.configure_slurm_ddp(self.num_nodes)\n        self.node_rank = self.determine_ddp_node_rank()\n\n        # nvidia setup\n        self.set_nvidia_flags(self.is_slurm_managing_tasks, self.data_parallel_device_ids)\n\n        # backward compatibility\n        if show_progress_bar is not None:\n            self.show_progress_bar = show_progress_bar\n\n        self._progress_bar_callback = self.configure_progress_bar(progress_bar_refresh_rate, process_position)\n\n        # logging\n        self.log_save_interval = log_save_interval\n        self.val_check_interval = val_check_interval\n\n        # backward compatibility\n        if add_row_log_interval is not None:\n            rank_zero_warn(""`add_row_log_interval` has renamed to `row_log_interval` since v0.5.0""\n                           "" and this method will be removed in v0.8.0"", DeprecationWarning)\n            if not row_log_interval:  # in case you did not set the proper value\n                row_log_interval = add_row_log_interval\n        self.row_log_interval = row_log_interval\n\n        # how much of the data to use\n        self.overfit_pct = overfit_pct\n        self.determine_data_use_amount(train_percent_check, val_percent_check,\n                                       test_percent_check, overfit_pct)\n\n        # AMP init\n        # These are the only lines needed after v0.8.0\n        # we wrap the user\'s forward with autocast and give it back at the end of fit\n        self.autocast_original_forward = None\n        self.use_native_amp = hasattr(torch.cuda, ""amp"") and hasattr(torch.cuda.amp, ""autocast"")\n        self.precision = precision\n        self.scaler = None\n\n        # TODO: remove for v0.8.0\n        self.amp_level = amp_level\n        self.init_amp(use_amp)\n\n        self.on_colab_kaggle = os.getenv(\'COLAB_GPU\') or os.getenv(\'KAGGLE_URL_BASE\')\n\n        # Callback system\n        self.on_init_end()\n\n    @property\n    def slurm_job_id(self) -> int:\n        try:\n            job_id = os.environ[\'SLURM_JOB_ID\']\n            job_id = int(job_id)\n\n            # in interactive mode, don\'t make logs use the same job id\n            in_slurm_interactive_mode = os.environ[\'SLURM_JOB_NAME\'] == \'bash\'\n            if in_slurm_interactive_mode:\n                job_id = None\n\n        except Exception:\n            job_id = None\n        return job_id\n\n    @classmethod\n    def default_attributes(cls):\n        init_signature = inspect.signature(Trainer)\n\n        args = {}\n        for param_name in init_signature.parameters:\n            value = init_signature.parameters[param_name].default\n            args[param_name] = value\n\n        return args\n\n    @classmethod\n    def get_init_arguments_and_types(cls) -> List[Tuple[str, Tuple, Any]]:\n        r""""""Scans the Trainer signature and returns argument names, types and default values.\n\n        Returns:\n            List with tuples of 3 values:\n            (argument name, set with argument types, argument default value).\n\n        Examples:\n            >>> args = Trainer.get_init_arguments_and_types()\n            >>> import pprint\n            >>> pprint.pprint(sorted(args))  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n            [(\'accumulate_grad_batches\',\n              (<class \'int\'>, typing.Dict[int, int], typing.List[list]),\n              1),\n             ...\n             (\'callbacks\',\n              (typing.List[pytorch_lightning.callbacks.base.Callback],\n               <class \'NoneType\'>),\n               None),\n             (\'check_val_every_n_epoch\', (<class \'int\'>,), 1),\n             ...\n             (\'max_epochs\', (<class \'int\'>,), 1000),\n             ...\n             (\'precision\', (<class \'int\'>,), 32),\n             (\'print_nan_grads\', (<class \'bool\'>,), False),\n             (\'process_position\', (<class \'int\'>,), 0),\n             (\'profiler\',\n              (<class \'pytorch_lightning.profiler.profilers.BaseProfiler\'>,\n               <class \'bool\'>,\n               <class \'NoneType\'>),\n              None),\n             ...\n        """"""\n        trainer_default_params = inspect.signature(cls).parameters\n        name_type_default = []\n        for arg in trainer_default_params:\n            arg_type = trainer_default_params[arg].annotation\n            arg_default = trainer_default_params[arg].default\n            try:\n                arg_types = tuple(arg_type.__args__)\n            except AttributeError:\n                arg_types = (arg_type,)\n\n            name_type_default.append((arg, arg_types, arg_default))\n\n        return name_type_default\n\n    @classmethod\n    def get_deprecated_arg_names(cls) -> List:\n        """"""Returns a list with deprecated Trainer arguments.""""""\n        depr_arg_names = []\n        for name, val in cls.__dict__.items():\n            if name.startswith(\'DEPRECATED\') and isinstance(val, (tuple, list)):\n                depr_arg_names.extend(val)\n        return depr_arg_names\n\n    @classmethod\n    def add_argparse_args(cls, parent_parser: ArgumentParser) -> ArgumentParser:\n        r""""""Extends existing argparse by default `Trainer` attributes.\n\n        Args:\n            parent_parser:\n                The custom cli arguments parser, which will be extended by\n                the Trainer default arguments.\n\n        Only arguments of the allowed types (str, float, int, bool) will\n        extend the `parent_parser`.\n\n        Examples:\n            >>> import argparse\n            >>> import pprint\n            >>> parser = argparse.ArgumentParser()\n            >>> parser = Trainer.add_argparse_args(parser)\n            >>> args = parser.parse_args([])\n            >>> pprint.pprint(vars(args))  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n            {...\n             \'check_val_every_n_epoch\': 1,\n             \'checkpoint_callback\': True,\n             \'default_root_dir\': None,\n             \'deterministic\': False,\n             \'distributed_backend\': None,\n             \'early_stop_callback\': False,\n             ...\n             \'logger\': True,\n             \'max_epochs\': 1000,\n             \'max_steps\': None,\n             \'min_epochs\': 1,\n             \'min_steps\': None,\n             ...\n             \'profiler\': None,\n             \'progress_bar_refresh_rate\': 1,\n             ...}\n\n        """"""\n        parser = ArgumentParser(parents=[parent_parser], add_help=False, )\n\n        blacklist = [\'kwargs\']\n        depr_arg_names = cls.get_deprecated_arg_names() + blacklist\n\n        allowed_types = (str, float, int, bool)\n\n        # TODO: get ""help"" from docstring :)\n        for arg, arg_types, arg_default in (at for at in cls.get_init_arguments_and_types()\n                                            if at[0] not in depr_arg_names):\n            arg_types = [at for at in allowed_types if at in arg_types]\n            if not arg_types:\n                # skip argument with not supported type\n                continue\n            arg_kwargs = {}\n            if bool in arg_types:\n                arg_kwargs.update(nargs=""?"")\n                # if the only arg type is bool\n                if len(arg_types) == 1:\n                    # redefine the type for ArgParser needed\n                    def use_type(x):\n                        return bool(parsing.str_to_bool(x))\n                else:\n                    # filter out the bool as we need to use more general\n                    use_type = [at for at in arg_types if at is not bool][0]\n            else:\n                use_type = arg_types[0]\n\n            if arg == \'gpus\':\n                use_type = Trainer._allowed_type\n                arg_default = Trainer._arg_default\n\n            parser.add_argument(\n                f\'--{arg}\',\n                dest=arg,\n                default=arg_default,\n                type=use_type,\n                help=\'autogenerated by pl.Trainer\',\n                **arg_kwargs,\n            )\n\n        return parser\n\n    def _allowed_type(x) -> Union[int, str]:\n        if \',\' in x:\n            return str(x)\n        else:\n            return int(x)\n\n    def _arg_default(x) -> Union[int, str]:\n        if \',\' in x:\n            return str(x)\n        else:\n            return int(x)\n\n    @staticmethod\n    def parse_argparser(arg_parser: Union[ArgumentParser, Namespace]) -> Namespace:\n        """"""Parse CLI arguments, required for custom bool types.""""""\n        args = arg_parser.parse_args() if isinstance(arg_parser, ArgumentParser) else arg_parser\n        args = {k: True if v is None else v for k, v in vars(args).items()}\n        return Namespace(**args)\n\n    @classmethod\n    def from_argparse_args(cls, args: Union[Namespace, ArgumentParser], **kwargs) -> \'Trainer\':\n        """"""\n        Create an instance from CLI arguments.\n\n        Args:\n            args: The parser or namespace to take arguments from. Only known arguments will be\n                parsed and passed to the :class:`Trainer`.\n            **kwargs: Additional keyword arguments that may override ones in the parser or namespace.\n                These must be valid Trainer arguments.\n\n        Example:\n            >>> parser = ArgumentParser(add_help=False)\n            >>> parser = Trainer.add_argparse_args(parser)\n            >>> parser.add_argument(\'--my_custom_arg\', default=\'something\')  # doctest: +SKIP\n            >>> args = Trainer.parse_argparser(parser.parse_args(""""))\n            >>> trainer = Trainer.from_argparse_args(args, logger=False)\n        """"""\n        if isinstance(args, ArgumentParser):\n            args = cls.parse_argparser(args)\n        params = vars(args)\n\n        # we only want to pass in valid Trainer args, the rest may be user specific\n        valid_kwargs = inspect.signature(cls.__init__).parameters\n        trainer_kwargs = dict((name, params[name]) for name in valid_kwargs if name in params)\n        trainer_kwargs.update(**kwargs)\n\n        return cls(**trainer_kwargs)\n\n    @property\n    def num_gpus(self) -> int:\n        gpus = self.data_parallel_device_ids\n        if gpus is None:\n            return 0\n        return len(gpus)\n\n    @property\n    def data_parallel(self) -> bool:\n        return self.use_dp or self.use_ddp or self.use_ddp2\n\n    @property\n    def progress_bar_callback(self):\n        return self._progress_bar_callback\n\n    @property\n    def progress_bar_dict(self) -> dict:\n        """""" Read-only for progress bar metrics. """"""\n        ref_model = self.model if not self.data_parallel else self.model.module\n        return dict(**ref_model.get_progress_bar_dict(), **self.progress_bar_metrics)\n\n    # -----------------------------\n    # MODEL TRAINING\n    # -----------------------------\n    def fit(\n            self,\n            model: LightningModule,\n            train_dataloader: Optional[DataLoader] = None,\n            val_dataloaders: Optional[Union[DataLoader, List[DataLoader]]] = None\n    ):\n        r""""""\n        Runs the full optimization routine.\n\n        Args:\n            model: Model to fit.\n\n            train_dataloader: A Pytorch\n                DataLoader with training samples. If the model has\n                a predefined train_dataloader method this will be skipped.\n\n            val_dataloaders: Either a single\n                Pytorch Dataloader or a list of them, specifying validation samples.\n                If the model has a predefined val_dataloaders method this will be skipped\n\n        Example::\n\n            # Option 1,\n            # Define the train_dataloader() and val_dataloader() fxs\n            # in the lightningModule\n            # RECOMMENDED FOR MOST RESEARCH AND APPLICATIONS TO MAINTAIN READABILITY\n            trainer = Trainer()\n            model = LightningModule()\n            trainer.fit(model)\n\n            # Option 2\n            # in production cases we might want to pass different datasets to the same model\n            # Recommended for PRODUCTION SYSTEMS\n            train, val = DataLoader(...), DataLoader(...)\n            trainer = Trainer()\n            model = LightningModule()\n            trainer.fit(model, train_dataloader=train, val_dataloaders=val)\n\n            # Option 1 & 2 can be mixed, for example the training set can be\n            # defined as part of the model, and validation can then be feed to .fit()\n\n        """"""\n        # bind logger and other properties\n        model.logger = self.logger\n        self.copy_trainer_model_properties(model)\n\n        # clean hparams\n        if hasattr(model, \'hparams\'):\n            parsing.clean_namespace(model.hparams)\n\n        # set up the passed in dataloaders (if needed)\n        self.__attach_dataloaders(model, train_dataloader, val_dataloaders)\n\n        # check that model is configured correctly\n        self.check_model_configuration(model)\n\n        # download the data and do whatever transforms we need\n        # do before any spawn calls so that the model can assign properties\n        # only on proc 0 because no spawn has happened yet\n        if not self._is_data_prepared:\n            model.prepare_data()\n            self._is_data_prepared = True\n\n        # Run auto batch size scaling\n        if self.auto_scale_batch_size:\n            if isinstance(self.auto_scale_batch_size, bool):\n                self.auto_scale_batch_size = \'power\'\n            self.scale_batch_size(model, mode=self.auto_scale_batch_size)\n            model.logger = self.logger  # reset logger binding\n\n        # Run learning rate finder:\n        if self.auto_lr_find:\n            self._run_lr_finder_internally(model)\n            model.logger = self.logger  # reset logger binding\n\n        # route to appropriate start method\n        # when using multi-node or DDP within a node start each module in a separate process\n        if self.use_ddp2:\n            if self.is_slurm_managing_tasks:\n                task = int(os.environ[\'SLURM_LOCALID\'])\n\n            # torchelastic or general non_slurm ddp2\n            elif \'WORLD_SIZE\' in os.environ and (\'GROUP_RANK\' in os.environ or \'NODE_RANK\' in os.environ):\n                task = int(os.environ[\'LOCAL_RANK\'])\n            self.ddp_train(task, model)\n        elif self.use_ddp:\n            if self.is_slurm_managing_tasks:\n                task = int(os.environ[\'SLURM_LOCALID\'])\n                self.ddp_train(task, model)\n\n            # torchelastic or general non_slurm ddp\n            elif \'WORLD_SIZE\' in os.environ and (\'GROUP_RANK\' in os.environ or \'NODE_RANK\' in os.environ):\n                task = int(os.environ[\'LOCAL_RANK\'])\n                self.ddp_train(task, model)\n\n            elif self.distributed_backend == \'cpu_ddp\':\n                self.__set_random_port()\n                self.model = model\n                mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\n\n            elif self.distributed_backend == \'ddp_spawn\':\n                model.share_memory()\n\n                # spin up peers\n                mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model, ))\n\n            elif self.distributed_backend == \'ddp\':\n                self.spawn_ddp_children(model)\n\n        # 1 gpu or dp option triggers training using DP module\n        # easier to avoid NCCL issues\n        elif self.use_dp:\n            self.dp_train(model)\n\n        elif self.use_horovod:\n            self.horovod_train(model)\n\n        elif self.single_gpu:\n            self.single_gpu_train(model)\n\n        elif self.use_tpu:  # pragma: no-cover\n            log.info(f\'training on {self.tpu_cores} TPU cores\')\n\n            #  COLAB_GPU is an env var available by default in Colab environments.\n            start_method = \'fork\' if self.on_colab_kaggle else \'spawn\'\n\n            # track for predict\n            self.model = model\n\n            # train\n            if self.tpu_id is not None:\n                self.tpu_train(self.tpu_id, model)\n            else:\n                xmp.spawn(self.tpu_train, args=(model,), nprocs=self.tpu_cores, start_method=start_method)\n\n            # load weights if not interrupted\n            self.load_spawn_weights(model)\n            self.model = model\n\n        # ON CPU\n        else:\n            # run through amp wrapper\n            if self.use_amp:\n                raise MisconfigurationException(\'amp + cpu is not supported.  Please use a GPU option\')\n\n            # CHOOSE OPTIMIZER\n            # allow for lr schedulers as well\n            self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n\n            self.run_pretrain_routine(model)\n\n        # return 1 when finished\n        # used for testing or when we need to know that training succeeded\n        return 1\n\n    def __attach_dataloaders(self, model, train_dataloader=None, val_dataloaders=None, test_dataloaders=None):\n        # when dataloader is passed via fit, patch the train_dataloader\n        # functions to overwrite with these implementations\n        if train_dataloader is not None:\n            model.train_dataloader = _PatchDataLoader(train_dataloader)\n\n        if val_dataloaders is not None:\n            model.val_dataloader = _PatchDataLoader(val_dataloaders)\n\n        if test_dataloaders is not None:\n            model.test_dataloader = _PatchDataLoader(test_dataloaders)\n\n    def run_pretrain_routine(self, model: LightningModule):\n        """"""Sanity check a few things before starting actual training.\n\n        Args:\n            model: The model to run sanity test on.\n        """"""\n        ref_model = model\n        if self.data_parallel:\n            ref_model = model.module\n\n        # give model convenience properties\n        ref_model.trainer = self\n\n        # set local properties on the model\n        self.copy_trainer_model_properties(ref_model)\n\n        # init amp. Must be done here instead of __init__ to allow ddp to work\n        if self.use_native_amp and self.precision == 16:\n            self.scaler = torch.cuda.amp.GradScaler()\n\n        # log hyper-parameters\n        if self.logger is not None:\n            # save exp to get started\n            self.logger.log_hyperparams(ref_model.hparams)\n\n            self.logger.save()\n\n        if self.use_ddp or self.use_ddp2:\n            torch_distrib.barrier()\n\n        # wait for all models to restore weights\n        if self.on_tpu and XLA_AVAILABLE:\n            # wait for all processes to catch up\n            torch_xla.core.xla_model.rendezvous(""pl.Trainer.run_pretrain_routine"")\n\n        elif self.use_horovod:\n            # wait for all processes to catch up\n            hvd.join()\n\n        # register auto-resubmit when on SLURM\n        self.register_slurm_signal_handlers()\n\n        # print model summary\n        # TODO: remove self.testing condition because model.summarize() is wiping out the weights\n        if self.proc_rank == 0 and self.weights_summary is not None and not self.testing:\n            if self.weights_summary in [\'full\', \'top\']:\n                ref_model.summarize(mode=self.weights_summary)\n            else:\n                raise MisconfigurationException(""weights_summary can be None, \'full\' or \'top\'"")\n\n        # track model now.\n        # if cluster resets state, the model will update with the saved weights\n        self.model = model\n\n        # set up checkpoint callback\n        self.configure_checkpoint_callback()\n\n        # restore training and model before hpc call\n        self.restore_weights(model)\n\n        # when testing requested only run test and return\n        if self.testing:\n            # only load test dataloader for testing\n            # self.reset_test_dataloader(ref_model)\n            self.run_evaluation(test_mode=True)\n            return\n\n        # check if we should run validation during training\n        self.disable_validation = not (self.is_overridden(\'validation_step\') and self.val_percent_check > 0) \\\n            and not self.fast_dev_run\n\n        # run tiny validation (if validation defined)\n        # to make sure program won\'t crash during val\n        if not self.disable_validation and self.num_sanity_val_steps > 0:\n            self.reset_val_dataloader(ref_model)\n\n            # hook and callback\n            ref_model.on_sanity_check_start()\n            self.on_sanity_check_start()\n\n            eval_results = self._evaluate(model,\n                                          self.val_dataloaders,\n                                          self.num_sanity_val_steps,\n                                          False)\n            _, _, _, callback_metrics, _ = self.process_output(eval_results)\n\n            self.on_sanity_check_end()\n\n            # verify that early stop has conditioned on a metric that exists\n            if self.enable_early_stop:\n                self.early_stop_callback._validate_condition_metric(callback_metrics)\n\n        # clear cache before training\n        if self.on_gpu and self.root_gpu is not None:\n            # use context because of:\n            # https://discuss.pytorch.org/t/out-of-memory-when-i-use-torch-cuda-empty-cache/57898\n            with torch.cuda.device(f\'cuda:{self.root_gpu}\'):\n                torch.cuda.empty_cache()\n\n        # CORE TRAINING LOOP\n        self.train()\n\n    def test(\n            self,\n            model: Optional[LightningModule] = None,\n            test_dataloaders: Optional[Union[DataLoader, List[DataLoader]]] = None\n    ):\n        r""""""\n\n        Separates from fit to make sure you never run on your test set until you want to.\n\n        Args:\n            model: The model to test.\n\n            test_dataloaders: Either a single\n                Pytorch Dataloader or a list of them, specifying validation samples.\n\n        Example::\n\n            # Option 1\n            # run test after fitting\n            test = DataLoader(...)\n            trainer = Trainer()\n            model = LightningModule()\n\n            trainer.fit(model)\n            trainer.test(test_dataloaders=test)\n\n            # Option 2\n            # run test from a loaded model\n            test = DataLoader(...)\n            model = LightningModule.load_from_checkpoint(\'path/to/checkpoint.ckpt\')\n            trainer = Trainer()\n            trainer.test(model, test_dataloaders=test)\n        """"""\n\n        self.testing = True\n\n        if test_dataloaders is not None:\n            if model:\n                self.__attach_dataloaders(model, test_dataloaders=test_dataloaders)\n            else:\n                self.__attach_dataloaders(self.model, test_dataloaders=test_dataloaders)\n\n        if model is not None:\n            self.model = model\n            self.fit(model)\n\n        # on tpu, .spawn means we don\'t have a trained model\n        # TODO: remove TPU spawn\n        elif self.use_tpu:  # pragma: no-cover\n            # attempt to load weights from a spawn\n            path = os.path.join(self.default_root_dir, \'__temp_weight_ddp_end.ckpt\')\n            test_model = self.model\n            if os.path.exists(path):\n                test_model = self.load_spawn_weights(self.model)\n\n            self.fit(test_model)\n        else:\n            self.run_evaluation(test_mode=True)\n\n        self.testing = False\n\n    def check_model_configuration(self, model: LightningModule):\n        r""""""\n        Checks that the model is configured correctly before training or testing is started.\n\n        Args:\n            model: The model to check the configuration.\n\n        """"""\n        # Check training_step, train_dataloader, configure_optimizer methods\n        if not self.testing:\n            if not self.is_overridden(\'training_step\', model):\n                raise MisconfigurationException(\n                    \'No `training_step()` method defined. Lightning `Trainer` expects as minimum a\'\n                    \' `training_step()`, `training_dataloader()` and `configure_optimizers()` to be defined.\')\n\n            if not self.is_overridden(\'train_dataloader\', model):\n                raise MisconfigurationException(\n                    \'No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a\'\n                    \' `training_step()`, `training_dataloader()` and `configure_optimizers()` to be defined.\')\n\n            if not self.is_overridden(\'configure_optimizers\', model):\n                raise MisconfigurationException(\n                    \'No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a\'\n                    \' `training_step()`, `training_dataloader()` and `configure_optimizers()` to be defined.\')\n\n            # Check val_dataloader, validation_step and validation_epoch_end\n            if self.is_overridden(\'val_dataloader\', model):\n                if not self.is_overridden(\'validation_step\', model):\n                    raise MisconfigurationException(\'You have passed in a `val_dataloader()`\'\n                                                    \' but have not defined `validation_step()`.\')\n                else:\n                    if not self.is_overridden(\'validation_epoch_end\', model):\n                        rank_zero_warn(\n                            \'You have defined a `val_dataloader()` and have defined a `validation_step()`,\'\n                            \' you may also want to define `validation_epoch_end()` for accumulating stats.\',\n                            RuntimeWarning\n                        )\n            else:\n                if self.is_overridden(\'validation_step\', model):\n                    raise MisconfigurationException(\'You have defined `validation_step()`,\'\n                                                    \' but have not passed in a `val_dataloader()`.\')\n\n        # Check test_dataloader, test_step and test_epoch_end\n        if self.is_overridden(\'test_dataloader\', model):\n            if not self.is_overridden(\'test_step\', model):\n                raise MisconfigurationException(\'You have passed in a `test_dataloader()`\'\n                                                \' but have not defined `test_step()`.\')\n            else:\n                if not self.is_overridden(\'test_epoch_end\', model):\n                    rank_zero_warn(\n                        \'You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to\'\n                        \' define `test_epoch_end()` for accumulating stats.\', RuntimeWarning\n                    )\n        else:\n            if self.testing and self.is_overridden(\'test_step\', model):\n                raise MisconfigurationException(\'You have defined `test_step()` but did not\'\n                                                \' implement `test_dataloader` nor passed in `.test(test_dataloader)`.\')\n\n\nclass _PatchDataLoader(object):\n    r""""""\n    Callable object for patching dataloaders passed into trainer.fit().\n    Use this class to override model.*_dataloader() and be pickle-compatible.\n\n    Args:\n        dataloader: Dataloader object to return when called.\n\n    """"""\n\n    def __init__(self, dataloader: Union[List[DataLoader], DataLoader]):\n        self.dataloader = dataloader\n\n        # cannot pickle __code__ so cannot verify if PatchDataloader\n        # exists which shows dataloader methods have been overwritten.\n        # so, we hack it by using the string representation\n        self.patch_loader_code = str(self.__call__.__code__)\n\n    def __call__(self) -> Union[List[DataLoader], DataLoader]:\n        return self.dataloader\n'"
pytorch_lightning/trainer/training_io.py,10,"b'""""""\nLightning can automate saving and loading checkpoints\n=====================================================\n\nCheckpointing is enabled by default to the current working directory.\nTo change the checkpoint path pass in::\n\n    Trainer(default_root_dir=\'/your/path/to/save/checkpoints\')\n\n\nTo modify the behavior of checkpointing pass in your own callback.\n\n.. code-block:: python\n\n    from pytorch_lightning.callbacks import ModelCheckpoint\n\n    # DEFAULTS used by the Trainer\n    checkpoint_callback = ModelCheckpoint(\n        filepath=os.getcwd(),\n        save_top_k=1,\n        verbose=True,\n        monitor=\'val_loss\',\n        mode=\'min\',\n        prefix=\'\'\n    )\n\n    trainer = Trainer(checkpoint_callback=checkpoint_callback)\n\n\nRestoring training session\n--------------------------\n\nYou might want to not only load a model but also continue training it. Use this method to\nrestore the trainer state as well. This will continue from the epoch and global step you last left off.\nHowever, the dataloaders will start from the first batch again (if you shuffled it shouldn\'t matter).\n\nLightning will restore the session if you pass a logger with the same version and there\'s a saved checkpoint.\n\n.. code-block:: python\n\n    from pytorch_lightning import Trainer\n\n    trainer = Trainer(\n        resume_from_checkpoint=PATH\n    )\n\n    # this fit call loads model weights and trainer state\n    # the trainer continues seamlessly from where you left off\n    # without having to do anything else.\n    trainer.fit(model)\n\n\nThe trainer restores:\n\n- global_step\n- current_epoch\n- All optimizers\n- All lr_schedulers\n- Model weights\n\nYou can even change the logic of your model as long as the weights and ""architecture"" of\nthe system isn\'t different. If you add a layer, for instance, it might not work.\n\nAt a rough level, here\'s what happens inside Trainer :py:mod:`pytorch_lightning.base_module.model_saving.py`:\n\n.. code-block:: python\n\n    self.global_step = checkpoint[\'global_step\']\n    self.current_epoch = checkpoint[\'epoch\']\n\n    # restore the optimizers\n    optimizer_states = checkpoint[\'optimizer_states\']\n    for optimizer, opt_state in zip(self.optimizers, optimizer_states):\n        optimizer.load_state_dict(opt_state)\n\n    # restore the lr schedulers\n    lr_schedulers = checkpoint[\'lr_schedulers\']\n    for scheduler, lrs_state in zip(self.lr_schedulers, lr_schedulers):\n        scheduler[\'scheduler\'].load_state_dict(lrs_state)\n\n    # uses the model you passed into trainer\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n""""""\n\nimport os\nimport re\nimport signal\nfrom abc import ABC\nfrom argparse import Namespace\nfrom subprocess import call\nfrom typing import Union\n\nimport torch\nimport torch.distributed as torch_distrib\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.loggers import LightningLoggerBase\nfrom pytorch_lightning.overrides.data_parallel import (\n    LightningDistributedDataParallel,\n    LightningDataParallel,\n)\nfrom pytorch_lightning.utilities import rank_zero_warn, parsing\n\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.xla_multiprocessing as xmp\nexcept ImportError:\n    XLA_AVAILABLE = False\nelse:\n    XLA_AVAILABLE = True\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n\nclass TrainerIOMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    model: LightningModule\n    on_gpu: bool\n    root_gpu: ...\n    resume_from_checkpoint: ...\n    use_ddp: bool\n    use_ddp2: bool\n    use_horovod: bool\n    checkpoint_callback: ...\n    proc_rank: int\n    weights_save_path: str\n    logger: Union[LightningLoggerBase, bool]\n    early_stop_callback: ...\n    lr_schedulers: ...\n    optimizers: ...\n    on_tpu: bool\n    num_training_batches: int\n    accumulate_grad_batches: int\n    use_amp: bool\n    use_native_amp: bool\n    scaler: ...\n\n    def get_model(self):\n        is_dp_module = isinstance(self.model, (LightningDistributedDataParallel,\n                                               LightningDataParallel))\n        model = self.model.module if is_dp_module else self.model\n        return model\n\n    # --------------------\n    # CHECK-POINTING\n    # --------------------\n    def restore_weights(self, model: LightningModule):\n        """"""\n        We attempt to restore weights in this order:\n        1. HPC weights.\n        2. if no HPC weights restore checkpoint_path weights\n        3. otherwise don\'t restore weights\n        """"""\n        # clear cache before restore\n        if self.on_gpu:\n            torch.cuda.empty_cache()\n\n        # if script called from hpc resubmit, load weights\n        did_restore_hpc_weights = self.restore_hpc_weights_if_needed(model)\n\n        # clear cache after restore\n        if self.on_gpu:\n            torch.cuda.empty_cache()\n\n        if not did_restore_hpc_weights:\n            if self.resume_from_checkpoint is not None:\n                self.restore(self.resume_from_checkpoint, on_gpu=self.on_gpu)\n\n        # wait for all models to restore weights\n        if self.use_ddp or self.use_ddp2:\n            # wait for all processes to catch up\n            torch_distrib.barrier()\n\n        # wait for all models to restore weights\n        if self.on_tpu and XLA_AVAILABLE:\n            # wait for all processes to catch up\n            torch_xla.core.xla_model.rendezvous(""pl.TrainerIOMixin.restore_weights"")\n\n        elif self.use_horovod:\n            # wait for all processes to catch up\n            hvd.join()\n\n        # clear cache after restore\n        if self.on_gpu:\n            torch.cuda.empty_cache()\n\n    # --------------------\n    # HPC SIGNAL HANDLING\n    # --------------------\n    def register_slurm_signal_handlers(self):\n        # see if we\'re using slurm (not interactive)\n        on_slurm = False\n        try:\n            job_name = os.environ[\'SLURM_JOB_NAME\']\n            if job_name != \'bash\':\n                on_slurm = True\n        except Exception:\n            pass\n\n        if on_slurm:\n            log.info(\'Set SLURM handle signals.\')\n            signal.signal(signal.SIGUSR1, self.sig_handler)\n            signal.signal(signal.SIGTERM, self.term_handler)\n\n    def sig_handler(self, signum, frame):  # pragma: no-cover\n        if self.proc_rank == 0:\n            # save weights\n            log.info(\'handling SIGUSR1\')\n            self.hpc_save(self.weights_save_path, self.logger)\n\n            # find job id\n            job_id = os.environ[\'SLURM_JOB_ID\']\n            cmd = \'scontrol requeue {}\'.format(job_id)\n\n            # requeue job\n            log.info(f\'requeing job {job_id}...\')\n            result = call(cmd, shell=True)\n\n            # print result text\n            if result == 0:\n                log.info(f\'requeued exp {job_id}\')\n            else:\n                log.warning(\'requeue failed...\')\n\n            # close experiment to avoid issues\n            self.logger.close()\n\n    def term_handler(self, signum, frame):\n        # save\n        log.info(""bypassing sigterm"")\n\n    # --------------------\n    # MODEL SAVE CHECKPOINT\n    # --------------------\n    def _atomic_save(self, checkpoint, filepath: str):\n        """"""Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\n\n        This will create a temporary checkpoint with a suffix of ``.part``, then copy it to the final location once\n        saving is finished.\n\n        Args:\n            checkpoint: The object to save.\n                Built to be used with the ``dump_checkpoint`` method, but can deal with anything which ``torch.save``\n                accepts.\n            filepath: The path to which the checkpoint will be saved.\n                This points to the file that the checkpoint will be stored in.\n        """"""\n        tmp_path = str(filepath) + "".part""\n        torch.save(checkpoint, tmp_path)\n        os.replace(tmp_path, filepath)\n\n    def save_checkpoint(self, filepath, weights_only: bool = False):\n        checkpoint = self.dump_checkpoint(weights_only)\n\n        if self.proc_rank == 0:\n            # do the actual save\n            try:\n                self._atomic_save(checkpoint, filepath)\n            except AttributeError as err:\n                if LightningModule.CHECKPOINT_KEY_HYPER_PARAMS in checkpoint:\n                    del checkpoint[LightningModule.CHECKPOINT_KEY_HYPER_PARAMS]\n                rank_zero_warn(\'Warning, `module_arguments` dropped from checkpoint.\'\n                               f\' An attribute is not picklable {err}\')\n                self._atomic_save(checkpoint, filepath)\n\n    def restore(self, checkpoint_path: str, on_gpu: bool):\n        """"""\n        Restore training state from checkpoint.\n        Also restores all training state like:\n        - epoch\n        - callbacks\n        - schedulers\n        - optimizer\n        """"""\n\n        # if on_gpu:\n        #     checkpoint = torch.load(checkpoint_path)\n        # else:\n        # load on CPU first\n        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n\n        # load model state\n        model = self.get_model()\n\n        # load the state_dict on the model automatically\n        model.load_state_dict(checkpoint[\'state_dict\'])\n\n        # give model a chance to load something\n        model.on_load_checkpoint(checkpoint)\n\n        if on_gpu:\n            model.cuda(self.root_gpu)\n\n        # restore amp scaling\n        if self.use_amp and self.use_native_amp and \'native_amp_scaling_state\' in checkpoint:\n            self.scaler.load_state_dict(checkpoint[\'native_amp_scaling_state\'])\n\n        # load training state (affects trainer only)\n        self.restore_training_state(checkpoint)\n\n    def dump_checkpoint(self, weights_only: bool = False) -> dict:\n        """"""Creating model checkpoint.\n\n        Args:\n            weights_only: saving model weights only\n\n        Return:\n             structured dictionary\n        """"""\n        checkpoint = {\n            \'epoch\': self.current_epoch + 1,\n            \'global_step\': self.global_step + 1,\n        }\n\n        if not weights_only:\n            if self.checkpoint_callback:\n                checkpoint[\'checkpoint_callback_best_model_score\'] = self.checkpoint_callback.best_model_score\n                checkpoint[\'checkpoint_callback_best_model_path\'] = self.checkpoint_callback.best_model_path\n\n            if self.early_stop_callback:\n                checkpoint[\'early_stop_callback_wait\'] = self.early_stop_callback.wait\n                checkpoint[\'early_stop_callback_patience\'] = self.early_stop_callback.patience\n\n            # save optimizers\n            optimizer_states = []\n            for i, optimizer in enumerate(self.optimizers):\n                optimizer_states.append(optimizer.state_dict())\n\n            checkpoint[\'optimizer_states\'] = optimizer_states\n\n            # save lr schedulers\n            lr_schedulers = []\n            for scheduler in self.lr_schedulers:\n                lr_schedulers.append(scheduler[\'scheduler\'].state_dict())\n\n            checkpoint[\'lr_schedulers\'] = lr_schedulers\n\n            # save native amp scaling\n            if self.use_amp and self.use_native_amp:\n                checkpoint[\'native_amp_scaling_state\'] = self.scaler.state_dict()\n\n        # add the module_arguments and state_dict from the model\n        model = self.get_model()\n\n        checkpoint[\'state_dict\'] = model.state_dict()\n\n        if model.hparams:\n            if hasattr(model, \'_hparams_name\'):\n                checkpoint[LightningModule.CHECKPOINT_NAME_HYPER_PARAMS] = model._hparams_name\n            # add arguments to the checkpoint\n            # todo: add some recursion in case of OmegaConf\n            checkpoint[LightningModule.CHECKPOINT_KEY_HYPER_PARAMS] = dict(model.hparams)\n\n        # give the model a chance to add a few things\n        model.on_save_checkpoint(checkpoint)\n\n        return checkpoint\n\n    # --------------------\n    # HPC IO\n    # --------------------\n    def restore_hpc_weights_if_needed(self, model: LightningModule):\n        """"""If there is a set of hpc weights, use as signal to restore model.""""""\n        did_restore = False\n\n        # look for hpc weights\n        folderpath = self.weights_save_path\n        if os.path.exists(folderpath):\n            files = os.listdir(folderpath)\n            hpc_weight_paths = [x for x in files if \'hpc_ckpt\' in x]\n\n            # if hpc weights exist restore model\n            if len(hpc_weight_paths) > 0:\n                self.hpc_load(folderpath, self.on_gpu)\n                did_restore = True\n        return did_restore\n\n    def restore_training_state(self, checkpoint):\n        """"""\n        Restore trainer state.\n        Model will get its change to update\n        :param checkpoint:\n        :return:\n        """"""\n        if \'optimizer_states\' not in checkpoint or \'lr_schedulers\' not in checkpoint:\n            raise KeyError(\n                \'Trying to restore training state but checkpoint contains only the model.\'\n                \' This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\'\n            )\n\n        if self.checkpoint_callback:\n            if \'checkpoint_callback_best_model_score\' in checkpoint:\n                self.checkpoint_callback.best_model_score = checkpoint[\'checkpoint_callback_best_model_score\']\n            else:\n                # Old naming until version 0.7.6\n                rank_zero_warn(\n                    \'Loading a checkpoint created with an old version of Lightning; \'\n                    \'this will not be supported in the future.\'\n                )\n                self.checkpoint_callback.best_model_score = checkpoint[\'checkpoint_callback_best\']\n            self.checkpoint_callback.best_model_path = checkpoint[\'checkpoint_callback_best_model_path\']\n\n        if self.early_stop_callback:\n            self.early_stop_callback.wait = checkpoint[\'early_stop_callback_wait\']\n            self.early_stop_callback.patience = checkpoint[\'early_stop_callback_patience\']\n\n        self.global_step = checkpoint[\'global_step\']\n        self.current_epoch = checkpoint[\'epoch\']\n\n        # Division deals with global step stepping once per accumulated batch\n        # Inequality deals with different global step for odd vs even num_training_batches\n        n_accum = 1 if self.accumulate_grad_batches is None else self.accumulate_grad_batches\n        expected_steps = self.num_training_batches / n_accum\n        if self.num_training_batches != 0 and self.global_step % expected_steps > 1:\n            rank_zero_warn(\n                ""You\'re resuming from a checkpoint that ended mid-epoch. ""\n                ""This can cause unreliable results if further training is done, ""\n                ""consider using an end of epoch checkpoint. ""\n            )\n\n        # restore the optimizers\n        optimizer_states = checkpoint[\'optimizer_states\']\n        for optimizer, opt_state in zip(self.optimizers, optimizer_states):\n            optimizer.load_state_dict(opt_state)\n\n            # move optimizer to GPU 1 weight at a time\n            # avoids OOM\n            if self.root_gpu is not None:\n                for state in optimizer.state.values():\n                    for k, v in state.items():\n                        if isinstance(v, torch.Tensor):\n                            state[k] = v.cuda(self.root_gpu)\n\n        # restore the lr schedulers\n        lr_schedulers = checkpoint[\'lr_schedulers\']\n        for scheduler, lrs_state in zip(self.lr_schedulers, lr_schedulers):\n            scheduler[\'scheduler\'].load_state_dict(lrs_state)\n\n    # ----------------------------------\n    # PRIVATE OPS\n    # ----------------------------------\n    def hpc_save(self, folderpath: str, logger):\n        # make sure the checkpoint folder exists\n        os.makedirs(folderpath, exist_ok=True)\n\n        # save logger to make sure we get all the metrics\n        logger.save()\n\n        ckpt_number = self.max_ckpt_in_folder(folderpath) + 1\n\n        if not os.path.exists(folderpath):\n            os.makedirs(folderpath, exist_ok=True)\n        filepath = os.path.join(folderpath, f\'hpc_ckpt_{ckpt_number}.ckpt\')\n\n        # give model a chance to do something on hpc_save\n        model = self.get_model()\n        checkpoint = self.dump_checkpoint()\n\n        model.on_hpc_save(checkpoint)\n\n        # do the actual save\n        # TODO: fix for anything with multiprocess DP, DDP, DDP2\n        try:\n            self._atomic_save(checkpoint, filepath)\n        except AttributeError as err:\n            if LightningModule.CHECKPOINT_KEY_HYPER_PARAMS in checkpoint:\n                del checkpoint[LightningModule.CHECKPOINT_KEY_HYPER_PARAMS]\n            rank_zero_warn(\'warning, `module_arguments` dropped from checkpoint.\'\n                           f\' An attribute is not picklable {err}\')\n            self._atomic_save(checkpoint, filepath)\n\n        return filepath\n\n    def hpc_load(self, folderpath, on_gpu):\n        filepath = \'{}/hpc_ckpt_{}.ckpt\'.format(folderpath, self.max_ckpt_in_folder(folderpath))\n\n        # load on CPU first\n        checkpoint = torch.load(filepath, map_location=lambda storage, loc: storage)\n\n        # load model state\n        model = self.get_model()\n\n        # load the state_dict on the model automatically\n        model.load_state_dict(checkpoint[\'state_dict\'])\n\n        # restore amp scaling\n        if self.use_amp and self.use_native_amp and \'native_amp_scaling_state\' in checkpoint:\n            self.scaler.load_state_dict(checkpoint[\'native_amp_scaling_state\'])\n\n        if self.root_gpu is not None:\n            model.cuda(self.root_gpu)\n\n        # load training state (affects trainer only)\n        self.restore_training_state(checkpoint)\n\n        # call model hook\n        model.on_hpc_load(checkpoint)\n\n        log.info(f\'restored hpc model from: {filepath}\')\n\n    def max_ckpt_in_folder(self, path, name_key=\'ckpt_\'):\n        files = os.listdir(path)\n        files = [x for x in files if name_key in x]\n        if len(files) == 0:\n            return 0\n\n        ckpt_vs = []\n        for name in files:\n            name = name.split(name_key)[-1]\n            name = re.sub(\'[^0-9]\', \'\', name)\n            ckpt_vs.append(int(name))\n\n        return max(ckpt_vs)\n'"
pytorch_lightning/trainer/training_loop.py,3,"b'""""""\nThe lightning training loop handles everything except the actual computations of your model.\n To decide what will happen in your training loop, define the `training_step` function.\n\nBelow are all the things lightning automates for you in the training loop.\n\nAccumulated gradients\n---------------------\n\nAccumulated gradients runs K small batches of size N before doing a backwards pass.\n The effect is a large effective batch size of size KxN.\n\n.. code-block:: python\n\n    # DEFAULT (ie: no accumulated grads)\n    trainer = Trainer(accumulate_grad_batches=1)\n\nForce training for min or max epochs\n------------------------------------\n\nIt can be useful to force training for a minimum number of epochs or limit to a max number\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(min_epochs=1, max_epochs=1000)\n\nForce disable early stop\n------------------------\n\nTo disable early stopping pass None to the early_stop_callback\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(early_stop_callback=None)\n\nGradient Clipping\n-----------------\n\nGradient clipping may be enabled to avoid exploding gradients.\n Specifically, this will `clip the gradient norm computed over all model parameters\n `together <https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_>`_.\n\n.. code-block:: python\n\n    # DEFAULT (ie: don\'t clip)\n    trainer = Trainer(gradient_clip_val=0)\n\n    # clip gradients with norm above 0.5\n    trainer = Trainer(gradient_clip_val=0.5)\n\nInspect gradient norms\n----------------------\n\nLooking at grad norms can help you figure out where training might be going wrong.\n\n.. code-block:: python\n\n    # DEFAULT (-1 doesn\'t track norms)\n    trainer = Trainer(track_grad_norm=-1)\n\n    # track the LP norm (P=2 here)\n    trainer = Trainer(track_grad_norm=2)\n\nSet how much of the training set to check\n-----------------------------------------\n\nIf you don\'t want to check 100% of the training set (for debugging or if it\'s huge), set this flag.\n\ntrain_percent_check will be overwritten by overfit_pct if `overfit_pct > 0`\n\n.. code-block:: python\n\n    # DEFAULT\n    trainer = Trainer(train_percent_check=1.0)\n\n    # check 10% only\n    trainer = Trainer(train_percent_check=0.1)\n\nPacked sequences as inputs\n--------------------------\n\nWhen using PackedSequence, do 2 things:\n1. return either a padded tensor in dataset or a list of variable length tensors\nin the dataloader collate_fn (example above shows the list implementation).\n2. Pack the sequence in forward or training and validation steps depending on use case.\n\n.. code-block:: python\n\n    # For use in dataloader\n    def collate_fn(batch):\n        x = [item[0] for item in batch]\n        y = [item[1] for item in batch]\n        return x, y\n\n    # In module\n    def training_step(self, batch, batch_idx):\n        x = rnn.pack_sequence(batch[0], enforce_sorted=False)\n        y = rnn.pack_sequence(batch[1], enforce_sorted=False)\n\n\nTruncated Backpropagation Through Time\n--------------------------------------\n\nThere are times when multiple backwards passes are needed for each batch.\n For example, it may save memory to use Truncated Backpropagation Through Time when training RNNs.\n\nWhen this flag is enabled each batch is split into sequences of size truncated_bptt_steps\n and passed to training_step(...) separately. A default splitting function is provided,\n however, you can override it for more flexibility. See `tbptt_split_batch`.\n\n.. code-block:: python\n\n    # DEFAULT (single backwards pass per batch)\n    trainer = Trainer(truncated_bptt_steps=None)\n\n    # (split batch into sequences of size 2)\n    trainer = Trainer(truncated_bptt_steps=2)\n\n\nNaN detection and intervention\n------------------------------\nWhen the `terminate_on_nan` flag is enabled, after every forward pass during training, Lightning will\ncheck that\n\n1. the loss you return in `training_step` is finite (not NaN and not +/-inf)\n2. the model parameters have finite values.\n\nLightning will terminate the training loop with an error message if NaN or infinite\nvalues are detected. If this happens, you should investigate numerically unstable operations\nin your model.\n\n.. code-block:: python\n\n    # DEFAULT (won\'t perform the NaN check)\n    trainer = Trainer(terminate_on_nan=False)\n\n    # (NaN check each batch and terminate on NaN or infinite values)\n    trainer = Trainer(terminate_on_nan=True)\n\n""""""\n\nimport atexit\nimport signal\nfrom abc import ABC, abstractmethod\nfrom typing import Callable\nfrom typing import Union, List\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.callbacks.base import Callback\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.loggers import LightningLoggerBase\nfrom pytorch_lightning.trainer.supporters import TensorRunningAccum\nfrom pytorch_lightning.utilities import rank_zero_warn\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nimport subprocess\n\ntry:\n    from apex import amp\nexcept ImportError:\n    APEX_AVAILABLE = False\nelse:\n    APEX_AVAILABLE = True\n\ntry:\n    import torch_xla.distributed.parallel_loader as xla_pl\n    import torch_xla.core.xla_model as xm\nexcept ImportError:\n    XLA_AVAILABLE = False\nelse:\n    XLA_AVAILABLE = True\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n# constant which signals should be catched for graceful trainer shutdown\nSIGNAL_TERMINATE = (\'SIGTERM\', \'SIGSEGV\', \'SIGINT\')\n\n\nclass TrainerTrainLoopMixin(ABC):\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    max_epochs: int\n    min_epochs: int\n    on_gpu: bool\n    use_ddp: bool\n    use_dp: bool\n    use_ddp2: bool\n    use_horovod: bool\n    single_gpu: bool\n    use_tpu: bool\n    data_parallel_device_ids: ...\n    check_val_every_n_epoch: ...\n    num_training_batches: int\n    val_check_batch: ...\n    num_val_batches: int\n    disable_validation: bool\n    fast_dev_run: ...\n    accumulation_scheduler: ...\n    lr_schedulers: ...\n    enable_early_stop: ...\n    early_stop_callback: ...\n    callback_metrics: ...\n    logger: Union[LightningLoggerBase, bool]\n    global_step: int\n    testing: bool\n    log_save_interval: float\n    proc_rank: int\n    row_log_interval: float\n    truncated_bptt_steps: ...\n    optimizers: ...\n    optimizer_frequencies: ...\n    accumulate_grad_batches: int\n    track_grad_norm: ...\n    model: LightningModule\n    interrupted: bool\n    running_loss: ...\n    progress_bar_dict: ...\n    reduce_lr_on_plateau_scheduler: ...\n    profiler: ...\n    batch_idx: int\n    precision: ...\n    train_dataloader: DataLoader\n    reload_dataloaders_every_epoch: bool\n    max_steps: int\n    min_steps: int\n    total_batch_idx: int\n    checkpoint_callback: ...\n    terminate_on_nan: bool\n    tpu_id: int\n\n    # Callback system\n    callbacks: List[Callback]\n    on_train_start: Callable\n    on_train_end: Callable\n    on_batch_start: Callable\n    on_batch_end: Callable\n    on_epoch_start: Callable\n    on_epoch_end: Callable\n    on_validation_end: Callable\n\n    @abstractmethod\n    def get_model(self):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def is_function_implemented(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def run_evaluation(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def transfer_batch_to_gpu(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def transfer_batch_to_tpu(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def clip_gradients(self):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def detect_nan_tensors(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def is_overridden(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def add_progress_bar_metrics(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def log_metrics(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def process_output(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def reset_train_dataloader(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def reset_val_dataloader(self, model):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def has_arg(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def train(self):\n        # add signal handlers for process kills\n        # def _signal_kill_handler(*args):\n        #     return TrainerTrainLoopMixin.run_training_teardown(self)\n        #\n        # orig_signal_handlers = {}\n        # for sig_name in SIGNAL_TERMINATE:\n        #     orig_signal_handlers[sig_name] = signal.signal(getattr(signal, sig_name),\n        #                                                    _signal_kill_handler)\n\n        # get model\n        model = self.get_model()\n\n        # load data\n        # if reload_dataloaders_every_epoch, this is moved to the epoch loop\n        if not self.reload_dataloaders_every_epoch:\n            self.reset_train_dataloader(model)\n        self.reset_val_dataloader(model)\n\n        # Train start events\n        with self.profiler.profile(\'on_train_start\'):\n            # callbacks\n            self.on_train_start()\n            # initialize early stop callback\n            if self.early_stop_callback is not None:\n                self.early_stop_callback.on_train_start(self, self.get_model())\n            # model hooks\n            model.on_train_start()\n\n        try:\n            # run all epochs\n            for epoch in range(self.current_epoch, self.max_epochs):\n                # reset train dataloader\n                if self.reload_dataloaders_every_epoch:\n                    self.reset_train_dataloader(model)\n                # set seed for distributed sampler (enables shuffling for each epoch)\n                if (self.use_ddp or self.use_horovod) \\\n                        and hasattr(self.train_dataloader, \'sampler\') \\\n                        and hasattr(self.train_dataloader.sampler, \'set_epoch\'):\n                    self.train_dataloader.sampler.set_epoch(epoch)\n\n                # update training progress in trainer and model\n                model.current_epoch = epoch\n                self.current_epoch = epoch\n\n                # changing gradient according accumulation_scheduler\n                self.accumulation_scheduler.on_epoch_start(self, self.get_model())\n\n                # stores accumulated grad fractions per batch\n                self.batch_loss_value = TensorRunningAccum(\n                    window_length=self.accumulate_grad_batches\n                )\n\n                # -----------------\n                # RUN TNG EPOCH\n                # -----------------\n                self.run_training_epoch()\n\n                if self.max_steps and self.max_steps == self.global_step:\n                    self.run_training_teardown()\n                    return\n\n                # update LR schedulers\n                self.update_learning_rates(interval=\'epoch\')\n\n                # early stopping\n                met_min_epochs = epoch >= self.min_epochs - 1\n                met_min_steps = self.global_step >= self.min_steps if self.min_steps else True\n\n                # TODO wrap this logic into the callback\n                # DO NOT DELETE\n                # early stopping as a (new Callback) class doesn\'t yet work because we have to know these\n                # trainer flags including the current epoch stuff\n                # all of this needs to go into the early stopping to clean up better\n                if self.enable_early_stop:\n                    if (met_min_epochs and met_min_steps) or self.fast_dev_run:\n                        should_stop = self.early_stop_callback.on_validation_end(self, self.get_model())\n                        # stop training\n                        stop = should_stop and met_min_epochs\n                        if stop:\n                            self.run_training_teardown()\n                            return\n\n            self.run_training_teardown()\n\n        except KeyboardInterrupt:\n            rank_zero_warn(\'Detected KeyboardInterrupt, attempting graceful shutdown...\')\n\n            # user could press ctrl+c many times... only shutdown once\n            if not self.interrupted:\n                self.interrupted = True\n\n                for proc in self.interactive_ddp_procs:\n                    subprocess.Popen.kill(proc)\n\n                self.run_training_teardown()\n\n    def run_training_epoch(self):\n\n        # get model\n        model = self.get_model()\n\n        # Epoch start events\n        with self.profiler.profile(\'on_epoch_start\'):\n            # callbacks\n            self.on_epoch_start()\n\n            # model hooks\n            if self.is_function_implemented(\'on_epoch_start\'):\n                model.on_epoch_start()\n\n        # track local dataloader so TPU can wrap each epoch\n        train_dataloader = self.train_dataloader\n\n        # on TPU we have to wrap it under the ParallelLoader\n        if self.use_tpu:\n            device = xm.xla_device(self.tpu_id)\n            train_dataloader = xla_pl.ParallelLoader(train_dataloader, [device])\n            train_dataloader = train_dataloader.per_device_loader(device)\n\n        # bookkeeping\n        outputs = []\n\n        # run epoch\n        for batch_idx, (batch, is_last_batch) in self.profiler.profile_iterable(\n                enumerate(_with_is_last(train_dataloader)), ""get_train_batch""\n        ):\n            # stop epoch if we limited the number of training batches\n            if batch_idx >= self.num_training_batches:\n                break\n\n            self.batch_idx = batch_idx\n\n            model.global_step = self.global_step\n\n            # ---------------\n            # RUN TRAIN STEP\n            # ---------------\n            _outputs = self.run_training_batch(batch, batch_idx)\n            batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\n\n            # only track outputs when user implements training_epoch_end\n            # otherwise we will build up unnecessary memory\n            if self.is_overridden(\'training_epoch_end\', model=self.get_model()):\n                outputs.append(batch_output)\n\n            # when returning -1 from train_step, we end epoch early\n            early_stop_epoch = batch_result == -1\n\n            # TODO: consolidate all actions that need to take place only after\n            # self.accumulate_grad_batches steps (optimizer step, lr update, global step increment)\n            if (self.batch_idx + 1) % self.accumulate_grad_batches == 0:\n                # update lr\n                self.update_learning_rates(interval=\'step\')\n\n            # ---------------\n            # RUN VAL STEP\n            # ---------------\n            is_val_check_batch = (batch_idx + 1) % self.val_check_batch == 0\n            can_check_epoch = (self.current_epoch + 1) % self.check_val_every_n_epoch == 0\n            can_check_val = not self.disable_validation and can_check_epoch\n            should_check_val = is_val_check_batch or early_stop_epoch\n            should_check_val = should_check_val or (is_last_batch and self.val_check_batch == float(\'inf\'))\n            should_check_val = can_check_val and should_check_val\n\n            # ---------------\n            # CHECKPOINTING, EARLY STOPPING\n            # ---------------\n            # fast_dev_run always forces val checking after train batch\n            if self.fast_dev_run or should_check_val:\n                self.run_evaluation(test_mode=self.testing)\n                self.call_checkpoint_callback()\n\n            # when logs should be saved\n            should_save_log = (batch_idx + 1) % self.log_save_interval == 0 or early_stop_epoch\n            if should_save_log or self.fast_dev_run:\n                if self.proc_rank == 0 and self.logger is not None:\n                    self.logger.save()\n\n            # when metrics should be logged\n            should_log_metrics = batch_idx % self.row_log_interval == 0 or early_stop_epoch\n            if should_log_metrics or self.fast_dev_run:\n                # logs user requested information to logger\n                self.log_metrics(batch_step_metrics, grad_norm_dic)\n\n            # progress global step according to grads progress\n            if (self.batch_idx + 1) % self.accumulate_grad_batches == 0:\n                self.global_step += 1\n            self.total_batch_idx += 1\n\n            # max steps reached, end training\n            if self.max_steps is not None and self.max_steps == self.global_step:\n                break\n\n            # end epoch early\n            # stop when the flag is changed or we\'ve gone past the amount\n            # requested in the batches\n            if early_stop_epoch or self.fast_dev_run:\n                break\n\n        if self.use_horovod:\n            hvd.join(hvd.local_rank() if self.on_gpu else -1)\n\n        # process epoch outputs\n        model = self.get_model()\n        if self.is_overridden(\'training_epoch_end\', model=model):\n            epoch_output = model.training_epoch_end(outputs)\n            _processed_outputs = self.process_output(epoch_output)\n            log_epoch_metrics = _processed_outputs[2]\n            callback_epoch_metrics = _processed_outputs[3]\n            self.log_metrics(log_epoch_metrics, {})\n            self.callback_metrics.update(callback_epoch_metrics)\n            self.add_progress_bar_metrics(_processed_outputs[1])\n\n        # when no val loop is present or fast-dev-run still need to call checkpoints\n        if not self.is_overridden(\'validation_step\') and not (self.fast_dev_run or should_check_val):\n            self.call_checkpoint_callback()\n\n        # Epoch end events\n        with self.profiler.profile(\'on_epoch_end\'):\n            # callbacks\n            self.on_epoch_end()\n            # model hooks\n            if self.is_function_implemented(\'on_epoch_end\'):\n                model.on_epoch_end()\n\n    def run_training_batch(self, batch, batch_idx):\n        # track grad norms\n        grad_norm_dic = {}\n\n        # track all metrics for callbacks\n        all_callback_metrics = []\n\n        # track metrics to log\n        all_log_metrics = []\n\n        if batch is None:\n            return 0, grad_norm_dic, {}, {}\n\n        # Batch start events\n        with self.profiler.profile(\'on_batch_start\'):\n            # callbacks\n            self.on_batch_start()\n            # hooks\n            if self.is_function_implemented(\'on_batch_start\'):\n                response = self.get_model().on_batch_start(batch)\n                if response == -1:\n                    return -1, grad_norm_dic, {}, {}\n\n        splits = [batch]\n        if self.truncated_bptt_steps is not None:\n            model_ref = self.get_model()\n            with self.profiler.profile(\'tbptt_split_batch\'):\n                splits = model_ref.tbptt_split_batch(batch, self.truncated_bptt_steps)\n\n        self.hiddens = None\n        for split_idx, split_batch in enumerate(splits):\n            self.split_idx = split_idx\n\n            for opt_idx, optimizer in self._get_optimizers_iterable():\n                # make sure only the gradients of the current optimizer\'s parameters are calculated\n                # in the training step to prevent dangling gradients in multiple-optimizer setup.\n                if len(self.optimizers) > 1:\n                    for param in self.get_model().parameters():\n                        param.requires_grad = False\n                    for group in optimizer.param_groups:\n                        for param in group[\'params\']:\n                            param.requires_grad = True\n\n                # wrap the forward step in a closure so second order methods work\n                def optimizer_closure():\n                    # forward pass\n                    with self.profiler.profile(\'model_forward\'):\n                        if self.use_amp and self.use_native_amp:\n                            with torch.cuda.amp.autocast():\n                                output_dict = self.training_forward(split_batch, batch_idx,\n                                                                    opt_idx, self.hiddens)\n                        else:\n                            output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\n\n                        # format and reduce outputs accordingly\n                        processed_output = self.process_output(output_dict, train=True)\n\n                    closure_loss, progress_bar_metrics, log_metrics, callback_metrics, self.hiddens = processed_output\n\n                    # accumulate loss\n                    # (if accumulate_grad_batches = 1 no effect)\n                    closure_loss = closure_loss / self.accumulate_grad_batches\n\n                    # backward pass\n                    model_ref = self.get_model()\n                    with self.profiler.profile(\'model_backward\'):\n                        model_ref.backward(self, closure_loss, optimizer, opt_idx)\n\n                    # track metrics for callbacks\n                    all_callback_metrics.append(callback_metrics)\n\n                    # track progress bar metrics\n                    self.add_progress_bar_metrics(progress_bar_metrics)\n                    all_log_metrics.append(log_metrics)\n\n                    if self.use_horovod:\n                        # Synchronize Horovod to ensure gradient manipulations (e.g., loss scaling) are valid\n                        optimizer.synchronize()\n\n                    # insert after step hook\n                    if self.is_function_implemented(\'on_after_backward\'):\n                        model_ref = self.get_model()\n                        with self.profiler.profile(\'on_after_backward\'):\n                            model_ref.on_after_backward()\n\n                    return closure_loss, callback_metrics\n\n                # calculate loss\n                loss, batch_output = optimizer_closure()\n\n                # check if loss or model weights are nan\n                if self.terminate_on_nan:\n                    self.detect_nan_tensors(loss)\n\n                # track total loss for logging (avoid mem leaks)\n                self.batch_loss_value.append(loss)\n\n                # gradient update with accumulated gradients\n                if (self.batch_idx + 1) % self.accumulate_grad_batches == 0:\n\n                    # track gradient norms when requested\n                    if batch_idx % self.row_log_interval == 0:\n                        if float(self.track_grad_norm) > 0:\n                            model = self.get_model()\n                            grad_norm_dic = model.grad_norm(\n                                self.track_grad_norm)\n\n                    # clip gradients\n                    if self.use_amp and self.use_native_amp:\n                        self.scaler.unscale_(optimizer)\n                    self.clip_gradients()\n\n                    # calls .step(), .zero_grad()\n                    # override function to modify this behavior\n                    model = self.get_model()\n                    with self.profiler.profile(\'optimizer_step\'):\n                        model.optimizer_step(self.current_epoch, batch_idx,\n                                             optimizer, opt_idx,\n                                             lambda: optimizer_closure()[0])\n\n                    # calculate running loss for display\n                    self.running_loss.append(self.batch_loss_value.mean())\n\n                    # reset for next set of accumulated grads\n                    self.batch_loss_value.reset()\n\n        # Batch end events\n        with self.profiler.profile(\'on_batch_end\'):\n            # callbacks\n            self.on_batch_end()\n            # model hooks\n            if self.is_function_implemented(\'on_batch_end\'):\n                self.get_model().on_batch_end()\n\n        # collapse all metrics into one dict\n        all_log_metrics = {k: v for d in all_log_metrics for k, v in d.items()}\n\n        # track all metrics for callbacks\n        self.callback_metrics.update({k: v for d in all_callback_metrics for k, v in d.items()})\n\n        return 0, grad_norm_dic, all_log_metrics, batch_output\n\n    def _get_optimizers_iterable(self):\n        if not self.optimizer_frequencies:\n            # call training_step once per optimizer\n            return list(enumerate(self.optimizers))\n\n        optimizer_freq_cumsum = np.cumsum(self.optimizer_frequencies)\n        optimizers_loop_length = optimizer_freq_cumsum[-1]\n        current_place_in_loop = self.total_batch_idx % optimizers_loop_length\n\n        # find optimzier index by looking for the first {item > current_place} in the cumsum list\n        opt_idx = np.argmax(optimizer_freq_cumsum > current_place_in_loop)\n        return [(opt_idx, self.optimizers[opt_idx])]\n\n    # @atexit.register\n    def run_training_teardown(self):\n        if hasattr(self, \'_teardown_already_run\') and self._teardown_already_run:\n            return\n        # Train end events\n        with self.profiler.profile(\'on_train_end\'):\n            # callbacks\n            self.on_train_end()\n            # model hooks\n            if self.is_function_implemented(\'on_train_end\'):\n                self.get_model().on_train_end()\n\n        if self.logger is not None:\n            self.logger.finalize(""success"")\n\n        # summarize profile results\n        self.profiler.describe()\n\n        self._teardown_already_run = True\n\n    def training_forward(self, batch, batch_idx, opt_idx, hiddens):\n        """"""\n        Handle forward for each training case (distributed, single gpu, etc...)\n        :param batch:\n        :param batch_idx:\n        :return:\n        """"""\n        # ---------------\n        # FORWARD\n        # ---------------\n        # enable not needing to add opt_idx to training_step\n        args = [batch, batch_idx]\n\n        if len(self.optimizers) > 1:\n            if self.has_arg(\'training_step\', \'optimizer_idx\'):\n                args.append(opt_idx)\n            else:\n                num_opts = len(self.optimizers)\n                raise ValueError(\n                    f\'Your LightningModule defines {num_opts} optimizers but \'\n                    f\'training_step is missing the ""optimizer_idx"" argument.\'\n                )\n\n        # pass hiddens if using tbptt\n        if self.truncated_bptt_steps is not None:\n            args.append(hiddens)\n\n        # distributed forward\n        if self.use_ddp or self.use_ddp2 or self.use_dp:\n            output = self.model(*args)\n\n        # Horovod\n        elif self.use_horovod and self.on_gpu:\n            batch = self.transfer_batch_to_gpu(batch, hvd.local_rank())\n            args[0] = batch\n            output = self.model.training_step(*args)\n\n        # single GPU forward\n        elif self.single_gpu:\n            gpu_id = 0\n            if isinstance(self.data_parallel_device_ids, list):\n                gpu_id = self.data_parallel_device_ids[0]\n\n            # Don\'t copy the batch since there is a single gpu that the batch could\n            # be referenced from and if there are multiple optimizers the batch will\n            # wind up copying it to the same device repeatedly.\n            batch = self.transfer_batch_to_gpu(batch, gpu_id)\n            args[0] = batch\n            output = self.model.training_step(*args)\n\n        # TPU support\n        elif self.use_tpu:\n            batch = self.transfer_batch_to_tpu(batch, self.tpu_id)\n            args[0] = batch\n            output = self.model.training_step(*args)\n\n        # CPU forward\n        else:\n            output = self.model.training_step(*args)\n\n        # allow any mode to define training_step_end\n        # do something will all the dp outputs (like softmax)\n        if self.is_overridden(\'training_step_end\'):\n            model_ref = self.get_model()\n            with self.profiler.profile(\'training_step_end\'):\n                output = model_ref.training_step_end(output)\n\n        # allow any mode to define training_end\n        # TODO: remove in 1.0.0\n        if self.is_overridden(\'training_end\'):\n            model_ref = self.get_model()\n            with self.profiler.profile(\'training_end\'):\n                output = model_ref.training_end(output)\n\n            rank_zero_warn(\'`training_end` was deprecated in 0.7.0 and will be removed 1.0.0.\'\n                           \' Use training_epoch_end instead\', DeprecationWarning)\n\n        return output\n\n    def update_learning_rates(self, interval: str):\n        """"""Update learning rates.\n\n        Args:\n            interval: either \'epoch\' or \'step\'.\n        """"""\n        if not self.lr_schedulers:\n            return\n\n        for lr_scheduler in self.lr_schedulers:\n            current_idx = self.batch_idx if interval == \'step\' else self.current_epoch\n            current_idx += 1  # account for both batch and epoch starts from 0\n            # Take step if call to update_learning_rates matches the interval key and\n            # the current step modulo the schedulers frequency is zero\n            if lr_scheduler[\'interval\'] == interval and current_idx % lr_scheduler[\'frequency\'] == 0:\n                # If instance of ReduceLROnPlateau, we need to pass validation loss\n                if lr_scheduler[\'reduce_on_plateau\']:\n                    monitor_key = lr_scheduler[\'monitor\']\n                    monitor_val = self.callback_metrics.get(monitor_key)\n                    if monitor_val is None:\n                        avail_metrics = \',\'.join(list(self.callback_metrics.keys()))\n                        raise MisconfigurationException(\n                            f\'ReduceLROnPlateau conditioned on metric {monitor_key}\'\n                            f\' which is not available. Available metrics are: {avail_metrics}.\'\n                            \' Condition can be set using `monitor` key in lr scheduler dict\'\n                        )\n                    lr_scheduler[\'scheduler\'].step(monitor_val)\n                else:\n                    lr_scheduler[\'scheduler\'].step()\n\n    def call_checkpoint_callback(self):\n        if self.checkpoint_callback is not None:\n            self.checkpoint_callback.on_validation_end(self, self.get_model())\n\n\ndef _with_is_last(iterable):\n    """"""Pass through values from the given iterable with an added boolean indicating if this is the last item.\n    See `https://stackoverflow.com/a/1630350 <https://stackoverflow.com/a/1630350>`_""""""\n    it = iter(iterable)\n    last = next(it)\n    for val in it:\n        # yield last and has next\n        yield last, False\n        last = val\n    # yield last, no longer has next\n    yield last, True\n'"
pytorch_lightning/trainer/training_tricks.py,9,"b'import math\nimport sys\nfrom abc import ABC, abstractmethod\nimport gc\nimport os\nfrom typing import Optional\n\nimport torch\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\n\nfrom pytorch_lightning import _logger as log\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.callbacks import GradientAccumulationScheduler\nfrom pytorch_lightning.loggers.base import DummyLogger\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom pytorch_lightning.utilities.memory import is_oom_error, garbage_collection_cuda\n\nEPSILON = 1e-6\nEPSILON_FP16 = 1e-5\n\n\nclass TrainerTrainingTricksMixin(ABC):\n\n    # this is just a summary on variables used in this abstract class,\n    #  the proper values/initialisation should be done in child class\n    gradient_clip_val: ...\n    precision: int\n    default_root_dir: str\n    progress_bar_callback: ...\n    on_gpu: bool\n\n    @abstractmethod\n    def get_model(self):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def save_checkpoint(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def restore(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    @abstractmethod\n    def fit(self, *args):\n        """"""Warning: this is just empty shell for code implemented in other class.""""""\n\n    def clip_gradients(self):\n\n        # this code is a modification of torch.nn.utils.clip_grad_norm_\n        # with TPU support based on https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md\n        if self.gradient_clip_val > 0:\n            model = self.get_model()\n            parameters = model.parameters()\n            max_norm = float(self.gradient_clip_val)\n            norm_type = float(2.0)\n            if isinstance(parameters, torch.Tensor):\n                parameters = [parameters]\n            parameters = list(filter(lambda p: p.grad is not None, parameters))\n            if norm_type == math.inf:\n                total_norm = max(p.grad.data.abs().max() for p in parameters)\n            else:\n                device = parameters[0].device\n                total_norm = torch.zeros([], device=device if parameters else None)\n                for p in parameters:\n                    param_norm = p.grad.data.pow(norm_type).sum()\n                    total_norm.add_(param_norm)\n                total_norm = (total_norm ** (1. / norm_type))\n            eps = EPSILON_FP16 if self.precision == 16 else EPSILON\n            clip_coef = torch.tensor(max_norm, device=device) / (total_norm + eps)\n            for p in parameters:\n                p.grad.data.mul_(torch.where(clip_coef < 1, clip_coef, torch.tensor(1., device=device)))\n\n    def print_nan_gradients(self) -> None:\n        model = self.get_model()\n        for param in model.parameters():\n            if (param.grad is not None) and torch.isnan(param.grad.float()).any():\n                log.info(param, param.grad)\n\n    def detect_nan_tensors(self, loss: Tensor) -> None:\n        model = self.get_model()\n\n        # check if loss is nan\n        if not torch.isfinite(loss).all():\n            raise ValueError(\n                \'The loss returned in `training_step` is nan or inf.\'\n            )\n        # check if a network weight is nan\n        for name, param in model.named_parameters():\n            if not torch.isfinite(param).all():\n                self.print_nan_gradients()\n                raise ValueError(\n                    f\'Detected nan and/or inf values in `{name}`.\'\n                    \' Check your forward pass for numerically unstable operations.\'\n                )\n\n    def configure_accumulated_gradients(self, accumulate_grad_batches):\n        if isinstance(accumulate_grad_batches, dict):\n            self.accumulation_scheduler = GradientAccumulationScheduler(accumulate_grad_batches)\n        elif isinstance(accumulate_grad_batches, int):\n            schedule = {1: accumulate_grad_batches}\n            self.accumulation_scheduler = GradientAccumulationScheduler(schedule)\n        else:\n            raise TypeError(""Gradient accumulation supports only int and dict types"")\n\n    def scale_batch_size(self,\n                         model: LightningModule,\n                         mode: str = \'power\',\n                         steps_per_trial: int = 3,\n                         init_val: int = 2,\n                         max_trials: int = 25,\n                         batch_arg_name: str = \'batch_size\'):\n        r""""""\n        Will iteratively try to find the largest batch size for a given model\n        that does not give an out of memory (OOM) error.\n\n        Args:\n            model: Model to fit.\n\n            mode: string setting the search mode. Either `power` or `binsearch`.\n                If mode is `power` we keep multiplying the batch size by 2, until\n                we get an OOM error. If mode is \'binsearch\', we will initially\n                also keep multiplying by 2 and after encountering an OOM error\n                do a binary search between the last successful batch size and the\n                batch size that failed.\n\n            steps_per_trial: number of steps to run with a given batch size.\n                Idealy 1 should be enough to test if a OOM error occurs,\n                however in practise a few are needed\n\n            init_val: initial batch size to start the search with\n\n            max_trials: max number of increase in batch size done before\n               algorithm is terminated\n\n        """"""\n        if not hasattr(model, batch_arg_name):\n            raise MisconfigurationException(f\'Field {batch_arg_name} not found in `model.hparams`\')\n\n        if hasattr(model.train_dataloader, \'patch_loader_code\'):\n            raise MisconfigurationException(\'The batch scaling feature cannot be used with dataloaders\'\n                                            \' passed directly to `.fit()`. Please disable the feature or\'\n                                            \' incorporate the dataloader into the model.\')\n\n        # Arguments we adjust during the batch size finder, save for restoring\n        self.__scale_batch_dump_params()\n\n        # Set to values that are required by the algorithm\n        self.__scale_batch_reset_params(model, steps_per_trial)\n\n        # Save initial model, that is loaded after batch size is found\n        save_path = os.path.join(self.default_root_dir, \'temp_model.ckpt\')\n        self.save_checkpoint(str(save_path))\n\n        if self.progress_bar_callback:\n            self.progress_bar_callback.disable()\n\n        # Initially we just double in size until an OOM is encountered\n        new_size = _adjust_batch_size(self, value=init_val)  # initially set to init_val\n        if mode == \'power\':\n            new_size = _run_power_scaling(self, model, new_size, batch_arg_name, max_trials)\n        elif mode == \'binsearch\':\n            new_size = _run_binsearch_scaling(self, model, new_size, batch_arg_name, max_trials)\n        else:\n            raise ValueError(\'mode in method `scale_batch_size` can only be `power` or `binsearch\')\n\n        garbage_collection_cuda()\n        log.info(f\'Finished batch size finder, will continue with full run using batch size {new_size}\')\n\n        # Restore initial state of model\n        self.restore(str(save_path), on_gpu=self.on_gpu)\n        os.remove(save_path)\n\n        # Finish by resetting variables so trainer is ready to fit model\n        self.__scale_batch_restore_params()\n        if self.progress_bar_callback:\n            self.progress_bar_callback.enable()\n\n        return new_size\n\n    def __scale_batch_dump_params(self):\n        # Prevent going into infinite loop\n        self.__dumped_params = {\n            \'max_steps\': self.max_steps,\n            \'weights_summary\': self.weights_summary,\n            \'logger\': self.logger,\n            \'callbacks\': self.callbacks,\n            \'checkpoint_callback\': self.checkpoint_callback,\n            \'early_stop_callback\': self.early_stop_callback,\n            \'enable_early_stop\': self.enable_early_stop,\n            \'auto_scale_batch_size\': self.auto_scale_batch_size,\n            \'train_percent_check\': self.train_percent_check,\n            \'model\': self.model,\n        }\n\n    def __scale_batch_reset_params(self, model, steps_per_trial):\n        self.auto_scale_batch_size = None  # prevent recursion\n        self.max_steps = steps_per_trial  # take few steps\n        self.weights_summary = None  # not needed before full run\n        self.logger = DummyLogger()\n        self.callbacks = []  # not needed before full run\n        self.checkpoint_callback = False  # required for saving\n        self.early_stop_callback = None\n        self.enable_early_stop = False\n        self.train_percent_check = 1.0\n        self.optimizers, self.schedulers = [], []  # required for saving\n        self.model = model  # required for saving\n\n    def __scale_batch_restore_params(self):\n        self.max_steps = self.__dumped_params[\'max_steps\']\n        self.weights_summary = self.__dumped_params[\'weights_summary\']\n        self.logger = self.__dumped_params[\'logger\']\n        self.callbacks = self.__dumped_params[\'callbacks\']\n        self.checkpoint_callback = self.__dumped_params[\'checkpoint_callback\']\n        self.auto_scale_batch_size = self.__dumped_params[\'auto_scale_batch_size\']\n        self.early_stop_callback = self.__dumped_params[\'early_stop_callback\']\n        self.enable_early_stop = self.__dumped_params[\'enable_early_stop\']\n        self.train_percent_check = self.__dumped_params[\'train_percent_check\']\n        self.model = self.__dumped_params[\'model\']\n        del self.__dumped_params\n\n\ndef _adjust_batch_size(trainer,\n                       batch_arg_name: str = \'batch_size\',\n                       factor: float = 1.0,\n                       value: Optional[int] = None,\n                       desc: str = None):\n    """""" Function for adjusting the batch size. It is expected that the user\n        has provided a model that has a hparam field called `batch_size` i.e.\n        `model.hparams.batch_size` should exist.\n\n    Args:\n        trainer: instance of pytorch_lightning.Trainer\n\n        batch_arg_name: field where batch_size is stored in `model.hparams`\n\n        factor: value which the old batch size is multiplied by to get the\n            new batch size\n\n        value: if a value is given, will override the batch size with this value.\n            Note that the value of `factor` will not have an effect in this case\n\n        desc: either `succeeded` or `failed`. Used purely for logging\n\n    """"""\n    model = trainer.get_model()\n    batch_size = getattr(model, batch_arg_name)\n    if value:\n        setattr(model, batch_arg_name, value)\n        new_size = value\n        if desc:\n            log.info(f\'Batch size {batch_size} {desc}, trying batch size {new_size}\')\n    else:\n        new_size = int(batch_size * factor)\n        if desc:\n            log.info(f\'Batch size {batch_size} {desc}, trying batch size {new_size}\')\n        setattr(model, batch_arg_name, new_size)\n    return new_size\n\n\ndef _run_power_scaling(trainer, model, new_size, batch_arg_name, max_trials):\n    """""" Batch scaling mode where the size is doubled at each iteration until an\n        OOM error is encountered. """"""\n    for _ in range(max_trials):\n        garbage_collection_cuda()\n        trainer.global_step = 0  # reset after each try\n        try:\n            # Try fit\n            trainer.fit(model)\n            # Double in size\n            new_size = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\'succeeded\')\n        except RuntimeError as exception:\n            # Only these errors should trigger an adjustment\n            if is_oom_error(exception):\n                # If we fail in power mode, half the size and return\n                garbage_collection_cuda()\n                new_size = _adjust_batch_size(trainer, batch_arg_name, factor=0.5, desc=\'failed\')\n                break\n            else:\n                raise  # some other error not memory related\n    return new_size\n\n\ndef _run_binsearch_scaling(trainer, model, new_size, batch_arg_name, max_trials):\n    """""" Batch scaling mode where the size is initially is doubled at each iteration\n        until an OOM error is encountered. Hereafter, the batch size is further\n        refined using a binary search """"""\n    high = None\n    count = 0\n    while True:\n        garbage_collection_cuda()\n        trainer.global_step = 0  # reset after each try\n        try:\n            # Try fit\n            trainer.fit(model)\n            count += 1\n            if count > max_trials:\n                break\n            # Double in size\n            low = new_size\n            if high:\n                if high - low <= 1:\n                    break\n                midval = (high + low) // 2\n                new_size = _adjust_batch_size(trainer, batch_arg_name, value=midval, desc=\'succeeded\')\n            else:\n                new_size = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc=\'succeeded\')\n        except RuntimeError as exception:\n            # Only these errors should trigger an adjustment\n            if is_oom_error(exception):\n                # If we fail in power mode, half the size and return\n                garbage_collection_cuda()\n                high = new_size\n                midval = (high + low) // 2\n                new_size = _adjust_batch_size(trainer, value=midval, desc=\'failed\')\n                if high - low <= 1:\n                    break\n            else:\n                raise  # some other error not memory related\n    return new_size\n'"
pytorch_lightning/utilities/__init__.py,0,"b'""""""General utilities""""""\n\nfrom pytorch_lightning.utilities.distributed import rank_zero_only, rank_zero_warn\nfrom pytorch_lightning.utilities.apply_func import move_data_to_device\nfrom pytorch_lightning.utilities.parsing import AttributeDict\n'"
pytorch_lightning/utilities/apply_func.py,4,"b'from collections import Mapping, Sequence\nfrom typing import Any, Callable, Union\n\nimport torch\n\n\ndef apply_to_collection(data: Any, dtype: Union[type, tuple], function: Callable, *args, **kwargs) -> Any:\n    """"""\n    Recursively applies a function to all elements of a certain dtype.\n\n    Args:\n        data: the collection to apply the function to\n        dtype: the given function will be applied to all elements of this dtype\n        function: the function to apply\n        *args: positional arguments (will be forwarded to calls of ``function``)\n        **kwargs: keyword arguments (will be forwarded to calls of ``function``)\n\n    Returns:\n        the resulting collection\n\n    """"""\n    elem_type = type(data)\n\n    # Breaking condition\n    if isinstance(data, dtype):\n        return function(data, *args, **kwargs)\n\n    # Recursively apply to collection items\n    elif isinstance(data, Mapping):\n        return elem_type({k: apply_to_collection(v, dtype, function, *args, **kwargs)\n                          for k, v in data.items()})\n    elif isinstance(data, tuple) and hasattr(data, \'_fields\'):  # named tuple\n        return elem_type(*(apply_to_collection(d, dtype, function, *args, **kwargs) for d in data))\n    elif isinstance(data, Sequence) and not isinstance(data, str):\n        return elem_type([apply_to_collection(d, dtype, function, *args, **kwargs) for d in data])\n\n    # data is neither of dtype, nor a collection\n    return data\n\n\ndef move_data_to_device(batch: Any, device: torch.device):\n    """"""\n    Transfers a collection of tensors to the given device.\n\n    Args:\n        batch: A tensor or collection of tensors. See :func:`apply_to_collection`\n            for a list of supported collection types.\n        device: The device to which tensors should be moved\n\n    Return:\n        the same collection but with all contained tensors residing on the new device.\n\n    See Also:\n        - :meth:`torch.Tensor.to`\n        - :class:`torch.device`\n    """"""\n    def to(tensor):\n        return tensor.to(device, non_blocking=True)\n    return apply_to_collection(batch, dtype=torch.Tensor, function=to)\n'"
pytorch_lightning/utilities/device_dtype_mixin.py,29,"b'from typing import Union, Optional\n\nimport torch\n\n\nclass DeviceDtypeModuleMixin(torch.nn.Module):\n    _device: ...\n    _dtype: Union[str, torch.dtype]\n\n    @property\n    def dtype(self) -> Union[str, torch.dtype]:\n        return self._dtype\n\n    @dtype.setter\n    def dtype(self, new_dtype: Union[str, torch.dtype]):\n        # necessary to avoid infinite recursion\n        raise RuntimeError(\'Cannot set the dtype explicitly. Please use module.to(new_dtype).\')\n\n    @property\n    def device(self) -> Union[str, torch.device]:\n        return self._device\n\n    @device.setter\n    def device(self, new_device: Union[str, torch.device]):\n        # Necessary to avoid infinite recursion\n        raise RuntimeError(\'Cannot set the device explicitly. Please use module.to(new_device).\')\n\n    def to(self, *args, **kwargs) -> torch.nn.Module:\n        """"""Moves and/or casts the parameters and buffers.\n\n        This can be called as\n        .. function:: to(device=None, dtype=None, non_blocking=False)\n        .. function:: to(dtype, non_blocking=False)\n        .. function:: to(tensor, non_blocking=False)\n        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n        floating point desired :attr:`dtype` s. In addition, this method will\n        only cast the floating point parameters and buffers to :attr:`dtype`\n        (if given). The integral parameters and buffers will be moved\n        :attr:`device`, if that is given, but with dtypes unchanged. When\n        :attr:`non_blocking` is set, it tries to convert/move asynchronously\n        with respect to the host if possible, e.g., moving CPU Tensors with\n        pinned memory to CUDA devices.\n        See below for examples.\n\n        Note:\n            This method modifies the module in-place.\n\n        Args:\n            device: the desired device of the parameters\n                and buffers in this module\n            dtype: the desired floating point type of\n                the floating point parameters and buffers in this module\n            tensor: Tensor whose dtype and device are the desired\n                dtype and device for all parameters and buffers in this module\n\n        Returns:\n            Module: self\n\n        Example::\n            >>> class ExampleModule(DeviceDtypeModuleMixin):\n            ...     def __init__(self, weight: torch.Tensor):\n            ...         super().__init__()\n            ...         self.register_buffer(\'weight\', weight)\n            >>> _ = torch.manual_seed(0)\n            >>> module = ExampleModule(torch.rand(3, 4))\n            >>> module.weight #doctest: +ELLIPSIS\n            tensor([[...]])\n            >>> module.to(torch.double)\n            ExampleModule()\n            >>> module.weight #doctest: +ELLIPSIS\n            tensor([[...]], dtype=torch.float64)\n            >>> cpu = torch.device(\'cpu\')\n            >>> module.to(cpu, dtype=torch.half, non_blocking=True)\n            ExampleModule()\n            >>> module.weight #doctest: +ELLIPSIS\n            tensor([[...]], dtype=torch.float16)\n            >>> module.to(cpu)\n            ExampleModule()\n            >>> module.weight #doctest: +ELLIPSIS\n            tensor([[...]], dtype=torch.float16)\n        """"""\n        # there is diff nb vars in PT 1.5\n        out = torch._C._nn._parse_to(*args, **kwargs)\n        device = out[0]\n        dtype = out[1]\n        if device is not None:\n            self._device = device\n\n        if dtype is not None:\n            self._dtype = dtype\n\n        return super().to(*args, **kwargs)\n\n    def cuda(self, device: Optional[int] = None) -> torch.nn.Module:\n        """"""Moves all model parameters and buffers to the GPU.\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing optimizer if the module will\n        live on GPU while being optimized.\n\n        Arguments:\n            device: if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        """"""\n\n        self._device = torch.device(\'cuda\', index=device)\n        return super().cuda(device=device)\n\n    def cpu(self) -> torch.nn.Module:\n        """"""Moves all model parameters and buffers to the CPU.\n        Returns:\n            Module: self\n        """"""\n        self._device = torch.device(\'cpu\')\n        return super().cpu()\n\n    def type(self, dst_type: Union[str, torch.dtype]) -> torch.nn.Module:\n        """"""Casts all parameters and buffers to :attr:`dst_type`.\n\n        Arguments:\n            dst_type (type or string): the desired type\n\n        Returns:\n            Module: self\n        """"""\n        self._dtype = dst_type\n        return super().type(dst_type=dst_type)\n\n    def float(self) -> torch.nn.Module:\n        """"""Casts all floating point parameters and buffers to float datatype.\n\n        Returns:\n            Module: self\n        """"""\n        self._dtype = torch.float\n        return super().float()\n\n    def double(self) -> torch.nn.Module:\n        """"""Casts all floating point parameters and buffers to ``double`` datatype.\n\n        Returns:\n            Module: self\n        """"""\n        self._dtype = torch.double\n        return super().double()\n\n    def half(self) -> torch.nn.Module:\n        """"""Casts all floating point parameters and buffers to ``half`` datatype.\n\n        Returns:\n            Module: self\n        """"""\n        self._dtype = torch.half\n        return super().half()\n'"
pytorch_lightning/utilities/distributed.py,0,"b""from functools import wraps\nimport warnings\n\n\ndef rank_zero_only(fn):\n\n    @wraps(fn)\n    def wrapped_fn(*args, **kwargs):\n        if rank_zero_only.rank == 0:\n            return fn(*args, **kwargs)\n\n    return wrapped_fn\n\n\ntry:\n    # add the attribute to the function but don't overwrite in case Trainer has already set it\n    getattr(rank_zero_only, 'rank')\nexcept AttributeError:\n    rank_zero_only.rank = 0\n\n\ndef _warn(*args, **kwargs):\n    warnings.warn(*args, **kwargs)\n\n\nrank_zero_warn = rank_zero_only(_warn)\n"""
pytorch_lightning/utilities/exceptions.py,0,b'class MisconfigurationException(Exception):\n    pass\n'
pytorch_lightning/utilities/memory.py,3,"b'import gc\nimport torch\n\n\ndef recursive_detach(in_dict: dict) -> dict:\n    """"""Detach all tensors in `in_dict`.\n\n    May operate recursively if some of the values in `in_dict` are dictionaries\n    which contain instances of `torch.Tensor`. Other types in `in_dict` are\n    not affected by this utility function.\n\n    Args:\n        in_dict:\n\n    Return:\n        out_dict:\n    """"""\n    out_dict = {}\n    for k, v in in_dict.items():\n        if isinstance(v, dict):\n            out_dict.update({k: recursive_detach(v)})\n        elif callable(getattr(v, \'detach\', None)):\n            out_dict.update({k: v.detach()})\n        else:\n            out_dict.update({k: v})\n    return out_dict\n\n\ndef is_oom_error(exception):\n    return is_cuda_out_of_memory(exception) \\\n        or is_cudnn_snafu(exception) \\\n        or is_out_of_cpu_memory(exception)\n\n\n# based on https://github.com/BlackHC/toma/blob/master/toma/torch_cuda_memory.py\ndef is_cuda_out_of_memory(exception):\n    return isinstance(exception, RuntimeError) \\\n        and len(exception.args) == 1 \\\n        and ""CUDA out of memory."" in exception.args[0]\n\n\n# based on https://github.com/BlackHC/toma/blob/master/toma/torch_cuda_memory.py\ndef is_cudnn_snafu(exception):\n    # For/because of https://github.com/pytorch/pytorch/issues/4107\n    return isinstance(exception, RuntimeError) \\\n        and len(exception.args) == 1 \\\n        and ""cuDNN error: CUDNN_STATUS_NOT_SUPPORTED."" in exception.args[0]\n\n\n# based on https://github.com/BlackHC/toma/blob/master/toma/cpu_memory.py\ndef is_out_of_cpu_memory(exception):\n    return isinstance(exception, RuntimeError) \\\n        and len(exception.args) == 1 \\\n        and ""DefaultCPUAllocator: can\'t allocate memory"" in exception.args[0]\n\n\n# based on https://github.com/BlackHC/toma/blob/master/toma/torch_cuda_memory.py\ndef garbage_collection_cuda():\n    """"""Garbage collection Torch (CUDA) memory.""""""\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n'"
pytorch_lightning/utilities/parsing.py,0,"b'import inspect\nfrom argparse import Namespace\n\n\ndef str_to_bool(val):\n    """"""Convert a string representation of truth to true (1) or false (0).\n    Copied from the python implementation distutils.utils.strtobool\n\n    True values are \'y\', \'yes\', \'t\', \'true\', \'on\', and \'1\'; false values\n    are \'n\', \'no\', \'f\', \'false\', \'off\', and \'0\'.  Raises ValueError if\n    \'val\' is anything else.\n\n    >>> str_to_bool(\'YES\')\n    1\n    >>> str_to_bool(\'FALSE\')\n    0\n    """"""\n    val = val.lower()\n    if val in (\'y\', \'yes\', \'t\', \'true\', \'on\', \'1\'):\n        return 1\n    elif val in (\'n\', \'no\', \'f\', \'false\', \'off\', \'0\'):\n        return 0\n    else:\n        raise ValueError(f\'invalid truth value {val}\')\n\n\ndef clean_namespace(hparams):\n    """"""Removes all functions from hparams so we can pickle.""""""\n\n    if isinstance(hparams, Namespace):\n        del_attrs = []\n        for k in hparams.__dict__:\n            if callable(getattr(hparams, k)):\n                del_attrs.append(k)\n\n        for k in del_attrs:\n            delattr(hparams, k)\n\n    elif isinstance(hparams, dict):\n        del_attrs = []\n        for k, v in hparams.items():\n            if callable(v):\n                del_attrs.append(k)\n\n        for k in del_attrs:\n            del hparams[k]\n\n\ndef get_init_args(frame) -> dict:\n    _, _, _, local_vars = inspect.getargvalues(frame)\n    if \'__class__\' not in local_vars:\n        return\n    cls = local_vars[\'__class__\']\n    spec = inspect.getfullargspec(cls.__init__)\n    init_parameters = inspect.signature(cls.__init__).parameters\n    self_identifier = spec.args[0]  # ""self"" unless user renames it (always first arg)\n    varargs_identifier = spec.varargs  # by convention this is named ""*args""\n    kwargs_identifier = spec.varkw  # by convention this is named ""**kwargs""\n    exclude_argnames = (\n        varargs_identifier, kwargs_identifier, self_identifier, \'__class__\', \'frame\', \'frame_args\'\n    )\n\n    # only collect variables that appear in the signature\n    local_args = {k: local_vars[k] for k in init_parameters.keys()}\n    local_args.update(local_args.get(kwargs_identifier, {}))\n    local_args = {k: v for k, v in local_args.items() if k not in exclude_argnames}\n    return local_args\n\n\ndef collect_init_args(frame, path_args: list, inside: bool = False) -> list:\n    """"""\n    Recursively collects the arguments passed to the child constructors in the inheritance tree.\n\n    Args:\n        frame: the current stack frame\n        path_args: a list of dictionaries containing the constructor args in all parent classes\n        inside: track if we are inside inheritance path, avoid terminating too soon\n\n    Return:\n          A list of dictionaries where each dictionary contains the arguments passed to the\n          constructor at that level. The last entry corresponds to the constructor call of the\n          most specific class in the hierarchy.\n    """"""\n    _, _, _, local_vars = inspect.getargvalues(frame)\n    if \'__class__\' in local_vars:\n        local_args = get_init_args(frame)\n        # recursive update\n        path_args.append(local_args)\n        return collect_init_args(frame.f_back, path_args, inside=True)\n    elif not inside:\n        return collect_init_args(frame.f_back, path_args, inside)\n    else:\n        return path_args\n\n\nclass AttributeDict(dict):\n    """"""Extended dictionary accesisable with dot notation.\n\n    >>> ad = AttributeDict({\'key1\': 1, \'key2\': \'abc\'})\n    >>> ad.key1\n    1\n    >>> ad.update({\'my-key\': 3.14})\n    >>> ad.update(mew_key=42)\n    >>> ad.key1 = 2\n    >>> ad\n    ""key1"":    2\n    ""key2"":    abc\n    ""mew_key"": 42\n    ""my-key"":  3.14\n    """"""\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(f\'Missing attribute ""{key}""\')\n\n    def __setattr__(self, key, val):\n        self[key] = val\n\n    def __repr__(self):\n        if not len(self):\n            return """"\n        max_key_length = max([len(str(k)) for k in self])\n        tmp_name = \'{:\' + str(max_key_length + 3) + \'s} {}\'\n        rows = [tmp_name.format(f\'""{n}"":\', self[n]) for n in sorted(self.keys())]\n        out = \'\\n\'.join(rows)\n        return out\n'"
tests/base/__init__.py,0,"b'""""""Models for testing.""""""\n\nfrom tests.base.datasets import TrialMNIST\nfrom tests.base.model_template import EvalModelTemplate\n'"
tests/base/dataloaders.py,0,"b'""""""Custom dataloaders for testing""""""\n\n\nclass CustomInfDataloader:\n\n    def __init__(self, dataloader):\n        self.dataloader = dataloader\n        self.iter = iter(dataloader)\n        self.count = 0\n\n    def __iter__(self):\n        self.count = 0\n        return self\n\n    def __next__(self):\n        if self.count >= 50:\n            raise StopIteration\n        self.count = self.count + 1\n        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = iter(self.dataloader)\n            return next(self.iter)\n'"
tests/base/datasets.py,9,"b'import logging\nimport os\nimport urllib.request\nfrom typing import Tuple, Optional, Sequence\n\nimport torch\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\n\nfrom tests import PACKAGE_ROOT\n\n#: local path to test datasets\nPATH_DATASETS = os.path.join(PACKAGE_ROOT, \'Datasets\')\n\n\nclass MNIST(Dataset):\n    """"""\n    Customized `MNIST <http://yann.lecun.com/exdb/mnist/>`_ dataset for testing Pytorch Lightning\n    without the torchvision dependency.\n\n    Part of the code was copied from\n    https://github.com/pytorch/vision/blob/build/v0.5.0/torchvision/datasets/mnist.py\n\n    Args:\n        root: Root directory of dataset where ``MNIST/processed/training.pt``\n            and  ``MNIST/processed/test.pt`` exist.\n        train: If ``True``, creates dataset from ``training.pt``,\n            otherwise from ``test.pt``.\n        normalize: mean and std deviation of the MNIST dataset.\n        download: If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n\n    Examples:\n        >>> dataset = MNIST(download=True)\n        >>> len(dataset)\n        60000\n        >>> torch.bincount(dataset.targets)\n        tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n    """"""\n\n    RESOURCES = (\n        ""https://pl-public-data.s3.amazonaws.com/MNIST/processed/training.pt"",\n        ""https://pl-public-data.s3.amazonaws.com/MNIST/processed/test.pt"",\n    )\n\n    TRAIN_FILE_NAME = \'training.pt\'\n    TEST_FILE_NAME = \'test.pt\'\n    cache_folder_name = \'complete\'\n\n    def __init__(self, root: str = PATH_DATASETS, train: bool = True,\n                 normalize: tuple = (0.5, 1.0), download: bool = True):\n        super().__init__()\n        self.root = root\n        self.train = train  # training set or test set\n        self.normalize = normalize\n\n        self.prepare_data(download)\n\n        if not self._check_exists(self.cached_folder_path):\n            raise RuntimeError(\'Dataset not found.\')\n\n        data_file = self.TRAIN_FILE_NAME if self.train else self.TEST_FILE_NAME\n        self.data, self.targets = torch.load(os.path.join(self.cached_folder_path, data_file))\n\n    def __getitem__(self, idx: int) -> Tuple[Tensor, int]:\n        img = self.data[idx].float().unsqueeze(0)\n        target = int(self.targets[idx])\n\n        if self.normalize is not None:\n            img = normalize_tensor(img, mean=self.normalize[0], std=self.normalize[1])\n\n        return img, target\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    @property\n    def cached_folder_path(self) -> str:\n        return os.path.join(self.root, \'MNIST\', self.cache_folder_name)\n\n    def _check_exists(self, data_folder: str) -> bool:\n        existing = True\n        for fname in (self.TRAIN_FILE_NAME, self.TEST_FILE_NAME):\n            existing = existing and os.path.isfile(os.path.join(data_folder, fname))\n        return existing\n\n    def prepare_data(self, download: bool):\n        if download:\n            self._download(self.cached_folder_path)\n\n    def _download(self, data_folder: str) -> None:\n        """"""Download the MNIST data if it doesn\'t exist in cached_folder_path already.""""""\n\n        if self._check_exists(data_folder):\n            return\n\n        os.makedirs(data_folder, exist_ok=True)\n\n        for url in self.RESOURCES:\n            logging.info(f\'Downloading {url}\')\n            fpath = os.path.join(data_folder, os.path.basename(url))\n            urllib.request.urlretrieve(url, fpath)\n\n\ndef normalize_tensor(tensor: Tensor, mean: float = 0.0, std: float = 1.0) -> Tensor:\n    tensor = tensor.clone()\n    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device)\n    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device)\n    tensor.sub_(mean).div_(std)\n    return tensor\n\n\nclass TrialMNIST(MNIST):\n    """"""Constrain image dataset\n\n    Args:\n        root: Root directory of dataset where ``MNIST/processed/training.pt``\n            and  ``MNIST/processed/test.pt`` exist.\n        train: If ``True``, creates dataset from ``training.pt``,\n            otherwise from ``test.pt``.\n        normalize: mean and std deviation of the MNIST dataset.\n        download: If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n        num_samples: number of examples per selected class/digit\n        digits: list selected MNIST digits/classes\n\n    Examples:\n        >>> dataset = TrialMNIST(download=True)\n        >>> len(dataset)\n        300\n        >>> sorted(set([d.item() for d in dataset.targets]))\n        [0, 1, 2]\n        >>> torch.bincount(dataset.targets)\n        tensor([100, 100, 100])\n    """"""\n\n    def __init__(self, root: str = PATH_DATASETS, train: bool = True,\n                 normalize: tuple = (0.5, 1.0), download: bool = False,\n                 num_samples: int = 100, digits: Optional[Sequence] = (0, 1, 2)):\n\n        # number of examples per class\n        self.num_samples = num_samples\n        # take just a subset of MNIST dataset\n        self.digits = digits if digits else list(range(10))\n\n        self.cache_folder_name = \'digits-\' + \'-\'.join(str(d) for d in sorted(self.digits)) \\\n                                 + f\'_nb-{self.num_samples}\'\n\n        super().__init__(\n            root,\n            train=train,\n            normalize=normalize,\n            download=download\n        )\n\n    @staticmethod\n    def _prepare_subset(full_data: torch.Tensor, full_targets: torch.Tensor,\n                        num_samples: int, digits: Sequence):\n        classes = {d: 0 for d in digits}\n        indexes = []\n        for idx, target in enumerate(full_targets):\n            label = target.item()\n            if classes.get(label, float(\'inf\')) >= num_samples:\n                continue\n            indexes.append(idx)\n            classes[label] += 1\n            if all(classes[k] >= num_samples for k in classes):\n                break\n        data = full_data[indexes]\n        targets = full_targets[indexes]\n        return data, targets\n\n    def prepare_data(self, download: bool) -> None:\n        if self._check_exists(self.cached_folder_path):\n            return\n        if download:\n            self._download(super().cached_folder_path)\n\n        for fname in (self.TRAIN_FILE_NAME, self.TEST_FILE_NAME):\n            path_fname = os.path.join(super().cached_folder_path, fname)\n            assert os.path.isfile(path_fname), \'Missing cached file: %s\' % path_fname\n            data, targets = torch.load(path_fname)\n            data, targets = self._prepare_subset(data, targets, self.num_samples, self.digits)\n            torch.save((data, targets), os.path.join(self.cached_folder_path, fname))\n'"
tests/base/mixins.py,0,b''
tests/base/model_optimizers.py,0,"b'from abc import ABC\n\nfrom torch import optim\n\n\nclass ConfigureOptimizersPool(ABC):\n\n    def configure_optimizers(self):\n        """"""\n        return whatever optimizers we want here.\n        :return: list of optimizers\n        """"""\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n\n    def configure_optimizers__empty(self):\n        return None\n\n    def configure_optimizers__lbfgs(self):\n        """"""\n        return whatever optimizers we want here.\n        :return: list of optimizers\n        """"""\n        optimizer = optim.LBFGS(self.parameters(), lr=self.learning_rate)\n        return optimizer\n\n    def configure_optimizers__multiple_optimizers(self):\n        """"""\n        return whatever optimizers we want here.\n        :return: list of optimizers\n        """"""\n        # try no scheduler for this model (testing purposes)\n        optimizer1 = optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer2 = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer1, optimizer2\n\n    def configure_optimizers__single_scheduler(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n        return [optimizer], [lr_scheduler]\n\n    def configure_optimizers__multiple_schedulers(self):\n        optimizer1 = optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer2 = optim.Adam(self.parameters(), lr=self.learning_rate)\n        lr_scheduler1 = optim.lr_scheduler.StepLR(optimizer1, 1, gamma=0.1)\n        lr_scheduler2 = optim.lr_scheduler.StepLR(optimizer2, 1, gamma=0.1)\n\n        return [optimizer1, optimizer2], [lr_scheduler1, lr_scheduler2]\n\n    def configure_optimizers__mixed_scheduling(self):\n        optimizer1 = optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer2 = optim.Adam(self.parameters(), lr=self.learning_rate)\n        lr_scheduler1 = optim.lr_scheduler.StepLR(optimizer1, 4, gamma=0.1)\n        lr_scheduler2 = optim.lr_scheduler.StepLR(optimizer2, 1, gamma=0.1)\n\n        return [optimizer1, optimizer2], \\\n            [{\'scheduler\': lr_scheduler1, \'interval\': \'step\'}, lr_scheduler2]\n\n    def configure_optimizers__reduce_lr_on_plateau(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n        return [optimizer], [lr_scheduler]\n\n    def configure_optimizers__param_groups(self):\n        param_groups = [\n            {\'params\': list(self.parameters())[:2], \'lr\': self.learning_rate * 0.1},\n            {\'params\': list(self.parameters())[2:], \'lr\': self.learning_rate}\n        ]\n\n        optimizer = optim.Adam(param_groups)\n        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n        return [optimizer], [lr_scheduler]\n'"
tests/base/model_template.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom tests.base.datasets import TrialMNIST, PATH_DATASETS\nfrom tests.base.model_optimizers import ConfigureOptimizersPool\nfrom tests.base.model_test_dataloaders import TestDataloaderVariations\nfrom tests.base.model_test_epoch_ends import TestEpochEndVariations\nfrom tests.base.model_test_steps import TestStepVariations\nfrom tests.base.model_train_dataloaders import TrainDataloaderVariations\nfrom tests.base.model_train_steps import TrainingStepVariations\nfrom tests.base.model_utilities import ModelTemplateUtils, ModelTemplateData\nfrom tests.base.model_valid_dataloaders import ValDataloaderVariations\nfrom tests.base.model_valid_epoch_ends import ValidationEpochEndVariations\nfrom tests.base.model_valid_steps import ValidationStepVariations\n\n\nclass EvalModelTemplate(\n    ModelTemplateData,\n    ModelTemplateUtils,\n    TrainingStepVariations,\n    ValidationStepVariations,\n    ValidationEpochEndVariations,\n    TestStepVariations,\n    TestEpochEndVariations,\n    TrainDataloaderVariations,\n    ValDataloaderVariations,\n    TestDataloaderVariations,\n    ConfigureOptimizersPool,\n    LightningModule\n):\n    """"""\n    This template houses all  combinations of model  configurations  we want to test\n\n    >>> model = EvalModelTemplate()\n    """"""\n\n    def __init__(self,\n                 *args,\n                 drop_prob: float = 0.2,\n                 batch_size: int = 32,\n                 in_features: int = 28 * 28,\n                 learning_rate: float = 0.001 * 8,\n                 optimizer_name: str = \'adam\',\n                 data_root: str = PATH_DATASETS,\n                 out_features: int = 10,\n                 hidden_dim: int = 1000,\n                 b1: float = 0.5,\n                 b2: float = 0.999,\n                 **kwargs) -> object:\n        # init superclass\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.drop_prob = drop_prob\n        self.batch_size = batch_size\n        self.in_features = in_features\n        self.learning_rate = learning_rate\n        self.optimizer_name = optimizer_name\n        self.data_root = data_root\n        self.out_features = out_features\n        self.hidden_dim = hidden_dim\n        self.b1 = b1\n        self.b2 = b2\n\n        # if you specify an example input, the summary will show input/output for each layer\n        # TODO: to be fixed in #1773\n        # self.example_input_array = torch.rand(5, 28 * 28)\n\n        # build model\n        self.__build_model()\n\n    def __build_model(self):\n        """"""\n        Simple model for testing\n        :return:\n        """"""\n        self.c_d1 = nn.Linear(\n            in_features=self.in_features,\n            out_features=self.hidden_dim\n        )\n        self.c_d1_bn = nn.BatchNorm1d(self.hidden_dim)\n        self.c_d1_drop = nn.Dropout(self.drop_prob)\n\n        self.c_d2 = nn.Linear(\n            in_features=self.hidden_dim,\n            out_features=self.out_features\n        )\n\n    def forward(self, x):\n        x = self.c_d1(x)\n        x = torch.tanh(x)\n        x = self.c_d1_bn(x)\n        x = self.c_d1_drop(x)\n\n        x = self.c_d2(x)\n        logits = F.log_softmax(x, dim=1)\n\n        return logits\n\n    def loss(self, labels, logits):\n        nll = F.nll_loss(logits, labels)\n        return nll\n\n    def prepare_data(self):\n        _ = TrialMNIST(root=self.data_root, train=True, download=True)\n\n    @staticmethod\n    def get_default_hparams(continue_training: bool = False, hpc_exp_number: int = 0) -> dict:\n        args = dict(\n            drop_prob=0.2,\n            batch_size=32,\n            in_features=28 * 28,\n            learning_rate=0.001 * 8,\n            optimizer_name=\'adam\',\n            data_root=PATH_DATASETS,\n            out_features=10,\n            hidden_dim=1000,\n            b1=0.5,\n            b2=0.999,\n        )\n\n        if continue_training:\n            args.update(\n                test_tube_do_checkpoint_load=True,\n                hpc_exp_number=hpc_exp_number,\n            )\n\n        return args\n'"
tests/base/model_test_dataloaders.py,0,"b'from abc import ABC, abstractmethod\n\nfrom tests.base.dataloaders import CustomInfDataloader\n\n\nclass TestDataloaderVariations(ABC):\n\n    @abstractmethod\n    def dataloader(self, train: bool):\n        """"""placeholder""""""\n\n    def test_dataloader(self):\n        return self.dataloader(train=False)\n\n    def test_dataloader__infinite(self):\n        return CustomInfDataloader(self.dataloader(train=False))\n\n    def test_dataloader__empty(self):\n        return None\n\n    def test_dataloader__multiple(self):\n        return [self.dataloader(train=False), self.dataloader(train=False)]\n'"
tests/base/model_test_epoch_ends.py,6,"b'from abc import ABC\n\nimport torch\n\n\nclass TestEpochEndVariations(ABC):\n\n    def test_epoch_end(self, outputs):\n        """"""\n        Called at the end of validation to aggregate outputs\n        :param outputs: list of individual outputs of each validation step\n        :return:\n        """"""\n        # if returned a scalar from test_step, outputs is a list of tensor scalars\n        # we return just the average in this case (if we want)\n        # return torch.stack(outputs).mean()\n        test_loss_mean = 0\n        test_acc_mean = 0\n        for output in outputs:\n            test_loss = self.get_output_metric(output, \'test_loss\')\n\n            # reduce manually when using dp\n            if self.trainer.use_dp:\n                test_loss = torch.mean(test_loss)\n            test_loss_mean += test_loss\n\n            # reduce manually when using dp\n            test_acc = self.get_output_metric(output, \'test_acc\')\n            if self.trainer.use_dp:\n                test_acc = torch.mean(test_acc)\n\n            test_acc_mean += test_acc\n\n        test_loss_mean /= len(outputs)\n        test_acc_mean /= len(outputs)\n\n        metrics_dict = {\'test_loss\': test_loss_mean.item(), \'test_acc\': test_acc_mean.item()}\n        result = {\'progress_bar\': metrics_dict, \'log\': metrics_dict}\n        return result\n\n    def test_epoch_end__multiple_dataloaders(self, outputs):\n        """"""\n        Called at the end of validation to aggregate outputs\n        :param outputs: list of individual outputs of each validation step\n        :return:\n        """"""\n        # if returned a scalar from test_step, outputs is a list of tensor scalars\n        # we return just the average in this case (if we want)\n        # return torch.stack(outputs).mean()\n        test_loss_mean = 0\n        test_acc_mean = 0\n        i = 0\n        for dl_output in outputs:\n            for output in dl_output:\n                test_loss = output[\'test_loss\']\n\n                # reduce manually when using dp\n                if self.trainer.use_dp:\n                    test_loss = torch.mean(test_loss)\n                test_loss_mean += test_loss\n\n                # reduce manually when using dp\n                test_acc = output[\'test_acc\']\n                if self.trainer.use_dp:\n                    test_acc = torch.mean(test_acc)\n\n                test_acc_mean += test_acc\n                i += 1\n\n        test_loss_mean /= i\n        test_acc_mean /= i\n\n        tqdm_dict = {\'test_loss\': test_loss_mean.item(), \'test_acc\': test_acc_mean.item()}\n        result = {\'progress_bar\': tqdm_dict}\n        return result\n'"
tests/base/model_test_steps.py,6,"b'from abc import ABC\nfrom collections import OrderedDict\n\nimport torch\n\n\nclass TestStepVariations(ABC):\n    """"""\n    Houses all variations of test steps\n    """"""\n\n    def test_step(self, batch, batch_idx, *args, **kwargs):\n        """"""\n        Default, baseline test_step\n        :param batch:\n        :return:\n        """"""\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat = self(x)\n\n        loss_test = self.loss(y, y_hat)\n\n        # acc\n        labels_hat = torch.argmax(y_hat, dim=1)\n        test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n        test_acc = torch.tensor(test_acc)\n\n        test_acc = test_acc.type_as(x)\n\n        # alternate possible outputs to test\n        if batch_idx % 1 == 0:\n            output = OrderedDict({\n                \'test_loss\': loss_test,\n                \'test_acc\': test_acc,\n            })\n            return output\n        if batch_idx % 2 == 0:\n            return test_acc\n\n        if batch_idx % 3 == 0:\n            output = OrderedDict({\n                \'test_loss\': loss_test,\n                \'test_acc\': test_acc,\n                \'test_dic\': {\'test_loss_a\': loss_test}\n            })\n            return output\n\n    def test_step__multiple_dataloaders(self, batch, batch_idx, dataloader_idx, **kwargs):\n        """"""\n        Default, baseline test_step\n        :param batch:\n        :return:\n        """"""\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat = self(x)\n\n        loss_test = self.loss(y, y_hat)\n\n        # acc\n        labels_hat = torch.argmax(y_hat, dim=1)\n        test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n        test_acc = torch.tensor(test_acc)\n\n        test_acc = test_acc.type_as(x)\n\n        # alternate possible outputs to test\n        if batch_idx % 1 == 0:\n            output = OrderedDict({\n                \'test_loss\': loss_test,\n                \'test_acc\': test_acc,\n            })\n            return output\n        if batch_idx % 2 == 0:\n            return test_acc\n\n        if batch_idx % 3 == 0:\n            output = OrderedDict({\n                \'test_loss\': loss_test,\n                \'test_acc\': test_acc,\n                \'test_dic\': {\'test_loss_a\': loss_test}\n            })\n            return output\n        if batch_idx % 5 == 0:\n            output = OrderedDict({\n                f\'test_loss_{dataloader_idx}\': loss_test,\n                f\'test_acc_{dataloader_idx}\': test_acc,\n            })\n            return output\n\n    def test_step__empty(self, batch, batch_idx, *args, **kwargs):\n        return {}\n'"
tests/base/model_train_dataloaders.py,0,"b'from abc import ABC, abstractmethod\n\nfrom tests.base.dataloaders import CustomInfDataloader\n\n\nclass TrainDataloaderVariations(ABC):\n\n    @abstractmethod\n    def dataloader(self, train: bool):\n        """"""placeholder""""""\n\n    def train_dataloader(self):\n        return self.dataloader(train=True)\n\n    def train_dataloader__infinite(self):\n        return CustomInfDataloader(self.dataloader(train=True))\n\n    def train_dataloader__zero_length(self):\n        dataloader = self.dataloader(train=True)\n        dataloader.dataset.data = dataloader.dataset.data[:0]\n        dataloader.dataset.targets = dataloader.dataset.targets[:0]\n        return dataloader\n'"
tests/base/model_train_steps.py,1,"b'import math\nfrom abc import ABC\nfrom collections import OrderedDict\n\nimport torch\n\n\nclass TrainingStepVariations(ABC):\n    """"""\n    Houses all variations of training steps\n    """"""\n    test_step_inf_loss = float(\'inf\')\n\n    def training_step(self, batch, batch_idx, optimizer_idx=None):\n        """"""Lightning calls this inside the training loop""""""\n        # forward pass\n        x, y = batch\n        x = x.view(x.size(0), -1)\n\n        y_hat = self(x)\n\n        # calculate loss\n        loss_val = self.loss(y, y_hat)\n\n        # alternate possible outputs to test\n        output = OrderedDict({\n            \'loss\': loss_val,\n            \'progress_bar\': {\'some_val\': loss_val * loss_val},\n            \'log\': {\'train_some_val\': loss_val * loss_val},\n        })\n        return output\n\n    def training_step__inf_loss(self, batch, batch_idx, optimizer_idx=None):\n        output = self.training_step(batch, batch_idx, optimizer_idx)\n        if batch_idx == self.test_step_inf_loss:\n            if isinstance(output, dict):\n                output[\'loss\'] *= torch.tensor(math.inf)  # make loss infinite\n            else:\n                output /= 0\n        return output\n'"
tests/base/model_utilities.py,1,"b'from torch.utils.data import DataLoader\n\nfrom tests.base.datasets import TrialMNIST\n\n\nclass ModelTemplateData:\n    hparams: ...\n\n    def dataloader(self, train):\n        dataset = TrialMNIST(root=self.data_root, train=train, download=True)\n\n        loader = DataLoader(\n            dataset=dataset,\n            batch_size=self.batch_size,\n            num_workers=3,\n            shuffle=train,\n        )\n        return loader\n\n\nclass ModelTemplateUtils:\n\n    def get_output_metric(self, output, name):\n        if isinstance(output, dict):\n            val = output[name]\n        else:  # if it is 2level deep -> per dataloader and per batch\n            val = sum(out[name] for out in output) / len(output)\n        return val\n'"
tests/base/model_valid_dataloaders.py,0,"b'from abc import ABC, abstractmethod\n\nfrom tests.base.dataloaders import CustomInfDataloader\n\n\nclass ValDataloaderVariations(ABC):\n\n    @abstractmethod\n    def dataloader(self, train: bool):\n        """"""placeholder""""""\n\n    def val_dataloader(self):\n        return self.dataloader(train=False)\n\n    def val_dataloader__multiple(self):\n        return [self.dataloader(train=False),\n                self.dataloader(train=False)]\n\n    def val_dataloader__infinite(self):\n        return CustomInfDataloader(self.dataloader(train=False))\n'"
tests/base/model_valid_epoch_ends.py,2,"b'from abc import ABC\n\nimport torch\n\n\nclass ValidationEpochEndVariations(ABC):\n    """"""\n    Houses all variations of validation_epoch_end steps\n    """"""\n    def validation_epoch_end(self, outputs):\n        """"""\n        Called at the end of validation to aggregate outputs\n\n        Args:\n            outputs: list of individual outputs of each validation step\n        """"""\n        # if returned a scalar from validation_step, outputs is a list of tensor scalars\n        # we return just the average in this case (if we want)\n        def _mean(res, key):\n            # recursive mean for multilevel dicts\n            return torch.stack([x[key] if isinstance(x, dict) else _mean(x, key) for x in res]).mean()\n\n        val_loss_mean = _mean(outputs, \'val_loss\')\n        val_acc_mean = _mean(outputs, \'val_acc\')\n\n        metrics_dict = {\'val_loss\': val_loss_mean.item(), \'val_acc\': val_acc_mean.item()}\n        results = {\'progress_bar\': metrics_dict, \'log\': metrics_dict}\n        return results\n\n    def validation_epoch_end_multiple_dataloaders(self, outputs):\n        """"""\n        Called at the end of validation to aggregate outputs\n\n        Args:\n            outputs: list of individual outputs of each validation step\n        """"""\n\n        # if returned a scalar from validation_step, outputs is a list of tensor scalars\n        # we return just the average in this case (if we want)\n        def _mean(res, key):\n            return torch.stack([x[key] for x in res]).mean()\n\n        pbar = {}\n        logs = {}\n        for dl_output_list in outputs:\n            output_keys = dl_output_list[0].keys()\n            output_keys = [x for x in output_keys if \'val_\' in x]\n            for key in output_keys:\n                metric_out = _mean(dl_output_list, key)\n                pbar[key] = metric_out\n                logs[key] = metric_out\n\n        results = {\'progress_bar\': pbar, \'log\': logs}\n        return results\n'"
tests/base/model_valid_steps.py,6,"b'from abc import ABC\nfrom collections import OrderedDict\n\nimport torch\n\n\nclass ValidationStepVariations(ABC):\n    """"""\n    Houses all variations of validation steps\n    """"""\n    def validation_step(self, batch, batch_idx, *args, **kwargs):\n        """"""\n        Lightning calls this inside the validation loop\n        :param batch:\n        :return:\n        """"""\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat = self(x)\n\n        loss_val = self.loss(y, y_hat)\n\n        # acc\n        labels_hat = torch.argmax(y_hat, dim=1)\n        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n        val_acc = torch.tensor(val_acc).type_as(x)\n\n        output = OrderedDict({\n            \'val_loss\': loss_val,\n            \'val_acc\': val_acc,\n            \'test_dic\': {\'val_loss_a\': loss_val}\n        })\n        return output\n\n    def validation_step__multiple_dataloaders(self, batch, batch_idx, dataloader_idx, **kwargs):\n        """"""\n        Lightning calls this inside the validation loop\n        :param batch:\n        :return:\n        """"""\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat = self(x)\n\n        loss_val = self.loss(y, y_hat)\n\n        # acc\n        labels_hat = torch.argmax(y_hat, dim=1)\n        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n        val_acc = torch.tensor(val_acc).type_as(x)\n\n        output = OrderedDict({\n            f\'val_loss_{dataloader_idx}\': loss_val,\n            f\'val_acc_{dataloader_idx}\': val_acc,\n        })\n        return output\n'"
tests/base/models.py,9,"b'from collections import OrderedDict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom tests.base.datasets import TrialMNIST\n\ntry:\n    from test_tube import HyperOptArgumentParser\nexcept ImportError:\n    # TODO: this should be discussed and moved out of this package\n    raise ImportError(\'Missing test-tube package.\')\n\nfrom pytorch_lightning.core.lightning import LightningModule\n\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim: tuple, img_shape: tuple):\n        super().__init__()\n        self.img_shape = img_shape\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *self.img_shape)\n        return img\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_shape: tuple):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n\n        return validity\n\n\nclass TestGAN(LightningModule):\n    """"""Implements a basic GAN for the purpose of illustrating multiple optimizers.""""""\n\n    def __init__(self, hidden_dim, learning_rate, b1, b2, **kwargs):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.learning_rate = learning_rate\n        self.b1 = b1\n        self.b2 = b2\n\n        # networks\n        mnist_shape = (1, 28, 28)\n        self.generator = Generator(latent_dim=self.hidden_dim, img_shape=mnist_shape)\n        self.discriminator = Discriminator(img_shape=mnist_shape)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_idx, optimizer_idx=None):\n        imgs, _ = batch\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hidden_dim)\n            z = z.type_as(imgs)\n\n            # generate images\n            self.generated_imgs = self(z)\n\n            # ground truth result (ie: all fake)\n            # put on GPU because we created this tensor inside training_loop\n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n\n            # adversarial loss is binary cross-entropy\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), valid)\n            tqdm_dict = {\'g_loss\': g_loss}\n            output = OrderedDict({\n                \'loss\': g_loss,\n                \'progress_bar\': tqdm_dict,\n                \'log\': tqdm_dict\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator\'s ability to classify real from generated samples\n\n            # how well can it label as real?\n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n\n            real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n\n            # how well can it label as fake?\n            fake = torch.zeros(imgs.size(0), 1)\n            fake = fake.type_as(fake)\n\n            fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), fake)\n\n            # discriminator loss is the average of these\n            d_loss = (real_loss + fake_loss) / 2\n            tqdm_dict = {\'d_loss\': d_loss}\n            output = OrderedDict({\n                \'loss\': d_loss,\n                \'progress_bar\': tqdm_dict,\n                \'log\': tqdm_dict\n            })\n            return output\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        b1 = self.b1\n        b2 = self.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        return DataLoader(TrialMNIST(train=True, download=True), batch_size=16)\n'"
tests/base/utils.py,4,"b'import os\n\nimport numpy as np\nimport torch\n\n# from pl_examples import LightningTemplateModel\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom tests import TEMP_PATH, RANDOM_PORTS, RANDOM_SEEDS\nfrom tests.base.model_template import EvalModelTemplate\n\n\ndef assert_speed_parity_relative(pl_times, pt_times, max_diff: float = 0.1):\n    # assert speeds\n    diffs = np.asarray(pl_times) - np.asarray(pt_times)\n    # norm by vanila time\n    diffs = diffs / np.asarray(pt_times)\n    assert np.alltrue(diffs < max_diff), \\\n        f""lightning {diffs} was slower than PT (threshold {max_diff})""\n\n\ndef assert_speed_parity_absolute(pl_times, pt_times, nb_epochs, max_diff: float = 0.6):\n    # assert speeds\n    diffs = np.asarray(pl_times) - np.asarray(pt_times)\n    # norm by vanila time\n    diffs = diffs / nb_epochs\n    assert np.alltrue(diffs < max_diff), \\\n        f""lightning {diffs} was slower than PT (threshold {max_diff})""\n\n\ndef run_model_test_without_loggers(trainer_options, model, min_acc: float = 0.50):\n    reset_seed()\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n\n    # correct result and ok accuracy\n    assert result == 1, \'amp + ddp model failed to complete\'\n\n    # test model loading\n    pretrained_model = load_model(trainer.logger,\n                                  trainer.checkpoint_callback.dirpath,\n                                  path_expt=trainer_options.get(\'default_root_dir\'))\n\n    # test new model accuracy\n    test_loaders = model.test_dataloader()\n    if not isinstance(test_loaders, list):\n        test_loaders = [test_loaders]\n\n    for dataloader in test_loaders:\n        run_prediction(dataloader, pretrained_model, min_acc=min_acc)\n\n    if trainer.use_ddp:\n        # on hpc this would work fine... but need to hack it for the purpose of the test\n        trainer.model = pretrained_model\n        trainer.optimizers, trainer.lr_schedulers = pretrained_model.configure_optimizers()\n\n\ndef run_model_test(trainer_options, model, on_gpu: bool = True, version=None, with_hpc: bool = True):\n    reset_seed()\n    save_dir = trainer_options[\'default_root_dir\']\n\n    # logger file to get meta\n    logger = get_default_logger(save_dir, version=version)\n    trainer_options.update(logger=logger)\n\n    if \'checkpoint_callback\' not in trainer_options:\n        # logger file to get weights\n        checkpoint = init_checkpoint_callback(logger)\n        trainer_options.update(checkpoint_callback=checkpoint)\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n\n    # correct result and ok accuracy\n    assert result == 1, \'amp + ddp model failed to complete\'\n\n    # test model loading\n    pretrained_model = load_model(logger, trainer.checkpoint_callback.dirpath)\n\n    # test new model accuracy\n    test_loaders = model.test_dataloader()\n    if not isinstance(test_loaders, list):\n        test_loaders = [test_loaders]\n\n    [run_prediction(dataloader, pretrained_model) for dataloader in test_loaders]\n\n    if with_hpc:\n        if trainer.use_ddp or trainer.use_ddp2:\n            # on hpc this would work fine... but need to hack it for the purpose of the test\n            trainer.model = pretrained_model\n            trainer.optimizers, trainer.lr_schedulers, trainer.optimizer_frequencies = \\\n                trainer.init_optimizers(pretrained_model)\n\n        # test HPC loading / saving\n        trainer.hpc_save(save_dir, logger)\n        trainer.hpc_load(save_dir, on_gpu=on_gpu)\n\n\ndef get_default_logger(save_dir, version=None):\n    # set up logger object without actually saving logs\n    logger = TensorBoardLogger(save_dir, name=\'lightning_logs\', version=version)\n    return logger\n\n\ndef get_data_path(expt_logger, path_dir=None):\n    # some calls contain only experiment not complete logger\n    expt = expt_logger.experiment if hasattr(expt_logger, \'experiment\') else expt_logger\n    # each logger has to have these attributes\n    name, version = expt_logger.name, expt_logger.version\n    # only the test-tube experiment has such attribute\n    if hasattr(expt, \'get_data_path\'):\n        return expt.get_data_path(name, version)\n    # the other experiments...\n    if not path_dir:\n        if hasattr(expt_logger, \'save_dir\') and expt_logger.save_dir:\n            path_dir = expt_logger.save_dir\n        else:\n            path_dir = TEMP_PATH\n    path_expt = os.path.join(path_dir, name, \'version_%s\' % version)\n    # try if the new sub-folder exists, typical case for test-tube\n    if not os.path.isdir(path_expt):\n        path_expt = path_dir\n    return path_expt\n\n\ndef load_model(logger, root_weights_dir, module_class=EvalModelTemplate, path_expt=None):\n    # load trained model\n    path_expt_dir = get_data_path(logger, path_dir=path_expt)\n    hparams_path = os.path.join(path_expt_dir, TensorBoardLogger.NAME_HPARAMS_FILE)\n\n    checkpoints = [x for x in os.listdir(root_weights_dir) if \'.ckpt\' in x]\n    weights_dir = os.path.join(root_weights_dir, checkpoints[0])\n\n    trained_model = module_class.load_from_checkpoint(\n        checkpoint_path=weights_dir,\n        hparams_file=hparams_path\n    )\n\n    assert trained_model is not None, \'loading model failed\'\n\n    return trained_model\n\n\ndef load_model_from_checkpoint(root_weights_dir, module_class=EvalModelTemplate):\n    # load trained model\n    checkpoints = [x for x in os.listdir(root_weights_dir) if \'.ckpt\' in x]\n    weights_dir = os.path.join(root_weights_dir, checkpoints[0])\n\n    trained_model = module_class.load_from_checkpoint(\n        checkpoint_path=weights_dir,\n    )\n\n    assert trained_model is not None, \'loading model failed\'\n\n    return trained_model\n\n\ndef run_prediction(dataloader, trained_model, dp=False, min_acc=0.50):\n    # run prediction on 1 batch\n    for batch in dataloader:\n        break\n\n    x, y = batch\n    x = x.view(x.size(0), -1)\n\n    if dp:\n        output = trained_model(batch, 0)\n        acc = output[\'val_acc\']\n        acc = torch.mean(acc).item()\n\n    else:\n        y_hat = trained_model(x)\n\n        # acc\n        labels_hat = torch.argmax(y_hat, dim=1)\n        acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n        acc = torch.tensor(acc)\n        acc = acc.item()\n\n    assert acc >= min_acc, f""This model is expected to get > {min_acc} in test set (it got {acc})""\n\n\ndef assert_ok_model_acc(trainer, key=\'test_acc\', thr=0.5):\n    # this model should get 0.80+ acc\n    acc = trainer.progress_bar_dict[key]\n    assert acc > thr, f""Model failed to get expected {thr} accuracy. {key} = {acc}""\n\n\ndef reset_seed():\n    seed = RANDOM_SEEDS.pop()\n    seed_everything(seed)\n\n\ndef set_random_master_port():\n    reset_seed()\n    port = RANDOM_PORTS.pop()\n    os.environ[\'MASTER_PORT\'] = str(port)\n\n\ndef init_checkpoint_callback(logger, path_dir=None):\n    exp_path = get_data_path(logger, path_dir=path_dir)\n    ckpt_dir = os.path.join(exp_path, \'checkpoints\')\n    os.mkdir(ckpt_dir)\n    checkpoint = ModelCheckpoint(ckpt_dir)\n    return checkpoint\n'"
tests/callbacks/__init__.py,0,b''
tests/callbacks/test_callbacks.py,1,"b'from pathlib import Path\n\nimport pytest\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning import Trainer, LightningModule\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom tests.base import EvalModelTemplate\nimport torch\n\n\ndef test_early_stopping_functionality(tmpdir):\n\n    class CurrentModel(EvalModelTemplate):\n        def validation_epoch_end(self, outputs):\n            losses = [8, 4, 2, 3, 4, 5, 8, 10]\n            val_loss = losses[self.current_epoch]\n            val_loss = torch.tensor(val_loss)\n            return {\'val_loss\': val_loss}\n\n    model = CurrentModel()\n\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        early_stop_callback=True,\n        overfit_pct=0.20,\n        max_epochs=20,\n    )\n    result = trainer.fit(model)\n    print(trainer.current_epoch)\n\n    assert trainer.current_epoch == 5, \'early_stopping failed\'\n\n\ndef test_trainer_callback_system(tmpdir):\n    """"""Test the callback system.""""""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    def _check_args(trainer, pl_module):\n        assert isinstance(trainer, Trainer)\n        assert isinstance(pl_module, LightningModule)\n\n    class TestCallback(Callback):\n        def __init__(self):\n            super().__init__()\n            self.on_init_start_called = False\n            self.on_init_end_called = False\n            self.on_sanity_check_start_called = False\n            self.on_sanity_check_end_called = False\n            self.on_epoch_start_called = False\n            self.on_epoch_end_called = False\n            self.on_batch_start_called = False\n            self.on_batch_end_called = False\n            self.on_validation_batch_start_called = False\n            self.on_validation_batch_end_called = False\n            self.on_test_batch_start_called = False\n            self.on_test_batch_end_called = False\n            self.on_train_start_called = False\n            self.on_train_end_called = False\n            self.on_validation_start_called = False\n            self.on_validation_end_called = False\n            self.on_test_start_called = False\n            self.on_test_end_called = False\n\n        def on_init_start(self, trainer):\n            assert isinstance(trainer, Trainer)\n            self.on_init_start_called = True\n\n        def on_init_end(self, trainer):\n            assert isinstance(trainer, Trainer)\n            self.on_init_end_called = True\n\n        def on_sanity_check_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_sanity_check_start_called = True\n\n        def on_sanity_check_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_sanity_check_end_called = True\n\n        def on_epoch_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_epoch_start_called = True\n\n        def on_epoch_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_epoch_end_called = True\n\n        def on_batch_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_batch_start_called = True\n\n        def on_batch_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_batch_end_called = True\n\n        def on_validation_batch_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_validation_batch_start_called = True\n\n        def on_validation_batch_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_validation_batch_end_called = True\n\n        def on_test_batch_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_test_batch_start_called = True\n\n        def on_test_batch_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_test_batch_end_called = True\n\n        def on_train_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_train_start_called = True\n\n        def on_train_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_train_end_called = True\n\n        def on_validation_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_validation_start_called = True\n\n        def on_validation_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_validation_end_called = True\n\n        def on_test_start(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_test_start_called = True\n\n        def on_test_end(self, trainer, pl_module):\n            _check_args(trainer, pl_module)\n            self.on_test_end_called = True\n\n    test_callback = TestCallback()\n\n    trainer_options = dict(\n        callbacks=[test_callback],\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2,\n        progress_bar_refresh_rate=0,\n    )\n\n    assert not test_callback.on_init_start_called\n    assert not test_callback.on_init_end_called\n    assert not test_callback.on_sanity_check_start_called\n    assert not test_callback.on_sanity_check_end_called\n    assert not test_callback.on_epoch_start_called\n    assert not test_callback.on_epoch_start_called\n    assert not test_callback.on_batch_start_called\n    assert not test_callback.on_batch_end_called\n    assert not test_callback.on_validation_batch_start_called\n    assert not test_callback.on_validation_batch_end_called\n    assert not test_callback.on_test_batch_start_called\n    assert not test_callback.on_test_batch_end_called\n    assert not test_callback.on_train_start_called\n    assert not test_callback.on_train_end_called\n    assert not test_callback.on_validation_start_called\n    assert not test_callback.on_validation_end_called\n    assert not test_callback.on_test_start_called\n    assert not test_callback.on_test_end_called\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n\n    assert trainer.callbacks[0] == test_callback\n    assert test_callback.on_init_start_called\n    assert test_callback.on_init_end_called\n    assert not test_callback.on_sanity_check_start_called\n    assert not test_callback.on_sanity_check_end_called\n    assert not test_callback.on_epoch_start_called\n    assert not test_callback.on_epoch_start_called\n    assert not test_callback.on_batch_start_called\n    assert not test_callback.on_batch_end_called\n    assert not test_callback.on_validation_batch_start_called\n    assert not test_callback.on_validation_batch_end_called\n    assert not test_callback.on_test_batch_start_called\n    assert not test_callback.on_test_batch_end_called\n    assert not test_callback.on_train_start_called\n    assert not test_callback.on_train_end_called\n    assert not test_callback.on_validation_start_called\n    assert not test_callback.on_validation_end_called\n    assert not test_callback.on_test_start_called\n    assert not test_callback.on_test_end_called\n\n    trainer.fit(model)\n\n    assert test_callback.on_init_start_called\n    assert test_callback.on_init_end_called\n    assert test_callback.on_sanity_check_start_called\n    assert test_callback.on_sanity_check_end_called\n    assert test_callback.on_epoch_start_called\n    assert test_callback.on_epoch_start_called\n    assert test_callback.on_batch_start_called\n    assert test_callback.on_batch_end_called\n    assert test_callback.on_validation_batch_start_called\n    assert test_callback.on_validation_batch_end_called\n    assert test_callback.on_train_start_called\n    assert test_callback.on_train_end_called\n    assert test_callback.on_validation_start_called\n    assert test_callback.on_validation_end_called\n    assert not test_callback.on_test_batch_start_called\n    assert not test_callback.on_test_batch_end_called\n    assert not test_callback.on_test_start_called\n    assert not test_callback.on_test_end_called\n\n    test_callback = TestCallback()\n    trainer_options.update(callbacks=[test_callback])\n    trainer = Trainer(**trainer_options)\n    trainer.test(model)\n\n    assert test_callback.on_test_batch_start_called\n    assert test_callback.on_test_batch_end_called\n    assert test_callback.on_test_start_called\n    assert test_callback.on_test_end_called\n    assert not test_callback.on_validation_start_called\n    assert not test_callback.on_validation_end_called\n    assert not test_callback.on_validation_batch_end_called\n    assert not test_callback.on_validation_batch_start_called\n\n\ndef test_early_stopping_no_val_step(tmpdir):\n    """"""Test that early stopping callback falls back to training metrics when no validation defined.""""""\n\n    class CurrentModel(EvalModelTemplate):\n        def training_step(self, *args, **kwargs):\n            output = super().training_step(*args, **kwargs)\n            output.update({\'my_train_metric\': output[\'loss\']})  # could be anything else\n            return output\n\n    model = CurrentModel()\n    model.validation_step = None\n    model.val_dataloader = None\n\n    stopping = EarlyStopping(monitor=\'my_train_metric\', min_delta=0.1)\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        early_stop_callback=stopping,\n        overfit_pct=0.20,\n        max_epochs=2,\n    )\n    result = trainer.fit(model)\n\n    assert result == 1, \'training failed to complete\'\n    assert trainer.current_epoch < trainer.max_epochs\n\n\ndef test_pickling(tmpdir):\n    import pickle\n    early_stopping = EarlyStopping()\n    ckpt = ModelCheckpoint(tmpdir)\n\n    early_stopping_pickled = pickle.dumps(early_stopping)\n    ckpt_pickled = pickle.dumps(ckpt)\n\n    early_stopping_loaded = pickle.loads(early_stopping_pickled)\n    ckpt_loaded = pickle.loads(ckpt_pickled)\n\n    assert vars(early_stopping) == vars(early_stopping_loaded)\n    assert vars(ckpt) == vars(ckpt_loaded)\n\n\n@pytest.mark.parametrize(\'save_top_k\', [-1, 0, 1, 2])\ndef test_model_checkpoint_with_non_string_input(tmpdir, save_top_k):\n    """""" Test that None in checkpoint callback is valid and that chkp_path is set correctly """"""\n    tutils.reset_seed()\n    model = EvalModelTemplate()\n\n    checkpoint = ModelCheckpoint(filepath=None, save_top_k=save_top_k)\n\n    trainer = Trainer(default_root_dir=tmpdir,\n                      checkpoint_callback=checkpoint,\n                      overfit_pct=0.20,\n                      max_epochs=2\n                      )\n    trainer.fit(model)\n\n    # These should be different if the dirpath has be overridden\n    assert trainer.ckpt_path != trainer.default_root_dir\n\n\n@pytest.mark.parametrize(\n    \'logger_version,expected\',\n    [(None, \'version_0\'), (1, \'version_1\'), (\'awesome\', \'awesome\')],\n)\ndef test_model_checkpoint_path(tmpdir, logger_version, expected):\n    """"""Test that ""version_"" prefix is only added when logger\'s version is an integer""""""\n    tutils.reset_seed()\n    model = EvalModelTemplate()\n    logger = TensorBoardLogger(str(tmpdir), version=logger_version)\n\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        overfit_pct=0.2,\n        max_epochs=2,\n        logger=logger\n    )\n    trainer.fit(model)\n\n    ckpt_version = Path(trainer.ckpt_path).parent.name\n    assert ckpt_version == expected\n'"
tests/callbacks/test_lr.py,0,"b'import pytest\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import LearningRateLogger\nfrom tests.base import EvalModelTemplate\n\n\ndef test_lr_logger_single_lr(tmpdir):\n    """""" Test that learning rates are extracted and logged for single lr scheduler. """"""\n    tutils.reset_seed()\n\n    model = EvalModelTemplate()\n    model.configure_optimizers = model.configure_optimizers__single_scheduler\n\n    lr_logger = LearningRateLogger()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=2,\n        val_percent_check=0.1,\n        train_percent_check=0.5,\n        callbacks=[lr_logger]\n    )\n    result = trainer.fit(model)\n    assert result\n\n    assert lr_logger.lrs, \'No learning rates logged\'\n    assert len(lr_logger.lrs) == len(trainer.lr_schedulers), \\\n        \'Number of learning rates logged does not match number of lr schedulers\'\n    assert all([k in [\'lr-Adam\'] for k in lr_logger.lrs.keys()]), \\\n        \'Names of learning rates not set correctly\'\n\n\ndef test_lr_logger_no_lr(tmpdir):\n    tutils.reset_seed()\n\n    model = EvalModelTemplate()\n\n    lr_logger = LearningRateLogger()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=2,\n        val_percent_check=0.1,\n        train_percent_check=0.5,\n        callbacks=[lr_logger]\n    )\n\n    with pytest.warns(RuntimeWarning):\n        result = trainer.fit(model)\n        assert result\n\n\ndef test_lr_logger_multi_lrs(tmpdir):\n    """""" Test that learning rates are extracted and logged for multi lr schedulers. """"""\n    tutils.reset_seed()\n\n    model = EvalModelTemplate()\n    model.configure_optimizers = model.configure_optimizers__multiple_schedulers\n\n    lr_logger = LearningRateLogger()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=2,\n        val_percent_check=0.1,\n        train_percent_check=0.5,\n        callbacks=[lr_logger]\n    )\n    result = trainer.fit(model)\n    assert result\n\n    assert lr_logger.lrs, \'No learning rates logged\'\n    assert len(lr_logger.lrs) == len(trainer.lr_schedulers), \\\n        \'Number of learning rates logged does not match number of lr schedulers\'\n    assert all([k in [\'lr-Adam\', \'lr-Adam-1\'] for k in lr_logger.lrs.keys()]), \\\n        \'Names of learning rates not set correctly\'\n    assert all(len(lr) == trainer.max_epochs for k, lr in lr_logger.lrs.items()), \\\n        \'Length of logged learning rates exceeds the number of epochs\'\n\n\ndef test_lr_logger_param_groups(tmpdir):\n    """""" Test that learning rates are extracted and logged for single lr scheduler. """"""\n    tutils.reset_seed()\n\n    model = EvalModelTemplate()\n    model.configure_optimizers = model.configure_optimizers__param_groups\n\n    lr_logger = LearningRateLogger()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=2,\n        val_percent_check=0.1,\n        train_percent_check=0.5,\n        callbacks=[lr_logger]\n    )\n    result = trainer.fit(model)\n    assert result\n\n    assert lr_logger.lrs, \'No learning rates logged\'\n    assert len(lr_logger.lrs) == 2 * len(trainer.lr_schedulers), \\\n        \'Number of learning rates logged does not match number of param groups\'\n    assert all([k in [\'lr-Adam/pg1\', \'lr-Adam/pg2\'] for k in lr_logger.lrs.keys()]), \\\n        \'Names of learning rates not set correctly\'\n'"
tests/callbacks/test_progress_bar.py,0,"b'import pytest\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ProgressBarBase, ProgressBar, ModelCheckpoint\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\n\n@pytest.mark.parametrize(\'callbacks,refresh_rate\', [\n    ([], 1),\n    ([], 2),\n    ([ProgressBar(refresh_rate=1)], 0),\n    ([ProgressBar(refresh_rate=2)], 0),\n    ([ProgressBar(refresh_rate=2)], 1),\n])\ndef test_progress_bar_on(callbacks, refresh_rate):\n    """"""Test different ways the progress bar can be turned on.""""""\n\n    trainer = Trainer(\n        callbacks=callbacks,\n        progress_bar_refresh_rate=refresh_rate,\n        max_epochs=1,\n        overfit_pct=0.2,\n    )\n\n    progress_bars = [c for c in trainer.callbacks if isinstance(c, ProgressBarBase)]\n    # Trainer supports only a single progress bar callback at the moment\n    assert len(progress_bars) == 1\n    assert progress_bars[0] is trainer.progress_bar_callback\n\n\n@pytest.mark.parametrize(\'callbacks,refresh_rate\', [\n    ([], 0),\n    ([], False),\n    ([ModelCheckpoint(\'../trainer\')], 0),\n])\ndef test_progress_bar_off(callbacks, refresh_rate):\n    """"""Test different ways the progress bar can be turned off.""""""\n\n    trainer = Trainer(\n        callbacks=callbacks,\n        progress_bar_refresh_rate=refresh_rate,\n    )\n\n    progress_bars = [c for c in trainer.callbacks if isinstance(c, ProgressBar)]\n    assert 0 == len(progress_bars)\n    assert not trainer.progress_bar_callback\n\n\ndef test_progress_bar_misconfiguration():\n    """"""Test that Trainer doesn\'t accept multiple progress bars.""""""\n    callbacks = [ProgressBar(), ProgressBar(), ModelCheckpoint(\'../trainer\')]\n    with pytest.raises(MisconfigurationException, match=r\'^You added multiple progress bar callbacks\'):\n        Trainer(callbacks=callbacks)\n\n\ndef test_progress_bar_totals():\n    """"""Test that the progress finishes with the correct total steps processed.""""""\n\n    model = EvalModelTemplate()\n\n    trainer = Trainer(\n        progress_bar_refresh_rate=1,\n        val_percent_check=1.0,\n        max_epochs=1,\n    )\n    bar = trainer.progress_bar_callback\n    assert 0 == bar.total_train_batches\n    assert 0 == bar.total_val_batches\n    assert 0 == bar.total_test_batches\n\n    trainer.fit(model)\n\n    # check main progress bar total\n    n = bar.total_train_batches\n    m = bar.total_val_batches\n    assert len(trainer.train_dataloader) == n\n    assert bar.main_progress_bar.total == n + m\n\n    # check val progress bar total\n    assert sum(len(loader) for loader in trainer.val_dataloaders) == m\n    assert bar.val_progress_bar.total == m\n\n    # main progress bar should have reached the end (train batches + val batches)\n    assert bar.main_progress_bar.n == n + m\n    assert bar.train_batch_idx == n\n\n    # val progress bar should have reached the end\n    assert bar.val_progress_bar.n == m\n    assert bar.val_batch_idx == m\n\n    # check that the test progress bar is off\n    assert 0 == bar.total_test_batches\n    assert bar.test_progress_bar is None\n\n    trainer.test(model)\n\n    # check test progress bar total\n    k = bar.total_test_batches\n    assert sum(len(loader) for loader in trainer.test_dataloaders) == k\n    assert bar.test_progress_bar.total == k\n\n    # test progress bar should have reached the end\n    assert bar.test_progress_bar.n == k\n    assert bar.test_batch_idx == k\n\n\ndef test_progress_bar_fast_dev_run():\n    model = EvalModelTemplate()\n\n    trainer = Trainer(\n        fast_dev_run=True,\n    )\n\n    progress_bar = trainer.progress_bar_callback\n    assert 1 == progress_bar.total_train_batches\n    # total val batches are known only after val dataloaders have reloaded\n\n    trainer.fit(model)\n\n    assert 1 == progress_bar.total_val_batches\n    assert 1 == progress_bar.train_batch_idx\n    assert 1 == progress_bar.val_batch_idx\n    assert 0 == progress_bar.test_batch_idx\n\n    # the main progress bar should display 2 batches (1 train, 1 val)\n    assert 2 == progress_bar.main_progress_bar.total\n    assert 2 == progress_bar.main_progress_bar.n\n\n    trainer.test(model)\n\n    # the test progress bar should display 1 batch\n    assert 1 == progress_bar.test_batch_idx\n    assert 1 == progress_bar.test_progress_bar.total\n    assert 1 == progress_bar.test_progress_bar.n\n\n\n@pytest.mark.parametrize(\'refresh_rate\', [0, 1, 50])\ndef test_progress_bar_progress_refresh(refresh_rate):\n    """"""Test that the three progress bars get correctly updated when using different refresh rates.""""""\n\n    model = EvalModelTemplate()\n\n    class CurrentProgressBar(ProgressBar):\n\n        train_batches_seen = 0\n        val_batches_seen = 0\n        test_batches_seen = 0\n\n        def on_batch_start(self, trainer, pl_module):\n            super().on_batch_start(trainer, pl_module)\n            assert self.train_batch_idx == trainer.batch_idx\n\n        def on_batch_end(self, trainer, pl_module):\n            super().on_batch_end(trainer, pl_module)\n            assert self.train_batch_idx == trainer.batch_idx + 1\n            if not self.is_disabled and self.train_batch_idx % self.refresh_rate == 0:\n                assert self.main_progress_bar.n == self.train_batch_idx\n            self.train_batches_seen += 1\n\n        def on_validation_batch_end(self, trainer, pl_module):\n            super().on_validation_batch_end(trainer, pl_module)\n            if not self.is_disabled and self.val_batch_idx % self.refresh_rate == 0:\n                assert self.val_progress_bar.n == self.val_batch_idx\n            self.val_batches_seen += 1\n\n        def on_test_batch_end(self, trainer, pl_module):\n            super().on_test_batch_end(trainer, pl_module)\n            if not self.is_disabled and self.test_batch_idx % self.refresh_rate == 0:\n                assert self.test_progress_bar.n == self.test_batch_idx\n            self.test_batches_seen += 1\n\n    progress_bar = CurrentProgressBar(refresh_rate=refresh_rate)\n    trainer = Trainer(\n        callbacks=[progress_bar],\n        progress_bar_refresh_rate=101,  # should not matter if custom callback provided\n        train_percent_check=1.0,\n        num_sanity_val_steps=2,\n        max_epochs=3,\n    )\n    assert trainer.progress_bar_callback.refresh_rate == refresh_rate\n\n    trainer.fit(model)\n    assert progress_bar.train_batches_seen == 3 * progress_bar.total_train_batches\n    assert progress_bar.val_batches_seen == 3 * progress_bar.total_val_batches + trainer.num_sanity_val_steps\n\n    trainer.test(model)\n    assert progress_bar.test_batches_seen == progress_bar.total_test_batches\n'"
tests/loggers/__init__.py,0,b''
tests/loggers/test_all.py,0,"b'import inspect\nimport pickle\n\nimport pytest\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import (\n    TensorBoardLogger, MLFlowLogger, NeptuneLogger, TestTubeLogger, CometLogger)\nfrom tests.base import EvalModelTemplate\n\n\ndef _get_logger_args(logger_class, save_dir):\n    logger_args = {}\n    if \'save_dir\' in inspect.getfullargspec(logger_class).args:\n        logger_args.update(save_dir=str(save_dir))\n    if \'offline_mode\' in inspect.getfullargspec(logger_class).args:\n        logger_args.update(offline_mode=True)\n    return logger_args\n\n\n@pytest.mark.parametrize(""logger_class"", [\n    TensorBoardLogger,\n    CometLogger,\n    MLFlowLogger,\n    NeptuneLogger,\n    TestTubeLogger,\n    # TrainsLogger,  # TODO: add this one\n    # WandbLogger,  # TODO: add this one\n])\ndef test_loggers_fit_test(tmpdir, monkeypatch, logger_class):\n    """"""Verify that basic functionality of all loggers.""""""\n    # prevent comet logger from trying to print at exit, since\n    # pytest\'s stdout/stderr redirection breaks it\n    import atexit\n    monkeypatch.setattr(atexit, \'register\', lambda _: None)\n\n    model = EvalModelTemplate()\n\n    class StoreHistoryLogger(logger_class):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.history = []\n\n        def log_metrics(self, metrics, step):\n            super().log_metrics(metrics, step)\n            self.history.append((step, metrics))\n\n    logger_args = _get_logger_args(logger_class, tmpdir)\n    logger = StoreHistoryLogger(**logger_args)\n\n    trainer = Trainer(\n        max_epochs=1,\n        logger=logger,\n        train_percent_check=0.2,\n        val_percent_check=0.5,\n        fast_dev_run=True,\n    )\n    trainer.fit(model)\n\n    trainer.test()\n\n    log_metric_names = [(s, sorted(m.keys())) for s, m in logger.history]\n    assert log_metric_names == [(0, [\'epoch\', \'val_acc\', \'val_loss\']),\n                                (0, [\'epoch\', \'train_some_val\']),\n                                (1, [\'epoch\', \'test_acc\', \'test_loss\'])]\n\n\n@pytest.mark.parametrize(""logger_class"", [\n    TensorBoardLogger,\n    CometLogger,\n    MLFlowLogger,\n    NeptuneLogger,\n    TestTubeLogger,\n    # TrainsLogger,  # TODO: add this one\n    # WandbLogger,  # TODO: add this one\n])\ndef test_loggers_pickle(tmpdir, monkeypatch, logger_class):\n    """"""Verify that pickling trainer with logger works.""""""\n    # prevent comet logger from trying to print at exit, since\n    # pytest\'s stdout/stderr redirection breaks it\n    import atexit\n    monkeypatch.setattr(atexit, \'register\', lambda _: None)\n\n    logger_args = _get_logger_args(logger_class, tmpdir)\n    logger = logger_class(**logger_args)\n\n    # test pickling loggers\n    pickle.dumps(logger)\n\n    trainer = Trainer(\n        max_epochs=1,\n        logger=logger\n    )\n    pkl_bytes = pickle.dumps(trainer)\n\n    trainer2 = pickle.loads(pkl_bytes)\n    trainer2.logger.log_metrics({\'acc\': 1.0})\n\n\n@pytest.mark.parametrize(""extra_params"", [\n    pytest.param(dict(max_epochs=1, auto_scale_batch_size=True), id=\'Batch-size-Finder\'),\n    pytest.param(dict(max_epochs=3, auto_lr_find=True), id=\'LR-Finder\'),\n])\ndef test_logger_reset_correctly(tmpdir, extra_params):\n    """""" Test that the tuners do not alter the logger reference """"""\n    tutils.reset_seed()\n\n    model = EvalModelTemplate()\n\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        **extra_params\n    )\n    logger1 = trainer.logger\n    trainer.fit(model)\n    logger2 = trainer.logger\n    logger3 = model.logger\n\n    assert logger1 == logger2, \\\n        \'Finder altered the logger of trainer\'\n    assert logger2 == logger3, \\\n        \'Finder altered the logger of model\'\n'"
tests/loggers/test_base.py,0,"b'import pickle\nfrom unittest.mock import MagicMock\n\nimport numpy as np\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import LightningLoggerBase, LoggerCollection\nfrom pytorch_lightning.utilities import rank_zero_only\nfrom tests.base import EvalModelTemplate\n\n\ndef test_logger_collection():\n    mock1 = MagicMock()\n    mock2 = MagicMock()\n\n    logger = LoggerCollection([mock1, mock2])\n\n    assert logger[0] == mock1\n    assert logger[1] == mock2\n\n    assert logger.experiment[0] == mock1.experiment\n    assert logger.experiment[1] == mock2.experiment\n\n    logger.close()\n    mock1.close.assert_called_once()\n    mock2.close.assert_called_once()\n\n\nclass CustomLogger(LightningLoggerBase):\n    def __init__(self):\n        super().__init__()\n        self.hparams_logged = None\n        self.metrics_logged = None\n        self.finalized = False\n\n    @property\n    def experiment(self):\n        return \'test\'\n\n    @rank_zero_only\n    def log_hyperparams(self, params):\n        self.hparams_logged = params\n\n    @rank_zero_only\n    def log_metrics(self, metrics, step):\n        self.metrics_logged = metrics\n\n    @rank_zero_only\n    def finalize(self, status):\n        self.finalized_status = status\n\n    @property\n    def name(self):\n        return ""name""\n\n    @property\n    def version(self):\n        return ""1""\n\n\ndef test_custom_logger(tmpdir):\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    logger = CustomLogger()\n\n    trainer = Trainer(\n        max_epochs=1,\n        train_percent_check=0.05,\n        logger=logger,\n        default_root_dir=tmpdir\n    )\n    result = trainer.fit(model)\n    assert result == 1, ""Training failed""\n    assert logger.hparams_logged == hparams\n    assert logger.metrics_logged != {}\n    assert logger.finalized_status == ""success""\n\n\ndef test_multiple_loggers(tmpdir):\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    logger1 = CustomLogger()\n    logger2 = CustomLogger()\n\n    trainer = Trainer(\n        max_epochs=1,\n        train_percent_check=0.05,\n        logger=[logger1, logger2],\n        default_root_dir=tmpdir\n    )\n    result = trainer.fit(model)\n    assert result == 1, ""Training failed""\n\n    assert logger1.hparams_logged == hparams\n    assert logger1.metrics_logged != {}\n    assert logger1.finalized_status == ""success""\n\n    assert logger2.hparams_logged == hparams\n    assert logger2.metrics_logged != {}\n    assert logger2.finalized_status == ""success""\n\n\ndef test_multiple_loggers_pickle(tmpdir):\n    """"""Verify that pickling trainer with multiple loggers works.""""""\n\n    logger1 = CustomLogger()\n    logger2 = CustomLogger()\n\n    trainer = Trainer(max_epochs=1, logger=[logger1, logger2])\n    pkl_bytes = pickle.dumps(trainer)\n    trainer2 = pickle.loads(pkl_bytes)\n    trainer2.logger.log_metrics({""acc"": 1.0}, 0)\n\n    assert logger1.metrics_logged != {}\n    assert logger2.metrics_logged != {}\n\n\ndef test_adding_step_key(tmpdir):\n    logged_step = 0\n\n    def _validation_epoch_end(outputs):\n        nonlocal logged_step\n        logged_step += 1\n        return {""log"": {""step"": logged_step, ""val_acc"": logged_step / 10}}\n\n    def _training_epoch_end(outputs):\n        nonlocal logged_step\n        logged_step += 1\n        return {""log"": {""step"": logged_step, ""train_acc"": logged_step / 10}}\n\n    def _log_metrics_decorator(log_metrics_fn):\n        def decorated(metrics, step):\n            if ""val_acc"" in metrics:\n                assert step == logged_step\n            return log_metrics_fn(metrics, step)\n\n        return decorated\n\n    model = EvalModelTemplate()\n    model.validation_epoch_end = _validation_epoch_end\n    model.training_epoch_end = _training_epoch_end\n    trainer = Trainer(\n        max_epochs=3,\n        default_root_dir=tmpdir,\n        train_percent_check=0.001,\n        val_percent_check=0.01,\n        num_sanity_val_steps=0,\n    )\n    trainer.logger.log_metrics = _log_metrics_decorator(\n        trainer.logger.log_metrics)\n    trainer.fit(model)\n\n\ndef test_with_accumulate_grad_batches():\n    """"""Checks if the logging is performed once for `accumulate_grad_batches` steps.""""""\n\n    class StoreHistoryLogger(CustomLogger):\n        def __init__(self):\n            super().__init__()\n            self.history = {}\n\n        @rank_zero_only\n        def log_metrics(self, metrics, step):\n            if step not in self.history:\n                self.history[step] = {}\n            self.history[step].update(metrics)\n\n    logger = StoreHistoryLogger()\n\n    np.random.seed(42)\n    for i, loss in enumerate(np.random.random(10)):\n        logger.agg_and_log_metrics({\'loss\': loss}, step=int(i / 5))\n\n    assert logger.history == {0: {\'loss\': 0.5623850983416314}}\n    logger.close()\n    assert logger.history == {0: {\'loss\': 0.5623850983416314}, 1: {\'loss\': 0.4778883735637184}}\n'"
tests/loggers/test_comet.py,0,"b'from unittest.mock import patch\n\nimport pytest\n\nfrom pytorch_lightning.loggers import CometLogger\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\n\n\ndef test_comet_logger_online():\n    """"""Test comet online with mocks.""""""\n    # Test api_key given\n    with patch(\'pytorch_lightning.loggers.comet.CometExperiment\') as comet:\n        logger = CometLogger(\n            api_key=\'key\',\n            workspace=\'dummy-test\',\n            project_name=\'general\'\n        )\n\n        _ = logger.experiment\n\n        comet.assert_called_once_with(\n            api_key=\'key\',\n            workspace=\'dummy-test\',\n            project_name=\'general\'\n        )\n\n    # Test both given\n    with patch(\'pytorch_lightning.loggers.comet.CometExperiment\') as comet:\n        logger = CometLogger(\n            save_dir=\'test\',\n            api_key=\'key\',\n            workspace=\'dummy-test\',\n            project_name=\'general\'\n        )\n\n        _ = logger.experiment\n\n        comet.assert_called_once_with(\n            api_key=\'key\',\n            workspace=\'dummy-test\',\n            project_name=\'general\'\n        )\n\n    # Test neither given\n    with pytest.raises(MisconfigurationException):\n        CometLogger(\n            workspace=\'dummy-test\',\n            project_name=\'general\'\n        )\n\n    # Test already exists\n    with patch(\'pytorch_lightning.loggers.comet.CometExistingExperiment\') as comet_existing:\n        logger = CometLogger(\n            experiment_key=\'test\',\n            experiment_name=\'experiment\',\n            api_key=\'key\',\n            workspace=\'dummy-test\',\n            project_name=\'general\'\n        )\n\n        _ = logger.experiment\n\n        comet_existing.assert_called_once_with(\n            api_key=\'key\',\n            workspace=\'dummy-test\',\n            project_name=\'general\',\n            previous_experiment=\'test\'\n        )\n\n        comet_existing().set_name.assert_called_once_with(\'experiment\')\n\n    with patch(\'pytorch_lightning.loggers.comet.API\') as api:\n        CometLogger(\n            api_key=\'key\',\n            workspace=\'dummy-test\',\n            project_name=\'general\',\n            rest_api_key=\'rest\'\n        )\n\n        api.assert_called_once_with(\'rest\')\n'"
tests/loggers/test_mlflow.py,0,"b'from pytorch_lightning.loggers import MLFlowLogger\n\n\ndef test_mlflow_logger_exists(tmpdir):\n    """"""Verify that basic functionality of mlflow logger works.""""""\n    logger = MLFlowLogger(\'test\', save_dir=tmpdir)\n    # Test already exists\n    logger2 = MLFlowLogger(\'test\', save_dir=tmpdir)\n    assert logger.run_id != logger2.run_id\n'"
tests/loggers/test_neptune.py,2,"b'from unittest.mock import patch, MagicMock\n\nimport torch\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import NeptuneLogger\nfrom tests.base import EvalModelTemplate\n\n\n@patch(\'pytorch_lightning.loggers.neptune.neptune\')\ndef test_neptune_online(neptune):\n    logger = NeptuneLogger(api_key=\'test\', project_name=\'project\')\n\n    created_experiment = neptune.Session.with_default_backend().get_project().create_experiment()\n\n    # It\'s important to check if the internal variable _experiment was initialized in __init__.\n    # Calling logger.experiment would cause a side-effect of initializing _experiment,\n    # if it wasn\'t already initialized.\n    assert logger._experiment == created_experiment\n    assert logger.name == created_experiment.name\n    assert logger.version == created_experiment.id\n\n\n@patch(\'pytorch_lightning.loggers.neptune.neptune\')\ndef test_neptune_offline(neptune):\n    logger = NeptuneLogger(offline_mode=True)\n\n    neptune.Session.assert_called_once_with(backend=neptune.OfflineBackend())\n    assert logger.experiment == neptune.Session().get_project().create_experiment()\n\n\n@patch(\'pytorch_lightning.loggers.neptune.neptune\')\ndef test_neptune_additional_methods(neptune):\n    logger = NeptuneLogger(api_key=\'test\', project_name=\'project\')\n\n    created_experiment = neptune.Session.with_default_backend().get_project().create_experiment()\n\n    logger.log_metric(\'test\', torch.ones(1))\n    created_experiment.log_metric.assert_called_once_with(\'test\', torch.ones(1))\n    created_experiment.log_metric.reset_mock()\n\n    logger.log_metric(\'test\', 1.0)\n    created_experiment.log_metric.assert_called_once_with(\'test\', 1.0)\n    created_experiment.log_metric.reset_mock()\n\n    logger.log_metric(\'test\', 1.0, step=2)\n    created_experiment.log_metric.assert_called_once_with(\'test\', x=2, y=1.0)\n    created_experiment.log_metric.reset_mock()\n\n    logger.log_text(\'test\', \'text\')\n    created_experiment.log_metric.assert_called_once_with(\'test\', \'text\')\n    created_experiment.log_metric.reset_mock()\n\n    logger.log_image(\'test\', \'image file\')\n    created_experiment.log_image.assert_called_once_with(\'test\', \'image file\')\n    created_experiment.log_image.reset_mock()\n\n    logger.log_image(\'test\', \'image file\', step=2)\n    created_experiment.log_image.assert_called_once_with(\'test\', x=2, y=\'image file\')\n    created_experiment.log_image.reset_mock()\n\n    logger.log_artifact(\'file\')\n    created_experiment.log_artifact.assert_called_once_with(\'file\', None)\n\n    logger.set_property(\'property\', 10)\n    created_experiment.set_property.assert_called_once_with(\'property\', 10)\n\n    logger.append_tags(\'one tag\')\n    created_experiment.append_tags.assert_called_once_with(\'one tag\')\n    created_experiment.append_tags.reset_mock()\n\n    logger.append_tags([\'two\', \'tags\'])\n    created_experiment.append_tags.assert_called_once_with(\'two\', \'tags\')\n\n\ndef test_neptune_leave_open_experiment_after_fit(tmpdir):\n    """"""Verify that neptune experiment was closed after training""""""\n    model = EvalModelTemplate()\n\n    def _run_training(logger):\n        logger._experiment = MagicMock()\n        trainer = Trainer(\n            default_root_dir=tmpdir,\n            max_epochs=1,\n            train_percent_check=0.05,\n            logger=logger\n        )\n        trainer.fit(model)\n        return logger\n\n    logger_close_after_fit = _run_training(NeptuneLogger(offline_mode=True))\n    assert logger_close_after_fit._experiment.stop.call_count == 1\n\n    logger_open_after_fit = _run_training(NeptuneLogger(offline_mode=True, close_after_fit=False))\n    assert logger_open_after_fit._experiment.stop.call_count == 0\n'"
tests/loggers/test_tensorboard.py,5,"b'from argparse import Namespace\n\nimport pytest\nimport torch\n\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\n\ndef test_tensorboard_automatic_versioning(tmpdir):\n    """"""Verify that automatic versioning works""""""\n\n    root_dir = tmpdir.mkdir(""tb_versioning"")\n    root_dir.mkdir(""version_0"")\n    root_dir.mkdir(""version_1"")\n\n    logger = TensorBoardLogger(save_dir=tmpdir, name=""tb_versioning"")\n\n    assert logger.version == 2\n\n\ndef test_tensorboard_manual_versioning(tmpdir):\n    """"""Verify that manual versioning works""""""\n\n    root_dir = tmpdir.mkdir(""tb_versioning"")\n    root_dir.mkdir(""version_0"")\n    root_dir.mkdir(""version_1"")\n    root_dir.mkdir(""version_2"")\n\n    logger = TensorBoardLogger(save_dir=tmpdir, name=""tb_versioning"", version=1)\n\n    assert logger.version == 1\n\n\ndef test_tensorboard_named_version(tmpdir):\n    """"""Verify that manual versioning works for string versions, e.g. \'2020-02-05-162402\' """"""\n\n    tmpdir.mkdir(""tb_versioning"")\n    expected_version = ""2020-02-05-162402""\n\n    logger = TensorBoardLogger(save_dir=tmpdir, name=""tb_versioning"", version=expected_version)\n    logger.log_hyperparams({""a"": 1, ""b"": 2})  # Force data to be written\n\n    assert logger.version == expected_version\n    # Could also test existence of the directory but this fails\n    # in the ""minimum requirements"" test setup\n\n\n@pytest.mark.parametrize(""name"", [\'\', None])\ndef test_tensorboard_no_name(tmpdir, name):\n    """"""Verify that None or empty name works""""""\n    logger = TensorBoardLogger(save_dir=tmpdir, name=name)\n    assert logger.root_dir == tmpdir\n\n\n@pytest.mark.parametrize(""step_idx"", [10, None])\ndef test_tensorboard_log_metrics(tmpdir, step_idx):\n    logger = TensorBoardLogger(tmpdir)\n    metrics = {\n        ""float"": 0.3,\n        ""int"": 1,\n        ""FloatTensor"": torch.tensor(0.1),\n        ""IntTensor"": torch.tensor(1)\n    }\n    logger.log_metrics(metrics, step_idx)\n\n\ndef test_tensorboard_log_hyperparams(tmpdir):\n    logger = TensorBoardLogger(tmpdir)\n    hparams = {\n        ""float"": 0.3,\n        ""int"": 1,\n        ""string"": ""abc"",\n        ""bool"": True,\n        ""dict"": {\'a\': {\'b\': \'c\'}},\n        ""list"": [1, 2, 3],\n        ""namespace"": Namespace(foo=Namespace(bar=\'buzz\')),\n        ""layer"": torch.nn.BatchNorm1d\n    }\n    logger.log_hyperparams(hparams)\n\n\ndef test_tensorboard_log_hparams_and_metrics(tmpdir):\n    logger = TensorBoardLogger(tmpdir)\n    hparams = {\n        ""float"": 0.3,\n        ""int"": 1,\n        ""string"": ""abc"",\n        ""bool"": True,\n        ""dict"": {\'a\': {\'b\': \'c\'}},\n        ""list"": [1, 2, 3],\n        ""namespace"": Namespace(foo=Namespace(bar=\'buzz\')),\n        ""layer"": torch.nn.BatchNorm1d\n    }\n    metrics = {\'abc\': torch.tensor([0.54])}\n    logger.log_hyperparams(hparams, metrics)\n'"
tests/loggers/test_trains.py,0,"b'import pickle\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import TrainsLogger\nfrom tests.base import EvalModelTemplate\n\n\ndef test_trains_logger(tmpdir):\n    """"""Verify that basic functionality of TRAINS logger works.""""""\n    model = EvalModelTemplate()\n    TrainsLogger.set_bypass_mode(True)\n    TrainsLogger.set_credentials(api_host=\'http://integration.trains.allegro.ai:8008\',\n                                 files_host=\'http://integration.trains.allegro.ai:8081\',\n                                 web_host=\'http://integration.trains.allegro.ai:8080\', )\n    logger = TrainsLogger(project_name=""lightning_log"", task_name=""pytorch lightning test"")\n\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        train_percent_check=0.05,\n        logger=logger\n    )\n    result = trainer.fit(model)\n\n    # print(\'result finished\')\n    logger.finalize()\n    assert result == 1, ""Training failed""\n\n\ndef test_trains_pickle(tmpdir):\n    """"""Verify that pickling trainer with TRAINS logger works.""""""\n    # hparams = tutils.get_default_hparams()\n    # model = LightningTestModel(hparams)\n    TrainsLogger.set_bypass_mode(True)\n    TrainsLogger.set_credentials(api_host=\'http://integration.trains.allegro.ai:8008\',\n                                 files_host=\'http://integration.trains.allegro.ai:8081\',\n                                 web_host=\'http://integration.trains.allegro.ai:8080\', )\n    logger = TrainsLogger(project_name=""lightning_log"", task_name=""pytorch lightning test"")\n\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        logger=logger\n    )\n    pkl_bytes = pickle.dumps(trainer)\n    trainer2 = pickle.loads(pkl_bytes)\n    trainer2.logger.log_metrics({""acc"": 1.0})\n    trainer2.logger.finalize()\n    logger.finalize()\n'"
tests/loggers/test_wandb.py,0,"b'import os\nimport pickle\nfrom unittest.mock import patch\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import WandbLogger\n\n\n@patch(\'pytorch_lightning.loggers.wandb.wandb\')\ndef test_wandb_logger(wandb):\n    """"""Verify that basic functionality of wandb logger works.\n    Wandb doesn\'t work well with pytest so we have to mock it out here.""""""\n    logger = WandbLogger(anonymous=True, offline=True)\n\n    logger.log_metrics({\'acc\': 1.0})\n    wandb.init().log.assert_called_once_with({\'acc\': 1.0})\n\n    wandb.init().log.reset_mock()\n    logger.log_metrics({\'acc\': 1.0}, step=3)\n    wandb.init().log.assert_called_once_with({\'global_step\': 3, \'acc\': 1.0})\n\n    logger.log_hyperparams({\'test\': None})\n    wandb.init().config.update.assert_called_once_with({\'test\': None}, allow_val_change=True)\n\n    logger.watch(\'model\', \'log\', 10)\n    wandb.init().watch.assert_called_once_with(\'model\', log=\'log\', log_freq=10)\n\n    assert logger.name == wandb.init().project_name()\n    assert logger.version == wandb.init().id\n\n\n@patch(\'pytorch_lightning.loggers.wandb.wandb\')\ndef test_wandb_pickle(wandb):\n    """"""Verify that pickling trainer with wandb logger works.\n\n    Wandb doesn\'t work well with pytest so we have to mock it out here.\n    """"""\n    class Experiment:\n        id = \'the_id\'\n\n    wandb.init.return_value = Experiment()\n\n    logger = WandbLogger(id=\'the_id\', offline=True)\n\n    trainer = Trainer(max_epochs=1, logger=logger)\n    # Access the experiment to ensure it\'s created\n    assert trainer.logger.experiment, \'missing experiment\'\n    pkl_bytes = pickle.dumps(trainer)\n    trainer2 = pickle.loads(pkl_bytes)\n\n    assert os.environ[\'WANDB_MODE\'] == \'dryrun\'\n    assert trainer2.logger.__class__.__name__ == WandbLogger.__name__\n    assert trainer2.logger.experiment, \'missing experiment\'\n\n    wandb.init.assert_called()\n    assert \'id\' in wandb.init.call_args[1]\n    assert wandb.init.call_args[1][\'id\'] == \'the_id\'\n\n    del os.environ[\'WANDB_MODE\']\n'"
tests/metrics/__init__.py,0,b''
tests/metrics/test_converters.py,18,"b'import numpy as np\nimport pytest\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning.metrics.converters import (\n    _apply_to_inputs, _apply_to_outputs, _convert_to_tensor, _convert_to_numpy,\n    _numpy_metric_conversion, _tensor_metric_conversion, _sync_ddp_if_available, tensor_metric, numpy_metric)\n\n\n@pytest.mark.parametrize([\'args\', \'kwargs\'],\n                         [pytest.param([], {}),\n                          pytest.param([1., 2.], {}),\n                          pytest.param([], {\'a\': 1., \'b\': 2.}),\n                          pytest.param([1., 2.], {\'a\': 1., \'b\': 2.})])\ndef test_apply_to_inputs(args, kwargs):\n    def apply_fn(inputs, factor):\n        if isinstance(inputs, (float, int)):\n            return inputs * factor\n        elif isinstance(inputs, dict):\n            return {k: apply_fn(v, factor) for k, v in inputs.items()}\n        elif isinstance(inputs, (tuple, list)):\n            return [apply_fn(x, factor) for x in inputs]\n\n    @_apply_to_inputs(apply_fn, factor=2.)\n    def test_fn(*func_args, **func_kwargs):\n        return func_args, func_kwargs\n\n    result_args, result_kwargs = test_fn(*args, **kwargs)\n    assert isinstance(result_args, (list, tuple))\n    assert isinstance(result_kwargs, dict)\n    assert len(result_args) == len(args)\n    assert len(result_kwargs) == len(kwargs)\n    assert all([k in result_kwargs for k in kwargs.keys()])\n    for arg, result_arg in zip(args, result_args):\n        assert arg * 2. == result_arg\n\n    for key in kwargs.keys():\n        arg = kwargs[key]\n        result_arg = result_kwargs[key]\n        assert arg * 2. == result_arg\n\n\ndef test_apply_to_outputs():\n    def apply_fn(inputs, additional_str):\n        return str(inputs) + additional_str\n\n    @_apply_to_outputs(apply_fn, additional_str=\'_str\')\n    def test_fn(*args, **kwargs):\n        return \'dummy\'\n\n    assert test_fn() == \'dummy_str\'\n\n\ndef test_convert_to_tensor():\n    for test_item in [1., np.array([1.])]:\n        result_tensor = _convert_to_tensor(test_item)\n        assert isinstance(result_tensor, torch.Tensor)\n        assert result_tensor.item() == 1.\n\n\ndef test_convert_to_numpy():\n    for test_item in [1., torch.tensor([1.])]:\n        result = _convert_to_numpy(test_item)\n        assert isinstance(result, np.ndarray)\n        assert result.item() == 1.\n\n\ndef test_numpy_metric_conversion():\n    @_numpy_metric_conversion\n    def numpy_test_metric(*args, **kwargs):\n        for arg in args:\n            assert isinstance(arg, np.ndarray)\n\n        for v in kwargs.values():\n            assert isinstance(v, np.ndarray)\n\n        return 5.\n\n    result = numpy_test_metric(torch.tensor([1.]), dummy_kwarg=2.)\n    assert isinstance(result, torch.Tensor)\n    assert result.item() == 5.\n\n\ndef test_tensor_metric_conversion():\n    @_tensor_metric_conversion\n    def tensor_test_metric(*args, **kwargs):\n        for arg in args:\n            assert isinstance(arg, torch.Tensor)\n\n        for v in kwargs.values():\n            assert isinstance(v, torch.Tensor)\n\n        return 5.\n\n    result = tensor_test_metric(np.array([1.]), dummy_kwarg=2.)\n    assert isinstance(result, torch.Tensor)\n    assert result.item() == 5.\n\n\ndef setup_ddp(rank, worldsize, ):\n    import os\n\n    os.environ[\'MASTER_ADDR\'] = \'localhost\'\n\n    # initialize the process group\n    dist.init_process_group(""gloo"", rank=rank, world_size=worldsize)\n\n\ndef ddp_test_fn(rank, worldsize):\n    setup_ddp(rank, worldsize)\n    tensor = torch.tensor([1.], device=\'cuda:0\')\n\n    reduced_tensor = _sync_ddp_if_available(tensor)\n\n    assert reduced_tensor.item() == dist.get_world_size(), \\\n        \'Sync-Reduce does not work properly with DDP and Tensors\'\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_sync_reduce_ddp():\n    """"""Make sure sync-reduce works with DDP""""""\n    tutils.reset_seed()\n    tutils.set_random_master_port()\n\n    worldsize = 2\n    mp.spawn(ddp_test_fn, args=(worldsize,), nprocs=worldsize)\n\n\ndef test_sync_reduce_simple():\n    """"""Make sure sync-reduce works without DDP""""""\n    tensor = torch.tensor([1.], device=\'cpu\')\n\n    reduced_tensor = _sync_ddp_if_available(tensor)\n\n    assert torch.allclose(tensor, reduced_tensor), \\\n        \'Sync-Reduce does not work properly without DDP and Tensors\'\n\n\ndef _test_tensor_metric(is_ddp: bool):\n    @tensor_metric()\n    def tensor_test_metric(*args, **kwargs):\n        for arg in args:\n            assert isinstance(arg, torch.Tensor)\n\n        for v in kwargs.values():\n            assert isinstance(v, torch.Tensor)\n\n        return 5.\n\n    if is_ddp:\n        factor = dist.get_world_size()\n    else:\n        factor = 1.\n\n    result = tensor_test_metric(np.array([1.]), dummy_kwarg=2.)\n    assert isinstance(result, torch.Tensor)\n    assert result.item() == 5. * factor\n\n\ndef _ddp_test_tensor_metric(rank, worldsize):\n    setup_ddp(rank, worldsize)\n    _test_tensor_metric(True)\n\n\ndef test_tensor_metric_ddp():\n    tutils.reset_seed()\n    tutils.set_random_master_port()\n\n    world_size = 2\n    mp.spawn(_ddp_test_tensor_metric, args=(world_size,), nprocs=world_size)\n\n\ndef test_tensor_metric_simple():\n    _test_tensor_metric(False)\n\n\ndef _test_numpy_metric(is_ddp: bool):\n    @numpy_metric()\n    def numpy_test_metric(*args, **kwargs):\n        for arg in args:\n            assert isinstance(arg, np.ndarray)\n\n        for v in kwargs.values():\n            assert isinstance(v, np.ndarray)\n\n        return 5.\n\n    if is_ddp:\n        factor = dist.get_world_size()\n    else:\n        factor = 1.\n\n    result = numpy_test_metric(torch.tensor([1.]), dummy_kwarg=2.)\n    assert isinstance(result, torch.Tensor)\n    assert result.item() == 5. * factor\n\n\ndef _ddp_test_numpy_metric(rank, worldsize):\n    setup_ddp(rank, worldsize)\n    _test_numpy_metric(True)\n\n\ndef test_numpy_metric_ddp():\n    tutils.reset_seed()\n    tutils.set_random_master_port()\n    world_size = 2\n    mp.spawn(_ddp_test_numpy_metric, args=(world_size,), nprocs=world_size)\n\n\ndef test_numpy_metric_simple():\n    _test_tensor_metric(False)\n'"
tests/metrics/test_metrics.py,23,"b""import numpy as np\nimport torch\n\nfrom pytorch_lightning.metrics.metric import Metric, TensorMetric, NumpyMetric\n\n\nclass DummyTensorMetric(TensorMetric):\n    def __init__(self):\n        super().__init__('dummy')\n\n    def forward(self, input1, input2):\n        assert isinstance(input1, torch.Tensor)\n        assert isinstance(input2, torch.Tensor)\n        return 1.\n\n\nclass DummyNumpyMetric(NumpyMetric):\n    def __init__(self):\n        super().__init__('dummy')\n\n    def forward(self, input1, input2):\n        assert isinstance(input1, np.ndarray)\n        assert isinstance(input2, np.ndarray)\n        return 1.\n\n\ndef _test_metric(metric: Metric):\n    input1, input2 = torch.tensor([1.]), torch.tensor([2.])\n\n    def change_and_check_device_dtype(device, dtype):\n        metric.to(device=device, dtype=dtype)\n\n        metric_val = metric(input1, input2)\n        assert isinstance(metric_val, torch.Tensor)\n\n        if device is not None:\n            assert metric.device in [device, torch.device(device)]\n            assert metric_val.device in [device, torch.device(device)]\n\n        if dtype is not None:\n            assert metric.dtype == dtype\n            assert metric_val.dtype == dtype\n\n    devices = [None, 'cpu']\n    if torch.cuda.is_available():\n        devices += ['cuda:0']\n\n    for device in devices:\n        for dtype in [None, torch.float32, torch.float64]:\n            change_and_check_device_dtype(device=device, dtype=dtype)\n\n    if torch.cuda.is_available():\n        metric.cuda(0)\n        assert metric.device == torch.device('cuda', index=0)\n        assert metric(input1, input2).device == torch.device('cuda', index=0)\n\n    metric.cpu()\n    assert metric.device == torch.device('cpu')\n    assert metric(input1, input2).device == torch.device('cpu')\n\n    metric.type(torch.int8)\n    assert metric.dtype == torch.int8\n    assert metric(input1, input2).dtype == torch.int8\n\n    metric.float()\n    assert metric.dtype == torch.float32\n    assert metric(input1, input2).dtype == torch.float32\n\n    metric.double()\n    assert metric.dtype == torch.float64\n    assert metric(input1, input2).dtype == torch.float64\n\n    if torch.cuda.is_available():\n        metric.cuda()\n        metric.half()\n        assert metric.dtype == torch.float16\n        assert metric(input1, input2).dtype == torch.float16\n\n\ndef test_tensor_metric():\n    _test_metric(DummyTensorMetric())\n\n\ndef test_numpy_metric():\n    _test_metric(DummyNumpyMetric())\n"""
tests/models/__init__.py,0,b''
tests/models/test_amp.py,3,"b'import os\n\nimport pytest\nimport torch\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\n\n@pytest.mark.spawn\n@pytest.mark.parametrize(""backend"", [\'dp\', \'ddp\'])\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""test requires GPU machine"")\ndef test_amp_single_gpu(tmpdir, backend):\n    """"""Make sure DP/DDP + AMP work.""""""\n    tutils.reset_seed()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        gpus=1,\n        distributed_backend=backend,\n        precision=16\n    )\n\n    model = EvalModelTemplate()\n    # tutils.run_model_test(trainer_options, model)\n    result = trainer.fit(model)\n\n    assert result == 1\n\n\n@pytest.mark.spawn\n@pytest.mark.parametrize(""backend"", [\'dp\', \'ddp\'])\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_amp_multi_gpu(tmpdir, backend):\n    """"""Make sure DP/DDP + AMP work.""""""\n    tutils.set_random_master_port()\n\n    model = EvalModelTemplate()\n\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        # gpus=2,\n        gpus=\'0, 1\',  # test init with gpu string\n        distributed_backend=backend,\n        precision=16\n    )\n\n    # tutils.run_model_test(trainer_options, model)\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    assert result\n\n\n@pytest.mark.spawn\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_amp_gpu_ddp_slurm_managed(tmpdir):\n    """"""Make sure DDP + AMP work.""""""\n    # simulate setting slurm flags\n    tutils.set_random_master_port()\n    os.environ[\'SLURM_LOCALID\'] = str(0)\n\n    model = EvalModelTemplate()\n\n    # exp file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    # exp file to get weights\n    checkpoint = tutils.init_checkpoint_callback(logger)\n\n    # fit model\n    trainer = Trainer(\n        max_epochs=1,\n        gpus=[0],\n        distributed_backend=\'ddp\',\n        precision=16,\n        checkpoint_callback=checkpoint,\n        logger=logger,\n    )\n    trainer.is_slurm_managing_tasks = True\n    result = trainer.fit(model)\n\n    # correct result and ok accuracy\n    assert result == 1, \'amp + ddp model failed to complete\'\n\n    # test root model address\n    assert trainer.resolve_root_node_address(\'abc\') == \'abc\'\n    assert trainer.resolve_root_node_address(\'abc[23]\') == \'abc23\'\n    assert trainer.resolve_root_node_address(\'abc[23-24]\') == \'abc23\'\n    assert trainer.resolve_root_node_address(\'abc[23-24, 45-40, 40]\') == \'abc23\'\n\n\ndef test_cpu_model_with_amp(tmpdir):\n    """"""Make sure model trains on CPU.""""""\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.4,\n        precision=16\n    )\n\n    model = EvalModelTemplate()\n\n    with pytest.raises((MisconfigurationException, ModuleNotFoundError)):\n        tutils.run_model_test(trainer_options, model, on_gpu=False)\n'"
tests/models/test_cpu.py,28,"b'import os\nimport platform\nfrom collections import namedtuple\n\nimport pytest\nimport torch\nfrom packaging.version import parse as version_parse\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom tests.base import EvalModelTemplate\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n\ndef test_cpu_slurm_save_load(tmpdir):\n    """"""Verify model save/load/checkpoint on CPU.""""""\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n    version = logger.version\n\n    # fit model\n    trainer = Trainer(\n        max_epochs=1,\n        logger=logger,\n        train_percent_check=0.2,\n        val_percent_check=0.2,\n        checkpoint_callback=ModelCheckpoint(tmpdir)\n    )\n    result = trainer.fit(model)\n    real_global_step = trainer.global_step\n\n    # traning complete\n    assert result == 1, \'cpu model failed to complete\'\n\n    # predict with trained model before saving\n    # make a prediction\n    dataloaders = model.test_dataloader()\n    if not isinstance(dataloaders, list):\n        dataloaders = [dataloaders]\n\n    for dataloader in dataloaders:\n        for batch in dataloader:\n            break\n\n    x, y = batch\n    x = x.view(x.size(0), -1)\n\n    model.eval()\n    pred_before_saving = model(x)\n\n    # test HPC saving\n    # simulate snapshot on slurm\n    saved_filepath = trainer.hpc_save(tmpdir, logger)\n    assert os.path.exists(saved_filepath)\n\n    # new logger file to get meta\n    logger = tutils.get_default_logger(tmpdir, version=version)\n\n    trainer = Trainer(\n        max_epochs=1,\n        logger=logger,\n        checkpoint_callback=ModelCheckpoint(tmpdir),\n    )\n    model = EvalModelTemplate(**hparams)\n\n    # set the epoch start hook so we can predict before the model does the full training\n    def assert_pred_same():\n        assert trainer.global_step == real_global_step and trainer.global_step > 0\n\n        # predict with loaded model to make sure answers are the same\n        trainer.model.eval()\n        new_pred = trainer.model(x)\n        assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1\n\n    model.on_epoch_start = assert_pred_same\n\n    # by calling fit again, we trigger training, loading weights from the cluster\n    # and our hook to predict using current model before any more weight updates\n    trainer.fit(model)\n\n\ndef test_early_stopping_cpu_model(tmpdir):\n    """"""Test each of the trainer options.""""""\n    stopping = EarlyStopping(monitor=\'val_loss\', min_delta=0.1)\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        early_stop_callback=stopping,\n        max_epochs=2,\n        gradient_clip_val=1.0,\n        overfit_pct=0.20,\n        track_grad_norm=2,\n        train_percent_check=0.1,\n        val_percent_check=0.1,\n    )\n\n    model = EvalModelTemplate()\n    tutils.run_model_test(trainer_options, model, on_gpu=False)\n\n    # test freeze on cpu\n    model.freeze()\n    model.unfreeze()\n\n\n@pytest.mark.spawn\n@pytest.mark.skipif(platform.system() == ""Windows"",\n                    reason=""Distributed training is not supported on Windows"")\n@pytest.mark.skipif((platform.system() == ""Darwin"" and\n                     version_parse(torch.__version__) < version_parse(""1.3.0"")),\n                    reason=""Distributed training is not supported on MacOS before Torch 1.3.0"")\ndef test_multi_cpu_model_ddp(tmpdir):\n    """"""Make sure DDP works.""""""\n    tutils.set_random_master_port()\n\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        gpus=None,\n        num_processes=2,\n        distributed_backend=\'ddp_cpu\'\n    )\n\n    model = EvalModelTemplate()\n    tutils.run_model_test(trainer_options, model, on_gpu=False)\n\n\ndef test_lbfgs_cpu_model(tmpdir):\n    """"""Test each of the trainer options.""""""\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        progress_bar_refresh_rate=0,\n        weights_summary=\'top\',\n        train_percent_check=0.2,\n        val_percent_check=0.2,\n    )\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    hparams.update(optimizer_name=\'lbfgs\',\n                   learning_rate=0.004)\n    model = EvalModelTemplate(**hparams)\n    model.configure_optimizers = model.configure_optimizers__lbfgs\n    tutils.run_model_test_without_loggers(trainer_options, model, min_acc=0.25)\n\n\ndef test_default_logger_callbacks_cpu_model(tmpdir):\n    """"""Test each of the trainer options.""""""\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        gradient_clip_val=1.0,\n        overfit_pct=0.20,\n        progress_bar_refresh_rate=0,\n        train_percent_check=0.01,\n        val_percent_check=0.01,\n    )\n\n    model = EvalModelTemplate()\n    tutils.run_model_test_without_loggers(trainer_options, model)\n\n    # test freeze on cpu\n    model.freeze()\n    model.unfreeze()\n\n\ndef test_running_test_after_fitting(tmpdir):\n    """"""Verify test() on fitted model.""""""\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    # logger file to get weights\n    checkpoint = tutils.init_checkpoint_callback(logger)\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        progress_bar_refresh_rate=0,\n        max_epochs=2,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        test_percent_check=0.2,\n        checkpoint_callback=checkpoint,\n        logger=logger\n    )\n    result = trainer.fit(model)\n\n    assert result == 1, \'training failed to complete\'\n\n    trainer.test()\n\n    # test we have good test accuracy\n    tutils.assert_ok_model_acc(trainer, thr=0.5)\n\n\ndef test_running_test_no_val(tmpdir):\n    """"""Verify `test()` works on a model with no `val_loader`.""""""\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    # logger file to get weights\n    checkpoint = tutils.init_checkpoint_callback(logger)\n\n    # fit model\n    trainer = Trainer(\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        test_percent_check=0.2,\n        checkpoint_callback=checkpoint,\n        logger=logger,\n        early_stop_callback=False\n    )\n    result = trainer.fit(model)\n\n    assert result == 1, \'training failed to complete\'\n\n    trainer.test()\n\n    # test we have good test accuracy\n    tutils.assert_ok_model_acc(trainer)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""test requires GPU machine"")\ndef test_single_gpu_batch_parse():\n    trainer = Trainer()\n\n    # batch is just a tensor\n    batch = torch.rand(2, 3)\n    batch = trainer.transfer_batch_to_gpu(batch, 0)\n    assert batch.device.index == 0 and batch.type() == \'torch.cuda.FloatTensor\'\n\n    # tensor list\n    batch = [torch.rand(2, 3), torch.rand(2, 3)]\n    batch = trainer.transfer_batch_to_gpu(batch, 0)\n    assert batch[0].device.index == 0 and batch[0].type() == \'torch.cuda.FloatTensor\'\n    assert batch[1].device.index == 0 and batch[1].type() == \'torch.cuda.FloatTensor\'\n\n    # tensor list of lists\n    batch = [[torch.rand(2, 3), torch.rand(2, 3)]]\n    batch = trainer.transfer_batch_to_gpu(batch, 0)\n    assert batch[0][0].device.index == 0 and batch[0][0].type() == \'torch.cuda.FloatTensor\'\n    assert batch[0][1].device.index == 0 and batch[0][1].type() == \'torch.cuda.FloatTensor\'\n\n    # tensor dict\n    batch = [{\'a\': torch.rand(2, 3), \'b\': torch.rand(2, 3)}]\n    batch = trainer.transfer_batch_to_gpu(batch, 0)\n    assert batch[0][\'a\'].device.index == 0 and batch[0][\'a\'].type() == \'torch.cuda.FloatTensor\'\n    assert batch[0][\'b\'].device.index == 0 and batch[0][\'b\'].type() == \'torch.cuda.FloatTensor\'\n\n    # tuple of tensor list and list of tensor dict\n    batch = ([torch.rand(2, 3) for _ in range(2)],\n             [{\'a\': torch.rand(2, 3), \'b\': torch.rand(2, 3)} for _ in range(2)])\n    batch = trainer.transfer_batch_to_gpu(batch, 0)\n    assert batch[0][0].device.index == 0 and batch[0][0].type() == \'torch.cuda.FloatTensor\'\n\n    assert batch[1][0][\'a\'].device.index == 0\n    assert batch[1][0][\'a\'].type() == \'torch.cuda.FloatTensor\'\n\n    assert batch[1][0][\'b\'].device.index == 0\n    assert batch[1][0][\'b\'].type() == \'torch.cuda.FloatTensor\'\n\n    # namedtuple of tensor\n    BatchType = namedtuple(\'BatchType\', [\'a\', \'b\'])\n    batch = [BatchType(a=torch.rand(2, 3), b=torch.rand(2, 3)) for _ in range(2)]\n    batch = trainer.transfer_batch_to_gpu(batch, 0)\n    assert batch[0].a.device.index == 0\n    assert batch[0].a.type() == \'torch.cuda.FloatTensor\'\n\n\ndef test_simple_cpu(tmpdir):\n    """"""Verify continue training session on CPU.""""""\n    model = EvalModelTemplate()\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.1,\n    )\n    result = trainer.fit(model)\n\n    # traning complete\n    assert result == 1, \'amp + ddp model failed to complete\'\n\n\ndef test_cpu_model(tmpdir):\n    """"""Make sure model trains on CPU.""""""\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.4\n    )\n\n    model = EvalModelTemplate()\n\n    tutils.run_model_test(trainer_options, model, on_gpu=False)\n\n\ndef test_all_features_cpu_model(tmpdir):\n    """"""Test each of the trainer options.""""""\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        gradient_clip_val=1.0,\n        overfit_pct=0.20,\n        track_grad_norm=2,\n        progress_bar_refresh_rate=0,\n        accumulate_grad_batches=2,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.4\n    )\n\n    model = EvalModelTemplate()\n    tutils.run_model_test(trainer_options, model, on_gpu=False)\n\n\ndef test_tbptt_cpu_model(tmpdir):\n    """"""Test truncated back propagation through time works.""""""\n    truncated_bptt_steps = 2\n    sequence_size = 30\n    batch_size = 30\n\n    x_seq = torch.rand(batch_size, sequence_size, 1)\n    y_seq_list = torch.rand(batch_size, sequence_size, 1).tolist()\n\n    class MockSeq2SeqDataset(torch.utils.data.Dataset):\n        def __getitem__(self, i):\n            return x_seq, y_seq_list\n\n        def __len__(self):\n            return 1\n\n    class BpttTestModel(EvalModelTemplate):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.test_hidden = None\n\n        def training_step(self, batch, batch_idx, hiddens):\n            assert hiddens == self.test_hidden, ""Hidden state not persistent between tbptt steps""\n            self.test_hidden = torch.rand(1)\n\n            x_tensor, y_list = batch\n            assert x_tensor.shape[1] == truncated_bptt_steps, ""tbptt split Tensor failed""\n\n            y_tensor = torch.tensor(y_list, dtype=x_tensor.dtype)\n            assert y_tensor.shape[1] == truncated_bptt_steps, ""tbptt split list failed""\n\n            pred = self(x_tensor.view(batch_size, truncated_bptt_steps))\n            loss_val = torch.nn.functional.mse_loss(\n                pred, y_tensor.view(batch_size, truncated_bptt_steps))\n            return {\n                \'loss\': loss_val,\n                \'hiddens\': self.test_hidden,\n            }\n\n        def train_dataloader(self):\n            return torch.utils.data.DataLoader(\n                dataset=MockSeq2SeqDataset(),\n                batch_size=batch_size,\n                shuffle=False,\n                sampler=None,\n            )\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    hparams.update(\n        batch_size=batch_size,\n        in_features=truncated_bptt_steps,\n        hidden_dim=truncated_bptt_steps,\n        out_features=truncated_bptt_steps\n    )\n\n    model = BpttTestModel(**hparams)\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        truncated_bptt_steps=truncated_bptt_steps,\n        val_percent_check=0,\n        weights_summary=None,\n        early_stop_callback=False\n    )\n    result = trainer.fit(model)\n\n    assert result == 1, \'training failed to complete\'\n'"
tests/models/test_gpu.py,6,"b'import os\n\nimport pytest\nimport torch\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.core import memory\nfrom pytorch_lightning.trainer.distrib_parts import parse_gpu_ids, determine_root_gpu_device\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\nPRETEND_N_OF_GPUS = 16\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""test requires GPU machine"")\n@pytest.mark.parametrize(\'gpus\', [1, [0], [1]])\ndef test_single_gpu_model(tmpdir, gpus):\n    """"""Make sure single GPU works (DP mode).""""""\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.1,\n        val_percent_check=0.1,\n        gpus=gpus\n    )\n\n    model = EvalModelTemplate()\n    tutils.run_model_test(trainer_options, model)\n\n\n@pytest.mark.spawn\n@pytest.mark.parametrize(""backend"", [\'dp\', \'ddp\', \'ddp2\'])\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_multi_gpu_model(tmpdir, backend):\n    """"""Make sure DDP works.""""""\n    tutils.set_random_master_port()\n\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        gpus=[0, 1],\n        distributed_backend=backend,\n    )\n\n    model = EvalModelTemplate()\n    # tutils.run_model_test(trainer_options, model)\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    assert result\n\n    # test memory helper functions\n    memory.get_memory_profile(\'min_max\')\n\n\n@pytest.mark.spawn\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_ddp_all_dataloaders_passed_to_fit(tmpdir):\n    """"""Make sure DDP works with dataloaders passed to fit()""""""\n    tutils.set_random_master_port()\n\n    trainer_options = dict(default_root_dir=tmpdir,\n                           progress_bar_refresh_rate=0,\n                           max_epochs=1,\n                           train_percent_check=0.1,\n                           val_percent_check=0.1,\n                           gpus=[0, 1],\n                           distributed_backend=\'ddp\')\n\n    model = EvalModelTemplate()\n    fit_options = dict(train_dataloader=model.train_dataloader(),\n                       val_dataloaders=model.val_dataloader())\n\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model, **fit_options)\n    assert result == 1, ""DDP doesn\'t work with dataloaders passed to fit().""\n\n\n@pytest.mark.spawn\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_multi_gpu_none_backend(tmpdir):\n    """"""Make sure when using multiple GPUs the user can\'t use `distributed_backend = None`.""""""\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.1,\n        val_percent_check=0.1,\n        gpus=\'-1\'\n    )\n\n    model = EvalModelTemplate()\n    with pytest.warns(UserWarning):\n        tutils.run_model_test(trainer_options, model)\n\n\n@pytest.fixture\ndef mocked_device_count(monkeypatch):\n    def device_count():\n        return PRETEND_N_OF_GPUS\n\n    monkeypatch.setattr(torch.cuda, \'device_count\', device_count)\n\n\n@pytest.fixture\ndef mocked_device_count_0(monkeypatch):\n    def device_count():\n        return 0\n\n    monkeypatch.setattr(torch.cuda, \'device_count\', device_count)\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([""gpus"", ""expected_num_gpus"", ""distributed_backend""], [\n    pytest.param(None, 0, None, id=""None - expect 0 gpu to use.""),\n    pytest.param(0, 0, None, id=""Oth gpu, expect 1 gpu to use.""),\n    pytest.param(1, 1, None, id=""1st gpu, expect 1 gpu to use.""),\n    pytest.param(-1, PRETEND_N_OF_GPUS, ""ddp"", id=""-1 - use all gpus""),\n    pytest.param(\'-1\', PRETEND_N_OF_GPUS, ""ddp"", id=""\'-1\' - use all gpus""),\n    pytest.param(3, 3, ""ddp"", id=""3rd gpu - 1 gpu to use (backend:ddp)"")\n])\ndef test_trainer_gpu_parse(mocked_device_count, gpus, expected_num_gpus, distributed_backend):\n    assert Trainer(gpus=gpus, distributed_backend=distributed_backend).num_gpus == expected_num_gpus\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([""gpus"", ""expected_num_gpus"", ""distributed_backend""], [\n    pytest.param(None, 0, None, id=""None - expect 0 gpu to use.""),\n    pytest.param(None, 0, ""ddp"", id=""None - expect 0 gpu to use.""),\n])\ndef test_trainer_num_gpu_0(mocked_device_count_0, gpus, expected_num_gpus, distributed_backend):\n    assert Trainer(gpus=gpus, distributed_backend=distributed_backend).num_gpus == expected_num_gpus\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([\'gpus\', \'expected_root_gpu\', ""distributed_backend""], [\n    pytest.param(None, None, ""ddp"", id=""None is None""),\n    pytest.param(0, None, ""ddp"", id=""O gpus, expect gpu root device to be None.""),\n    pytest.param(1, 0, ""ddp"", id=""1 gpu, expect gpu root device to be 0.""),\n    pytest.param(-1, 0, ""ddp"", id=""-1 - use all gpus, expect gpu root device to be 0.""),\n    pytest.param(\'-1\', 0, ""ddp"", id=""\'-1\' - use all gpus, expect gpu root device to be 0.""),\n    pytest.param(3, 0, ""ddp"", id=""3 gpus, expect gpu root device to be 0.(backend:ddp)"")\n])\ndef test_root_gpu_property(mocked_device_count, gpus, expected_root_gpu, distributed_backend):\n    assert Trainer(gpus=gpus, distributed_backend=distributed_backend).root_gpu == expected_root_gpu\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([\'gpus\', \'expected_root_gpu\', ""distributed_backend""], [\n    pytest.param(None, None, None, id=""None is None""),\n    pytest.param(None, None, ""ddp"", id=""None is None""),\n    pytest.param(0, None, ""ddp"", id=""None is None""),\n])\ndef test_root_gpu_property_0_passing(mocked_device_count_0, gpus, expected_root_gpu, distributed_backend):\n    assert Trainer(gpus=gpus, distributed_backend=distributed_backend).root_gpu == expected_root_gpu\n\n\n# Asking for a gpu when non are available will result in a MisconfigurationException\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([\'gpus\', \'expected_root_gpu\', ""distributed_backend""], [\n    pytest.param(1, None, ""ddp""),\n    pytest.param(3, None, ""ddp""),\n    pytest.param(3, None, ""ddp""),\n    pytest.param([1, 2], None, ""ddp""),\n    pytest.param([0, 1], None, ""ddp""),\n    pytest.param(-1, None, ""ddp""),\n    pytest.param(\'-1\', None, ""ddp"")\n])\ndef test_root_gpu_property_0_raising(mocked_device_count_0, gpus, expected_root_gpu, distributed_backend):\n    with pytest.raises(MisconfigurationException):\n        Trainer(gpus=gpus, distributed_backend=distributed_backend).root_gpu\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([\'gpus\', \'expected_root_gpu\'], [\n    pytest.param(None, None, id=""No gpus, expect gpu root device to be None""),\n    pytest.param([0], 0, id=""Oth gpu, expect gpu root device to be 0.""),\n    pytest.param([1], 1, id=""1st gpu, expect gpu root device to be 1.""),\n    pytest.param([3], 3, id=""3rd gpu, expect gpu root device to be 3.""),\n    pytest.param([1, 2], 1, id=""[1, 2] gpus, expect gpu root device to be 1.""),\n])\ndef test_determine_root_gpu_device(gpus, expected_root_gpu):\n    assert determine_root_gpu_device(gpus) == expected_root_gpu\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([\'gpus\', \'expected_gpu_ids\'], [\n    pytest.param(None, None),\n    pytest.param(0, None),\n    pytest.param(1, [0]),\n    pytest.param(3, [0, 1, 2]),\n    pytest.param(-1, list(range(PRETEND_N_OF_GPUS)), id=""-1 - use all gpus""),\n    pytest.param([0], [0]),\n    pytest.param([1, 3], [1, 3]),\n    pytest.param(\'0\', [0]),\n    pytest.param(\'3\', [3]),\n    pytest.param(\'1, 3\', [1, 3]),\n    pytest.param(\'2,\', [2]),\n    pytest.param(\'-1\', list(range(PRETEND_N_OF_GPUS)), id=""\'-1\' - use all gpus""),\n])\ndef test_parse_gpu_ids(mocked_device_count, gpus, expected_gpu_ids):\n    assert parse_gpu_ids(gpus) == expected_gpu_ids\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize([\'gpus\'], [\n    pytest.param(0.1),\n    pytest.param(-2),\n    pytest.param(False),\n    pytest.param([]),\n    pytest.param([-1]),\n    pytest.param([None]),\n    pytest.param([\'0\']),\n    pytest.param((0, 1)),\n])\ndef test_parse_gpu_fail_on_unsupported_inputs(mocked_device_count, gpus):\n    with pytest.raises(MisconfigurationException):\n        parse_gpu_ids(gpus)\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize(""gpus"", [[1, 2, 19], -1, \'-1\'])\ndef test_parse_gpu_fail_on_non_existent_id(mocked_device_count_0, gpus):\n    with pytest.raises(MisconfigurationException):\n        parse_gpu_ids(gpus)\n\n\n@pytest.mark.gpus_param_tests\ndef test_parse_gpu_fail_on_non_existent_id_2(mocked_device_count):\n    with pytest.raises(MisconfigurationException):\n        parse_gpu_ids([1, 2, 19])\n\n\n@pytest.mark.gpus_param_tests\n@pytest.mark.parametrize(""gpus"", [-1, \'-1\'])\ndef test_parse_gpu_returns_None_when_no_devices_are_available(mocked_device_count_0, gpus):\n    with pytest.raises(MisconfigurationException):\n        parse_gpu_ids(gpus)\n'"
tests/models/test_grad_norm.py,0,"b'import torch\nimport pytest\nimport numpy as np\n\nfrom pytorch_lightning import Trainer, seed_everything\n\nfrom pytorch_lightning.loggers import LightningLoggerBase\nfrom pytorch_lightning.utilities import rank_zero_only\n\nfrom tests.base import EvalModelTemplate\nfrom tests.base.utils import reset_seed\n\n\nclass OnlyMetricsListLogger(LightningLoggerBase):\n    def __init__(self):\n        super().__init__()\n        self.metrics = []\n\n    @rank_zero_only\n    def log_metrics(self, metrics, step):\n        self.metrics.append(metrics)\n\n    @property\n    def experiment(self):\n        return \'test\'\n\n    @rank_zero_only\n    def log_hyperparams(self, params):\n        pass\n\n    @rank_zero_only\n    def finalize(self, status):\n        pass\n\n    @property\n    def name(self):\n        return \'name\'\n\n    @property\n    def version(self):\n        return \'1\'\n\n\nclass ModelWithManualGradTracker(EvalModelTemplate):\n    def __init__(self, norm_type, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.stored_grad_norms, self.norm_type = [], float(norm_type)\n\n    # validation spoils logger\'s metrics with `val_loss` records\n    validation_step = None\n    val_dataloader = None\n\n    def training_step(self, batch, batch_idx, optimizer_idx=None):\n        # just return a loss, no log or progress bar meta\n        x, y = batch\n        loss_val = self.loss(y, self(x.flatten(1, -1)))\n        return {\'loss\': loss_val}\n\n    def on_after_backward(self):\n        out, norms = {}, []\n        prefix = f\'grad_{self.norm_type}_norm_\'\n        for name, p in self.named_parameters():\n            if p.grad is None:\n                continue\n\n            # `np.linalg.norm` implementation likely uses fp64 intermediates\n            flat = p.grad.data.cpu().numpy().ravel()\n            norm = np.linalg.norm(flat, self.norm_type)\n            norms.append(norm)\n\n            out[prefix + name] = round(norm, 3)\n\n        # handle total norm\n        norm = np.linalg.norm(norms, self.norm_type)\n        out[prefix + \'total\'] = round(norm, 3)\n        self.stored_grad_norms.append(out)\n\n\n@pytest.mark.parametrize(""norm_type"", [1., 1.25, 1.5, 2, 3, 5, 10, \'inf\'])\ndef test_grad_tracking(tmpdir, norm_type, rtol=5e-3):\n    # rtol=5e-3 respects the 3 decmials rounding in `.grad_norms` and above\n\n    reset_seed()\n\n    # use a custom grad tracking module and a list logger\n    model = ModelWithManualGradTracker(norm_type)\n    logger = OnlyMetricsListLogger()\n\n    trainer = Trainer(\n        max_epochs=3,\n        logger=logger,\n        track_grad_norm=norm_type,\n        row_log_interval=1,  # request grad_norms every batch\n    )\n    result = trainer.fit(model)\n\n    assert result == 1, ""Training failed""\n    assert len(logger.metrics) == len(model.stored_grad_norms)\n\n    # compare the logged metrics against tracked norms on `.backward`\n    for mod, log in zip(model.stored_grad_norms, logger.metrics):\n        common = mod.keys() & log.keys()\n\n        log, mod = [log[k] for k in common], [mod[k] for k in common]\n\n        assert np.allclose(log, mod, rtol=rtol)\n'"
tests/models/test_hooks.py,5,"b'from unittest.mock import MagicMock\n\nimport pytest\nimport torch\n\nfrom pytorch_lightning import Trainer\nfrom tests.base import EvalModelTemplate\n\n\n@pytest.mark.parametrize(\'max_steps\', [1, 2, 3])\ndef test_on_before_zero_grad_called(max_steps):\n\n    class CurrentTestModel(EvalModelTemplate):\n        on_before_zero_grad_called = 0\n\n        def on_before_zero_grad(self, optimizer):\n            self.on_before_zero_grad_called += 1\n\n    model = CurrentTestModel()\n\n    trainer = Trainer(\n        max_steps=max_steps,\n        num_sanity_val_steps=5,\n    )\n    assert 0 == model.on_before_zero_grad_called\n    trainer.fit(model)\n    assert max_steps == model.on_before_zero_grad_called\n\n    model.on_before_zero_grad_called = 0\n    trainer.test(model)\n    assert 0 == model.on_before_zero_grad_called\n\n\ndef test_training_epoch_end_metrics_collection(tmpdir):\n    """""" Test that progress bar metrics also get collected at the end of an epoch. """"""\n    num_epochs = 3\n\n    class CurrentModel(EvalModelTemplate):\n\n        def training_step(self, *args, **kwargs):\n            output = super().training_step(*args, **kwargs)\n            output[\'progress_bar\'].update({\'step_metric\': torch.tensor(-1)})\n            output[\'progress_bar\'].update({\'shared_metric\': 100})\n            return output\n\n        def training_epoch_end(self, outputs):\n            epoch = self.current_epoch\n            # both scalar tensors and Python numbers are accepted\n            return {\n                \'progress_bar\': {\n                    f\'epoch_metric_{epoch}\': torch.tensor(epoch),  # add a new metric key every epoch\n                    \'shared_metric\': 111,\n                }\n            }\n\n    model = CurrentModel()\n    trainer = Trainer(\n        max_epochs=num_epochs,\n        default_root_dir=tmpdir,\n        overfit_pct=0.1,\n    )\n    result = trainer.fit(model)\n    assert result == 1\n    metrics = trainer.progress_bar_dict\n\n    # metrics added in training step should be unchanged by epoch end method\n    assert metrics[\'step_metric\'] == -1\n    # a metric shared in both methods gets overwritten by epoch_end\n    assert metrics[\'shared_metric\'] == 111\n    # metrics are kept after each epoch\n    for i in range(num_epochs):\n        assert metrics[f\'epoch_metric_{i}\'] == i\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""test requires GPU machine"")\ndef test_transfer_batch_hook():\n\n    class CustomBatch:\n\n        def __init__(self, data):\n            self.samples = data[0]\n            self.targets = data[1]\n\n    class CurrentTestModel(EvalModelTemplate):\n\n        hook_called = False\n\n        def transfer_batch_to_device(self, data, device):\n            self.hook_called = True\n            if isinstance(data, CustomBatch):\n                data.samples = data.samples.to(device)\n                data.targets = data.targets.to(device)\n            else:\n                data = super().transfer_batch_to_device(data, device)\n            return data\n\n    model = CurrentTestModel()\n    batch = CustomBatch((torch.zeros(5, 28), torch.ones(5, 1, dtype=torch.long)))\n\n    trainer = Trainer()\n    # running .fit() would require us to implement custom data loaders, we mock the model reference instead\n    trainer.get_model = MagicMock(return_value=model)\n    batch_gpu = trainer.transfer_batch_to_gpu(batch, 0)\n    expected = torch.device(\'cuda\', 0)\n    assert model.hook_called\n    assert batch_gpu.samples.device == batch_gpu.targets.device == expected\n'"
tests/models/test_horovod.py,2,"b'import json\nimport os\nimport platform\nimport shlex\nimport subprocess\nimport sys\n\nimport pytest\nimport torch\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom tests.base import EvalModelTemplate\nfrom tests.base.models import TestGAN\n\ntry:\n    from horovod.common.util import nccl_built\nexcept ImportError:\n    HOROVOD_AVAILABLE = False\nelse:\n    HOROVOD_AVAILABLE = True\n\n\n# This script will run the actual test model training in parallel\nTEST_SCRIPT = os.path.join(os.path.dirname(__file__), \'data\', \'horovod\', \'train_default_model.py\')\n\n\ndef _nccl_available():\n    if not HOROVOD_AVAILABLE:\n        return False\n\n    try:\n        return nccl_built()\n    except AttributeError:\n        # Horovod 0.19.1 nccl_built() does not yet work with Python 3.8:\n        # See: https://github.com/horovod/horovod/issues/1891\n        return False\n\n\ndef _run_horovod(trainer_options, on_gpu=False):\n    """"""Execute the training script across multiple workers in parallel.""""""\n    tutils.reset_seed()\n    cmdline = [\n        \'horovodrun\',\n        \'-np\', \'2\',\n        sys.executable, TEST_SCRIPT,\n        \'--trainer-options\', shlex.quote(json.dumps(trainer_options))\n    ]\n    if on_gpu:\n        cmdline += [\'--on-gpu\']\n    exit_code = subprocess.call(\' \'.join(cmdline), shell=True, env=os.environ.copy())\n    assert exit_code == 0\n\n\n@pytest.mark.skipif(sys.version_info >= (3, 8), reason=""Horovod not yet supported in Python 3.8"")\n@pytest.mark.skipif(platform.system() == ""Windows"", reason=""Horovod is not supported on Windows"")\ndef test_horovod_cpu(tmpdir):\n    """"""Test Horovod running multi-process on CPU.""""""\n    trainer_options = dict(\n        default_root_dir=str(tmpdir),\n        gradient_clip_val=1.0,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        distributed_backend=\'horovod\',\n        deterministic=True,\n    )\n    _run_horovod(trainer_options)\n\n\n@pytest.mark.skipif(sys.version_info >= (3, 8), reason=""Horovod not yet supported in Python 3.8"")\n@pytest.mark.skipif(platform.system() == ""Windows"", reason=""Horovod is not supported on Windows"")\ndef test_horovod_cpu_implicit(tmpdir):\n    """"""Test Horovod without specifying a backend, inferring from env set by `horovodrun`.""""""\n    trainer_options = dict(\n        default_root_dir=str(tmpdir),\n        gradient_clip_val=1.0,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        deterministic=True,\n    )\n    _run_horovod(trainer_options)\n\n\n@pytest.mark.skipif(sys.version_info >= (3, 8), reason=""Horovod not yet supported in Python 3.8"")\n@pytest.mark.skipif(platform.system() == ""Windows"", reason=""Horovod is not supported on Windows"")\n@pytest.mark.skipif(not _nccl_available(), reason=""test requires Horovod with NCCL support"")\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_horovod_multi_gpu(tmpdir):\n    """"""Test Horovod with multi-GPU support.""""""\n    trainer_options = dict(\n        default_root_dir=str(tmpdir),\n        gradient_clip_val=1.0,\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        gpus=1,\n        deterministic=True,\n        distributed_backend=\'horovod\'\n    )\n    _run_horovod(trainer_options, on_gpu=True)\n\n\n@pytest.mark.skipif(sys.version_info >= (3, 8), reason=""Horovod not yet supported in Python 3.8"")\n@pytest.mark.skipif(platform.system() == ""Windows"", reason=""Horovod is not supported on Windows"")\n@pytest.mark.skipif(not _nccl_available(), reason=""test requires Horovod with NCCL support"")\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""test requires GPU machine"")\ndef test_horovod_transfer_batch_to_gpu(tmpdir):\n\n    class TestTrainingStepModel(EvalModelTemplate):\n        def training_step(self, batch, *args, **kwargs):\n            x, y = batch\n            assert str(x.device) != \'cpu\'\n            assert str(y.device) != \'cpu\'\n            return super(TestTrainingStepModel, self).training_step(batch, *args, **kwargs)\n\n        def validation_step(self, batch, *args, **kwargs):\n            x, y = batch\n            assert str(x.device) != \'cpu\'\n            assert str(y.device) != \'cpu\'\n            return super(TestTrainingStepModel, self).validation_step(batch, *args, **kwargs)\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = TestTrainingStepModel(hparams)\n\n    trainer_options = dict(\n        default_root_dir=str(tmpdir),\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        gpus=1,\n        deterministic=True,\n        distributed_backend=\'horovod\'\n    )\n    tutils.run_model_test_without_loggers(trainer_options, model)\n\n\n@pytest.mark.skipif(sys.version_info >= (3, 8), reason=""Horovod not yet supported in Python 3.8"")\n@pytest.mark.skipif(platform.system() == ""Windows"", reason=""Horovod is not supported on Windows"")\ndef test_horovod_multi_optimizer(tmpdir):\n    model = TestGAN(**EvalModelTemplate.get_default_hparams())\n\n    trainer_options = dict(\n        default_root_dir=str(tmpdir),\n        progress_bar_refresh_rate=0,\n        max_epochs=1,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        deterministic=True,\n        distributed_backend=\'horovod\'\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    assert result == 1, \'model failed to complete\'\n\n    assert len(trainer.optimizers) == 2\n    for i, optimizer in enumerate(trainer.optimizers):\n        assert hasattr(optimizer, \'synchronize\'), \'optimizer has not been wrapped into DistributedOptimizer\'\n\n    def get_model_params(model):\n        return set([p for p in model.parameters()])\n\n    def get_optimizer_params(optimizer):\n        return set([p for group in optimizer.param_groups for p in group.get(\'params\', [])])\n\n    assert get_model_params(model.generator) != get_model_params(model.discriminator)\n    assert get_model_params(model.generator) == get_optimizer_params(trainer.optimizers[0])\n    assert get_model_params(model.discriminator) == get_optimizer_params(trainer.optimizers[1])\n'"
tests/models/test_hparams.py,9,"b'import os\nimport sys\nfrom argparse import Namespace\n\nimport pytest\nimport torch\nfrom omegaconf import OmegaConf, Container\n\nfrom pytorch_lightning import Trainer, LightningModule\nfrom pytorch_lightning.utilities import AttributeDict\nfrom tests.base import EvalModelTemplate\nimport pickle\nimport cloudpickle\n\n\nclass SaveHparamsModel(EvalModelTemplate):\n    """""" Tests that a model can take an object """"""\n    def __init__(self, hparams):\n        super().__init__()\n        self.save_hyperparameters(hparams)\n\n\nclass AssignHparamsModel(EvalModelTemplate):\n    """""" Tests that a model can take an object with explicit setter """"""\n    def __init__(self, hparams):\n        super().__init__()\n        self.hparams = hparams\n\n\n# -------------------------\n# STANDARD TESTS\n# -------------------------\ndef _run_standard_hparams_test(tmpdir, model, cls):\n    """"""\n    Tests for the existence of an arg \'test_arg=14\'\n    """"""\n    # test proper property assignments\n    assert model.hparams.test_arg == 14\n\n    # verify we can train\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=2, overfit_pct=0.5)\n    trainer.fit(model)\n\n    # make sure the raw checkpoint saved the properties\n    raw_checkpoint_path = _raw_checkpoint_path(trainer)\n    raw_checkpoint = torch.load(raw_checkpoint_path)\n    assert LightningModule.CHECKPOINT_KEY_HYPER_PARAMS in raw_checkpoint\n    assert raw_checkpoint[LightningModule.CHECKPOINT_KEY_HYPER_PARAMS][\'test_arg\'] == 14\n\n    # verify that model loads correctly\n    model = cls.load_from_checkpoint(raw_checkpoint_path)\n    assert model.hparams.test_arg == 14\n\n    # todo\n    # verify that we can overwrite the property\n    # model = cls.load_from_checkpoint(raw_checkpoint_path, test_arg=78)\n    # assert model.hparams.test_arg == 78\n\n    return raw_checkpoint_path\n\n\n@pytest.mark.parametrize(""cls"", [SaveHparamsModel, AssignHparamsModel])\ndef test_namespace_hparams(tmpdir, cls):\n    # init model\n    model = cls(hparams=Namespace(test_arg=14))\n\n    # run standard test suite\n    _run_standard_hparams_test(tmpdir, model, cls)\n\n\n@pytest.mark.parametrize(""cls"", [SaveHparamsModel, AssignHparamsModel])\ndef test_dict_hparams(tmpdir, cls):\n    # init model\n    model = cls(hparams={\'test_arg\': 14})\n\n    # run standard test suite\n    _run_standard_hparams_test(tmpdir, model, cls)\n\n\n@pytest.mark.parametrize(""cls"", [SaveHparamsModel, AssignHparamsModel])\ndef test_omega_conf_hparams(tmpdir, cls):\n    # init model\n    conf = OmegaConf.create(dict(test_arg=14, mylist=[15.4, dict(a=1, b=2)]))\n    model = cls(hparams=conf)\n\n    # run standard test suite\n    raw_checkpoint_path = _run_standard_hparams_test(tmpdir, model, cls)\n    model = cls.load_from_checkpoint(raw_checkpoint_path)\n\n    # config specific tests\n    assert model.hparams.test_arg == 14\n    assert model.hparams.mylist[0] == 15.4\n\n\ndef test_explicit_args_hparams(tmpdir):\n    """"""\n    Tests that a model can take implicit args and assign\n    """"""\n\n    # define model\n    class TestModel(EvalModelTemplate):\n        def __init__(self, test_arg, test_arg2):\n            super().__init__()\n            self.save_hyperparameters(\'test_arg\', \'test_arg2\')\n\n    model = TestModel(test_arg=14, test_arg2=90)\n\n    # run standard test suite\n    raw_checkpoint_path = _run_standard_hparams_test(tmpdir, model, TestModel)\n    model = TestModel.load_from_checkpoint(raw_checkpoint_path, test_arg2=120)\n\n    # config specific tests\n    assert model.hparams.test_arg2 == 120\n\n\ndef test_implicit_args_hparams(tmpdir):\n    """"""\n    Tests that a model can take regular args and assign\n    """"""\n\n    # define model\n    class TestModel(EvalModelTemplate):\n        def __init__(self, test_arg, test_arg2):\n            super().__init__()\n            self.save_hyperparameters()\n\n    model = TestModel(test_arg=14, test_arg2=90)\n\n    # run standard test suite\n    raw_checkpoint_path = _run_standard_hparams_test(tmpdir, model, TestModel)\n    model = TestModel.load_from_checkpoint(raw_checkpoint_path, test_arg2=120)\n\n    # config specific tests\n    assert model.hparams.test_arg2 == 120\n\n\ndef test_explicit_missing_args_hparams(tmpdir):\n    """"""\n    Tests that a model can take regular args and assign\n    """"""\n\n    # define model\n    class TestModel(EvalModelTemplate):\n        def __init__(self, test_arg, test_arg2):\n            super().__init__()\n            self.save_hyperparameters(\'test_arg\')\n\n    model = TestModel(test_arg=14, test_arg2=90)\n\n    # test proper property assignments\n    assert model.hparams.test_arg == 14\n\n    # verify we can train\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=2, overfit_pct=0.5)\n    trainer.fit(model)\n\n    # make sure the raw checkpoint saved the properties\n    raw_checkpoint_path = _raw_checkpoint_path(trainer)\n    raw_checkpoint = torch.load(raw_checkpoint_path)\n    assert LightningModule.CHECKPOINT_KEY_HYPER_PARAMS in raw_checkpoint\n    assert raw_checkpoint[LightningModule.CHECKPOINT_KEY_HYPER_PARAMS][\'test_arg\'] == 14\n\n    # verify that model loads correctly\n    model = TestModel.load_from_checkpoint(raw_checkpoint_path, test_arg2=123)\n    assert model.hparams.test_arg == 14\n    assert \'test_arg2\' not in model.hparams  # test_arg2 is not registered in class init\n\n    return raw_checkpoint_path\n\n# -------------------------\n# SPECIFIC TESTS\n# -------------------------\n\n\ndef test_class_nesting():\n\n    class MyModule(LightningModule):\n        def forward(self):\n            ...\n\n    # make sure PL modules are always nn.Module\n    a = MyModule()\n    assert isinstance(a, torch.nn.Module)\n\n    def test_outside():\n        a = MyModule()\n        _ = a.hparams\n\n    class A:\n        def test(self):\n            a = MyModule()\n            _ = a.hparams\n\n        def test2(self):\n            test_outside()\n\n    test_outside()\n    A().test2()\n    A().test()\n\n\n@pytest.mark.xfail(sys.version_info >= (3, 6), reason=\'OmegaConf only for Python >= 3.8\')\ndef test_omegaconf(tmpdir):\n    class OmegaConfModel(EvalModelTemplate):\n        def __init__(self, ogc):\n            super().__init__()\n            self.ogc = ogc\n            self.size = ogc.list[0]\n\n    conf = OmegaConf.create({""k"": ""v"", ""list"": [15.4, {""a"": ""1"", ""b"": ""2""}]})\n    model = OmegaConfModel(conf)\n\n    # ensure ogc passed values correctly\n    assert model.size == 15.4\n\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=2, overfit_pct=0.5)\n    result = trainer.fit(model)\n\n    assert result == 1\n\n\n# class SubClassEvalModel(EvalModelTemplate):\n#     any_other_loss = torch.nn.CrossEntropyLoss()\n#\n#     def __init__(self, *args, subclass_arg=1200, **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.save_hyperparameters()\n#\n#\n# class SubSubClassEvalModel(SubClassEvalModel):\n#     pass\n#\n#\n# class AggSubClassEvalModel(SubClassEvalModel):\n#\n#     def __init__(self, *args, my_loss=torch.nn.CrossEntropyLoss(), **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.save_hyperparameters()\n#\n#\n# class UnconventionalArgsEvalModel(EvalModelTemplate):\n#     """""" A model that has unconventional names for ""self"", ""*args"" and ""**kwargs"". """"""\n#\n#     def __init__(obj, *more_args, other_arg=300, **more_kwargs):\n#         # intentionally named obj\n#         super().__init__(*more_args, **more_kwargs)\n#         obj.save_hyperparameters()\n#\n#\n# class DictConfSubClassEvalModel(SubClassEvalModel):\n#     def __init__(self, *args, dict_conf=OmegaConf.create(dict(my_param=\'something\')), **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.save_hyperparameters()\n#\n#\n# @pytest.mark.parametrize(""cls"", [\n#     EvalModelTemplate,\n#     SubClassEvalModel,\n#     SubSubClassEvalModel,\n#     AggSubClassEvalModel,\n#     UnconventionalArgsEvalModel,\n#     DictConfSubClassEvalModel,\n# ])\n# def test_collect_init_arguments(tmpdir, cls):\n#     """""" Test that the model automatically saves the arguments passed into the constructor """"""\n#     extra_args = {}\n#     if cls is AggSubClassEvalModel:\n#         extra_args.update(my_loss=torch.nn.CosineEmbeddingLoss())\n#     elif cls is DictConfSubClassEvalModel:\n#         extra_args.update(dict_conf=OmegaConf.create(dict(my_param=\'anything\')))\n#\n#     model = cls(**extra_args)\n#     assert model.batch_size == 32\n#     model = cls(batch_size=179, **extra_args)\n#     assert model.batch_size == 179\n#\n#     if isinstance(model, SubClassEvalModel):\n#         assert model.subclass_arg == 1200\n#\n#     if isinstance(model, AggSubClassEvalModel):\n#         assert isinstance(model.my_loss, torch.nn.CosineEmbeddingLoss)\n#\n#     # verify that the checkpoint saved the correct values\n#     trainer = Trainer(default_root_dir=tmpdir, max_epochs=2, overfit_pct=0.5)\n#     trainer.fit(model)\n#     raw_checkpoint_path = _raw_checkpoint_path(trainer)\n#\n#     raw_checkpoint = torch.load(raw_checkpoint_path)\n#     assert LightningModule.CHECKPOINT_KEY_HYPER_PARAMS in raw_checkpoint\n#     assert raw_checkpoint[LightningModule.CHECKPOINT_KEY_HYPER_PARAMS][\'batch_size\'] == 179\n#\n#     # verify that model loads correctly\n#     model = cls.load_from_checkpoint(raw_checkpoint_path)\n#     assert model.batch_size == 179\n#\n#     if isinstance(model, AggSubClassEvalModel):\n#         assert isinstance(model.my_loss, torch.nn.CrossEntropyLoss)\n#\n#     if isinstance(model, DictConfSubClassEvalModel):\n#         assert isinstance(model.dict_conf, DictConfig)\n#         assert model.dict_conf == \'anything\'\n#\n#     # verify that we can overwrite whatever we want\n#     model = cls.load_from_checkpoint(raw_checkpoint_path, batch_size=99)\n#     assert model.batch_size == 99\n\n\ndef _raw_checkpoint_path(trainer) -> str:\n    raw_checkpoint_paths = os.listdir(trainer.checkpoint_callback.dirpath)\n    raw_checkpoint_paths = [x for x in raw_checkpoint_paths if \'.ckpt\' in x]\n    assert raw_checkpoint_paths\n    raw_checkpoint_path = raw_checkpoint_paths[0]\n    raw_checkpoint_path = os.path.join(trainer.checkpoint_callback.dirpath, raw_checkpoint_path)\n    return raw_checkpoint_path\n\n\nclass LocalVariableModelSuperLast(EvalModelTemplate):\n    """""" This model has the super().__init__() call at the end. """"""\n\n    def __init__(self, arg1, arg2, *args, **kwargs):\n        self.argument1 = arg1  # arg2 intentionally not set\n        arg1 = \'overwritten\'\n        local_var = 1234\n        super().__init__(*args, **kwargs)  # this is intentionally here at the end\n\n\nclass LocalVariableModelSuperFirst(EvalModelTemplate):\n    """""" This model has the _auto_collect_arguments() call at the end. """"""\n\n    def __init__(self, arg1, arg2, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.argument1 = arg1  # arg2 intentionally not set\n        arg1 = \'overwritten\'\n        local_var = 1234\n        self.save_hyperparameters()  # this is intentionally here at the end\n\n\n@pytest.mark.parametrize(""cls"", [\n    LocalVariableModelSuperFirst,\n    # LocalVariableModelSuperLast,\n])\ndef test_collect_init_arguments_with_local_vars(cls):\n    """""" Tests that only the arguments are collected and not local variables. """"""\n    model = cls(arg1=1, arg2=2)\n    assert \'local_var\' not in model.hparams\n    assert model.hparams[\'arg1\'] == \'overwritten\'\n    assert model.hparams[\'arg2\'] == 2\n\n\n# @pytest.mark.parametrize(""cls,config"", [\n#     (SaveHparamsModel, Namespace(my_arg=42)),\n#     (SaveHparamsModel, dict(my_arg=42)),\n#     (SaveHparamsModel, OmegaConf.create(dict(my_arg=42))),\n#     (AssignHparamsModel, Namespace(my_arg=42)),\n#     (AssignHparamsModel, dict(my_arg=42)),\n#     (AssignHparamsModel, OmegaConf.create(dict(my_arg=42))),\n# ])\n# def test_single_config_models(tmpdir, cls, config):\n#     """""" Test that the model automatically saves the arguments passed into the constructor """"""\n#     model = cls(config)\n#\n#     # no matter how you do it, it should be assigned\n#     assert model.hparams.my_arg == 42\n#\n#     # verify that the checkpoint saved the correct values\n#     trainer = Trainer(default_root_dir=tmpdir, max_epochs=2, overfit_pct=0.5)\n#     trainer.fit(model)\n#\n#     # verify that model loads correctly\n#     raw_checkpoint_path = _raw_checkpoint_path(trainer)\n#     model = cls.load_from_checkpoint(raw_checkpoint_path)\n#     assert model.hparams.my_arg == 42\n\n\nclass AnotherArgModel(EvalModelTemplate):\n    def __init__(self, arg1):\n        super().__init__()\n        self.save_hyperparameters(arg1)\n\n\nclass OtherArgsModel(EvalModelTemplate):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.save_hyperparameters(arg1, arg2)\n\n\n@pytest.mark.parametrize(""cls,config"", [\n    (AnotherArgModel, dict(arg1=42)),\n    (OtherArgsModel, dict(arg1=3.14, arg2=\'abc\')),\n])\ndef test_single_config_models_fail(tmpdir, cls, config):\n    """""" Test fail on passing unsupported config type. """"""\n    with pytest.raises(ValueError):\n        _ = cls(**config)\n\n\ndef test_hparams_pickle(tmpdir):\n    ad = AttributeDict({\'key1\': 1, \'key2\': \'abc\'})\n    pkl = pickle.dumps(ad)\n    assert ad == pickle.loads(pkl)\n    pkl = cloudpickle.dumps(ad)\n    assert ad == pickle.loads(pkl)\n'"
tests/models/test_restore.py,4,"b'import glob\nimport logging as log\nimport os\nimport pickle\n\nimport cloudpickle\nimport pytest\nimport torch\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom tests.base import EvalModelTemplate\n\n\n@pytest.mark.spawn\n@pytest.mark.parametrize(""backend"", [\'dp\', \'ddp\'])\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_running_test_pretrained_model_distrib(tmpdir, backend):\n    """"""Verify `test()` on pretrained model.""""""\n    tutils.set_random_master_port()\n\n    model = EvalModelTemplate()\n\n    # exp file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    # exp file to get weights\n    checkpoint = tutils.init_checkpoint_callback(logger)\n\n    trainer_options = dict(\n        progress_bar_refresh_rate=0,\n        max_epochs=2,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        checkpoint_callback=checkpoint,\n        logger=logger,\n        gpus=[0, 1],\n        distributed_backend=backend,\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n\n    log.info(os.listdir(tutils.get_data_path(logger, path_dir=tmpdir)))\n\n    # correct result and ok accuracy\n    assert result == 1, \'training failed to complete\'\n    pretrained_model = tutils.load_model(logger,\n                                         trainer.checkpoint_callback.dirpath,\n                                         module_class=EvalModelTemplate)\n\n    # run test set\n    new_trainer = Trainer(**trainer_options)\n    new_trainer.test(pretrained_model)\n\n    # test we have good test accuracy\n    tutils.assert_ok_model_acc(new_trainer)\n\n    dataloaders = model.test_dataloader()\n    if not isinstance(dataloaders, list):\n        dataloaders = [dataloaders]\n\n    for dataloader in dataloaders:\n        tutils.run_prediction(dataloader, pretrained_model)\n\n\ndef test_running_test_pretrained_model_cpu(tmpdir):\n    """"""Verify test() on pretrained model.""""""\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    # logger file to get weights\n    checkpoint = tutils.init_checkpoint_callback(logger)\n\n    trainer_options = dict(\n        progress_bar_refresh_rate=0,\n        max_epochs=3,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        checkpoint_callback=checkpoint,\n        logger=logger\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n\n    # correct result and ok accuracy\n    assert result == 1, \'training failed to complete\'\n    pretrained_model = tutils.load_model(\n        logger, trainer.checkpoint_callback.dirpath, module_class=EvalModelTemplate\n    )\n\n    new_trainer = Trainer(**trainer_options)\n    new_trainer.test(pretrained_model)\n\n    # test we have good test accuracy\n    tutils.assert_ok_model_acc(new_trainer)\n\n\ndef test_load_model_from_checkpoint(tmpdir):\n    """"""Verify test() on pretrained model.""""""\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    trainer_options = dict(\n        progress_bar_refresh_rate=0,\n        max_epochs=2,\n        train_percent_check=0.4,\n        val_percent_check=0.2,\n        checkpoint_callback=ModelCheckpoint(tmpdir, save_top_k=-1),\n        default_root_dir=tmpdir,\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    trainer.test()\n\n    # correct result and ok accuracy\n    assert result == 1, \'training failed to complete\'\n\n    # load last checkpoint\n    last_checkpoint = sorted(glob.glob(os.path.join(trainer.checkpoint_callback.dirpath, ""*.ckpt"")))[-1]\n    pretrained_model = EvalModelTemplate.load_from_checkpoint(last_checkpoint)\n\n    # test that hparams loaded correctly\n    for k, v in hparams.items():\n        assert getattr(pretrained_model, k) == v\n\n    # assert weights are the same\n    for (old_name, old_p), (new_name, new_p) in zip(model.named_parameters(), pretrained_model.named_parameters()):\n        assert torch.all(torch.eq(old_p, new_p)), \'loaded weights are not the same as the saved weights\'\n\n    new_trainer = Trainer(**trainer_options)\n    new_trainer.test(pretrained_model)\n\n    # test we have good test accuracy\n    tutils.assert_ok_model_acc(new_trainer)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""test requires multi-GPU machine"")\ndef test_dp_resume(tmpdir):\n    """"""Make sure DP continues training correctly.""""""\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    trainer_options = dict(\n        max_epochs=1,\n        gpus=2,\n        distributed_backend=\'dp\',\n    )\n\n    # get logger\n    logger = tutils.get_default_logger(tmpdir)\n\n    # exp file to get weights\n    # logger file to get weights\n    checkpoint = tutils.init_checkpoint_callback(logger)\n\n    # add these to the trainer options\n    trainer_options[\'logger\'] = logger\n    trainer_options[\'checkpoint_callback\'] = checkpoint\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    trainer.is_slurm_managing_tasks = True\n    result = trainer.fit(model)\n\n    # track epoch before saving. Increment since we finished the current epoch, don\'t want to rerun\n    real_global_epoch = trainer.current_epoch + 1\n\n    # correct result and ok accuracy\n    assert result == 1, \'amp + dp model failed to complete\'\n\n    # ---------------------------\n    # HPC LOAD/SAVE\n    # ---------------------------\n    # save\n    trainer.hpc_save(tmpdir, logger)\n\n    # init new trainer\n    new_logger = tutils.get_default_logger(tmpdir, version=logger.version)\n    trainer_options[\'logger\'] = new_logger\n    trainer_options[\'checkpoint_callback\'] = ModelCheckpoint(tmpdir)\n    trainer_options[\'train_percent_check\'] = 0.5\n    trainer_options[\'val_percent_check\'] = 0.2\n    trainer_options[\'max_epochs\'] = 1\n    new_trainer = Trainer(**trainer_options)\n\n    # set the epoch start hook so we can predict before the model does the full training\n    def assert_good_acc():\n        assert new_trainer.current_epoch == real_global_epoch and new_trainer.current_epoch > 0\n\n        # if model and state loaded correctly, predictions will be good even though we\n        # haven\'t trained with the new loaded model\n        dp_model = new_trainer.model\n        dp_model.eval()\n\n        dataloader = trainer.train_dataloader\n        tutils.run_prediction(dataloader, dp_model, dp=True)\n\n    # new model\n    model = EvalModelTemplate(**hparams)\n    model.on_train_start = assert_good_acc\n\n    # fit new model which should load hpc weights\n    new_trainer.fit(model)\n\n    # test freeze on gpu\n    model.freeze()\n    model.unfreeze()\n\n\ndef test_model_saving_loading(tmpdir):\n    """"""Tests use case where trainer saves the model, and user loads it from tags independently.""""""\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    trainer_options = dict(\n        max_epochs=1,\n        logger=logger,\n        checkpoint_callback=ModelCheckpoint(tmpdir)\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n\n    # traning complete\n    assert result == 1, \'amp + ddp model failed to complete\'\n\n    # make a prediction\n    dataloaders = model.test_dataloader()\n    if not isinstance(dataloaders, list):\n        dataloaders = [dataloaders]\n\n    for dataloader in dataloaders:\n        for batch in dataloader:\n            break\n\n    x, y = batch\n    x = x.view(x.size(0), -1)\n\n    # generate preds before saving model\n    model.eval()\n    pred_before_saving = model(x)\n\n    # save model\n    new_weights_path = os.path.join(tmpdir, \'save_test.ckpt\')\n    trainer.save_checkpoint(new_weights_path)\n\n    # load new model\n    hparams_path = tutils.get_data_path(logger, path_dir=tmpdir)\n    hparams_path = os.path.join(hparams_path, \'hparams.yaml\')\n    model_2 = EvalModelTemplate.load_from_checkpoint(\n        checkpoint_path=new_weights_path,\n        hparams_file=hparams_path\n    )\n    model_2.eval()\n\n    # make prediction\n    # assert that both predictions are the same\n    new_pred = model_2(x)\n    assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1\n\n\ndef test_model_pickle(tmpdir):\n    model = EvalModelTemplate()\n    pickle.dumps(model)\n    cloudpickle.dumps(model)\n'"
tests/trainer/__init__.py,0,b''
tests/trainer/test_checks.py,0,"b'import pytest\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer, LightningModule\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\n\n# TODO: add matching messages\n\n\ndef test_wrong_train_setting(tmpdir):\n    """"""\n    * Test that an error is thrown when no `training_dataloader()` is defined\n    * Test that an error is thrown when no `training_step()` is defined\n    """"""\n    tutils.reset_seed()\n    hparams = EvalModelTemplate.get_default_hparams()\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1)\n\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.train_dataloader = None\n        trainer.fit(model)\n\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.training_step = None\n        trainer.fit(model)\n\n\ndef test_wrong_configure_optimizers(tmpdir):\n    """""" Test that an error is thrown when no `configure_optimizers()` is defined """"""\n    tutils.reset_seed()\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1)\n\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate()\n        model.configure_optimizers = None\n        trainer.fit(model)\n\n\ndef test_wrong_validation_settings(tmpdir):\n    """""" Test the following cases related to validation configuration of model:\n        * error if `val_dataloader()` is overridden but `validation_step()` is not\n        * if both `val_dataloader()` and `validation_step()` is overridden,\n            throw warning if `val_epoch_end()` is not defined\n        * error if `validation_step()` is overridden but `val_dataloader()` is not\n    """"""\n    tutils.reset_seed()\n    hparams = EvalModelTemplate.get_default_hparams()\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1)\n\n    # check val_dataloader -> val_step\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.validation_step = None\n        trainer.fit(model)\n\n    # check val_dataloader + val_step -> val_epoch_end\n    with pytest.warns(RuntimeWarning):\n        model = EvalModelTemplate(**hparams)\n        model.validation_epoch_end = None\n        trainer.fit(model)\n\n    # check val_step -> val_dataloader\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.val_dataloader = None\n        trainer.fit(model)\n\n\ndef test_wrong_test_settigs(tmpdir):\n    """""" Test the following cases related to test configuration of model:\n        * error if `test_dataloader()` is overridden but `test_step()` is not\n        * if both `test_dataloader()` and `test_step()` is overridden,\n            throw warning if `test_epoch_end()` is not defined\n        * error if `test_step()` is overridden but `test_dataloader()` is not\n    """"""\n    hparams = EvalModelTemplate.get_default_hparams()\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1)\n\n    # ----------------\n    # if have test_dataloader should  have test_step\n    # ----------------\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.test_step = None\n        trainer.fit(model)\n\n    # ----------------\n    # if have test_dataloader  and  test_step recommend test_epoch_end\n    # ----------------\n    with pytest.warns(RuntimeWarning):\n        model = EvalModelTemplate(**hparams)\n        model.test_epoch_end = None\n        trainer.test(model)\n\n    # ----------------\n    # if have test_step and NO test_dataloader passed in tell user to pass test_dataloader\n    # ----------------\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.test_dataloader = LightningModule.test_dataloader\n        trainer.test(model)\n\n    # ----------------\n    # if have test_dataloader and NO test_step tell user to implement  test_step\n    # ----------------\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.test_dataloader = LightningModule.test_dataloader\n        model.test_step = None\n        trainer.test(model, test_dataloaders=model.dataloader(train=False))\n\n    # ----------------\n    # if have test_dataloader and test_step but no test_epoch_end warn user\n    # ----------------\n    with pytest.warns(RuntimeWarning):\n        model = EvalModelTemplate(**hparams)\n        model.test_dataloader = LightningModule.test_dataloader\n        model.test_epoch_end = None\n        trainer.test(model, test_dataloaders=model.dataloader(train=False))\n'"
tests/trainer/test_dataloaders.py,9,"b'import platform\n\nimport pytest\nimport torch\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Subset\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\n\ndef test_fit_train_loader_only(tmpdir):\n\n    model = EvalModelTemplate()\n    train_dataloader = model.train_dataloader()\n\n    model.train_dataloader = None\n    model.val_dataloader = None\n    model.test_dataloader = None\n\n    model.validation_step = None\n    model.validation_epoch_end = None\n\n    model.test_step = None\n    model.test_epoch_end = None\n\n    trainer = Trainer(fast_dev_run=True, default_root_dir=tmpdir)\n    trainer.fit(model, train_dataloader=train_dataloader)\n\n\ndef test_fit_val_loader_only(tmpdir):\n\n    model = EvalModelTemplate()\n    train_dataloader = model.train_dataloader()\n    val_dataloader = model.val_dataloader()\n\n    model.train_dataloader = None\n    model.val_dataloader = None\n    model.test_dataloader = None\n\n    model.test_step = None\n    model.test_epoch_end = None\n\n    trainer = Trainer(fast_dev_run=True, default_root_dir=tmpdir)\n    trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)\n\n\n@pytest.mark.parametrize(""dataloader_options"", [\n    dict(train_percent_check=-0.1),\n    dict(train_percent_check=1.1),\n    dict(val_check_interval=1.1),\n    dict(val_check_interval=10000),\n])\ndef test_dataloader_config_errors(tmpdir, dataloader_options):\n\n    model = EvalModelTemplate()\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        **dataloader_options,\n    )\n\n    with pytest.raises(ValueError):\n        trainer.fit(model)\n\n\ndef test_multiple_val_dataloader(tmpdir):\n    """"""Verify multiple val_dataloader.""""""\n\n    model = EvalModelTemplate()\n    model.val_dataloader = model.val_dataloader__multiple\n    model.validation_step = model.validation_step__multiple_dataloaders\n    model.validation_epoch_end = model.validation_epoch_end_multiple_dataloaders\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=1.0,\n    )\n    result = trainer.fit(model)\n\n    # verify training completed\n    assert result == 1\n\n    # verify there are 2 val loaders\n    assert len(trainer.val_dataloaders) == 2, \\\n        \'Multiple val_dataloaders not initiated properly\'\n\n    # make sure predictions are good for each val set\n    for dataloader in trainer.val_dataloaders:\n        tutils.run_prediction(dataloader, trainer.model)\n\n\ndef test_multiple_test_dataloader(tmpdir):\n    """"""Verify multiple test_dataloader.""""""\n\n    model = EvalModelTemplate()\n    model.test_dataloader = model.test_dataloader__multiple\n    model.test_step = model.test_step__multiple_dataloaders\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    trainer.fit(model)\n    trainer.test()\n\n    # verify there are 2 test loaders\n    assert len(trainer.test_dataloaders) == 2, \\\n        \'Multiple test_dataloaders not initiated properly\'\n\n    # make sure predictions are good for each test set\n    for dataloader in trainer.test_dataloaders:\n        tutils.run_prediction(dataloader, trainer.model)\n\n    # run the test method\n    trainer.test()\n\n\ndef test_train_dataloader_passed_to_fit(tmpdir):\n    """"""Verify that train dataloader can be passed to fit """"""\n\n    # only train passed to fit\n    model = EvalModelTemplate()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    fit_options = dict(train_dataloader=model.dataloader(train=True))\n    result = trainer.fit(model, **fit_options)\n\n    assert result == 1\n\n\ndef test_train_val_dataloaders_passed_to_fit(tmpdir):\n    """""" Verify that train & val dataloader can be passed to fit """"""\n\n    # train, val passed to fit\n    model = EvalModelTemplate()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    fit_options = dict(train_dataloader=model.dataloader(train=True),\n                       val_dataloaders=model.dataloader(train=False))\n\n    result = trainer.fit(model, **fit_options)\n    assert result == 1\n    assert len(trainer.val_dataloaders) == 1, \\\n        f\'`val_dataloaders` not initiated properly, got {trainer.val_dataloaders}\'\n\n\ndef test_all_dataloaders_passed_to_fit(tmpdir):\n    """"""Verify train, val & test dataloader(s) can be passed to fit and test method""""""\n\n    model = EvalModelTemplate()\n\n    # train, val and test passed to fit\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    fit_options = dict(train_dataloader=model.dataloader(train=True),\n                       val_dataloaders=model.dataloader(train=False))\n    test_options = dict(test_dataloaders=model.dataloader(train=False))\n\n    result = trainer.fit(model, **fit_options)\n    trainer.test(**test_options)\n\n    assert result == 1\n    assert len(trainer.val_dataloaders) == 1, \\\n        f\'val_dataloaders` not initiated properly, got {trainer.val_dataloaders}\'\n    assert len(trainer.test_dataloaders) == 1, \\\n        f\'test_dataloaders` not initiated properly, got {trainer.test_dataloaders}\'\n\n\ndef test_multiple_dataloaders_passed_to_fit(tmpdir):\n    """"""Verify that multiple val & test dataloaders can be passed to fit.""""""\n\n    model = EvalModelTemplate()\n    model.validation_step = model.validation_step__multiple_dataloaders\n    model.validation_epoch_end = model.validation_epoch_end_multiple_dataloaders\n    model.test_step = model.test_step__multiple_dataloaders\n\n    # train, multiple val and multiple test passed to fit\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    fit_options = dict(train_dataloader=model.dataloader(train=True),\n                       val_dataloaders=[model.dataloader(train=False),\n                                        model.dataloader(train=False)])\n    test_options = dict(test_dataloaders=[model.dataloader(train=False),\n                                          model.dataloader(train=False)])\n\n    trainer.fit(model, **fit_options)\n    trainer.test(**test_options)\n\n    assert len(trainer.val_dataloaders) == 2, \\\n        f\'Multiple `val_dataloaders` not initiated properly, got {trainer.val_dataloaders}\'\n    assert len(trainer.test_dataloaders) == 2, \\\n        f\'Multiple `test_dataloaders` not initiated properly, got {trainer.test_dataloaders}\'\n\n\ndef test_mixing_of_dataloader_options(tmpdir):\n    """"""Verify that dataloaders can be passed to fit""""""\n\n    model = EvalModelTemplate()\n\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    results = trainer.fit(model, val_dataloaders=model.dataloader(train=False))\n    assert results\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    results = trainer.fit(model, val_dataloaders=model.dataloader(train=False))\n    assert results\n    trainer.test(test_dataloaders=model.dataloader(train=False))\n\n    assert len(trainer.val_dataloaders) == 1, \\\n        f\'`val_dataloaders` not initiated properly, got {trainer.val_dataloaders}\'\n    assert len(trainer.test_dataloaders) == 1, \\\n        f\'`test_dataloaders` not initiated properly, got {trainer.test_dataloaders}\'\n\n\n@pytest.mark.skip(\'TODO: speed up this test\')\ndef test_train_inf_dataloader_error(tmpdir):\n    """"""Test inf train data loader (e.g. IterableDataset)""""""\n    model = EvalModelTemplate()\n    model.train_dataloader = model.train_dataloader__infinite\n\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1, val_check_interval=0.5)\n\n    with pytest.raises(MisconfigurationException, match=\'infinite DataLoader\'):\n        trainer.fit(model)\n\n\n@pytest.mark.skip(\'TODO: speed up this test\')\ndef test_val_inf_dataloader_error(tmpdir):\n    """"""Test inf train data loader (e.g. IterableDataset)""""""\n    model = EvalModelTemplate()\n    model.val_dataloader = model.val_dataloader__infinite\n\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1, val_percent_check=0.5)\n\n    with pytest.raises(MisconfigurationException, match=\'infinite DataLoader\'):\n        trainer.fit(model)\n\n\n@pytest.mark.skip(\'TODO: speed up this test\')\ndef test_test_inf_dataloader_error(tmpdir):\n    """"""Test inf train data loader (e.g. IterableDataset)""""""\n    model = EvalModelTemplate()\n    model.test_dataloader = model.test_dataloader__infinite\n\n    trainer = Trainer(default_root_dir=tmpdir, max_epochs=1, test_percent_check=0.5)\n\n    with pytest.raises(MisconfigurationException, match=\'infinite DataLoader\'):\n        trainer.test(model)\n\n\n@pytest.mark.parametrize(\'check_interval\', [50, 1.0])\n@pytest.mark.skip(\'TODO: speed up this test\')\ndef test_inf_train_dataloader(tmpdir, check_interval):\n    """"""Test inf train data loader (e.g. IterableDataset)""""""\n\n    model = EvalModelTemplate()\n    model.train_dataloader = model.train_dataloader__infinite\n\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_check_interval=check_interval\n    )\n    result = trainer.fit(model)\n    # verify training completed\n    assert result == 1\n\n\n@pytest.mark.parametrize(\'check_interval\', [1.0])\n@pytest.mark.skip(\'TODO: speed up this test\')\ndef test_inf_val_dataloader(tmpdir, check_interval):\n    """"""Test inf val data loader (e.g. IterableDataset)""""""\n\n    model = EvalModelTemplate()\n    model.val_dataloader = model.val_dataloader__infinite\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_check_interval=check_interval,\n    )\n    result = trainer.fit(model)\n\n    # verify training completed\n    assert result == 1\n\n\ndef test_error_on_zero_len_dataloader(tmpdir):\n    """""" Test that error is raised if a zero-length dataloader is defined """"""\n\n    model = EvalModelTemplate()\n    model.train_dataloader = model.train_dataloader__zero_length\n\n    # fit model\n    with pytest.raises(ValueError):\n        trainer = Trainer(\n            default_root_dir=tmpdir,\n            max_epochs=1,\n            train_percent_check=0.1,\n            val_percent_check=0.1,\n            test_percent_check=0.1\n        )\n        trainer.fit(model)\n\n\n@pytest.mark.skipif(platform.system() == \'Windows\', reason=\'Does not apply to Windows platform.\')\ndef test_warning_with_few_workers(tmpdir):\n    """""" Test that error is raised if dataloader with only a few workers is used """"""\n\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    trainer_options = dict(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n\n    train_dl = model.dataloader(train=True)\n    train_dl.num_workers = 0\n\n    val_dl = model.dataloader(train=False)\n    val_dl.num_workers = 0\n\n    train_dl = model.dataloader(train=False)\n    train_dl.num_workers = 0\n\n    fit_options = dict(train_dataloader=train_dl,\n                       val_dataloaders=val_dl)\n    test_options = dict(test_dataloaders=train_dl)\n\n    trainer = Trainer(**trainer_options)\n\n    # fit model\n    with pytest.warns(UserWarning, match=\'train\'):\n        trainer.fit(model, **fit_options)\n\n    with pytest.warns(UserWarning, match=\'val\'):\n        trainer.fit(model, **fit_options)\n\n    with pytest.warns(UserWarning, match=\'test\'):\n        trainer.test(**test_options)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=\'Test requires multiple GPUs\')\ndef test_dataloader_reinit_for_subclass():\n\n    class CustomDataLoader(torch.utils.data.DataLoader):\n        def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\n                     batch_sampler=None, num_workers=0, collate_fn=None,\n                     pin_memory=False, drop_last=False, timeout=0,\n                     worker_init_fn=None, dummy_kwarg=None):\n            super().__init__(dataset, batch_size, shuffle, sampler, batch_sampler,\n                             num_workers, collate_fn, pin_memory, drop_last, timeout,\n                             worker_init_fn)\n\n            self.dummy_kwarg = dummy_kwarg\n\n    trainer = Trainer(\n        gpus=[0, 1],\n        num_nodes=1,\n        distributed_backend=\'ddp\',\n    )\n\n    class CustomDummyObj:\n        sampler = None\n\n    result = trainer.auto_add_sampler(CustomDummyObj(), train=True)\n    assert isinstance(result, CustomDummyObj), ""Wrongly reinstantiated data loader""\n\n    result = trainer.auto_add_sampler(CustomDataLoader(list(range(1000))), train=True)\n    assert isinstance(result, torch.utils.data.DataLoader)\n    assert isinstance(result, CustomDataLoader)\n    assert hasattr(result, \'dummy_kwarg\')\n\n    # Shuffled DataLoader should also work\n    result = trainer.auto_add_sampler(CustomDataLoader(list(range(1000)), shuffle=True), train=True)\n    assert isinstance(result, torch.utils.data.DataLoader)\n    assert isinstance(result, CustomDataLoader)\n    assert hasattr(result, \'dummy_kwarg\')\n\n    class CustomSampler(torch.utils.data.Sampler):\n        pass\n\n    # Should raise an error if existing sampler is being replaced\n    with pytest.raises(MisconfigurationException, match=\'DistributedSampler\'):\n        trainer.auto_add_sampler(\n            CustomDataLoader(list(range(1000)), sampler=CustomSampler(list(range(1000)))), train=True)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 3, reason=\'Test requires multiple GPUs\')\ndef test_batch_size_smaller_than_num_gpus():\n    # we need at least 3 gpus for this test\n    num_gpus = 3\n    batch_size = 3\n\n    class CurrentTestModel(EvalModelTemplate):\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # batch norm doesn\'t work with batch size 1, we replace it\n            self.c_d1_bn = torch.nn.ReLU()\n\n        def training_step(self, *args, **kwargs):\n            output = super().training_step(*args, **kwargs)\n            loss = output[\'loss\']\n            # we make sure to add some metrics to the output dict,\n            # this is essential for this test\n            output[\'progress_bar\'] = {\'train_loss\': loss}\n            return output\n\n        def train_dataloader(self):\n            dataloader = super().train_dataloader()\n            # construct a dataset with a size that is not divisible by num_gpus\n            # therefore the last batch will have a size < num_gpus\n            size = num_gpus * batch_size + (num_gpus - 1)\n            dataset = Subset(dataloader.dataset, range(size))\n            dataloader = DataLoader(\n                dataset,\n                batch_size=self.batch_size,\n                drop_last=False,\n            )\n            return dataloader\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    hparams[\'batch_size\'] = batch_size\n    model = CurrentTestModel(**hparams)\n\n    trainer = Trainer(\n        max_epochs=1,\n        train_percent_check=0.1,\n        val_percent_check=0,\n        gpus=num_gpus,\n    )\n\n    # we expect the reduction for the metrics also to happen on the last batch\n    # where we will get fewer metrics than gpus\n    result = trainer.fit(model)\n    assert 1 == result\n'"
tests/trainer/test_lr_finder.py,1,"b'import pytest\nimport torch\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\n\ndef test_error_on_more_than_1_optimizer(tmpdir):\n    """""" Check that error is thrown when more than 1 optimizer is passed """"""\n\n    model = EvalModelTemplate()\n    model.configure_optimizers = model.configure_optimizers__multiple_schedulers\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=1\n    )\n\n    with pytest.raises(MisconfigurationException):\n        trainer.lr_find(model)\n\n\ndef test_model_reset_correctly(tmpdir):\n    """""" Check that model weights are correctly reset after lr_find() """"""\n\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=1\n    )\n\n    before_state_dict = model.state_dict()\n\n    _ = trainer.lr_find(model, num_training=5)\n\n    after_state_dict = model.state_dict()\n\n    for key in before_state_dict.keys():\n        assert torch.all(torch.eq(before_state_dict[key], after_state_dict[key])), \\\n            \'Model was not reset correctly after learning rate finder\'\n\n\ndef test_trainer_reset_correctly(tmpdir):\n    """""" Check that all trainer parameters are reset correctly after lr_find() """"""\n\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=1\n    )\n\n    changed_attributes = [\'callbacks\', \'logger\', \'max_steps\', \'auto_lr_find\',\n                          \'early_stop_callback\', \'accumulate_grad_batches\',\n                          \'enable_early_stop\', \'checkpoint_callback\']\n    attributes_before = {}\n    for ca in changed_attributes:\n        attributes_before[ca] = getattr(trainer, ca)\n\n    _ = trainer.lr_find(model, num_training=5)\n\n    attributes_after = {}\n    for ca in changed_attributes:\n        attributes_after[ca] = getattr(trainer, ca)\n\n    for key in changed_attributes:\n        assert attributes_before[key] == attributes_after[key], \\\n            f\'Attribute {key} was not reset correctly after learning rate finder\'\n\n\ndef test_trainer_arg_bool(tmpdir):\n    """""" Test that setting trainer arg to bool works """"""\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n    before_lr = hparams.get(\'learning_rate\')\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=2,\n        auto_lr_find=True\n    )\n\n    trainer.fit(model)\n    after_lr = model.learning_rate\n    assert before_lr != after_lr, \\\n        \'Learning rate was not altered after running learning rate finder\'\n\n\ndef test_trainer_arg_str(tmpdir):\n    """""" Test that setting trainer arg to string works """"""\n    model = EvalModelTemplate()\n    model.my_fancy_lr = 1.0  # update with non-standard field\n\n    before_lr = model.my_fancy_lr\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=2,\n        auto_lr_find=\'my_fancy_lr\'\n    )\n\n    trainer.fit(model)\n    after_lr = model.my_fancy_lr\n    assert before_lr != after_lr, \\\n        \'Learning rate was not altered after running learning rate finder\'\n\n\ndef test_call_to_trainer_method(tmpdir):\n    """""" Test that directly calling the trainer method works """"""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    before_lr = hparams.get(\'learning_rate\')\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=2,\n    )\n\n    lrfinder = trainer.lr_find(model, mode=\'linear\')\n    after_lr = lrfinder.suggestion()\n    model.learning_rate = after_lr\n    trainer.fit(model)\n\n    assert before_lr != after_lr, \\\n        \'Learning rate was not altered after running learning rate finder\'\n\n\n@pytest.mark.skip(\'TODO: speed up this test\')\ndef test_accumulation_and_early_stopping(tmpdir):\n    """""" Test that early stopping of learning rate finder works, and that\n        accumulation also works for this feature """"""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    before_lr = hparams.get(\'learning_rate\')\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        accumulate_grad_batches=2,\n    )\n\n    lrfinder = trainer.lr_find(model, early_stop_threshold=None)\n    after_lr = lrfinder.suggestion()\n\n    assert before_lr != after_lr, \\\n        \'Learning rate was not altered after running learning rate finder\'\n    assert len(lrfinder.results[\'lr\']) == 100, \\\n        \'Early stopping for learning rate finder did not work\'\n    assert lrfinder._total_batch_idx == 100 * 2, \\\n        \'Accumulation parameter did not work\'\n\n\ndef test_suggestion_parameters_work(tmpdir):\n    """""" Test that default skipping does not alter results in basic case """"""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=3,\n    )\n\n    lrfinder = trainer.lr_find(model)\n    lr1 = lrfinder.suggestion(skip_begin=10)  # default\n    lr2 = lrfinder.suggestion(skip_begin=80)  # way too high, should have an impact\n\n    assert lr1 != lr2, \\\n        \'Skipping parameter did not influence learning rate\'\n\n\ndef test_suggestion_with_non_finite_values(tmpdir):\n    """""" Test that non-finite values does not alter results """"""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(hparams)\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=3\n    )\n\n    lrfinder = trainer.lr_find(model)\n    before_lr = lrfinder.suggestion()\n    lrfinder.results[\'loss\'][-1] = float(\'nan\')\n    after_lr = lrfinder.suggestion()\n\n    assert before_lr == after_lr, \\\n        \'Learning rate was altered because of non-finite loss values\'\n'"
tests/trainer/test_optimizers.py,5,"b'import pytest\nimport torch\n\nfrom pytorch_lightning import Trainer\nfrom tests.base import EvalModelTemplate\n\n\ndef test_optimizer_with_scheduling(tmpdir):\n    """""" Verify that learning rate scheduling is working """"""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n    model.configure_optimizers = model.configure_optimizers__single_scheduler\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    results = trainer.fit(model)\n    assert results == 1\n\n    init_lr = hparams.get(\'learning_rate\')\n    adjusted_lr = [pg[\'lr\'] for pg in trainer.optimizers[0].param_groups]\n\n    assert len(trainer.lr_schedulers) == 1, \\\n        \'lr scheduler not initialized properly, it has %i elements instread of 1\' % len(trainer.lr_schedulers)\n\n    assert all(a == adjusted_lr[0] for a in adjusted_lr), \\\n        \'Lr not equally adjusted for all param groups\'\n    adjusted_lr = adjusted_lr[0]\n\n    assert init_lr * 0.1 == adjusted_lr, \\\n        \'Lr not adjusted correctly, expected %f but got %f\' % (init_lr * 0.1, adjusted_lr)\n\n\ndef test_multi_optimizer_with_scheduling(tmpdir):\n    """""" Verify that learning rate scheduling is working """"""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n    model.configure_optimizers = model.configure_optimizers__multiple_schedulers\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    results = trainer.fit(model)\n    assert results == 1\n\n    init_lr = hparams.get(\'learning_rate\')\n    adjusted_lr1 = [pg[\'lr\'] for pg in trainer.optimizers[0].param_groups]\n    adjusted_lr2 = [pg[\'lr\'] for pg in trainer.optimizers[1].param_groups]\n\n    assert len(trainer.lr_schedulers) == 2, \\\n        \'all lr scheduler not initialized properly, it has %i elements instread of 1\' % len(trainer.lr_schedulers)\n\n    assert all(a == adjusted_lr1[0] for a in adjusted_lr1), \\\n        \'Lr not equally adjusted for all param groups for optimizer 1\'\n    adjusted_lr1 = adjusted_lr1[0]\n\n    assert all(a == adjusted_lr2[0] for a in adjusted_lr2), \\\n        \'Lr not equally adjusted for all param groups for optimizer 2\'\n    adjusted_lr2 = adjusted_lr2[0]\n\n    assert init_lr * 0.1 == adjusted_lr1 and init_lr * 0.1 == adjusted_lr2, \\\n        \'Lr not adjusted correctly, expected %f but got %f\' % (init_lr * 0.1, adjusted_lr1)\n\n\ndef test_multi_optimizer_with_scheduling_stepping(tmpdir):\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n    model.configure_optimizers = model.configure_optimizers__multiple_schedulers\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    results = trainer.fit(model)\n    assert results == 1\n\n    init_lr = hparams.get(\'learning_rate\')\n    adjusted_lr1 = [pg[\'lr\'] for pg in trainer.optimizers[0].param_groups]\n    adjusted_lr2 = [pg[\'lr\'] for pg in trainer.optimizers[1].param_groups]\n\n    assert len(trainer.lr_schedulers) == 2, \\\n        \'all lr scheduler not initialized properly\'\n\n    assert all(a == adjusted_lr1[0] for a in adjusted_lr1), \\\n        \'lr not equally adjusted for all param groups for optimizer 1\'\n    adjusted_lr1 = adjusted_lr1[0]\n\n    assert all(a == adjusted_lr2[0] for a in adjusted_lr2), \\\n        \'lr not equally adjusted for all param groups for optimizer 2\'\n    adjusted_lr2 = adjusted_lr2[0]\n\n    # Called ones after end of epoch\n    assert init_lr * 0.1 ** 1 == adjusted_lr1, \\\n        \'lr for optimizer 1 not adjusted correctly\'\n    # Called every 3 steps, meaning for 1 epoch of 11 batches, it is called 3 times\n    assert init_lr * 0.1 == adjusted_lr2, \\\n        \'lr for optimizer 2 not adjusted correctly\'\n\n\ndef test_reduce_lr_on_plateau_scheduling(tmpdir):\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n    model.configure_optimizers = model.configure_optimizers__reduce_lr_on_plateau\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    results = trainer.fit(model)\n    assert results == 1\n\n    assert trainer.lr_schedulers[0] == \\\n        dict(scheduler=trainer.lr_schedulers[0][\'scheduler\'], monitor=\'val_loss\',\n             interval=\'epoch\', frequency=1, reduce_on_plateau=True), \\\n        \'lr schduler was not correctly converted to dict\'\n\n\ndef test_optimizer_return_options():\n\n    trainer = Trainer()\n    model = EvalModelTemplate()\n\n    # single optimizer\n    opt_a = torch.optim.Adam(model.parameters(), lr=0.002)\n    opt_b = torch.optim.SGD(model.parameters(), lr=0.002)\n    scheduler_a = torch.optim.lr_scheduler.StepLR(opt_a, 10)\n    scheduler_b = torch.optim.lr_scheduler.StepLR(opt_b, 10)\n\n    # single optimizer\n    model.configure_optimizers = lambda: opt_a\n    optim, lr_sched, freq = trainer.init_optimizers(model)\n    assert len(optim) == 1 and len(lr_sched) == 0 and len(freq) == 0\n\n    # opt tuple\n    model.configure_optimizers = lambda: (opt_a, opt_b)\n    optim, lr_sched, freq = trainer.init_optimizers(model)\n    assert len(optim) == 2 and optim[0] == opt_a and optim[1] == opt_b\n    assert len(lr_sched) == 0 and len(freq) == 0\n\n    # opt list\n    model.configure_optimizers = lambda: [opt_a, opt_b]\n    optim, lr_sched, freq = trainer.init_optimizers(model)\n    assert len(optim) == 2 and optim[0] == opt_a and optim[1] == opt_b\n    assert len(lr_sched) == 0 and len(freq) == 0\n\n    # opt tuple of 2 lists\n    model.configure_optimizers = lambda: ([opt_a], [scheduler_a])\n    optim, lr_sched, freq = trainer.init_optimizers(model)\n    assert len(optim) == 1 and len(lr_sched) == 1 and len(freq) == 0\n    assert optim[0] == opt_a\n    assert lr_sched[0] == dict(scheduler=scheduler_a, interval=\'epoch\',\n                               frequency=1, reduce_on_plateau=False, monitor=\'val_loss\')\n\n    # opt single dictionary\n    model.configure_optimizers = lambda: {""optimizer"": opt_a, ""lr_scheduler"": scheduler_a}\n    optim, lr_sched, freq = trainer.init_optimizers(model)\n    assert len(optim) == 1 and len(lr_sched) == 1 and len(freq) == 0\n    assert optim[0] == opt_a\n    assert lr_sched[0] == dict(scheduler=scheduler_a, interval=\'epoch\',\n                               frequency=1, reduce_on_plateau=False, monitor=\'val_loss\')\n\n    # opt multiple dictionaries with frequencies\n    model.configure_optimizers = lambda: (\n        {""optimizer"": opt_a, ""lr_scheduler"": scheduler_a, ""frequency"": 1},\n        {""optimizer"": opt_b, ""lr_scheduler"": scheduler_b, ""frequency"": 5},\n    )\n    optim, lr_sched, freq = trainer.init_optimizers(model)\n    assert len(optim) == 2 and len(lr_sched) == 2 and len(freq) == 2\n    assert optim[0] == opt_a\n    assert lr_sched[0] == dict(scheduler=scheduler_a, interval=\'epoch\',\n                               frequency=1, reduce_on_plateau=False, monitor=\'val_loss\')\n    assert freq == [1, 5]\n\n\ndef test_none_optimizer_warning():\n\n    trainer = Trainer()\n\n    model = EvalModelTemplate()\n    model.configure_optimizers = lambda: None\n\n    with pytest.warns(UserWarning, match=\'will run with no optimizer\'):\n        _, __, ___ = trainer.init_optimizers(model)\n\n\ndef test_none_optimizer(tmpdir):\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n    model.configure_optimizers = model.configure_optimizers__empty\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2\n    )\n    result = trainer.fit(model)\n\n    # verify training completed\n    assert result == 1\n\n\ndef test_configure_optimizer_from_dict(tmpdir):\n    """"""Tests if `configure_optimizer` method could return a dictionary with `optimizer` field only.""""""\n\n    class CurrentModel(EvalModelTemplate):\n        def configure_optimizers(self):\n            config = {\n                \'optimizer\': torch.optim.SGD(params=self.parameters(), lr=1e-03)\n            }\n            return config\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = CurrentModel(hparams)\n\n    # fit model\n    trainer = Trainer(default_save_path=tmpdir, max_epochs=1)\n    result = trainer.fit(model)\n    assert result == 1\n'"
tests/trainer/test_trainer.py,26,"b'import glob\nimport math\nimport os\nimport pickle\nimport types\nfrom argparse import Namespace\n\nimport cloudpickle\nimport pytest\nimport torch\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Callback, LightningModule\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.core.saving import load_hparams_from_tags_csv, load_hparams_from_yaml, save_hparams_to_tags_csv\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.trainer.logging import TrainerLoggingMixin\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\n\ndef test_no_val_module(tmpdir):\n    """"""Tests use case where trainer saves the model, and user loads it from tags independently.""""""\n\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    trainer = Trainer(\n        max_epochs=1,\n        logger=logger,\n        checkpoint_callback=ModelCheckpoint(tmpdir)\n    )\n    # fit model\n    result = trainer.fit(model)\n    # training complete\n    assert result == 1, \'amp + ddp model failed to complete\'\n\n    # save model\n    new_weights_path = os.path.join(tmpdir, \'save_test.ckpt\')\n    trainer.save_checkpoint(new_weights_path)\n\n    # assert ckpt has hparams\n    ckpt = torch.load(new_weights_path)\n    assert LightningModule.CHECKPOINT_KEY_HYPER_PARAMS in ckpt.keys(), \'module_arguments missing from checkpoints\'\n\n    # load new model\n    hparams_path = tutils.get_data_path(logger, path_dir=tmpdir)\n    hparams_path = os.path.join(hparams_path, \'hparams.yaml\')\n    model_2 = EvalModelTemplate.load_from_checkpoint(\n        checkpoint_path=new_weights_path,\n        hparams_file=hparams_path\n    )\n    model_2.eval()\n\n\ndef test_no_val_end_module(tmpdir):\n    """"""Tests use case where trainer saves the model, and user loads it from tags independently.""""""\n\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    # fit model\n    trainer = Trainer(\n        max_epochs=1,\n        logger=logger,\n        checkpoint_callback=ModelCheckpoint(tmpdir)\n    )\n    result = trainer.fit(model)\n\n    # traning complete\n    assert result == 1, \'amp + ddp model failed to complete\'\n\n    # save model\n    new_weights_path = os.path.join(tmpdir, \'save_test.ckpt\')\n    trainer.save_checkpoint(new_weights_path)\n\n    # load new model\n    hparams_path = tutils.get_data_path(logger, path_dir=tmpdir)\n    hparams_path = os.path.join(hparams_path, \'hparams.yaml\')\n    model_2 = EvalModelTemplate.load_from_checkpoint(\n        checkpoint_path=new_weights_path,\n        hparams_file=hparams_path\n    )\n    model_2.eval()\n\n\ndef test_gradient_accumulation_scheduling(tmpdir):\n    """"""\n    Test grad accumulation by the freq of optimizer updates\n    """"""\n\n    # test incorrect configs\n    with pytest.raises(IndexError):\n        assert Trainer(accumulate_grad_batches={0: 3, 1: 4, 4: 6})\n        assert Trainer(accumulate_grad_batches={-2: 3})\n\n    with pytest.raises(TypeError):\n        assert Trainer(accumulate_grad_batches={})\n        assert Trainer(accumulate_grad_batches=[[2, 3], [4, 6]])\n        assert Trainer(accumulate_grad_batches={1: 2, 3.: 4})\n        assert Trainer(accumulate_grad_batches={1: 2.5, 3: 5})\n\n    # test optimizer call freq matches scheduler\n    def _optimizer_step(self, epoch, batch_idx, optimizer,\n                        optimizer_idx, second_order_closure=None):\n        # only test the first 12 batches in epoch\n        if batch_idx < 12:\n            if epoch == 0:\n                # reset counter when starting epoch\n                if batch_idx == 0:\n                    self.prev_called_batch_idx = 0\n\n                    # use this opportunity to test once\n                    assert self.trainer.accumulate_grad_batches == 1\n\n                assert batch_idx == self.prev_called_batch_idx\n                self.prev_called_batch_idx += 1\n\n            elif 1 <= epoch <= 2:\n                # reset counter when starting epoch\n                if batch_idx == 1:\n                    self.prev_called_batch_idx = 1\n\n                    # use this opportunity to test once\n                    assert self.trainer.accumulate_grad_batches == 2\n\n                assert batch_idx == self.prev_called_batch_idx\n                self.prev_called_batch_idx += 2\n\n            else:\n                if batch_idx == 3:\n                    self.prev_called_batch_idx = 3\n\n                    # use this opportunity to test once\n                    assert self.trainer.accumulate_grad_batches == 4\n\n                assert batch_idx == self.prev_called_batch_idx\n                self.prev_called_batch_idx += 3\n\n        optimizer.step()\n\n        # clear gradients\n        optimizer.zero_grad()\n\n    model = EvalModelTemplate()\n    schedule = {1: 2, 3: 4}\n\n    trainer = Trainer(accumulate_grad_batches=schedule,\n                      train_percent_check=0.1,\n                      val_percent_check=0.1,\n                      max_epochs=2,\n                      default_root_dir=tmpdir)\n\n    # for the test\n    trainer.optimizer_step = _optimizer_step\n    model.prev_called_batch_idx = 0\n\n    trainer.fit(model)\n\n\ndef test_loading_meta_tags(tmpdir):\n    """""" test for backward compatibility to meta_tags.csv """"""\n    tutils.reset_seed()\n\n    hparams = EvalModelTemplate.get_default_hparams()\n\n    # save tags\n    logger = tutils.get_default_logger(tmpdir)\n    logger.log_hyperparams(Namespace(some_str=\'a_str\', an_int=1, a_float=2.0))\n    logger.log_hyperparams(hparams)\n    logger.save()\n\n    # load hparams\n    path_expt_dir = tutils.get_data_path(logger, path_dir=tmpdir)\n    hparams_path = os.path.join(path_expt_dir, TensorBoardLogger.NAME_HPARAMS_FILE)\n    hparams = load_hparams_from_yaml(hparams_path)\n\n    # save as legacy meta_tags.csv\n    tags_path = os.path.join(path_expt_dir, \'meta_tags.csv\')\n    save_hparams_to_tags_csv(tags_path, hparams)\n\n    tags = load_hparams_from_tags_csv(tags_path)\n\n    assert hparams == tags\n\n\ndef test_loading_yaml(tmpdir):\n    tutils.reset_seed()\n\n    hparams = EvalModelTemplate.get_default_hparams()\n\n    # save tags\n    logger = tutils.get_default_logger(tmpdir)\n    logger.log_hyperparams(Namespace(some_str=\'a_str\', an_int=1, a_float=2.0))\n    logger.log_hyperparams(hparams)\n    logger.save()\n\n    # load hparams\n    path_expt_dir = tutils.get_data_path(logger, path_dir=tmpdir)\n    hparams_path = os.path.join(path_expt_dir, \'hparams.yaml\')\n    tags = load_hparams_from_yaml(hparams_path)\n\n    assert tags[\'batch_size\'] == 32 and tags[\'hidden_dim\'] == 1000\n\n\ndef test_dp_output_reduce():\n    mixin = TrainerLoggingMixin()\n\n    # test identity when we have a single gpu\n    out = torch.rand(3, 1)\n    assert mixin.reduce_distributed_output(out, num_gpus=1) is out\n\n    # average when we have multiples\n    assert mixin.reduce_distributed_output(out, num_gpus=2) == out.mean()\n\n    # when we have a dict of vals\n    out = {\n        \'a\': out,\n        \'b\': {\n            \'c\': out\n        }\n    }\n    reduced = mixin.reduce_distributed_output(out, num_gpus=3)\n    assert reduced[\'a\'] == out[\'a\']\n    assert reduced[\'b\'][\'c\'] == out[\'b\'][\'c\']\n\n\n@pytest.mark.parametrize([""save_top_k"", ""save_last"", ""file_prefix"", ""expected_files""], [\n    pytest.param(-1, False, \'\', {\'epoch=4.ckpt\', \'epoch=3.ckpt\', \'epoch=2.ckpt\', \'epoch=1.ckpt\', \'epoch=0.ckpt\'},\n                 id=""CASE K=-1  (all)""),\n    pytest.param(1, False, \'test_prefix_\', {\'test_prefix_epoch=4.ckpt\'},\n                 id=""CASE K=1 (2.5, epoch 4)""),\n    pytest.param(2, False, \'\', {\'epoch=4.ckpt\', \'epoch=2.ckpt\'},\n                 id=""CASE K=2 (2.5 epoch 4, 2.8 epoch 2)""),\n    pytest.param(4, False, \'\', {\'epoch=1.ckpt\', \'epoch=4.ckpt\', \'epoch=3.ckpt\', \'epoch=2.ckpt\'},\n                 id=""CASE K=4 (save all 4 base)""),\n    pytest.param(3, False, \'\', {\'epoch=2.ckpt\', \'epoch=3.ckpt\', \'epoch=4.ckpt\'},\n                 id=""CASE K=3 (save the 2nd, 3rd, 4th model)""),\n    pytest.param(1, True, \'\', {\'epoch=4.ckpt\', \'last.ckpt\'},\n                 id=""CASE K=1 (save the 4th model and the last model)""),\n])\ndef test_model_checkpoint_options(tmpdir, save_top_k, save_last, file_prefix, expected_files):\n    """"""Test ModelCheckpoint options.""""""\n\n    def mock_save_function(filepath, *args):\n        open(filepath, \'a\').close()\n\n    # simulated losses\n    losses = [10, 9, 2.8, 5, 2.5]\n\n    checkpoint_callback = ModelCheckpoint(tmpdir, save_top_k=save_top_k, save_last=save_last,\n                                          prefix=file_prefix, verbose=1)\n    checkpoint_callback.save_function = mock_save_function\n    trainer = Trainer()\n\n    # emulate callback\'s calls during the training\n    for i, loss in enumerate(losses):\n        trainer.current_epoch = i\n        trainer.callback_metrics = {\'val_loss\': loss}\n        checkpoint_callback.on_validation_end(trainer, trainer.get_model())\n\n    file_lists = set(os.listdir(tmpdir))\n\n    assert len(file_lists) == len(expected_files), \\\n        ""Should save %i models when save_top_k=%i"" % (len(expected_files), save_top_k)\n\n    # verify correct naming\n    for fname in expected_files:\n        assert fname in file_lists\n\n\ndef test_model_checkpoint_only_weights(tmpdir):\n    """"""Tests use case where ModelCheckpoint is configured to save only model weights, and\n     user tries to load checkpoint to resume training.\n     """"""\n    model = EvalModelTemplate()\n\n    trainer = Trainer(\n        max_epochs=1,\n        checkpoint_callback=ModelCheckpoint(tmpdir, save_weights_only=True)\n    )\n    # fit model\n    result = trainer.fit(model)\n    # training complete\n    assert result == 1, \'training failed to complete\'\n\n    checkpoint_path = list(trainer.checkpoint_callback.best_k_models.keys())[0]\n\n    # assert saved checkpoint has no trainer data\n    checkpoint = torch.load(checkpoint_path)\n    assert \'optimizer_states\' not in checkpoint, \'checkpoint should contain only model weights\'\n    assert \'lr_schedulers\' not in checkpoint, \'checkpoint should contain only model weights\'\n\n    # assert loading model works when checkpoint has only weights\n    assert EvalModelTemplate.load_from_checkpoint(checkpoint_path=checkpoint_path)\n\n    # directly save model\n    new_weights_path = os.path.join(tmpdir, \'save_test.ckpt\')\n    trainer.save_checkpoint(new_weights_path, weights_only=True)\n    # assert saved checkpoint has no trainer data\n    checkpoint = torch.load(new_weights_path)\n    assert \'optimizer_states\' not in checkpoint, \'checkpoint should contain only model weights\'\n    assert \'lr_schedulers\' not in checkpoint, \'checkpoint should contain only model weights\'\n\n    # assert restoring train state fails\n    with pytest.raises(KeyError, match=\'checkpoint contains only the model\'):\n        trainer.restore_training_state(checkpoint)\n\n\ndef test_model_freeze_unfreeze():\n\n    model = EvalModelTemplate()\n\n    model.freeze()\n    model.unfreeze()\n\n\ndef test_resume_from_checkpoint_epoch_restored(tmpdir):\n    """"""Verify resuming from checkpoint runs the right number of epochs""""""\n\n    hparams = EvalModelTemplate.get_default_hparams()\n\n    def _new_model():\n        # Create a model that tracks epochs and batches seen\n        model = EvalModelTemplate(**hparams)\n        model.num_epochs_seen = 0\n        model.num_batches_seen = 0\n        model.num_on_load_checkpoint_called = 0\n\n        def increment_epoch(self):\n            self.num_epochs_seen += 1\n\n        def increment_batch(self, _):\n            self.num_batches_seen += 1\n\n        def increment_on_load_checkpoint(self, _):\n            self.num_on_load_checkpoint_called += 1\n\n        # Bind methods to keep track of epoch numbers, batch numbers it has seen\n        # as well as number of times it has called on_load_checkpoint()\n        model.on_epoch_end = types.MethodType(increment_epoch, model)\n        model.on_batch_start = types.MethodType(increment_batch, model)\n        model.on_load_checkpoint = types.MethodType(increment_on_load_checkpoint, model)\n        return model\n\n    model = _new_model()\n\n    trainer_options = dict(\n        progress_bar_refresh_rate=0,\n        max_epochs=2,\n        train_percent_check=0.65,\n        val_percent_check=1,\n        checkpoint_callback=ModelCheckpoint(tmpdir, save_top_k=-1),\n        default_root_dir=tmpdir,\n        early_stop_callback=False,\n        val_check_interval=1.,\n    )\n\n    trainer = Trainer(**trainer_options)\n    # fit model\n    trainer.fit(model)\n\n    training_batches = trainer.num_training_batches\n\n    assert model.num_epochs_seen == 2\n    assert model.num_batches_seen == training_batches * 2\n    assert model.num_on_load_checkpoint_called == 0\n\n    # Other checkpoints can be uncommented if/when resuming mid-epoch is supported\n    checkpoints = sorted(glob.glob(os.path.join(trainer.checkpoint_callback.dirpath, \'*.ckpt\')))\n\n    for check in checkpoints:\n        next_model = _new_model()\n        state = torch.load(check)\n\n        # Resume training\n        trainer_options[\'max_epochs\'] = 2\n        new_trainer = Trainer(**trainer_options, resume_from_checkpoint=check)\n        new_trainer.fit(next_model)\n        assert state[\'global_step\'] + next_model.num_batches_seen == training_batches * trainer_options[\'max_epochs\']\n        assert next_model.num_on_load_checkpoint_called == 1\n\n\ndef _init_steps_model():\n    """"""private method for initializing a model with 5% train epochs""""""\n    model = EvalModelTemplate()\n\n    # define train epoch to 5% of data\n    train_percent = 0.5\n    # get number of samples in 1 epoch\n    num_train_samples = math.floor(len(model.train_dataloader()) * train_percent)\n\n    trainer_options = dict(\n        train_percent_check=train_percent,\n    )\n    return model, trainer_options, num_train_samples\n\n\ndef test_trainer_max_steps_and_epochs(tmpdir):\n    """"""Verify model trains according to specified max steps""""""\n    model, trainer_options, num_train_samples = _init_steps_model()\n\n    # define less train steps than epochs\n    trainer_options.update(\n        default_root_dir=tmpdir,\n        max_epochs=3,\n        max_steps=num_train_samples + 10\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    assert result == 1, ""Training did not complete""\n\n    # check training stopped at max_steps\n    assert trainer.global_step == trainer.max_steps, ""Model did not stop at max_steps""\n\n    # define less train epochs than steps\n    trainer_options.update(\n        max_epochs=2,\n        max_steps=trainer_options[\'max_epochs\'] * 2 * num_train_samples\n    )\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    assert result == 1, ""Training did not complete""\n\n    # check training stopped at max_epochs\n    assert trainer.global_step == num_train_samples * trainer.max_epochs\n    assert trainer.current_epoch == trainer.max_epochs - 1, ""Model did not stop at max_epochs""\n\n\ndef test_trainer_min_steps_and_epochs(tmpdir):\n    """"""Verify model trains according to specified min steps""""""\n    model, trainer_options, num_train_samples = _init_steps_model()\n\n    # define callback for stopping the model and default epochs\n    trainer_options.update(\n        default_root_dir=tmpdir,\n        early_stop_callback=EarlyStopping(monitor=\'val_loss\', min_delta=1.0),\n        val_check_interval=2,\n        min_epochs=1,\n        max_epochs=2\n    )\n\n    # define less min steps than 1 epoch\n    trainer_options[\'min_steps\'] = math.floor(num_train_samples / 2)\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    assert result == 1, ""Training did not complete""\n\n    # check model ran for at least min_epochs\n    assert trainer.global_step >= num_train_samples and \\\n        trainer.current_epoch > 0, ""Model did not train for at least min_epochs""\n\n    # define less epochs than min_steps\n    trainer_options[\'min_steps\'] = math.floor(num_train_samples * 1.5)\n\n    # fit model\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n    assert result == 1, ""Training did not complete""\n\n    # check model ran for at least num_train_samples*1.5\n    assert trainer.global_step >= math.floor(num_train_samples * 1.5) and \\\n        trainer.current_epoch > 0, ""Model did not train for at least min_steps""\n\n\ndef test_benchmark_option(tmpdir):\n    """"""Verify benchmark option.""""""\n\n    model = EvalModelTemplate()\n    model.val_dataloader = model.val_dataloader__multiple\n\n    # verify torch.backends.cudnn.benchmark is not turned on\n    assert not torch.backends.cudnn.benchmark\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        benchmark=True,\n    )\n    result = trainer.fit(model)\n\n    # verify training completed\n    assert result == 1\n\n    # verify torch.backends.cudnn.benchmark is not turned off\n    assert torch.backends.cudnn.benchmark\n\n\ndef test_testpass_overrides(tmpdir):\n    # todo: check duplicated tests against trainer_checks\n    hparams = EvalModelTemplate.get_default_hparams()\n\n    # Misconfig when neither test_step or test_end is implemented\n    with pytest.raises(MisconfigurationException, match=\'.*not implement `test_dataloader`.*\'):\n        model = EvalModelTemplate(**hparams)\n        model.test_dataloader = LightningModule.test_dataloader\n        Trainer().test(model)\n\n    # Misconfig when neither test_step or test_end is implemented\n    with pytest.raises(MisconfigurationException):\n        model = EvalModelTemplate(**hparams)\n        model.test_step = LightningModule.test_step\n        Trainer().test(model)\n\n    # No exceptions when one or both of test_step or test_end are implemented\n    model = EvalModelTemplate(**hparams)\n    model.test_step_end = LightningModule.test_step_end\n    Trainer().test(model)\n\n    model = EvalModelTemplate(**hparams)\n    Trainer().test(model)\n\n\ndef test_disabled_validation():\n    """"""Verify that `val_percent_check=0` disables the validation loop unless `fast_dev_run=True`.""""""\n\n    class CurrentModel(EvalModelTemplate):\n\n        validation_step_invoked = False\n        validation_epoch_end_invoked = False\n\n        def validation_step(self, *args, **kwargs):\n            self.validation_step_invoked = True\n            return super().validation_step(*args, **kwargs)\n\n        def validation_epoch_end(self, *args, **kwargs):\n            self.validation_epoch_end_invoked = True\n            return super().validation_epoch_end(*args, **kwargs)\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = CurrentModel(**hparams)\n\n    trainer_options = dict(\n        progress_bar_refresh_rate=0,\n        max_epochs=2,\n        train_percent_check=0.4,\n        val_percent_check=0.0,\n        fast_dev_run=False,\n    )\n\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n\n    # check that val_percent_check=0 turns off validation\n    assert result == 1, \'training failed to complete\'\n    assert trainer.current_epoch == 1\n    assert not model.validation_step_invoked, \\\n        \'`validation_step` should not run when `val_percent_check=0`\'\n    assert not model.validation_epoch_end_invoked, \\\n        \'`validation_epoch_end` should not run when `val_percent_check=0`\'\n\n    # check that val_percent_check has no influence when fast_dev_run is turned on\n    model = CurrentModel(**hparams)\n    trainer_options.update(fast_dev_run=True)\n    trainer = Trainer(**trainer_options)\n    result = trainer.fit(model)\n\n    assert result == 1, \'training failed to complete\'\n    assert trainer.current_epoch == 0\n    assert model.validation_step_invoked, \\\n        \'did not run `validation_step` with `fast_dev_run=True`\'\n    assert model.validation_epoch_end_invoked, \\\n        \'did not run `validation_epoch_end` with `fast_dev_run=True`\'\n\n\ndef test_nan_loss_detection(tmpdir):\n\n    class CurrentModel(EvalModelTemplate):\n        test_batch_inf_loss = 8\n\n        def training_step(self, batch, batch_idx, optimizer_idx=None):\n            output = super().training_step(batch, batch_idx, optimizer_idx)\n            if batch_idx == self.test_batch_inf_loss:\n                if isinstance(output, dict):\n                    output[\'loss\'] *= torch.tensor(math.inf)  # make loss infinite\n                else:\n                    output /= 0\n            return output\n\n    model = CurrentModel()\n\n    # fit model\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_steps=(model.test_batch_inf_loss + 1),\n        terminate_on_nan=True\n    )\n\n    with pytest.raises(ValueError, match=r\'.*The loss returned in `training_step` is nan or inf.*\'):\n        trainer.fit(model)\n        assert trainer.global_step == model.test_step_inf_loss\n\n    for param in model.parameters():\n        assert torch.isfinite(param).all()\n\n\ndef test_nan_params_detection(tmpdir):\n\n    class CurrentModel(EvalModelTemplate):\n        test_batch_nan = 8\n\n        def on_after_backward(self):\n            if self.global_step == self.test_batch_nan:\n                # simulate parameter that became nan\n                torch.nn.init.constant_(self.c_d1.bias, math.nan)\n\n    model = CurrentModel()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_steps=(model.test_batch_nan + 1),\n        terminate_on_nan=True\n    )\n\n    with pytest.raises(ValueError, match=r\'.*Detected nan and/or inf values in `c_d1.bias`.*\'):\n        trainer.fit(model)\n        assert trainer.global_step == model.test_batch_nan\n\n    # after aborting the training loop, model still has nan-valued params\n    params = torch.cat([param.view(-1) for param in model.parameters()])\n    assert not torch.isfinite(params).all()\n\n\ndef test_trainer_interrupted_flag(tmpdir):\n    """"""Test the flag denoting that a user interrupted training.""""""\n\n    model = EvalModelTemplate()\n\n    class InterruptCallback(Callback):\n        def __init__(self):\n            super().__init__()\n\n        def on_batch_start(self, trainer, pl_module):\n            raise KeyboardInterrupt\n\n    interrupt_callback = InterruptCallback()\n\n    trainer = Trainer(\n        callbacks=[interrupt_callback],\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2,\n        progress_bar_refresh_rate=0,\n        logger=False,\n        default_root_dir=tmpdir,\n    )\n    assert not trainer.interrupted\n    trainer.fit(model)\n    assert trainer.interrupted\n\n\ndef test_gradient_clipping(tmpdir):\n    """"""\n    Test gradient clipping\n    """"""\n\n    model = EvalModelTemplate()\n\n    # test that gradient is clipped correctly\n    def _optimizer_step(*args, **kwargs):\n        parameters = model.parameters()\n        grad_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2) for p in parameters]), 2)\n        assert (grad_norm - 1.0).abs() < 0.01, ""Gradient norm != 1.0: {grad_norm}"".format(grad_norm=grad_norm)\n\n    trainer = Trainer(\n        max_steps=1,\n        max_epochs=1,\n        gradient_clip_val=1.0,\n        default_root_dir=tmpdir\n    )\n\n    # for the test\n    model.optimizer_step = _optimizer_step\n    model.prev_called_batch_idx = 0\n\n    trainer.fit(model)\n\n\ndef test_gpu_choice(tmpdir):\n    trainer_options = dict(\n        default_save_path=tmpdir,\n    )\n    # Only run if CUDA is available\n    if not torch.cuda.is_available():\n        return\n\n    num_gpus = torch.cuda.device_count()\n    Trainer(**trainer_options, gpus=num_gpus, auto_select_gpus=True)\n\n    with pytest.raises(RuntimeError, match=r\'.*No GPUs available.*\'):\n        Trainer(**trainer_options, gpus=num_gpus + 1, auto_select_gpus=True)\n\n\n@pytest.mark.parametrize(""trainer_kwargs,expected"", [\n    pytest.param(\n        dict(distributed_backend=None, gpus=None),\n        dict(use_dp=False, use_ddp=False, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=1)\n    ),\n    pytest.param(\n        dict(distributed_backend=""dp"", gpus=None),\n        dict(use_dp=False, use_ddp=False, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=1)\n    ),\n    pytest.param(\n        dict(distributed_backend=""dp"", gpus=None),\n        dict(use_dp=False, use_ddp=False, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=1)\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp"", gpus=None),\n        dict(use_dp=False, use_ddp=False, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=1)\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp"", num_processes=2, gpus=None),\n        dict(use_dp=False, use_ddp=True, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=2)\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp"", num_nodes=2, gpus=None),\n        dict(use_dp=False, use_ddp=True, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=1)\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp_cpu"", num_processes=2, gpus=None),\n        dict(use_dp=False, use_ddp=True, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=2)\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp2"", gpus=None),\n        dict(use_dp=False, use_ddp=False, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=1)\n    ),\n    pytest.param(\n        dict(distributed_backend=None, gpus=1),\n        dict(use_dp=False, use_ddp=False, use_ddp2=False, num_gpus=1, on_gpu=True, single_gpu=True, num_processes=1),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() == 0, reason=""GPU needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=""dp"", gpus=1),\n        dict(use_dp=True, use_ddp=False, use_ddp2=False, num_gpus=1, on_gpu=True, single_gpu=True, num_processes=1),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() == 0, reason=""GPU needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp"", gpus=1),\n        dict(use_dp=False, use_ddp=True, use_ddp2=False, num_gpus=1, on_gpu=True, single_gpu=True, num_processes=1),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() == 0, reason=""GPU needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp_cpu"", num_processes=2, gpus=1),\n        dict(use_dp=False, use_ddp=True, use_ddp2=False, num_gpus=0, on_gpu=False, single_gpu=False, num_processes=2),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() == 0, reason=""GPU needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp2"", gpus=1),\n        dict(use_dp=False, use_ddp=False, use_ddp2=True, num_gpus=1, on_gpu=True, single_gpu=False, num_processes=1),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() == 0, reason=""GPU needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=None, gpus=2),\n        dict(use_dp=False, use_ddp=True, use_ddp2=False, num_gpus=2, on_gpu=True, single_gpu=False, num_processes=2),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""Multiple GPUs needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=""dp"", gpus=2),\n        dict(use_dp=True, use_ddp=False, use_ddp2=False, num_gpus=2, on_gpu=True, single_gpu=False, num_processes=1),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""Multiple GPUs needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp"", gpus=2),\n        dict(use_dp=False, use_ddp=True, use_ddp2=False, num_gpus=2, on_gpu=True, single_gpu=False, num_processes=2),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""Multiple GPUs needed"")]\n    ),\n    pytest.param(\n        dict(distributed_backend=""ddp2"", gpus=2),\n        dict(use_dp=False, use_ddp=False, use_ddp2=True, num_gpus=2, on_gpu=True, single_gpu=False, num_processes=1),\n        marks=[pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""Multiple GPUs needed"")]\n    ),\n])\ndef test_trainer_config(trainer_kwargs, expected):\n    trainer = Trainer(**trainer_kwargs)\n    assert trainer.use_dp is expected[""use_dp""]\n    assert trainer.use_ddp is expected[""use_ddp""]\n    assert trainer.use_ddp2 is expected[""use_ddp2""]\n    assert trainer.num_gpus == expected[""num_gpus""]\n    assert trainer.on_gpu is expected[""on_gpu""]\n    assert trainer.single_gpu is expected[""single_gpu""]\n    assert trainer.num_processes == expected[""num_processes""]\n\n\ndef test_trainer_subclassing():\n    model = EvalModelTemplate()\n\n    # First way of pulling out args from signature is to list them\n    class TrainerSubclass(Trainer):\n\n        def __init__(self, custom_arg, *args, custom_kwarg=\'test\', **kwargs):\n            super().__init__(*args, **kwargs)\n            self.custom_arg = custom_arg\n            self.custom_kwarg = custom_kwarg\n\n    trainer = TrainerSubclass(123, custom_kwarg=\'custom\', fast_dev_run=True)\n    result = trainer.fit(model)\n    assert result == 1\n    assert trainer.custom_arg == 123\n    assert trainer.custom_kwarg == \'custom\'\n    assert trainer.fast_dev_run\n\n    # Second way is to pop from the dict\n    # It\'s a special case because Trainer does not have any positional args\n    class TrainerSubclass(Trainer):\n\n        def __init__(self, **kwargs):\n            self.custom_arg = kwargs.pop(\'custom_arg\', 0)\n            self.custom_kwarg = kwargs.pop(\'custom_kwarg\', \'test\')\n            super().__init__(**kwargs)\n\n    trainer = TrainerSubclass(custom_kwarg=\'custom\', fast_dev_run=True)\n    result = trainer.fit(model)\n    assert result == 1\n    assert trainer.custom_kwarg == \'custom\'\n    assert trainer.fast_dev_run\n\n    # when we pass in an unknown arg, the base class should complain\n    with pytest.raises(TypeError, match=r""__init__\\(\\) got an unexpected keyword argument \'abcdefg\'""):\n        TrainerSubclass(abcdefg=\'unknown_arg\')\n\n\ndef test_trainer_pickle(tmpdir):\n    trainer = Trainer(\n        max_epochs=1,\n        default_root_dir=tmpdir\n    )\n    pickle.dumps(trainer)\n    cloudpickle.dumps(trainer)\n'"
tests/trainer/test_trainer_cli.py,0,"b'import inspect\nimport pickle\nimport sys\nfrom argparse import ArgumentParser, Namespace\nfrom unittest import mock\n\nimport pytest\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\n\n\n@mock.patch(\'argparse.ArgumentParser.parse_args\',\n            return_value=Namespace(**Trainer.default_attributes()))\ndef test_default_args(tmpdir):\n    """"""Tests default argument parser for Trainer""""""\n\n    # logger file to get meta\n    logger = tutils.get_default_logger(tmpdir)\n\n    parser = ArgumentParser(add_help=False)\n    args = parser.parse_args()\n    args.logger = logger\n\n    args.max_epochs = 5\n    trainer = Trainer.from_argparse_args(args)\n\n    assert isinstance(trainer, Trainer)\n    assert trainer.max_epochs == 5\n\n\n@pytest.mark.parametrize(\'cli_args\', [\n    [\'--accumulate_grad_batches=22\'],\n    [\'--print_nan_grads\', \'--weights_save_path=./\'],\n    []\n])\ndef test_add_argparse_args_redefined(cli_args):\n    """"""Redefines some default Trainer arguments via the cli and\n    tests the Trainer initialization correctness.\n    """"""\n    parser = ArgumentParser(add_help=False)\n    parser = Trainer.add_argparse_args(parent_parser=parser)\n\n    args = parser.parse_args(cli_args)\n\n    # make sure we can pickle args\n    pickle.dumps(args)\n\n    # Check few deprecated args are not in namespace:\n    for depr_name in (\'gradient_clip\', \'nb_gpu_nodes\', \'max_nb_epochs\'):\n        assert depr_name not in args\n\n    trainer = Trainer.from_argparse_args(args=args)\n    pickle.dumps(trainer)\n\n    assert isinstance(trainer, Trainer)\n\n\ndef test_get_init_arguments_and_types():\n    """"""Asserts a correctness of the `get_init_arguments_and_types` Trainer classmethod.""""""\n    args = Trainer.get_init_arguments_and_types()\n    parameters = inspect.signature(Trainer).parameters\n    assert len(parameters) == len(args)\n    for arg in args:\n        assert parameters[arg[0]].default == arg[2]\n\n    kwargs = {arg[0]: arg[2] for arg in args}\n    trainer = Trainer(**kwargs)\n    assert isinstance(trainer, Trainer)\n\n\n@pytest.mark.parametrize(\'cli_args\', [\n    [\'--callbacks=1\', \'--logger\'],\n    [\'--foo\', \'--bar=1\']\n])\ndef test_add_argparse_args_redefined_error(cli_args, monkeypatch):\n    """"""Asserts thar an error raised in case of passing not default cli arguments.""""""\n\n    class _UnkArgError(Exception):\n        pass\n\n    def _raise():\n        raise _UnkArgError\n\n    parser = ArgumentParser(add_help=False)\n    parser = Trainer.add_argparse_args(parent_parser=parser)\n\n    monkeypatch.setattr(parser, \'exit\', lambda *args: _raise(), raising=True)\n\n    with pytest.raises(_UnkArgError):\n        parser.parse_args(cli_args)\n\n\n# todo: add also testing for ""gpus""\n@pytest.mark.parametrize([\'cli_args\', \'expected\'], [\n    pytest.param(\'--auto_lr_find --auto_scale_batch_size power\',\n                 {\'auto_lr_find\': True, \'auto_scale_batch_size\': \'power\', \'early_stop_callback\': False}),\n    pytest.param(\'--auto_lr_find any_string --auto_scale_batch_size\',\n                 {\'auto_lr_find\': \'any_string\', \'auto_scale_batch_size\': True}),\n    pytest.param(\'--early_stop_callback\',\n                 {\'auto_lr_find\': False, \'early_stop_callback\': True, \'auto_scale_batch_size\': False}),\n])\ndef test_argparse_args_parsing(cli_args, expected):\n    """"""Test multi type argument with bool.""""""\n    cli_args = cli_args.split(\' \') if cli_args else []\n    with mock.patch(""argparse._sys.argv"", [""any.py""] + cli_args):\n        parser = ArgumentParser(add_help=False)\n        parser = Trainer.add_argparse_args(parent_parser=parser)\n        args = Trainer.parse_argparser(parser)\n\n    for k, v in expected.items():\n        assert getattr(args, k) == v\n    assert Trainer.from_argparse_args(args)\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 7),\n    reason=""signature inspection while mocking is not working in Python < 3.7 despite autospec""\n)\n@pytest.mark.parametrize([\'cli_args\', \'extra_args\'], [\n    pytest.param({}, {}),\n    pytest.param({\'logger\': False}, {}),\n    pytest.param({\'logger\': False}, {\'logger\': True}),\n    pytest.param({\'logger\': False}, {\'checkpoint_callback\': True}),\n])\ndef test_init_from_argparse_args(cli_args, extra_args):\n    unknown_args = dict(unknown_arg=0)\n\n    # unkown args in the argparser/namespace should be ignored\n    with mock.patch(\'pytorch_lightning.Trainer.__init__\', autospec=True, return_value=None) as init:\n        trainer = Trainer.from_argparse_args(Namespace(**cli_args, **unknown_args), **extra_args)\n        expected = dict(cli_args)\n        expected.update(extra_args)  # extra args should override any cli arg\n        init.assert_called_with(trainer, **expected)\n\n    # passing in unknown manual args should throw an error\n    with pytest.raises(TypeError, match=r""__init__\\(\\) got an unexpected keyword argument \'unknown_arg\'""):\n        Trainer.from_argparse_args(Namespace(**cli_args), **extra_args, **unknown_args)\n'"
tests/trainer/test_trainer_tricks.py,1,"b'import pytest\nimport torch\n\nimport tests.base.utils as tutils\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.utilities.exceptions import MisconfigurationException\nfrom tests.base import EvalModelTemplate\n\n\ndef test_model_reset_correctly(tmpdir):\n    """""" Check that model weights are correctly reset after scaling batch size. """"""\n    tutils.reset_seed()\n\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=1\n    )\n\n    before_state_dict = model.state_dict()\n\n    trainer.scale_batch_size(model, max_trials=5)\n\n    after_state_dict = model.state_dict()\n\n    for key in before_state_dict.keys():\n        assert torch.all(torch.eq(before_state_dict[key], after_state_dict[key])), \\\n            \'Model was not reset correctly after scaling batch size\'\n\n\ndef test_trainer_reset_correctly(tmpdir):\n    """""" Check that all trainer parameters are reset correctly after scaling batch size. """"""\n    tutils.reset_seed()\n\n    model = EvalModelTemplate()\n\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=1\n    )\n\n    changed_attributes = [\'max_steps\',\n                          \'weights_summary\',\n                          \'logger\',\n                          \'callbacks\',\n                          \'checkpoint_callback\',\n                          \'early_stop_callback\',\n                          \'enable_early_stop\',\n                          \'train_percent_check\']\n\n    attributes_before = {}\n    for ca in changed_attributes:\n        attributes_before[ca] = getattr(trainer, ca)\n\n    trainer.scale_batch_size(model, max_trials=5)\n\n    attributes_after = {}\n    for ca in changed_attributes:\n        attributes_after[ca] = getattr(trainer, ca)\n\n    for key in changed_attributes:\n        assert attributes_before[key] == attributes_after[key], \\\n            f\'Attribute {key} was not reset correctly after learning rate finder\'\n\n\n@pytest.mark.parametrize(\'scale_arg\', [\'power\', \'binsearch\'])\ndef test_trainer_arg(tmpdir, scale_arg):\n    """""" Check that trainer arg works with bool input. """"""\n    tutils.reset_seed()\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    before_batch_size = hparams.get(\'batch_size\')\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=1,\n        auto_scale_batch_size=scale_arg,\n    )\n\n    trainer.fit(model)\n    after_batch_size = model.batch_size\n    assert before_batch_size != after_batch_size, \\\n        \'Batch size was not altered after running auto scaling of batch size\'\n\n\n@pytest.mark.parametrize(\'scale_method\', [\'power\', \'binsearch\'])\ndef test_call_to_trainer_method(tmpdir, scale_method):\n    """""" Test that calling the trainer method itself works. """"""\n    tutils.reset_seed()\n\n    hparams = EvalModelTemplate.get_default_hparams()\n    model = EvalModelTemplate(**hparams)\n\n    before_batch_size = hparams.get(\'batch_size\')\n    # logger file to get meta\n    trainer = Trainer(\n        default_save_path=tmpdir,\n        max_epochs=1,\n    )\n\n    after_batch_size = trainer.scale_batch_size(model, mode=scale_method, max_trials=5)\n    model.batch_size = after_batch_size\n    trainer.fit(model)\n\n    assert before_batch_size != after_batch_size, \\\n        \'Batch size was not altered after running auto scaling of batch size\'\n\n\ndef test_error_on_dataloader_passed_to_fit(tmpdir):\n    """"""Verify that when the auto scale batch size feature raises an error\n       if a train dataloader is passed to fit """"""\n\n    # only train passed to fit\n    model = EvalModelTemplate()\n    trainer = Trainer(\n        default_root_dir=tmpdir,\n        max_epochs=1,\n        val_percent_check=0.1,\n        train_percent_check=0.2,\n        auto_scale_batch_size=\'power\'\n    )\n    fit_options = dict(train_dataloader=model.dataloader(train=True))\n\n    with pytest.raises(MisconfigurationException):\n        trainer.fit(model, **fit_options)\n'"
tests/utilities/__init__.py,0,b''
tests/utilities/test_apply_func.py,12,"b""import numbers\nfrom collections import namedtuple\n\nimport numpy as np\nimport torch\n\nfrom pytorch_lightning.utilities.apply_func import apply_to_collection\n\n\ndef test_recursive_application_to_collection():\n    ntc = namedtuple('Foo', ['bar'])\n\n    to_reduce = {\n        'a': torch.tensor([1.]),  # Tensor\n        'b': [torch.tensor([2.])],  # list\n        'c': (torch.tensor([100.]),),  # tuple\n        'd': ntc(bar=5.),  # named tuple\n        'e': np.array([10.]),  # numpy array\n        'f': 'this_is_a_dummy_str',  # string\n        'g': 12.  # number\n    }\n\n    expected_result = {\n        'a': torch.tensor([2.]),\n        'b': [torch.tensor([4.])],\n        'c': (torch.tensor([200.]),),\n        'd': ntc(bar=torch.tensor([10.])),\n        'e': np.array([20.]),\n        'f': 'this_is_a_dummy_str',\n        'g': 24.\n    }\n\n    reduced = apply_to_collection(to_reduce, (torch.Tensor, numbers.Number, np.ndarray),\n                                  lambda x: x * 2)\n\n    assert isinstance(reduced, dict), ' Type Consistency of dict not preserved'\n    assert all([x in reduced for x in to_reduce.keys()]), 'Not all entries of the dict were preserved'\n    assert all([isinstance(reduced[k], type(expected_result[k])) for k in to_reduce.keys()]), \\\n        'At least one type was not correctly preserved'\n\n    assert isinstance(reduced['a'], torch.Tensor), 'Reduction Result of a Tensor should be a Tensor'\n    assert torch.allclose(expected_result['a'], reduced['a']), \\\n        'Reduction of a tensor does not yield the expected value'\n\n    assert isinstance(reduced['b'], list), 'Reduction Result of a list should be a list'\n    assert all([torch.allclose(x, y) for x, y in zip(reduced['b'], expected_result['b'])]), \\\n        'At least one value of list reduction did not come out as expected'\n\n    assert isinstance(reduced['c'], tuple), 'Reduction Result of a tuple should be a tuple'\n    assert all([torch.allclose(x, y) for x, y in zip(reduced['c'], expected_result['c'])]), \\\n        'At least one value of tuple reduction did not come out as expected'\n\n    assert isinstance(reduced['d'], ntc), 'Type Consistency for named tuple not given'\n    assert isinstance(reduced['d'].bar, numbers.Number), \\\n        'Failure in type promotion while reducing fields of named tuples'\n    assert reduced['d'].bar == expected_result['d'].bar\n\n    assert isinstance(reduced['e'], np.ndarray), 'Type Promotion in reduction of numpy arrays failed'\n    assert reduced['e'] == expected_result['e'], \\\n        'Reduction of numpy array did not yield the expected result'\n\n    assert isinstance(reduced['f'], str), 'A string should not be reduced'\n    assert reduced['f'] == expected_result['f'], 'String not preserved during reduction'\n\n    assert isinstance(reduced['g'], numbers.Number), 'Reduction of a number should result in a tensor'\n    assert reduced['g'] == expected_result['g'], 'Reduction of a number did not yield the desired result'\n"""
tests/models/data/horovod/train_default_model.py,0,"b'""""""\nThis script is meant to be executed from `../../test_horovod.py`.\n\nBecause Horovod uses a parallel programming model similar to MPI, unit tests for collective\nops like allreduce need to be run in parallel. The most common approach for running parallel\nHorovod workers is to launch multiple replicas of the training script via the `horovodrun`\ncommand-line tool:\n\n.. code-block:: bash\n\n    horovodrun -np 2 python train_default_model.py ...\n\nIndividual test parameters are configured by the serialized `--trainer-options` JSON object.\n\nAn non-zero exit code from this script on any rank will indicate failure, while a zero exit code\nacross all ranks indicates success.\n""""""\n\nimport argparse\nimport json\nimport os\nimport sys\n\nimport horovod.torch as hvd\n\nPATH_HERE = os.path.abspath(os.path.dirname(__file__))\nPATH_ROOT = os.path.join(PATH_HERE, \'..\', \'..\', \'..\', \'..\')\nsys.path.insert(0, os.path.abspath(PATH_ROOT))\n\nfrom pytorch_lightning import Trainer  # noqa: E402\nfrom pytorch_lightning.callbacks import ModelCheckpoint  # noqa: E402\nfrom tests.base import EvalModelTemplate  # noqa: E402\nfrom tests.base.utils import set_random_master_port, run_model_test  # noqa: E402\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--trainer-options\', required=True)\nparser.add_argument(\'--on-gpu\', action=\'store_true\', default=False)\n\n\ndef run_test_from_config(trainer_options):\n    """"""Trains the default model with the given config.""""""\n    set_random_master_port()\n\n    ckpt_path = trainer_options[\'default_root_dir\']\n    trainer_options.update(checkpoint_callback=ModelCheckpoint(ckpt_path))\n\n    model = EvalModelTemplate()\n    run_model_test(trainer_options, model, on_gpu=args.on_gpu, version=0, with_hpc=False)\n\n    # Horovod should be initialized following training. If not, this will raise an exception.\n    assert hvd.size() == 2\n\n    if args.on_gpu:\n        trainer = Trainer(gpus=1, distributed_backend=\'horovod\', max_epochs=1)\n        # Test the root_gpu property\n        assert trainer.root_gpu == hvd.local_rank()\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    run_test_from_config(json.loads(args.trainer_options))\n'"
