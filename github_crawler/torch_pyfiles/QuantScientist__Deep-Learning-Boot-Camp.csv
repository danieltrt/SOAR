file_path,api_count,code
day03/kaggle_data.py,0,"b'""""""\nUtility functions to load Kaggle Otto Group Challenge Data.\n\nSince these data/functions are used in many notebooks, it is better\nto centralise functions to load and manipulate data so\nto not replicate code.\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\n\ndef load_data(path, train=True):\n    """"""Load data from a CSV File\n    \n    Parameters\n    ----------\n    path: str\n        The path to the CSV file\n        \n    train: bool (default True)\n        Decide whether or not data are *training data*.\n        If True, some random shuffling is applied.\n        \n    Return\n    ------\n    X: numpy.ndarray \n        The data as a multi dimensional array of floats\n    ids: numpy.ndarray\n        A vector of ids for each sample\n    """"""\n    df = pd.read_csv(path)\n    X = df.values.copy()\n    if train:\n        np.random.shuffle(X)  # https://youtu.be/uyUXoap67N8\n        X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n        return X, labels\n    else:\n        X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n        return X, ids\n        \n        \ndef preprocess_data(X, scaler=None):\n    """"""Preprocess input data by standardise features \n    by removing the mean and scaling to unit variance""""""\n    if not scaler:\n        scaler = StandardScaler()\n        scaler.fit(X)\n    X = scaler.transform(X)\n    return X, scaler\n\n\ndef preprocess_labels(labels, encoder=None, categorical=True):\n    """"""Encode labels with values among 0 and `n-classes-1`""""""\n    if not encoder:\n        encoder = LabelEncoder()\n        encoder.fit(labels)\n    y = encoder.transform(labels).astype(np.int32)\n    if categorical:\n        y = np_utils.to_categorical(y)\n    return y, encoder'"
day03/mnist_data.py,0,"b'# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\n\n\ndef maybe_download(filename, work_directory):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not tf.gfile.Exists(work_directory):\n    tf.gfile.MakeDirs(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.size()\n    print(\'Successfully downloaded\', filename, size, \'bytes.\')\n  return filepath\n\n\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef extract_images(filename):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n  print(\'Extracting\', filename)\n  with tf.gfile.Open(filename, \'rb\') as f, gzip.GzipFile(fileobj=f) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST image file: %s\' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\n\n\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\n\n\ndef extract_labels(filename, one_hot=False):\n  """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n  print(\'Extracting\', filename)\n  with tf.gfile.Open(filename, \'rb\') as f, gzip.GzipFile(fileobj=f) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST label file: %s\' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\n\n\nclass DataSet(object):\n\n  def __init__(self, images, labels, fake_data=False, one_hot=False,\n               dtype=tf.float32):\n    """"""Construct a DataSet.\n\n    one_hot arg is used only if fake_data is true.  `dtype` can be either\n    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n    `[0, 1]`.\n    """"""\n    dtype = tf.as_dtype(dtype).base_dtype\n    if dtype not in (tf.uint8, tf.float32):\n      raise TypeError(\'Invalid image dtype %r, expected uint8 or float32\' %\n                      dtype)\n    if fake_data:\n      self._num_examples = 10000\n      self.one_hot = one_hot\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          \'images.shape: %s labels.shape: %s\' % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      if dtype == tf.float32:\n        # Convert from [0, 255] -> [0.0, 1.0].\n        images = images.astype(numpy.float32)\n        images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n\n  @property\n  def images(self):\n    return self._images\n\n  @property\n  def labels(self):\n    return self._labels\n\n  @property\n  def num_examples(self):\n    return self._num_examples\n\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1] * 784\n      if self.one_hot:\n        fake_label = [1] + [0] * 9\n      else:\n        fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\n\n\ndef read_data_sets(train_dir, fake_data=False, one_hot=False, dtype=tf.float32):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n\n  if fake_data:\n    def fake():\n      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n    data_sets.train = fake()\n    data_sets.validation = fake()\n    data_sets.test = fake()\n    return data_sets\n\n  TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n  TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n  TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n  TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n  VALIDATION_SIZE = 5000\n\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n\n  data_sets.train = DataSet(train_images, train_labels, dtype=dtype)\n  data_sets.validation = DataSet(validation_images, validation_labels,\n                                 dtype=dtype)\n  data_sets.test = DataSet(test_images, test_labels, dtype=dtype)\n\n  return data_sets'"
day03/word2vec.py,0,"b'from gensim.models import word2vec\nfrom os.path import join, exists, split\nimport os\nimport numpy as np\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    """"""\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    """"""\n    model_dir = \'word2vec_models\'\n    model_name = ""{:d}features_{:d}minwords_{:d}context"".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name):\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print(\'Loading existing Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 2       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(""Training Word2Vec model..."")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don\'t plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print(\'Saving Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights\n\nif __name__==\'__main__\':\n    import data_helpers\n    print(""Loading data..."")\n    x, _, _, vocabulary_inv = data_helpers.load_data()\n    w = train_word2vec(x, vocabulary_inv)\n'"
day03/word_embedding.py,0,"b'import numpy as np\nimport re\nimport itertools\nfrom collections import Counter\n""""""\nOriginal taken from https://github.com/dennybritz/cnn-text-classification-tf\n""""""\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    """"""\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels():\n    """"""\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    """"""\n    # Load data from files\n    positive_examples = list(open(""./data/word_embeddings/rt-polarity.pos"", encoding=\'ISO-8859-1\').readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(""./data/word_embeddings/rt-polarity.neg"", encoding=\'ISO-8859-1\').readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    x_text = [s.split("" "") for s in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef pad_sentences(sentences, padding_word=""<PAD/>""):\n    """"""\n    Pads all sentences to the same length. The length is defined by the longest sentence.\n    Returns padded sentences.\n    """"""\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i in range(len(sentences)):\n        sentence = sentences[i]\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n    return padded_sentences\n\n\ndef build_vocab(sentences):\n    """"""\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    """"""\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return [vocabulary, vocabulary_inv]\n\n\ndef build_input_data(sentences, labels, vocabulary):\n    """"""\n    Maps sentencs and labels to vectors based on a vocabulary.\n    """"""\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return [x, y]\n\n\ndef load_data():\n    """"""\n    Loads and preprocessed data for the MR dataset.\n    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n    """"""\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    x, y = build_input_data(sentences_padded, labels, vocabulary)\n    return [x, y, vocabulary, vocabulary_inv]\n\n\ndef batch_iter(data, batch_size, num_epochs):\n    """"""\n    Generates a batch iterator for a dataset.\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int(len(data)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]'"
day05/qlearn.py,0,"b'#!/usr/bin/env python\nfrom __future__ import print_function\n\nimport argparse\nimport skimage as skimage\nfrom skimage import transform, color, exposure\nfrom skimage.transform import rotate\nfrom skimage.viewer import ImageViewer\nimport sys\nsys.path.append(""game/"")\nimport wrapped_flappy_bird as game\nimport random\nimport numpy as np\nfrom collections import deque\n\nimport json\nfrom keras import initializations\nfrom keras.initializations import normal, identity\nfrom keras.models import model_from_json\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD , Adam\nimport tensorflow as tf\n\n#YOUR CONFIGURATION\nMODE = \'Run\' \n#MODE = \'Train\'\n#YOUR CONFIGURATION\n\n\n\nGAME = \'bird\' # the name of the game being played for log files\nCONFIG = \'nothreshold\'\nACTIONS = 2 # number of valid actions\nGAMMA = 0.99 # decay rate of past observations\nOBSERVATION = 3200. # timesteps to observe before training\nEXPLORE = 3000000. # frames over which to anneal epsilon\nFINAL_EPSILON = 0.0001 # final value of epsilon\nINITIAL_EPSILON = 0.1 # starting value of epsilon\nREPLAY_MEMORY = 50000 # number of previous transitions to remember\nBATCH = 32 # size of minibatch\nFRAME_PER_ACTION = 1\nLEARNING_RATE = 1e-4\n\nimg_rows , img_cols = 80, 80\n#Convert image into Black and white\nimg_channels = 4 #We stack 4 frames\n\ndef buildmodel():\n    # FILL ME\n    # MODEL BUILDING\n    # HINT: input_shape=(img_rows,img_cols,img_channels)\n\n    # model = YOUR CODE...\n\n    #END OF MODEL BUILDING\n\n    return model\n\ndef trainNetwork(model,args):\n    #INITS - NOT INTERESTING\n    # open up a game state to communicate with emulator\n    game_state = game.GameState()\n    # store the previous observations in replay memory\n    D = deque()\n    # get the first state by doing nothing and preprocess the image to 80x80x4\n    do_nothing = np.zeros(ACTIONS)\n    do_nothing[0] = 1\n    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n    x_t = skimage.color.rgb2gray(x_t)\n    x_t = skimage.transform.resize(x_t,(80,80))\n    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n    #In Keras, need to reshape\n    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n    if args[\'mode\'] == \'Run\':\n        OBSERVE = 999999999    #We keep observe, never train\n        epsilon = FINAL_EPSILON\n        print (""Now we load weight"")\n        model.load_weights(""model.h5"")\n        adam = Adam(lr=LEARNING_RATE)\n        model.compile(loss=\'mse\',optimizer=adam)\n        print (""Weight load successfully"")    \n    else:                       #We go to training mode\n        OBSERVE = OBSERVATION\n        epsilon = INITIAL_EPSILON\n    #END OF INITS - NOT INTERESTING\n\n    t = 0\n    while (True):\n        loss = 0\n        Q_sa = 0\n        action_index = 0\n        r_t = 0\n        a_t = np.zeros([ACTIONS])\n        #choose an action epsilon greedy\n        if t % FRAME_PER_ACTION == 0:\n            if random.random() <= epsilon:\n                print(""----------Random Action----------"")\n                action_index = random.randrange(ACTIONS)\n                a_t[action_index] = 1\n            else:\n                #FILL ME\n                #s_t is the state\n                #q is the prediction of your model\n                \n                #q = YOUR CODE...\n\n                #FILL ME\n                max_Q = np.argmax(q)\n                action_index = max_Q\n                a_t[max_Q] = 1\n\n        #We reduced the epsilon gradually\n        if epsilon > FINAL_EPSILON and t > OBSERVE:\n            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n        #run the selected action and observed next state and reward\n        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n        x_t1 = skimage.transform.resize(x_t1,(80,80))\n        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))\n        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n        # store the transition in D\n        D.append((s_t, action_index, r_t, s_t1, terminal))\n        if len(D) > REPLAY_MEMORY:\n            D.popleft()\n        #only train if done observing\n        if t > OBSERVE:\n            #sample a minibatch to train on\n            minibatch = random.sample(D, BATCH)\n            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 80, 80, 4\n            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n            \n            \n            #Now we do the experience replay\n            for i in range(0, len(minibatch)):\n                state_t = minibatch[i][0]\n                action_t = minibatch[i][1]   #This is action index\n                reward_t = minibatch[i][2]\n                state_t1 = minibatch[i][3]\n                terminal = minibatch[i][4]\n                # if terminated, only equals reward\n\n                inputs[i:i + 1] = state_t    #I saved down s_t\n               \n\n\n                #VERY IMPORTANT! UNDERSTAND THIS\n                targets[i] = model.predict(state_t) \n                Q_sa = model.predict(state_t1)\n                if terminal:\n                    targets[i, action_t] = reward_t\n                else:\n                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n                #END OF VERY IMPORTANT! UNDERSTAND THIS\n\n\n\n\n            loss += model.train_on_batch(inputs, targets)\n        s_t = s_t1\n        t = t + 1\n        # save progress every 10000 iterations\n        if t % 1000 == 0:\n            print(""Now we save model"")\n            model.save_weights(""model.h5"", overwrite=True)\n            with open(""model.json"", ""w"") as outfile:\n                json.dump(model.to_json(), outfile)\n        state = """"\n        if t <= OBSERVE:\n            state = ""observe""\n        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n            state = ""explore""\n        else:\n            state = ""train""\n        print(""TIMESTEP"", t, ""/ STATE"", state, \\\n            ""/ EPSILON"", epsilon, ""/ ACTION"", action_index, ""/ REWARD"", r_t, \\\n            ""/ Q_MAX "" , np.max(Q_sa), ""/ Loss "", loss)\n    print(""Episode finished!"")\n    print(""************************"")\n\ndef playGame(args):\n    model = buildmodel()\n    trainNetwork(model,args)\n\ndef main():\n    playGame({\'mode\':MODE})\n\nif __name__ == ""__main__"":\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    from keras import backend as K\n    K.set_session(sess)\n    main()\n'"
docker/jupyter_notebook_config.py,0,"b'# Copyright 2015 The deep-ml.com Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport sys\nsys.path.append(\'/root/inst/bin/\')\n\n# print (os.environ[""PATH""])\n\nfrom distutils.spawn import find_executable\nwhichCling = find_executable(\'cling\')\n\nprint (\'cling =\' + str(whichCling))\n\nc.NotebookApp.ip = \'*\'\nc.NotebookApp.port = 7842\nc.NotebookApp.open_browser = False\nc.MultiKernelManager.default_kernel_name = \'python2\'\n\n# sets a password if PASSWORD is set in the environment\nc.NotebookApp.token = \'\'\n#c.NotebookApp.password = \'\'\n# password is eric=123\nc.NotebookApp.password=\'sha1:65db47cf7e0d:d440485d58ec9fcc8b587c0aa96864f2f1816edd\'\n'"
Kaggle-PyTorch/PyTorch-Ensembler/main-dogs-cats.py,10,"b'from __future__ import print_function\n\nimport argparse\nimport sys\n\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom kdataset import *\nfrom utils import *\n\n# Random seed\n\nmodel_names = sorted(name for name in nnmodels.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(nnmodels.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch Ensembler\')\n\nprint(""Available models:"" + str(model_names))\n\nparser.add_argument(\'--validationRatio\', type=float, default=0.90, help=\'test Validation Split.\')\nparser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'Adam or SGD\')\nparser.add_argument(\'--lr_period\', default=10, type=float, help=\'learning rate schedule restart period\')\nparser.add_argument(\'--batch_size\', default=16, type=int, metavar=\'N\', help=\'train batchsize\')\n\nparser.add_argument(\'--num_classes\', type=int, default=12, help=\'Number of Classes in data set.\')\nparser.add_argument(\'--data_path\', default=\'d:/db/data/cat-dog/train/\', type=str, help=\'Path to train dataset\')\nparser.add_argument(\'--data_path_test\', default=\'d:/db/data/cat-dog/test/\', type=str, help=\'Path to test dataset\')\nparser.add_argument(\'--valid_path\', default=\'d:/db/data/cat-dog/valid\', help=\'path to the valid data folder\')\nparser.add_argument(\'--dataset\', type=str, default=\'catdog\', choices=[\'cats\'], help=\'Choose between data sets\')\n\n# parser.add_argument(\'--arch\', metavar=\'ARCH\', default=\'simple\', choices=model_names)\nparser.add_argument(\'--imgDim\', default=3, type=int, help=\'number of Image input dimensions\')\nparser.add_argument(\'--img_scale\', default=224, type=int, help=\'Image scaling dimensions\')\nparser.add_argument(\'--base_factor\', default=20, type=int, help=\'SENet base factor\')\n\nparser.add_argument(\'--epochs\', type=int, default=70, help=\'Number of epochs to train.\')\nparser.add_argument(\'--current_time\', type=str, default=datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'),\n                    help=\'Current time.\')\n\nparser.add_argument(\'--lr\', \'--learning-rate\', type=float, default=0.0005, help=\'The Learning Rate.\')\nparser.add_argument(\'--momentum\', type=float, default=0.95, help=\'Momentum.\')\nparser.add_argument(\'--decay\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n\n# Checkpoints\nparser.add_argument(\'--print_freq\', default=400, type=int, metavar=\'N\', help=\'print frequency (default: 200)\')\nparser.add_argument(\'--save_path\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\nparser.add_argument(\'--save_path_model\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\nparser.add_argument(\'--start_epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--workers\', type=int, default=0, help=\'number of data loading workers (default: 0)\')\n# random seed\nparser.add_argument(\'--manualSeed\', type=int, default=999, help=\'manual seed\')\n\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\n\nif not os.path.isdir(args.save_path):\n    os.makedirs(args.save_path)\n\n# Use CUDA\nargs = parser.parse_args()\nargs.use_cuda = args.ngpu > 0 and torch.cuda.is_available()\nuse_cuda = args.use_cuda\n\nif args.manualSeed is None:\n    args.manualSeed = 999\nfixSeed(args)\n\ntry:\n    from pycrayon import CrayonClient\nexcept ImportError:\n    CrayonClient = None\n\ndef train(train_loader, model, criterion, optimizer, args):\n    if args.use_cuda:\n        model.cuda()\n        criterion.cuda()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acc = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (images, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if args.use_cuda:\n            images, target = images.cuda(), target.cuda()\n            images, target = Variable(images), Variable(target)\n        # compute y_pred\n        y_pred = model(images)\n        loss = criterion(y_pred, target)\n\n        # measure accuracy and record loss\n        prec1, prec1 = accuracy(y_pred.data, target.data, topk=(1, 1))\n        losses.update(loss.data[0], images.size(0))\n        acc.update(prec1[0], images.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'TRAIN: LOSS-->{loss.val:.4f} ({loss.avg:.4f})\\t\' \'ACC-->{acc.val:.3f}% ({acc.avg:.3f}%)\'.format(loss=losses, acc=acc))\n            if use_tensorboard:\n                exp.add_scalar_value(\'tr_epoch_loss\', losses.avg, step=epoch)\n                exp.add_scalar_value(\'tr_epoch_acc\', acc.avg, step=epoch)\n\n    return float(\'{loss.avg:.4f}\'.format(loss=losses)), float(\'{acc.avg:.4f}\'.format(acc=acc))\n\ndef validate(val_loader, model, criterion, args):\n    if args.use_cuda:\n        model.cuda()\n        criterion.cuda()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    acc = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (images, labels) in enumerate(val_loader):\n\n        if use_cuda:\n            images, labels = images.cuda(), labels.cuda()\n            images, labels = Variable(images, volatile=True), Variable(labels)\n\n        # compute y_pred\n        y_pred = model(images)\n        loss = criterion(y_pred, labels)\n\n        # measure accuracy and record loss\n        prec1, temp_var = accuracy(y_pred.data, labels.data, topk=(1, 1))\n        losses.update(loss.data[0], images.size(0))\n        acc.update(prec1[0], images.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 400== 0:\n            print(\'VAL:   LOSS--> {loss.val:.4f} ({loss.avg:.4f})\\t\'\'ACC-->{acc.val:.3f} ({acc.avg:.3f})\'.format(loss=losses, acc=acc))\n\n        if i % 100 == 0:\n            if use_tensorboard:\n                exp.add_scalar_value(\'val_epoch_loss\', losses.avg, step=epoch)\n                exp.add_scalar_value(\'val_epoch_acc\', acc.avg, step=epoch)\n\n    print(\' * Accuracy {acc.avg:.3f}\'.format(acc=acc))\n    return float(\'{loss.avg:.4f}\'.format(loss=losses)), float(\'{acc.avg:.4f}\'.format(acc=acc))\n\n\ndef loadDB(args):\n    # Data\n    print(\'==> Preparing dataset %s\' % args.dataset)\n\n    classes, class_to_idx, num_to_class, df = find_classes(args.data_path)\n\n    train_data = df.sample(frac=args.validationRatio)\n    valid_data = df[~df[\'file\'].isin(train_data[\'file\'])]\n\n    train_set = SeedDataset(train_data, args.data_path, transform=train_trans)\n    valid_set = SeedDataset(valid_data, args.data_path, transform=valid_trans)\n\n    t_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n    v_loader = DataLoader(valid_set, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n\n    dataset_sizes = {\n        \'train\': len(t_loader.dataset),\n        \'valid\': len(v_loader.dataset)\n    }\n    print(dataset_sizes)\n    print(\'#Classes: {}\'.format(len(classes)))\n    args.num_classes = len(classes)\n    args.imgDim = 3\n\n    return t_loader, v_loader, train_set, valid_set, classes, class_to_idx, num_to_class, df\n\ncolumns = [\'id\', \'label\']\n\nfrom ktransforms import *\n\n# train_trans = transforms.Compose([\n#     transforms.RandomSizedCrop(args.img_scale),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],),\n#     RandomErasing(),\n# ])\n\n## Augmentation + Normalization for full training\ntrain_trans = transforms.Compose([\n    transforms.RandomSizedCrop(args.img_scale),\n    PowerPIL(),\n    transforms.ToTensor(),\n    normalize_img,\n    RandomErasing()\n])\n\n## Normalization only for validation and test\nvalid_trans = transforms.Compose([\n    transforms.Scale(256),\n    transforms.CenterCrop(args.img_scale),\n    transforms.ToTensor(),\n    normalize_img\n])\n# valid_trans = transforms.Compose([\n#     transforms.Scale(256),\n#     transforms.CenterCrop(args.img_scale),\n#     transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n#     #RandomErasing(),\n# ])\n# valid_trans=ds_transform_raw\n\ntest_trans = valid_trans\n\ndef testImageLoader(image_name):\n    """"""load image, returns cuda tensor""""""\n#     image = Image.open(image_name)\n    image = Image.open(image_name).convert(\'RGB\')\n    image = test_trans(image)\n#     image = Variable(image, requires_grad=True)\n    image = image.unsqueeze(0)\n    if args.use_cuda:\n        image.cuda()\n    return image\n\n\ndef testModel(test_dir, local_model, sample_submission):\n    print (\'Testing model\')\n    # print (\'Testing model: {}\'.format(str(local_model)))\n    if args.use_cuda:\n        local_model.cuda()\n    local_model.eval()\n\n    df_pred = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n    df_pred[\'id\'].astype(int)\n    for index, row in (sample_submission.iterrows()):\n        #         for file in os.listdir(test_dir):\n        int_id=int(row[\'id\'])\n        currImage = os.path.join(test_dir, str(int(int_id)) + \'.jpg\')\n        if os.path.isfile(currImage):\n            X_tensor_test = testImageLoader(currImage)\n            if args.use_cuda:\n                X_tensor_test = Variable(X_tensor_test.cuda())\n            else:\n                X_tensor_test = Variable(X_tensor_test)\n\n            y_pred = model(X_tensor_test)\n            # predicted_val = (local_model(X_tensor_test)).data.max(1)[1]  # get the index of the max log-probability\n            # get the index of the max log-probability\n            smax = nn.Softmax()\n            smax_out = smax(y_pred)[0]\n            cat_prob = smax_out.data[0]\n            dog_prob = smax_out.data[1]\n            prob = dog_prob\n            if cat_prob > dog_prob:\n                prob = 1 - cat_prob\n            prob = np.around(prob, decimals=6)\n            prob = np.clip(prob, .0001, .999)\n\n            # p_test = (predicted_val.cpu().numpy().item())\n            # df_pred = df_pred.append({\'id\': int(row[\'id\']), \'label\': num_to_class[int(p_test)]}, ignore_index=True) # for lables\n            df_pred = df_pred.append({\'id\': int(int_id), \'label\': prob}, ignore_index=True) # for proba\n\n    df_pred[\'id\'].astype(int)\n    return df_pred\n\nif __name__ == \'__main__\':\n\n    # tensorboad\n    use_tensorboard = False\n    # use_tensorboard = True and CrayonClient is not None\n\n    if use_tensorboard == True:\n        cc = CrayonClient(hostname=\'http://192.168.0.3\')\n        # cc.remove_all_experiments()\n\n    trainloader, valloader, trainset, valset, classes, class_to_idx, num_to_class, df = loadDB(args)\n    print(\'\xc3\x87lasses {}\'.format(classes))\n    models = [\'vggnet\']\n    for i in range (1,5):\n        for m in models:\n            runId = datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n            fixSeed(args)\n            model = selectModel(args, m)\n            recorder = RecorderMeter(args.epochs)  # epoc is updated\n            model_name = (type(model).__name__)\n\n            exp_name = datetime.datetime.now().strftime(model_name + \'_\' + args.dataset + \'_%Y-%m-%d_%H-%M-%S\')\n            if use_tensorboard == True:\n                exp = cc.create_experiment(exp_name)\n            # if model_name ==\'NoneType\':\n            #     EXIT\n            mPath = args.save_path + \'/\' + args.dataset + \'/\' + model_name + \'/\'\n            args.save_path_model = mPath\n            if not os.path.isdir(args.save_path_model):\n                mkdir_p(args.save_path_model)\n            log = open(os.path.join(args.save_path_model, \'log_seed_{}_{}.txt\'.format(args.manualSeed,\n                                                datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'))), \'w\')\n            print_log(\'Save path : {}\'.format(args.save_path_model), log)\n            print_log(state, log)\n            print_log(""Random Seed: {}"".format(args.manualSeed), log)\n            print_log(""python version : {}"".format(sys.version.replace(\'\\n\', \' \')), log)\n            print_log(""torch  version : {}"".format(torch.__version__), log)\n            print_log(""cudnn  version : {}"".format(torch.backends.cudnn.version()), log)\n            print_log(""Available models:"" + str(model_names), log)\n            print_log(""=> Final model name \'{}\'"".format(model_name), log)\n            # print_log(""=> Full model \'{}\'"".format(model), log)\n            # model = torch.nn.DataParallel(model).cuda()\n            model.cuda()\n            cudnn.benchmark = True\n            print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n            print(\'Batch size : {}\'.format(args.batch_size))\n\n            criterion = torch.nn.CrossEntropyLoss()  # multi class\n\n            if args.optim is \'adam\':\n                optimizer = torch.optim.Adam(model.parameters(), args.lr)  # L2 regularization\n            else:\n                optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=state[\'momentum\'],\n                                            weight_decay=state[\'weight_decay\'], nesterov=True)\n\n            for epoch in tqdm(range(args.start_epoch, args.epochs)):\n                train_result, accuracy_tr = train(trainloader, model, criterion, optimizer, args)\n                # evaluate on validation set\n                val_loss, accuracy_val = validate(valloader, model, criterion,args)\n\n                recorder.update(epoch, train_result, accuracy_tr, val_loss, accuracy_val)\n                mPath = args.save_path_model + \'/\'\n                if not os.path.isdir(mPath):\n                    os.makedirs(mPath)\n                recorder.plot_curve(os.path.join(mPath, model_name + \'_\' + runId + \'.png\'), args, model)\n\n                if (float(val_loss) < float(0.15)):\n                    print (""*** EARLY STOPPING ***"")\n                    s_submission = pd.read_csv(\'catdog-sample_submission.csv\')\n                    s_submission.columns = columns\n                    df_pred = testModel(args.data_path_test, model, s_submission)\n\n                    pre = args.save_path_model + \'/\' + \'/pth/\'\n                    if not os.path.isdir(pre):\n                        os.makedirs(pre)\n                    fName = pre + str(val_loss)\n                    torch.save(model.state_dict(), fName + \'_cnn.pth\')\n                    csv_path = str(fName + \'_submission.csv\')\n                    df_pred[\'id\'] = df_pred[\'id\'].astype(int)\n                    df_pred.to_csv(csv_path, columns=(\'id\', \'label\'), index=None)\n                    print(csv_path)\n'"
Kaggle-PyTorch/PyTorch-Ensembler/main-seedlings.py,10,"b'from __future__ import print_function\n\nimport argparse\nimport sys\n\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom kdataset import *\nfrom utils import *\nfrom pycrayon import *\n# Random seed\n\nmodel_names = sorted(name for name in nnmodels.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(nnmodels.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch Ensembler\')\n\nprint(""Available models:"" + str(model_names))\n\nparser.add_argument(\'--validationRatio\', type=float, default=0.90, help=\'test Validation Split.\')\nparser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'Adam or SGD\')\nparser.add_argument(\'--lr_period\', default=10, type=float, help=\'learning rate schedule restart period\')\nparser.add_argument(\'--batch_size\', default=16, type=int, metavar=\'N\', help=\'train batchsize\')\n\nparser.add_argument(\'--num_classes\', type=int, default=12, help=\'Number of Classes in data set.\')\nparser.add_argument(\'--data_path\', default=\'d:/db/data/seedings/train/\', type=str, help=\'Path to train dataset\')\nparser.add_argument(\'--data_path_test\', default=\'d:/db/data/seedings/test/\', type=str, help=\'Path to test dataset\')\nparser.add_argument(\'--dataset\', type=str, default=\'seeds\', choices=[\'seeds\'], help=\'Choose between data sets\')\n\n# parser.add_argument(\'--arch\', metavar=\'ARCH\', default=\'simple\', choices=model_names)\nparser.add_argument(\'--imgDim\', default=3, type=int, help=\'number of Image input dimensions\')\nparser.add_argument(\'--img_scale\', default=224, type=int, help=\'Image scaling dimensions\')\nparser.add_argument(\'--base_factor\', default=20, type=int, help=\'SENet base factor\')\n\nparser.add_argument(\'--epochs\', type=int, default=70, help=\'Number of epochs to train.\')\nparser.add_argument(\'--current_time\', type=str, default=datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'),\n                    help=\'Current time.\')\n\nparser.add_argument(\'--lr\', \'--learning-rate\', type=float, default=0.0005, help=\'The Learning Rate.\')\nparser.add_argument(\'--momentum\', type=float, default=0.95, help=\'Momentum.\')\nparser.add_argument(\'--decay\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n# parser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225], help=\'Decrease learning rate at these epochs.\')\n# parser.add_argument(\'--gammas\', type=float, nargs=\'+\', default=[0.1, 0.1],help=\'LR is multiplied by gamma on schedule, number of gammas should be equal to schedule\')\n\n# Checkpoints\nparser.add_argument(\'--print_freq\', default=50, type=int, metavar=\'N\', help=\'print frequency (default: 200)\')\nparser.add_argument(\'--save_path\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\nparser.add_argument(\'--save_path_model\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\nparser.add_argument(\'--start_epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--workers\', type=int, default=0, help=\'number of data loading workers (default: 0)\')\n# random seed\nparser.add_argument(\'--manualSeed\', type=int, default=999, help=\'manual seed\')\n\n# Random Erasing\nfrom ktransforms import *\nparser.add_argument(\'--p\', default=0.32, type=float, help=\'Random Erasing probability\')\nparser.add_argument(\'--sh\', default=0.4, type=float, help=\'max erasing area\')\nparser.add_argument(\'--r1\', default=0.3, type=float, help=\'aspect of erasing area\')\n\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\n\nif not os.path.isdir(args.save_path):\n    os.makedirs(args.save_path)\n\n# Use CUDA\nargs = parser.parse_args()\nargs.use_cuda = args.ngpu > 0 and torch.cuda.is_available()\nuse_cuda = args.use_cuda\n\nif args.manualSeed is None:\n    args.manualSeed = 999\nfixSeed(args)\n\n\ndef train(train_loader, model, criterion, optimizer, args):\n    if args.use_cuda:\n        model.cuda()\n        criterion.cuda()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acc = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (images, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if args.use_cuda:\n            images, target = images.cuda(), target.cuda()\n            images, target = Variable(images), Variable(target)\n        # compute y_pred\n        y_pred = model(images)\n        loss = criterion(y_pred, target)\n\n        # measure accuracy and record loss\n        prec1, prec1 = accuracy(y_pred.data, target.data, topk=(1, 1))\n        losses.update(loss.data[0], images.size(0))\n        acc.update(prec1[0], images.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        # if i % args.print_freq == 0:\n        if i % 400 == 0:\n            print(\'TRAIN: LOSS-->{loss.val:.4f} ({loss.avg:.4f})\\t\' \'ACC-->{acc.val:.3f}% ({acc.avg:.3f}%)\'.format(loss=losses, acc=acc))\n            if use_tensorboard:\n                exp.add_scalar_value(\'tr_epoch_loss\', losses.avg, step=epoch)\n                exp.add_scalar_value(\'tr_epoch_acc\', acc.avg, step=epoch)\n\n    return float(\'{loss.avg:.4f}\'.format(loss=losses)), float(\'{acc.avg:.4f}\'.format(acc=acc))\n\ndef validate(val_loader, model, criterion, args):\n    if args.use_cuda:\n        model.cuda()\n        criterion.cuda()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    acc = AverageMeter()\n\n    # switch to evaluate mode\n    # model.eval()\n\n    end = time.time()\n    for i, (images, labels) in enumerate(val_loader):\n\n        if use_cuda:\n            images, labels = images.cuda(), labels.cuda()\n            images, labels = Variable(images, volatile=True), Variable(labels)\n\n        # compute y_pred\n        y_pred = model(images)\n        loss = criterion(y_pred, labels)\n\n        # measure accuracy and record loss\n        prec1, temp_var = accuracy(y_pred.data, labels.data, topk=(1, 1))\n        losses.update(loss.data[0], images.size(0))\n        acc.update(prec1[0], images.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 400 == 0:\n            print(\'VAL:   LOSS--> {loss.val:.4f} ({loss.avg:.4f})\\t\'\'ACC-->{acc.val:.3f} ({acc.avg:.3f})\'.format(\n                loss=losses, acc=acc))\n\n        if i % 50 == 0:\n            if use_tensorboard:\n                exp.add_scalar_value(\'val_epoch_loss\', losses.avg, step=epoch)\n                exp.add_scalar_value(\'val_epoch_acc\', acc.avg, step=epoch)\n\n    print(\'FINAL ACC: {acc.avg:.3f}\'.format(acc=acc))\n    return float(\'{loss.avg:.4f}\'.format(loss=losses)), float(\'{acc.avg:.4f}\'.format(acc=acc))\n\n# def train(train_loader, model, epoch, optimizer):\n#     model.train()\n#\n#     for batch_idx, (data, target) in ((enumerate(train_loader))):\n#         correct = 0\n#         if use_cuda:\n#             data, target = data.cuda(), target.cuda()\n#         data, target = Variable(data), Variable(target)\n#         optimizer.zero_grad()\n#         output = model(data)\n#         loss = criterion(output, target)\n#         loss.backward()\n#         pred = output.data.max(1)[1]  # get the index of the max log-probability\n#         correct += pred.eq(target.data).cpu().sum()\n#         accuracy = 100. * correct / len(data)\n#         optimizer.step()\n#         if batch_idx % 200 == 0:\n#             print(\'TRAIN: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.3f}%)\'.format(\n#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n#                        100. * batch_idx / len(train_loader), loss.data[0],\n#                 correct, len(data),\n#                 accuracy))\n#\n#\n# def test(test_loader, model, epoch):\n#     #     model.eval()\n#     test_loss = 0\n#     correct = 0\n#     for data, target in (test_loader):\n#         if use_cuda:\n#             data, target = data.cuda(), target.cuda()\n#         data, target = Variable(data, volatile=True), Variable(target)\n#         output = model(data)\n#         test_loss += criterion(output, target).data[0]\n#         pred = output.data.max(1)[1]  # get the index of the max log-probability\n#         correct += pred.eq(target.data).cpu().sum()\n#\n#     test_loss = test_loss\n#     test_loss /= len(test_loader)  # loss function already averages over batch size\n#     accuracy = 100. * correct / len(test_loader.dataset)\n#     print(\'\\nVAL: Average loss: {:.6f}, Accuracy: {}/{} ({:.3f}%)\\n\'.format(\n#         test_loss, correct, len(test_loader.dataset),\n#         accuracy))\n#\n#     return test_loss, accuracy\n\n\ndef loadDB(args):\n    # Data\n    print(\'==> Preparing dataset %s\' % args.dataset)\n\n    classes, class_to_idx, num_to_class, df = find_classes(args.data_path)\n    print(\'\xc3\x87lasses {}\'.format(classes))\n\n    train_data = df.sample(frac=args.validationRatio)\n    valid_data = df[~df[\'file\'].isin(train_data[\'file\'])]\n\n    train_set = SeedDataset(train_data, args.data_path, transform=train_trans)\n    valid_set = SeedDataset(valid_data, args.data_path, transform=valid_trans)\n\n    t_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n    v_loader = DataLoader(valid_set, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n\n    dataset_sizes = {\n        \'train\': len(t_loader.dataset),\n        \'valid\': len(v_loader.dataset)\n    }\n    print(dataset_sizes)\n    print(\'#Classes: {}\'.format(len(classes)))\n    args.num_classes = len(classes)\n    args.imgDim = 3\n\n    return t_loader, v_loader, train_set, valid_set, classes, class_to_idx, num_to_class, df\n\n\n# train_trans = transforms.Compose([\n#     transforms.RandomSizedCrop(args.img_scale),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],),\n#     RandomErasing(),\n# ])\n\n## Augmentation + Normalization for full training\ntrain_trans = transforms.Compose([\n    transforms.RandomSizedCrop(args.img_scale),\n    PowerPIL(),\n    transforms.ToTensor(),\n    RandomErasing(),\n    normalize_img,\n])\n\n## Normalization only for validation and test\nvalid_trans = transforms.Compose([\n    transforms.Scale(256),\n    transforms.CenterCrop(args.img_scale),\n    transforms.ToTensor(),\n    normalize_img\n])\n# valid_trans = transforms.Compose([\n#     transforms.Scale(256),\n#     transforms.CenterCrop(args.img_scale),\n#     transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n#     #RandomErasing(),\n# ])\n# valid_trans=ds_transform_raw\n\ntest_trans = valid_trans\n\ndef testImageLoader(image_name):\n    """"""load image, returns cuda tensor""""""\n#     image = Image.open(image_name)\n    image = Image.open(image_name).convert(\'RGB\')\n    image = test_trans(image)\n#     image = Variable(image, requires_grad=True)\n    image = image.unsqueeze(0)\n    if args.use_cuda:\n        image.cuda()\n    return image\n\n\ndef testModel(test_dir, local_model, sample_submission):\n    # print (\'Testing model: {}\'.format(str(local_model)))\n    if args.use_cuda:\n        local_model.cuda()\n    local_model.eval()\n\n    columns = [\'file\', \'species\']\n    df_pred = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n    #     df_pred.species.astype(int)\n    for index, row in (sample_submission.iterrows()):\n        #         for file in os.listdir(test_dir):\n        currImage = os.path.join(test_dir, row[\'file\'])\n        if os.path.isfile(currImage):\n            X_tensor_test = testImageLoader(currImage)\n            #             print (type(X_tensor_test))\n            if args.use_cuda:\n                X_tensor_test = Variable(X_tensor_test.cuda())\n            else:\n                X_tensor_test = Variable(X_tensor_test)\n\n                # get the index of the max log-probability\n            predicted_val = (local_model(X_tensor_test)).data.max(1)[1]  # get the index of the max log-probability\n            #             predicted_val = predicted_val.data.max(1, keepdim=True)[1]\n            p_test = (predicted_val.cpu().numpy().item())\n            df_pred = df_pred.append({\'file\': row[\'file\'], \'species\': num_to_class[int(p_test)]}, ignore_index=True)\n\n    return df_pred\n\nif __name__ == \'__main__\':\n\n    # tensorboad\n    use_tensorboard = False\n    # use_tensorboard = True and CrayonClient is not None\n\n    if use_tensorboard == True:\n        cc = CrayonClient(hostname=\'http://192.168.0.3\')\n        cc.remove_all_experiments()\n\n\n    trainloader, valloader, trainset, valset, classes, class_to_idx, num_to_class, df = loadDB(args)\n    models = [\'simple\']\n    for i in range (1,5):\n        for m in models:\n            runId = datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n            fixSeed(args)\n            model = selectModel(args, m)\n            recorder = RecorderMeter(args.epochs)  # epoc is updated\n            model_name = (type(model).__name__)\n\n            exp_name = datetime.datetime.now().strftime(model_name + \'_\' + args.dataset + \'_%Y-%m-%d_%H-%M-%S\')\n            if use_tensorboard == True:\n                exp = cc.create_experiment(exp_name)\n\n            # if model_name ==\'NoneType\':\n            #     EXIT\n            mPath = args.save_path + \'/\' + args.dataset + \'/\' + model_name + \'/\'\n            args.save_path_model = mPath\n            if not os.path.isdir(args.save_path_model):\n                mkdir_p(args.save_path_model)\n            log = open(os.path.join(args.save_path_model, \'log_seed_{}_{}.txt\'.format(args.manualSeed, runId)), \'w\')\n            print_log(\'Save path : {}\'.format(args.save_path_model), log)\n            print_log(state, log)\n            print_log(""Random Seed: {}"".format(args.manualSeed), log)\n            print_log(""python version : {}"".format(sys.version.replace(\'\\n\', \' \')), log)\n            print_log(""torch  version : {}"".format(torch.__version__), log)\n            print_log(""cudnn  version : {}"".format(torch.backends.cudnn.version()), log)\n            print_log(""Available models:"" + str(model_names), log)\n            print_log(""=> Final model name \'{}\'"".format(model_name), log)\n            # print_log(""=> Full model \'{}\'"".format(model), log)\n            # model = torch.nn.DataParallel(model).cuda()\n            model.cuda()\n            cudnn.benchmark = True\n            print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n            print(\'Batch size : {}\'.format(args.batch_size))\n\n            criterion = torch.nn.CrossEntropyLoss()  # multi class\n            # optimizer = torch.optim.Adam(model.parameters(), args.lr)  # L2 regularization\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n\n\n            for epoch in tqdm(range(args.start_epoch, args.epochs)):\n                train_result, accuracy_tr = train(trainloader, model, criterion, optimizer, args)\n                # evaluate on validation set\n                val_result, accuracy_val = validate(valloader, model, criterion,args)\n\n                recorder.update(epoch, train_result, accuracy_tr, val_result, accuracy_val)\n                mPath = args.save_path_model + \'/\'\n                if not os.path.isdir(mPath):\n                    os.makedirs(mPath)\n                recorder.plot_curve(os.path.join(mPath, model_name + \'_\' + runId + \'.png\'), args, model)\n\n\n                if (float(accuracy_val) > float(90.0)):\n                    print (""*** EARLY STOPPING ***"")\n                    s_submission = pd.read_csv(args.data_path + \'sample_submission.csv\')\n                    s_submission.columns = [\'file\', \'species\']\n                    df_pred = testModel(args.data_path_test, model, s_submission)\n\n                    pre = args.save_path_model + \'/\' + \'/pth/\'\n                    if not os.path.isdir(pre):\n                        os.makedirs(pre)\n                    fName = pre + str(accuracy_val)\n                    torch.save(model.state_dict(), fName + \'_cnn.pth\')\n                    csv_path = str(fName + \'_submission.csv\')\n                    df_pred.to_csv(csv_path, columns=(\'file\', \'species\'), index=None)\n                    # df_pred.to_csv(csv_path, columns=(\'id\', \'is_iceberg\'), index=None)\n                    print(csv_path)\n'"
Kaggle-PyTorch/PyTorch-Ensembler/main-statoil.py,9,"b'from __future__ import print_function\n\nimport argparse\nimport sys\n\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nimport numpy as np\nfrom utils import *\n# from losses import Eve\n# from pycrayon import *\nn_folds = 10\nmodel_names = sorted(name for name in nnmodels.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(nnmodels.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch CIFAR10 and 100 Training\')\n\nprint(""Available models:"" + str(model_names))\n\nparser.add_argument(\'--validationRatio\', type=float, default=0.11, help=\'test Validation Split.\')\nparser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'Adam or SGD\')\nparser.add_argument(\'--lr_period\', default=20, type=float, help=\'learning rate schedule restart period\')\nparser.add_argument(\'--batch_size\', default=64, type=int, metavar=\'N\', help=\'train batchsize\')\n\nparser.add_argument(\'--num_classes\', type=int, default=1, help=\'Number of Classes in data set.\')\nparser.add_argument(\'--data_path\', default=\'/mnt/extDisk/nati/Deep-Learning-Boot-Camp/Kaggle-PyTorch/PyTorch-Ensembler/statoil/input\', type=str, help=\'Path to dataset\')\nparser.add_argument(\'--dataset\', type=str, default=\'statoil\', choices=[\'statoil\', \'statoil\'],\n                    help=\'Choose between Statoil.\')\n\n# parser.add_argument(\'--num_classes\', type=int, default=10, help=\'Number of Classes in data set.\')\n# parser.add_argument(\'--data_path\', default=\'d:/db/data/cifar10/\', type=str, help=\'Path to dataset\')\n# parser.add_argument(\'--dataset\', type=str, default=\'cifar10\',choices=[\'cifar10\', \'Iceberg\'],help=\'Choose between Cifar10/100 and ImageNet.\')\n\n\n# parser.add_argument(\'--arch\', metavar=\'ARCH\', default=\'senet\', choices=model_names)\nparser.add_argument(\'--imgDim\', default=2, type=int, help=\'number of Image input dimensions\')\nparser.add_argument(\'--base_factor\', default=32, type=int, help=\'SENet base factor\')\n\nparser.add_argument(\'--epochs\', type=int, default=66, help=\'Number of epochs to train.\')\nparser.add_argument(\'--current_time\', type=str, default=datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'),\n                    help=\'Current time.\')\n\n# parser.add_argument(\'--learning_rate\', type=float, default=0.0005, help=\'The Learning Rate.\')\nparser.add_argument(\'--lr\', \'--learning-rate\', type=float, default=0.0005, help=\'The Learning Rate.\')\nparser.add_argument(\'--momentum\', type=float, default=0.95, help=\'Momentum.\')\nparser.add_argument(\'--decay\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225],\n                    help=\'Decrease learning rate at these epochs.\')\nparser.add_argument(\'--gammas\', type=float, nargs=\'+\', default=[0.1, 0.1],\n                    help=\'LR is multiplied by gamma on schedule, number of gammas should be equal to schedule\')\n# Checkpoints\nparser.add_argument(\'--print_freq\', default=50, type=int, metavar=\'N\', help=\'print frequency (default: 200)\')\nparser.add_argument(\'--save_path\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\nparser.add_argument(\'--save_path_model\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start_epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--workers\', type=int, default=0, help=\'number of data loading workers (default: 0)\')\n# random seed\nparser.add_argument(\'--manualSeed\', type=int, default=999, help=\'manual seed\')\nparser.add_argument(\'--use_tensorboard\', type=bool, default=False, help=\'Log to tensorboard\')\nparser.add_argument(\'--tensorboard_ip\', type=str, default=\'http://192.168.1.226\', help=\'tensorboard IP\')\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\n\nif not os.path.isdir(args.save_path):\n    os.makedirs(args.save_path)\n\n# Use CUDA\nargs = parser.parse_args()\nargs.use_cuda = args.ngpu > 0 and torch.cuda.is_available()\nuse_cuda = args.use_cuda\n\nif args.manualSeed is None:\n    args.manualSeed = 999\nfixSeed(args)\n\n\ndef BinaryTrainAndValidate(model, criterion, optimizer, runId, debug=False):\n    if args.use_cuda:\n        model.cuda()\n        criterion.cuda()\n    all_losses = []\n    val_losses = []\n\n    for epoch in tqdm(range(args.epochs)):\n        # adjust_learning_rate(optimizer,epoch, args)\n        model.train()\n        tqdm.write(\'\\n==>>Epoch=[{:03d}/{:03d}]], {:s}, LR=[{}], Batch=[{}]\'.format(epoch, args.epochs, time_string(),\n                                                                                    state[\'lr\'],\n                                                                                    args.batch_size) + \' [Model={}]\'.format(\n            (type(model).__name__), ), log)\n\n        running_loss = 0.0\n        # for i, row_data in tqdm (enumerate(trainloader, 1)):\n        for i, row_data in (enumerate(trainloader, 1)):\n            img, label = row_data\n            if use_cuda:\n                img, label = Variable(img.cuda(async=True)), Variable(label.cuda(async=True))  # On GPU\n            else:\n                img, label = Variable(img), Variable(label)  # RuntimeError: expected CPU tensor (got CUDA tensor)\n\n            out = model(img)\n            loss = criterion(out, label)\n            running_loss += loss.data[0] * label.size(0)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            all_losses.append(running_loss / (args.batch_size * i))\n        predicted_tr = (model(img).data > 0.5).float()\n        accuracy_tr = (predicted_tr == label.data).float().mean() * 100\n\n        # model.eval()\n        eval_loss = 0\n        for row_data in testloader:\n            img, label = row_data\n            if use_cuda:\n                img, label = Variable(img.cuda(async=True), volatile=True), Variable(label.cuda(async=True),\n                                                                                     volatile=True)  # On GPU\n            else:\n                img = Variable(img, volatile=True)\n                label = Variable(label, volatile=True)\n            out = model(img)\n            loss = criterion(out, label)\n            eval_loss += loss.data[0] * label.size(0)\n\n        val_losses.append(eval_loss / (len(testset)))\n\n        predicted_val = (model(img).data > 0.5).float()\n        # predictions_val = predicted_val.cpu().numpy()\n        accuracy_val = (predicted_val == label.data).float().mean() * 100\n\n        if debug is True:\n            tqdm.write(\'-->LOSS T/V:[{:.6f}/{:.6f}%], ACC T/V:[{:.6f}/{:.6f}%]\'.format(running_loss / (len(trainset)),\n                                                                                       eval_loss / (len(testset)),\n                                                                                       accuracy_tr, accuracy_val))\n            if args.use_tensorboard:\n                exp.add_scalar_value(\'tr_epoch_loss\', running_loss / (len(trainset)), step=epoch)\n                exp.add_scalar_value(\'tr_epoch_acc\', accuracy_tr, step=epoch)\n\n                exp.add_scalar_value(\'val_epoch_loss\',  eval_loss / (len(testset)), step=epoch)\n                exp.add_scalar_value(\'val_epoch_acc\', accuracy_val, step=epoch)\n\n        val_result = float(\'{:.6f}\'.format(eval_loss / (len(testset))))\n        train_result = float(\'{:.6f}\'.format(running_loss / (len(trainset))))\n\n        recorder.update(epoch, train_result, accuracy_tr, val_result, accuracy_val)\n        mPath = args.save_path_model + \'/\'\n        if not os.path.isdir(mPath):\n            os.makedirs(mPath)\n        recorder.plot_curve(os.path.join(mPath, model_name + \'_\' + runId + \'.png\'), args, model)\n        logger.append([state[\'lr\'], train_result, val_result, accuracy_tr, accuracy_val])\n\n        if (float(val_result) < float(0.175) and float(train_result) < float(0.175)):\n            print_log(""=>>EARLY STOPPING"", log)\n            df_pred = BinaryInference(model, args)\n            savePred(df_pred, model, str(val_result) + \'_\' + str(epoch), train_result, args.save_path_model)\n            # break\n            continue\n        adjust_learning_rate(optimizer, epoch, args)\n\n    tqdm.write(\'TRAIN Loss: {:.6f}\'.format(running_loss / (len(trainset))), log)\n    tqdm.write(\'VALIDATION Loss: {:.6f}\'.format(eval_loss / (len(testset))), log)\n\n    val_result = \'{:.6f}\'.format(eval_loss / (len(testset)))\n    train_result = \'{:.6f}\'.format(running_loss / (len(trainset)))\n\n    return val_result, train_result\n\n\ndef loadDB(args,n_folds=5,current_fold=0):\n    # Data\n    print(\'==> Preparing dataset %s\' % args.dataset)\n    if args.dataset == \'statoil\':\n        args.num_classes = 1\n        args.imgDim = 2\n        trainloader, testloader, trainset, testset = getStatoilTrainValLoaders(args,n_folds,current_fold)\n\n    return trainloader, testloader, trainset, testset\n\n\nif __name__ == \'__main__\':\n\n    if args.use_tensorboard == True:\n        cc = CrayonClient(hostname=args.tensorboard_ip)\n        cc.remove_all_experiments()\n\n    # ensembleVer2(\'./pth_old/raw/iceResNet/\', \'./pth_old/ens2/ens_ice800files.csv\')\n    # MinMaxBestBaseStacking(\'./pth_old/2020/\', \'./pth_old/2020/0.1339.csv\',\'./pth_old/2020/final_mix_900_files_base01344.csv\')\n    # ensembleVer2(\'./log/statoil/IceResNet/pth\', \'./ens_ice_98989898989898989.csv\')\n    # ensembleVer2(\'./log/DenseNet/pth/\', \'./pth_old/ens2/ens_densnet_1_hours.csv\')\n\n    # vis = visdom.Visdom(port=6006)\n    \n    # for i in tqdm(range(0, 51)):\n    ids = pd.DataFrame()\n    oof = pd.DataFrame()\n    for i in range(n_folds):\n        trainloader, testloader, trainset, testset = loadDB(args,n_folds,i)\n        models = [\'resnext\']\n        for m in models:\n            runId = datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n            fixSeed(args)\n            model = selectModel(args, m)\n            model_name = (type(model).__name__)\n\n            exp_name = datetime.datetime.now().strftime(model_name + \'_\' + args.dataset + \'_%Y-%m-%d_%H-%M-%S\')\n            if args.use_tensorboard == True:\n                exp = cc.create_experiment(exp_name)\n\n\n            mPath = args.save_path + \'/\' + args.dataset + \'/\' + model_name + \'/\'\n            args.save_path_model = mPath\n            if not os.path.isdir(args.save_path_model):\n                mkdir_p(args.save_path_model)\n            log = open(os.path.join(args.save_path_model, \'log_seed_{}_{}.txt\'.format(args.manualSeed, runId)), \'w\')\n            print_log(\'Save path : {}\'.format(args.save_path_model), log)\n            print_log(state, log)\n            print_log(""Random Seed: {}"".format(args.manualSeed), log)\n            print_log(""python version : {}"".format(sys.version.replace(\'\\n\', \' \')), log)\n            print_log(""torch  version : {}"".format(torch.__version__), log)\n            print_log(""cudnn  version : {}"".format(torch.backends.cudnn.version()), log)\n            print_log(""LR :"" + str(args.lr), log)\n            print_log(""Available models:"" + str(model_names), log)\n            print_log(""=> Final model name: \'{}\'"".format(model_name), log)\n            print_log(""=> Log to TENSORBOARD: \'{}\'"".format(args.use_tensorboard), log)\n            print_log(""=> TENSORBOARD ip:\'{}\'"".format(args.tensorboard_ip), log)\n            # print_log(""=> Full model \'{}\'"".format(model), log)\n            # model = torch.nn.DataParallel(model).cuda()\n            model.cuda()\n            cudnn.benchmark = True\n            print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n\n            if args.num_classes == 1:\n                criterion = torch.nn.BCELoss()\n            else:\n                criterion = torch.nn.CrossEntropyLoss()\n            if args.optim is \'adam\':\n                optimizer = torch.optim.Adam(model.parameters(), args.lr)  # L2 regularization\n            else:\n                optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=state[\'momentum\'],\n                                            weight_decay=state[\'weight_decay\'], nesterov=True)\n\n            # print_log(""=> Criterion \'{}\'"".format(str(criterion)), log)\n            # print_log(""=> optimizer \'{}\'"".format(str(optimizer)), log)\n\n            title = model_name\n            logger = Logger(os.path.join(args.save_path_model, runId + \'_log.txt\'), title=title)\n            logger.set_names([\'LearningRate\', \'TrainLoss\', \'ValidLoss\', \'TrainAcc.\', \'ValidAcc.\'])\n            recorder = RecorderMeter(args.epochs)  # epoc is updated\n\n            val_result, train_result = BinaryTrainAndValidate(model, criterion, optimizer, runId, debug=True)\n            #if (float(val_result) < float(0.165) and float(train_result) < float(0.165)):\n                #df_pred = BinaryInference(model)\n            df_pred_oof, df_pred_test, ids_and_labels = BinaryInferenceOofAndTest(model,args,n_folds=n_folds,current_fold=i)\n            \n            ids = pd.concat([ids,ids_and_labels],axis=0)\n            oof = pd.concat([oof,pd.DataFrame(df_pred_oof)],axis=0)\n            \n            print(\'oof: {}, ids: {}\'.format(df_pred_oof.shape,ids_and_labels.shape))\n            print(\'i = \'.format(i))\n            \n            savePred(df_pred_test, model, val_result, train_result, args.save_path_model)\n            logger.close()\n            logger.plot()\n    ids_and_labels.to_csv(args.save_path_model + \'/ids_and_labels.csv\')\n    oof.to_csv(args.save_path_model + \'/oof_preds.csv\')\n    \n'"
Kaggle-PyTorch/PyTorch-Ensembler/main-tf-audio.py,27,"b'from __future__ import print_function\n\nimport time\n\nimport matplotlib\nimport torch.nn as nn\nimport torch.nn.init as init\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport os\nimport datetime\nimport pandas as pd\nfrom torch.utils.data import TensorDataset\n\nimport torch.nn.parallel\nfrom sklearn.utils import shuffle\nfrom torch.autograd import Variable\nfrom torch.utils.data import TensorDataset\nfrom torchvision.transforms import *\nimport argparse\nimport csv\nimport os\nimport os.path\nimport shutil\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nimport datetime\nimport random\n\nfrom utils import *\n\n# model_names = sorted(name for name in nnmodels.__dict__ if name.islower() and not name.startswith(""__""))\n\nparser = argparse.ArgumentParser(description=\'PyTorch SENet for TF commands\')\n\nparser.add_argument(\'--dataset\', type=str, default=\'tf\', choices=[\'tf\'], help=\'Choose between data sets\')\nparser.add_argument(\'--train_path\', default=\'d:/db/data/tf/2018/train\', help=\'path to the train data folder\')\nparser.add_argument(\'--test_path\', default=\'d:/db/data/tf/2018/test\', help=\'path to the test data folder\')\nparser.add_argument(\'--valid_path\', default=\'d:/db/data/tf/2018/valid\', help=\'path to the valid data folder\')\nparser.add_argument(\'--test_audio\', default=\'d:/db/data/tf/test/audio/\', help=\'path to the valid data folder\')\n\nparser.add_argument(\'--save_path\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\nparser.add_argument(\'--save_path_model\', type=str, default=\'./log/\', help=\'Folder to save checkpoints and log.\')\n\nparser.add_argument(\'--epochs\', default=30, type=int, metavar=\'N\', help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=16, type=int, metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.0005 * 2 * 2 , type=float, metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\', help=\'momentum\')\nparser.add_argument(\'--weight-decay\', default=1e-4, type=float, metavar=\'W\', help=\'weight decay\')\nparser.add_argument(\'--print-freq\', default=400, type=int, metavar=\'N\', help=\'print frequency\')\nparser.add_argument(\'--test\', default=False, help=\'evaluate model on test set\')\n\nparser.add_argument(\'--validationRatio\', type=float, default=0.11, help=\'test Validation Split.\')\nparser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'Adam or SGD\')\nparser.add_argument(\'--imgDim\', default=1, type=int, help=\'number of Image input dimensions\')\nparser.add_argument(\'--img_scale\', default=224, type=int, help=\'Image scaling dimensions\')\nparser.add_argument(\'--base_factor\', default=20, type=int, help=\'SENet base factor\')\n\nparser.add_argument(\'--current_time\', type=str, default=datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'),help=\'Current time.\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--workers\', type=int, default=0, help=\'number of data loading workers (default: 0)\')\n# random seed\nparser.add_argument(\'--manualSeed\', type=int, default=999, help=\'manual seed\')\n\nparser.add_argument(\'--num_classes\', type=int, default=12, help=\'Number of Classes in data set.\')\n\n\n# feature extraction options\nparser.add_argument(\'--window_size\', default=.02, help=\'window size for the stft\')\nparser.add_argument(\'--window_stride\', default=.01, help=\'window stride for the stft\')\nparser.add_argument(\'--window_type\', default=\'hamming\', help=\'window type for the stft\')\nparser.add_argument(\'--normalize\', default=True, help=\'boolean, wheather or not to normalize the spect\')\n\nimport librosa\nimport numpy as np\n\nAUDIO_EXTENSIONS = [\n    \'.wav\', \'.WAV\',\n]\n\nargs = parser.parse_args()\n\nstate = {k: v for k, v in args._get_kwargs()}\n\nif not os.path.isdir(args.save_path):\n    os.makedirs(args.save_path)\n\ndef fixSeed(args):\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if args.use_cuda:\n        torch.cuda.manual_seed(args.manualSeed)\n        torch.cuda.manual_seed_all(args.manualSeed)\n\n# Use CUDA\nargs = parser.parse_args()\nargs.use_cuda = args.ngpu > 0 and torch.cuda.is_available()\nuse_cuda = args.use_cuda\n\nif args.manualSeed is None:\n    args.manualSeed = 999\nfixSeed(args)\n\n\n\nimport librosa\nimport numpy as np\nimport librosa\nimport numpy as np\n\nAUDIO_EXTENSIONS = [\n    \'.wav\', \'.WAV\',\n]\n\n\ndef is_audio_file(filename):\n    return any(filename.endswith(extension) for extension in AUDIO_EXTENSIONS)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    num_to_class = dict(zip(range(len(classes)), classes))\n    return classes, class_to_idx, num_to_class\n\n\n\n\ndef make_dataset(dir, class_to_idx):\n    spects = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if is_audio_file(fname):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    spects.append(item)\n    return spects\n\n\ndef spect_loader(path, window_size, window_stride, window, normalize, max_len=101):\n    y, sr = librosa.load(path, sr=None)\n    # n_fft = 4096\n    n_fft = int(sr * window_size)\n    win_length = n_fft\n    hop_length = int(sr * window_stride)\n\n    # STFT\n    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n                     win_length=win_length, window=window)\n    spect, phase = librosa.magphase(D)\n\n    # S = log(S+1)\n    spect = np.log1p(spect)\n\n    # make all spects with the same dims\n    # TODO: change that in the future\n    if spect.shape[1] < max_len:\n        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n        spect = np.hstack((spect, pad))\n    elif spect.shape[1] > max_len:\n        spect = spect[:max_len, ]\n    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n    spect = torch.FloatTensor(spect)\n\n    # z-score normalization\n    if normalize:\n        mean = spect.mean()\n        std = spect.std()\n        if std != 0:\n            spect.add_(-mean)\n            spect.div_(std)\n\n    return spect\n\n\nclass TFAudioDataSet(data.Dataset):\n    def __init__(self, root, transform=None, target_transform=None, window_size=.02,\n                 window_stride=.01, window_type=\'hamming\', normalize=True, max_len=101):\n        classes, class_to_idx, idx_to_class = find_classes(root)\n        print (\'\xc3\x87lasses {}\'.format(classes))\n        spects = make_dataset(root, class_to_idx)\n        if len(spects) == 0:\n            raise (RuntimeError(\n                ""Found 0 sound files in subfolders of: "" + root + ""Supported audio file extensions are: "" + "","".join(\n                    AUDIO_EXTENSIONS)))\n\n        self.root = root\n        self.spects = spects\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = spect_loader\n        self.window_size = window_size\n        self.window_stride = window_stride\n        self.window_type = window_type\n        self.normalize = normalize\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (spect, target) where target is class_index of the target class.\n        """"""\n        path, target = self.spects[index]\n        spect = self.loader(path, self.window_size, self.window_stride, self.window_type, self.normalize, self.max_len)\n        if self.transform is not None:\n            spect = self.transform(spect)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return spect, target\n\n    def __len__(self):\n        return len(self.spects)\n\nbest_prec1 = 0\n\n\ndef testAudioLoader(image_name):\n    """"""load image, returns cuda tensor""""""\n#     image = Image.open(image_name)\n    image = spect_loader(image_name, args.window_size, args.window_stride, args.window_type, args.normalize, max_len=101)\n#     image = Variable(image, requires_grad=True)\n    image = image.unsqueeze(0)\n    if args.use_cuda:\n        image.cuda()\n    return image\n\ndef testModel(test_dir, local_model, sample_submission):\n    print (\'Testing model: {}\'.format(str(local_model)))\n\n    classes, class_to_idx, idx_to_class = find_classes(args.train_path)\n\n    if args.use_cuda:\n        local_model.cuda()\n    local_model.eval()\n\n    columns = [\'fname\', \'label\']\n    df_pred = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n    #     df_pred.species.astype(int)\n    for index, row in (sample_submission.iterrows()):\n        #         for file in os.listdir(test_dir):\n        currImage = os.path.join(test_dir, row[\'fname\'])\n        if os.path.isfile(currImage):\n            print (currImage)\n            X_tensor_test = testAudioLoader(currImage)\n            #             print (type(X_tensor_test))\n            if args.use_cuda:\n                X_tensor_test = Variable(X_tensor_test.cuda())\n            else:\n                X_tensor_test = Variable(X_tensor_test)\n            predicted_val = (local_model(X_tensor_test)).data.max(1)[1]  # get the index of the max log-probability\n            p_test = (predicted_val.cpu().numpy().item())\n            df_pred = df_pred.append({\'fname\': row[\'fname\'], \'label\': idx_to_class[int(p_test)]}, ignore_index=True)\n\n    print(\'Testing model done: {}\'.format(str(df_pred.shape)))\n    return df_pred\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acc = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (images, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if use_cuda:\n            images, target = images.cuda(), target.cuda()\n            images, target = Variable(images), Variable(target)\n\n        # compute y_pred\n        y_pred = model(images)\n        loss = criterion(y_pred, target)\n\n        # measure accuracy and record loss\n        prec1, prec1 = accuracy(y_pred.data, target.data, topk=(1, 1))\n        losses.update(loss.data[0], images.size(0))\n        acc.update(prec1[0], images.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'TRAIN: LOSS-->{loss.val:.4f} ({loss.avg:.4f})\\t\' \'ACC-->{acc.val:.3f}% ({acc.avg:.3f}%)\'.format(loss=losses, acc=acc))\n\n    return  float(\'{loss.avg:.4f}\'.format(loss=losses)), float(\'{acc.avg:.4f}\'.format(acc=acc))\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    acc = AverageMeter()\n\n    # switch to evaluate mode\n    # model.eval()\n\n    end = time.time()\n    for i, (images, labels) in enumerate(val_loader):\n\n        if use_cuda:\n            images, labels = images.cuda(), labels.cuda()\n        images, labels = Variable(images, volatile=True), Variable(labels)\n\n        # compute y_pred\n        y_pred = model(images)\n        loss = criterion(y_pred, labels)\n\n        # measure accuracy and record loss\n        prec1, temp_var = accuracy(y_pred.data, labels.data, topk=(1, 1))\n        losses.update(loss.data[0], images.size(0))\n        acc.update(prec1[0], images.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'VAL:   LOSS--> {loss.val:.4f} ({loss.avg:.4f})\\t\'\'ACC-->{acc.val:.3f} ({acc.avg:.3f})\'.format(loss=losses, acc=acc))\n    print(\' * Accuracy {acc.avg:.3f}\'.format(acc=acc))\n    return float(\'{loss.avg:.4f}\'.format(loss=losses)), float(\'{acc.avg:.4f}\'.format(acc=acc))\n\n\ndef save_checkpoint(state, is_best, acc):\n    filename= args.save_path_model + \'/\' + str(acc) + \'_checkpoint.pth.tar\'\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.lr * (0.1**(epoch // 30))\n    for param_group in optimizer.state_dict()[\'param_groups\']:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(y_pred, y_actual, topk=(1, )):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = y_actual.size(0)\n\n    _, pred = y_pred.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(y_actual.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n\n    return res\n\n\nclass TestImageFolder(data.Dataset):\n    def __init__(self, root, transform=None):\n        images = []\n        for filename in os.listdir(root):\n            if filename.endswith(\'jpg\'):\n                images.append(\'{}\'.format(filename))\n\n        self.root = root\n        self.imgs = images\n        self.transform = transform\n\n    def __getitem__(self, index):\n        filename = self.imgs[index]\n        img = Image.open(os.path.join(self.root, filename))\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, filename\n\n    def __len__(self):\n        return len(self.imgs)\n\n\nimport errno\nimport time\n\ndef mkdir_p(path):\n    \'\'\'make dir if not exist\'\'\'\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\nclass RecorderMeter(object):\n    """"""Computes and stores the minimum loss value and its epoch index""""""\n\n    def __init__(self, total_epoch):\n        self.reset(total_epoch)\n\n    def reset(self, total_epoch):\n        assert total_epoch > 0\n        self.total_epoch = total_epoch\n        self.current_epoch = 0\n        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n        self.epoch_losses = self.epoch_losses - 1\n\n        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n        self.epoch_accuracy = self.epoch_accuracy\n\n    def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n        assert idx >= 0 and idx < self.total_epoch, \'total_epoch : {} , but update with the {} index\'.format(\n            self.total_epoch, idx)\n        self.epoch_losses[idx, 0] = train_loss\n        self.epoch_losses[idx, 1] = val_loss\n        self.epoch_accuracy[idx, 0] = train_acc\n        self.epoch_accuracy[idx, 1] = val_acc\n        self.current_epoch = idx + 1\n        return self.max_accuracy(False) == val_acc\n\n    def max_accuracy(self, istrain):\n        if self.current_epoch <= 0: return 0\n        if istrain:\n            return self.epoch_accuracy[:self.current_epoch, 0].max()\n        else:\n            return self.epoch_accuracy[:self.current_epoch, 1].max()\n\n    def plot_curve(self, save_path, args, model):\n        title = \'PyTorch Model:\' + str((type(model).__name__)).upper() + \', DataSet:\' + str(args.dataset).upper() + \',\' \\\n                + \'Params: %.2fM\' % (\n            sum(p.numel() for p in model.parameters()) / 1000000.0) + \', Seed: %.2f\' % args.manualSeed\n        dpi = 80\n        width, height = 1200, 800\n        legend_fontsize = 10\n        scale_distance = 48.8\n        figsize = width / float(dpi), height / float(dpi)\n\n        fig = plt.figure(figsize=figsize)\n        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n        y_axis = np.zeros(self.total_epoch)\n\n        plt.xlim(0, self.total_epoch)\n        plt.ylim(0, 1.0)\n        interval_y = 0.05 / 3.0\n        interval_x = 1\n        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n        plt.yticks(np.arange(0, 1.0 + interval_y, interval_y))\n        plt.grid()\n        plt.title(title, fontsize=18)\n        plt.xlabel(\'EPOCH\', fontsize=16)\n        plt.ylabel(\'LOSS/ACC\', fontsize=16)\n\n        y_axis[:] = self.epoch_accuracy[:, 0] / 100.0\n        plt.plot(x_axis, y_axis, color=\'g\', linestyle=\'-\', label=\'tr-accuracy/100\', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_accuracy[:, 1] / 100.0\n        plt.plot(x_axis, y_axis, color=\'y\', linestyle=\'-\', label=\'val-accuracy/100\', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 0]\n        plt.plot(x_axis, y_axis, color=\'r\', linestyle=\':\', label=\'tr-loss\', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 1]\n        plt.plot(x_axis, y_axis, color=\'b\', linestyle=\':\', label=\'val-loss\', lw=4)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        if save_path is not None:\n            fig.savefig(save_path, dpi=dpi, bbox_inches=\'tight\')\n            # print(\'---- save figure {} into {}\'.format(title, save_path))\n        plt.close(fig)\n\n\ndef main():\n    global args, best_prec1\n\n    if not os.path.isdir(args.save_path):\n        os.makedirs(args.save_path)\n    # fixSeed(args)\n    models = [\'senet\']\n\n    for m in models:\n        model = selectModel(args, m)\n        # model = models.__dict__[args.arch]()\n        model_name = (type(model).__name__)\n\n        mPath = args.save_path + \'/\' + args.dataset + \'/\' + model_name + \'/\'\n        args.save_path_model = mPath\n        if not os.path.isdir(args.save_path_model):\n            mkdir_p(args.save_path_model)\n\n        print(""Ensemble with model {}:"".format(model_name))\n        print(\'Save path : {}\'.format(args.save_path_model))\n        print(state)\n        print(""Random Seed: {}"".format(args.manualSeed))\n        import sys\n        print(""python version : {}"".format(sys.version.replace(\'\\n\', \' \')))\n        print(""torch  version : {}"".format(torch.__version__))\n        print(""cudnn  version : {}"".format(torch.backends.cudnn.version()))\n        print(""=> Final model name \'{}\'"".format(model_name))\n        # print_log(""=> Full model \'{}\'"".format(model), log)\n        # model = torch.nn.DataParallel(model).cuda()\n        model.cuda()\n        cudnn.benchmark = True\n        print(\'    Total params: %.2fM\' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n        print(\'Batch size : {}\'.format(args.batch_size))\n\n        if args.use_cuda:\n            model.cuda()\n            # model = torch.nn.DataParallel(model).cuda()\n\n        cudnn.benchmark = True\n        # Data loading code\n        train_dataset = TFAudioDataSet(args.train_path, window_size=args.window_size, window_stride=args.window_stride,\n                                       window_type=args.window_type, normalize=args.normalize)\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0,\n                                                   pin_memory=False, sampler=None)\n        valid_dataset = TFAudioDataSet(args.valid_path, window_size=args.window_size, window_stride=args.window_stride,\n                                       window_type=args.window_type, normalize=args.normalize)\n        val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=None, num_workers=0,\n                                                   pin_memory=False, sampler=None)\n        test_dataset = TFAudioDataSet(args.test_path, window_size=args.window_size, window_stride=args.window_stride,\n                                      window_type=args.window_type, normalize=args.normalize)\n\n        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=None,\n                                                  num_workers=0,\n                                                  pin_memory=False, sampler=None)\n\n        # define loss function (criterion)\n        criterion = nn.CrossEntropyLoss()\n        if args.use_cuda:\n            criterion.cuda()\n\n        optimizer = optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n        recorder = RecorderMeter(args.epochs)  # epoc is updated\n        runId = datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n\n        if args.test:\n            print(""Testing the model and generating  output csv for submission"")\n            s_submission = pd.read_csv(\'tf-sample-submission.csv\')\n            s_submission.columns = [\'fname\', \'label\']\n\n            checkpoint = torch.load(\'./log/tf/ResNeXt/checkpoint.pth.tar\')\n            model.load_state_dict(checkpoint[\'state_dict\'])\n\n            df_pred= testModel (args.test_audio, model, s_submission)\n            pre = args.save_path_model + \'/\' + \'/pth/\'\n            if not os.path.isdir(pre):\n                os.makedirs(pre)\n            fName = pre + str(\'.83\')\n            # torch.save(model.state_dict(), fName + \'_cnn.pth\')\n            csv_path = str(fName + \'_submission.csv\')\n            df_pred.to_csv(csv_path, columns=(\'fname\', \'label\'), index=None)\n            print(csv_path)\n\n            return\n\n\n            # for epoch in range(args.start_epoch, args.epochs):\n        for epoch in tqdm(range(args.start_epoch, args.epochs)):\n            adjust_learning_rate(optimizer, epoch)\n            # train for one epoch\n            tqdm.write(\'\\n==>>Epoch=[{:03d}/{:03d}]], LR=[{}], Batch=[{}]\'.format(epoch, args.epochs,\n                                                                                        state[\'lr\'],\n                args.batch_size) + \' [Model={}]\'.format(\n                (type(model).__name__), ))\n\n            train_result, accuracy_tr=train(train_loader, model, criterion, optimizer, epoch)\n            # evaluate on validation set\n            val_result, accuracy_val = validate(val_loader, model, criterion)\n\n            recorder.update(epoch, train_result, accuracy_tr, val_result, accuracy_val)\n            mPath = args.save_path_model + \'/\'\n            if not os.path.isdir(mPath):\n                os.makedirs(mPath)\n            recorder.plot_curve(os.path.join(mPath, model_name + \'_\' + runId + \'.png\'), args, model)\n\n            # remember best Accuracy and save checkpoint\n            is_best = accuracy_val > best_prec1\n            best_prec1 = max(accuracy_val, best_prec1)\n\n            if float(accuracy_val) > float(70.0):\n                print(""*** EARLY STOPPING ***"")\n                save_checkpoint({\n                    \'epoch\': epoch + 1,\n                    \'state_dict\': model.state_dict(),\n                    \'best_prec1\': best_prec1,\n                }, is_best, best_prec1)\n\n                print(""Testing the model and generating  output csv for submission"")\n                s_submission = pd.read_csv(\'tf-sample-submission.csv\')\n                s_submission.columns = [\'fname\', \'label\']\n                df_pred = testModel(args.test_audio, model, s_submission)\n                pre = args.save_path_model + \'/\' + \'/pth/\'\n                if not os.path.isdir(pre):\n                    os.makedirs(pre)\n                fName = pre + str(\'.83\')\n                # torch.save(model.state_dict(), fName + \'_cnn.pth\')\n                csv_path = str(fName + \'_submission.csv\')\n                df_pred.to_csv(csv_path, columns=(\'fname\', \'label\'), index=None)\n                print(csv_path)\n\n        test_loss, test_acc = validate(test_loader, model, criterion)\n        print(\'Test: {}, {}\'.format(test_loss, test_acc))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Kaggle-PyTorch/PyTorch-Ensembler/utils.py,29,"b'\'\'\'Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\'\'\'\nfrom __future__ import print_function, absolute_import\n\nimport errno\nimport time\nimport numpy as np\nimport matplotlib\nimport torch.nn as nn\nimport torch.nn.init as init\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport os\nimport datetime\nimport pandas as pd\nimport torch.nn.parallel\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.autograd import Variable\nfrom torch.utils.data import TensorDataset\nfrom torchvision.transforms import *\n\nimport nnmodels as nnmodels\n\nfrom os import listdir\nimport sys\n\n# __all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n# __all__ = [\'get_mean_and_std\', \'init_params\', \'mkdir_p\', \'AverageMeter\', \'accuracy\']\n\ndef print_log(print_string, log):\n    print(""{}"".format(print_string))\n    log.write(\'{}\\n\'.format(print_string))\n    log.flush()\n\n\ndef savefig(fname, dpi=None):\n    dpi = 500 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n\n\ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        if name in [\'Train Acc.\', \'Valid Acc.\']:\n            plt.plot(x, 100 - np.asarray(numbers[name], dtype=\'float\'))\n        else:\n            plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n\n    def __init__(self, fpath, title=None, resume=False):\n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume:\n                self.file = open(fpath, \'r\')\n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')\n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume:\n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):\n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n\n    def __init__(self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.plot()\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        legend_text = [\'WRN-28-10+Ours (error 17.65%)\', \'WRN-28-10 (error 18.68%)\']\n        plt.legend(legend_text, loc=0)\n        plt.ylabel(\'test error (%)\')\n        plt.xlabel(\'epoch\')\n        plt.grid(True)\n\n\ndef time_string():\n    ISOTIMEFORMAT = \'%Y-%m-%d %X\'\n    string = \'[{}]\'.format(time.strftime(ISOTIMEFORMAT, time.gmtime(time.time())))\n    return string\n\n\ndef get_mean_and_std(dataset):\n    \'\'\'Compute the mean and std value of dataset.\'\'\'\n    dataloader = trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\'==> Computing mean and std..\')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef init_params(net):\n    \'\'\'Init layer parameters.\'\'\'\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode=\'fan_out\')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\ndef mkdir_p(path):\n    \'\'\'make dir if not exist\'\'\'\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass TrainningValidationSplitDataset(torch.utils.data.Dataset):\n    def __init__(self, full_ds, offset, length):\n        self.full_ds = full_ds\n        self.offset = offset\n        self.length = length\n        assert len(full_ds) >= offset + length, Exception(""Parent Dataset not long enough"")\n        super(TrainningValidationSplitDataset, self).__init__()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        return self.full_ds[i + self.offset]\n\n\ndef trainTestSplit(dataset, val_share):\n    val_offset = int(len(dataset) * (1 - val_share))\n    # print(""Offest:"" + str(val_offset))\n    return TrainningValidationSplitDataset(dataset, 0, val_offset), TrainningValidationSplitDataset(dataset, val_offset,\n                                                                                                    len(dataset) - val_offset)\n\ndef createNewDir(BASE_FOLDER):\n    parquet_dir = os.path.join(BASE_FOLDER, datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'))\n    os.makedirs(parquet_dir)\n    return parquet_dir\n\n\ndef savePred(df_pred, local_model, val_score, train_score, save_path):\n    pre = save_path + \'/\' + \'/pth/\'\n    if not os.path.isdir(pre):\n        os.makedirs(pre)\n    fName = pre + str(val_score) + \'_\' + str(train_score)\n    torch.save(local_model.state_dict(), fName + \'_cnn.pth\')\n    csv_path = str(fName + \'_submission.csv\')\n    df_pred.to_csv(csv_path, columns=(\'id\', \'is_iceberg\'), index=None)\n    print(csv_path)\n\n\ndef MinMaxBestBaseStacking(input_folder, best_base, output_path):\n    sub_base = pd.read_csv(best_base)\n    all_files = os.listdir(input_folder)\n\n    # Read and concatenate submissions\n    outs = [pd.read_csv(os.path.join(input_folder, f), index_col=0) for f in all_files]\n    concat_sub = pd.concat(outs, axis=1)\n    cols = list(map(lambda x: ""is_iceberg_"" + str(x), range(len(concat_sub.columns))))\n    concat_sub.columns = cols\n    concat_sub.reset_index(inplace=True)\n\n    # get the data fields ready for stacking\n    concat_sub[\'is_iceberg_max\'] = concat_sub.iloc[:, 1:6].max(axis=1)\n    concat_sub[\'is_iceberg_min\'] = concat_sub.iloc[:, 1:6].min(axis=1)\n    concat_sub[\'is_iceberg_mean\'] = concat_sub.iloc[:, 1:6].mean(axis=1)\n    concat_sub[\'is_iceberg_median\'] = concat_sub.iloc[:, 1:6].median(axis=1)\n\n    # set up cutoff threshold for lower and upper bounds, easy to twist\n    cutoff_lo = 0.67\n    cutoff_hi = 0.33\n\n    concat_sub[\'is_iceberg_base\'] = sub_base[\'is_iceberg\']\n    concat_sub[\'is_iceberg\'] = np.where(np.all(concat_sub.iloc[:, 1:6] > cutoff_lo, axis=1),\n                                        concat_sub[\'is_iceberg_max\'],\n                                        np.where(np.all(concat_sub.iloc[:, 1:6] < cutoff_hi, axis=1),\n                                                 concat_sub[\'is_iceberg_min\'],\n                                                 concat_sub[\'is_iceberg_base\']))\n    concat_sub[[\'id\', \'is_iceberg\']].to_csv(output_path,\n                                            index=False, float_format=\'%.12f\')\n\n\ndef ensembleVer2(input_folder, output_path):\n    print(\'Out:\' + output_path)\n    csv_files = [f for f in os.listdir(input_folder) if f.endswith(\'.csv\')]\n    model_scores = []\n    for i, csv in enumerate(csv_files):\n        df = pd.read_csv(os.path.join(input_folder, csv), index_col=0)\n        if i == 0:\n            index = df.index\n        else:\n            assert index.equals(df.index), ""Indices of one or more files do not match!""\n        model_scores.append(df)\n    print(""Read %d files. Averaging..."" % len(model_scores))\n\n    # print(model_scores)\n    concat_scores = pd.concat(model_scores)\n    print(concat_scores.head())\n    concat_scores[\'is_iceberg\'] = concat_scores[\'is_iceberg\'].astype(np.float32)\n\n    averaged_scores = concat_scores.groupby(level=0).mean()\n    assert averaged_scores.shape[0] == len(list(index)), ""Something went wrong when concatenating/averaging!""\n    averaged_scores = averaged_scores.reindex(index)\n\n    stacked_1 = pd.read_csv(\'statoil-submission-template.csv\')  # for the header\n    print(stacked_1.shape)\n    sub = pd.DataFrame()\n    sub[\'id\'] = stacked_1[\'id\']\n\n    sub[\'is_iceberg\'] = np.exp(np.mean(\n        [\n            averaged_scores[\'is_iceberg\'].apply(lambda x: np.log(x))\n        ], axis=0))\n\n    print(sub.shape)\n    sub.to_csv(output_path, index=False, float_format=\'%.9f\')\n    print(""Averaged scores saved to %s"" % output_path)\n\n\n# Convert the np arrays into the correct dimention and type\n# Note that BCEloss requires Float in X as well as in y\ndef XnumpyToTensor(x_data_np, args):\n    x_data_np = np.array(x_data_np, dtype=np.float32)\n\n    if args.use_cuda:\n        X_tensor = (torch.from_numpy(x_data_np).cuda())  # Note the conversion for pytorch\n    else:\n        X_tensor = (torch.from_numpy(x_data_np))  # Note the conversion for pytorch\n\n    return X_tensor\n\n\n# Convert the np arrays into the correct dimention and type\n# Note that BCEloss requires Float in X as well as in y\ndef YnumpyToTensor(y_data_np, args):\n    y_data_np = y_data_np.reshape((y_data_np.shape[0], 1))  # Must be reshaped for PyTorch!\n\n    if args.use_cuda:\n        #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float\n    else:\n        #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #\n        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float\n\n    return Y_tensor\n\n\nclass RecorderMeter(object):\n    """"""Computes and stores the minimum loss value and its epoch index""""""\n\n    def __init__(self, total_epoch):\n        self.reset(total_epoch)\n\n    def reset(self, total_epoch):\n        assert total_epoch > 0\n        self.total_epoch = total_epoch\n        self.current_epoch = 0\n        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n        self.epoch_losses = self.epoch_losses - 1\n\n        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n        self.epoch_accuracy = self.epoch_accuracy\n\n    def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n        assert idx >= 0 and idx < self.total_epoch, \'total_epoch : {} , but update with the {} index\'.format(\n            self.total_epoch, idx)\n        self.epoch_losses[idx, 0] = train_loss\n        self.epoch_losses[idx, 1] = val_loss\n        self.epoch_accuracy[idx, 0] = train_acc\n        self.epoch_accuracy[idx, 1] = val_acc\n        self.current_epoch = idx + 1\n        return self.max_accuracy(False) == val_acc\n\n    def max_accuracy(self, istrain):\n        if self.current_epoch <= 0: return 0\n        if istrain:\n            return self.epoch_accuracy[:self.current_epoch, 0].max()\n        else:\n            return self.epoch_accuracy[:self.current_epoch, 1].max()\n\n\n    def plot_curve(self, save_path, args, model):\n        title = \'PyTorch-Ensembler:\' + str((type(model).__name__)).upper() + \',LR:\' + str(args.lr) +  \',DataSet:\' + str(args.dataset).upper() + \',\' + \'\\n\'\\\n                + \',Params: %.2fM\' % (sum(p.numel() for p in model.parameters()) / 1000000.0) + \',Seed: %.2f\' % args.manualSeed + \\\n                "",Torch: {}"".format(torch.__version__) + "", Batch:{}"".format(args.batch_size)\n\n        dpi = 80\n        width, height = 1200, 800\n        legend_fontsize = 14\n        scale_distance = 48.8\n        figsize = width / float(dpi), height / float(dpi)\n\n        fig = plt.figure(figsize=figsize)\n        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n        y_axis = np.zeros(self.total_epoch)\n\n        plt.xlim(0, self.total_epoch)\n        plt.ylim(0, 1.0)\n        interval_y = 0.05 / 3.0\n        interval_x = 1\n        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n        plt.yticks(np.arange(0, 1.0 + interval_y, interval_y))\n        plt.grid()\n        plt.title(title, fontsize=18)\n        plt.xlabel(\'EPOCH\', fontsize=16)\n        plt.ylabel(\'LOSS/ACC\', fontsize=16)\n\n        y_axis[:] = self.epoch_accuracy[:, 0] / 100.0\n        plt.plot(x_axis, y_axis, color=\'g\', linestyle=\'-\', label=\'tr-accuracy/100\', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_accuracy[:, 1] / 100.0\n        plt.plot(x_axis, y_axis, color=\'y\', linestyle=\'-\', label=\'val-accuracy/100\', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 0]\n        plt.plot(x_axis, y_axis, color=\'r\', linestyle=\':\', label=\'tr-loss\', lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 1]\n        plt.plot(x_axis, y_axis, color=\'b\', linestyle=\':\', label=\'val-loss\', lw=4)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        if save_path is not None:\n            fig.savefig(save_path, dpi=dpi, bbox_inches=\'tight\')\n            # print(\'---- save figure {} into {}\'.format(title, save_path))\n        plt.close(fig)\n\n\ndef set_optimizer_lr(optimizer, lr):\n    # callback to set the learning rate in an optimizer, without rebuilding the whole optimizer\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return optimizer\n\n\nimport math\n\n\n# https://github.com/gngdb/pytorch-cifar-sgdr/blob/master/main.py\ndef sgdr(period, batch_idx):\n    # returns normalised anytime sgdr schedule given period and batch_idx\n    # best performing settings reported in paper are T_0 = 10, T_mult=2\n    # so always use T_mult=2\n    batch_idx = float(batch_idx)\n    restart_period = period\n    while batch_idx / restart_period > 1.:\n        batch_idx = batch_idx - restart_period\n        restart_period = restart_period * 2.\n\n    radians = math.pi * (batch_idx / restart_period)\n    return 0.5 * (1.0 + math.cos(radians))\n\n\n# def adjust_learning_rate(optimizer, epoch):\n#     global lr\n#     """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n#     lr = lr * (0.01 ** (epoch // 10))\n#     for param_group in optimizer.state_dict()[\'param_groups\']:\n#         param_group[\'lr\'] = lr\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    """"""Sets the learning rate to the initial LR decayed by 10 after 20 and 40  and 60 epochs""""""\n    # global lr\n    lr = args.lr * (0.5 ** (epoch // 33)) * (0.5 ** (epoch //  20)) * (0.5 ** (epoch //  55))\n    print (\'adjust_learning_rate: {} \'.format(lr))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef fixSeed(args):\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if args.use_cuda:\n        torch.cuda.manual_seed(args.manualSeed)\n        torch.cuda.manual_seed_all(args.manualSeed)\n\n\ndef getStatoilTrainValLoaders(args,n_folds=5,current_fold=0):\n    fixSeed(args)\n    local_data = pd.read_json(args.data_path + \'/train.json\')\n    \n    skf = StratifiedKFold(n_splits=n_folds,random_state=2018)\n    x=local_data[\'id\'].values\n    y=local_data[\'is_iceberg\'].values\n    for i,(train_ind,val_ind) in enumerate(skf.split(X=x,y=y)):\n        if i<current_fold:\n            pass\n        else:\n            tr_data = local_data.iloc[train_ind,:]\n            val_data = local_data.iloc[val_ind,:]\n            break\n    \n    # local_data = shuffle(local_data)  # otherwise same validation set each time!\n    # local_data = local_data.reindex(np.random.permutation(local_data.index))\n\n    tr_data[\'band_1\'] = tr_data[\'band_1\'].apply(lambda x: np.array(x).reshape(75, 75))\n    tr_data[\'band_2\'] = tr_data[\'band_2\'].apply(lambda x: np.array(x).reshape(75, 75))\n    tr_data[\'inc_angle\'] = pd.to_numeric(tr_data[\'inc_angle\'], errors=\'coerce\')\n    tr_data[\'inc_angle\'].fillna(0, inplace=True)\n    \n    val_data[\'band_1\'] = val_data[\'band_1\'].apply(lambda x: np.array(x).reshape(75, 75))\n    val_data[\'band_2\'] = val_data[\'band_2\'].apply(lambda x: np.array(x).reshape(75, 75))\n    val_data[\'inc_angle\'] = pd.to_numeric(val_data[\'inc_angle\'], errors=\'coerce\')\n    val_data[\'inc_angle\'].fillna(0, inplace=True)\n\n    band_1_tr = np.concatenate([im for im in tr_data[\'band_1\']]).reshape(-1, 75, 75)\n    band_2_tr = np.concatenate([im for im in tr_data[\'band_2\']]).reshape(-1, 75, 75)    \n    #band_3_tr = (band_1_tr+band_2_tr)/2\n    local_full_img_tr = np.stack([band_1_tr, band_2_tr], axis=1)#,band_3_tr], axis=1)\n\n    band_1_val = np.concatenate([im for im in val_data[\'band_1\']]).reshape(-1, 75, 75)\n    band_2_val = np.concatenate([im for im in val_data[\'band_2\']]).reshape(-1, 75, 75)\n    #band_3_val = (band_1_val+band_2_val)/2\n    local_full_img_val = np.stack([band_1_val, band_2_val], axis=1)#,band_3_val], axis=1)\n    \n    \n    train_imgs = XnumpyToTensor(local_full_img_tr, args)\n    train_targets = YnumpyToTensor(tr_data[\'is_iceberg\'].values, args)\n    dset_train = TensorDataset(train_imgs, train_targets)\n    \n    val_imgs = XnumpyToTensor(local_full_img_val, args)\n    val_targets = YnumpyToTensor(val_data[\'is_iceberg\'].values, args)\n    dset_val = TensorDataset(val_imgs, val_targets)\n\n    # local_train_ds, local_val_ds = trainTestSplit(dset_train, args.validationRatio)\n    \n    local_train_ds, local_val_ds = dset_train, dset_val\n    local_train_loader = torch.utils.data.DataLoader(local_train_ds, batch_size=args.batch_size, shuffle=False,\n                                                     num_workers=args.workers)\n    local_val_loader = torch.utils.data.DataLoader(local_val_ds, batch_size=args.batch_size, shuffle=False,\n                                                   num_workers=args.workers)\n    return local_train_loader, local_val_loader, local_train_ds, local_val_ds\n\n\ndef selectModel(args, m):\n    model = None\n    print(""==> Creating model \'{}\'"".format(m))\n    if m.startswith(\'senet\'):  # block, n_size=1, num_classes=1, num_rgb=2, base=32\n        # model = nnmodels.senetXX_generic(args.num_classes, args.imgDim, args.base_factor)\n        model = nnmodels.senet32_RG_1_classes(args.num_classes, args.imgDim)\n        args.batch_size = 64\n        args.batch_size = 64\n        args.epochs = 66\n        args.lr =  0.0007 # do not change !!! optimal for the Statoil data set\n\n    if m.startswith(\'densenet\'):\n        model = nnmodels.densnetXX_generic(args.num_classes, args.imgDim)\n        args.batch_size = 32\n        args.batch_size = 32\n        args.epochs = 30\n        args.lr = 0.05\n    if m.startswith(\'minidensenet\'):\n        model = nnmodels.minidensnetXX_generic(args.num_classes, args.imgDim)\n        args.batch_size = 32\n        args.batch_size = 32\n        args.epochs = 35\n        args.lr = 0.005 * 2\n    if m.startswith(\'vggnet\'):\n        model = nnmodels.vggnetXX_generic(args.num_classes, args.imgDim)\n        args.batch_size = 64\n        args.batch_size = 64\n        args.epochs = 88\n        args.lr = 0.0005\n    if m.startswith(\'resnext\'):\n        model = nnmodels.resnetxtXX_generic(args.num_classes, args.imgDim)\n        args.batch_size = 16\n        args.batch_size = 16\n        args.epochs = 66\n        args.lr = 0.0005\n    if m.startswith(\'lenet\'):\n        model = nnmodels.lenetXX_generic(args.num_classes, args.imgDim)\n        args.batch_size = 64\n        args.batch_size = 64\n        args.epochs = 88\n\n    if m.startswith(\'wrn\'):\n        model = nnmodels.wrnXX_generic(args.num_classes, args.imgDim)\n        args.batch_size = 16\n        args.batch_size = 16\n        args.epochs = 34\n        args.lr = 0.0005*2\n\n    if m.startswith(\'simple\'):\n        model = nnmodels.simpleXX_generic(args.num_classes, args.imgDim)\n        args.batch_size = 256\n        args.batch_size = 256\n        args.epochs = 120\n\n    # if m.startswith(\'unet\'):\n    #     model = nnmodels.unetXX_generic(args.num_classes, args.imgDim)\n    #     args.batch_size = 64\n    #     args.batch_size = 64\n    #     args.epochs = 50\n\n    # if m.startswith(\'link\'):\n    #     model = nnmodels.linknetXX_generic(args.num_classes, args.imgDim)\n    #     args.batch_size = 64\n    #     args.batch_size = 64\n    #     args.epochs = 50\n\n    return model\n\ndef BinaryInferenceOofAndTest(local_model,args,n_folds = 5,current_fold=0):\n    if args.use_cuda:\n        local_model.cuda()\n    local_model.eval()\n    df_test_set = pd.read_json(args.data_path + \'/test.json\')\n    df_test_set[\'band_1\'] = df_test_set[\'band_1\'].apply(lambda x: np.array(x).reshape(75, 75))\n    df_test_set[\'band_2\'] = df_test_set[\'band_2\'].apply(lambda x: np.array(x).reshape(75, 75))\n    df_test_set[\'inc_angle\'] = pd.to_numeric(df_test_set[\'inc_angle\'], errors=\'coerce\')\n    # df_test_set.head(3)\n    print(df_test_set.shape)\n    columns = [\'id\', \'is_iceberg\']\n    df_pred_test = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n    # df_pred.id.astype(int)\n    for index, row in df_test_set.iterrows():\n        rwo_no_id = row.drop(\'id\')\n        band_1_test = (rwo_no_id[\'band_1\']).reshape(-1, 75, 75)\n        band_2_test = (rwo_no_id[\'band_2\']).reshape(-1, 75, 75)\n        # band_3_test = (band_1_test + band_2_test) / 2\n        full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n\n        x_data_np = np.array(full_img_test, dtype=np.float32)\n        if args.use_cuda:\n            X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda())  # Note the conversion for pytorch\n        else:\n            X_tensor_test = Variable(torch.from_numpy(x_data_np))  # Note the conversion for pytorch\n\n        # X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors\n        predicted_val = (local_model(X_tensor_test).data).float()  # probabilities\n        p_test = predicted_val.cpu().numpy().item()  # otherwise we get an array, we need a single float\n\n        df_pred_test = df_pred_test.append({\'id\': row[\'id\'], \'is_iceberg\': p_test}, ignore_index=True)\n        \n    df_val_set = pd.read_json(args.data_path + \'/train.json\')\n    \n    skf = StratifiedKFold(n_splits=n_folds,random_state=2018)\n    x=df_val_set[\'id\'].values\n    y=df_val_set[\'is_iceberg\'].values\n    columns = [\'id\', \'is_iceberg\']\n    for i,(train_ind,val_ind) in enumerate(skf.split(X=x,y=y)):\n        if i<current_fold:\n            pass\n        else:\n            ids_and_labels = df_val_set.iloc[val_ind,[2,4]]\n            df_val_set = df_val_set.iloc[val_ind,:]\n            break\n            \n    df_val_set[\'band_1\'] = df_val_set[\'band_1\'].apply(lambda x: np.array(x).reshape(75, 75))\n    df_val_set[\'band_2\'] = df_val_set[\'band_2\'].apply(lambda x: np.array(x).reshape(75, 75))\n    df_val_set[\'inc_angle\'] = pd.to_numeric(df_val_set[\'inc_angle\'], errors=\'coerce\')\n    # df_test_set.head(3)\n    print(df_val_set.shape)\n    columns = [\'id\', \'is_iceberg\']\n    df_pred_val = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n    # df_pred.id.astype(int)\n    for index, row in df_val_set.iterrows():\n        rwo_no_id = row.drop(\'id\')\n        band_1_test = (rwo_no_id[\'band_1\']).reshape(-1, 75, 75)\n        band_2_test = (rwo_no_id[\'band_2\']).reshape(-1, 75, 75)\n        # band_3_test = (band_1_test + band_2_test) / 2\n        full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n\n        x_data_np = np.array(full_img_test, dtype=np.float32)\n        if args.use_cuda:\n            X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda())  # Note the conversion for pytorch\n        else:\n            X_tensor_test = Variable(torch.from_numpy(x_data_np))  # Note the conversion for pytorch\n\n        # X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors\n        predicted_val = (local_model(X_tensor_test).data).float()  # probabilities\n        p_test = predicted_val.cpu().numpy().item()  # otherwise we get an array, we need a single float\n        \n        df_pred_val = df_pred_val.append({\'id\': row[\'id\'], \'is_iceberg\': p_test}, ignore_index=True)\n\n    return df_pred_val, df_pred_test, ids_and_labels\n\n\ndef BinaryInference(local_model, args):\n    if args.use_cuda:\n        local_model.cuda()\n    local_model.eval()\n    df_test_set = pd.read_json(args.data_path + \'/test.json\')\n    df_test_set[\'band_1\'] = df_test_set[\'band_1\'].apply(lambda x: np.array(x).reshape(75, 75))\n    df_test_set[\'band_2\'] = df_test_set[\'band_2\'].apply(lambda x: np.array(x).reshape(75, 75))\n    df_test_set[\'inc_angle\'] = pd.to_numeric(df_test_set[\'inc_angle\'], errors=\'coerce\')\n    # df_test_set.head(3)\n    print(df_test_set.shape)\n    columns = [\'id\', \'is_iceberg\']\n    df_pred = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n    # df_pred.id.astype(int)\n    for index, row in df_test_set.iterrows():\n        rwo_no_id = row.drop(\'id\')\n        band_1_test = (rwo_no_id[\'band_1\']).reshape(-1, 75, 75)\n        band_2_test = (rwo_no_id[\'band_2\']).reshape(-1, 75, 75)\n        # band_3_test = (band_1_test + band_2_test) / 2\n        full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n\n        x_data_np = np.array(full_img_test, dtype=np.float32)\n        if args.use_cuda:\n            X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda())  # Note the conversion for pytorch\n        else:\n            X_tensor_test = Variable(torch.from_numpy(x_data_np))  # Note the conversion for pytorch\n\n        # X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors\n        predicted_val = (local_model(X_tensor_test).data).float()  # probabilities\n        p_test = predicted_val.cpu().numpy().item()  # otherwise we get an array, we need a single float\n\n        df_pred = df_pred.append({\'id\': row[\'id\'], \'is_iceberg\': p_test}, ignore_index=True)\n\n    return df_pred\n\n\ndef find_classes(fullDir):\n    classes = [d for d in os.listdir(fullDir) if os.path.isdir(os.path.join(fullDir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    num_to_class = dict(zip(range(len(classes)), classes))\n\n    train = []\n    for index, label in enumerate(classes):\n        path = fullDir + label + \'/\'\n        for file in listdir(path):\n            train.append([\'{}/{}\'.format(label, file), label, index])\n\n    df = pd.DataFrame(train, columns=[\'file\', \'category\', \'category_id\', ])\n\n    return classes, class_to_idx, num_to_class, df\n\ndef accuracy(y_pred, y_actual, topk=(1, )):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = y_actual.size(0)\n\n    _, pred = y_pred.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(y_actual.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n\n    return res\n\n\nimport random\nfrom math import floor\n\n# adapted from https://github.com/mratsim/Amazon-Forest-Computer-Vision/blob/master/main_pytorch.py\ndef train_valid_split(dataset, test_size=0.25, shuffle=False, random_seed=0):\n    """""" Return a list of splitted indices from a DataSet.\n    Indices can be used with DataLoader to build a train and validation set.\n\n    Arguments:\n        A Dataset\n        A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n        Shuffling True or False\n        Random seed\n    """"""\n    length = dataset.__len__()\n    indices = list(range(1, length))\n\n    if shuffle == True:\n        random.seed(random_seed)\n        random.shuffle(indices)\n\n    if type(test_size) is float:\n        split = floor(test_size * length)\n    elif type(test_size) is int:\n        split = test_size\n    else:\n        raise ValueError(\'%s should be an int or a float\' % str)\n    return indices[split:], indices[:split]\n'"
day02-PyTORCH-and-PyCUDA/PyTorch/data_util.py,0,"b'import gzip\nimport os\nfrom os import path\nimport numpy as np\n\nimport sys\nif sys.version_info.major < 3:\n    import urllib\nelse:\n    import urllib.request as request\n\n\nDATASET_DIR = \'datasets/\'\n\nMNIST_FILES = [""train-images-idx3-ubyte.gz"", ""train-labels-idx1-ubyte.gz"",\n               ""t10k-images-idx3-ubyte.gz"", ""t10k-labels-idx1-ubyte.gz""]\n\n\ndef download_file(url, local_path):\n    dir_path = path.dirname(local_path)\n    if not path.exists(dir_path):\n        print(""Creating the directory \'%s\' ..."" % dir_path)\n        os.makedirs(dir_path)\n\n    print(""Downloading from \'%s\' ..."" % url)\n    if sys.version_info.major < 3:\n        urllib.URLopener().retrieve(url, local_path)\n    else:\n        request.urlretrieve(url, local_path)\n\n\ndef download_mnist(local_path):\n    url_root = ""http://yann.lecun.com/exdb/mnist/""\n    for f_name in MNIST_FILES:\n        f_path = os.path.join(local_path, f_name)\n        if not path.exists(f_path):\n            download_file(url_root + f_name, f_path)\n\n\ndef one_hot(x, n):\n    if type(x) == list:\n        x = np.array(x)\n    x = x.flatten()\n    o_h = np.zeros((len(x), n))\n    o_h[np.arange(len(x)), x] = 1\n    return o_h\n\n\ndef load_mnist(ntrain=60000, ntest=10000, onehot=True):\n    data_dir = os.path.join(DATASET_DIR, \'mnist/\')\n    if not path.exists(data_dir):\n        download_mnist(data_dir)\n    else:\n        # check all files\n        checks = [path.exists(os.path.join(data_dir, f)) for f in MNIST_FILES]\n        if not np.all(checks):\n            download_mnist(data_dir)\n\n    with gzip.open(os.path.join(data_dir, \'train-images-idx3-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        trX = loaded[16:].reshape((60000, 28 * 28)).astype(float)\n\n    with gzip.open(os.path.join(data_dir, \'train-labels-idx1-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        trY = loaded[8:].reshape((60000))\n\n    with gzip.open(os.path.join(data_dir, \'t10k-images-idx3-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        teX = loaded[16:].reshape((10000, 28 * 28)).astype(float)\n\n    with gzip.open(os.path.join(data_dir, \'t10k-labels-idx1-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        teY = loaded[8:].reshape((10000))\n\n    trX /= 255.\n    teX /= 255.\n\n    trX = trX[:ntrain]\n    trY = trY[:ntrain]\n\n    teX = teX[:ntest]\n    teY = teY[:ntest]\n\n    if onehot:\n        trY = one_hot(trY, 10)\n        teY = one_hot(teY, 10)\n    else:\n        trY = np.asarray(trY)\n        teY = np.asarray(teY)\n\n    return trX, teX, trY, teY\n'"
day03/Advanced_Keras_Tutorial/word2vec.py,0,"b'from gensim.models import word2vec\r\nfrom os.path import join, exists, split\r\nimport os\r\nimport numpy as np\r\n\r\ndef train_word2vec(sentence_matrix, vocabulary_inv,\r\n                   num_features=300, min_word_count=1, context=10):\r\n    """"""\r\n    Trains, saves, loads Word2Vec model\r\n    Returns initial weights for embedding layer.\r\n   \r\n    inputs:\r\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\r\n    vocabulary_inv  # dict {str:int}\r\n    num_features    # Word vector dimensionality                      \r\n    min_word_count  # Minimum word count                        \r\n    context         # Context window size \r\n    """"""\r\n    model_dir = \'word2vec_models\'\r\n    model_name = ""{:d}features_{:d}minwords_{:d}context"".format(num_features, min_word_count, context)\r\n    model_name = join(model_dir, model_name)\r\n    if exists(model_name):\r\n        embedding_model = word2vec.Word2Vec.load(model_name)\r\n        print(\'Loading existing Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\r\n    else:\r\n        # Set values for various parameters\r\n        num_workers = 2       # Number of threads to run in parallel\r\n        downsampling = 1e-3   # Downsample setting for frequent words\r\n        \r\n        # Initialize and train the model\r\n        print(""Training Word2Vec model..."")\r\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\r\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\r\n                            size=num_features, min_count = min_word_count, \\\r\n                            window = context, sample = downsampling)\r\n        \r\n        # If we don\'t plan to train the model any further, calling \r\n        # init_sims will make the model much more memory-efficient.\r\n        embedding_model.init_sims(replace=True)\r\n        \r\n        # Saving the model for later use. You can load it later using Word2Vec.load()\r\n        if not exists(model_dir):\r\n            os.mkdir(model_dir)\r\n        print(\'Saving Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\r\n        embedding_model.save(model_name)\r\n    \r\n    #  add unknown words\r\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\r\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\r\n                                                        for w in vocabulary_inv])]\r\n    return embedding_weights\r\n\r\nif __name__==\'__main__\':\r\n    import data_helpers\r\n    print(""Loading data..."")\r\n    x, _, _, vocabulary_inv = data_helpers.load_data()\r\n    w = train_word2vec(x, vocabulary_inv)\r\n'"
day03/Advanced_Keras_Tutorial/word_embedding.py,0,"b'import numpy as np\r\nimport re\r\nimport itertools\r\nfrom collections import Counter\r\n""""""\r\nOriginal taken from https://github.com/dennybritz/cnn-text-classification-tf\r\n""""""\r\n\r\ndef clean_str(string):\r\n    """"""\r\n    Tokenization/string cleaning for all datasets except for SST.\r\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\r\n    """"""\r\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\r\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\r\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\r\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\r\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\r\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\r\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\r\n    string = re.sub(r"","", "" , "", string)\r\n    string = re.sub(r""!"", "" ! "", string)\r\n    string = re.sub(r""\\("", "" \\( "", string)\r\n    string = re.sub(r""\\)"", "" \\) "", string)\r\n    string = re.sub(r""\\?"", "" \\? "", string)\r\n    string = re.sub(r""\\s{2,}"", "" "", string)\r\n    return string.strip().lower()\r\n\r\n\r\ndef load_data_and_labels():\r\n    """"""\r\n    Loads MR polarity data from files, splits the data into words and generates labels.\r\n    Returns split sentences and labels.\r\n    """"""\r\n    # Load data from files\r\n    positive_examples = list(open(""../data/word_embeddings/rt-polarity.pos"", encoding=\'ISO-8859-1\').readlines())\r\n    positive_examples = [s.strip() for s in positive_examples]\r\n    negative_examples = list(open(""../data/word_embeddings/rt-polarity.neg"", encoding=\'ISO-8859-1\').readlines())\r\n    negative_examples = [s.strip() for s in negative_examples]\r\n    # Split by words\r\n    x_text = positive_examples + negative_examples\r\n    x_text = [clean_str(sent) for sent in x_text]\r\n    x_text = [s.split("" "") for s in x_text]\r\n    # Generate labels\r\n    positive_labels = [[0, 1] for _ in positive_examples]\r\n    negative_labels = [[1, 0] for _ in negative_examples]\r\n    y = np.concatenate([positive_labels, negative_labels], 0)\r\n    return [x_text, y]\r\n\r\n\r\ndef pad_sentences(sentences, padding_word=""<PAD/>""):\r\n    """"""\r\n    Pads all sentences to the same length. The length is defined by the longest sentence.\r\n    Returns padded sentences.\r\n    """"""\r\n    sequence_length = max(len(x) for x in sentences)\r\n    padded_sentences = []\r\n    for i in range(len(sentences)):\r\n        sentence = sentences[i]\r\n        num_padding = sequence_length - len(sentence)\r\n        new_sentence = sentence + [padding_word] * num_padding\r\n        padded_sentences.append(new_sentence)\r\n    return padded_sentences\r\n\r\n\r\ndef build_vocab(sentences):\r\n    """"""\r\n    Builds a vocabulary mapping from word to index based on the sentences.\r\n    Returns vocabulary mapping and inverse vocabulary mapping.\r\n    """"""\r\n    # Build vocabulary\r\n    word_counts = Counter(itertools.chain(*sentences))\r\n    # Mapping from index to word\r\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\r\n    # Mapping from word to index\r\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\r\n    return [vocabulary, vocabulary_inv]\r\n\r\n\r\ndef build_input_data(sentences, labels, vocabulary):\r\n    """"""\r\n    Maps sentencs and labels to vectors based on a vocabulary.\r\n    """"""\r\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\r\n    y = np.array(labels)\r\n    return [x, y]\r\n\r\n\r\ndef load_data():\r\n    """"""\r\n    Loads and preprocessed data for the MR dataset.\r\n    Returns input vectors, labels, vocabulary, and inverse vocabulary.\r\n    """"""\r\n    # Load and preprocess data\r\n    sentences, labels = load_data_and_labels()\r\n    sentences_padded = pad_sentences(sentences)\r\n    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\r\n    x, y = build_input_data(sentences_padded, labels, vocabulary)\r\n    return [x, y, vocabulary, vocabulary_inv]\r\n\r\n\r\ndef batch_iter(data, batch_size, num_epochs):\r\n    """"""\r\n    Generates a batch iterator for a dataset.\r\n    """"""\r\n    data = np.array(data)\r\n    data_size = len(data)\r\n    num_batches_per_epoch = int(len(data)/batch_size) + 1\r\n    for epoch in range(num_epochs):\r\n        # Shuffle the data at each epoch\r\n        shuffle_indices = np.random.permutation(np.arange(data_size))\r\n        shuffled_data = data[shuffle_indices]\r\n        for batch_num in range(num_batches_per_epoch):\r\n            start_index = batch_num * batch_size\r\n            end_index = min((batch_num + 1) * batch_size, data_size)\r\n            yield shuffled_data[start_index:end_index]'"
day03/solutions/sol_111.py,0,"b'ann = ANN(2, 10, 1)\n%timeit -n 1 -r 1 ann.train(zip(X,y), iterations=2)\nplot_decision_boundary(ann)\nplt.title(""Our next model with 10 hidden units"")\n'"
day03/solutions/sol_112.py,0,"b'ann = ANN(2, 10, 1)\n%timeit -n 1 -r 1 ann.train(zip(X,y), iterations=100)\nplot_decision_boundary(ann)\nplt.title(""Our model with 10 hidden units and 100 iterations"")\n'"
day03/solutions/sol_221_1.py,0,"b""from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=SGD(), \n              metrics=['accuracy'])"""
day03/solutions/sol_221_2.py,0,"b""from keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=SGD(), \n              metrics=['accuracy'])\n    \nmodel.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs=100, \n          batch_size=128, verbose=True, callbacks=[early_stop]) """
day03/solutions/sol_223.py,0,"b""from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten, Activation\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adadelta\n\ninput_shape = (3, 32, 32)\nnb_classes = 10\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=Adadelta(),\n              metrics=['accuracy'])"""
day03/solutions/sol_52.py,0,"b""from keras.datasets import mnist\nfrom keras.utils import np_utils\n\nimg_rows, img_cols = 28, 28\nnb_classes = 10\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n    \nX_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\nX_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train /= 255\nX_test /= 255\n\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_test shape:', y_test.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\ny_train = np_utils.to_categorical(y_train, nb_classes)\ny_test = np_utils.to_categorical(y_test, nb_classes)"""
day04/imagenet_based_prediction/imagenet_utils.py,0,"b""import numpy as np\r\nimport json\r\n\r\nfrom keras.utils.data_utils import get_file\r\nfrom keras import backend as K\r\n\r\nCLASS_INDEX = None\r\nCLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\r\n\r\n\r\ndef preprocess_input(x, dim_ordering='default'):\r\n    if dim_ordering == 'default':\r\n        dim_ordering = K.image_dim_ordering()\r\n    assert dim_ordering in {'tf', 'th'}\r\n\r\n    if dim_ordering == 'th':\r\n        x[:, 0, :, :] -= 103.939\r\n        x[:, 1, :, :] -= 116.779\r\n        x[:, 2, :, :] -= 123.68\r\n        # 'RGB'->'BGR'\r\n        x = x[:, ::-1, :, :]\r\n    else:\r\n        x[:, :, :, 0] -= 103.939\r\n        x[:, :, :, 1] -= 116.779\r\n        x[:, :, :, 2] -= 123.68\r\n        # 'RGB'->'BGR'\r\n        x = x[:, :, :, ::-1]\r\n    return x\r\n\r\n\r\ndef decode_predictions(preds, top=5):\r\n    global CLASS_INDEX\r\n    if len(preds.shape) != 2 or preds.shape[1] != 1000:\r\n        raise ValueError('`decode_predictions` expects '\r\n                         'a batch of predictions '\r\n                         '(i.e. a 2D array of shape (samples, 1000)). '\r\n                         'Found array with shape: ' + str(preds.shape))\r\n    if CLASS_INDEX is None:\r\n        fpath = get_file('imagenet_class_index.json',\r\n                         CLASS_INDEX_PATH,\r\n                         cache_subdir='models')\r\n        CLASS_INDEX = json.load(open(fpath))\r\n    results = []\r\n    for pred in preds:\r\n        top_indices = pred.argsort()[-top:][::-1]\r\n        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\r\n        results.append(result)\r\n    return results\r\n"""
day05/KerasRLTutorial/qlearn.py,0,"b'#!/usr/bin/env python\nfrom __future__ import print_function\n\nimport argparse\nimport skimage as skimage\nfrom skimage import transform, color, exposure\nfrom skimage.transform import rotate\nfrom skimage.viewer import ImageViewer\nimport sys\nsys.path.append(""game/"")\nimport wrapped_flappy_bird as game\nimport random\nimport numpy as np\nfrom collections import deque\n\nimport json\nfrom keras import initializations\nfrom keras.initializations import normal, identity\nfrom keras.models import model_from_json\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD , Adam\nimport tensorflow as tf\n\n#YOUR CONFIGURATION\nMODE = \'Run\' \n#MODE = \'Train\'\n#YOUR CONFIGURATION\n\n\n\nGAME = \'bird\' # the name of the game being played for log files\nCONFIG = \'nothreshold\'\nACTIONS = 2 # number of valid actions\nGAMMA = 0.99 # decay rate of past observations\nOBSERVATION = 3200. # timesteps to observe before training\nEXPLORE = 3000000. # frames over which to anneal epsilon\nFINAL_EPSILON = 0.0001 # final value of epsilon\nINITIAL_EPSILON = 0.1 # starting value of epsilon\nREPLAY_MEMORY = 50000 # number of previous transitions to remember\nBATCH = 32 # size of minibatch\nFRAME_PER_ACTION = 1\nLEARNING_RATE = 1e-4\n\nimg_rows , img_cols = 80, 80\n#Convert image into Black and white\nimg_channels = 4 #We stack 4 frames\n\ndef buildmodel():\n    # FILL ME\n    # MODEL BUILDING\n    # HINT: input_shape=(img_rows,img_cols,img_channels)\n\n    # model = YOUR CODE...\n\n    #END OF MODEL BUILDING\n\n    return model\n\ndef trainNetwork(model,args):\n    #INITS - NOT INTERESTING\n    # open up a game state to communicate with emulator\n    game_state = game.GameState()\n    # store the previous observations in replay memory\n    D = deque()\n    # get the first state by doing nothing and preprocess the image to 80x80x4\n    do_nothing = np.zeros(ACTIONS)\n    do_nothing[0] = 1\n    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n    x_t = skimage.color.rgb2gray(x_t)\n    x_t = skimage.transform.resize(x_t,(80,80))\n    x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n    #In Keras, need to reshape\n    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n    if args[\'mode\'] == \'Run\':\n        OBSERVE = 999999999    #We keep observe, never train\n        epsilon = FINAL_EPSILON\n        print (""Now we load weight"")\n        model.load_weights(""model.h5"")\n        adam = Adam(lr=LEARNING_RATE)\n        model.compile(loss=\'mse\',optimizer=adam)\n        print (""Weight load successfully"")    \n    else:                       #We go to training mode\n        OBSERVE = OBSERVATION\n        epsilon = INITIAL_EPSILON\n    #END OF INITS - NOT INTERESTING\n\n    t = 0\n    while (True):\n        loss = 0\n        Q_sa = 0\n        action_index = 0\n        r_t = 0\n        a_t = np.zeros([ACTIONS])\n        #choose an action epsilon greedy\n        if t % FRAME_PER_ACTION == 0:\n            if random.random() <= epsilon:\n                print(""----------Random Action----------"")\n                action_index = random.randrange(ACTIONS)\n                a_t[action_index] = 1\n            else:\n                #FILL ME\n                #s_t is the state\n                #q is the prediction of your model\n                \n                #q = YOUR CODE...\n\n                #FILL ME\n                max_Q = np.argmax(q)\n                action_index = max_Q\n                a_t[max_Q] = 1\n\n        #We reduced the epsilon gradually\n        if epsilon > FINAL_EPSILON and t > OBSERVE:\n            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n        #run the selected action and observed next state and reward\n        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n        x_t1 = skimage.transform.resize(x_t1,(80,80))\n        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))\n        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n        # store the transition in D\n        D.append((s_t, action_index, r_t, s_t1, terminal))\n        if len(D) > REPLAY_MEMORY:\n            D.popleft()\n        #only train if done observing\n        if t > OBSERVE:\n            #sample a minibatch to train on\n            minibatch = random.sample(D, BATCH)\n            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 80, 80, 4\n            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n            \n            \n            #Now we do the experience replay\n            for i in range(0, len(minibatch)):\n                state_t = minibatch[i][0]\n                action_t = minibatch[i][1]   #This is action index\n                reward_t = minibatch[i][2]\n                state_t1 = minibatch[i][3]\n                terminal = minibatch[i][4]\n                # if terminated, only equals reward\n\n                inputs[i:i + 1] = state_t    #I saved down s_t\n               \n\n\n                #VERY IMPORTANT! UNDERSTAND THIS\n                targets[i] = model.predict(state_t) \n                Q_sa = model.predict(state_t1)\n                if terminal:\n                    targets[i, action_t] = reward_t\n                else:\n                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n                #END OF VERY IMPORTANT! UNDERSTAND THIS\n\n\n\n\n            loss += model.train_on_batch(inputs, targets)\n        s_t = s_t1\n        t = t + 1\n        # save progress every 10000 iterations\n        if t % 1000 == 0:\n            print(""Now we save model"")\n            model.save_weights(""model.h5"", overwrite=True)\n            with open(""model.json"", ""w"") as outfile:\n                json.dump(model.to_json(), outfile)\n        state = """"\n        if t <= OBSERVE:\n            state = ""observe""\n        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n            state = ""explore""\n        else:\n            state = ""train""\n        print(""TIMESTEP"", t, ""/ STATE"", state, \\\n            ""/ EPSILON"", epsilon, ""/ ACTION"", action_index, ""/ REWARD"", r_t, \\\n            ""/ Q_MAX "" , np.max(Q_sa), ""/ Loss "", loss)\n    print(""Episode finished!"")\n    print(""************************"")\n\ndef playGame(args):\n    model = buildmodel()\n    trainNetwork(model,args)\n\ndef main():\n    playGame({\'mode\':MODE})\n\nif __name__ == ""__main__"":\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    from keras import backend as K\n    K.set_session(sess)\n    main()\n'"
Kaggle-PyTorch/PyTorch-Ensembler/kdataset/__init__.py,0,b'from .icedataset import *\nfrom .seedingsdataset import *\nfrom .eyedataset import *\n'
Kaggle-PyTorch/PyTorch-Ensembler/kdataset/icedataset.py,5,"b'import random\n\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom torch.utils.data.dataset import Dataset\nfrom os.path import join\n\nimport torch.utils.data as data\nfrom PIL import Image\nfrom torchvision import transforms\nimport os\n# __all__ = (\'SeedlingDataset\')\n\nIMG_EXTENSIONS = [\n    \'.jpg\',\n    \'png\'\n]\n\ndef default_loader(input_path):\n    input_image = (Image.open(input_path)).convert(\'RGB\')\n    return input_image\n\nclass IcebergCustomDataSet(Dataset):\n    """"""total datasets.""""""\n\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        sample = {\'image\': self.data[idx, :, :, :], \'labels\': np.asarray([self.labels[idx]])}\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        image, labels = sample[\'image\'], sample[\'labels\']\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        # image = image.transpose((2, 0, 1))\n        image = image.astype(float) / 255\n        return {\'image\': torch.from_numpy(image.copy()).float(),\n                \'labels\': torch.from_numpy(labels).float()\n                }\n\n\nclass RandomHorizontalFlip(object):\n    """"""Horizontally flip the given PIL.Image randomly with a probability of 0.5.""""""\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            img (PIL.Image): Image to be flipped.\n\n        Returns:\n            PIL.Image: Randomly flipped image.\n        """"""\n        image, labels = sample[\'image\'], sample[\'labels\']\n\n        if random.random() < 0.5:\n            image = np.flip(image, 1)\n\n        return {\'image\': image, \'labels\': labels}\n\n\nclass RandomVerticallFlip(object):\n    """"""Horizontally flip the given PIL.Image randomly with a probability of 0.5.""""""\n\n    def __call__(self, sample):\n        image, labels = sample[\'image\'], sample[\'labels\']\n        if random.random() < 0.3:\n            image = np.flip(image, 0)\n        return {\'image\': image, \'labels\': labels}\n\n\nclass RandomTranspose(object):\n    def __call__(self, sample):\n        image, labels = sample[\'image\'], sample[\'labels\']\n        if random.random() < 0.7:\n            image = np.transpose(image, 0)\n        return {\'image\': image, \'labels\': labels}\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        # TODO: make efficient\n        img = tensor[\'image\'].float()\n        for t, m, s in zip(img, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return {\'image\': img, \'labels\': tensor[\'labels\']}\n'"
Kaggle-PyTorch/PyTorch-Ensembler/kdataset/seedingsdataset.py,1,"b""from os.path import join\n\nimport torch.utils.data as data\nfrom PIL import Image\nfrom torchvision import transforms\nimport os\n# __all__ = ('SeedlingDataset')\n\nIMG_EXTENSIONS = [\n    '.jpg',\n    'png'\n]\n\nto_tensor = transforms.Compose([transforms.ToTensor()])\n\n\ndef default_loader_scale(input_path, size=150):\n    input_image = (Image.open(input_path)).convert('RGB')\n    if size is not None:\n        input_image = input_image.resize((size, size), Image.ANTIALIAS)\n    return input_image\n\n\ndef default_loader(input_path):\n    input_image = (Image.open(input_path)).convert('RGB')\n    return input_image\n\n\nclass SeedDataset(data.Dataset):\n    def __init__(self, labels, root_dir, transform=None):\n        self.labels = labels\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img_name = self.labels.iloc[idx, 0]  # file name\n        fullname = join(self.root_dir, img_name)\n        # image = Image.open(fullname).convert('RGB')\n        image = default_loader(fullname)\n        labels = self.labels.iloc[idx, 2]  # category_id\n        #         print (labels)\n        if self.transform:\n            image = self.transform(image)\n        return image, labels\n\n\nclass SeedTestImageFolder(data.Dataset):\n    def __init__(self, root, transform=None):\n        images = []\n        for filename in os.listdir(root):\n            if filename.endswith('jpg'):\n                images.append('{}'.format(filename))\n\n        self.root = root\n        self.imgs = images\n        self.transform = transform\n\n    def __getitem__(self, index):\n        filename = self.imgs[index]\n        img = Image.open(os.path.join(self.root, filename))\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, filename\n\n    def __len__(self):\n        return len(self.imgs)"""
Kaggle-PyTorch/PyTorch-Ensembler/ktransforms/__init__.py,0,"b'""""""Useful utils\n""""""\nfrom .transforms import *\n\n'"
Kaggle-PyTorch/PyTorch-Ensembler/ktransforms/transforms.py,8,"b'from __future__ import absolute_import\n\nfrom torchvision.transforms import *\n\nfrom PIL import Image, ImageDraw\nimport numpy as np\nimport torch\n\nimport torchvision\nimport random\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport numbers\nimport math\nimport torch\nimport torch\nimport random\nimport PIL.ImageEnhance as ie\nimport PIL.Image as im\n\n# adapted from https://github.com/kuangliu/pytorch-retinanet/blob/master/transform.py\n# https://github.com/mratsim/Amazon-Forest-Computer-Vision/blob/master/src/p_data_augmentation.py\n\nnormalize_img = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\ndef draw(img, boxes):\n    draw = ImageDraw.Draw(img)\n    for box in boxes:\n        draw.rectangle(list(box), outline=\'red\')\n    img.show()\n\ndef draw2(img):\n    img.show()\n\n\nclass GroupRandomCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img_group):\n\n        w, h = img_group[0].size\n        th, tw = self.size\n\n        out_images = list()\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n\n        for img in img_group:\n            assert(img.size[0] == w and img.size[1] == h)\n            if w == tw and h == th:\n                out_images.append(img)\n            else:\n                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n\n        return out_images\n\n\nclass GroupCenterCrop(object):\n    def __init__(self, size):\n        self.worker = torchvision.transforms.CenterCrop(size)\n\n    def __call__(self, img_group):\n        return [self.worker(img) for img in img_group]\n\n\nclass GroupRandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    """"""\n    def __init__(self, is_flow=False):\n        self.is_flow = is_flow\n\n    def __call__(self, img_group, is_flow=False):\n        v = random.random()\n        if v < 0.5:\n            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n            if self.is_flow:\n                for i in range(0, len(ret), 2):\n                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n            return ret\n        else:\n            return img_group\n\n\nclass GroupNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n        rep_std = self.std * (tensor.size()[0]//len(self.std))\n\n        # TODO: make efficient\n        for t, m, s in zip(tensor, rep_mean, rep_std):\n            t.sub_(m).div_(s)\n\n        return tensor\n\n\nclass GroupScale(object):\n    """""" Rescales the input PIL.Image to the given \'size\'.\n    \'size\' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.worker = torchvision.transforms.Scale(size, interpolation)\n\n    def __call__(self, img_group):\n        return [self.worker(img) for img in img_group]\n\n\nclass GroupOverSample(object):\n    def __init__(self, crop_size, scale_size=None):\n        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n\n        if scale_size is not None:\n            self.scale_worker = GroupScale(scale_size)\n        else:\n            self.scale_worker = None\n\n    def __call__(self, img_group):\n\n        if self.scale_worker is not None:\n            img_group = self.scale_worker(img_group)\n\n        image_w, image_h = img_group[0].size\n        crop_w, crop_h = self.crop_size\n\n        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n                normal_group.append(crop)\n                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n\n                if img.mode == \'L\' and i % 2 == 0:\n                    flip_group.append(ImageOps.invert(flip_crop))\n                else:\n                    flip_group.append(flip_crop)\n\n            oversample_group.extend(normal_group)\n            oversample_group.extend(flip_group)\n        return oversample_group\n\n\nclass GroupMultiScaleCrop(object):\n\n    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n        self.scales = scales if scales is not None else [1, 875, .75, .66]\n        self.max_distort = max_distort\n        self.fix_crop = fix_crop\n        self.more_fix_crop = more_fix_crop\n        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n        self.interpolation = Image.BILINEAR\n\n    def __call__(self, img_group):\n\n        im_size = img_group[0].size\n\n        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n                         for img in crop_img_group]\n        return ret_img_group\n\n    def _sample_crop_size(self, im_size):\n        image_w, image_h = im_size[0], im_size[1]\n\n        # find a crop size\n        base_size = min(image_w, image_h)\n        crop_sizes = [int(base_size * x) for x in self.scales]\n        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n\n        pairs = []\n        for i, h in enumerate(crop_h):\n            for j, w in enumerate(crop_w):\n                if abs(i - j) <= self.max_distort:\n                    pairs.append((w, h))\n\n        crop_pair = random.choice(pairs)\n        if not self.fix_crop:\n            w_offset = random.randint(0, image_w - crop_pair[0])\n            h_offset = random.randint(0, image_h - crop_pair[1])\n        else:\n            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n\n        return crop_pair[0], crop_pair[1], w_offset, h_offset\n\n    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n        return random.choice(offsets)\n\n    @staticmethod\n    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n        w_step = (image_w - crop_w) // 4\n        h_step = (image_h - crop_h) // 4\n\n        ret = list()\n        ret.append((0, 0))  # upper left\n        ret.append((4 * w_step, 0))  # upper right\n        ret.append((0, 4 * h_step))  # lower left\n        ret.append((4 * w_step, 4 * h_step))  # lower right\n        ret.append((2 * w_step, 2 * h_step))  # center\n\n        if more_fix_crop:\n            ret.append((0, 2 * h_step))  # center left\n            ret.append((4 * w_step, 2 * h_step))  # center right\n            ret.append((2 * w_step, 4 * h_step))  # lower center\n            ret.append((2 * w_step, 0 * h_step))  # upper center\n\n            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n\n        return ret\n\n\nclass GroupRandomSizedCrop(object):\n    """"""Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n    This is popularly used to train the Inception networks\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_group):\n        for attempt in range(10):\n            area = img_group[0].size[0] * img_group[0].size[1]\n            target_area = random.uniform(0.08, 1.0) * area\n            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n                x1 = random.randint(0, img_group[0].size[0] - w)\n                y1 = random.randint(0, img_group[0].size[1] - h)\n                found = True\n                break\n        else:\n            found = False\n            x1 = 0\n            y1 = 0\n\n        if found:\n            out_group = list()\n            for img in img_group:\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                assert(img.size == (w, h))\n                out_group.append(img.resize((self.size, self.size), self.interpolation))\n            return out_group\n        else:\n            # Fallback\n            scale = GroupScale(self.size, interpolation=self.interpolation)\n            crop = GroupRandomCrop(self.size)\n            return crop(scale(img_group))\n\n\nclass Stack(object):\n\n    def __init__(self, roll=False):\n        self.roll = roll\n\n    def __call__(self, img_group):\n        if img_group[0].mode == \'L\':\n            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n        elif img_group[0].mode == \'RGB\':\n            if self.roll:\n                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n            else:\n                return np.concatenate(img_group, axis=2)\n\n\nclass ToTorchFormatTensor(object):\n    """""" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """"""\n    def __init__(self, div=True):\n        self.div = div\n\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n        else:\n            # handle PIL Image\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n            # put it from HWC to CHW format\n            # yikes, this transpose takes 80% of the loading time/CPU\n            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        return img.float().div(255) if self.div else img.float()\n\n\nclass IdentityTransform(object):\n\n    def __call__(self, data):\n        return data\n\nclass RandomErasing(object):\n    def __init__(self, EPSILON = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.EPSILON = EPSILON\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.EPSILON:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size()[2] and h <= img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    #img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    #img[1, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    #img[2, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                    #img[:, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(3, h, w))\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[1]\n                    # img[0, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(1, h, w))\n                return img\n\n        return img\n\ndef random_crop(img, boxes):\n    \'\'\'Crop the given PIL image to a random size and aspect ratio.\n    A crop of random size of (0.08 to 1.0) of the original size and a random\n    aspect ratio of 3/4 to 4/3 of the original aspect ratio is made.\n    Args:\n      img: (PIL.Image) image to be cropped.\n      boxes: (tensor) object boxes, sized [#ojb,4].\n    Returns:\n      img: (PIL.Image) randomly cropped image.\n      boxes: (tensor) randomly cropped boxes.\n    \'\'\'\n    success = False\n    for attempt in range(10):\n        area = img.size[0] * img.size[1]\n        target_area = random.uniform(0.56, 1.0) * area\n        aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n        w = int(round(math.sqrt(target_area * aspect_ratio)))\n        h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n        if random.random() < 0.5:\n            w, h = h, w\n\n        if w <= img.size[0] and h <= img.size[1]:\n            x = random.randint(0, img.size[0] - w)\n            y = random.randint(0, img.size[1] - h)\n            success = True\n            break\n\n    # Fallback\n    if not success:\n        w = h = min(img.size[0], img.size[1])\n        x = (img.size[0] - w) // 2\n        y = (img.size[1] - h) // 2\n\n    img = img.crop((x, y, x+w, y+h))\n    boxes -= torch.Tensor([x,y,x,y])\n    boxes[:,0::2].clamp_(min=0, max=w-1)\n    boxes[:,1::2].clamp_(min=0, max=h-1)\n    return img, boxes\n\n\nclass Lighting(object):\n    """"""Lighting noise(AlexNet - style PCA - based noise)""""""\n\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = eigval\n        self.eigvec = eigvec\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone() \\\n            .mul(alpha.view(1, 3).expand(3, 3)) \\\n            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n            .sum(1).squeeze()\n\n        return img.add(rgb.view(3, 1, 1).expand_as(img))\n\n\nclass Grayscale(object):\n    def __call__(self, img):\n        gs = img.clone()\n        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n        gs[1].copy_(gs[0])\n        gs[2].copy_(gs[0])\n        return gs\n\n\nclass Saturation(object):\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Brightness(object):\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = img.new().resize_as_(img).zero_()\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Contrast(object):\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        gs.fill_(gs.mean())\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass RandomOrder(object):\n    """""" Composes several transforms together in random order.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        if self.transforms is None:\n            return img\n        order = torch.randperm(len(self.transforms))\n        for i in order:\n            img = self.transforms[i](img)\n        return img\n\n\nclass ColorJitter(RandomOrder):\n    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n        self.transforms = []\n        if brightness != 0:\n            self.transforms.append(Brightness(brightness))\n        if contrast != 0:\n            self.transforms.append(Contrast(contrast))\n        if saturation != 0:\n            self.transforms.append(Saturation(saturation))\n\n\nclass RandomFlip(object):\n    """"""Randomly flips the given PIL.Image with a probability of 0.25 horizontal,\n                                                                0.25 vertical,\n                                                                0.5 as is\n    """"""\n\n    def __call__(self, img):\n        dispatcher = {\n            0: img,\n            1: img,\n            2: img.transpose(im.FLIP_LEFT_RIGHT),\n            3: img.transpose(im.FLIP_TOP_BOTTOM)\n        }\n\n        return dispatcher[random.randint(0, 3)]  # randint is inclusive\n\n\nclass RandomRotate(object):\n    """"""Randomly rotate the given PIL.Image with a probability of 1/6 90\xc2\xb0,\n                                                                 1/6 180\xc2\xb0,\n                                                                 1/6 270\xc2\xb0,\n                                                                 1/2 as is\n    """"""\n\n    def __call__(self, img):\n        dispatcher = {\n            0: img,\n            1: img,\n            2: img,\n            3: img.transpose(im.ROTATE_90),\n            4: img.transpose(im.ROTATE_180),\n            5: img.transpose(im.ROTATE_270)\n        }\n\n        return dispatcher[random.randint(0, 5)]  # randint is inclusive\n\n\nclass PILColorBalance(object):\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        alpha = random.uniform(1 - self.var, 1 + self.var)\n        return ie.Color(img).enhance(alpha)\n\n\nclass PILContrast(object):\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        alpha = random.uniform(1 - self.var, 1 + self.var)\n        return ie.Contrast(img).enhance(alpha)\n\n\nclass PILBrightness(object):\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        alpha = random.uniform(1 - self.var, 1 + self.var)\n        return ie.Brightness(img).enhance(alpha)\n\n\nclass PILSharpness(object):\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        alpha = random.uniform(1 - self.var, 1 + self.var)\n        return ie.Sharpness(img).enhance(alpha)\n\n\n# Check ImageEnhancer effect: https://www.youtube.com/watch?v=_7iDTpTop04\n# Not documented but all enhancements can go beyond 1.0 to 2\n# Image must be RGB\n# Use Pillow-SIMD because Pillow is too slow\nclass PowerPIL(RandomOrder):\n    def __init__(self, rotate=True,\n                 flip=True,\n                 colorbalance=0.4,\n                 contrast=0.4,\n                 brightness=0.4,\n                 sharpness=0.4):\n        self.transforms = []\n        if rotate:\n            self.transforms.append(RandomRotate())\n        if flip:\n            self.transforms.append(RandomFlip())\n        if brightness != 0:\n            self.transforms.append(PILBrightness(brightness))\n        if contrast != 0:\n            self.transforms.append(PILContrast(contrast))\n        if colorbalance != 0:\n            self.transforms.append(PILColorBalance(colorbalance))\n        if sharpness != 0:\n            self.transforms.append(PILSharpness(sharpness))\n\ndef default_loader(input_path):\n    input_image = (Image.open(input_path)).convert(\'RGB\')\n    return input_image\n\n\nif __name__ == ""__main__"":\n    trans = torchvision.transforms.Compose([\n        GroupScale(256),\n        GroupRandomCrop(224),\n        Stack(),\n        ToTorchFormatTensor(),\n        GroupNormalize(\n            mean=[.485, .456, .406],\n            std=[.229, .224, .225]\n        )]\n    )\n\n    im = default_loader(\'lena_299.png\')\n\n    color_group = [im] * 3\n    rst = trans(color_group)\n\n    gray_group = [im.convert(\'L\')] * 9\n    gray_rst = trans(gray_group)\n\n    trans2 = torchvision.transforms.Compose([\n        GroupRandomSizedCrop(256),\n        Stack(),\n        ToTorchFormatTensor(),\n        GroupNormalize(\n            mean=[.485, .456, .406],\n            std=[.229, .224, .225])\n    ])\n    print(trans2(color_group))\n\n    img = default_loader(\'lena_299.png\')\n\n    boxes = torch.Tensor([[48, 240, 195, 371], [8, 12, 352, 498]])\n    img, boxes = random_crop(img, boxes)\n\n    draw(img, boxes)\n\n    print(img.size)'"
Kaggle-PyTorch/PyTorch-Ensembler/losses/BCELoss.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nimport torch.nn as nn\n\n\nclass BCELoss(nn.Module):\n    def __init__(self, sign=1):\n        super(BCELoss, self).__init__()\n        self.sign = sign\n        self.main = nn.BCELoss()\n\n    def forward(self, input, target):\n        output = self.main(input, target)\n        output = torch.mul(output, self.sign)\n        return output\n\n    def cuda(self, device_id=None):\n        super(BCELoss, self).cuda(device_id)\n        self.main.cuda()\n'"
Kaggle-PyTorch/PyTorch-Ensembler/losses/__init__.py,0,b'from .eve import *'
Kaggle-PyTorch/PyTorch-Ensembler/losses/eve.py,2,"b'import math\nfrom torch.optim import Optimizer\n\n\nclass Eve(Optimizer):\n    """"""\n    implements Eve Algorithm, proposed in `IMPROVING STOCHASTIC GRADIENT DESCENT WITH FEEDBACK`\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999, 0.999), eps=1e-8,\n                 k=0.1, K=10, weight_decay=0):\n\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        k=k, K=K, weight_decay=weight_decay)\n        super(Eve, self).__init__(params, defaults)\n\n    def step(self, closure):\n        """"""\n        :param closure: closure returns loss. see http://pytorch.org/docs/optim.html#optimizer-step-closure\n        :return: loss\n        """"""\n        loss = closure()\n        _loss = loss.data[0]  # float\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'m_t\'] = grad.new().resize_as_(grad).zero_()\n                    # Exponential moving average of squared gradient values\n                    state[\'v_t\'] = grad.new().resize_as_(grad).zero_()\n                    # f hats, smoothly tracked objective functions\n                    # \\hat{f}_0 = f_0\n                    state[\'ft_2\'], state[\'ft_1\'] = _loss, None\n                    state[\'d\'] = 1\n\n                m_t, v_t = state[\'m_t\'], state[\'v_t\']\n                beta1, beta2, beta3 = group[\'betas\']\n                k, K = group[\'k\'], group[\'K\']\n                d = state[\'d\']\n                state[\'step\'] += 1\n                t = state[\'step\']\n                # initialization of \\hat{f}_1\n                if t == 1:\n                    # \\hat{f}_1 = f_1\n                    state[\'ft_1\'] = _loss\n                # \\hat{f_{t-1}}, \\hat{f_{t-2}}\n                ft_1, ft_2 = state[\'ft_1\'], state[\'ft_2\']\n                # f(\\theta_{t-1})\n                f = _loss\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                m_t.mul_(beta1).add_(1 - beta1, grad)\n                v_t.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                m_t_hat = m_t / (1 - beta1 ** t)\n                v_t_hat = v_t / (1 - beta2 ** t)\n\n                if t > 1:\n                    if f >= state[\'ft_2\']:\n                        delta = k + 1\n                        Delta = K + 1\n                    else:\n                        delta = 1 / (K + 1)\n                        Delta = 1 / (k + 1)\n\n                    c = min(max(delta, f / ft_2), Delta)\n                    r = abs(c - 1) / min(c, 1)\n                    state[\'ft_1\'], state[\'ft_2\'] = c * ft_2, ft_1\n                    state[\'d\'] = beta3 * d + (1 - beta3) * r\n\n                # update parameters\n                p.data.addcdiv_(-group[\'lr\'] / state[\'d\'],\n                                m_t_hat,\n                                v_t_hat.sqrt().add_(group[\'eps\']))\n\n        return loss\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/__init__.py,0,b'from .densenet import *\nfrom .googlenet import *\nfrom .lenet import *\nfrom .minidensenet import *\nfrom .resnext import *\nfrom .senet import *\nfrom .vgg import *\nfrom .wrn import *\nfrom .simplenet import *\nfrom .unet import *\nfrom .linknet import *\n'
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/densenet.py,5,"b'\'\'\'DenseNet in PyTorch.\'\'\'\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'densnetXX_generic\', \'DenseNet169\', \'DenseNet121\']\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n\n        self.mp = torch.nn.MaxPool2d(1, 1)\n        # self.avgpool = torch.nn.AvgPool2d(2,2)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n\n        out = self.conv2(F.relu(self.bn2(out)))\n        # out = self.mp(out)\n        # out = self.avgpool(out)\n\n        # print (x.data.shape)\n\n        out = torch.cat([out, x], 1)\n        out = self.mp(out)\n        # out = self.avgpool(out)\n        # print(out.data.shape)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=1, n_dim=3):\n        super(DenseNet, self).__init__()\n        self.growth_rate = growth_rate\n\n        num_planes = 2 * growth_rate\n        self.conv1 = nn.Conv2d(n_dim, num_planes, kernel_size=3, padding=1, bias=False)\n\n        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n        num_planes += nblocks[0] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n        num_planes += nblocks[1] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n        num_planes += nblocks[2] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n        num_planes += nblocks[3] * growth_rate\n\n        self.bn = nn.BatchNorm2d(num_planes)\n\n        self.linear = nn.Linear(448, num_classes)\n        self.sig = nn.Sigmoid()\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n        out = out.view(out.size(0), -1)\n        # print (out.data.shape)\n        out = self.linear(out)\n        out = self.sig(out)\n\n        return out\n\n\ndef densnet(**kwargs):\n    """"""\n    Constructs a ResNet model.\n    """"""\n    return DenseNet(**kwargs)\n\n\n# block, nblocks, growth_rate=12, reduction=0.5, num_classes=1, n_dim=3\ndef densnetXX_generic(num_classes, n_dim):\n    model = DenseNet(Bottleneck, [6, 12, 24, 12], growth_rate=4, num_classes=num_classes, n_dim=n_dim)  # 56\n    return model\n\n\ndef DenseNet121():\n    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n\n\ndef DenseNet169():\n    return DenseNet(Bottleneck, [6, 12, 32, 32], growth_rate=16)\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/googlenet.py,3,"b""'''GoogLeNet with PyTorch.'''\nimport torch\nimport torch.nn as nn\n\n\nclass Inception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n        super(Inception, self).__init__()\n        # 1x1 conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 3x3 conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 5x5 conv branch\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n\n        # 3x3 pool -> 1x1 conv branch\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        return torch.cat([y1, y2, y3, y4], 1)\n\n\nclass GoogLeNet(nn.Module):\n    def __init__(self):\n        super(GoogLeNet, self).__init__()\n        self.pre_layers = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(True),\n        )\n\n        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.linear = nn.Linear(1024, 10)\n\n    def forward(self, x):\n        out = self.pre_layers(x)\n        out = self.a3(out)\n        out = self.b3(out)\n        out = self.maxpool(out)\n        out = self.a4(out)\n        out = self.b4(out)\n        out = self.c4(out)\n        out = self.d4(out)\n        out = self.e4(out)\n        out = self.maxpool(out)\n        out = self.a5(out)\n        out = self.b5(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n# net = GoogLeNet()\n# x = torch.randn(1,3,32,32)\n# y = net(Variable(x))\n# print(y.size())\n"""
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/lenet.py,2,"b""'''LeNet in PyTorch.'''\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = ['lenetXX_generic']\n\n\nclass LeNet(nn.Module):\n    def __init__(self, num_classes=12, num_rgb=3):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(num_rgb, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(44944, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)\n\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        # print(out.data.size())\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = out.view(out.size(0), -1)\n        # print(out.data.size())\n        out = self.fc3(out)\n        # out = self.sig(out)\n        return out\n\n\ndef lenetXX_generic(num_classes, num_rgb):\n    model = LeNet(num_classes, num_rgb)  # 56\n    return model\n"""
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/linknet.py,4,"b'import torch.nn as nn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass conv2DBatchNorm(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size,  stride, padding, bias=True):\n        super(conv2DBatchNorm, self).__init__()\n\n        self.cb_unit = nn.Sequential(nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                               padding=padding, stride=stride, bias=bias),\n                                 nn.BatchNorm2d(int(n_filters)),)\n\n    def forward(self, inputs):\n        outputs = self.cb_unit(inputs)\n        return outputs\n\n\nclass deconv2DBatchNorm(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size,  stride, padding, bias=True):\n        super(deconv2DBatchNorm, self).__init__()\n\n        self.dcb_unit = nn.Sequential(nn.ConvTranspose2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                               padding=padding, stride=stride, bias=bias),\n                                 nn.BatchNorm2d(int(n_filters)),)\n\n    def forward(self, inputs):\n        outputs = self.dcb_unit(inputs)\n        return outputs\n\n\nclass conv2DBatchNormRelu(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size,  stride, padding, bias=True):\n        super(conv2DBatchNormRelu, self).__init__()\n\n        self.cbr_unit = nn.Sequential(nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                                padding=padding, stride=stride, bias=bias),\n                                 nn.BatchNorm2d(int(n_filters)),\n                                 nn.ReLU(inplace=True),)\n\n    def forward(self, inputs):\n        outputs = self.cbr_unit(inputs)\n        return outputs\n\n\nclass deconv2DBatchNormRelu(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True):\n        super(deconv2DBatchNormRelu, self).__init__()\n\n        self.dcbr_unit = nn.Sequential(nn.ConvTranspose2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                                padding=padding, stride=stride, bias=bias),\n                                 nn.BatchNorm2d(int(n_filters)),\n                                 nn.ReLU(inplace=True),)\n\n    def forward(self, inputs):\n        outputs = self.dcbr_unit(inputs)\n        return outputs\n\n\nclass unetConv2(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm):\n        super(unetConv2, self).__init__()\n\n        if is_batchnorm:\n            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, 3, 1, 0),\n                                       nn.BatchNorm2d(out_size),\n                                       nn.ReLU(),)\n            self.conv2 = nn.Sequential(nn.Conv2d(out_size, out_size, 3, 1, 0),\n                                       nn.BatchNorm2d(out_size),\n                                       nn.ReLU(),)\n        else:\n            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, 3, 1, 0),\n                                       nn.ReLU(),)\n            self.conv2 = nn.Sequential(nn.Conv2d(out_size, out_size, 3, 1, 0),\n                                       nn.ReLU(),)\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs\n\n\nclass unetUp(nn.Module):\n    def __init__(self, in_size, out_size, is_deconv):\n        super(unetUp, self).__init__()\n        self.conv = unetConv2(in_size, out_size, False)\n        if is_deconv:\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n        else:\n            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2]\n        outputs1 = F.pad(inputs1, padding)\n        return self.conv(torch.cat([outputs1, outputs2], 1))\n\n\nclass segnetDown2(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(segnetDown2, self).__init__()\n        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        unpooled_shape = outputs.size()\n        outputs, indices = self.maxpool_with_argmax(outputs)\n        return outputs, indices, unpooled_shape\n\n\nclass segnetDown3(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(segnetDown3, self).__init__()\n        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n        self.conv3 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        outputs = self.conv3(outputs)\n        unpooled_shape = outputs.size()\n        outputs, indices = self.maxpool_with_argmax(outputs)\n        return outputs, indices, unpooled_shape\n\n\nclass segnetUp2(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(segnetUp2, self).__init__()\n        self.unpool = nn.MaxUnpool2d(2, 2)\n        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n\n    def forward(self, inputs, indices, output_shape):\n        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n        outputs = self.conv1(outputs)\n        outputs = self.conv2(outputs)\n        return outputs\n\n\nclass segnetUp3(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(segnetUp3, self).__init__()\n        self.unpool = nn.MaxUnpool2d(2, 2)\n        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n        self.conv3 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n\n    def forward(self, inputs, indices, output_shape):\n        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n        outputs = self.conv1(outputs)\n        outputs = self.conv2(outputs)\n        outputs = self.conv3(outputs)\n        return outputs\n\n\nclass residualBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, n_filters, stride=1, downsample=None):\n        super(residualBlock, self).__init__()\n\n        self.convbnrelu1 = conv2DBatchNormRelu(in_channels, n_filters, 3,  stride, 1, bias=False)\n        self.convbn2 = conv2DBatchNorm(n_filters, n_filters, 3, 1, 1, bias=False)\n        self.downsample = downsample\n        self.stride = stride\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.convbnrelu1(x)\n        out = self.convbn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass residualBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, n_filters, stride=1, downsample=None):\n        super(residualBottleneck, self).__init__()\n        self.convbn1 = nn.Conv2DBatchNorm(in_channels,  n_filters, k_size=1, bias=False)\n        self.convbn2 = nn.Conv2DBatchNorm(n_filters,  n_filters, k_size=3, padding=1, stride=stride, bias=False)\n        self.convbn3 = nn.Conv2DBatchNorm(n_filters,  n_filters * 4, k_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.convbn1(x)\n        out = self.convbn2(out)\n        out = self.convbn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass linknetUp(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super(linknetUp, self).__init__()\n\n        # B, 2C, H, W -> B, C/2, H, W\n        self.convbnrelu1 = conv2DBatchNormRelu(in_channels, n_filters/2, k_size=1, stride=1, padding=1)\n\n        # B, C/2, H, W -> B, C/2, H, W\n        self.deconvbnrelu2 = nn.deconv2DBatchNormRelu(n_filters/2, n_filters/2, k_size=3,  stride=2, padding=0,)\n\n        # B, C/2, H, W -> B, C, H, W\n        self.convbnrelu3 = conv2DBatchNormRelu(n_filters/2, n_filters, k_size=1, stride=1, padding=1)\n\n    def forward(self, x):\n        x = self.convbnrelu1(x)\n        x = self.deconvbnrelu2(x)\n        x = self.convbnrelu3(x)\n        return x\n\nclass LinkNet(nn.Module):\n\n    def __init__(self, feature_scale=4, n_classes=1, is_deconv=True, in_channels=2, is_batchnorm=True):\n        super(LinkNet, self).__init__()\n        self.is_deconv = is_deconv\n        self.in_channels = in_channels\n        self.is_batchnorm = is_batchnorm\n        self.feature_scale = feature_scale\n        self.layers = [2, 2, 2, 2] # Currently hardcoded for ResNet-18\n\n        filters = [64, 128, 256, 512]\n        filters = [x / self.feature_scale for x in filters]\n\n        self.inplanes = filters[0]\n\n\n        # Encoder\n        self.convbnrelu1 = conv2DBatchNormRelu(in_channels=3, k_size=7, n_filters=64,\n                                               padding=3, stride=2, bias=False)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        block = residualBlock\n        self.encoder1 = self._make_layer(block, filters[0], self.layers[0])\n        self.encoder2 = self._make_layer(block, filters[1], self.layers[1], stride=2)\n        self.encoder3 = self._make_layer(block, filters[2], self.layers[2], stride=2)\n        self.encoder4 = self._make_layer(block, filters[3], self.layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n\n\n        # Decoder\n        self.decoder4 = linknetUp(filters[3], filters[2])\n        self.decoder4 = linknetUp(filters[2], filters[1])\n        self.decoder4 = linknetUp(filters[1], filters[0])\n        self.decoder4 = linknetUp(filters[0], filters[0])\n\n        # Final Classifier\n        self.finaldeconvbnrelu1 = nn.Sequential(nn.ConvTranspose2d(int(filters[0]), int(32/feature_scale), 3, 2, 1),\n                                      nn.BatchNorm2d(32/feature_scale),\n                                      nn.ReLU(inplace=True),)\n        self.finalconvbnrelu2 = conv2DBatchNormRelu(in_channels=int(32/feature_scale), k_size=3, n_filters=int(32/feature_scale), padding=1, stride=1)\n        self.finalconv3 = nn.Conv2d(int(32/feature_scale), n_classes, 2, 2, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                                 kernel_size=1, stride=stride, bias=False),\n                                       nn.BatchNorm2d(planes * block.expansion),)\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        # Encoder\n        x = self.convbnrelu1(x)\n        x = self.maxpool(x)\n\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4)\n        d4 += e3\n        d3 = self.decoder3(d4)\n        d3 += e2\n        d2 = self.decoder2(d3)\n        d2 += e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconvbnrelu1(d1)\n        f2 = self.finalconvbnrelu2(f1)\n        f3 = self.finalconv3(f2)\n\n        return f3 \n\ndef linknetXX_generic(num_classes, num_rgb):\n    model = LinkNet(n_classes=num_classes, in_channels=num_rgb)  # 56\n    return model\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/minidensenet.py,4,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'minidensnetXX_generic\']\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, nChannels, growthRate):\n        super(Bottleneck, self).__init__()\n        interChannels = 4 * growthRate\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(interChannels)\n        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat((x, out), 1)\n        return out\n\n\nclass SingleLayer(nn.Module):\n    def __init__(self, nChannels, growthRate):\n        super(SingleLayer, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = torch.cat((x, out), 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, nChannels, nOutChannels):\n        super(Transition, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n                               bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass MiniDenseNet(nn.Module):\n    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck, n_dim):\n        super(MiniDenseNet, self).__init__()\n\n        nDenseBlocks = (depth - 4) // 3\n        if bottleneck:\n            nDenseBlocks //= 2\n\n        nChannels = 2 * growthRate\n        self.conv1 = nn.Conv2d(n_dim, nChannels, kernel_size=3, padding=1, bias=False)\n        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks * growthRate\n        nOutChannels = int(math.floor(nChannels * reduction))\n        self.trans1 = Transition(nChannels, nOutChannels)\n\n        nChannels = nOutChannels\n        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks * growthRate\n        nOutChannels = int(math.floor(nChannels * reduction))\n        self.trans2 = Transition(nChannels, nOutChannels)\n\n        nChannels = nOutChannels\n        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks * growthRate\n\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        if bottleneck == False:\n            self.fc = nn.Linear(768, nClasses)\n        else:\n            self.fc = nn.Linear(432, nClasses)\n\n        self.sig = nn.Sigmoid()\n        self.num_classes =nClasses\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n        layers = []\n        for i in range(int(nDenseBlocks)):\n            if bottleneck:\n                layers.append(Bottleneck(nChannels, growthRate))\n            else:\n                layers.append(SingleLayer(nChannels, growthRate))\n            nChannels += growthRate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.dense3(out)\n        # print(out.data.shape)\n        out = F.avg_pool2d(F.relu(self.bn1(out)), 8)\n        out = out.view(out.size(0), -1)\n        print(out.data.shape)\n        # out = self.fc(out)\n        if self.num_classes == 1:  # BCE Loss,\n            out = self.sig(out)\n        return out\n\n\n# model = MiniDenseNet(growthRate=8, depth=20, reduction=0.5,bottleneck=True, nClasses=1)\n\n\n# def densnet(**kwargs):\n#     """"""\n#     Constructs a ResNet model.\n#     """"""\n#     return MiniDenseNet(**kwargs)\n\n\ndef minidensnetXX_generic(num_classes, n_dim):\n    model = MiniDenseNet(growthRate=48, depth=20, reduction=0.5, bottleneck=True, nClasses=num_classes, n_dim=n_dim)\n    return model\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/resnext.py,3,"b'import math\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\n__all__ = [\'resnext\', \'resnetxtXX_generic\']\n\n\nclass ResNeXtBottleneck(nn.Module):\n    expansion = 4\n    """"""\n    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n    """"""\n\n    def __init__(self, inplanes, planes, cardinality, base_width, stride=1, downsample=None):\n        super(ResNeXtBottleneck, self).__init__()\n\n        D = int(math.floor(planes * (base_width / 64.0)))\n        C = cardinality\n\n        self.conv_reduce = nn.Conv2d(inplanes, D * C, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_reduce = nn.BatchNorm2d(D * C)\n\n        self.conv_conv = nn.Conv2d(D * C, D * C, kernel_size=3, stride=stride, padding=1, groups=cardinality,\n                                   bias=False)\n        self.bn = nn.BatchNorm2d(D * C)\n\n        self.conv_expand = nn.Conv2d(D * C, planes * 4, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_expand = nn.BatchNorm2d(planes * 4)\n\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        bottleneck = self.conv_reduce(x)\n        bottleneck = F.relu(self.bn_reduce(bottleneck), inplace=True)\n\n        bottleneck = self.conv_conv(bottleneck)\n        bottleneck = F.relu(self.bn(bottleneck), inplace=True)\n\n        bottleneck = self.conv_expand(bottleneck)\n        bottleneck = self.bn_expand(bottleneck)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        return F.relu(residual + bottleneck, inplace=True)\n\n\nclass GenericResNeXt(nn.Module):\n    """"""\n    ResNext optimized for the Cifar dataset, as specified in\n    https://arxiv.org/pdf/1611.05431.pdf\n    """"""\n\n    def __init__(self, block, depth, cardinality, base_width, num_classes, n_dim):\n        super(GenericResNeXt, self).__init__()\n\n        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n        assert (depth - 2) % 9 == 0, \'depth should be one of 29, 38, 47, 56, 101\'\n        layer_blocks = (depth - 2) // 9\n\n        self.cardinality = cardinality\n        self.base_width = base_width\n        self.num_classes = num_classes\n        self.sig = nn.Sigmoid()\n\n        self.conv_1_3x3 = nn.Conv2d(n_dim, 64, 3, 1, 1, bias=False)\n        self.bn_1 = nn.BatchNorm2d(64)\n\n        self.inplanes = 64\n        self.stage_1 = self._make_layer(block, 64, layer_blocks, 1)\n        self.stage_2 = self._make_layer(block, 128, layer_blocks, 2)\n        self.stage_3 = self._make_layer(block, 256, layer_blocks, 2)\n        self.avgpool = nn.AvgPool2d(8)\n        self.classifier = nn.Linear(4096, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal(m.weight)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, self.cardinality, self.base_width, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, self.cardinality, self.base_width))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv_1_3x3(x)\n        x = F.relu(self.bn_1(x), inplace=True)\n        x = self.stage_1(x)\n        x = self.stage_2(x)\n        x = self.stage_3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        # print(x.data.shape)\n        x = self.classifier(x)\n        if self.num_classes == 1:  # BCE Loss,\n            x = self.sig(x)\n        return x\n\n\ndef resnext29_16_64(num_classes=1):\n    """"""Constructs a ResNeXt-29, 16*64d model for CIFAR-10 (by default)\n    \n    Args:\n      num_classes (uint): number of classes\n    """"""\n    model = GenericResNeXt(ResNeXtBottleneck, 29, 16, 64, num_classes)\n    return model\n\n\ndef resnetxtXX_generic(num_classes, n_dim):\n    # block, depth, cardinality, base_width\n    model = GenericResNeXt(ResNeXtBottleneck, 29, 4, 4, num_classes=num_classes, n_dim=n_dim)  # 56\n    return model\n\ndef resnext(**kwargs):\n    """"""\n    Constructs a ResNet model.\n    """"""\n    return GenericResNeXt(**kwargs)\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/senet.py,1,"b'\nimport math\n\nimport torch.nn as nn\nfrom torchvision.models import ResNet\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes * 4, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef se_resnet18(num_classes):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet34(num_classes):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet50(num_classes):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet101(num_classes):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet152(num_classes):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\nREDUCTION=16\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=REDUCTION):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, reduction),\n            # nn.ReLU(inplace=True),\n            nn.PReLU(),\n            nn.Linear(reduction, channel),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass IceSEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, reduction=REDUCTION):\n        super(IceSEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1,\n                                                  stride=1, bias=False),\n                                        nn.BatchNorm2d(planes))\n\n    def forward(self, x):\n        residual = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass IceResNet(nn.Module):\n    def __init__(self, block, n_size=1, num_classes=1, num_rgb=2, base=32):\n        super(IceResNet, self).__init__()\n        self.base = base\n        self.num_classes = num_classes\n        self.inplane = self.base  # 45 epochs\n        # self.inplane = 16 # 57 epochs\n        self.conv1 = nn.Conv2d(num_rgb, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, self.inplane, blocks=2 * n_size, stride=2)\n        self.layer2 = self._make_layer(block, self.inplane * 2, blocks=2 * n_size, stride=2)\n        self.layer3 = self._make_layer(block, self.inplane * 4, blocks=2 * n_size, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n        self.fc = nn.Linear(int(8 * self.base), num_classes)\n        nn.init.kaiming_normal(self.fc.weight)\n        self.sig = nn.Sigmoid()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride):\n\n        layers = []\n        for i in range(1, blocks):\n            layers.append(block(self.inplane, planes, stride))\n            self.inplane = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        # x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        # print (x.data.size())\n        x = self.fc(x)\n\n        if self.num_classes == 1:  # BCE Loss,\n            x = self.sig(x)\n        return x\n\n\ndef senet16_RGB_10_classes(num_classes=10, num_rgb=3):\n    model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, 16)  # 56\n    return model\n\n\ndef senet16_RG_1_classes(num_classes=1, num_rgb=2):\n    model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, 16)  # 56\n    return model\n\n\ndef senet32_RG_1_classes(num_classes=1, num_rgb=2):\n    model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, 32)  # 56\n    return model\n\ndef senet32_RGB_1_classes(num_classes=1, num_rgb=3):\n    model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, 32)  # 56\n    return model\n\ndef senet64_RG_1_classes(num_classes=1, num_rgb=2):\n    model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, 64)  # 56\n    return model\n\n\ndef senet128_RG_1_classes(num_classes=1, num_rgb=2):\n    model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, 128)  # 56\n    return model\n\ndef senetXX_generic(num_classes, num_rgb, base):\n    model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, base)  # 56\n    return model\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/simplenet.py,15,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport math\n\nuse_gpu = torch.cuda.is_available()\n\n\n# class SimpleNet(nn.Module):\n#     def __init__(self, num_classes=1, n_dim=3):\n#         super(SimpleNet, self).__init__()\n#         self.conv1 = nn.Conv2d(n_dim, 32, 3, stride=1)\n#         self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n#\n#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n#         self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n#         self.dense1 = nn.Linear(179776, out_features=512)\n#         self.dense1_bn = nn.BatchNorm1d(512)\n#         self.dense2 = nn.Linear(512, (num_classes))\n#\n#     def forward(self, x):\n#         x = F.relu(self.conv1(x))\n#         x = F.relu(F.dropout(F.max_pool2d(self.conv2(x), 2), 0.25))\n#         x = F.relu(self.conv3(x))\n#         x = F.relu(F.dropout(F.max_pool2d(self.conv4(x), 2), 0.25))\n#         x = x.view(x.size(0), -1)\n# #         print (x.data.shape)\n#         x = F.relu(self.dense1_bn(self.dense1(x)))\n#         x = x.view(x.size(0), -1)\n# #         print (x.data.shape)\n#         x = self.dense2(x)\n#\n#         return x\n\ndropout = torch.nn.Dropout(p=0.30)\nrelu=torch.nn.LeakyReLU()\npool = nn.MaxPool2d(2, 2)\n\n\nclass ConvRes(nn.Module):\n    def __init__(self, insize, outsize):\n        super(ConvRes, self).__init__()\n        drate = .3\n        self.math = nn.Sequential(\n            nn.BatchNorm2d(insize),\n            nn.Dropout(drate),\n            torch.nn.Conv2d(insize, outsize, kernel_size=2, padding=2),\n            nn.PReLU(),\n        )\n\n    def forward(self, x):\n        return self.math(x)\n\n\nclass ConvCNN(nn.Module):\n    def __init__(self, insize, outsize, kernel_size=7, padding=2, pool=2, avg=True):\n        super(ConvCNN, self).__init__()\n        self.avg = avg\n        self.math = torch.nn.Sequential(\n            torch.nn.Conv2d(insize, outsize, kernel_size=kernel_size, padding=padding),\n            torch.nn.BatchNorm2d(outsize),\n            torch.nn.LeakyReLU(),\n            torch.nn.MaxPool2d(pool, pool),\n        )\n        self.avgpool = torch.nn.AvgPool2d(pool, pool)\n\n    def forward(self, x):\n        x = self.math(x)\n        if self.avg is True:\n            x = self.avgpool(x)\n        return x\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self,num_classes, n_dim):\n        super(SimpleNet, self).__init__()\n        self.num_classes=num_classes\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n        self.cnn1 = ConvCNN (n_dim,32,  kernel_size=7, pool=4, avg=False)\n        self.cnn2 = ConvCNN (32,32, kernel_size=5, pool=2, avg=True)\n        self.cnn3 = ConvCNN (32,32, kernel_size=5, pool=2, avg=True)\n\n        self.res1 = ConvRes (32,64)\n\n        self.features = nn.Sequential(\n            self.cnn1, dropout,\n            self.cnn2,\n            self.cnn3,\n            self.res1,\n        )\n\n        self.classifier = torch.nn.Sequential(\n            nn.Linear(1024, (num_classes)),\n        )\n\n        self.sig=nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        # print (x.data.shape)\n        x = self.classifier(x)\n        if (self.num_classes == 1):\n                x = self.sig(x)\n        return x\n\n    #         return F.log_softmax(x)\n\n\ndef simpleXX_generic(num_classes, imgDim):\n    # depth, num_classes = 1, widen_factor = 1, dropRate = 0.0\n    model = SimpleNet(num_classes=num_classes, n_dim=imgDim)  # 56\n    return model\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/unet.py,4,"b""\nimport math\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.nn as nn\nfrom torchvision.models import ResNet\n\n__all__ = ['unet',  'unetXX_generic']\n\nclass unetConv2(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm):\n        super(unetConv2, self).__init__()\n\n        if is_batchnorm:\n            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, 2, 1, 0),\n                                       nn.BatchNorm2d(out_size),\n                                       nn.ReLU(), )\n            self.conv2 = nn.Sequential(nn.Conv2d(out_size, out_size, 2, 1, 0),\n                                       nn.BatchNorm2d(out_size),\n                                       nn.ReLU(), )\n        else:\n            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, 2, 1, 0),\n                                       nn.ReLU(), )\n            self.conv2 = nn.Sequential(nn.Conv2d(out_size, out_size, 2, 1, 0),\n                                       nn.ReLU(), )\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs\n\n\nclass unetUp(nn.Module):\n    def __init__(self, in_size, out_size, is_deconv):\n        super(unetUp, self).__init__()\n        self.conv = unetConv2(in_size, out_size, False)\n        if is_deconv:\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n        else:\n            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2]\n        outputs1 = F.pad(inputs1, padding)\n        return self.conv(torch.cat([outputs1, outputs2], 1))\n\n\nclass unet(nn.Module):\n    def __init__(self, feature_scale=2, n_classes=1, is_deconv=True, in_channels=3, is_batchnorm=True):\n        super(unet, self).__init__()\n        self.is_deconv = is_deconv\n        self.in_channels = in_channels\n        self.is_batchnorm = is_batchnorm\n        self.feature_scale = feature_scale\n\n        filters = [128, 128, 128, 128, 128]\n        filters = [int(x / self.feature_scale) for x in filters]\n\n        # downsampling\n        self.conv1 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv2 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv3 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n\n        # self.conv4 = unetConv2(filters[2], filters[2], self.is_batchnorm)\n        # self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.center = unetConv2(filters[2], filters[3], self.is_batchnorm)\n\n        # upsampling\n        # self.up_concat4 = unetUp(filters[4], filters[3], self.is_deconv)\n        self.up_concat3 = unetUp(filters[3], filters[2], self.is_deconv)\n        self.up_concat2 = unetUp(filters[2], filters[1], self.is_deconv)\n        self.up_concat1 = unetUp(filters[1], filters[0], self.is_deconv)\n\n        # final conv (without any concat)\n        self.final = nn.Conv2d(filters[0], n_classes, 1)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        maxpool3 = self.maxpool3(conv3)\n        print(maxpool3.data.shape)\n        # conv4 = self.conv4(maxpool3)\n        # print (conv4.data.shape)\n        # maxpool4 = self.maxpool4(conv4)\n        # print (maxpool4.data.shape)\n        center = self.center(maxpool3)\n        print(center.data.shape)\n        # up4 = self.up_concat4(conv4, center)\n        up3 = self.up_concat3(conv3, center)\n        up2 = self.up_concat2(conv2, up3)\n        up1 = self.up_concat1(conv1, up2)\n\n        final = self.final(up1)\n        print(final.data.shape)\n        # final = F.Sigmoid(final)\n\n        return final\n\ndef unetXX_generic(n_classes, in_channels):\n    model = unet(n_classes=n_classes, in_channels=in_channels)\n    return model"""
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/vgg.py,3,"b""'''VGG11/13/16/19 in Pytorch.'''\nimport torch.nn as nn\n\n__all__ = ['vggnetXX_generic']\n\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name, num_classes, num_rgb):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name], num_rgb)\n        self.num_classes=num_classes\n\n        self.classifier = nn.Linear(2048, num_classes)\n\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        # print (out.data.size())\n        out = self.classifier(out)\n        if (self.num_classes == 1):\n            out = self.sig(out)\n        return out\n\n    def _make_layers(self, cfg, num_rgb):\n        layers = []\n        in_channels = num_rgb\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n\n\n# net = VGG('VGG11')\n# x = torch.randn(2,3,32,32)\n# print(net(Variable(x)).size())\n\ndef vggnetXX_generic(num_classes, num_rgb,type='VGG16'):\n    model = VGG(type, num_classes, num_rgb)  # 56\n    return model\n"""
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/wideresnet.py,3,"b'# import math\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n#\n#\n# class BasicBlock(nn.Module):\n#     def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n#         super(BasicBlock, self).__init__()\n#         self.bn1 = nn.BatchNorm2d(in_planes)\n#         self.relu1 = nn.ReLU(inplace=True)\n#         self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n#                                padding=1, bias=False)\n#         self.bn2 = nn.BatchNorm2d(out_planes)\n#         self.relu2 = nn.ReLU(inplace=True)\n#         self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n#                                padding=1, bias=False)\n#         self.droprate = dropRate\n#         self.equalInOut = (in_planes == out_planes)\n#         self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n#                                padding=0, bias=False) or None\n#     def forward(self, x):\n#         if not self.equalInOut:\n#             x = self.relu1(self.bn1(x))\n#         else:\n#             out = self.relu1(self.bn1(x))\n#         out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n#         if self.droprate > 0:\n#             out = F.dropout(out, p=self.droprate, training=self.training)\n#         out = self.conv2(out)\n#         return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n#\n# class NetworkBlock(nn.Module):\n#     def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n#         super(NetworkBlock, self).__init__()\n#         self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n#     def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n#         layers = []\n#         for i in range(nb_layers):\n#             layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n#         return nn.Sequential(*layers)\n#     def forward(self, x):\n#         return self.layer(x)\n#\n# class WideResNet(nn.Module):\n#     def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n#         super(WideResNet, self).__init__()\n#         nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n#         assert((depth - 4) % 6 == 0)\n#         n = (depth - 4) / 6\n#         block = BasicBlock\n#         # 1st conv before any network block\n#         self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n#                                padding=1, bias=False)\n#         # 1st block\n#         self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n#         # 2nd block\n#         self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n#         # 3rd block\n#         self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n#         # global average pooling and classifier\n#         self.bn1 = nn.BatchNorm2d(nChannels[3])\n#         self.relu = nn.ReLU(inplace=True)\n#         self.fc = nn.Linear(nChannels[3], num_classes)\n#         self.nChannels = nChannels[3]\n#\n#         self.sig = nn.Sigmoid()\n#\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n#             elif isinstance(m, nn.BatchNorm2d):\n#                 m.weight.data.fill_(1)\n#                 m.bias.data.zero_()\n#             elif isinstance(m, nn.Linear):\n#                 m.bias.data.zero_()\n#     def forward(self, x):\n#         out = self.conv1(x)\n#         out = self.block1(out)\n#         out = self.block2(out)\n#         out = self.block3(out)\n#         out = self.relu(self.bn1(out))\n#         out = F.avg_pool2d(out, 8)\n#         out = out.view(-1, self.nChannels)\n#         out = self.fc(out)\n#         if self.num_classes == 1:  # BCE Loss,\n#             out = self.sig(out)\n#         return out\n#\n# def wrnXX_generic(num_classes, num_rgb):\n#     model = WideResNet(3,10 num_rgb)\n#     model = IceResNet(IceSEBasicBlock, 1, num_classes, num_rgb, base)  # 56\n#     return model\n#\n#\n# def sesnet(**kwargs):\n#     """"""\n#     Constructs a ResNet model.\n#     """"""\n#     return ResNet(**kwargs)\n'"
Kaggle-PyTorch/PyTorch-Ensembler/nnmodels/wrn.py,3,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'wrn\', \'wrnXX_generic\']\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.PReLU()\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.PReLU()\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n        self.equalInOut = (in_planes == out_planes)\n        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n                                                                padding=0, bias=False) or None\n\n    def forward(self, x):\n        if not self.equalInOut:\n            x = self.relu1(self.bn1(x))\n        else:\n            out = self.relu1(self.bn1(x))\n        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        out = self.conv2(out)\n        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n\n\nclass NetworkBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n        super(NetworkBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n\n    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes=1, widen_factor=1, imgDim=1, dropRate=0.0):\n        super(WideResNet, self).__init__()\n        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n        assert (depth - 4) % 6 == 0, \'depth should be 6n+4\'\n        n = int((depth - 4) / 6)\n        block = BasicBlock\n        self.num_classes = num_classes\n        # 1st conv before any network block\n        self.conv1 = nn.Conv2d(imgDim, nChannels[0], kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n        # 2nd block\n        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n        # 3rd block\n        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(nChannels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(4096, self.num_classes)\n        self.nChannels = nChannels[3]\n        self.sig = nn.Sigmoid()\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.relu(self.bn1(out))\n        out = F.max_pool2d(out, 4)\n        # out = F.avg_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        # print(out.data.shape)\n        out = self.fc(out)\n\n        if self.num_classes == 1:  # BCE Loss,\n            out = self.sig(out)\n        return out\n\n\ndef wrnXX_generic(num_classes, imgDim):\n    # depth, num_classes = 1, widen_factor = 1, dropRate = 0.0\n    model = WideResNet(depth=10, num_classes=num_classes, widen_factor=4, imgDim=imgDim, dropRate=0.25)  # 56\n    return model\n\n\ndef wrn(**kwargs):\n    """"""\n    Constructs a Wide Residual Networks.\n    """"""\n    model = WideResNet(**kwargs)\n    return model\n'"
day05/KerasRLTutorial/game/flappy_bird_utils.py,0,"b'import pygame\nimport sys\ndef load():\n    # path of player with different states\n    PLAYER_PATH = (\n            \'assets/sprites/redbird-upflap.png\',\n            \'assets/sprites/redbird-midflap.png\',\n            \'assets/sprites/redbird-downflap.png\'\n    )\n\n    # path of background\n    BACKGROUND_PATH = \'assets/sprites/background-black.png\'\n\n    # path of pipe\n    PIPE_PATH = \'assets/sprites/pipe-green.png\'\n\n    IMAGES, SOUNDS, HITMASKS = {}, {}, {}\n\n    # numbers sprites for score display\n    IMAGES[\'numbers\'] = (\n        pygame.image.load(\'assets/sprites/0.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/1.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/2.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/3.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/4.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/5.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/6.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/7.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/8.png\').convert_alpha(),\n        pygame.image.load(\'assets/sprites/9.png\').convert_alpha()\n    )\n\n    # base (ground) sprite\n    IMAGES[\'base\'] = pygame.image.load(\'assets/sprites/base.png\').convert_alpha()\n\n    # sounds\n    if \'win\' in sys.platform:\n        soundExt = \'.wav\'\n    else:\n        soundExt = \'.ogg\'\n\n    SOUNDS[\'die\']    = pygame.mixer.Sound(\'assets/audio/die\' + soundExt)\n    SOUNDS[\'hit\']    = pygame.mixer.Sound(\'assets/audio/hit\' + soundExt)\n    SOUNDS[\'point\']  = pygame.mixer.Sound(\'assets/audio/point\' + soundExt)\n    SOUNDS[\'swoosh\'] = pygame.mixer.Sound(\'assets/audio/swoosh\' + soundExt)\n    SOUNDS[\'wing\']   = pygame.mixer.Sound(\'assets/audio/wing\' + soundExt)\n\n    # select random background sprites\n    IMAGES[\'background\'] = pygame.image.load(BACKGROUND_PATH).convert()\n\n    # select random player sprites\n    IMAGES[\'player\'] = (\n        pygame.image.load(PLAYER_PATH[0]).convert_alpha(),\n        pygame.image.load(PLAYER_PATH[1]).convert_alpha(),\n        pygame.image.load(PLAYER_PATH[2]).convert_alpha(),\n    )\n\n    # select random pipe sprites\n    IMAGES[\'pipe\'] = (\n        pygame.transform.rotate(\n            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n        pygame.image.load(PIPE_PATH).convert_alpha(),\n    )\n\n    # hismask for pipes\n    HITMASKS[\'pipe\'] = (\n        getHitmask(IMAGES[\'pipe\'][0]),\n        getHitmask(IMAGES[\'pipe\'][1]),\n    )\n\n    # hitmask for player\n    HITMASKS[\'player\'] = (\n        getHitmask(IMAGES[\'player\'][0]),\n        getHitmask(IMAGES[\'player\'][1]),\n        getHitmask(IMAGES[\'player\'][2]),\n    )\n\n    return IMAGES, SOUNDS, HITMASKS\n\ndef getHitmask(image):\n    """"""returns a hitmask using an image\'s alpha.""""""\n    mask = []\n    for x in range(image.get_width()):\n        mask.append([])\n        for y in range(image.get_height()):\n            mask[x].append(bool(image.get_at((x,y))[3]))\n    return mask\n'"
day05/KerasRLTutorial/game/wrapped_flappy_bird.py,0,"b'import numpy as np\nimport sys\nimport random\nimport pygame\nimport flappy_bird_utils\nimport pygame.surfarray as surfarray\nfrom pygame.locals import *\nfrom itertools import cycle\n\nFPS = 30\nSCREENWIDTH  = 288\nSCREENHEIGHT = 512\n\npygame.init()\nFPSCLOCK = pygame.time.Clock()\nSCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\npygame.display.set_caption(\'Flappy Bird\')\n\nIMAGES, SOUNDS, HITMASKS = flappy_bird_utils.load()\nPIPEGAPSIZE = 100 # gap between upper and lower part of pipe\nBASEY = SCREENHEIGHT * 0.79\n\nPLAYER_WIDTH = IMAGES[\'player\'][0].get_width()\nPLAYER_HEIGHT = IMAGES[\'player\'][0].get_height()\nPIPE_WIDTH = IMAGES[\'pipe\'][0].get_width()\nPIPE_HEIGHT = IMAGES[\'pipe\'][0].get_height()\nBACKGROUND_WIDTH = IMAGES[\'background\'].get_width()\n\nPLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n\n\nclass GameState:\n    def __init__(self):\n        self.score = self.playerIndex = self.loopIter = 0\n        self.playerx = int(SCREENWIDTH * 0.2)\n        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n        self.basex = 0\n        self.baseShift = IMAGES[\'base\'].get_width() - BACKGROUND_WIDTH\n\n        newPipe1 = getRandomPipe()\n        newPipe2 = getRandomPipe()\n        self.upperPipes = [\n            {\'x\': SCREENWIDTH, \'y\': newPipe1[0][\'y\']},\n            {\'x\': SCREENWIDTH + (SCREENWIDTH / 2), \'y\': newPipe2[0][\'y\']},\n        ]\n        self.lowerPipes = [\n            {\'x\': SCREENWIDTH, \'y\': newPipe1[1][\'y\']},\n            {\'x\': SCREENWIDTH + (SCREENWIDTH / 2), \'y\': newPipe2[1][\'y\']},\n        ]\n\n        # player velocity, max velocity, downward accleration, accleration on flap\n        self.pipeVelX = -4\n        self.playerVelY    =  0    # player\'s velocity along Y, default same as playerFlapped\n        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n        self.playerAccY    =   1   # players downward accleration\n        self.playerFlapAcc =  -9   # players speed on flapping\n        self.playerFlapped = False # True when player flaps\n\n    def frame_step(self, input_actions):\n        pygame.event.pump()\n\n        reward = 0.1\n        terminal = False\n\n        if sum(input_actions) != 1:\n            raise ValueError(\'Multiple input actions!\')\n\n        # input_actions[0] == 1: do nothing\n        # input_actions[1] == 1: flap the bird\n        if input_actions[1] == 1:\n            if self.playery > -2 * PLAYER_HEIGHT:\n                self.playerVelY = self.playerFlapAcc\n                self.playerFlapped = True\n                #SOUNDS[\'wing\'].play()\n\n        # check for score\n        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n        for pipe in self.upperPipes:\n            pipeMidPos = pipe[\'x\'] + PIPE_WIDTH / 2\n            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n                self.score += 1\n                #SOUNDS[\'point\'].play()\n                reward = 1\n\n        # playerIndex basex change\n        if (self.loopIter + 1) % 3 == 0:\n            self.playerIndex = next(PLAYER_INDEX_GEN)\n        self.loopIter = (self.loopIter + 1) % 30\n        self.basex = -((-self.basex + 100) % self.baseShift)\n\n        # player\'s movement\n        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n            self.playerVelY += self.playerAccY\n        if self.playerFlapped:\n            self.playerFlapped = False\n        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n        if self.playery < 0:\n            self.playery = 0\n\n        # move pipes to left\n        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n            uPipe[\'x\'] += self.pipeVelX\n            lPipe[\'x\'] += self.pipeVelX\n\n        # add new pipe when first pipe is about to touch left of screen\n        if 0 < self.upperPipes[0][\'x\'] < 5:\n            newPipe = getRandomPipe()\n            self.upperPipes.append(newPipe[0])\n            self.lowerPipes.append(newPipe[1])\n\n        # remove first pipe if its out of the screen\n        if self.upperPipes[0][\'x\'] < -PIPE_WIDTH:\n            self.upperPipes.pop(0)\n            self.lowerPipes.pop(0)\n\n        # check if crash here\n        isCrash= checkCrash({\'x\': self.playerx, \'y\': self.playery,\n                             \'index\': self.playerIndex},\n                            self.upperPipes, self.lowerPipes)\n        if isCrash:\n            #SOUNDS[\'hit\'].play()\n            #SOUNDS[\'die\'].play()\n            terminal = True\n            self.__init__()\n            reward = -1\n\n        # draw sprites\n        SCREEN.blit(IMAGES[\'background\'], (0,0))\n\n        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n            SCREEN.blit(IMAGES[\'pipe\'][0], (uPipe[\'x\'], uPipe[\'y\']))\n            SCREEN.blit(IMAGES[\'pipe\'][1], (lPipe[\'x\'], lPipe[\'y\']))\n\n        SCREEN.blit(IMAGES[\'base\'], (self.basex, BASEY))\n        # print score so player overlaps the score\n        # showScore(self.score)\n        SCREEN.blit(IMAGES[\'player\'][self.playerIndex],\n                    (self.playerx, self.playery))\n\n        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n        pygame.display.update()\n        #print (""FPS"" , FPSCLOCK.get_fps())\n\tFPSCLOCK.tick(FPS)\n        #print self.upperPipes[0][\'y\'] + PIPE_HEIGHT - int(BASEY * 0.2)\n        return image_data, reward, terminal\n\ndef getRandomPipe():\n    """"""returns a randomly generated pipe""""""\n    # y of gap between upper and lower pipe\n    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n    index = random.randint(0, len(gapYs)-1)\n    gapY = gapYs[index]\n\n    gapY += int(BASEY * 0.2)\n    pipeX = SCREENWIDTH + 10\n\n    return [\n        {\'x\': pipeX, \'y\': gapY - PIPE_HEIGHT},  # upper pipe\n        {\'x\': pipeX, \'y\': gapY + PIPEGAPSIZE},  # lower pipe\n    ]\n\n\ndef showScore(score):\n    """"""displays score in center of screen""""""\n    scoreDigits = [int(x) for x in list(str(score))]\n    totalWidth = 0 # total width of all numbers to be printed\n\n    for digit in scoreDigits:\n        totalWidth += IMAGES[\'numbers\'][digit].get_width()\n\n    Xoffset = (SCREENWIDTH - totalWidth) / 2\n\n    for digit in scoreDigits:\n        SCREEN.blit(IMAGES[\'numbers\'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n        Xoffset += IMAGES[\'numbers\'][digit].get_width()\n\n\ndef checkCrash(player, upperPipes, lowerPipes):\n    """"""returns True if player collders with base or pipes.""""""\n    pi = player[\'index\']\n    player[\'w\'] = IMAGES[\'player\'][0].get_width()\n    player[\'h\'] = IMAGES[\'player\'][0].get_height()\n\n    # if player crashes into ground\n    if player[\'y\'] + player[\'h\'] >= BASEY - 1:\n        return True\n    else:\n\n        playerRect = pygame.Rect(player[\'x\'], player[\'y\'],\n                      player[\'w\'], player[\'h\'])\n\n        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n            # upper and lower pipe rects\n            uPipeRect = pygame.Rect(uPipe[\'x\'], uPipe[\'y\'], PIPE_WIDTH, PIPE_HEIGHT)\n            lPipeRect = pygame.Rect(lPipe[\'x\'], lPipe[\'y\'], PIPE_WIDTH, PIPE_HEIGHT)\n\n            # player and upper/lower pipe hitmasks\n            pHitMask = HITMASKS[\'player\'][pi]\n            uHitmask = HITMASKS[\'pipe\'][0]\n            lHitmask = HITMASKS[\'pipe\'][1]\n\n            # if bird collided with upipe or lpipe\n            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n\n            if uCollide or lCollide:\n                return True\n\n    return False\n\ndef pixelCollision(rect1, rect2, hitmask1, hitmask2):\n    """"""Checks if two objects collide and not just their rects""""""\n    rect = rect1.clip(rect2)\n\n    if rect.width == 0 or rect.height == 0:\n        return False\n\n    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n\n    for x in range(rect.width):\n        for y in range(rect.height):\n            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n                return True\n    return False\n'"
