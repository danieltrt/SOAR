file_path,api_count,code
dla.py,12,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport math\nfrom os.path import join\n\nimport torch\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\n\nBatchNorm = nn.BatchNorm2d\n\n__all__ = [\'res2net_dla60\']\n\n\nmodel_urls = {\n    \'res2net_dla60\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net_dla60_4s-d88db7f9.pth\',\n    \'res2next_dla60\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2next_dla60_4s-d327927b.pth\',\n\n}\n\n\n\n\nclass Bottle2neck(nn.Module):\n    """"""\n    RexNeXt bottleneck type C\n    """"""\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, baseWidth=28, scale = 4):\n        """""" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: \'normal\': normal set. \'stage\': frist blokc of a new stage.\n        """"""\n        super(Bottle2neck, self).__init__()\n        if stride != 1:\n            stype = \'stage\'\n        else:\n            stype = \'normal\'\n        width = int(math.floor(planes * (baseWidth/128.0)))\n        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(width*scale)\n        \n        if scale == 1:\n          self.nums = 1\n        else:\n          self.nums = scale -1\n        if stype == \'stage\':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, \n                        padding=dilation, dilation=dilation, bias=False))\n          bns.append(BatchNorm(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width*scale, planes, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.stype = stype\n        self.scale = scale\n        self.width  = width\n\n    def forward(self, x, residual=None):\n        if residual is None:\n            residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n          if i==0 or self.stype==\'stage\':\n            sp = spx[i]\n          else:\n            sp = sp + spx[i]\n          sp = self.convs[i](sp)\n          sp = self.relu(self.bns[i](sp))\n          if i==0:\n            out = sp\n          else:\n            out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype==\'normal\':\n          out = torch.cat((out, spx[self.nums]),1)\n        elif self.scale != 1 and self.stype==\'stage\':\n          out = torch.cat((out, self.pool(spx[self.nums])),1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottle2neckX(nn.Module):\n    """"""\n    RexNeXt bottleneck type C\n    """"""\n    expansion = 2\n    cardinality = 8\n    def __init__(self, inplanes, planes, stride=1, dilation=1, scale = 4):\n        """""" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: \'normal\': normal set. \'stage\': frist blokc of a new stage.\n        """"""\n        super(Bottle2neckX, self).__init__()\n        if stride != 1:\n            stype = \'stage\'\n        else:\n            stype = \'normal\'\n        cardinality =  Bottle2neckX.cardinality\n        width = bottle_planes = planes * cardinality // 32\n        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(width*scale)\n        \n        if scale == 1:\n          self.nums = 1\n        else:\n          self.nums = scale -1\n        if stype == \'stage\':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, \n                        padding=dilation, dilation=dilation, groups=cardinality, bias=False))\n          bns.append(BatchNorm(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width*scale, planes, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.stype = stype\n        self.scale = scale\n        self.width  = width\n\n    def forward(self, x, residual=None):\n        if residual is None:\n            residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n          if i==0 or self.stype==\'stage\':\n            sp = spx[i]\n          else:\n            sp = sp + spx[i]\n          sp = self.convs[i](sp)\n          sp = self.relu(self.bns[i](sp))\n          if i==0:\n            out = sp\n          else:\n            out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype==\'normal\':\n          out = torch.cat((out, spx[self.nums]),1)\n        elif self.scale != 1 and self.stype==\'stage\':\n          out = torch.cat((out, self.pool(spx[self.nums])),1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\n\nclass Root(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, residual):\n        super(Root, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, 1,\n            stride=1, bias=False, padding=(kernel_size - 1) // 2)\n        self.bn = BatchNorm(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.residual = residual\n\n    def forward(self, *x):\n        children = x\n        x = self.conv(torch.cat(x, 1))\n        x = self.bn(x)\n        if self.residual:\n            x += children[0]\n        x = self.relu(x)\n\n        return x\n\n\nclass Tree(nn.Module):\n    def __init__(self, levels, block, in_channels, out_channels, stride=1,\n                 level_root=False, root_dim=0, root_kernel_size=1,\n                 dilation=1, root_residual=False):\n        super(Tree, self).__init__()\n        if root_dim == 0:\n            root_dim = 2 * out_channels\n        if level_root:\n            root_dim += in_channels\n        if levels == 1:\n            self.tree1 = block(in_channels, out_channels, stride,\n                               dilation=dilation)\n            self.tree2 = block(out_channels, out_channels, 1,\n                               dilation=dilation)\n        else:\n            self.tree1 = Tree(levels - 1, block, in_channels, out_channels,\n                              stride, root_dim=0,\n                              root_kernel_size=root_kernel_size,\n                              dilation=dilation, root_residual=root_residual)\n            self.tree2 = Tree(levels - 1, block, out_channels, out_channels,\n                              root_dim=root_dim + out_channels,\n                              root_kernel_size=root_kernel_size,\n                              dilation=dilation, root_residual=root_residual)\n        if levels == 1:\n            self.root = Root(root_dim, out_channels, root_kernel_size,\n                             root_residual)\n        self.level_root = level_root\n        self.root_dim = root_dim\n        self.downsample = None\n        self.project = None\n        self.levels = levels\n        if stride > 1:\n            self.downsample = nn.MaxPool2d(stride, stride=stride)\n        if in_channels != out_channels:\n            self.project = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels,\n                          kernel_size=1, stride=1, bias=False),\n                BatchNorm(out_channels)\n            )\n\n    def forward(self, x, residual=None, children=None):\n        children = [] if children is None else children\n        bottom = self.downsample(x) if self.downsample else x\n        residual = self.project(bottom) if self.project else bottom\n        if self.level_root:\n            children.append(bottom)\n        x1 = self.tree1(x, residual)\n        if self.levels == 1:\n            x2 = self.tree2(x1)\n            x = self.root(x2, x1, *children)\n        else:\n            children.append(x1)\n            x = self.tree2(x1, children=children)\n        return x\n\n\nclass DLA(nn.Module):\n    def __init__(self, levels, channels, num_classes=1000,\n                 block=Bottle2neck, residual_root=False, return_levels=False,\n                 pool_size=7, linear_root=False):\n        super(DLA, self).__init__()\n        self.channels = channels\n        self.return_levels = return_levels\n        self.num_classes = num_classes\n        self.base_layer = nn.Sequential(\n            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n                      padding=3, bias=False),\n            BatchNorm(channels[0]),\n            nn.ReLU(inplace=True))\n        self.level0 = self._make_conv_level(\n            channels[0], channels[0], levels[0])\n        self.level1 = self._make_conv_level(\n            channels[0], channels[1], levels[1], stride=2)\n        self.level2 = Tree(levels[2], block, channels[1], channels[2], 2,\n                           level_root=False,\n                           root_residual=residual_root)\n        self.level3 = Tree(levels[3], block, channels[2], channels[3], 2,\n                           level_root=True, root_residual=residual_root)\n        self.level4 = Tree(levels[4], block, channels[3], channels[4], 2,\n                           level_root=True, root_residual=residual_root)\n        self.level5 = Tree(levels[5], block, channels[4], channels[5], 2,\n                           level_root=True, root_residual=residual_root)\n\n        self.avgpool = nn.AvgPool2d(pool_size)\n        self.fc = nn.Conv2d(channels[-1], num_classes, kernel_size=1,\n                            stride=1, padding=0, bias=True)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_level(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes:\n            downsample = nn.Sequential(\n                nn.MaxPool2d(stride, stride=stride),\n                nn.Conv2d(inplanes, planes,\n                          kernel_size=1, stride=1, bias=False),\n                BatchNorm(planes),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample=downsample))\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(inplanes, planes, kernel_size=3,\n                          stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                BatchNorm(planes),\n                nn.ReLU(inplace=True)])\n            inplanes = planes\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        y = []\n        x = self.base_layer(x)\n        for i in range(6):\n            x = getattr(self, \'level{}\'.format(i))(x)\n            y.append(x)\n        if self.return_levels:\n            return y\n        else:\n            x = self.avgpool(x)\n            x = self.fc(x)\n            x = x.view(x.size(0), -1)\n\n            return x\n\n    def load_pretrained_model(self, data_name, name):\n        assert data_name in dataset.__dict__, \\\n            \'No pretrained model for {}\'.format(data_name)\n        data = dataset.__dict__[data_name]\n        fc = self.fc\n        if self.num_classes != data.classes:\n            self.fc = nn.Conv2d(\n                self.channels[-1], data.classes,\n                kernel_size=1, stride=1, padding=0, bias=True)\n        try:\n            model_url = get_model_url(data, name)\n        except KeyError:\n            raise ValueError(\n                \'{} trained on {} does not exist.\'.format(data.name, name))\n        self.load_state_dict(model_zoo.load_url(model_url))\n        self.fc = fc\n\n\n\n\ndef res2net_dla60(pretrained=None, **kwargs):\n    Bottle2neck.expansion = 2\n    model = DLA([1, 1, 1, 2, 3, 1],\n                [16, 32, 128, 256, 512, 1024],\n                block=Bottle2neck, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net_dla60\']))\n    return model\n\ndef res2next_dla60(pretrained=None, **kwargs):\n    Bottle2neckX.expansion = 2\n    model = DLA([1, 1, 1, 2, 3, 1],\n                [16, 32, 128, 256, 512, 1024],\n                block=Bottle2neckX, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2next_dla60\']))\n    return model\n\n\nif __name__ == \'__main__\':\n    images = torch.rand(1, 3, 224, 224).cuda(0)\n    model = res2next_dla60(pretrained=True)\n    model = model.cuda(0)\n    print(model(images).size())\n'"
res2net.py,8,"b'\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport torch.nn.functional as F\n__all__ = [\'Res2Net\', \'res2net50\']\n\n\nmodel_urls = {\n    \'res2net50_26w_4s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_26w_4s-06e79181.pth\',\n    \'res2net50_48w_2s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_48w_2s-afed724a.pth\',\n    \'res2net50_14w_8s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_14w_8s-6527dddc.pth\',\n    \'res2net50_26w_6s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_26w_6s-19041792.pth\',\n    \'res2net50_26w_8s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_26w_8s-2c7c9f12.pth\',\n    \'res2net101_26w_4s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net101_26w_4s-02a759a1.pth\',\n}\n\n\nclass Bottle2neck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale = 4, stype=\'normal\'):\n        """""" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: \'normal\': normal set. \'stage\': first block of a new stage.\n        """"""\n        super(Bottle2neck, self).__init__()\n\n        width = int(math.floor(planes * (baseWidth/64.0)))\n        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width*scale)\n        \n        if scale == 1:\n          self.nums = 1\n        else:\n          self.nums = scale -1\n        if stype == \'stage\':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False))\n          bns.append(nn.BatchNorm2d(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width*scale, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stype = stype\n        self.scale = scale\n        self.width  = width\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n          if i==0 or self.stype==\'stage\':\n            sp = spx[i]\n          else:\n            sp = sp + spx[i]\n          sp = self.convs[i](sp)\n          sp = self.relu(self.bns[i](sp))\n          if i==0:\n            out = sp\n          else:\n            out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype==\'normal\':\n          out = torch.cat((out, spx[self.nums]),1)\n        elif self.scale != 1 and self.stype==\'stage\':\n          out = torch.cat((out, self.pool(spx[self.nums])),1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Res2Net(nn.Module):\n\n    def __init__(self, block, layers, baseWidth = 26, scale = 4, num_classes=1000):\n        self.inplanes = 64\n        super(Res2Net, self).__init__()\n        self.baseWidth = baseWidth\n        self.scale = scale\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample=downsample, \n                        stype=\'stage\', baseWidth = self.baseWidth, scale=self.scale))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, baseWidth = self.baseWidth, scale=self.scale))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef res2net50(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50 model.\n    Res2Net-50 refers to the Res2Net-50_26w_4s.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_26w_4s\']))\n    return model\n\ndef res2net50_26w_4s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_26w_4s\']))\n    return model\n\ndef res2net101_26w_4s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net101_26w_4s\']))\n    return model\n\ndef res2net50_26w_6s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 6, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_26w_6s\']))\n    return model\n\ndef res2net50_26w_8s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 8, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_26w_8s\']))\n    return model\n\ndef res2net50_48w_2s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_48w_2s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 48, scale = 2, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_48w_2s\']))\n    return model\n\ndef res2net50_14w_8s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_14w_8s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 14, scale = 8, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_14w_8s\']))\n    return model\n\n\n\nif __name__ == \'__main__\':\n    images = torch.rand(1, 3, 224, 224).cuda(0)\n    model = res2net101_26w_4s(pretrained=True)\n    model = model.cuda(0)\n    print(model(images).size())\n'"
res2net_v1b.py,8,"b'\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport torch.nn.functional as F\n__all__ = [\'Res2Net\', \'res2net50_v1b\', \'res2net101_v1b\']\n\n\nmodel_urls = {\n    \'res2net50_v1b_26w_4s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_v1b_26w_4s-3cf99910.pth\',\n    \'res2net101_v1b_26w_4s\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net101_v1b_26w_4s-0812c246.pth\',\n}\n\n\nclass Bottle2neck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale = 4, stype=\'normal\'):\n        """""" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: \'normal\': normal set. \'stage\': first block of a new stage.\n        """"""\n        super(Bottle2neck, self).__init__()\n\n        width = int(math.floor(planes * (baseWidth/64.0)))\n        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width*scale)\n        \n        if scale == 1:\n          self.nums = 1\n        else:\n          self.nums = scale -1\n        if stype == \'stage\':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False))\n          bns.append(nn.BatchNorm2d(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width*scale, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stype = stype\n        self.scale = scale\n        self.width  = width\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n          if i==0 or self.stype==\'stage\':\n            sp = spx[i]\n          else:\n            sp = sp + spx[i]\n          sp = self.convs[i](sp)\n          sp = self.relu(self.bns[i](sp))\n          if i==0:\n            out = sp\n          else:\n            out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype==\'normal\':\n          out = torch.cat((out, spx[self.nums]),1)\n        elif self.scale != 1 and self.stype==\'stage\':\n          out = torch.cat((out, self.pool(spx[self.nums])),1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Res2Net(nn.Module):\n\n    def __init__(self, block, layers, baseWidth = 26, scale = 4, num_classes=1000):\n        self.inplanes = 64\n        super(Res2Net, self).__init__()\n        self.baseWidth = baseWidth\n        self.scale = scale\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n        )\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.AvgPool2d(kernel_size=stride, stride=stride, \n                    ceil_mode=True, count_include_pad=False),\n                nn.Conv2d(self.inplanes, planes * block.expansion, \n                    kernel_size=1, stride=1, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample=downsample, \n                        stype=\'stage\', baseWidth = self.baseWidth, scale=self.scale))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, baseWidth = self.baseWidth, scale=self.scale))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef res2net50_v1b(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_v1b model.\n    Res2Net-50 refers to the Res2Net-50_v1b_26w_4s.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_v1b_26w_4s\']))\n    return model\n\ndef res2net101_v1b(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_v1b_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net101_v1b_26w_4s\']))\n    return model\n\ndef res2net50_v1b_26w_4s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_v1b_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net50_v1b_26w_4s\']))\n    return model\n\ndef res2net101_v1b_26w_4s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_v1b_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net101_v1b_26w_4s\']))\n    return model\n\ndef res2net152_v1b_26w_4s(pretrained=False, **kwargs):\n    """"""Constructs a Res2Net-50_v1b_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2Net(Bottle2neck, [3, 8, 36, 3], baseWidth = 26, scale = 4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2net152_v1b_26w_4s\']))\n    return model\n\n\n\n\n\nif __name__ == \'__main__\':\n    images = torch.rand(1, 3, 224, 224).cuda(0)\n    model = res2net50_v1b_26w_4s(pretrained=True)\n    model = model.cuda(0)\n    print(model(images).size())\n'"
res2next.py,9,"b'from __future__ import division\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport torch\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'res2next50\']\nmodel_urls = {\n    \'res2next50\': \'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2next50_4s-6ef7e7bf.pth\',\n}\n\nclass Bottle2neckX(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, baseWidth, cardinality, stride=1, downsample=None, scale = 4, stype=\'normal\'):\n        """""" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            baseWidth: base width.\n            cardinality: num of convolution groups.\n            stride: conv stride. Replaces pooling layer.\n            scale: number of scale.\n            type: \'normal\': normal set. \'stage\': frist blokc of a new stage.\n        """"""\n        super(Bottle2neckX, self).__init__()\n\n        D = int(math.floor(planes * (baseWidth/64.0)))\n        C = cardinality\n\n        self.conv1 = nn.Conv2d(inplanes, D*C*scale, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn1 = nn.BatchNorm2d(D*C*scale)\n\n        if scale == 1:\n          self.nums = 1\n        else:\n          self.nums = scale -1\n        if stype == \'stage\':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n          convs.append(nn.Conv2d(D*C, D*C, kernel_size=3, stride = stride, padding=1, groups=C, bias=False))\n          bns.append(nn.BatchNorm2d(D*C))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(D*C*scale, planes * 4, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.downsample = downsample\n        self.width  = D*C\n        self.stype = stype\n        self.scale = scale\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n          if i==0 or self.stype==\'stage\':\n            sp = spx[i]\n          else:\n            sp = sp + spx[i]\n          sp = self.convs[i](sp)\n          sp = self.relu(self.bns[i](sp))\n          if i==0:\n            out = sp\n          else:\n            out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype==\'normal\':\n          out = torch.cat((out, spx[self.nums]),1)\n        elif self.scale != 1 and self.stype==\'stage\':\n          out = torch.cat((out, self.pool(spx[self.nums])),1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Res2NeXt(nn.Module):\n    def __init__(self, block, baseWidth, cardinality, layers, num_classes, scale=4):\n        """""" Constructor\n        Args:\n            baseWidth: baseWidth for ResNeXt.\n            cardinality: number of convolution groups.\n            layers: config of layers, e.g., [3, 4, 6, 3]\n            num_classes: number of classes\n            scale: scale in res2net\n        """"""\n        super(Res2NeXt, self).__init__()\n\n        self.cardinality = cardinality\n        self.baseWidth = baseWidth\n        self.num_classes = num_classes\n        self.inplanes = 64\n        self.output_size = 64\n        self.scale = scale\n\n        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], 2)\n        self.layer3 = self._make_layer(block, 256, layers[2], 2)\n        self.layer4 = self._make_layer(block, 512, layers[3], 2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)  \n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, self.baseWidth, self.cardinality, stride, downsample, scale=self.scale, stype=\'stage\'))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, self.baseWidth, self.cardinality, scale=self.scale))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\ndef res2next50(pretrained=False, **kwargs):\n    """"""    Construct Res2NeXt-50.\n    The default scale is 4.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = Res2NeXt(Bottle2neckX, layers = [3, 4, 6, 3], baseWidth = 4, cardinality=8, scale = 4, num_classes=1000)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'res2next50\']))\n    return model\n\nif __name__ == \'__main__\':\n    images = torch.rand(1, 3, 224, 224).cuda(0)\n    model = res2next50(pretrained=True)\n    model = model.cuda(0)\n    print(model(images).size())\n'"
